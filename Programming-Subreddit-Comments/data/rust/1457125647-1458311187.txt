Ah, thanks so much! :D (I used `rustfmt`/`cargo fmt`, so the code's prettiness isn't *entirely* my doing, heh.) My main reasons for choosing JSON were 1. personal bias, as I like JSON better :P 2. better compatibility with non-Rust languages (AFAIK), and 3. not knowing about the `toml` crate. Oops. The whole point of the library is to abstract these details out anyway, so it's not *super* relevant, but perhaps TOML would have been more Rustic. Ah well. I'm eventually hoping to support build targets other than `macos`/`unix`/`windows`, but I'll have to look into what the conventions are for the new platforms. I also wish there were an easy way to make this work on non-nightly Rust (arg, `serde`!), but I haven't had good results playing with `syntex` and `build.rs`. I'll give it another go later.
I think Swift's classes are more like Rust's trait objects.
Since Swift only has Reference Counting, how does it deal with cycles? Or are you required to do some kind of special annotation for weak pointers?
RAII: [Resource Acquisition Is Initialization](https://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization)
As /u/vks_ says, Swift's classes are like sort of like `Arc&lt;Trait&gt;`: they're heap allocated, and therefore, reference counted. &gt; What are Swift classes that Rust structs aren't? Just because you can construct similar things in a language doesn't mean that they're the same thing. For example, you can write an object system in C, and then use it, but that doesn't mean C has classes. Swift's has a language construct, "class", which combines data + functionality into one thing. Rust keeps them seaparate. Yes, a struct with an `impl` block is kinda similar, in a sense, but also different.
Heap allocated and able to participate in inheritance hierarchies.
&gt; Rust does not require the programmer to type explicit deallocation calls, but rather statically determines when data is no longer referenced and inserts calls to the memory deallocation function at compile time. Or rather, Rust statically determines when the data is about to go out of scope and enforces that it's no longer referenced after that point. Subtle difference, but a source of a lot of lifetime confusion and the reason `rustc` doesn't need to solve the halting problem.
DISREGARD PARAGRAPH, SEE EDIT I'm really having trouble translating this C++ code to Rust. What I'm trying to do is print all the lines in a set of files using a pool of threads. I do not have any restrictions on the order the lines are printed (i.e., I don't need FIFO or LIFO semantics). My idea in C++ was to have a vector of open file streams, each with an associated mutex. Then, create a thread pool which continuously loops through the vector, trying to find a mutex which it can immediately get ownership of (try_lock() instead of lock()). Once it locks down a file stream, it checks if it's open, it reads a line from the file, checks for eof (closes the stream if eof), and unlocks the mutex. Then it prints the line read (if not eof and not closed) to stdout and continues. I didn't add a condition to make the threads stop for simplicity, so they just keep checking and finding closed streams at the end. Anyways, here is the C++ that I was able to code: http://pastebin.com/f7Wpj9nu, how do I translate this to Rust? This is how I thought I would do it in Rust: http://pastebin.com/BNuFP9H9, but it does not compile, and I've tried doing various fixes (adding ref mut's, etc) but I think I fundamentally do not understand the errors I'm making. The Rust code is a little different from the C++: instead of associated mutexes of type Mutex&lt;()&gt;, I made a Vec&lt;Mutex&lt;Option&lt;File&gt;&gt;&gt;: a closed file should be replaced with None, and I create a BufReader whenever a file is found to read a line. On a side not, this is likely very inefficient, and I should use a Vec&lt;Mutex&lt;Option&lt;BufReader&gt;&gt;&gt; instead (maybe?), and in general this design might not be the most effective for what I'm trying to do (probably better to just make a vector of Option&lt;BufReader&gt;, then use an atomic operation to swap the Option&lt;BufReader&gt; with a None, use the BufReader, then put it back into the vector after reading a line, but I would still like to know how to make a vector of mutexes and use them). Anyways, does anyone have any advice for me? Sorry this is so long :( If you do have any advice, I greatly appreciate it! EDIT: Ok, I think I finally got it. http://pastebin.com/50Bv6XDt Now my question is, is this idiomatic Rust code? Is there a better way to do the mem::replace mem::swap stuff?
I was referring to [this bug report](https://bugzilla.mozilla.org/show_bug.cgi?id=1252041). Admittedly a rather oblique reference.
Truth.
Good idea!
A small correction: in real time systems, predictability is much more important than speed, the code merely needs to be fast enough to meet all deadlines. Ada isn't really known for being extremely fast (it's not slow either), but it gives you the tools to produce code that will run predictably and allows the scheduling of tasks to be analysed. See the Ravenscar profile for more information (includes no dynamic memory allocation ever, all tasks must run at a known period using a never ending loop, all tasks are created at launch time, and IIRC the priority ceiling inheritance protocol is used for exclusive access to resources which completely eliminates deadlocks).
It's more intuition than hard facts, really. I think that there is a big overlap in people interested in ML and trying out Rust, namely those interested in trying out new stuff in general. I also feel like there currently aren't that many frameworks in Rust that can be picked up to really build things with them (or are perceived as such by a wider developer audience), and I think Leaf can open up a new niche there.
I'm making some stuff using SDL2, and I want to recreate the effect of GIMPs perspective tool. I want to take a texture, then stretch it so the corners of the texture line up with the points I've given. Like this: http://i.imgur.com/oDEchpF.png How would I go around making this?
Drop is tied to ownership though- a reference will still never affect where `drop` runs, even though the word "static" isn't quite true because of drop flags.
What is the idiomatic way to add flags in a function? Example: I have a function threshold(), which binarizes an image. Now there are multiple eays to binarize it, for example THRESH_BIN, THRESH_BIN_INV and THRESH_TOZERO. Would I have three seperate functions for this, or add and enum argument? (or is something like threshold::&lt;Method::ThreshBinInv&gt;() possible ?)
That would have been a benefit of toml, as it doesn't need serde. That said, perhaps [lib_json](https://github.com/valarauca/lib_json/) can get you some part of the way?
/r/playrust
I'm still working on the hook system for imag, my PIM suite for the command line. Besides that I'm working on some utility libraries which I want to be implemented before introducing the high level modules. After these are merged (they are not yet merged as my two friends are very slow with their reviews) I will start implementing the simple modules and if things work out as great as I expect them to be, I will start working on more complex modules. I already figured out how to integrate taskwarrior and I'm currently thinking about a way to integrate ledger-cli. Imag codebase can be found here: github.com/matthiasbeyer/imag
Can someone give me an example what could be done with a machine intelligence framework? PR maybe a (simple) use case? I'm not sure whether I understand what this is, actually.
I feel that a blog article is more useful for this type of example/documentation that has a limited... lifetime.
Congrats to you and your team! This is a huge benchmark for your work and hopefully something that helps put Rust even more strongly on the map. P.S. I'm a little bitter as now I'll have to wait a few more days before I post my write up about rusty-machine. I'm afraid it doesn't really stand up to this!
I just started working with Rust last week so this is perfect! It's only a 15 minute drive from my house too. Think there will be any talks for beginners? I can only work with Rust in my spare time so I'm sure progress will be slow.
Do you mean by wrapping it in a Mutex or something else?
 enum Option&lt;T&gt; { Some(T), LLVM }; ;)
What about the same content being pushed to another doc? edit: but anyway &gt; Including some but not others isn't a good look It looks just like examples. "it works on any language, for example, A, B and C works" &gt; Do we now have to make CI depend on all of those other languages too? It wouldn't be a bad idea.. but it's out of scope of Rustc itself.
I enjoyed reading the article, but it has a few misinformations In Swift structs can be passed to functions by reference, they aren't copied all the time. RC algorithms also have stop the world pauses when releasing big data structures that cause cascate of delete operations. They also can cause more cache misses than GC algorithms. However Swift also does ellision, thus reducing the amount of retain/relase. The author apparently refers to it, bur I understood it as something different. Java compilers also do stack allocation for objects, with the difference that one doesn't have control when it happens. 
Diverging functions are related to he concept of the bottom type https://en.m.wikipedia.org/wiki/Bottom_type#In_programming_languages
There's a good reason for glossing over drop flags, we're simply waiting for MIR to kill them. :P (And the nomicon does have a section on them, for those who need to know.)
On the [project I'm trying it with](https://github.com/jeremyletang/rust-sfml), the badge [doesn't render properly](http://i.imgur.com/UIncB2N.png). Not sure what the problem is. EDIT: I think the GitHub cache messes it up. Does the image have `Cache-Control: no-cache` in the http header? That might solve this problem.
https://github.com/github/markup/issues/224 might have something to do with it. Try setting `Cache-Control: no-cache` in the HTTP header.
~~clippy.bashy.io seems to be down~~
I actually wrote a JS utility that does that a while back ([here](http://killercup.github.io/grock/)). This might also work with Rust.
502 Bad Gateway - probably means the rust daemon has crashed and nginx was a proxy.
This is great! Would it be possible to allow custom features? [libpnet](https://github.com/libpnet/libpnet) has quite a lot of auto-generated code which triggers clippy lints - there are `#[allow]` attributes for the code, but they're only enabled with the `clippy` feature enabled.
Pool deallocation still requires the programmer to call `Unchecked_Deallocation()`, right? Stack "deallocation" is also considered safe. You have to use `'Unchecked_Access` before you can bypass the lifetime checks. Sorry, "accessibility level checks" :P. I felt like I understood the level checks at one point, but I forget details. It seems anonymous access types (`access T`) can be used as function parameters. In this case they carry dynamic lifetime information. In other cases, they don't. The Ada rationale document justifies this inconsistency by saying that programmers know access types are special :). You can probably hack around using named access types as generic parameters too. Voila, lifetime parameters. However this being Ada, I suspect it gets impractically long-winded if you try and write it like Rust code.
What if we picked just _one_ language as a "for instance" example? 2 or more, people will fight about their language not included, but just 1, we might get away with it. This would also avoid the problem of having a bazillion languages in CI. Just one is probably do-able.
Searching for "rust drain" with Google turns up the stable docs for vec::Drain as the first result for me, so it may just take some time before Google picks up on your intentions. It's not perfect, vec::Drain isn't really helpful (we want Vec::drain, note capitalization), but at least it's Rust documentation, right? DuckDuckGo though... yeah, drain repair tutorials, but bangs help with that. :)
I get "502 Bad Gateway" from nginx when clinking on your link :(
A bit off topic...but what kind of simulations? I have been working on a paper that has almost that exact topic - using rust for large scientific simulations in astrophysics. Perhaps we could discuss it sometime!
Sorry, guess it became too popular to quickly and crashed. It's back up now!
Oh! Nice!
This is not (yet) supported. But we are discussing on how to approach this. If you have any input, please let us know: https://github.com/ligthyear/clippy-service/issues/17 !
Small typo in the header: "*Annoucing*: Clippy Service", should be *Announcing*.
Clippy is a developer tool, so you don't need to ship it with your code. The README specifies how you can make clippy an optional feature of your crate. Just use that with multirust; and your crate stays on stable.
I'm getting 324 empty response error. :(
You get the result you observe because `RefCell` is not a pointer - it is just a storage place which allows one to modify its contents through a shared reference. When you clone a `RefCell`, you also clone the data inside it. In order to share a `RefCell` you can use either `&amp;` or `Rc`: let data = RefCell::new(p); let c1 = &amp;data; let c2 = &amp;data; c1.borrow_mut().x = 5; assert_eq!(c2.borrow().x, 5); Sometimes, however, it is unwieldy to store the `RefCell` once and pass references to it. Another way to share the same `RefCell` in multiple places is using `Rc`, a reference-counted smart pointer: let c1 = Rc::new(RefCell::new(p)) let c2 = c1.clone() c1.borrow_mut().x = 5; assert_eq!(c2.borrow().x, 5); This way the ownership over the `RefCell` is shared - its final lifetime is determined by the combined lifetime of all `Rc`s obtained via `clone()` from the initial one. Note that now `clone()` is invoked on `Rc`, cloning the pointer instead of the data inside it. 
I can't offer much help on many of those topics, but if you happen to want to discuss particle simulations in rust, shoot me a message! I could show you what I've got. 
Looks like `RefCell&lt;Trait&gt;` is a shortcut for `RefCell&lt;Trait + 'static&gt;` while you probably want `RefCell&lt;Trait + 'a&gt;`. You might also want to scrap the references and use `Rc`.
It works now, thanks! &gt; You might also want to scrap the references and use `Rc`. Would that also work for synchronizing changes? What is the difference between `RefCell` and `Rc`? (I didn't really understand from the docs and stackoverflow). As far as I understand, `Rc` cannot hold mutable data (does not provide interior mutability), which I would like to have.
Then what would I gain by wrapping my `RefCell` inside an `Rc` instead of storing a `&amp;RefCell`?
So basically I get to remove lifetime specifiers from everything? That's great. (I just added them everywhere).
I'm using an image trait `trait Image { type Pixel; // functions for accessing and manipulating individual Pixels }` and have a load of functions with an I: Image bound `fn foo&lt;I: Image&gt;(image: &amp;I) -&gt; I { // loop through pixels, updating each in turn }` For performance reasons I'd like to introduce something along the lines of `trait DirectImage: Image { fn row(&amp;self) -&gt; &amp;[Self::Pixel]; fn row_mut(&amp;mut self) -&gt; &amp;mut [Self::Pixel]; }` If I know I'm working with a DirectImage I can write a faster implementation of various functions. What are the options for allowing this? I could 1. Write two versions of every function: foo, foo_but_faster. This is sad for several obvious reasons. 2. Wait for impl specialisation to land, and provide a separate Fooable trait for every function I want to specialise, and write a default and specialised impl. In order that calling these functions would look the same as calling functions without a trait/specialisation, I'd then need to provide wrappers `fn foo&lt;I: Fooable&gt;(image: &amp;I) -&gt; I { image.foo() }` This looks like it would work, but adds quite a bit of boilerplate and wouldn't work on stable for the next several months at least. Are there any alternatives I've missed?
The Ariane 5 failure was the result of a string of errors, notably re-using a proof made within the bounds of flight parameters for the Ariane 4 without checking if the proof still holds within the flight parameters for Ariane 5 (which it didn't).
Thank you very much! That is much better, and thank you very much for taking the time to help me! It seems obvious to me now that you showed me, but I was really very lost beforehand!
This isn't a technical comparison, but I think important nonetheless: the biggest difference is probably that very few people are interested in learning or using Ada. Those that are interested in using Ada are mainly people that are already invested in the language.
Maybe you could extract the low-level functions into a `mod os_interface` and have a `cfg` directive that imports a different module that contains mocks if you're testing. Or just give up and call them integration tests.
Funny enough, it was that it was _too_ deterministic, most people I talked to expected it to be different each run, but that was very unlikely to happen. Your point still stands though.
What prevents using traits and implementing those traits? trait Fork { fn fork(&amp;self) -&gt; Result&lt;(), u8&gt;; } mod os { fn the_real_extern_fork_function() -&gt; Result&lt;(), u8&gt; { Ok(()) } struct OsProvidedFork; impl super::Fork for OsProvidedFork { fn fork(&amp;self) -&gt; Result&lt;(), u8&gt; { the_real_extern_fork_function() } } } mod test { struct FailingFork(u8); impl super::Fork for FailingFork { fn fork(&amp;self) -&gt; Result&lt;(), u8&gt; { Err(self.0) } } } fn main() { } This is how I've been approaching any type of dependency injection I've needed. I trust in monomorphization and the compiler to make the code as fast as it should be.
Being able to compile functions into stackless state-machine coroutines would make a LOT of things possible, including continuations (which is what you can use to implement very expressive backtracking). Think of it like this: you call a function which gives you a state machine. Then, whenever you want to "save" a checkpoint, you just clone the state machine at its current state. If you come across a "failure", just call the state machine with a new argument! It's like time travel.
I believe libgreen was the runtime, not the part concerned with code generation. I think the answer I'm looking for is related to static analysis done by rustc during compilation of procs. But I could be wrong.
Hi brson, I'm very honored that the Rust project lead responded to this post. I've read that thread before, and I was also keeping up with the Go language's redesigns, so it is entirely understandable to me why segmented stacks simply had to go. The reason why I asked this question was because I was thinking about the need for segmented stacks and it occurred to me that if you make the careful restriction that 1) procs be 100% cooperative and 2) all cooperation and inversion of control happen inside of the lexical scope, then it is very simple to allocate a stack only for the bindings that need to persist during a scheduler yield, as well as being able to compile the proc to a very simple state machine. Let me explain. Consider the following theoretical asynchronous function: proc do_it(r: Receiver&lt;i32&gt;) { let a = 2; let b = 5; let c = do_more(a, b); if condition() { } else loop { let d = r.recv(); some_fun(d); } } I imagine that under "old Rust", any one of do_more, condition, r.recv(), and some_fun could have triggered a scheduler yield! Obviously there is almost no cheap way to allocate stacks in advance and so segmented stacks are needed. However, imagine that all yields had to be lexical: proc do_it(r: AsyncReceiver&lt;i32&gt;) { let a = 2; let b = 5; let c = do_more(a, b); if condition() { ; } else loop { match r.try_recv() { Ok(d) =&gt; { some_fun(d); yield; }, Empty =&gt; { yield; continue; }, Closed =&gt; { break; } } } Not only do you have a finer control over when each green thread suspends (in this case, it's in the loop after a single receive), but we can determine the max size of the persistent stack we need: about 26 bytes, sans alignment. We know this because the functions that are called can't possibly trigger a yield, so their results can be stored on the worker thread's stack, while bindings like a, b, c, and r, and the procedure's state, can be stored in the procedure struct itself. I am of course assuming a compilation strategy that turns do_it into its own state machine, much like C#'s async/await or the stateful crate/plugin for Rust. My point in saying all this is that I respectfully disagree with you: I think segmented stacks are probably a dead end, and instead Rust should support an expressive coroutines/yield construct that traits can be implemented for. Stackless coroutines compiled as state machines can efficiently express everything from continuations, to generators, to green threads, to asynchronous code... So I think that's where the money is, so to speak.
Can you explain how this is relevant to Rust?
Exactly. I think that some level of integration tests will be irreplaceable (it tests something different than pure unit tests). That being said, I am liking some of the suggestions here.
I personally agree with everything you've said, but I've been waiting on `impl Trait` and MIR before beginning any serious work around generators. However, if you're okay with AST-level desugaring, only `impl Trait` is truly needed and can be substituted with boxed trait objects for now, which is what /u/erickt has been doing in his [stateful](https://github.com/erickt/stateful/) compiler plugin experiment ([blog post 1](https://www.reddit.com/r/rust/comments/42zcci/stateful_an_inprogress_experimental_syntax/), [blog post 2](https://www.reddit.com/r/rust/comments/44dvlu/stateful_a_plugin_for_generatorsasyncawaitetc/)).
I can't answer your question myself, but one of the top Google results for "rust cross compile" is: https://github.com/japaric/rust-cross/blob/master/README.md which looks quite complete.
May someone share an example of working with rawsockets in Rust? 
As an alternative to directly compiling to Windows, you could setup AppVeyor to do it for you. It will run on Windows VMs with proper toolchains installed, you'll have to install Rust, which you can look for many crates that have an `appveyor.yml` as examples.
In the bincode crate (https://github.com/TyOverby/bincode) there is a serialize_usize() used in src/serde/writer.rs, but I see no definition for it anywhere. I assume it's derived somehow, but I can't figure out how, could somebody explain to me how this works?
This is the real, only answer. Getting a working C toolchain is the hard part, pretty much universally. For Linux -&gt; Windows in particular, there's no good option unless mingw and its libs works well for you. IME trying to do some stuff with the DirectX SDK and needing to link against MSVC-generated code, no dice. If your needs are simpler, it can work.
On stable Rust, if I want to use `libc` in my tests, I do the following: #[cfg(test)] mod tests { use super::*; extern crate libc; … I'm also importing libc in `super`, but this now fails on Nightly. What's the proper pattern to use here?
In scientific code compile-time matrix dimensions for large matrices do matter, a lot. For this reason Eigen3 offers partial compile-time dimensions. Say you have a 10000x4 matrix, where 10000 might be a run-time dimension, but you _always_ have 4 columns. If you are doing row-wise operations, the fact that the number of columns is known at compile-time is the difference between LLVM's auto-vectorizer triggering or giving up. This easily results in speed-ups of 2-4x for this kind of code. Examples of applications where this is the case would be any application in which you need to solve a system of partial differential equations, where you typically have a very low, compile-time constant number of variables you solve for, but in a very large number of points: HFT, any kind of fluid, particles, solid simulations... just to name a few. Eigen3 even offers views with partial compile-time dimensions for, e.g., iterating over sub blocks of the matrices, and even when you don't know the exact run-time size, Eigen3 also offers compile-time upper-bounds on the matrix size. That's why the signature of most Eigen3 types have 6 template parameters: memory layout, `matrix&lt;#of rows, #of cols, max #of rows, #of cols, option bits&gt;`. At the end of the day the problem is LLVM auto-vectorizer, but the 3 ingredients you need for it to produce "lean" code is: compile-time alignment information, compile-time aliasing information (e.g. `restrict`), and compile-time bounds, offsets, ... of the iteration. If you take one of these out, even if the code might get vectorized for some set of inputs, the resulting code will be bloated.
Yeah, impl Trait being the only dependency is the same conclusion I came to for a really solid prototype. But actually, in the context of green threading, you're doing a heap allocation anyway so there's no reason to not just Box the trait anyway. Yes, this entire post was the result of me discovering stateful and finally understanding how brilliant the idea is. Ultimately it would be nice to have rustc on board so it can do all of the fancy analysis that stateful currently reimplements manually.
Cool, that's exactly what I'm looking for. I think there are very probably some antipatterns in unsafe Rust that we want to lint against – similar to what C-targetted tools do. So /u/aturon, do you have a wishlist of things we could lint against? :-)
If you have already imported libc you should be able to just do `use libc;` (if super is the root) or `use super::libc;` inside the tests module. I might be missing something though.
`serialize_usize` is a default method on the [Serializer](http://tyoverby.com/bincode/serde/ser/trait.Serializer.html) trait which means that types implementing `Serializer` can opt to not implement the method to get the default behaviour. You can see that it is a default method as it has `{ ... }` after the method declaration in the documentation. http://tyoverby.com/bincode/src/serde/ser/mod.rs.html#75
[removed]
You may want to repost this in /r/playrust.
[libpnet](https://github.com/libpnet/libpnet) is the crate you will likely need to use. The documentation is available [here](https://octarineparrot.com/assets/libpnet/doc/pnet/) and you can find two examples in the github repo, [here](https://github.com/libpnet/libpnet/tree/master/examples).
Thank you. Google gives me results like https://github.com/rust-lang/rfcs/issues/1081 which is too tense. Not clear what should I use after reading that. 
Anything is better than noise. Even looking at numbers of repos on github is more meaningful. Not everything can be broken down to a number. 
Can't stress how much I am enjoying this series. I'm gonna be doing some architecture simulation work soon, and am toying with using it as an excuse to learn Rust now. Thanks for putting all the work into it!
Does anyone honestly think Rust is already more popular than Go?
Tiobe isn't a study its as valid as rolling a die and discussing the result and you are dumber after you have wasted your time doing so
If no one else offers, I would be willing to take it from you.
&gt; but this now fails on Nightly. What's the failure here?
I think looking at tags on stackoverflow might be instructive http://stackoverflow.com/tags Redmonk is somewhat interesting https://redmonk.com/sogrady/category/programming-languages/
Thanks, great to hear! I'd love to see some of that :)
I realized that ndarray has some of the flavour of ndslice naturally, by virtue of array views, where you can make an array out of a slice, see this example: https://bluss.github.io/rust-ndarray/master/ndarray/fn.aview_mut1.html
Rust and Swift generics are quite similar. A number of requested features already exist in Rust, and a number of design challenges are already addressed in Rust (e.g. orphan rules). Others have clear application to Rust. Variadic generics would allow for generic code over tuples and functions/closures, which would be very useful. Higher-kinded types has to be one of the most requested features for Rust. So, I can see a couple ways this is relevant to Rust. Just being able to see Rust's generics through a funhouse mirror is interesting in itself, but also seeing discussion of how a similar system could be expanded can be directly reflected back onto Rust. Certainly, it would've been better if the OP had posted a specific observation about the article as it relates to Rust in order to start a discussion about it, though.
I feel like this would get more attention if the title had been more descriptive. :) You say this is the "Rust version", does that mean that it is a port from a different language? (Also, I love the ASCII art!)
I don't think @ works here. You have to do /u/[usernamehere]
Awesome. Added to my last project: https://github.com/buntine/barcoders
So are C and C++, and all other languages that promote pointers as a good idea...
Funny you mention "game". How could they possibly exclude search terms about Go, the game? Even if you take their [methodology](http://www.tiobe.com/tiobe_index?page=programminglanguages_definition) into account, how would they exclude Go programs in other language counting towards Go? For obvious reasons the BoGII (Board Game Implementation Index) is equally flawed.
Nice!
I learned a lot from your answer, thanks! I think that numerical computing is a great domain for Rust, and that we need to learn from different languages currently dominating this area, including both python and C++. I think Rust is not yet ready to take on Eigen-like precise code generation, but it is worth keeping in mind. Do you have good pointers to high level descriptions like this about what Eigen does?
&gt; I'd love to hear about any bugs Sorry about that, but having had to delve into that in the past (and ultimately having decided to let [appdirs](https://github.com/ActiveState/appdirs) handle it because life was too short to deal with that crap) I fear all of your paths are partially or completely wrong: &gt; Mac OS X: ~/Library/Preferences According to [the Library Directory Details](https://developer.apple.com/library/ios/documentation/FileManagement/Conceptual/FileSystemProgrammingGuide/MacOSXDirectories/MacOSXDirectories.html), `~/Library/Preferences` is specifically for Cocoa [Preferences and Settings](https://developer.apple.com/library/ios/documentation/Cocoa/Conceptual/UserDefaults/Introduction/Introduction.html#//apple_ref/doc/uid/10000059i): (emphasis mine) &gt; Contains the user’s preferences. You should **never create files in this directory yourself**. To get or set preference values, you should always use the NSUserDefaults class or an equivalent system-provided interface. Random arbitrary configuration files should to into `~/Library/Application Support/` &gt; Other Unix/Linux: ~/.config If you're attempting to follow [XDG Base Directories](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html) you're supposed to store configuration data wherever `$XDG_CONFIG_HOME` points to, and default to `$HOME/.config` only if it's missing or empty. Of course that's only correct on Linux, and even then only on Linux distros which attempt to follow XDG Base Directories, and XDG provides no way to know that, since the spec authors defined defaults if the envvars are *missing* (even though it defines seriously controversial paths, like `$XDG_CONFIG_DIRS` defaulting to `/etc/xdg`…). Do note that [there are distros which (and probably users who) define `$XDG_CONFIG_HOME` to something other than `~/.config`](http://unix.stackexchange.com/questions/24347/why-do-some-applications-use-config-appname-for-their-config-data-while-other), so hardcoding `~/.config` is — if anything — less correct than just hardcoding `$HOME` and dumping dotfiles in there (dumping crap into `$HOME` has historical precedent for it) &gt; Windows: %USERPROFILE%\AppData\Roaming (a.k.a. %APPDATA%) This is not correct on e.g. Windows XP[0], and could be changed at any point in the future. You're supposed to resolve [CSIDL_APPDATA](https://msdn.microsoft.com/en-us/library/bb762494.aspx) (or `CSIDL_LOCAL_APPDATA` for the local data store instead of roaming) and pass it to [SHGetFolderPath](https://msdn.microsoft.com/en-us/library/bb762181.aspx) *or* resolve `FOLDERID_RoamingAppData` and pass it to [SHGetKnownFolderPath](https://msdn.microsoft.com/en-us/library/bb762188.aspx). [0] by default roaming application data was in `%USERPROFILE%\Application Data`, and local was in `%USERPROFILE%\Local Settings\Application Data`
You are a wonderful person for putting so much work into that comment!!! I will fix that all ASAP. (And thanks for the `appdirs` link!)
I am continuing work with [Serkr](https://github.com/mAarnos/Serkr). Last week I dealt with several problems related to parsing, CNF transformation (i.e. transforming formulae in full first order logic into conjunctive normal form) and preprocessing. Now Serkr can analyze almost all (19 files out of around 8000 still exhibit pathological behaviour) CNF problems found in the TPTP library, even ones with more than 2 million clauses (the file size is something like 1/4GB) without any problems. The bigger problem is FOF. It has the fun property that if the CNF transformator is fed a certain kind of formula with n clauses, it will try to output 2^n clauses. And this n can easily be 300 or so. And this certain kind of formula often turns up. So I am slowly implementing a technique called 'formula renaming' which should turn this exponential behaviour into linear and allow me to analyze all of FOF (with a few exceptions). I am also running rustfmt on Serkr. I've formatted most of the program already, but there are some annoying parts left which I hope to finish soon. 
Preparing the [0.5 release of `multipart`](https://github.com/cybergeek94/multipart/pull/27) to land, and hopefully mentoring some beginners on creating sample projects for the new features. Also I should probably get around to switching the license to Apache/MIT.
How dedicated of mentees are you looking for? I'm fairly busy being in school and working overtime every week. I look into rust whenever I have time. It would be great to have a mentor for projects and further development, but I may sit on one for over a week. Also I'm just beginning this and doing so with only mild programming knowledge otherwise (I've done the code academy for python but never did much with it, and use bash semi frequently).
Each individual sample project isn't very involved at all; it mostly serves as an introduction to the new APIs. We're talking an hour of work at the most per each, though depending on how many people are interested you could take on as many as you want.
I will likely be PMing you in about a month then when I've had enough time to feel reasonably comfortable with basics. Thanks!
Ok, cool. I'd like to land the PR in the next week or two, though I might do it without samples so I can have a more relaxed window for anyone wanting to try their hand at them.
YW. I have edited some stuff, and fixed the link (to the appdirs repository rather than the ActiveState organisation, sorry about that). Fighting!, I wouldn't qualify dealing with that a fun endeavour, but saving others from having to (and contributing to the cross-platform quality of rust software) is a great purpose.
I've been trying to write a procedural planet generation program. I've implemented an icosphere generator, and have the basics of a random tectonic plate generator done. My current roadblock is figuring out how to draw everything graphically. I like the look of `glium` but the only UI library out there seems to be `conrod` and I just can't seem to figure out how to bind the two together. I keep getting some weird build errors that don't make sense and I'm too tired to try to dig into the mess of crates that `conrod` pulls in to try to figure out what's going wrong. Besides from that I'm really loving Rust. It's my go-to language for almost anything now.
I figured that preferences are a type of user data, but user data (which you could definitely use the crate for!) aren't preferences :P Yeah, I took one look at those fallbacks and noped right out the door. Now there's an idea for my next Rust project. `extern crate goat_sacrifice`. Though I guess it'd be more like `pub fn&lt;T&gt; sacrifice(offering: T) where T: Sacrificial`...
The primary purpose of a (non-research) programming language is to be used for building software, not to cause interest, right?
This might rather be due to your "filter bubble" though... For instance most of those I know from my university do know Go and know what it's used for (e.g. "high-perf PHP replacement"), but literally none of them ever even heard of Rust as a language.
**TLDR**: Just use Python (or anything else you're comfortable with) for the UI, and embed Rust for high-performance stuff. I've started *very* preliminary experimentation for a native Rust toolkit. Thus far, it's mostly involved designing and throwing away a neat design, that might have eliminated UI races, on account of The Real World being made of pain and suffering and stupid re-entrant event handlers, let alone absolutely *no one* documenting whether any given construct is or is not thread safe or atomic or even if it can send other messages because writing UI code wasn't hard enough, now was it? I also wouldn't hold your breath on my account. At least on Windows, this means starting with user32 and working my way up to UXTheme and DirectWrite and heaven-only knows what I'm going to do about Windows 10. Probably nothing. And I don't *have* a Mac so that's completely out of the picture. Oh, that reminds me, as far as I know, you can't even build an actual GUI executable for Windows with Cargo. Hell, I wasted a day and a half figuring out how to trick Cargo into letting me even link a resource file! Right now, I personally would be inclined to do the UI in Python (using wx or Qt), with any heavy lifting done by some embedded Rust. Rust is kind-of a ball of suck in this area right now, compounded by the fact that Rust doesn't support classical OO primitives, which makes it a huge pain to try and bind any of the existing UI toolkits. I mean, sure, there's gtk-rs, but that's hideously ugly on anything that isn't a Linux distro with a configured GTK theme. Sorry, didn't mean to rant.
Rant as much as you want about TIOBE - as technologists and people with interest in methodologies. TIOBE has reach - so a rising spot in TIOBE (as random as it may be) is an indicator that can be used for advertisement, as many people do use it as an indicator. It's of absolutely no use to fight against that. That doesn't mean that we need to put TIOBE front and center or even actively advertise that. But keeping track of the fact is immensely useful.
I don't suppose there's anything you can do to get Sublime to stop trying to open `&lt;std macros&gt;` in error listing as though it were a real file? Out of everything annoying ST3 does with Rust code, that's one of my most hated ones.
It's impossible for me to know really.
Great crate! I know I've seen several projects where this would've saved a lot of wheel-reinventing. I also like the way that you wrap the various error types in a single enum, allowing the user to decide on what levels of granularity and stages of the process they want to engage error handling. My question is this: my understanding is that the key elements of a crate like this are: * resolution of the config path and providing a reader/writer for it. * wrapping that reader/writer in a dead-simple serialization and deserialization ui without *requiring* the user to first make a lot of decisions about how they want to handle errors, etc. These are great value-adds. And I see no reason to limit the market of this crate to those projects interested in using JSON configs, as they're not value-adds particular to JSON. (e.g. the project otherwise does not attempt to establish conventions as the nature of the config file) So my question is, is the limitation to JSON a design choice (for simplicity/establishing conventions), or simply a matter of what is implemented already / elegant to implement at this point in time (e.g. rust stable does not support compiler plugins, so using serde is less elegant) - and likely more formats (TOML, etc) will become available over time.
Hmm... I really don't know what the problem might be. I tried this exact code on the [main website](https://www.rust-lang.org/): fn main() { let s = "abc\ndef\nasdf\njklö\n".to_string(); let mut ret = String::new(); for line in s.lines() { ret.push_str(line); } println!("{}", ret) } It works just as I would expect it to work. However, here in this code, `s` is a `String` or a `mut String`. Don't know what you `s` type is. It even works if a make `s` a `&amp;String` or `&amp;mut String` and even if `s` is a `&amp;'static str`. So... I guess the source of the problem either lies before the loop or in the `// some conditions` section.
You should not have thins kind of error on safe code. It seems to be a bug of your version of Rust. What is your OS and your version of Rust? Your code seems to work fine on the playground: http://is.gd/QgKepN
I think maybe he makes another string somewhere, that causes to turn into `String` instead of `&amp;str`, which is not going to concat unless you do `&amp;String`.
There are lots of details to keep in mind (as pointed out by /u/masklinn's comment), so it might be wise not to reinvent all the spokes of your wheel — for XDG, there's [xdg-basedir](https://crates.io/crates/xdg-basedir) Edit: I'm not sure if there are similar libraries for other operating systems, but I think it makes sense to have a library for each OS, and a library that creates a common interface for all of them, instead of bundling all the logic into one library.
Yeah. I know about licenses, I just want to hear from people instead of from bland lawyer talk. The licenses may be compatible with it but I'd still feel like a scumbag using someone else's code if they didn't want me to.
&gt; Just use Python (or anything else you're comfortable with) for the UI, and embed Rust for high-performance stuff. I suppose I could do that. This means I have to write a C interface for my Rust application, right? I'd prefer not to, because that would increase complexity of the code by a huge amount. Would it be possible to run a local webserver and have the UI as HTML instead? It wouldn't look native, but I suppose it could look modern and good.
This is definitely a bug in the compiler, make sure you are using the latest version (1.7.0 stable). Other than that, we'll need more details about your platform. If there is nothing strange about your setup, you'd want to submit it as a bug to the rust compiler on github.
Hi! Yes, there is a Python version - but I wanted to run the program faster and to lean how to program with Rust so... :-) Your remark for the title is good... :-/
^^Disclaimer: ^^I'm ^^wxWidgets ^^developer. For wxWidgets there is wxC which is used for several existing bindings (at least wxHaskell and wxErlang if I'm not mistaken) and I think it's probably the best, or at least the most realistic, way to create wxRust too. The trouble is that there are a lot of classes to wrap and I think it needs to be automated in some way, so if I could work on it (and I really wish I could but, realistically, this is just not going to happen unless I'm hired by Mozilla to do it), I would probably start by adding support for Rust to SWIG and try to generate low level 1-to-1 wrappers automatically before wrapping them in a higher level and most Rust-y API. If anybody is interested in working on this, I would definitely like to help this effort, but unfortunately I can't promise anything more than this. But, anyhow, this won't help OP because it will take at least a few months to achieve anything useful.
Has anyone looked into porting Tk?
The core team at least is interested in supporting commercial projects in Rust. We've gone so far as to have meetings with production users to collect feedback.
FWIW if I had work I genuinely resented being used in commercial products, I'd probably GPL/AGPL it. I usually am not that bothered if someone uses my work commercially, so I mostly MIT/BSD it. I sometimes think it would be nice if there was a license that managed to convey 'if you happen get rich using this work, please either contribute back code or chuck a minor windfall at the author'. I realise that's far too woolly to ever make binding or be practical, but it's what I would probably actually like out of a typical OSS contribution :-).
Can I just say that I love the fact that a segfault is a compiler bug in (safe) Rust?
Totally fine with it! I might write some commercial applications myself sometime. Appreciate donations, though. People working on the Piston project can list themselves [here](https://github.com/PistonDevelopers/piston/blob/master/CONTRIBUTING.md#donate_money) if they accept money donations.
I like the piston project, and I feel kind of bad because I'm remaking a lot of the stuff they've done, simply because their design doesn't fit the way I like to design.
Sure, that's essentially how Atom works (via [Electron](https://github.com/atom/electron)). You could easily write an HTTP API layer in your application (RESTful, websockets, etc), which is consumed by a local HTML/CSS/JS browser app. You could bundle them together like Electron, or just ship the components individually. A benefit of this is that your app could run remotely, while the UI runs on as a client elsewhere. If it makes sense, obviously :)
&gt; using someone else's code if they didn't want me to The way to communicate their wants is the license. They are not entitled to not want you to do things expressly allowed by the license.
http://is.gd/88FPBi
I was just telling someone about Rust on Friday. In particular, that libraries can encode patterns of statically determining that splitting some data between threads without locking is safe. He found it very interesting, and this is a great new example of it.
It's based on a google trends keyword search. The problem with searching for 'rust' is that you'll get many hits for the game instead of the language.
XDG data home is for storing resources, or assets, of a program, so that users don't need to mess up the global `/usr/share`. This includes fonts, themes, icons, sounds, game assets (textures, models, levels... even game saves), etc. XDG config home is for storing configurations of a program, so that users don't need to mess up the global `/etc`. This includes which font to use (name of the font, not the font itself), which theme to use (name of the theme, not the theme itself), screen resolution (games), etc. I think preferences fit the latter better
Well, I'm fine with using libraries I built commercially, as long as one fulfills all the conditions of the license I chose. And for applications I built... well... obviously I don't want someone to distribute it commercially.
They are entitled to want whatever they want, they just don't have legal rights left. E.g. I would prefer code I write not be used in bombs, that doesn't mean I have any right to stop it being used that way, just I would prefer it wasn't. I do feel like the majority of the rust community is happy with commercial use of their code though. (PS. If you use GPL code in a bomb, do you have to deliver an offer for the source code to whoever you kill with it?)
Is there any progress with Qt Rust bindings?
Python for UI makes a pretty bloated and poorly performing result. Same as Java. Bad idea.
If you like to give some feedback, open up a post on the brand new forum: http://discourse.piston.rs/ Lot of stuff is still experimental, so I guess we need a phase were we collect feedback and sort this out before settling on a final design.
Speaking only for myself, I would be quite happy if the crates I made are used in commercial products, including closed source. I would like to see Rust be successful for an extremely broad class of use cases.
This week will be a short one for me, because I'll be at JavaLand from Tuesday to Thursday. At least on Friday I'll meet some of you at the Rust Table of Regulars. In other news, I've pushed a hopelessly incomplete Range implementation to my github as a starting point for my range set. My current goal is to do without clones, and I'm not sure if that's possible with `std::collections::btreeset`... Edit: Also started contributing to [mutant](https://github.com/Geal/mutant).
Thanks for this, I couldn't make it to rust-camp last year because of it's location, and it is much easier for me to get to PA :)
Countries can (and will) happily create exceptions to copyright for their military, so there isn't much point to putting a 'no bombs' restriction in your license :(
I'd often tell people I'd help out with stuff: &gt;If you use this code to become a billionaire, you'd better at least by me a large pizza But I don't think I documented it anywhere, too bad.
Perhaps the name of the lib is a bit misleading, because I did intend it to be used for e.g. game saves, levels, etc. I figure that putting config data inside `$XDG_DATA_HOME` is better than putting user data inside `$XDG_CONFIG_HOME`. Also, AFAIK, the XDG spec itself doesn't make much of an effort to differentiate the two, anyway...
No reason anymore. I initially had a struct called `Preferences`, but now that I've typedef'd `PreferencesMap` and allowed arbitrary serializable types, I should rename it :)
Is there any way I can implement the add operator for owned values without having it consume one of the values? I tried impl Add for Vector3f { type Output = Vector3f; fn add(self, other: Vector3f) -&gt; Vector3f But this consumes my vectors when I adding them with the + operator. impl&lt;'a, 'b&gt; Add&lt;&amp;'b Vector3f&gt; for &amp;'a Vector3f { type Output = Vector3f; fn add(self, other: &amp;'b Vector3f) -&gt; Vector3f But this only works with references, which doesn't make me happy either. Or is it really supposed to be this way, that if I have owned values of my vectors I first have to get a reference to them and then pass it to the add operation because otherwise the value would be consumed?
I did consider including xdg-basedir, but resolving the one variable (with a fallback) can be done with [a single `match` statement](https://github.com/AndyBarron/preferences-rs/blob/45e68cec670468b0ee5473e4503cc2012b5d9e37/src/lib.rs#L190), so I decided not to pull in the dependency. Having separate OS-specific micro-crates is actually a pretty good idea, might try it if I have time!
&gt; E.g. I would prefer code I write not be used in bombs, that doesn't mean I have any right to stop it being used that way, just I would prefer it wasn't. I guess that depends on whether you had the freedom to choose the license. If you did, how much credit do you expect for claiming to have a particular preference?
The GPL/AGPL does not prevent people from using code commercially, otherwise for example Android would never have been possible in its current form.
My general feel of the Rust community is kind of a, "I'd love if my code is useful for you," and it doesn't really matter whether it's in an open source or proprietary project.
I sure hope Rust sees some uptake in commercial products, especially those security relevant ones. If, say [clippy](https://github.com/Manishearth/rust-clippy) is helpful to build it, great!
Also &gt; Give the authors credit for their code
Working on my naive package manager, [mpm](https://github.com/0X1A/mpm). Haven't had a chance to work on it. Currently getting around to creating database files. Ran into an issue where if someone would want to change into a directory under `build` or any scriptable item then they wouldn't be able to as I was executing each element of the array as a `Command::new`. I ended up just appending each line with `;` and executing them all as one argument to `sh -c`.
I released version 0.2 of [cfg](https://github.com/pczarn/cfg) as well as an unoptimized Earley recognizer/parser, [gearley-simplified](https://github.com/pczarn/gearley-simplified). More advanced libraries are still underway. To take a break from parsing, I revived a small refactor of HashMap and will begin writing overflow support for MIR.
Back to work on embed_lang. Going to try and detach the symbol table and the reference counted types that are currently stored in the virtual machine so that it could eventually be Sync and thus used in a multi-threaded environment.
You can create a D::Error from a &amp;str by calling `decoder.error(s)`
&gt; It only restricts the product if the company considers its source code a trade secret Also only if you distribute the code in any way. If you don't distribute your code (eg. run it on your own servers), then you don't have to publish it.
Exactly. It's like Gartner's magic quadrant (not that I wanted anything magic in my programming languages) – decision makers read this stuff, notice a trend and if we're lucky decide to have someone look into it. ;-)
I'm only one voice, and I'm not currently a rustacean - yet - but I am quite fond of open source thingamabobs and have 5-10 years of web development and programming experience. As stated, people are likely to include licenses with their works, and also, large commercial companies who want closed-source products with open source components are very likely to use those components regardless. Personally, for the (non-rust) things I create, I do not care if some one uses it, no matter the openness or purpose, commercial or otherwise. Everything I do, outside of work at least, is as free as free can be. Take it, use it, change it, sell it, whatever. 
Also, what are you going to do about it? They're the ones with bombs. 
You are looking for /r/playrust. This subreddit is about [the programming language Rust](https://www.rust-lang.org/)
Is the `Vector3f` something like three floating point values? In that case, making it implement/derive `Copy` might be the most convenient. For types that can't or shouldn't be `Copy` I think you'll have to be explicit about the ownership.
I'll soon hopefully be making some money off of closed source code that depends on free, open source code. I plan to donate some money down the crate's dependency tree even though the common licenses obviously do not require such a thing. But really, how much compensation is reasonable? I truly consider 90% or more of my project's value to come from its constituents, but I couldn't see myself actually donating that percentage. As an open source dev, this irreconcilable contradiction tears me up, so [I wrote about it here](http://skylerlipthay.com/open-source-exploitation.html).
That sounds more like it. :)
I'm not a lawyer, but I think it's enough to distribute a file alongside your program with copyright credits. But the end user has to have some way to find this copyright attribution. For something like an Android app, I think you need a menu like "credits" or "copyright notice" that attributes proper credit (in a way that's acceptable per the license), because the user usually has no way to find a copyright file. But even for programs where users are expected to inspect its files, it's good to have a menu or command line flag (perhaps `--version`) that also gives credit to everyone with copyright over your program (perhaps "Uses code from XXX library version YYY copyright ZZZ. See the COPYING file for details"). See eg what Chromium does. It has an "About Chromium" option on its menu that says: &gt; Chromium &gt; Copyright 2016 The Chromium Authors. All rights reserved. &gt; Chromium is made possible by the Chromium open source project and other open source software. Linking to `chrome://credits/` which gives proper attribution to the developers of all libraries used by Chromium.
First of all, most OSS communities I've seen are happy to have their stuff used commercially. Slightly fewer are happy to have their stuff used in proprietary software. Rust libraries are usually licensed however the creator wants it: if it's MIT, it's cool to use it in proprietary software. It's true for me anyway.
I'd wait with using Conrod with Glium at the moment, as Piston is going through a heavy upgrade. Got plans for making Glium easier to use, but it has to wait because doing that now would be a waste of time, since it needs to be changed later. Would love to see a text based format to interface with the procedural generator. Settings in - planet out. Imagine exporting a planet to another tool which adds cities and infrastructure, then another tool that adds geometry for physics, then another tool for navigation meshes. A whole world generated with just a few clicks! My plan for such tools is to use [Piston-Meta](https://github.com/pistondevelopers/meta) to describe the syntax of formats, so other tools can read the data without relying on the same data structures or break when there are changes in the format. Currently used in Eco and Dyon.
That's why the AGPL exists.
There are two clear-cut cases. If you don't intend to have the user manually edit the files with a text editor (perhaps because it's binary data), then `XDG_DATA_HOME` is the correct choice. And if your own program never writes to the preferences (instead choosing a default value for them), and expects the user to create the file themselves only if they want to change the default value, then it goes to `XDG_CONFIG_HOME`. But then the config should be a text file. But if you both intend the user to change the file using a text editor, but also writes to it yourself (eg: when the user changes the configurations on a config option inside your program), then it's less clear. I'd still say `XDG_CONFIG_HOME` is the right place, unless you're dumping unreadable files like very big XMLs (but then you should expect anyone to manually edit it..). Ideally a program that stores both user-editable files and other kinds of assets would store on both directories, at least on Linux. Note that some programs, like Chromium, blatantly abuse ~/.config to store profile data that should go to `XDG_DATA_HOME`. The Chromium profile is more analogous to a "game save" than a config, because it's not intended to be edited by hand by the users, and even contains binary data (such as SQLite databases) Another thing: if you library supports moving the preferences directory, it should still read the old directory if the new location doesn't exist, for backwards compatibility (perhaps then moving it to the new location and setting a symlink, or something fancy like that).
From the TOS: &gt;…including, without limitation, the development, design, manufacture, or production of nuclear missiles or chemical or biological weapons. So I guess conventional bombs with iTunes would be okay.
Unless you add a default implementation for the new trait method, it's definitely a breaking change. FYI: Only if your crate is version `x.y.z` will you have to bump `x` for a breaking change. For `0.y.z`, bumping `y` is fine, and I believe in the case of `0.0.z`, any change at all indicates breakage. (This refers to "breaking" in the sense that `crate = "^x.y.z"` won't update to a newer version with breaking changes.)
Maybe something like this?: macro_rules! in_order { ($first:expr, $second:expr, $($rest:tt)*) =&gt; { in_order!($first, $second) &amp;&amp; in_order!($second, $($rest)*) }; ($first:expr, $second:expr) =&gt; { $first &lt; $second }; ($first:expr) =&gt; { true }; } 
Are there metrics on how many views TWiR gets every week?
If this is what you're using (https://doc.rust-lang.org/rustc-serialize/rustc_serialize/trait.Decodable.html), then this is a Trait. The D::Error is called an associated type, and is defined for President. So assuming that President is your own type, there will be a "type Error = MyErrorType", where MyErrorType is some kind of enum or struct that handles errors. https://doc.rust-lang.org/book/associated-types.html
Does it mean I can't listen to my sinister playlist on iTunes when I make zombie virus? 
Magical recursion is Magical, thanks
&gt; If you're thinking, "but then other companies can redistribute that code" that's not really an issue either. I'm pretty sure that in a consumer market you'll find yourself competing with several paid and free clones. &gt; It's pretty easy to release open source code that can only realistically be supported by the original developer. Supported?
In my mind, the *intended* use is the former case, but the file is human-readable JSON just in case (and also for language interop). I want the lib to be as simple as possible, so I'm very hesitant to provide implementations that do extremely similar things but save in different locations. I figure that if a user moves their preferences directory (e.g. overriding `$XDG_DATA_HOME`), they probably *don't* want their old config to carry over (or will be savvy enough to copy the files into the new location).
It feels like a contradiction for an extension trait to have required methods. `IterTools` doesn't.
Plagiarize as in remove attribution? What makes you choose a license which doesn't require attribution?
I actually think the one by /u/antoyo might be a bit better, since it uses `expr` for rest instead of `tt`. Should give you better error messages if you write it incorrectly.
This is one reason we need sealed traits.
Licensing is a legal choice for me. It says what you can and can't do with the source code in the eyes of the law. &gt; If relying on that permission is unethical, what then, is the permission meaningless, do you not have agency? It's not meaningless. It's a legal grant that says "you can do X and it will be legal." It is **not** an ethical grant that says, "you can do X and burntsushi will consider it to be ethical."
I get the impression from /u/_VZ_'s post that it is probably not that simple. Thanks for a link to rust-bindgen though, I've never seen it and I may have a use for it soon.
If you are using [Chrono](https://github.com/lifthrasiir/rust-chrono)'s [`DateTime`](https://lifthrasiir.github.io/rust-chrono/chrono/datetime/struct.DateTime.html) type, the subtraction operator should work (and `num_nanoseconds()` method should give the whole number of nanoseconds in the Duration).
Haha, it could be fun to try the GObject introspection tools with wxC.
Thank you. That was exactly what I was looking for.
&gt; openly developed on Github That's not quite accurate, IMO. Recently each week's issue hasn't been out in the open until it's been pushed in a single commit and then immediately published, so corrections or addendums can't be made until it's gone out.
Is the SIMD crate ever going to be moved back into the standard library? I understand it is unstable subject to change/breakage, but it seems like a really killer feature for a language like Rust to have as standard.
According to the SemVer spec, the versioning rules apply to what you have officially declared as part of the public API. What ever breaking changes you make to your private API do not affect the version number.
Uhh. Chrono DateTimes implement the Sub trait and return a Duration as the result, sooo... I'm a little confused about what's going wrong.
This is probably simply due to the work being done at the last minute. I know when I published twir I did it right before publication.
If the draft isn't made public before being published then a) no one can help out and b) it's not being developed in the open. Edit: make the draft public a week early and work on it on the public repo so people can submit addendums and corrections before it's published. Then the burden of editing is lessened somewhat and it doesn't have to all be done at the last minute.
Ah, makes sense. The parent poster wrote: &gt; Oh yeah the MIT notice forgot that. I guess you can read that in a couple of different ways.
Sorry; I was going to find one in the morning, but looks like you beat me to it.
That's also possible, but there's a tradeoff on documentation surface and understandability to be made. I like taking a closure for very local decisions and a trait for "chunkier" logic. It's a gradient and I don't have a great vocabulary for describing it yet. \^_\^
Didn't know you could just subtract one DateTime from another and have a Duration returned. 
Debating what to do with [rust-kanren](https://github.com/wartman4404/rust-kanren). I'm not really happy enough with it to publish it anywhere official like crates.io, but I'm also kind of tired of trying to fix all the things I don't like about it.
We do not collect browser side analytics. /u/cmrx64 controls the servers so he might have some information regarding server side metrics. 
Um surprised to see swift above objective-C
Something like: When function called first time with argument XXX it returns A, with YYY it returns V, then you call another function with XXX. Again call first function with XXX, it returns C. Maybe something for when you have to check that mocking function is not called when you call testing function with XXX. Basically this http://mockito.org/. it's so much better and more readable when you mocking right in your test, rather than outside.
I've created a draft for next issue and will try to do the same after publishing every future issue. https://github.com/cmr/this-week-in-rust/blob/master/drafts/2016-03-14-this-week-in-rust.md
This is my first technical blog post - I was inspired by all of the awesome posts that find their way to this sub. Hopefully I didn't do too badly. This post is (somewhat indirectly) about rusty-machine, a general purpose machine learning library implemented entirely in rust. I wanted to explain why I think rust is a great language for machine learning. To do this I've walked through an example solution to a machine learning problem. There isn't a huge amount of information on rusty-machine in this post. I'm planning on writing up some more detail in the near future. Thanks all!
Also, there are currently ~1100 subscribers to the TWiR email newsletter (and constantly increasing).
It's the various feed readers. After filtering out requests to atom.xml and rss.xml, you get 8.08% unknown browser, 12.61 unknown OS.
Rust needs better C++ interop.
Thanks cmr!
Really? IMHO everything is unit testable. All you need is an isolated "run single instruction" function so that you can execute an instruction without running the pipeline. Then, an instruction test first assigns stuff to the CPU registers, then uses the function to run the test instruction, and then checks a set of registers against expected values. It is a lot of redundant typing, but it works.
It would still be a breaking change if the trait were sealed. Image `fn f &lt;T: Foo + Bar&gt;(t: T)`. Let `Foo` be the trait that gets a new method, and `Bar` be a trait that (for whatever reason) already has a method with that name, that is used in `f`. Code will fail to compile with the updated library, so adding the method was a breaking change. 
What if you add a method to a trait and implement it for a type. If someone has made a trait with the same method name on that type it's going to collide. Is that considered a breaking change? I ask because I think it happened with Clap. The new method was being introduced in std nightly as unstable and broke Clap. 
The [related RFC](https://github.com/rust-lang/rfcs/pull/1105) explicitely went the way of "Minor changes should require at most minor amounts of work upon upgrade." So in case of adding methods that would lead to ambiguity, it is always possible for users to use UFCS up front and there for only a minor change: [Minor change: adding a defaulted item](https://github.com/aturon/rfcs/blob/api-evolution/text/0000-api-evolution.md#minor-change-adding-a-defaulted-item)
Reminds me of the license in [FuckItJS](https://github.com/mattdiamond/fuckitjs) &gt; If the Author of the Software (the "Author") needs a place to crash and you have a sofa available, you should maybe give the Author a break and let him sleep on your couch. &gt; &gt; If you are caught in a dire situation wherein you only have enough time to save one person out of a group, and the Author is a member of that group, you must save the Author.
CRATE OF THE WEEK. OMGWTFBBQ. I am flattered :D!
I am writing a cgroups crate that interacts with libcgroup, and I have an issue with one of the ffi functions, using a double-pointer. I have basically the following: enum CGroup {} extern "C" { fn cgroup_free(cgroup: *mut *mut CGroup); } Whilst I do have a `*mut CGroup` I am at a loss for how to get a `*mut *mut CGroup`. Any help appreciated :)
I didn't test it, but it should be possible using `feature` in your `Cargo.toml`. Your main file could look somewhat similar to this: #![cfg_attr(feature="without_std", no_std)] #[cfg(not(feature="without_std"))] use std::fs::Path as MyPath; #[cfg(feature="without_std")] use stuff::fs::MyPath as MyPath;
The intention is, yes, but not until it's ready.
&gt; heavy lifting done by some embedded Rust Noob here - how does one embed Rust into Python? Googled around but only found that it would involve C programming and that is... well, it adds to the complexity of the program way too much.
Aha, interesting. I had only seen the `#![no_std]` crate attribute, which seems to be a static either/or proposition. So [features] default = [] stuff = ["use_std"] should make my library no_std by default, and std if dependended-on with `features = ["stuff"]`?
It doesn't involve writing C, it involves using the C calling convention. Until there are specialised binding generators, that's probably about the best you can do when trying to talk to Rust from Python.
Ok, thanks for the explanation. Still way above my level, though. Intermediate Python with "hello, world" Rust are not the best base for such a feat.
- It should be available in the next nightly (tomorrow) - The current implementation behaves the same as the `try!` macro - there is no `catch` yet
Oh I see, use_std is a normal feature internal to the crate (not some sort of magical cargo flag), and depending on it I either set the `no_std` crate attribute or not (as /u/LEmp_Evrey showed) and define the additional helpers or not. Neat, thanks.
There's [already a libterm](https://github.com/Stebalien/term), from the bad old days of libextra. I guess these days it's just called `term`, but `libterm` is an unnecessarily confusing and uncreative name. This crate should *definitely* use that crate, for termcap/terminfo, instead of hardcoding terminal-specific constants.
Don't scare them! :O
You can only use this operator if the function you're currently working in returns a Result, correct? (Like the try! macro?)
hmm, maybe it’ll work for `Option`s then as well? `let thing = maybe_make_thing()?` ⇒ let thing = match maybe_make_thing() { Some(x) =&gt; x, None =&gt; return None, //propbably no .into() here }
- https://github.com/rust-lang/rfcs/blob/master/text/0243-trait-based-exception-handling.md This can really be game changing, especially when combined with "Exception type upcasting". Hope to see a new chapter on them in the Rust book. Err(e) =&gt; break 'here Err(e.into()) I'd like to spend some time to think what might be the best practice to make the most of `into()` here, without causing conflicts. By the way, I noticed the implemented desugaring would yield `From::from(T)` rather than `Into::into(self)`. Any particular reasons?
The actual segfault occurs in printw called from [here](https://gitlab.com/Morosko/man-rust/blob/master/src/main.rs#L65). It's possible that the culprit is the code you highlighted, since theoretically it could be corrupting program state that is only used there, but it seems somewhat unlikely. Do you have a reason for thinking it's the part you pointed out?
I had actually been contemplating implementing GMMs next. They shouldn't require a huge amount of additional work. It would be great to have someone try out the library and get some feedback. I'll try to push this up and get GMMs working soon. Disclaimer: if performance is really* important to you then you may be best sticking with a more mature library! \* It is still fast!
It's to enable slander lawsuits when people complain about iTunes bombing out ;P
My reason is that since I added this code the error occurred and when I remove it again, it works again. Since I don't get a more useful error message I'm lost, cause I'm pretty new to rust and don't know any other debug methods. What do you suggest then ? 
To get a more useful error message in segfaults, you should run the binary under a debugger (e.g. lldb, like I suggested yesterday). Anyways, Changing that line to `printw(&amp;format!("{}\n", line)[..9124]);` and smaller makes it work for me, anything bigger segfaults. `line[9120.. 9130] == "e: %s &lt;pat"`, specifically printing the s after the percent sign breaks it. This is the first time a `%` pattern is used like that in the string. My best guess is that the ncurses library isn't handling it properly.
If we make `?` trait based we are basically "stabilizing" a partial Monad interface. I can't wait to see what people do with it, but at the same time i'm a bit worried that people will use it all over the place.
Thanks for your work here, /u/japaric!
Paging /u/brson!
That's odd, since `Into` covers a larger set of types (it's blanket implemented for `From`).
Yours actually seems more optimal. I don't know how the implementation is set up, but yours doesn't strictly have to consume the trailing token trees at all, whereas the expression version has to go through and check that it has a comma separate sequence of expressions on each iteration. You could even imagine an optimized implementation linearizing your version by recognizing that you're always passing the unprocessed tail to another invocation of the same macro. The only real benefit of the expression version is that it verifies the format on the first iteration, so you don't end up with a macro expansion error behind several layers of macro expansions.
This is probably just drawn from the way `try!` works, which is itself a historical artifact. I agree that using `Into` is strictly better.
Will this be released in 1.8 stable in a few weeks or will it have to go through the beta first? Or will it be even longer?
It'll have to go through the beta first, and before that can happen it'll have to be approved for stabilization. That'll take a few release cycles at least (plus I'm pretty sure the RFC isn't fully implemented yet).
Yeah, that's probably the reason.
Really commendable stuff!
Yeah, the lib prefix implies a C(++) project also
It has some issues, at the moment the `?` + `catch`-block do not follow the monad laws at all (even for an "immediately evaluating monad"). This makes it impossible to implement anything which does not carry all its state in the type itself (ie. no extra fields with data can be used). I hope this can be changed, since it would be much more useful while still allowing `return`, `break` and similar to work inside of it. An example of something which supports a limited set of monads would be the [immediate do strawman](https://www.reddit.com/r/rust/comments/43cwc4/a_strawman_idea_of_an_immediate_do_notation_macro/). I detailed some in a blog post: [Rust: The `?` Operator](http://m4rw3r.github.io/rust-questionmark-operator/).
It's controversial. I personally am on team "only `Result&lt;T, E&gt;`", because `Option&lt;T&gt;` isn't for error handling. That said, I also like monads, and combinators, and if we're not gonna get `do` for a loooong time...
The process is: * An RFC is written, and accepted. * An implementation lands at some point. * It goes through at least one full cycle of being unstable * At some point, we vote to stabilize it or not. I would expect this syntax to be unstable for at least a release or two while we work out some of the kinks. This was one of the most discussed and longest-running RFCs we've ever had, being hasty now would do it a disservice.
Well, this sounds pretty specific. Thank you. I'll take a closer look. I'm sorry for not using lldb myself. Hadn't had much time in school.
It's not a problem that you didn't running it myself really isn't an issue. Sorry if my original post came across as passive agressive, I was just trying to point you in the right direction for future issues ;)
If you're interested, `step_by` (tracked [here](https://github.com/rust-lang/rust/issues/27741)) is blocked on (among other things) `zero_one` (tracked [here](https://github.com/rust-lang/rust/issues/27739)).
60fps is suspicious. That is a common refresh rate on most monitors. It makes me think that vsync is on.
While technically a breaking change, adding a method that causes a name conflict which can be trivially resolved using UFCS (i.e. change `t.bar()` to `Bar::bar(&amp;t)`) was defined as acceptable breakage in [RFC 1105](https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md). (Not to say such a change should be done often or lightly!)
I suppose I'm less on team "include `Option`", and more on team "don't special-case `Result`". Though I certainly do understand the concerns.
Looking at the debug tools visible in the servo demo and the additional "no sweat" pretty much confirms this 😄
[No need to use map.](http://is.gd/lMtweT)
Broadly, yes, but it's a bit more nuanced than that. Check out the ["Principles" section](https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md#principles-of-the-policy) in the RFC I linked.
I think "same hardware" deserves serious quotes here, the point of webrender is to offload part of the work to the GPU, existing browsers can't do that. That's why WebRender gets orders of magnitude better performances, because it uses hardware older browsers are unable to make use of. And now I'm wondering how WebRender fares when running on a crummy IGP.
I think in practice using this with option would be a less verbose form of `if let Some(foo) = option().map(method).and_then(method) { .. }` (instead, it would be `let foo = option()?.method().method()?;`) It reminds me of the Ruby `try` method (though technically that's more like a mix between `map` and `and_then`)
I agree. My feeling is: we can overload the dereference operator, why shouldn't we overload `?`?
Sure. That's what I meant about monads and combinators :) But if `?` is an _error-handling_ feature, then I don't think it should apply to Option.
There aren't that many features to support in rendering, it's not a "very small percentage" that we support. The reason Servo is fast in this case is because it uses the GPU in a way that nobody else does. The speedup is quite obviously due to that; any additional speedup due to not being 100% there would be negligible compared to it. This argument makes sense for DOM speed or even perhaps layout speed, but not really rendering speed. (We had a CPU rendering solution too, which was much slower than the new GPU stuff by a large factor. The GPU stuff is faster than other browsers by the same factor -- it's pretty obviously the fact that we use the GPU that causes this speedup)
I ran it on llvmpipe mesa, it worked fine for regular sites. Pathological cases went boom, but they go boom in any browser except Servo+WR (on a _real_ graphics card)
Please file a bug? We should fix this.
Thanks for your interest and the feedback! I think I'm in agreement with you - visualization does seem less important. Especially as many people would still prefer to export the data and run visualizations with their preferred tool (this is how I tend to work). That said, the data handling is definitely important and currently lacking. There is also a lot of room to add more choices whilst retaining the homogeneity.
It's stressing the rendering code. On all actual browsers, rendering computations are made on the CPU, the new WebRender backend of Servo tries to delegate a large chunk of the computations to the GPU instead (like video games do). Patrick Walton did [a presentation](http://pcwalton.github.io/slides/webrender-talk-022016/) about it in February (20th if the slides title is accurate) that was linked on r/rust if you wish for more details.
Thanks for the deck link. So it looks like they're using OpenGL vs other 2d graphics APIs. Makes sense.
`dt.timestamp()*1000 + (dt.nanoseconds()/1_000_000) as i64` seems to work.
I forgot in my original post, thank you for grinding away to provide another tool to the community. I know just enough about the inner workings of most these models to know it couldn't of been easy. 
Zephyr contributor and rust lurker here, that's going to be very interesting for both sides, I guess. At first, targeting the QEMU "board" running the microkernel, then a real board, then, if it makes sense, making a nanokernel version of the rust support. Already giving me ideas :-)
I can come up with some pretty exaggerated numbers, so thanks for bringing me back down to Earth ;) Thank you for your work on Piston! I have a really fun 3D project planned and I'm excited to use it.
Would it be possible to get a 'Call for Participation' like ThisWeekInRust?
`Err`...`Ok`.
Why are people so interested in K Means clustering? DBSCAN seems to be better in performance and results.
The RFC also adds `catch` or whatever its name is, which is an inline such environment that try or ? returns from, and you can use it (and nest) inside a function.
See http://servo.github.io/servo-starters/
But it's also stressing the CPU much less than other browsers, and I expect CPUs to be less power-efficient than GPUs.
If I had to guess I'd say it uses less power. If pages are rendered extremely fast then the time to go from idle -&gt; full power -&gt; idle would be much shorter than for a cpu. 
*shrug* Obviously not everything is suited for this model (and I agree, text editors is probably one of those not suited). But plenty of things just need a thin interface and will never actually munge around lots of data, so HTML/CSS/JS works just fine. 
I tried [Brave](https://github.com/brave/browser-laptop) today, which is a browser based on Electron, so I guess it should be super suitable. But I couldn't get it to work in any of the branches I tried since Node seemed to keep falling over on some errors. And running `npm install` pulled down 350MB of dependencies which seems like an awful lot considering how Electron should really have most of the stuff already there. Do you have examples of this style of program that works and isn't terrible? 
You might have a good point! I'd welcome the addition of DBSCAN to the library ;)
Don't all browsers use the GPU to some extent, just not very well?
Ah, I see. If the purpose of this project is to reduce hassle for the people coming from Flask, I guess retaining the current order is also a reasonable choice. But, as always, bikeshedding is super fun!
Well… some day I would love to live in a world without business models and capitalism. But for practical short-term considerations your position is understandable. It's not like I am only writing free software as a day job either :P
Good idea!
For that, you need to get hardware specific documentation, since OpenGL/DirectX and other abstractions hide details from you. AMD has been fairly open with their hardware specs, I would search for "GCN architecture" (Graphics Core Next). 
I'm very interested in this, but have only just started looking at Zephyr. Zephyr is great because the code base is small and comprehensible. It's built on a tiny nanokernel that works independently of the larger microkernel. It's got an easy to install and test toolchain. What I'd like to do here is write API bindings for the nanokernel, then the microkernel, then rewrite the nanokernel in Rust, then the microkernel. Let me know if anybody puts serious work into this. Zephyr looks like a nice route for creating a serious Rust-based realtime programming platform.
Probably depends on the task and the GPU. On the desktop, CPUs range from consuming 45W-&gt;120W whereas GPUS are all over the board, with an upward limit around 250W-&gt;300W. If your GPU task isn't effectively using the GPU (only uses a fraction of the compute units), it is possible that you are wasting power. Admittedly, I don't know how GPUs do clocking on their individual compute units, so that will really change the answer. The finer the grain of control they have, the less power will be consumed.
Indeed, this is foreign territory so I can only speculate. From pcwalton's earlier presentation showing a demo running at 300 FPS that allegedly is &lt;1 FPS on other browsers, I'd guess it would be possible to throttle your rendering task to 60 FPS, which, naively speaking, would mean your GPU is running at 20% capacity and still performing 60x better than a CPU running at 100% capacity. In any case, the metric of frames per unit of power is still tremendously in WebRender's favor.
Does it support file uploads yet? If not, might I suggest integration with [multipart](https://github.com/cybergeek94/multipart)? It can be in Pencil or in multipart, doesn't matter hugely. 
Would the appropriate place to file a bug be in the servo repository or webrender repository?
If it implements Copy it should implement Debug. Is an adage I read somewhere. It more or less covers you explanation. As FileHandlers, Channels, and Iterators rarely have Copy (unless you deal with base i32's) 
Agreed. Especially in the mobile environment where the GPU TDP is pretty limited and where the engineers will have spent a lot of time trying to optimize for power over performance. The dedicated graphics card story may be something different for desktops and laptops, however, the integrated CPU/GPU will probably also be geared more towards power savings (particularly because the CPU thermal envelope is more limited than a dedicated card.)
The WR repo is fine.
Don't forget about function pointers and closures...
Maybe e17 can fulfill your requirements? It writen in C and crossplatform.
Great job! And as we have more and more framework and servers, I think the rust web ecosystem should start to work on something like WSGI/Ring(clojure) seriously to standardize the request/response struct. And also define a bridge for server developers and framework developers. I remember there was a project called conduit for that. But it doesn't seem to be well accepted. 
This will be less of an issue when [Incremental Compilation](https://github.com/rust-lang/rfcs/blob/master/text/1298-incremental-compilation.md) comes along.
Thanks. Now that I've seen your example, the documentation for `nanosecond()` makes a lot more sense.
What about templating? Jinja is more or less integral part of Flask, but I don't see any equivalent here.
So most of the tasks we send to WR are basically peanuts for GPUs. I'd wager that the GPU power consumption isn't much. Especially when you cap it at 60FPS.
[**@jperkin**](https://twitter.com/jperkin): &gt;[2016-03-08 09:15:26 UTC](https://twitter.com/jperkin/status/707132609969725441) &gt;Ok, rust\-1.8.0pre1 package now available in SmartOS 2015Q4/x86\_64 repository for people to play with. Let me know of any issues! Enjoy. ---- [^[Mistake?]](/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=/49mzbe%0A%0APlease leave above link unaltered.) [^[Suggestion]](/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/joealcorn/TweetPoster) [^[Issues]](https://github.com/joealcorn/TweetPoster/issues) 
*Wow,* thats... amazing...
I would suggest filing a bug at https://github.com/servo/servo , or in asking on the #servo IRC channel on irc.mozilla.org: https://chat.mibbit.com/?server=irc.mozilla.org%3A%2B6697&amp;channel=%23servo
omg omg yes! Flask is my favorite way to quickly set up services (thanks /u/mitsuhiko!). Great to see something similar in Rust! (though Iron is pretty similar and I like it too)
Great! Performance is not that much important to me. (I think that calculation of response to event within one second is OK.) I like Rust safety.
Thanks a lot for mentioning this - once Piston goes through its upgrade, I'll be sure to add an example of using conrod with `glutin` and `glium`.
A GPU is extremely more power efficient then a CPU due to the fact that you are splitting cores up, more slower/smaller cores is more efficient then less larger/faster ones. This is why smart phones started using multi cores regardless of the performance benefits. This is why you don't mine bitcoins on a CPU but a GPU (or better), because of power consumption, not only hardware cost. You also end up emitting much, much less heat for obvious reasons with GPUs, less power consumption generally means less heat. 
Why not build `multirust.rs` into Cargo? I ask because: 1. It would mean fewer tools for the Rust ecosystem as a whole. Ideally most package managers / users would only ever deal directly with `cargo`. 2. This could allow a future where crates could specify the versions of Rust they are able to run with. 2.1. Such that teams of Rust devs don't have to keep their versions in sync manually, per project. 2.2. Automates away any worries that build machines for prod binaries are consistent with development/test machines. 2.3. Form of documentation/search filters for usable crates; If for whatever reason, your project is stuck on an "old" stable. 3. This is generally consistent with the build systems for large companies I've worked at. i.e. The build system controls the different build tools, and their versions. 
Thanks /u/brson! So excited for rustup.rs. :)
AFAICT, cargo doesn't make the same mistakes that seem to cause the big problems there. The clone seems to be a full one, and there's a directory structure in place that should prevent huge directories like the `Specs` one mentioned in that issue. So things seem to be a lot better in Cargo-land, but I'm not that familiar with the internals and my knowledge of git internals is probably somewhat outdated by now, so maybe there are other problems left that I'm not aware of.
&gt; So yeah: Any information about traffic? And is there a plan for the future? I don't want to see as many negative responses to the Rust project as in the linked thread above :/ Having read the original and HN threads, the issue with CocoaPods seems to have been not so much using github for their index as the way they did so: * using shallow clones (and doing so inconsistently[0]), while Git technically has support for shallow clones there are performance problems associated including [downright pathological behavior](http://thread.gmane.org/gmane.comp.version-control.git/288403) (linked from the CocoaPods bug): &gt; The first is a non-shallow clone; it takes a few hundred milliseconds to generate the pack, and the result is a few hundred kilobytes. The second is a shallow clone (logged from a real-world request); it sends 200 times as many objects, totaling 270MB, and takes almost a minute of CPU. * having a problematic physical layout, all the packages are in a long flat directory which is problematic in many FS (because traversing it is expensive), and is additionally problematic in Git because any change to a subtree (which is every commit on Specs) recreates the top-level tree object (it's a COW datastructure, think HAMT): https://github.com/CocoaPods/CocoaPods/issues/4989#issuecomment-193835710 --- **Cargo has neither issue**: * it uses git "normally" with regular "deep" clones and fetches which is github's bread and butter. * its index is [a two-level prefix-tree](https://github.com/rust-lang/crates.io-index) (which is one of the fixes suggested for cocoapods, and something git itself does internally, check out `.git/objects` in a local repo) Cargo also creates less stuff as each package's metadata is a single list of json objects where cocoapods uses a folder and a json file per version, not sure that has an impact though. Note that Homebrew [also uses github as its package index](https://github.com/Homebrew/homebrew/tree/master/Library/Formula)[1], and its lead developer *works at github* ([and commented on the cocoapods issue](https://github.com/CocoaPods/CocoaPods/issues/4989#issuecomment-193801376)), so using github as an index doesn't seem to be inherently problematic. [0] the specs repository is initially cloned with `--depth=1` but later fetches are not, and if a fetch brings in a merge commit which goes across the shallow limit it will apparently cause git to very very inefficiently select all the objects necessary to "promote" the local repository to a full copy. [1] it apparently gets away with the flat structure because the tree is relatively small and there are no subtrees, it's a flat list of files
Is there a reason this post is using `stainless` from github rather than crates.io?
Thanks for the detailed response! Basically, my experience is: * Websites which use no or minimal JS (like Reddit) are fast. * As JS, AJAX, and various frameworks have become more pervasive and "sophisticated" over the years, my web browsing experience has gotten progressively slower. * The slowest websites are the ones which seem to use the most JS (like Twitter, or ones I try to not even visit any more like Wired, Verge, etc.). * I don't believe I've yet encountered a no-JS or minimal-JS website that was even remotely slow. This is the sense in which I identified JavaScript as the most likely culprit. That said, there *are* some websites which seem to use plenty of JS and dynamic stuff yet feel acceptably fast (like Facebook), though not as fast as minimal-JS sites. So your suggestion is that a more plausible theory would be that slowness is caused by JavaScript triggerring costly layouts? Another thing is that after browsing some amount, usually around half a day or so, the whole browser becomes so slow that I just have to restart it. This also seems related to JS - my suspicion is that it might be the GC, which Firefox's built-in profiling seemed to show as well, when I tried it yesterday. (It was using around 2.5GB at that time but I had many more gigabytes free, so was not under any direct memory pressure.)
In addition to /u/engstad answer, you can find some open information from Sony on their SCEE research web site. Additionally if you can get hold of the PS2 Linux manuals, the PS2 high level API was made available to the Sony Linux distribution.
There's a section in the post headed "Templating", though it's using handlebars rather than a Jinja port.
I would rather have it as a separate project, but have cargo an option to install it as well as a way to transparently use it so that: &gt; You are trying to use package foo which requires Rust x.y.z/x.y.z-beta/nightly which is not installed in your system. Would you like cargo to install it and switch to Rust x.y.z/x.y.z-beta/nightly for this project? If the user types yes, cargo could configure multirust, fetch the appropiate rust version, and set things up for that local project. Updating to more modern Rust versions would be even easier, since with multirust support cargo could handle that as well.
The last couple of months has been big for rusty-cheddar; it has undergone a name change to [rusty-binder](https://gitlab.com/rusty-binder/rusty-binder) and is no longer a C header generator, it is now an extensible bindings generator for any language (in theory)! Unfortunately this is all still very much WIP so don't expect too much for another month or two.
&gt; It's a meaningless and futile debate. I disagree. This debate also exists in the Rust world as well, regarding panic vs Result. I do think it's a good thing that panic isn't seen as error handling. &gt; I consider ? and catch to be general control flow constructs which are especially useful for error handling, not dedicated error handling constructs. I think this is really what it comes down to. I'd never suggest that, for example, `do` notation be restricted to `Result&lt;T, E&gt;` only. But that's because `do` isn't about errors, it's about the algebraic properties of the data and code.
I don't want to get into an argument about subjectivity, so let's leave it at that. :)
I think part of it is not exactly _javascript_ being slow, but all these frameworks dirtying style all over the place based on complex JS. Style dirtying is sort of like I/O, it can become a bottleneck. The speed of the JS interpreter/JIT itself is not that big a deal (it can be though, depends). So yeah, this can trigger costly relayouts and whatnot. If GC is an issue in Firefox try going to about:memory and checking the GC heap sizes.
Do you need a `()`? ^^^I ^^^pronounce ^^^unit ^^^as ^^^"hug"
See https://github.com/rust-lang/cargo/issues/2452
No worries! Great job.
I have but what should I look for?
Er, no idea, sorry :) When I use it I usually check the perf of a specific page and sometimes use it to manually GC.
Instead of [using a loop to match regexes in your router](https://github.com/fengsp/pencil/blob/97c899e97e9ac52024c272a091916f144e5cee09/src/routing.rs#L258), you might consider using a [`RegexSet`](http://doc.rust-lang.org/regex/regex/struct.RegexSet.html) to determine which rules matched the given path. Once you know the rules that matched, you can then run `captures` on only those rules. (If your web app has only a few routes, this is unlikely to matter. But if it has a few hundred, maybe it will help.)
Comparing this to to my personal workflow: - I don't release unless a full set of clean tests run. - I never `cargo update`, because it makes it *really* hard to tell if the minimum versions you've specified in `Cargo.toml` **actually** work. - I format all my tags as `vM.N.P`. - I don't have a `-pre` suffix... though I hadn't considered doing so before. - After releasing, I update the online docs. It might be a good idea to have this command structured as a sequence of sub-commands that can be overridden or added to. Having a way to shell out to external programs in the pipeline (and abort if they fail) would be neat. --- As an aside: what I'd like to see are package-local cargo subcommands that can be *either* a script *or* a package. The reason is that I have an `update-docs.py` script that I use in my repos. I end up having inconsistent copies of varying ages in my repos, because I want anyone checking the code out to be able to correctly build the docs. But that's not only a duplication of effort, but you now also need Python 3, and it's not immediately obvious that these scripts are there. It'd be awesome if I could just say "I depend on (package X | script Y) as a package-specific utility". That way, I can have package-specific scripts runnable through Cargo without requiring the user to install it globally, faff with their `PATH`, or install Yet Another Dependency.
Perhaps an option for tag prefix could address this? 
I think it'd be nice to just give the path to the `Cargo.toml` (defaulting to the `Cargo.toml` in the CWD) and figure it out from there? If people standardize around `cargo release`, then this also has the benefit of uniform naming of tags.
Would release sub-commands or a `Cargo.toml` `[release]` section be a better solution? I'm concerned about long commands and portability. (Particularly between Linux and Windows.)
I will be doing this from now on
Regarding indexing by non-usize, you might be best off creating a `Memory` struct, which impls Index&lt;u16&gt;,wraping a Box&lt;[u8; 0x1000]&gt;. You can also add things like `read_u16` to read in big endian 
Coincidentally, I just finished a prototype, written in Python, of the exact same thing last night. https://github.com/ereichert/vors And I started working on the Rust version last night as well. https://github.com/ereichert/cargo-release/tree/develop It's good to see that some of the ideas I had about this shared by others. It's good to know I was on the right track. I like the idea of configuration via Cargo.toml. I'm going to keep working on it.
I have been meaning to enter a feature request for this, when one does an `update stable` and it pulls in 1.7.0, it should also note that it has 1.7.0 in its database so I can later pin to a specific version w/o having to redownload 1.7.0
Stainless looks awesome. Is there anything stopping it from writing test names more readably, like [many spec runners do](https://mochajs.org/#reporters)? I assume you'd have to built your own test runner for this.
&gt; Twitter Did a quick check on switching from main timeline and notifications. Takes 1.4s of JS time when doing it. Of the 1400 ms: * 600ms is spent on cleaning up the currently visible page. It looks like each tweet has its own teardown routine and they add up to a lot. This is the sort of thing I mentioned about obvious algorithms breaking the rules. I assume this can be made much faster but it would require actually working through the code. * 170 ms is spent on "cleanData". This is the sort of serialization that's JS performance limited. * 44 ms is spent in a single forced layout before rebuilding the page starts * 378 ms is spent building the new page, mostly in attachChild calls. * 150 ms is spent setting up the images/video previews. Of this set, 83% is spent on DOM manipulation. Servo will reduce some fraction of this by a large amount but without benchmarking I'm not sure which parts and by how much. The code isn't pathologically bad (there's only one forced layout) but it's not super efficient either. I could poke holes at specifics but I know it's hard to get DOM perf out of a large codebase that has a lot of people working on it. The best generic approach I know of is virtual dom diffing (React) but if Servo improves things enough it might obsolete the vdom approach. &gt; Wired, Verge, etc. These run a ridiculous number of analytics/tracking scripts.
I'd like this too. The same thing is true of archived toolchains. multirust should realize they are all the same thing.
[Here's the status for both](https://wiki.debian.org/Teams/RustPackaging). Looks like it's in Debian unstable at least.
I'll +1 the request not to change the name of the whole project to rustup. It's churn with no real benefit.
I think you already found the answer. Under the hood stainless just transforms the tests into "normal" Rust tests so it's restricted by what the underlying implementation provides.
Being subscribed to a few knife subreddits, I wasn't sure what to expect
I use `.ok()` for that a lot, but I'm sure that's a horrible practice.
&gt; I never cargo update, because it makes it really hard to tell if the minimum versions you've specified in Cargo.toml actually work. This is a huge shortcoming in cargo that I have barely seen discussed. Is there any potential solution to this? This also doesn't really solve the problem as you could add a version, which isn't the newest, cargo will download a newer version and you don't know if it works. Maybe we need a cargo sub-command to build with the oldest supported versions to make sure it works with those as well?
Would it be? I thought drop transfers ownership which forces the drop mechanism to be called when drop "ends" Could dropping the result cause unintended side-effects that `let _ = ...` wouldn't. Or does `let _` also cause the drop to run immediately?
Maybe it's the simplified example, but is this so bad? type BigObject = u32; struct Cache { assets: Vec&lt;BigObject&gt; } impl Cache { // return value implicitly gets same lifetime as self pub fn get_vector(&amp;self) -&gt; &amp;Vec&lt;BigObject&gt; { &amp;self.assets } } struct Consumer1&lt;'a&gt; { // storing references requires lifetimes vector_of_things: &amp;'a Vec&lt;BigObject&gt; } impl &lt;'a&gt; Consumer1&lt;'a&gt; { // need to know that the cache lives longer than the object to store a reference pub fn init&lt;'b: 'a&gt;(&amp;'a mut self, cache: &amp;'b Cache) { self.vector_of_things = cache.get_vector(); } // no explicit lifetime needed pub fn just_use(&amp;self, cache: &amp;Cache) { println!("{}", cache.get_vector().len()); } } fn main() { let c = Cache{ assets: vec![] }; let mut c1 = Consumer1{ vector_of_things: &amp;vec![] }; c1.just_use(&amp;c); c1.init(&amp;c); } It seems technically possible to elide lifetimes in struct declarations but from what I have found that is not allowed currently due to unanswered questions. Does every reference get the same lifetime? Do they get separate lifetimes? Are any of them related at all? Are they assumed `'static`? What about consistency with elision in functions? How does the compiler find the "source" lifetime? What are you attempting that gets so out of control?
If you want to return and store references, you'll have to bite the bullet and deal with lifetimes, no way around it. For this case [these](http://is.gd/9R72xZ) are the lifetime annotations you'd need to add (I also commented as to why each one is required).
I usually use `unwrap`, since it's relatively rare that I expect the program to keep working properly if a function errors out.
`_` has the advantage of being more generally applicable, since it's just a pattern and can appear in any other capacity that patterns can, e.g.: let (foo, _, qux) = thing_that_returns_a_three_tuple();
You can also prefix a variable you won't use with an _ to stop the compiler from giving a warning. This is useful if you won't use the variable but want to document what it would have been. Ie in the fall through case of a match statement bind to _invalid
&gt; Edit: Interesting to see how let _ = ... and let a = ... changes the drop behavior even though a is unused. Yes, this is a subtlety. `_` never binds in the first place, and so it acts like a temporary. `a` does bind to a value, which then goes out of scope at the usual time, regardless of use.
Yes, this is my preference as well.
Go for it! Let me know of any functional of ergonomic issues you find - I'm always looking to improve it!
I usually use `.expect("that should never happen")` (string differs) to make very clear that I am (mis)assuming there. 
(How) Can I start helping out with TWIR? (Is there anything I should or shouldn't work on? I guess RFCs, Final Comment Periods, and the like should be done very soon before publishing.)
Like people said its an explicit way to ignore a variable, this works then a variable starts with `_` this means not only that you can use `_` has an explicit throw away but you could us it has a named unused throw away, like arguments on an over writable trait method like `fn f(&amp;self, _name: &amp;str)` where the user might want to use `name`, but that by default you don't, because argument names even fall in the documentation. 
`.expect("shit's fucked up")`, until it happens during a demo.
Thank you very much for the example. It helps alot. My problem though becomes more apparent when you wrap the consumers in a parent as well (as is the case in my codebase ... I have a parent that wraps consumers of this cache). Here is where I can get to: http://is.gd/JkGwbk At this point, the lifetimes are now moving further up and if in future I decide to also wrap this parent object... they'll move further. Also, how do I initialize the Consumer when one of its fields is a reference to a vector? This is the point at which I was thinking I was doing it wrong and perhaps I should Box something somewhere ... but I'm just not sure if I'm heading in the right direction or not :(
This sounds interesting - can you expand on what you mean? The cache is _technically_ lazily static.. so this could be what I'm looking for?
Yeah, without the compiler supporting lifetime elision on structs I don't think what you want is possible with direct references. You may be able to use [`Rc`](https://doc.rust-lang.org/std/rc/) or a plain `Box` instead, though I'm not convinced you should.
I never use expletives in code as a strict rule. It's funny until it blows up in another persons face. :)
It does bind and not warn, although it is rarely used (only when the destructor performs a useful cleanup action). I use in inside rust-stm, to properly set and later reset a thread local boolean, even when the scope is left with a panic.
You mean rust-laser-build?
It binds. That's why you see occasional stuff like [this line in Servo where the destructor is used to guarantee that stuff is called](https://github.com/servo/servo/blob/master/components/layout/layout_thread.rs#L825).
Oh, that's cute. Every time I've donated code to the open source community, it's been a *donation*, without any expectation of reward for my work. Generally speaking, the work was the reward. If you showed up on my doorstep with a briefcase full of cash, I'm not going to mind of course, but really I don't expect it nor want it. Are people living in third world countries *exploiting* aid workers who come and help them build a house or obtain clean drinking water? Of course not. Am I being exploited by the impoverished open source community who uses my donation of time and effort to enrich humanity in my very small and insignificant way? Nope. I know for a fact a bunch of projects made money using my Cocoa/Objective-C implementation of an IRC Client library in their iOS applications, and I really don't give a shit. It was pretty funny though when I was getting emails from people asking how to get it to build on iOS 8. :D
Is ignoring an error a clippy lint? If it isn't, then should it be?
What exactly are you trying to do that requires a ping/traceroute/DNS lookup library?
Thanks, I saw that while I was searching yesterday and was hoping that someone had started work on them already and I just didn't find them.
The problem you face is that ping uses raw IP (not TCP) sockets. It requires root or CAP_NET_ADMIN to function. From a security POV you are better off delegating this to a separate process. That being said it's probably pretty straight forward to write a ping clone in rust.
Use slices for all they're worth. Everywhere you can. In update, receive a `&amp;[u8]` argument. Use .clone_from_slice() or some even better method to fill your internal buffer from the slice efficiently. The chunks loop in compute_block is possibly not as efficient as it should be (profile). Investigate if crate byteorder can improve this? Otherwise you could use memcpy to transfer data from the byte buffer to the word array. Integers have byte swapping methods if you need them (.to_be()). Performant implementations of md5 often partially unroll the main loop.
`match` takes a value and evaluates to the arm whose pattern matches the value. It's like `switch` except that instead of matching exact values, it can also match (or "destructure") certain kinds of structures: enums, structs, and borrowed pointers (off the top of my head). `Ok(val) =&gt; val` is extracting the value contained within the result of the `parse` call and evaluating the `match` as a whole to that. Also note that `match` is an *expression*, like addition or calling a function. There's no implicit returning going on here; the `match` construct *as a whole* evaluates to the result of the arm that matched.
Websites where you can ask questions are this subreddit or on the forum at users.rust-lang.org. You can also ask questions in the rust IRC channels on the server mozilla.org, there is a particular #rust-beginners channel for beginner questions. A big difference between Rust and Python/Java is that in Rust "everything is an expression." (Technically, statements beginning with `let` are not expressions). So every statement inside the body of a function except for `let`s are evaluated to a value, just like this match statement. Some of this is obviously like Python and Java: `2 + 2` evaluates to `4` and `foobar()` evaluates to whatever `foobar` returns. But this also applies to control structures, so `if true { "Hello, world!" } else { "Goodbye, world!" }` is an expression that evaluates to `"Hello, world!"`. You can do things like `let foobar = if baz() { ... } else { .. };` because of this.
Can anyone elaborate on this? Are there significant performance differences between a Vec and a slice? And, if so, are those differences present even when the size of the Vec is known upfront and no inserts take place? 
As a fellow newbie, I'll try to add some more information. Rust doesn't do exceptions. So how do you handle anomalies then? Through the return value. To make that a bit easier, Rust has a standardized way to do so: put the result in a data structure that can be a "normal" return value, or an error. This struct, called Result, is an "enum", which means that its direct value is either Ok(&lt;something&gt;) or Err(&lt;something&gt;) and as you can guess, the return value for a successful return is put in Ok(), the error in Err(). E.g. std::str::parse::&lt;i32&gt; returns Result&lt;i32, Err(something)&gt;. Then the question becomes: how to get it out? That's what match does. It matches a (return) value against a series of patterns and the list of patterns must be exhaustive. So, if you match the result from parse against Ok(x) and Err(y), it will succeed matching against Ok(x) when parse returns Ok(some number) and against Err(y) when parse returns some Err(...). The syntax lets you bind the value inside Ok(...) and Err(...) to a local variable. So after Ok(val) =&gt; you know that the return result was Ok and the value that it wrapped has been put in val.
It's a good way to define default behaviour for the result of a match
I am slightly confused about what's going on, could you provide a full code sample, with the error you're getting?
Thanks! Much more elegant than exceptions.
r should have been a borrow_mut(), that might clarify a bit more. but for instance, lets say I want to access the keys, if I do let some_struct = SomeStruct::new(); let keys = some_struct.elems().keys(); // Error: elems doesn't live long enough The other problem I'm having might require a more involved post, I need to think about it more. 
Also check the information about ways to make error handling easier, like the try! macro and functions like .and_then(). It can be much to absorb at once, but take it in slowly; it's quite cool: https://doc.rust-lang.org/book/error-handling.html
Yes, but there are still things like `Value` you don't explain anywhere. A full program would be very useful.
If you don't mind to be root to use your program, you can use the socket crate which gave you access to raw socket. If you search code example, you can take a look at this project: https://github.com/leonard-IMBERT/ping_catcher. It's a simple library I wrote that listen for ICMP messages. The code is free to use.
For an example of Jim's writing style, see O'Reilly's promotional teaser for this book from last year: http://www.oreilly.com/programming/free/files/why-rust.pdf And if anyone ends up purchasing this and has feedback for Jim, I'd be happy to relay it to him. :) EDIT: And thank heavens they didn't screw up the animal on the cover. :P
I am so exited for this book.
Amazing! Are there plans for a kindle version? (O'Reilly site is currently down for maintenance)
Site seems to work here, and says that it offers ePub, Mobi, and PDF formats for ebooks.
Couldn't it be ported to rust macros quite easily?
Looks great, are there/will there be exercises in the book?
I've been working on a crate that provides an interface to `libcgroup` in the hope of providing a solution for https://github.com/rust-lang/rust/issues/31406 The crate will also provide a `spawn_with_cgroup(cmd: Command, cgroup: CGroup) -&gt; Result&lt;Child&gt;` function as well as `CGroup` and `Controller` types. At the moment the basic `CGroup` and `Controller` impls are done - so I can create, fetch, add controllers and groups. I'm just waiting on work to relinquish the copyright so that I can release it (we have a process for this). 
I miss the animal :&lt;.
Another perspective I think. 
Where is his 'blog? Google doesn't seem to know. Thanks.
Isn't the cpu/memory/energy consumption more interesting if it is limited to the same refresh rate?
Yes, as /u/Hauleth says, I am very interested in having multiple resources, generally. Different people learn differently, and so even if it covers the same concepts, if it covers them in a different way, that's valuable. I'm also excited because a lot of people consider a technology to have "made it" if there's an O'Reilly book on it; it's a sign of maturity, and that can only be helpful.
Is there a nice intro or tutorial like "hello.rs in SmartOS"? Looks very interesting, just a bit overwhelming. 
Awesome, I'm looking forward to playing around with this.
"Page not found" for me :/
Let's hope that Rust being #1 wasn't just a placeholder ;)
At first, I was not looking forward to inviting a co-author on, but when I thought of asking Jason, it was obvious that the book would benefit immensely. Jason and I have worked together on other large projects, and he's always been a great influence and a great contributor.
Thanks! That's who I was thinking of. I don't know why I got them confused. Thanks also for the rec. Will try. 
In the `let mut people = [...]` case, `people` is an array of type `[i32; 5]` on the stack. In the `let people = &amp;mut [...]` case there's still an `[i32; 5]` array on the stack, but the `people` variable contains a `&amp;mut` borrow of the array instead of the array itself. The `people.sort_by(...)` makes no difference since [std::slice::sort_by](http://doc.rust-lang.org/std/primitive.slice.html#method.sort_by) takes `&amp;mut self` as invocant, and the `.` operator will take automatically pass a `&amp;mut people` in the plain array case to `sort_by`. 
I'm probably going to end up taking you up on your offer. I'm busy working my day job and in the process of learning and building my first project with Rust. I'll start digging into this early next week. 
It might be instructive to look at the MIR output in the playpen links. They're nearly identical, its just that the `&amp;mut` variant has an extra variable on the stack in `main` for `people` (since `people` is a reference to the array and not the array itself it needs both the array and the reference), and there's an additional indirection in the `bb0` section to dereference `people`. tmp4 = &amp;mut (*var0); You can see similar differences in the ASM output but I find them less readable. No doubt the differences would go away under optimization.
&gt; Our book will hopefully make sense to anyone, but we're more addressing people like ourselves: experienced C and C++ programmers who want a better type system (and just a better ontological approach overall, I think!) […] Thanks to this comment I just preordered it on Amazon -- now I’ve got something to look forward to … in November =) 
I look forward to hearing from you! There's a little group of us forming over in [#libpnet on freenode](https://webchat.freenode.net/?channels=%23libpnet) if you want to talk a little more real-time (though, admittedly, that's subject to timezone differences and working hours...)
&gt; You should prefer taking a generic iterable type over than Iter&lt;u8&gt; specifically; &gt; You don't need to return &amp;Md5; in fact, going from &amp;mut Md5 to &amp;Md5 is counter-productive. Could you elaborate more about those two points please?
Yes, we usually do too. The function used by OP was orginally intended to be used in a different way, but the ergonomics didn't quite work out and we haven't gotten around to rewriting it, so we just ignore the return value (which doesn't indicate something bad happened).
&gt; i guess not all things in life are this simple. apparently not but that looks quite nice ;)
His whole point is wrong and has little justification. If making things faster didn't make them more usable, we would have stopped doing it decades ago. Modern optimizing compilers are as "unreliable" as airplanes are dangerous: a few people are irrationally afraid of it, and it almost never happens. On the contrary, making code faster means that it takes less energy to run, which means that it costs less money to use and demands less from the environment. At the scale of a single program this is small potatoes but across all of the code ever written -- because it's all been touched by optimizing compilers, mostly -- it adds up to a big deal. If optimization was actually that much of a risk, do you really think the whole multitrillion-dollar industry would have been using it for the last 30 years? Hating on optimizing compilers is just something that nerds do to pretend they're smarter than other nerds, because reliability is hella chic.
It seems in scala, you can't "rebind": val _ = foo() val _ = bar() is an error: Error: _ is already defined as value _ val _ = bar() ^
I'm not sure exactly what his/her points are, as the post is extremely short and not directed towards Rust. He/She is correct in saying that compilers cannot do many optimizations that a human can, but this is fairly common knowledge. On a side note, I find it odd that the author chose to highlight the example of moving a piece of data to the stack from the heap heap as something the compiler cannot optimize. The prime example of compilers being unable to optimize without risking code breakage is when a function has pointer parameters and they aren't marked "restrict" (I'm talking about C here). Generally the programmer thinks about decisions such as where data is being stored in memory, and designs accordingly. Before I learned about assembly, I certainly wouldn't have realized that the compiler has to consider corner cases like when 2 pointer parameters are the same value, or that you're loading the value of a variable from memory rather than caching it in a register if you're dereferencing instead of making a local copy. Idk though lol
I have been working on the public website, and have added a new forum for users to use at: https://discourse.redox-os.org This is modeled after https://users.rust-lang.org and is completely open and allows anyone to sign up, unlike our mattermost chat. I hope people who are interested will sign up and start posting. There are also detailed kernel and libstd docs at https://doc.redox-os.org We have an ISO release (WIP) at https://static.redox-os.org
I actually saw it. The page has more stats. I think the interest in rust has increased with 436.00%. And a job in Hadoop earns more than in F#. Things like that.
Looks like this was an accidental leak, let's hold off on celebrating until this gets officially released next week. :P 
Wow, thanks for actually investigating this! Needless to say I'm a lot more excited about Servo knowing that it'll actually help improve my experience. Out of curiosity what makes analytics/tracking scripts (and ad-serving in general) slow things down so much? Have a hard time believing it's the network traffic, on a 100Mbit connection. And given the function they perform it doesn't seem like the scripts *should* be that complicated/intensive...
&gt; Out of curiosity what makes analytics/tracking scripts (and ad-serving in general) slow things down so much? Prepare to be amazed at the [awfulness of analytics/tracking](http://blog.lmorchard.com/2015/07/22/the-verge-web-sucks/). They tend to not impact things that heavily post-page load aside from keeping your phone antenna active.
http://webcache.googleusercontent.com/search?q=cache:stackoverflow.com/research/developer-survey-2016
The section on strings looks a little murky. I don't know *any* Swift, so I can only comment on Rust. I think in general, it would probably help to have small code samples in the text to add more clarity. I'll just note some things I saw. &gt; This caught my attention in part because dealing with things like strings (or other pass-by-value types) in Swift is rather more straightforward than in Rust. The outcomes are much the same, but since all Strings in Swift are passed by value (never by reference), you simply don’t have to think about modification — even safe modification! All `String`s are passed by value in Rust too, where ownership is transferred. An `&amp;str` can be used to lend out an immutable borrow to a `String`. `String`s can't meaningfully satisfy `Copy` since the actual string data is on the heap. (I note that `&amp;str` *does* satisfy `Copy`.) &gt; Rust’s strings are good, but not quite as sophisticated (presumably for simplicity around the memory mapping). Some examples would help here? &gt; All Rust String or str instances are composed of utf32 Unicode scalars, encoded as utf8 sequences. It doesn’t have some of the convenience methods Swift does for getting any of the other representations. That said, I expect this should show up rarely if at all in my ordinary usage. Importantly, the fundamental storage is the same: both use scalars. UTF-{8, 16, 32} are character encodings while Unicode itself is more like the abstraction. UTF-{8, 16, 32} describe how Unicode codepoints can be encoded into bytes. Saying that the "storage uses scalars" is not quite accurate, or at least, is not precise enough. If you said that to me without any other context, I'd assume you meant that the representation in memory was `Vec&lt;char&gt;`. Certainly, that's one way to represent a string. Maybe Swift does it that way, I don't know. In Rust, the representation is UTF-8. (I think it's clear from your writing that you do know this! :-) I just wanted to add some clarity here.) In general, Rust's standard library takes the "UTF-8 everywhere" approach (with some exceptions). Decoding (or encoding) other character encodings is best left to external crates like [`encoding`](https://crates.io/crates/encoding). (I note that Go's approach to strings, with respect to "UTF-8 everywhere," is quite similar to Rust's approach. Including the bit about transcodings being done in an external library. One key difference is that Go uses UTF-8 by convention, where as Rust uses UTF-8 by force.)
Rust does not allow you to run code every time something moves, and has no plans to change this. There are three reasons for this: * Unsafe code should be able to move a value using `ptr::copy`. This makes implementing safe, performant, generic abstractions on top of unsafe code much easier; compare implementations of the STL with the Rust's libcollections. * The compiler should be able to optimize away moves with no observable change in program behavior. * Since move constructors run "between" the time when a value is owned by one thing and when it's owned by another, it's unclear how the API should be designed anyway. 
As a reminder, we always welcome posts on [Rust Code Review](http://codereview.stackexchange.com/questions/tagged/rust); just peruse the [how to ask](http://codereview.stackexchange.com/help/how-to-ask) docs (which is useful for any place, really). ---- 1. Make sure to include your full code; don't leave out any `extern` or `use` statements. When I code review, I compile and run your code to make sure I'm not misleading anyone. 1. Using [`expect`](http://doc.rust-lang.org/core/result/enum.Result.html#method.expect) can shorten the error handling by panicking case. 1. There's no need to have a type specifier on `number`, as that type can be inferred. I'd probably roll it into one line as well. 1. I'd make the short match arms only take one line. 1. Defining functions inside other functions is nice, but I only use it when the inner function is really tightly coupled to the outer context. I'd suggest moving it out to a top-level function. 1. I want to **commend you** for using `gen_range`. Many people just use modulo and never know the biased random numbers they are generating! ---- extern crate rand; use std::env; use rand::Rng; fn main() { let args: Vec&lt;String&gt; = env::args().collect(); match args.len() { 1 =&gt; println!("No arguments passed"), 2 =&gt; { let number = args[1].parse().expect("Not an int"); let result = roll(number); println!("{}", result); }, _ =&gt; println!("Error: too many arguments passed"), } } fn roll(number: i32) -&gt; i32 { rand::thread_rng().gen_range(1, number) } *If* it makes sense, I'd suggest changing your code to handle arbitrary amounts of numbers on the command line. This both simplifies the code and makes it more powerful: fn main() { for arg in env::args().skip(1) { let number = arg.parse().expect("Not an int"); let result = roll(number); println!("{}", result); } } In **The Future**, you might be able to use *slice patterns* like this. Note that it removes an error case where you have to do bounds-checking on `args[1]`: match &amp;args[..] { [_] =&gt; println!("No arguments passed"), [_, ref num] =&gt; { let number = num.parse().expect("Not an int"); let result = roll(number); println!("{}", result); }, _ =&gt; println!("Error: too many arguments passed"), } Writing it in this way also shows that if someone invokes your program with **0** arguments (it's hard to do but possible), you will say you had "too many arguments"!
Which browser do you use? I remember having issues to login on crates.io with some browsers.
Firefox.
More info, I tried on Windows and I can login in both Chrome and Firefox. I tried deleting cookies on my Linux box, still can't login on Firefox or Chromium.
 while true { loop {} } _chuckles quietly_
I found the issue, this explains it: https://meta.discourse.org/t/github-oauth-does-not-work/29640. If you go to `http://users.rust-lang.org`, you can't login. If you go to `https://users.rust-lang.org`, you can login. Usually you get redirected from http to https, but for some reason I wasn't.
Miss what animal? We got a crab! :)
The main reason C++ has move constructors is because it has no way to invalidate things once you've moved from them. The move constructor is responsible for setting the original object in a safe state so that it can be destroyed later. When moving a heap allocation, for example, it has to set the original to some dummy pointer so the actual allocation that was moved doesn't accidentally get freed when the original is destroyed. In Rust the original is simply invalidated, so there's no need to set the original to some dummy value since the original no longer exists to be dropped, thus eliminating the need for move constructors.
My use case wasn't actual related to invalidating the old value but rather updating some internal self referential pointers within the moved structure since they become invalid after being moved.
Makes sense. Thanks for the explanation :-)
rr: literally a step backwards in debugger technology. 
Exactly: a small step backwards for a debugger, a big leap forward for debugging. :-)
I read through the follow-up responses (all helpful too - love the Rust community!), as well as your responses. I ran into pretty similar issues trying to write a game framework. Rust doesn't lend itself to storing references to other objects. This seems annoying at first, but IMO it can lead to simpler and easier-to-follow code. In your case, I bet you can get away with simply passing the reference into a struct method. In your `impl Consumer`, block, you could add a function `pub fn do_stuff(&amp;mut self, &amp;Vec&lt;BigObject&gt;)`, and have the struct do its business there. This obviously won't work in every case, and it may not work for you here; but it might be a good starting point. My philosophy in Rust is that if I'm wrestling with the compiler, it's for good reason, and I'll change my design :)
Hey thanks for the response! I agree.. and so I actually settled on a fairly simplistic idea that works great. Basically, I call a `precache` method on my `Cache` that returns me a `CacheResult`. This object has an `index` and `length` for the assets to cache for a particular instance. Then, consumers use the `CacheResult` index to index into the `Vec&lt;BigObject&gt;` _field_ of the `Cache` instance (rather than trying to do it via a method call). This seems to satisfy the checks and works a treat.
&gt;&gt;Rust’s strings are good, but not quite as sophisticated (presumably for simplicity around the memory mapping). &gt; &gt; Some examples would help here? I think the author meant lack of "embedded expressions" in Rust, considering that to achieve something similar in Rust would require `format!` which is comparatively very verbose...
Well... you know when your data is gonna be moved (being passed as T instead of &amp;T or &amp;mut T), so why don't you just call come `t.fixme()` function after moving? Yes, it is possibly unsafe, as you might forget to call this function at some point, but moving isn't intended to modify the moved data anyways.
&gt; Are you running your test suite with different optimization levels and comparing results Of course. I am running my test suite in debug and release modes, with/without ASAN,UBSAN,Valgrind (MSAN is hard to setup but I use it sometimes and I am not doing anything that requires TSAN). I try to code floating-point math so that I get very similar results in debug mode and with `-ffast-math`. &gt; How does one diagnose and deal with compiler errors? Just two things, be scientific (hypothesis/test/check), and bisect (never use "linear search"). If you truly hit a compiler optimization error, 90% of the time that will be due to undefined behavior in your code. If I am running the whole sanitizer suite I know whick kind of undefined behavior it cannot be (e.g. ASan catches all out of bounds memory accesses, MSan catches all pointers to any stack-frame whose life time has ended, usage of uninitialied memory, ... UBSan catches all signed integer overflows,...), but I follow the same approach to track both. I basically start in release and reproduce the error. Then I search the optimization levels till the error disappears. Typically most errors disappear already from `-Ofast` to `-O2`. Then I check out the list of optimization options between the level that works and the one with the error, and start turning those on manually till I find the culprit. If you are unlucky it will be a combination of options. Afterwards I just check git, and think: from the last commit, what thing have I implemented where this optimization could trigger, does any of that has something to do with some form of undefined behavior that could be exploited by this optimization. Sometimes it is just obvious, some times it isn't. If it is not obvious, I revert to the last commit and manually try to reproduce step-by-step the changes to the one that triggers the error. Once I find the culprit, I try to isolate it. Sometimes this can be done, or can't, but at least at that point it typically is clear if your code has UB. If it has, you fix it, if you don't know: you can generate 2 preprocessed files that differ minimally from each other and fill a bug report. Tools like delta or creduce help, but they have a steep learning curve and with the preprocessed files some other people can use them. &gt; then refactoring application code until stable? Well if it is UB in your code you fix it. Otherwise, you already know where, what, and partially why (which optimization triggers it). You can code things a bit differently and maybe the error disappears. Just leave a comment pointing to the bug report and be done with it. If the error is a front-end error (e.g. it implements some part of the programming language incorrectly), then when you post the bug report people will give you a couple of workarounds or tell you exactly which feature is not implemented or buggy, so that you can recode it without using that feature that way. I hate doing this every time, but at the same time I love to see how fast these things are actually fixed. If everybody would track down these issues our tooling would be amazing. Even if you cannot help develop the compiler, doing this is as if you were writing an unit test for it. That does help.
Very Linux specific, but there exists a facility to send pings without being root by using ICMP sockets (albeit a specific sysctl must be set appropriately and it's 0% portable). Example from stack overflow: [ICMP sockets (linux)](http://stackoverflow.com/questions/8290046/icmp-sockets-linux)
The problem here is obviously that your code is not optimized at all. To demonstrate that I made a simple optimization to `update()` which reads chunks directly from `bytes` instead of copying them byte for byte into `buffer` only for it to be consumed in `compute_block()`... After that the code runs about about 60% faster already and thus only about twice as slow as the highly optimized OpenSSL variant. I'm sure one could continue on that and make it even faster... https://gist.github.com/lhecker/4ddfc000dafa9fffd961
Indeed it is! I was a bit confused why the compiler thought I should be giving it only 1 type argument rather than 2 as the documentation said, until I figured it out...
C++ code cannot link to C++ code if the standard library and/or compiler are not the same. How would Rust be able to do what C++ cannot?
I think it would have no means to detect that the value moved and the raw pointers should be updated. (or at least I don't see how - is there a way?)
Indeed, replacing `find` to actually do the right job : fn find(s: &amp;[u8], p: &amp;[u8]) -&gt; Option&lt;usize&gt; { 'outer: for x in 0..(s.len() - p.len() + 1) { for y in 0..p.len() { if s[x+y] != p[y] { continue 'outer; } } return Some(x); } None } yields more plausible results: 100000 loops with u32 Duration { secs: 0, nanos: 12562234 } 100000 loops with u64 Duration { secs: 0, nanos: 8413542 } 100000 windows with u32 Duration { secs: 0, nanos: 12175651 } 100000 windows with u64 Duration { secs: 0, nanos: 3248245 } 
I was just asking because you said rust had no unsafe pointers. Nothing to do with detecting moved values. 
There was a mix up with the constant being specified. Everything should be fixed now and the results are even stranger. If you have a second, can you follow the playpen link and make sure all is correct. Here are the new results: 100000 loops with u32 Duration { secs: 0, nanos: 3879406 } 100000 loops with u64 Duration { secs: 0, nanos: 11796660 } 100000 windows with u32 Duration { secs: 0, nanos: 6668634 } 100000 windows with u64 Duration { secs: 0, nanos: 2556914 } 
I noticed the same thing compiling gtk-rs. It used to take a very long time, now it takes less than 30 seconds.
I was the one who originally came up with Rust's moving concept and I learned something from this post (your third reason, which is a completely valid concern). This is an awesome explanation--thank you for writing it!
Huh, that's surprising. I recently recompiled racer and deps (including syntex_syntax) using a nightly 1.9 rustc, and it took 6-7mins on my system, and I think syntex was a big part of that; this is on a core i7 laptop with 8gb ram, so not really a low-resource system. 
On x86_64, simple instructions for 32-bit and 64-bit are exactly as fast. The only difference is that 32-bit takes up less space, so you might be able to fit more in your cache.
Wow, the `String` type seems quite complex in Swift, I wonder how performance fairs...
Hm, I know I've heard it somewhere though. Is it true for floating point operations or something? Or am I just completely off base?
Internal self-referential structures are already immovable; [try it yourself](https://play.rust-lang.org/?gist=f561191d2885d44777cc&amp;version=stable).
I've heard Rust categorized as both "high" and "low" level by different people. Personally I'd consider it a low level language because, even though Rust takes away a lot of pain points of memory management that are present in C, you still need to know low-level concepts such as min/max values for a {8,16,32,64}-bit type, stack vs heap allocation, etc. These are concepts that are almost always abstracted away from you in a high-level language, and you can be an expert programmer in Python, et al., without needing to know any of those concepts. However, Rust and other low-level languages require you to know them, but in doing so, they give you more control over your application's performance. I'd say your second and third categorizations are accurate. The community is proving that Rust can be used for nearly any application, from web apps to operating systems, by building these things every day. Also, welcome to the Rust community! If you need help while you're learning, there are many people here who will be willing to answer questions for you. Rust has one of the most helpful communities of any programming language I've interacted with.
Sorry, that was my oversight. Running with `--release` makes it run at a comparable speed. Also, on the subject of `iter().cloned()`, I understand from the docs that that will copy the array in the process of iterating it, and my intuition says that that is unnecessary, since nothing is mutated. (So we can just use "the same" values from `bytes`.) However, am I right in thinking that if I did not *not* copy the array at some point, I'd need to guarentee that `bytes` lasted as long as the Md5 struct, because the latter would then contain a reference to the former? (Yay for C teaching bad habits. /s)
I will likely have to take advantage of the community to figure this all out. Thank you!
I've read through the Swift string implementation, it is indeed complex, and pretty interesting. A Swift string has three possible memory representations: * Buffer of ASCII bytes * Buffer of UTF-16 code units * Reference to NSString (only when bridged to Objective-C runtime) Not counting the third case, the `String` struct is basically a slice into the buffer (similar to Rust `&amp;str`) plus an object that owns the string (similar to Rust `Arc&lt;String&gt;`). Copying a string bumps the reference count (similar to Rust `.clone()` auto-inserted by the compiler). Mutating a string causes copy-on-write (similar to Rust `Arc::make_mut()`). This provides the illusion that strings are pure value types just like numbers, while avoiding the worst of the performance problems. In particular, slicing is fast, and repeatedly appending to a string is `O(n)` as it should be. Overall, it's a very interesting engineering tradeoff. Unlike Rust, it's possible to use strings without having to think about who owns them, whether to pass by reference or value, when you need to copy, etc. On the flip side, reasoning about performance is much more difficult because there's so much magic hidden under the hood.
Did the lecturer define how a language is categorized into "high" or "low" with more than an example? How would C++ be categorized? While I'd say C++ qualifies as low-level, with recent additions to C++ it has gotten much closer to being a high-level language. I'd say the takeaway here is that Rust fits nicely into both(high and low), as it kind of brings high-level concepts into a low-level context. /u/NeuroXc also makes a very good point in stating that you need to know about stack/heap etc. in order to (efficiently) work with Rust, which would rather categorize it as low-level. At the same time, C# also has (stack-based) value-types and reference-counted everything else, so it's really not only the low-level languages that require you to know about such things. I think if you have an arbitrary-precision integer/floating point library for Rust, it can easily start to feel very "high-level" as well. Edit: also, for learning rust, I'd recommend starting with the official book on www.rust-lang.org. for any and all questions (as small as they might be), you can ask in one of the official IRC channels, such as #rust-beginners or #rust. Click here: https://www.rust-lang.org/community.html
Instruction-level profiling works well in Instruments for me. Usually I just reverse engineer the assembly back to the Rust in my head, although I know that's hardly ergonomic. I believe there's a Cargo "profile" profile to do this properly (emit DWARF into the executables so Instruments can process it); /u/acrichto would know more.
I am specifically surprised by the benchmark published by [lastmac](https://www.reddit.com/user/lastmac): [here](http://lexborisov.github.io/myhtml/bm/time_0_100.png). html5ever is faster than Google's Gumbo, but is lagging behind myhtml by a factor of 5x roughly, and I must admit I had expected it to tuned a bit given its use in Servo.
One day the box keyword will be stabilized. One day....
This is how I feel about `Range::step_by`
In case anyone is feeling ambitious: [here](https://github.com/servo/html5ever/issues?q=is%3Aissue+is%3Aopen+label%3Aperformance) are all issues of html5ever labeled `performance` :)
&gt; Instruction-level profiling works well in Instruments for me. Usually I just reverse engineer the assembly back to the Rust in my head, although I know that's hardly ergonomic. Damn. Looks like I have no choice but to start learning assembly and reading assembly traces. &gt; I believe there's a Cargo "profile" profile to do this properly (emit DWARF into the executables so Instruments can process it) It seems you can load symbols from an external dSYM file (with the File &gt; Symbols menu), but apparently it refuses to load whatever it is rustc generates.
Turns out the solution is as simple as "don't treat this as desugaring but rather a proper first-class compiler-implemented construct", not sure /u/pnkfelix agrees though.
[removed]
/r/playrust/
Is there a general policy on the style used for passing named arguments to functions? Is the Python-style exception to the "one space per side of a binary operator" rule the norm? That is, should it be "parameter=something" or "parameter = something"? I know the whole "don't sweat the small stuff" principle, but I try to make my style as unoffensive as possible to agreed upon standards in whatever language I am writing in.
Dyon is one option: https://github.com/PistonDevelopers/dyon It intends to be a "native" Rust scripting language. Still in development. There are various scheme interpreters (written in Rust). I tried binding Guile to Rust, but it was just too annoying to work with.
I do not know how to program. Every attempt to try to learn Rust starting from no programming background resulted in me realizing that I needed to back up, and find a more novice approach. I had no idea of what I didn't know. I had to read a bunch of discussions just to figure out that computer science, and algorithm, courses might be the first step (I'm still not entirely sure that's the case). I was thrilled to find this free MIT intro lecture, as it's allowed me to have some concept of what programming is pretty quickly. I don't wish to learn Python first, but it seems I will be while using this course. If there were a more direct approach that I knew of, I'd take it. This is the only approach from zero background that I've been able to recognize for myself.
Tomaka has a pretty good lua binding. https://github.com/tomaka/hlua
Interesting feature of Dyon is that it has no garbage collector. It uses lifetimes and scope-based cleanup like in Rust, but it elides much more eagerly, I think.
Huh, really? Could you file a bug in Rust? We may be able to fix this.
But why is the `by` argument of the Step trait of type `&amp;Self`? Some kind of integer would make much more sense IMO.
So good it's the standard
Basic infrastructure finalized in the next couple of weeks, starting to bear fruit 3 months or so after that.
"Little web server"? 160MB sounds excessive IMO, it's what libsyntax needed, last time I looked, and that's pretty large. Does it use that much memory when compiling on x64, and could you post a log of `cargo rustc --release -- -Z time-passes`? (which includes memory usage) If nothing else, I hope to find some low-hanging fruit.
In my world, rust is a high level language. Here is the way I see it. Many will argue that the thing that makes rust a low level language is the fact that it makes you think about memory and memory layout. In my humble opinion, that is the wrong line to draw for high/low level language. While you can trigger the escape hatch in rust and get to really complex memory management scenarios (through unsafe), rust is by and large memory safe by design. It may be argued that because you can do bad stuff with memory, rust is low level. But I disagree. Even in a GCed language, it is easy to allocate lots of memory in such a way that it is impossible for the GC to collect it. In other words, you still have to manage memory in a memory managed language (even if only a little). In rust as in other GCed languages, the language helps you and holds your hand (or berates you?). Living outside of unsafe means that you don't have to really manage memory, the language is forcing you to do it right. While it is possible in rust to make memory error that aren't possible in other languages, it is also not possible to make data errors (I would argue are memory errors) in rust that are possible in other languages. For example, rust doesn't let you NPE because it banned null. So the fact that rust offers memory safety makes the argument of memory management being the low level line moot IMO. I don't think a language needs a GC to be considered high level. Rust does, however, offer a lot of high level constructs, closures, data structures, generics, interfaces, the works. Compare that to C where it is all structs and functions. That, to me, is the thing that really divides high from low level languages. Now, with this line, it could be argued that C++ is a high level language. And I think you would be correct in that argument as well, with the cravat that C++ is a lower level language than rust (it provides less language features and forces memory managment onto the programmer with little help). In some ways, I would argue that Rust is a higher level language that other languages like JavaScript, Java, or Python. The only languages that I would classify as "higher level" are the group of statically typed functional languages such as Haskell, F#, and Scala. That being said, I don't think any of what I said really matters. The definition as pointed out by other is ambiguous. I don't think it serves much purpose any more as pretty much any language invented after C could easily be argued as high level (and that is the majority of languages in use today). What matters to me is "What is this language bringing to the table". In this case, rust brings high performance and low memory with many good abstractions at the cost of a more complex mental model for the programmer and missing abstractions that are tricky tricky to implement (and some abstractions that can't be implemented without the escape hatch). Just my 2 cents. **Edit** I should note that I agree with the general sentiment of what the other commenters are saying. I figured I would just offer my stronger view of "It is high level" rather than the "It can be considered either". But I don't really care about what level it hits enough to argue with someone over it ;). I'm more offering a view and justifications for that view.
I think it's because Rc implements Deref. Thus, because Borrow is generic, you need to tell it which borrow you're calling. That is, both of these will work: let b1: &amp;Rc&lt;S&gt; = rc.borrow(); let b2: &amp;S = rc.borrow(); And clone doesn't know which of these to clone.
Ranges are not limited to integers or even numbers
Thank you for doing so! &gt; That being said, I don't think any of what I said really matters. The definition as pointed out by other is ambiguous. I don't think it serves much purpose any more as pretty much any language invented after C could easily be argued as high level (and that is the majority of languages in use today). It seems that way to me, at this point, too.
This article is about the idea of "Axioms", from older proposals of C++ Concepts: https://akrzemi1.wordpress.com/2012/01/11/concept-axioms-what-for/ The article lists several very interesting uses and purposes for Axioms (and I think there's no point for me to summarize them here), and probably something similar to axioms could be added to Rust traits. Some people have already expressed in some cases the implicit desire for something similar in Rust, like (from http://maniagnosis.crsr.net/2015/07/abstracted-algebra-in-rust.html ): &gt;For a semigroup, the operation should be associative: (x `op` y) `op` z == x `op` (y `op` z). Unfortunately, I have not figured out how to enforce that. Maybe later. The Rust designers (rightfully) don't want to add features to Rust unless they are quite important, and I respect that. But I'd still like to know what Rust designers think about the idea of something like the Axioms feature added to Rust Traits. Thank you. Edit: this is not an enhancement request. I am not currently in need for Axioms in Rust, despite they look seductive.
More 160mb total system memory usage, which was my concern - whether system memory would be enough during the compile. Compare with haskell which needs well over 1gb for some builds, and even more with multiple compile threads.
...and then you post to the wrong subreddit. /r/playrust :)
At last it looks like `#[deprecated]` will be available in stable Rust 1.9. Library authors take note!
The ```by``` argument should represent the *number* of elements to skip, shouldn't it? It doesn't matter what the elements are. Iow, for a range of cows ( actual milk producing ones ;) ) a ```step_by``` of 3 should give me each third cow in the range, but I don't understand what would be the result of ```step_by``` "lucy" (given that there is a cow named "lucy").
To be fair, there is a 'Book' link on the homepage. :)
I completely missed it. I think a subpage for documentation with the Book, the API, the wiki, ... would be better.
[Here's the benchmarking code](https://github.com/lexborisov/benchmark-html-persers/tree/master/benchmark) if anyone's interested, looks like you have to install all the libraries yourself though.
Also check rotor https://github.com/tailhook/rotor . See the examples and three articles linked at the top of the README. While it's not just async IO with callbacks, it offers a more formal way of implementing network protocols using explicit state machines (which can be composed into higher level state machines) instead of callback spaghetti code.
There is also [Ketos](https://github.com/murarth/ketos), a rusty Lisp implemented in Rust.
Actually, even when two pointer parameters are not the same, they may alias: if you have a pointer to an array and a second pointer to a subslice, their addresses differ but you can still access the same memory without triggering UB (AFAIK).
1) I would consider Rust to be high level. Yeah, you work with memory, etc. but a major goal of rust is to provide practical abstractions whenever it won't hurt Rust's other goals. You can program in a very Haskell-y manner, and Haskell is, arguably, an extremely high level language. Also, while you're managing memory, you're doing it in an abstracted way -- lifetimes, borrowing, are both abstractions for memory management. Rust has essentially introduced high level features for memory management. 2) Definitely General. It seems Rust is good for programming just about anything native. 3) Compiled, of course. But compiled / interpreted are becoming pretty useless metrics. Compiled languages can be interpreted, interpreted languages can be compiled (actually, most interpreted languages are compiled to some extent on-the-fly anymore). Compiled vs. interpreted is mostly a historical artifact.
But it does expose them and a lot of the core types are implemented unsafely under the covers. I was going for self referentiality with raw pointers for performance reasons (though your code was still very instructive, thank you). From the discussion it does sound like there are some drawbacks. I think what I was really going for was a "Move" trait that only activated when the actual physical location of the data changed, rather than just a change of ownership. However I don't know enough about Rust's implementation to know if that could even be implemented reasonably so that the existing optimisations were still possible (the callback on the Move trait would presumably be executed after the type was placed in the new location). I think I mostly wanted to know if this was low hanging fruit that might not be too hard to add in to the existing compiler design :)
[It's not hard to do by hand](http://is.gd/3AXtKX): fn main() { let v = vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; let num_chunks = 4; for chunk in v.chunks((v.len() + num_chunks - 1) / num_chunks) { println!("{:?}", chunk); } } 
What made it too annoying to work with? I was thinking of doing the same thing myself. 
Hold up, I call shenanigans: that's just pasting user input in `stdout`.
You can use `split_at` to split it into two. You can then split those again to get 4: fn main() { let v = vec![0, 1, 2, 3, 4, 5, 6, 7]; let len = v.len(); let (l,r) = v.split_at(len / 2); let (a,b) = l.split_at(len / 4); let (c,d) = r.split_at(len / 4); println!("{:?} {:?} {:?} {:?}", a, b, c, d); } You can also do it mutably with `split_at_mut`.
Thanks. Not because of Deref, though. Answer below is the correct one, it's because of reflexive implementation of Borrow.
As this is (in its current state) rather useless, I'd just like to see whether you Rustaceans have any comments I could use to make it better (and hopefully inspire me to write more!)
I'm not sure that can be done safely without allocation (and to have it work well on all possible hash key combinations we'd need another generic bound in HashMap). If it's collision-prone it could be used to HashDoS an application. Suffice to say, I'd rather have them get it right than get it now.
- No `user_data` when registering a C/Rust function to be called by Scheme code. This meant that you needed to generate a wrapper function for every Rust function you wanted to expose to Scheme. - Use of `longjmp`/`setjmp` for error handling, meaning that if you called `scm_sum` on 2 Scheme values in Rust, it could call `longjmp` and exit your function, skipping destructors. I worked around this by forcing the user to typecheck values before doing anything with them. I also realized that I should be working directly on project I wanted to work on (a game), and not on a tangential thing before I knew if I needed it or not. I guess the issues were solvable, I just realized that I shouldn't have been wasting my time on them at the moment. I have a bit of code at https://github.com/lidavidm/guile-rs, though it's not very well written.
That looks like it only applies to bignums; there's `ceil` on floats, but why go through the conversion to float and back to int (with associated error checking) when you can just do an add and a sub?
&gt; the callback on the Move trait would presumably be executed after the type was placed in the new location On its own, that would be completely useless, because you need to know where it used to be to rewrite the pointers. And an API like this would be far too difficult to use correctly: #[lang(move_trait)] trait Move { /// Called after an instance of Self is moved. unsafe fn move(old: &amp;Self, new: &amp;mut Self); } This violates Rust's aliasing guarantees if the object in question contains `&amp;mut` references to something, you change it through `new`, and you read it through `old`. Using raw pointers doesn't fix that, since you're casting to references whenever you use it (modulo intrinsics, of which field access is not one): the only sound way to have aliased mutable data is with UnsafeCell. Expecting the user to just not alias isn't realistic, because a Move type could contain another Move type, in which case the move callbacks should be run either inside-out or outside-in. If it's done inside-out, the inner move type will write to `new`, and the outer one will read from `old`, and **BOOM! UB!**. If it's done outside-in, the outer move type will write to `new`, the inner one will read from `old`, and boom undefined behavior. If we're going to magically use a cell here, we need one that doesn't actually add anything to the runtime representation of the type it wraps: that means either UnsafeCell itself or Cell. Cell is right out, since it requires the type it wraps to implement Copy, and UnsafeCell is almost as hard as just not aliasing through those pointers. If I was going to design this API, I'd like to do it like this: #[lang(move_trait)] trait Move2 { /// Override moving Self. The default is ptr::copy_nonoverlapping. unsafe fn move(src: *const Self, dest: *mut Self); } Here, the ownership story is much clearer: ownership is transferred from `src` to `dest` when you copy it yourself, and `src` is off-limits after the copy is done. Unfortunately, this API definitely cannot work, because it doesn't handle the case where a type that implements `Move2` contains another type that implements `Move2`. You got any suggestions for a feasible API? Just marking it `unsafe` doesn't make the need for decent language design go away: if it's too hard to get right (where I define "too hard to get right" as "reasonable people doing reasonable things can be bitten in the butt because someone else did something equally reasonable"), people will use it anyway and will get it wrong. 
+1 for hlua. It is impressive how easy is to integrate Lua in Rust code with it.
The distinction being made here is that Swift strings follow value *semantics*. A copy of a String does not observably alias the original. Mutation of the copy does not affect the original, and vice versa. The fact that this is achieved through pointers using copy-on-write is an implementation detail.
On this topic, are there any generalized async libraries that don't assume you have low level control of i/o? For example, imagine I want async postgres queries. `rust-postgres` isn't written using `mio`, so I assume I can't use any of the coroutine libraries that are commonly mentioned here. Is there anything better than a futures library?
Your change is actually considered more idiomatic; since it's in a `card` module, the name `card::CardState` would be considered worse than `card::State`. So, even if this weren't private, but public, it's totally fine. If I'm making a game with your cards, and I have a `game::State`, that disambiguates, even if they're both `State`.
Imagine ranges in a multi-dimensional wrapping space, there can be several paths between two points. For example, your space is `[|0;9|]x[|0;9|]` (so a pair of numbers between 0 and 9), you make it wrapping, so that 9+1 = 0, and with element-wise addition: (a1,a2) + (b1,b2) = (a1+b1, a2+b2). Now, you have the range (0,0)..(9,9). You can `step_by` (1,1), which will give your the items (0,0), (1,1), (2,2), etc... You can also step_by (1,3), which will give you the items (0,0), (1,3), (2,6), (3,0), (4,3), (5,6), (6,0), (7,3), (8,6) and (9,9)
Raw pointers don't have ownership, so when they go out of scope, nothing happens. Just like a reference. If you use `unsafe` to call `malloc` to get memory, then yeah, you're gonna have to free it manually.
So if I passed a ref to an `unsafe` block, once it gets to the end of the block the compiler will continue to treat the ref as it otherwise would?
Thank you for the info ! Did anyone read the first chapters ? How did you find it ?
How does early access work? Like if I was to buy the ebook edition would I get the full version for free when it's eventually released or would I only get a discount or something?
It says you'll get access to additional chapters as they become available, which I assume means you're paying in advance for the full e-book, and also a sneak preview on stuff as it's being finished.
Hmm...with the promo code as well I might as well get it so. 
The only time this is dangerous is in macro expansion. Otherwise it's perfectly idiomatic!
How does this compare to "The Book" (https://doc.rust-lang.org/book/)? Also, in case anyone is unaware, "The Book" is currently (as of Mar 12, 2016) being rewritten: https://github.com/rust-lang/book 
If I'm not mistaken, [the Redox OS Book](https://github.com/redox-os/book) uses [mdBook](https://github.com/azerupi/mdBook) to generate their online documentation in the form you see there. If you're interested in free blogging, you could certainly use Github Pages to statically host the files that are generated by mdBook which is free and reliable. Edit: If you have Rust and Cargo installed, you can use cargo install mdbook to get started really quick.
The writing is excellent, for a great free introduction to the style download http://www.oreilly.com/programming/free/why-rust.csp As for what is currently available in the early access material, there are 153 pages of content with specific chapters (pages 65-153) on * Ownership and Moves * References and Borrowing * Expressions * Enums and Patterns The previous booklet from Jim Blandy, "Why Rust" is not the front matter of the book "Programming Rust". All fresh new material.
Fantastic talk.
Very good explanation ... Waiting for the next series on how &amp;T and mut &amp;T 's are to be used 
You've also introduced a bug, since you process the whole chunks of the incoming message first even if there's unprocessed data waiting in the buffer from the last `update` call. However, thanks for the idea. I managed to shave about 40% off the runtime, even doing it entirely safely. 
I don't know a lot about compilers and how they work, but is it possible to compile Rust to C? As in, if I was working at a job where they wanted C code but I am much more familiar with Rust, could it be algorithmically transformed and would readability be maintained? If so, wouldn't from-Rust C code always be safe?
I got an error message: "You did not meet the criteria for this discount." Bummer.
I'm not sure what you mean by off-the-record encryption, but Signal does have perfect forward secrecy and plausible deniability for group messages AFAIK. *Edit:* [Source](https://www.whispersystems.org/blog/private-groups/).
This is a mathematical description of a problem not a software one and I find the math here irrelevant. What is the data structure that is used here? Another example would be a two dimensional field that contains a cow in each cell. I could step by (1, 1) as in your example to get the cows in the diagonal but that just shows that the step by abstraction should have the same dimensions as the data structure, iow a tuple, It does not show that it should use the Self type. 
&gt; I'm curious how they are doing Off-The-Record encryption while also supporting group messages. Not at all an expert, but I think the ID keys of the other group members are included inside the plaintext and that same message is sent to each user. &gt; Theres also the problem of forwarding/storing the messages on 3rd party servers for offline peers I don't know anything about this app, but the axolotl protocol stores public ID and public ephemeral keys, as well as encrypted ciphertext payloads, on a third party server; the third party knows no secrets.
I had the same error when I tried to pre-purchase the physical book. The coupon code worked for the ebook for me.
What about "cobalt-org/cobalt.rs", a static site generator written in Rust. 
From the way this question is phrased, I'm not sure if you realise that allocation on the stack is not the same as allocation on the heap. Unless you're running destructors, stackframe freeing is literally just addition to a register. For example if you have some value on the stack which is X bytes long, when you return from a function you just add extra X to the stack address. Any kind of heap allocation, deallocation and metadata will cost you more cycles.
The rust compiler uses LLVM and I remember that there is an LLVM backend that generates C code. I don't know how readable it is but probably not very much. Rust has some very high level construct and those are not visible at the level where LLVM operates so they would be probably transformed into a less readable format. In general I think it's very hard/impossible to transform some source code into another readable one when the two layers communicate in a way that cannot encode the original intent of the author. If the source layer only gives a bunch of lower level operations then the destination layer must use some kind of reverse engineering to find out the original intent from that, which is very hard (possibly impossible). About the safety of the generated C code. My intuition tells me that it would be safe but I'm not sure. In this case the C backend would be generating (unsafe) C code and there's a lot of weird things in the C language. I wouldn't be surprised if in some weird edge case the generated C code would be optimized in a way that would do something totally different and unsafe.
That's nice, but.. it's a book that's very far from being released. I'm not a fan of paying for unfinished products, even if they are about Rust.
That's very much like Rust 0.1
Do you know how memory is managed in Ketos?
I'm guessing panic::recover is not meant to be used most of the time ?
Is this satire?
He sounds like a baby worrying that his favorite toy might go out of fashion!
Did you read the replies? Countering him point by point? Why did you post this? This isn't constructive or helpful to anyone.
Thanks for your reply. I found another way which is not very optimal but seem to work fine: fn deserialize_vm_image&lt;D&gt;(de: &amp;mut D) -&gt; Result&lt;Option&lt;Ref&gt;, D::Error&gt; where D: serde::Deserializer { let deser_res: json::Value = try!(serde::Deserialize::deserialize(de)); match deser_res { obj @ json::Value::Object(_) =&gt; { let res: Option&lt;Ref&gt; = try!(json::from_value(obj).map_err(|e| { serde::de::Error::custom("{:?}", e) })); Ok(res) } json::Value::String(ref s) if &amp;*s == "" =&gt; { Ok(None) } _ =&gt; { Err(serde::de::Error::custom("image field has wrong format")) } } }
Ugh its starting to seem like if the language isn't Swift, people are just going to trash on it for dumb reasons.
I bought it at full price the other day when linked in this subreddit. Worth it so far for me... I feel like I understand a couple of Rust-specific concepts a little better now after reading through and looking at the hand-drawn diagrams. I am one of those people who benefits from learning the same concept via multiple sources ... so I am very glad I bought this book and can't wait to see where it ends up.
I only found https://github.com/lucidd/rust-promise which is outdated :/
So, if any, is that good or bad?
Basically, any host which allows you to serve plain HTTP on a port will do. I currently host my own websites with WebFaction, for example, and they allow you to run arbitrary processes and have an option where their nginx server acts as a reverse proxy to a given port which can be anything; so I can (and have for experimentation only) run a Rust web server there. That kind of thing can work well for small things. If you’re doing something with a VPS or Heroku or any of those sorts of things, it’s going to be fairly straightforward. No WSGI or FastCGI or such, just plain HTTP, probably reverse proxied through the likes of nginx for confidence at present.
I believe it uses reference counting.
&gt; . i dont see any tyrannical moderation attack squad atm. No, but it's slated to enter the spec sometime aorund Q2 this year
[You can get 50% off until the 19th I think it is](https://np.reddit.com/r/rust/comments/49syqo/jim_blandys_programming_rust_book_from_oreilly_is/d0waghq), worth the risk imho.
I bought it on release and so far I can say it has been very good at explaining some concepts I did not fully understand before, for 50% off I think it's well worth it.
But not on the book cover ... it looks kinda boring.
&gt; including stack frames No, that's not how stacks work. "Freeing a stack frame" is nothing more than bumping a register. This is no different from what C does. Arguing that heap allocation is better than stack allocation for general variables is a non-starter, even if bumping a register was expensive (it isn't), cache locality would erase any other speed issues. Languages without value types (i.e. structs which are completely on the stack) are at a clear disadvantage here. For example, even simple objects in Java get allocated on the heap (there are upsides to this too, but it's not a choice a non-GC language should make). RAII is used to manage the heap only. So if you allocate a box or a vector, this will be freed when it goes out of scope. If you create a regular struct not containing any boxes or vectors (or other heap allocations), it goes on the stack in the cheap way described above. Freeing at scope exit can be slower than freeing blocks of memory at once, agreed. However, note that this is orthogonal to garbage collection: you can have garbage collectors which attempt to free on scope exit (cycle collectors), and you can have regular allocators which try to free memory smartly (I think jemalloc does this). 
Actually the JVM can allocate objects on the stack if escape analysis proves they are confined in the stack frame.
&gt; tyrannical moderation attack squad I removed the post. Today's tyranny was brought to you by: * Code of Conduct violations * Lack of constructiveness * General consensus amongst comments that this isn't the kind of content we want here
I'm hosting my website on a generic Ubuntu 64bit VPS, which happens to match my own computer, so my deployment steps are simply: 1. Compile it locally, 2. upload, 3. run. and it works perfectly. I'm currently not using nginx or any other tool for load balancing, but I don't expect any significant traffic. It's just my portfolio. It's generally not much harder than that (depending on the machine setup), since it's just a program. You may have to compile it on site, though, if you can't do it locally.
Will do. Also going to try to look at the perf output. Where is a more appropiate place to post the results of this? 
Ah right I forgot about that! But it's nice to hear that you were able to fix it. BTW: You might want to use `slice::clone_from_slice()` instead of copying chunks bytewise. Oh and I'd wrap [this](https://gist.github.com/lhecker/4ddfc000dafa9fffd961#file-md5-example-rs-L146-L153) into a small `#[inline(always)]` method and then either *unroll* [those 4 sections](https://gist.github.com/lhecker/4ddfc000dafa9fffd961#file-md5-example-rs-L127-L142) as can be seen [here](https://github.com/openssl/openssl/blob/master/crypto/md5/asm/md5-x86_64.pl#L175-L190), or by changing it to use 4 distinct/seperate loops. I'm somewhat sure that this will boost your performance even further - just take the OpenSSL code above as a reference.
Oh, yeah, forgot that Java does this. (Go is excellent at doing this, btw. I'm not sure how well Java does it)
&gt; This is a mathematical description of a problem not a software one and I find the math here irrelevant. That's sad, because the root mechanism is basically just maths: for i in (a..b).step_by(c) { /* ... */ } Is more or less the same (if we ignore the inclusive/exclusive range issue) as: let i = a; while i &lt; b { i += c; /* ... */ } Now, how can this make any sense if `a`, `b` and `c` are not of the same type?
I have deployed 4 rust-based website so far. I clone and compile the source code on the server and put it behind nginx (via proxy_pass) to enable ssl, collect analytics and serve static files. I use [supervisor](http://supervisord.org/) to keep it running all the time. Updating the website is as simple as: git pull cargo build --release supervisorctl restart &lt;your_site_name&gt; Please let me know if you want any more details about my setup!
Interestingly enough, their "security whitepaper" says nothing about the security of group chats. It only extensively describe 1:1 messaging security.
This seems to imply to me that MOST-to-ALL machine architectures are designed to expect stack frames for ALL functions/continuations. Am I reading you right?
Some comments, but I don't know enough to say what the exact cause is. Was cargo install downloading different new crates on the different computers? If so that could add network time to one laptop but not the other. Running it twice in a row and measuring the second time is a way around this. The different operating systems might have a significant impact, not just because the compiler is slightly different, but because config flags mean you are actually compiling different code on the different operating systems. Looking at the CPUs: - i7: 2.1 ghz (Edit: Turbo up to 3.1, but 8 minutes is long enough that this might not matter much, as it probably gets too hot for turbo relatively quickly) - i3: 2 ghz And the i3 is 2 generations ahead of the i7, so better microcode could easily make up the tiny difference in ghz, giving it better single threaded performance. If the compiler is mostly blocked waiting on one thread then the i3 could be the faster CPU here. I'm currently compiling lalrpop myself and it does look like it is mostly running on a single CPU, making the above very relevant.
I've been assuming that associated functions are good candidates for optimization if they end up returning q constant, but maybe it's just wishful thinking.
This information is in the title already.
Ok throttling doesn't seem to be a problem: [Clock speed and Temps](http://i.imgur.com/8IaEQuk.png) It isn't exactly 3.1ghz, but quite a bit faster than 2ghz. The drop in the end is when cargo finished. The temps are far from critical btw: my sensors tell me "(high = +87.0°C, crit = +105.0°C)". Oh and: this second test took exactly 7m48s again ;) 
Cool! I've needed that feature in the past, when parsing network protocols, and I'm definitely going to need it again. Thank you for that work!
Compiling easily maxes out the CPU on my laptop-harddisk (so as slow as you can get) based systems, I find it hard to believe it's bottlenecking on disk read on systems with SSDs.
Is supervisor needed? Does the app crash?
`libc::system`? - http://doc.rust-lang.org/nightly/libc/fn.system.html Also check the [newbie Q&amp;A thread](https://www.reddit.com/r/rust/comments/49avjy/hey_new_rust_users_got_an_easy_question_ask_here/)
SSE intrinsics are, but LLVM is free to optimize things behind the scenes. If you look at the release-mode assembly for [this](https://play.rust-lang.org/?gist=71d0da8beb138c04f62d&amp;version=stable), the inner loop of `sum` is vectorized.
By shellcode, do you mean the [security exploit](https://en.wikipedia.org/wiki/Shellcode)?
Do you see his point? Iteration over a set is more generic than a vector space, and he is saying `step_by` should be defined only in terms of iteration. I think his idea is very similar to - http://en.cppreference.com/w/cpp/iterator/advance
His version of `step_by()` would be such that for i in (a..b).step_by(by) { ... } // by: usize? translates to let mut r = a..b; while let Some(i) = r.next() { ... for _ in (1..by) { drop(r.next()); } }
Moxie [blogged about it](https://www.whispersystems.org/blog/private-groups/).
I don't know. It's not very Rustic, but if it works for you, it works for you. The issue might be scaring off contributors with a weird style, but if it's not a project which you're worried about that, then I don't see a problem (although I personally don't prefer the style; my style is to have a very flat namespace with perhaps only one or two levels of modules at most).
Usually that means that a library does not do any dynamic memory allocations (akak, heap allocations), and instead purely works with the stack and static memory (stack and static allocations). --- One common example of allocation-less Rust code is the use of references to existing data, for example `&amp;str` slices, as opposed to heap allocating a copy for operations that need to wrk with the data for a while.
Usually I would throw stuff like this at CPU performance speculations: i7 has, compared to i3,... - ... better/deeper branch prediction. - ... more efficient caching techniques. - ... more µCode cache. - ... better out of order execution. - ... more execution units. However, this is why this doesn't matter in the case of `rustc`: - Forget branch prediction. Branches in compilers are usually heavily unpredictable. And misspredicted branches can stall the CPU for 10-20 cycles depending on the exact CPU. And branches in desktop software, especially in compilers, are very common. - Most of the stuff a compiler does is string/integer processing, step by step. There is not much to gain from SIMD instructions except for fast pattern matching in strings. And pattern matching in strings is only interesting for the tokeniser. So... there is still something to gain, but because compilers are compilers, it isn't much. ... and then I remembered that Xeon-Time. What a machine! How?!
Someone posted a link to his JSON library here on Reddit, but I can't remember who and where. So much for the example part. Now, in addition to /u/Kimundi I'll answer the question why someone would rather avoid using heap allocation. First, the most obvious reason anyone will tell you is that heap allocation is way slower than simple stack allocation, as a whole bunch of memory management is involved. However, a second and also very interesting reason is that one could use such software on embedded devices without complex memory management or in operating system development. Embedded devices usually don't have a lot of memory available, so a zero-allocation-library gives you total control of its memory usage, which is very important. And if you are developing something like an operating system, you might not yet have any memory management for heap allocation available. However, both scenarios might also require the library to not depend on `::std` but on `::core`.
&gt; i7 has, compared to i3,... &gt; * ... better/deeper branch prediction &gt; * ... more efficient caching techniques &gt; * ... better out of order execution In this case the i7 is two generations older chip than the i3, so I don't think any of the above are true. Within a generation I would not expect any difference in the above.
Great work! This is likely to shave a few ns from some benchmarks where we can do without utf8 checking.
If you're referring to escape analysis in Java 7, [that's done by the JIT compiler](http://docs.oracle.com/javase/7/docs/technotes/guides/vm/performance-enhancements-7.html#escapeAnalysis), not at initial compile time.
I guess the only thing missing is a way to explicitly move them an the same time as updating pointers - as it stands they aren't very useful
Unfortunately, I don't understand assembly fluently enough to follow what the OpenSSL code is doing, but I split out the loops and now it runs at 1.33x the speed of the OpenSSL implementation on my system. I have a hunch the next sticking point in my new revision is [this loop](https://gist.github.com/anonymous/77eb10aa9472d63201e7#file-lib-rs-L129-L131) but I don't really want to resort to a `transmute` call because 1) one point of the exercise was to prove to myself that Rust could automatically rule out Heartbleed-like bugs (i.e. preserve safety) without significant runtime cost, 2) doing a call from `[u8;64]` to `[u32;16]` doesn't appear to be allowed by the spec of `transmute`, which says that the two types must be the same size. (Or have I misunderstood what that means when applied to arrays?) Also, I ran into trouble with the borrow checker after updating `compute_block` to take the data buffer as a seperate argument, since it needs to update the Md5 struct at the end, and can't borrow both the struct and one of its members at the same time. I'm not sure what the best way around that is, so I went with `Option` and then using the member if passed `None`. (Although that feels kinda inelegant.) I don't think that path is being taken enough to impact performance, but I don't know how `Option` works under the hood, so I definitely want to hear if there's a better way of doing things.
When you move the stack pointer back or forward, you don't do anything to the bytes in memory. There's no need. The stack pointer just keeps track of the top of the stack. If a function returns and another function is called, that new function *can* touch the same memory region of the stack as the previous function used. You can observe this effect in C and C++, which don't require stack-allocated values to be initialized before use: #include&lt;iostream&gt; void foo() { int val = 2048; } void bar() { int prevVal; cout &lt;&lt; prevVal; } int main() { foo(); // Prints "2048", or it could print garbage because the stack layout // of the two functions might not be the same. bar(); } In C, nothing else is done here. You have to manually clean up resources and free allocations. In C++, objects can have destructors which can automatically perform cleanup when the object falls out of scope; however, this only takes effect if the whole object is on the stack (was not created with the `new` operator, doesn't have pointer fields). Otherwise, you have to use the `delete` operator on objects created with `new` (though C++ has had container types that do this for you for years). In Rust, for types that implement the special `Drop` trait (or types containing other types implementing `Drop`), Rust inserts calls to their "drop glue" which handles invoking the `Drop` implementation at the end of a scope (the end of a function block, moving closure, or inner block). For example, with `Vec`, this goes through and invokes the drop glue on each of its elements (if applicable; this isn't done on types that don't need it), and then frees the allocation. You can have compile-time analysis and optimization with a GC'd language, but limiting reflection and aliasing in a GC'd environment is just artificially restrictive. A GC is useful *because* it allows you to alias objects. It saves you a lot of cognitive overhead; that's why GC'd languages are still very useful for rapid prototyping or any area where you want source code that's easier to digest and maintain (web development, for example). No one ever said GCs were obsolete, they just can't keep up with manual memory management in performance-critical applications. Whenever someone talks about allocating memory, they are usually referring to allocating memory from the heap. Stack allocation is all handled by the compiler, OS, and CPU architecture; it's the default in C, C++, and Rust, so you don't need to do anything special to use it. You just declare variables and put values in them, then the compiler calculates how much the stack will need to grow when the function containing those variables is called (and how much the stack will shrink when the function returns), and then emits instructions to move the stack pointer accordingly (not necessarily `push` and `pop`, as those only move one value onto the stack at a time). Primitives in Java are also stack allocated, as well as the handles to objects (including the implied `this` handle in instance methods), but Java thread stacks are managed by the JVM so it works slightly differently. Here's a simple example in Rust: fn main() { let vec: Vec&lt;u8&gt; = (0 .. 16).collect(); } This creates a new `Vec&lt;u8&gt;` and initializes it with 16 8-bit integers `0, 1, 2, ... 14, 15`. Is this stack or heap allocated? Think about it for a second. Time's up, it's both! The actual `vec` variable here, the `Vec` struct, it's on the stack. Contained in that struct are a `*mut u8` and two `usize` values: the pointer to the allocation on the heap (where those bytes are being stored), a length value (how many elements are in the vector), and capacity (how many elements the vector can hold without reallocating). Those three values are on the stack, but `Vec` can use them to manage our allocation on the heap where our bytes are stored. This has gotten pretty long, so I hope it clears everything up for you!
If the panic occurs during another panic, then the process aborts as we don't really have a sane way to deal with that. But as for just panicking in `Drop` during normal execution, [that's easy to test](http://is.gd/o6tDKz). Spoilers: the panic proceeds as normal. There's probably loads of edge-cases but the straightforward example is handled sanely. It's just generally considered bad behavior; no one likes APIs with undocumented panic conditions.
More cache on Xeon?
I haven't looked at the innards yet, but what are the differences between this crate and something like [rust-ansi-term](https://github.com/ogham/rust-ansi-term) (which you mention was helpful as a reference), [term-painter](https://github.com/LukasKalbertodt/term-painter), or [term](https://github.com/Stebalien/term)? Is it style differences, or internal differences too? I like the API builder style of this crate, so it could be really cool!
Yes. 8M on the Xeon, 6M on the i7, 3M on the i3. (I assume this is L3 cache, they are numbers from intels [site](http://ark.intel.com/products/65732/Intel-Xeon-Processor-E3-1230-v2-8M-Cache-3_30-GHz)) The Xeon also has a much better clockspeed, 3.3 GHz (3.7 turbo)
I’ve been with them a few years now. I like them as a host, though as my own technical skills have matured and as VPSes have become cheaper, I am considering shifting to a VPS. I’ve never had any major problems with them, and the one or two minor problems that have arisen have been resolved quickly and effectively. For someone who is comfortable in a shell but who would prefer not to manage the entire OS, I would recommend WebFaction.
Hi Kbknapp ! Thanks you for your interst ! I didn't knew term-painter ! Glad you brought it up, there is very interesting use of the `term` crate which I will probably copy in some way or another. So, `colored` is built to be a technical equivalent of `ansi-term`, because I liked the approach a lot. Basically, it build a light type around the string, which implement `Display` so that the string will be colored as asked. As the trait is implemented for both the string type and the light type, it gives this really nice API where we can chain methods around strings and be displayed. `term` is the outsider of the four as it's mainly a tty controller crate. The api is build around querying and acting on Terminal primitives on ANSI or Windows terminals. `term-painter` use `term` internally and seems to be a good compromise. Only problem, the API is not quite as good as it could be. My plan is to support windows via a slightly degraded api where only one color and style is possible at a time (kind of what does `term-painter` with `Blue.with || {}`) but keep as much as possible a naive and natural feeling. Probably via new macros, like `tprintln!`. I'll see what's working. Tl;Dr: like `ansi_term`, it makes pretty strings via ANSI escapes. It don't use `term`. It has the nicer API of all, and that's what I will try to keep as features are implemented. :)
According to [a variant of your test](https://play.rust-lang.org/?code=struct%20PanicDrop%3B%0A%0Aimpl%20Drop%20for%20PanicDrop%20{%0A%20%20%20%20fn%20drop%28%26mut%20self%29%20{%0A%20%20%20%20%20%20%20%20panic!%28%29%3B%0A%20%20%20%20}%0A}%0A%0Astruct%20NoisyDrop%3B%0A%0Aimpl%20Drop%20for%20NoisyDrop%20{%0A%20%20%20%20fn%20drop%28%26mut%20self%29%20{%0A%20%20%20%20%20%20%20%20println!%28%22Goodbye%2C%20cruel%20world!%22%29%3B%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20_a%20%3D%20NoisyDrop%3B%0A%20%20%20%20let%20_b%20%3D%20PanicDrop%3B%0A}&amp;version=stable), it looks like a panicking drop will prevent other destructors in the scope from running. If you replace the second line of `main` with just `panic!()`, then the noisy destructor does run.
Thank you. I've been having the biggest issues with playing stupid game with byte buffers to do basic pattern matching. 
They have not crashed so far but it's nice knowing they would restart should anything happen. 
This issue [has been known about since 2014](https://github.com/rust-lang/rust/issues/14875). It's another one that's blocked on MIR.
Yay, yet **another** crate for printing colors to the terminal that has *very* **incorrect** terminal handling. Use libterm. It's not hard. It handles terminfo. It handles Windows.
This is how I see it colloquially used as well.
&gt; It's another one that's blocked on MIR What's MIR?
Yeah, I was just trying to keep things simple. Generally these seem to be abstracted away by userspace APIs like Win32 and Pthreads/libc.
Man, that sounds like heaven. Thanks for the info.
Thanks for the info. supervisor sounds helpful
I claimed my json library was zero allocation. Link: https://github.com/valarauca/lib_json What I was saying there are zero re-allocations of strings. What the library is doing is returning pointers that point within the originally barrowed buffer. This is *unsafe* behavior as if one field gets owned unsafely and modified it can invalidate all other fields. But in a pure Rust environment the type system/barrow checker will stop this from happening. The trade off is better performance, less memory, less OS interaction. The reason for specifying *allocationless* is the normal Rust Json libraries: Serde, and Serialize. They return a String not a &amp;amp; str which is technically creating a new allocation, copying the contents, then validating its is indeed a utf-8 valid string. P.S.: Yes BTreeMaps/Vectors technically allocate to the Heap. But I ignored that as avoiding STDLIB makes life very difficult.
You use [unit files](https://www.freedesktop.org/software/systemd/man/systemd.service.html). There are examples at the bottom. [Here's](https://medium.com/@johannes_gehrs/getting-started-with-systemd-on-debian-jessie-e024758ca63d#.a0w7t3pg2) a blog that gives a good overview of what it can do. 
You could have used linked lists to avoid Vecs altogether, and linked binary trees to avoid BTrees. Perhaps someone will create such a thing someday.
The current `StepBy` is not a trait, hence not implemented for anything. It's just an inherent method of `Range*` types. What you describe: 1. is a perfectly valid use case 2. has nothing to do with ranges, and only relates to iterators 3. can also be done like this iter.enumerate().flat_map(|(i, v)| if i % n == 0 { Some(val) } else { None }) Or, using [itertools](https://crates.io/crates/itertools) iter.batching(|it| it.nth(n)) edit: there is even simpler with itertools actually: iter.step(n)
Thanks for the info. :-)
I'm preparing next month's first [clippy](https://github.com/Manishearth/rust-clippy) workshop in Frankfurt. We will do pair programming to create or extend lints and I'll mentor in person. Apart from that, I'm looking into tests and procedural macros so I can help set up real mutation testing in Rust.
Thanks for your tests! :) I honestly doubt that `lalrpop`, being a parser generator, has cfg flags at all. Maybe /u/nikomatsakis could tell us? Assuming that cfg flags aren't responsible for the difference, I'm really surprised that it works faster on Windows. I *think* the vast majority of Rust's hardcore users work on Linux. There should be some more tests for sure!
&gt; wait 30+ minutes I hope when the new code generator work lands, this will improve. Other than that, nice to see systems programming work being done in Rust. Good luck with the project.
Still working on the basics of my GBA emulator. I'm almost done implementing the execution of all ARM state instructions and I've done a lot of refactoring, including splitting up way too big files and simplifying functions by detecting similar patterns, which are then moved into new private functions. The next step would be to add yet an other huge unit test to check whether all instructions will be executed as I would expect. From there on I'll propably push the first progress onto a public GitHub repo.
Damn, Frankfurt is out of my ticket's reach...
I wish the author good progress. For Science (TM)! Makes me wonder about whether it's sensible and possible to use multiple cores in the future to compile a single crate, having [this discussion here](https://www.reddit.com/r/rust/comments/4a8fqc/partly_offtopic_what_hardware_mostly_influences/) in mind.
Rust is slow to compile?
Rust has (AFAIK) neither incremental nor parallel compilation yet. Also LLVM IR produced by compiler is still suboptimal which mean that LLVM passes are quite time consuming. But it is improving. 
it was easy to [add support for it](https://github.com/Geal/nom/commit/41f87f896ad03d1f2344be1146c19363b0e9172a), but it would be great to have the regex API generic over `&amp;[u8]` and `&amp;str` instead of requiring another module.
Nice, I've been following this for a while. Great to see it is still being worked on.
I realized yesterday that the Vulkan specs don't allow pipeline barriers within a render pass when building a command buffer, except under some restrictive conditions. Which means that handling pipeline barriers automatically would add a bigger overhead than expected. Which means that I'm going to change the design of command buffers and provide a low-level unsafe API which can be wrapped around with higher-level and safe APIs. The user should then be able to generate safe wrappers thanks to a macro (or a plugin once they are stable), [like it's the case for render passes](https://github.com/tomaka/vulkano/blob/b2713c745c6b3eaefef4e919c906f4efd3d0d305/vulkano/examples/teapot.rs#L143-L164). 
I am continuing my work with [Serkr](https://github.com/mAarnos/Serkr). Last week I ran rustfmt on every file of the program, which was a bit annoying due to the way rustfmt mangles some stuff. I also added the 'Clippy as a service' badge to the repository. Unfortunately the autogenerated code triggers loads of warnings so it is kinda useless as of now. Seems like that is going to be fixed soon though. Currently, Serkr has two parsers: one for tests and one for real use. I am currently working on getting rid of the test parser as it is mostly useless. After that is done I can get back on my work on trying to reduce the amount of pathological cases which freeze the program. I have some solution ideas but they are kinda hackish, and I am hoping there is some nice algorithmic way to fix everything. We'll see how it goes.
The cache misses on read would be horrible. 
Some current CPUs even have dedicated circuitry to handle stack operations to make function prologues and epilogues as fast as possible. ([PDF](http://www.agner.org/optimize/microarchitecture.pdf), see the stack engine sessions in some of the uArchs).
&gt; purely works with the stack and static memory Frequently, libraries that are zero-allocation will accept slices, and the client of those libraries may be using them on heap-allocated memory. But the zero-allocation guarantee means that the library doesn't allocate on the heap itself, not that it doesn't work with heap memory.
&gt; If you don't know anything about dynamic linking on x86-64 GNU systems for ELF, that's totally OK, because as far as I can tell, no one really does anymore. I was experimenting with building a linker (kinda, see below) in rust recently as well, and came to the exact same conclusion. For example, the best sources I found were documents only available on SCOs website (which is crazy considering SCOvLinux and the fact that they are pretty much dead as a company), and the LLVM source code for some constants. The spec for the most basic linking on x64 (even before thread local storage and all that stuff) is scattered across three different PDFs, and even then what I found was obviously incomplete when compared to the dylibs LLVM outputs. There are practically no blog posts on the topic, let alone nice writeups about how anything works. What I was working on wasn't a traditional dynamic linker, but a library capable of loading (simple) dynamic libraries into (mmaped) memory, doing the relocation required to make them execute properly, executing the initialization and "destructor" code in the libraries (dynamic libraries execute code when loaded and unloaded even if you don't use them), and looking up symbols in them to be run. I did this mostly as a learning exercise, I can't imagine any practical use. If people are interested I'll put some time into cleaning the library up and publish it some time this week.
Yes. I'd rather not use such a thing if I can have Vecs and BTrees. Still, for systems that for whatever reason cannot rely on an allocator, this could be a worthwhile solution.
The semantics between them are quite different. I'm not sure a generic API is a good idea.
Recently completed a [Comparison of a DFA minimization algorithm in Rust, C# and C++](https://github.com/WalkerCodeRanger/dfaMinimizationComparison).The read me contains my notes on what I liked and didn't like about Rust. This is the largest program I have ever written in Rust and in some ways my first attempt to write serious code rather than a little example. 
Woot! https://blogs.dropbox.com/tech/2016/03/magic-pocket-infrastructure/ was posted as well, they say more posts with more details in the future!
&gt; MAJOR: properly init dynamic linker's TLS. This terrifies me. Hah. I was looking at the readme specifically to see if I could figure out how they chose to handle this. In glibc the code for doing this is a bit on the subtle side.
I think part of the problem is that the ELF ABI describes only the required parts but not everything a real implementation needs to do. As far as I can tell, much of what the dynamic linker does makes sense but isn't a simple consequence of elf. Instead it's a complex consequence of lots of other subtle things like the limitations/requirements of exec, TLS, position independent code, and pointer identity across shared objects.
Yes, absolutely! But those crates are rather "small" and get compiled quickly in contrast to `lalrpop` or `lalrpop-snap`...
Also, if anyone has any questions about this project or its use of Rust, I'm happy to answer them.
Like what? I'm happy to field proposals. I'd even be willing to look at other implementations. Do you know of other featureful regex engines that operate on streams? The fundamental problem is that constant time indexing ends up being pretty important. A `Iterator&lt;char&gt;` also means that a whole class of optimizations aren't available, such as `memchr`. The DFA also needs to be able to run *backwards* on the input. I looked into this a few years ago, and this was my conclusion: https://github.com/rust-lang-nursery/regex/issues/25 I think requiring `&amp;str` or `&amp;[u8]` is a very reasonable constraint. I don't think I'd call it inflexible. If the text you want to search is too big to fit into memory, you can: 1. Use memory maps, where the OS will handle what's actually in memory. (This is now trivial thanks to `bytes::Regex`.) 2. Divide your search text up into chunks and run the regex on each. Your problem here is if you care about really long matches or matches that cross chunk boundaries. You might be able to solve that by choosing chunks in a clever way (e.g., line by line). If that still doesn't solve your problem, then I think you're safely in "niche" territory and should explore solutions other than regex. :-) TL;DR - Tell me about the problem you're trying to solve, and maybe I can help you solve it. ("I want matching on `Iterator&lt;char&gt;`" isn't a problem, it's a solution. :-))
I'm glad you like it! Honestly, if existing projects like Snowmew were more actively worked on, I probably wouldn't have started Amethyst. I for one will stick around for as long as I can.
Are you using nightly? or just stable version of rust?
In addition to other comments; I would add that allocations have a bad worst case latency; so making a code not rely on allocations is one of the prerequisites for having real-time safe code. &gt; Also, does anyone have any examples of something that they've written/rewritten to avoid allocations? In my [dbus](https://crates.io/crates/dbus) bindings I have some structs that are strings with some additional constraints, e g, they are valid dbus object paths, or signatures, etc. These are stored as `Cow&lt;CStr&gt;` internally, so you can use it both with `CString` (allocation) or `CStr` (allocation-less), depending on what would fit the library user. Since the dbus C library works with zero terminated strings I've used `CStr` rather than `str` as the internal format, to avoid allocation when calling the C library. 
Wow, I didn't know they're using Rust for such an important and huge project! Bigger then even Servo perhaps?
This is awesome! I needed to do some "search &amp; replace" in a binary file and this make my life so much easier! :)
What I've done in my code for safe non-copy parsing is to pass around a token consisting of an Rc to a 64MB buffer plus a Range&lt;usize&gt; identifying a parsed field. The leavings of incomplete data at the end of the 64MB get copied to the start of the next buffer, but that's not much.
&gt; Are you happy using rust ? Yes, overall the team has been very pleased with it. Compile times are the only serious complaint. &gt; Will you use rust for other projects ? Yes, we have our rust "Dropbox standard library" in reasonably good shape at this point, so creating more services at Dropbox in Rust is pretty easy now. If we have another really performance sensitive project come up, I imagine we'll be using it--although the final decision is always up to the Tech Lead of the particular project, so there's no single definitive answer I, personally, can give to that question. &gt; How many lines of rust code are you using in production ? About 60k of our own, about 300k incl crates. &gt; Are you going to hire rust developers ? Yes, we're talking to a few members of the community. But, in general, Dropbox hires good developers more than $lang developers. Different projects demand different things, so we wouldn't hire someone that said "I only write $lang".
We're pinned to a particular nightly right now, as we rely on a fair amount of features that are still stabilizing. I imagine we'll be on a stable by this summer.
Not technically working on it yet, but I finally figured out what I can do as my first "new to rust" project: Write a simple Lisp (probably scheme-like). So i'll be starting on that next time I have the motivation to learn rust! I'm thinking about following short little guides like http://www.norvig.com/lispy.html and converting them to rust in order to get started.
That would need some serious safety-endangering stack shuffling shenanigans, but you'd basically need an up-front sized array of values that can contain indices into the array (with other values). Then you could use this to pull the pointer structure. Alternatively, you could write a SAX2-like callback mechanism + possibly some state machine code (where your state machine *could* be non-allocating depending on the needs of the user).
I had expected to at least find a link to some benchmarks. Also regarding performance information, I once had a [blog post](http://llogiq.github.io/2015/10/03/fast.html) with some tricks (one of which is to control allocations, especially if you are streaming lines). Also you can have the compiler emit LLVM IR or assembly, this gives you a good idea of what rustc does to your code. Finally, a lot of under-the-hood stuff is still changing (and will likely get even faster in time), so it's hard to give advice that will stand the test of time.
You tried.
Based on your reddit history I'm assuming you want to execute a given sequence of bytes as CPU instructions ;) Using the [mmap crate](https://crates.io/crates/mmap) this can be done as follows: extern crate mmap; use std::{mem, ptr}; use mmap::*; fn main() { let shellcode = [0xebu8, 0xfe]; let opts = [ MapOption::MapReadable, MapOption::MapWritable, MapOption::MapExecutable ]; let mapping = MemoryMap::new(shellcode.len(), &amp;opts).unwrap(); unsafe { ptr::copy(shellcode.as_ptr(), mapping.data(), shellcode.len()); mem::transmute::&lt;_, fn()&gt;(mapping.data())(); } } In this case, the shellcode `[0xebu8, 0xfe]` performs an infinite loop on x86.
Does your daemon have to communicate with other in-house services? If so, what transport layer and serialization method do you use? UNIX domain sockets, plain TCP, HTTP, JSON-RPC, ProtoBuf?...
This sounds like a fun problem I may give it a go. I spend 10-20minutes looking into this. I'd need a way to transmute types without std::mem Any hit how to do that?
Are you going to open source anything?
&gt; it's really only useful in debug builds I think it's the opposite but incurs a slight performance penalty.
That would be awesome!
I just finished a small experiment on drawing bitmapped graphics in a terminal, I found a neat trick to increase the resolution beyond what's possible by simply coloring character cells http://www.blitzcode.net/rust.shtml#term_gfx https://github.com/blitzcode/term-gfx
Actually on a Friday.
...and can you elaborate on that? :) I'm a rookie Rust programmer (~1000 LOC so far) and I don't think I've faced that restriction yet, or even heard of it.
Reflect deeply on what it would mean to call `clone()` on an `&amp;Clone`; embrace the zen of oh-my-god-dynamically-sized-types-why-are-you-such-a-pain-geez-maybe-we-should-just-have-everything-implicitly-heap-allocated-virtual-and-behind-pointers-gee-i-wonder-why-no-language-has-ever-done-that-before.
I dug bit deeper into this. I used lalrpop-snap as the benchmark, as it is bit more selfcontained and compiles significantly faster than full lalrpop. `cargo build --release` took 101 seconds on Windows, 122 seconds on Linux, so the slowdown is still quite pronounced. With `cargo rustc --release -- -Z time-passes -Z time-llvm-passes` I saw that the time difference is accounted in "llvm module passes [0]", digging deeper into LLVM timings didn't reveal much more details. The biggest pass ("Loop Invariant Code Motion") contributed the most to the time difference (21 seconds vs 33 seconds), but the LLVM build used by rustc just seems a lot slower overall on Linux than on Windows. Maybe rustc LLVM is compiled with different options on Linux than on Windows?
thank mr gankro (Seriously I'm getting more lost by the minute)
See https://doc.rust-lang.org/nightly/error-index.html#E0038, specifically https://doc.rust-lang.org/nightly/error-index.html#method-references-the-self-type-in-its-arguments-or-return-type
Hi LEmp_Evray, thanks for the kind words. :) I am not convinced by the usefulness of skipping the copy and the UTF-8 check, though. For one because "never trust the user input". The check has to be done somewhere. I also think that exposing lifetimes in API will directly affect its beginner-friendlyness. All that for what ? Skipping a string copy ? I really don't think it's worth the trade off. If you want raw perf, you don't use stdout with colors but an mmaped file, or DMA, or any fast block storage will do better. I will keep it in mind though, and if other people ask for it I'll consider it. For now, I'll keep it simple. About windows support, more I look at it, more I think it won't be easy nor pretty. I think libterm will be usefull in a certain range, and after that... It's open source, please contribute if you can.
(I might be getting this wrong -- experienced rustaceans please let me know!) There are lots of things you can do with a generic type, which you cannot do with a trait object. The difference between those two things is pretty subtle though. Here's [an example of a generic type](http://is.gd/u0FZm1): fn print_clone&lt;T: Clone + Debug&gt;(x: &amp;T) { let myclone = x.clone(); println!("{:?}", myclone); } Note that the argument `x` has a generic type `T`, which is declared earlier with `&lt;&gt;` brackets. That means that whenever you use `print_clone` with a new type, the compiler is going to generate a separate version of that function in the compiled binary. Often this is nice, because it answers all the "what method am I calling" questions at compile time. But on the other hand, it makes the resulting binary bigger, and it makes compilation take longer, and sometimes you need to avoid that. An alternative is to avoid generic types, and use a trait object instead. Here's [an example of a trait object](http://is.gd/YmcyYT): fn print_debug(x: &amp;Debug) { println!("{:?}", x); } Note that `T` doesn't show up anywhere. The compiled code isn't going to end up knowing anything about what `x`'s type really is. Instead it's just going to get a pointer at runtime to the methods it needs to call. That's a little slower than the generic version, but it avoids all the binary bloat problems. However, there's a big problem with trait objects: because the compiled code doesn't know exactly what type is involved, it doesn't know the *size* of the object. That's ok if the object is behind a reference or in a `Box` (where some other code elsewhere that *did* know the size must've put it), but it's not ok for doing things like returning an object by value. So if we tried to write `print_clone` from before [using a trait object](http://is.gd/r3OJ7W): fn print_clone(x: &amp;Clone) { let myclone = box x.clone(); println!("{:?}", myclone); } Rust is going to yell at us: error: the trait `core::clone::Clone` cannot be made into an object [E0038] help: run `rustc --explain E0038` to see a detailed explanation note: the trait cannot require that `Self : Sized` The core problem is that `myclone` is a variable that lives on the stack, and the stack has to know how big everything is at compile time. Lots of different versions of this problem can come up. As /u/Gankro mentioned, many languages get around this problem by having everything be a pointer, and relying on some kind of garbage collection to manage the lifetimes.
I doubt it, it's a fundamental restriction (the error index explains _why_ it's fundamental) that can't go away for languages that don't heap-all-the-things. In this case, you can make the method return `Box&lt;Trait&gt;` just fine, which is the equivalent of heap-all-the-things' approach anyway.
What most of the developers use for writing rust code?
Wouldn't this magical feature called "reflection" (at runtime) help with that? I'd think that still knowing the type and its fields (or even just their size?) would be enough to "clone" it at runtime.
&gt; But, in general, Dropbox hires good developers more than $lang developers. Different projects demand different things, so we wouldn't hire someone that said "I only write $lang". As a teenager who is a long way away from working somewhere but an avid programmer (Python, Ruby + Rails) who's recently done a bit of playing around with Rust (and I *love* it so far), I have a few questions. - What particular languages would be useful to look at? - I assume by "good developer" you mean an intelligent person who can adapt to different situations in programming without being tied to one language, but what advice would you give to become a "good developer"? Edit: Might as well jump in and ask this question whilst I can. I'm a big fan of open-source and primarily use Linux, but (just out of interest) what is the general opinion of those two at Dropbox? Can you guys work from pretty much any OS you want so long as the work gets done?
I was struggling with it for a few days until I managed to fix the rust-on-raspberry-pi Docker image for OS X. Posting here for posterity, as in the process of looking for the solution I visited this subreddit a few times.
Thanks for posting this! I always like to see more cross-compilation resources, and once we have teeny bit more infrastructure in place, should be even easier.
How do you pin the version? What process do you use for migrating to a newer version? Are there issues with some libraries getting behind?
I thought this was the error, but I can't make the lifetimes of `&amp;self` and `GetMe&lt;'a&gt;` unrelated because the implementation of `all` in the original code requires the `GetMe&lt;'a&gt;` to live at least as long as `&amp;self` (it gets a value out of a `HashMap`) EDIT: I fixed the issue! Turns out I can make the type of Input have to implement `for&lt;'b&gt; GetMe&lt;'b&gt;` instead of `GetMe&lt;'a&gt;` and then while in the real project code I used to have an impl that looked like this: trait GetMe&lt;'a&gt; { fn get() -&gt; Self; } impl&lt;'a, T&gt; GetMe&lt;'a&gt; for &amp;'a T { fn get() -&gt; Self { unimplemented!() } } I changed that to trait GetMe&lt;'a&gt; { type Out; fn get() -&gt; Self::Out; } impl&lt;'a, T&gt; GetMe&lt;'a&gt; for &amp;'static T { type Out = &amp;'a T; fn get() -&gt; Self::Out { unimplemented!() } } Which means that in implementors I can write impl&lt;'a&gt; Processor&lt;'a&gt; for Something { type Input = &amp;'static u32; } And it infers the correct lifetime! I do not have the command of the English language necessary to explain how ecstatic I am about getting this to work.
In C, there's two ways to do dynamically sized things on the stack, [alloca](http://man7.org/linux/man-pages/man3/alloca.3.html) and [VLA's](https://en.wikipedia.org/wiki/Variable-length_array). So I'm left wondering why "need to know the size at compile time for stack allocations" is a restriction in Rust.
Isn't it too soon? I hope there are some performance enhancement then..
Because neither of them lets you return values the caller doesn't know the size of.
Is there something specific you're worried about WRT performance? I would be more worried about compatibility, personally, but you have to ship something sometime, and getting it into the hands of even more people is a good way to get good feedback.
&gt; Probably. We have an in-house futures-based I/O framework. This would be so great.
We have an in-house build system that manages all our pinning for us. Rust and every version of the dependencies is all pinned to a specific revision that's mirrored internally (so that we don't have to depend on crates.io servers for deployments). The "next gen" version of this system at Dropbox is built on Bazel. (bazel.io)
If you want to attempt the regular kind of buffer overflow exploit, it should be possible in safe Rust using this soundness bug. Have fun! [See issue](https://github.com/rust-lang/rust/issues/32242)
Why can't the callee act like alloca? My understanding is that there isn't a defined ABI at this point, as long as the function isn't set to be callable by C, I would think you could work something out.
Just vim + emacs. Maybe a bit of racer here and there, but not consistently.
Do you use the [protobuf](https://crates.io/crates/protobuf/) crate or something else?
Maybe [gnuplot](https://github.com/SiegeLord/RustGnuplot)?
Can you file a bug stating explicitly what went wrong? Acid2 has been part of the automated test suite for about a year and we don't allow commits through that break it.
Can you describe and/or file a bug stating specifically what was slow?
Most of the problems with building the compiler itself is that we bootstrap and so have to build the compiler twice. (I sometimes wish we didn't at this point.)
It's possible that by-value DSTs could allow this, but it's worth noting that your stack would then have a dynamic size (e.g. you're doing a bunch of allocas).
Well, learning Rust would be a great way to start.
There's this: https://servo.github.io/servo-starters/
Make sure to check out YouCompleteMe if you haven't already! It has support for Rust via Racer, and IMO offers a much better experience than using Racer directly.
&gt; Trait objects are awkward with object-safety restrictions, so generics aren't quite as useful as we'd like in Rust. Generics are... somewhat independent of trait objects. That is, you can write `fn foo&lt;T: SomeTrait&gt;(x: T) { ... }` without ever using `SomeTrait` in a trait object.
Only if your day to day involves 3 websites. But it indicates some amount of progression.
Certainly. I'm just wondering if I should be hyped about Firefox neXt :) I know that was never the stated goal of servo, I'm just wondering if it is starting to evolve beyond just an experimental browser.
In which case I was wrong. Thanks for clearing my misunderstanding up.
Does specialisation help?
Maybe. It's hard to say without actually attempting an implementation. Many parts of the API would also need to change (look at all places that use `&amp;str` or `&amp;[u8]`, or anything that hangs on to a reference to the search text).
If it's on the AUR, I'll install the shit out of it.
Yeah, question... *why is there PHP+SQL?* You could gain allot from just using [Diesel](http://diesel.rs), [*its faster then SQL*](http://bikeshed.fm/49) (hand written queries, idiomatic C, ect...) because of heavy usage of Rusts type system for safety, it can manage to skip a whole lot of run time checks by type verifying queries at compile time, providing safety with no overhead. 
Thank you for the comments. * I agree - the life_left Cell wrap would probably be the best solution. * I'll look into the &amp;[&amp;str] proposal, thanks. Can you recommend any reading material about this kind of usage? * Yeah, the almost identical code path for horizontal/vertical is bothering me. I'll work on that. * That check was proposed by Clippy, as a replacement for the previous code (I left it commented out). I'm not sure if your version is more readable or not :) Again, thanks for the review!
Honest question, wouldn't it be better to keep working it instead of getting support issues and having to do more triage?
About the `&amp;[T]` vs. `Vec&lt;T&gt;` – I don't where exactly to point you at for further reading, but that's combination of two rules: * Prefer references in function arguments if you just read from them (so `&amp;Vec&lt;T&gt;` instead of `Vec&lt;T&gt;`). Simple `Copy` things like integers are of course exception to this rule. So in general, `&amp;T` instead of `T` unless you need `T`. * Take the least concrete type as argument (so `&amp;[T]` instead of `&amp;Vec&lt;T&gt;`). The slice can come from anywhere – eg. middle of a vector or even not from the vector at all. Same for `&amp;String` vs `&amp;str`. The `&amp;Vec` is also a double pointer indirection, which is bad. Also, thanks to Rust's deref coertion, you can just pass `&amp;Vec&lt;T&gt;` to function taking `&amp;[T]` while going the other way is a lot uglier. Speaking of horizontal/vertical, [here's my attempt](https://gist.github.com/krdln/91930672643e453e0a26). But I encourage you to try it on your own before looking. I've also made a few other changes: * Changed `ship_array` creation to iterator + collect, * In your test, you use `let mut` and assignment. But conceptually, all these arrays are separate things. So `let` them be separate!
Is there an apt-source, or is it only source builds for now? Edit: back at my comp now, I didn't find one after a bit of looking so probably just sources while it's still in heavy development.
Fair enough, I just think there is a lot of work to do. Another benefit is exposure, I've heard of Servo and browser.html but forgot. I'll be looking at contributing, if you have a link for setting up a dev environment.
I believe the library team is waiting to add generic collection traits like that until the type system is able to express them better. [Relevant discussion](https://internals.rust-lang.org/t/collection-traits-take-2/1272).
Good on you for posting the answer! Future people will rejoice!
Is it? Even if I just want to "help them identify issues"?
* /u/carols10cents gave a [talk about delving into Rust history](http://confreaks.tv/videos/rustcamp2015-navigating-the-open-seas). * [The rust-dev archive](https://mail.mozilla.org/pipermail/rust-dev/) has a wealth of historical information, but it's not easy to dig through. Later in the language's development, most discussion moved to GitHub issues and [RFCs](https://github.com/rust-lang/rfcs/). * /u/graydon2 gave a lightning talk about the Rust 1.0 release, which he summarized [here](https://graydon2.dreamwidth.org/214016.html). * [This thread on LtU](http://lambda-the-ultimate.org/node/4009) was one of the first public discussions of Rust, and includes comments by Brendan Eich.
As an embeddable web rendering engine/toolkit servo can make a lot of sense. It's never going to fully replace Firefox but typical desktop browsing isn't the only use case it has (unlike FF to an extent)
&gt; The expression &amp;file_bytes[0..127] is actually a pointer + length pair What do you mean by this? I would have expected that to be a slice, are you saying a slice is basically a tuple with 2 entries? if I had a better mental model of what was going on underneath it would help me tremendously. &gt; If you want to read data directly into your struct, you can use transmute to turn a reference to the struct into &amp;[u8], and then call read directly into it. Can you give me an example of that? Also, in your example, what is that syntax? I don't understand what the 0u64 is. u64? why the prepended 0? let nums = [0u64, 1]; 
I'm not the author, but it's perhaps the same reason why crates.io is built on top of Ember.js + postgres?
[removed]
&gt; I have no idea what that means. what I want is for the header to be of type InesHeader, which most assuredly has an ines_identifier property in it. It's the semicolon. In Rust, when you use a block as an expression, the last expression is used as the value of the entire block, but only if it doesn't end in a semicolon. That lets you write something like `let func = |x| { let y = x + 3; y };`. If the statement does end with a semicolon, then the block just returns `()`, the empty type. &gt; To be frank, I'm starting to consider moving on from rust, this is code I could write in 5 minutes in C or C++, and I bet you I could figure out how to write the equivalent in most languages in under 30 minutes. You're still learning. The thing with Rust is that the language makes it unusually difficult to write a program that compiles, but much easier to check afterwards that it's correct. You also get a much richer standard library than C++, and `cargo` makes it a lot easier to incorporate external libraries. &gt; every time I turn around rust is doing something unexpected. Why would that call return an empty type? It's so that you can write both this: fn first(x: i32) -&gt; i32 { x + 3 } and this: fn second(x: i32) -&gt; () { x + 3; } without a `return` statement. The biggest gain from that syntax is making lambdas shorter, since they're more heavily used in Rust than in C++.
&gt; It's the semicolon. In Rust, when you use a block as an expression, the last expression is used as the value of the entire block, but only if it doesn't end in a semicolon. That was actually the first thing I went looking for (I was aware of that behavior), but I went looking for the semicolon at the end of the unsafe block. I guess the silver lining is that I did understand the error message after all. &gt; You're still learning. The thing with Rust is that the language makes it unusually difficult to write a program that compiles, but much easier to check afterwards that it's correct. I think for me part of the frustration is that so much of the documentation I find online is outdated and irrelevant. I'll find 5 different recommendations for how to do something, only none of them actually work. Thank you for your patience.
&gt; Dropbox standard library Is this open source? If not, is there any chance to release it?
The language has had a long and public development cycle, and people aren't always good about noting which version they're working with. You can always get up-to-date standard library API docs from either your local install or [here](https://doc.rust-lang.org/stable/std/). There's also a beginner's guide [here](https://doc.rust-lang.org/book/), a set of examples [here](http://rustbyexample.com/), and a guide to low-level Rust programming [here](https://doc.rust-lang.org/nomicon/).
&gt; many languages get around this problem by having everything be a pointer, and relying on some kind of garbage collection to manage the lifetimes. Isn't `box x.clone()` putting it behind a pointer? I mean, shouldn't this work? fn print_clone(x: &amp;Clone) { let myclone = Box::new(x.clone()); println!("{:?}", myclone); }
Not really, no, moves on return are supposed to become writes to the indirect return pointer, if the return value cannot be immediate. I guess I can see some scheme where you `memmove` the DST to the top of your stack frame, and jump to your caller with now a VLA at the bottom of their stack frame. There are several issues with this though: * copying is wasteful * `memmove` can be slower than `memcpy` (as it has to deal with overlapping source and destination) * you are likely allocating the result so that's another copy * LLVM won't let you do this without a painful layer of trampolines written in assembly and abusing TCO But hey, you've changed my opinion - not impossible, just not practical, so if anyone wants to do this, it would be a cool trick (I think you can do parts of it, at least, in the language with inline assembly).
It doesn't seem to work. I guess the problem is, some code somewhere has to know `x`'s size and do an allocation on the heap? `clone()` knows the size, but it doesn't do allocations. The code that calls `clone()` through the vtable doesn't know the size. [Another one Huon Wilson's articles](https://huonw.github.io/blog/2015/01/peeking-inside-trait-objects/) hints at stuff like this being possible in the future? &gt; The `size` and `align` fields store the size of the erased type, and its alignment requirements; these are essentially unused at the moment since the information is embedded in the destructor, but will be used in future, as trait objects are progressively made more flexible.
crossposted to r/pgrogramming for more insights. https://www.reddit.com/r/programming/comments/4ah1yf/the_epic_story_of_dropboxs_exodus_from_the_amazon/
Isn't the type of `Box::new(x.clone())`, `Box&lt;Clone&gt;`? That should be a legit trait object. Yeah, I don't get this. Is there any way to write a function `fn f(x: &amp;Clone) -&gt; Box&lt;Clone&gt;`? (besides not using `x` and returning something else)
Thanks!
I'm working on an interpreter for a toy language, mostly to have a non-trivial project to learn Rust. I seem to be fighting with the borrow checker a lot less now, but I still feel like I've got an overly poor understanding of what I'm doing with Rust. Interestingly for me, I'm finding I really like working with Rust, even with the frustrations of learning.
Unless you do something very specific (split stack, coroutines, shadow stack, etc.) - no. Your stack on Linux is normally preallocated and grows down automatically up to some limit (usually set by ulimit). There's a guard page below the available stack which causes stack expansion when something triggers it. If you want details, check mmap / MAP_GROWSDOWN. But it's only one way as far as I understand - mainly because the system can't really tell what can be unallocated and what's just currently unused.
More info: http://www.meetup.com/Rust-Rhein-Main/events/229564640/
This year is gonna be rustastic! Loving this.
Why wouldn't it replace Gecko in Firefox someday?
We already code against the standard. However, not everything is standardized, and not everything in the standard is high priority. Also, not everything is caught by standard testsuites like web-platform-tests, it's important to catch these bugs through regular use.
Pretty much. This has been our attitude towards the DOM, and I expect it to be the same for CSS.
Okay great, I'm really looking forward to it and thank you for the great work so far!
fair enough. I did actually read through https://doc.rust-lang.org/book/README.html the other day, but it's going to take time and a few more readings before it all sticks. I literally just started writing rust code today, so there's a lot I still need to pick up. Thanks again for your patience.
Perhaps you should use Linux? It seems that almost every Rust programmer, and the entire team developing Rust and Servo, is nearly exclusive to Linux with a side of Mac OS X.
`s` is already `&amp;String`, not `String`. Try `let () = s; banana.contains(&amp;s)` and check the error message. I think the idea is `filter()` just examines what's in the sequence without consuming it, so it doesn't take `String` by move.
You can still munmap part of your stack. But it would take some extra cost to restore/move the the guard page to the right place. (I think, not 100% sure) So to answer your specific question, I don't believe you'd see the memory reduction in top if you entered a function which takes 1mb of stack space and then returned from it. Then again... Why would you want to? Stack is usually soft-limited to just a few MB on Linux.
Except people aren't going to find it. There's not enough information to figure out how the problem could relate to anybody else's problem without looking at the nitty gritty.
Hmm, actually displaying most websites correctly would be nice too :-P
The plan *is* to replace gecko in the long run. However I'd be happier to see a new browser, something super lightweight. I don't particularly want extensions or bookmark syncing or any of that crap that loads plugins like flash. I like a nice simple UI which respects my OS. 
While I get your sentiment it will need to work on windows for it to become popular.
Can/will servo provide an alternative to Selenium WebDriver where you can drive a Firefox browser with java code?
This could actually be remarkably useful for me...
&gt; Either make it a [u8; 4] or use this when you're decoding. And use [this](https://doc.rust-lang.org/stable/std/primitive.u32.html#method.to_le) when encoding!
&gt; I'll be completely frank in stating that most of what you mention went completely over my head. Sorry about that! My comment was more like an out-loud thought experiment to see how easy it would be to satisfy your constraint. To be fair, it only makes sense to me because I've been working on this library for years! I hope to write some blogs posts in the future that are more penetrable.
The site layout is a bit broken at the moment because of Content-Security-Policy. We are working on fixing it. /cc /u/cmrx64 - maxcdn.bootstrapcdn.com needs to be whitelisted. 
Since TWiR is published on Monday, if you'd like to contribute, you can make a PR on Sunday which would usually cover all RFCs, FCPs for that week. And if something is missed, I can always add them at the last moment before publishing.
Both Rust and Servo are already pretty popular in the dev community as it is, so I doubt Windows users would add much to the fray. Most good programmers use Linux as their OS of choice anyway.
For once yeah, if only because the build configuration used by a larger rust app interests me.
I've been following TWIR since ~30 issues and it keeps improving. Thanks everyone who is participating!
Thank you for the kind words. This is mostly /u/nasa42's work, I only add the crate of the week (and notable updates since a few issues).
I don't think this has ever been the plan. Its a possibility, but not one I expect to happen. A new browser is a more likely possibility, and replacing at least *some* of gecko with parts of servo is a plan. But dropping servo into Firefox is a rather nontrivial task which involves a lot of Firefox specific features like XUL. Not part of the plan, but it is something that could happen. I personally don't expect that avenue to be pursued.
Good news! I'll be glad to test Servo on my phone once there is an APK (even if just alpha).
Congrats on all this publicity! Are you up for talking about it at an upcoming Rust meetup?
XUL. (Or at least, that's one huuuuge thing that'd be needed before such a thing was viable, from what I understand.)
In some cases, needed size could be calculated beforehand. The simplest example is returning same type of trait object without any conditionals. E.g. fn foo(x: &amp;Clone) -&gt; Clone { let myclone = x.clone(); myclone; } In this case compiler could generate another "hidden function", which would return pre-calculated size. The caller would call that one first to find out how much to pre-allocate and then it would call the real function and pass it the pointer to allocated space. In cases of variable size (like if condition { obj1 } else { obj2 }) it could just take max - some more bytes on the stack should not be a problem. In cases it can not reliably pre-calculate it would just emit error.
&gt; Reflect deeply on what it would mean to call clone() on an &amp;Clone; Rust tells you: "cloning a trait object and putting the result on the stack does not make any sense, you cannot do that, use `Box`/`Rc`/`Arc`/...". What does C++ tell you? Either it will silently make a copy of a pointer or slice your object. If you really want to clone you are on your own and have to devise your own mechanism for that. And good luck with it. There are some conventions but the language doesn't make any of them safe to use (*): virtual functions called `clone`? Sure why not, just don't forget to write them on _every_ member of the hierarchy and don't make any kind of mistakes or your objects will get silently sliced... (*) Since C++11 you have `override` and `final` but these checks are opt-in: you basically have to know that what you are doing is hard enough to get right that you should add a keyword to ask the compiler for a check. This doesn't help novices that have no idea what they are doing. At least Rust prevents them from shooting themselves in the foot.
The gif demo of browser.html at https://github.com/browserhtml/browserhtml is way more visually appealing than the last time that I looked at it. Well done! Though I'm not sure I like the idea of having to click twice to switch tabs. :P
I'm mainly waiting for reviews on the hook system for [imag](https://github.com/matthiasbeyer/imag) and a bunch of other PRs (~10), as they are ready IMHO, though my project guys are _soooo_ slow with review. imag is my personal information management suite for the commandline. Current state: Core is implemented, hook system must be merged. After that, some utility libraries must be merged (they are ready, PRs are there) and then I will start working on modules (a simple one is already done).
Two questions: - What drove the move to Rust precisely? Most of the players in the industry are somewhat reluctant about moving to Rust. I find it amazing that such a decision was reached, even going to the point of using a fixed nightly. Second one: Do you feel like the ecosystem is mature enough when it comes to distributed systems? Anyway, that's a bold move, it would be nice if that sparked more interest in the language as a result.
This means that after you use the first DST you need to start adding run-time offsets to compute pointers between stack frames. The more DSTs you have, the more run-time offsets you need to add/substract to get a pointer to a member of a stack frame far away. Is this a problem? I don't think so. Compilers can probably compute intermediate offsets and use those. Without DSTs compilers don't have to worry about this at all.
Although not enough to ensure that Servo actually compiles and runs on Windows, apparently. Actions speak much louder than words.
The year of Rust on the desktop!
Agreed in general, but in our particular case, our desire to use particular generics in particular parts of the codebase was specifically for their use as trait objects--and in ways that would violate object safety rules.
Sure! We can do something at the SF meetup.
This is amazing! I would love to see a more detailed writeup on how this was done. EDIT: I totally cracked up at the Swift Wind bit. That flags menu looks very extensive, is this based on some prior work?
Thanks for taking the time to talk it out with me. &gt; not impossible, just not practical Unfortunately, all I was really arguing was that it wasn't impossible. I've never dealt with LLVM, so, while I completely believe you, I also don't really understand that side of things. &gt; `memmove` can be slower than `memcpy` (as it has to deal with overlapping source and destination) Whoops, you're right, that would have to be memmove. One other approach I can think of, is to avoid the copy and instead temporarily waste memory. Basically make it so that when the callee returns, it's entire stack frame becomes part of the caller's stack frame. How bad that is depends on a few things, like how much stack the callee was using, and how long the caller lives, and how many levels of these types of functions are chained. But you might be able to do something akin to a tail call optimization here. Any locals or arguments that aren't needed anymore at the point this happens can be dropped before we allocate DST. But you'll probably still "leak" at least your return address and a few other things.
&gt; you can only hope that LLVM never makes it to v38.0 itself. :) That's what v380.0 is for :P
Any time you want to have a type conceptually use a type without having a member with that type, you can add `PhantomData&lt;MT&gt;` to the struct and the type checker will no longer complain.
Not sure why you've used a tree. Surely it's simpler to just keep splitting from the left: let (a, rest) = v.split_at(len / 4); let (b, rest) = rest.split_at(len / 4); let (c, d) = rest.split_at(len / 4);
Thanks, that worked for that error. Now how do I get the Len is not implemented for MT error to go away?
I would love to see servo graduate to being Mozilla's 2nd browser, rather than a FF replacement. But I also understand why asking a corporation to support 2 separate web browsers that only minimally share code is an unreasonable thing to ask :)
In the original post if I understand correctly, he says that he doesn't need MT to implement Len, he wants to know how to stop the type hierarchy requiring it to
But MT implements Sized; it doesn't need a .len() function.
Hey, thanks for gold! A nice surprise and much appreciated :)
I've made a pull request that allocates less (and uses the number of characters, not bytes, for ngram length): https://github.com/despawnerer/langid-rs/pull/1
&gt; [...] languages designed specifically for the new world of massively distributed online systems. Apple has one called Swift [...] Hm, not so sure about this..
Not quite yet, but there's some work going on in this space. If this is something you care about, we could use your help! The experience and requirements gathering would be particularly helpful. The issue is the mirroring: you can have a private repository full of your own crates, but it won't automatically mirror the ones on crates.io.
You can find a bit of that "some work going on" that /u/steveklabnik1 mentioned in [this cargo PR](https://github.com/rust-lang/cargo/pull/2361). This also has some discussion on the topic (including: "There's no clear immediate user of this PR…") which you could chime in on!
The multirust change was the biggest one for me personally. Having to locate and copy version of the stdlib into folders was a pain.
You have a lot of good ideas in there, especially the IO/Networking stuff. I'm not so familiar with Rusts traits yet, so I didn't use them. I also tried to use a trait, so the methods can take both an &amp;[u8] and a &amp;Vec&lt;u8&gt;, but that didn't work out right. But I could create a class CertificateLoader or something to do that stuff. I should also move the certificate verification logic to the new class CertificateVerificator. I aimed at providing a strong framework for digital signatures. That involves some kind of revoke-system and the easiest yet secure way to do that, is a HTTPS request to some server. But you're right, I'm not so happy with the 9000 dependencies either. I think I also need to clarify something: The revoke server can run on any system. The system is designed, that many systems using this crate share a revoke-server. For example a group of web servers share one (or more) revoke-servers. Therefore someone using this crate *doesn't* need to install PHP. Is there maybe some way like #ifdef in C? Then I would only need the hyper dependency, *IF* the user decides to use a revoke-server. But you're right, I should explain these things in the docs. But when I read docs, I (mostly) don't want to know WHY the developer decided to do it that way, if it works. Edit: How would you use that iterator? I'm only familiar with iterators to walk through arrays and such.
Soft-limited. That means you can just run "ulimit -s unlimited" and ignore it. As to why - it's because for most apps running over a few MB of stack means something went very wrong and it should crash now. Just a valid default limit. Same thing happens with only 1024 (by default) open file descriptors.
It was actually pretty easy. I wasn't reading the C++ code while I coded, so I thought Rust and didn't try to simply translate the C++. *But* there was one feature I was missing: **Function overloading**. I tried implementing something like that with a trait, so I could accept both &amp;[u8] and &amp;Vec&lt;u8&gt;, but that didn't work out, because I was not thinking the right way or something. :D I don't remember writing the C++, but writing Rust was relatively painless. Even though I'm learning a new language, it felt kind of familiar, which is maybe because I know C++, Haskell and Ruby.
Whoops! Sorry, didn't see yours – could have saved me some work there :). I fear I'm still mildly confused by Reddit's UI (as you might infer from me not logging in for a month...)
Thanks, missed that!
&gt; This is one of the rare occasions where I'm happy that I wasn't quoted. Manish's quote is much more positive. Nominating for the quote of the week in the next TWiR edition. ^^muhahahha
&gt; so I could accept both &amp;[u8] and &amp;Vec&lt;u8&gt; That just works! fn foo(_: &amp;[u8]) {} fn main() { let y = &amp;[1,2,3]; foo(y); let z = vec![1,2,3]; foo(&amp;z); } Specifically, it works because `Vec` implements the `Deref` trait: https://doc.rust-lang.org/src/collections/vec.rs.html#1215
Yes, you can do anything C would generally speaking (*thought to an extent you need to use Rust nightly right now, plugins are not stable yet, which is why some things require syntex + build scripts*), you should be able to do things like conditional compilation. You can do something like the `cfg!` macro: https://doc.rust-lang.org/nightly/std/macro.cfg!.html; so you can do conditionals based on configuration, but that's not always enough. Otherwise you can use attributes for conditional compilation: https://doc.rust-lang.org/reference.html#conditional-compilation with `#[cfg()]` on a lot of types, but not exactly all types. But with a fine control of macros, you should be able to use that to do everything you need, there's a crate that uses it for conditionally creating a macro: http://alexcrichton.com/cfg-if/cfg_if/index.html, which i find very useful. I don't know exactly how i would do what you want thought, this is has close has it gets in my brain right now. ------------ Well its pretty easy to understand Rust traits, really they're like classes but without data. Instead usually you want to compose your data. For example, if you have type `Thing` you can implement `Iterator` on it, depending on the iterator: 1. you can start to compose it by doing `.iter()` 2. then you keep composing/building it with other methods `.chain(..).zip(..).map(..)` each time you wrap the iterator into another iterator structure of each kind, so it becomes something like: `Map{ iter: Zip{ a: Chain { .. }, b: ..} } f: ..}`, a composed set of types. This is why its called composition. 3. Then you use it via its methods `.next()`, `collection()` or `.fold()` or you iterate over it with a `for` loop. -------------- For `&amp;Vec&lt;u8&gt;`, I'm not sure, i don't understand how you had issues (i don't think you should use that either), `&amp;[u8]` works for vecs, *because its a slice?* I mean this should work: http://is.gd/do1Z9g: trait A { fn f(t: &amp;[u8]) {} } struct S; impl A for S {} fn f(t: &amp;[u8]) {} fn main() { let v = vec![1,2,3]; S::f(&amp;v); f(&amp;v); } 
You don't have to: just click the pin. The idea behind the tab strip working this way was to cater to users who open a few tabs at a time and close them (the majority) as well as users who use tons of tabs (a large minority). An always-present tab strip is a distraction for the majority, as it usually sits there empty or with only a couple of tabs in it, so it's hidden by default. But you can permanently get a classic tab strip with one click. BTW, having a tab strip on the side opens the door for lots of tools for heavy tab users: tree style tabs, smart sorting of tabs, etc. Like everything, this is a huge work in progress and things about it will almost certainly change. So don't worry too much about the UI either way: it's an experiment.
Also, some programs that use a lot of threads will set their stack size smaller than the default to avoid running out of address space.
This [reduced code](https://play.rust-lang.org/?gist=3ebcfb16d470c353052e&amp;version=nightly) shows the error that is happening. fn takes_mut(_: &amp;mut str) {} trait Processor&lt;'a&gt; { fn bar&lt;'b: 'a&gt;(es: &amp;'b str) {} fn foo&lt;'b: 'a&gt;(&amp;mut self, es: &amp;'b mut str) { /* 'c */ { Self::bar(es); } takes_mut(es); } } I have labelled the lifetime of the block **'c**. So now there are 3 lifetimes **'a**, **'b** and **'c** with **'b** : **'a** : **'c**. In order to call the function "bar" rust must take a "&amp;str". "es" won't do, as it has type "&amp;**'b** mut str", but it can take "&amp;\*es" (dereferencing first and then taking a reference). But which lifetime to take? Well the definition of bar says that a reference can have any lifetime so long as it outlives **'a**. We would like to use **'c** so that our borrow ends as soon as possible, but **'c** does not outlive **'a** so we can't use it; but **'b** outlives **'a** and **'a** outlives **'a**. So we can use "&amp;**'b** \*es" or "&amp;**'a** \*es". But note that both of these references have a lifetime that outlive the scope of the function "foo"! That is why the "previous borrow ends here" - at the end of "foo". Every valid lifetime we can choose for this reference lives at least as long as the scope of the function "foo". But this means that when we try to take a mutable reference to *es, it is already borrowed. In your case with multiple traits running around the situation is no different. The function "all" cannot be called without taking a reference that will outlive the scope of the function "foo", so you can't take a mutable reference to *es after calling it. Hope this clears things up as it is very confusing at first!
This name helps googleability, thanks!
You use an iterator by implementing its [next](https://doc.rust-lang.org/std/iter/trait.Iterator.html#tymethod.next) method: https://doc.rust-lang.org/std/iter/trait.Iterator.html; anything else is left to the iterator. A lot of types will want to impliment iterator for you if it makes sense, like Vec. There are also different types of iterators, because some iterators are infinite, others are exact known sizes, others can be double ended. Examples in `std::iter`: https://doc.rust-lang.org/std/iter/index.html
Sorry to confuse you, but I know how iterators work. I was curious how you would construct a Certificate from an iterator?
Does... does Windows have a terminal?
It has a console, which serves the same purpose, just with API calls instead of escape codes.
On one hand, it's a shame when the good lessons of `curses` and `terminfo` are lost (`curses` was the original React!), but on the other hand, 99.9% of all the terminal emulators in use today are compatible with the "easy" 80% of VT100 escapes, and most of them support xterm extensions that aren't handled by terminfo at all, like mouse events, UTF-8 and true-colour. I wish there was a modern terminal library as general as ncurses but without the pre-VT100 compatibility limitations... but at this point it's probably too late.
Nominate all you want :-P
Cool, thanks
What API would such a library write to? I've thought about making such a thing as a side project, but I honestly don't know whether there's an API I could target or whether I'd also be writing my own term window as well...
Upvoting this because I just ran into the same thing. Thanks for the explanation!
Thanks, but when I use this with shellcode that calls /bin/sh, it segmentation faults. EDIT: Nevermind, I figured it out.
https://retep998.github.io/doc/kernel32/fn.GetConsoleScreenBufferInfoEx.html
A `Box&lt;[T]&gt;` is two things: a pointer to some memory and an integer which says how many `T`s are in the memory. The `T`s themselves are on the heap.
I think the "easiest" way in the meantime would be to set internally redirect the hostnames for crates.io and github on the build machines, so that it just thinks it's pulling from crates.io. That might work. But kudos. Also trying to introduce rust at work and we'd have the same problem.
&gt;P.S.: Termion means thermit in Magyar. No:-)I don't know where you got that but it is not true.
Rust and a js web page can effectively do tcp together. That's basically what websockets is. So he used json for his messages and has js logic to manipulate the dom as he sees fit based on the messages.
&gt; I would like to know what's your motivation behind Termion ? I needed a library, which lets you easily make interactive text based program, without to much hassle. &gt; What do you want to achieve and why term is not enough ? `term` is entirely different. It is not for interactive text-based programs, it is a terminal formatting library. It supports setting colors, moving the cursor, and reading information about the terminal. Whereas, termion supports things like non-blocking stdin, raw mode, better cursor movement, 256-color mode, special key input, console size, text formatting, password input, and so on. &gt; The diversity of the terminal emulators out there and the different variations in the implementation the cough standard make it really difficult to have a good test coverage. What would be your goals on the matter ? Yeah, that's right. I try to stick with the well-supported escape codes, which hopefully means better support. Termion doesn't support exotic escape codes. &gt; In the end, I am asking why I would chose termion as a dependance over term or a libterm binding ? As said before, they're entirely different libraries. Choose what you need. They don't replace each other.
Oh, god.
Stupid question: Do anyone use a non-ANSI terminal?
Still, you say you usw your own net stack. That could be something other people would like to use.
&gt; without Huh? So are you saying that good C++ programmers use a borrow checker in their heads, or that references and smart pointers are reasonably close (unlike concepts used in other languages)
It is not entirely clear from your post - do you want your contained types to be able to serialize to different types of messages? If not, you should make T an output parameter to Serialize/Deserialize, rather then input. If you do this, there will be no longer need to mention MT in MsgVec.
Thanks for all your hard work, I really enjoy TWIR!
My pleasure. It'll be a fun way to get people into compiler hacking :-) My plan is to have a USB stick with the nightly compiler docs + checked out clippy.git. Also I'm going to reserve some 2-5 easy issues for us to work on – the goal is to kickstart teams that can work independently after the workshop.
Sloowly working on my TBS game [Zone of Control](https://github.com/ozkriff/zoc), as always :) Trying to add city tiles right now: http://i.imgur.com/JTdUgX5.png ([Other screenshots for devlog](http://imgur.com/a/FmVVl))
Hi! I think those links will be interesting for you -- https://doc.rust-lang.org/book/, http://rustbyexample.com/, https://play.rust-lang.org/ 
In my case, a Certificate not only consists of a bytestring for the public key, but also an optional private key, metadata and an expiration date. These are many different data types. I'm not sure how to do that with your approach. Would you give more than one iterator maybe?
Thanks, I'm reading it now
[I certainly hope they will.](https://github.com/withoutboats/notty) More seriously, the issue really has to do with the fact that there is no singular ANSI terminal. Like you support 256 colors: I'm not sure what encoding you're using, but xterm supports two different ways of selecting colors, and the many other terminals that are in use are not necessarily consistent in which of these they implement and how. There are many other commands of varying degree of importance which may be implemented or may not, depending on the terminal.
We have similar requirements for building Debian packages without fetching remote resources. In order to build cargo (which itself depends on ~50 crates, that must be locally available) we follow an approach very similar to [cargo-vendor](https://crates.io/crates/cargo-vendor). The main differences with a true "local mirror" as in [cargo PR#2361](https://github.com/rust-lang/cargo/pull/2361) are that 1) we ship a "directory source of unpacked tarballs" (to ease inspection/patching) and 2) there are no "sha256 checksums" constraints (sources and binaries are digitally signed upon upload, though). That said, I'm not sure if it fits your case as we don't exactly "host a local mirror". In case, you can check the whole stuff [directly here](https://anonscm.debian.org/cgit/pkg-rust/cargo.git/tree/debian/README.source?h=debian/sid)
You could set up a small VM someday though - it might be easier than you think... :) (I do the same for Linux, since I'm only using Windows and OS X.)
Fixed, thanks!
This has been postponed to say the least. The devs are trying to add the necessary features conservatively. This is the newest rfc: https://github.com/Kimundi/rfcs/blob/conservative-impl-trait-the/text/0000-conservative-impl-trait.md
This paper doesn't have a date on it. Is it recent?
Sorry, I wasn't clear enough. I meant compile-time error - if compiler can not generate working code.
Just wondering: how does **'a** outlive **'a**? Is it better to say **'a** is not shorter than **'a**? (Or **'a** is greater than or equal to **'a**?)
In semver, 0.x.y does not provide any stability.
Thanks for the meaning of termion. I was wondering if it is newly discovered fundamental particle which all terminals are made up of. :-)
That's the point: gtk-rs will certainly have other breaking changes in the future. Once it'll be stabilized enough, then we'll upgrade the version to 0.x.y.
I thought `=` allowed you do to this, and my experiment now with that seems to work.
Cargo's implementation currently makes `0.y.z` the same as `x.y.z`. This is the only deviation from semver; many other implementations do it as well, since it's what people expect in practice. (I'm personally against it, but what are you gonna do)
Yes, this is what `=` is for.
I might misunderstood the [doc](http://doc.crates.io/crates-io.html#pre-10-versions) but that's not what I understand... :-/
Thanks for the confimation! EDIT: oh right, I was confused by the difference between `gtk = "0.x.y"` and `gtk = "=0.x.y"`.
Currently I am only aware of [orbtk](https://github.com/redox-os/orbtk), and [conrod](https://github.com/PistonDevelopers/conrod). I guess you could argue Servo is technically one as well, technicaly. I think it's reasonable to assume that as time goes on more projects related to gui's will pop up.
I did some work to setup a local mirror at Dropbox using my findings from writing this post: http://gmjosack.github.io/posts/dissecting-cratesio-minimum-mirror/ If you have any questions I could try to answer them.
Our system is just a wrapper around the GObject's implementation (https://developer.gnome.org/gobject/stable/gobject-memory.html). I draw a parallel with `Rc&lt;RefCell&lt;T&gt;&gt;` because in both cases we have a reference-counted shared reference (the `Rc` part) which is mutable but opaque so the methods which take `GObject*` could be seen as taking `&amp;self` with a `RefCell` inside if you squint hard enough.
[New gtk-rs version is out!](https://www.reddit.com/r/rust/comments/4an3vq/new_gtkrs_version_is_out/)
I believe the real feature such systems should strike for is zero maintenance. If you require library authors to manually register to the list, and make changes, this means the website is going to have a hard time growing. Instead, one should automatically gather the list and organize it in a viewable manner. Like, we already have the package configuration in `Cargo.toml`, so you can extract the homepage from there and add the library into categories based on `keywords` field. Next step is to crawl github and gitlab (to the very least) and find all these repositories yourself, while still having a custom way of adding stuff that's beyond the crawler access.
Maybe I'm wrong, but this really doesn't look like constraint programming to me, as the variables' domains are independent. This is a general optimization problem, hence the question being about the genetic algorithm. To answer OP, there are a couple of libraries out there, but I really don't know how solid/friendly/maintained they are. Check out the list on [crates.io](https://crates.io/search?q=genetic).
Are we ever going to see these IO libraries? Rust is definitely missing a great networking stack for server apps right now.
I asked a similar question a while ago. As a reference, [here](https://www.reddit.com/r/rust/comments/49c95n/nativelooking_ui_with_rust/) is that post.
What's a thermit?
That's awesome, thank you! I didn't realize that Rust had type quantifiers -- this is quite useful.
Thanks for your suggestions. Half of what you mention is already implemented. Authors only need to provide crates.io or repository URL and rest of the information is automatically fetched.
I'm most interested in 1-4 since those were described as "developers decided it’s *not* a bug". They don't sound like egregious issues but I'd like to read the examples and justifications for why they're not bugs. For instance, if (1) rejects a more ergonomic version of code, then I'd consider that a usability problem. If it only rejects obscure code that is technically correct, then sure, mark it as "won't fix" or maybe a low priority improvement. I'm most curious about (3) and (4). Were the "forced" lifetimes in (3) truly identical to unforced? If so, rejection would be unintuitive. The only reason I can think of where (4) *wouldn't* be a bug is if the expression results in something that cannot currently be expressed as the return type of a function. In all cases I would be interested so know the real causes and resolutions instead of my semi-informed speculations.
This would make the toolkit non-native (with respect to native OS-specific controls) then?
Really cool progress. I loved the term "breaking improvements".
I'm currently trying to pass a mutable vector to threads. I'm working on a project where I'm implementing the same algorithm in several different programming languages as close as possible to the original. In C I just passed a pointer to the array to the threads. I have attempted to do the same thing in Rust. let mut matrix = vec![0; (rows * cols) as usize]; //lanuch threads for i in 0 .. threads{ thread::spawn(move || t_rand(i, threads, rows, cols, seed, &amp;mut matrix)); } I understand now that in Rust I can't simply do that because of the ownership rules. Now, I am very new to Rust (day two) so of course my syntax is probably not 100% correct. One thing that I have tried, but could not get to work was passing a slice of the original vector to the thread. The idea being that the tread only claims ownership over a subsection of the vector and thus does not violate the ownership of any other thread. However, I haven't been able to get this to work either because it's impossible or because I don't fully understand slices and ownership syntax. My question boils down to this: I have a vector that I would like to share among threads so they can work on it concurrently. What is the way Rust would prefer this be done?
If you've done a lot of work in C(++), you're used to running through these proofs in your head... where does the memory live, who references it, who frees it, how do we make sure all references are dropped when it's freed, etc. Rust essentially works through these proofs with you. But you're more familiar with the general exercise if you've coded C and C++, since I think you recognize it as a version of what you've always needed to do in malloc/free/pointer type scenarios.
Yes, I discuss in a sibling comment that we're working on open sourcing some of that.
&gt; Also a Box&lt;[T]&gt; is clearly different to a &amp;[T] since the latter also consists of a pointer to some memory and a size, but does not own the heap memory. That does *not* require a difference in representation at runtime. The information that one value is `Box&lt;[T]&gt;` and the other is `&amp;[T]` is used at compile time to let the compiler insert the equivalent to a `free` call when the box is dropped, while it doesn't insert something like that when dropping references/slices.
I'm going through the Rust by Example tutorial, and was wondering what the difference between the below two would be: fn apply&lt;F&gt;(f: F) where F: Fn() { f() } fn apply&lt;F: Fn()&gt;(f: F) { f() } Both seem to be saying the same thing, that f is of type F which is in turn of type Fn(), so why even have the different ways of declaring it? Edit: I know that technically, Fn() is a trait which is not a type but practically the two seem to have little difference in a function declaration.
That is a great point about the lack of specification. Most people do not consider this when looking to rewrite something. All of the safe languages you listed are garbage collected. I do think Rust has certain properties that are making people seriously consider moving core programs/libs from C/C++ to Rust.
I'm having trouble with implementing generic structs. I'm trying to learn generics in Rust by doing a simple Circle struct that only takes "radius" as a parameter. Then I will implement a generic function for Circle called "get_area" which just calculates the area. Here's my code that I cannot figure out how to get working. struct Circle&lt;T&gt; { radius: T, } impl&lt;T&gt; Circle&lt;T&gt; { fn get_area(self) -&gt; T { let Circle{radius} = self; let pi: f64 = 3.14159262; let retval: T = (pi as T) * radius * radius; retval } } fn main() { let c1 = Circle {radius: 5}; let radius_of_c1 = c1.get_area(); println!("Radius of c1: {}", radius_of_c1); } There errors I'm getting from rustc: Compiling structs v0.1.0 (file:///Users/Alex/Documents/Projects/structs) src/main.rs:9:26: 9:33 error: binary operation `*` cannot be applied to type `T` [E0369] src/main.rs:9 let retval: T = (pi as T) * radius * radius; ^~~~~~~ src/main.rs:9:26: 9:33 help: run `rustc --explain E0369` to see a detailed explanation src/main.rs:9:26: 9:33 note: an implementation of `std::ops::Mul` might be missing for `T` src/main.rs:9 let retval: T = (pi as T) * radius * radius; ^~~~~~~ src/main.rs:9:26: 9:33 error: non-scalar cast: `f64` as `T` src/main.rs:9 let retval: T = (pi as T) * radius * radius; ^~~~~~~ error: aborting due to 2 previous errors Could not compile `structs`. Looking online at Stackoverflow and some other places they mention using FromPrimerative, which is now deprecated as of Rust 1.0.0, so I'm stumped. I come from a C background, so this is a bit new! Please help!
&amp;gt; safe languages like [...] Java Just no. Java is by no means a safe language. NullPointerException anyone? But for the other parts of your comment: yes, totally! (I'm a nixos user as well)!
Can't exploit a nullptr exception on most (all?) platforms. edit: Let me clarify - you can never (within reason ie: something has to go crazy wrong) exploit a null ptr exception in Java. However, in a language like C, it's going to depend a bit more on the platform, but AFAIK every major platform will terminate a process that tries to access addr 0.
So first you must have a way to get a pi value in your type T, and sorry, `pi as T` doesn't magically work here. You could of course create a trait that allows us to get a pi value (either by associated constant or by method). Second, your T must implement the `std::ops::Mul` trait (as the error message tells you). You can do this by requiring that `T: Mul` (this is called a trait bound, and in this case means that our `T` type must have a multiply implementation).
I've been following this rust course on github: http://cis198-2016s.github.io/schedule/. It has some homework exercises so you can get your hands on rust 
Great! I'll sync with you offline about setting something up.
The `&lt;T: Bound&gt;` syntax is easier to read for simple bounds, but it's not capable of describing all possible bounds. Something like `where &lt;T as Iterator&gt;::Item: Clone` isn't something you can express without a `where` clause.
Did you try `cargo:rustc-link-lib=dylib=bar`? Also since you're specifying which libraries to link to in the build script, there is no need for that `#[link]` attribute in your code.
You can't configure this in the Cargo.toml as far as I know, but there is the `cargo rustc` command which you can use in place of `cargo build` to pass extra flags to rustc along with all of the flags that Cargo adds. It works like this: cargo rustc [cargo flags] -- [rustc flags] The Cargo flags section can be omitted but the `--` is necessary to tell Cargo where its flags end and the rustc flags begin. So something like cargo rustc -- -Z verbose is probably what you want.
Thanks. Do you know if this a hidden command? when I consult the manpages on cargo, the rustc command is not in there. 
It's sometimes possible to exploit a null pointer. If you take a large offset from a null pointer, like `a[1 &lt;&lt; 20]`, you can reach back into allocated memory and access out-of-bounds.
In Java? Java's bounds checked.
Panics triggering vulnerabilities (other than straightforward DoS) seems unlikely, given that exceptions typically don't trigger vulnerabilities in other languages. I think the vast majority of vulnerabilities in Rust apps will be straightforward logic errors: filesystem races, broken password reset logic, stuff like that. SQL injection and crypto misuse and the like are possible, but unlikely *if* we as a community maintain a high standard of quality in our library ecosystem (which we have so far).
Okay that makes a lot of sense on the &lt;String&gt; example. I feel very overwhelmed by all the syntax. I went through a majority of the pages in the rust book at doc.rust-lang.org/book and I thought I had a pretty good idea of things, then I started going through the source code of some projects on Github and it's like reading a foreign language again. Like this "&lt;T: Mul&lt;Output=T&gt; + HasPi + Copy&gt;" for example doesn't look anything like what I've seen on the generics page in the book. Are there any resources on the language's syntax? I feel completely overwhelmed. Seems like there's a lot of defined syntax in comparison to C. Thanks, Minno!
&gt; Are there any resources on the language's syntax? The book covers the most common parts, so once you've gotten that down you should be able to just search out answers for whatever parts that are left that you don't recognize. I don't think there's any up-to-date comprehensive guide. There's a reference [here](http://doc.rust-lang.org/reference.html), but it "tends to be out of date", as stated on [the main documentation page](http://doc.rust-lang.org/). It's only really useful if you see a bit of syntax and you have no idea what to call it so that you can google for more helpful descriptions. There are also a lot of examples [here](http://rustbyexample.com/) that cover different sorts of syntax. &gt; Seems like there's a lot of defined syntax in comparison to C. Yes, C is a minimalist language. Rust just flat-out has more features and tries to be more explicit than C, so as a trade-off it can't be as lean.
&gt; But that wouldn't involve dereferencing a null pointer,. It involves a null pointer, but not dereferencing it. And the error is really a bounds checking issue and would trigger a whole other exception in Java. But that wouldn't involve dereferencing a null pointer,. It involves a null pointer, but not dereferencing it. And the error is really a bounds checking issue and would trigger a whole other exception in Java.
In this case, it's simply because you forgot a lifetime parameter: impl&lt;'a&gt; Person&lt;'a&gt; { fn add_name(&amp;mut self, name: &amp;'a str) { self.name = name; } } This will ensure that `name` will live as long as `Person`. However, if you want to give the user the option of copying or borrowing, you can use `Cow` like /u/whataloadofwhat suggested. It'd look something like this: struct Person&lt;'a&gt; { name: Cow&lt;'a, str&gt;, } impl&lt;'a&gt; Person&lt;'a&gt; { // `name` can be `String`, `&amp;'a String`, or `&amp;'a str` fn add_name&lt;N: Into&lt;Cow&lt;'a, str&gt;&gt;&gt;(&amp;mut self, name: N) { self.name = name.into(); } } Personally, I love `Cow`. I use it everywhere when I want to give the API user the option of moving or borrowing. I (ab)use this a lot in [`multipart::client::lazy::Multipart`](http://cybergeek94.github.io/multipart/doc/multipart/client/lazy/struct.Multipart.html#method.add_text), where I want the most flexibility for clients. Edit: fix code
Not sure this is a "simple" question, but since it boils to emulating inheritance, there's probably a simple way to do it. I'm looking to implement a "secure box" which is like a `Box&lt;T&gt;` in every way, except when it gets destroyed, it *also* zeroes out the memory before it does the equivalent of a `free` call. Now, I think I understand how to write a completely independent class that works like a Box and does what I want, but I'd also like to inherit all the implementations Box already has, and since Rust doesn't have inheritance I'm lost as to how to do that in a useful way. Any suggestions? (I don't need to be selective about this, so if there's a way to hook into the deallocator itself and do the zeroing *there*, that would also solve the problem, but that doesn't seem to be an option.)
There's a long tradition of FLOSS projects sticking with `0.` for a *loooong* time. Reason being that starting your version with `1.` implies some sort of feature completeness: `frobnitz-0.8.2` really can't frob nitzes in the way a proper frobnitzer is supposed to. I know that's going to be unintuitive to Mozilla people, especially after the Firefox version explosion (are we at three digits, yet? Lost count), but that's how it is. I was actually genuinely surprised when mplayer finally released its `1.0`. At a time where I was using mplayer2, which by now is dead again, but long live mpv... `0.14.0`.
Yes. My original question was about where the code that manages the behavioural difference is. When I used the phrase "carries around" I did not mean to imply the data was embedded in the thick pointer itself, though it probably wasn't very clear.
This will be interesting to watch and potential model for other projects to follow. So excellent.
I guess you'd want to look [in liballoc](https://github.com/rust-lang/rust/blob/master/src/liballoc/boxed.rs) but it's kind of scattered with no real documentation on how it works because these internal interfaces are highly unstable. Maybe someone else knows more. And slices are built into the language though some of their behavior is specified [in libcore](https://github.com/rust-lang/rust/blob/master/src/libcore/slice.rs). I hope I'm answering your question -- it's not 100% clear.
Thanks for the pointer about `Deref`. Sorry for not being clear earlier - there's no issue with the data being copied, I just want to make sure that all copies of it are zeroed out before my program loses control of them. (So nobody can stalk the heap afterwards and get sensitive data out.)
Careful, though - in C it's undefined behavior, so compilers can perform weird optimizations based on assuming it doesn't happen.
Like basically any other major language? C#, c, c++, python..
Er, that's not true. `malloc` (unlike C++ `new`) is allowed to return null. But yes, most code is unlikely to be broken by null-related optimizations in practice... [with some exceptions](http://blog.mycre.ws/articles/bind-and-gcc-49/).
I know I know, that's what I was trying to get at with the 2nd paragraph `mem::forget` and `Rc` cycles--I was around when the `scoped`/`drain` fiasco happened. But the way I understand it, formally, because of `Drop`, you can---pedantically---argue it's linearly typed. Since you can't get a return value out of that, it is of course not quite the same. E.g. I would love to be forced to call `close` on a file and have to check the `Result`.
I feel silly now :) Do callers have to add `Borrowed`/`Owned` explicitly most of the time, or is there some trait magic that makes things just work?
Wrong subreddit yo
Okay thanks. I was messing around and found out that the Mul&lt;Output=T&gt; part is a Type defined and used in the Mul struct. But I have a question about what the angle brackets mean in this case. Can you get at any value in a struct and set it using the angle brackets? While asking about angle brackets: What does &lt;T as HasPi&gt; mean? I see that it uses the :: notation afterwards, so is wrapping "T as HasPi" inside angle brackets treating it like a method within something like String::new()? Last question, where it says "fn pi() -&gt; Self" in the trait, what does the Self mean? Why isn't it something generic? Thanks!
Can you (or Kyle or Niko) give any insight to the four issues without links that were not considered bugs? What did they look like in code? Why were they not bugs?
I've been writing a lot of DSP code over the past two years with Rust and I feel like I might have finally refined my experiences down into something useful with this crate. Real-time audio threads are generally renown for requiring very performance sensitive code. For example, in a low-latency audio stream (say 64 frames per buffer) with a DAC requesting audio at a sampling rate of 44100hz you can expect your application's "audio callback" to be called with demands for moar samplez about 689 times per second. This kind of code can rarely afford to request system resources directly, wait for other threads, etc. In other words, [time waits for nothing](http://www.rossbencina.com/code/real-time-audio-programming-101-time-waits-for-nothing). Thus in the past most low-level audio DSP libraries have traditionally been written in C or C++. Although audio development rarely gets called out specifically when discussing Rust's target audience, I can't help but feel the two are a perfect match - for all the same reasons why Rust makes for a neat alternative to C and C++ in general :) With this crate revamp I've tried to provide a set of fast, fundamental building blocks for audio DSP: - The [**Sample** trait](http://rustaudio.github.io/sample/sample/trait.Sample.html) to remain generic across bit-depth. - The [**Frame** trait](http://rustaudio.github.io/sample/sample/frame/trait.Frame.html) to remain generic over channel layout. - The [**Signal** trait](http://rustaudio.github.io/sample/sample/signal/index.html) for working with **Iterators** that yield **Frames**. - The [**slice** module](http://rustaudio.github.io/sample/sample/slice/index.html) for working with slices of **Samples** and **Frames**. - The [**conv** module](http://rustaudio.github.io/sample/sample/conv/index.html) for fast conversions between slices, frames and samples. - The [**types** module](http://rustaudio.github.io/sample/sample/types/index.html) for provided custom sample types. - The [**rate** module](http://rustaudio.github.io/sample/sample/rate/struct.Converter.html) for sample rate conversion and scaling. Any feedback/critique/questions/help appreciated :) Edit: big thanks to /u/cmrx64 who just added support for `no_std`!
The solution used to be to add an [`IntoCow`](https://doc.rust-lang.org/std/borrow/trait.IntoCow.html) trait bound so that the callers wouldn't have to worry about the details, but I see it's now deprecated. You should be able to add an `Into&lt;Cow&lt;'a, str&gt;&gt;` bound to your functions and call `.into()` in the body instead, though.
Sadly, this is Yet Another Misleading Graph, since the fill under the curve goes to the lower end of the Y scale, which is in the negatives. That makes the growth seem always positive, and much larger than it really is (especially for the Python one!) Brought to you by the "just-use-a-pretty-graphing-library-without-looking-at-the-result-twice" crowd.
Well, [Humpty_Dumpty](https://github.com/Manishearth/humpty_dumpty) should be reasonably...close. ;-)
I'm interested in the difference of this boring `fold()` and `flat_map()` in runtime performance (if any): a.iter().fold(Vec::&lt;T&gt;::new(), |mut acc, v| { acc.extend(v); acc });
Ooh, I didn't know the curl crate would do that, that's neat. Looks like it's configuring a super minimal curl as well, that's nice. I'll have a go trying that. May end up leaving the curl that's there in the container though, as cargo build seems to always rebuild stuff when I re-enter it at the moment.
sure! i just like using syntactic sugar for more than the most basic tasks. it has the advantage that you intuitively know what to do: index/slice a thing? use `thing[0]` or `thing[1..3]`. not there? damn, is it called `thing.slice()` or `thing.index()` or `thing.subset()` or `thing.get()` or `thing.at()`, …?
No need to call it useless. It's not useless, just limited in its scope. The current set of index traits can't express everything we want to do. The simplest example is the indexing on a bit set, which should return a boolean, but the data structure does not store booleans and doesn't really have bool references to give out. Two RFCs about indexing extensions: - IndexAssign https://github.com/rust-lang/rfcs/pull/1129 - IndexMove, IndexSet https://github.com/rust-lang/rfcs/issues/997
The reason it's defined that way is probably because you want the lifetime of the returned value to match `self`. That's the reason I can think of off the top of my head. fn index&lt;'a&gt;(&amp;'a self, index: Idx) -&gt; &amp;'a Self::Output; // note: lifetimes are elided in documentation If the trait returned `Self::Output` instead of `&amp;'a Self::Output`, there would be no way (that I can think of) to specify that the returned value's lifetime should match `self`. impl Index&lt;usize&gt; for MyType { type Output = &amp;'b SomeType; // what lifetime goes here? fn index&lt;'a&gt;(&amp;'a self, index: usize) -&gt; Self::Output { // 'a exists only in this scope ... } } 
Very nice. I'm actually working on a, free and open source, podcast client for Android in my spare time, and is currently rewriting the model in Rust for portability (and fun of course). One of the next steps would be to implement variable playback speed (using PSOLA) which is currently handled by some old C code. I assume this project would be a good starting point?
that makes sense, but it’s an issue with expressivity, not a fundamental one. similar with Iterator: you can’t have `next` return a reference which lives as long as the loop body. we need something in rust that makes us able to express such cases without falling back to a `slice` function (in the `Index` case) or `while let` (in the `Iterator` case)
I wish I knew how to do it. I tried building it against armv7, but it failed, I guess it requires some additional changes to the toolchains. Passing these options to rustc fails, as target-cpu requires some additional attributes. It seems to me this requires some LLVM knowledge. This is all pretty steep for a newcomer, I just wanted a basic solution that will work on my setup, hence not digging into optimizing it all for armv7.
After reading this, I realized that there is a subtle inconsistency between the `struct` syntax and the `trait` syntax. That is, the former uses commas to separate items, whereas the latter uses semicolons. C, C++, and D don't have this "problem" because they only use semicolons. Why were commas chosen over semicolons to define `struct`/`enum`s? Anyone knows the history?
3MB is too big for such a simple application. Are you sure dead code is eliminated at least during the link phase?
Huh is orbtk crossplatform or redox only?
That's exactly what I needed, thanks!
Thanks. I'm not a native English speaker, so some esoteric spellings like this confuse me.
So something like this? (pseudocode) pub trait Index&lt;Idx&gt; where Idx: ?Sized { type Output&lt;'a&gt;: Deref; fn index(&amp;'a self, index: Idx) -&gt; Self::Output&lt;'a&gt;, } That seems roughly on the same level of abstraction as other Rust traits.
if i recall correctly it uses orbclient which has a redox and a sdl backend. But i might be total wrong about that. EDIT// works on linux.
Not long before 16384
hmm, not deref. in memory, a matrix can be laid out like this (or the other way round) __ __ __ 0▏▕1▏▕2▏▕3 __ __ __ __ __ __ __ __ __ 3▏▕4▏▕5▏▕6 -&gt; 0▏▕1▏▕2▏▕3▏▕4▏▕5▏▕6▏▕7▏▕8▏▕9 6▏▕7▏▕8▏▕9 ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ so a slice isn’t (necessarily) continuous: __ __ __ ▏▕ ██ ▏▕ __ __ __ __ __ __ __ __ __ ▏▕ ██ ▏▕ -&gt; ▏▕ ██ ▏▕ ▏▕ ██ ▏▕ ▏▕ ██ ▏▕ ▏▕ ██ ▏▕ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ ¯¯ but still can be represented as just a few `usize`s
it’s already used for that: impl Index&lt;Range&lt;usize&gt;&gt; for ... {} // x[1..2] impl Index&lt;RangeFrom&lt;usize&gt;&gt; for ... {} // x[1..] impl Index&lt;RangeTo&lt;usize&gt;&gt; for ... {} // x[..2] impl Index&lt;RangeFull&gt; for ... {} // x[..]
Getting accurate *free* metrics is harf
Seeing such an article in a publication that usually concerns itself with Enterprise Java and similar stuff makes me disproportionally happy ;-)
Yep, both should result in a `Vec` containing every element.
That's absolutely right.
Yeah, the mass-cleanup basically made the first graph unreadable. The "Total Subscribers" tab is more useful.
If you want to hook into the allocator itself, see the [custom allocator API](http://doc.rust-lang.org/book/custom-allocators.html). You could write your own allocator that wraps one of the standard ones, with some added code on deallocate.
Given that they were in discussion with the developers, it could well be that no issue was formalized.
Is this compiler magic specific to the Index trait? Or can I return lvalues on my own custom functions? In this context I think of lvalues as C++'s references but Rust does not expose these directly. EDIT: Basically can I create my own functions that return lvalues? To which I think the answer is 'no' after reading the linked rust reference.
Everything should work transparently after `--target=armv7-...` provided you installed the `armv7` [crates](http://static.rust-lang.org/dist/rust-std-nightly-armv7-unknown-linux-gnueabihf.tar.gz). I didn't give you the full command line for your current setup, trusting you knew which features you wanted to enable. Here's a working example targeting Cortex-A53 based SoCs (`armv8` actually, no idea if your cross-tools are going to handle it), like Rpi3 or Odroid C2: `-C target-cpu=cortex-a53 -C target-feature=+v8,+vfp4,+neon` You can also try stuff like: `-C llvm-args=-force-target-max-vector-interleave=4` 
A bit human-centristic today, are we? /r/botsrights
I asked the same question on this sub a while back. It's a bit of a syntactic wart IMO, but a minor one at that. TBH the only solution maintaining backwards compatibility would be to allow both, which seems... Ugly :P
The docs say that "an `#[allocator]` crate cannot transitively depend on a crate which needs an allocator", so could you elaborate on how it's possible to wrap an existing allocator with a new one? (Or did I misinterpret that, and *one* layer of allocators depending on another is OK?)
I'd give my firstborn to experience it once! ^(just kidding lol)
Yeah, and that's what crate bit-vec does actually.
Ah my example was not so good, in my actual code I don't want actually want to mutate the original variables, I just want to hold references to them in a certain order: http://is.gd/Dmfy2r fn main() { } use std::mem; fn foo(mut a: &amp;i32, mut b: &amp;i32) { if *a &gt; *b { mem::swap(&amp;mut a, &amp;mut b); } } I just want to hold references `a` and `b` in a certain order. This is what I want; I just wished I could reuse the parameter names: http://is.gd/OqB9LR #![allow(dead_code)] #![allow(unused_variables)] fn main() { } fn foo(a: &amp;i32, b: &amp;i32) { let (lo, hi) = if *a &lt; *b { (a, b) } else { (b, a) }; } In the end the second is probably way better than dirty `mem::swap`ing, but I am still curious why my original code is incorrect according to Rust. I suspect it has to do with the order of parameter variable destruction and Rust doesn't understand that in this case it just doesn't matter.
D'oh that makes a lot of sense, thanks.
No need for a firstborn. Just keep on learning. Don't give up, your day will come.
The trends are just for number of votes on StackOverflow, so there are a lot of things that could affect it. Lower churn in the language since 1.0 means there are fewer questions about new changes, documentation has been improving and so there's less need to ask questions, etc. Anecdotally, it doesn't seem to me like SO is super active for Rust compared to reddit, discourse and IRC, and if that's true the numbers of participants will be even smaller, so even a few people becoming less active on SO would have a large percentage impact.
Commas are used properly in my opinion, truly (even pattern matching uses them), traits are the things that should also have commas, that and macros.
/u/comex just linked to an example where it did.
Indexing sugar is interesting because there's two things people generally expect for simple types (like arrays, vectors, hashmaps): - `e = x[y]` should give the value, e.g. if `x: Vec&lt;u32&gt;`, then `e: u32`, - `e = &amp;x[y]` should give a pointer *into* `x`, not be equivalent to `tmp = x[y]; e = &amp;tmp`. Satisfying both of these requirements with a single trait means that `x[y]` essentially desugars to `*x.index(y)`, which only works usefully with reference return values. These two properties don't necessarily hold directly for more complicated types, but some variant of them usually will/is usually desirable. There are ways we could customise this (see `IndexMove` etc.), but having a simple uniform meaning for everything has its benefits too.
Hear hear! I, too love Cow (and have [blogged](https://llogiq.github.io/2015/07/09/cow.html) about it in the past) and think it is criminally underused.
I've updated this site to the new 2016 most loved languages, http://lovedjobs.com/ – It highlights where in the US you can find a job in the languages. It was my first app in php. 
This is the main point, to me. We just can't express a trait that allows for both a return value tied to self and not tied to self at the same time. It has to be one or the other. Whether or not `&amp;foo[bar]` borrows `foo` for the statement or longer shouldn't be dependent on how a trait is implemented. Especially given that `Index` can be used generically.
When I used to have C++ questions I would always end up on SO, because it seemed like a great plcae to ask. With rust, that's less the case. 99% of the time my question is handled within IRC. The other 1% of the time, I ask here, and that's it. I've never gone to SO for rust, it seems like the last place I'd check.
But the compiler does have information about the shape passed in. Since this is a function with generic parameters, it's monomorphized (specialized) during compilation for each use of it for a specific type. On the other hand, if you had: fn print_area(shape: &amp;HasArea) { ... } Then this would result in a runtime virtual call. Rust doesn't keep the vtable as a member of the object like C++, but rather uses fat pointers, which bundle the struct pointer and vtable pointer together as a single value. When you convert a `&amp;Circle` to a `&amp;HasArea`, the compiler will emit code to add `Circle`'s vtable pointer to the fat pointer.
Thanks, this is the information I want. Is there something I can read (other than the source code) to get these kinds of details, or are these details still considered volatile in Rust? &gt; But the compiler does have information about the shape passed in. Since this is a function with generic parameters, it's monomorphized (specialized) during compilation for each use of it for a specific type. Wouldn't this emit an unnecessarily large amount of binary data for something like `fn foobar&lt;T: Drop&gt;(shape: T)`? What if this function has external linkage, then how would it be monomorphized for external use cases? This doesn't make much sense. &gt; Then this would result in a runtime virtual call. Rust doesn't keep the vtable as a member of the object like C++, but rather uses fat pointers, which bundle the struct pointer and vtable pointer together as a single value. This makes more sense, but does it pass the vtable pointer for _every_ reference/borrow, or will it only pass a structure pointer if the compiler needs it (ie. virtual calls are present)?
&gt; Is there something I can read (other than the source code) to get these kinds of details, or are these details still considered volatile in Rust? FWIW, the book talks about monomorphization when you use trait bounds vs vtables when you use triat objects.
Is there a reason why this needs `;`? can't it just as well be `let bar: Baz,`?
In my eyes, you just gave a good reason to disallow using Index for a Matrix. It's very, very unclear as to whether Index would return a row or a column. Why not have row and column functions?
I think it's so that field and method declarations can be put in the same `trait` block without making people's heads explode.
&gt; What if this function has external linkage, then how would it be monomorphized for external use cases? Rust includes serialized ASTs in library binary for generic functions so that it can be monomorphized when linked. This is like how C++ template functions can be monomorphized because they are in the header.
&gt; What if this function has external linkage, then how would it be monomorphized for external use cases? Rust doesn't have "external linkage", instead, all items from `extern crate` have all information necessary for all possible usecases, stored in the metadata of that crate. That includes the AST (or soon, MIR) of every generic function (and functions marked with `#[inline]`). &gt; but does it pass the vtable pointer for every reference/borrow What could you even do without the vtable? There is no meaningful way of using the data pointer by itself, other than casting it to a raw pointer. And LLVM removes unused arguments anyway - it does help that we pass fat pointers as two arguments at that level, they get a bit easier to optimize.
It has the minor problem that the Rust results are mostly for mechanics.
http://contain-rs.github.io/bit-vec/src/bit_vec/lib.rs.html#169-170
Ah, alright. That makes more sense (although, still this can produce a lot of extra data -- thankfully Rust doesn't do this for passed references).
This is a reddit about rust programming language, not the game. :)
Tell that to Jonathan Blow 😉. Though, I'll point out that a number (though not necessarily all) of the solutions you cited have significant overhead.
I just put in automated data cleansing; I was scrubbing. The results should stay relevant from now on. There are not a lot of Rust jobs out there. You can also see Rust results listed on http://paulsizemore.com/city-tech-stack/ – a matrix crossing US Cities with programming languages. It looks like Fort Worth, TX has a lot of Rust jobs right now. 
Thanks for the info. I was more interested in how the flags are actually implemented (ie. the bit pattern itself) and what to avoid if I ever needed to mutate a structure as binary rather than its fields. I might work with some C libraries that do their share of hacks, like casting one structure to another: larger_struct foo = *(larger_struct*) &amp;base_struct; Could this possibly interfere with the way drop flags currently work in rust if I tried to write equivalent code? What if for some reason I wanted to cast some arbitrary memory to a structure in Rust, does it interfere with this 'bit pattern' for drop flags?
You can beat around the bush making vague guesses, but fundamentally it means: - There were less people on SO interested in rust than a year ago. Which probably means: - There were less people interested in rust than a year ago. Anecdotally, that seems pretty spot on to me; It seems to me there was huge excitement and interest in the 1.0 release; it was somewhat messy with the situation of stability and nightly, learning the language is quite difficult and there have been relatively few big-name win stories with it, and interest waned. There are still lots of people using it and good progress happening, but that -5% is probably indicative of, broadly, a surge in interest, followed by a drop in interest, for whatever reason. It's not really surprising to see this sort of pattern. It happened with go too, as I recall. I doubt very much it is to do with IRC, reddit and users.rust-lang.org being 'better channels'. It's clear from sitting on IRC that the number of people actively seeking help and looking into the language is much lower than a year ago, and rust is no different to other language in terms of channel other than SO being available to find help on.
If you're looking for some of the more obscure implementation details of rust, there's always the Rustonomicon! https://doc.rust-lang.org/nightly/nomicon/
Just stop using references! (No, just kidding, but most of us were probably tempted at one point:)
Note: I am NOT the author of this.. I am just sharing this project which I think is really cool. The real author is Geordon Worley, his repo is in the description of the video link. 
My [unleakable](https://github.com/Sgeo/unleakable) crate, which I was planning on releasing this week, turned out to not actually provide the guarantees I wanted to promise. But I learned some stuff about lifetimes in the process, so going to try to resurrect an old API idea I had for [take_mut](https://github.com/Sgeo/take_mut).
Without a repr tag, Rust makes no guarantee about the layout of structs (in general; this is not only limited to drop flags). You can't rely on mutating a struct as binary to work, and while you technically can do it with some hacks, you really shouldn't.
I've read most of what I've been interested in that (particularly the drop flag section). This part needs to be elaborated for other readers: &gt; As of Rust 1.0, the drop flags are actually not-so-secretly stashed in a hidden field of any type that implements Drop. Rust sets the drop flag by overwriting the entire value with a particular bit pattern. This actually suggests that programmers might interfere with with Rust's drop mechanics via certain unsafe operations on members/variables - zeroing/nulled structures, casting structures to other structures with larger counterparts, treating structures as arbitrary data -- because they could actually change the value of the drop flag that is somehow dependent on this 'particular bit pattern'. In practice, this 'drop pattern' actually ends up being zeroed memory - which absolutely could (and should be) a valid state for any structure before it has actually been dropped. From what I've seen so far, this also means Rust will **insert extra padding to ensure there is inaccessible data that it can zero for drop flags** (WTF??): struct Foo { x: u32, y: u32 } impl Drop for Foo { fn drop(&amp;mut self) {} } fn main() { let size: usize = std::mem::size_of::&lt;Foo&gt;(); println!("size: {}", size); } Remove the `Drop` implementation and you see the structure shrinks by four bytes (implementing traits other than `Drop` expectedly has no effect on the structure size). Apparently these drop mechanics [changed before the 1.0 release](http://internals.rust-lang.org/t/attention-hackers-filling-drop/1715) from a "zeroing" drop to a "filled" drop. The same problems still occur with this implementation, and it still relies on structure padding. These drop mechanics actually interfere with some use cases and are unintuitive. Not only that, but there's an extremely unnecessary amount of overhead with zeroing/filling the entire structure and comparing the entire structure for drop flags. I can't say I'm as impressed as I was with this language.
The documentation seems to claim otherwise about the layout of structs and describes padding just like in C -- and there are FFI examples of Rust structures being analogous to C structures. Either way, my previous comment/discovery about drop flags has left me with a bad impression of Rust so far.
Do any of you have a good idea for a beginner's project that is actually useful? I've completely run out of ideas (I used them all on other programming languages that are more suited to them). I guess I want something that takes advantage of Rust. Thanks for reading and thanks in advance for any ideas! :)
Most hits have a misspelling of "Trust" as "T rust" due to C&amp;P-Errors and those pesky automatic line breaks.
That's nothing. Forgetting that Python doesn't have implicit "last expression" returns, but *will* return `None` if you don't give it anything else... *that's* a pain.
my first instant response without seeing anything else is "don't name it MatrixView" go with "MatrixSlice" since it matches the array vs slice concept so well. A standard, even if it is a bad one, is useful (and I'm not sure 'slice' is a bad standard concept).
&gt; I'm hoping it's still ok for me to ask for a review here. I've done a fair amount of coding in Rust but I'm still having trouble with a few things. Definitely! It is even encouraged and we want to make sure that your review experience is a pleasant one. FWIW, I would say Rust has a "Review culture"
It would match neither the struct syntax, nor the usual let declaration syntax let. 
I'm not sure _why_ these Drop flags are needed, but it seems they are needed at this point. So your struct has some additional field. This happens in Rust in a some instances. Another example are enums aka "tagged unions", where the actual struct size is the "tag" plus the largest struct in the union plus padding (?). If you want to "mutate a structure as binary rather than its fields", use #[repr(C)]. That is the only way that the ordering of the fields of the struct is guaranteed. (unless you want to shoot yourself in the foot). Note that implementing the "Drop" Trait does *NOT* screw with the struct silently. You get a fat warning: 9:1: 13:2 warning: implementing Drop adds hidden state to types, possibly conflicting with `#[repr(C)]`, #[warn(drop_with_repr_extern)] on by default 4:1: 7:2 note: the `#[repr(C)]` attribute is attached here example: http://is.gd/7GLKS0 So bottom line: Rust is not magic. It's just a very good thought-through language that strives to do the best it can :)
&gt; Satisfying both of these requirements with a single trait you mean a single trait method. Index could well have been: pub trait Index&lt;Idx&gt; where Idx: ?Sized { type Output: ?Sized; fn index_ref(&amp;self, index: Idx) -&gt; &amp;Self::Output; fn index_val(&amp;self, index: Idx) -&gt; Self::Output { *self.index(idx) } } (but that would make no sense)
I am a C# developer by day and I have been leaving parenthesis off many statements/expressions at work. I've also been labelling variables entirely the Rust way... as in: `public void my_method_name(some_argument: String)` instead of `public void MyMethodName(string someArgument)`. I literally spend all of my non-work time looking at or writing Rust so there really is no escaping for me. I am normally pretty good at context switching so I can't explain why Rust seems to be sticking :/
it would obviously return columns and rows depending on what you pass: given the same syntactic sugar as python, i.e. that `foo[a, b]` desugars to `foo[(a, b)]`: mat[1, 2] //element in row 1, col 2 mat[1..3, 2] //column view containing 2 elements of column 2 mat[1, ..5] //row view containing the first 4 elements of row 1 mat[1.., ..] //matrix slice view containing everything but row 0
If you do any kind of FFI in Rust, you need to be aware of what can be passed through the FFI. Not every data structure in Rust can be compatible with C. (that would limit Rust too much). Here's the relevant chapter, especially the "Interoperability with foreign code": https://doc.rust-lang.org/book/ffi.html#interoperability-with-foreign-code Bottom line: Only pass #[repr(C)] structs, or pointers/references to it, Box and Option&lt;Box&gt; and function pointers operating on those types (and Option of such functions if you need a nullable pointer). Edit: Oh and primitive types such as i32 are ok, too :) DO NOT pass Traits, enums, structs without repr(C) etc. through an FFI. They don't have a guaranteed memory layout and C code usually cannot represent them easily. Unless, of course, you know exactly what the FFI is doing :)
The operators like `Add&lt;MatrixView&lt;T&gt;&gt;` for `Matrix&lt;T&gt;` should be implemented to reuse existing memory. It can also work that way with `Sub` or even `Mul` for square matrices. I would probably implement `Matrix&lt;T&gt; op MatrixView&lt;T&gt;` as a base case and let the others implementation reuse that impl either by `&amp;` or `.clone()`/`into_matrix()`. And regarding the code redundancy, using a macro to generate impls is quite common practice, I think. You can also simplify the `MatrixView` to just: pub struct MatrixView&lt;'a, T: 'a&gt; { original_cols: usize, rows: usize, cols: usize, data: &amp;[T], } (The `data` can be created from original `Matrix` by slicing so it already contains information from the `windows_corner`). This approach has one pointer-indirection less which could be faster and could make life easier life for the optimizer. But it's up to you – if it's not faster and is less readable – don't go with this approach.
There's ongoing work to move the drop flags off to stack from the structs. However, it's blocked by the work on MIR. I think that /u/eddyb (...or someone else working on the compiler insides?) estimated recently, that there's still some weeks after the MIR is in a workable condition and after that, some months to go before we start to see the work bearing fruit in related areas. Maybe give it half a year and then try Rust again? Also, there's an active osdev community around Rust that is sure to nag about (and contribute work for) the lower level things – including this. These things are definitely going to improve over time.
Bit of an overstatement, but Serde and Diesel both use them and they are quite important when you're building a web API. :) Sounds like I should just use Syntex, which is fine I guess. Thanks!
rustc-serialize is not too bad if you don't want to use serde. Yeah, with diesel syntex is the best option. 
I kept the crates same as the last time, but I welcome suggestions. The primary limitation is that it must be possible to build the crate with fairly old versions of rustc in order to track rustc performance.
Serde doesn't need them, but sadly, the usage without is badly documented.
It is very much. :)
I think you're in the wrong subreddit.
Have you tried adding `metal-sheet-feet-wall = "*"` to your manifest? (Try /r/playrust) 
That really common practice is actually invalid when dealing with anything other than individual bytes due to endianness. If you have a bunch of bytes, you have a bunch of bytes and you have to read at specific offsets with the right endianness (see the `byteorder` crate for this - or `lense` for an approach where you can write a `struct` definition for a similar purpose). On such a struct you *really* wouldn't want a destructor, even when the endianness problem doesn't apply. What is that destructor even supposed to do on a POD? &gt; I can't even safely operate on uninitialized data Neither can you in C or C++. Working with uninitialized data can easily turn into UB. In Rust, a type with a destructor has extra rules *even when completely ignoring drop flags*, specifically to do with unwinding in unsafe code: if you call an arbitrary user-provided function, they may panic and your destructors must not cause memory unsafety under any circumstances, *or at least* not trigger at all (if you can't do anything less significant). This is very similar to exception safety in C++, although you don't have to deal with copy/move constructors/operators potentially unwinding (an explicit `Clone::clone` call could still panic, though).
You should be using `const` though, where possible ;)
The real amazing thing is that development was done in the open with many iterations on the design before they settled on what we have now. Many things will come to Rust in time that make it even more power- and useful. The only reason we don't have them yet is because no one got around to implement them.
I wasn't super keen on the name I came up with. `MatrixSlice` is definitely better - and I agree that standards are important!
Thanks for the help! I like your suggestion to simplify `MatrixView`. This would also let me create a more efficient iterator as I could consume the view (`into_iter`) and mutate the data by truncating it. I agree it is a little less readable but I don't intend for the user to directly interact with the view right now. I didn't really follow what you meant by the changes to the operators. It seems to me that it would be more efficient to implement view vs. view and cast `Matrix` to a *redundant* `MatrixView`by just having the `original_rows` and `original_cols`be the same as the the view `cols` and `rows`. Am I misunderstanding your suggestion?
In the toying around with Rust that I have already done, avoiding `Drop` is what I've been doing to get predictable behavior and memory layout from my structures -- I do wish I could work with types that implement `Drop` the same way, though: use std::mem; struct Foo { x: i32, y: i32 } impl Drop for Foo { fn drop(&amp;mut self) {} } fn main() { unsafe { let mut foo: Foo = mem::uninitialized(); /* should compile to nothing except the stack pointer moving by sizeof(Foo) */ /* ...do things with 'foo', lazily initializing it */ /* at the end of this scope (since 'foo' hasn't been moved), it will try to drop the value */ /* the program will likely crash at this point since foo's drop flag data is still garbage */ } } This isn't exactly an edge case either (`Vec` does this), it's quite practical for large structures instead of initializing with static data every time you build a new instance. Doing this with values that have a destructor would be be equally as practical. Unless there's some version of `mem::uninitialized()` that will emit another instruction to initialize the piece of memory relevant to the drop flag (or is that all the memory? I still don't know because of the lack of documentation), this is a pretty common scenario that would affect more than just me.
&gt; Is that ever going to change, Yes. &gt; and if so, when can we expect that to happen? Unclear. It's being actively worked on, but software estimates are notoriously difficult.
It's a bit more subtle than that. A lot of Rust's safety features are good for maintainability, change over time. But a lot of games ship once and then never get used again. That's really at the core of it. I'm not sure I agree, but then again, I'm not a world-famous game developer.
/u/minno is right, though `@T` was never _truly_ garbage collected, it was just refcounted + a simple cycle detector. The idea was to make them truly GC'd at some point, but that didn't happen.
&gt; What does &lt;T as HasPi&gt; mean? This is covered in https://doc.rust-lang.org/book/ufcs.html &gt; what does the Self mean? It's the type of `self` in a method, ie, the type that's implementing the trait.
Does anyone know what gave the TOML crate such a speed boost when type checking that hurt the compile time of all the other crates?
You may have the wrong subreddit, but it sounds like you, too, are interested in both safety and ambitious projects, so why don't you stick around?