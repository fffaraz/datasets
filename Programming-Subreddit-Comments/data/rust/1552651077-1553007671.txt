&gt; This proves my previous point. You might use that to argue that the library doesn't have great documentation, yet I googled "petgraph bread first search" and got that as the first link, so I don't know how much weight that argument has. At the same time, it is also an argument about the level of effort that you invested into learning the library (you didn't bother to google the obvious thing, you didn't bother to invest one minute into asking a question, etc.). &gt; A library user shouldn't have to file issues and ask the tired devs who don't answer anyway in order to understand how to use it. It should be clear and simple. Not asking questions because everything should be self explanatory is not a very good way to learn anything. Particularly because asking questions is a fundamental part of the learning process, and the real world isn't perfect, which is why most APIs aren't really self-explanatory. 
Hello, I'm trying to use the `?` operator in a thread fn some_function() -&gt; Result&lt;(), io::Error&gt; { let handler = thread::spawn(move || { for path in &amp;paths { let string = get_string_from_file(path)?; println!("{:?}", string); } }); handler.join(); Ok(()) } But I get the follwoing error: error[E0277]: the `?` operator can only be used in a function that returns `Result` or `Option` (or another type that implements `std::ops::Try`) --&gt; src/main.rs:79:27 | 79 | let string = get_string_from_file(path)?; | ^^^^^^^^^^^^^^^^^^^^^^^^^^^ cannot use the `?` operator in a function that returns `()` | = help: the trait `std::ops::Try` is not implemented for `()` = note: required by `std::ops::Try::from_error` When not using threads it worked, so I'm guessing it has something to do with the thread/move? Can someone explain, or point me in the right direction?
https://github.com/flier/rust-t1ha
I think you're looking at the wrong sub-reddit. /r/playrust is what you want 
They've been working on a Switch version, but the compile times are brutal, and the lack of a keyboard or IDE is problematic.
&gt;Not asking questions because everything should be self explanatory is a weird path to follow through life. Asking questions is a fundamental part of the learning process. Only if I had the time to learn every poorly designed api. &gt;At the same time, it is also an argument about the level of effort that you invested into learning the library (you didn't bother to google the obvious thing, you didn't bother to invest one minute into asking a question, etc.). I believe I mentioned that my problem **isn't** bfs traversal. &amp;#x200B; Look, I know you are trying to do the right thing defending petgraph. I am just a simple man who wrote a library and shared it because I was frustrated with petgraph and because I believe the community should not encourage lazy and poor design under the pretext that "People should invest more time in learning a tool". I chose Rust to make things happen, not to argue with the documentation for 6 days. &amp;#x200B;
f
You are claiming that petgraph is poorly designed. That claim has zero weight coming from somebody that didn't bother to google what they were looking for, not couldn't invest one minute in asking a question. Do you understand why petgraph is designed the way it is? If the answer is no, how can you claim that its design is bad ?
In addition to hashing, you could look into "last edited" property of files that most filesystems have. For example, Cargo and Make work that way (compare "last edited" between source files and output files to determine if needs to recompile)
Thanks to all for the suggestions 
There is none yet
Yes, I do, but it is irrelevant. The design is bad because it is hard to use. You are free to use petgraph if you think its design better or it suits you more. I am using this for my project.
Thanks, i'll give it a go!
Building a compiler from the source code is not as simple as saying `./x.py build`. There are lots of config options to choose from, and subtle problems with build- or host- systems that have to be solved. It is *much* easier to take an approach that is already guaranteed to work (otherwise those binaries wouldn't have been there), and then adjust it for your liking (or until you realize that your requirements can't be fulfilled), than doing all from scratch only to find that some minor adjustment you thought useful actually breaks your compiler (in theory, this shouldn't happen; but it does happen a lot in practice, when your target is as poorly-supported as MinGW).
You should check if the flash works correctly by writing test patterns and verifying them. There is no guarantee, that ST ever tested the upper 64k of the flash.
See: https://github.com/rust-lang/rust/issues/10184
Thanks for the link, that helps explain it a bit.
That's great to hear! Glad you enjoy it :\]
Should be. The surprising part here is really that the 53kb reduction before compression turns into literally nothing with compression.
https://github.com/bodil/meowhash-rs
Sure. You should be able to find out how the official binaries are built from those docs. (And, even then, it still should *just work*. MinGW is still a tier 1 platform, after all.)
Seeing as you are clearly limited with the maximum value of u8, can you give a reason behind this? I can't think of a use case .
This came up while porting some C++ code and tests over to Rust (i.e. the \`360.0f32 as u8\` occurred only in a test, whereas the 1st and 2nd examples cropped up as return values from functions).
It's interesting that the first value is wrapping around (360-256=104), and that the third value is saturating. I wonder what precisely causes the second one's garbage though, even if it's undefined behavior.
https://mollyrocket.com/meowhash here is a crate; https://crates.io/crates/meowhash
aHash: https://github.com/tkaitchuck/ahash
So something was accidentally stabilized, and it is unknown whether that something might need to be removed in the future. I kind of expect a warning to land the day after such things are discovered. 
If these files come from untrusted sources, you definitely need a cryptographic hash.
It's on the cards. The AST hasn't quite stabilized yet, and we're hoping to abstract more operations on them. We'd need to sort out of an API that is both powerful enough and provides some guarantee of correctness.
When it's done and ready, I'm almost certain it will be posted to the GitHub issue.
fair point
Also on --release it always outputs 0, 0 and 255.
I would take a wild guess that the third value is converted at compile time, and the the first at runtime?
Nice! Here are some thoughts: &amp;#x200B; \- please add a license. \- you could use rayon with `process_multiple` to speed it up. \- did you have a look at other image filters written in Rust? &amp;#x200B;
Hey! I'm a DevOps person who writes some Rust code on the side! I am more than happy to get your pipeline working!!
- Will do! - I'll take a look at that later - I did not -- I just referenced the Sobel operator Wikipedia page as well as the image crate's documentation 
https://github.com/ferrous-systems/flamegraph/issues/1 :D
Who is thestinger and why are they no longer involved in Rust ? Looks like they were doing a lot of stuff pre 1.0.
This is fixed in master, thanks for the report. It's been a real pleasure working with Marcus on this - he's done some real heavy lifting, and realized the improvements I had when I started the new_algo branch.
And it'd be surprising if it wasn't posted here too â€” and upvoted a lot â€” very quickly.
It can not be the final "n" because that would be abbreviated f6n and not fn.
gtfo
:)
its Work 
Formerly strcat. He was impossible to work with. https://slash-r-slash-rust.github.io/archived/2u1dme.html
It is long, and very complicated. I don't think getting into the details is particularly useful.
&gt; A static string is almost always one that's compiled into the binary itself, which could be a decent guarantee that it doesn't come from malicious user input. (`lazy_static!` complicates this a little.) There's a much easier way to circumvent this restriction using [`Box::leak`](https://doc.rust-lang.org/std/boxed/struct.Box.html#method.leak) fn requires_static_str(a: &amp;'static str) { println!("{}", a); } fn main() { let a = Box::leak(format!("Static string with number {}", 4).into_boxed_str()); requires_static_str(a); } [(Playground)](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a5fd831b2479762b9007dbabeb7a2290)
In Rust, "safe" has a specific meaning. The answer to that is always "yes". But I don't think that's the question you're actually trying to get at. I think the question you're asking is "can this unwrap panic", and as far as I know, the answer is "no" in this specific situation. &amp;#x200B;
Ow wow that's kind of horrifying :-D
dupe of https://www.reddit.com/r/rust/comments/azw2vh/introducing_nowrust/
I have read the thread and I was surprised at the amount of personal attacks. Can you stop it please?
Having checked out the file sizes for my app, you are almost certainly measuring the size of a React debug build. Which is a *lot* bigger than the production verison. I'm including React in my web app (https://bettermeet.app - you'll need to login with meetup to view it). My vendor.js file with React, Redux, and React-Router is 60kb transferred (195kb uncompressed). My app.js (containing my non-library app code) is an extra 9kb transferred (37kb uncompressed) on top of that.
Should be possible with imgui. https://github.com/Gekkio/imgui-rs , https://github.com/ocornut/imgui/issues/306 . Haven't used it before, but looks good.
We could really do with one of these toolchain managers that works for every language.
Benchmarks? I guess it's ahead of Postgres and MySQL? &gt; (Mostly) Open source &gt; Only a very minor part of RediSQL is not releases as open source This could mean anything. I assume only some features are paid? Then that's what it should say. If the basic db is fully open source, you can call it that.
Really curious about the upcoming optimization, /u/rabidferret. Your talk about optimizing crates.io was interesting.
This is just wrong. See glandium's comment and stop spreading disinformation, thanks.
\&gt; Can you stop it please? This is an archived post... From 2015.
No. This is instant UB.
Hell, copy pasting into a Google doc may catch a bunch of these as well
His sentence itself wasn't neutral.
A thread continuously acquiring a lock is going to starve other threads unless you use a completely fair lock, which isn't available in std. (I'm pretty sure std uses an "occasionally fair" lock so the other thread won't starve *indefinitely*, but your results aren't surprising for a toy application.)
Benchmark for databases are quite hard to do and as engineer I am a little reluctant to publicaly say to much. Anyhow given a simple table for 3 integers "create table foo(a int, b int, c int);" we can insert on that table at 130kHz using redis-bachmark. So 130000 insertions for second. Again it depends on the hardware and thousands of other details.
Thanks for you insights, so my approach is correct and I just have to another lock? 
The playground server local clock is probably set to UTC, which is a common and preferred setup for servers. What's the problem?
Cross posted to [StackOverflow](https://stackoverflow.com/questions/55187397/how-to-merge-two-elements-of-a-list-in-rust).
Not important, and possibly even a disservice to users. I've seen people on forums try to use older Rust versions, and it's such a massive pain. Endless problems with a dependency of a dependency of a dependency accidentally using some newer feature. Until Cargo adds support for minimal rust version and includes that in dependency resolution, use of old Rust is just absolutely impractical. So suggesting to users that an old version of Rust is a valid target may even be harmful. If they believe it and try to use something ancient like Rust 1.17 they'll be horrified how broken and unusable everything is. 
Thanks for the recommendations. Programming Rust was released in 2015, would you say its still worth reading with all the updates to Rust since?
I'm in pretty much the same situation.
Thank you. I've written my own library for similar reasons. Heavily optimized for certain data and operations, petgraph's O(n) removal being the main reason.
If this is undefined behavior, it's a bit surprising to me that there isn't a lint for it. 
Interesting. It's messed up because UB, but looking at the assembly for the debug build is interesting. (I'm not a fan of the AT&amp;T syntax, but it's what the playground generates.) The first one is a relatively straightforward compilation. movl $1135869952, 116(%rsp) cvttss2si 116(%rsp), %eax movb %al, %cl movb %cl, 199(%rsp) The instruction `cvttss2si`, in Rust terms, casts `f32` to `i32`. (And if the rounded variable can't be represented in `i32` the most negative value is produced.) The least-significant byte is taken, some horribly round-about unoptimized assembly duplicates it to several stack variables and eventually starts calling the functions which implement `println!` formatting and output. So it's essentially `360.0f32 as i32 as u8`. In the second one, it seems the compiler interprets `360.0f32 as u8` as undefined, roughly equivalent to `mem::uninitialized::&lt;u8&gt;()`. The assembly takes whatever value remains in `al` after calling `_print` and just uses that. Again, there's a lot of faffing before the calls, but that's the value which is taken as belonging to `a`. In the third one, leaq .L__unnamed_4(%rip), %rax a constant value stored at `.L__unnamed_4` is loaded at used as the value of `a`. The assembly shows .L__unnamed_4: .byte 255 In short you're seeing three things: - a reasonable implementation of the cast which produces garbage when the value overflows the destination - a strange implementation which interprets some garbage lying around in a register as the value of `a` - a reasonably compiled result of a constant expression (saturating conversion) which doesn't agree with the other two. 
Why wouldn't you use cryptographic function? If it's for backup you probably want to use something that can't accidentally have collisions. Just use `blake2b`.
Two problems: - `handler.join();` means you're throwing away whatever the thread returns. So you won't detect an error. - Since `handler.join()` can return anything, the type parameter `T` of `JoinHandle&lt;T&gt;` and `spawn&lt;F, T&gt;` isn't constrained. The compiler can't figure out what type you want to return from the closure, so it's defaulting to `()` and giving you an error. You can solve the second problem with explicit typing `let handler: JoinHandle&lt;Result&lt;(), io::Error&gt;` but that still leaves the first one as a logic bug. Or you can put `handler.join()` in return position, which will fix the first bug (passing the error to your caller) and give the compiler enough information to infer `T = Result&lt;(), io::Error&gt;`.
RFCs are usually for features, not implementation. This sort of redesign work has been going on for quite a while. This is a huge step though.
I mean, it depends on what you are trying to achieve. The OS can always decide to schedule your threads serially unless one of them actually blocks on the other, which these don't do because they don't need to.
That sounds correct to me.
I know it's not as streamlined simply because different languages have different build systems and didn't support all versions of a given language without configuration, Nix can probably do the trick. There are 4 versions of node in the base packages, and with a little scripting you can pretty much point it at whatever source tree you want. While there's only one rust version in the base system, nixpkgs-mozilla it's capable of installing any version that rustup can. Pinning a given toolchain version is done with a single shell.nix. I'm vastly oversimplifying and I feel bad for constantly advertising Nix, but it does seem like it does much of what you ask for.
Depending on file size, you might want to look into the incremental approach that `dupe-krill` [uses](https://github.com/kornelski/dupe-krill/blob/master/README.md#the-method). As always: benchmark, benchmark, benchmark!
&gt; (I'm not a fan of the AT&amp;T syntax, but it's what the playground generates.) You can change this in the playground's Config menu.
/r/playrust. Also cheating is bad and you should feel bad.
Yes. And while the Haskell code is faster than the Rust code working with UTF-16, the Rust code is even faster than the Haskell code if converted to UTF-8 first, as [you can see in this chart](https://tech.channable.com/posts/2019-03-13-how-we-made-haskell-search-strings-as-fast-as-rust.html#results). &amp;#x200B;
Would Nix give you the ability to have multiple versions simultaneously installed that can be easily switched between for different projects using different versions? IMO that's the key feature that these sort of solutions provide.
I agree with the sentiment but I'd write it as "C++ lets me do things that are probably correct, but might not be."
&gt; it claims I'm a bit skeptical of the claims and there is no prototype that one can try out, so until that changes, it kind of has a bit of a vaporware feeling to me. 
You might want to make a demo page for it using rust-wasm that would ask a user for an image and then paint processed image on a canvas. See [https://rustwasm.github.io/docs/book/](https://rustwasm.github.io/docs/book/) and [https://github.com/rustwasm/wasm-bindgen/tree/master/examples/canvas](https://github.com/rustwasm/wasm-bindgen/tree/master/examples/canvas).
Yep, I also have similar feeling because they didn't release any proof so we can believe it yet Hope they really achieve that though
Define "installed". Present in the user/system environment? No. Present on disk able to be used in builds? Absolutely!
V sounds too good to be true, so that's what I'm going to assume about it until proven otherwise.
Itâ€™s not spreading disinformation. Actually did not know that. Glad to know itâ€™s possibile.
Is it? There's been quite a few hashes released since Murmur appeared.
Looks F awesome. But also a bit too good to be true. I will keep learning Rust! :-)
After looking at the website and some of the other stuff it almost feels like a scam of some sort, but if it is this is a ridiculous amount of effort to put in. If this is legitimate that's pretty cool though.
4 examples, 6 FAQ question answers, nothing in the blog, a documentation shorter than most individual doc pages on rust.... "Does V use LLVM? No. V compiles directly to machine code. It's one of the main reasons it's so light and fast. Right now only x64 architecture is supported. V can also emit human readable C, which can then be compiled to run on any platform." lol.
Could you provide a reference for those of us without the time to watch a hour long video?
The problem is that the full version would be more like "C++ let's me do things that are correct under very specific limited circumstances I _think_ are applicable at this moment, and will continue to be for the immediate future, but might not be long term. Rust makes me either prove that those conditions are true at compile time, or else tell it that I'm doing something that can't be proven to be safe and any problems are totally my fault". If it's categorized as unsafe, there's some set if circumstances in which it isn't safe to do that thing. You can be fairly confident that those circumstances don't apply at the moment, but long term it's entirely too easy to accidentally introduce one of those edge cases and break an invariant you didn't even know was there.
Iâ€™ve used it. Depends on what you are trying to do. Some things are pretty straightforward. There are some obvious API differences compared to the canonical python interface. It is being actively developed, so that is a plus. 
Why doesn't this code work? ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e74af44c00ca13fcce08ed9b08f381e0)) use std::thread; fn main() { run_threads(|| println!("Hello!")); } pub fn run_threads&lt;F&gt;(entrypoint: F) where F: Fn() -&gt; (), F: Send + 'static, { for _i in 0..10 { thread::spawn(move || entrypoint()).join().unwrap(); } } The compiler gives me the following error: error[E0382]: use of moved value: `entrypoint` --&gt; src/main.rs:13:23 | 13 | thread::spawn(move || entrypoint()).join().unwrap(); | ^^^^^^^ ---------- use occurs due to use in closure | | | value moved into closure here, in previous iteration of loop | = note: move occurs because `entrypoint` has type `F`, which does not implement the `Copy` trait I found that I can make the error go away by [inlining `run_threads` into `main`](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7af21a7830c748fef76e7fcbb2af1e660), but I want to keep it as a separate function. I was also able to get rid of the error by [moving entrypoint into an `Arc`](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=8c050f6a619897b88961b79e284821da), but that seems like overkill, since it wasn't necessary when the code was all in `main`
Only until the N-th element. https://doc.rust-lang.org/src/core/iter/iterator.rs.html#315
I was introduced by this blog article [My Personal Complaints about Programming in Go](https://boyter.org/posts/my-personal-complaints-about-golang/) Hope it's not some scam, never heard of this kind of scam in tech world before
I'm not an expert in compilers but isn't LLVM a good thing? I would assume if it was easy to produce optimized machine code noone would bother. 
Was this the one that popped up a few weeks ago? (On r/programming I think...) In any case, I can't really take any language seriously that says it follows Rust's borrow checking style. It's just to genius and complicated to reimplement so quickly. Even all the geniuses at Mozilla and everyone in the Rust project working on it are struggling in figuring out how to upgrade it. It's state of the art, and we're still discovering things about it. I'd need to see some serious professors/academics to be able to rival it. A project from nowhere? No way. Also, if someone realizes Rust's strengths, I'm skeptical that they'd also be in love with Go's. There's a huge difference in philosophy between Rust and Go's type checking, and if you like Rust's borrow checker, how the hell would you like Go's type systemâ€½ Any sane person would just switch to Rust, and try to improve it to fix any shortcomings it has in comparison to Go, instead of making something entirely new. After all, Rust's async is almost done, and with it, Rust should surely be flying past all of Go's strengths, within a year or two. (Except for startup cost, maybe. ðŸ˜…)
I believe your last sentence is correct. Here's a hint from the error :) note: move occurs because `entrypoint` has type `F`, which does not implement the `Copy` trait
How about [this](https://github.com/search?q=synth+filename%3ACargo.toml&amp;type=Code)? If a project depends on the library, it will be listed in its Cargo.toml. This doesn't include transitive dependencies though.
Yeah, that seems fishy to me. Particularly given the compile time claims as well as the compiler size claims.
So does the for `x in self` run the filter automatically and only go into the loop when it returns a true?
`filter` returns a new iterator that's defined [like this](https://doc.rust-lang.org/src/core/iter/mod.rs.html#1519-1522). `nth` then runs on that new iterator.
Ahhhh. I get it. Thank you kindly!
Rust version of program in [this comparison here](https://vlang.io/compare) written by someone who just started learning the language. For example this fn next(cursor: &amp;mut Arc&lt;Mutex&lt;usize&gt;&gt;) -&gt; usize { let result: LockResult&lt;MutexGuard&lt;usize&gt;&gt; = cursor.lock(); let mut guard: MutexGuard&lt;usize&gt; = result.unwrap(); let mut temp = guard.deref_mut(); *temp = *temp+1; return *temp; } Can be rewritten as fn next(cursor: &amp;Arc&lt;Mutex&lt;usize&gt;&gt;) -&gt; usize { let mut lock = cursor.lock().unwrap(); *lock += 1; *lock } Or even better fn next(cursor: &amp;AtomicUsize) -&gt; usize { cursor.fetch_add(1, Ordering::Relaxed) } I don't mention it doesn't even make sense to use `cursor` at all.
Seriously, a whole programming language just to make an IM/Gmail (not email, just Gmail) client.
You really find my guilt. I loved both Rust and Go and have some unsatisfied things in both language. With Go I admire its simplicity philosophy but unsatisfied with some of its features. But I got shits done quickly and efficiently with it. With Rust I admire its novelty feature with memory management, the advanced type system. But came from high level language I always feels like a newbie in Rust world. There are many discussions in this sub which I don't even know what they were talking about. Don't know a good way to get more advance into Rust yet.
(10 lines of code -&gt; 30 lines of assembly) -&gt; abstraction -&gt; (5 lines of code -&gt; 60 lines of assembly) &amp;#x200B; (10 lines of code -&gt; 30 lines of assembly) -&gt; zero cost abstraction -&gt; (5 lines of code -&gt; 30 lines of assembly)
So many people on reddit thinking it's a scam... I'll take it as a compliment :) Online playground will be available in two days, the first public release is in May.
&gt; Carefully tested for *correcteness* and tuned for performance Gotta appreciate the irony in typo'ing the word correctness
Yes, the rust example is terrible. If someone could spend a couple of minutes to improve it, that'd be great. I'd update it.
That would be the worst scam ever. Spending so much time on the website and the docs to earn $40 on Patreon :)
This holds true if the scope is limited to things like memory and thread safety. However the borrow checker wont save you from other categories of problems like business logic flaws for example. So in a way one could write a similar assertion that Rust let's me do things that are correct under limited circumstances. My original point is that C++ doesn't make me write more incorrect code than correct code. The balance is not as tipped as this leads one to believe, at least from my experience: &gt;C++ lets me do things that *might* be correct, but probably aren't...
The key question is how difficult it is to call into the other language from Rust. If you can't do it easily, then you might write a *framework* in Rust, but it's going to be unfeasibly bothersome to write the actual tests in Rust.
Broken link, first paragraph in "*a need for speed*": https://htts//crates.rs/indextree
&gt; V can translate your C/C++ code to human readable V code. --- &gt; Volt and V are the projects I've been working on for about a year. Starting from Feb 2018, I've been working on them full-time. Can't find much about the guy that seems to be behind this besides his [Volt](https://volt-app.com/about) about page. The [github](https://github.com/vlang-io/V/graphs/contributors) repo has 3 other contributors, that seem somewhat more real than the first guy. I'm waiting to see if it goes beyond April 1st.
I haven't, but it definitely looks interesting!
Awesome, thanks.
And it can translate C++ into human-readable V code? Wo0oaowAw!
I assumed that there was a trade off between cryptographic safety and speed, that's why. And thanks for your suggestion. 
[Previously.](https://news.ycombinator.com/item?id=19086712)
"V is for Vaporware!"
They're taking donation on Patreon so it's not real then it's a scam The volt app has demo binary but we should not log account with that unproven app yet
I'd argue C++ makes it harder to reason about the correctness of the code and therefore does lead to writing more incorrect code. For small snippets or small projects, and a sufficiently experienced developer the odds of introducing a bug may be so low to be ignorable, but as the codebase grows, and as less experienced developer are introduced, the odds of making not just mistakes, not subtle and difficult to find mistakes approaches 100%. The time it takes to reach that point is *significantly* shorter in the case of C++ than it is for Rust. In particular the flexibility and lack of consistent best practices in C++ makes it a veritable minefield of subtle (and some not so subtle) bugs.
The [examples here](https://vlang.io/compare) seem to be the clearest indication of what they're trying to do. Here's the V code: fn main() { resp := http.get('https://hacker-news.firebaseio.com/v0/topstories.json')? ids := json.decode([]int, resp.body)? // ? propagates the error mut cursor := 0 for _ in 0..8 { go fn() { for cursor &lt; ids.len { lock { // Without this lock block the program will not compile id := ids[cursor] cursor++ } url := 'https://hacker-news.firebaseio.com/v0/item/$id.json' resp := http.get(url)? story := json.decode(Story, resp.body)? println(story.title) } }() } runtime.wait() // Waits for all coroutines to finish } My scattered impressions: - "Rust + goroutines" seems like it could be cool and useful. My understanding is that that's what Rust _was_ back in the day, long before 1.0, so there's got to be room for such a thing. That said, the amount of effort the Golang team put into their goroutine-aware standard library is extraordinary -- like, how do you do async DNS when libc [only provides synchronous functions](https://medium.com/where-the-flamingcow-roams/asynchronous-name-resolution-in-c-268ff5df3081)? -- and it might be challenging for a small open source project to replicate that. - A built-in `lock` block seems like it could be convenient sometimes, but it runs against the grain of "lock data not code". IMO that piece of advice has been very successful since Alan Cox(?) came up with it (when?), and languages like Rust and V that offer mutexes-as-generic-containers support it really well. - `runtime.wait()` seems kind of sketchy to me. It assuming that none of the libraries you called started any long-lived background goroutines. It would be more robust to add some kind of `WaitGroup` type like Golang's.
I feel like the lack of transparency is a little offputting to most people since we're basically in the golden age of open source software right now
Thanks this helps to clear many things The author of V said that the solution for memory management is still not clear yet. it only works on simple cases for now Look like V still has very long road ahead and there is no guarantee they can keep the language simple when the memory management is fully implemented
Hah I really don't get why people are being quite so negative on this. I think it's too early for anyone to be looking at this and make judgements, until it's actually released. Good luck, cool project!
Thanks! Most people are actually very positive about it. It just got to HN front page again for the second time this month. On Reddit the discourse is very different for some reason.
Oh you're the author Hope you all the best on the project Do you have any date to publish it yet? 
I actually have a much better understanding now. Will post an update soon.
&gt; I'd argue C++ makes it harder to reason about the correctness of the code agreed &gt; as the codebase grows, and as less experienced developers are introduced, the odds of making not just mistakes, not subtle and difficult to find mistakes approaches agreed &gt; The time it takes to reach that point is significantly shorter in the case of C++ than it is for Rust agreed &gt; the flexibility and lack of consistent best practices in C++ agreed, this is the bane of large messy C++ projects. In this regard Rust does help.
runtime.wait is a temporary hack &gt; lock data not code yeah, this is a better approach. I'll think about the best way to implement this.
Yes. I also think the author is a little too confident in what optimizations the compiler might be able to do in the future, e.g. "and this [copy of an array mutated by a function] will later be optimized by the compiler of course".
Reddit is terrible, although /r/rust is better than usual. I wish you the best on this project! I think a lot of the choices are fine (not using LLVM *is* good for a "scratch" compiler optimized for compile speed). I also think it's very ambitious and it's likely that some of the things you promise will be much harder to deliver than you suspect. But even so, I think there's a lot to be learned from the effort, so I think it's worthwhile. I support Zig for similar reasons and am also looking forward to Jai. 
There may be a faster one out there, but not by a lot. That's impossible since murmur already doesn't so a whole lot, making it almost impossible to be much faster. That said, it sounds like the main road forward for you, if you need speed, is to hash less data. Can't you simply only hash a part of the file instead of the entire file?
June. Early access in May.
I hope I'm real :/
I think people're skeptical about a language the simplicity of Go and memory management model of Rust Also there is no proof on the language website yet The online playground will be a good start 
Fantastic!!! I think Rust has a huge hole in its profiling story. I called this out recently and even identified microprofile as something that could help fill the gap. Iâ€™ll be trying this out for sure.
Looks neat. Does it work with any browser or did you make it chrome-exclusive (since you keep saying to use chrome)?
It seems like this is in regards to debug builds specifically -- in another place on the page, it's stated: &gt; For now V emits C and uses GCC/Clang for optimized production builds. This way you get access to sophisticated optimization. &gt; &gt; Such builds are compiled â‰ˆ150 times slower than V development builds (but are still faster than C++ builds). 
If you were running a ponzi scheme, you would tell your investors what they want to hear, and when they ask how you hope to achieve it, you say "Trust me. We'll get there eventually." This language is promising a lot, and all the requests for details on how it will achieve these goals are met with "It's not done yet" and little else. That's why people are being skeptical of it. Even a single blog post that says "here's how we plan to do these things" would alleviate much of that. Of course, the blog is not implemented yet either.
I sure hope so, that'd be really great to see all promises fulfilled :) What can we expect from the first release, something stable and well defined? Reading what you wrote in different places it sounds like you are still figuring things out.
Didn't Amazon open-source something like this fairly recently?
I was reading another thread where the creator was talking up the language, and everyone was skeptical because you couldn't even try it. Same. Let me try it or GTFO.
If you're doing backups, disk IO or network IO is going to be your bottleneck, while blake2b can hash ~1GiB/s . If you parallelize it, there's no way it's going to be a problem. In `rdedup` I run a rolling sum, blake2, deflate, and encryption of the data, and multiple SSDs in btrfs pool are still the bottleneck.
&gt; And even when it is, of course it will be a prototype and not something that will immediately compete with Go/Rust. And yet that is not at all how the website is phrased, hence the skepticism.
&gt; It's not done yet Can you point to "all the requests" which I meet with "not done yet"?
is it V for vendetta, tho?
You can't mutate/use the car while others are observing it - unless you invoke internal mutability. I enjoy the book analogy. There exists one book and it may be transferred and read by many but it can only be written in with a pen that the owner retains but doesn't use (unless internal mutability) while others are reading it.
pretty sure I've seen clippy warnings about this
LLVM is a mixed bag. It doesn't support every platform, so using it constrains you a bit, it doesn't offer every optimization, so it may not be better than optimizing yourself, and it is somewhat complex and requires that you be able to understand it, so it isn't as open to newcomers who may not. It also has fairly regular-ish breaking changes, so you won't get new developments for free, and someone might prefer to just implement those directly in their own codebase, rather than trying to wrangle the whole LLVM platform.
You can't lend the car to someone else to drive (&amp;mut) while you're in the process of driving anyone else in it (&amp;)?
There isn't one for this specifically to my knowledge. There are [a few cast lints](https://rust-lang.github.io/rust-clippy/master/index.html#cast_), some of which will trigger for the code above (specifically, `cast_possible_truncation` and `cast_sign_loss`), but they aren't warning about the float-as-int UB. It is subtly different.
**Amendment:** I now have the following. A trait, the Associated type wasn't neccessary, and i got rid of the macros. Almost like I had it in the beginning and as someone suggested few days ago: pub trait Unfry { fn unfry&lt;T&gt;(&amp;self) -&gt; T where for&lt;'de&gt; T: Deserialize&lt;'de&gt;,; } impl Unfry for Bacon { fn unfry&lt;T&gt;(&amp;self) -&gt; T where for&lt;'de&gt; T: Deserialize&lt;'de&gt; { let mut decr_bytes: Vec&lt;u8&gt; = vec![]; for block in &amp;self.data { for byte in u128::to_le_bytes(*block).iter() { decr_bytes.push(*byte); } } bincode::deserialize(&amp;decr_bytes).unwrap() // not so nice. } } That works: let p = bacon.unfry::&lt;Person&gt;(); and I know that this should work too. I have done it years ago once. Insteaf of \`\`\`Person\`\`\` I want that &amp;str put there. There were some crazy operators that made this happen: let ty_str = bacon.descr.unwrap().entry("Type".to_string()).or_default(); let p = bacon.unfry::&lt;%ty_str%&gt;(); // doesn't work, but it was similar. Any idea?
Yep. Just go to any project and run `nix-shell`, and it'll load an environment from shell.nix. You can also pass one as an argument. It even integrates with direnv.
Sorry to ask this, but any reason to not using serde?
Yes. But you could instead just look over this thread and the linked HN thread and respond to the people who are already expressing their doubts. Or you could write the blog post that I literally just requested.
Here's my take on it: https://gist.github.com/oconnor663/7a9035c4dcf0e364db10a07f428a3a59 use serde::Deserialize; use std::sync::Mutex; #[derive(Deserialize, Debug)] struct Story { by: String, id: i32, score: i32, time: i32, title: String, } fn main() { let story_ids: Vec&lt;u64&gt; = reqwest::get("https://hacker-news.firebaseio.com/v0/topstories.json") .unwrap() .json() .unwrap(); let cursor = Mutex::new(0); crossbeam_utils::thread::scope(|s| { for _ in 0..8 { s.spawn(|_| loop { let index = { let mut cursor_guard = cursor.lock().unwrap(); let index = *cursor_guard; if index &gt;= story_ids.len() { return; } *cursor_guard += 1; index }; let story_url = format!( "https://hacker-news.firebaseio.com/v0/item/{}.json", story_ids[index], ); let story: Story = reqwest::get(&amp;*story_url).unwrap().json().unwrap(); println!("{}", story.title); }); } }) .unwrap(); } This would be simpler using an `AtomicUsize` instead of a `Mutex`, but I think demonstrating what the `Mutex` looks like is a big part of the example, so I kept it in there. While I was writing this, I noticed that the V example reads `for cursor &lt; ids.len` without locking `cursor`. Is that a race against the write (which excludes other writes but not reads)? Is the V compiler expected to catch that?
Thanks for the suggestion. The C++ API makes a lot of sense to me; it has a lot of basic examples. The Rust example seem quite complex in comparison. Between that and the lack of [documentation](https://docs.rs/imgui/0.0.21/imgui/) I'm not really sure where to start.
When you inline `run_threads`, it knows that `entrypoint` is copyable because it doesn't capture any non-`Copy` values. However, that is lost in `run_threads` because `F` doesn't have the `Copy` bound. If you wanted to support non-`Copy` captures in `entrypoint` then `Arc` would be the correct way to go.
Iâ€™ve seen it run in Firefox, but have from day 1 decided to only support 1 browser. Iâ€™m just not very experienced with cross browser issues, plus I donâ€™t have a team to test that itâ€™s working - I think my time is better spent elsewhere. But of course if someone wants to make fixes for another browser, then pull requests are welcome :)
If I have a library crate with a separate tests folder, is there any reasonable way to define quickcheck::Arbitrary on my types in a way that won't require me to add quickcheck as a regular (non-dev) dependency? I can't implement Arbitrary in the tests module since the type definitions are considered to be in an external crate.
Way too many red flags for me to trust the claims right now...I'd love to see some actual executable programs (yes, I know about Volt), some non-trivial code, or deeper technical descriptions of some of the features. Once more stuff gets released, it's going to be easier to have a reasonable opinion about this language, but right now it's too hand-wavy and lacking in substance. For example, let's take a look what the [language comparison](https://vlang.io/compare) has to say about V: &gt; No runtime &gt; Fearless concurrency (no data race guarantee at compilation) Ok, no runtime...although example code uses something like goroutines (+ literally a library/module called "runtime"), so there has to be something that handles the execution model of these "lightweight threads" or whatever you want to call them. And some compile-time checking of data races (probably something similar to Rust). &gt; Open source release in mid 2019. Ok, sounds like they're just tidying up stuff for open sourcing, and the language is in a good shape (why would they make so many claims if they didn't have them implemented already). But then, look at [this comment](https://github.com/vlang-io/V/issues/18#issuecomment-473293311): &gt; I haven't fully figured out concurrency yet. It will be similar to Go. ...so concurrency hasn't been figured out, yet there won't be a runtime and the language will be open sourced in a couple of months. Doesn't sound like a very promising combination to me (of course, open source != production ready / feature-complete in any way). And it's not just the language...the author (based on available information I assume there's just one author) seems to have written several projects in V, including "cross-platform widget toolkit" (not trivial at all to do well), and "C/C++ to V translator" with the following description: &gt; This tool supports the latest standard of notoriously complex C++ and allows full automatic conversion to human readable code. Supporting latest C++ standard and generating human readable code automatically from any C++ code? Yeah right, I'll believe when I see it. It would be invaluable to have a runnable binary or a playground where I could input arbitrary C++17 code and see the resulting "human readable" V code. Calling this a scam seems unfair (although money is involved through Patreon...), because I don't think the author has a malicious intent behind this project. But it feels like this is one of those "marketing and promises first, implementation later" kind of projects, which tend to overpromise and underdeliver (or never deliver at all). The internet is full of technical projects which have extraordinary claims, often centered around "simplicity". Everything is simple if you only solve a tiny toy example without actually having a deep understanding of the problem at hand (which is fine as long as you state truthfully what the project can and can't do).
Yes. Itâ€™s called hawk tracer and has a rust port. Itâ€™s pretty cool. But yeah this is cool too!
why are you loling at that?
so you are compiling to plan9 assembly and using the `go` backend to emit your code while performing absolutely zero optimizations just like go does? --- I am interested to learn how you plan to translate C++ to V. Parsing C++ is basically intractable at this point, and there are &lt;10 parsers in the world. It'll be dope if a new one comes out. Grouping C/C++ makes me a bit nervous as they have different syntax, and require different parsers. Which seems to imply you likely wrote a `C` parser (with limited) macro support, and enough flexibility to handle some C++03 oddities?
*serde vs bincode*: i didn't know serde does that in and from [u8]. bincode makes it easy. **my trait unfry** vs other the I have several levels of serialization and deserialization. The ```struct Bacon``` may be completetly be serialized or ony parts of it. For instance. This example is a partially fried ```struct Bacon``` where only the field data has been serialized (into [u8] and then encrypted into same-size blocks: Bacon { state: Fried, descr: Some( { "Cipher": "bacon::ciphers::chacha20::ChaCha20", "Tag": "15312753509570874580", "Type": "bacon::examples::Person" } ), data: [ 306788827833058366652492261066692497051, 223633534245159493758292083705409290958, 151814850034441911900588203175550408408 ] } The same bacon before a client sends the bacon over the net,i it will be completely serialized into [u8], on the server side, it gets deserialized into a ```struct Bacon```, but then I need finer grains settings to decrypted and then deserialze the data part, which can be of any struct too. Maybe a String or something more complex. 
Maybe a bicycle or motorbike with a caravan trailer. You can have any number of people in the caravan at once (&amp;), but only when it isn't being driven. It can only have one driver (&amp;mut), when there can be noone in the caravan, because that would be dangerous.
daaamn that's great!!!
I just want to commend you on the overall aim of this as well as bringing it to Rust. Hunting down performance when it's needed is always a terror. When tuning machine learning models on GPUs, NVIDIA's Visual Profiler kept me sane - the live and capture views you show are the exact type of feature that saved me from hellish and ineffective benchmarking! :) I'll try microprofile-rust for some Rust machine learning / NLP work and see how it goes! I love thinking of training batches in ML like frames in a computer game :P
If I'm not mistaken, your example is roughly equivalent to this: - https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015 - https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018 Note that the only difference is which Rust "edition" to use, and that is enough to make the latter work. The feature you're looking for is called "non-lexical lifetimes": https://doc.rust-lang.org/edition-guide/rust-2018/ownership-and-lifetimes/non-lexical-lifetimes.html
Probably something for the embedded working group.
For what you're describing you might want to consider writing something up yourself with nanovg-rs.
Great work, man! Nice
By the way, for extra fun, check this out: - https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=764b14028898dcba788a3220fe389130 Doesn't compile. Try commenting out the whole `Drop` implementation. Now it compiles. This is because "non-lexical lifetimes" don't actually affect when your variables are dropped, so if you manually implement `Drop` for a type, you won't be able to casually re-borrow like in your example because the compiler can no longer prove that you won't access the thing you borrowed.
I think the point is that LLVM itself is not very fast. If your goal is to build a fast compiler, picking something else and/or doing it yourself is a valid choice.
Thanks! Yes, this was fixed, and the compiler is expected to catch that. I just haven't updated the /compare page. Does it need `use reqwest;`? 
Y'know what? That's perfect.
Android Q has a new [SurfaceControl API](https://developer.android.com/reference/android/view/SurfaceControl). Do you think this does what we need?
Because it's poorly worded. You can emit machine code and claim that it emits "human readable C". The author is extremely shady and vague. Just look at r/programming and HN comments. Yes, I know that the author claims it emits machine code in debug builds and C code in release builds.
Have you tried `rust-lldb`? It adds a pretty printer for Rust to LLDB.
&gt; Also, if someone realizes Rust's strengths, I'm skeptical that they'd also be in love with Go's. There's a huge difference in philosophy between Rust and Go's type checking, and if you like Rust's borrow checker, how the hell would you like Go's type systemâ€½ "Also, if someone realizes C's strengths, I'm skeptical that they'd also be love with Python's." 
i donâ€™t know what you are on about
also some cars have this neat feature where if you take a picture of them you can make infinite of those cars(with keys) other cars require that you have the car though
Facebook's XXH3: https://fastcompression.blogspot.com/2019/03/presenting-xxh3.html
problem with that it hits all the packages named synth or compile to an executable named synth
Since you can't mutate the car while others are observing it, a better analogy would be something like a motorcycle (as mentioned by another redditor). Everyone can stand around and look at it (&amp;), admire the nice paint job, and measure the size of the wheels. But, as soon as it starts moving, people can't observe it. Your friend can ride it (&amp; mut), but others can't observe (&amp;) it while your friend is riding it.
&gt; Does it need `use reqwest;`? The 2018 edition changed how imports work, so that fully qualified paths don't require `use` or `extern crate`. You can also `use` macros now, which is cool.
Thanks so much for the detailed response and example! Makes sense now. Should I try to avoid doing this double mutable borrow even though it compiles ok?
Experienced programmer, looking for a good book to start into Rust. What book is up to date and a great place to start writing rust?
Can't help you with that.
To be clear, this is already in production, and has been for a week now
You should propose this to the Cargo team. Crates.io is managed by a separate team, and we won't be adding a flag to make this synchronous on the web server, as it would defeat the purpose of the change for us
i mean, the end of your post contradicts the start. rust could benefit from having a fast debug build (cranelift) and c backend desperately so its especially weird to pile onto a guy who is doing something rust ought to be.
Thanks. I already implemented \`NodeType(Node)\` wrappers. The problem is that grouping wrappers in a set of enums is a bad idea in my case for the following reasons: there are too many different possible classifications applicable to the same data type, and a lot of them overlap in their meaning. For example a syntax object can be at the same type an \`Element\` in \`Object|Element\` hierarchy, it is also \`GreaterElement\` in hierarchy of elements, there is also a notion whether it can contain Objects or not ( Object container can be some Elements or Recursive Objects )... Creating all possible permutations of these enums would also require creating adapters for translation from one enum to another, which works if I go from a smaller category to a bigger, but it does not work the other way. &amp;#x200B; In scala this problem is very nicely solved by \`sealed trait\`. I can have a case class that implements an arbitrary set of sealed traits and then I can use \`match\` to downcast any given trait to a specific implementation, and compiler will ensure that match arms are exhaustive. I wish rust had a similar feature &amp;#x200B; &amp;#x200B;
It has 1550 stars on github. Of course someone tried it and thought it was worth it.
I'd model driving the car as needing exclusive &amp;mut access. You're using the resource, not just observing it with others. The passengers might get a read only handle at each cycle of the world, but they don't get readable access while you're using/mutating the car. Actually, if you've got cars and multiple people acting independently, you'll want to look into the actor model. [Type-safe &amp; high-perf distributed actor systems with Rust](https://www.youtube.com/watch?v=qr9GTTST_Dk)
You might also be interested in https://doc.rust-lang.org/stable/std/iter/index.html#for-loops-and-intoiterator
why you've ever thought about implementig that on the web server? it would be pretty dumb idea :D it's better if cargo tool will implement sync by waiting for response from web server :)
Is this something you really need to do? Usually handlers like that are better left to end-users to configure in their own binaries.
The library is a large language runtime that would benefit from human-readable messages if something internal goes wrong.
I'm a bit puzzled by where the problem lies though. Setting up a panic handler just involves calling [set_hook](https://doc.rust-lang.org/std/panic/fn.set_hook.html) at some point during program execution, and the `human-panic` crate does this under the hood with its [setup_panic!](https://docs.rs/human-panic/1.0.1/human_panic/macro.setup_panic.html) macro. So it should be sufficient to just call that sometime during your runtime's initialization with no `lazy_static` or `Once` required.
Is it? I said it's poorly worded and contradicts itself. Peopling pilling up not because he is working on this, but because the whole thing is very shady and he is dodging a lot of questions. The author made a lof claims with no proof, lame excuses, tried to promote it on reddit and HN multiple times (under a different name). Github account is very light.
The runtime has no specific initialization. It's just a library.
Sure, I took some of that to be aspirational rather than the current situation, but I see that reading each statement as a hard claim would warrant skepticism. Or perhaps I was skeptical enough to dismiss the claims out of hand and just consider it a cool project that may or may not work out in the future. I'm not sure. Either way, I think it's just not a great look for the rust community to be so hostile to a new language that claims to be competing in the same space, to the point of speculating that it's a scam. It's not wrong to be skeptical, but I don't think the tone in some of these comments reflects the usual friendliness and welcoming attitude of the rust community.
Ah, fair enough. You'd described it as a large runtime so I imagined there was a single point-of-entry that had to be called to get things all set up. But if that's not the case then I don't imagine there's a good answer, as the feature isn't really designed to be invoked by libraries and `ctor` isn't working consistently for whatever reason.
Another fun claim: &gt; V can translate your entire C/C++ project and offer you the safety, simplicity, and up to 200x compilation speed up. 
Wonder how long that translation takes. 
As someone who has starred things on GitHub, this is a *bold* assumption to make. :-p
I recommend the official one called [The Book](https://doc.rust-lang.org/book/).
If youâ€™re lucky it will complete before the inevitable heat death of the universe. 
Oh, very interesting! That looks sufficient at a first glance...
You can add your `Arbitrary` impls in an inline submodule and `#[cfg]` it so it only compiles for tests (then quickcheck can remain in `dev-dependencies`): pub struct Foo { ... } #[cfg(test)] mod test_impls { use quickcheck::Arbitrary; impl Arbitrary for Foo { ... } } 
This is a pretty specific example, but there was a rocket (Ariane 4) with software that needed to convert from f64 to u16. So there are definitely use cases. I only know about it because the same software was ported to Ariane 5, which (very simplified) was a faster rocket, caused the conversion to overflow, and ultimately the rocket self-destructed on its first launch. [Source](https://www.youtube.com/watch?v=sijTOXxGO7I&amp;feature=youtu.be&amp;t=280)
Of course, gdb is also an option. I'm not really sure how it compares to lldb. &gt; I know dbg! is an option - but that feels like a going back to the time when debuggers didn't exist) Debugging with print statements is, I think, a lot nicer than C (for instance) since debugging things like segfaults is much less common, and the `Debug` trait makes it easy to print arbitrary things. And I kind of like just doing that, for simple things. But it's a matter of preference, and a debugger is definitely a tool worth having for the cases where it's particularly helpful.
You might consider refactoring this as a library as well and publish it on crates.io so people can use it in their own code. You can have a `main.rs` and a `lib.rs` with the former referencing the latter as just `edgy`.
[https://github.com/jgrant27/jngmisc/tree/master/rust/machin-pi](https://github.com/jgrant27/jngmisc/tree/master/rust/machin-pi)
Works really great! My only issues are that I can't figure out how to disable everything by default, and only enable with a feature, the live view seems to not work (browser can't connect to host:1338, dosen't exist), and I can't seem to toggle showing/hiding groups in the timer view
&gt; Should I try to avoid doing this double mutable borrow even though it compiles ok? I don't see any reason to avoid it in general. With features like this, the compiler won't let you shoot yourself in the foot, so "if it works, it works". 
Using `.filter(closure)` on an iterator doesn't actually do anything on its own. All it really does is create a new iterator by wrapping the old one up inside a new type. Think of it like creating a factory class in Java: it doesn't actually generate anything you tell it to. `.nth(n)` is different. It's a method that actually performs iteration by calling `.next()` on the iterator `n` times and then returning the next item it produces (if any).
Is Pathfinder participating in the SVG rendering, or just the text?
Having external dependencies screw around with global systems like panic handlers implicitly is *very* un-Rusty. Unless you're willing to ask your crate's users to call a setup function (as you should) you can't do this and it's a very good thing that you can't too.
Crate-specific panic handlers would be very handy in this case.
Wells its very close to C# style. Why not just do it the way they do it?
Rustacean.
Pathfinder is rendering everything you see there. The SVG is converted to Pathfinder's internal form using resvg once. Pathfinder then does all the transforms, clipping, tiling, curve conversion, GPU data processing, upload, and rasterization. It happens continuously from scratch every frame, so the vector scene effectively has infinite resolution. On my phone (Pixel 1) and laptop it achieves 60 FPS.
Can it translate memory management errors too?
Oh word, thank you. I was expecting this to fall apart when I went to use Foo's Arbitrary instance in another test module, but it definitely works. So when I declare trait implementations on Foo within foo.rs' #[cfg(test)] module, those trait instances are then available in other modules with the test trait?
I am tring to something so easy but i dont have any idea how i can do it. Lets say i have 3 struct(or enum or trait i dont know) and i have an another struct(or something else). struct One{ a:i32 } struct Two{ b:i32 } struct Three{} struct Choose{ a: i32, str: Two } It does not have to be struct. Can be something else like Enum, trait. Which subject do i have to look?
There's no visibility for trait implementations; if it's in a compiled module and the type and trait are both accessible then the implementation is accessible. Anything marked `#[cfg(test)]` is only compiled when compiling the crate for testing (I forget if this applies to dependencies as well).
There are a few that list it as a dependency, at a quick glance. 
Respect for efforts, specialy for documentation but promises is soo high. 
I used it - not problem. Created model in keras, converted it and used in rust via the tensorflow.
I've been using it and I would recommend it, even though it still feels like a work in progress in some aspects 
You have to initialize the Struct. [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=39623297194466d0bc40a729b6ce331a](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=39623297194466d0bc40a729b6ce331a)
Well that just absolutely spectacular!
TIL you can have embedded function definitions in rust
This is what I see with rust-lldb. When I type p opt, I expect to just see the value of a URL inside (which is printed all the way in the bottom), but it also executed the next line (which I don't want).
Debugger support is basically unusable both gdb and lldb from personal experience. println is great but uh It gets worse when your code base is 200k lines and every time you add dbg or println it takes like 2 minutes to compile to run it. Again personal experience :(
I think the most notable thing to me is that I don't see any guarantee of memory safety. There's a "Safety" blurb on the site that says "no global state, no null, no undefined values", but I'm guessing they just have unsafe memory management for anything that's not simple RAII.
About 20% slower than compilation. 
Thanks, didn't know about C# lock. It looks very similar and it also locks code, not data.
I didn't promote this on HN/reddit at all lol &amp;#x200B; Especially not under different names. &amp;#x200B; What questions am I dodging?
Hi everyone, This is my first Rust project, built out of the desire to be able to backup my files to a USB faster. I have extended it to become a faster and user friendly version of cp, rm, and rsync for local directories. It does not support copying to any remote server (yet!). Any suggestions would be great!
An IMAP client is planned for the future. Using Gmail API is easier for now.
Oh, I just saw the use of crossbeam\_utils. I know in Rust the usage of 3rd party packages is encouraged, but I think it'd be fair to Go and V to use built-in language tools, so that all examples can be compiled without dependencies. &amp;#x200B; serde is fine, because there's no json decoding in stdlib, but you can spawn threads in Rust.
Awesome!
Huh, well I'll be excited to see it when the demo is released then!
I don't think so, not for all platforms at once at least. If you know how to install on Linux, OSX and Windows independently, though, it shouldn't be too bad editing that into .travis.yml and appveyor.yml. Travis is nice but it's largely dependent on manual scripts to add packages. With some Linux packages you con directly tell it to install something available in the Ubuntu repos with "apt" configuration but nothing that I know of does all platforms.
Hi! I'm getting a strange borrow checker error that I cannot figure out. The following code: struct Node { next: Option&lt;Box&lt;Node&gt;&gt;, } impl Node { fn child(&amp;mut self) -&gt; Option&lt;&amp;mut Node&gt; { if let Some(n) = self.next.as_mut() { Some(n) } else { None } } fn tail(&amp;mut self) -&gt; &amp;mut Node { let mut node = self; while let Some(child) = node.child() { node = child; } node } } fails with error[E0499]: cannot borrow `*node` as mutable more than once at a time --&gt; src/lib.rs:19:9 | 14 | fn tail(&amp;mut self) -&gt; &amp;mut Node { | - let's call the lifetime of this reference `'1` 15 | let mut node = self; 16 | while let Some(child) = node.child() { | ---- first mutable borrow occurs here ... 19 | node | ^^^^ | | | second mutable borrow occurs here | returning this value requires that `*node` is borrowed for `'1` But this is a little strange to me! The second `node` at line 19 isn't really doing another borrow, it's just returning the mutable reference we already have! Is there another way to write this method better? (In my actual use-case, I have a trie and I want a mutable reference to the node corresponding to some byte-prefix, which I also implemented with a similar `while let` function to walk down the trie).
I was looking in `widget::config`, and for the way you're doing it I think it would be better to have the keys be an enum and also to store the values in an array of options. Then you don't have to have the overhead associated with hashmaps. A problem that doesn't fix is that you can get yourself in trouble by giving `Configurable::set` a different key and value pair, e.g. the autoclip key and a color value. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=65c40c2392c102b0e33409d621870ace) Alternatively, I don't see much benefit of doing it that way over just having a struct with an option field for each type. Then you can just expose those fields as public or make getters/setters. If you still want it to be accessed like a map, you can have a struct for each config value that implements a key trait. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3f3223fc1b24e2ac6d4f848d185c0115)
Process macros should be usable for this on stable with https://github.com/dtolnay/proc-macro-hack.
No plan9 asm or go compiler is used. It emits machine code directly. Like TCC. C/C++ translator uses clang parser.
Hey, I was just wondering, how did you come across that article? I got lots of traffic from it, but it wasn't posted on HN or reddit.
I think "encouraged" might be underselling it a bit :) Idiomatic Rust programs rely on 3rd party crates for things that other languages would consider basic infrastructure: - non-const global variables via `lazy_static` - random number generation via `rand` - regex support via `regex` - converting between integers and byes via `byteorder` - HTTP requests via `reqwest` or `hyper` (which we're using here) Scoped threads like those provided by `crossbeam` actually used to be in the standard library, until a [soundness issue](https://github.com/rust-lang/rust/issues/24292) was discovered in the old API prior to Rust 1.0. Given the ubiquity of Cargo and 3rd party crates, there hasn't been much pressure to reintroduce a new scoped threads API into std. Note that I'm not using scoped threads because they're less verbose. (If we were collecting their error returns they'd probably be a little more verbose on balance.) Instead I'm using them because they allow child threads to read data on the parent thread's stack. This is another one of those capabilities that feels a lot like basic language infrastructure. If we can't use the parent's stack, then we have to make sure that any shared data winds up in a heap-allocated `Arc`. Here's that version, which doesn't depend on `crossbeam`, though of course it still depends on `reqwest`: https://github.com/rust-lang/rust/issues/24292. The differences here are relatively minor, but I worry that in general the "no 3rd party deps" rule is going to give people the wrong idea about Rust. It's a small-stdlib language. It would be kind of like trying to write Go code without any heap allocation. It's doable, but it's not really the best example to use for a language comparison at an introductory level.
You are talking about what is essentially building a framework, not a library. &amp;#x200B; \*You\* call libraries and tell them to do things. Frameworks do things the way they want and then call into \*your\* stuff where \*it\* allows it. &amp;#x200B; So either you need to go with the first and trust that the user knows to call your setup code that prepares things appropriately or you need to build a framework that only allows user to provide specific hook in code. &amp;#x200B; The first is far more 'rusty' and suggested but the second option has viable reasons. You \*can\* split the difference if you set things up correctly through creating single point of entry objects which 'guard' the usage of the internal API, but I will let you know, your users will hate you.
Thank you very much for your time! Do I have your permission to use this code on https://vlang.io/compare ?
Sure thing. You have my permission.
Yeah, sealed traits would work here, though enums look like they would be more or less equivalent? Youâ€™d have to do â€˜intoâ€™ up/down casting yourself, but the decalaration of those From impls could be generated. Given that itâ€™s usually a good idea to generate (using macros or just boring code gen) the tree itself, that doesnâ€™t seem like to much additional complexity. That is, sealed traits are definitely better here, but seem to require roughly the same big O of code. What would be in theory strictly better are OCamlâ€™s polymorphic variants: with them, you donâ€™t need to give a to every subset
Here is a link for those who missed it: https://github.com/AlexEne/rust_hawktracer
Did you attempt to implement data-parallel algorithms in Rust? Over the years I have built a few things in OpenMP. I tried to rewrite an image kernel convolution algorithm in Rust using Rayon. Iâ€™m sure I was doing something wrong, but the result was much slower than the serial version, not to mention the C version. Although Iâ€™m absolutely positive this can be done in safe Rust, and may even compete with the C performance, OpenMP is amazing and easy. Iâ€™m just not convinced itâ€™s worth doing if youâ€™re working with a structure of builtin C types (floats, ints, etc), because the FFI is trivial. In fact, it was in the course of this experiment that I discovered OpenACC means I can use the same exact code on my GPU. Really love to hear your thoughts about this 
You should probably tell what is pushrod in your announcement.
Done. https://vlang.io/compare Thanks again. Of course using a mutex for incrementing an int is ridiculous. I'll need to come up with a better example.
&gt; So something was accidentally stabilized, independently of whether it can be made to work in the future or not, there was no RFC for this, no FCP, no approval from the team members no nothing. &gt; &gt; Allowing these bugs to remain in the language feels like "design by randomness". I don't have anything to say other than that I agree entirely. I'm quite unhappy about the current state of things wrt. the issue you reference. &gt; How long have you know about this bug, and why didn't a warning PR land the same week that this was discovered? See: + https://github.com/rust-lang/rust/issues/56254 + https://github.com/rust-lang/rust/pull/58739 &gt; I feel kind of disappointed about how this is handled. You and me both!
The correct terminology is 'nested', but it doesn't stop there! You can have embedded structs, enums, type aliases, `use` declarations... even embedded `impl`s that have effect only within a given scope!
How did you figure out the basics? Or were you already familiar with Dear Imgui?
Very nice, I was considering writing something similar myself. What I think could be better are the benchmark sections. Testing larger directories (eg 100k, 1m files and 10gb, 100gb total bytes) across hdds and ssds. That's what I mostly need rsync for. 
Mostly by looking at the function list in the online docs (rust version) and reading the C imgui examples. The Rust API closely matches the C one. 
Functions are just closures that don't allow you to capture variables from the enclosing block. Have you read The Book?
Who reads the ENTIRE book???
Haha, right? Only people with too much time on their hands! /s
Perhaps look at my [`rolling-crc-rs`](http://github.com/BartMassey/rolling-crc-rs) crate? It's in a pretty half-finished form (see the issue tracker), which is why it never made it to &lt;http://crates.io&gt;. However, it provides the basic functionality needed by a "diffing" rsync implementation: find only those blocks that need to be copied to make two files match. Even as slow as this crate is, that might be a win in terms of disk IO performance: there's also likely some big speedups that could be doneâ€¦
You're building without optimizations. Try `rustc -O3`.
You're in debug mode. use rustc -O3
I don't know how you wrote your code, but my experience is that the speed of Rust code can vary greatly just by restructuring it. Explicit SIMD, for example, can give your program a massive boost. Also make sure you're designing your program to be cache-friendly, and be sure to use the `--release` flag if you want fast code.
Now, it seems to me, that Rust does not run loop and just print result. Because time is not changed.
Compile time optimisations do this. If you don't want it to happen you need to blackbox something, e.g. the input 1000_0000. (https://docs.rs/bencher/0.1.5/bencher/fn.black_box.html)
I've never had to hash a file ;) I generally find myself looking for fast hash on *small* keys (32/64 bits), and all those fancy Murmur/City/... don't help at all for those key sizes.
Blackboxing the input won't even help here - you need to blackbox the output, or the loop will still be eliminated.
It's quite likely. Your can use a `black_box` on the input to prevent it from fully evaluating the computation at compilation-time... ... but you'll likely still be disappointed in this case because LLVM should optimize the loop to its closed formula anyway: `sum i for i in [0, n)` is `n * (n + 1) / 2`. And black-boxing the 0 may not help either, since `sum i for i in [start, end)` is just `sum i for i in [0, end) - sum i for i in [0, start)`, that is `end * (end + 1) / 2 - start * (start + 1) / 2`, which I expect LLVM to recognize too.
it was posted on Go subreddit yesterday
/r/playrust
r/lostredditors 
Here is an over-simplified example with rustc: [https://godbolt.org/z/-WK5\_4](https://godbolt.org/z/-WK5_4) pub fn max(a: u32, b: u32, c: u32) -&gt; u32 { vec![a, b, c].into_iter().max().unwrap() } To compute the maximum of three integers the function above utilizes quite a lot of abstractions: * a variable-size vector (by macro `vec!`) * a consuming iterator over the vector (by calling `.into_iter()`) * a method .max() which returns the maximum (if any) of values in the iterator All of them are general-purpose constuctions / abstractions that works for any data type which implements some traits, but the resulting assembly is suprisingly simple: mov eax, esi cmp edi, esi cmova eax, edi cmp eax, edx cmovbe eax, edx ret All these abstractions leave no trace in the assembly: * The compiler knows that the vector has fixed size (3), so it didn't actually allocate the vector on the heap * the construction of the iterator didn't actually happen * logic of .max() is inlined and unrolled At the end the compiler gives bare minimum assembly (2 cmp and 2 cmov) for which seems to be a lot of abstractions in the source code. C++ (with gcc), another language that emphasizes zero-cost abstraction, shows similar behavior: [https://godbolt.org/z/pb1VrH](https://godbolt.org/z/pb1VrH) the function definition: #include &lt;algorithm&gt; unsigned max(unsigned a, unsigned b, unsigned c) { return std::max({a, b, c}); } And as the control group here is the result of Swift: [https://godbolt.org/z/zlxC7R](https://godbolt.org/z/zlxC7R) definition: func max(a: UInt, b:UInt, c:UInt) -&gt; UInt { return [a, b, c].max()! }
## Converting Vec&lt;NonZeroU8&gt; to Vec&lt;u8&gt; I hope the title is clear! :) How can convert this without performance overhead?
NonZeroU8 seems to be `#[repr(transparent)]` so you should probably be able to just transmute it. There are also crates that try to provide in-place map operations, for example https://crates.io/crates/map_in_place (which I'd probably read through before using because I'm paranoid, but YMMV)
I agree that blaming the second `node` is a bit confusing. But the issue is still that you have two mutable borrows of the struct simultaneously, the one bound to `self` in line 15 and the one bound to `child` in line 16. The reason why line 19 is blamed is a bit complicated. If you removed that line, the compiler with non-lexical-lifetimes can figure out that the value stored in `node` is no longer used after the call to `child`. That means that the value can be dropped so that there's only one mutable borrow. This behavior might be what you are expecting. But since `node` is used after the loop, the value needs to be kept around at least until the `let` binding. In that brief moment there are two mutable borrows, which is not allowed. I would implement this using recursion instead: fn tail(&amp;mut self) -&gt; &amp;mut Node { if let Some(ref mut n) = self.next{ n.tail() } else { self } }
Cool! I've just released a new version today.
Looks nice, might give it a try.
I use it for a face detection pipeline, but I'm waiting on [one issue](https://github.com/tensorflow/rust/issues/192) to be resolved.
Nice! Does that mean that you've effectively created a Pathfinder backend for resvg? Would it make sense to upstream it?
amazing!
Fixed, thanks!
Hey Rust community. This is my implementation of the Realworld Backend API spec using the Actix web framework. I'm posting it here in hopes that some community members could take the opportunity to help review it for soundness and also maybe come up with some basic approach for how to do unit testing/CI for it. From what I'm aware, it works as intended when tested against the provided Postman collection, but there might be cases I haven't thought to account for in a proper live environment with a live frontend server. If anyone would be able to offer their help, I would much appreciate it. Thanks!
Though you could blackbox the addition.
I don't know the answer, but you may want to read: [https://internals.rust-lang.org/t/how-to-support-rust-debugging-post-tromey/9207](https://internals.rust-lang.org/t/how-to-support-rust-debugging-post-tromey/9207) 
There is also another use of debugger. Learning tool, we can use it to figure out a codebase faster. Such approach has helped me a lot in the past and it helps me with Rust. Yeah, we do have OK type system, but seeing actual data and how it transforms through the program is really helpful. (For me, at least) So far I have tried only vscode-lldb, I will probably try CLion debugger next week too. Experience was not terrible, but it can be significantly better. 
The only problem with this analogy is that I feel it doesn't encode some key aspects of ownership particularly intuitively, namely move / copy. I sometimes feel like ill-thought analogies can often complicate things further, like adding an additional layer of abstraction - sometimes obfuscating lower level concepts for the sake of it.
Imgui seems reasonable and I think it's got everything in your need list (not sure about the nice to haves though, haven't seen curvy lines or file pickers yet although I'm sure it wouldn't be too difficult to extend). I made a starter project of using imgui with ggez (which is a pretty popular rust game engine), check it out! Hope it helps: [https://github.com/iolivia/imgui-ggez-starter](https://github.com/iolivia/imgui-ggez-starter)
Imgui seems reasonable and I think it's got everything in your need list (not sure about the nice to haves though, haven't seen curvy lines or file pickers yet although I'm sure it wouldn't be too difficult to extend). I made a starter project of using imgui with ggez (which is a pretty popular rust game engine), check it out! Hope it helps, the docs were not amazing so I had to dig quite a bit through the source code to get to this hehe! [https://github.com/iolivia/imgui-ggez-starter](https://github.com/iolivia/imgui-ggez-starter)
I want to create something similar. But I have no idea how to implement something like this. Somebody who wrote a similar tool said he used imgui, so thatâ€™s probably a good start. It would be could if you could share your progress.
How is 16/03/2017 related to Pi? Every date is in there somewhere, so why today?
To disable everything you use the feature "disabled" ``` [dependencies.microprofile] version = "0.0.2" features = ["disabled"] ``` Right now it only affects the macros, so you have to make sure you use the macros, and not the functions. With regards to not being able to connect - Are you calling flip regularly? or using autoflip? the function needs to be called regularly for the ui to work. If you still have problems, I suggest you bump the loop in the test to something like 100000 https://github.com/jonasmr/microprofile-rust/blob/master/src/lib.rs#L273 then you should be able to run `cargo test` on the microprofile-rust project and connect to `localhost:1338`, and see if you get the problem there. 
I used `parking_lot`'s mutex (which is fair) and my issue is solved thank you very much !
Its a bit late, but Pi day is 14/03, or as americans write it, 3.14
2017? Welcome to the future :)
Yeah but what I'm trying to do is make a feature on my crate to enable mixroprofile, and otherwise have it use the disabled feature by default. Yeah I also use init, enable all groups, dump html, and shutdown (they don't seem to have macro equivalents), but those can be wrapped in cfg!(). I'll try out the test and see if it works. Also, are you willing to take bug reports for Firefox? Or do you not want to support it whatsoever
The loop might still be collapsed if you blackbox the output but not the input.
Not sure if you are the author but here are some typos on page 43: &gt; There is only every 1 mutable reference to a given data item across all scoped 
Does pathfinder has it's own text layout? 
If the nested function is not allowed to capture variables from the enclosing block, then it isn't a closure anymore ðŸ˜‰
Yeah, you really want to blackbox both. Or, well, what you _really_ want is a better benchmark.
Is this for the Titan U2F tokens? Neat! Is there a way to actually flash it on my own key, or do you need some special developer level access?
That was a really good talk at FOSDEM. Even better was that it took place outside of the Rust devroom. So a broader audience could see what Rust is all about. The room was packed. 
Floating point to RGB values is pretty commonplace.
Floating point to RGB values is pretty commonplace.
[description and video](https://fosdem.org/2019/schedule/event/microkernel_written_in_rust/)
Seems like a great project ! Does it detect moved and renamed files ?
Though, it seems fragile in the sense of compilation stability rather than run-time. For example, if \`AritmeticEncoder\` is defined in another crate, then a later version of the crate that adds \`Drop\` will automatically break the compilation of the code that relies on NLL, right?
Noob question: What's really annoying for me is that I constantly have to use `{:?}`. It's the same situation every time. I use `{}` and compile. I get an error. I add the `:?` and then it works. I don't get it. Why can't the normal `{}` not just display my output correctly? Is this a performance problem? A safety problem? What am I not seeing here? 
Contrary to literally everyone else on this sub or in programming in general I really do not like generics. I feel like it makes things much more messy, harder to read, more entangled. And why? Just to save a few lines of code? Same with traits. I could just do everything with methods. This will be more typing, but for me it looks like it is MUCH easier to read, more verbose, etc. But: Are there any disadvantages besides more lines of code? If I do not use any generics and traits (and yes I know I will be using them indirectly via the standard library, but let's ignore that for now) are there any disadvantages I will experience? Will my code be slower? Is there anything of absolute importance that I will not be able to do? (And again just to be sure: I am talking about the code that I(!) will be writing. Of course I will indirectly be using std-code that uses traits/generics.)
Wow. I am amazed how much was done here. I'm glad ARM is supporting this development. You made a very efficient use of the time doing so much stuff! This is really exciting!
I faced the exact same problem few days ago and I was quite surprised on how hard it was to find a concrete example. Here was my take on it: https://github.com/boastful-squirrel/miniserve/blob/targz/src/errors.rs And here is how I map the errors to mine: https://github.com/boastful-squirrel/miniserve/blob/targz/src/archive.rs Note that I'm quite a beginner so my code might not be the best.
What is "correctly" supposed to mean? `{}` is already used with the `Display` trait, and there are quite a lot of types that implement both `Debug` and `Display` with different output.
Yes I have, maybe I didn't notice that
Or ISO 8601 date, 2019-3-14
Also nested multiline comments. fn main() { println!("Hello nested multiline comments."); /* println!("Outer comment 1"); /* println!("Inner comment 1"); */ println!("Outer comment 2"); */ }
From Min 25 to Min 60. It's kind of hard to pin point where that happens exactly. Basically they start discussing two phase borrows, and trying to figure out what can / can't the Stacked Borrows models actually do. And at around Min 43 they start discussing whether these models can handle the bug that was stabilized. The discussion is a bit mixed between arguing that one shouldn't ban / add things to the language just because they are easy / hard to model, with "but this is a bug that we did not design" and "but the bug is in stable, so why break it", and back again to "because we did not design it, and its hard to model" and full circle to "we shouldn't just add features that are easy to model".
If I have my own struct. I can derive Debug over it and then just use `:?` and I can print pretty much anything I want. Why can't I do the same for Display? If I derive Display over the same struct I cannot just print it with `{}`. Why can the Debug trait do it but the Display trait can't?
The standard library doesn't provide the ability to derive(Display) at all, but there are some third-party crates that do: https://crates.io/crates/derive_more
I'll work on refactoring this as a library. I didn't use the filter3x3 method as I would have to apply two different kernels (two different filter calls), and then still iterate over both to calculate the magnitude. This neatens it up to just one iteration.
Ah OK. Interesting. Thanks! I think the problem is I just haven't understood why traits even exist. They seem completely useless in comparison to just using impl methods. I wrote about my problems trying to understand traits in another post in this thread: https://old.reddit.com/r/rust/comments/azqo9c/hey_rustaceans_got_an_easy_question_ask_here/eins5xk/ Maybe it's better to understand this first. But thanks so much for your help!
This is interesting! I'll give it a look.
I have to say, that sounds a lot more accessible than "A monad is just a monoid in the category of endofunctors, what's the problem?" 
Thanks for the response, although I'm not sure I quite understand. I guess in my mental model, calling `self.child()` doesn't "create" a second mutable borrow -- instead I think of it as kind of "swapping" it with another mutable reference. I kind of understand why the compiler wouldn't be able to infer that in the `while let Some` case (because it'd have to understand the semantics of `node = child`, which admittedly seems pretty tricky), but then why doesn't the following work: fn tail(&amp;mut self) -&gt; &amp;mut Node { if let Some(n) = self.child() { n.tail() } else { return self; } } At least in my naive opinion, it should be "easy" to know that the mutable reference from `self.child()` doesn't last into the `else` case (whether I do it in the else or use an early return for the `if` statement doesn't seem to matter). The crux of your solution seems to be about using `self.next` instead and not the recursion -- the following compiles just fine. fn tail(&amp;mut self) -&gt; &amp;mut Node { let mut node = self; while let Some(ref mut child) = node.next { node = child; } node }
So there are 4 parts to writing a REPL: 1. Read (a line) * You can use stdin: https://stackoverflow.com/questions/30186037/how-can-i-read-a-single-line-from-stdin * Much nicer (arrow keys support, history, etc.): https://crates.io/crates/rustyline 2. Evaluate (this step is up to you) 3. Print - use `println!` 4. Loop - use `loop` or `while` depending on how you want to structure your code 
xxh3, see http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html?m=1
It works! Thanks!
You can definitely go faster than murmur!
Redox is an amazing effort and Iâ€™m looking forward to VM images for this.
I donâ€™t know how you make a feature like that(I would also like to know). The functions you mention -do- have macros, look near the bottom of src/lib.rs. I wonâ€™t be fixing the Firefox bugs, sorry!
Ooh, rustyline sounds like what I need. Thank you for the recommendation!
Iâ€™ve used this lib and it was fairly straightforward to implement with lots of features: https://github.com/murarth/linefeed
Hello All, I am working on the AoC problems (day4, part1). At one point, I have a ref to a vector of u32 Ranges that I want to iterate over. This is how it looks like: fn process_range_vec(ranges: &amp;Vec&lt;Range&lt;u32&gt;&gt;) -&gt; u32 { let mut midnight_hour = vec![0u32; 60]; for range in ranges.iter() { for idx in range.clone() { midnight_hour[idx as usize] += 1; } } midnight_hour.into_iter().sum() } What I am curious about is that I have a reference to a Vec, and I can smoothly call .iter() on it, to have an iterator over references to ranges. At this point I want to do the same on my references to Ranges, and I get `the trait \`std::iter::Iterator\` is not implemented for \`&amp;std::ops::Range&lt;u32&gt;\`` message. I thought rustc would coerce the reference into the type here, but this doesn't happen (Deref is not implemented, I assume), so this won't work unless I clone the Range. (Also, I expected the range to be of known size, but copy also doesn't work in this case.) &amp;#x200B; Sorry if this is a noob question, I just wanted to know if there is a better way to do this, or if I understood something wrong. Many thanks! &amp;#x200B;
Awesome, I'll take a look and compare it to rustyline.
Why did the author have to talk to arm's legal team about creating the port?
\&gt; Unsafe Rust â€¦ Disables the comprehensive compiler checks to permit C/C++ like type and memory operation &amp;#x200B; Actually, this is not true. \`unsafe\` does not disable the borrow checker, it allows raw pointer operation. Someone from the core already wrote an article that addresses this misconception. &amp;#x200B;
Looks great!
Alright, thanks!
It's very resilient, but there is a security vulnerability at the base of the neck.
Post your entire program.
As somebody who occasionally starts writing games for fun, this strikes me as a good idea. Will be keeping an eye on this.
 let args: Vec&lt;String&gt; = std::env::args().collect(); let remote_addrs = &amp;args[2]; let mut buf: Vec&lt;u8&gt; = vec![]; let mut bacon = Bacon::new(BaconState::Unfried, None, args[4].clone()); bacon = cipher.encrypt(bacon); buf = bincode::serialize(&amp;bacon).unwrap(); match socket.send_to(&amp;buf, &amp;remote_addrs) { Ok(size) =&gt; { println!("{:?} bytes sent to {:?}", size, &amp;remote_addrs); }, Err(e) =&gt; { println!("Cannot sent to {:?}: {:?}", &amp;remote_addrs, e);} } The entire [file on github](https://github.com/aspera-non-spernit/bacon/blob/dev/examples/client.rs) 
Can you provide a simplified example, or at least show the parameters you're running your program with? 
You'd be surprised how often people translate from 2nd languages into 3rd languages (or vis versa), you can get quite good at a language even if it isn't your first :p &amp;#x200B;
I am starting the program with: // starting server // cargo run --example server (public_or_local_bind_addr:port $ cargo run --example server 123.123.123.123:62100 // starting client, all three examples // cargo run --example client [which example] local_bind_ip remote_addr:port (optional argument) $ cargo run --example client string 127.0.0.1 123.123.123:62100 "Hello World" $ cargo run --example client person | chat) 127.0.0.1 123.123.123:62100 "John Doe" $ cargo run --example client chat 127.0.0.1 123.123.123:62100 "Hello World" &gt;&gt; reads from stdin 
123.123.123 is most definitely not a valid IPv4 address. 
That's correct.
Thanks for the advice! I actually did considering doing an actual file diff, but I couldn't find a crate that provided any speedup over my naive implementation, so I dropped it. I might come back to it in the future, or if you complete yours ;)
Well, that's what's causing your error. Unless it's a placeholder for a real address and you mistyped it. 
If you didn't have traits, Rust would have to have some other way to verify that a function that took a "method" implementer actually had those methods and they meant the correct thing. So we could imagine a Rust that, instead of having traits, just looked for any valid method called `fmt` with the correct signature on the given type. How would it do so? Would we take for granted that any method called `fmt` on a type is intended to be a method for displaying that type? What about displaying for debug? I suppose we could create a different method called `fmt_debug`. Then, what about things that can be debugged with optional formatting information? Not every type wants to implement all of those, so we'll need new unique method names for each of those. Worse still, there's no immediately identifiable solution to how we'd write generic functions that took types with specific methods. We could do something like the following: ``` fn print_twice&lt;T&gt;(t: T) where T: { fmt&lt;F&gt;(&amp;T, f: &amp;mut F) -&gt; std::fmt::Result where F: {...} // What do we put here? } { // Use t.fmt(...) // Use t.fmt(...) again } ``` So we now have to know all the types recursively, instead of being able to just have an opaque reference to a trait.
Of course it is, it's the same as 123.123.0.123.
I do regularly test with larger directories, but didn't benchmark any of them because the benchmarks would take ages to complete. I'm not a fan of globals either, but in this particular case, I found that having a local one would make my code quite a bit messier (since I needed it for logging as well). I agree, there is definitely some refactoring that could be done though.
Having trouble telling if you solved this problem. If there's missing fields in a JSON payload, have you been able to respond with a clear error message to the client? Right now mine does a bad request, which is great, but having trouble figuring out how to say "field foo is missing"
&gt;**Does V use LLVM?** &gt; &gt;No &amp;#x200B; &gt;For now V emits C and uses GCC/Clang for optimized production builds. So it does use LLVM. &amp;#x200B;
I'm pretty sure Rust doesn't parse this notation though? Might be wrong though, been a while since I had to deal with it. 
At the moment no. I did consider it, but in the worst case I'd have to compare each file with every other file and it just didn't seem like that would provide any speedup. If you have a better implementation or have a reason to disagree, I'll definitely look into it though!
the addr is &amp;#x200B; [176.221.42.130:64100](https://176.221.42.130:64100)
dlopen's changes aren't (necessarily) atomic
Welp. This reads like an introductory pwnme exercise and/or a PoC from five months in the future, explaining how this-or-that Rust program secretly had the next [goto fail](https://nakedsecurity.sophos.com/2014/02/24/anatomy-of-a-goto-fail-apples-ssl-bug-explained-plus-an-unofficial-patch/). enum AuthenticationStatus { Success, Fail, UnexpectedError } impl AuthenticationStatus { fn has_admin_privilege(&amp;self) -&gt; Privileges { match self { Success =&gt; true, Fail =&gt; false, UnexpectedError =&gt; false } } } fn main() { let auth: AuthenticationStatus = client_authenticate(); let has_admin = auth.has_admin_privilege(); //... } It seems clear to me that the compiler should intervene *somehow*, and a warning isn't enough.
It does, it's standard.
Thanks a lot for your articles, the examples are very useful to understand rust concepts. 
That's actually one of the problems I was facing and decided to come back to later -- I'm not sure how to generate the custom error response on when serde fails to deserialize a body in which some field is missing. Actix already generates a generic "bad request" response, but it doesn't tell me more. I was hoping maybe someone with more experience could address it, or if this is the right way to go about solving the problem. They don't detail a particular way to return an error aside from [this](https://github.com/gothinkster/realworld/tree/master/api#errors-and-status-codes), which validator should do directly.
The [Armenian alphabet](https://www.omniglot.com/writing/armenian.htm) has got you covered.
Small repro, tested on Linux: use std::io::Result; use std::net::UdpSocket; fn main() -&gt; Result&lt;()&gt; { let socket = UdpSocket::bind(&amp;"127.0.0.1:50000")?; socket.send_to(b"hello\n", &amp;"176.221.42.130:64100")?; Ok(()) } socket(AF_INET, SOCK_DGRAM|SOCK_CLOEXEC, IPPROTO_IP) = 3 bind(3, {sa_family=AF_INET, sin_port=htons(50000), sin_addr=inet_addr("127.0.0.1")}, 16) = 0 sendto(3, "hello\n", 6, MSG_NOSIGNAL, {sa_family=AF_INET, sin_port=htons(64100), sin_addr=inet_addr("176.221.42.130")}, 16) = -1 EINVAL (Invalid argument) close(3) = 0 
I work using Python (many colleages use Matlab but I did not like ir for many). Last year I have checked other languages as Rust and Julia. I like Rust a lot, but its community is not very science focus. In my opinion, it should improve several issues: Calling other languages as python or R (calling C works nicely, by the way). Its type system is to strict numerically, adding an int and float implies to add 'x as f32' or similar. It could nice to have operations allowing use directly the operation without that (I am new, please, tell me if I am wrong). Mores science oriented libraries (plots, statistics, ...). To summarize, it is a very nice language, but as any recent language, it's community guide it it in specific fields (like web, database,...)
[Nope](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=443ae0584b546f356f106f8ce2668375) :(
Thank you for your help. I have no idea what it means.
Thanks! I updated the description. I just posted it late at night, so I wasn't thinking about it so carefully.
Not yet. The text is actually just paths. I suspect we'll eventually want a `pathfinder_text` crate that uses `skribo` to do the text layout.
This is a small program that reproduces your problem. The second part is the trace of the exact system calls it runs. Now that I'm looking at it, the problem _might_ be that you're binding to 127.0.0.1, so the kernel assumes you want to talk on the loopback interface, and you're trying to talk to the outside network. Try binding to 0.0.0.0 instead.
What about something like: bucket files based on file size and modification time, and for any buckets with more than one file in them, bucket further by hashing the contents.
You should be able to use Rayon in many of the same ways you'd use OpenMP as far as parallelizing an iterator goes. One issue could be that your problem size was small enough that the start up time for the threads was large enough to cause the code to run slower. Although, I couldn't say for certain without viewing the Rust and C codes. Also, I just want to say you're probably better off using OpenMP's offload capabilities rather than OpenACC. I've heard from a number of people while working on an exascale project that the industry is pushing for OpenMP over OpenACC to easily offload work to different accelerators.
Cool. that worked :) thank you very much. 
How hard to make it workable alike dropbox? I need a sync for photos that work with iOS/Android, and dropbox remove the SDK for that
It's normal. When you bind your to some port on `127.0.0.1`, the OS will try to use that source address. But since your destination is not localhost, that won't work. You need to let the OS pick any source address, which you can do with `0.0.0.0`: let socket = UdpSocket::bind(&amp;"0.0.0.0:50000")?; socket.send_to(b"hello\n", &amp;"176.221.42.130:64100")?; 
That is for the most minimal. A more interesting challenge is be alike: &amp;#x200B; [https://bpython-interpreter.org/screenshots.html](https://bpython-interpreter.org/screenshots.html). &amp;#x200B; The ideal is to have something like &amp;#x200B; [https://github.com/prompt-toolkit/python-prompt-toolkit](https://github.com/prompt-toolkit/python-prompt-toolkit) &amp;#x200B; so anyone doing reps can benefit. &amp;#x200B; &amp;#x200B;
Depends on who you're asking :-). https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=27ed7c34c60de82f6d358fe4e3f75c27 It works for `UdpSocket::send_to`.
Huh. This is actually kind of interesting. Wonder if it's worth filing a bug.
Dunno, it's more or less standard; also e.g. `2130706433` resolves to `127.0.0.1`.
Not filing a bug for ToSocketAddr behaving like this, filing a bug for Ipv4Addr _not_ behaving like this.
The first parameter is the default value, not the type. So in the example, you'd use `let mut arr = [[0u32; 50]; 50];`. In your case, you would use `let mut arr = [[Pixel { r: 0, g: 0, b: 0 }; 50]; 50];` after adding `#[derive(Copy, Clone)]` to the struct.
The thing here is that you indicate the type of the array, not its value You must give a value that will be copied for it to work (so Pixel must derive Clone and Copy) `#[derive(Copy, Clone)]` `struct Pixel {` `r: u8,` `g: u8,` `b: u8,` `}` `fn main() {` `let mut arr: [[Pixel; 50]; 50] = [[Pixel { r: 0, g: 0, b: 0}; 50]; 50];` `// do something` `}` &amp;#x200B;
A very interesting project. Here is a parallelized version of Sobel in a Rust project with support for many other image filters: [https://github.com/jblindsay/whitebox-tools/blob/master/src/tools/image\_analysis/sobel\_filter.rs](https://github.com/jblindsay/whitebox-tools/blob/master/src/tools/image_analysis/sobel_filter.rs) &amp;#x200B; User manual: [https://jblindsay.github.io/wbt\_book/available\_tools/image\_processing\_tools\_filters.html](https://jblindsay.github.io/wbt_book/available_tools/image_processing_tools_filters.html)
You must provide the default value for the array constructor syntax [[0u32; 50]; 50] Iâ€™m on mobile so I canâ€™t provide a full example but I hope this helps
Well, as soon as `skribo` will be released, `usvg` will stop emitting text and will emit only paths. Making it truly micro SVG, since SVG text layout is pretty complex. Not sure if this is a deal breaker for this project.
Oh, that's actually perfect, since that means `usvg` would just be doing what `pathfinder_text` was going to do.
And note that the explicit type (`[[Pixel; 50]; 50]`) is not required here.
You could... but then you're benchmarking the overhead of `black_box` rather than addition. Specifically, due to the way `black_box` works, it will potentially require a function call, likely force memory reads and writes (instead of having values in registers), etc...
The only operations that actually do anything are the ones that return values from the iterator, and they stop as soon as they've obtained that value for you. ::collect and ::nth are two examples of this. The rule of thumb is that if it returns an iterator, the function hasn't consumed anything from the base iterator.
https://docs.rs/actix-web/0.7.18/actix_web/dev/struct.JsonConfig.html
&gt; Not filing a bug for ToSocketAddr behaving like this, filing a bug for Ipv4Addr not behaving like this. Sorry, my comment made no sense. I think that behaviour is standard, but not something `IpV4Addr` would do. E.g. `inet_pton` doesn't understand `123.123.123`, while `gethostbyname` does, without performing a DNS lookup. I imagine the Rust situation is similar, although not exactly equivalent since `SocketAddr` is an (address, port) pair, while `gethostbyname` only cares about addresses.
I've actually decided to dig into it, and it turns out `ToSockAddrs` falls back to `getaddrinfo` if it can't parse the string you give it, so Rust's own IP address parser does not handle this case, but the fallback does.
Given the following code: ```rust trait Test&lt;'a&gt; { fn get(self) -&gt; &amp;'a i32; } struct TestStruct&lt;'a, T: Test&lt;'a&gt;&gt; { t: T, } ``` I get: error[E0392]: parameter `'a` is never used 5 | struct TestStruct&lt;'a, T: Test&lt;'a&gt;&gt; { | ^^ unused type parameter If I change the code to: ```rust trait Test&lt;'a&gt; { fn get(self) -&gt; &amp;'a i32; } struct TestStruct&lt;T: Test&gt; { t: T, } ``` I get: error[E0637]: `&amp;` without an explicit lifetime name cannot be used here --&gt; src/lib.rs:5:22 | 5 | struct TestStruct&lt;T: Test&gt; { | ^^^^ explicit lifetime name needed here I get that I could just use the first case with `PhantomData`, but that feels wrong. Wtf?
Thank you! This fixed the compiler issue I was running in to. Don't quite understand these `#[derive()]` things. Rust is way harder than PHP, and I feel like I'm riding a bike without training wheels for the first time. Definitely worth it though!
Besides what the others have said, it's often better to store your image as a 1D array, and convert the coordinates, e.g. `k = y * width + x` in one way, and `y = k / width; x = k % width` in reverse. This works better once you want non-constant sizes.
&gt; Rust has a very rich standard library Finally! I guess I am not the only one looking at the Rust standard library from the perspective of the C and C++ libraries :)
Ah, thank you. I'll try using that when I find the time to work on it more. I'm currently busy with some tough IRL stuff so :(
thank you for the help! I didn't know I needed a default value for this kind of thing.
The primary reason traits exist isn't to group methods for organizational purposes, but to describe to the compiler the characteristics of a type and what it is capable of doing. That's why they're called â€œtraits.â€ For example, you can write a generic function like this: fn do_something&lt;T&gt;(value: T) where T: Display { ... } This function accepts any type that implements `Display`. This trait isn't the best example, but it means you can write generic functionality that anyone can use simply by implementing the correct traits. A more useful example is `for x in y { ... }`. The `y` in the `for` expression can be any type that implements the `IntoIterator` trait. That way you can make your own collection types and make them work with `for` just by implementing that trait. TLDR: If you are familiar with Java or Go, they are similar to interfaces. A trait object (such as a `Box&lt;Display&gt;`) is equivalent to a Go interface that cannot be `nil`. 
It does help. Thanks for taking the time to type all those hard to find characters on your phone!
This is somewhat about the difference between `Iterator` and `IntoIterator`, a.k.a something that does the iterating itself and something that can be converted to an iterator. A `&amp;Vec&lt;T&gt;` can make a `slice::Iter`, which is an `Iterator&lt;Item = &amp;T&gt;`. `slice::Iter` has the next method that mutates itself and returns successive references into the vec. Looking at `Range&lt;u32&gt;`, we see that it is an `Iterator&lt;Item = u32&gt;`, so it needs to be mutated in order to give off `u32`s (The next method returns `range.start` and increases it by one). The reason that `Range`s aren't `Copy` even though they are just a pair is that `Iterator`s behave confusingly when they are `Copy`, because copying happens implicitly. I think the way you are doing this is fine, since the clone is very cheap and it probably gets optimized into a simple loop anyways. BTW, you almost always want a slice (`&amp;[T]`) as a parameter instead of a reference to a vec (`&amp;Vec&lt;T&gt;`), since it's more general and only gives up access to reading the capacity (not length) of the vec, which is almost never needed.
`#[derive()]` is what is known as a *compiler macro*: basically, Rust code that is executed at compile-time. In this case, `#[derive(Copy, Clone, Default)]` is the same as adding: impl Copy for Pixel {} impl Clone for Pixel { #[inline] fn clone(&amp;self) -&gt; Pixel { *self // Because we can copy Pixel without problems, dereferencing is safe. } } impl Default for Pixel { #[inline] fn default() -&gt; Pixel { Pixel { r: Default::default(), g: Default::default(), b: Default::default(), } } } `Copy` basically means that you are allowed to `memcpy` the whole struct without problems. This is because if your struct contains a pointer (e.g. a `Vec` or `Box`), there can be safety problems.
Dude. This is so cool! 
I have fixed that already. The server responds. :) I am happy. You should get 21 bytes back with that code. It crashed before, but hould be working now. 
Thanks for linking this!
note sure that nested multi line comments are different from normal multi line comments in the result
If you compile it with optimizations, it gives following output: `0 sec 0 ms 4999999950000000` [https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=e109698032f4d4c3ff0c640c7fda14ba](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=e109698032f4d4c3ff0c640c7fda14ba)
Probably because he's employed by arm.
When declaring a lifetime on a struct it needs to be tied to something in the struct itself. If you can't do that, you can try using a [higher-rank trait bound](https://doc.rust-lang.org/nomicon/hrtb.html): trait Test&lt;'a&gt; { fn get(self) -&gt; &amp;'a i32; } struct TestStruct&lt;T: for&lt;'a&gt; Test&lt;'a&gt;&gt; { t: T, } It's a bit hard to say whether this is a good idea or not in your real code.
Currently, `resvg` renders text as paths, so I'm not sure if this will make any difference in performance. The problem with storing glyph IDs is that you have to store a lot of additional information too (position, rotation, decoration, fill and stroke). Which can't be dumped back to the SVG and this is an important part of the `usvg`.
This compiles. trait Test&lt;'a&gt; { fn get(self) -&gt; &amp;'a i32; } struct TestStruct&lt;'a&gt; { t: Test&lt;'a&gt;, } &amp;#x200B;
&gt; Will my code be slower? No. &gt; Is there anything of absolute importance that I will not be able to do? Strictly speaking, no. You're not losing the ability to describe something to the computer. You're losing one dimension of expressing things to other human beings through your source code: ad-hoc polymorphism, because Rust implements it using traits (which is the Rust jargon for typeclasses). &gt; I had to do everything manually anyway. Yes, in that specific example I would agree that the extra abstraction of the `Display` trait obscures the meaning of your code more than it helps. (It doesn't help that the `std::fmt` system in general is about as clear a mud. I mean, it makes sense to me *now* but I had to get a lot of familiarity with generics before it cleared up.) &gt; If I had 1000 different structs I would need to write 1000 different trait impls, correct? So how is this better than just writing 1000 different impl methods? If you do that, the 1000 intrinsic methods will each have their own name. A trait allows you to express that the types are similar enough that those methods should share one trait-method name. Then you can write 40 functions which use that trait method and only have to write 40 of them, not 40,000. Honestly at that point you'd probably be considering function pointers or macros instead of writing a bazillion functions - and that's fine, but traits and type parameters would solve the problem more easily. If you don't have complexity in how something is used, then the extra work to make a trait isn't worthwhile.
Does this mean I don't have to use glookle's droid bot? 
Looks quite promising. Here are a few things that caught my attention: Basic types: rune // alias for i32, represents a Unicode code point Why not u32? Unicode code points are never negative AFAIK. ['a', 'b'] is an array of strings ([]string). Since strings are enclosed in single quotes, how do you write down a rune literal? As a number? The for .. in loop is used for going through elements of an array. What about iterators? for i := 0; i &lt; 10; i++ { println(i) } Why is `i` not declared as `mut`? This looks inconsistent to me. Also, this C-like for loop is kind of superfluous when there are iterators (I do hope there are!) Unlike C, break statement [in a switch statement] is not needed at the end of every block. So how can we match on multiple expressions at once? Rust supports this: match day { MONDAY | FRIDAY | SUNDAY =&gt; println!("6"), TUESDAY =&gt; println!("7"), THURSDAY | SATURDAY =&gt; println!("8"), _ =&gt; println!("9"), } Java 12 will support this: switch (day) { case MONDAY, FRIDAY, SUNDAY -&gt; System.out.println(6); case TUESDAY -&gt; System.out.println(7); case THURSDAY, SATURDAY -&gt; System.out.println(8); default -&gt; System.out.println(9); } &gt;Option types I find this name misleading, since it also contains an error variant. It's Rust equivalent is `Result&lt;T, E&gt;`: user := repo.find_user_by_id(10) or { eprintln(err) return } I assume that `err` is the error variant from the `Option`. Is there a way to choose a different name for `err`? This might be important in nested `or` blocks. json.decode(User, input) Since `User` is a type, not a value, I'm curious how this is implemented without reflection! Is `decode` a special case? And is there a reason for not declaring it as `json.decode&lt;User&gt;(input)`? &gt;**Is there garbage collection?** &gt; &gt;No. V's memory management is similar to Rust but much easier to use. More information about it will be posted in the near future. I'm very curious about this.
I would be worth to add to readme what a Nozbe actually is.
This clarifies it, thank you! (In the meantime I also found out that Deref allows coercion to a different type, so that wasn't a good idea.)
You're very welcome. I hope that it helps out.
Not really a question but I'd like to practice writting FFI/C bindings for libraries written in Rust. Do you know of any lib that would benefit from a C FFI?
What do you mean? The only major thing in Rust's standard library that isn't in C++'s, afaict, is command execution.
That's super helpful to know, thanks you man.
Well, filesystem stuff wasn't standardized in C++ until last year and there still isn't any networking in the C++ stdlib. There's a lot of stuff the rust stdlib has that the C++ stdlib doesn't. 
The next time you come across something like that, try `errno` and `strace` to see what's going wrong with the system call. claire@rokh:~$ errno 22 EINVAL 22 Invalid argument Well, that's not particularly informative. It would help to know what system call failed, and that's what `strace` will show. Alternatively you could read the standard library source code. From that I'd guess that the failing system call is `sendto`. claire@rokh:~$ man 2 sendto | grep -C9 EINVAL Connection reset by peer. EDESTADDRREQ The socket is not connection-mode, and no peer address is set. EFAULT An invalid user space address was specified for an argument. EINTR A signal occurred before any data was transmitted; see signal(7). EINVAL Invalid argument passed. EISCONN The connection-mode socket was connected already but a recipient was specified. (Now either this error is returned, or the recipient specification is ignored.) EMSGSIZE The socket type requires that message be sent atomically, and the size of the message to be sent made this impossible. ENOBUFS The output queue for a network interface was full. This generally indicates that the interface has stopped sending, but may be caused by transient congestion. (Normally, Well, okay, that doesn't help much, but might as well skim the entire manpage. `EINVAL` tells me that some invariant was broken, so also the overview at `man 7 udp` &gt; All errors documented for socket(7) or ip(7) may be returned by a send or receive on a UDP socket. Alright, `man 7 ip` &gt; EINVAL Invalid argument passed. For send operations this can be caused by sending to a blackhole route. A ha! Let's double-check the route. claire@rokh:~$ traceroute -s 127.0.0.1 8.8.8.8 traceroute to 8.8.8.8 (8.8.8.8), 30 hops max, 60 byte packets connect: Invalid argument That looks familiar. claire@rokh:~$ strace traceroute -s 127.0.0.1 8.8.8.8 {...much noise related to loading dynamic libraries...} bind(3, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr("127.0.0.1")}, 28) = 0 setsockopt(3, SOL_IP, IP_MTU_DISCOVER, [0], 4) = 0 setsockopt(3, SOL_SOCKET, SO_TIMESTAMP, [1], 4) = 0 setsockopt(3, SOL_IP, IP_RECVTTL, [1], 4) = 0 fcntl(3, F_SETFL, O_RDONLY|O_NONBLOCK) = 0 setsockopt(3, SOL_IP, IP_TTL, [1], 4) = 0 connect(3, {sa_family=AF_INET, sin_port=htons(33434), sin_addr=inet_addr("8.8.8.8")}, 28) = -1 EINVAL (Invalid argument) {...more noise related to internationalizing the error message...} It seems like it's not possible to route from `127.0.0.1` to `8.8.8.8`, and that error is reported with errno 22. Maybe that makes sense already. If not, it's a good question to ask. (It's not possible to route them because that would mean sending a packet with a return address of `127.0.0.1`. You *could* send it anyway, but don't expect that packet to be delivered. And even if it was, the other end won't know where to send a response.) Even without finding the answer in the documentation or 100% understanding what a black-hole route is and how to check for one, it's possible to observe the OS's behavior and make some decent guess at how to fix the problem. `strace` is the tool /u/WellMakeItSomehow was using and it helps a *lot* to have some familiarity with it. 
To clarify on Debug vs Display: Debug is for a straightforward representation of your type, i.e. directly showing the fields of a struct or the elements of a vector, or the values in a set. This is used for, well, debugging. You use this when you want to quickly see what something is, and is useful for something like logging. This can easily be derived because it all follows a predefined format: A struct just looks like how you would create a struct, etc. Display is a "nice" representation of your type. This is meant for user-facing types. For example, you might want to show a duration to the user as `12h32m42s`, not `Duration { hours: 12, minutes: 32, seconds: 42 }`, or whatever the internal representation happens to be. This cannot be derived (without more details) because *you* are deciding how your type should be shown to the user, and you might want a different representation for different types, even if they have the same structure.
Hmm, there's networking stuff in the Rust stdlib?
just give us the reddit &amp;#x200B;
Yupp! Check out [std::net](https://doc.rust-lang.org/std/net/index.html).
`strace` is the bee's knees :-). But I have to admit I spent a lot more time on this than I should have taken me to realize what the problem was. And `sockaddr_in`, you should know I didn't miss you at all.
https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1af565a6b505f0e005fc7e7dbe3eafba
oh rust doesn't like unterminated comments good to know
Isn't that the same as Java's `synchronized(..) {..}` block?
I just looked through that thing, and I gotta say, anyone who has the patience to go through something that long and inconveniently written (for scrolling) and find typos and grammar mistakes, by gods, they deserve an up vote. Bless you man, bless you.
Hey all, thought Iâ€™d share my latest mad science technique for making web assembly web components. This problem has stumped me for awhile, but I think the method I went with is least crazy. Basically instead of es6 web components,I use html events to notify wasm when a new web component has been created. From there you can listen to various lifecycle events of a web component. Itâ€™s a really tricky problem to route callbacks from JS side to components but I think I made it somewhat more logical. Iâ€™m curious if I can make this more ergonomic without too much framework.
Yeah, the functions in the `imageops` module don't compose very well. That's one of my laments with the API design of that crate. It still serves its purpose well enough and the maintainers have worked hard to cover as many use cases as they can.
Thanks for the information! I honestly just want basic machine learning with tensorflow, so all of your information is great!
Looks really nice, thanks for sharing!
That name could be [somewhat confusing](https://en.wikipedia.org/wiki/NZB).
I strongly agree with this, and I have used this strategy in many projects. Using a helper struct or simply some helper functions to access elements from the array will be much simpler (and maybe faster) regarding both your code and the compiled code.
Thanks ðŸ˜‚
You can implement `Index&lt;(usize, usize)&gt;` and `IndexMut&lt;(usize, usize)&gt;`, but an iterator interface might be better in the long run.
I think it's super cool that you're here taking feedback from the Rust community. Best of luck with your language!
This is probably obvious, but as a beginner, one my biggest mistakes was deriving as many traits as I could for every struct I wrote. Remember, traits are exactly what they seem: they are traits of the entity. Implementing Copy for a struct means you are telling the compiler that a value of this type can be arbitrarily duplicated without consequence. This may not matter at this point but it may later on. For example, numeric types can be copied around without much effect to your program (in fact this behavior makes things much cleaner) but for the state of a game board (Conwayâ€™s game of life?) a handle to a file, this behavior could cause problems. 
I'm a little torn on scientific Rust. IMO the ultimate end-user language for almost all sciences outside of CS is Python. Interactivity is key! Where I'd like to see scientific Rust head is to the library level. We have ndarray as a numpy competitor, but I don't see too much reason to not just use numpy. What I really want is a tensor math library with flexible backends for CPU, GPU, and distributed. AutumnAI had the right idea but gave up too soon.
Indeed. It depends on what the op needs to do with the struct in the long run though. Keep in mind that indexing does bounds checking, so it will be more expensive than an iterator. However, if you choose an iterator, you will need to add your own method/trait for enumerating by coordinate if you still need the index of the pixels. You could also just map the enumeration with that formula for converting index to pixels. On a side note, would you consider implementing Index and IndexMut with both (usize, usize) and just usize with different implementations (one by coordinate, one by raw array location) bad practice? This is not very relevant to the opâ€™s question but I used this strategy within a personal project of mine was wondering if that was a bad idea.
Now that you mention it... yeah I think Iâ€™m mistaken about OpenACC. It seems to have a different model and different directives than the OpenMP model, and you wouldnâ€™t compile the same code with both? I learned how to do this from old Nvidia â€œOpenACCâ€ tutorial, but I remember testing my OpenMP code on the PGI compiler, and with some combination of flags I was able to get automatic GPU acceleration. Anyway whatever itâ€™s called thatâ€™s what I meant.
And last, moving = selling the painting. You no longer have any way to access it and the new owner can do everything you were able to.
This sub is for the Rsut programming language. You want to post in /r/playrust.
&gt; However, if you choose an iterator, you will need to add your own method/trait for enumerating by coordinate if you still need the index of the pixels. Yeah. I suppose one of the algebra crates already has such a type, but I've never looked into them. &gt; On a side note, would you consider implementing Index and IndexMut with both (usize, usize) and just usize with different implementations (one by coordinate, one by raw array location) bad practice? [...] I used this strategy within a personal project of mine was wondering if that was a bad idea. If you need that, go for it, it will be easy to change it later (thanks to the compiler). I don't think I've needed both indexing forms at once, but that doesn't mean anything. The only thing you might want to be aware of is that it's sometimes useful to leave a bit of slop at the end of each line (stride vs. width). It makes SIMD easier, and SIMD (and SoA vs. AoS) tends to break all these nice abstractions we're used to. And if you do that, you might also want to use a planar format (3 matrices for R, G, B instead of one with structs), or to shuffle your data to one during processing. So I say don't worry about it now.
Not too helpful, but I debug with CLion and VS Code on Mac OS and it works well. CLion is more polished / requires less configuration.
Thank you very much. That was very informative. Especially the tools. The numbers tell me nothing. Completely different world. Icould try to read ther docs, but never strace or the tools you mentioned. Maybe I'll need them more as I go deeper into networking with rust. Thanks.
Sorry, I reasoned wrongly. Disregard what I said about the cause, I clearly don't know the inner workings well enough for that. But to help you mental model, think of calling `self.child()` as copying the mutable reference to the function and binding it to `self`. There are now two mutable references to the node (`self` in the outer function and `self` in the inner function), but since they aren't both in scope, it's fine. If the inner function does not return a reference, the second mutable reference is dropped when the function ends, so there's still only one mutable reference afterwards. But in this case, the second mutable reference is manipulated and then returned to the outer function so that there are now two mutable references. So references aren't swapped but rather copied. If they are mutable, then one of the two will need to be dropped. Unfortunately the compiler can't always figure out lifetimes in branches properly. It recently got a a lot smarter (without which none of these code snippets would have worked), but the rules have gotten more complicated as a result. I think the issue with calling the child function is that the reference is wrapped in an `Option`. Since the compiler does not know that the `None` variant doesn't contain a reference it thinks that the `else` branch might contain two references. But using `ref mut` makes it so that the reference is only stored when the pattern matches. I'm not entirely sure of that though.
***laughs in Go***
Many many thanks - this is great. I like the second suggestion with the macros. It gives me some ideas of how I can expand my `Widget` library with macros as well. This should make it easier to program for soon.
&gt;I suppose one of the algebra crates already has such a type I was trying to go with the DIY approach the op seemed to be going with. There's plenty of image processing libraries out there, so I would assume that this is for learning purposes. That said, you're 100% correct in that using a crate would likely be the better approach here. &gt;The only thing you might want to be aware of is that it's sometimes useful to leave a bit of slop at the end of each line (stride vs. width). It makes [SIMD](https://en.wikipedia.org/wiki/SIMD) easier, and SIMD (and [SoA vs. AoS](https://en.wikipedia.org/wiki/Structure_of_arrays)) tends to break all these nice abstractions we're used to. And if you do that, you might also want to use a planar format (3 matrices for R, G, B instead of one with structs), or to shuffle your data to one during processing. I have heard of SIMD and dealt with it a bit when I was researching LLVM, but I haven't really encountered it in Rust before. Could you point be to some resources or crates that deal with this? Thank you for bringing it to mind though! You have been very helpful to me as well as the OP (though I can't speak for them, I'm sure they would agree)
OpenACC was kinda meant as a short term solution for heterogeneous compute solutions (offloading to the GPUs) until OpenMP got it's offloading story further along. So, you can use very similar style of pragmas for OpenACC that you might see with the more modern OpenMP 4+ versions. However, OpenACC has really only been supported by PGI while OpenMP has been supported by a number of different compilers. Now that OpenMP has really improved their offloading capabilities, I would say in version 4.5+, OpenACC has become less and less relevant. So, you were correct in your original statement about using OpenACC if you'd been using a PGI compiler.
Wrong sub friend 
Where is the right sub mate
You probably want to post this on /r/playrust. This is the subreddit for the programming language Rust.
/r/playrust
Cheers boys 
Oh, that's interesting. Does it mean that implementing `Drop` is actually a breaking change?
Thanks!
Currently that is not the aim of this project unfortunately
Hmm I will add it as an issue.
"as" dosen't convert between types. It reinterprets the memory as that type.
&gt; Ruster(not sure if that's a word) We usually go by Rustacean (a pun on crustacean) :) I think (interactive) debugging is a still a genuine pain point with Rust. Work is ongoing to improve the situation, but I believe it has been lower priority than it might otherwise have been because with the compiler checking memory safety, thread safety, and restricting mutation to a single location in memory, most people find they rarely need a debugger in Rust. Don't be afraid to reach for `dbg!()` (and `#derive(Debug)`). It might seem primitive, but in practice I find it to be highly productive. I expect debugger support will catch up eventually.
This is pretty interesting. Great way to practically learn rust!
Could you point me to the docs where it says that using this function is UB? because I am 100% sure they wouldn't put a function in the library that was instantly UB, so..... hmm:/
That's not good. But there's a way to fix it: Call `drop()` on a variable as soon as it's no longer required, not when it goes out of scope. See this code as an example: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5c649fcb5bffcf9d18019fba99095dcb](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5c649fcb5bffcf9d18019fba99095dcb)
Thanks, I'll try
[Ensembl](https://en.wikipedia.org/wiki/Ensembl) is a genome browser and the highest profile bioinformatics project in the world. This is a great coup.
This subreddit is for the Rust programming language. You want r/playrust.
'docsets' is a folder generated in the working directory.
Suggestion: validate sub-command. When you copy to USB that you got on the cheap from Amazon or some old hard drive that has been in the humid basement for the last 3 years, it would be good to check that all the files read ok and md5sum of the file on USB matches that in the source. Thus you can be sure that all your data is verifiably ok. Make sure it only uses hashing and doesn't skip if date / size is the same -- the idea is to validate all files by reading and hashing src and destination. Nice tool -- I'll give it a try!
Added as an issue, thanks!
Real missed opportunity to call it A Tock On Titan
Robin, Jeremy, et al, &gt; + Current status of the Arm port &gt; ++ Code continually checked into â€œaarch64â€ branches for each Redox component on gitlab How do you foresee merging these [active Redox kernel branches](https://gitlab.redox-os.org/redox-os/redox/branches/active) will go? How are the other components' changes going? (I didn't see aarch64 stuff for ion but [relibc aarch64 related merge requests](https://gitlab.redox-os.org/search?group_id=&amp;project_id=64&amp;repository_ref=master&amp;scope=merge_requests&amp;search=aarch64) seem to have gone well, afaict.) What plans do you have to work up the stack on top of [jD91mZM2's network patches](https://gitlab.redox-os.org/redox-os/redox/issues/1184)?
Youâ€™ll need some kind of dev kit otherwise it would be trivial to backdoor an end user titan
Until you have to do something simple like format a date and you're stuck wondering who thought picking an arbitrary date to format against instead of standard formatters like "yyyyMMdd" was a good idea. I'll forever be mad about this
This is awesome, thank you! ðŸ‘ðŸ‘
Oh, I see. Were you planning on storing glyph paths inside `&lt;defs&gt;` and then referring to them with `&lt;use&gt;`? This would save space in the SVG. Also, one could also imagine a caching renderer that would cache rasterized versions of objects inside `&lt;defs&gt;`, which would basically be a glyph cache.
*installing now* This seems absolutely perfect. Thank you for making this 
Wow this is seriously cool, both in terms of the software itself and the fact that Rust + WASM is part of the stack.
Not necessarily though? Like, you could have the secure element control firmware updates and just wipe everything if you flash it. New firmware, new private key, all your old tokens are dead. 
RemindMe! 2 days
I will be messaging you on [**2019-03-19 05:59:25 UTC**](http://www.wolframalpha.com/input/?i=2019-03-19 05:59:25 UTC To Local Time) to remind you of [**this link.**](/r/rust/comments/b1ih91/v_language_new_programming_language_inspired_by/eipqr7e/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[/r/rust/comments/b1ih91/v_language_new_programming_language_inspired_by/eipqr7e/]%0A%0ARemindMe! 2 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! eipqtox) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
In Cargo.toml you set license to MIT. Why LICENSE file is Apache one?
I get that TCP sockets are the underlying structure that epoll and therefore Tokio is based on. However, given that TCP seems to require an event loop, yielding, and timers, what if Tokio operated directly on an IP socket and utilizing timers, async/await (i.e. futures), we made TCP connections as if they were any other application middleware? While bugs would need to be ironed out, it seems like it would not only improve the performance characteristics but seems like it might dramatically simplify the overall implementations (i.e. no epoll, mio, or non-protocol specific tcp code, minimally more involved Tokio code).
RemindMe! 2 years
Fantastic work. Thank you so much for doing this.
Or, you know, if they fall in love with C and only want to do things C is good at, then I *would* be skeptical that they'd love Python in the same way and as much. That said, Go has less to like about it than Rust does. If you use C, there are valid reasons to use Python, just like with Rust, there are valid reasons to use Go. I do think, however, that those reasons to use Go will be dwindling, that is, Rust's async story is going stable. So there isn't much a Rust lover will want that Go has and Rust doesn't. Python and C, though, do have mutually exclusive features/philosophies. I mean, Rust and Go do too, but I'm not really gonna jump ship to go because of it's low bar in getting started coding. Especially when libraries take time to learn, and I prize correctness and static checking. Really, this isn't the equivalence you could have done to argue against my point there. Tell me what else I'd want to use Go for. Tell me what else Rust can learn from Go. Preferably stuff so un-Rust-like that it would be better to abandon Rust wholesale in favor of another language.
As a biochemist and huge Rust fan, this is awesome! 90% of the code for my PhD project (analyzing proteomics data) is written in Rust.
*OwO, what's this? * It's your **2nd Cakeday** ludee0! ^(hug)
I've been following Rust since 1.0, but haven't done an actual big project yet (annoyingly). Simply by continually reading stuff here, I actual know quite a bit, in addition to reading the book, which is fascinating and we'll written. Some stuff I obviously don't understand, especially some really technical/comp-sci-y stuff that I try to read casually. You don't really need to understand. My best recommendation would be to read the book and try a bunch of programming exercises in Rust. The compiler is amazing, and tells you precisely what's wrong with that error number, so it's really easy to debug whatever you're working on (in comparison to eg OCaml, which just says syntax error ðŸ˜–). In addition, casually browse this subreddit from time to time, or maybe read what people ask in the help megathread, and see what you can understand. Or you could work through a big project. I think that with Rust, it's just more intimidating than other languages. With other languages, they hide their complexity, and it pops up when you're neckdeep in a project working with several libraries and setting up tests. Rust front loads that and is straightforward with you; you have full control. It's intimidating, but loads better, and I constantly wish I was writing Rust instead of whatever other languages (eg C), which I just end up writing Rust-like anyway. Good luck!
What's also interesting is that both the third and the first have their to u8 on the println, but the second one doesn't. In any case, I'd expect the compiler to optimize both at runtime. But whatever, it's undefined behavior.
I'm not sure it will work. Mainly because in many cases text should be represented as a single object (for gradients, patterns, decorations). Anyway, SVG usually contains a little of text, so it will not be a performance problem. At least I never saw an SVG with a lot of text. At the moment, filters are far bigger problem.
I would love a stream where you implement traceroute, don't think there's any library for that yet.
I understand your concerns. Petgraph is nearly not maintained anymore. The algorithm refactor PR is there for nearly half a year not merged. I actually is happy that someone finally started to make a new crate.
Functions can be coerced to zero-sized closures. They emphatically are *not* closures. They're items (part of a module or block) while closures are expressions of an unnameable type.
`impl`s have a global effect, no matter where they are located. A type either implements a trait or it doesn't. Don't use nested `impl`s. The only time you'd need to is when implementing a trait for a nested/block scoped struct, enum, union, or trait object and if you're doing that, you should put it into a module all proper.
Thank you for the feedback! I've added a section in the README. I had extreme tunnel vision about how everybody would know the product when it is in fact, quite niche.
It depends on a dummy assembler instruction. https://github.com/rust-lang/rfcs/issues/1484. It shouldn't even push/pop the stack. I'll test this, I want to know now :)
&gt; "openldap" which has bindings to C library, hasn't been updated since March '17. That doesn't have to be a problem, if the API of the C library is stable.
[The bindings](https://github.com/coder543/rust-cldap/blob/master/src/lib.rs) don't appear to have anything but bind and search. So I could indeed open LDAP connection and search LDAP objects, but not modify or create anything.
Generally working on raw IP packets like that requires administrator rights on a system so that can't be done in most cases. But doing this all in userland is something that is done sometimes. A lot of the faster network cards (100gb) require bypassing the kernel for everything in order to achieve full load.
Okay, that's "slightly" limiting :) I was just saying that in general, a binding doesn't have to be updated every month since most fixes are likely made in the underlying library. But it's different if it isn't reasonably feature complete.
wat, you're supposed to pick how you want it formatted yourself
Thank you! This will help a great deal! 
I'll see if I can manage using imgui, will post my wip if I start making reasonable progress
In other languages when you're feeling kinky you develop an HTTP server at TCP level just for the lulz. In Rust you implement TCP itself...
\&gt; a couple of other things &amp;#x200B; This includes every function marked with the unsafe keyword, including FFI bindings and things lite transmute.
This is fantastic crate, thank you! But... how can I run my binary with parameters? Like â€˜cargo run â€” -s ome pa ra me tersâ€™?
Perhaps it's dual licensed?
The syntax with `mut cursor := 0` is nice and I wish Rust would pick it up to reduce a bit the visual noise.
Why not start one yourself?
^ This, if /u/keeperB5 is up for it Iâ€™d 100% be down to contribute. 
Hot tip: open Instruments profiles in https://www.speedscope.app/
/r/playrust
Out of curiosity, how constrained did you feel due to the relative lack/immaturity of scientific computing crates? And what libraries did you end up using?
Good point about the local clock on the Rust Playground server. UTC is in fact the local time on that server. However, here's the problem. When I run [this code](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7368b9a36300558a89b0c057a0f33c03) (Rust Playground link) on my server, where the local time is not UTC, I get what seems to me to be an inconsistency or outright error. Here is the output of the code on my server: Utc::now() = 2019-03-17 11:03:44.796861045 UTC Utc::now().naive_utc() = 2019-03-17 11:03:44.796861045 Utc::now().naive_local() = 2019-03-17 11:03:44.796861045 Local::now() = 2019-03-17 07:03:44.796871660 -04:00 Local::now().naive_utc() = 2019-03-17 11:03:44.796871660 Local::now().naive_local() = 2019-03-17 07:03:44.796871660 I was expecting the result in the third line above (for `Utc::now().naive_local()`) to be the same as the result on the last line (for `Local::now().naive_local()`) when the local time is different than UTC. I thought that was the purpose of the `naive_local()` method. 
I think the only way that you can achieve memory safety with no GC and without Rust's borrow checker's complexity is by adopting linear (or even ordered!) types in the language. Somewhere on the HN comment thread, I thought I saw a comment that says that the author thought that restricting mutability to method receivers would help in statically analyzing memory safeness, which hints to me that the author doesn't really understand substructural logic and is trying to make up random restrictions in the language in hopes that it would simplify the check. At this point, I'd say that the author vastly underestimated the work that has been done on Rust's borrow checker and overestimated their ability to improve upon the status quo for borrowck. Nonetheless, it is a great initiative and I admire the efforts put into it, but it *really* needs work. A lot more.
Probably worthwhile to fork one of the projects and continue it, or, ask the maintainer to transfer rights to you.
How many tcp sessions are you planning? 
I believe thatâ€™s what [asdf](https://github.com/asdf-vm/asdf) aims to achieve.
This situation reminds me of https://www.xkcd.com/927/
Iâ€™ll give this a spin today. Thanks!
Py, rust, wasm, and -while not my favorite, but still a solid choice- react, webgl... Is this a dream? Ensembl you rock! I hope other genomic platforms will follow you! 
I'm working on a new project and have got to the point where I need graphing support. I was going to use petgraph, but I share many of your concerns about the documentation. My graphing needs are relatively simple and your documented examples are clear and very easy to follow. I'm going to use graphlib. Thanks for making it available.
I've never done it - is it that much work to simply use the c lib directly?
For a lot of my projects (embedded, mostly) a debugger quite literally is impossible. Heck, even using print statements is often impossible. When I can use a debugger, I often reach for GDB. I don't usually use the rust-gdb script, but that is there. Here is a good blog on how to use gdb for Rust. http://smallcultfollowing.com/babysteps/blog/2018/09/21/office-hours-0-debugging-with-gdb/
&gt; I don't know if it will switch to dynamic dispatch. Dynamic dispatch only happens if you call a trait method on a pointer to `dyn Trait`, something like `&amp; dyn Trait` or `Box&lt;dyn Trait&gt;`. (For reasons of backwards compatibility those types can also be written without the `dyn` keyword.) You can only create a pointer for dynamic dispatch if the trait is "object safe," meaning that all methods can be called dynamically: - methods can't return `Self` - methods can't introduce type parameters, except that - (if I understand correctly) methods *can* introduce lifetime parameters This method doesn't have a type or lifetime parameter. It simply refers to a parameter declared for the trait. The type system remembers the two lifetimes declared for an instance of `&amp;'b mut dyn Trait&lt;'a&gt;`; that's completely anagolous to `&amp;'b mut Vec&lt;&amp;'a usize&gt;` and in both cases the type system understands when methods interact with the inner lifetime.
Const fn, why does this not work? struct Foo&lt;T&gt;(T); impl&lt;T&gt; Foo&lt;T&gt; { pub const fn from((x, _): (T, i32)) -&gt; Foo&lt;T&gt; { Foo(x) } } Does not compile: "constant functions cannot evaluate destructors" However no destructors are being evaluated! Note this only happens if I destructure like this, all potential instances that may be destroyed (T) are always moved back out so why is this an error?
My guess is we'll do one or two more :) 
`for&lt;'a&gt;` is more restrictive than making `'a` a parameter of the struct; it means "for any lifetime no matter how long or short". So it's not useful here because the only ways to return `&amp;'a` for any `'a` are to return `'static` or risky unsafe business conjuring a lifetime from thin air. `for&lt;'a&gt;` is very useful when a method takes `&amp;'a` as an input. It promises to the caller that the reference can't escape to unrelated storage: any lifetime no matter how short.
Iâ€™m somewhat surprise to see such differences in benchmarks, for a workload that I would expect to be IO-bound. How much of that do you think can be attributed to multi-threading? How does `--sequential` mode compare, in these benchmarks? How about with forcing rayonâ€™s thread pool to a single thread? (`--sequential` appears to use a different code path.) Or could the different access patterns affect the kernelâ€™s IO cache? Do benchmarks change significantly if you run `sync` after each command? (And include it in the timing measurements.) Speaking of caching, did your benchmarks use hyperfineâ€™s warmup run or cache-clearing functionality?
&gt;Maybe I'll need them more as I go deeper into networking with rust. Thanks. You'll certainly need them for networking or anything similarly os-flavored on a unix-like. Well, maybe "necessary" is too strong a word, but it'll be more difficult than necessary without them. (Aside: out of curiosity yesterday, I sat down with an overview of how debuggers work. It turns out that strace uses the same API as debuggers, the `ptrace` system call, which lets the tracer program inspect and manipulate a thread much like the "front panel" controls of early computers. So `strace` tells the os "pause before and after each system call" and uses that pause to read arguments and return values out of the threads' CPU registers.)
Hi, `ldap3` author here. I wouldn't call the crate abandoned; I just don't want to spend time and effort on porting the code to the more recent `tokio` revision, only to do the same thing again when async/await stabilizes. The dependency on `tokio-core` is unfortunate if you want to drive everything asynchronously _and_ from a single event loop, but you can always split LDAP interaction to a separate thread. Moreover, the crate works well enough for synchronous connections, if you keep in mind that the connection struct is non-`Send`. As for incomplete LDAP support... What do you feel is missing, aside from intermediate responses and in-library referral chasing, both rather niche features in an already niche protocol?
That is *strange*, but I don't know enough about how debuggers (and specifically llvm with the Rust wrapper) work to explain it fully. It would be nice if the debugger can call `&lt;$VarType as Debug&gt;::fmt` to format a value for me. But that is a bit dangerous - that method might be implemented with arbitrary code which isn't necessarily async-safe. It *should* work. So the debugger - I'm imagining a Rust-specific and safety-first debugger here - could sandbox that call somehow, even putting it in a separate task with memory mapping tricks to guarantee that it can't have side effects on the program being debugged. `reqwest` is single-threaded, right? If so I think that situation sounds like a bug in lldb, at least in how it interacts with Rust. If not, maybe there's some option missing related to multi-threaded debugging. And maybe the optimizer is reordering when the request starts, so there's some uncertainty about the relationship between program counter and source line. But it sounds like a bug.
/r/rustjerk
Not all options are listed in the readme. You can forward args with \`--args\`, e.g: \`cargo instruments --args foo bar "-o file.fmt"\`
oops, I must have copied the wrong license out of another project. Intended license is MIT.
It looks like you're manually scanning the filter over the image. Convolutions are often implemented in practice by first applying a Fourier Transform to the image and filter. In Fourier space, the convolution becomes an element-wise multiplication. Then you do an inverse Fourier transform to get the final image. This reduces the time complexity from O(n^2\) to O(n*log(n)). Still though, it might not pay off at this scale. It'd be cool to see a performance comparison. https://en.m.wikipedia.org/wiki/Convolution_theorem
Desktop link: https://en.wikipedia.org/wiki/Convolution_theorem *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^244883
Some previous discussion [here.](https://www.reddit.com/r/rust/comments/aofoj1/project_ideas_specifically_for_rust/)
...or talk with one or more of the existing crate authors about contributing?
The way you do that in go is by modeling it after some arbitrary date rather than identifiers (like the identifier for a year number is y so four character year is yyyy). In go that would be 2006 
For my first real learning project I picked a RPC API from the myriad of web services and just implemented every endpoint in a CLI app. I ended up learning quite a few useful libraries.
Thanks, that will do :) 
Haha! I don't really mind the namespace pollution with some obscure networking protocol which is probably not even in use anymore. ... Is what I thought initially, but apparently it's still a thing, lol
The example is so distilled I didn't want to assume what the author was doing. It could be that he used return type as an example but is using arguments in his actual code. For instance, if I were to answer directly about this code sample, I would tell him to just use \`i32\` instead of \`&amp;i32\`.
Your problem is the `::` after `stdin`. It should just be `io::stdin()`.
Thanks! The tutorial I'm reading had it wrong then.
I think you can use any date though? 2006 is just a common example date in Go. Been writing more Rust/Haskell recently so might need to refresh my memory 
Do you get that on any app (e.g. a hello world), or on a specific one? `LNK1181` sounds like "cannot open input file filename". Do you have any native dependencies?
I was tryd to compile some code that i took from example of GTK-RS. Another programms works fine. Now i took just full example and tryd to compile: Did that mean that GTK-RS works only on GNU toolchain?
Julia also provides an interactive environment, and looks much better for "scientific" work than Python, at least at a language level. It's almost two decades younger than Python, so of course it will take some time to catch up. I know it's way behind right now but my long term bet in this space is on Julia. &amp;#x200B; Rust the language still lacks some important features vis-a-vis C++, its main competitor in scientific libraries. Const generics and specialization are the main ones I can think of. I read that those are high priority items for this year, but who knows if anything will happen? &amp;#x200B; I agree about Autumn. Maybe someone will pick up the project and run with it? There's a Swift TensorFlow project headed by Chris Lattner that's pushing Swift for TF (obviously) but I think Rust would have been a fine choice. At least Rust runs on Linux and Windows, whereas Swift is mostly a Mac only thing.
Yes, you'll need the GNU toolchain, and also [GTK](https://www.gtk.org/download/windows.php).
Can you give some links how to setup gcc for gnu? I can't find anything 
&gt; Yes, you'll need the GNU toolchain Why is that? GTK itself is [buildable with MSVC.](https://wiki.gnome.org/Projects/GTK/Win32/MSVCCompilationOfGTKStack)
I personally don't feel constrained at all - *but* I love writing my own libraries. Learning how things work drives both my scientific and programming pursuits, so I like understanding how the various libraries out there work by writing my own. It's rare for me to use any dependencies outside of my own projects in any of my projects. So far I have used Rayon and indicatif. The data I work with is also generally well structured, and the majority of processing power is spent doing numeric computations (K-means, dbscan, averaging, normalizing, matrix transpositions etc), so Rust is an ideal solution. In my case, a `HashMap&lt;&amp;str, Vec&lt;f64&gt;&gt;` or `HashMap&lt;&amp;str, HashMap&lt;&amp;str, Vec&lt;f64&gt;&gt;` is significantly easier to work with than a Pandas Dataframe. The only thing I really miss is something akin to matplotlib. My project involves generating 100,000's of bar graphs with fixed variables (all have 2 groups of 5 bars, Y axis from 0-10) - and this takes an ENORMOUS amount of time to do with MPL. I wrote my own bar graph crate which was orders of magnitude faster but ultimately ended up going back to matplotlib because it's pretty. Ultimately I'm not too sure how much of usage Rust will get in the scientific community (biology oriented at least). I **love** to program, and have been doing so for 10+ years, so lifetime semantics/borrow checker, static typing, structs, tagged enums are major pluses in my mind. I just see a very large activation energy to get scientists (who generally don't like to program) to make the switch. I'll do a full blown post and publish some more of the crates I've written later this year (after paper is out.... hopefully)
Wrong sub, see /r/playrust. This sub is about Rust the programming language,
Probably none of the gtk-rs contributors so far have cared enough to write and debug the `build.rs` extensions necessary to detect and adjust for a build on the `msvc` triples.
cheers &amp;#x200B;
Hi! Your [reply](https://github.com/inejge/ldap3/issues/29#issuecomment-424989573) in the GitHub issues kinda seemed to be worded to imply tokio-core is too much work, which I then interpreted to mean the project as abandoned. I am really glad to hear this is not the case. Your crate is the most feature complete out of the three by far. I have yet to try any of the ldap crates because they all seemed like a dead-end. Now that I know otherwise, I will certainly give ldap3 a go! Thank you for your work so far.
In addition, I don't know if OP wants to build their own GTK.
Funny you should mention this now. One of my first Rust projects (and the only one I've published so far) is a [project template for writing CLI tools in Rust](https://github.com/ssokolow/rust-cli-boilerplate) which I just started working on again a few days before you posted this. The one caution I have about it is that, when I started it, `error-chain` was the big thing in error handling and I haven't familiarized myself with `failure` yet, so I've been leaving migrating off `error-chain` for later while I bring other things up to snuff which I can get done more quickly. At the moment, I'm just writing the last few tests in `test_justfile.py`. (I've been writing the supporting tooling that's not part of the template in Python but, with the possible exception of the "`cargo-generate` is incompatible with justfile syntax, so invent my own 'new project' templater" script, I'll probably rewrite it all in Rust once I move the template into a `template/` subfolder so I can have a separate `Cargo.toml` in the root of the template repo for the tooling.)
Reimplemnting *nix coreutils is pretty good as you get exposed to using most of the features the language has to offer. 
I can't even find the error message they said they got in the any of the GTK-RS repo sources... curious about which aspect in came from.
The error message comes from the linker because it can't find the GTK binaries.
I love writing rust and I love the correctness guarantees I get! I think rust is a good choice she you're moving from prototype to production, so it is a good choice for translational research (my area). I actually do my prototyping in rust as Well, but I'm not sure it's a good choice for everybody - it's nice to have a repl so you don't have to keep compiling everything. I even use rust for things like cmd line scripts.
 [RFC 911 `const fn`](https://github.com/rust-lang/rfcs/blob/master/text/0911-const-fn.md) says: &gt; Only simple by-value bindings are allowed in arguments, e.g. x: T. While by-ref bindings and destructuring can be supported, they're not necessary and they would only complicate the implementation. and &gt; - struct/enum values are not allowed if their type implements Drop, So what you're asking for is not currently part of the language. The compiler is only checking the signature of the function and isn't checking if the body actually needs to call `drop` or not.
I meant: &gt;compiler said to change my MSVC ABI rust to GNU.
Check out the official tutorial. It's how most people learn Rust. https://doc.rust-lang.org/book/
I can see it picking up steam in the HPC space a few years from now. I've got to find the time, but I'd like to help in this scene by copying the design of some of the new libraries being built for next-gen HPC clusters and making Rust equivalent libraries. I could also see Rust being used to write a lot of the kernels that would be used in scripting languages like python or julia. However, I agree with you that it's probably not going to be used to write your typical scripts used by most scientists.
You want /r/playrust
Ty I am new to reddit
You do, however, fail to read the sidebar. This isn't that Rust. This is the programming language Rust.
Oh okay I am sorry sir
What is the difference between "impl trait" in function signatures and such "plain trait"?
Not to bikeshed too hard, but would it be possible to use the same pattern as other cargo tools, using `--` as a separator? e.g. cargo instruments --plugin_args -- --binary_args Similar to cargo-test, cargo-run, cargo-bench, etc. 
Honestly I'd love to, I just couldn't figure out how to get it to work, and I wanted to ship a 0.1. Is definitely near the top of my list though!
Do you have any examples of the HPC libraries? I'm not too familiar with that space and I'd like to learn.
If only https://gobyexample.com/time-formatting-parsing
&gt; rustfmt is an opinionated code formatter for Rust. In my opinion, opinionated formatters are the way to go. Thereâ€™s no arguing about code formatting when everything adheres to the same standard. That's overly optimistic. I'm so used to maintaining my coding style by hand that my reaction to a code formatter that can't be configured to follow my style is to throw out the code formatter... and I can't be the only one who thinks this way. As-is, I run `cargo +nightly fmt` infrequently (maybe once a week) to catch overlooked stylistic mistakes, then use `git gui` to commit the changes I accept and revert the rest, because `rustfmt.toml` isn't quite versatile enough to encode my rules around where and where not to wrap long argument lists. (And it also doesn't seem to even *have* an option to disable tall formatting of my `#[allow(...)]` after configuring "warn on everything by default" for Clippy.)
This repo has certainly helped me!
So there's a ton out there and you can really go down a rabbit hole quite easily. I'd say that some major math libraries are PETSc, Hypre, Trilinos, MAGMA, FFTW, and SUNDIALS. A lot of these libraries also build upon MPI to do the cross node communications. I'm more familiar with applications related to finite element methods and the US's Department of Energy related projects, so I'm probably missing some international libraries that are also popular. A lot of DOE labs are also opening up a several of their libraries, so if you go through their Github or Bitbucket profiles that's another way to see how some of these HPC libraries can look. You might be interested in this project since its related to biochemistry: http://www.nwchem-sw.org/index.php/Main_Page 
I am by no means an expert in this field (so feel free to call me out on anything!), but by my tests, multi-threading has the greatest speedup in directories with large quantities of small files (e.g. node_modules, .git), where the accesses have a non-negligible overhead compared to file IO. Currently the performance worsens with copying large files, but this is something I'm working to improve. Using both `--sequential` or forcing rayon to use a single thread generally performs no better/slightly worse than their standard counterparts. My benchmarks use hyperfine's `--warmup 5` and `--prepare`. Adding `sync` to `--prepare` does not change performance very much.
Ah yes, that was further discussed [here](https://www.reddit.com/r/rust/comments/b0i625/classic_unix_utilities_make_great_beginner/?utm_source=share&amp;utm_medium=ios_app) .
&gt;In many machine learning applications, this can be solved by working entirely in the log domain. Unfortunately, this does not work for Baum-Welch inference because probabilities must be added together, which canâ€™t be done in the log domain. Hey, what? Check out Stamp's "A Revealing Introduction to Hidden Markov Models" for how to implement log-scaling for HMMs (including Baum-Welch). It even has pseudo-code for all of the algorithms.
How many c4 does a autoturret want to get destroyed
Thank you! Honestly I just followed what other Actix users were doing and I think it was a good way to take advantage of the way Actix generates error responses from that trait. It certainly made things a lot clearer, especially on the server logging side.
&gt;I think you can use any date though? I mean, how would that even work?
&gt; it also locks code, not data Nope, it locks using an object (so, data), like `synchronized` in Java. 
Hello everyone. Here is my easy question. Probably for @razrfalcon. I try to use resvg to render out a PNG file. Problem is I can't get my head around how do I provide rendering backend. I included resvg in Cargo.toml, but `cargo build --release --features "cairo-backend"` doesn't work (obviously) because that's for the whole project @razrfalcon provide on github (I quess). What's the correct workflow?
I downloaded &amp; installed the same spec last night and I figured the problem wasn't that VS didn't get installed correctly. The real problem is that you're supposed to install VSC++ build tools from Microsoft's build tools page so that rustup can utilize `link`. You probably downloaded VS's installer and attempted to get the VSC++ build tools with. Try the link offered by the rustup installer.
Just in case, I usually profile like this: &amp;#x200B; \`\`\` sample myproc -f myproc.data \`\`\` &amp;#x200B; And then I simply use [https://github.com/brendangregg/FlameGraph](https://github.com/brendangregg/FlameGraph) scripts to parse sample output and generate flamegraph which is super nice. &amp;#x200B; It provides less information than Instruments, but it is enough for my use cases.
You can use the log-sum-exp trick.
Think you're looking for /r/playrust 
Hey everyone, I'm trying to build my own shell as a learning project. I have a good amount of it done, but one thing I want to add is autocomplete and the ability to hit the up arrow to cycle through history. Currently I read lines from stdin then parse them, but for autcomplete I need to stop and do something if the tab or up arrow character is hit, immediately after, without waiting for a newline character. IIRC in C I did this with getchar(). Is there a way in Rust to just read in characters one at a time instead of waiting for the newline character? I've searched endlessly but cannot find anything. Thanks in advance.
Why does the this (admittedly strange) code ``` fn g() -&gt; impl std::fmt::Display { panic!("AAAA") } ``` give this strange type error ``` | 11 | fn g() -&gt; impl std::fmt::Display { | ^^^^^^^^^^^^^^^^^^^^^^ `()` cannot be formatted with the default formatter | = help: the trait `std::fmt::Display` is not implemented for `()` ``` What am I trying to do? I'm trying to explore what you can and can't do with `impl Trait`.
I'm really enjoying these! Keep them coming! 
Rust version? This is way deeper than I can comment on, but try to search/post to the rust bug tracker: https://github.com/rust-lang/rust/issues. Super good people there, and you'll get much better responses. 
In that case you may find https://github.com/ferrous-systems/flamegraph interesting :)
Oh shoot yeah I should have mentioned that. It's a nightly. I updated the post.
&gt; Later, when rustc needs to zero some stack memory, it uses movaps, which requires the destination to be 16-byte aligned, and I get SIGSEGV. This sounds like a compiler issue. Before using `movaps` it needs to force stack alignment in the function, as you can't make assumptions about your stack alignment safely. As an example, if you were to pass the function into a C function as a callback, there's no guarentee that the callback caller will respect the 16-byte alignment requirement. 
What tutorial is it? Most people who write tutorials are happy to have types corrected.
Sounds like a bug in nightly. Can you make a small program that reproduces this behavior? Afaik on x86_64 the stack *must* always be 16-byte aligned when a function call happens, so if a bug is making rustc is violate this it is no small deal.
For me the key projects were Conway's Life and a small text-only game. YMMV.
Ah, I see. You're mistaken about the purpose of `naive_local()`. The purpose of `naive_local`, per the docs, is to give the `NaiveDateTime` *for the `TimeZone` of the `DateTime` object*. `Utc::now()` returns a `DateTime` with the "UTC" timezone. Therefore `naive_local()` returns the local time in the "UTC" timezone, which is the same value returned by `naive_utc()`, by definition.
&gt; Afaik on x86_64 the stack must always be 16-byte aligned when a function call happens There is no requirement for that. It may be a restriction that compilers put in place, but there is no limit by the processor that `rsp` must be 16-byte aligned. Sometimes you'll see something like: push rsp ; Save rsp and rsp, 0xFFFFFFFFFFFFFFF0 ; Force alignment sub rsp, 128 ; Size of local variables ... pop rsp ret which is boilerplate inserted by the compiler to align the stack before allocating space for the local variables. This would be necessary for instructions like `movaps` where you need 16 byte alignment, but otherwise it's in no way required. (note the example doesn't use `rbp` - that changes it a little bit, but it's the same general idea)
I feel like the `CC` crate (which GTK-RS does use) should easily be able to abstract away any minor differences there might be for cases like this. It's not much more than a matter of "are we looking for `.a`s, or are we looking for `.libs`".
Interesting blog post, but I think their solution was to fix some hand written assembly to setup the 16-byte stack alignment that gcc was expecting. They were using the default `-mpreferred-stack-boundary=num` setting and didn't need to change it.
Oh whoops, good catch. I should have read the post closer, you're correct. My brain was like, "yeah, setting `-mpreferred-stack-boundary` will fix the stack boundary."
Hey is any of your code open source? I just wrote my own k-means implementation and am trying to improve on it, I'd love to see if I can get any insights from yours. 
It's not a processor restriction, but an ABI restriction. The System V ABI requires that the stack is 16-byte aligned before the `call` instruction happens. Of course, this might be different on systems which use a different ABI.
It's because your saying return something that implements the Display type. The panic macro returns ```!``` which is the never type and can be coerced into any return type. This results in the type of the macro in this case have the type of ```()```. ```()``` doesn't implement Display and is why you get the error
Interesting, I've never noticed that. Regardless, for OP's issue the stack will still need to be re-aligned inside the function to account for stored registers and the return address being adding to the stack which can result in the stack becoming unaligned. Ideally the compiler should know exactly how much padding it needs to add, but the `and rsp, $padding` approach is safer in the event the function is being called from a non-conformant function.
It's probably not a tutorial but an [urlo thread](https://users.rust-lang.org/t/how-to-get-user-input/5176). Pro tip: a lot of what you find on the Internet is invalid.
You'll have to go outside or around the standard library because the default tty parameters won't give the program less than a line of input at a time. That is also true in C. You'll need [termios](https://docs.rs/termios/0.3.1/termios/) at least. 
The main difference IMO between Rust traits and Java interfaces is that in Rust, the traits implemented by a type are not "frozen" on type declaration and can be expanded by other code, as long as one heeds the orphan rule. Getting around this limitation in Java requires the use of a Decorator pattern.
Yes it is! My K-means implementation could certainly be improved - it was one of the first things I wrote in Rust. Tbh, I'm not doing too much K-means stuff these days, so I haven't gone back to revisit it. [K-means](https://github.com/lazear/rust-kmeans) [Dbscan (also on crates.io)](https://github.com/lazear/dbscan) I meant more about publishing the stuff for my project. I don't want to say too much besides that I'm working in a "hot" research area and we're developing new methods to study protein-protein interactions. The code is written specifically to analyze data generated by this method, and publishing it would allow a knowledgeable reader to figure out what we're doing. 
I'm excited about *both* parts of this project: the [KaTeX](http://katex.org) support looks great for documenting many things. Thanks for sharing this!
Note: My experience is with the ARM AAPCS; I'm not familiar with the C ABI on X86. I'm trusting the posters that indicate that the x86 C ABI requires the stack to be 16-byte aligned. If your _start sets up the stack, then it probably does not follow the C ABI. If your _start needs to realign the stack, then it definitely does not follow the C ABI. As a result, you will need to mark _start a `#[naked]` function. Based on reading the RFCs, you cannot safely have Rust code in a `#[naked]` function, so you will need its body to consist entirely of inline assembly. Since you mention having a `rust_start` function, you are likely aware of this already. In order for you to call a function from assembly, you need to know its ABI. Because Rust's ABI is undefined, you need to mark `rust_start` as `extern "c"`. Then when you call `rust_start` from `_start` you will need to make sure you're following the C ABI for the platform. There's an example (for ARM/Thumb, not x86) in [libtock-rs](https://github.com/tock/libtock-rs/blob/master/src/entry_point.rs), which performs stack setup in `_start` and calls to `rust_start`. Note that there are some known bugs in this code (and an open PR to fix them), but those known bugs are Tock-specific, so that shouldn't effect its use as an example for you. In Tock, the kernel expects `_start` to follow the C ABI, but `_start` is still marked as `#[naked]` because it resets the stack (it uses its own stack location rather than the stack the kernel gives it), and rustc/llvm probably isn't expecting inline assembly to do that.
Thanks for the help. I also found (linefeed)[https://crates.io/crates/linefeed] which handles the autocompletion stuff for you. May try that out too.
&gt; I'm not sure why this is; it seems like an aligned move would be faster, and thus be the choice of the optimizer. Depends on the CPU. The Agner Fogg instruction tables say they've been equally fast since new processors in 2011. AMD Zen(2017-present): No alignment penalty. 2 loads per cycle, 1 store. Intel Skylake (2015-present): No alignment penalty. 2 loads per cycle, 1 store. AMD Excavator (2015): Same as modern processors. Intel Haswell (2013): Same as modern. AMD Steamroller (2014): Same as modern. Haswell, Piledriver, Bulldozer (back to 2011): Same as modern Intel Sandy Bridge (2011): No alignment penalty. 1 load, 1 store per cycle. Intel Nehalem (2008): Significant (~2-4x) penalty any unaligned. One aligned load or store per cycle. (Probably can do both in a cycle; I haven't read the details.) AMD K10 (2007): 2x penalty for unaligned stores. One aligned store, two loads per cycle. Intel Merom (2006): 2x penalty unaligned load, 4x penalty unaligned store. One load or store per cycle. Intel Prescott (2004): 2x penalty unaligned load, 4x penalty unaligned store. One load per cycle. Two cycles per store. AMD K8 (2003): No penalty. Two cycles per load or store. Anything older is 32-bit. The SSE1 instructions are actually eight years older than x86_64 (Intel P6, 1995) and it probably mattered a lot back then. The K8 is interesting: no penalty but I suspect SSE wasn't any faster than 64-bit scalar instructions. Since `movaps` isn't needed, I would guess it's simpler for the code-generator's cost model to simply ignore that it exists. Both variants have the same length in generated code: two-byte opcodes plus the addressing mode.
exercism.io has been my favourite way to learn. Small problems to munch on, and you get to see how others solved the same problems to help you learn.
impl trait in function signatures is just syntactic sugar for a generic (with some very minor differences), so you will still get static dispatch.
This all seems pretty messy to me, and the fact that you need extern C and unsafe blocks even for a simple example turns me off from this
All web assembly needs externs, other frameworks just hide it. I show my examples using unsafe because it's less indirection of what's actually happening. The reader is free to implement their own wrapper functions, wasm-module focus is simply presenting a consistent api based off web IDL generation to the best as possible of what one can do to the DOM. &amp;#x200B; The added bonus of my API being the way it is is that one can use non-Rust languages against the same API. I know its not everyone's cup of tea but it is: * no magic * requires no code generation setup on Rust side * requires no knowledge of javascript glue together &amp;#x200B;
&gt; This results in the type of the macro in this case have the type of `()` Is this some defaulting rule? I don't see how you go from "ccan be coerced into any return type" to 'let's coerce to unit'.
Maybe you could support \`-- --foo\` and have that convert that into whatever is appropriate to give to the \`instruments\` binary (which I asssume is \`--args\`), and then if you want to actually pass flags to \`instruments\` itself then define a separate way to do that, such as \`-Xinstruments --foo -Xinstruments --bar\` the way compilers usually do for piping arguments through to lower-level tools?
Interestingly, your code compiles with the `never_type` feature enabled. The semantics of `!` are currently being worked on, so it's feature gated right now. My guess is that without it enabled, `impl Trait` doesn't want to have it's actual type be `!`, because you wouldn't be able to actually write `!` as the return type. As to why it's coerced to `()` specifically, I suppose it's because that's the type carrying the least amount of information.
Minor nit, but if we were using offsets to rsp where we didn't need/want to fix up the alignment, we wouldn't typically push/restore rbp. The stack would have whatever we sub'd for local variables, and the saved Code Segment (CS) register value and instruction pointer (RIP). (Apologies if I stuffed that up somehow, but I didn't want someone who might be reading along to ingest misinformation.) 
And for full measure, the Intel guide lists MOVAPS. (Though I just confirmed that the latency and throughput are identical between MOVAPS and MOVUPS at this point, per Intel docs.) MOVAPS is still favored over MOVAPD due to the latter taking one more byte to encode. Two (-ish, port 3 is also used for stores) ports became available for load operations as of Sandy Bridge.
You're correct. You only need to use `rbp` in my example *because* we're correcting the stack alignment at runtime. We don't know how many bytes we're skipping, so we can no longer use static offsets into `rsp` to access parameters (since they're above the unknown sized padding). I probably could have worded it better, but that was what I was trying to convey. 
FWIW I've encountered similar issues but only with Windows. On Linux the debug symbols seem to be properly embedded within the binary such that copying/moving the binary has no effect on backtraces.
As I read your "without the..." bit, you were describing a case where we weren't adjusting the rbp, and then said "8 for stored stack pointer", which sounded like rbp, rather than a segment pointer. I think we're on the same page, and hopefully anyone nerdy enough to read, understand, and care about that bit is now too ;-).
&gt; rather than a segment pointer Well, you don't usually use segment registers in 32bit or 64bit x86 - only in real mode or sometimes in 32 bit "unreal mode", but yes in those cases you need to store the `cs` register along with the `ip` register :P Now I want to write DOS code again...
What are your hobbies or interests outside of programming? Any industries you are interested in? 
This sounds like a follow-up to [#59171](https://github.com/rust-lang/rust/issues/59171), isn't it? The thing you are not aware of is the push of the return address. So, before calling your main you align your stack to 16 bytes (0x60), but then right before calling it, the compiler inserts code for pushing the return address (0x68). Next the `push eax` tries to align it again to 16 bytes (0x70). So the push is correct on rust side
Another pro tip: r/rust is no exception. I've seen lots of people suggest code that they haven't tested and that doesn't actually work. I've been guilty of it myself.
I learned assembly a while back. When the "flat" memory model was introduced, I was convinced it was *better* than sliced bread. At least the semantics that I recall are that we may not use segment:offset notation much these days, but "call" pushes both onto the stack, just as "ret" pops them both back off to figure out where to go on function exit. If that's not still true, I actually missed it, somehow. (I haven't written much assembly lately. Mostly just use it on ocassion to figure out what the heck the compiler has done with my code.) Fun side note: I was introduced to the concept of tail call optimization by reading a disassembly that "called" a function using jmps and esp-indexed variables. I still miss being able to use real mode sometimes though.
most of them are connected with programming in one way or another :) apart from some cool stuff like low-level/systems programming, I love music and learn playing bass. Also drawing, but I suck at that :D I've also got an idea to make a guitar processor pedal with Raspberry Pi or Arduino, and something like drum machine &amp; launchpad with Raspberry Pi. But those are plans for distant future
I suggest you to look at the Not-Yet-Awesome-Rust: https://github.com/not-yet-awesome-rust/not-yet-awesome-rust I think you can find a plethora of ideas there. 
I'm pretty similar to yourself, I work in the pro audio industry. Pedals are a hobby of mine. RPis and Arduinos are great, but have you looked at some of the ARM boards that can be targeted by Rust? It's a good project, lots of stuff to work on there, both in terms of audio and embedded systems in Rust. For example, the STMF32 (m4 and m7 core) boards are pretty cheap and you can target them with Rust, they have high quality audio IO and you have enough processing there for relatively beefy effects. 
Sorry your post wasn't received very well (going by the votes). Here's an idea you may like - https://www.reddit.com/r/rust/comments/b0whup/simple_project_idea_for_beginners_a_hex_editor/ I may be able to give you high level pointers on how to work on this
Thanks! Random guess: maybe the cost of doing syscalls (switching back and forth between user space and kernel space) is significant when touching many small files, but can be parallelized to some extent?
And really simple paintings, printed out en masse by computer printers, are Copy. You don't bother to sell them, you just xerox them if someone else wants them.
I'd like to get started with the [Apache Arrow](https://arrow.apache.org/) [Rust's codebase]( https://github.com/apache/arrow/tree/master/rust) - I have taken one of the [issues](https://issues.apache.org/jira/projects/ARROW/issues/ARROW-4596?filter=myopenissues) on the related DataFusion project to get started. It's a super interesting project and it could provide some useful inspiration when it comes to designing a dataframe library for Rust (DataFusion's scope is a little bit larger than that). Hopefully I will also have time to polish [bulk quantile](https://github.com/jturner314/ndarray-stats/pull/26) computation for [ndarray-stats](https://github.com/jturner314/ndarray-stats) and get started with the next post in the [Scientific Computing: A Rust adventure](https://www.lpalmieri.com/posts/2019-02-23-scientific-computing-a-rust-adventure-part-0-vectors/) series.
What are some easy-medium difficulty things that I can implement on my minigrep project from the rust book? Thought of looking at ripgrep and trying to copy some functionality but since I am quite a novice at programming I am having difficulties identifying what things are hard to implement or not. 
Trying to get back to [toyrand](http://github.com/BartMassey/toyrand) and [toyrand-rs](http://github.com/BartMassey/toyrand-rs) during my academic break. Preparing somewhat for my upcoming [Rust class](http://web.cecs.pdx.edu/~bart/syllabi/cs510rust-spring2019.html). Looking at the latest versions of the [Exercism Rust track](https://exercism.io/my/tracks/rust) which I've been using for exercises. Thinking about coming back to [rolling-crc-rs](http://github.com/BartMassey/rolling-crc-rs), although I wish someone would pick it up and finish it for me â€” the core functionality is there, but it needs to be sped up and cleaned up fairly badly.
Cool project!
Looks pretty solid! I notice that you don't directly depend on Amethyst or Specs, but you do use Serde and Ron, which I know Amethyst uses for configuration, so that means that it shouldn't be too hard to use this with Specs, which is awesome! The fact that you can use something general like Serde and instantly get compatibility with a more heavyweight library without directly depending on it is pretty dang cool to be honest.
With any luck at having free time, a Game Boy renderer.
What program is it? How are you compiling it? On what platform?
Neat. I need to write a GB renderer this week, so I'll give it a look.
Sorry, I've added that to my question: The program in question is firefox, compiling is done via `./mach build` as firefox usually is. Build environment is archlinux docker image.
You might want to look at how the official Arch package is build then: https://git.archlinux.org/svntogit/packages.git/tree/trunk/PKGBUILD?h=packages/firefox
That is the PKGBUILD I'm using (with few patches to C++ part of firefox and keybindings) so I have now idea why it's not working
Are you building from mozilla-central or from the stable release?
&gt; matplotlib Out of interest, do you know what causes things to be slow? Is it because you have to push around so much data in Python, or is it just the rendering part?
I've tried both the hg checkout as the archlinux PKGBUILD does and the tarball from https://archive.mozilla.org/pub/firefox/releases/65.0.2/source/ here. Both end with the same error.. Rust was updated to 1.33 3 days back (https://git.archlinux.org/svntogit/packages.git/log/trunk?h=packages/rust), could that be relevant? Is it possible that these functions are only in 1.32?
Oof there are a few factual slips in here: 1) the thing with the hash and then square brackets is one form of "proc macro" / "procedural macro", there's also macro_rules macros though. 2) calling `expect` to unwrap the window result is not unsafe. Proof: you didn't use an unsafe block anywhere :P now it can _panic_, but a panic is "safe" under rust's safety model. And 20 hours to edit? Just walk over the complete code without the asciinimia stuff if it's draining that much from you. Code progress is more important, and the graph paper doodles are great, so I don't think you need coding session playback if it's a 20 to 3 ratio.
It's possible, yes.
I am going to continue working on [Zola v0.6](https://github.com/getzola/zola), a static site generator. Unless I find some bug or get too busy, it *should* be released around this weekend.
Could someone explain to me the meaning of "local" in the chrono crate? Does it mean the same thing in all contexts? Here is why I ask. In the [chrono documentation](https://docs.rs/chrono/0.4.6/chrono/), it states that "local" is "the system local time zone." Based on that definition, I expect based on the name of the of the [naive\_local()](https://docs.rs/chrono/0.4.6/chrono/struct.DateTime.html#method.naive_local) method that it will translate a [DateTime](https://docs.rs/chrono/0.4.6/chrono/struct.DateTime.html) variable into its naive form in the "the system local time zone". This expectation is bolstered by the documentation for the naive\_local() method, which says that it "Returns a view to the naive local datetime." Or, to put it another way, I would expect from the definition of "local" and "UTC" that naive\_local() and [naive\_utc()](https://docs.rs/chrono/0.4.6/chrono/struct.DateTime.html#method.naive_utc) would be sort of like mirror images of one another. One takes a DateTime variable and puts it in naive form in the local (system) time zone, and the other takes a DateTime variable and puts it in the naive format in the UTC time zone. However, when I run [this code](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7368b9a36300558a89b0c057a0f33c03) on my system, I get the following: Utc::now() = 2019-03-17 11:03:44.796861045 UTC Utc::now().naive_utc() = 2019-03-17 11:03:44.796861045 Utc::now().naive_local() = 2019-03-17 11:03:44.796861045 Local::now() = 2019-03-17 07:03:44.796871660 -04:00 Local::now().naive_utc() = 2019-03-17 11:03:44.796871660 Local::now().naive_local() = 2019-03-17 07:03:44.796871660 As I would expect, the second and fifth lines (generated by the naive\_utc() method) generate basically the same output. On the other hand, the third and sixth lines, which are generated by the naive\_local() method, generate different output. So I return to my original question: What does "local" mean in the chrono crate? (This question is prompted by [this discussion](https://www.reddit.com/r/rust/comments/azqo9c/hey_rustaceans_got_an_easy_question_ask_here/eij22qk) in last week's "Got an easy question" thread. I felt it would get buried if it was continued there.)
Always excited to see machine learning in Rust :)
Rust does not follow semantic versioning?
It does, Firefox just relies on unstable compiler features for now.
I'm nearly done with a project I've been working heavily on - A friendly command line interface for Nozbe (a task management app). It's been my first time using rust for anything non-trivial, and so far it's been a blast barring a few head scratching moments where I forgot to use a as_ref or something. My code is at https://github.com/reisub0/nzb
A module in Rust can actually either be define in a different file or in the same file, so this shouldn't be to difficult to do. You could make a bash script like this perhaps; for f in "a.rs" "b.rs" "c.rs" do fb=$(basename "$f") echo "mod $fb {" &gt;&gt; main.rs cat "$f" &gt;&gt; main.rs echo "}" &gt;&gt; main.rs done 
Awesome, exactly what I want from my browser :/ Thank you for help with this, I guess I'll go look if firefox has commit for this already.
There's nothing wrong with using unstable compiler features. "Unstable" in Rust compiler parlance does not mean "generates unstable code", it means "exposes an unstable API".
Still working on [Eko](https://github.com/eko-lang/eko), a simple scripting language written in Rust. Still in the middle of the rewrite...
I'm [starting to add case expressions](https://github.com/brendanzab/rust-nbe-for-mltt/compare/literal-patterns_ to my [`rust-nbe-for-mltt`](https://github.com/brendanzab/rust-nbe-for-mltt) dependently typed programming language experiment. Just working doing literals for now, but eventually I hope to add support for matching on dependent records and inductive types.
There are some established generators with large state ([xoshiro512](http://xoshiro.di.unimi.it/), [PCG](http://www.pcg-random.org/using-pcg-cpp.html#extended-32-bit-generators) and of course the Mersenne Twister).
I don't worry about "generates unstable code" more about the fact that it's pain to compile on rolling distro (since when new rust is out they can (and did) stop working) :/ But yeah, partially it's on me for not really understanding what's going on :)
That's the unfortunate reality of depending on unstable APIs. That said, Firefox was pretty close to building entirely on stable Rust last time I checked, so progress is being made.
&gt; this takes an ENORMOUS amount of time to do with MPL. Some years ago, I used [PyQtGraph](http://www.pyqtgraph.org/) for faster-than-MPL plotting, but it is not as pretty. Now there is the successor [VisPy](http://vispy.org/), but I never used it. You could also consider using Julia's plotting (but their script startup time is much worse than for Python).
I really like Julia, but the massive startup times make it unsuitable for some use cases of Python.
I'm trying to use libwebp from wasm for node. The plan is to compile libwebp to a static lib and then link it to the rust code that is compiled to wasm like in alexcrichton's wasm-sodium example. Afterwards I will look at the game of life wasm example. Right now I've compiled it and generate bindings because the other crates are not working :/ The search path of the crates is OUTPUT_DIR/build but the static lib is at least on windows also put in an additional folder Debug/Release. Maybe that's a windows or rust 1.33 issue. The next step is to remove the cmake and bindgen dependency like in libwebp-sys. This will make the node step easier. After that I will make the api more rusty and probably release it to the world.
Asking from the other direction, why would it be useful to disallow this? The `place expression` definition corresponds to things whose lifetime are tracked aka things you can borrow. This finegrained tracking allows you to borrow `a.foo` and `a.bar` at once. So we have to propagate the place of `e1` when using `e1[e2]` so we can check overlapping borrows. But we don't really gain anything by forcing `e1` to have a place and might lose some optimization opportunities uncovere by inlining/constant folding/data flow analysis.
magic
For exercises: https://adventofcode.com/2015/day/1 Another good site for exercises: https://app.codesignal.com/ For real life project/inspiration: https://hackaday.com/blog/
Super awesome stuff. Thanks for all the work you do
Hoping to complete a local password manager. So far so good! Just running into time constraints like "no, it's not okay to coffee for 12h straight without taking a food break!â€
I've finished implementing the [pattern matching compiler](https://github.com/Lapz/match) described by Peter Sesoft and I plan on implementing the one described in [The Implementation of Functional Programming Languages](https://www.microsoft.com/en-us/research/publication/the-implementation-of-functional-programming-languages/).
Still working on https://github.com/svenstaro/miniserve The user can now download a folder: the server compresses it in tar.gz, and then the download can start. Sadly, it seems tar-rs is not asyncio-ready, so I cannot stream the result, which is really annoying for sharing big folders.
What are some easy-medium difficulty things that I can implement on my minigrep project from the rust book? Thought of looking at ripgrep and trying to copy some functionality but since I am quite a novice at programming I am having difficulties identifying what things are hard to implement or not. 
Thanks for the ping! I'm not quite sure what I learned from giving this course, so here is just a unsorted list of things I can think of right now: - I tried to have many "live tasks" in my lecture where students would get out their notebooks and try something. Sometimes these tasks are just "thinking tasks", for example one time I showed them a bunch of different errors ([slide 3 here](https://github.com/LukasKalbertodt/programmieren-in-rust/blob/master/slides/09-Error-Handling.pdf)) and told them to categorize them into two categories -- that worked out nicely. I think the students liked those tasks to make the lecture more interesting. And from a student's perspective, I also very much like to think during a lecture instead of just listen. So I wish I would have done this even more. - I taught the way I like to learn: from the ground up. The boring syntax in all detail first, *then* we can use it. For example, I never liked the first "quick start" chapter of the book because I personally thought "oh my god, but what are these things? I don't know anything yet! Slow down!". However, I don't think it was a wise decision to use "my" style of teaching in the lecture. I think many other people first want to see how something is used and want to know the details afterwards. If I were to give another lecture, I would probably try to find a middle way between both kind of teaching. Show something interesting first (but not too much new stuff), then go into detail about it and then show more interesting things about it. As another data point about this: many students got the impression from my lecture that concurrent programming/multithreading in Rust is difficult. That's because I only talked about it at the end of my lecture. Why? Because there is quite some knowledge necessary to understand the signature of `thread::spawn`. And I really wanted them to understand the signature. However, again, I think that was not a good decision. It's probably better to use `thread::spawn` way earlier already and say "don't worry about that" and later explain all the things which will then lead to "ah, that's what that means!" moments. - Looking at compiler errors together in the lecture and talking about it is important I think. My students were mostly used to Java -- and those error messages (from `javac`) are usually really bad. So it's worth spending some time to train people to actually look at Rust's error messages. Otherwise many will assume that they are as useless as the error messages from other compilers. - I think it's important to convey the motivation and the enthusiasm about Rust to the students. Rust is not flawless and it's fine to criticize it from time to time (e.g. I kinda made fun of the `..` vs. `...` (in match patterns) vs. `..=` stuff where nothing is consistent). But I think it's very harmful to do that too often. Instead, many design decisions should be explained and the advantages of many things should be emphasized. That way students get really motivated about learning Rust. - I also think I made the mistake of trying to discuss everything of Rust. For example `macro_rules!` is something that I would not discuss next time. I would mention that it exists and I would maybe give out an optional task about it. But what I failed to understand is that people can and will learn this additional stuff on their own if they are interested. Practicing the important bits of Rust is more important I think (now). - My course didn't end in an exam, but in a programming project (the size of roughly one full week of work). And for the most part, I would say that was absolutely the right decision. Sure, deciding on grades is a bit more difficult of you have an exam, but I think exams for programming languages are not useful at all (at least exams on paper). And there were really quite a few cool projects in the end! At least that's what I can think of now. If anyone has any questions, I'm happy to answer! I'd be really interested in giving a Rust lecture again. Especially, since since I gave mine, the language changed a lot -- at least things relevant to learning. NLL, these match bindings and a lot of other things would make learning and teaching a lot easier I think! 
Then in cargo the license should also mention this
Sounds like a really cool project! As one of the many who have written gameboy emulators in rust, feel free to reach out if you have questions or are confused by anything.
What is a good pattern for dealing with generics that need different function implementations depending on their type? I know there's a specialisation RFC somewhere which is still ongoing, so more interested in any common patterns used to design around it. E.g. a trivial example which doesn't work: struct Value&lt;T&gt; { value: T } impl&lt;T&gt; Value&lt;T&gt; { fn new(value: T) -&gt; Self { Self { value } } } impl Value&lt;f32&gt; { fn new(value: f32) -&gt; Self { assert!(!value.is_nan()); Self { value } } } So wondering what any common ways to design around it are. Different `new` constructors for different types? Avoiding generics in this case and duplicating a bunch of code?
I'm writing a SQL database for learning purposes. What would be the way you'd model a database Page (the smallest unit of disk operations) in terms of a struct? I'm thinking of something like this: `pub struct Page&lt;T&gt; {` `header: Header,` `slots: Vec&lt;u16&gt;,` `data: Vec&lt;T&gt;,` `}` Where Page is generic over T. A few difficulties I'm finding: \- How can I store the tuples inside the page? Ideally, the data array should grow as it needs (so its really a vector) but should not exceed its own maximum size (remembering there may be a slot array growing from the end of the free space block). \- How can I unserialize the byte representation of this into a struct back again? 
&gt; which is also useful for chaining like This is one of the keys, I feel. If one couldn't do `&amp;` or `&amp;mut` on a temporary, then one probably couldn't chain `&amp;self` or `&amp;mut self` methods on temporaries (only `self` ones). Around when the `Iterator` trait was being introduced (sometime before Rust 1.0), this didn't work, and it was exceedingly annoying to not be able to write things like `slice.iter().all(f)`, since one was instead forced to write `let mut iter = slice.iter(); iter.all(f)`.
Spent some time attempting to start a re-work of [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis). I reserved the [`uom-macros`](https://crates.io/crates/uom-macros) crate as I'm looking to transition to function-style procedural macros.
Stated working on re-implementing [GigaNotes](https://giganotes.com) core in rust using [Neon](https://neon-bindings.com/) library. As the result, I expect to significantly improve desktop and android applications' performance.
Trying to figure out how to get the STM32F3 Discovery board's i2s interface talking to an audio codec.
I'm in the middle of separating my `syswall` CLI application to a library and CLI and have just [released the library-side to crates.io](https://crates.io/crates/syswall). Hopefully I'll get time to finish and polish it in the next few days, at which point it will all be pushed to [the GitHub repository](https://github.com/polaris64/syswall)!
Thanks! The answer to why [I thought] it would it be useful to disallow this is, I was thinking too much of assignments, and too little about other place expression contexts. 
There are few options: * provide default implementation for a trait, and specialised ones when needed * macros (though they are expanded before type information is known, so you have to be creative with their usage) * extract specialised functionality into its own method, let generic functions call them
I made a library to help render pixels to a screen. I'm very new to any sort of graphical programing, and for the project I'm working on I just wanted a simple way to draw an array of pixels. Most of the stuff I found, however, was people saying "learn OpenGL" or "Learn SDL" which seemed like a lot of effort for just popping some pixels on the screen. I made [Flatland](https://gitlab.com/as-i-do/flatland) (by learning a tiny bit of OpenGL) to allow a user to just render their array of pixels. With one import change you can swap between rendering the pixels to the terminal or a glutin window. It's really rudimentary, and I'm not gonna push it to cargo ATM (it's missing proper error handling and a bunch of event mappings), but it's what I had wished existed when I started and I learned a lot making it.
Great vids so far! One small nitpick: the `#[something(thing)]` notation is called an "attribute", not a macro. Macros are `something!(stuff)`.
I've used https://github.com/lpenz/rust-sourcebundler for rust tictactoe in codingame, i also used an environment variable for local testing and running in prod https://github.com/tomcraven/tictactoe/blob/master/src/main.rs#L146 this meant i could just copy paste the output of rust-sourcebundler directly into codingame (after commenting out the rayon code, no support for that in codingame if i remember correctly...) 
More data mining on [crates.io](https://crates.io) stuff. Why are databases such a pain? All I want is to walk trees, have structures containing arrays, and various other things they're not designed to do well. It would be really nice if there were a tool that was designed to do those things AND do relational queries, though; it doesn't seem like that should be too much to ask. ...And ideally it wouldn't use SQL.
The "no invalid state is representable" is a huge step forward. Congratulation!
That won't manage external dependencies unfortunately ;/ 
Nice project ! Currently I use the Chrome plugin to sync a local file to Codingame, so it should work like a charm with this solution (you should try it, make the life really easier :) )
It is already possible to use KaTeX with the current rustdoc. You should set RUSTDOCFLAGS to something like `RUSTDOCFLAGS="--html-in-header KaTeX.html"`, with KaTeX.html containing something like &lt;!-- Load KaTeX (https://katex.org/) to render equations --&gt; &lt;link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous"&gt; &lt;script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"&gt; &lt;/script&gt; &lt;script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous" onload="renderMathInElement(document.body, {delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false} ]});"&gt; &lt;/script&gt; This is the code I use to generate this documentation: https://lumol.org/lumol/latest/lumol/energy/struct.BornMayerHuggins.html
well this past Saturday (so I guess last week technically) I released my first crate. [simple_xml_serialize](https://crates.io/crates/simple_xml_serialize) and an accompanying proc_macro crate [simple_xml_serialize_macro](https://crates.io/crates/simple_xml_serialize_macro). I always liked golang's xml serialization utilities so I wanted something to work like that, where I could annotate my struct fields with different attributes to specify how the data gets serialized.
Link please :)
I managed to write [a web server with WebSocket support](https://github.com/anlumo/cellardoor/blob/master/cellardoor/src/main.rs) using async/await! that can also serve static files. I had to write the upgrade request response manually, because the WebSocket example hyper supplies only supports talking to hyper and not a web browser, and the WebSocket library has no support for hyper. Its hyper example only runs a hyper instance in the same process, but doesnâ€™t actually integrate. I canâ€™t recommend using async/await! right now, the environment is a huge mess, with two incompatible versions, where every crate expects something different.
For systems that care about low power and sleep modes, the author may be correct. But in the automotive world we like to avoid having interrupts triggered by off-chip signals. Noise on such a pin can bring your system down via DoS. You're better off polling in a timer ISR or any other deterministic time slot. There are reasons chips have input capture, output compare, pwm and other means of collecting and producing signals without direct bit banging.
NYAR owner here -- thanks for the plug! :) Another plug I'd give is for David Tolnay's [Request for Implementation](https://github.com/dtolnay/request-for-implementation), which might be more straightforward to jump into if OP doesn't want to do as much design work or domain learning (though there will always be that!).
I should have formulated my question differently - what makes 'trait as type' (not behind reference) in structure different from generic parameter constrained by trait? (like explicit parameter or impl trait in function signature)
I'm probably going to spend most of my time working on [tarpaulin](https://github.com/xd009642/tarpaulin), I'm doing a moderately large refactoring to try and make it more robust to the fickle/complex nature of ptrace/IPC
If you look at [the source of `naive_local`](https://docs.rs/chrono/0.4.6/src/chrono/datetime.rs.html#233-235) literally all it's doing is taking the UTC time (always being stored internally) and adding the stored offset of the timezone to it. Semantically, it's getting the time _local to the timezone of the `DateTime` instance_. So `Local::now().naive_local()` is getting the local time for the system timezone, `Utc::now().naive_local()` is semantically equivalent to `naive_utc()`. Also the `Display` impl of `DateTime` [prints the time as determined by `naive_local()`](https://docs.rs/chrono/0.4.6/src/chrono/datetime.rs.html#529-533). So `naive_utc()` and `naive_local()` might be less confusing if they were named `without_offset()` and `with_offset()` instead.
Working on some cryptography: 1. Implementing RSA-FDH, an RSA-based scheme that optionally supports blind signing. https://github.com/phayes/fdh-rs 2. FDH - A full domain hash (needed for RSA-FDH). https://github.com/phayes/rsa-fdh 3. Benaloh-Challange - a cryptographic technique for challenging the honesty of a device: https://github.com/phayes/benaloh-challenge This last one is my favourite because of just how *weird* it is. 
With uniform paths that shouldn't be a problem? 
I guess the error is with `stdin::()` try removing the colons.
I have been working on a command line tool, [eca1d](https://github.com/Frojdholm/eca1d), for exploring cellular automata. I needed this for a course and wanted to try to write some Rust. Constructive criticism is welcome! I found that Rust was very easy to use to develop this kind of application and I like that it compiles down to a single binary which you can just put anywhere compared to the Python scripts I had before. Next I want to add the possibility to print the rules in similar style to the simulation and be able to specify the rule in binary.
I thought cloning a Client would be expensive and lose the pooling beneifts, but it turns out both the Client and Pool are cheap to clone and mostly contain Arc containers in their struct. Cloning the client isn't too bad. You could also wrap it in a Arc yourself if you want to make the operation even cheaper, but not sure if it's worth it.
&gt; When you ask RsvgHandle how big it is, in reality it should look at you and whisper in your ear, "how big do you want it to be?". Lol
It doesn't exist yet! :P
Would you like to try this- https://www.reddit.com/r/rust/comments/b0whup/simple_project_idea_for_beginners_a_hex_editor/ This may also help you get a feel for the type system since you'll be dealing with raw bytes, characters, `String`s and `str`s.
I'm trying to get started with writing rust but I'm having some trouble with the 'image' crate. &amp;#x200B; `extern crate image;` `use image::{Rgb};` `use image::png::PNGEncoder;` `use image::color::ColorType::RGB;` `use std::fs::File;` `fn write_image(filename: &amp;String) {` `let mut file = File::create(filename).unwrap();` `let enc = PNGEncoder::new(file);` `enc.encode(` `&amp;[0, 0, 0],` `1,` `1,` `RGB` `);` `}` &amp;#x200B; When I try to compile this code I get &amp;#x200B; `error[E0603]: module \`color\` is private` `--&gt; src\main.rs:6:12` `|` `6 | use image::color::ColorType::RGB;` `|` &amp;#x200B; This code does indeed seem private ([`https://github.com/PistonDevelopers/image/blob/master/src/color.rs`](https://github.com/PistonDevelopers/image/blob/master/src/color.rs) ) (im guessing because of no 'extern...') but im confused as to how I'm supposed to access this enum to give to the PNGEncoder.encode method. &amp;#x200B; I feel like im misunderstanding the import system here perhaps?
Do you mean `struct Foo(Trait)` versus `struct Foo&lt;T: Trait&gt;(T)`? The former is not really possible, because bare trait objects are unsized and can thus only be accessed from behind a fat pointer (e.g. reference).
Hey! It is a common practice for rust libraries to have private modules at the lib layer, and then specify which part of the private module you want to export by having a `pub use` in `lib.rs`. Check out https://github.com/PistonDevelopers/image/blob/master/src/lib.rs. This makes the API a bit more clean too. So to fix what you have, you should just need to change `use image::color::ColorType::RGB;` to `use image::RGB;`! ` 
Startup times have dramatically improved since 1.0 (and Pkg3 with it) and further improvement is high on the developers list of things to fix. There are similar issues here in Rust land too. I tend to think of both issues as being "fixable enough", meaning, Julia gets faster startup times before Python can run fast, and Rust compiler gets faster before C or C++ can be made safe. &amp;#x200B; Another thing to consider in the question of "scientific Rust" is the fact that the Rust community is overloading averse and the scientific programming community is not, IME. Rust is a tough sell against C++ for writing scientific libraries.
Iâ€™m releasing my wasm port of Fuse.js, [wafu](https://github.com/heyimalex/wafu). Fuse is a popular javascript fuzzy searching library, and I thought itâ€™d be a good candidate for translation to Rust! Performance of wafu in firefox is already regularly 4x that of Fuse, but mostly it was a fun way to learn a lot about Rust and wasm :) Iâ€™m also working on fixing a really obvious bug in my [bitap](https://github.com/heyimalex/bitap) implementation, and hoping to write a bunch more tests and check out fuzzing because it seems applicable and Iâ€™ve never done it before.
Ah cool! Thanks for mentioning the wasm-sodium example, Iâ€™ve been trying to do this and getting stuck.
I haven't actually profiled it, but I'm sure it's due to the rendering part. The Python script basically reads in the processed data and just makes graphs, that's it. Now whether it's MPL that's slow, or just file IO (saving thousands of PNGs) is a different question.
Yeah I've seen a couple different backends that I could swap out. At the moment, the amount of time it takes is bearable. Each experiments generates about 3500 graphs, which can take 5-10 minutes even when splitting that across threads. I've been meaning to look into Julia, maybe I'll give it a shot 
Hey thanks, that makes sense now.
Trying to build `rustc` for iOS. Stdlib builds fine, but building stage0 compiler artifacts first drops the `dylib` crate type for some reason, then fails saying: ``` Unknown debugging option: dual-proc-macros ``` Cross-compiling other Rust code with `cargo build â€”target &lt;arch&gt;-apple-ios` works ok.
True, but the industry is slowly but steadily settling on a hybrid approach. &amp;#x200B; You have a cyclic and deterministic core layer with safety critical functions written for SPS/PLCs with limited primitives (interrupts disabled), which will get verified and certified. But on top of that, you get an application layer, which is basically arbitrary code, interrupts et al included. Both layers CAN share the same chip, depending on how hard the constraints are. In that case, hardware watchdogs will be deployed to make sure that the application layer will terminate. If the application layer hangs/crashes the system, it will forcefully restart into the core layer. This way, you can have IOT style development workflows and quick iteration cycles without compromising on safety or core functionality aspects.
Thank you! Iâ€™ll try to add annotations on the video to fix up the fact mistakes. People seem to like the doodle part better, Iâ€™ll keep this in mind for next time.
Since you're now waiting for an interrupt, it would be nice to put the uc into sleep mode. Here's a short discussion about how to enable sleep mode and get the lowest power draw (choose your voltage regulator wisely): [https://electronics.stackexchange.com/questions/79160/stm32f103-low-power-mode-stop?rq=1](https://electronics.stackexchange.com/questions/79160/stm32f103-low-power-mode-stop?rq=1) &amp;#x200B; &amp;#x200B;
You can also always just take a screenshot if you think you're at an important moment, and just toss the screenshot later if you don't use it. Usually my tutorials/guides are markdown format, so i just paste in a code block if what I'm talking about.
[removed]
This is so cool. I work in bioinformatics web dataviz as well (on [iobio](http://iobio.io/)), and actually just pushed my first Rust service to staging this morning. In my experience, academia tends to lag behind in terms of tech stack and design (not saying this is always a bad thing). I'm pleasantly surprised to see Ensembl using some cutting edge stuff.
How do I build an executable binary for my project? &amp;#x200B; I'm using Cargo, and building/running my code that has a couple of dependencies, I've found the binary file in the target/debug folder, but this seem to need dependencies installed. &amp;#x200B; What's the next step to get a nice independent executable/binary (I'm building on OSX).
oh thx, I knew const fn is still in its infancy but I misinterpreted the error message. I worked around it by passing as 2 separate arguments instead of a tuple. (I wanted a tuple for slightly complex reasons which I was also able to work around.)
AFAIK, ThinLTO is still enabled. Where are you seeing that it is disabled?
Given that it is supposed to be fast, how does perf compare to, say, [toolshed](https://crates.io/crates/toolshed)?
[This](https://github.com/rust-lang/cargo/blob/master/src/cargo/core/profiles.rs#L498) makes it look like release doesn't set LTO. I also did a quick test project that contains a library crate and an exe. The library was just: fn f() -&gt; i32 { 0 } and the exe was: fn main() { println!("{}", my_library::f()); } The call to `f` isn't inlined in release unless I also set `lto = "thin"` in the `Cargo.toml` file for the workspace.
Damn, this is really cool. I feel like this could be extended to also have "Frames" such that you can allocate something similar to a stack frame, which has the same API as Bump, but when dropped or when reset gets called on it, resets to the location it was created at. That way you could have smaller phases where you deallocate a bunch such that the bump allocator overall doesn't use too much memory (which also should yield better cache locality).
The last change in that file looks relevant: https://github.com/rust-lang/cargo/commit/2b6fd6f0ffe25fc8861d140f2031ea2c460b0ebd#diff-0733902e442081f2c1a716db9eb7f058R234 Maybe ThinLTO hasn't landed on by default in stable yet? Looks like it will be happening in 1.34. Also, how are you testing whether or not the call was inlined? ThinLTO doesn't provide any guarantees about what will/won't be inlined anyway so I'm not sure that's a valid test.
Yeah it broke everyone's code using fat lto with panic = abort or something (I forgot, but it broke mine and lots of other people's code).
Is it possible to change the type of a variable inside a function with a feature flag without wrapping the whole function in an *#[cfg(...)]*? ``` fn foo() { let bar = if cfg!(...) { 0f32 } else { 0f64 }; } ``` does not work because of type instable branches.
&gt; Maybe ThinLTO hasn't landed on by default in stable yet? Looks like it will be happening in 1.34. I'm on the latest nightly (1.35) so I don't think that makes a difference. &gt; Also, how are you testing whether or not the call was inlined? ThinLTO doesn't provide any guarantees about what will/won't be inlined anyway so I'm not sure that's a valid test. By inspecting the binary in a disassembler. I get that the function isn't guaranteed to be inlined, but if ThinLTO were the default then surely adding `lto = "thin"` would have no effect?
Are you using FFI? If not your binary should be already statically linked and should only depend on your platform. If yes, it depends if you link dynamically to an external lib or not. Do you?
&gt; By inspecting the binary in a disassembler. I get that the function isn't guaranteed to be inlined, but if ThinLTO were the default then surely adding lto = "thin" would have no effect? That's true. There's no chance you have `CARGO_INCREMENTAL=0` set in your environment right? Or `build.incremental=false` in your "~/.cargo/config`?
Nope, just checked and the configuration is all clean. Since posting I've found [this comment](https://github.com/rust-lang/rust/issues/47866#issuecomment-362430876), which seems to confirm that ThinLTO is no longer the default. But I can't find any explanation or discussion about re-enabling it.
The default thin LTO is only run on a crate level (to combine codegen unit results), not across crates. No global LTO is performed by default.
I can solve my problem with an conditional type alias.
That seems to explain it, thanks! Why is the default restricted to crate level? I was hoping that having ThinLTO by default would effectively make the `inline` attribute obsolete.
Cross-crate LTO can take a loong time and a lot of RAM with big and many dependencies.
Even if it's thin? I thought that's the problem that ThinLTO is meant to solve? (E.g. http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html)
I have only run the criterion benchmarks included in the repo; haven't directly compared against any other bump allocator. That said, they *should* be roughly the same speed since there isn't too much design space in the realm of bump allocation :-p The only technique I can think of that `bumpalo` doesn't do is avoid the bounds check with a guard page, because that isn't available on wasm, where I needed a bump allocator.
That's a neat idea!
Very hard to do refactors this large on things with such long lived APIs. Keep up the good work! 
Well that's neat. Thanks much!
Trying to figure out best way to map aliased const memory into different address ranges in my [Zmu](https://github.com/jjkt/zmu) emulator. Use case is to support micros like STM32 that can alias for example 0x0800\_0000 to 0x0000\_0000. Hard part is to figure out how to do it without performance degration due to additional branching on each simulated bus access.
I do want to be clear that I think showing the `Arc` version instead of the `thread::scope` version makes the Rust example longer, more difficult to understand, less efficient, and (I would argue) less idiomatic, in exchange for reducing the number of external deps from 3 to 2. It kind of paints Rust in an unflattering light, in the context of a cross language comparison. Then again, there's no purely objective comparison that would satisfy everyone anyway, and if you wrote out all the caveats explicitly no one would read them, and I can accept that :) Also it'll probably make sense to rewrite the example again once async IO stabilizes.
That's really exciting! Would it be a good idea to automate the generation of rust bindings by creating new annotations for gobject-introspection about the protocol that defines which functions have to be called in what order?
Would it be possible to add this as a feature?
I was interested to learn that this uses essentially a segmented stacks approach, where a bump is a linked list of chunks. This makes sense of course because you can't invalidate references into the bump from outside of it, but it made me curious. For dodrio, am I right that there should be no live references into either dom bump from outside of them during the period of dom construction? So if the dom implementation were more entertwined with the allocator, could you just store addresses as offsets from the bump and use a resizing strategy instead of the linked list? However, it also occurs to me that this is likely not a big win, since my understanding is that the main problem with segmented stacks is when you are pushing/popping stack frames across a segment boundary during a hot loop, which doesn't apply here since you never pop chunks. Is there anything I'm not understanding correctly?
Thanks! I had missed the xoshiro thing, and that PCG has some funny super-long-period mode now. I'm not sure quite how much these generators really are going to give me an ultra-large user-configurable internal state, which is what I'm looking for. I'll probably continue to play with toyrand for now: it's fun in any case. But really do appreciate the refs. 
That probably was incremental, not ThinLTO.
Oh right, nvm then. I only vaguely remember what caused it.
IME the "thin" in thin LTO means "thinner." It's better than alternatives but it's still excruciating for large builds. 
You can use the `#[cfg]` attribute on statements: #[cfg(foo)] let bar = 0f32; #[cfg(not(foo))] let bar = 0f64; 
You don't really need the complexity of session type notation to do that in Rust though, only to define a bunch of structs (likely wrappers on the actual core object / handle) which get moved into functions. The thing is, while you can do that in most statically typed languages unless you derive Copy Rust natively prevents backtracking the session (exactly the problem Raymond Chen asked about [at the end of their last post](https://devblogs.microsoft.com/oldnewthing/20190318-00/?p=102324)). The only thing you can't do in Rust is require some sort of fallible cleanup call, because it provides affine types but not linear types.
r/playrust is the subreddit you're looking for
It is certainly possible! I'd be interested in merging a PR that added this behind a feature.
Maybe i'm the only one, but since you ask for wishes I'll come with one: I would like to see something like [XlsxWriter](https://github.com/jmcnamara/XlsxWriter) in Python for Rust. It's a library for writing excel files. It's a lot of xml parsing string manipulation, and there are libraries to help with that already. There are good examples to follow like xlsx for python and [a good one for JS as well](https://www.npmjs.com/package/excel4node). &amp;#x200B; There is a library today called [Simple Excel Writer](https://github.com/outersky/simple_excel_writer) that's a very basic excel writer but it's a bit limited when it comes to things like formatting and is not updated to often. I even started on porting [XlsxWriter myself](https://github.com/cfsamson/xlsx_rs) but I just don't have the time work on it enough to push it forward.
Being able to hand in the Cairo context and have the library figure out dimensions etc sounds like a very nice change.
&gt; So if the dom implementation were more intertwined with the allocator, could you just store addresses as offsets from the bump and use a resizing strategy instead of the linked list? That seems possible, yes. But copying on resize is not great for perf, and the linked list isn't a big deal since it is very rarely traversed; only on `Drop` and `reset` essentially. &gt; the main problem with segmented stacks is when you are pushing/popping stack frames across a segment boundary during a hot loop, Yeah, it breaks the amortization of having allocation that is super fast most of the time, but is very occasionally slow so that the common case is fast. It creates degenerate situations where the common case can be that slow path every time. &gt; which doesn't apply here since you never pop chunks. Calling `reset` will free all but one chunk back to the global allocator, but because of the chunk size doubling behavior, everything should still be amortized. It might make sense to add a `Bump::with_capacity` constructor though...
I think it's likely you're doing something wrong if you want to literally do this, consider creating a trait for the values that can be in `Value`, which allows checking the value, and implement it for `f32` and anything else you might want in the type.
We are playing with the f3 discovery board and the raspberripy zero at the rust-roma meetup too. Do you have a repo of your project?
&gt; An another solution I am not aware of? The solution you're not aware of would normally be to wrap it in an `Arc`, although as /u/jeomegn also said, it seems a `Client` already uses `Arc` internally, so you can just clone it.
Is this a problem specific to rust? I've used LTO on huge C++ projects without issue.
Awesome :-) No repo yet but I'll definitely put the code up once I get something working!
Impressive! 
No, it's definitely an issue in C++. But it depends on what you're building and the number of objects you're linking against. 
thank you, this sounds like a great idea!
&gt;Provide binary compatible API for Rust project settings Does this mean I can share project settings across platforms? Or is this forward/backward compatibility? 
Thank you so much for Zola, it's the absolute bee's knees!
Are there any implementation of a "pretty" panic, which just outputs the string passed and exits with an error code? &amp;#x200B; So instead of something like this: thread 'main' panicked at 'Cannot parse month: b', src/libcore/option.rs:1038:5 note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace. Just output this: Cannot parse month: b And have it work with `expect`?
The semantic is different regarding the drop behavior. If you would automatically borrow every move, the consequence would be that you can't drop the value in the receiving function when you don't need it anymore. You would have to keep every allocation until you go back to the original caller - which is often unwanted and unneeded.
&gt;FFI I'm not sure what this is, is it Foreign Language Interface? For when you use other language libraries wrapped in Rust code, like C++/Python libraries? &amp;#x200B; In which case, no, I'm not using FFI, where will I find my binaries, in the debug folder I mentioned before?
&gt; I'm OK with Python You could come contribute to the [`RustPython`](https://github.com/RustPython/RustPython) project! It's still pretty early so there are loads of things to do.
I've played around a bit with a proc macro for cbindgen. Here's the current progress: https://github.com/jrmuizel/cbindgen-macro
Interesting. Can you point to a concrete example of something like this? I'm still getting acquainted with borrows and deeper lifetimes.
ðŸ‘
1. ensure "rustc" is nightly 2. ensure "rustc" is receiving "-Ctarget_features=+sse2,+sse3,+ssse3, etc" to enable those features this will require some digging in the build scripts 
Sure, to force every move into a borrow wouldn't work, but if the API specifies a borrow, why can't the compiler auto-borrow? It just seems like syntatic sugar?
Continuing work on [Plexus](https://github.com/olson-sean-k/plexus), a polygonal mesh processing crate. In particular, I'm refactoring types and traits for representing index buffers and wrestling with [mkdocs](https://www.mkdocs.org) and GitHub pages to create a [website](https://plexus.rs) with a [user guide](https://plexus.rs/user-guide/graphs) (inspired by [nalgebra.org](https://nalgebra.org)).
Interestingly enough the `println!` macro does do that. Presumably that's because there are no situations where you would want printing a value to consume that value, or at least none that I can think of. However, the same can't be said for functions in general. Passing ownership vs passing a borrow involves different semantics and so Rust has opted to make that choice explicitly visible in source code.
 use std::fs::File; use std::io::prelude::*; fn main() -&gt; std::io::Result&lt;()&gt; { let file = File::open("foo.txt")?; read_lots_of_stuff_and_then_do_lots_of_work(&amp;file)?; Ok(()) } fn read_lots_of_stuff_and_then_do_lots_of_work(mut file: &amp;File) -&gt; std::io::Result&lt;()&gt; { let mut contents = String::new(); file.read_to_string(&amp;mut contents)?; now_do_lots_of_work(); Ok(()) } &amp;#x200B; If you're in windows, the file will be locked until `now_do_lots_of_work` finishes, as `file` won't be dropped until `read_lots_of_stuff_and_then_do_lots_of_work` returns back to main.
Yeah I have actually laughed at that
Itâ€™s great! Thank you for your work!
How huge? Have you tried using it on Chrome? :) 
&gt; This makes sense (in contrast to reallocating the entire bump when it grows) because you can't invalidate references into the bump from outside of it, but it made me curious. I don't follow. Why would you need to reallocate the entire bump on grow ? 
I've recently finished up an MVP of a simple(ish) database migration manager I've named [movine](https://github.com/yronbitsaw/movine) which I plan to use in some of my other projects. The goal of Movine is to be smarter about migrations by keeping track of the hash of the up and down sql for each migration. This way it can tell if the locally stored migration is different from what was run on the database, or if there were migrations run on the database that are not present locally (for instance if they were developed &amp; run in a branch). My next steps are to clean up the code and hopefully abstract the database interfacing parts so that people can use it with databases other than postgres. Mainly I need to clean up the error handling, because right now if something is not perfect it just dumps the raw error messages.
Each chunk has a fixed size and if you don't want to OOM you need to allocate more memory somehow if you reach that size. In bumpalo, a Bump is a linked list of chunks as documented in the README.
I'd argue that's more of a "minimalist" panic than a "pretty" one, but you can replace the default panic hook to do this with[`std::panic::set_hook`](https://doc.rust-lang.org/std/panic/fn.set_hook.html) like so: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=c898e59f5ecc40471e3fe077523ed797
But you can continue to allocate more memory adjacent to the old one, so what problem do chunks solve?
Read or mmap file to a &amp;str then. `mydb.contains(mystr)`
I know I have been slipped up on several occasions by not knowing if a C++ function call was a non const reference or not. So at least `&amp;mut` should be explict IMO.
I think you're misunderstanding the proposal (which wasn't worded very well, I think). It isn't about using moves instead of borrows, it's about auto-borrowing when calling functions that take references already.
&gt; But you can continue to allocate more memory adjacent to the old one. Why do you believe this?
I am not sure I understand the question. I believe it because that's how memory works ?
Yeah, by FFI they mean dependencies on C libraries or other language libraries. So if you're not calling into another language and none of your dependencies are calling into another language, then your executable should already be independent.
I appreciate what JetBrains doing for tooling development, make Rust development easier each release, thank you for your work.
I'm not sure whether this is the appropriate place but every now and then when I give IntelliJ a go with rust, I have weird issues pop up, claiming that there are things like [unresolved references](https://i.imgur.com/WNan7LO.png). When I run cargo check, or I use VSCode with VIM, I don't seem to get these errors.
There's two "allocations": - the backing store of OS memory, which needs to be requested for a program to be able to read/write somewhere (this can happen via `Box` or `Vec`, or even `malloc` or directly down to `mmap`) - the pointers returned by this library, allocated via some sort of bump pointer code out of the pieces of memory above The linked list of chunks is the first bit. Each chunk is of limited size, and there's no guarantee one will be able to allocate a directly adjacent chunk once each fills up, meaning one has to be able to handle having more than one chunk.
&gt; Ultimately the chunks are allocated by the global allocator and nothing allows you to grow them infinitely. That's not really how it works. Managing chunks is so common that pretty much all hardware has native support for doing this. So ultimately the chunks are managed by the CPU, according to the page tables that the memory subsystem of your operating system is in charge of managing. So if I were implementing a bump allocator, I would just allocate sufficient virtual memory once on initialization, and then as the pointer is bumped (or decremented if stuff is dropped), I'll just either map / unmap physical pages as needed, or let the operating system transparently do it for me (e.g. on page fault, map a new adjacent page). 
The human-panic crate tries to go for "helpful to humans" instead of "pretty" but might still be interesting :)
I'm trying to wrap my head around how compilers work. From my understanding, Rust uses LLVM as a cross-platform "backend". If that's the case, what is the purpose of the different toolchains? On Windows, for example, do gnu and msvc output different IR for LLVM?
If you like writing grep, the [CLI book](https://rust-lang-nursery.github.io/cli-wg/tutorial/index.html) has a different take on it. Maybe not the same as adding more features but might be interesting (e.g. adding integration tests).
bumpalo is targeting wasm
Where does it say that? That... would explain quite a bit.
Hey /u/fitzgen, apologies for the off topic comment, but I have a question regarding dodrio and I suppose to a lesser extent bumpalo too. Concerning diffing two vdom trees in dodrio, is there somewhere you could point me that would teach me how to solve this problem myself?? I'm trying to find a way to achieve something similar
Yes, this has been worked out in the issue and I've now finally fixed what I needed to on my side.
I suppose the readme does not mention it, but I know it because I know that fitzgen wrote the library as a part of developing dodrio.
For some reason vscode plugin is much faster. I tried intellij, but it got very laggy on even 100LOC project. 
I actually typed too fast that WASM would explain it. The only way to allocate memory on WASM is using `memory.grow`, which is essentially already a bump allocator, and there is no way to free memory. So isn't that essentially already a bump allocator that never frees? 
I'm not sure exactly what your question is... If you want to learn more about Dodrio, I suggest reading its source, since it isn't very big yet. https://github.com/fitzgen/dodrio If you want to learn about vdom diffing in general, a quick googling turns up these links, which look promising: * https://medium.com/@gethylgeorge/how-virtual-dom-and-diffing-works-in-react-6fc805f9f84e * https://medium.com/@deathmood/how-to-write-your-own-virtual-dom-ee74acc13060
And the borrows it does sometimes affect codegen with very visible results in terms of performance.
The above is just an example really. What I'm really trying create is a struct that can contain any type of number, except for NaNs. &amp;#x200B; Was mostly trying to look for a way of avoiding using a trait and having a separate impl for every numeric type (u8, i8, u16, i16, u32, i32, f32, f64 etc.). &amp;#x200B; Looking around at other crates, seems that people just use macros for this a lot, so maybe that's the solution for now.
After looking through the site it seems that it might not be intended for you to be allowed to use external dependencies.
Because you want to reuse memory after some phase is over and you destroy the bump arena, you have to integrate with the global allocator. If you grow memory yourself, no one else will ever be able to use it, and you can't shrink memory. In general, not every single allocation is bump allocated either (e.g. there are likely allocations outside of Dodrio rendering) so it makes sense to have a layered design that works well with other allocators.
If you grow memory with bumpalo, and then "free it", nobody else can use it right? or is there an API / convention to pass e.g. chunks you don't need to other allocators ? In other words, if your application has many "phases", and each phase has its own allocator, at the end of the application, the memory usage will be the sum of the maximum memory usages of all "phases".
Yes, and ThinLTO on Chrome is usable during development. 
I often need to merge one struct definition into another, e.g. as one adds some derived fields to the original, but should otherwise remain identical. Is there a crate with convenience macros for that? Copy-pasting and maintaining the same fields in two places gets tedious. Note that I need both structs to be flat for readability reasons at sites accessing the fields, so I cannot simply include one in the other.
JetBrains IDEs just have a minimum hardware requirement that's pretty high. Once you're above a certain minimum hardware threshold, the experience gets enormously better, but it's hard to know where the threshold is. My laptop struggles to be completely responsive with their IDEs, but they're just super smooth and responsive on my R7 2700X desktop.
&gt; but optimization-wise full LTO has almost no effects over full LTO I think you meant "full LTO has almost no effects over thin LTO"?
yes, that's what I meant, on chrome thin LTO works across "crates" (TUs)
It allocates chunks from the global allocator, and returns freed chunks back to the global allocator. Excess chunks are automatically freed when resetting the bump allocator. Yes, you should try to reuse allocators, but the freeing of excess chunks means it isn't as bad as the sum of the maximum memory usage of all phases.
&gt; It allocates chunks from the global allocator, and returns freed chunks back to the global allocator. How does it do that ?
Yes. By global allocator, I mean `std::alloc::alloc`. bumpalo is not a global allocator.
Thanks, that makes sense.
&gt; If you grow memory with bumpalo, and then "free it", nobody else can use it right? or is there an API / convention to pass e.g. chunks you don't need to other allocators ? You can't free memory that you got by growing memory directly. You can't give it to the global allocator because from the global allocator's point of view, it is an invalid free: a free of some block that was never allocated. The other alternative would be shrinking memory, but that doesn't work either: first off, there is no instruction for that right now (maybe one day), but also if anyone else grew memory in the meantime, then shrinking wouldn't work because your chunk isn't at the end of memory anymore. Allocators in general don't have an API for adding new, previously unknown chunks of memory. For example, the `Alloc` trait doesn't have such a trait method (and I don't think it really belongs; maybe in a *new* extension trait). There is no reason they *couldn't* but in general they don't and assume that they will be the ones getting new, fresh chunks of memory. &gt; In other words, if your application has many "phases", and each phase has its own allocator, at the end of the application, the memory usage will be the sum of the maximum memory usages of all "phases". Yes, if you aren't getting and returning chunks through the global allocator. Note that it could bloat even further: you might have a logical phase per object and have many objects (e.g. a compiler optimization pass over a CFG might be a phase, and you might run that pass on many CFGs, potentially in parallel) and if you aren't sharing the same bump arena every time you execute that phase, then your memory footprint would get even bigger.
Do you mean the bounds check when allocating from the current chunk? I'm having trouble understanding how a guard page would eliminate that. I suppose you could `mmap` as much RAM as you'd ever allow it to use, followed by the guard region (a mapping with no read or write permissions). Then you wouldn't need to have the code path which actually allocates new chunks. You just keep counting up in virtual address space. As you cross page boundaries, the kernel handles minor page faults by assigning new physical memory for you. (If you want `reset` to return the physical RAM to the kernel, you need to do `munmap` or `madvise(MADV_DONTNEED)` or `madvise(MADV_FREE)`.) But wouldn't you still need a bounds check, in case a single allocation is larger than the entire guard region? Although I suppose it'd be a small code path and totally cold. A large contiguous chunk of virtual address space might also be a deal-breaker on 32-bit platforms.
Global state? In *my* Rustâ€½
lol, everything is nice until your app has two entry points
That's perfect, thanks!
&gt; But wouldn't you still need a bounds check, in case a single allocation is larger than the entire guard region? Although I suppose it'd be a small code path and totally cold. Correct: it would only allow you to eliminate bounds checks for allocations that are smaller than the guard page(s) you have. This works because allocation is parameterized over the generic `T` that it is allocating, so the size check could be easily compiled away by LLVM.
That sounds like an excellent situation for composing structs.
&gt; Reformat File with Rustfmt uses correct config file now Finally!
It's the same IR and machine languages. The difference happens after compiling, during linking and loading. I'll write up more later and try to find some resources to hyperlink, but the short of it is that gnu and musl and msvc are different and slightly incompatible implementations of Posix libc. And the incompatibilities are greater in machine language because Posix doesn't say how machine language interfaces are supposed to work, only the interface in C (plus any C-compatible high level language). Linking/loading are responsible for deciding where in memory to locate the various machine-language subprograms and statically allocated variables, even the stack (which is more or less a multi-megabyte thread local static). Some of those items will be loaded from the libc implementation (and closely related essential libraries like libunwind and libpt). So all those pieces need to work with each other and with Rust's `std` and `core` crates. Thus different toolchains.
Is there any documentation available that describes the work being done? 
Please have someone audit code like this. Your API is UB and is a misuse of `unsafe`. In particular, your API permits getting two mutable borrows of the same value simultaneously using only safe code. That's wrong. You are also mutating a global static without synchronizing. That's UB too (data race).
Ah, of course. I had a total brainfart and completely forgot linking and libc were things. Thanks!
Do note that two live `&amp;mut` to a resource or, heaven forbid, a `&amp;` and a `&amp;mut`, are still UB even in the single threaded case. If you want aliasing mutability you need to use `UnsafeCell`/`*mut`. As this is a case of "sometimes unsafe doesn't matter" / "sometimes unsafe is required", I'd _definitely_ make the `get_mut` functions unsafe if not all of the functions. Keep in mind also that while wasm might be limited to a single worker/thread currently, that's not planned to be forever. I don't think a single threaded wasm module will just break when multi-worker wasm arrives, but I'd still be wary personally. Ideally we'd have something like a global for single-threaded-only-on-pain-of-UB environments that allow `!Sync` things like `RefCell`, but I don't know how practical that is.
&gt; Library writers (especially in web assembly) ... _This library isn't gauranteed multi-thread safe for now_ They're obviously not caring about a multi-threaded environment. But yes, even in a no-multithreading-on-pain-of-UB environment, the safe aliasing of `&amp;mut` is UB in Rust's borrowing model and needs to be `unsafe` at best. Also, posting it to r/rust with such a title as "painless globals" is bound to get some drive-by audits :P
You can see the chat logs for our team meetings, which happen in the #crates-io channel on Discord
Hey there! What does UB mean? I know this library is dirty, but it's made my life so much easier in web assembly land. Maybe there's some better way I could use RefCell to at least ensure some healthy constraints at runtime.
&gt; They're obviously not caring about a multi-threaded environment. Right, I understand why someone might think this, but it doesn't matter. Their API would let me commit a data race (UB) using only safe code. That's a misuse of `unsafe`. (Others have mentioned the possibility of someone restricting the compiler to a single threaded mode, which would make it possible to do something like this safely without explicit synchronization, sans the part about getting two simultaneous mutable borrows to the same location in memory. But such a thing does not exist yet.)
[Undefined Behavior](https://raphlinus.github.io/programming/rust/2018/08/17/undefined-behavior.html). Rust _guarantees_ that a `&amp;mut` is not aliased, and uses this guarantee to optimize. Breaking the guarantee means anything can happen, including seemingly illogical things. Don't do it. It's a contract from the programmer to the optimizer that you won't do something so that the optimizer has room to optimize.
Thanks! What is this UB people keep saying?
See my other comment. You _do not_ want to play chicken with Undefined Behavior.
/u/CAD1997 gave you the short answer. The longer answer is that before using `unsafe`, you should have a firm grasp on what undefined behavior is and what it means to build safe APIs. In particular, you'll want to the [Rustonomicon](https://doc.rust-lang.org/nomicon/) a very careful read. It's probably the best introductory resource on `unsafe` for Rust right now. &gt; Maybe there's some better way I could use RefCell to at least ensure some healthy constraints at runtime. If you're manipulating shared global mutable state, then you must either have some kind of synchronization or expose it as an `unsafe` API to users of your crate. (The latter is kind of a buzz kill, because you'd wind up spreading `unsafe` everywhere any time use `globals`. So you should go for the former.)
&gt; Please have someone audit code like this before asking others to use it. This is a fairly harsh requirement for people to share code. Just something simple like "knowing what `&amp;mut` means" would be sufficient. `cargo publish` should require proving that you've read the book.
Out of respect for you all, I made the whole api unsafe and put lots of dangerous emojis in the README. I still think this library is super useful!
I know. That's why I mentioned chome because in my mind that is a huge project and there are issues using full LTO on it.
To be honest, the more I reflected on this library, I realize that I made it mostly for myself to make my life easier, but it could be misleading to someone new. So I made the whole api unsafe and lots of warnings in the README.
You can't upload a crate with the same version, [crate.io](https://crate.io) will refuse it but you want the README and docs to be updated so no choice, bump the patch version.
Are you using rustc or cargo to compile and run?
\[Not-Yet-Awesome Rust\](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust) (shameless plug!) and \[Request for Implementation\](https://github.com/dtolnay/request-for-implementation) would be my go-to repos for finding something to know where a project might be useful! If you're really just interested in learning, there have been numerous suggestions to try writing simple tools like POSIX utilities. That might also be interesting to you!
I can confirm that, in my laptop (mbp retine 13â€œ 2013) itâ€™s almost impossible to dev using intellij, but in my desktop runs amazingly, also I tried a lot to like vscode with RLS but Iâ€™m used to the smooth experience that you have using intellij ide, the experience itself itâ€™s way better right now, you get the errors in almost no time, quick fixes suggestions, etc. Iâ€™m fascinated by the people that can/like to work with text editors instead of an IDE, but for me itâ€™s faster with an IDE than a text editor.
The above comment was assuming not using the global allocator and seeing where growing memory directly takes you, sorry if that wasnâ€™t clear.
It shouldn't matter, as once you pass ownership, you're not using the results anywhere else. So who cares if the new owner mutates it? And anyways, if you really want to know, you'll be able to look up the API.
I strongly suggest reading the book start to finish before diving in. Having a decent understanding of the concepts and how the language works will make your first inevitable conflicts with the borrow checker a lot less frustrating.
Autoborrowing gets brought up a bunch, there's decent interest to make this happen. However, there's also a common counterargument: people prefer moves to be "explicit", i.e. they like that `foo(x)` unambiguously reads as "x was moved". There are various arguments as to why that's not actually so useful, it's a complex debate. I feel like it's on the lang team's radar to look into this, though. &gt; I vaguely remember than an older version of rust would do this I don't think this has ever been the case
Have you looked into warp? I found it great for HTTP + WebSockets.
I skipped the higher abstractions, since I felt that they're even harder to get working with async/await!. However, that day of trying to get my implementation working might have added enough experience to allow me to take that on. Warp looks exactly like the server I need, thanks!
ThinLTO is automatically used assuming you have incremental compilation enabled (and not -O0). In Cargo, the default release profile has incremental compilation disabled. An experiment was done a little over a month ago to enable incremental compilation for release builds. It was intentionally a temporary thing (about 1 week) in order to collect benchmark results in https://lolbench.rs/ to see what the performance impact would be. Incremental builds can be substantially faster, but the main concern is if it slows down runtime performance. There is a tracking issue here: https://github.com/rust-lang/rust/issues/57968
Have you tried: ``` let client = reqwest::Client::new(); let res = client.get("http://google.com").send(); match res { Ok(v) =&gt; { let document = Document::from_read(res)?; // continue... }, Err(e) =&gt; println!("Error {}", e), } ``` In case of error, you should use [`eprintln`](https://doc.rust-lang.org/stable/std/macro.eprintln.html) instead of `println`. It would be treated as an error.
You can use loops to retry until you get an Ok (but you should probably timeout at some point): let client = reqwest::Client::new(); let res = client.get("http://google.com").send()?; let document = loop { let result = Document::from_read(res).unwrap(); if let Ok(data) = result { break data; } }; 
When would you prefer to consume a value?
I think what you are looking for is either [Mutex](https://doc.rust-lang.org/std/sync/struct.Mutex.html) or [RwLock](https://doc.rust-lang.org/std/sync/struct.RwLock.html). Just wrap your `Vec` in that. Both of those data types are the thread-safe primitives for mutability across threads. This is the shared memory approach to concurrency in Rust. Depending on your use case, you may be able to use the safer message passing approach. I'd read up on [Fearless Concurrency](https://doc.rust-lang.org/1.30.0/book/second-edition/ch16-00-concurrency.html) in the Rust book to understand Rust's approach to parallel computing.
Hi, what you are trying to do is not possible directly, you can't create a struct name by sticking tt together, you can do something like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=25d75076636afbfe72db2fa759ba8f27) or you can use a crate like [mashup](https://crates.io/crates/mashup) or [paste](https://crates.io/crates/paste) and keep your approach. I haven't tried any of them so I can't make any recommendation, if you want a larger selection just search "concat ident" on [crates.io](https://crate.io). Or you could go with a [procedural macro](https://doc.rust-lang.org/book/ch19-06-macros.html#procedural-macros-for-generating-code-from-attributes), it's totally different but can do a lot of things regarding the AST. &amp;#x200B;
PSA: the one true rust IDE is qtcreator + neovim embedded
Rust macros work on tokens, not text. Putting two tokens next to each other doesn't combine them together, it needs to be done explicitly. Macros can't produce new identifiers ([without some hacks](https://github.com/dtolnay/paste)), but it's easy to just pass in the name as well. macro_rules! integer_input { ($num:expr, $name:ident) =&gt; { /// Represents input_int_register_$i. Its meaning depends on the /// programming stored in the robot. pub struct $name { /// The new value for the input register. pub value: i32, } impl CommandField for $name { fn name(&amp;self) -&gt; &amp;'static str { concat!("input_int_register_", stringify!($num)) } fn size(&amp;self) -&gt; usize { size_of::&lt;i32&gt;() } fn serialize(&amp;self, buf: &amp;mut [u8]) { NetworkEndian::write_i32(&amp;mut buf[0..4], self.value) } } }; } integer_input!(0, IntegerInputRegister0); integer_input!(1, IntegerInputRegister1);
You can replace the `println` with any block of code, including a `break` statement that pops you out of a loop that you're in.
You can ensure relevance by requiring a return value from a closure (thus ensuring linearity by combination with affinity). For example, to ensure that `Val` is used linearly, you structure your API like this: fn ensures_linear(f: F) where F: FnOnce(Val) -&gt; Val Then your users will have to pass a callback to `ensures_linear` in order to get a `Val`, and since they need to hand another `Val` back you've enforced no drops.
I don't see how you've enforced anything, at the end of the day you can just drop one of the `Val` on the ground without doing anything with it.
That Rustonomicon book that burntsushi recommended is full of important info. You can take it as a sign of the importance of this problem that there's a whole book on it :) The biggest downside of C and C++ is that they make it too easy to trigger UB, and the fact that safe Rust can prevent UB without sacrificing performance is pretty much the whole reason Rust exists.
I didn't see the first meeting on the working groups calendar -- will the governance meeting be by invitation only?
simplest example - std::convert::From&lt;T&gt;
Yes, it stands for Foreign Function Interface. Depending if you build in debug or optimised release mode, `cargo build --release` what you very likely want to do when eventually deploying, you'll find it in *target/debug* or *target/release*. The reason it still depends on the platform - in your case OSX - is the runtime (memory allocator,...) and platform specific syscalls, when you are interacting with the world. So you would need to recompile for every platform. Plus I am not sure you could use the same binary when also targeting an ancient version of your OS.
Is this better than emacs+rust-mode racer?
Did not realise that. Nice \o/
Isn't fibonacci recursive thus not parallelizeable?
You move them into bin/ and run cargo run --bin file Probably also possible to run them via VS Code, not sure how though
I'd advice you to 1) Change HashMap to Vec 2) Change recursion to cycle Something like this: ```rust let dp = Vec::new(n+1); dp[0] = 0; //dp for dynamic programming dp[1] = 1; for k in 2..=n { dp[k] = dp[k-2] + dp[k-1]; } println!(dp[n]); ``` I believe perfomance of this code suits you. If n is quite a big, you should somehow deal with overflow (e.g. calc modulo 1e9 + 7). If n is very big (I think threshold is something about 1e6) you should use faster algorithms, e.g. matrix multiplication.
I am fighting the type system once again. My main problem is that I have am Option&lt;&amp;char&gt; but need an Option&lt;char&gt;. How can I convert that? Here is the smallest example I could build: [https://play.integer32.com/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=14195420f4b9d67b1218c88b85e1950a](https://play.integer32.com/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=14195420f4b9d67b1218c88b85e1950a) Thank you for your help.
intellij-rust's issue tracker has a [few variations of unresolved reference issues](https://github.com/intellij-rust/intellij-rust/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+unresolved) currently open.
`opt.cloned(0)` should do what you want.
I don't think there is a calendar for all the working group meetings (or at least I'm not aware of it). Did you mean the compiler team calendar with all the compiler working groups meetings in it?
I was talking about a mutable borrow which is temporary. Moving is totally fine.
So if I wanted to have a build for another OS I would have to build again using a VM or a regular install?
Are you actually using the 2018 edition? It will be in your cargo.toml if it is. You can use `cargo fix --edition` to automatically add it as well as fix the use statments.
Clean has uniqueness types not linear types, no? I thought this was also the case for rust, at least that was what I was under the impression mut references were. But someone that actually knows rust correct me if I am wrong.
Itâ€™s not local to my crate, itâ€™s an Extern module. Iâ€™m using rust v1.33 stable, so it should be in there. `use` works as expected for Extern Crates in other places.
&gt; Weâ€™ll be announcing a first meeting soon kick things off, and try to settle for periodic meetings and topical meetings after that. Practically speaking: we still need to wrangle peoples schedules. Follow that post and you'll get notified. The first group is indeed invitation only currently, the task groups will be open to join. The initial group is really just for setup and we have previously not had good experience with WGs starting very big. We have very good experiences with WGs growing very big ;).
Cargo is not a package manager.
I don't know how to implement your `ThreadedDataPool` idea in a way that convinces the Rust compiler it's safe from data races, but I'm a just a beginner. If you're just looking for a way to efficiently collect a bunch of values from a bunch of threads, maybe something like Crossbeam's `ArrayQueue` ([docs](https://docs.rs/crossbeam/*/crossbeam/queue/struct.ArrayQueue.html)) will work? (It probably depends on unsafe code outside of the standard library.) BTW, `Arc` solves a related but different problem: if multiple threads point to a piece of data, how do you know when you can deallocate it? The "atomic" refers to the "reference count" it maintains; when it goes to zero, the target data structure can be safely deallocated. There remains the separate problem of convincing Rust that your threads won't access the target data in a way that could cause a data race. Like /u/furnavi says, the standard solution is to wrap it in a `Mutex`, but it looks like you're trying to avoid the locking overhead.
Can we please stop downvoting OPs comments? I find it highly immature to respond this way to someone who clearly wants to learn.
You should. Please never create multiple versions of something with the same version number. Does not matter if it's just a piece of documentation that changed, just a single typo that's fixed. Never, ever do that. And if you see examples of people/groups doing differently, call them out on it. In a friendly, but decisive manner.
Correct. But I could use it to install binaries from it. Unfortunately cargo doesn't make it easy to separate bins from libs, but I could do something like: admin$ cargo install exa; cp .cargo/bin/exa /usr/local/bin where admin is a sudoer with an cargo install alias something like that.
1. You don't necessarily need a GUI to log in to a RADIUS network - nmcli can do it in most cases. 2. You can build Arch packages for Rust software - in fact, exa [is already in the repos](https://www.archlinux.org/packages/community/x86_64/exa/). 3. Cargo is not a package manager and doesn't do most of the things a good package manager needs to do. 4. If you want TFS, you'll have to build a kernel module for that, and considering TFS itself is not even stable, that sounds like a terrible idea. 5. You don't need to install _anything_ for your "RRPR" stack - all of this stuff will get compiled into your final binary. Why do you need it to be a "stack"? The whole idea of a "stack" is basically dead now that we have proper async web servers for basically every language out there. 6. You can't replace POSIX tools with non-POSIX-compliant tools and expect things not to explode.
Yeah, macros are the way to make an impl for every numeric type.
Isn't the compiler clever enough to figure that out? 
&gt; 6. You can't replace POSIX tools with non-POSIX-compliant tools and expect things not to explode. This is especially important. The "standard tools" that have been rewritten in Rust are very much not drop-in replacements of the originals.
This is fine for immutable borrows, but it's a problem for mutable ones. For immutable borrows I don't think that it's a big enough ergonomics problem to justify the extra magic.
&gt; Allocators in general don't have an API for adding new, previously unknown chunks of memory. For example, the Alloc trait doesn't have such a trait method (and I don't think it really belongs; maybe in a new extension trait). There is no reason they couldn't but in general they don't and assume that they will be the ones getting new, fresh chunks of memory. They don't for good reason: in general it would be impossible to abstract about all the properties that memory would have in a real system, and that the allocator might need to know about. In WASM these issues are insignificantly smaller: there is only one property (the address space, which currently is always 0), and that's it. 
&gt; Iâ€™m using rust v1.33 stable, so it should be in there. Only if you started the project with a version of rust that was already 2018 by default.
I think you want to go to /r/rustgame
Did you try `use ::gl;` and does the line number for the error `no gl in the root` match the use statement?
Consider a webserver that accepts a request, passes a part of the data to an auth service. Now, we don't really need that part of the data again so consider that consumed. Maybe we want a threadpool that accepts closures but we only want that closure to be stored in the pool and we don't want to let a library user to try and invoke that closure from a different place.
I prefer the semantics the way they are because it lets me reason about what a function will do with an argument at the call site, not the function definition.
1. You don't necessarily need a GUI to log in to a RADIUS network - nmcli can do it in most cases. Does nmcli come with archlinux.iso? Can I run it from the live cd / chroot and login to it? I need other things too. A custom iso is a good idea and speeds up the installation.
If you do it right, it's iterative.
I believe it does, at least on archiso. The official images only have wpa_supplicant, but even that is (theoretically) enough - it'll be kind of painful, but you can definitely make it work. 
You can find the article series now also on [dev.to](https://dev.to) [https://dev.to/gruberb/web-programming-in-rust-02x-deploy-your-first-app-1k05](https://dev.to/gruberb/web-programming-in-rust-02x-deploy-your-first-app-1k05)
Sounds like a bad idea. A MITM attack could point cargo to some other register containing compromised sources for `exa`, and you would just be installing malware as sudo. One of the jobs of package managers is making sure these things do not happen.
Lock-free data structures are complicated, you cannot use `Vec` directly, as it's non-atomic. This was my attempt to implement this structure in question: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=75ffa90714ae7bf41efcaed52e781cb1. I tried my best to ensure its correctness, but chances are it's subtly wrong.
If you want to learn about multithreading, I suggest you pick a different problem. I'd go as far as to claim that the most efficient ways to compute Fibonacci numbers are either inherently serial or don't require any kind of iteration/recursion at all. There's an exact formula to derive Fibonacci numbers but it includes the golden ratio as irrational number if I recall correctly. So, it's difficult to get exact results beyond 52 bits if you use double precision floats. There's also a ["square-and-multiply" version](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=dce48be846ff5124ae69f23449eb7b6c) which is super efficient, can be done purely with integers and is usually not well known. It only requires O(log(n)) iterations. 
Honestly, your gifs made me want to use this IDE. I'm already a huge fan of Rider (much better and lighter than Visual Studio), I'll definitelly give a try to IntelliJ Rust!
I am pretty sure there is also a constant time solution. But the iterative one is usually best.
&gt;2700X Looks like 3570k is out of scope, because for me, IDEA is unbelievably slow.
As someone who worked on SAT solving in grad school, I just wanted to say how much I'm enjoying this series. It's superbly well-written and really shows the reasoning steps well. Thanks much for sharing it!
I'd recommend to read [The project layout](https://doc.rust-lang.org/cargo/reference/manifest.html#the-project-layout) section in The Cargo Book. You have couple of options ... Place your binaries into the `src/bin` folder and run them with `cargo run --bin $name` (`cargo run --bin another`, ...). Keep them in place and update your `Cargo.toml` to something like: ```toml [[bin]] name = "main" path = "src/bin/main.rs" [[bin]] name = "another" path = "src/bin/another.rs" ``` From the Cargo docs: &gt; Do note, however, once you add a `[[bin]]` section (see below), Cargo will no longer automatically build files located in `src/bin/*.rs`. Instead you must create a `[[bin]]` section for each file you want to build. In other words, once you add `[[bin]]` section for one binary, you have to add similar sections for all other binaries, because Cargo will no longer look for them. In case of multiple binaries, I'd also rename `main.rs` to something else. AFAIK the VS Code Rust plugin doesn't support what you want. At least it didn't when I was playing with it. But you can configure custom task in the `.vscode/tasks.json` file ... ```json { "version": "2.0.0", "tasks": [ { "type": "shell", "label": "Run another", "command": "cargo", "args": [ "run", "--bin", "another" ], "problemMatcher": [ "$rustc" ] } ] } ``` ... and then you can hit `Cmd-Shift-P` -&gt; `&gt;Tasks: Run Task` -&gt; `Run another`, which is the `tasks.json:tasks[].label`.
You're right, I did misunderstand what you both were talking about. It seems to me that because `memory.grow` would need to be shared with other allocators, you can't take advantage of its similarities to a bump allocator, so there would be no benefit to using it directly for bumpalo, and there are significant downsides to doing so (such as the inability to free chunks).
I'm trying to get started with gtk-rs but somehow I can't really wrap my head around it. The API documentation isn't all that helpful, I found a lot of structs don't show which methods are implemented on them. Is there any good gtk-rs tutorial out there (ideally for someone who hasn't dealt with gtk before)?
So a combination of "memory is freed sooner so it's more optimized" and "smaller scopes are safer"?
Yes and no. It is possible to compile for another platform, referred to as cross compiling. See for example https://github.com/japaric/rust-cross. But might be a bit out of date.
I should add that cross compiling in reality only works within certain limits. To my knowledge it is, for example, not trivially possible to compile for OSX under Linux. A VM or an install is probably easier.
Depends on what is important for you! The best way is probably just to try it.
I've been using this crate in my demo mail reader and it's been working well. https://www.vandenoever.info/blog/2018/09/16/browsing_your_mail_with_rust_and_qt.html
To be more precise, only if the `Cargo.toml` file, section `[package]`, contains `edition = "2018"` line.
&gt; Does not matter if it's just a piece of documentation that changed, just a single typo that's fixed. Never, ever do that. What problems are caused from not bumping the patch version from making a commit to fix a typo?(that'll end up in the next version bump via a commit that derives actual meaning for a bump/release)? Doesn't that just encourage delaying changes and batching them all into a single generic commit, or otherwise attributing a version per commit... which may involve pairing a git tag.. though that rather loses the point/value of such if every commit has one? Or did I misunderstand, and you meant **don't publish/release** additional changes **as an existing version**? (this would make more sense imo) 
Well.... intermittent fasting is a thing
&gt; Another thing to consider in the question of "scientific Rust" is the fact that the Rust community is overloading averse and the scientific programming community is not, IME. Rust is a tough sell against C++ for writing scientific libraries. Rust has sum types and traits though, which decreases the need for overloads.
Oh, interesting point, thanks. I'm not sure it's worthwhile, though. The current approach of using the global allocator means that memory mappings are reused (if the global allocator is sane). If you switch to directly calling `mmap`/`munmap`, you either do a lot more syscalls or have to start reusing/freelisting your Bumps. (Maybe slow syscalls, tooâ€”I've had trouble lately with an application having poor tail latency because these sorts of VM operations were waiting on a global semaphore because some other cgroup on the machine was tight on RAM or something.)
I'd recommend a procedural macro. They are extremely straight-forward fundamentally, although they can definitely get complex as things get more complex. For a proc macro that does what OP is trying to do, it's as simple as to_string()ing the TokenStream, and calling format!(...).parse().unwrap()
Do you want the user to enter math formulas or Rust code?
Database pages usually have a uniform size (chosen based on device and os characteristics) and aren't accessed serially; they're accessed randomly. If you're serializing anything, it'll be serialized before it goes into a Page. Often the database engine will reimplement a bunch of os features related to caching pages and scheduling io. This isn't necessary for a simple DB. But you probably should be using `mmap` or `pread`/`pwrite` (and don't forget `fsync`!) to load and store pages to disk. I think you should find a textbook or tutorial targeted towards C or Pascal or Ada or maybe Fortran because those languages would design their in-memory data structures more like what Rust needs. Expect to write some unsafe as well.
writing a parser and interpreter to handle math expressions is not too bad, you can either do that or find a crate that does it already. 
In Rust code, it is understandable and i think it might be easier to implement. &amp;#x200B;
I assume it would be easier to interpret e.g. S-expression based math formulas than writing a Rust code interpreter.
Yeah i tried using that a bit before, but couldnt quite figure out how to connect the dots. What I've been trying is to have a custom error that can turn into an http response, but I can't clone the JsonPayload error: https://github.com/agmcleod/sc-predictions-server/blob/master/src/app.rs#L36
I did try that, but I've always failed at the part "interprete the code" 
Thinking about it, you're right
Hi there! I have been playing to achieve the exact same goal (fast math function evaluation), and I discovered that there's a very small C compiler, named TinyCC, which can be "retrofitted" to do such a task. One month ago I managed to get a rust binding working, you can find it [here](https://github.com/francesco-cattoglio/libtcc-sys). There is a test that deals with compiling a function that takes a float and returns a float, it should be a good starting point for you! I don't know if it will actually work on anything but linux, since I had no time to test, and the build script needs changes. But I am a Rust newbie and had no time to put in this project right now. A note on performance: I run some tests using libtcc in C++, performance can be amazing when compared to other function evaluation methods. In particular, even if TinyCC lacks optimizations, function eval is up to 4 times faster when compared to [muParser library](https://github.com/beltoforion/muparser), and 2 times faster than a naive OpenCL-based solution (using POCL so that OpenCL runs on the CPU).
thanks a lot for your response ! i will look into it :)
Hey all, [OxidizeConf](https://oxidizeconf.com) is the first conference focused on Embedded Rust, and we've just announced our (almost complete) speaker list! The conference will be April 26th-29th in Berlin, and it would be great to see you all there!
are the chat logs public. I don't have a discord account.
Before I forget: libtcc is NOT thread safe, some parts of compilation routines use some shared state. To run the test you should `cargo test -- --test-threads=1`, otherwise bad things will happen and laundry will be eaten.
https://docs.rs/meval/0.2.0/meval/ might be helpful.
Wow, that's exactly what I was thinking of. Thank you!
this might help: https://youtu.be/kzDuHh6kolk its in fsharp but you can structure rust very similarly as rust enums are also discriminated unions
Thanks!
&lt;3
yw. There are other options that might be helpful as well. I know the redox project has the calculate crate which would also be worth looking at.
If the user has the rust toolchain installed, it should be possible to pass the code to a file, add some 'magic' (necessary imports, namespacing, FFI etc. around the codestring), compile the file to a dynamic object and load that at runtime, similar to a .dll/.so plugin system. This would have a little overhead whenever the code is updated due to invoking `rustc`, but if you want to run intensive computations, this will likely give the best performance. This may well be overkill if you just want a simple WolframAlpha-like graph plotter or Excel-like data munger, though.
If you're okay with writing the formula with other language. I've done something similar (hot code reload) with lua: [https://devpoga.org/post/parsing-streaming-data-actix-lua/](https://devpoga.org/post/parsing-streaming-data-actix-lua/) Although the example is about actix, the general idea should be the same.
Whoop, I made a thing too: [pupil](https://docs.rs/pupil/0.1.3/pupil/) Does `f64` math and allows custom constants and functions to be defined.
Regarding the Iterator: `xs.iter().zip(0..xs.len())` should be equal to `xs.iter().enumerate()` \- except that the returned values are swapped. Interestingly using enumerate instead of zip results in the exact same code as your "manual" loop: [https://godbolt.org/z/nqO8hP](https://godbolt.org/z/nqO8hP) (If i didn't miss anything)
Cool, thanks. I tried a similar approach, but for some reason thought the keyword was "copy"
I am not saying you need to bump the version for every commit. But you should not create a new release with a version number for which there already exists a release. No matter how small and insignificant the change is.
Very nice. Is there a description how it works? Do you send and parse Gremlin bytecode?
Taking a look at ArrayQueue, it doesn't seem there's any simple way to extract the information into a big vector or array without another thread sitting at the other end and packaging it. And yup, I'm mass point plotting with this, so the more overhead I can avoid, the better.
Hi, for now it's send the gremlin script in text format for evaluation. Would be nice in the futures to support it as GLV and generate the bytecode.
&gt; Or did I misunderstand, and you meant don't publish/release additional changes as an existing version? (this would make more sense imo) I suspect so, yes. Creating a new commit isn't the same as creating a new version. I'm not sure where the confusion about multiple versions came from---it doesn't look like the OP is confused about that.
&gt; I'm wondering what the usual thing people do here since inline docs are essentially part of the code If it's an important doc fix, then I'll put out a patch release after the PR is merged. If it's a typo or otherwise not pressing, then I just let those sorts of changes collect until I subjectively feel like a new release is warranted.
I would be weary of the data function. "push" calls in other thread may cause a race on the top few elements if the data slice is passed on before they can finish writing.
Woah, I thought crates never reach 1.0.0...
I can't seem to find the playground on vlang.io. Is it up yet?
Is a no go for Rust. Automatic borrow will implicitly affect (expand) the object's lifetime.