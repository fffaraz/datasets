As far as I know it has only been integrated with Vim. It shouldn't be too hard to integrate with other editors, I don't know enough about them to say for sure. I would love to have a PR or instructions I can link to for this if you know Atom :-)
Isn't there that beautify plugin? I saw it calls `rustfmt` but I had problems with that and reverted to the terminal.
Thanks for the help! I was able incorporate your gist into my real project and it's working great! &gt; The reference-reforging trick I showed above is a little more complicated because of the Boxes; basically, you need to deref &amp;(mut) Box&lt;T&gt; to &amp;(mut) T before attempting to cast to *const(/mut) T. That's why those extra asterisks and ampersands are needed. Thanks for the detailed explanation. Took me a few times reading this before that syntax started to make some sense. I wasn't accounting for the boxes at first so it seemed like there was an extra asterisk in there.
What I typically do is `cargo doc --open`, which should open your default browser to the correct html file once it is generated. There's a gotchya that if your crate doesn't have any documented items this won't actually open anything or generate `index.html`. I think unexported items may not count, so try starting `main.rs` with `//! placeholder` -- as a document to the crate, since main is not exported. This also documents the dependencies, which is super awesome (except for not including their readmes).
`unsafe` totally slipped my mind! Thanks!
/u/handle0174 hit the nail on the head, if you use module level documentation //! Prints "Hello, world!". fn main() { println!("Hello, world!"); } instead you'll get some docs.
Thanks :) This was exactly what I needed.
Or maybe it would just be a `do!` macro, I haven't really tried to design it
http://www.ncameron.org/perf-rustc/ is the one I've been working on
Well we can, technically, do all the same optimisations that LLVM does, but that would be a waste of effort. One of main ones would be inlining at the MIR level. This is especially useful for small generic functions that get used for many different types, most of the methods on `Option&lt;T&gt;` for example. Currently we will generate a LLVM function for each monomorphisation, which can result in many identical or almost-identical copies of the same function. This slows down LLVM's optimisations a bit, since it has more functions to work with. Personally, I'd be interested in investigating partial monomorphisation at the MIR level, instantiating some, but not all, of the type parameters in a function. Not sure how useful it would be, but it seems like an avenue worth exploring. The remainder are Rust-specific analyses that make use of things like move semantics and aliasing to optimize code. Ultimately though, Rust is low-level enough that there's not much we can do better than LLVM can. There's plans to implement some basic optimisations at the MIR level, this PR for example and also constant propagation. They're simple enough that duplicating LLVM's functionality isn't really much effort. 
&gt; Is borrowck performed on the HIR or the MIR? I believe it's on MIR, which is why MIR is a prerequisite/enabler for more advanced borrowing features like non-lexical lifetimes. &gt; Is rustc written such that it would be reasonable to think of it in terms of three transition stages, rathern than 4 intermediate representations? That's a good way to think about it, and then optimisation passes become `MIR -&gt; MIR` and `LLVM IR -&gt; LLVM IR` transitions. (There's also a `LLVM IR -&gt; machine code` pass to get the final code that is actually executed.)
Something useful might be Rust exploiting its better knowledge of data representation, rather than relying on LLVM to try to deduce the semantics from the in-memory format and whatever annotations we can provide. For instance, iterators sometimes don't optimise particularly well, because the null-pointer optimisation for `x: Option&lt;&amp;T&gt;` can leave LLVM confused. In that case, `x` is a plain pointer, and constructing it via `Some(reference)` is a noop, and we're just hoping that `reference` being guaranteed non-null is enough for LLVM to work things out. The result is some things equivalent to `Some(reference).is_some()` aren't reliably optimised to `true` (i.e. LLVM can easily lose track of the fact that `reference` is guaranteed to not be NULL). However, getting optimisations like that to work would require inlining in MIR, which would be unfortunate to duplicate with LLVM (e.g. requires some sort of cost-model to decide when to inling, possibly tuned per-platform). (That said, inlining is a prerequisite to get the most out of most optimisations, so it's almost certainly inevitable if we're going to do a lot of optimisations on MIR.)
Thanks for the help. I know there is a `--markdown-playground-url` flag, but I don't really see how it changes things. I'm also busy atm, so I will look into it later :)
Generally I believe the plan is to essentially throw away the old representations as compilation progresses, although mapping information has to be kept around so that error messages can be reported properly. Borrow check is currently performed on the HIR, but Niko wants to move it to be performed on the MIR.
IIRC, if it's set, then Rustdoc generates the playground links.
Thanks! I definitely won't use build scritps to generate code as Strings, if I did this I'd just end up writing my own AST enum and pretty-printer in order to generate the strings. The procedural macros look to be a lot what I'm looking for, since I could just manipulate ASTs and compile them without pretty-printing or generating intermediate files. I'm not too worried about stability yet, since I'm just starting my project and don't even know if I'll ever fully realize it. But does that mean it's not likely to make it into the stable compiler, or just that it's unstable now and they don't want to commit to anything?
There are also optimization passes on `MachineInstr`s that run after instruction selection. These are surprisingly sophisticated (e.g. `MachineLICM`).
Is it nightly only? $ cargo install --git https://github.com/rust-lang-nursery/rustfmt No such subcommand 
I think it requires latest cargo, but works on stable rust. In practice that means it's only waiting for the latest cargo to be released (which should happen within two releases at most). I use multirust and this works: multirust run nightly cargo install --git https://github.com/rust-lang-nursery/rustfmt
And you thought the error handling post was long... Hah! API docs should be self contained: http://burntsushi.net/rustdoc/fst/
The HIR for a crate can be dumped via the `--unpretty`option, and the MIR for a function can be dumped in graphviz's dot format via `#[rustc_mir(graphviz = "some-file")]`, with nightly. E.g. for `basic.rs`: #![feature(rustc_attrs)] #[rustc_mir(graphviz = "basic.dot")] fn main() { if true { let x = 1; } else { 2 + 3; } } `rustc basic.rs --unpretty hir -Z unstable-options` looks like: #![feature(rustc_attrs)] #[prelude_import] use std::prelude::v1::*; #[macro_use] extern crate std as std; #[rustc_mir(graphviz = "basic.dot")] fn main() { if true { let x = 1; } else { 2 + 3; } } And after a normal compile, `basic.dot` contains: digraph Mir { BB0[label=&lt;&lt;TABLE ALIGN="LEFT"&gt;&lt;TR&gt;&lt;TD&gt;BB(0)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;if(Constant { span: basic.rs:4:8: 4:12, ty: bool, literal: Value { value: Bool(true) } }) -&amp;gt; [BB₃, BB₄]&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;][shape="none"]; BB1[label=&lt;&lt;TABLE ALIGN="LEFT"&gt;&lt;TR&gt;&lt;TD&gt;BB(1)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;return&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;][shape="none"]; BB2[label=&lt;&lt;TABLE ALIGN="LEFT"&gt;&lt;TR&gt;&lt;TD&gt;BB(2)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;diverge&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;][shape="none"]; BB3[label=&lt;&lt;TABLE ALIGN="LEFT"&gt;&lt;TR&gt;&lt;TD&gt;BB(3)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;Var₀ = Constant { span: basic.rs:5:17: 5:18, ty: i32, literal: Value { value: Int(1) } }&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;goto -&amp;gt; BB₅&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;][shape="none"]; BB4[label=&lt;&lt;TABLE ALIGN="LEFT"&gt;&lt;TR&gt;&lt;TD&gt;BB(4)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;Temp₁ = Add(Constant { span: basic.rs:7:9: 7:10, ty: i32, literal: Value { value: Int(2) } },Constant { span: basic.rs:7:13: 7:14, ty: i32, literal: Value { value: Int(3) } })&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;drop Temp₁&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;goto -&amp;gt; BB₅&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;][shape="none"]; BB5[label=&lt;&lt;TABLE ALIGN="LEFT"&gt;&lt;TR&gt;&lt;TD&gt;BB(5)&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;goto -&amp;gt; BB₁&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;][shape="none"]; BB0 -&gt; BB3[label="0"]; BB0 -&gt; BB4[label="1"]; BB3 -&gt; BB5[label="0"]; BB4 -&gt; BB5[label="0"]; BB5 -&gt; BB1[label="0"]; } Which can be [rendered](http://i.imgur.com/6JiFG9t.png) (with e.g. `dot -Tpng basic.dot` using the graphviz tools). dotdash (author of the PR) also created some [before/after renderings](https://imgur.com/a/vC9GC) for the PR. 
Yeesh! Typo. Nice find! Fixed. :-) Thanks!
Thanks! That's very kind of you to say! :)
Higher-ranked polymorphism is about more than just monads. A concrete example of something I'd like to see that it would enable is generic owning references. What that means is to be able to encapsulate the ownership a value inside an object which provides access to an arbitrary borrow into it. So an owning reference could own a vector and provide access to an arbitrary slice of it, or to a reference to an arbitrary element of it. You could then move the owning reference around just like a vector and know that the slice it provides access to remains valid. The challenge with doing this is that you don't want to be limited to a particular type of reference. The [owning reference library] is artificially limited because it requires you to use shared references (i.e. `&amp;T`) for the reference part of the owning reference, which rules out interesting patterns like using mutex or `RefCell` guards, for example, or even `&amp;mut` references. [owning reference library]: http://kimundi.github.io/owning-ref-rs/owning_ref/index.html
I often find it easiest to represent any kind of graph-esque types using indexable data structures. It means that there is always a single, clear owner of the data (the single data structure) and allows for easily traversing the graph in all kinds of fancy ways safely. I'm not sure exactly what kind of tree you're aiming to make, but here's my attempt at a [rose_tree](https://github.com/mitchmindtree/rose_tree-rs). If you want to see other examples of this kind of inter-relational, indexable data structures, you can also see [daggy](https://github.com/mitchmindtree/daggy) and [petgraph](https://github.com/bluss/petulant-avenger-graphlibrary) (which rose_tree and daggy use internally). Edit: I should mention that /u/neutralinostar and I have also [been discussing](https://github.com/bluss/petulant-avenger-graphlibrary/issues/13) a [`Walker`](http://mitchmindtree.github.io/daggy/daggy/walker/trait.Walker.html) trait - an abstraction inspired by the `std` `Iterator` trait but designed specifically for safely traversing indexable graphs in clever ways *without borrowing them* - this allows us to do things like safely mutably borrow multiple nodes/edges at a time.
Hi /u/sezna, I live in the North Austin area. I am open to coming to a place that is convenient for both of us. Please let me know.
"Outputs must also have an additive identity, I, such that the following laws hold: x + I = 0" Not x + I = x?
There are the syntax::ast classes (in src/libsyntax) which make up the AST.
A lot of the downsides of memory maps you mention can be mitigated through the use of APIs like [mlock](http://linux.die.net/man/2/mlock) and [madvise](http://man7.org/linux/man-pages/man2/madvise.2.html) (I imagine there are equivalents for Windows, but I haven't checked). Using these means re-introducing a lot of the complexity that using memory maps saves you from, though.
Great read! Just one nitpick: You wrote dist("foo", "fobo") == 2 It should be 1 (just one insertion), right? Since you're comparing yourself to gzip &amp; xz, I'd like to hear in gory details about the memory representation you've used! Especially, are you using smaller number of bytes (or maybe even bits) to represent the most common state transitions (+1 &amp; 0, I guess)? Forgive me, if it's documented nicely in the source somewhere, I'm on mobile, so I've only skimmed the repo. *Edit:* I should look in `raw::{node,pack}.rs`, right?
What is the nursery actually?
Another advantage of indexable data structures is, that they most likely give you a higher cache coherence. The memory footprint of the graph structure is lower, because the real data is kept elsewhere and the real data can be kept in a continous memory region.
Since you're building for x86_64, could the "Interrupt Stack Table" feature be helpful here? I e, if the interrupt moves to another stack and leaves the current stack untouched, then perhaps the red zone optimisation could still work. If not, could the red zone work provided this: 1) you don't use the first few bytes of the red zone (that the interrupt clobbers itself by saving CS/RIP) 2) the interrupt handler looks like this: sub esp, 128 push lots of registers handle interrupt pop lots of registers add esp, 128 iret 
It's a GitHub org where libraries (and now tools) live if they are in active development and on the right track for inclusion (not guaranteed) in the rust-lang org (which is meant to indicate a stable/mature library). You can think of the stuff that lives here as recommended by the Rust community, and probably a good idea to use, but you shouldn't bet your product on it.
In this case you most likely don't need a tree at all.
Plugins currently have access to all of the compiler’s internals. Stabilizing all of it means that almost any change becomes impossible, so we’ll need another solution. There’s discussion at https://internals.rust-lang.org/t/the-future-of-syntax-extensions-and-macros/2889 about what kind of API to expose.
Typed unions?
I use HTitle, which hides the title bar in GNOME when maximized. Perhaps you'd be interested?
A very interesting read, even if I never tried to hack on a kernel. Thanks for writing and sharing this !
Haha, I have the same feeling. I do happen to have the entire reddit corpus on my disk, so it might be cool to give that a whirl. Details are here: https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/
Would you believe that I did actually proof read the post? :P Fixed! Thanks!
&gt; It should be 1 (just one insertion), right? Ooo. Yes! Great eye. Fixed! Thanks! &gt; Since you're comparing yourself to gzip &amp; xz, I'd like to hear in gory details about the memory representation you've used! Especially, are you using smaller number of bytes (or maybe even bits) to represent the most common state transitions (+1 &amp; 0, I guess)? Forgive me, if it's documented nicely in the source somewhere, I'm on mobile, so I've only skimmed the repo. Edit: I should look in raw::{node,pack}.rs, right? Right, so, umm, great question! Unfortunately, this is indeed one part of the `fst` crate that is *not* documented. The format is rather complex, so I've been putting off actually writing down the format. Firstly, a lot of the tricks I employed are not my own. I got some of them from the third and fourth papers linked here: http://blog.burntsushi.net/transducers/#references --- I also got some of them from studying Lucene's implementation, which I think, in turn, got some ideas from [morfologik](https://github.com/morfologik/morfologik-stemming). Secondly, you're right, in order to completely understand the format, you will have to read the code. `src/raw/node.rs` is where it's at. `src/raw/pack.rs` is also used, but they're mostly just wrappers around `byteorder` calls. The problem is that the code is missing some high level overview. The high order bit is that most states are represented by a single byte. The basic structure of a state is something like this: * The first byte encodes meta information about the state. Maybe whether it is final, maybe how many transitions it has, or maybe even the actual transition itself if it can fit. It also includes the type of state (listed below). * The next bytes might contain the number of transitions if it couldn't fit in the previous byte. * The bytes after that contain pack sizes. * The bytes after that correspond to the inputs of each transition in lexicographic order. Each input consumes one byte. * The bytes after that are the transition addresses, i.e., the pointers for each transition to other nodes. The addresses are delta encoded with respect to the current node. * The bytes after that are the output values, if any exist. If a state has all zero outputs, then we can drop this section completely. Other important things to mention: * States are compiled *backwards*. Namely, in order to get "most states compiled to one byte" to work, you need a way of representing a pointer to the next state without actually encoding its address, since it will usually consume at least an additional byte or two. The key is observing that there is infact quite a bit of locality in the FST and that the most common types of structure in the FST are long sequences of states where each state has actually one transition. These strings of states are usually compiled one after another, which means that they live next to each other in the encoded FST. So if you leave out the transition address, we can assume that it points to the "next" node. Unfortunately, the problem with this is that states are compiled in reverse order, so there's no way to actually jump to the start of the next state because you don't know how many bytes the previous (errm, next) state consumed, so you can't jump to it implicitly. However, if you encode the states in reverse order, then you know that the previous (errm, next) state starts at the byte immediately preceding your "one byte state." So to be clear, the algorithm presented in the article requires that states near the end of the FST be compiled first. To compensate for that, we write the actual state backwards too. So the list above this one? Flip it around. The state byte actually comes last (i.e., it is at a higher address in virtual memory than any other byte in that state). * Transitions are *packed*, which means a state with N transitions requires `N * k` bytes where `k` is the number of bytes required to encode the largest integer in the transition addresses/outputs. This means we get fast random access. An alternative is to use varints, which could save some space. This is kind of the short pole in the tent though, because the number of states with more than a few transitions is very small compared to the number of states with 1 or 2 transitions. Still, it's worth a try. The code splits the below states into three cases. In the code, each state has a `compile` method, which is responsible for encoding the state. Most of the rest of the methods on the state are responsible for decoding it. * [Any state](https://github.com/BurntSushi/fst/blob/38f0ec66535509ce28db609046db3d4907f7f29f/src/raw/node.rs#L425). If a state doesn't fit the below two cases, then this can handle it. * [A non-final state](https://github.com/BurntSushi/fst/blob/38f0ec66535509ce28db609046db3d4907f7f29f/src/raw/node.rs#L293) with no outputs that points to the previous state compiled. If the input on the transition is "common" (e.g., `a-z`), then it can be encoded in one byte. If the input is bigger than that, then it takes two bytes. * [A non-final state with one transition](https://github.com/BurntSushi/fst/blob/38f0ec66535509ce28db609046db3d4907f7f29f/src/raw/node.rs#L338). This lies somewhere between "any state" and "non-final with one transition pointing to the previous state compiled." It lets us encode a bigger range of common inputs into the initial state byte. And when it comes time to lookup a transition, you need to do case analysis over the type of the state: https://github.com/BurntSushi/fst/blob/38f0ec66535509ce28db609046db3d4907f7f29f/src/raw/node.rs#L120 The `raw/node.rs` code has been rewritten about 3 times now. It was hard for me to write, and since I'm not a compression wizard, I bet I made some amateur mistakes and have missed some opportunities!
I'd really appreciate 3200x1800 if it's not too much to ask!
[Here](http://blog.burntsushi.net/transducers/#trie-construction) You wrote: "Consider a set with the keys mon, tues and wed..." But the following FSA contains "mon", "thurs", "tues".
Time spent in LLVM is reduced, but not by much. Here are graphs for time spent in type checking/translation vs LLVM passes. http://imgur.com/a/eTrVe
The kinds of blogs you write are the kind I imagined as I was learning things, but never got around to writing. Thanks for setting a good example. I will aspire to follow your lead.
Ahhh, that makes sense. Sounds like exactly why I never go to midtown in general ;)
Yes, it works with atom-beautify
0_o Didn't see that coming. I'm guessing the linux-only constraint is largely the desire to be libc free and use syscalls directly, which AFAIK isn't really supported by Windows. It's nice to see an unwinding-free system, though. It'd be really cool if the compiler properly understood that so you could move out of `&amp;mut`s temporarily.
Yeah I'm with you on this. I read it, laughed at how shocked I was that this was made and continued to appreciate the readme. Fantastic work /u/AlekseiPetrov. What a pleasant surprise this morning :)
We use it in Servo today, and it works well. Servo's on the nightlies though.
Cool, this looks really helpful for sorting out the fiddly pieces of my implementation.
Windows doesn't support the lowest level system call interface, where you literally put a code in rax to say what system call you want, other arguments in other registers, and call the 'syscall' CPU instruction. The reason is that Windows frequently rearranges the table of what numbers correspond to what calls. The only supported way of issuing a system call is going through the DLL like you said. On Linux, if you try to do that, Linus bites your head off. They do not break the ABI. Full stop.
? The first example of enums in the rust tutorial is a proper ADT: enum Message { Quit, ChangeColor(i32, i32, i32), Move { x: i32, y: i32 }, Write(String), } It's a sum of products. A language having ADTs means that it has first-class support for working with sum of product of types. I don't think it's especially misused here.
This should be more a user decision though, no?
There are many ways to do syscalls, the way of Linux is not the only model. Windows approach, which is not unique among commercial OSes, allows to refactor the kernel and drivers, while keeping applications running. Good luck keeping drivers portable on Linux.
NTDLL is hands off and there are good reasons for it. Not all OSes do syscalls the same way. Back in the Win16 days it used to be common to code directly to the Windows API to avoid adding the height of the C runtime to the applications. Hence why Windows API contains functions that replicate C functions, e.g. ZeroMemory () instead of memset(). It would be great if Rust could get rid of it, but I do understand it is not possible, given the constraints. Replacing LLVM as the backend would not make any sense.
Yeah, I've seen it. I haven't quite absorbed what they're doing differently, but it took me about two days of hacking to get my implementation that does the same thing to work. I don't mean that to boast myself either. It was completely 100% because of the insights provided by /u/julesjacobs: http://julesjacobs.github.io/2015/06/17/disqus-levenshtein-simple-and-fast.html My suspicion is that Lucene's implementation does something clever to have faster startup times, but I'm not sure that is impossible with the route I took. Needs more study. I did skim the paper they used though. I estimate that it would take a solid month of focused study for me to grok... It's extremely dense.
Yes, what happened in D and Tango was a disaster for the language. I wouldn't encourage a separate standard library. In fact, I wouldn't call it "standard library": if it's standard, there's only one. I would suggest renaming it to something else.
The Linux kernel is routinely refactored without breaking syscalls. The reason closed-source drivers break often across Linux versions is because they're linking against the kernel directly instead of going through the syscall interface (how could they?). 
It seems there's one standard with multiple implementations: https://en.wikipedia.org/wiki/C_standard_library In the case of lrs it looks like it's "The standard library in Rust isn't good enough, here's a separate standard library. Want to use lrs? Don't use Rust". In fact it's called "lrs-lang", which suggests it's kind of a separate language? I don't know...
Yes, but why the authors of irs-lang don't contribute to Rust instead of doing a separate project? That contributes to a split, not to a unification and better results for everyone.
Servo has a new website, created by nerith, lucywyman, and with a cool screenshot by notriddle. Love it.
Looks interesting. A lot of the things it's doing to make it all come together could be done in tree with slight modifications, e.g. the rustc driver itself already has internal support for substituting an arbitrary crate for std. I wonder why they decided to write their own core though. I really want it to be suitable for all use cases.
I really hope the default cram-everything-far-to-the-right behavior gets changed...
Ahh... I just glanced over the readme. I took it as this was something akin to musl, not a fork of the langauge. Thanks for the information!
An &amp;mut is semantically just a move that the compiler "rethreads" back to the origin. Of course no moves actually occur, but this is why mem::replace is semantically sound; it's just changing the value that will be threaded back. The only reason you can't temporarily leave an &amp;mut uninitialized is because of exception safety. Because the program can unwind at any time, all &amp;muts need to be init at all times. If the compiler knew some section of code didn't unwind, it could allow an &amp;mut to be moved out temporarily.
&gt; Windows doesn't support the lowest level system call interface, where you literally put a code in rax to say what system call you want, other arguments in other registers, and call the 'syscall' CPU instruction But is there a compelling reason to avoid system dlls? The only difference between this and 'syscall' interface is the calling convention.
Neat project but what is up with the 'autocommit' commit messages? Makes it really hard to follow the changes in the project.
It may be useful on embedded devices with very restricted memory. You can run a small kernel on them and then write the programs in rust.
I wouldn't make width and height public, that means that anyone can change those values and cause tile() and tile_mut() to return incorrect values. Instead, I'd have getter methods.
&gt; What I typically do is `cargo doc --open`. Oh man, I needed this, thanks.
LLVM can take care of the actual code-gen for cross compiling, but the standard library calls into different OS-dependent APIs, so you need to have the libstd binaries for each target available for linking.
As you note, the existence of iterators makes direct indexing much less prevalent in Rust than it is in C. There does exist a method on slices, `.get`, that you can use when you want an Option: http://doc.rust-lang.org/std/primitive.slice.html#method.get &gt; a lint for fixed-sized arrays being indexed with constants outside their bounds I can imagine a lint for this being added someday (ideally it would be a hard error, but that will need to wait for Rust 2.0). In the meantime it's something that I can see being added to Clippy, which is a collection of community-provided lints: https://github.com/Manishearth/rust-clippy/
Do I get it right that Go doesn't have the problem as it does not depend on libc, basically re-implementing it with whatever there is in syscall package?
Sorry; the command I'm using is literally `cargo add`, but the repo is here: https://github.com/killercup/cargo-edit The default behavior for *that* implementation is to add the crate using the latest version on crates.io, explicitly, hence my question. What you're saying is basically that, if I add something that way, it's stuck there unless I update it myself some way or another?
If it adds a constraint that looks like this: foo = "1.2.3" Then that's the same as foo = "^1.2.3" _not_ foo = "=1.2.3" This is because this is the default, because this is usually what you want. It will update from `&gt;=1.2.3` to `&lt;2.0.0`, when you run `cargo update`.
how difficult would you expect it to be to extend this to run on the various BSDs? i'm fine abandoning windows, but linux specific things make me sad
&gt; What would be the proper syntax for this? A chain of if-else expressions. &gt; Better yet, is there a formatter that I do not know about that will do this for me? Sure is: https://play.rust-lang.org/?gist=6841966e086f7660cb2d&amp;version=stable See http://doc.rust-lang.org/std/fmt/index.html for full documentation.
&gt; While rust decided to make leaking objects unconditionally safe. Leaking leads to undefined behavior in lrs. One of the reasons that Rust's `std` didn't take this route is that it was very complicated to nail down. For instance, it is very hard to guarantee that things don't (semantically) leak when you've got non-trivial threading APIs: a dead lock leaks all the data owned by the threads involved. I suspect the approach of [making `fork` safe](https://github.com/lrs-lang/lib/blob/809b58e6cdf2ae12be7131c8f006be0c07f2c967/Documentation/adoc/on_unsafe.adoc#why-using-fork2-cannot-be-unsafe) compounds this, because it means that you can effectively leak everything owned by other threads (of course this doesn't matter so much for `scoped` specifically, but if leaking itself is undefined behaviour...).
&gt; but both do not compile. If they don't compile we generally need to see the error messages because it's difficult to tell just from a code snippet what's wrong. In this case, however, you need to use [if guards](http://doc.rust-lang.org/nightly/book/patterns.html#guards) or normal if-else statements. &gt; is there a formatter that I do not know about that will do this for me? [Yes!](http://is.gd/7jMvsd) The [syntax](http://doc.rust-lang.org/nightly/std/fmt/index.html#syntax) is a little difficult to decipher but I think what you need is `format!("{:0&gt;#9}", num)`. Edit: As /u/CrumblingStatue noted you are better off with `format!("{:09}", num)`.
Ah, ok. I was under the impression that if a large chain of if-else expressions exists in the code, it can usually be replaced by an if-else. thanks!
&gt; The reason is that Windows frequently rearranges the table of what numbers correspond to what calls. What's the reason for this?
&gt; One of the reasons that Rust's std didn't take this route is that it was very complicated to nail down. I'm describing the current state of affairs in lrs. Another concern is that `Leak` requires to many annotations downstream, but there isn't much code using lrs right now so that hasn't been tested yet. It's possible that lrs will, at some point, switch to the rust solution. &gt;a dead lock leaks all the data owned by the threads involved I'm not sure how this is the case unless by "leaks" you mean that destructors don't run at the end of the program. A correct program does not rely on threads making progress and a program where one thread deadlocks is equivalent to a program where one thread stops making progress indefinitely. I don't see how this can lead to undefined behavior which is the main concern here. &gt;I suspect the approach of making fork safe compounds this, because it means that you can effectively leak everything owned by other threads Ah, I should have read the whole comment before I started replying. Like I said above, a correct program does not rely on other threads making progress and thus a correct program does not become incorrect when all other threads are killed (through fork or otherwise). *edit:* Note that, while I said above that leaking leads to undefined behavior in lrs, this is, of course, a simplification. Leaking everything by calling `exit_group(2)` does clearly not cause undefined behavior.
Or potentially `T: Into&lt;Cow&lt;'a, str&gt;&gt;` which will let the user of the function move a String into it if they have a String already.
They sometimes add or remove system calls in a service pack on a maintained version even after a newer version had been released. Therefore, if you add a system call in XP SP2, and add another one in Windows 7 released, then the numbers will be different. [Here](http://j00ru.vexillium.org/ntapi_64/) is a table showing the full list. It mostly stays the same, with just a few changes most of the time, but they can be seen to remove a system call in a minor release on at least one occasion. (Perhaps they changed the DLL to implement a particular function in userland rather than as a system call?) Windows 8.1 renumbered everything, also. I don't know why.
&gt; For example, if you don't want the signal handlers to be reset, you're out of luck. Could a Rust program that uses the stdlib just write non-portable code and directly call the POSIX interface?
Instructions to build and run through the command line will probably dispel any misunderstanding that this software is ready to use. (well, unless the visitor doesn't actually intend to use Servo and doesn't notice there is no nice "download" button)
We don't have those – these are proof-related compile-time intermediates used to prove the correctness of the code according to pre- and post-conditions as well as invariants (which are also expressed within the type system).
Yeah I didn't really understand the deadlock claim. The only kind of leak you're concerned with is the mem::forget kind, right? In other words, the compiler believing something is gone, but the dtor hasn't run. Threads blocking won't cause this. mem::forget and Rc cycles will do this.
Sometimes you know enough about your data or usecase to make these optimizations. No need to spit on those who do.
Ah ok, yeah it was just a syntax unrecognized compiler error, as in those kinds of match expressions aren't valid. It wasn't really an issue with me not knowing why it wouldn't compile, just what the correct way to write this logic would be. Sorry! And yeah, I used that and it works. That syntax is quite obtuse, however. Ah well, thanks!
Well on Windows calling the DLL *is* the ABI and it's stable.
Thanks. The design reminds me a lot of [this paper](http://www.cs.princeton.edu/~dpw/papers/tal-toplas.pdf).
Yep, this is indeed what I need. Thanks!
I guess I meant to ask, what might prvals look like if they were added to Rust? (This might help me better understand what prvals really are :P)
Requires nightly because it uses so many unstable features. I still have only just scratched the surface on Rust's type system but I am still psyched to learn as much as I did. Eager for tips on how to improve and PRs.
My immediate worry exactly. While I appreciate the effort and really like what lsr is trying to achieve and I understand the root of /u/AlekseiPetrov frustration, I really do hope that it will not end up as ecosystem split. So my hope is lsr, and libstd are and will be API-compatibile, and swappable, which seems to be the case.
&gt; Same. Same.
It avoids an allocation sometimes
I assume you've [read my blog post](http://huonw.github.io/blog/2015/08/simd-in-rust/)? The situation hasn't changed much since then: it still requires a nightly compiler, there's basic support for a lot of stuff but there's still some large missing pieces. However, there's still people doing interesting things with it e.g. some [text processing](https://github.com/servo/servo/blob/4052a22a1c6826cb18f03de9d2f6aed82d809e71/components/gfx/text/glyph.rs#L550-L576) in servo. I'm happy to answer more questions if you have them/want clarification.
Cool library! &gt; Eager for tips `*` dependencies can cause pain down the road, when libraries make breaking changes that cause your code to stop working. Using an explicit version will help `cargo` help you have code that compiles into the future, e.g. the `Cargo.toml` could look like: libc = "0.2" lazy_static = "0.1" errno = "0.1" rand = "0.3" (The versions are listed on crates.io, and the `cargo search` subcommand is helpful for finding versions, as is [`cargo-edit`](https://github.com/killercup/cargo-edit)'s `cargo-add`.)
Makes sense &amp; good luck. (Hopefully it'll be magically fixed next time you get the chance to play with it. :) )
&gt; Love it. Ditto.
Do we have a repository to store the std library for each common platform, that way we can just have a script that pulls that too?
1. Array OOB panicking is worse than static prevention, but it's better than OOB in C (which is allowed to retroactively sell your soul to the devil). 2. If you want Option&lt;&amp;T&gt;, as already mentioned, use `array.get(i)`. 3. `array.get(i)` returning a None is exactly as tracable as `array[i]`; that is, you need to figure out where the out-of-bounds index came from. Since out-of-bounds indexes usually indicate a bug, and bugs that can't be found until runtime are what panics are for, out-of-bounds indexes should usually panic.
Ah, segattach is similar to mmap. I don't think modifying the page table has a chance of being faster, though- shared pages are either part of the kernel (and thus should be mapped with the global flag, which means `mov cr3` won't invalidate them) or less frequent than unshared program code/data (which means you're effectively trading a small number of hardware page table walks for a large number of software page table walks plus explicit `invlpg`s). Further, newer CPUs can use PCIDs so a `mov cr3` won't invalidate anything. Would definitely be interested in seeing benchmarks, though, especially if there's some situation specific to Redox that makes it more worthwhile.
The OP is definitely good. :-) But Rust is also a _fantastically_ fun language for OS development once you get bootstrapped up to the point that you have working interrupts and some kind of I/O. The biggest limitation is probably memory allocation: Kernels want to gracefully recover when no memory is available, and Rust's higher-level libraries like `collections` prefer to `panic!`. But the combination of `libcore`, custom `--target` specs defined in JSON, and the ability to share hardware libraries using cargo makes for a very pleasant experience. And it's going to get a lot better.
I would have to dig my old UNIX stuff to check that in regards to unices. However there are many more OSes than just UNIX clones and Windows.
That page says &gt; the invlpg instruction, which should be used instead of the above method when doing small mapping modifications I asked in the first place because I've never seen an OS do it the way you describe and I was curious if there was something Redox was doing, maybe sharing address spaces between several processes or something.
Agreed. The core team does have the formal power to reject consensus on rfcs, but they won't use it, and if they do there would be outrage. The only time they make major decisions are: - When an rfc is on the fence (in which case you can't call it dictatorial) - When deciding the overall direction of the project (which is transparent in the reports and can be debated later), which in turn affects if some RfCs may be rejected as "not the right time for this". The other decisions are all made as part of the subteams.
Wouldn't that be [1, 100] though? Never encountered the {1, ..100} notation before.
It is, but it kind of has to be. It's very similar to Python's format strings of you've ever used them. 
I guess `cargo add foo@1.2.0` (there are also flags for this) is what you're after :)
To follow up on this, here's an example of using the `pcre` crate to compile a regular expression into machine code: https://gist.github.com/ArtemGr/ce53c3a0f378585f162c
Plus now there's a competing standard library which doesn't panic (lrs) as was posted on the subreddit today.
That's not useful for OS development though. It specifically targets Linux and only Linux.
Well, in Russian language "user" ("пользователь") has masculine grammatical gender, so sometimes it is difficult to always use gender-neutral pronounces when using English. I personally do this all the time, unfortunately :)
&gt; But there's a simpler explanation than political conspiracy or corporate power plays or technical incompetence: the priorities of the developers simply differ from yours. And before you start casting the Rust developers as totalitarians, I assure you that I can find innumerable instances of community members successfully engaging with the developers and guiding the project. There's plenty of gray area in between "political conspiracy or corporate power plays or technical incompetence" and "everything was roses, people just had different priorities". I wish we put the same kind of conscious effort into being honest and reflective about the strengths and failures of Rust's development process as we do with respect to the language itself (and beyond just the impossible-to-ignore things like "we didn't have a moderation team" or "there wasn't a single woman on the teams page, and still aren't many"). Without meaning to "take their side" or anything -- I basically avoided participating in most discussions they were part of back then because I didn't have the emotional energy for their stridency -- I found/find strcat's and mahkoh's complaints resonating with me *often*. And I don't intend this as any kind of conscious boycott or retaliation (very far from it), just as honest self-observation, but my experiences with the pre-1.0 "process" put me off of wanting to work on any new RFCs for a long, long time. &gt; I assure you that I can find innumerable instances of community members successfully engaging with the developers and guiding the project. Sure, but "it often did work" and "it often didn't work (as well as it should have)" can be simultaneously true. And even when something did end up with a positive resolution in the end, that doesn't mean it wasn't (excessively? unnecessarily?) frustrating and exhausting to get there. &gt; Just as with strcat, your downfall was that you took the rejection of your ideas too personally. To turn this around a little, and try to pin down my sense of what was wrong a little better, I think one of the problems I felt was precisely that there was insufficient appreciation of the fact that contributors are human beings with feelings, and that RFCs aren't the product of a robotic unicorn pooping them out for the "the core team" (or whatever the right umbrella is for the people with power in the organization) to consider and dispense with at their leisure, but of people writing them on their own time out of passion, which are precious and *finite* resources. (To cite a particular example I came across again recently, seeing [this well-crafted, comprehensive RFC](https://github.com/rust-lang/rfcs/pull/223) by gereeter (I can only imagine how much effort went into it) get shot down with the equivalent of "that's nice, but nah" still grates.) 
I just moved loop invariants out of loops and then used `#pragma omp parallel` on some of the loops and then build using `-fopenmp`. I don't have the code anymore.
&gt;this well-crafted, comprehensive RFC by gereeter (I can only imagine how much effort went into it) I've noticed this usually happens with especially large or important changes. Basically, the core team holds a position "If you want to do it right - do it yourself" for such changes, which is quite understandable, I'd do the same thing if I had my own projects. If there are not enough resources for "doing it yourself" at the moment, but someone proposes it, then the change is postponed with explanation "We don't feel quite ready for this just yet blah blah", which can be translated as "Such a tasty feature, we want to design it ourself!". When resources become available, then the same idea is reopened by someone from the core team.
`CString::new` takes a `&amp;str` and packages it inside a `ptr::Unique` struct. Unique is used to express ownership (wrapping a raw pointer), but you don't own the data you point to -- it's a &amp;str, so it could go out of scope the moment `CString::new` returns. You must use a lifetime parameter on CString to ensure that the derived CString does not outlive the input `&amp;str`, either that, or you can only accept `&amp;'static str` input.
Here's the [Error Conventions document](https://github.com/rust-lang/rfcs/blob/master/text/0236-error-conventions.md) that establishes an area where a panic is ok, it's called *contract violation* and refers to a situation where out of bounds is typically a programmer bug. Right now I'm exploring indexing, algorithms implemented using indexes, and I'm using this trick where we can use the type system to “brand” indices and ranges so that they can be trusted to be in bounds. The result is that you can use such trusted indexes for a limited scope, and you know it will never panic, and no bounds checks need to be inserted. I think this is extremely promising, although with its limitations. Removing bounds checks may or may not improve performance on the algorithm, it depends. See [indexing](https://github.com/bluss/indexing), totally in a WIP state, but I think the [algorithms](https://github.com/bluss/indexing/blob/master/algorithms.rs) are pretty readable and rad!
Yeah noexcept is for sure one of the solutions we'll probably eventually grow. Can you elaborate on the problems? Keeping in mind panics are untyped in Rust and otherwise don't need to be declared.
It's considered good style, by many outside of Rust, but also, in Rust code. It's more consistent, each element looks the same. And it also makes diffs much, much nicer when you have to add or remove something to the end of the list.
Note that that RfC is a year old, predating the governance model which was put in place precisely to address such concerns.
How easy will it be to port rust applications to Redox? I'm programming a small game with rustbox (which uses termbox)
It's really a shame that it was done with fixed vector sizes and not in a style closer to ISPC. Now a program written for 4 wide SIMD will never get a speedup from a wider SIMD lane and something hard coded to x8 won't automatically adjust back down to a smaller width. 
&gt; Windowing is achieved by opening window:/x/y/width/height/title, which returns a window file where pixels can be written and events can be read. Windows will eventually be moved and the title could be changed using the rename syscall What is your long-term plans for graphics stack? I suppose it is obvious that you'll want to abandon your basic SVGA framebuffer graphics at some point, put to accomplish that you need actual graphics drivers which are notoriously difficult to write. Do you think porting Linux stack (DRM/Mesa/Wayland) is feasible?
Here is my work: https://github.com/GrahamDennis/dot-rust/pull/2 Critique welcome :D I might need to change `Vec&lt;Arrows&gt;` into another structure like `[Arrows]`. I dislike the fact that it needs cloning, atm. Also need a way to add pictures to doc, the text doesn't do comment justice.
I have begun porting Mesa, osmesa (http://www.mesa3d.org/osmesa.html) could be used to generate the pixel array to start. The VBE framebuffer can still be used directly. Of course, this will change in the future.
s/the core team/the relevant subteam/, and the point still holds.
Nice description, thanks! I see the "schemes" architecture being worth getting a shot at. Consider me partially sold ;) Where do you guys communicate? Did you consider having a gitter room like gfx and glutin do?
It's only tested regularly on 64 bit, but 32 bit builds should work.
Eventually you have to have some kind of governing body. We could become committee based and the point would hold there. Etc etc. And sanxiyn's point was about the core team post-governance-rfc, which doesn't hold in practice.
Could I remind everyone of rule #2? Chains of comments like this decrease the visibility of the interesting and insightful comments made by others. There are subreddits better suited to redditisms like this.
&gt; You can just keep stacking impl Traits to hide the implementation details. This would become Rust's version of an abstract data type. Sure, that works for returning, but you still have to parameterize every data structure on every type parameter that's used anywhere inside of it, which just kind of violates any nice feeling of encapsulation. It's kind of nice to be able to create a type `Foo` that contains some closure, but contains a _specific_ closure of a _specific_ type. It's much nicer to be able to just say Foo, instead of Foo&lt;F&gt; in any place that isn't a function return type, and then recursively solving the same problem. The thing is that Foo really _isn't_ generic. It's specific, but in a hard-to-describe way, and those are two different ideas. It would be nice to more neatly express the second, which we'll soon be able to do for function return types. &gt; I thought about this before, too, but then you would not be able to say that two types implementing the same trait are the same, e.g. fn foo&lt;T: Trait&gt;(t1: T, t2: T). Absolutely - my suggestion is much more akin to lifetime elision. If I don't need all the power of explicit lifetimes, it's nice to have a simpler syntax for the common case that doesn't require them. The _vast_ majority of the time, when I use generics for function parameters, I use the type in one place, and it would be really nice to make that as clean and intuitive as possible. &gt; Besides, writing a trait in type position currently indicates a trait object Ah yeah I was only thinking of the common case of something like `fn foo(x: Trait)`, which I believe (?) isn't used for anything else right now. It would make `fn foo(x: Box&lt;Trait&gt;)` ambiguous, so we could just re-use the `impl` keyword: `fn foo(x: impl Trait)`. &gt; &gt; One of the big headaches with haskell is when your functions end up being parameterized over a huge list of typeclasses/generics, and I sort of see this happening in rust too, especially as more things need Deref to work smoothly. &gt; I have thought about this recently, and there's no good solution There's no perfect solution, but it makes a huge difference if the syntax as clean, clear, and compact as possible, especially for common cases that new-ish users are likely to hit.
&gt; it's unfortunate that I have to hover-wait on each of the sidebar icons You can expand it out if you click the arrow at the bottom
Also schooled in the U.S., I was taught that you always use "he or she" or "they". As were most people I know in multiple countries. It is fine to pick a gender in examples, but when talking about a generic person of unspecified gender, you should not pick one.
zlib, libpng, freetype, and lua have not required any changes, except to the config.sub, to add -redox* as a recognized operating system tar has not required code changes in its codebase, but has required more implementations to be ported in newlib. These functions will then be available immediately in more programs. SDL required a few graphics and input functions to be written, to deal with the window: scheme. This was incredibly simple, because of SDL using pixel maps in the same manner that the window: scheme does, and because of the window: events matching many of the SDL events. The biggest issue right now with porting Firefox, and other applications, is GTK+, which will also require graphics functions and input functions to be written for the window: scheme. Also, there will need to be a Berkley socket layer written to provide compatibility. Again, these things only have to be done once. I have been working on porting a simpler browser, netsurf, to start.
&gt; If Rust governance was not dictatorial, or equivalently, if strcat, dobkeratops, mahkoh, etc. were in core team, there would be lots of horse trading. This is not what we observe. Relative to pre-1.0, the core team meetings barely have any technical decisions being made (except maybe really obvious ones that everyone agrees on) nowadays. From my point of view (which is admittedly less involved with Rust than it used to be, since my focus now is on Servo) there just haven't been that many contentious decisions to make post-1.0. The time to make massive decisions that affect the character of the language, or to remove features that large numbers of people are relying on (e.g. unwinding), has passed. So I don't think this is actually true as stated—the backwards compatibility constraint means that the focus has shifted from "what should the vision of Rust be" to "how should Rust evolve, given the vision that it now has".
I wonder if I used "she" despite being male, would that be a fair compromise? I dislike "they" even though it feels like it works, and "he or she" distracts from the point. If I had to pick, I would stick with "they." But I was personally never bothered by "he" or "she," I just took it as a shortcoming of English. Which one do you use?
Schemes are programs built independently of the kernel, and can be written in other languages as well. The must define an entry point (main), and they must accept messages and handle them, which can be done either on a single thread or on one thread per message, it is up to the scheme.
If it becomes more complicated, then we have fpath(window) return window:/id/, then a readdir(window:/id/) will return things like x, y, width, height, title, etc. The design of the window scheme is not complete, but no matter what is decided, the current set of syscalls should be able to handle it
Ha, I do them one at a time. Next up is ANSI terminal support, I think.
What's the mechanism for accepting messages? I'm guessing there's some standard library that provides a message passing system?
Oh, yes, I agree this wasn't the best choice. Gonna wait until you move to something friendlier, like Gitter, or even simple IRC.
That's unfortunate. :/
Pet request: Golang. There are a load of really interesting new tools, as well as clones of old standbys, written in Go. Once you have the toolchain, utils will usually just work. It'd be nice to have them over early. :) Python would also be awesome but likely tricky to port entirely: too much C..
&gt; I did no longer want my free contributions to be used by a project that gave nothing in return. You're using its compiler in lrs, is that really nothing?
What are your plans for graphics cards?
Very high goal there. With how fast this appears to be moving I don't doubt this will get somewhere. I can't wait see where this goes!
Whoa. That would be nice. You can't use the regular std library with OSDev, so anything that resembles it is a win.
{1, ..100} represents a finite set of natural numbers whereas [1, 100] would be used to represent the set of all real numbers in that closed interval.
Can you work remotely? 
Thank you for responding! You've confirmed what I discovered while I was trying to write the application, so I guess I'm not missing anything major.
The main problem is that in C++ `noexcept` is close to useless. It is only used for move construction/assignment and destruction, and the standard way to check for those is to use `&lt;type_traits&gt;`. These can use noexcept internally, but users can also specialize them for their types, which might have been better than introducing a new keyword. Below is an explanation of some rough edges of noexcept in C++, and why it is almost never used. First: - noexcept is a contract between a function declaration and a caller: `auto i_dont_throw_exceptions() noexcept(true) { throw 0; }` That function always throws, but I state with noexcept that it never throws. If it throws, `std::terminate` (C++'s abort) will be called (no unwinding will happen). Exceptions are not part of the type system, so the compiler cannot check anything and must just believe you. In Rust this would probably be unsafe. - Generic code can be conditionally noexcept to propagate it correctly. It is however extremely hard to get this right, and very verbose. For something trivial like swap for `pair` it looks like this: `void swap(pair&amp; p) noexcept( noexcept(swap(first, p.first)) &amp;&amp; noexcept(swap(second, p.second)));` Basically one has a `noexcept(boolean expression)` function annotation. To compute the boolean expression, one has to use the `noexcept(expr)` boolean operator to check the noexcept-ness of every single expression in the function body, and "outside" the function body like conversion/construction from a return expressions to the return value. - What about deducing noexcept automatically with `noexcept(auto)` (rejected proposal)? In C++17 noexcept is part of the type system so this could potentially be done. Some might argue that using it could be dangerous since the noexcept-ness of a function could change silently, breaking its contract with their callers. But it would be less dangerous than writing the error prone code above. - How much value does correct noexcept propagation add? Very little. Most generic code (even trivial things like `std::plus`) doesn't do it. And if a single annotation is missing somewhere you land in `noexcept(false)` which is where you started with zero effort. - How much value does it add if everything works fine? Well you can ask the compiler if an expression is noexcept and do conditional logic depending on that. If noexcept is: - `true`: a given expression will crash your program if it tries to throw, - `false`: you must assume that a given expression always throws even if it is clear it doesn't (everything is `noexcept(false)` by default, including `void foo() {}`). So... I guess the problems of noexcept in C++ is that it is an extra feature, that is not really necessary for the problems that it actually solves. it is hard and time consuming to use it correctly, and even then, it gives you very little value for the effort. Supposing that noexcept would be part of the type system (which it is in C++17), and that `throw` would also be part of the type system (which it isn't), and supposing we had `noexcept(auto)` (which was rejected), and supposing that the compiler would then be able to tell yo that if something is noexcept: - `true`: static guarantee that there is no `throw` anywhere in the code path of a given expression. - `false`: static guarantee that there is a `throw` somewhere in the code path of a given expression. then `noexcept` would be much more useful. I don't know if something like that is possible, but at that point, one could probably make exceptions part of the type system and query which kind of exceptions an expression can throw statically, as well as have exhaustive catch clauses. I don't know the wins of something like this. Java devs hate on checked exceptions often, but from my perspective that adds more value than what currently noexcept does.
We haven't settled on an exact protocol for audio, so it may change in this direction or some other direction in the future.
The implementation of `FromIterator` (which is used in `.collect()`) [for `Vec`](http://doc.rust-lang.org/stable/src/collections/vec.rs.html#1150) shows that the lower bound is used to set an initial size, so there should only be one allocation if that lower bound is large enough. I think that the "standard" iterators should have a correct lower bound if their lengths are known.
The position title says senior in it. Would you prefer they list a number if years of experience wanted? From what I'm given to understand, there is a running joke that those numbers are never realistic 
The SF office is the Rust mecca because it has *three whole devs* (and dherman/brson sometimes). I think everywhere else is a single person. Maybe pnkfelix and SimonSapin are in the same place?
Ah, neat, I didn't know about `size_hint`. That means it will work efficiently even if you throw something like `take` into the chain.
Yep! Some interesting exceptions: * The old BitVec and VecMap didn't know their lengths. * `filter`, `filter_map`, `take_while`, `skip_while` have a lower bound of `0`. * `flat_map` barely knows anything (but can do some stuff if it's down to two sub-iterators)
Yes, much of the team is remote, and Mozilla is set up well for it. We also have offices in several locations around the world.
No, I would like to know the team's understanding of what "senior" means, since, like "number of years experience", the word "senior" doesn't really communicate much by itself, and the only thing that resembles a "requirements" section of the ad just says •BS in Computer Science or equivalent experience •Excellent written and verbal communication skills which is the same as what's requested for the non-senior Servo job, and which is less than I'd have expected for a senior role. (One reason "senior" in the title conveys very little to me is that, during my last year of graduate school---in philosophy, mind you, and not even a technical branch of philosophy---I through the good offices of a friend got a job interview at a very large technology and entertainment company in Los Gatos. They offered me a job with the title "Senior Software Engineer". I thought that was kind of crazy (and I have since heard that they're somewhat free with both job offers and near-term dismissals), but they were ready to grace me with the title. So if I see "senior", I want to know: what does that mean to the people who are actually doing the hiring?)
At this point, `noexcept` works like a type-level `pure` tag, but in which the "side effect" is exceptions rather than I/O or mutating memory. For what is worth, the dependently typed [F* programming language](https://en.wikipedia.org/wiki/F*_(programming_language\)) (from Microsoft Research) has a type system that can express effects: whether a function is pure (and total), whether it can diverge, whether it can raise an exception, or mutate references, do I/O, etc. From [the tutorial](https://www.fstar-lang.org/tutorial/): &gt; For instance, in ML `(canRead "foo.txt")` is inferred to have type `bool`. However, in F*, we infer `(canRead "foo.txt" : Tot bool)`. This indicates that `canRead "foo.txt"` is a pure total expression, which always evaluates to a boolean. For that matter, any expression that is inferred to have type-and-effect `Tot t`, is guaranteed (provided the computer has enough resources) to evaluate to a `t`-typed result, without entering an infinite loop; reading or writing the program's state; throwing exceptions; performing input or output; or, having any other effect whatsoever. &gt; On the other hand, an expression like `(FileIO.read "foo.txt")` is inferred to have type-and-effect `ML string`, meaning that this term may have arbitrary effects (it may loop, do IO, throw exceptions, mutate the heap, etc.), but if it returns, it always returns a string. The effect name `ML` is chosen to represent the default, implicit effect in all ML programs. &gt; `Tot` and `ML` are just two of the possible effects. Some others include: &gt; * `Dv`, the effect of a computation that may diverge; &gt; * `ST`, the effect of a computation that may diverge, read, write or allocate new references in the heap; &gt; * `Exn`, the effect of a computation that may diverge or raise an exception.
Oh, thanks for confirming that, I thought I remembered it without the "Senior" but wasn't certain.
Ooh... +1
Returning iterators works great, but storing them doesn't, especially if they're iterators with closure parameters. Take [Playform's surroundings_loader](https://github.com/bfops/playform/blob/master/common/surroundings_loader.rs) as an example. Getting this to build involved writing a _manual_ closure type and _explicitly writing down the iterator_. These surroundings loaders are stored explicitly stored for players and mobs, which are in turn stored in other structs, etc. I see two options: one, which I've taken here, is to make these types _painfully_ explicit. The other, which I like even less, is to propagate them up through _all of these structs_. The type of the whole world would be parameterized on the type of a tiny detail in another _crate_, a detail which will only ever be of exactly one type. Neither of these options is good. (There is a third option, which only somewhat counts, because it's not actually equivalent: we can box these types where it's convenient, which is silly because it means solving a code hygiene issue with runtime costs, minor as they may be). I'd like to be able to approach this in a way that matches how I think about it internally, which is "there exists a type here, it's some long finicky thing, somewhat likely to change, and I don't care to know precisely what it is, but it looks kind of like **this**".
The Android build is 32-bit by default, and while we don't yet run automated tests on Android, we do automated builds, and our manual testing means that we should notice and fix things if they break for 32-bit architectures.
Yes, I sit a few meters from Felix in the office, but I only occasionally contribute to Rust itself :)
Honestly if you told me that you implemented all of the code that makes strings work in rustc/std, I would believe you. If you linked me a PR with someone else working on that stuff, I might just assume it's a sockpuppet. (I count all Servo devs as Rust devs because they do a ton of great work in the wider ecosystem, which is what this job entails)
Yes, as long as the inner iterators provides good estimates.
An elaboration: it would be possible for the compiler to track noexcept at the type level internally for sweet optimizations, and externally for semantic boons like moving out of &amp;mut. However it might be reasonable to leave it initially as a bit of a black-box that doesn't work cross-fn, like lifetime disjointness. So you can do basic re-assignment/matching knowing that can't panic.
Servo doesn't even have that, Servo has one person per office. The closest we have to two people in one office is two people occasionally working from one Chicago Starbucks :) Once the new Servo positions get filled this will probably change.
brson and pcwalton are frequently in the same place, contrary to any assertions that they are in fact the same person.
3rd most prolific contributor, just behind Ms2ger and pcwalton!
Something I periodically think about is additive/subtractive borrows/ownership. Basically, you could have `&amp;'a.left mut x` and `&amp;'a.right mut x` simultaneously, and each would give you access to different parts of the value `x`. Other ideas are the ability to track that through to a unique mutable subborrow which you can freeze to get temporary immutable access to the parent value. Then there's the question of whether this is expressive enough (potentially in combination with clever wrapper types) to allow for statically tracked parent/sibling references in a tree data structure, at which point my brain starts to melt trying to work out all the parameters around trying to design such a thing as I keep going in circles trying to address various potential soundness holes. At the very least, I think you'd need linear types so you can ensure that values can only by dropped when you have complete ownership.
Ah, right, but they're all from way back when. Brson occasionally contributes and comes to some of the meetings (and probably is involved in some design decisions), but doesn't actively contribute code anymore.
Personally, I use something like the below. It never allows an unbroken line of indentation from the signature into the body, and it avoids hiding information on the right side of the line where you might miss it. Also, it's easy to do by hand, unlike that nasty visual layout some crazy people seem to like. ;) fn foo&lt;T&gt;(barbaz: T) where T: Bar + Baz { // stuff } fn foo&lt;T, F, G&gt;(barbaz: T, func: F, bar: G) where T: Bar + Baz, F: Fn(T) -&gt; T, G: Bar, { // stuff } fn foo&lt;T, F, G&gt;( bazbazgambulputty: T, func: F, bar: G, more_bar: G, a_horse_walks_into_a_bar: G, at_this_point_i_should_probably_use_a_struct_for_these: G, ) -&gt; T where T: Bar + Baz + Gambulputty&lt;Seriousness=NotEvenABit, Something=Else&gt;, F: Fn(T) -&gt; T, G: Bar, { // stuff } 
Yeah, unless I'm missing something, your `T` type parameter is entirely redundant and it's what's causing all that noise. You need a lifetime parameter but no type parameter. Using `PhantomData&lt;&amp;'a i8&gt;` is fine (it could really be a reference to almost anything). Remove T and the pain should be gone. The lifetime is inferred from the input. In the case of from_raw, it will be inferred to whatever fits, which is of course a hazard of unsafe code. Just like how dereferencing a raw pointer: `&amp;*ptr` creates a reference of arbitrary lifetime too. By the way, you may want to use the c_char typedef since it varies between i8 and u8 depending on platform.
Thanks, I wasn't aware that you could do that. afaik, I can't use the `c_char` typdef without linking against libc, which is a no-go for me. It's only `u8` on arm though so, I can create my own `type` decl.
I consider the libs you publish to be basically an extension of the standard library and use a lot of them in my own projects!
Technically, yeah, but the ability to include more than one type in a variant isn't fundamental to the semantics, e.g. enum Foo { A(u8, u16), B { x: String, y: String } is essentially equivalent to: enum Foo { A((u8, u16)), B(BStruct) } struct BStruct { x: String, y: String }
&gt; Basically, you could have &amp;'a.left mut x and &amp;'a.right mut x simultaneously, and each would give you access to different parts of the value x. The discussions in the past have been about things like struct Foo { x: u8, y: u8 } impl Foo { fn x&lt;'a&gt;(&amp;'a mut self{x}) -&gt; &amp;'a mut u8 { &amp;mut self.x } fn y&lt;'a&gt;(&amp;'a mut self{y}) -&gt; &amp;'a mut u8 { &amp;mut self.y } } I.e. basically listing a subset of the fields that can be accessed. &gt; Other ideas are the ability to track that through to a unique mutable subborrow which you can freeze to get temporary immutable access to the parent value I don't think this is enough, at least not without being very careful (e.g. you have to handle concurrency), and also doesn't seem to solve the general problem. It probably does solve calling `&amp;self` functions, though. As you say... mind-bending. &gt; At the very least, I think you'd need linear types so you can ensure that values can only by dropped when you have complete ownership. I strongly suspect Rust's affine types are enough for this (i.e. it seems like letting something go out of scope without using it, which is what affinity adds over linearity, is fine)
It would be great if this time we got a kernel that [actually focuses on security](http://www.washingtonpost.com/sf/business/2015/11/05/net-of-insecurity-the-kernel-of-the-argument/) by *design*. 
I'd love to be able to use this OS as a library, to build unikernel apps. @jackpot51 is this something you envision being possible?
Thanks for the examples!
One thing I haven't been able to figure out yet is how to get it to do that for multi-line functions *without* a where clause. Did I just miss an option? fn foo(barbaz: T, func: F, bar: G) { // stuff }
Continuing to stress Rust. Is a linker error in a pure Rust crate necessarily a compiler error?
I'm about to go to bed, so apologies if my replies are a bit terse. :) &gt; Mut keyword is probably used very often (am I right?) In my experience looking at Rust codebases, less than 20% of variable bindings are declared as mutable (and sometimes much less than even that). Feel free to check for yourself by grepping for `"let "` and `"let mut "` and taking the difference. Making this shorter probably wouldn't be much of a benefit. Also, Rust is one of those languages that tries to avoid having multiple ways to say the same thing. Also, Rust is already symbol-heavy enough as it is. :P &gt; I think memory could be allocated automatically on stack or heap depending on the usage of varible. Maybe this would work in a scripting language based on Rust, but Rust is designed to make costs explicit. The stack and the heap have a pronounced difference in cost, so Rust distinguishes them. &gt; Erret would allow different error return types I recommend reading the chapter on error handling in the Rust book (https://doc.rust-lang.org/nightly/book/error-handling.html) to see how Rust uses the `From` trait to make it possible for a function to return multiple different types of error types without burdening the author too much. I recommend you read that entire book, in fact. :) &gt; compile-time reflection Rust uses syntax extensions for this, which essentially allow your code to be transformed based upon arbitrary code execution at compile-time. They're not especially pleasant to write at the moment nor are they currently covered under the backwards-compatibility guarantee, however. But this is how, for example, Serde (https://crates.io/crates/serde) implements serialization and Regex (https://crates.io/crates/regex) statically turns regexes into state machines. &gt; range types Yes, this would be great. I wonder if something like this exists in a third-party crate yet.
&gt; mut variables - I think that many variables are mutable. Mut keyword is probably used very often (am I right?) what do you think about making it shorter - replace it with tild a (~) character? I'm often surprised at just how many bindings in my code do not contain `mut` (grep says 27 out of 170 let bindings in cargo-edit have a `mut`). By the way, in pre-1.0-Rust (circa 2014), `~` was used for what is now `Box`. A lot of effort went into getting rid of to many sigils and instead use (short) keywords.
tl;dr $ cargo install cargo-extras $ cargo --list **Missing Commands** After going through the [third party subcommands wiki](https://github.com/rust-lang/cargo/wiki/Third-party-cargo-subcommands) and the [awesome-rust list of subcommands](https://github.com/kud1ing/awesome-rust#build-system) there are three notable subcommands missing from `cargo-extras` that I'd like to get solved, but may need some assistance from the owners themselves. So if you own any of the following: * [cargo-edit](https://github.com/killercup/cargo-edit) EDIT: [Being solved](https://github.com/kbknapp/cargo-extras/issues/1) * [cargo-clippy](https://github.com/arcnmx/cargo-clippy) * ~~[cargo-emscripten](https://github.com/tomaka/cargo-emscripten)~~ Please see the issues in the `cargo-extras` repo if you can provide any assistance. **Add Your Subcommands** Nothing personal, I just went by the two lists above, I didn't search all of github for commands. If you've got a subcommand that isn't included yet, put in a PR or file an issue to have it added :)
Thank you for the statistics! Looks interesting.
I think that for range types to be ergonomic and usable, we need type-level integers. Range types would be also great for reducing compiler checks for out-of-bound-index errors.
Is there a way to detect the width of the SIMD vector on the machine and then go from there? Possibly switching functions out? 
Thank you for information! Other comment already mentioned low usage of mut keyword. Looks interesting. &gt; scripting language based on Rust Such scripting language would be very useful. I often write programs which don't need super-high performance but writing them in Python feels too slow. (Also, I hate dynamic types - debugging is nightmare.) &gt; I recommend reading the chapter on error handling in the Rust book I already read the Rust book but I've missed this. Thank you! I'm thinking that there could be a macro which would help with writing impl From for such return types, because I suspect tis feature will be used often (but maybe not, who knows :)) &gt; Rust uses syntax extensions Yeah, I know about it. Very useful thing but seems quite complicated.
Ah ok, good to know. Thanks!
[This might help explain the situation with `tt`s](https://danielkeep.github.io/tlborm/book/mbe-syn-source-analysis.html#token-trees).
I don't work in HR, so I can't tell you anything other than I make a pretty run-of-the-mill Bay Area salary.
And powerpc! I don't use it now, but I learned C on that platform and I still always think chars are unsigned until I think it over. :)
Could you make specific suggestions?
Well my linux implementation driver (which I never work on X( ) was a separate driver than the vty driver. That's a brilliant idea about writing a compatible ncurses implementation targeting it though, I was just going to do my own minimal shell.
This sounds like Rust :-D What features are actually missing from Rust?
Actually in python lists and dicts behave quite similar; you can `[index]` or `.get(_)` with the former throwing KeyError (or something) while the latter may return `None`.
Even though it's more verbose, slices have get_unchecked and get_unchecked_mut unsafe methods.
`fn_brace_style = "AlwaysNextLine"`
Although OS dev interests me and I've done some simple bare metal development on ARM before, I've never quite figured out how to understand all the ins &amp; outs of the machine. E.g. I've got a Rasberry Pi, but my only reference for IO etc is a multi-thousand page document which is extremely obtuse. Where are you getting all the information you need for a PC architecture?
&gt;native multidimensional arrays, instead using the array of (array of (array)) approach which is not so hot on cache locality (speed) How is it possible to have something better for cache locality than the array of (array of (array)) approach, which is a single piece of memory underneath? I mean, physical memory is 1D, so any representation of higher-dimension array must be disjointed along all dimensions but one, isn't it?
IMO while there are theoretical reasons as to why fortran is used in scicomp, the actual reasons are mostly about libraries, and this is why no new language can displace fortran unless it both shares the featureset and is easily integrated with fortran, like C++ is with C. I'm a physics student and I've chatted with tons of profs about this. The consensus seems to be that the scientific (at least physics) community has too much invested in fortran and fortran libs to move off to a relatively barren new language (even C++ is "relatively barren" if you're looking at scicomp tools in fortran for some fields) I've used Rust for scicomp-y things where I didn't need libraries, and it was pretty ergonomic. Explicit integer casting was annoying but that was basically my only gripe.
I disagree. First, it's much too long for some not too uncommon functionality. Second, `_.get_or_none().map_or(foo, bar)` looks weird. Also `if let Some(x) = _.get_or_none() { .. }`. The additional characters don't pull their weight. Finally, it's too late now; stability is more important than bikeshedding.
How on earth do you have time to be a physics student *and* be so involved with Rust?
Once we get type-level integers I don't think that'll to too far off.
[Moved to stable a week or two ago.](http://doc.rust-lang.org/stable/book/error-handling.html)
If you talk about dynamically allocated arrays, you don't even need type-level integers.
I don't *always* want next line though, only for multi-line signatures. fn foo() { &amp;nbsp; fn bar(a: i32, b: i32) {
The point was about `let c = b * a` not compiling, which requires type-level integers (but could be hacked with [typenum](https://github.com/paholg/typenum) anyway).
Yes, it's happening slowly, and in some fields, but not all of them. Condensed matter physics, for example, seems stuck to fortran.
Hum sorry I know nothing about FORTRAN and rust xD in Reality I don't know code and I speak bad english... BUT I'm a fan of free Software and of mozilla and I have heard on phoronix.com than the USA gouvernment, Nvidia and the nuclear department are currently developping an LLVM FORTRAN backend because it is still necessary for scientific compute, source :http://phoronix.com/scan.php?page=news_item&amp;px=NVIDIA-Fortran-LLVM-NNSA And the two links on my first comment describe specific features of FORTRAN that could be interesting for rust, but you rust devs are better placed than me to know what Feature take, so please go on the links and sorry for my ignorance :p
You need to bound the generic `T` with the trait `Mul`, like this: fn foo&lt;T&gt;(lhs: T, rhs: T) -&gt; T where T: Mul&lt;T, Output=T&gt; { lhs * rhs } Thus, you can use the `*` operator. edit: Oh, of course, you need to `use std::ops::Mul`as well.
But with dynamically allocated arrays you can't know the size at compile time and if you don't know the size at compile time then you can't check that the matrix multiplication is valid at compile time.
&gt; we need type-level integers Type-level constants in general would be a big boon for this kind of stuff. I want range floats as well! type MyRange&lt;T: From&lt;int&gt;&gt; = Range&lt;T, 0..12&gt;; :)
We need to implement IEEE-754 in the type system too! :D
There isn't, and I'm not aware of any proposals. You need to *either* use some sort of conversion trait *or* build the value manually out of `One`s and `Zero`es. Actually, you can also define a new trait specifically for constructing the value(s) you want, and implement that for all relevant types, though that might be overkill. Keep in mind that most conversion traits are *really* simple and designed to be inlined, so it's fairly likely that there won't be any overhead in release builds at runtime.
I disagree; this is not the exact feature we really want. - Most array sizes are not known at compile time (data read from a file, passed in as a dynamic array, etc), so this check wouldn't be possible most of the time. - 6 can coincidentally be the value of distinct and generally non-exchangable quantities. We should be tracking quantities, not comparing values. - Failing at compile time is nice, but what really matters is *failing consistently*. Doing so at run time, with a good error message, is nearly as good. An example implementation (one dimensional) is at [1]: we create unique type markers for different quantities, can reuse them in sizes of different arrays that need to be consistent, and using the wrong marker causes compilation errors. Did I mention this works in stable rust? [1] https://play.rust-lang.org/?gist=4135d7ce4f1663380609&amp;version=stable
Please keep experimenting! as someone writing numerical code in rust, I haven't found bounds checking cost to matter, but using the wrong index type is a thing. The worst is when the index used happens to be small enough to not be out of bounds... no panic, just algorithms that don't work. Another approach to a similar goal: https://play.rust-lang.org/?gist=4135d7ce4f1663380609&amp;version=stable I'm curious what you think the differences between the approaches are. Specifically, why you use lifetimes for markers, the above seems to get by with just phantomdata.
`[1000; [1000; u32]]` is not an array of pointers to arrays. `Vec&lt;Vec&lt;u32&gt;&gt;` is.
 fn new() -&gt; Self { let mut list = List{head: None, tail: ptr::null_mut()}; list.tail = &amp;mut list.head as *mut Option&lt;Box&lt;Node&gt;&gt;; /// ... list } is wrong, specifically `&amp;mut list.head as *mut Option&lt;Box&lt;Node&gt;&gt;`. `list` is local to the stack here, and so is list.head. When `list` is moved out of the function, the address of `list.head` changes, but the pointer doesn't get updated. This can be fixed by making `head` a `Box&lt;Option&lt;Node&gt;&gt;` instead: http://is.gd/QOJKqE Be careful with this design, however, there probably are soundness issues with mutably aliased nodes. Implementing a linkedlist is not really a good beginners exercise in Rust. If you still want to do it, or are not a beginner, I recommend going through http://cglab.ca/~abeinges/blah/too-many-lists/book/, an entire book detailing different types of linked lists and how you make them in Rust.
You want r/playrust. The language got here first. :)
I disagree. A linked list is a perfect beginner exercise for Rust. It shows the benefits and the pitfalls of the language, and forces one to think about and utilize references, lifetimes, stack and heap, etc, etc, etc. However, it is absolutely terrible at introducing unsafe Rust.
Err... I meant only the recursive boxed enum implementation. I missed the "learn about raw pointers" bit. Anyway, creating some other container, like a vector, would probably be better for introducing raw pointers, because recursively dereferencing seems like a gun aimed directly at your feet.
Yeah, Valgrind found errors whenever Rust tried to free the `Vec` memory. I'm compiling to a dylib at the moment, but will probably compile to a static lib at some point in the future.
Thanks Manish! I think the key learning for me here is that the address of list.head changes because it moved and that explains the issue. I am aware that this is probably not a good design for Linked List. I precisely want to understand pitfalls of using unsafe, raw pointers &amp; move semantics, etc. I will checkout the link about the linked lists! 
&gt; if the code doesn't reach the forget calls, then won't it also not reach the end of the function and therefore not free the memory? No, that's not how does Rust work. The destructors are always called in Rust, no matter if it's a normal end of function, early return, or panic. (Of course with the exception of the situation when you leak the object with `forget`, Rc loop or in some other way. Destructors are also not run in the case of panic while panicing (that just triggers instant abort)). Anyway, panic can cause stack unwinding into another language which is [a really bad idea anyway](https://doc.rust-lang.org/nomicon/unwinding.html) (Honestly, I don't really know how to idiomatically handle that in stable Rust). Anyway, that's nice that it was easy for you to refactor from vecs to slices, because that's what slices are for! I also want to address one question you've asked in the OP: &gt; Is there a way, perhaps using the function signature, **or using explicit lifetimes**, that I can tell Rust to not free the slice, without using `mem::forget`? Remember, explicit lifetimes can never change the meaning of your code. They are just removed after borrow-checker passes. There are just annotation, that makes you able to prove for the compiler that your code handles memory in the safe way.
This looks great. Perhaps Ranged could be a trait, so that you can distinguish between dynamic ranges and static ranges. In the future that will allow you to create "3 times N" matrix where one dimension is static and the other dimension is dynamic. Sooner or later people will want index arithmetic. I wonder if something like the following is possible let m = range!("n_rows", 4); let n = range!("n_cols", 3); let mn = product_range!("n_cells", m, n); // Edited let i = m.vet(1).expect("row index out of bounds"); let j = n.vet(1).expect("Column index out of bounds"); let k = mn.axpy(m, i, j); // Sets k=4*i+j ensuring 0&lt;=k&lt;12 (Edited several times)
Just use the traits in num, they will optimize to what you want. There's not a difference in cost between `as` expressions and these trait methods.
Could you give an example? I can't figure it out at all.
This is just re-implementing mutual exclusion and thread scheduling, but less optimally. Why have multiple threads if all but one of them are blocking? Just use a mutex or a rwlock.
Yes.
Fair enough, although that interface could perfectly well be encapsulated in the guise of a mutex lock/unlock from the perspective of the worker threads. The implementation of `RwLock` uses `unsafe` to give you access to the guarded data using the lifetime of the [stack-local lock object](https://doc.rust-lang.org/std/sync/struct.RwLockReadGuard.html), and the same scheme would work with your custom scheduler thread.
It's multi-reader, single writer: N threads can read simultaneously, but they have to stop reading to give the writer a chance to write. &gt; but less optimally With a mutex/rwlock, the cost is paid for every read/write. If the workload consists of lots of small reads/writes (e.g. 50-100 nanos), this can add significant overhead. With my approach, if the readers are allowed to run for 300 millis then they pay no synchronization cost during that time (apart from checking the channel from the monitor thread to see if it's time to stop, but this is cheaper than taking a mutex, and can be done e.g. only once every 100 reads, assuming a continuous stream of reads).
So here's an idea that would probably fit your scheme; box your data and have the messages include the box. That way only the active thread owns the data and it can transfer ownership back to the scheduling thread when it's done.
A theoretical advantage of this scheme is not having to pass a pointer to the data throughout the call stack, instead accessing a global variable and relying on the scheduling to ensure access serialization. This may convey some slight benefit in terms of performance and convenience.
&gt; With a mutex/rwlock, the cost is paid for every read/write. Not if you only acquire the lock once at the top level (see `RwLock` for multiple concurrent readers) and pass the acquired raw reference through the call stack. Passing it has a very slight overhead in terms of register allocation and stack spill for function arguments but I would not expect it to matter in a typical case.
Thanks, I didn't know that was possible. I tried something like it before and had issues with the borrow checker, so I thought it wasn't possible, but I must have just been approaching it wrong.
This would fit my scheme most cleanly. I'm not so familiar with how boxes are handled so I didn't realise they would allow this.
I don't think it looks weird personally, but again that is personal taste. Also, it would be uncommon if [] did not panic. You're absolutely right about stability though, I accept that my personal gripes are far less important than not breaking existing code.
Isn't it easier to simply chain the free pages into a free list and just unhook one when needed? The free pages are literally free, you can write anything into them, so it's easy to link them together. To me, your solution seems unnecessarily complex but I guess you'll have to replace it anyway. I look forward to seeing the next step:) 
Thanks, I'll try that.
The problem is that we can't write to these physical frames unless we map them to some virtual address. So writing e.g. a next pointer to each frame would require ~~identity~~ mapping all physical memory, at least temporary. This would make it more complicated, too. The big advantage of this more complicated approach is that we don't need to allocate any memory since we're reusing Multiboot's memory map. Another benefit is performance. If you have 16 gigabytes of memory, constructing/filling some kind of free list can take very long. Thanks for the question, I should really add some motivation to the post. Edit: Of course it would not be required to identity map all physical frames, a temporary mapping would suffice.
Another awesome post. So excited about this series, and I've heard a lot of great things from others, too. Keep it up!
Unless I am missing something the first gigabyte is already identity mapped and it would be very easy to identity map everything. However, I can accept your performance argument:) If you really want to use all available memory in this allocator then it is faster without touching the pages. But you probably don't need 16 gigabyte memory for paging:)
So we'd need `extern "Fortran" `?
&gt; it would be very easy to identity map everything Yes, that's right. But the goal is to remove all non required identity mapping to increase safety. The next post will create a paging module and the post thereafter will remap the kernel so that e.g. writing to the `.text.` won't be allowed anymore. &gt; But you probably don't need 16 gigabyte memory for paging Yes, but then we would need another frame allocator to manage the remaining frames… Of course this is possible, but I'm not sure if it's less complicated. But I'm open to alternative solutions :).
Honestly, I don't have a clearly better plan. I am just arguing for fun and to discuss alternatives:) It is hard to say that something is better without a clear goal and I don't know where this project is headed. You probably have a much better insight into that:) 
good point https://medium.com/@brianbbad/could-safecoin-be-the-next-bitcoin-18c249a0781e#.pbo8mv1my https://www.talented.scot/Advert/180-Open-Source-Software-Engineer-%28C--C--Ruby--Java--Rust%29-Jobs-Scotland-Ayrshire.aspx
&gt;A presence on Stack Overflow, where you have asked or answered hard questions and been recognised by others for your answers Are they looking for someone who is internet famous? Why is this desirable? 
Thank you very much! I have big plans for this series, so I will definitely continue it ;)
I was running into [this](https://github.com/sfackler/rust-openssl/issues/255) issue which should be an El Capitan only issue and is easily solved for Homebrew users by running $ brew install openssl $ OPENSSL_INCLUDE_DIR=/usr/local/opt/openssl/include cargo brew cargo-extras `cargo install` should also work fine (just replace `cargo brew` with `cargo install`). And yes, I am. `cargo brew` can install cargo-extras absolutely fine so you're good on that front. If you want to include `cargo brew` in cargo-extras I'm completely happy with that and completely happy to help as well, `cargo brew` is `cargo install`able and can even install itself so I don't envision any issues. *However* since `cargo brew` requires Homebrew, including it in cargo-extras might bother users who have no intention of using Homebrew.
Hmm… I wasn't sure about that. I just assumed the memory map is correct. It looks like this for 5GiB: start: 0x0, length: 0x9fc00 start: 0x100000, length: 0xbfee0000 start: 0x100000000, length: 0x80000000 So GRUB created the memory map for Protected Mode, where this area is not available? And by switching to Long Mode it became available? Why is that? Sorry for the many questions :) 
How far do you want to take this? Is this going to be an actually usable OS (in some sense of usability)? Do you plan to finish it alone or plan to have contributors? Any design documents?:) I'm curious:)
Other commenters have addressed the specifics; I want to point out a general lesson: If you think you know something about concurrency, but you are bypassing all synchronization mechanisms, then you are probably wrong. Aside from particular CPU's (and you didn't mention a CPU restriction), a memory barrier is needed between accesses to the same memory from different CPU cores. Synchronization primitives take care of the memory barriers for you.
Because cool startups think it is not enough to spend 8h a day in the office, one still needs to go home answer questions on SO and upload code into Github. It is their offer, so they are free to place whatever requirements they wish, but it severely limits those of us that have a life outside work and aren't allowed to do such tasks by our customers during work hours. 
I interviewed with them with zero Stack Overflow presence - I don't think it's actually a requirement. It looks like they're getting fairly desperate for developers right now - in the past couple of weeks I've been contacted by loads of recruiters for this position.
The method name vet caught my attention. I'm doing an experiment with trusted marked indices and ranges at https://github.com/bluss/indexing it might be interesting
Yeah I wrote that example. Bluss made it into a crate: https://github.com/bluss/indexing Sounds like what they want, indeed. Edit: the keyword here is generativity. In this case we're basically hijacking universal typing and closures (in particular, the fact that a function can claim to work with "any" lifetime) to create existential types. Note that this trick is *very* brittle unless you know what you're doing!
Python gets used when speed isn't that big a deal. Same for matlab. I'm talking about specialized routines / libraries for different fields which do calculations specific to the field and take lots of time. While python and matlab do get used at times for from-scratch things, fortran is still used by many fields and there's no moving off to other things on the horizon.
Bad tooling is one of things that's pushed me away from other technologies/languages. I'm sure these languages/technologies are great, but I really don't have enough time to debug complicated installation issues if things don't work on the first try, and in most cases it's not worth it. I'm sure great tooling is one reason Rust has been gaining so much traction recently, and the simple language benefits wouldn't have been enough to get more people to try it. (OT: I find your terminal prompt strange :P)
My prompt is great! All I ever care about is what folder I'm in, and also I get to pretend I'm invoking methods of it!
Haha, I feel like the `::` would just confuse my brain, but I guess I get the appeal.
Updated my comment with these changes, thanks.
&gt; You're right. The plan is to manage a small amount of memory seperately as soon as we need contiguous physical frames for e.g. DMA. I originally had that plan (for a different problem I ran into relating to how I was originally going to handle paging), and I can tell you first hand trying to handle two separate pools of memory is awkward. More-over though, it becomes useless once you realize that you're probably going to write a full physical-memory-allocator so that you can efficiently allocate contiguous pages, and once you do that you can make that memory-allocator allocate everything. (Though there is something to be said that having multiple pools of memory isn't actually a bad design in some cases, since you're probably going to have to lock the memory-manager on every allocation. If you have pools, all the pools can be locked separate from each-other and you can allocate multiple pages at the same time. In this case though, both pools can allocate the same type of memory.) &gt; I think it's possible to use the freed frames itself as backing store. That way the frame stack can dynamically shrink and grow depending on the used memory. I hope I'll find a non-ugly way to do it :). You could - You could make a linked-list out of the free-frames and use that. Essentially you'd be making the standard linked-list allocator (Attach every page into a linked-list, take off the first page when you allocate), but delay actually setting up the linked-list until pages are actually used and freed - It's a good idea to improve a standard linked-list approach. I was thinking of storing page-numbers in an array and doing it that way, but it's a but more complex. That said, storing the page-numbers into a preallocated array would avoid the below issue: The ugly part comes when pages aren't mapped in when you do a free. If you want to make a simple linked-list of free'd pages, then you actually have to do it via the physical addresses of the pages (Since they don't have a constant virtual-address), and you'll have to map and unmap the pages as you free and allocate more pages. IMO, it actually worse then it sounds: You'll face the annoying issue that your physical-memory-manager will need to map pages into virtual addresses, and generally such a map could require allocating pages to use for the page-table entries which actually map your page at a virtual address. You'll dead-lock your allocator, or corrupt your free-list if you're not careful about how you do this in respect to the lock you have protecting your allocator. IE. In the allocator you can't just call some generic code to map a page at a virtual address, because you have to hold the allocator lock the entire time - If you don't, someone could come along and mess with the allocator's free list while you're attempting to map an already free-page to modify it's links in the free-list. But, if you don't release the lock and the generic page mapping code attempts to allocate a page for the page-table, then your kernel will dead-lock because it will attempt to take the allocator-lock, which will never be released because the same thread already took it. You'll either need to do a little bit of fancy locking, or have separate ways to map page so they can be accessed by the allocator, which preferably never attempts to allocate memory (And if it does, doesn't attempt to take the allocator lock multiple times). &gt; That's the alternative if the frame stack design doesn't work out cleanly. Which allocator design would you recommend? Currently, I'm using a bitmap-less buddy-allocator. I got the idea reading about the buddy-allocator in the Linux Kernel, and it's amazing how simple it is - I'd be happy to explain it if you're interested, it took me a while to understand buddy-allocators because the info out there isn't really amazing in my opinion. The only disadvantage to my design is that you do need some type of data structure to represent every page (A bit memory heavy), but I've already made use of that structure in a few different places in my kernel so it's not a loss. (You can avoid the per-page structure if you use a bitmap). My per-page structure is cacheline aligned, so they come-out to be 32-bytes or 64-bytes long. I was worried about how much memory the structures would use, but on a system with 1GB of memory it would only use 16MB of space. It's a decent chunk, but it scales with the amount of memory (A 128MB system would only use 2MB), so at the end of the day it's really not a big deal - If you do the math the table will use 1.5% of the available memory. I allocate that memory on boot using a simple allocator like the one you outlined - These structures are obviously never going to be free'd, so it works fine.
The bodies of all of these methods for these types are literally just `as` expressions. LLVM will certainly be able to inline them and then do exactly what it would do if they were written as `as` expressions in your source code. Parameterized types are statically dispatched and don't have run time costs.
Why does it look like something's broken? The Multiboot memory map gives you physical RAM, and there are memory mapped I/O regions just before both the 1M and 4G marks of the physical address space, so that output for 5GiB looks fine to me.
I'm working through the book as a newbie to the language. (Debating if I want to use Rust or F# as a cross platform language for various radio related applications. I need to explore more and figure out how to build state machines in functional languages.) One observation, the documentation and cargo are freaking awesome compared to some of the other languages I use for work. I just wish there was a way to find Rust folks in the local area. 
Glad to read this! I did something slightly different when I was playing around with this a few weeks ago: https://github.com/steveklabnik/steveklabnik.github.io/blob/rumprun/Makefile#L3
Hehe, so this was actually the source of my angst and hunt across the internet to find a solution (and subsequent blog post). I originally tried `mkisofs` (which appears to be a clone of `genisoimage`) and was unable to make the image writable, no matter how I tried to finagle permissions or options. I always received a `"Read-only file system (os error30)"` error when trying to write to the mounted drive from inside the unikernel. Did you happen to try writing to the drive from the unikernel? If so, I'd love to know what you did...it would be much simpler than the `dd` method! :)
Wow, thank you for the detailed answer! &gt; a full physical-memory-allocator so that you can efficiently allocate contiguous pages Is it really needed? In which cases do you need contiguous pages (except for DMA)? And how much DMA memory do you allocate? &gt; trying to handle two separate pools of memory is awkward I agree that the idea of multiple memory pools seems to be complicated. But a seperate pool for DMA memory could make some things easier, too. I kind of like the idea that I can simply ignore physical memory fragmentation :). &gt; I was thinking of storing page-numbers in an array and doing it that way Me too, except that the array is variable sized like a vector. If the array is full, we use the next freed frame to increase its capacity. And if it's empty, we can decrease the capacity to use the last available frames. Of course it needs some large portion of virtual memory to access it, but this shouldn't be a problem in 64-bit mode. However, it has the “physical allocator needs to map virtual pages" issue, too. &gt; You'll face the annoying issue that your physical-memory-manager will need to map pages into virtual addresses, and generally such a map could require allocating pages to use for the page-table entries which actually map your page at a virtual address. You'll dead-lock your allocator Yes, that's a really ugly problem. I stumbled over it when I tried to implement the frame stack the other day. It really took me some time to understand why the Rust compiler was always complaining. By the way, this is something where Rust really shines. If the page mapping code is modeled correctly using `&amp;mut`, Rust won't compile if you try to create such a circular reference. &gt; You'll either need to do a little bit of fancy locking Something like that is my idea. Instead of an allocator lock, we use a global paging lock. The page mapping code exclusively borrows the paging lock and the frame allocator and can now {de}allocate frames when needed. The `allocate_frame` and `deallocate_frame` method borrow the global paging lock exclusively as an additional argument and are thus allowed to call the page mapping code, too (which re-borrows `&amp;mut self` as allocator). It's a bit complicated without code but the implementation is not quite finished yet. If everything works out, you will see it in the next post :). &gt; I'd be happy to explain it if you're interested, it took me a while to understand buddy-allocators because the info out there isn't really amazing in my opinion I agree completely. I think I understood the concept, but I don't know how to actually implement one. So I would really appreciate an explanation!
Ah, boo. Was hoping for a nicer method :(
Yeah quickly skimmed over the options and was like "euuugh sh-bangs nooo". Not super rational, just my preference.
I just wanted to comment on something, this: match fs::create_dir("./.werker-logs") { Err(e) =&gt; { match e.kind() { io::ErrorKind::AlreadyExists =&gt; {}, _ =&gt; panic!("could not create log directory: {}", e), } }, Ok(..) =&gt; {} } Is better written as: if let Err(e) = fs::create_dir("./.werker-logs") { match e.kind() { io::ErrorKind::AlreadyExists =&gt; {}, _ =&gt; panic!("could not create log directory: {}", e), } }
&gt; Is it really needed? In which cases do you need contiguous pages (except for DMA)? And how much DMA memory do you allocate? &gt; &gt; I agree that the idea of multiple memory pools seems to be complicated. But a seperate pool for DMA memory could make some things easier, too. I kind of like the idea that I can simply ignore physical memory fragmentation :). A better question would probably be: How common is it to know ahead of time that a buffer you're allocating is never going to be used for DMA? If you do hard-drive DMA, it can do DMA on any 64MB-aligned address, and there's really no reason not to take advantage of this. Unless you know ahead of time that there is no chance your buffer would ever be used for DMA, then you *need* to allocate it from the DMA pool, or else you risk attempting to do DMA on a non-supported address. You could do some sort of marking of which things need to take DMA addresses, but I suspect that, since that info will end-up 'bubbling up', tons of interfaces will end-up with the DMA requirement that seem to have no need. More-over, the large majority of allocations in your kernel are either going to be single pages, or done via a separate general-purpose allocator, meaning that there won't actually be a difference between an allocation from the DMA pool, and the 'regular' pool. I completely understand the want to do a complete map for your kernel memory so you can avoid having to allocate contiguous areas of memory, but it doesn't solve much of a problem and creates a nightmare to attempt to manage. Any allocations can trigger more allocations, and also any allocation require modifying every copy of the kernel's memory map (And every process gets it's own page-table). The idea sounds nice on paper, but stinks when you actually try to implement it. If you directly map the kernel's memory on boot, then everybody already has a consistent view of the kernel's memory. &gt; Me too, except that the array is variable sized like a vector. If the array is full, we use the next freed frame to increase its capacity. And if it's empty, we can decrease the capacity to use the last available frames. Of course it needs some large portion of virtual memory to access it, but this shouldn't be a problem in 64-bit mode. However, it has the “physical allocator needs to map virtual pages" issue, too. &gt; &gt; Yes, that's a really ugly problem. I stumbled over it when I tried to implement the frame stack the other day. It really took me some time to understand why the Rust compiler was always complaining. By the way, this is something where Rust really shines. If the page mapping code is modeled correctly using &amp;mut, Rust won't compile if you try to create such a circular reference. &gt; &gt; Something like that is my idea. Instead of an allocator lock, we use a global paging lock. The page mapping code exclusively borrows the paging lock and the frame allocator and can now {de}allocate frames when needed. The allocate_frame and deallocate_frame method borrow the global paging lock exclusively as an additional argument and are thus allowed to call the page mapping code, too (which re-borrows &amp;mut self as allocator). &gt; &gt; It's a bit complicated without code but the implementation is not quite finished yet. If everything works out, you will see it in the next post :). I would really recommend just calculating and allocating that array directly on boot, so you don't have to deal with variable-sized idea. Generally speaking, anything variable-sized is going to be a headache in kernel code. It has a max size it can be, and you can reasonably make it a bit smaller then that. With addresses only be 8 bytes, you can get 512 of them in one 4K page, so 1GB of memory would require a 2MB 'stack'. That's not really that bad, and even if the variable-sized version would only need to have a max of 1MB, that's only 1MB of difference. I'd be wary about a 'global-paging lock'. It's not exactly clear what data it is that it's locking (And locks should really always be about locking data, not code), and having such a big lock brings performance penalties (Though it's not really a big worry at this stage). More over though, having a 'global-paging lock' doesn't actually solve the problem, it just prevents you from an 'actual' dead-lock (where you spin-forever trying to take a lock). The code itself is still circular - If you call `allocate_frame()`, it needs to read the first entry in the free-list (Because it wants to use that free-page for the allocation). Taking that entry off the free-list requires a call to the mapping code to map that entry so it can be read (Note that we have to make this call before we actually do the allocation, because we need to update the free-list. We need to read that entry to get the address of the next page in the free-list). If the mapping code calls `allocate_frame()` again, then while this doesn't dead-lock because you hold the global-paging lock, it still loops forever because the second call to `allocate_frame()` is going to try to allocate the same entry in the free-list the first one did (And call the mapping code again, which will call `allocate_frame()` again...). The bigger issue here is that, by allowing the two to recursively call each-other, you're attempting multiple allocations and mappings at the same time, and you don't prevent them from just allocating or mapping the same addresses you already called them to do. Allocating the stack on boot may solve part of the problem, but you still have other issues to account for. `allocate_frame()` can now get a frame off of the free-list without having to map it, but it still has to map it before returning it's address. When you do that call to the mapping it could call `allocate_frame()` again, and when now when you allocate your next frame (Which should work correctly) you'll call the mapping code a second time to map this new frame into place. You have to ensure that at some point the loop here will end without you getting a stack-overflow (Not very easy to guarantee), and you have to keep track of all the mappings you've "promised to map", so that the recursive calls of the mapping logic don't just map to the same places as the earlier call. This also isn't very easy to guarantee. Regardless of what you do, you either can't allow your allocator to call the page mapping code when it does the actual allocation, or you can't allow your page mapping code to call the allocator. When I was originally attempting this, I actually 'solved' this by doing the later - I set aside a section of memory that was identity mapped and had a separate allocator for it. Whenever paging code needed a page to use as an entry in the page table, it would ask *that* allocator instead of the regular one. This avoids the loop because the second allocator only allocates pages that already have virtual addresses we can access them at, so it will never call back into the mapping code, and every mapping can be done without calling the normal allocator. In some ways, you could combine this idea with your DMA idea and just treat them one in the same - Keep a section of memory for DMA/paging/etc. that is identity mapped to the physical memory, and keep a section of memory that is virtual and can be mapped on demand. But the question of how much memory dedicate to the DMA/paging/etc. pool is now a harder problem to solve. Personally, while it may feel that I'm shooting your ideas down, if you have an idea then I would roll with it and try to get it working - There's no one right way to do it really, and it's the best way to learn what's a good idea and what's not a good idea. I do think you'll find though that while your idea is appealing, it creates some hard to solve problems - And identity mapping the kernel space and then having it allocate physically contiguous pieces of memory really solves the problem. But if you get it working in that way that you like the more power too you. I'm going to put the buddy allocator stuff in a separate reply, this is getting a little long...
 &gt; I agree completely. I think I understood the concept, but I don't know how to actually implement one. So I would really appreciate an explanation! You may or may not find this [link helpful](https://groups.google.com/forum/#!topic/linux.kernel/7oKtsQavV6Q), but it's a post to the LKML detailing how they removed the bitmap from the buddy-allocator. Something about that post made it all click for me, so you might find it helpful information. I have all of that information down below though: The basics of understanding a buddy-allocator is really understanding what 'buddy' refers to. It refers to the fact that every page has a corresponding 'buddy' page. When you combine a page with it's 'buddy', it creates a new "page" that is twice as big as your original page. This new page is said to be one order higher then the original two pages. Every order doubles the size of the "pages" that it holds, and the lowest order size is a set size. So, if your buddy allocator is going to allocate 4K chunks, then the size of the 0th order would be 4K. The size of the 1st order would be 8K, the size of the 2nd order would be 16K, etc.. You can continue this as many orders as you want (More orders allows for larger allocations, at the cost of some performance. Note that you're not allowed to 'skip' orders. So if your allocator has orders 1 and 8, it has to have the orders 2 through 7 as well). Every order contains a list of 'entries' in that order. I call them 'entries' in an attempt avoid confusion. For order 0, it's entries are 4K pages, but for order 1, it's entries are 8K "pages" instead. Calling them entries avoids the confusing about what size the "pages" are. All of the operations we do on one order (Allocate an entry of that order size, or free an entry of that order size) can be done on any other order, so there's no need to specify pages. It's important to understand that every entry has a *single* buddy. Generally you go by even and odd addresses. So, for every "even" entry (If you think about the page-frame-numbers, an even page-frame number indicates an 'even' page, and an odd page-frame-number indicates an odd page), the corresponding "odd" entry is it's buddy. This restricts some combinations, it's possible for two entries to be physically contiguous, but not allowed to be combined because they are not part of the same even-odd pair (For example, entries 1 and 2 are not buddies, even though they are physically contiguous. entry 1's buddy is 0, and entry 2's buddy is 3, always.). If you want to allocate an entry of size order 0, you first check if there are any empty entries in order 0. If there are none, then you check if there are any free entries in order 1. If there are, then you "break" one of those entries into it's two buddies, add both of those buddies to the order 0 list, and then remove one of them from the list and return it as your allocated entry. If order 1 didn't have any entries, then you would just keep going up until you hit the max order (IE. Attempt to break an order 2 page in half to get two order 1 pages, and then break one of those two order 1 pages to get an order 0 page). Also note that this works for any entry size, not just order 0 - The same steps works. Thus, allocating a order 2 entry doesn't require touching any of the information from order's 1 or 0. Also note that every time you break a page you leave another unallocated page of the same size, thus only some of the allocations have to actually go through breaking a page, the others can simply pull an entry off the list. Whenever you free an entry, you supply the order to the free function, and then treat the free'd address as an entry of that order and add it to that order's list of free entries. Once you've done that, you check the status of that entry's buddy. Since you just free'd one entry, and you only combine entries with their buddy, the buddy must either not be free, or be a free entry of the same order (Being a member of a higher order entry isn't possible, that would require combining that entry with an entry that isn't it's buddy). If that entry's buddy is free, then you combine both of them together into one entry of the next sized order and repeat everything with the new entry that has a higher order. Every 'order' contains a list of the entries currently contained in it. Note that an order's list doesn't also contain the entries in higher orders. Thus, it's entirely possible that order 0 is completely empty, but order 1 has 10 entries in it. This is because when you move two buddy entries up into a higher-order, those two entries completely leave the lower order and can't be allocated as the lower order size any longer. This sounds annoying, but actually makes things simple. Conceptionally, it may seem like all of this is complex, but my implementation is [less then 300 lines long](https://github.com/DSMan195276/protura/blob/master/src/mm/palloc.c) (Sorry, it's C), and the actual logic for the allocator is probably closer to 150 lines, or a bit less. It's just shifting things around and adding them to the correct list. The crazier stuff is when you attempt to remove the bitmap. I haven't actually mentioned where you need a bitmap (And my code doesn't use one), but it really comes down to marking which entries an order currently holds, and which an order doesn't hold. I always mentioned each order containing a 'list' of the free and not free entries, but really you'd normally instead have a bitmap, where a '1' in the bitmap means you have that entry, and a 0 indicates you don't. This is generally simple and easy to implement. My code avoids the bitmap by having each order keep a list of free `struct page` objects. The page you put into the list is the lowest address of that entry. For a 16K entry, it's made up of 4 4K pages, but the pages are all contiguous, so we take the `struct page` of the first page in that contiguous series and attach that page onto the list (The other pages go unattached to any lists). You can find the starting page of the corresponding buddy entry by simply taking your current page-frame-number and XORing the bit corresponding to the order your in. For order 0, you flip bit 0, for order 2, you flip bit 2, etc... It's a pretty cool feature. Ex: If you have an entry that starts at page 4 in order 1, it's corresponding buddy starts at page 6. Since order 1 is 8K, this makes sense - page 4 and 5 are buddies that make-up a single 8K entry, and pages 6 and 7 are also buddies that make-up a single 8K entry. Together, 4, 5, 6, and 7 can make-up a single 16K order 2 entry. This technique is used to quickly find a page's buddy from the starting page-frame-number, and after you get the buddy's `struct page`, the `order` field in it's `struct page` tells you what order it is currently contained in, so you can quickly check whether or not you can combine your two entries into a single one with a higher order. (IE. If the order is equal to your current order, combine them, if it's not equal, don't combine them). Thus, when you allocate you just get the first order's list - If it's not empty, you take one entry from the list and return it. If it is empty, then you call `break_page()` to attempt to break-apart an entry from a higher-order then our current one. If that works then we take one of those broken entries and return it. Freeing is the opposite - Check for the buddy entry, if it's in the same order as the entry being free'd, combine them and 'free' the new combined entry instead. When the buddy isn't free, you stop and add our current entry onto the current order's free list.
So, IIUC, the indices fn accepts a container v and a closure, and runs the closure passing it a wrapper around v and indices such that the wrapped v can only be accessed with these indices. But IIUC, this protection is valid only for the closure?
Interestingly, I get a "403 FORBIDDEN!": &gt; Either the address you are accessing this site from has been banned for previous malicious behavior or the action you attempted is considered to be hostile to the proper functioning of this system. &gt; &gt; The detected reason(s) you were blocked are: &gt; RDSNET is a constant source of spam and attacks (HN-0090)." So almost the entirety of Romania is banned? I can probably access it from my uni, since education is separate (alongside various branches of govt), but it still seems a bit excessive to ban more than half of a country.
Depends on your local area, but giving a shout-out on rust-users may work.
I really don't like using ZD Block because I never know who is getting block and I'm sure it does gets a lot of false positives, just like yourself :( But unfortunately without the filter in place, the form does get hammered by SPAM bots and it is then a real pain to go back into the forum and clean up the mess the bots create.
Well then use the new version of captcha which requires the user to select images? That one is bot-safe so far.
&gt; Structs initialized at different places will have distinct addresses, even if they are zero sized. This is so that they can be freed independently of each other. Is this true? I was under the impression that certain containers (like Vectors) were able to store N zero-sized objects using zero bytes per object. A quick search *seems* to support this idea - I came across [this](http://www.wabbo.org/blog/2014/03aug_09aug.html) blog post analyzing an odd bit of code, and it *looks* like user-facing parts of the std library will actually return a completely invalid pointer to 0x00000001 if the object pointed to is zero-sized, in certain cases. This seems to imply that we can't rely upon zero-sized structs having unique addresses.
I don't think such a thing exists yet but it would be awesome because it would lead a lot of people into trying Rust. 
Yeah, I think I was incorrect there.
It also bans the entirety of the IP ranges from my server provider (OVH), which is the biggest in France.
&gt; &amp;gt;a memory barrier is needed between accesses to the same memory from different CPU cores Is this still needed if atomicity isn't required? Yes. In the absence of a memory barrier there's no guarantee that a CPU will *ever* see the updated value. It might have it in cache, and so won't read it from memory in the absence of a cache invalidation instruction. Intel chips tend to have snooping caches, so a memory write *will* invalidate the cache, but some CPUs don't pay the transistor cost for that. 
Only if you really need specialized algorithms. Fitting data and doing matrix calculations is fine as both languages are using specialized algorithms written in Fortran ages ago. All I wanted to say is that nowadays you don't need fortran for such tasks. 
I think you're mostly on the right track. One thing that might be confusing is that `use` starts from the "root" namespace whereas everything else starts from where you are. So these are all the same: // c.rs use a::f3; pub fn f4() { f3() } // c.rs use a; pub fn f4() { a::f3() } // c.rs pub fn f4() { ::a::f3() } // c.rs use ::a; // superfluous :: pub fn f4() { a::f3() } &gt; Is this the "proper" way to split a project into multiple files? I read the rust book section on modules and crates, but there the code is split into separate crates. To me that seems overly convoluted, I don't really get what the advantage of doing that is. Splitting things into several crates can help with compilation times for huge projects (if you change one file, often only that crate needs to be recompiled). &gt; In some languages, you can simply put a bunch of files in the same directory and whenever you need to access one file from another you just "import" it, without having to care about what other files are using this one too. Is there a way to do something like this in Rust? You can use [the include macro](http://doc.rust-lang.org/std/macro.include!.html) but this is generally frowned upon, i e, not "Rust-y".
Of course. And scientists use all that regularly. But fortran is deeply embedded in scicomp (at least, some fields of physics) for a reason, and that reason is not fitting/matrix stuff. It's the rest of the library support, and a little bit about how the language works.
The Zeus forum was upgraded to the very new 3.x version and after that upgrade it was configure to use the very latest 3.x captcha. But from my experience, for a little while it worked better than 2.x captcha, but eventually, it was also got cracked :( The Zeus forum has been active for over a decade and over that time I have only seen a massive increase in SPAM traffic. In that time, I've also tried many of the phpBB anti-SPAM modules as suggested on the phpBB forums and while some do seem to slow the SPAM, eventually they all seem to fail :( Now, as I said earlier, I don't doubt the ZB solution is killing users that are **NOT** spam bots and that is unfortunate :( But the reality is, as a forum moderator, ZB does make my live easier as I no longer have to constantly monitor the forum for spam and porn postings. It is not a good feeling when you visit your forum, 24 hours later, only to see it has been flooded with hundreds of links to porn sites, all with explicit embedded porn photos. 
Can you post me an example of this new captcha method being broken by bots already? This is my favorite forum software for now, which sadly does not have AntiSpam yet :/ https://github.com/flarum/flarum http://flarum.org/features/ Let's see how they will solve the issue.
Excellent, thanks! I'll give this a shot, always happy to avoid using `dd`. It was just the only hammer I knew :)
Are u hire remote?
Oh, forgot the last thing: I *think* i read somewhere that I needed to use `&amp;*foo` to get a `String` to be a `&amp;str` but it looks like I was mistaken here. I also thought it to be weird to use that notation everytime. Thanks.
I *think* [rusty-cheddar](https://github.com/Sean1708/rusty-cheddar) will be in a usable state (though a very basic, unpolished state) by the end of the week. Rusty-cheddar is a program reads a Rust source file and produces a C header file for the bits which are callable from C. 
You can read about `String` vs `&amp;str` here: http://doc.rust-lang.org/book/strings.html - you'll need to have read and understood the chapters on ownership and lifetimes first. The short version is that `String` is an owned string, which may grow and be modified; whereas `&amp;str` is a "view" of a string - you cannot modify it. Which you use depends on what you're trying to achieve - if you need to modify a string, then `String` is useful. If you're simply observing a string, for the purposes of printing it out for example, then you'd use `&amp;str`.
&gt; I got bored around here. Here's the full code at the end. I think some bits could be simplified even further e.g. `std::env::Args` is an `ExactSizeIterator` so you don't need to `collect()` it to get its length.
Considering the nature of C and the nature of the code written with it, it's probably an impossibility. I'm investigating porting a medium sized C library to a safer, more functional language and the only thing being converted is the basic design and function, a translation utility would create a bigger mess. (The darn thing needs to be redesigned anyway.) 
&gt; The "book" found on the rust website helps you but tends to just skip some essential explanations of the syntax which you have to look up yourself - not a big issue though. Anytime you find something like this, please file an issue at github.com/rust-lang/rust (or, even feel free to contribute your own explanation). /u/steveklabnik is tasked with the book and _happily_ takes up every feedback. Feedback like that is very valuable, because we are only a beginner once.
Greetings. Right now rustc is in transition as far as how you get it goes. There is currently a ton of work being put into an installer for officially supported branches. Getting the source from github is still the most reliable way to get and compile rust/racer/anything not available from cargo. As we see the toolset finalize more I expect that we will see more packages come in the way you describe with more inbuilt tooling. Right now there is just too much fluctuation to finalize an IDE, but we are so close.
That would be cool if you implement strict segregation of duties in your multiuser engine, so there is no single almighty super-user, but several with different abilities. Meaning that a rights-admin cannot grant/revoke access rights if there is no request from user itself or someone else. And everything is logged.
Thanks! This is my github activity right now: [more than 20 pull requests pending](https://pbs.twimg.com/media/CT8gtwiXAAAhd9C.png) ;)
I took a look at a few libraries, and each has its own vocabulary. `many0` and `many1` come from Parsec, in Haskell. There are a few examples in the [tests directory](https://github.com/Geal/nom/tree/master/tests), a [tutorial](https://fnordig.de/2015/07/16/omnomnom-parsing-iso8601-dates-using-nom/), and a few libraries on github to take inspiration. I plan to write more and more examples, now that the 1.0 is out!
Yes, crates.io reverse dependencies. Also, [this](https://github.com/search?utf8=%E2%9C%93&amp;q=filename%3ACargo.toml+nom). It took a while, but it was worth it :D
Could you show me the code? By default, nom does not keep track of the position of line endings, but it should be possible keep that in state somewhere.
Awesome news. nom is a great success story for Rust.
thanks!
Are you confusing this with an upstream pull request to the library? The library maintainers are depending on his package, meaning their maintenance burden is limited to updating and fixing BC breaks when they update the dependency. He took that cycle out for them leaving no extra work here.
Yeah that's very nice, but your "in an age" seems to be a jab directed at library maintainers in general.
Is there a shim to make nom look like lalrpop so nmatsakis won't ban you from the Rust project? :P
I've been gradually speeding up my [regex-dfa](https://github.com/jneem/regex-dfa) crate, but as it grows the edit-compile-run cycle is getting pretty long. This week, I'll try and split it into multiple crates: one for compiling regexes down to DFAs, one for running DFAs, and maybe also one for some data structures. Besides improving compile times this will also force me to improve my internal APIs, which are a bit of a mess right now.
So, quick question. I have a Masters degree and roughly 8 years writing tooling, parsers, and two large multi threaded, plugin based, 'always up', services in the industrial sector.....but I've written almost nothing in rust. Would I still be a viable candidate if I need to get up and running in rust (as I'm doing now), or is it a "know rust or it doesn't matter" type of situation?
Is that really all that's holding back this? Windows support isn't even really needed for 99.9% of web server applications. But if that is the case then it may be time to contribute to mio
Yes, hyper is taking cross-platform seriously, which means that all platforms need to be supported before it'll merge. or at least, that was the status last time I checked.
Well, "character" isn't well defined in unicode, so it depends. But `String::pop()` returns a `char`, which is a unicode scalar value, so we'll go with that :) Easiest way is to use an iterator: let s = String::from("hello world"); let last_two: Vec&lt;char&gt; = s.chars().rev().take(2).collect(); println!("{:?}", last_two); Take the chars, reverse it, then take two.
People have already answered some of the issues, but I've added my own. &gt; return types - In "my language" I was thinking about infering return type. Rust could infer return types but it's not done on purpose. The thing is that a function header is an interface, and you must be explicit about what you promise to people. Closures, OTOH, are not interfaces (they generally aren't called by name, but instead passed around as values) so they will try to deduce the return and argument type. The idea is that I can give you a list of function headers and you should be able to deduce what they are doing and how you can use them without reading any other documentation. Types is a fundamental part of this. &gt; compile-time reflection Rust already has some forms of compile time reflection. I think it would be very useful to at least have more powerful macros for reflection. The problem is that adding macros to the library will very quickly pollute the namespace, macros, in general, still need some love and rethinking. What I'd like is to have a macro that takes a data type and returns that data type with all the fields being references to something else. &gt; range types There's already crates out there than offer numeric types. You could easily extend this system to your own. This system could be done a library but no one has quite gotten this yet. Also all array indices can be range types already. Look at the [Index trait](https://doc.rust-lang.org/std/ops/trait.Index.html) to see more about this. But you can already do something like `&amp;num_list[2..4]` to get a slice but `num_list` a `[T]` or `Vec&lt;T&gt;` or `[T;10]` or even a `String`!.
It needs to be seamless because that's the only way a large number of people will start using it. Rust doesn't have many benefits in scicomp. Segfaults aren't as big a deal -- logic errors happen more often, and the software is run locally so a segfault is just another runtime error. Preventing runtime errors is awesome, but Rust has an ease of use tradeoff there. There's a general attitude in programmy physics to "get things done", for example you may find a lot of "bad" code which everyone is fine with. I've not seen this so much in astrophysics (IIRC they don't use fortran anyway), but it's there in condensed matter/nano, somewhat in NLD, and it seems to be there in other fields too. So Rust doesn't really fit into this philosophy. And C FFI in Rust isn't just a matter of declarations. That's okay. You need to convert between string types, manage allocations, and do a lot of other unsafe-guarding things. Which is necessary in fortran too. On the other hand, C++-C "FFI" is straightforward, just works. Unless we can get that kind of FFI (which we aren't, because unsafe), I don't see Rust or any other language being adopted by the physics community (or the subsets which use fortran a lot). They're a stubborn bunch.
Thank you very much for the great explanation! You should really publish it somewhere, it's too good for just a reddit comment ;). I will definitely take a closer look at your implementation when I have some time. 
Theoretically this is impossible, just like it's impossible to prove if a program will halt or not. But we don't care about hitting every edge case and proving everything correct. Often a 90% solution can solve more than 90% of the use cases. So what would we need to even get this going? * A large corpus of C code to experiment with. * A tool to build C AST and display it in a reasonable to experiment fashion. * A tool which attempts to convert the C AST into idiomatic Rust code with generally the same semantics. * Some kind of reasonable way to interact with the conversion tool which makes this not a "try and then throw it away on anything not a toy since it's so bad". One example of what would be needed is a way to group parts of the C code base into sections and then convert sections at a time. We should match common C code libraries vs common Rust libraries. If I'm doing something in SDL2 in C I would expect an equivalent code base using the SDL2 bindings in Rust. The same for GTK, QT, Win32API, etc. Those last bits, where library to library matching occurs, would be something that would be 'cleanup and usability' after a basic system is working. But without it, the system would be mostly useless. It would be the tedious but necessary grunt work at that stage. The interesting bit is the code grouping part. If we can just convert *sections* at a time, and more importantly interactively suggest sections to convert, the usability of the system becomes much greater at that point. I could foresee something like this being as useful as valgrind or debuggers have become. Something that at first seemed magical and impossible, but now we see as necessary and reasonable once we recognize that we don't need to solve perfectly the whole problem, but instead give better insight into the underlying problem as well as some automation of the domain. Example of things we could do: Here are some C structures and here are some functions which seem to use/take these structures. Should these be methods on these structures? Here are some examples of how we could convert them, select the conversion which seems reasonable. Should we group these structures and methods into a module? named what? etc etc. We could look at this as a whole code transformation refactoring tool where we are slicing off sections of code to work in rust and then reorganizing the rust code in that area.
Hey this is cool! I am working on a similar thing (simpler) with websockets and broadcast messages as part of my robot's software. We should talk. Are you on IRC?
Congratulations and thank you! Nom is awesome!
I have just updated the code and tried to implement all of your suggestions, thanks again. Did I miss anything or does it look fine now?
I'm actively working on async io support in hyper. As it works, I'll be pushing it to a branch on GitHub, but I won't make a release to crates.io until the windows support is in mio. Hyper currently supports Windows, it feels wrong to release a new version that takes that away. 
Using my own suggestions against me! I am forever shamed! \^_\^
The other important part is that a `&amp;String` can be used in place of a `&amp;str` due to [`Deref`](http://doc.rust-lang.org/std/ops/trait.Deref.html) and [deref coercions](http://doc.rust-lang.org/book/deref-coercions.html).
I started writing a small client/server chatting application which will be using sockets, and maybe in the future GTK. I just realized that I need to practice a little more with `std::thread`, and the `std::net` crates. On other news, I'm attempting to contribute to `cargo`! Fun stuff!
Its not like i am complaining or anything.. I noticed this when 1.3 came out and i had 1.2 installed will racer and sublime. I installed 1.3 and did not update the racer to point to the latest source. Auto complete had stopped working and it took me a while to realize that i forgot to update racer. Anyways its young and its gonna be better.
Thanks! I find myself at that fun stage known as 'fight the borrow checker'. I'll get passed it, but it's where I'm at right now.
I'm doing essentially the same thing. The way I use to "get good" with a new language is to write an IRC bot and bot management program. I've done one while learning Java, Python 3, and now I'm Rust. Well, I'm still working on the Rust implementation, but I'll eventually get there.
It gets easier over time. I barely even think about it now.
What's the point of deterministic password generation if you end up having to store and sync the metadata anyway? You need to store the site names so you don't end up trying to login with a different password on a `login.` or `www.` subdomain, etc. You also need to store a version/counter for each entry so that you can change your passwords. Why not just use unique entropy to generate each entry at that point? Then you wouldn't have to worry about a malicious site trying to guess your master password by making guesses against `hash(masterPass + fullName + siteName + counter)` and comparing the output to the password you sent them.
Do you consider "ò" and "ò" to be a single character? The first grapheme was generated by the sequence of two code points: [latin small leter o](http://www.fileformat.info/info/unicode/char/006f/index.htm) followed by a [combining character](https://en.wikipedia.org/wiki/Combining_character), the [combining grave accent](http://www.fileformat.info/info/unicode/char/0300/index.htm) The second grapheme is [latin small leter o with grave](http://www.fileformat.info/info/unicode/char/00f2/index.htm), a single code point. They look identical (and they should), but they are actually encoded differently. Paste it [here](https://r12a.github.io/apps/conversion/). Some escape codes (in HTML e Javascript strings respectively) Do you consider "o&amp;#x0300;" and "&amp;#x00F2;" to be a single character? Do you consider \"o\u0300\" and \"\u00F2\" to be a single character? (something interesting: Reddit processes HTML escapes, so `"o&amp;#x0300;" and "&amp;#x00F2;"` in a comment is actually rendered as "o&amp;#x0300;" and "&amp;#x00F2;" -- but if you see the "source" of my comment with RES, note that in the top of the comment I didn't use HTML escape codes, I pasted the graphemes themselves) I think that any application handling text should first consider if iterating over *grapheme clusters* wouldn't be a better idea, as pointed [in this comment](https://www.reddit.com/r/rust/comments/3t2pmy/how_do_i_get_the_last_two_characters_in_a_string/cx2n3w1). I mean, your application will probably behave strangely if it processes ò as two different characters! I was searching some article to read on this subject, but I didn't find anything straightforward. The closest was [this](https://developer.apple.com/library/mac/documentation/Cocoa/Conceptual/Strings/Articles/stringsClusters.html) from a Cocoa doc, and it even warns that sometimes units of text go beyond grapheme clusters (and that's why you need to use an Unicode library to sort text, or convert from lowercase to uppercase - don't roll your own!).
Merging it without Windows support would make Windows developers have a harder time.
Yep, if it needs to store any file (that can't be generated just with the master password), it's better to generate a random password for each site, then encrypt it with the master password, and store that instead. That way, you guarantee the entropy of the password for each site is unrelated to your master password, and also unrelated to other sites.
Nom is cool but, wow, that syntax is hard to read.
I have been refactoring [handlebars-iron](https://github.com/sunng87/handlebars-iron) to make it work for different sources of templates: https://github.com/sunng87/handlebars-iron/pull/19
Hmm. This is one of those things I'm glad doesn't work properly. Concatenating identifiers always sounds like a great idea; until people start using it extensively. Because when it's introduced, it usually is as a getter/setter pair, and you think "that sounds reasonable, and that sure does look like a lot of boilerplate writing those out every time." But then people start getting clever with it. They start building little embedded mini languages out of concatenated identifiers. Now you see `foo_with_bazzy_bar` in a backtrace somewhere, and you grep your codebase, that identifier doesn't appear within it at all. And `foo`, `bazzy` and `bar` occur in way too many places to be helpful tracking the problem down; you need to somehow know that the invocation of `frob!(baz, bar)` inside of a `foo!()` macro caused that to be generated. One of the great things about macro hygiene is that it makes it clear where identifiers are defined. You write out the identifier in the place that it is defined. It's relatively easy to track source location where something was defined, as the source location can travel along with the identifier. Once you start allowing concatenating identifiers, where do you track the source location of `foo_with_bazzy_bar` back to? It makes "Jump to Definition" much harder to implement reasonably. Do you jump to the place where one particular piece of the identifier came from? Do you jump the last of the big hairy macros that did the last two concatenations of identifier pieces? Do you track location on a sub-identifier level? And of course, even if you are able to set up the tooling properly and answer the above questions, none of that helps for just grepping through the source, which is a common thing for people coming in who don't necessarily have a full development environment with all of the bells and whistles set up, say, someone who's just a user of the package and just trying to track down some nasty bug that they found. I depend on enough third party software, in a variety of languages some of which I know well and some of which I have only passing familiarity with and no tooling set up for, that it really helps to make tracing through the codebase to figure out what's going on as easy as possible. Now, that's not to say that I'm wholly against the idea. I've done some pretty hairy macro abuses myself in the past. I have used `syntax-case` in Racket (née PLT Scheme) to define a custom dialect that included string interpolation, by adding a macro that expanded string literals into string concatenations along with a generic `(any-&gt;string ...)` wrapped around expressions, so you could write `"1 + 2 = $(+ 1 2)"`, and have syntax highlighting and source location tracking all actually work properly (thanks to their good support for building up syntax objects with proper hygiene and source location objects). However, in the end, we realized that expression interpolation like this, while a neat demo, wasn't actually all that helpful, and took that feature out. And I've come across several cases of metaprogramming gone too far, with abuses of CPP macros using `##` to build up identifiers that were then hard to track down, to uses of `method_missing?` in Ruby that give you all these weird little mini-languages that you can't interpret unless you learn each one individually. I've eventually come to sour on the idea of powerful metaprogramming like this. One of the major issues is that your native, host language is already compositional. Either building up identifiers in macros using things like `##` or `concat_idents!`, or parsing them out dynamically using `method_missing?`, means that you're inventing an entirely new, special case way of composing functionality, that probably doesn't have as well thought out a syntax, as good tooling, or is as general, as your existing, native methods of composition in the language, such as function application or method calls. Now, I do support a macro system as a good escape hatch; for those things for which the native syntax just doesn't quite let you get it right, for cases where there's just some really excessive repetition that can't be factored away in the native syntax. But from the perspective of someone who has spent a lot of my time recently trying to debug and understand a myriad of third-party codebases, I hope that we can go light on them, and this particular feature is one that I really wish to be used as sparingly as absolutely possible, since it breaks one of the most general purpose tools that I have for understanding a new code base, grepping for identifiers. Yikes, sorry for the rant. Don't mean to be too negative, just been burned a few too many times in the past.
Do you have any good references on the IRC spec, i tried writing a irc bot but i just couldn't get the signals right and it was pretty hard to debug.
EDIT: rewritten this post a bit to not sound very negative. If you try to use it for production, be prepared to search and fix any problems in this stack. Iron/Hyper at this point are still changing all the time, and I can anecdotally say that I have seen some strange issues in some cases (page not loading, http headers as response body). I was not satisfied guestimating, so I created a simple project (displays comics xkcd-style) to check it out. I tried to keep everything minimal. My current conclusion is that the Iron should not be used for anything bigger at this point. Things needed (at least what I noticed): - Better compile times or manual separation of independent code into multiple crates; - Better multipart/form support. As others also noted, you still need some kind of proxy in front (probably). I am using apache because that's what was installed of that server. Something like varnish might be better. Comics site: http://expertcrab.com (yes, not found page is all that you should see now). Code: https://github.com/Nercury/comics-rs/blob/master/src/main.rs 
Do people really encounter these kinds of problems? For example, the issue this PR fixes looks extremely contrived... unless you work with Greek-Latin keyboard maybe? Is Greek question mark even used in modern Greek? I know a common keyboard related mistake with Cyrillic-Latin keyboards is mixing Latin 'c' and Cyrillic 'с', but this kind of problem is not addressed by this PR.
Niiice. Thanks! :D
I agree that it would be, however, our preference is for developers experienced with Rust experience. It maybe that we need need to alter this thinking as we move forward. 
+1 I have been looking for an example like this too. I think in the end, I will just pass buffers directly to openssl, instead of letting it handle IO.
We would look at devs working remotely. If you're interested pls send in your CV to careers@maidsafe.net.
This is intended as a 'nice to have'. If someone applied who we felt had the right attitude..etc...and met the essential requirements we would hire them
I have a master degree and contributed on Rust but I live in Paris so I don't know if Mozilla is recruiting in here (seems really weird). Well, I try and I'll see what we'll happen I guess ?
The Greek question mark is a weird example; the hyphen stuff I've definitely run into copy-pasting code for other languages from mailing lists. Someone types their code someplace that converts a -- in their C code into an em dash; then the code doesn't compile.
That would probably work, but it feels really wrong. My goal is to check if the last two characters in my string are equal to a key in my HashMap, so I think an elegant solution would avoid creating a new string or changing an existing one.
Thanks! I need the last two characters in correct order (so "Hello" -&gt; "lo") but if I take(2) of the reverse string I get them in the wrong order ("ol"). Do I just reverse again or is there a more elegant way? Edit: I ended up doing last_two = last_two.pop().unwrap().to_string() + &amp;last_two; It works, but it looks really bad. 
The main problem is that the openssl crate does not think about async at all. If you look here you'll find a discussion of how it could work with some example code that might help you. That seems to be as far as anyone got. https://github.com/sfackler/rust-openssl/issues/249
It's weird because rustc devs have previously said they don't want lots of niche lints due to performance. Otherwise clippy has some that could be imported.
This lint in question is only invoked when the invalid character is found, so it has no extra cost for normal cases.
Do you have the chat application up on a hosted VCS? I'd be interested in taking a peak!
Nice, I'll play with this later today! &gt; I'm also sure I've made almost every mistake in the book as far as producing good libraries in Rust is concerned [You can easily prevent that :)](https://pascalhertleif.de/artikel/good-practices-for-writing-rust-libraries/) (I sent a PR to not use wildcard dependencies, btw.)
Cool! First critique, take `&amp;[u8]` instead of `&amp;Vec&lt;u8&gt;`. `Vec&lt;T&gt;` automatically derefs into `&amp;[T]`, so current functionality would remain the same but you'd also be able to pass `u8` slices. `&amp;Vec&lt;T&gt;` is such a common anti-pattern in Rust that [clippy has a lint for it](https://github.com/Manishearth/rust-clippy/wiki#ptr_arg).
From first quick check: - do not use `*` as package version matcher, this will be disallowed soon on Crates.io - make `image` optional dependency, this will make compilation faster for people who don't need to generate barcodes
The "no more lints" argument isn't so much about cost, as it is that they're extremely bikesheddy and there's no REALLY clear rule for what should be in and what should be out.
This might look a little better: let s = "hello"; let last_two_at = s.char_indices().rev().map(|(i, _)| i).nth(1).unwrap(); let last_two = &amp;s[last_two_at..]; println!("{:?}", last_two); Note that it will panic if `s.len() &lt; 2`. Oh, /u/mbrubeck's comment already had this! https://www.reddit.com/r/rust/comments/3t2pmy/how_do_i_get_the_last_two_characters_in_a_string/cx2s1l9
From the first example: let barcode = EAN13::new("750103131130".to_string()).unwrap(); let encoded: Vec&lt;u8&gt; = ean13.encode();` shouldn't that be `barcode.encode` on the second line?
I wonder how much slower this is. This is essentially the same thing openssl is doing right?
That's a reasonable use case. When I've written my own unit test system, I decided to just have test names be strings rather than identifiers to allow descriptive and readable names along with making it easy to compose names like this. I don't know how wedded Rust's unit test system is to free functions, but if it could be extend it to allow calling a function to return an iterator over pairs of strings and closures which would each be considered a test, then you could easily get this without a lot of macro magic.
No, "should this be a lint that comes with rustc" is bikesheddy, not the lint itself.
Well I guess for *semantic* lints it's easy to get into bikesheds, but syntax? ...or maybe better put "keyboard-related errors". GHC e.g. also computes edit distance of identifies and thus suggests possible corrections for possible typos when it can't find a definition. In any case: What's bad about bikesheds if people actually all do agree about the colour? The issue with bikesheds is fruitless discussion, not additional code.
Wayland. I'm feeling the pressure, apparently some people are actually using it and reporting issues ! :O
I think that this conversation is talking about two separate things. One is why this change was accepted upstream if other useful lints like the ones in clippy are not. The answer there would be that this isn't a lint; this is a hard error in the lexer, and all that's being done is improving the error message. I don't think there's a lot of disagreement about providing better error messages; better error messages are just better (you can get into debates if they start getting seriously verbose, but this is adding a single extra sentence of explanation). The other is about whether some lints should be accepted upstream, rather than staying in clippy. I think the issue is not just the bikesheddy nature of them; there are lots of design decisions that are bikesheddy, but those are dealt with by having teams that can make a final decision and consist of people that we generally trust to make a good one. The issue with lints is when they cause more harm than good. Lots of people like to make their code warning-free; so a lot of time if new lints are introduced, people will rush to fix their code. But I've seen lints that people "fix" with incorrect code. Even when not committing incorrect code, doing a big pass of fixing lints can add a burden; `git blame` in conjunction with good commit messages can be a very effective tool at figuring out "why is this code the way it is", but it becomes a lot harder to use when many of the lines that you try to look at turn out to be several layers of "lint fixup", "whitespace fix", etc. before you actually get back to the real meaningful commit. Churn like this can also cause merge conflicts, and the resolution of those has the possibility of introducing errors. The problem with lints is that due to their imprecise nature, they can sometimes highlight things that aren't actually problems, so they are not universally valuable, and there is a cost associated with them. Now, that's not to say that no new lints should ever make it in. But the bar should be reasonably high. Having `rust-clippy` as a place to collect lints that don't meet that bar, or which is a testing ground before allowing lints to migrate into the main compiler if they prove to be very useful and give very few spurious failures, seems like a reasonable compromise.
It's also similar to https://github.com/rust-lang/rust/issues/13677
I like this: fn suffix(s: &amp;str) -&gt; Option&lt;(char, char)&gt; { let suffix_len = 2; if s.len() &lt; suffix_len { return None } let mut iter = s.chars().skip(s.len() - suffix_len).take(suffix_len); Some((iter.next().unwrap(), iter.next().unwrap())) } 
If you're looking for more information on top of what /u/llogiq said, take a look here: * [RFC 1242, where it was introduced](https://github.com/rust-lang/rfcs/blob/master/text/1242-rust-lang-crates.md) * [This forum post, with post-introduction discussion](https://internals.rust-lang.org/t/psa-movement-of-rust-lang-crates/2671)
Added.
I'm working on a piece of software that uses irc, but is not a normal irc client for chatting. I'm using the irc library, which works fine. Creating another irc library is cool (many others have done so, if you check crates), but an irc server has not been done. The aforementioned irc library is implementing server stuff, but is nowhere near completion. Its unexplored, so why don't you try that instead?
I don't know how often the Greek question mark mistake happens, but I have seen people suggesting a find replace on someone else's code as a prank. So it's quite cool that rust can detect that. 
We used to have `--pretty=typed`, but it's been gone for a while.
I finished my Brainfuck... thing and I am slowly documenting it for a series of blog posts. Have to get it on paper before I forget how it works! On the more practical side, I'm working on my research project's software. I recently managed to split it from one giant crate into a spiderweb of smaller crates, so hopefully it compiler faster now (still need to benchmark that). Currently I'm fighting with the compiler to try and get my RUST_BACKTRACE line numbers back. After that I'll move on to improving the web interface which combines iron and websockets, and needs to have fewer deadlocks.
Thanks, that would have been ideal. Could you say if it was removed because of maintenance burden, or because there was a technical challenge? 
Cool, thanks for the info! And thanks /u/llogiq too :)
I have no idea, I only noticed last week, when I was triaging issues.
As others mentioned, this is not a lint, this is an additional note on an error message. Thus, there must first be an error parsing the code to get this note; unlike a lint.
&gt; Though probably better suited for clippy Clippy wouldn't be able to catch it because it would never get past the parsing stage.
Paris is one of the preferred locations for the Servo job, actually.
Yep, it's on github. But I think I did a crap job, and I might rewrite it. Here it is [[link](https://github.com/psyomn/architecture-notes/tree/master/languages/rust/chat-rs)]. I'm probably going to work on it more, and if it's not a complete disaster, maybe create it's own repo. I dump a lot of code there, where I'm trying out things (so sorry if the structure is confusing). (Also, not complete yet) Btw, `clap` was a delight to work with! I'm stalking its repo every now and then and checking to see if some issue seems possible for me to tackle!
&gt; Do you need the client or the server? Both. Even incomplete / non-functional code would be much appreciated.
Do you have javascript disabled or something? He mentions at the beginning some things might take a minute to load.
Nope, and the other images/graphs do show up, as they do in part 1.
Basically in part 0 I only see the memory diagrams. The graph that (I take it) should be in the section headed "What is Garbage Collection" doesn't appear. Getting reference errors (simple_gc_structure is not defined), and a couple of syntax errors with unexpected [: (index):188 Uncaught SyntaxError: Unexpected token [ (index):203 Uncaught SyntaxError: Unexpected token [ (index):238 Uncaught ReferenceError: simple_gc_structure is not defined(anonymous function) @ (index):238 (index):261 Uncaught ReferenceError: simple_gc_structure is not defined(anonymous function) @ (index):261 (index):963 Uncaught SyntaxError: Unexpected token [ (index):261 Uncaught ReferenceError: simple_gc_structure is not defined(anonymous function) @ (index):261 (index):963 Uncaught SyntaxError: Unexpected token [
I share similar feelings. The ideas being discussed sound interesting, but there is no documentation to look at. 
look at that! Okay, I see the same problem here.
It looks like it is still there in 1.4 using `--unpretty=hir,typed` (unstable pretty printer?): rustc -Z unstable-options --unpretty=hir,typed src/main.rs 
This also works! I added it like this: [DllImport("External/prime.dll", EntryPoint = "is_prime")] [return: MarshalAs(UnmanagedType.I1)] private static extern bool IsPrime(uint n);
Yep! I'll fix it.
I believe `asm!` can take parameters which get copied into certain registers for the assembly block. I don't know the exact syntax, though. If you're already doing that for `$offset`, why not do it for `PAGE` too?
Because Rust is not C, it is its own language, with a non-terrible module system, not based on textual substitution, like C's header files. It is kind of like asking why doesnt Python just let you do a `#include` just because some people want to use some C code. Instead, you would need to use `ctypes` or `cffi`. Its the same thing in Rust, where you would use `bindgen` (already linked in this thread).
I guess it was one of these [examples](https://github.com/tailhook/rotor-http/tree/master/examples).
Jules' article was indeed great, and Paul Masurel's own work on the same subject is definitely worth a read http://fulmicoton.com/posts/levenshtein/
If there are any questions or you need help to get involved in Piston, post a comment!
[Part 1](https://medium.com/@paulcolomiets/asynchronous-io-in-rust-36b623e7b965) has a decent explanation of the library and links to some examples in the library repo. Part II is more of a project update. Though it could use more explanation, its hard to follow what's changed vs what's planned as a change vs what a random musing of the author is. Particularly, the removal of the Scope types is a pretty big change that needed more explanation.
Yes, I've seen them. But first: which one exactly, second: these are examples of `rotor-http`, not plain `rotor`.
Yes, the (there undocumented) assumption behind the statement "Gc&lt;T&gt; is Copy` is that one is doing some form of stack scanning that is integrated with LLVM. (It goes a bit further than that, really. I may get into some of the difficulties here in a later post.)
It would be nice to see a description of problems that are much more amenable with a garbage collector.
https://www.reddit.com/r/rust/comments/3j4bx2/designing_a_gc_in_rust/cun2l7a https://gist.github.com/pnkfelix/62f0d514e6f883382da2#why-support-gc https://news.ycombinator.com/item?id=10149599 http://manishearth.github.io/blog/2015/09/01/designing-a-gc-in-rust/, motivation section Also there was a pretty thorough motivation for this in one of pnkfelix's gists
Yup, I saw that article too. Also very good. For whatever reason though, I found that Jules' formulation just clicked. I also had to replicate this: &gt; But this DFA works on unicode characters, while their dictionary structure is encoded in UTF-8. Rather than converting UTF-8 on the fly, they prefer to convert the DFA itself to UTF-8, which I found pretty nifty? Which nobody has really written much about to my knowledge. Perhaps that will be the topic of my next blog post!
Yeah, Racket (called PLT Scheme back when I was using it) has one of the best macro systems I've used. And I'm not exactly against implementing this; a good, powerful macro system can be used well. I just feel a sense of reservation about this, since generated identifiers is on of the features of macro systems I feel is easiest to misuse. One of the biggest reasons for using generated identifiers in a language like Scheme or Racket is that the convention for new types of object is to have a `make-&lt;type&gt;` constructor, a `&lt;type&gt;?` predicate, and getters and setters like `&lt;type&gt;-&lt;member&gt;` and `set-&lt;type&gt;-&lt;member&gt;!`. That's a lot of boilerplate, along with the underlying struct definition, so automating that by appending names together can be handy. However, in Rust we don't have those. Constructors are just `T::new` by convention, but could be called anything. Type predicates are not something you use often, as you usually either know the type of an object or are just using it through a generic parameter or trait object, though you can use `Any` if you really want to, which then works as `object.is&lt;T&gt;`. You usually just access fields on structs directly, or destructure enums and tuple structs, rather than using getters and setters, so while there are cases where you need getters and setters, they happen less often. So, a lot of the motivation for good reasons to build up identifiers like this don't exist in Rust, in a way they do in Racket. And I just [checked over how I handled "jump to definition" in the system I had worked on](https://github.com/lambda/halyard/blob/master/runtime/collects/halyard/private/tags.ss) (it's been a while since I touched this), and actually, I had had to do an entirely separate syntax expansion pass that just collected up every kind of definition, and added that source location to a database along with all of the identifiers it defined. For macros that were defined in external packages, I had to approximate their behavior. This approach wouldn't work very well for a general purpose IDE, but it worked fine for our custom editor for our custom dialect.
Compile all the inline static functions while you're at it
You're using es6 destructuring assignments, e.g. `var [rf, d] = simple_gc_structure();`. These [aren't supported](https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment#Browser_compatibility) outside of Gecko (and possibly Safari).
The error means exactly what it says: `read_line` wants a string (as specified in the documentation), and you've given it an integer. You can't do that. Here is code that works: use std::io; fn main() { // `Vec`s are dynamically growable; there's no *need* to pre-initialise // them. If you're *really* worried, you can use `Vec::with_capacity(2)`. // Also, you should usually try to avoid giving things the same name as // other constructs (like naming a `Vec` variable `vec` which is both // confusing and semantically meaningless (a vec of *what*?)). let mut nums = vec![]; let mut buffer = String::new(); // `1..2` is a sequence that consists entirely of `1`. // See &lt;http://doc.rust-lang.org/nightly/book/loops.html#for&gt;. // Also, we don't need `x`, so don't bother binding it to a name. for _ in 0..2 { // `read_line` requires a mutable pointer to a `String` to read its // contents into. This is spelled out by its documentation: // &lt;http://doc.rust-lang.org/std/io/struct.Stdin.html#method.read_line&gt;. io::stdin().read_line(&amp;mut buffer).expect("Failed to read"); // You can't store a string in storage for an integer... or any *other* // type, for that matter. Rust is not dynamically or loosely typed. // To convert a string into some other type, you can use `.parse()`. let num: i32 = buffer.parse().expect("Invalid number"); // Push the number on to the end of the vec. nums.push(num); // As noted in the documentation (which you should always read), // `read_line` doesn't clear the buffer, so we have to do it ourselves. buffer.clear(); } // Display the numbers. println!("nums: {:?}", nums); } Also, see [The Rust Book: Vectors (Accessing Elements)](http://doc.rust-lang.org/nightly/book/vectors.html#accessing-elements).
Since Rust has static typing, you have to consider that vec contains Integers and not Strings. read_line reads to a String, meaning that you can't expect it to read to an Integer type. What you should do is create a separate buffer variable and parse it as an integer like so: use std::io; fn main() { let mut vec = vec![0, 0]; for x in 0..2 { let mut buf = String::new(); io::stdin().read_line(&amp;mut buf) .ok() .expect("Failed to read"); vec[x] = buf.trim().parse() .ok() .expect("Integers only!"); } for num in vec { println!("num = {}", num); } } Vectors do indeed start from 0, and the "x..y" range syntax is non-inclusive of the end value. So 0..1 will loop just 0, 0..2 will loop 0 and 1, 0..x goes from 0 to (x-1). Although in general if possible you should avoid using the range syntax when you have better options available. See this: use std::io; fn main() { let mut vec = vec![0, 0]; for pos in vec.iter_mut() { let mut buf = String::new(); io::stdin().read_line(&amp;mut buf) .ok() .expect("Failed to read"); *pos = buf.trim().parse() .ok() .expect("Integers only!"); } for num in vec { println!("num = {}", num); } } Now you can fill the vector regardless of how many spaces you allocated beforehand. Ideally, though, you'd use the vector push function and Vec::new() so you can read in infinite numbers, checking for an empty (or maybe non-integer) input to decide when to terminate. Depends on the application, of course.
&gt;However this is generally not what you want as it'll consume the vector Do you have a source on this? As far as I'm aware the "for line in vector" syntax is just a shortcut for .iter()
 for line in vector will consume, as it calls `into_iter()` but for line in &amp;vector will call `iter()`. Sometimes, `vector` is itself a reference, and so then it may look more like the first even if the output is the second.
I also created the `r-result` ~~crate~~ package on npm if you want to use the Result type in JavaScript.
It's hard to diagnose the error without seeing your code. I'd assume there was an issue with one of your println! type statements. Macros in Rust can give problematic error messages at times, you'll understand why as you get better at Rust. While most other errors are highly informative this seems to be one of the exceptions. What I might suggest is counting the number of {}'s in your println! string and seeing if it matches up with the number of arguments following it.
I read this article a little while ago: http://benchling.engineering/crispr-aws-lambda/ It's about calling C++ from node-gyp, but I'm wondering if you could use the same tool for calling directly into Rust? Alternatively I think you could create a Python (ctypes or cffi) module or a Java (JNI) module that calls directly into Rust, right? I'm not sure how that would play out with AWS Lambda though.
&gt; Thinking of prime_pi(m) - prime_pi(n)? Yeah, that should work, although you may want `prime_pi(m) - prime_pi(n - 1)` depending if you want to exclude counting `n` (when it is prime) or not.
Just wanted to say how awesome piston is! I've been playing around with Piston2d and it's really easy to use and setup. I've also ported some old raw opengl code to glium and was impressed how easy that was. No more countless hours debugging opengl calls, it either just works or gives a compile time error. 
What is the difference between piston, gfx, and glium?
Very exciting! Love seeing people explore how best to apply Rust's idioms to particular tasks. :)
Very happy to see stb_truetype being ported to Rust! I'd love to see all the stb libraries (https://github.com/nothings/stb) ported eventually (hint hint for anyone looking for a project to get their hands dirty with Rust).
 &gt; I'm not sure what you mean by root module From what I read in the rust book, it seemed like to have a *mod.rs* file, you need to have a folder named after the module which the *mod.rs* file represents. But you cleared that up for me now, thank you.
There are a ton of repos under the PistonDevelopers organization: https://github.com/PistonDevelopers/ and they seem quite happy to hand out membership to anyone who takes an interest in any of the projects there. For a start, you could just try using Piston and see what sharp edges you run up against (it may not take long :) ) and work on improving whatever you see. If you're looking for something more directed, any of the projects that the OP mentions are going to have people working on them, and you should be able to ask about them in the various communities on /r/rust_gamedev and the #rust-gamedev IRC channel at irc.mozilla.org (https://chat.mibbit.com/?server=irc.mozilla.org%3A%2B6697&amp;channel=%23rust-gamedev). For even more direction, go work on long_void's Rust port of stb_truetype (https://github.com/PistonDevelopers/truetype). :) And as I mention in a different comment in this thread, there are a whole lot of single-purpose (and single-file) stb libraries that could use pure-Rust implementations: https://github.com/nothings/stb (Paging /u/i_r_witty as well.) EDIT: Oh, and since you mentioned C# experience, I'm sure the Rust plugin for Visual Studio could use some expertise: https://github.com/PistonDevelopers/VisualRust
I've a question. Suppose that you have an application that uses Conrod and Glium. Will they use different OpenGL contexts? Is there a performance impact? About font rendering: is there a Rust library that implements Valve's signed distance field algorithm? (or variations of it).
&gt; Then you wouldn't have to worry about a malicious site trying to guess your master password by making guesses against I think you don't have to worry about that if you hash function uses HMAC or something similar, but I don't know if this password manager uses HMAC.
[I did](https://github.com/serde-rs/serde/issues/183). The issue is that it's not clear how phantom data should be handled; their use case with lifetimes and unsafe is quite different to their use case as just a means to add extra type safety via phantom types, so it might not be okay to skip them in all cases.
I see. It's not a problem in Haskell, because Haskell has no subtyping hence no variance. Rust can't remove subtyping, since it's used for lifetime. Complex tradeoffs indeed.
I can recommend https://github.com/txus/kleisli for many implementations of monads in Ruby.
I'd love to see something like this in wide use. From the readme: &gt; There are plans to make rusty-cheddar cargo aware and possibly even a full rustc replacement, but there are ongoing issues with these. Can you elaborate?
What sort of research project?
In that case you want your session to take ownership of the name, so use `String` and copy out the data.
What about the calling convention used? I don't know much about FFI, but maybe this is relevant :P
Oh ? Well, I just have to hope to get a chance to have an interview then ! :)
Changes that are going to be mentioned in changelogs are usually marked with `relnotes` label. https://github.com/rust-lang/rust/labels/relnotes
A page that redirected to the blog.
It changes how drop works, I think. Also, when you introduce reference types, `&amp;T` and `&amp;mut T` differ in variance, i.e. if the lifetime can be coerced to another.
The [Rust Release Explorer](http://ashleygwilliams.github.io/rust-release-explorer/) also scrapes the Rust issues by release.
ah, excellent; will fix pronto.
&gt; Also, when you introduce reference types, &amp;T and &amp;mut T differ in variance, i.e. if the lifetime can be coerced to another. Can you elaborate on that ? You mean that: struct Foo&lt;'a&gt; { _f: PhantomData&lt;&amp;'a ()&gt; } and struct Foo&lt;'a&gt; { _f: PhantomData&lt;&amp;'a mut ()&gt; } will behave differently regarding coercions, even if the type is still `Foo&lt;'a&gt;` ?
Hey those are pretty nifty, thanks for the link. I'll try my hand at porting some of them.
Where can I get started with getting involved with Conrod?
I'm guessing here, but I'm thinking that the project is currently using the GNU ABI on Windows version of Rust, which has some limitations for mmap. 
So this is definitely an improvement.
Sorry for the late reply! &gt;A better question would probably be: How common is it to know ahead of time that a buffer you're allocating is never going to be used for DMA? If you do hard-drive DMA, it can do DMA on any 64MB-aligned address, and there's really no reason not to take advantage of this. Good point. Maybe a seperate pool for contiguous pages would work better. If you just need a single frame, use the frame stack. Else use the contiguous pool. But then another allocator (e.g. a buddy allocator) would be needed for this contiguous pool… And if we have a buddy allocator already, why not use it to manage the whole memory… So you're probably right :) I think the problem is DMA for blocks larger than 4096 bytes. For other DMA operations, we just need to lock a page in memory. The question is how often these “big DMAs” are needed. Do you have any experience on this? &gt; any allocation require modifying every copy of the kernel's memory map (And every process gets it's own page-table) If we use the same P2 or P3 table for the kernel memory in every page table, this shouldn't cause big problems (besides invalidating the TLB for all cores). Or do I miss something? &gt; anything variable-sized is going to be a headache in kernel code I'm not quite sure. As soon as we have a working heap, variable sized data structures shouldn't be a problem. &gt; I'd be wary about a 'global-paging lock'. Yeah, you're absolutely right. I will use a lock per page table instead. &gt; The code itself is still circular - If you call allocate_frame(), it needs to read the first entry in the free-list (Because it wants to use that free-page for the allocation). Taking that entry off the free-list requires a call to the mapping code to map that entry so it can be read I don't need to call the mapping code on each `allocate_frame()` because I use a vector-like data structure to store the frames (instead of the frames itself). So to allocate a frame, we just need to pop a value off the stack. And to deallocate a frame, we do a simple push. There are 2 exceptions: 1) If the stack is full (capacity == length) and a deallocate occurs, we don't push the freed frame (we can't). Instead, we use it to increase the stack's capacity. This could require up to 3 `allocate_frame` calls if the corresponding P1, P2, and P3 tables don't exist yet. So we need to make sure, that there are at least 3 frames in the stack left. We can ensure this by setting the minimum capacity to 1 page (it can store 512 frames). So there are at least 512 free frames left and the 3 allocate calls are simple pops. 2) If the stack is empty (length == 0) and an allocate occurs, we decrease the capacity if possible (mimium capacity is 1 page). Thus one frame is freed, that we can return. However, freeing one frame can cause empty P1, P2, and P3 tables, which are then freed, too. So it's possible that `deallocate_frame` gets called up to 3 times. But since the stack was empty before (else we wouldn't have decreased the capacity) and has a minimum capacity of 512, these calls are simple pushs. What do you think? &gt; while it may feel that I'm shooting your ideas down Don't worry, it doesn't :) I really appreciate your feedback. It's always difficult to see problems that appear in the long term.
For variance, `fn(T) -&gt; T` should mean the type is invariant in `T`, while `T` and `*const T` yield covariance. For [dropck implications](https://github.com/rust-lang/rfcs/blob/master/text/0769-sound-generic-drop.md#phantom-data) I don't know for sure, does it treat the function case differently than the `PhantomData&lt;T&gt;` case, I think it should! However, since there is no unsafe code involved, the dropck rules around PhantomData shouldn't matter in this struct — Rust must be sound anyway.
Ah, sorry, right.
It's a portable device with various sensors, smartphone-controlled. Rust is used for everything from data collection to web interface.
So.. unsoundness?
Yes.
We've wanted to get better at having change logs in advance, but there just aren't enough hours in the day. I bet if someone wanted to try to pitch in on this, a lot of people would appreciate it!
A better solution is to provide an API to synthesize tests, and avoid macros altogether. As I suggested on the [internals message board thread](https://internals.rust-lang.org/t/the-future-of-syntax-extensions-and-macros/2889/5?u=briansmith), it would be easy to add a `run_test("test_name", || { test body })` that had the same effect as `#[test] test_name() { test body }`, and then we wouldn't need macros for tests or benchmarks. And, we need to have such facilities anyway, for dealing with tests that are data-driven (which, IMO, most tests should be).
I might be mistaken or misremembering, but historically unbound lifetimes have been a problem because *regardless* of the variance, it gives the constraint solver carte-blanche. In particular, if you have an unbound parameter `'a`, and then pass it to some interface that requires it to be equal to `'b`, it sees a variable with a single constraint, `= 'b`, and correctly solves the system by just setting `'a = 'b`. HashMap's iterator was briefly busted for this reason, and it would have been caught by unused_parameters.
interesting. How'd you run into this?
In the dropck RFC it appears `fn(T) -&gt; T` is skipped (does not count as owning `T`), see [this section](https://github.com/rust-lang/rfcs/blob/master/text/0769-sound-generic-drop.md#when-does-one-type-own-another). It shouldn't matter with no unsafe code, right, so unsafe code just has to be more precise. Edit: What scares me though is when we add more implications to PhantomData, they become more important to do right (and have more rules for doing so?). First I learned only about variance implications, then about dropck, but will there be more? :-)
Ok, gotcha. Thanks for the help!
Not sure that there is a nice way. I probably did it in a similar way ([lookup table](https://github.com/nwin/rauta/blob/master/src/server.rs#L23) and [RwLock](https://github.com/nwin/rauta/blob/master/src/client.rs#L66)). I don't see how you could avoid it. The alternative would be to keep copies of the user information everywhere and propagate every change via a channel.
Oh, I guess I meant the notes, not issues. Yeah, it looks tricky to keep up with.
OMG this is awesome! In case anyone else is wondering like I was, this is absolutely allowed by the AWS terms of service, running binaries using Node is in the Lambda FAQ: https://aws.amazon.com/lambda/faqs/ 
Can someone explain how the impl&lt;T: Magic&gt; Magic for T { ... } block works at all? IIRC that is *supposed* to work, it's some further downstream thing which is causing the unsoundness. I have been reading constraints as something like "if type `Ty` implements trait `Tr`, then ..." which here would suggest the uninformative "if T implements Magic, then you can implement Magic for T thus".
[Here](https://www.reddit.com/r/rust/comments/3tco32/shattering_a_crate_in_pursuit_of_compile_times/)'s some data about compile times.
I wrote a wiki page for those new to Github: https://github.com/PistonDevelopers/piston/wiki/Using-Github-and-Git Open up an issue [here](https://github.com/pistondevelopers/piston/issues) for things you want to discuss!
Look at [issues](https://github.com/pistondevelopers/conrod/issues), poke around in the code and test it! Try to get a picture of what is happening and ask people on #rust-gamedev when you get stuck. You can also ask on [/r/rust_gamedev](https://www.reddit.com/r/rust_gamedev/).
This is kind of similar to what I have but then User must be immutable, right?
Speaking of which, do you know of anyone who is publicly tracking Rust ports of high-profile libraries from other languages? I think it might be a good way to expose low-hanging fruit to people looking to break in to non-trivial Rust projects (like me), while also providing some assurance that they aren't needlessly duplicating effort.
It can work for pure marker traits as well, which makes for a more minimal example case. (I.e. without the question of where that Deref implementation comes from). Look out, mutable aliases! trait Magic: Copy {} impl&lt;T: Magic&gt; Magic for T {} fn wand&lt;T: Magic&gt;(x: T) -&gt; (T,T) { (x,x) } fn main() { let mut a = 0; let (m1, m2) = wand(&amp;mut a); } EDIT: like steveklabnik1 I missed the link to the issue, which includes basically the same example.
Conrod will use Glium for rendering through the glium_graphics backend. The 2D graphics API is generic, so you need an extra library to use it with a lower level API. Using Glium directly gives you more control over performance, so we design the libraries such that you can optimize where it matters. This is a common pattern for Piston, that you use a set of libraries for a specific use case, such as UI and AI, and then combine it with custom/optimized code designed for a project. I guess you meant [this paper](http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf)? No one has implemented it yet as far as I know.
Thanks for the help! See discussion [here](https://github.com/PistonDevelopers/hematite/pull/210#issuecomment-144814447).
Right, so but my question is, *why* is `Magic` implemented for this type? What's the algorithm that concludes that it is?
Here is a page explaining how to contribute: https://github.com/PistonDevelopers/piston/blob/master/CONTRIBUTING.md Sorry for all the comments!
I don't know of any such list, sorry. The nearest thing might be to take a look at the curated list of Rust libs at awesome-rust (https://github.com/kud1ing/awesome-rust) and see which of those are currently just wrappers over libraries in other languages. If people are finding the wrappers useful, then there's a good chance that they'll find pure-Rust replacements useful as well (possibly even if those replacements are less mature).
Great data! These are the sort of reductions that I hope the incremental compilation effort will automatically provide, though there will still be work to do in reducing the persistent speedbump in LLVM (fingers crossed that MIR will help with this). Are the frame rate numbers the average of multiple trials? The discrepancies in the performance of the non-optimized builds don't particularly make sense to me. I'm also not certain why crate-splitting has an effect on the binary size of optimized builds.
I did not average any trials (yet); the runtime benchmark is entirely unscientific. I agree that the differences among the debug builds in that row are likely not significant. Also, 30 FPS is the maximum. So the message there is basically that debug builds are CPU-bound for this task and release builds are not. I should come up with a better runtime benchmark. I was also uncertain what happened with the binary sizes (though again I don't care). I would have expected the code size to go down, because I lose a bunch of inlining opportunities by shattering. I thought maybe if there is less inlining, then there is more debuginfo because more individual functions, but the effect is still present in the debugless builds, so I'm stumped. Maybe there are multiple copies of dependencies and something fails to get deduplicated?
I work on a compiler plugin allowing to do something very similar to what you want. Here is the [git repository](http://git.tuxfamily.org/tql/tql.git). With this compiler plugin, you can do things like: let results = sql!(Person.all()); or even more complex queries like: sql!(Table.filter((field1 == "value2" || field2 &lt; 100) &amp;&amp; field1 == "value1")) sql!(Table.filter(date.year() == 2015 &amp;&amp; date.month() == 10 &amp;&amp; date.day() == 26 &amp;&amp; date.hour() == 1 &amp;&amp; date.minute() == 39 &amp;&amp; date.second() &gt; 0)) You can see more in the [tests directory](http://git.tuxfamily.org/tql/tql.git/tree/tql_macros/tests). This is converted to an SQL string at compile-time with type checking. **This is alpha quality. This has not been used in any project. Use it at your own risk. There are bugs and things to do.** As for now, only PostgreSQL is supported. I will add examples in the following days and documentation in the following weeks. The syntax is similar to the one used in the Django ORM (except you don't have to specify `.objects`, I use `join()` instead of `select_related()` and there are other minor differences).
I have started to implement it, but in the end I gave up because it isn't worth it for font rendering (except maybe for 3D). Generating the distance fields just takes to long. You could do it upfront, but then you need to come up with some kind of image format. And why would you do that when FreeType is fast and memory is cheap?
I'd say it isn't strictly necessary. For me, this interest only arose because so many projects on github advertise that they're only building with the nightly version. This created the interest to find out what they actually require from these upcoming versions. I recognize that it's the plan to change this bleeding-edge behavior over time, isn't it? Reference: http://blog.rust-lang.org/2014/10/30/Stability.html
Well, new features always land as unstable, so in that sense, no, nightly-only will never go away. But as more things become stable, more programs should be able to build without needing the absolute latest things.
What problems specifically?
[removed]
But that requires syscalls which aren't safe! ;-) **Edit**: Not to mention that the playground doesn't know file I/O.
I ran into this and another issue when I accidentally 'implied' DerefMut on all references that a function I used, which lead to an ICE complaining about the `deref_mut` function not being implemented on the type. Eddyb then reduced my code down to the code reported in the issue. I had then stripped out the associated types down to just the trait and eventually decided to create a hexdump for fun. :-)
For move semantics, String addition could be useful e.g. `s = "Hi, ".to_owned(); g = s + " Rustacean!"; // s moved into g`
The eta reduction would have to be `char::to_ascii_uppercase`, and it doesn't work anyway because the signature isn't right (`to_ascii_uppercase` takes `&amp;self`).
In your article, you mention it was too hard to cross compile to Amazon linux from OSX. Have you tried just compiling inside a Debian docker container?
Hi, I'm the author of rustorm, and codegenta I've just added this example for your usecase: https://github.com/ivanceras/codegenta/blob/master/examples/test_em_product.rs /// run using cargo run --release --example test_em_product fn main(){ let pool = ManagedPool::init("postgres://postgres:p0stgr3s@localhost/bazaar_v6",1).unwrap(); let db = pool.connect().unwrap(); let em = EntityManager::new(db.as_ref()); let products = em.get_all::&lt;Product&gt;(); println!("products: {:#?}", products); }
I'm very curious if that would be faster, but it would definitely be more complex and brittle. A complete example, though probably out of date, is this repo from [alexcrichton](https://github.com/alexcrichton/rust-ffi-examples/tree/master/node-to-rust). It avoids calling node-gyp directly, and instead uses a helper called ffi.
Yes that's what I first tried (with Ubuntu Server). There were GLIBC incompatibilities, spinning an EC2 instance was faster.
I see. I had been under the impression that nightly progresses to stable in a few weeks except for things that are thrown out. Thanks for the heads up. Would you still recommend the stable compiler for most users?
Can't a match guard do what you ask much more explicitly without adding much bulk? match word { [] | [_] =&gt; true, [first, middle.., last] if first == last =&gt; is_palindrome(middle), _ =&gt; false, } I'm still a novice in rust, maybe this doesn't work for some reason.
Yes this works, but it's more verbose than the `same` variant and it's certainly debatable, if it's "better" or "worse" than my first solution...
True. I often like to ignore the complexity of the real world :P It was merely an example anyway and I wouldn't use that in production. Also: there is already a rather nice solution without recursion (and with some adjustments it will work for grapheme thingies, too). word.chars() .zip(word.chars().rev()) .take(word.len()/2) .all(|(a, b)| a == b) The `take()` part is optional of course.
I had a similar problem in implementing domain objects. I ended up with implementing a Wrapper struct that contains a RefCell/ Cell(for Copy) and wrapped every mutable field of the original struct. This way the actual struct (User in your case) can be immutable and passed around.
It depends on what you're doing. On principle I have to suggest stable, but honestly I always use nightly cuz I know it has the best version of everything. :P
Is rust your first language? Who is the target audience of the book?
&gt; Good point. Maybe a seperate pool for contiguous pages would work better. If you just need a single frame, use the frame stack. Else use the contiguous pool. But then another allocator (e.g. a buddy allocator) would be needed for this contiguous pool… And if we have a buddy allocator already, why not use it to manage the whole memory… So you're probably right :) &gt; &gt; I think the problem is DMA for blocks larger than 4096 bytes. For other DMA operations, we just need to lock a page in memory. The question is how often these “big DMAs” are needed. Do you have any experience on this? This occurred to me a little while ago, but you're actually going to need *another* allocator - You're mapping code is essentially going to be an allocator, where it allocates virtual addresses to be used for pages, so I don't think you can avoid writing another allocator if you're going to be mapping the virtual addresses for the kernel. You're right on the DMA point - I was thinking DMA larger then a page, but honestly from what I'm doing that type of DMA seems somewhat unlikely. &gt; If we use the same P2 or P3 table for the kernel memory in every page table, this shouldn't cause big problems (besides invalidating the TLB for all cores). Or do I miss something? Yes you're right, that's a pretty good plan actually. &gt; I'm not quite sure. As soon as we have a working heap, variable sized data structures shouldn't be a problem. Well I should clarify - Variable-sized data structures aren't a huge problem with a working allocator, but variable-sized data structures in the *allocator* are a bit of a problem. &gt; Yeah, you're absolutely right. I will use a lock per page table instead. I'd personally just put a lock over the kernel page table - Every modification first has to take the lock. Per pable-table locks could be cool (You could then do some concurrent page-table modification actually), but you'd have to allocate them separately from the page-table pages, so it would be a fair bit more work. &gt; What do you think? I think you need to as yourself what type of address `allocate_frame()` returns, and `deallocate_frame()` takes in: Virtual or physical. This detail is important to recognize in your description, because you seem to be going back and forth. The big issue is with the P1, P2, and P3 tables - If `allocate_frame()` just returns a virtual address without calling the mapping code, then you don't actually need the mapping code at all (Because your page already have virtual addresses that you assigned). But, if `allocate_frame()` returns a virtual address and calls the mapping code to assign it, then you can't use it to allocate the P1, P2 and P3 tables - That allocation will never work because a virtual address is not assignable at that point. If `allocate_frame()` returns a physical addresses instead, then you can't modify that page and so you still can't use it for a P1, P2, and P3 without mapping it somewhere. IMO, the best way to avoid the problem is to make sure your mapping code never calls your general allocator - Have a separate allocator with already-mapped pages ready to go to be used as page-tables. But, then you're back where you were before with needing a separate allocator. Also notable, the pages in your frame stack need virtual addresses too, and since they're not contiguous you need to put them in a linked-list type structure. That's not really extremely hard, but it is something you need to do. 
It [does allow file I/O](http://is.gd/cxewTf), but it doesn't allow access to /dev/mem.
I'm just coming to Rust from an enterprise Java background. I'm reading Rust documents concurrently, and I really do appreciate the book. I've also found the Awesome-rust repo on Github, which is nice. One of the things I found most useful were the practical sections wherein a small program is written and thoroughly described. The dining philosophers solution is one such example. The language reference in the rest of the book is good, albeit dense. Do you have any plans to write more example programs demonstrating Rust concepts? Something like a Rust cookbook or a multi-part tutorial detailing the construction of a larger project would be great, and a book I would pay for. The packt book, Rust Essentials, is kind of lacking as it doesn't add much more value than the official book. 
Thanks. See my comment up thread, I think you'll like the changes.
I think I've got a pretty good handle on them/when the compiler wants them, but I don't quite understand why the compiler can't just figure it out in some cases. Like: struct A&lt;'a&gt; { x: &amp;'a i32, } Why can't the compiler just figure it out? For any reference inside a struct, couldn't it easily be assumed that it must live at least as long as the struct? For the simple case above, the best explanation I received was that specifying the lifetime marks the struct as containing a reference, which might be helpful to know when writing other structs that use it. Basically, I don't have a good handle on the kinds of situations that would absolutely *require* specific lifetimes.
Basically, because this is the simplest possible case, and anything else falls apart. So rather than add a special case rule for the simplest possible case, no rules is easier. (and the other point remains true)
But keep in mind that this comes with overhead, especially when in a multithreaded environment (Arc&lt;Mutes&lt;_&gt;&gt;)
Ah. Looks like a trivial bug though.
I think it's totally reasonable for Rust to use ld.gold by default, the legwork just hasn't been done yet. I also don't think it's appropriate to make a blanket statement of "never use trait objects" for purposes like this. In terms of an API exposed it may be a good recommendation but in terms of an implementation detail there's no reason to *not* use trait objects for your internal function arguments and such.
You're welcome, thanks for the thanks :)
So I was thinking about the summations and I realized that would defeat part of my purpose here which is to simulate incremental compilation. e.g. for "incremental (bluefox)" it needs to compile the bluefox crate and the main crate and link them, but not compile any of the other 9 crates. But I suppose I should therefore sum the timings for bluefox+main. I also want to compile all the crates individually (except the main one) and compare the timings to see who is heaviest. I'm playing with `codegen-units` now (and seeing no effect in the sharded condition, unfortunately). Linking is next.
(Comments in more detail on the RFC) I'm pretty strongly opposed to this: the equality check is potentially expensive, and is very subtle. It's also not the most obvious thing to me as a reader. The match guard version isn't much longer, and it's a *lot* more explicit. It's also worth pointing out that this already works fine if `same` is a constant. If it's a variable, more code needs to be run, and the match guard makes that clearer imo.
As a current undergraduate student, I found the lifetimes section [here](https://www.ralfj.de/projects/rust-101/part06.html) to be the most helpful out of all documents I've read. Though I still had to read it a few times to fully understand it. When he says "means that &amp;v also has to borrow v for the entire duration of the function", the function he is referring to is rust_foo, not head. 
&gt; -C link-args=fuse-ld=gold I had to do `cargo rustc -- -C link-args=-fuse-ld=gold` - notice the `-` before 'fuse'. Gave a 18% improvement on a small / medium crate. Not bad!
`cargo rustc -- &lt;args&gt;` passes the arguments to the main crate only. Setting `$RUSTC` to a shim script changes the invocation for all dependencies as well. I'm not sure it actually matters for the linker, though.
I don't quite understand what this does: [] | [_] =&gt; true, Could someone explain this to me? Thanks :)
So, to reiterate, my question is, given the declaration trait Magic: { } // no supertrait! impl&lt;T: Magic&gt; Magic for T { } how do specific `T`s end up being `Magic`. Perhaps this is stunningly obvious to everyone, so that everyone thinks I'm asking a different question from the one I am, but to me the whole thing looks somewhat tricky, since I read the above as stating that it's a constraint on `T` that it be `Magic` for it to be `Magic`, so there, too, I would think that any given `T`'s being `Magic` would require assuming what is to be proven. (Dinking around with some Haskell examples led me to this SO answer: http://stackoverflow.com/a/7201464 &gt; What you want to express is "Whenever a type is an instance of Eq and EuclideanDomain, use this rule to make an instance for Ord." But this is inexpressible in Haskell. The line instance (EuclideanDomain a, Eq a) =&gt; Ord a where &gt; actually means, "Use this rule to make an Ord instance for any type. It's an error if instances of EuclideanDomain and Eq aren't in scope". which suggests my reading was incorrect. But it also suggests that the above example should still not work, since at the time of `Magic` instance creation there isn't *already* a `Magic` instance in scope, unless, of course, the about-to-be-created instance counts. Which would answer my question, but then I won't understand what `impl&lt;T: Magic&gt; Magic for T` gets you that `impl&lt;T&gt; Magic for T` doesn't.)
When you're matching on a slice, Rust has no idea how many elements are eventually going to be contained within that slice. And because matches must cover all cases, Rust will yell at you if you don't account for all possible number of elements. If you just had a pattern like this: match word { [first, middle.., last] =&gt; blah } Then what you're saying to the compiler is "I want you to return `blah` when the slice has a first and a last element (and they have to be distinct), with zero or more elements in between". This accounts for all cases when the slice contains two or more elements. But what about if the slice contains zero or one elements? That's what the `[] | [_]` pattern is doing there. It could also be written out like this: match foo { [] =&gt; true, [the_only_element] =&gt; true, To make it shorter we're just using the `|` pattern (the "or" pattern) to say "if you match this OR this". Also, since we don't care about using the element that is matched in the one-element case, we use the `_` pattern (the "I don't care about this" pattern) to tell Rust to not bother nagging us about unused variables.
I'm not too familiar with multirust, but as far as I remember, it doesn't download Rust _sources_ by default, so it's not really a multirust issue, is it? Either way, multirust could/should gain a command that outputs paths relevant to the current toolchain (location of rustc, cargo, src, docs, etc.), and you could parse that in your .vimrc with some vimscript.
Ah I just didn't get that [] meant 0 and [_] meant 1 element. Im familiar with the rest of that :) I incorrectly assumed that [_] meant "anything".
Lots of the diagnostics here are wonderful.
I know this isn't strictly related to Rust, but I consider Elm and Rust to be very similar in terms of philosophy when it comes to language design and community, just filling different places on the high-to-low-level spectrum. A lot of the improvements in this release of potential interest to Rust folks. There was a lot of focus on type-error-messages, which is something Elm and Rust have both put a lot of effort into. Also, Elm now implements tail-recursion for "self-tail-calls", basically converting them into a while-loop. Something similar could in principle be done for Rust, even if full tail-recursion is not practical/possible.
"Cannot move out of borrowed content" was the bane of my existence one day. This kind of cleared it up for me, thank you. I'll have to go over this in full when I get a chance. Last I tried to work on Rust, I was incredibly confused about how to copy data from a vector and hashmap, and I just couldn't get past it. Working with Option and Ok/Err was hard enough for me to learn, then combining it with a borrowed element error was just a PITA.
In that case, I would consider it a feature to suggest to multirust, to add an option to make it also download sources and set `$RUST_SRC_PATH` accordingly. It can take a while to clone the Rust source tree, though, so it probably shouldn't do it automatically.
But in the general case, if one wanted to combine two libraries built on top of OpenGL on the same program, would it require two contexts? Is there a significant cost? An example is having Rust bindings for [freetype-gl](https://github.com/rougier/freetype-gl) that, among other things, contains an implementation of fonts rendered with distance fields, and using it in a Piston game.
I love the idea of using text color to highlight where's the error (where's the type mismatch, etc).
I laughed at "Elm compiler is now producing the best error messages I have ever worked with." And then I cried when I saw the beautiful formatting. And again when I saw it prevented cascading errors. I am now jealous of Elm.
Have you actually implemented this scheme elsewhere? Do you actually *know* that it's faster? Or even correct?
I am working on the current install pipeline, unofficially because I'm not on the tools team. Let me investigate a bit and while I am working on something now I was just about to get racer working for my personal use, maybe I can offer some insight. Again, I am not an official member of the tooling team but I am planning on making significant commits in the near future and this is in the domain of what I'm planning on setting up an rfc for.
&lt;3
&gt; Why can't the compiler just figure it out? For any reference inside a struct, couldn't it easily be assumed that it must live at least as long as the struct? Sure, we *could* allow elision so that `struct A { x: &amp;i32 }` is the same as the lifetime version. But this has a few downsides, e.g. a major one is that it hides the fact that the type contains a lifetime, which is an important part of its signature. The lifetime isn't there to say that the reference outlives the struct (that's always true: the things a struct contains are valid for at least as long as the struct itself), it is there to allow the reference to be connected to the thing it points to when the struct is created, e.g. fn foo&lt;'a&gt;(x: &amp;'a i32) -&gt; A&lt;'a&gt; { A { x: x } } This sort of elision has been also discussed [elsewhere](https://news.ycombinator.com/item?id=10484568).
I've always preferred the state machine approach. Now I know why!
`struct&lt;'a&gt;` means: hey compiler, this struct contains something that can only be used for a finite amount of time (I'll specify exactly how long when I instantiate it). Give me an error if I try to use it longer. `struct&lt;'a, T&gt;` means: hey compiler, this struct contains something that can only live so long. I'm also going to use a generic data type T, which I'll define when I instantiate it. `struct&lt;'a, T: 'a&gt;` means: hey compiler, this struct contains something that can only live so long. I'm also going to use a generic data type T, which needs to live at least that long. `struct&lt;'a, 'b&gt;` means: hey compiler, this struct contains two things with different restrictions on how long they can live. Naturally, you'll need to make sure I comply with the shortest. struct X&lt;'a, T&gt; { x: &amp;'a T, } Hey compiler: I'd like to define a struct called X. It contains a reference, which means that X can't outlive the reference. It also contains something of type T, which may live for any length of time. Naturally, this means X can't outlive T. Hey user: T doesn't necessarily live at least as long as a. You can't have a reference that outlives the thing it points to. I can figure out this restriction on my own, but since it puts restrictions on how the struct can be used it needs to be in the signature, not just the body.
/u/pcwalton has [a talk](https://air.mozilla.org/sprocketnes-practical-systems-programming-in-rust/) on building an [NES emulator](https://github.com/pcwalton/sprocketnes) in Rust. It's a little old now, but maybe you could use some bits from it!
Very cool! Will it support arbitrary tail calls, i.e. if the function being called in tail position is a variable/parameter, not a fixed name?
Thanks for working on the book. 
&gt; Naturally, this means X can't outlive T. What does it mean to live longer than a type? Do types die? Isn't this like saying, "X can't outlive int32?" If you can offer an example of when such a statement would make sense, it would help me tremendously.
Also it would be nice if multirust had an option to download src as well.
IMO Rust could really use something like this.
I knew about SprocketNES (I think I even made a pull request for that once) but I've never seen this talk. I've looked at the code of some other Game Boy emulators and I think I've got a pretty decent idea of how to represent instructions and how to do the dispatch on instructions. I might do some benchmarking before I commit to this approach. I'm trying to keep the code documented and I've written tests for a couple things, I hope I can keep it up. Here's the source: https://gitlab.wambo.at/GyrosOfWar/gameboy-rs/
Yeah, but you're not colorblind.
Oh, I'm not arguing that we shouldn't do it at all, but we should have an option to turn it off.
Then add ´--machine-readable-errors´
You can use different libraries sharing the same context. Since OpenGL uses a global state, you need to be careful, though.
Hi, nom handles that kind of use case from the beginning. Every nom parser can return [either a value, an error, or an indication than more data is needed (eventually, how much data is needed)](https://github.com/Geal/nom/blob/master/src/internal.rs#L56-L75). So you need to build a state machine able to handle those states: * initial * header recognized, parsing sequences * sequence ended, looking for optional header and quality * ended * Error You make a separate parser for each state, and you pattern match on the result of the parser for the current state. If there's an error, stop entirely, if the parser returns `Incomplete`, refill the buffer, if there's a value, act on it (either go to the next state, or produce a value in your iterator). Actually, this would make a very good example code for nom. Could you hop on Mozilla's IRC, on #nom? I'll help you go through that parser :)
That looks like a good explanation for why explicit lifetimes are necessary. It doesn't really give any intuition for how lifetimes work.
For the ones who have been disappointed by the slowness of Atom, I must say VS Code is much faster and usable. You should really give it a try!
You're welcome!
i’m on :)
Problems I ran into: * rust.racerPath needs to be set even if it's in your path * The repo is missing its .vscode/settings.json and .vscode/tasks.json which will result in missing task errors if you try to debug the extension.
I really like how it highlights the code causing the problem with `|&gt;` marks and how it separates the code from the surrounding text with empty lines. It makes finding the problematic code much easier.
I've updated extension, both issues should be gone. Thanks for the feedback. :)
To make it prettier, you could use a macro: macro_rules! header_match { ( $header:ident with $typ:ty =&gt; $arm:block, $( $typ2:ty =&gt; $arm2:block, )* ) =&gt; { if $header.is::&lt;$typ&gt;() { $arm } $(else if $header.is::&lt;$typ2&gt;() { $arm2 })* }; } header_match!{header with headers::Accept =&gt; { println!("Accept"); }, headers::Allow =&gt; { println!("Allow"); }, };
Oh, pretty. Thanks. edit: Works!
Since new VSCode supports debugging, will it work on rust too ? 
Not automatically. Someone would have to write a [debugger extension](https://code.visualstudio.com/docs/extensions/example-debuggers) to interface with gdb/lldb.
I don't think that self-recursive tail calls are "the most important case" of tail call elimination, at all. 
You amazing, perfect human. I've been using VSCode exclusively for rust, I'm glad to see this.
I'm a CS undergrad, I know some other languages superficially, but never heard that before. I'm ok with learning it being a challange though, I'm doing it for fun, I posted this as a feedback.
As a partially colorblind guy, I agree that it's no reason NOT to do it, but I do ask you to not ONLY do so. Too many times I've seen information only transmitted through colors (Cisco Jabber has such a status...) and I am just unable to get anything out of it :x
I know this is silly compared to most of the other comments here, but I love that the compiler talks in first person. It might give me someone to direct my rage at when battling lifetime issues...
No. My example runs correct and fine, but it is *NOT* correspondent to the documentation. Look at spawn_pipe: it passes the 0th descriptor of fds to the writer_action function passed to it. The documentation and example says that the 0th descriptor is the read descriptor. If you read the source of the example at the link I included, you can see that. Also, I don't really know what's wrong with my variable names. I thought my point is very concise and clear.
Swap all occurrences of `reader_action` with `writer_action`. It won't change semantics of your code as it is just variable naming.
Actually it boils down to the order of parameter names in the spawn_pipe function prototype. My bad.
This kinda sounds like what pre-strcat rust might have aimed for had it had Microsoft's research budget
Indeed. For that, the best approach IMO would be examples annotated with color-coded regions which would correspond to stack scopes that end up in lifetime parameters. Or maybe numbered instead of color-coded, as the subtype ordering of lifetimes loosely follows stack address values (as the stack grows downwards, values lower down have shorter lifetimes).
[RIP LIBGREEN](http://vignette1.wikia.nocookie.net/jadensadventures/images/c/ca/Slimer1.jpg/revision/latest?cb=20130415035425) 
Well, at least now they could evaluate where Midori ended up and "just" implement it (if it seems like a decent design for Rust), rather than have to go through five different designs to get to a good spot. Ultra cheap co-routines (which may live on another thread if it was spawned that way) and the trick of switching to a traditional stack for the non-blocking code (i.e. linked stack overhead is only in the actual async coordination code, which is minor) seems like pretty clearly good things to me (if you need pre-emption etc. you could use existing OS threads). 
[Image](http://imgs.xkcd.com/comics/compiler_complaint.png) **Title:** Compiler Complaint **Title-text:** Checking whether build environment is sane ... build environment is grinning and holding a spatula. Guess not. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/371#Explanation) **Stats:** This comic has been referenced 26 times, representing 0.0292% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cx7d0kf)
good job!!
I might be wrong about this, but I think that the File type is only meant to be used with actual files on a filesystem, not just any arbitrary file descriptor. If you create a pipe with libc::pipe, you might have to read and write to it with libc::read and libc::write.
Why not write it as an AST lint pass?
Is it compiler chain, or the fact that Rust seeks to appeal to C++ developers?
&gt; `struct&lt;'a, T: 'a&gt;` means: hey compiler, this struct contains something that can only live so long. I'm also going to use a generic data type T, which needs to live at least that long. This one has always been confusing to me, because unlike something like the bound `T: SomeTrait`, where it does make sense to say that the `T` I'll pick later (i.e. the type itself) implements `SomeTrait`, here it's a little odd to speak of the `T` having a lifetime. If I choose `T` to be `Vec&lt;u32&gt;`, for instance, it's not that `Vec&lt;u32&gt;` has lifetime `'a`. It's that *some particular* vector has that lifetime. It's also a little odd because I can do this: struct Foo&lt;'a, 'b, T: 'a, S: 'b&gt; { a: &amp;'a T, b: &amp;'b S } Saying that I'll have a struct with references to two pieces of data of possibly different types, and the references will have possibly different unrelated lifetimes, but there doesn't seem to be a way to declare a struct with references to two pieces of data of definitely the same type but possibly different unrelated lifetimes (you can use the above struct definition and a type alias like `RealFoo&lt;'a, 'b, T&gt; = Foo&lt;'a, 'b, T, T&gt;;` but you can't say `struct Foo&lt;'a, 'b, T: 'a, T: 'b&gt;`).
We can't control the OS kernel, and being able to run on popular operating systems has been a design goal from the beginning. We can't switch off paging, we're at the mercy of syscalls and context switching, and we're always going to have to call a lot of C code (even if just kernel32.dll—there's literally nothing else you can do on Windows). The C code was precisely what caused a lot of the problems with split stacks. So we can't "just" implement Midori, even if we had could do all the compiler things that they did.
There have been at least proto-proposals for async/await-style concurrency. I'd be happy to see more motion in that direction. I'm less sure about linked stacks as an implementation strategy, given how much trouble we had with them—and we implemented many of the optimizations described there (assembly stack switching, stack segment caching, doubling growth strategy). We'd need to implement the other optimizations that the blog post mentioned in the compiler, and even then I'd be worried about FFI problems. Having a fast FFI has been part of the design criteria of Rust from the beginning, and there's no way to call syscalls on Windows without FFI. Midori, on the other hand, could replace everything, even the kernel, so FFI performance wasn't an issue.
I'm just talking about the async stuff, really. Where he says processes, read "rust threads".
I can't really tell but, given this extension, what *possibilities* would a VS extension (like Visual Rust now) have that couldn't be provided inside VS Code with an extension also?
You should disregard anyone telling you what to learn (unless that person pays you and you want them to keep doing that). Rust is definitely not an easy first language, or even second language, but its a lot easier to learn something you want than something someone told you to learn.
If you wish to avoid `libc` stuff, why not just use Rust threads instead of fork, and channels instead of reading/writing from a file descriptor?
Because I don't think there's any way to have a start and finish, so you wouldn't be able to get the includes or the guards in there. Although having said that I'm now wondering if there's a way to carry a buffer around and then implement Drop to write the guards and buffer to the file. I'll have to look into that, thanks.
Context: A few days ago a there was a blog post about the new Elm release. Many user here were very excited about their error pretty printing feature, which provided a very clean output from the compiler. Since I wanted this feature myself too (for debugging Redox), I decided to write a temporary utility for pretty printing Rustc output. Let me know if you find bugs or other issues!
For anyone interested, I wrote this: https://github.com/Ticki/dybuk
Ya, that is a pretty cool feature. Hmm, maybe with a compiler plugin we could have a `grammar!` macro. Leave no good idea unstolen!
Of course, I was too focused on `check_item` to even think about walking the entire crate. Thanks for the tips! 
New version is on marketplace, added rustfmt support and format-on-save feature(experimental).
Related: https://github.com/rust-lang/rust/issues/3533 I'd like to see rust have a structured logging mode TBH. Alternatively, allow for "span handler" plugins which replace the default printer.
Agree, dybuk is obviously only a temporary solution. A implementation as a plugin (not sure that's possible with the current API, though) or directly in rustc would be more flexible and better.
Cool!
I'm terrible at naming stuff, it's just a strange spelling of debug.
Perl and perl6 have been doing it for a few decades and it has worked pretty well (Its funny when perl steals ideas from perl6). I hope you guys copy the culture of excellency, documentation and testing perl and perl6 have, which are killer features and lacking everywhere. Perl6 takes grammars further and dogfoods on it. The language is implemented as a core called nqp (not quite perl), which is enough to implement the grammar parsing, and then they compile the grammar of the language to add the many remaining features. There are wishes to do it fully and bootstrap perl6 as just a complete grammar, but it was unwise to do until the syntax was stable. They will release the first stable version of the syntax this Hanukkaha or Christmas, and then the efforts to bootstrap it will start. The language has the ability to define a grammar and then turn it on in the same file, or do an eval in something one wants to parse. Therefore it isn't necessary to compile a new parser like lex/yacc do, the main parser of the language is able to parse other languages. Perl6 won't be context-free and won't even need multiple passes to be able to do it. One grammar the spec mandates is perl 5, which can be inlined in perl6 programs. There is an optional grammar for inline python already ([It allows a wrapper for pyplot in 9 lines](https://gist.github.com/awwaiid/ef3f0abcfa96e34977b4)). The work in syntatic macros (a feature being stolen from lisp) doesn't need to touch core, they can define the grammar for it and when it's ready, it will be included with the main grammar and made available to people. 
Beautiful!
Thank you.
it has to be across processes, or I would :/
I recently undertook a rust project that had an explicit test driven approach. Rust made this very easy. The language lends itself to a functional approach, which leads to easily composeable code. The compiler performs the heavy lifting when it comes to common bugs, so you can focus on other things. Writing tests was simple and natural When I wrote a function I could drop down into my 'test' module and write my unit tests right then and there. For this project it wasn't used, but I've used 'quickcheck' in other projects and it's very cool and simple to use. There were no extra tools required, just cargo. Maintaining coverage was really simple. We managed to have two tests for every function, and when it came down to deploying this service, everything worked, which is, frankly, just insane. There were other issues when deploying unrelated to the rust code, but 2000 lines of code *just worked*. A significant part of that was the language lending itself so well to a test-based approach.
Have you read the [testing chapter][1] of the rust book? I don't actually understand what controllability or a few other of those words mean in this context. [1]: https://doc.rust-lang.org/book/testing.html
My biggest concern about the testability of Rust is the lack of support for coverage analysis tools (basically, it boils down to gcov I think). Other than that, I have found both the tooling support as well as the language features quite adequate to make testing of code an easy task.
New version is out, added 'Go To Definition' support.
Ok, thanks! 
&gt; lends itself to a function approach functional* &gt; easily composeable code composable* &gt; The compiler carries the heavy lifting performs* the heavy lifting 
travis-cargo makes it easy to use kcov and upload the results to coveralls. The results are usable but there are some weird corner cases (enum cases not getting recognized as used, some files seem to be ignored).
Thanks, fixed.
You should probably take a gander at the Code of Conduct, you seem to be missing what makes the Rust community so effective. This kind of behavior, along with telling people they should not learn rust because they are unaware of what "function arguments" are is the kind of elitist behavior you can leave at the door.
Nice work!
Mistakes are also not people but you consider fixing them to be a bad behaviour.
I wonder if you have any guidelines you could share (I've fallen out of practice with job-applying) about what kind of information belongs in the resumé and what kind in the "Why are you interested in this position?" not-a-cover-letter box? The "official" parts are basically "I went to a university for a few years and have a day job working on an embedded C++/Qt GUI client", while the more interesting and "closer to Rust" stuff is all self-educated and on-my-own-time activity. (Like, as a specific example of the kind of questions I'm thinking of, should I link Rust RFCs I've written, and where...?)
There is no shame in not knowing something, don't worry about this individual. Nobody, and I mean nobody had an easy time learning Rust, this language would be dead if it wasn't for the community constantly helping each other and the all around friendly attitude of the project. Anybody telling you they "know" Rust and it's "easy" can be disregarded as somebody who thinks they know a lot based on the bit they do know.
With the utmost respect, you should have waited a teensy bit before taking offense on behalf of someone that didn't need it: https://www.reddit.com/r/rust/comments/3tpegb/how_testable_is_rust/cx85qup
Spelling corrections are a distraction from the topic, please use private messages for this in the future.
I find it very easy to isolate components in Rust, because there is no inheritance or other convoluted form of dependency. If you want to create an hierarchy that reuses data, you must compose the types explicitly (include the supertype as a field), this is called "composition over inheritance". When we want to give a certain behavior to a set of types, we create a trait and implement it for each type. The trait impl itself can be tested independently. The usual practice of Rust is having tests right below where something is defined. That's cool. Another thing that makes Rust easier to test is that it has no exceptions. On some languages, it's hard to understand which set of exceptions you need to test for each piece of code. (Rust has panics, that typically terminate the program; but panics aren't an usual technique for error handling, and instead signal that the program encountered an error that isn't recoverable).
I think that you have to put an asterisk to dereference when you perform the match for elem in &amp;map{ match *elem { // &lt;- Add asterisk here Field::Empty =&gt; print!(". "), Field::Hit{ship:ship} =&gt; print!("x ") } } EDIT: See also https://doc.rust-lang.org/book/iterators.html
It looks like you should use Option&lt;Ship&gt; But as for the matching, the enum should be declared Hit(Ship) &amp; then `map[3] = Field::Hit(Ship{lenght:5,life_left:5})` &amp; match as `Field::Hit(ship)` PS you misspelt length
Well said. 
Understood, I'll leave it for an example of it going both ways. Or one way, not here to fight.
&gt; You really don't know what a function signature is? In the future please don't employ feigned surprise here. We strive to make this an environment where people aren't afraid to ask questions or admit that they don't understand something.
The screenshot should link to https://raw.githubusercontent.com/redox-os/redox/master/img/screenshots/File_manager.png rather than https://github.com/redox-os/redox/blob/master/img/screenshots/File_manager.png
This is fairly unidiomatic Rust – I think you could remove some let bindings (and pull the `.collect::&lt;LinkedList&lt;_&gt;&gt;()` out of the `match` statement) to make it more palatable. Also why are you using `LinkedList`s at all? `Vec`s are usually faster. In line 141, you don't need to clone `all_data`, just prepend an `&amp;` and keep your memory profile lean. You may want to run [clippy](https://github.com/Manishearth/rust-clippy) on your code.
I always fail with the images..... EDIT: [Patch sent.](https://github.com/redox-os/website/pull/22)
Probably late response, but here I did the "matching by type" thing: https://github.com/Nercury/comics-rs/blob/master/src/headers.rs
Alternatively you can use refs in match arms: match elem { &amp;Field::Empty =&gt; print!(". "), &amp;Field::Hit{ship} =&gt; print!("x ") }
I'm talking more about the development of the actual language and not writing code in the Rust program if you get what I mean. 
I'm on mobile right now, but that sounds like a case for ~~iter().as_ref()~~ `iter().by_ref()` instead of prepending `&amp;`.
I've not had a look at your code but from what you've said and what /u/llogiq has said it shows you've ported in the sense of code rather than functionality. You translated it without introspection of what it means in that particular language. Kind of like how german jokes don't always translate well to english. Have a good think about how you can make it perform better in rust, rather than translating it verbatim to rust.
* You may want to specify more type signatures in order to get a better feel as to whether you're creating references and why, and see if you can remove these indirections. * not all iterators need to be collected, `killer_moves` doesn't for instance, you can just `.next()` the iterator itself to get either the first element or a `None`. * `match` on booleans or on Some/None is sorta weird I think, you'd generally use `if/else` for the first and `if let/else` or HOFs for the second, there's less syntactic overhead. * the final collection of the best score can be way simplified using min/max and a conditional (min_by and max_by would be even better but are not stable) by just swapping the tuple's items in order to make them naturally ordered: // make tuples naturally ordered by storing (score, move) instead of (move, score) let it = all_data.iter().map(|&amp;(&amp;best_move, best_score)| (best_score, best_move)); if maximize_or_minimize { it.max() } else { it.min() } .map(|(best_score, best_move)| (Some(best_move), best_score)) .unwrap_or((None, 0)) which can be further simplified by generating a sequence of `(score, move)` in the first place: let all_data = best_scores.into_iter().zip(valid_moves).collect::&lt;Vec&lt;_&gt;&gt;(); // […] let it = all_data.into_iter(); if maximize_or_minimize { it.max() } else { it.min() } .map(|&amp;(best_score, best_move)| (Some(best_move), best_score)) .unwrap_or((None, 0)) * Finally &gt; Converting to usize was also very annoying - search for usize in this file to see how many times I had to do this. The way to fix that was to make `HEIGHT` and `WIDTH` (and `column` where it appears which means `ab_minimax` returns `(Option&lt;usize&gt;, i32)`) `usize` rather than `u32`. After that, only the casts in `myincr` remains (and you can remove one of those by replacing `arr[idx as usize] = arr[idx as usize] + 1` by `arr[idx as usize] += 1`) 
FYI then: https://en.wikipedia.org/wiki/Dybbuk It was my first thought as well.
I think you mean `by_ref`. And it doesn't do anything in that case, I think the issue is the items are being pattern-matched, but `.iter()` yields `&amp;T`s not `T`s, so the code needs to match `&amp;(_, _)` not `(_, _)`. Sadly the signature shown by the error message really doesn't help, only the "expected/found" part hints at the issue, if you've already encountered it.
Regarding font rendering, I started a pure-rust implementation (https://github.com/google/font-rs), but in its current form it's fairly feature-poor. It is, however, quite fast. When I have more time, I'd like to push it further along, but that might be a while.
This function was never stabilized. If you look at the [1.3 docs for this function](https://doc.rust-lang.org/1.3.0/std/net/struct.UdpSocket.html#method.join_multicast), you see that it's marked as deprecated, in favor of the [`net2` crate](https://crates.io/crates/net2). The `net2` crate is designed to be a place where new networking functionality can be experimented with before actually being added to the standard library. In the `net2` crate, it looks like you would want [`net2::UdpSocketExt::join_multicast_v4`](https://doc.rust-lang.org/net2-rs/net2/trait.UdpSocketExt.html#tymethod.join_multicast_v4) or [`net2::UdpSocketExt::join_multicast_v6`](https://doc.rust-lang.org/net2-rs/net2/trait.UdpSocketExt.html#tymethod.join_multicast_v6). It's a bit unfortunate that the deprecation period was so short, and there is now no longer a reference in the docs, so you have to know to go back through the old versions of the docs, but that's part of the life of unstable APIs, you can't really depend on them still being around or being the same.
Thanks. All I had been able to find was the [1.1 docs for this function](https://doc.rust-lang.org/1.1.0/std/net/struct.UdpSocket.html#method.join_multicast), where it is marked as unstable. I assumed that it has been (re)moved, just no idea where to.
Thanks, masklinn - your feedback is by far the most useful so far. Much appreciated ; I will try everything you suggested.
strcat came in and changed a ton of shit... He takes credit for iterators, being a driving force for getting rid of libgreen and making OS threads the one true thread style, and general being an efficient systems language, as opposed to something more like Erlang or ML.
Maybe a silly question but can redox run rustc/cargo?
Just replacing `LinkedList` by `Vec` cut the running time in half here, from 0.223 to 0.117, while C is at 0.056. Some further simplifications brought it down to 0.096, I can post a gist if wanted.
`&amp;HashMap` specifies an immutable reference to a hash map. If you want a mutable one you need to do `&amp;mut HashMap`.
I'd like to see it, "only" got a 25% improvement when I did that. Stable or nightly?
Not yet! The stdlib needs to work first.
playpen: http://is.gd/oTN7S0
Oh I see it's not (entirely) a difference between machines or builds, I was running the Rust project directly rather than using the builtin benchmark and performance change is different (way bigger on the benchmark, I don't get quite 50% but fairly close and the CPU is loaded so the stats are not quite correct) Thanks.
First of all you should have a look at the `Cargo.lock` local to your project. Only this will tell you which version of `url` are required. The global registry won't help you much here. After that you can go on and see if you can trim it down to a single version. This might require to update dependencies.
In order to find out the conflict url versions, I deleted ~/.cargo, and run `cargo build` again, all packages in current ~/.cargo should be the dependencies of my project. problem is that some packages depending on specifically version of url `v0.2.*` and some packages, like rustless, depending on latest version of url `*`, the latest version of url is `v0.5.*` so this conflict happened. it's looks like a compiler problem. here is my project dependencies: &gt;[dependencies] &gt;postgres = "*" &gt;r2d2 = "*" &gt;r2d2_postgres = "*" &gt;typemap = "*" &gt;time = "*" &gt;rustc-serialize = "*" &gt;iron = "*" &gt;uuid = "*" &gt;valico = "*" &gt;url = "*" &gt;regex = "*" &gt;regex_macros = "*" &gt;rustless = "*" &gt;hyper = "*" &gt;mount = "*" &gt;staticfile = "*"
Okay, here is the code with the other simplifications I talked about: https://github.com/birkenfeld/Score4/tree/more_improvements And here is a "less functional" version with a custom iterator adaptor that is faster (0.073 seconds): https://github.com/birkenfeld/Score4/tree/less_functional
Apart from your missing `&amp;mut` problem, given that before you add `n` to the hashmap, you have to add all numbers in the range `[2..(n-1)]`, why aren't you simply using a `Vec`, indexing by `n - 2`? E.g. [http://is.gd/6dayj1]. In addition I would like to point out that your `fib` function accepts `i32`, but you do not handle negative numbers; calling fib with `-5` will stack overflow. I would recommend either extending your definition into the negative numbers, or accepting `u32` instead of `i32` and avoid the problem.
Yes, it is possible. `where` syntax was invented exactly for this task: pub fn from_bytes&lt;T&gt;(bytes: &amp;[u8]) -&gt; T where T: FromStr, T::Err: Debug { T::from_str(str::from_utf8(bytes).unwrap()).unwrap() }
I wonder if Redox could take inspiration from some of the things [Midori did](https://www.reddit.com/r/rust/comments/3tjrfh/efficient_async_computation_in_midori/) :)
&gt; Good ideas, I'll try that! Beside those suggestions by masklinn, there are several other ways to improve the code. There code could become a bit more strongly typed, this isn't to make the code faster, but to make it stronger (example: the D language version uses Cell.Orange/Cell.Yellow instead of *cell = 1i32). Perhaps some i32 could be replaced by u32 where they don't need to become negative. To try to improve score_board you can also try to represent the table as 1D array, where the rows have a length of 8 (and the last column is ignored), to access the items without multiplications by 7 (just a shift suffices). (Normally this use of powers of 2 rows needs to be done carefully for complex interactions with the cache, but this table is tiny, so I don't think the cache causes much problems here). Edit: also take a look at the second C++ version, that uses a: typedef unsigned short BoardData[height];
Ahh, I see.
I really wish that the official Rust discussion forum didn't require third-party JavaScript. I wonder how popular Discourse would be if people didn't know Jeff Atwood made it.
You can get a HashMap-like interface on the Vec if you use [`vec_map`](https://crates.io/crates/vec_map).
Have you come across [rustfmt](https://github.com/rust-lang-nursery/rustfmt#rustfmt-)? That sounds almost exactly like what you're trying to achieve.
Sorry, first time. :) Now it seems to be ok.
Someone should buy dont-use-wildcard-dependencies.com. (Also, I can't wait for crates.io to reject crates that use those.)
What would be the point of optimizing this program, exactly? Looks like it will spend practically all of its time waiting for I/O. I don't think there is anything that can be done here to make it noticeably faster.
In terms of best practice. Maybe you know better ways to code the same functions? It's to optimize my Rust code and learn better ways. There is a lack of good tutorials out there. 
Ah good point about the unsigned. Thanks!
What's wrong with third party JavaScript?
My biggest gripe is that it's completely nonfunctional without JavaScript.
 let format = ".txt".to_string(); let file_name = filename + &amp;format; Should really be let format = ".txt"; let file_name = filename + format; Currently `".txt"` is a `&amp;str`, `.to_string()` allocates a heap allocated growable `String`, and then `&amp;` (before format) creates an `&amp;String` which is then coerced back to a `&amp;str`. It is more efficient (you don't have a heap allocation) to just use the `&amp;str` `".txt"` directly. Naturally this can be reduced down to let file_name = filename + ".txt"; or even just let file_name = output.to_string() + ".txt"; Also, if you want, you can re-use the name `filename` instead of having `file_name` and `filename`. I personally think it looks better.
&gt; I personally hate fixed headers [No JavaScript required](https://html5-demos.appspot.com/static/css/sticky.html)
Analytics and tracking.
* Rust strings are multiline, you don't need multiple `println!` to print multiple lines * heed the compiler's warnings, it tells you that you have unused variables and imports, fix that * you don't have to use `match` everywhere, `Option` and `Result` (amongst others) have plenty of methods for that, which you can chain e.g. let input = input.trim().parse::&lt;u32&gt;() .map_err(|_| format!("{} is not a number", input)) .unwrap(); or let input = input.trim().parse::&lt;u32&gt;() .unwrap_or_else(|_| panic!("{} is not a number", input)); (also demonstrates a sometimes more readable use of generic-return functions) * don't allocate when you don't need to e.g. let filename = output.to_string(); let format = ".txt".to_string(); let file_name = filename + &amp;format; should be let format = ".txt"; let file_name = format!("{}{}", output, format); no need for 3 allocations when a single suffices (and of course `format` can be folded directly in the format string as in `let file_name = format!("{}.txt", format);` * There's usually isn't much point to reusing OpenOptions (it's a smallish stack structure), the norm is a one-shot chain: let file = OpenOptions::new().write(true).open(path); also unclear why you open the same file twice and ignore the first opening (which Rust tells you about too) . Also if you want to check that a path exists the correct way is `fs::metadata(path).is_ok()` (and `fs::metadata(path).is_err()` to check that it does not), and the way you check if the file exists is just weird, `ok` is completely pointless (since you're panicing otherwise so it can only be `true`) and you're storing the output of the match (which is `()` so useless). * `if` takes a boolean. `if ok == true` is redundant, `if ok` suffices * learn to return results, your sub-functions have no reason to panic anywhere * functions are nice, create more functions. The first dozen lines of your sub-functions are always identical and ultimately create a `String`, that can trivially be extracted into a e.g. `get_filename` utility function here's a slightly cleaned up version: https://gist.github.com/anonymous/8c1b42a4abf230ae6820
Been reading this today, almost the whole thing and I must say that the order is atrocious. Many of the chapters are great but they reference content that is covered hours of reading later. Imo it should read top to bottom if you call it a book. Maybe start with a few data type examples so I know what I'm reading in the getting started part. If "Syntax and Semantics" had been placed first it would have been a great improvement.
I've been reading this today and I'm of the same opinion as OP. As far as programming language books go the order is the worst I've encountered.
A tad optimistic for code bound by file and user-input IO.
A private message can easily include a link to the post, and doesn't have the side effect of cluttering up the thread for thousands of other readers for a purpose that swiftly becomes irrelevant as soon as the original poster fixes the spelling.
Hmm, that's an idea. However, we want to provide feedback rather than just say "change this", and to give good contextual descriptions of the problems I'd probably need to parse both files anyway.
I hate dynamic types.
Even something as simple as `vec![0; n]` needs optimizations enabled not to be super slow though. (This appears inside `BufReader::new`)
Return types - I know it's good to have definite interface. What I was thinking was to allow the language to auto-generate the interface for you (it could also allow some sort of optimization). In case you are building library and the function is public, you would have to define interface and never change it after release build. (Necessary changes would have to be versioned.)
Glad to see it being worked on. I was going to add a couple of imap methods to that library myself. Replacing `Result&lt;...&gt;` with an explicit `Result&lt;..., IMAPError&gt;`seems to go against the pattern of defining a package-wide Result type, such as http://doc.rust-lang.org/std/io/type.Result.html, http://doc.rust-lang.org/std/fmt/type.Result.html, etc. The main problem I had with the library is that it's pretty much impossible to use it without some knowledge of the underlying IMAP protocol. What's a `sequence_set` in `fetch(&amp;mut self, sequence_set: &amp;str, query: &amp;str)`? Turns out it's an IMAP thing that allows you to specifiy a *range* of emails in the currently selected folder. A meaningful API must wrap it inside a well-documented struct instead of making the library user learn the protocol. On the other hand, sequences are powerful, you can wipe an entire folder with just try_s! (imap.run_command_and_check_ok ("STORE 1:* +FLAGS \\Deleted")); try_s! (imap.run_command_and_check_ok ("EXPUNGE")); so I guess the sequence API should be able to express things like an infinite range (1:*). P.S. I guess it's rather obvious that an IMAP library asks to be separated into low-level and high-level APIs. High-level API might go like "Borrow a mutable list of emails from that folder. Now do something with them, while Rust borrow checker forbids me from switching to another folder by accident". And the low-level API might read like "Switch to that folder. Get email IDs from the current folder. Get email headers from the current folder. Add flag Deleted to email number 5." It should map the protocol, basically.
&gt; seems to go against the pattern of defining a package-wide Result type Seems like a good 'todo' for the future, thanks. &gt; The main problem I had with the library is that it's pretty much impossible to use it without some knowledge of the underlying IMAP protocol. I try to stick to the IMAP terminology where possible, so that the code benefits form the protocol's own documentation. Ideally documentation will make this easier. I can also hopefully implement something like Into/From for SequenceSet to make it easier to use the API. Or maybe a macro for generating valid commands/sequenceset's. &gt; High-level API might go like "Borrow a mutable list of emails from that folder. Now do something with them, while Rust borrow checker forbids me from switching to another folder by accident". This is essentially what exists right now, in terms of "get inbox, retrieve emails, borrow checker prevents accidentally doing something invalid" since inboxes exist within the scope of an IMAPClient::Selected object only - you couldn't ever get another inbox without reselecting another folder and then pattern destructuring it for a new one. But I haven't built any of the cool high level stuff like borrowing a *mutable* list of emails. I really like that idea. I have essentially *only* implemented enough to fetch a list of emails, none of the other stuff yet. But all of that can be implemented on the Mailbox struct. Thanks. Cool ideas.
It hangs my phone for several seconds, and whenever I've scrolled up/down far enough to trigger the loading of more comments. I absolutely hate discourse. Who thought it was a good idea anyway to use some (apparently rather heavy) javascript to delay sending me comments that I'm going to read anyway? The posts tend to be mostly text, I don't get why the website doesn't send me chunks of at least 100 comments at a time.
Hey so thanks for the answers, but besides that, do you know how can I get some test statistics regarding the Rust repository? We are having problems with that, we tried to follow a tutorial but we couldn't get anything. If anyone has anything please tell us. :)
&gt; I think that a sense of style comes with experience, the students should probably run the auto-indent tool themselves instead of correcting style manually. Yeah, this is a debate we've had among the course staff. Many students in the course don't have much programming experience, and aren't terribly interested (their main discipline isn't software), so over-reliance on tools is a problem for us. If there's a tool that can do something for the students, most of them will use it blindly and never pay attention to what it's doing (or check to make sure its output matches our style guide properly). For this reason our lecturer is against the students using auto-formatters. The tool is also used to automate a large amount of our marking, so even if most of the students use a formatter it's still needed. This is mostly just as a learning experience for me - we already have a tool in Python that accomplishes this for us well.
The `Refcell::borrow()` function returns a really short-lived reference. (Actually it returns a wrapper, which returns short-lived references). There's no way that you can match that lifetime with the `'r` (which you're trying to do, when pushing the `task` onto `LinkedList&lt;&amp;'r Task&gt;`. By writing impl&lt;'r&gt; Runtime&lt;'r&gt; { methods... } you say that methods will work with *any* lifetime `'r`, which caller decides to use. I think that the proper way to refactor this code would be to store tasks as `Rc&lt;RefCell&lt;Task&gt;&gt;` everywhere in the `Runtime` struct. So `HashMap&lt;String, Rc&lt;RefCell&lt;Task&lt;'r&gt;&gt;&gt;` and `stack: LinkedList&lt;Rc&lt;RefCell&lt;Task&lt;'r&gt;&gt;&gt;`. In Rust you can't easily store references to the things owned by a field in a struct in another field of the same struct. Either you use `Rc`, or make a layered design, or use raw pointers (or use [owning_ref](https://crates.io/crates/owning_ref) which won't work in your case (it allows to store a single reference, but you want a list of them)). And unfortunately, `RefCell` is contagious. If you wrap a `T` in it in one place, you'll most likely have to wrap it everywhere else too. And also, could you show the definition of `Task&lt;'r&gt;` and `RuntimePtr&lt;'r&gt;`? Maybe knowing what that `'r` is for would make it easier to understand your design.
OK, then. I sort of wanted to avoid `Rc` if I could, but it looks like I might not be able to. Raw pointers are probably more valid than people give credence to, but I'd like to avoid them too, if only to build my safe Rust skills. Also, here's `Task&lt;'t&gt;`: /// A single build task. pub struct Task&lt;'t&gt; { /// The name of the task. pub name: &amp;'t str, /// A list of task names that must be ran before this task. pub deps: Vec&lt;&amp;'t str&gt;, /// A reference to this task's callback function. func: state::Reference, } `RuntimePtr&lt;'r&gt;` is just a type alias for an unsafe mutable pointer to the struct itself: /// A raw pointer to a runtime object. pub type RuntimePtr&lt;'r&gt; = *mut Runtime&lt;'r&gt;; It is *only* used to hand out unsafe pointers to some other unsafe FFI functions elsewhere. When initialized, the runtime is put in a box, which the pointer always points to: pub fn new() -&gt; Result&lt;Box&lt;Runtime&lt;'r&gt;&gt;, Error&gt; { let mut runtime = Box::new(Runtime { tasks: HashMap::new(), default_task: None, stack: LinkedList::new(), ptr: 0 as RuntimePtr, state: lua::State::new(), }); // Store a raw self pointer to this runtime object. runtime.ptr = &amp;mut *runtime as RuntimePtr; // A bunch of other stuff... Ok(runtime) }
Thanks for the link!
&gt; I wonder how far discourse has come to being like a frontend for a mailing list, I know there were some discussions about that a while back. You can get posts as email and reply to them from email. Works just fine.
From what I understand (correct me if I'm wrong), you want to make that `'r` to be the lifetime of `Runtime` itself. And in `Task` you want to store references to strings allocated as a key in a `tasks` hashmap. If that's the case, then it won't work, sorry. The simplest solution would be to change every `&amp;str` to `String` and get rid of that `'r` parameter. If you really want the strings to be allocated somewhere else, then why not store `&amp;str` instead of `String` in hashmap and `default_task`? Regarding `RefCell`s, it's usually better to just wrap one field, rather than the whole struct. In your case it would be something like that, I guess: pub struct Task&lt;'t&gt; { pub name: &amp;'t str, pub deps: Vec&lt;&amp;'t str&gt;, func: RefCell&lt;state::Reference&gt;, } I also don't really see a point in storing self-referring `RuntimePtr` -- you can always derive it from `self`. And one more thing, if your code requires the `Runtime` to have a stable location in memory, make sure you don't hand out the `Box&lt;Runtime&gt;` to the user -- make a newtype: `struct PublicRuntime(Box&lt;PrivateRuntime&gt;)`. If you want to avoid `Rc`, you can always just store task names in the `stack`. (The harder way of avoiding `Rc` would be to store all tasks somewhere else in the call stack, and keep just `&amp;Task` in the `Runtime`, but it's probably not worth the mess).
[Perl5 parsing is undecidable](http://www.perlmonks.org/?node_id=663393). How could it possibly have a grammar for it?
I'd suggest posting on the Rust users forum too.
A common subset.
Is it worth while to eliminate wildcard dependencies all together?
I have written a bunch of Rust code to replace C code in [*ring*](https://github.com/briansmith/ring) and [libwebpki](https://github.com/briansmith/webpki). If your goal is good Rust code then a converter wouldn't help much, because good C code is very different from good Rust code in terms of organization, even as far as simple things like how you loop through arrays. Also, things that look the same in Rust and C aren't always the same; for example, integer overflow is treated very differently (in debug mode, at least).
It's great! I'm using that plugin all the time. Sadly, I don't have time either and lacking knowledge :/ I hope you'll find someone!
Super slow is relative, the codepaths of the test program perform 2 to 3 human interactions and 1~2 FS in ~100 lines of rust. The speed of the rust code itself is unlikely to have any effect on it.
Ok, good to know.
As always, comment here or on rust-users. Myself I'm going to see what I can do to help implementing my recently accepted RFC. Also find the time to work on my Cow lint. Hopefully.
I'll probably post something once I've got something to show - I'm still sitting in the planning stage so it might be a while.
I've just published my first crate! [Barcoders](https://github.com/buntine/barcoders), a barcode-encoding library. I'm still finalising the documentation. It's close, I promise. I took some advice from this subreddit and have turned each generator into an optional feature so you only need to compile the stuff you want to use. Next steps are support for Postscript and SVG. I'd also like to add a few of the less-common symbologies.
By the way, here is my totally-not-a-hack way to verify bcrypt-encrypted password stored in the database: http://is.gd/KJbAaG If someone has a better solution, I'll take it!
Now that I've finally decided on a UI for [rusty-cheddar](https://github.com/Sean1708/rusty-cheddar) (my plugin for automatically generating C header files from Rust source code), I just need to figure out how to test the damn thing. Unit tests are fine, but for something like this I think integration tests are gonna be what actually catches the bugs.
It looks good! I have more or less the same design for nom's consumers (in 1.0), except I added a way to send messages to data sources and previous consumers, for seeking or state machine purposes. Hey, since it looks like Rust parser combinators libraries always end up building a data streaming platform, what would you think of joining the effort, and making a separate pipes-like library out of what we did?
[A code evaluator IRC bot thing.](https://github.com/angelsl/rs-evalbot) Inspired by Rust's playpen bot. Also my first Rust program. Ironic that I contributed to rustc (the non-Rust parts) without actually having written Rust beforehand.
Awesome thanks. Reading to do.
Just a note for Windows users: `cargo build |&amp; dybuk` should be replaced with `cargo build 2&gt;&amp;1 | dybuk` From what I can tell, this works universally, so if you are having problems, try the alternate command instead.
Awesome! I'm looking forward for contributing. What's in future developments?
Which RFC?
It's not exotic stuff. It's just that I don't know any Rust library that lets me easily do the same thing. I tried one (don't remember the name) but couldn't get it to work (I would get a result, but not the correct one). I'm not familiar enough with encryption to hack my way inside this.
[Deprecation for Everyone](https://github.com/rust-lang/RFCs/pull/1270)
Rescue is on its way. https://github.com/tildeio/ember-cli-fastboot
IIRC that was fixed shortly after he reported it. I haven't had any problems in a very long time.
&gt; I currently load the whole file into memory. yes, that’s what i’d also, except that my .gz file is 500MB and its content 2.2GB, and *then* you start actually doing stuff ;) &gt; I can only guess that you want a buffer that is big enough that you're not spending all your time in IO, but low enough to not eat up all your memory. that’s what i’m thinking as well, but i don’t know how buffering will work best: before decompressing, between decompressing and parsing, or both. &gt; I know that with my String 'solution', you could just split the string into multiple &amp;str's or whatever, and iterate on those totally. my parsed structure basically is just a sequence of `(&amp;str, &amp;str, &amp;str)` which slice the whole (decompressed) file. --- if i use the whole file solution i have the added disadvantage that i don’t know how big the file is after decompressing, but i can wing it by using a vector of the biggest size encountered so far (~2.2GB) and just grow that if necessary.
My goal is to release version 0.1 of my new parser combinator library: [Chomp](https://github.com/m4rw3r/chomp). And probably write a blog post about why, what and how. So far it is going pretty well, while I am writing it I am building a control-sequence stream-parser using all the tools. Just recently I managed to remove the last parts of the custom buffering that I had for the old "hand-written" parser. In the next few days I should be able to switch to using crates.io instead of a local path :)
Yup, it is. There goes my last explanation ;)
There's an RFC somewhere you might want to check out...
My experience with reading large-ish (100 MB+) files from SSD using `Read::read_to_end` is that it uses quite a lot of time in kernel compared to buffered solutions (and `mmap`) as well as some measurable overhead in userland. From my benchmarks and tests when writing parser combinators with streaming parsing from disk, reading the whole file into memory was objectively worst performing by a huge margin (~0.4 seconds for reading the whole file vs ~0.05 for buffering and mmap, 200 MB file on a MacBook Pro 15" Late 2013). Even a reallocating buffer (when the parser needed more memory to complete one chunk of input) performed much better than reading the whole file at once. I do not know how `mmap` behaves in the case of holding references to lots of different parts of the file at once, but just reading sequentially using it is very fast. The buffered solution is equal in speed to `mmap` for my tests.
This actually helped with the issue of the File scope problem, but introduced some other problems such as not maintaining the vector; but I found that much and was able to set_len my way out. Thanks! :D
As Johnny_Bob replied below, you can "fix" this by using a type parameter instead of an associated type: trait Doc&lt;Op&gt; { fn apply(&amp;mut self, op: &amp;Op); } It's not the nicest solution though. Adding a lifetime parameter to the trait itself might be the nicest way to go, though. It adds the ability, but not requirement, to have lifetime-constrained things inside the trait implementation. I *think* that when you use the trait, you'll be able to elide it pretty often. It seems like really what you'd like is impl Doc for StringDoc { type Op = forall&lt;'a&gt; StringOp&lt;'a&gt;; fn apply(&amp;mut self, op: &amp;Self::Op) { self.content.append(&amp;mut op.content.clone()); } } but rust doesn't support things like this directly. There might be a crafty way of constructing this though. Does anybody know if there's an RFC for things like this, or GADTs?
The good uncle has had one of his benchmarks (renderer) included in the phoronix test suite. You misspelled his nick.
Damnit. You're right. Problem though and since I'm wary about bandwagoning on this sudden rulechange just because I got mad. I don't know if a rule-change is necessary. Let me break down why I think the users interactions with the community do not fall in line with how the community handles itself without any emotion. [This](http://graydon2.dreamwidth.org/209581.html) is the standard we strive to conform to. I think it's the most important piece of newcomer documentation because it sets the context for the next document everyone should read, the [Code of Conduct.](https://www.reddit.com/r/rust/comments/2rvrzx/our_code_of_conduct_please_read/) This is how we are expected to treat eachother, and if you are like me and don't really care how the CoC is worded you can imagine that it boils down to two things. 1) Be constructive in your criticism and as professional as possible. In this community the methods we use to utilize this are RFC's on github, casual chatter on IRC, and if there's ever a problem that needs attention there is always administration somewhere who will listen to you even if what you want to do is complain about things that rust doesn't do. 2) If the quality of the technical information suffered because of the spelling errors it would have been plenty fine to do what the user in question did. But it is readable. If I was in a conversation with another individual and they said "The compiler carries the heavy lifting" while talking about rust I don't think I would correct them. That's all. I don't want to see the community degrade to what communities have in the past become very bureaucratic and politically charged and the only way to do that is to keep a healthy attitude and ensure everybody understands the standards. A lot of energy has already been put into keeping the community attitude healthy and the effort has shown in the diverse talent the project has attracted. That's all. I was wrong too, we all need to square eachother away. Nobody in this situation is wrong the way we conveyed our thoughts was incorrect and as the community grows it's going to be important to remind newcomers of the standards. I love standards. 
I'm on chrome beta and there's no sticky here.
*some particular* `&amp;Vec&lt;u32&gt;` has a lifetime, and different particular instances can have different lifetimes. The type doesn't have *a* lifetime.
&gt; let input = input.trim().parse::&lt;u32&gt;() &gt; .unwrap_or_else(|_| panic!("{} is not a number", input)); Shouldn't this be more like this? let input = input.trim().parse::&lt;u32&gt;() .expect(format!("{} is not a number", input))
Well, when I measure "super slow" relative to the speed of compiled rust, it's usually about a 10x difference--that is, the debug code is about ten times slower than the optimized version, so... yeah. :p
&gt; Join the core Rust team! Small note, getting this job does not mean you get to be on the core team. EDIT: to be clear, it doesn't prevent you from doing so either. What I mean is that "core team" != "Mozilla employees working on Rust", and don't get any particular preferential treatment when choosing who goes on what team.
Agreeing with illogiq, I love reading about what everyone is doing...that's what this thread is for! Please keep posting :)
Yeah, it seems to be a running joke in all the weekly update articles. If you look back through them, you'll see a winky face ";)" and a screenshot on some random piece of computing hardware. I also thought it was legit until I saw it repeat the next week with a different device, and guessed it's probably a joke. Would be nice if there was a small disclaimer under the image.
Every function you write is an API that will get reviewed at some point. Even if it's not public, it's naive to assume that internal code won't get used by someone that doesn't know what the function does. It could easily be you in a few months. We could think of exceptions to the rule, but they'd be so rare and the benefit so small, and the chances of it getting misused so high, and the pain it would cause so common, that it's better to avoid it all together. I have not talked about the part where you could change the return type accidentally and not realize it, breaking other people's code but not yours, because that is another can of worms entirely. Again the one part were it makes sense is with closures (which already work like this) because the closure is declared on the same place it's used.
What would I use nom for? I'm not familiar with the term 'parser combinator'. I get that you build parsers for it, but what part of being a 'parser combinator' makes nom useful for this?
This looks nice. BTW: Would it be possible to add an arrow that points directly at the error. (ie at "foo")?
I really like something like this filename.rs:1:1: 8:2 error: this is an error 1| a 2| b 3| c ...
I am much happier without the path repeated, it's really annoying, and I always cut it out of documentation examples.
Not sure when you tested, and what your methodology was, but here's a bit of background info: read_to_end improved a fair bit around 1.3-1.4 iirc. That said, basic use of it still leaves a bunch of performance on the table: File's read_to_end implementation doesn't use any information regarding the file size to set the capacity of the Vec in advance, so it has to realloc/memcpy a bunch to read a large file. I *suspect* that if you add capacity to the Vec in advance you'll see better performance. The reason it doesn't use file size information is to do with a concern that doing the stat call on the file will inhibit syscall whitelisting efforts - more here: https://github.com/rust-lang/rust/pull/27159
nom is a library that allows you to build big, complex parsers by combining smaller ones together. With a traditional (shift/reduce) parser, you describe the whole grammar at once, and modifying one part of the grammar can cause conflicts with other parts. With a parser-combinator library, every parser is a module that you can use as part of a larger parser. For example, you might write an integer parser, and then use that as part of a mathematical-expression parser.
Should we consider the formatting [Dybuk](https://github.com/Ticki/dybuk) uses?
It's perfectly possible, but um, it's out of scope for the current issue, because that's got to do with how we handle different errors and warnings, and whether we've chosen single-line or multi-line for them :)
Yeah, I looked into that, but I feel like it's just too "flashy"! (so many colors &amp; characters!). Shall we just stick to the classic output of rustc please? :)
Interesting. Thank you.
When churning through a lot of data quickly, re-using the same Vec or slice can be much faster than allocating a new one for each datum.
Although every member of the core team has been employed by Mozilla to some extent...
That has to do with the way the particular error was emitted. It can be emitted with the span of the whole trait or just the name, it currently uses the entire trait (which makes sense IMO). 
Actually Steve and Yehuda were employed first. http://blog.rust-lang.org/2014/12/12/Core-Team.html explicitly mentions Yehuda's work on Cargo, which is what he was hired to do. Steve's been a contractor since before 2015, as far as I recall. Huon is the only outlier. Huon is a beautiful butterfly.
Ah, I had been floating Yehuda's name for core team membership for months before that, that may explain my misrememory. And yes we &lt;3 our /u/dbaupp. :)
Sorry, didn't receive anything substantial on the thread to promote it to QotW. If you have any nominations, do let me know at https://users.rust-lang.org/t/twir-quote-of-the-week/328
If there are substantial amount of Russian resources coming out each week, we should probably start a Russian edition of TWiR. :-)
The macro is very impressive! Do you plan to release it in crates.io?
How did you find out? I thought I kept my wings and probiscus well hidden. :(
Then why is it written like `&amp;'a Vec&lt;u32&gt;`, with the lifetime annotation right in the middle of the type expression?
Slightly relatedly: It could be nice to have a horizontal line to indicate the beginning of a new error. Otherwise it can get confusing to distinguish between errors and notes ---------------------------- error: ... note: ... note: ... ---------------------------- error: ... note: ... 
Hovering over flowers was a giveaway...
Has anyone done a survey of existing code to actually find out how many people are using each style?
Sorry if it was not obvious. It's just a running joke (hence the ";)"). I promise to include a disclaimer in the next version :). If you want a actual picture: http://dictator.redox-os.org/content/public/upload/img20151112175129_0_o.jpg
That's right it's just a joke. I'll remember to include a disclaimer next time :)
Not that I know, but it does seem fairly easily greppable if you've got all of crates.io on-disk.
Some relevant Stack Overflow stuff: * [How to get struct reference from boxed trait?](http://stackoverflow.com/questions/33687447/how-to-get-struct-reference-from-boxed-trait) * [Is there any way cast to a generic T, where T is an arbitrary trait type?](http://stackoverflow.com/questions/33794138/is-there-any-way-cast-to-a-generic-t-where-t-is-an-arbitrary-trait-type)
The first link provides a workable solution! Thanks!
I like the reduction in duplication of the file name, but it would be nice to keep `home/wafflespeant/Desktop/multi-line-warn.rs:1:1` as a single thing, at least once. Perhaps I'm alone in this, but I use it in Iterm2 to jump straight to that line/column in my editor. I'm pretty sure most editors can handle the `path:line:column` syntax...
Right, the space should be removed, I guess. This was mentioned by /u/Manishearth in the PR.
It is pretty close to being useful right now, so I hope I will be able to publish it soon. I just need to write some more docs and tests and one or two more examples, as well as rename a struct or two and a few traits.
Just for completeness, you can also use an enum to keep track of the shapes. This approach works best if you have a finite set of shapes. enum Shape { Sphere(Sphere) Mesh(Box&lt;Mesh&gt;) // .. add more shapes here } 
 - Start curating more easy bugs. - Fix random diagnostics bugs - Mentor, mentor, mentor 
I'd definitely recommend looking at /u/birkenfeld 's solution: https://github.com/birkenfeld/Score4/blob/less_functional/Rust/src/main.rs#L121 I think that's the most idiomatic code out of all suggestions. `IteratorExt` is not strictly neccessary (you could do the same with the for loop moved to `ab_minimax`). I'd also recommend relying less on indices and iterating over columns directly.
:+1: However, it's a breaking change, so we can't really do that.
That's a fair point. :) Are block documentation comments are considered bad practice--I'm not seeing them in the book chapter? 
[rust-crypto](https://github.com/DaGenix/rust-crypto/blob/db7fe6caf38315d94f7e3dfb6257cd1f876f1c39/src/bcrypt.rs#L160) provides bcrypt api, in PHP bcrypt is default for password storage ( I think since 5.0 ) but I don't know what 'cost' level is the default. Hopefully that helps, rust-crypto is a great lib, I've been using it for hmac-sha in a library I am working on
I just started a Markdown parser to learn Rust. I'm pretty stoked!
May I ask why? Reflowing `///` is such a pain when writing long module/crate level documentation. I never knew the block-level comments existed, so I've always just put up with it. But now that I know...I desperately want to use `/** */` forever and everything E.g. [docs like this](https://github.com/polyfractal/bounded-spsc-queue/blob/master/src/lib.rs#L221-L272) and [this](https://github.com/polyfractal/Turbine/blob/master/src/lib.rs#L180-L242) are a pain with triple slashes, especially because markdown parsing can get finicky if the formatting isn't *just so*
There's &lt;https://github.com/servo/ipc-channel&gt; that might be of interest.
Ok, I'll check that out, thanks!
Steve and a number of other Rust devs are convinced that, because IDEs/smart editors can automatically comment sections with single line comments, block comments are unnecessary. Luckily, when this was proposed, the push back was so... heated (perhaps even overly!), that it got dropped, and they made it into 1.0.
Why are those are this two examples more of a pain than anything else? They look pretty standard to me. Also doesn't your editor handle reflowing docstrings for you? 
Ah. Well, I'd hate to see them go. I have missed block comments in pretty much every language that doesn't have them (that I used, of course). I might've even been part of the push-back and just forgotten. It's sometimes hard to keep track with a project this open :)
&gt; because IDEs/smart editors I am generally anti IDE, or language features that require IDEs, but I'm not aware of any editor at all that doesn't allow for commenting out a section of code with line comments.
&gt; Reflowing /// is such a pain when writing long module/crate level documentation. With vim, it's a single key: `=`. It reflows that all just fine.
Nothing particularly special, they just have text that wraps. In fact, they look like most long examples...which is rather my point! &gt; Also doesn't your editor handle reflowing docstrings for you? It looks like Atom has [a plugin that might work](https://atom.io/packages/docblockr), and it claims to support Rust, so I'll give it a shot. As I said in the ticket, I don't *really* care at the end of the day. But it is annoying if you don't have editor support (and not everyone uses Vim, etc). I see the technical reason for wanting to remove it (less code == less complexity to maintain). I just don't see why it's considered "bad practice". ¯\\_(ツ)_/¯ 
&gt; Well, it technically does, but I'm like the only person who knows about it ;) It's also not like I haven't used languages with block comments extensively either, but I never use them, or at least, haven't in probably a decade. Ah, see Perl doesn't have them either, but it does have POD style documentation, so that sometimes gets abused as block comments. &gt; This is the only feature of block comments that line comments don't have, and I can't imagine ever actively doing this. it's probably down to habit and style, as you said above :) Absolutely :) &gt; Visual consistency. I can see that reasoning. I use both line and block comments in code extensively, so of course my preference would be different.
&gt; Ah, see Perl doesn't have them either, but it does have POD style documentation, so that sometimes gets abused as block comments. Yup, that's exactly the same in Ruby. Down to the syntax.
I tried coercing earlier, no luck: pub trait Foo { fn hello(&amp;mut self) { unsafe { let raw_obj: raw::TraitObject = mem::transmute(self as &amp;mut Foo); } } } Error: error: the trait `core::marker::Sized` is not implemented for the type `Self` [E0277]
&gt; Or an option to include it? You mean, an option to *exclude* it, right :) ? One of the usability benefits of Clang's error messages was actually being colored; it took a while for gcc to catch up.
Yes please! Actually, headers are one advantage of the Dybuk scheme: &gt; +----- some/path/file.rs: 4 -----+ is isolating errors from one-another. Similarly, a different color also assists the brain in correctly parsing the information.
Yeah, I'm used to similar support in other languages (Java, PHP, etc). Maybe I've just gotten so used to IDE support in Rust being minimal for the last `n` years that I've stopped looking to see if it has gotten better. I assumed Atom + highlighting + racer was about as good as it was going to get for the time being (without using that eclipse plugin). So I'll take fault on this one for being lazy and not looking around for better options/plugins :)
&gt; I have never see this in any docs or code. https://doc.rust-lang.org/book/method-syntax.html
Sure, why not. I think having it on by default would be preferable.
I have not come across implementation of a trait itself without doing it for a specific type. That is, I have seen ``impl Foo for T`` but never ``impl Foo``. That link simply shows implementation of struct types. Not what I was talking about though. May be they are the same thing. Still I can't wrap my head around implementing a trait itself.
Ahh, I see what you mean. I was thinking syntactically, you're thinking 'the difference between a struct and a trait here'.
I've always just used sqlite for these types of things. If you absolutely need human readable, then TOML is a decent choice. But it wouldn't be my first tool to reach for.
Yeah, that's sort of the plan - a compiler arg to switch between both the outputs.
Okay so because I'm short for time, I'm going to show you the quickest, dirtiest, hacky-ist way possible. It looks like this: extern crate toml; #[derive(Debug)] struct Data { database: String, username: String, password: String, } fn main() { let toml = r#"[database] database = "asdf" username = "qwer" password = "1234""#; let table = toml::Parser::new(toml).parse().unwrap(); let database = table.get("database").unwrap(); let data = Data { database: database.as_table().unwrap().get("database").unwrap().as_str().unwrap().to_string(), username: database.as_table().unwrap().get("username").unwrap().as_str().unwrap().to_string(), password: database.as_table().unwrap().get("password").unwrap().as_str().unwrap().to_string(), }; println!("{:?}", data); } Obviously this is, uh, just a little fragile. I don't recommend you literally do this, but it might get you going, at least. Worth noting that `database` here is a http://doc.rust-lang.org/std/collections/struct.BTreeMap.html so you can look at its methods for better ways of doing some of this... but. I see that the TOML crate also has support for &gt; This library also supports using the standard Encodable and Decodable traits with TOML values This would be a much better path.
I would much prefer something like Dybuk as well
Thanks for the suggestion, but I figured it out shortly after posting. It was a little hard to find but the answer was within `toml::Value`. I appreciate you taking the time to look at this.
I did that. Maybe [this will be useful](https://github.com/angelsl/rs-evalbot/blob/master/evalbot/src/cfg.rs).
ah great!
oh thanks for this! 
Since TOML is encodable/decodable (or serializable/deserializable if you use serde), it's easiest to just decode straight into a struct. This is essentially the boilerplate that I use for each project (replacing panics for return types depending on your error handling preferences): /// Attempt to load and parse the config file into our Config struct. /// If a file cannot be found, return a default Config. /// If we find a file but cannot parse it, panic pub fn parse(path: String) -&gt; Config { let mut config_toml = String::new(); let mut file = match File::open(&amp;path) { Ok(file) =&gt; file, Err(_) =&gt; { error!("Could not find config file, using default!"); return Config::new(); } }; file.read_to_string(&amp;mut config_toml) .unwrap_or_else(|err| panic!("Error while reading config: [{}]", err)); let mut parser = Parser::new(&amp;config_toml); let toml = parser.parse(); if toml.is_none() { for err in &amp;parser.errors { let (loline, locol) = parser.to_linecol(err.lo); let (hiline, hicol) = parser.to_linecol(err.hi); println!("{}:{}:{}-{}:{} error: {}", path, loline, locol, hiline, hicol, err.desc); } panic!("Exiting server"); } let config = Value::Table(toml.unwrap()); match toml::decode(config) { Some(t) =&gt; t, None =&gt; panic!("Error while deserializing config") } } 
Yup. I've used TOML in more than one project for this reason. I have a service / setting GUI project which uses TOML for the settings file and the settings GUI mostly just read and writes the TOML file. The settings GUI was originally super buggy as we pushed it out so the settings file had to be user editable since we where using it on every other install so sqlite was never a possible solution. Now it's so solid that we could remove the TOML file and no one would notice. Being able to have someone open the settings TOML file and edit it was a life saver during development. These are the kind of things which can save your bacon in deployment.
I'm using Rust's type inference in this case. You'll notice that the final match is the last statement in the function and doesn't have a semi-colon; which means it either panics with an error, or returns the value of `toml::decode` from the function. So Rust is inspecting the return type of the function (` -&gt; Config`) and decoding into a `Config` struct. If you were writing this out normally, you'd supply a type on the variable itself so that Rust has enough information to decode into the right struct: let config: Config = toml::decode(foo); Edit: Also note, you'll need to include rustc-serialize in your project for decode/encode to work, and tag your struct with `RustcDecodable/RustcEncodable`. 
This is awesome! Super easy. However, I got it going in stable rust. See my edited post, or [my comment](https://www.reddit.com/r/rust/comments/3u3xea/toml_to_struct/cxbrhke) here :)
Funny alone is not enough, it should also either give some insight into Rust, be heartwarming (especially during winter), be relatable or all of the above.
And what are benefits of completely removing non-doc block comments? Will the code compile faster or something?
Well, manually parsing is trivial when you don't care about error handling. Less so when you do.
But that reformats the code... Eg. I have a Vim plugin that highlights lines added, removed, changed (see screenshot in https://github.com/airblade/vim-gitgutter). During development, if I would be forced to use block of inline comments, the whole informational value for that block is gone. And sometimes some development have to be done in foreign setup (somebody workstation, custom shortcuts etc.), or just plain "nano" etc. In bad languages like Python, there are reason for lack of block comments (since the syntax is heavily depends on white-spaces, and parsing speed might be more important etc.). But does removing `/* */` have a single benefit in Rust?
Sure, the project I pulled that snippet from is here, if you want to see the fully worked example: - Config parsing code (Config struct right at the top): https://github.com/polyfractal/cormorant/blob/master/src/config.rs - Config file: https://github.com/polyfractal/cormorant/blob/master/config.toml 
Thanks!
I've been exploring Markov/Stochastic games alot lately and using Rust to do it, so I'm mostly going to continue working on that. The goal is to use the stuff I'm working on at my job once it's more thought out/explored. A nice side effect is I've ended up with a nice little library that I could clean up and release if I wanted to, but I'm certainly not that knowledgeable in the area beyond my reading/playing and I don't know if it'd even be useful to anybody so we'll see.
Nice function. You should put this in a separate repository and put it on crates.io, That would be great.
Well, I'd be happy to find a solution first and then propose it as a Serde patch. If I knew a solution I would've sent a PR already :) If a general solution for Serde is not possible, I'd alternatively be happy to find a local solution that works for me.
I'm not sure about that -- how long does the website live? I suppose it could just clone the quote. Or perhaps a `Cow&lt;'static, str&gt;` to avoid allocation in case of a timeless quote.
You certainly didn't resubmit a text post, no.
It looks fine at this end? I deleted the first incorrect URL link to which you replied and just re-did the process. But the second time getting the details the right way around. Is there something that looks wrong?
&gt; But that reformats the code... So does adding newlines for `/*`, unless I'm misunderstanding what you mean. Looks like it's just a couple of keystrokes in nano: http://lists.gnu.org/archive/html/help-nano/2015-11/msg00003.html Not as convenient as one, but that's why you use nano in the first place: even less than vim and emacs. Vim, Sublime, Emacs, Kate, Atom, every editor has something to do this, because there are very popular languages that don't have block comments at all. &gt; But does removing /* */ have a single benefit in Rust? See my large comment downthread about simplicity.
You're right, there is only one `Vec&lt;u32&gt;`-the-type, and that type has a lifetime of `'static`. That does not mean that ah instance of `Vec&lt;u32&gt;` must live forever; it means that you can keep a `Vec&lt;u32&gt;` until `'static` ends (which is never). The lifetime of the type is a maximum; it is not a minimum or an exact extent. This is just a degenerate case of things like `&amp;'a Vec&lt;u32&gt;`. You can drop an instance of `&amp;'a Vec&lt;u32&gt;` whenever you want, or even not create it at all, but you cannot have one outside of `'a`. 
&gt; So does adding newlines for /*, unless I'm misunderstanding what you mean. But newline adds modification only in two places: where I've added `/*` and where I've added `*/`. Everything in between stays untouched. While prefixing everything with `//` modifies every single line. &gt; Looks like it's just a couple of keystrokes in nano Few keystrokes that I need to google, as I'm not a nano user. That's my point. Even being Vim user, I don't know how to do that in Vim, from the top of my head. I never have to, as it's purely a problem of languages that put superficial form over substance. &gt; See my large comment downthread about simplicity. Found it. Nothing convincing, sorry. I do agree that blocks of `//` look better, consistency is nice, etc. But these are all not real arguments, just preferences. If Rust were to remove block comments, I would went on a hunger strike in front on Mozilla's office. :D I think I'm going to put couple of block comments in some of my public projects, now, just to make sure any attempt in the future would break even more code... Just to be sure.
Its a method implemented on the trait objext, not the type. Why do you want a default impl here? Default impls apply to concrete types, not trait objects, and you're trying to treat them as such.
&gt;I'm going to see what I can do to help implementing my recently accepted RFC. FWIW, I have it 2/3 implemented. I need to get my shit together and finish all the tests etc. It's pretty boring to implement, mostly a copypaste from the existing stability pass.
The author has benchmarks in the blog posts listed in the Readme. Not exactly bad: https://m4rw3r.github.io/parser-combinator-experiments-part-3/
I know it's an old post, but here is an example why HKT would help: http://www.sparxeng.com/blog/software/an-example-of-what-higher-kinded-types-could-make-possible-in-c It's written in C# (which does not have HKT). My understanding is that, HKT removes code duplication... Example: List&lt;T&gt; // List is an actual type and T is a "parameter to an actual type" T&lt;List&gt; // T can be anything (HKT) that takes a parameter List So you go 1 level up. That means you can write some very general code and apply them to many things. Instead of writing the same code multiple times with specific variations. Note: I probably know less about this than you, so please be skeptical of my interpretation.
Cool. Can you link your branch so I can at least review?
I believe [this post](http://aturon.github.io/blog/2015/09/28/impl-trait/) represents the latest on impl Trait.
 take_while take_while1 Not sure how I feel about that naming scheme. Edit: nom uses those names too. Hm. Odd.
I believe the [owning_ref](https://crates.io/crates/owning_ref) crate will do what you need here. The other option is just to use Rc here if you are only moving the data in once, the over head should only be one add and a word per intermediate container. That all said it's probably easiest just to return the structs you need and let return value optimization take care of the rest. Most rust programs I've written tend to want this pattern because it makes ownership much easier. You tend to do some initialization to build your data then hand out reference of it and its internals to functions and other data structures. This makes it easier to move all of the values at once and makes mutability and ownership much easier to manage. If you need extra speed for this type of work the emplacement api is coming that will help guarantee that your data is constructed where it needs to be rather than constructed then moved or copied. 
Yea, you might be right. But it's still an issue I'd like the authors of the proposal to address and think of :)
Well, I asked exactly because there's not so much stuff to make up a whole separate digest. Besides, sticking to existing digest would help exposure. I understand if you'd like to avoid too many language-specific links there, though.
I'll have a look at owning_ref, thanks! I'd like to avoid passing back values. The structures are created once, then only processed using references, so it would be a bit silly to recreate and return them every time. There's no mutability either - whole api takes only references. I'm not sure what you mean by value optimization. Things like vectors wouldn't magically compress 2 identical immutable values into one memory location... you mean something else, right? Edit: owning_ref::BoxRef looks exactly like what I wanted!
Build scripts already allow you to make a complete mockery of the idea of repeatable builds. Just whack in a reference to `rand` and *bam*, the reproducibility is gone.
Finally released [`wayland-client` 0.4](https://crates.io/crates/wayland-client). It now takes advantage of type information newly provided in the XML wayland protocol files, thus avoiding the need for users to cast all enums into u32 to use them ! Following that, I thus released [`wayland-kbd` 0.3](https://crates.io/crates/wayland-kbd), newly updated to the new iterator-based scheme for events handling. (Because callbacks are a nightmare in Rust, while iterators are shiny and loving.) Now, I need to update `wayland-window` to this new scheme, and then I'll be able to tackle to big work : update the wayland backend for `glutin` using these crates, and finally make it usable (and activated by default !). There are still a few things I'm not very happy with: - items generated by the `bitflag!` macro are not scoped, and it causes me some trouble, forcing me to create submodules holding the bitflag struct and its associated constants (talking about generated code here !) - In a plan to handle protocol extensions of wayland (they are going to be very mainstream), as using the generator, adding them to the bindings is almost free. However, the event iterator need to distinguish between the events of various protocols, and adding a new variant to an enum is a breaking change... I'm not decided about how to handle this...
Very excited about this! Thanks /u/nick29581 for your work figuring this stuff out - it's a really tough problem. How much inspiration are you taking from other languages with strong macro systems, like Racket for example?
Any tips for encouraging autovectorisation?
I thought the same at first, but once I realized what they represented I guess I'm decently ok with it. Alternatives being overly verbose, or just as cryptic (`take_while_one_or_more`, or `take_while_oom` repectively...)
I highly recommend Andrew Gallant's excellent blog post: * http://blog.burntsushi.net/rust-error-handling/ which has now been merged into the Rust Book: * https://doc.rust-lang.org/book/error-handling.html It introduces all kinds of techniques to get the code tight and easy to read.
The main rule is "make the code simple". More specifically: - try to write everything to be vectorised as loops over values contiguous in memory (e.g. `&amp;[T]` and `Vec&lt;T&gt;` instead of `Vec&lt;Box&lt;T&gt;&gt;`, `HashMap&lt;K, T&gt;` or `BTreeMap&lt;K, T&gt;`), stepping forward by one element at a time - ensure all function calls inside the loop body get inlined away (this likely means no trait objects/virtual calls) - make each loop iteration as independent as possible, or, if not, ensure the only dependencies are simple (a commutative and associative reduction, such as multiplication, integer addition, maximum or minimum). A simple reduction means it can be done independently in each lane of a SIMD register, and then at the end, the reduction can be performed horizontally on all the lanes, to get the final answer - consider unrolling loops to reassociate/rearrange floating point additions, since the compiler is unlikely to be able to do it automatically. Floating point addition isn't associative, e.g. `(1 + 1e100) + -1e100 != 1 + (1e100 + -1e100)`, so it can change the final answer to change the order of operations, and changing the result is something optimisations shouldn't do. For instance, for `a: &amp;[f64]`, the naive summing loop will essentially look like `((a[0] + a[1]) + a[2]) + a[3] + ...` but can be vectorised much more efficiently if written as `(a[0] + a[2] + ...) + (a[1] + a[3] + ...)` (i.e. have each iteration of the loop handle two elements, summing into separate accumulator variables) - use primitive types - avoid memory operations that depend on the values (i.e. don't index an array with them) - generally avoid branches, although floating-point `abs` and min/max are fine - complicated operations like the built-in `sin`/`cos` rarely vectorise well (it is possible to write vectorised implementations, that compute the sin/cos/... of multiple values at once, but the built-in ones aren't this) - do an array-of-structs (`Vec&lt;(f64, f64&gt;)`) to struct-of-arrays (`(Vec&lt;f64&gt;, Vec&lt;f64&gt;)`) transformation so that the same operations are being performed on values that are adjacent in memory
Yeah, you can cast `&amp;T` to `&amp;Trait` only if `T` is a concrete type.
I believe the vectorizer is already pretty eager so it's more about structuring your code so that vectorization is possible to begin with. There's two different kinds of vectorization: performing the work of several iterations at once, and combining similar calculations in a single iteration into a fewer number of vector instructions. It's a little bit technical and focuses on C++, but LLVM's documentation on the vectorizer helps give some insight into the kind of cases it can optimize: http://llvm.org/docs/Vectorizers.html#features (I can't say for certain whether it can do all these optimizations on IR generated by `rustc`.) Generally, the vectorizer is pretty good at optimizing loops as long as they don't abuse control flow too much or have too many side-effects. If you're just performing some calculations in a tight loop, LLVM will probably vectorize it without a second thought. If you're printing to stdout and inserting elements into a `HashMap`, some sections might be vectorized but most of them probably won't be, because each element can trigger entirely different behavior. I created a sample of a few different functions which vectorize cleanly: http://is.gd/gq0axi If you select "Release" and then hit "LLVM IR" and search for the function names, you should see under each a line that reads: br label %vector.body That's a clear indicator that the function was vectorized, and in fact in each `%vector.body` label we can see operations on what is effectively an `i32x4`, for example in the vectorized loop for `sum`: %5 = add &lt;4 x i32&gt; %wide.load, %vec.phi %6 = add &lt;4 x i32&gt; %wide.load13, %vec.phi11 I'm not quite sure what those operands are, but `add &lt;4 x i32&gt;` is definitely a SIMD instruction. 
ML-like Rust sounds interesting where can I have a glimpse of what it looked like? Are there any documentation / code snippets left?
It's called "polygon" on http://crates.io, I'm going to be implementing it for `IntoIterator&lt;Item=[f64; 2]&gt;` and `IntoIterator&lt;Item=(f64, f64)&gt;`. Was wondering if anyone could check my trait signatures and derived implementations before I go through the hastle of doing the implementation to find it doesn't work. Edit: also, are there any other things you'd like to see. Scale/Enlarge comes to mind for resizing (obviously with derived scale_about, scale_x and scale_y). Edit2: Spotted that if I add a shadow method to the required collides_with can be moved to derived. Also the return type of collides_with should be bool.
Whoops you're right storing the values won't help you here. That's I get for trying to answer a question while falling a asleep. 🙂 However some of what o said can help you here. For tree and graph structures it's often easier ( and faster ) to have a linear place in memory to store all of the components. You can use an arena to allocate you values then put the refs in your data structure. If you want even more flexibility you can store the parts of the tree on a vector and store the indices to hold the shape. Nick Cameron has a [great article]( http://featherweightmusings.blogspot.com/2015/04/graphs-in-rust.html?m=1) about these two strategies for building graphs and the Rc strategy. 
I thought so. Cool.
Why use `macro!` instead of just a plain `macro` keyword? As far as I can tell it [is reserved as a keyword](https://github.com/rust-lang/rust/blob/master/src/libsyntax/parse/token.rs#L601), and I don't see any reason to add the `!`. From what I understand `macro_rules!` uses it because it is itself implemented as a macro, but this is an implementation detail of the compiler that regular users don't care about and should not seep out into its interface, IMO.
That would be awesome! Do not hesitate to ask if you have questions or want me to add stuff to Chomp :)
I have to thank /u/saposcat for the name, he got the idea while we were talking parser combinators on irc. 
Does this mean that there would not be a size limit for (say) `PartialEq` implementation of slices, and those of tuples?
If it is sensical to provide a ReadTransport, then implement the trait directly. The user knows the type of the objects they are passing around in most cases, and a wrapper struct is just confusing. Have a look at how Iron does it.
No, you think of type-level numbers, which have nothing to do with macros.
Nice! I had actually assumed it was a play on words, like "chomping away at bytes" ;)
These changes don't seem to address my biggest gripe with existing procedural macros/compiler plugins/whatever you want to call them - there's no access to type information. This is the biggest source of ugliness in the implementation of `pnet_macros` - it's really difficult to generate correct code when you have to guess about types based on their names (you end up generating incorrect code and giving horrible error messages to the user if they make a mistake). You also end up having to add lots of extra annotations to specify types for things when it should be possible to derive them automatically.
Is something like generating such implementations during compilation, based on usage, planned?
Yes, so your method is supposed to apply on trait _objects_ not on trait implementors, and the direct impl does that. You won't need to reimplement it. https://play.rust-lang.org/?gist=2b55fff6544ea51aa7e6&amp;version=nightly The difference here is that `Bar` is a concrete type, _not a trait object_, and it doesn't make sense to transmute it to the raw representation of one. However, trait objects like `Box&lt;Foo&gt;` or `&amp;mut Foo` which are obtained from something of type `Bar` are fine. What are you trying to do here, though? Why do you need to transmute to `raw::TraitObject`? You shouldn't be transmuting anything other than a trait object to that.
I tend to agree. Scala macros have typer always accessible and you can call it on some intermediate AST or get the type of a symbol if you want to get the types. E.g. https://github.com/umd-mith/banana-utils/blob/master/prefixes/src/main/scala/prefixes/Prefixes.scala
That would no longer be a macro, it would be a compiler plugin. That might happen in the future, but it's not on the cards in the near term. The difficulty is that to have type information you have to operate much later in the compile process and none of the data in the compiler is meant for consumption by anyone other than the compiler. If you changed the HIR (the AST no longer exists at this stage) you'd also have to change a whole bunch of side-tables in non-trivial ways. A possible solution is that a 3rd party library can try and type generated AST, but since the rest of the program may not be parseable, let alone typable, you might have problems.
Racket's namespacing is probably more than we want because of the way it works across multiple phases and so forth. It also isn't an exact fit with the Rust module system. The hygiene systems they use do work across both systems though, which is useful! Yes, the scope sets work is the hygiene algorithm I'm proposing to use in the blog post.
&gt; http://blog.burntsushi.net/rust-error-handling/ Agree there's too much going on - 3 things really. The receive, the insert and the call to ready(). Even extracting those parts into functions though still leaves a lot of Option handling. A mixture of combinators from torsten_scheck's comments and the early returns looks to be the way.
Wouldn't you have to box the vector which heap allocates it anyway? Or are we able to return unboxed traits now? Edit: being a moron, they now all return `&amp;Iterator&lt;Item=T&gt;`
It seems to me like everything that makes the core language smaller is going to make the error messages more confusing. I.e. makes the language developer's life easy at the expense of users. Are there any indications that people are planning to re-implement Rust? If not the whole "re-use the macro library" point is kind of moot.
A Vec is basically struct Vec&lt;T&gt; { data: *mut T, length: usize, capacity: usize } where data is a pointer to the beginning of a heap allocated sequence of T's. There is no need, nor is there any point, in heap allocating a Vec. I'm not sure what the part about returning unboxed trait objects refers to. 
So, I have a couple of questions: * How long is this internship? * Is it a remote internship? If not, where should we be located? * Can someone still studying for their Master's degree in CS apply?
Similar to transforming javascript callbacks -&gt; promises you can use the methods on `Option` and `Result` to compose each of these steps instead, something like fn accept_udp_connection(&amp;mut self, event_loop: &amp;mut EventLoop&lt;MioServer&gt;, token: mio::Token, events: mio::EventSet) { let mut buf = Vec::&lt;u8&gt;::new(); let new_tok = self.udp_server .recv_from(&amp;mut buf) .map_err(|e| format!("Receive failed {:?}", e)) .and_then(|addr| addr.ok_or("Socket not ready to receive")) .map(|addr| self.udp_transactions.insert_with(|tok| UdpTransaction::new(addr, tok, buf))) .and_then(|new_tok| new_tok.ok_or("Unable to start new transaction. Add to slab failed.")); match new_tok { Ok(new_tok) =&gt; { println!("Token for this transaction is {:?}", new_tok); self.udp_transactions[new_tok].socket_ready(event_loop, token, events, &amp;self.udp_server); } Err(e) =&gt; { println!(e); } }; } Basically converting every possible failure condition into a string error in the final result. It could be nicer if there were a method that consumed the result like a match (`match` below) and one that ran a block only if the result were an error or not and ignored the return value (`tap` below, it would need the counterpart `tap_err`, basically `fn tap&lt;F&gt;(self, op: F) -&gt; Result&lt;T, E&gt; where F: FnOnce(T) { self.map(|a| { op(a); a }) }`). That would provide a more consistent looking method like: fn accept_udp_connection(&amp;mut self, event_loop: &amp;mut EventLoop&lt;MioServer&gt;, token: mio::Token, events: mio::EventSet) { let mut buf = Vec::&lt;u8&gt;::new(); let new_tok = self.udp_server .recv_from(&amp;mut buf) .map_err(|e| format!("Receive failed {:?}", e)) .and_then(|addr| addr.ok_or("Socket not ready to receive")) .map(|addr| self.udp_transactions.insert_with(|tok| UdpTransaction::new(addr, tok, buf))) .and_then(|new_tok| new_tok.ok_or("Unable to start new transaction. Add to slab failed.")) .tap(|new_tok| println!("Token for this transaction is {:?}", new_tok)) .match( |new_tok| self.udp_transactions[new_tok].socket_ready(event_loop, token, events, &amp;self.udp_server), |e| println!(e)); } 
Usually, we've managed to also have features that mitigate bad error messages, like the `rustc_on_unimplemented` attribute. &gt; Are there any indications that people are planning to re-implement Rust I'm not sure how serious anyone is, but it's asked about pretty often. There's lots of reasons why such a thing would be really great to have, it just takes a lot of time and resources.
Since you say that the objects will live forever, maybe you can try using [lazy_static](https://crates.io/crates/lazy_static) crate? (But beware, the `get()` method has some penalty. Fortunately, it returns `&amp;'static T`, so you can manipulate just those). You can also combine it with [`Cow`](https://doc.rust-lang.org/nightly/std/borrow/enum.Cow.html) -- you can store the heavily reused leaves as `Borrowed(&amp;T)` and keep the less common or top-level as `Owned`. Note that in `Cow`, the reference in `Borrowed` variant could be static (nice for combining it with `lazy_static`), but does not have to (you can allocate the commonly used schemas eg. in the beginning of `main`). Oh, and instead of using `lazy_static`, remember that you can also transmute `Box&lt;T&gt;` to `&amp;'static T`. (there is even an RFC somewhere to stabilize that as a safe method).
Ah, Tendril is a nice library, sadly not directly compatible with either Nom or Chomp from what I can tell since both work on plain slices. Though with Tendril you do need to copy at least once (ie. fill some buffer, give a reference of that buffer to Tendril and then tendril copies it into the heap), with Nom and Chomp you can work directly on the buffer if the format allows for it. Would be interesting to see how much I would have to adapt Chomp to get it to parse Tendrils, but I suspect it might be too much of a change (ie. completely different types inside of `Input` and `ParseResult`, totally different contents of parsers, at least the combinators should still work with very minor adjustments).
If all of the `Object` implementations *must* be intersectable, what advantage does this have over: trait Object: Intersectable /*+ AnyOtherNecessaryThings*/ {} impl Object for Sphere {} impl Intersectable for Sphere { // ... }
&gt; async IO library or some other high performance concurrency/parallelism solution https://github.com/carllerche/mio &gt; big successful software made with Rust - but maybe it's too early for that. Yes, it is. It's not even been a year since 1.0, and most big successful software is closed source behind closed doors so you don't get to hear about it till it's finished. Quite a few startups are using Rust, and Dropbox is doing stuff with it too.
&gt; It seems to me like everything that makes the core language smaller is going to make the error messages more confusing. Did removing greenthreading make error messages more confusing? Did moving `~[T]` to `Vec&lt;T&gt;`? I don't think there's a clear relationship here. What specifically makes you think macro error messages would be more confusing if macros were implemented as a syntax extension instead of built-in? If the interface for syntax extensions to report errors isn't good enough, that's the problem even if macro! isn't a syntax extension. A smaller language with a larger standard library is easier to learn and reason about, because the primitive concepts of the language are fewer. And being easier for language designers to implement has large payoffs for users, because it will be easier for them to design a good language. There are a lot less opportunities for layering issues, leaky abstractions, and other design problems if the macro! DSL is across a defined, stable boundary from the compiler.
&gt; I was researching Iron web framework but from what i see it's really slow for real world examples that include IO (db) operations, comparing it to how fast Rust in general is/should be. Go is a lot faster than Iron. Iron is much [slower](https://github.com/iron/iron/issues/396#issuecomment-149060830) than [nickel](http://nickel.rs) or using [hyper](http://hyper.rs/) directly at the moment. (Be sure to compile with optimisations, too.)
This is a good idea! Have you submitted a PR to rust.vim?
I think https://github.com/dpc/mioco is it? I don't know, but lots of folks are using mio for a myriad of things, so I'd assume these things exist.
That's correct, I have a pending PR that makes it zero-copy if you're willing to use the set_len unsafe function https://github.com/servo/tendril/pull/22
I wrote it can be subjective opinion because i made it after reading other people opinions, not my own experience. Mostly this was other people problems, from what i remember it was about IO in general, that even sync IO needs to be polished, that standard library for IO have basic operations only, that there was a lot going on with changing concepts in last year (or maybe it was opinion about pre 1.0), with changing internals etc.
It looks promising, thx
All of the problems that you listed are missing libraries; making the version 1.0 was intended to signal that arbitrary breakage in the language was over and that the time to build that stuff has come.
Speaking from my own experience as an intern on Rust this year, I don't know if the experience will be the same in future, but I would really hope so: &gt; Is it a remote internship? If not, where should we be located? I was working in San Francisco, and Mozilla provided a lot of assistance with the relocations (flights, accommodation, visas if required etc.) for pretty much all the interns, including interns from outside the US like me. &gt; Can someone still studying for their Master's degree in CS apply? I interned while doing my Masters (in statistics). --- It was a really great time, and I encourage anyone who is remotely interested/slightly eligible to apply.
It depends how you look at it, from what i've read async io/green threads were part of the Rust language in design some time ago but were removed. I am not an expert in programming languages but i always thought that in most languages things like for example green threads (in Go for example) or other concurrency/parallelism primitives in other langugaes are part of the language, and it's hard to call them libraries. In this context i would not necessary say that all my concerns were about libraries. After all it was a part of the language in Rust design.
&gt; that there was a lot going on with changing concepts in last year (or maybe it was opinion about pre 1.0) It was likely the pre-1.0 opinion, Rust was very shifty back than. &gt; that standard library for IO have basic operations only Yeah, there's a politics (which grows from the Stability Guarantee) to let complex things mature out-of-std before merging them in. That also applies to things that might have different interfaces and/or implementations, in order not to "bless" a design that might not be the best in all cases. So there isn't, say, a native `mmap` interface in the standard library. But in the Rust ecosystem it isn't considered a problem, you just use an external crate (such as https://crates.io/crates/mmap).
Which brings up a question I've had in my mind. Is the "LRBWETMLL" still being worked on? It seems there is still a few sections incomplete. 
I had to take the last 3 months off of basically everything to write my entire thesis from scratch. I have two weeks to finish my thesis, after which I have a brief ~3 week window to do whatever I want. I hope to clean up Lists and Nomicon in that time. Depending on... things... I may or may not be able to continue working on them afterwards.
Rust is a lower-level language than Go and it has a much smaller runtime - no garbage collection, no scheduling mechanism, etc. At one point this was not true: there was a time when Rust had both green threads and garbage collection. But in Rust as it stands today, these are all library concerns. The language you should compare to is C++, in which these are also not a part of the language.
Ah no pressure, a thesis certainly takes priority. I've been meaning to start the linked lists, and thought I'd just wait a bit longer until I saw it completed. I'll probably start on it this winter.
It doesn't make sense to mix file operations w/ non blocking network operations. The other stuff (thread pools / multi-threaded event loop) is pretty use case specific. I can't really think of a good use case to have a multi-threaded event loop built into the library.
Shifting the unsafe keyword to a new syntax item, rustUnsafeKeyword, would be acceptable, but it’d still be `hi def link rustUnsafeKeyword rustKeyword`.
I use bold for mine &gt; syn keyword rustTodo unsafe
The AVX-512 variants of scatter/gather take a mask that determines which lanes of the vector are enabled, so if your code is predicated in any way then you can get a substantial performance increase.
No, i know better what i meant. I meant in languages like for **example** Go, for **example** green threads. Maybe phrased it wrong, english is not my native language. Maybe that clarifies this bettter. I have corrected that comment now. I meant concurrency/parallelism in general as part of the language, in different shapes. And of course not in all languages. But Rust had that in it's design that's why i mentioned that in this way. I hope it's clear for you now what i meant.
For 1, I am currently working on a post to clear that up (in more of an embedded context, but I've already done a dive on linking and might as well extend it to work for everyone). But from I can see, there's a `-nodefaultlibs` passed to `cc`, and then no explicit reference to the C++ standard library. Hence the undefined references to `new` and `delete`. Wait, no I lied, there's two copies of `-L "dylib=stdc++"` being passed in later.... I have no idea why that's failing then....
I don't know much about ray-tracers. If all objects have the same property then I agree that there is no need to use an object trait. If you want a list with dissimilar objects, some of which implement a given trait `Foo` then you can create a trait `Object` which must be implemented by all the members of the list. trait Object: Intersectable { fn as_foo(&amp;self) -&gt; Option&lt;&amp;Foo&gt; {} // .. more methods here } I tried to make the point that you almost never need to use the Any-trait, but maybe I was unclear....
Ah, gotcha. Well, the beauty of ray tracers is that they don't need to know how to draw anything in particular; they just kind of detect whatever's in front of them. So, the only thing they need to know is how to hit the object with a ray, which is completely trivial for any type of shape I'd be interested in supporting. Rasterizers, on the other hand, need to know how to draw each particular shape. This will require different data for each different type of shape, along with a different technique (that will vary from rasterizer to rasterizer). Some rasterizers won't be able to support all types of shapes, either (for instance, an OpenGL-based rasterizer wouldn't be able to draw a "perfect" sphere or vector shapes other than polygons). For this reason, I thought it most elegant for the rasterizers themselves to handle all of that by implementing drawing routines for whichever shapes they support. 
Yes, but I hear we're trying to make monomorphisation less greedy in the long run?
&gt; I have read that you want to attract people coming from JS,ruby,python etc. In this context, we don't necessarily mean that we want people to use Rust as a straight-up replacement for those languages. Instead what we mean is that we want people coming from dynamic backgrounds to be able to effectively wield a systems language without having to worry about the fact that they don't have thirty years of systems programming experience. IOW, we want Rust to be able to supplant other systems languages for the kind of things that, say, Python programmers need systems languages for (e.g. writing native code language extensions, which would typically be written in C). We also hope to make the broader domain of systems programming accessible to a whole new audience of people who may not want to put in the effort needed to write bulletproof C code.
That would have to probably be a compiler plugin that generates HIR from type info - which, among other things, means you don't even get to use paths, but you have to resolve everything yourself, and the rest of the code can't refer to items you've generated, at most use your impls. Also, one problem with adding onto HIR is coherence - while the existing impl coherence is relatively simple, what happens when you've added new impls that can change the outcome of inference? Should intra-function type-checking/inference run after such plugins? Probably, given how you also need to use the new impls from existing functions. But that means you can't refer to any of your generated types or impls outside of the function bodies. A potentially saner way to do this is have both a procedural macro and a compiler plugin: the macro creates the skeleton of your impl (using something like `impl Trait` for associated or field types and empty bodies for methods), and the plugin fills in the gaps before any compiler pass can make a bad decision based on the lack of information. I can see that working for generators and `pnet_macros` (if my understanding of what it does is correct), at least.
Despite being data race safe, Rust has no concept of concurrency at the language level. Its all in libraries. Also, while Rust doesn't have greenthreading, Rust _does_ have strong support for system threads in the standard libraries, which are a concurrency mechanism. Even if they aren't the absolute best concurrency primitive for your particular use case.
The way I would expect it/want it to work is to have the compiler do everything except codegen, so I have some form of AST/IR which has had full type checking etc done (similar to what lints get access to, I guess), then manipulate that, and have the whole process start again - whether that's through generating Rust source and parsing it, or going directly to some AST. This kind of recursive invocation is incredibly powerful, and is essentially what D does (though it doesn't expose any kind of AST - you have strings/templates/compile time function evaluation/mixins, which you can use to analyse/manipulate the code).
[removed]
Concurrency/parallelism in general is part of the standard library (channels, threads, signaling). Green threads, on the other hand, aren't. I'm not sure why you'd want to jump to green threads when real multithreading is built-in and is much safer to use compared to other languages, thanks to rust's type system and compiler. I looked and saw a coroutine library (https://github.com/rustcc/coroutine-rs) but like many things, it requires rust-nightly.
I just started a degree in Computer Engineering (Spain) this year. While I have a bachelor's degree already, I suppose I can't apply, right? As I have no PhD yet nor am working on a Masters.
I saw your reply just now, sorry... Which OS do you have? I tried both 1.6 nightly and 1.4 stable, with no luck...
From what I can gather in the issue discussion, it seems that the approach that hyper is using to handle a request is running into a problem, and that switching to an acceptor thread and work queue would be a better fit. Is this something that mioco could provide? Or would the hyper contributors have to build something from scratch? 
Yep, that's what I'm falling back on, though it is very annoying that you need to build your own adaptor to return an owned stack allocated iterator.
That sounds like an awfully inefficient process, and it would *not even work* because the items you're creating won't be there the first time around, so any code depending on them will fail to compile.
How would you go about storing the information inside the AngleIterator?
Good one. Here's where it can end up a bit inconvenient. If you had a simple slice, then iterating will return a `std::slice::Iter&lt;'a, f64&gt;`, so then you will end up with `struct AngleIterator&lt;'a&gt;(std::slice::Iter&lt;'a, f64&gt;);`. But if you have used map() and other functions, the type will be a bit more complicated. Which leaves you with two options; use the complicated type (let the compiler tell you what type it expects and copy in its answer), or do the map yourself when you implement `next` for AngleIterator. 
And `env!`.
I was hoping to keep it as flexible for the implementor as possible, and so although I will be using mapping for the implementations I do, I want to keep it flexible, for example a polygon is commonyl defined as a list of points, but it can also be defined as a list of angles and distances. And so, restricting it to a specific adaptor type would be non optimal. As there's no way to do an owned iterator trait object which would acheive this goal, would boxing the iterator to make a more flexiple api really be that bad? Or returning `&amp;mut Iterator&lt;Item=...&gt;` be good enough (although I'm not sure how that would play with iterator methods that require ownership like `map`, and it feels like I'd get into ownership and lifetime hell trying to do that.
Maybe it could even be made to blink, like the blink tag in HTML4 or so... If I've ever learned anything about people's appreciation of the blink tag, then this should suffice in never having `unsafe` be used unnecessarily.
Other than lack of libraries (which is a temporary issue) I doubt that writing web apps in Rust is any harder than say, PHP. I actually optimized the speed of a PHP application by benchmarking it and actually fixing slow functions in PHP code. Since Rust is compiled it can be optimized ahead of time by the compiler. If less work is needed to fight against performance issues or common errors that Rust eliminates then you could be more productive in Rust.
IMO, it depends on your use case. If you have polygons with millions of points, then returning a `Box&lt;Iterator&gt;` might make sense to save memory. If you just have a few points, then a `Vec&lt;f64&gt;` (as in your original design) should be fine as well. Returning a `&amp;mut Iterator` seems just weird, at least for this use case. Not sure how that would work out in any reasonable way for the implementor.
I think what is missing is abstract struct. UI toolkit require a lot of inheritance and trait require you to rebuild every attributes every time you inherit from them.
Alright, any questions you have left?
I'm busy this summer, but next time.
I don't think UI toolkiy must necessarily require inheritance, many toolkits are implemented this way but it doesn't mean there are no other ways. 
Can vouch for the firefox socks. Best socks in my collection.
I think I'll be good between this and the docs on crates.io. I'm with family today, so I probably won't get to poke around late this evening or tomorrow. Good luck with the blog!
&gt; I found the number of required dependencies for such thing very scary. In general, Rust keeps a pretty small standard library, so to some degree, this is expected. We have a process to move community crates into ones maintained by the Rust team itself, and so I imagine that over time, this won't change a lot, though you may depend on more "official" crates, relatively speaking. Regardless, this is cool! I wonder if you couldn't split out the SOAP stuff into a separate thing, even if it's not fully working, but it could be a beginning.
Yes, but you could not change the standard library to use major new, idiomatic Rust features without breaking backwards compatibility.
For parsing JSON, try the `rustc_serialize` crate: * https://crates.io/crates/rustc-serialize * https://doc.rust-lang.org/rustc-serialize/rustc_serialize/json/index.html Your "on a site" suggests you want to make an HTTP request. For that, use `hyper`: * http://hyper.rs/hyper/hyper/index.html * https://crates.io/crates/hyper And in case you’re not familiar with Cargo and crates.io yet: * http://doc.crates.io/ * http://doc.crates.io/crates-io.html
That chapter is gold.
What do you mean about statics? Are statics `Sync` because of lang internals? That makes sense, I guess. I thought `Send` and `Sync` were only lang items to enable multi-trait trait objects (until that feature is generalized).
Do they do this every year? If there's one place where I'd love to intern, it's Mozilla. I didn't pay much attention to it before Rust, but I'm liking this company more and more. And learning alongside Rust's core team? I'd _pay_ for that :)
This is the result of work that started [Back in may](https://github.com/TechEmpower/FrameworkBenchmarks/pull/1636), and they've finally done another round! A few of us got multiple Rust entries in. So, not a spectacular showing, but not a terrible one either. Having this data over time will be nice. I wonder what we can do to get higher up? There's also a need to write some of the examples; Rust is missing from several categories.
&gt; Should i learn modern C++ knowing that is coming C++17 and from what i read it will have a lot of goodies from Rust, modules, it will be safer etc. Don't hold your breath... - modules: out of C++17, will be published as a TS when Microsoft and Clang agree, no hard date that I know of (the contentious point is how to handle macros in modules AFAIK) - it will be safer: there's talk about lifetime annotations in C++, but I did not hear anything about getting them in C++17 So, no modules in the short term, and no safety either. And C++17 is 2 years from now, who knows where Rust will be then? Personally, I'd choose with what I have available right now and maybe, if feeling adventurous, is coming in the next few months.
This would definitely be a good idea. Monadic parsing may in some ways be easier to use but 95% of the time applicative parsing is enough and provides clearer parsers. I was a bit worried that combine's way of writing monadic parsers would be to verbose and cumbersome but it turns out I only needed it in one or two places. Nice work on pulling your ideas together into a library /u/m4rw3r!
Yep! The posted program is exactly what I did this summer. Same as last year, same as next year.
&gt; I wonder what we can do to get higher up? Non-blocking I/O. This would avoid having to spawn threads and communicate with channels. 
Rust 1.0 was never meant to be "complete" or "mature". It was a milestone for stability, guaranteeing that code you write now will compile with future versions of Rust (up to 1.0).
The only other category I found them in was Plaintext. Not exactly spectacular but there's also a lot of room for improvement. 
**[in Plaintext benchmark]** I am probably missing something, but how can Iron be faster than hyper? I thought Iron was using hyper under the hood? 
I found this suspicious as well.
Nice! Any chance of seeing a [piston hello world](https://github.com/PistonDevelopers/piston-examples/blob/master/src/hello_world.rs)?
I randomly browse this sub as a c# developer and if this is the quality of standard answers, I'm 100 going to dive into rust this weekend! Seems I can get all the help I could ever want from this amazing community :) defintely seems like one of the best programming language subs I've come across! 
I sometimes forget how exceptional this community is. I interact with other lang communities, the way I do with rust, asking questions and expecting nice helpful responses. The difference in response can be staggering. 
I'm not sure when they actually ran the tests, they accumulate updates and then run them every so often. I'd assume they would have been run recently.
But do they accumulate updates manually via waiting for people to file pull requests, or do they allow languages with package managers to automatically update their dependencies without manual intervention? Whether or not the tests were run recently won't matter if they're still using code from May. :P
And here I can't even get Iron framework over 100-ish requests per second thanks to some bug or performance regression related to keep-alive... and that's if I can even compile the damn thing.
Pretty sure what D does is different from what /u/mrmonday described, i.e. it doesn't get all the way up to codegen to repeat the process again, but instead parses the generated strings immediately and integrates them into the compilation.
Yeah, coming from C++ and Java this place is so unbelievably friendly to anyone who has questions. It's awesome.
The first think you want to get out of the way is the bounds checking. You can use some iterators for that for i in 0..self.rows { let sj = i * self.cols + j; new_data[i] = self.data[sj..sj + self.cols].iter().zip(v[..self.cols].iter()).map(|&amp;a, &amp;b| a + b).sum(); } kinda neat!
That's more like `__LINE__`, not `#line`. I need to be able to *set* the line and source file that are used for error reporting, I don't need to get it.
No. That's not very helpful, but it's the truth. The only way to transform source and maintain source locations is to do it *within* the compiler, or as a syntax extension.
I do not. Someone else might, though.
The script should run cargo install, so the latest at the time of running.
Remember the default is ^, so it's the latest compatible, not literally 0.5.2
This seems a little beyond my scope for now but it is a great reference. Thank you!
Thanks for your comment! Though it's great to have a solution I can't claim to fully understand it. I'm reading through the resource posted by /u/neutralinostar to try and get a better idea but I'm afraid I need to understand bounds checking first!
Sure, one of rust's aims is to be memory safe, so when you access an array, slice or vec like this: `data[i]` there is code inserted to check that `i` is in *bounds*. It will look something like this effectively: if i &lt; data.len() { // load the `i:th` offset from `data`'s data pointer. } else { panic!("index out of bounds"); } So the "problem" for a tight loop is twofold - These conditionals impact performance by being executed. This is however a much smaller effect than you might think, due to *branch prediction*! - The conditionals and the risk of panic will inhibit optimizations, for example autovectorization of the loop, i.e. optimizations that can make it many times more efficient. The second point is normally the killer. Fortunately, the optimizer can very often infer that the index is always in bounds, and then it can remove bounds checks. This is what happens for example in the “best” formulations of the loops that I show in the linked forum post. 
Yes it is neat *but*.. :-) As I [address here](https://users.rust-lang.org/t/how-to-zip-two-slices-efficiently/2048) there is a problem with zip. I'd like to describe it like this: Iterators are bounds-checked too. Not the per-element bounds check that you think of, but for each iteration, we have to check if the iterator has reached its end yet, and the loop is finished? That's a “bounds check”. The problem with the codegen for `zip` as it is right now, is that it joins two iterators, and unfortunately for each lap of the loop, it has to do both of the iterator's bounds checks. This would be avoidable if the optimizer could infer that one of them will always be the shorter one, then it could use just one conditional per lap of the loop. This is very important because the two-conditional version of the loop does not fit any loop optimization passes and so it inhibits further optimizations and autovectorization.
Awesome! Thanks for making it clearer.
FWIW, you need to escape "\^", or en-*code* it as `^`.
This is just the regular normal kind of hygiene as we currently apply it to variables, but also applied to type variables (and indeed types and other names too). Totally on the agenda.
Glad to know; `__ThingDecodeParam0` and co. were a *real* pain to deal with. :P
I am interested in solving this problem, but it is unclear how to do so. #line is a preprocessor directive but Rust has no preprocessor, so we can't simply copy the design.
I disagree with the author's idea of privacy hygiene. I think that macros should *always* be equivalent to the same code after running something like `expand_macros` (and suitable identifier renaming). If you can't access `a::f` without a macro, you shouldn't be able to access it with a macro either.
Ok, so all Rust entries scored higher in JSON test than in Plaintext test? I find this a *serious* problem with consistency and trustworthiness of these results. Screenshots of relevant results: http://imgur.com/a/DtLvU
 /: I see, that's a shame. It's impossible to fix it into the std lib? 
If I understand it correctly then the iron test is also using hyper 0.5.2: https://github.com/TechEmpower/TFB-Round-11/blob/master/peak/linux/2015-09-11-final/latest/logs/iron/out.txt
Whoops, you're right.
I don't think it's impossible to fix, from what I understand we want rustc + llvm to optimize this much better and most seem to think it's possible.
I found this very odd as well
i think the magic [happens here](https://github.com/rust-lang/rust/blob/b12a3582b113c0f9c217f6311ec0d47fd0f79016/src/libsyntax/parse/mod.rs#L48). wee need a way to modify the `Span`s emitted here, maybe…
we have `syntax_extensions!()` and `#[attributes]`. attributes either apply to the `#[next_item]` or the `#![enclosing_item]`, so they’re not line based, but AST-based. for full flexibility we’d need to use syntax extensions. appearing on any line they could be used to set the line counter to whatever when encountered.
It may have additional meanings, but isn't it still "danger" ? Is the traffic light "stop" red? Do common Chinese web sites show input errors in red? Do browsers and office suites highlight typos with red?
Yes, sorry, I'd had far too little sleep when I posted that. It's been a long time since I worked on a D compiler, I don't remember how it all fits together. It's probably more useful if I just show some code (you can try it out at http://dlang.org/): import std.stdio; // The first set of parameters are compile time/template parameters, the // latter are runtime `alias v` means some symbol v - I didn't add a // constraint on the function to specify it should be a struct or class, // but could have string makeBar(alias v)() { string ret = "struct Bar {"; // .tupleof gives a tuple of the members // typeof() gives the type of an expression foreach (i, ty; typeof(v.tupleof)) { ret ~= ty.stringof ~ " " ~ v.tupleof[i].stringof ~ ";\n"; } return ret ~ "}"; } struct Foo { int a; } // mixin() allows you to turn arbitrary strings into code - the strings can // come from evaluating functions mixins are allowed to generate mixins - // there's also a non-string mixin which isn't shown here mixin(makeBar!Foo()); void main() { Bar b; writefln("%s", b); } You can do far more interesting things than this, but there's an example which shows off a few possible things. You can also add various predicates based on the types of things. I can write up more interesting examples if you've got ideas/are curious about what the limits of it are etc.
First code snippet under section "Crossroads": impl&lt;'a&gt; From&lt;&amp;'a Blob&gt; for Bar { fn from(blob: &amp;'a Blob) -&gt; blob.bar } Missing return type on the function?
For what it's worth, D has a `#line`, with no preprocessor. It's special cased in the lexer - http://dlang.org/lex.html#special-token-sequence
Great article. Is it weird that I really like these type system features (From, phantom data, Void enums) but have currently no idea where to use them?
No, that's not weird. It's perfectly feasible to write good Rust code without relying on the type system too much (also perhaps you use them indirectly via libraries). However if you write libraries, your users may well use those features in their code. Edit: If you find out that lots of them do, you may pull some of their code into your library to make it easier for them.
&gt; [...] we implement `From&lt;&amp;Bar&gt; for &amp;Blob` and while we’re at it implement `From&lt;Bar&gt; for &amp;Buzz` [...] impl&lt;'a&gt; From&lt;&amp;'a Blob&gt; for &amp;'a Bar { fn from(blob: &amp;'a Blob) -&gt; &amp;'a Bar { &amp;blob.bar } } impl&lt;'a&gt; From&lt;&amp;'a Buzz&gt; for Bar { fn from(buzz: &amp;Buzz) -&gt; Bar { Bar{ num: buzz.num as i32 } } } I think something was mixed up here. ;-) Very interesting read nonetheless. :-)
What happens with `#[derive(Deserialize)]` when the input data doesn’t match the format of your structs? I mostly use `rustc_serialize` to get a [`Json`](rustc_serialize::json::Json) enum out of bytes.
i like it! i’d mention that from is very useful for standard error handling: a common wrapping Error looks something like this (if you’re lazy and don’t `impl std::error::Error`): #[derive(Debug)] pub enum Error { TooLong(usize), Io(io::Error) } impl From&lt;io::Error&gt; for Error { fn from(e: io::Error) -&gt; Error { Error::Io(e) } } then you’re able to simply use `try!()` to wrap another error into yours. fn foobar() -&gt; Result&lt;String, Error&gt; { let foo = try!(some_io_operation()); match foo.len() { 0..12 =&gt; Ok(foo), n =&gt; Err(Error::TooLong(n)), } } --- in fact, that’s so common that i have this: macro_rules! impl_error_wrapper(( $( $wrapper:ident :: $alias:ident ( $wrapped:ty ) ),* ) =&gt; { $( impl From&lt;$wrapped&gt; for $wrapper { fn from(e: $wrapped) -&gt; $wrapper { $wrapper::$alias(e) } } )* }); impl_error_wrapper!(Error::Io(io::Error));
hmm, sure. empty lines don’t need to be mapped to anything it’s sufficient to say “all following items starting with the next one (`i`) belong to line 265 + `n`” (with `n` being the offset the items have to the line of `i`
Thanks. I've just added this (with attribution of course).
By the way, I just remember where I read about some fantastic type system work in `std::collections::BTreeMap`: http://cglab.ca/~abeinges/blah/rust-btree-case/
Nice article! &gt; From and Into are like mirrors of each other. From abstracts over the source type, whereas Into abstracts over the target type. There’s a blanket Into&lt;T&gt; impl for each T with a matching From&lt;_&gt; implementation, so if you are a library author, you should consider implementing From&lt;_&gt; for your types and only resort to Into&lt;_&gt; implementations for types you cannot implement From for because of the orphan rule (which is sadly underdocumented. Error E0117 has the goods, if you want to read further). Maybe something like this should be added to the docs of `From` and `Into`?
Okay, I apologize. But it does annoy me that, when I use macros in any language other than Racket, I get error messages in terms of what the macro expands to, instead of in terms of the abstraction the macro is supposed to provide.
it really is glorious, porting to it as we speak
Cool. But... this here #[allow(mutable_transmutes)] fn transmute_to_u64&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut [u64] { unsafe { let p = self.as_mut_ptr(); std::mem::transmute(std::slice::from_raw_parts(p, self.len() / 8)) } } is an alignment issue waiting to happen. Since you create the Keccak state like this let mut a: [u8; PLEN] = [0; PLEN]; it's not guaranteed to be aligned like an `[u64; 25]`. This is a portability problem because non-x86/x64 platforms might not support `u64` memory access for unaligned memory addresses. You could solve it by replacing it with a `[u64; 25]` and reinterpreting it as `[u8; 200]`, so, basically, the other way around. And then there is the issue with endianness. In the interest of portability you should probably conditionally disable this "little endian based memory hack" and replace it with something else that restores the proper byte ordering for big/mixed endian machines. IIRC there is something like a `#[cfg(little_endian)]` (etc). Cheers!
It's on the list.
Should I count this as a Crate of the Week nomination?
Hi! Thanks for advices. I will fix transmute in next version ;) Regarding endianness, it should be irrelevant cause there are only bitwise operations performed on data.
It works beautifully, my only "complaint" is that if I close the "cargo run" window it doesn't kill (nor ask to) the process.
You're probably looking for the subreddit for the rust game. This sub is for the rust programming language. Try /r/playrust
idk, how do I update ;_;
We are talking about type checking at two different levels: type-checking the final program vs. type-checking macros. Of course, macro expansion must happen before the final macro-expanded program is type-checked. I didn't intend to suggest that this order could somehow be reversed. However, macros themselves perform compile-time computation, whose intermediate values aren't necessarily AST nodes, and it's this compile-time computation that I want to type-check. This can already be done with compiler plugins, but writing a compiler plugin is a tedious process, because: * The compiler plugin must be its own separate crate, so if you're writing a series of normal macros, and only then discover that you actually need a compiler plugin, say, because you want custom error messages, you'll be facing a lot of work just moving your code to a different crate, as well as tweaking things here and there. * In order to test your plugin, you must make a second separate crate, because a compiler plugin can't use itself, of course. So it would be great if procedural macros didn't require compiler plugins.
Your reply doesn't answer his question and detracts from the conversation. Please avoid that kind of behavior here. This kind of nitpicking is starting to be a common thing on the rust subreddit. We can be pleasant, but come on man.
&gt; So it would be great if procedural macros didn't require compiler plugins. With nrc's proposal, they won't. :-) It may be a while before that will resolve everything you've mentioned though; he wrote in his earlier post that at least at first you'll need to tag the crate they're in as containing procedural macros, though eventually that should be reduced from the entire crate to smaller scopes.
No, seriously, *please* don't use names like that. It's not constructive, detracts from the conversation, and has the potential to alienate those who would otherwise contribute to the discussion. As you say, /r/rust is not the venue to voice those opinions.
I guess I'm just not sure if that's actually compliant with the RFC. But 2 seems like a huge pain to deal with from an API standpoint, users of the API will have to worry about state transitions "between" the lines of code. If I can get away with 1 I suppose it's likely the simplest way to do it.
https://github.com/rust-lang/rfcs/pull/16
Whatever solution you end up with will be a good reference for the most idiomatic interpretation of "session types" in Rust, so good luck!
I've never seen that paper until you mentioned it so we both learned something tonight! The macro they present is close to what I thought of when I was dabbling with using Rust's linear types to enforce this stuff, so I would wager that there is a very convergent design lurking (which also suggests that Rust is well suited to this problem).
Also, in Rust, attempting to allocate too much memory can result in a call to alloc::oom(), which (intentionally) tries to execute an illegal instruction. This can lead to an invalid opcode error (which will show up in dmesg), rather than an OOM killer message, but it has the same fundamental cause (attempting a huge memory allocation).
If we've had `winapi`, then naturally we can also have `nix`. :-) Which also brings us to `mio` which hasn't got a CotW yet.
How does Rust know how much memory is 'too much', though?
This is a great video. 
The allocator calls LLVM's `abort` intrinsic, which translates to a `ud2` instruction on x86. You have to be careful when printing an error message in an OOM situation since any printing logic that allocates (as a simple `println!` might) is no good. You'd probably want some logic that calls the relevant system calls directly but no-one's gotten around to writing that yet.
[It's a bug](https://github.com/rust-lang/rust/issues/14674) that rust doesn't print any message on abort.
A vector is always contiguous yes.
Macros let you: * Write an HTML parser which is as readable as the specification itself (https://github.com/servo/html5ever). * Extend the language with regular expressions support (http://blog.burntsushi.net/rust-regex-syntax-extensions/). * Support inline SQL queries that are validated at compile time (https://github.com/sfackler/rust-postgres-macros). * and many other things... Be creative :-)
Thats configurable in sysctl and large allocations also fail in default settings. 
I don't think a vector is guaranteed to be contiguous. It's just a contiguous data structure. It can be fragmented. Does rust/jemalloc make a guarantee here?
Oh you're right, I was only thinking of it as contiguous in the virtual memory address space.
Rust definitely feels well suited to the problem.
So why would the Tcp listener stop working, and not just the stream process (which is what allocated too much)? The main which was looping the Tcp listener didnt quit, which doesnt quite make sense.