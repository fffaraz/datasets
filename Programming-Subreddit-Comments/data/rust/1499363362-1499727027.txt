For 2., do you want to avoid rewriting the whole file to preserve comments and formatting? Or another reason?
Wow, that's a great Tool! I think I'll find a use case for this very soon.
Yeah I know of `impl Trait` I guess. I was also for other reasons wondering if there was a way to nominally write out the type of complex iterator adaptors. For other reasons like writing out types without inference or just linting and restricting it should be possible to write out nominally the full type of an adaptor and I guess all that is needed is to nominally give closures a name. Can you do that? I understand that closures are just a struct which thus has a nominal type which implements one of the `Fn*` traits so can I in lieu of the syntax of `|i| &amp;slice[i..]` actually define a struct some-where like: struct SomeClosure&lt;'a, X&gt; (&amp;'a [X]); And implement the `Fn(usize) -&gt; &amp;'a [X]` trait on it so then I could just use `SomeClosure&lt;'a, X&gt;` inside the nomina type of the adaptor and instead of passing it `|i| &amp;slice[i..]` just pass it `SomeClosure(&amp;slice)`?
&gt; preserve Yes, I need preserve the maximum of content, so `git diff` show only one line change, to simplify review of my pull requests.
&gt; Whether transpiling Java or writing a JVM, reference counting isn't quite going to cut it, since Java's garbage collection can handle cycles. Yeah, I guess I was thinking that you could have a "Transpiles to rust but with some caveats" like reference cycling being an issue.
I'm working on a lossless TOML parser that will do just that, then. Down the line, I'd like to add the functionality curently in cargo-edit into cargo itself.
Here is a list of alternative licenses compatible with GPLv3: https://www.gnu.org/licenses/license-list.en.html#GPLCompatibleLicenses. Notable mentions being Apache v2, Boost, Modified BSD and Mozilla Public License.
They're very commonly used on windows. No ide of OSX.
I don't see a lot of memory usage, but Chrome goes nuts with CPU whenever I hover. If I move my mouse around that lil area, I see like 170% in `top`.
Now all you need is a cross-platform way to read one character at a time (with optional timeout), and we'll finally have a proper cross-platform curses-style lib. 
That is a strange issue. Do u have a screenshot?
Nice! I'm writing a parser and this will come in handy!
I've pushed my example here - https://nest.pijul.com/shell/pleingres - it uses the mopa crate and an autoderived trait to create a `Box&lt;Request + HandleRow&gt;` equivalent that can be converted back into its concrete type at the end of the `SendRequest` future. The single use of `unsafe` is fine since we know that the type will be the same, but it could be replaced with `*try!(r).downcast().unwrap()` if you want to be doubly certain and avoid potential weird internal bugs. Unrelated: I tried to create a branch in Nest to push this to but didn't really understand what's going on. I also may have broken my account slightly by creating a repo which I can't delete in my poking at it. Aaand I'm not 100% certain why my patch doesn't appear in order in the patches list. Aside from that, pretty cool, I'll watch out for Pijul being in a more stable place, as I love darcs and it seems pretty similar.
I like this idea a lot. Our profession has a real problem with massively overengineered architecture tools, but artifact is like fresh summer rain.
Are you sure that quick-xml only supports UTF-8 encoded data? I had the impression it was more flexible than that.
I think cargo is missing a really critical feature - the ability to collect and report the licenses (and any additional notices) for the crates being pulled in. Ideally, each crate includes a license document and a use notice, referenced from the Cargo.toml. That way, reviewing the terms of included components is straightforward and collecting the use notices for included crates that need to be distributed with applications using those crates is nearly automatic.
great
Yes, this is why libraries should be ~~LGPL~~, MIT or Apache license.
Yes, I believe that would also work. But it is a huge amount of boilerplate, and at that point you might as well implement `Iterator` on a custom struct and return that directly, rather than implementing a custom `Fn`.
yes I do get that efficient stack uses remains the #1 priority; My observation, however, is that the type is written far more often as *a parameter* (1) than as a declaration of a variable: I think what I'm really after then is something like this.. type Foo = &amp;struct{ ..... }; .. then a means of saying 'UnRef&lt;Foo&gt;' when I do need to declare a return value / local. (most locals are inferred though) I realise i can do that right now by declaring ```Foo_s, Foo=&amp;Foo_s```, but the question is 'could this be done without needing to increase the number of idents' - use types which are computed from each other rather than manual labels/(breakable) naming conventions (although i did used to write Foo,PFoo a lot back in C..) I wonder if it would make sense to 'pattern match' type-params... imagine something like.. type MyVertex1 = &amp;struct{....} type MyVertex2 = &amp;struct{....} struct Mesh&lt;&amp;VERTEX&gt; { vertices:Vec&lt;VERTEX&gt;, triangles:Vec&lt;[Index;3]&gt; } Mesh&lt;MyVertex1&gt; , etc note (1) ... the declarations are important since those are what you read most of the time .. looking through docs etc. i can see if struct names were refs it could start to get confusing in the reverse scenario of declaring 'vectors of..' etc 
Yeah, I guess a nice thing about returning `SliceTails&lt;'a, T&gt;` instead of `impl Iterator&lt;Item=&amp;'a [T]&gt;` is that `SliceTails` is now its own struct with its own documentation you can write for it. You an also do things like `mem::size_of::&lt;SliceTails&lt;i32&gt;&gt;()` to get the size or something like that you can with the other one. But yeah I guess the easiest way is to just have `struct SliceTails&lt;'a, T&gt; ( &amp;'a [T]);` and implement the Iterator on that which won't look nearly as clean as `(0..slice.len()).map(|i| &amp;slice[i..])`
 cargo run --example quick_xml data/ISO-8859-5.xml Text: "" Declaration version="1.0" encoding="ISO-8859-5" standalone="yes" Text: "\n" Start: text Utf8Error(Utf8Error { valid_up_to: 5, error_len: Some(1) }) So yes, it fails to convert `[u8]` to `str`. And this is not a quick_xml error. But it doesn't handle encoding by itself. I'm not sure that it can be called "Supported". 
While it's not included with cargo, there is a 3rd-party [license subcommand](https://github.com/onur/cargo-license) which does what you want. It even tallies them up into "&lt;license_name&gt; (&lt;crate count&gt;): list, of, crate, names" lines for easier reading.
Given how Rust links everything into a single binary by default, the LGPL's "must provide a way to swap in new versions of the LGPLed code" requirement devolves into a very GPL-like situation. (Typically, it's done either by dynamic linking or by distributing the `.o` files produced prior to the final linking phase along with the binary.)
&gt; This means you can mutate the name, such as by reassigning it to a new value. Can you show me some example code? I've never seen a mutable label before.
Is this because rustup likes to download binaries?
The perpetrator was sent to room 101 for rerustification.
Your example is using `str::from_utf8`, so of course it fails if the data isn't UTF-8. However, quick-xml doesn't *require* the data to be UTF-8 (it works on raw byte slices) if you handle it with care. Other libs might use `String` / `&amp;str` internally so they can't cope with non-UTF-8 data.
trying out diesel with sqlite and dotenv, and i am putting my database code in a child package which will keep the dependencies organized. i'm new to these packages and creating child packages. i set the db file env path to current directory, which ends up putting it in the child package's directory rather than the root directory. i'm trying to understand how this works when i eventually want to compile a release binary: will the db file end up in the root/working directory? do i need to manually specify parent path to root in the .env like `../database.db`? should i instead pick a static location? is the .db location easily configurable on release (does .env sit around for users to touch)? will it find the database if the program is called from different working directories? edit: i played around enough to actually get my modified-from-the-example code to compile and found out a couple things. when running from the context of the parent package, the .env and .db need to be in the root, but i have to create/move the .db in the child directory to run migrations and then return it to root. i'm starting to think that this just isn't going to run smoothly unless i move the database code to the root level as a module instead of abstracting it as a separate package. i wanted to do this so the database functionality could be left out of compilation as a feature. i guess for now i'm going to ignore that.
That's a small part of it, but it's a good start.
I don't use antivirus software so I'm not sure; I just know we've gotten bug reports. I remember them being for rustc and cargo, not rustup, though.
Thanks for the tip. Our new website maintainer seems to have managed to rollback the content to an older revision. Can't get good help these days... Fwiw, cmbrandenburg's commentary makes sense to me.
idiomatically, a "flag" is a "bool" struct JavaClass { name: String, // etc... is_public: bool, is_final: bool, is_super: bool, is_interface: bool, is_abstract: bool, is_synthetic: bool, is_annotation: bool, is_enum: bool, } is probably something along the lines of what you're looking to do. then later on, let myClass: JavaClass = ... // a JavaClass is initialized here if myClass.is_public { do_things() } you could breakout the flags into a separate struct if that's useful, but directly embedding them into an overarching struct for each Java Class could make sense too.
No, no, my concern isn't efficient stack usage. My concern is that if I want to use something as a value type in C#, I *can't* unless I want to go write a whole new type. In Rust, the person using the type is allowed to make that decision. In C#, it's down to the implementer of the type. My annoyance has nothing to do with the stack and everything to do with the additional work involved. The end result of that is basically that no one ever uses structs for anything in C# unless they absolutely have to.
Thanks! Can I include this in a "reviews" section? 
 let mut x = 5; x = 6; assert_eq!(x, 6); { let y = &amp;mut x; *y = 7; } assert_eq!(x, 7); 
I would rather it remained supported. It's very convenient when switching databases.
Ya, I just figured that I could use something with the optimizations of bitflags without having to worry about the details. Another thing I want to do is treat an enum as constant integers so I don't have to specify them in 2 places. (One to decode the integer to an enum, and again to preform computation on the enum) Can someone please stabilize [TryFrom](https://doc.rust-lang.org/std/convert/trait.TryFrom.html) and [TryInto](https://doc.rust-lang.org/std/convert/trait.TryInto.html) and make them derivable for C-like enums?
No problem. It's actually something I learnt with my first code review too!
&gt; something with the optimizations of bitflags what optimizations? Accessing individual bits of memory is really slow/inefficient compared to allocating a byte for each flag, unless your greatest concern is with memory usage instead of performance. &gt; Can someone please stabilize TryFrom and TryInto They will be stabilized when they're ready. I'm just a random community member. I can't just go push a button because some other community member asked for them to be stabilized, sadly. &gt; Another thing I want to do is treat an enum as constant integers [this is definitely possible](https://rustbyexample.com/custom_types/enum/c_like.html)
I guess I've never seen one in a function signature. Why would you ever take mut x over x: &amp;mut ?
&gt; &gt; something with the optimizations of bitflags &gt; what optimizations? Accessing individual bits of memory is really slow/inefficient compared to allocating a byte for each flag, unless your greatest concern is with memory usage instead of performance. TIL. I thought that minimizing memory accesses would improve performance in general and that keeping things in a single register would reduce memory accesses. &gt; &gt; Another thing I want to do is treat an enum as constant integers &gt; &gt; [this is definitely possible](https://rustbyexample.com/custom_types/enum/c_like.html) Can I do this? enum Num { One = 1, Five = 5, } match i as u32 { Num::One as u32 =&gt; {...} Num::Five as u32 =&gt; {...} i =&gt; panic!("Unsupported number: {}", i), }
Yes, right now everyone who want performance but can't use nightly are writing wrapper structs, [similar to those in standard library](https://doc.rust-lang.org/src/core/iter/mod.rs.html#1529). (I meant to reply to parent of parent post, but you get the idea)
given how oracle has acted toward its software at literally every opportunity, why should i trust them with containers?
&gt; this crate might be helpful: https://crates.io/crates/enum_primitive &gt; documentation &gt; it makes it easy to basically do TryFrom / TryInto with an enum. Don't you just love it when something that should be standard like this is instead provided through a third party library? \s
The two don't have the same use case. `x: &amp;mut T` is a mutable reference to something outside the function. `mut x: T` is an owned value which is mutable within the function. For example: fn reversed&lt;T&gt;(mut v: Vec&lt;T&gt;) -&gt; Vec&lt;T&gt; { v.reverse(); v } Without the `mut`, you wouldn't be able to call `Vec::reverse`. `v` is owned, so you can't see the mutation outside the function except via the return value. Doing this is basically shorthand for: fn reversed&lt;T&gt;(v: Vec&lt;T&gt;) -&gt; Vec&lt;T&gt; { let mut v = v; v.reverse(); v }
&gt; Can I do this? Not precisely, because casting isn't supported in a match arm like that, as far as I know, but you can [this crate](https://crates.io/crates/enum_primitive) will make it easy. Look at this [documentation](https://andersk.github.io/enum_primitive-rs/enum_primitive/). it makes it easy to basically do TryFrom / TryInto with an enum. So `Num::from(i).expect("Unsupported number")` is basically the same as your code, once you're using that library.
Rust believes in having a small standard library, but a great package management system. Only the really hard stuff and the really common stuff seem to be in the standard library. I don't think the functionality that crate offers is used commonly enough to justify integrating it into the standard library, but that's just my opinion.
You almost never want `mut x: &amp;mut T` as a parameter. The only reason you'd need it is if you were passing an `&amp;mut &amp;mut T` referencing `x` to a function for some reason, or if for some reason you wanted to reassign the parameter within the body of the function.
The priority aspect is critical. A crate for this doesn't exist I don't think, I'm expecting to have to roll my own, but I don't know where to start. A simple, naive version would use a mutex but that sounds really slow.
It covers what you describe within the bounds of what is both immediately feasible and reasonable to infer without additional details. I suspect I'd agree with whatever else you want, but I'll need you to go into more detail.
\[INSERT EXPECTED LINK TO /r/unexpectedfactorial HERE\]
I was wondering whether it'd be possible to somehow detect panics at compile, so I created this crate to help me with finding possible panics. It's WIP/PoC, but it should be somewhat usable already.
There is also [cargo-lichking](https://github.com/Nemo157/cargo-lichking) - tries to check compatibility, but makes explicitly no claims that it is authoritative.
Because Oracle is a big company with many different teams doing very different things. 
OK, [I did](https://www.reddit.com/r/unexpectedfactorial/comments/6loruj/rrust_50_number_of_crates_libraries_to_publish/). Thanks for the tip. :)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/unexpectedfactorial] [\[\/r\/Rust\] 50! {Number of crates (libraries) to publish this year}](https://np.reddit.com/r/unexpectedfactorial/comments/6loruj/rrust_50_number_of_crates_libraries_to_publish/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
This is why I hate `mut` as a keyword, semi rant incoming: `mut` is a _lie_. `&amp;mut` denotes an _exclusive reference_. This is a reference that permits you to do certain things to the target which can lead to undefined behaviour if another reference in the same scope also exists; mutation is a very common thing that often but not always falls under this. `&amp;` is a shared reference this allows you to do things for which it is fine if other references exist in the same scope. In fact in the stdlib mutation is performed via shared references all the time because there are quite a few mutations which are completely safe through shared references and you can create structures that can mutate through shared references without any unsafe block if you so desire. This is _completely unrelated_ to `let mut var = ...` which _is_ about mutation. Using `mut var : Type` in function signatures is similar to `let mut x : Type = ...`; this introduces a mutable binding meaning that you change whatever the binding points to. The following is valid rust to illustrate: let i = &amp;mut 0; *i = 1; //i = 2; // however this wouldn't be allowed The binding `i` itself is immutable; it shall point to one memory location only however since I took an exclusive reference to the memory location that i points to I might not be able to change the memory location but I can use the dereference operator to produce an lvalue and change the contents of the memory location; changing the contents of the memory location like that requires an exclusive reference. However I can go even further: use std::cell::Cell; let i = &amp;Cell::new(0); i.set(1); I took a shared reference now and I can mutate the contents of the memory location? Indeed I can because the `Cell` datatype comes with certain restrictions that `*i = ...` does not come with that ensure that this is totally safe and cannot lead to undefined behaviour. Cell has a lot of restrictions on the type of data that it can contain and retrieve to ensure this is fine. It turns out that mutating simple integers could in fact be done through a shared reference; the type system just isn't granular enough for this but `Cell` places the appropriate restrictions on its contents that allow for this. To allow a simple `i = 1`, as in changing the actual memory location that `i` points to I must declare `let mut i` which is what this permits; actual mutation. Now here comes the bullshit: It turns out that in order to take an exclusive reference to a variable that variable must be marked mutable; ths is bullsht and unecessary and further increases the confusion that `let mut` and `&amp;mut` are some-how related. `let mut` vs `let` is a lint, nothing more. If you change every `let` to `let mut` your code wil continue to work and compile through generating warnings of unecessary mutability. You can make a variable that was once immutable mutable just by shadowing it with a mutable version of itself; that's how little this is worth. However if you were to change all shared references to exclusive references your program would in all but a few cases refuse to compile; permitting that would allow undefined behaviour. So the `let mut` syntax says two completely unrelated things compressed into one: - you can take an exclusive reference to this variable - you can change what this variable points to (mutation) Which further adds to the confusion about `mut` as a keyword, it was chosen to make it simpler for beginners but it tells a lie and at some point you have to unlearn the bullshit they tell you at first. _do not_ treat `&amp;mut` like "this is a reference through which you can mutate" but treat it like "the type system says you cannot take another reference to this object while this one is in scope. For the sake of keeping your functions powerful it is very important that you not take an exclusive reference when a shared reference works of course as an exclusive reference is far more restrictive in what you can do with it. 
I tend to be wary of third party libraries because it's difficult, if not impossible, to accurately judge how well supported they are. If something is in std, I know that it will be getting updates and should be rock solid. Another thing is that while some choice is good, having too many libraries that do the same thing means that it's difficult to choose between them and support ends up split between all of them. When I want to use functionality that's split between multiple nearly identical, poorly supported packages, instead of picking one to use, I tend to just roll my own because I know that I will support my own library for as long as I need to. But then when someone else comes along, there's now another poorly supported alternative and they decide to roll their own because none of the other package developers can be trusted to keep their libraries up to date. The only reason I ran into the problems that prompted me to make this thread was because there are 2 java class file parses on crates.io. One has only 10 downloads, negligible documentation, and an unidiomatic interface. The other is on version 0.0.1, has no documentation at all, and I can't even find a way to include it in my project.
The output of `diesel print-schema` should work for you on multiple dbs
Real world example from the standard library (`src/libstd/io/mod.rs`) fn read_exact(&amp;mut self, mut buf: &amp;mut [u8]) -&gt; Result&lt;()&gt; { while !buf.is_empty() { match self.read(buf) { Ok(0) =&gt; break, Ok(n) =&gt; { let tmp = buf; buf = &amp;mut tmp[n..]; } Err(ref e) if e.kind() == ErrorKind::Interrupted =&gt; {} Err(e) =&gt; return Err(e), } } if !buf.is_empty() { Err(Error::new(ErrorKind::UnexpectedEof, "failed to fill whole buffer")) } else { Ok(()) } } We mutate the contents of `buf`, so it's an `&amp;mut`, and we change what the label `buf` refers to (to subsets of the slice), so we have `mut buf`.
I don't mean multiple databases connected at a time, I mean working interchangeably different databases with similar but not identical schema without code changes.
This is so frackin awesome :D I love text based formats. Beautiful work!
As the /u/SteveMcQwark showed, they have different use cases. A `mut x: T` lets you reassign x inside the function, but not mutate the value it was attached to, so the value is still immutable to the caller. (also note that `mut x: T` is owned) A `mut x: T` is useful if you want to reassign x in the body of the function for some reason. This is somewhat niche, but occasionally quite useful. I've most commonly used it when working with tree data structures, for example where the argument is to the root element to start some search process. Rather than make recursive calls to descend the tree, I update the current position. To that end, I have actually used `mut x: &amp;mut T` before, where I'm performing some mutating operation on the tree as I descend it.
When you talk about efficiency, what do you mean? Assuming you're talking about lock free structures, they are just that; not necessarily faster than locked ones. I imagine for a structure like a priority queue in particular the complexity to do any operation would be high enough that it would hardly make a difference using a mutex or a lock free version. Additionally, keep in mind that an uncontested mutex unlock only takes some nanoseconds and a RWLock can be used to reduce contention if it fits your particular use case. If you are truly committed to making the queue lock free though you may want to look into using [crossbeam](https://github.com/crossbeam-rs/crossbeam). And remember, "Make it work, make it right, make it fast."
Here's a real life example from a Red-Black tree implementation: fn search(&amp;self, mut index: usize) -&gt; Option&lt;usize&gt; { if index &gt; self.len() { return None; } let mut search_idx = self.root_idx; loop { let rank = self.get_child_size(search_idx, Dir::Left); if rank == index { return Some(search_idx); } search_idx = if index &lt; rank { match self.get_child_idx(search_idx, Dir::Left) { Some(idx) =&gt; idx, None =&gt; { if cfg!(test) { panic!("No left child!") } return None; }, } } else { match self.get_child_idx(search_idx, Dir::Right) { Some(idx) =&gt; { index -= rank + 1; idx } None =&gt; { if cfg!(test) { panic!("No right child from {} getting index {}!", search_idx, index) } return None; }, } }; } } In this case, it's *mostly* about being able to reuse the name index, though in some other cases it makes it possible to collapse a base case and subsequent cases into a single loop. So far I think I've only actually had a use for this when writing "naturally recursive" code in an iterative form, though there's probably other uses out there.
There must be a minimal schema that is common between them though. Otherwise your code which depends on it would fail to compile
...led by one group of people who'll burn the world down as soon as they smell money. i'm fine with the tech &amp; the teams, it's the corporate leadership that keeps me at arms length.
Yeah, I just didn't want to have to find it by hand. But supporting hideous legacy use cases isn't necessarily the best case to keep a feature.
In the world of Python, things go to stdlib to die, as it has been said. Being in the standard library in a language definitely doesn't guarantee updates and being the best, but Rust does try to make that a reality. It can do that by keeping the stdlib small, but high quality. Third party crates are definitely not always satisfactory, but if they're not satisfactory, I can assure you that what would hypothetically be in the stdlib would be even less satisfactory, since apparently no one even cares enough about its absence to make a decent crate for it, let alone to maintain it in stdlib. In this case, the enum_primitives crate provides a simple convenience. You could easily re-implement all of its functionality, but most people would rather not, if given the option.
Making it fast is actually part of it correct - there's a deadline aspect to this. Since every operation i'd be performing mutates the underlying queue a RWLock wouldn't help. Instead the plan I had concocted was to have a single mutex around it, but to only wake consumers up if the min value changes.
It needs the ability to blacklist GPL crates via configuration. It's far too easy to accidentally import one without knowing. I've complained about this on reddit a couple of times but never actually filed an issue so maybe I'm going about it the wrong way... (While I'm complaining, far too many crates rely on cmake being installed as well. `cargo install`ing anything has vegas-like odds. Bonus points if you're on stable or Windows. This problem will only get worse as crates.io will approach npm as time approaches infinite, where npm is the upper bound of awfulness.)
Awesome work! One thing though, I suggest you to place the command line instruction at the top of your readme. 
To be frank, yeah that's not a use case I'm super interested in supporting -- That said, I'm not sure what I'm going to do yet. I'm pretty confident that if nothing else, we will treat `diesel print-schema` as the "canonical" way to do things, and if `infer_schema!` sticks around, it'll be "and also we have this thing which is kinda nice if it's able to work for your use case. It seems like having it as the "default" causes more problems than it solves though. Maybe a big portion of that is documentation.
Software distribution requires complying with the licenses of all of its components. Almost all license have documentary requirements. Most BSD and MIT variants require that you carry the exact text of the license, including specific copyright statements and personal or organizational identifiers. Copyleft licenses require offers of source. Then there's complex licenses - license amendments, compound licenses (A &amp; B, A | B). There's recently been a lot of [work in industry to develop processes](https://wiki.linuxfoundation.org/_media/openchain/openchainspec-1.1.pdf) for parts of the software supply chain to [document these kinds of things](https://spdx.org/spdx-specification-21-web-version), and commercial vendors are starting to catch on. I took a look at the copyright information distributed with the Firefox on my ubuntu system, and it's 34kB, 738 lines long. There are (depending on how you count) about 22 separately licensed components referenced there, all of which had to be hand researched, reviewed, compiled into a list, and formatted. That may not actually sound too bad, and actually it's not - I'm surprised it's so short. The version of google chrome I'm using has 163 components. But whether you're talking 22 or 163, every time the version of one of those components is updated, you have to re-verify copyright statements, license text, etc. Now imagine a programming language with a rich ecosystem of small, tightly focused, high quality components, each separately licensed. The effort of review and compliance scales with the number of components. I'm curious how many crates servo currently incorporates into its build. Rust wants to be better than other languages. Its crate system already gives it a leg up on C and C++. A logical extension of the code and build encapsulation that crates provide is use and distribution obligations management. 
After almost a year of procrastination, finally managed to implement Hindley Milner type inference and inference of mutually recursive definitions in the frontend of my lisp-syntactic, haskell-semantic, toy lang! Next step is adapting the LLVM backend to work with these newly added goodies, but that should be comparatively easy (i hope)!
&gt; It needs the ability to blacklist GPL crates via configuration. It's far too easy to accidentally import one without knowing. Actually, consider the implications of the LGPL with rust - it's all statically linked by default. That extends the LGPL terms to the entire statically linked unit. There are a number of problematic licenses (at least in the eyes of corporate lawyers) - non-commercial requirements, "do no evil", "buy me a beer if you see me". Making things even more complicated is the dependency chains in crates - a minor bump to the version of a crate could have a ripple effect in dependencies that leads to entirely different licensing obligations. It's not the kind of thing I expect to be a common problem, but with the way things stand it's something that would almost certainly be overlooked.
Fascinating. Also seems very unstable and difficult to guarantee this will always work in future Rust compilers, but interesting nonetheless.
&gt; This problem will only get worse as crates.io will approach npm as time approaches infinite, where npm is the upper bound of awfulness. In case my other comments didn't make it apparent, independent of my development tasks I also review third party components and produce legal notices to be distributed with our software. The npm ecosystem makes me want to vomit. Having to individually acquire and inspect each component, and create a separate notice - it's specifically because of npm that I'm speaking up here for rust and saying that it's important to do better.
It's an implementation of a public spec.
&gt; This is why I hate mut as a keyword, semi rant incoming: mut is a lie. We fought this battle pre-1.0, "the mutapocalypse." There's pros and cons to each way, for sure.
The NSA makes some pretty amazing FOSS software too. The great thing about free software is you can judge the project itself, don't judge the people behind it.
I know, I wasn't there at the time and if I I was blood would be spilt. Say no to the heretics who worship the false idols of `&amp;mut`. `&amp;` and `&amp;uniq` is the way to go. Al-ilaah `&amp;uniq`.
This is certainly interesting and Rust should probably provide something like this without a hack. Like a macro that generates a warning or something when it isn't optimized out at certain optimization levels for a programmer to indicate her belief that it will be optimized out and still defaults to a panic when it's not optimized out. It would certainly make reasoning about performance more effective.
What's the difference between this and [`compile_error()`](https://doc.rust-lang.org/nightly/std/macro.compile_error.html)?
This is really neat! Seems like it could benefit from a heuristic to not convert hyphens in text labels into lines.
&gt; The great thing about free software is you can judge the project itself, don't judge the people behind it. The people are part of the project. If it's not practical to work with the developers of a project (i.e. report bugs, submit patches, etc), then my confidence in its long-term usability as a solution to my problems is substantially weakened.
If I understand this correctly, `dont_panic!()` causes a *link*-time error whereas `compile_error()` causes a *compile*-time error. One of the things that happens between compile-time and link-time is the optimizations. So a `dont_panic!()` invocation inside dead code may be optimized away and thus cause no error, whereas a `compile_error()` invocation will always fail the compilation if it is ever present anywhere (usually emitted by a macro to signal an error). Whereas `compile_error()` is useful for macro error signaling, `dont_panic!()` is useful for asserting that certain paths are unreachable "enough" that the optimizer removes them.
Are there benchmarks you can show? I'm skeptical that the extra instructions to shift the bits makes a difference compared to the costs of cache misses. 
You shouldn't be using from_utf8 for quick-xml. Instead you should, as specified in the examples, use all the `decode` fns. It will look for xml encoding declaration and then use encoding_rs crate to decode it properly.
&gt; sounds Try it and see.
If they release software under a free software license, you can use it under that license and be assured that you will continue to be able to use it under that license no matter what they decide to do in the future. You can also inspect the code, and there's a good chance that other users will be inspecting the code, so it's pretty hard for them to hide anything shady in it. There's no real reason to avoid free software just because of who wrote it, as long as it's reasonable quality software.
Well, I'm already reimplementing a bigger crate just so I could use it in an even bigger project. At least being in the stdlib is correlated with being stable and documented.
On the docs page: &gt;Like asser but calls dont_panic!() instead of panic!() asser -&gt; assert?
in the course of recreating this I realized it's completely unneeded: https://play.rust-lang.org/?gist=0cf11b9b42bd9becbbacb7996ff1512e&amp;version=stable&amp;backtrace=0 if you look at it this will make sense - I don't need to add the `&amp;mut` to `a` that's preventing this from compiling, it's already an `&amp;mut`. I was trying to do `&amp;mut &amp;mut a` I guess. Still figuring this stuff out... 
Dmitry Vyukov covered the topic of concurrent priority queues here: http://www.1024cores.net/home/lock-free-algorithms/queues/priority-queues If there are relatively few discrete priorities, it's probably best to have one data structure (a lock-free queue or a stack) per priority. But if you really want a more generic lock-free priority queue, then a skiplist might be the way to go. At the moment we don't have any off-the-shelf implementations, but Crossbeam will most probably have it this year.
https://github.com/Kixunil/dont_panic/pull/1
/u/carols10cents https://www.youtube.com/watch?v=eJ9-d9jheMI Sorry not sorry
I made a pull request to enable non-utf8 parsing on your example for quick_xml.
Note that `term-painter` appears to be based on `term`, which in turn cannot do coloring in Windows MSYS terminals. `termcolor`, on the other hand, does support that. /plug [Cargo uses it now.](https://github.com/rust-lang/cargo/blob/9629f99dd0a87d4c8fb8953fd88bb8c9db946a9c/Cargo.toml#L48) Of course, the `term` crate can do a bit more than just coloring with a terminal, which `termcolor` won't do!
&gt; what optimizations? Accessing individual bits of memory is really slow/inefficient compared to allocating a byte for each flag, unless your greatest concern is with memory usage instead of performance. I would rather say "memory usage instead of execution speed". Performance encompasses both ideas. And optimizing memory usage is not an uncommon thing to do (admittedly not as common as optimizing for speed). Although it looks like OP did actually mean execution speed, so it's good that you pointed this out.
If your concerns about the authorship involve the long-term prospects of the project, then there's every reason to avoid it in favor of something more likely to be still be well maintained N years down the line.
I just added this to my command-line tic-tac-toe game, and it seems to be working pretty well for me (although so far, I've only tried it on linux). And as Lokathor mentioned, a character-at-a-time input crate is needed at which point I'll be able to do arrow-key selectable cells instead of numeric row/col input.
/u/steveklabnik1 just needs to quick slacking and publish Alex's firmware update
how do properly escape plain text? quotes ', ", or ` seem to work but they are still visible, is there a way to escape "invisibly"? edit: I guess quotes work, to an extent, some things still get turned into vector, like equals "="
Reminds me of the punchline in xkcd 376 comic.
Thanks a lot, I appreciate the link, looks like great reading.
eeee-pock!!!! 
That's hilarious. And also useful information for me. I just moved to North Carolina from Boston, and though I never made it out to a Boston Rust meetup (largely due to a newborn making evenings away from home more difficult), I decided to start up a group down here ([Triangle Rustaceans](https://meetup.com/triangle-rustaceans)). I was considering mimicking your libzblitz hack night for our next meeting, but given your experience, we may go with a more standard project night instead.
Also, typing a single character in the right pane takes about 5 seconds (Firefox Nightly on Mac).
Thanks a lot for your comprehensive comparison! As the author of quick-xml, I have small comments for it, I hope people will do the same for the other crates to be fair: - as already said, it supports non-utf8 files, you should just not use `from_utf8` fns but `unescape_and_decode`s (I've done a PR) - for DOM parsing, there is [minidom](https://crates.io/crates/minidom). They switched to quick-xml recently. It is not very fair but as you mix xml5ever and html5ever ... I have never used it myself. - for the bench, you can probably speed it up (at least for large xml) if you clear the buffer at the end of the loop (like what you've done for the example) - there is one point that you do not mention though. The writer is missing one important feature: it doesn't escape characters at the moment, nor does it encode stuff (it takes bytes directly), so I guess the support is *partial* only (I don't know what the other crates are doing). This is probably the biggest point to address before I can think of a 1.0 version
The obvious conclusion is that we can't adopt "epoch" name because there is this wide disagreement on pronunciation. People are now arguing over why exactly chickens lay eggs and we need drastic measures if we ever wish to land this RFC.
Pretty sure you'll love it!
https://en.oxforddictionaries.com/definition/epoch
Can people help me review this code for safety? Main additions from latest version of `take_mut` is addition of [sdroege](https://github.com/sdroege)'s `take_or_recover`, switching the code base to use `std::panic`, and a new scoped API. Still needs documentation, and also I should probably try to get compiletest-rs working.
Indexing *is* at present somewhat dissimilar from calling a function—there’s overlap between the two in what you can do, but some quite different functionality between them. I think that merging the two concepts would yield a better language. It’s really quite common that I want to use indexing for things where I just can’t because of one limitation in the `Index` trait family or another. `f(x) = y;` instead of `f.set(x, y);` and things like that would be *so* useful.
&gt; Ya, I just figured that I could use something with the optimizations of bitflags without having to worry about the details. How many different Java classes would you have? If you have 16 flags per class and 50,000 different classes loaded at once, using bitflags will save you less than a megabyte of RAM.
Niko and I and a couple others ended up cribbing issues from some of our projects. Couple that with asking folks to work in groups, and it actually turned out pretty good. I think we'll do it again. It is important to select the right level of issue. I.e., what someone can get done in a few hours. Having an implementation plan sketched out in each issue also proved really useful.
&gt; what optimizations? Accessing individual bits of memory is really slow/inefficient compared to allocating a byte for each flag It really isn't. Accessing a flag bit can be done via simple bitmasking which takes a couple extra machine insns at worst[1]. The one real advantage of bytes for `bool`'s is that they can be individually addressed, but this is precisely what's not practically relevant in a flag struct (or, for that matter, a bitvector). [1] The same is also largely true of very small enums or integral ranges BTW, they should also be packed within a machine word/subword in many cases.
I was just looking for something like this! Thanks!! Unfortunately, I'm getting a compile error saying my getter method is private :( If I can isolate and reproduce I'll open an issue
https://stackoverflow.com/a/4241994 it depends on how much cache pressure you’re under, but caches are larger than ever these days, and most of the time I believe separate bytes is the best of both worlds. It isn’t the overkill of C++ 32-bit boolean, and most of the time you don’t have enough booleans to even use up most of the 8 bits in a byte as bit fields, so you might save half the memory at double the number of instruction cycles, leading to a wash on cache performance versus CPU time. So, as always, it depends, but I think the Rust standard implementation is the right one for most people.
[Log-log plot](https://a.doko.moe/ipdmuk.svg) From eyeballing these few data points, Rust looks linear and Haskell quadratic.
Wouldn't &amp;exc (exclusive) be more talkative than &amp;uniq? Or maybe it should have been Scala-like val / var for bindings, keeping mut for references. I had never realized how confused the _mut_ double usage got me at first. You're totally right, and now that it is too late, the documentation should at least include a big warning sign about mutable binding vs mutable reference.
&gt; Convenience features are missing (such as init-update-finish signatures). *ring* doesn't implement init-update-finish signatures intentionally, and won't do so unless/until somebody shares a really good use case. But we aren't completely opposed to the idea. Rather, we're trying to encourage one-shot uses because I-U-F isn't possible for Ed25519, and we'd like to have a uniform API that works for all signature types if possible. &gt; It also depends on Rayon which in turn brings in more dependencies than I want in a straight crypto library. Only the build.rs depends on Rayon, to compile the C code in parallel. The *ring* library itself doesn't have a dependency on Rayon. *ring*'s developers go out of our way to minimize third-party dependencies.
Hi You might wanna take a look at crust. [ https://github.com/NishanthSpShetty/crust ] An attempt to transpile C/C++ to rust.
rust is born for enterprise software~~ 
Unique reference, exclusive reference, that's all the same. The proposed keyword in this discussion was just `&amp;uniq` so I roll with it. I just think it's important to not call it a "mutable reference" and to not arouse the impression that it is some-how related to `let mut` which is naught but a lint. I remember actually weirdly feeling guilty like I was hacking and doing something wrong when I designed datatypes that used `&amp;self` but clearly mutated; how wrong I was that was the absolute right choice because using `&amp;self` over `&amp;mut self` made them far more flexible as they didn't need to take an exclusive reference.
I'm a bit confused, do people actually type these ascii diagrams out by hand or is a tool used to generate them? Typing them out by hand seems extremely tedious, especially for circuit diagrams where I would use a CAD tool designed for that purpose. What am I missing here? Technically speaking, this is very impressive and it seems you've solved a tough problem, but I just can't imagine iterating on a diagram or making a complex one from scratch in a reasonable amount of time.
I think the compiler could “just” implement an optimization pass that replaces uses of `Arc` with `Rc` or even omits the reference counting entirely based on static escape analysis, similar to what the Swift compiler does. It “just” requires the compiler (re-?)gaining implicit knowledge of `Arc`. I think this is probably inevitable, not just for Rust, but also for C++ and `std::shared_ptr`.
Kudos to /u/dzamlo for the help in getting a bunch of these issues resolved! With some grease left to go for parsing cases, I'm starting to feel like this library might actually be usable. :)
It sounds like a removal of a license option should be considered a SemVer breaking change—but worse than that, one that is also a breaking change in dependent crates. This is the sort of thing that crates.io would ideally validate.
whoa - `&amp;self` methods can mutate? 
This is a concern for pretty much any free software project. Many people depend on projects written by one or two anonymous people on the internet. This particular one happens to be a drop-in replacement for another tool, so you could just switch back to the original if you need. Also, Oracle has maintained plenty of free software for many years, using something from them is likely a lot less risky than using something from some random person on the internet. Yes, they closed off a few things after buying them, like Solaris, but because it was free software it has been forked and other people have been maintaining it as IllumOS.
I still haven't gotten to dive into Rust yet, but I really like the blitz and everything that it stands for. Library development in Rust is an opportunity to plant a flag in this burgeoning ecosystem. I think it might be really cool for this evolve into something like Boost (basically a blitz plus voting plus a distro), which continues to be a driving force in the C++ world nearly 20 years after its inception.
Example: https://is.gd/zVp5DN This is one of the reasons like I said why the `&amp;mut` keyword is a complete lie; no unsafe blocks were used in the making of this code because what I am doing here is totally safe. Essentially `Cell` is your basic primitive that allows mutation through a `&amp;self` reference and allows you to update its contents. So why does it escape the normal unsafety and doesn't need an exclusive reference? Because `Cell` enforces an all-or-nothing access. You cannot obtain a reference to the contents of the cell. You either get the entire value out of the cell through the shared reference or you put a new value in it. If you could actually obtain a reference to the insides of the Cell then this would be a recipe for undefined behaviour; you could in fact construct the mother of all unsafe functions `mem::transmute` if you could do this. Let's say Cell had a method `ref` that obtained a reference to its contents: // create a cell holding a Result of types X and Y and store an X value in it let cell : Cell&lt;Result&lt;X, Y&gt;&gt; = Cell::new(Ok(some_value_of_type_x)); // obtain a reference to the X value inside of the cell let ref_to_X : &amp;X = cell.ref().as_ref().unwrap(); // mutate the cell to contain a Y value, Cell can do this with a &amp;self reference remember cell.set(Err(some_y_value)) // but the variable ref_to_X still exits and stil has type &amp;X but the memory location it points to now holds a value of type Y // we have successfully cheated the type system and transmuted X to Y; this can be done for any arbitrary type // this is why Cell _cannot_ give out a reference and this is why cell is perfectly safe because it doesn't hand out references to its inner contents // and this is why you normally need an exclusive reference to mutate or stuff like this could happen and we could just cheat the type system.
Really cool looking, I will probably use this in the future (would have been useful for some stack diagrams I had to make recently for instance, would probably still clean them up in inkscape afterwards). Would be nice if there was a 'real' way to export instead of inspect element and copying the svg source out. Word spacing is a bit odd in some cases, e.g the following ends up with a much larger gap between "are" and "you" then "who" and "are". +-------------+ | Who are you | +-------------+ Unfortunately typing is extremely slow on the initial document, firefox, linux, a few unused gb of ram (8gb total), i5-7200U cpu. Console output is at the bottom. Deleting all the initial text makes it reasonably responsive. q= main.js:3:1 Loading webassembly version main.js:16:5 wasm has loaded.. main.js:25:9 trying binaryen method: native-wasm svgbob-editor.w.js:190:7 asynchronously preparing wasm svgbob-editor.w.js:190:7 binaryen method succeeded. svgbob-editor.w.js:190:7 run() called, but dependencies remain, so not running svgbob-editor.w.js:190:7 pre-main prep time: 2766 ms svgbob-editor.w.js:190:7 Yes, why are you looking at here svgbob-editor.w.js:187:7 Because you want to know what technology stack being used.. svgbob-editor.w.js:187:7 It is built with Rust, stdweb and svgbob svgbob-editor.w.js:187:7 Compiled to wasm and use asmjs if your browser don't support wasm yet. svgbob-editor.w.js:187:7 Rust Evangelism Strike Force!.... reporting in... :D svgbob-editor.w.js:187:7 For Vim-ers, :set virtualedit=all, SHIFT-R, then starts drawing, move stuffs around: CTRL-V jjjlllll 1vp svgbob-editor.w.js:187:7 Sorry for the stream of consciousness comment, it's a bit late here..
Huh. When `impl Trait` gets stablized, all of the iterator structs in `std` could theoretically be replaced (yes, I know, breaking changes and whatnot).
Thanks /u/Gankro, /u/carols10cents and /u/manishearth -- this was *so* fun to do! Looking forward to doing another one soon!
Sure!
To get more information of the error, could the filename and line number be encoded into the function (symbol) name?
Escaping text is not yet thought out. I will have to add it in the next free weekend. I'm thinking of using double quotes to to signal put the text as is. Also for the `=` I'll refine the algorithmn whether or not to interpret it as graph or text. Take a look at / or - when used next to text, they dont render as drawing element. 
It's true that typing diagrams by hand is quiet tedious, especially if you are using a really simple text editors like notepad or textedit. But for power users of vim or emacs, they could easily do these without much difficulty. The editor I've written on this demo was a quick POC to ease the tedious typing of diagrams by predicting where you intend the cursor to go next. The important point of this project is not really the editor, but the portability and simplicity of text. You can paste the ascii diagram in your source codes, your own notetaking app, to Readme files. The circuit diagramming is not meant to be comprehensive like their CAD counterparts, it just happend that when I was trying to paste an ascii circuit diagram, it renders it nicely too. I find it useful for academic uses (such as online quizzes involving circuitry) without having the teachers or students being greated with the complexity of CAD tools. 
I'm confused on [linked_list.append](https://doc.rust-lang.org/src/collections/linked_list.rs.html#285) If self doesn't have a tail, why does a mem::swap make sense? Can't it still have an element in the self node that you want to keep? If self does have Some(tail), why make other_head.prev point to that tail instead of to the self node? My apologies for the jumbled questions. I'm not sure if my confusion is caused by rust or linked lists generally
tanks for de podkast but buy new mic?
&gt; But for power users of vim or emacs, they could easily do these without much difficulty Interesting, are there any gifs out there of people typing these? From the perspective of someone who very rarely makes ascii diagrams, I don't find it very simple because I need to learn which characters and character combinations make the type of lines I expect, and if I need to make adjustments, those changes might cascade and affect everything else. But I'm also a lowly Sublime Text user who can't imagine the kind of text editing which would make working with these easy :P
release first crate for tiny robots
[According to Wiktionary](https://en.wiktionary.org/wiki/epoch), both pronunciations are used in both the U.S. and the U.K., with the U.S. throwing in a couple more, but, I'm Canadian and I've only ever heard the Oxford "ee-pock" (iːˌpɒk) pronunciation in the flood of American documentaries I grew up on. Plus, "eh-pck" (ɛpˌək) sounds too confusable with "epic".
You might want to check out http://fossa.io/ It's a paid service, but I think it does what you're looking for already, and has rust/cargo support out of the box.
Yeah, something like __warning__ or __error__ attribute in GCC would be awesome.
Exactly
I'll add minidom. &gt;you mix xml5ever and html5ever I'm not. &gt;it doesn't escape characters at the moment Will test other crates for this and add it to the table.
Good idea but probably not due to hygiene.
If it ever stops working, it's an indication of a compiler bug. Same technique is used in case of gcc and warning/error attributes. I think FORTIFY_SOURCE uses it. Also there is an escape hatch in the form of --features=panic - that switches to normal panicking behavior.
All of us except Carol were using good conference room mics, the only problem is equalization and our setup doesn't get us different channels (nor do I wish to deal with that). In general we're trying to avoid extraneous effort and focus on delivering content.
`#[link_name = concat!("some_prefix_", file!(), line!())]` etc. could work except: 1. macro invocations are not yet supported in attributes 2. the file path including slashes might be an invalid symbol name
Diesel doesn't seem to support transaction isolation levels (serializable, with retries automatically handled by the ORM, is a much, much better default than most database engines actually provided). Are there any plans to introduce this functionality?
I'll try to make one. 
Thanks, I'm glad you liked it.
I think you could put this in the drop handler for a type, and then use it to prove that an arbitrary expression won't panic. Something like: let guard = DontPanic::new(); let value = some_expression(); forget(guard); 
Enums in Postgres are substantially faster and take up substantially less space. Portability is rarely an issue because people almost never switch database engines.
That's an excellent idea! Thank you very much!
Interesting approach. 1. Why is that? 2. Line number would be still much better than nothing at all.
Why is 2 space indent the default? Please consider switching it to 4, which aligns with almost all existing Rust code. See also the example in fmt-rfcs: https://github.com/rust-lang-nursery/fmt-rfcs/blob/master/example/lists.rs
Yeah, I was also thinking about using `Rc&lt;Arc&lt;T&gt;&gt;` so that within thread one could use non-atomic operations and use them only when sending.
Merged, thanks guys!
Can this catch panics in dependencies (like `std`) that don’t themselves use this crate?
Unfortunately not. /u/CUViper has a [good idea](https://www.reddit.com/r/rust/comments/6lopj1/dont_panic_crate_that_causes_linking_error/djw4vt7/) on this though.
Some documentation for the scoped API would be useful to have, I'm not entirely sure how it's exactly supposed to behave.
Or just offer a fixed-width font in the output as well. With a modern font it might even look good :) Great tool (and demo) though!
&gt; Now if only cargo would push Cargo.lock to crates.io. That would introduce more problems then it solves, even in that case.
Keep in mind that `self` is not a single node, but a `LinkedList`. From other uses of `self.tail`, I think that if it is `None` you always have an empty list. So `empty++other` is just `other`, and the swap is useful because `&amp;mut other` has to be still valid afterwards (and be empty). 
https://github.com/rust-unofficial/awesome-rust ?
What about an event loop library? Reacting to mouse events and window resizing could be useful too.
Obviously, performance and space usage will depend a lot on the length of the strings. If they're short, and if the enum/lookup columns make up a small proportion of the columns in a table, it's not necessarily a big difference. I don't see any reason why the lookup table wouldn't be cached about as well as pg_enum is. Re: portability, for a library like diesel that tries to present an abstraction for multiple backends, I think it makes sense to prioritize portable functionality. But maybe the whole discussion becomes moot once diesel includes a mechanism that allows for enum support in a third party crate (like rabidferret says).
Postgres enums don't use strings at all (and there's no join at query execution time). They are stored as integers, and queries against them are converted at query compilation time (this is why the transactional machinery for enums is quite tricky). So the difference in performance is substantial. And sure, a third party plugin is probably the right solution here. I'm more commenting because I don't want people to have the impression that there's no point to using Postgres enums.
[stdx](https://github.com/brson/stdx) and [stdx-dev](https://github.com/llogiq/stdx-dev)
&gt; queries against them are converted at query compilation time If your query says `where enum_column = 'some value'`, it makes sense to me that the enum value could be converted to an int at query compilation time. But if the query string has `where enum_column = ?` and the enum value is only known at execution time, then it seems like there'd have to be a string lookup at execution time. If you go the lookup table route with varchars, and if your values are 3 bytes long, then the stored representation is [4 bytes (including length)](https://www.postgresql.org/docs/9.1/static/datatype-character.html), right? Then the varchar values occupy the same amount of storage that enum values would. Of course the varchars in your lookup table could be longer than 3 bytes. But how much of a difference does that make, proportionally? Depends on how long they are, and on how much other stuff there is in the row.
I haven't put much thought into it's API yet - I figure I'll wait until I have something usable before doing that. I'm mostly doing this for fun. It isn't clear to me that there'll be much benefit to using a Rust re-implementation over the official SQLite library (with some Rust bindings). There's a bunch of hairy code in the native library, but their bug/vulnerability rates seems to be low. It may not be worth trading stability for marginally more safety. &gt; perhaps a macro for building queries Assuming I get something usable, I was hoping to try and get it to work with Diesel. 
I am super excited that the RLS is getting so much attention! Good luck with the project! I was going to ask if support for crates with both a lib and a bin would fall under the scope of this project, but I just saw that you already made [a PR for this](https://github.com/rust-lang-nursery/rls/pull/363) and it is merged already too! Woohoo! This is huge! Most of my crates use that structure (lib + bin) and it made it impossible to use the RLS for my projects.
To steal the limelight for a second - if you're not into the whole Discord thing, there's the Rust room on Matrix - https://matrix.to/#/#rust:matrix.org Matrix is a federated chat service - you can run your own server and talk to people on other servers from it - which implements many modern features that we expect from chat systems these days. I like to think of it as "what IRC should be in 2017". The current most functional client is Riot, which comes in web, desktop, Android and iOS versions.
I commented regarding the spacing here. https://www.reddit.com/r/rust/comments/6lm8zd/svgbob_create_good_looking_diagrams_using_plain/djv38mq/ As for the svg output, I'll soon add, Export to tex, Export to svg. But I doubt editing it in inkscape would be that ergonomic since the drawing elements are not really grouped into an organized way. A lot of work will still have to done in order to have an organized and pleasant SVG document.
I find infer_schema! pretty neat when the project uses Sqlite, but I don't think I would use it otherwise. In my own projects where I use Sqlite and Diesel, I have set up a [build.rs](https://github.com/revolverhuset/fishsticks/blob/master/build.rs) that runs the migrations, and does some fudging to have it integrate well in the project. With this setup, the migrations are _always_ the specification of the schema. It's pretty neat, and less of a hurdle for contributors :) Of course, that is a specific usecase wherein Sqlite is appropriate.
https://github.com/Geal/nom if you want parse something
Oooh, neat. Thanks! One question: I don't understand the distinction you are trying to make with Event parser vs Push parser. My initial reaction is that those would be the same. Can you elaborate on that? :) Also, did you look at support for pull parsing?
I also ran into this issue and discovered that this only happens when the program is executed using `cargo run`. Try to run the binary in `target\debug` directly and everything should work as expected. There seems to be a race condition when executing through `cargo`. In my case I wanted to write a file before exiting and this sometimes succeeded and sometimes failed. My guess is that both `cargo` and the launched program catch the CTRL-C. For whatever reason `cargo` terminates its child process before exiting.
Well it's just not implemented. Attributes are moving towards a model where this makes sense (from a simple rigid syntax), but they're not to a point where macro invocations in them work.
I think `Event` means pull parser.
Trivial using cmd on windows using the winapi crate. I have no idea where you'd start with linux. Ncurses does it i think, i just wouldn't know where to look at all.
Would be nice if the visual studio code extension could have a macro expansion in the peek window.
&gt; In the case of the label's caption, it should have some machinery to detect that the caption's only dependency is "state.counter" and it should only re-render whenever that specific field in the state is changed. What are your plans for achieving that?
That took care of it; thanks!
I'm glad to hear that! I hope that workspace support will make RLS usable for even more people :)
I'm not good with terminology, really. AFAIU I should rename *Event parser* to *Pull parser*, but xml5ever calls itself a push parser even though it produces tokens/event through the callback. Also, xml5ever support feeding data by chunks. I don't know what a proper name for this.
I thought macros just spit out text with some hints for compiler to ensure hygiene. Do I understand something incorrectly?
Since this strictly expands on the set of allowable operations within the type-system, it is not possible to evaluate the safety of `take_mut` in isolation: It would be possible for another crate to also expose a "safe" API, which would be made unsafe by the presence of this functionality. It reminds me of axioms in logic - this is adding a new axiom, which does not in itself make the "logic system of safe rust" inconsistent, but there are other possible axioms that could have been added, which *would* be inconsistent with this one.
Yeah, we were brainstorming a bit how such a feature could be look like. Ideally we'd want to support multi-step macro expansion preview, but unsure how we could present stepping functionality to the user. Until we get there, getting a complete expansion preview seems reasonable, but ideally we should use LSP for that. However VSCode extension could do its own thing on receiving relevant LSP response, i.e. open the expansion results in peek window. We still have to think how this should be done, thanks for suggestion!
Alright, I'll try to clarify for you :) "Feeding data by chunks" makes it a streaming parser. I'm not certain of the name of the converse kind of parser, but I'd maybe call it a buffered parser. It would be valuable to have a "Streaming: yes/no"-row in the table. "Push parser" is when you give data to the parser, and it immediately parses the data. The result of the parsing could be many things, including calling callbacks/triggering events (I guess this would make it an "Event parser" maybe?) or returning an in-memory representation (DOM). "Pull parser" is when you ask the parser for what's next, and the parser will pull data from its datasource as needed to answer that question. [.NET's XmlReader](https://msdn.microsoft.com/en-us/library/system.xml.xmlreader.read\(v=vs.110\).aspx) is an example of a pull parser. While a streaming push parser could always be used asynchronously, a pull parser may or may not support an asynchronous datasource. This would also be valuable information to have in your table :) Did that clarify anything? Note: Although I have made suggestions for things you could add to your table, take that as suggestions for future improvement. The table is already valuable! :) Edit: Thinking about it a bit more, I guess the pull/push distinction only makes sense for streaming parsers. If the API is something like `fn parse(buffer: &amp;str) -&gt; Document`, there is no such distinction to be made.
please no I couldn't figure out how to do the simplest thing in it then I found https://github.com/J-F-Liu/pom and wrote my parser
Additional upside of using Matrix - you can bridge over to the Mozilla IRC server and access the main Rust channels. Works surprisingly well, and it keeps you logged in so you can see what messages have been sent while you've been offline/if you've been mentioned.
Maybe an attribute that generates the code would be better? Something like #[dont_panic] let value = some_expression(); 
Fair point. The project itself is in 4 spaces, but I mostly write JavaScript, with 2 spaces indent, so I defaulted to 2. I'll consider changing this, though. Thanks.
Ahh, I see what you mean. I definitely agree that, in this day and age, it really should be handled in the package manager.
A closure might be better.
Of the new (to me) crates that I've worked with recently, [rusoto](https://github.com/rusoto/rusoto) has impressed me the most. I've only used it for sending emails via SES but `rusoto_core` + `rusoto_ses` and a few minutes building a [`SendEmailRequest`](https://rusoto.github.io/rusoto/rusoto_ses/struct.SendEmailRequest.html) and it just worked™.
What is it ? It is not updated since 2 years according to crates.io
A list of crates considered (by the curator) to be best-in-class for what they do.
Thanks a lot! &gt;"Feeding data by chunks" makes it a streaming parser. But what about xml5ever. You can [feed](https://ygg01.github.io/docs/xml5ever/xml5ever/tokenizer/struct.XmlTokenizer.html#method.feed) data by chunks and it will produce events for the current chunk. Other parsers in the table doesn't support this (afaik). But it looks like anything that doesn't create a DOM is a streaming parser, even if we have all the data already. &gt;I guess the pull/push distinction only makes sense for streaming parsers Yes. I should probably change *No* for *sxd-document* to `-`, because it's not relevant for it.
&gt; Write fearless COBOL bindings for WebRender! yup sounds like a plan
This isn't an answer to your question, but would a work-stealing approach work for you? The basic idea would be that each consumer maintains a priority queue which only it touches. Consumers are fed through conventional mpsc queues; they loop around draining the mpsc queue onto the priority queue, then processing the top item on the priority queue. Producers select consumers at random. That already gets you rough priority; a high-priority item will never be entirely blocked by low-priority items, and although you can end up in a situation where high-priority items are queued up on with consumer, while other consumers work on low-priority items, it's unlikely (the probability is, er, left an exercise for the reader). A small hack to reduce the probability further would be to use [two random choices](http://brooker.co.za/blog/2012/01/17/two-random.html). Items can be thread-safely marked as having been processed. Each producer puts its item onto *two* random consumers' queues, and when consumers pop an item off their priority queue to work on, they atomically attempt to mark it, and discard it if it's already marked. Ownership of the underlying item requires an Arc in this situation, i think. You can then try to add work-stealing. I'm not sure how you do that in Rust. One possibility would be a special kind of item (with maximum priority) which tells a consumer to send half of its priority queue to another consumer; then a consumer with an empty priority queue could send one of those to a randomly-selected other consumer, and then wait for items to come in. Apologies if this is wrong or unhelpful!
You really should call `abort` instead of `exit`. I notice this was raised in the [previous thread](https://www.reddit.com/r/rust/comments/45sr77/take_mut_011_take_a_t_from_a_mut_t_temporarily/) you posted.
Hi /u/ivanceras It's super neat... great job 😸 ! Do you know if a library exists to do the opposite ? (diagram image -&gt; plain text)
Would it make sense to put the DB file in a temporary directory or the user's data directory? That's what I did on one of my programs. Check out the crates [app_dirs](https://docs.rs/app_dirs/1.1.1/app_dirs/) and [tempdir](https://doc.rust-lang.org/tempdir/tempdir/index.html) for more info.
app_dirs with UserData dir would work well for this project. i'll see if that fixes my problem, thanks! edit: it seems that diesel's `infer_schema!` only likes env params for locating the database when it tries to compile the schema. i'm not savvy about rust macros yet so i'd guess maybe it has something to do with being unable to call app_dirs before runtime, or maybe the macro just isn't able to handle this situation. however since i really want the storage location flexibility of app_dirs i'll probably just move to a different db impl instead of diesel for now.
This is pretty neat. I think a WYSIWYG editor for this format would be awesome. As impressive as this is, I can't see myself building diagrams in plain text like that. But I would love to generate text-readable documentations with diagrams.
Re-reading my original comment, I realize it may have come off a bit dismissive and flippant. Apologies if so. The `ring` project oozes with commitment and care. I'm very appreciative of what it provides and aware of how much time it saves me! Thank you. Thinking about what you said, I understand the reason for not providing an I-U-F signature interface. Getting such an API right across the universe of signature methods...eek. My (admittedly weak) argument for I-U-F signatures: the use-case is building a signed message chucks at a time. I call `update` on each chunk as it is computed. This way I do not need to coalesce all of the chunks in a single temp buffer before signing. For big messages this is an enormous memory savings; for larger-than-memory messages it is the only way. A counter argument is that digests already provide I-U-F, so hash the message chuck-by-chunk and sign that. I respond that this is not going to work in all situations as `sign(hash(byte stream))` != `sign(byte stream)` and some situations require the latter. Coming at it from the larger perspective, though, I see how a consistent interface for this is challenging. &gt; ring's developers go out of our way to minimize third-party dependencies. And I thank you for it!
Here's also a very good Discord Server that is dedicated entirely to Rust: https://discord.me/rust-lang We have some very good discussions about Rust there all the time and by now I absolutely prefer it over the Subreddit or the IRC whenever I need some help.
checkk out 24 days of rrust series :)
Do you know if this affects custom cargo subcommands? That would be a bit annoying I think, although most of subcommands probably don't want to override default ctrl-c behaviour
Did you check out https://github.com/kevinmehall/rust-peg? I found it super easy to use
Relevant: https://github.com/AltSysrq/lmdb-zero lmdb-zero does have one (IMHO) ergonomic problem in that you need to use something like the Rust rental crate when you want to put the env and db instances (self referential structs) together inside another struct. Aside from this, I see lmdb-zero and the danburkert lmdb crates both use zero copy for reads. They both provide transaction and cursor support. I'm curious about other pros/cons... 
Can someone explain what `$crate` is? This https://github.com/rust-lang/rust/pull/42902 PR seems to indicate it's now a keyword, but I don't understand what it does, why you'd used it, etc. 
My first non-trivial Rust project. The Roughtime protocol aims to achieve rough time synchronization in a secure way that doesn't depend on any particular time server, and in such a way that, if a time server does misbehave, clients end up with cryptographic proof of it.
Yes, please do. I'll get to it ASAP. edit: ah-hah, I'm an idiot; I forgot to make all generated functions public. This has been fixed.
There's some limited usage; I distinctly remember seeing a presentation one time about a company in Tokyo who was using Rust, but I can't remember the details :(
Then xml5ever is a streaming parser and the others aren't. xml5ever looks like an event-based streaming push-parser. From what you say, it sounds like the others are event-based non-streaming parsers. There are other APIs that would also qualify as streaming, for example if the parser accepts a [Read](https://doc.rust-lang.org/std/io/trait.Read.html) object. I guess this invalidates my previous assertion that a push parser could always be used asynchronously. If the parser takes a Read object, it might end up doing blocking IO, but it is still streaming. If the parser only accepts a buffer containing a full document, it is definitely not streaming. _Edit:_ Okay, I looked into xml-rs, and it is a blocking (non-async) streaming pull parser :) &gt; Yes. I should probably change No for sxd-document to -, because it's not relevant for it. I agree. (Or you could even use "N/A", but maybe "-" looks better)
First time I've heard about roughtime. Sounds very exciting, maybe even something that servo or firefox can use one day.
I was thinking about the following: Let's take the above example. When adding the label component to the tree, I could call the caption callback with some kind of wrapper around the 'state' object. There would have to be some indirection in the "state.counter" call ("state.counter()"), though. But that's something macros could help with too, I suppose. Anyway, this kind of dependency tracking was used in the Javascript library Knockout, and it works fairly well. The drawback is that it will not properly track dependencies if there's some sort of conditional state access in the caption callback. Suggestions to make this more robust are welcome :) Another option would be to do some kind of intelligent diff of the trees (where it doesn't matter HOW caption changed, just THAT it changed), I might try that as an alternative.
https://rustjobs.rs might be good
Attributes simply don't have macro parsing. The places where a macro invocation can occur are hardcoded. Also, each attribute will be able to allow parsing macros or not, in each part of its syntax. But `link_name` and other string-valued attributes can simply parse an expression, which allows macros. There is already something in the compiler for parsing an expression and expanding it, expecting a string literal to be produced (e.g. `format_args!`, used by `println!`, `format!`, etc. takes a string literal but `format!(concat!("{", "}"), 42)` also works), it's just that attributes haven't been hooked up to it.
1) From some googling, it looks like the cause may have been a stack overflow, where your function call stack got so deep that there wasn't any more memory for it. 2) No, the program was terminated by the operating system instantly. 3) In this case, you would not have gotten more information, and RUST_BACKTRACE=1 has no overhead, as far as I know, it just changes the output you get during a Rust panic... which is different from OS-level termination. 4) The best place to look would be the core file. It's possible that your copy of Linux is configured not to save core dumps, in which case, there's nothing you can do about it right now but change that configuration. If you do get a core dump, you can at least load it up in a debugger (like `gdb`) and see what caused the program to be terminated. It would show you which function was executing, how deep the call stack was, etc.
NicoVideo
I wrote [a nom parser](https://github.com/myfreeweb/unixbar/blob/53d425c4cd4619fa30e8bf3e562421019dd870a6/src/widget/bspwm.rs#L19-L38) once, it was pretty easy… but the input language was rather simple. https://github.com/Marwes/combine is excellent for programming-language-ish things though!
&gt; What are the likely suspects for that particular message? Perhaps some SSE instruction that your CPU does not support? Or perhaps some function pointer or return address got corrupted somehow. &gt; I was buffering log messages and the last one is several minutes before the crash. Lets say I had written a proper Drop impl that saves the remaining buffer - would it have run in this situation? As far as I'm aware execution an illegal instruction is not a panic, and no unwinding will take place. To implementing `Drop` would not have helped you. &gt; The program was running in release mode. Would running in debug mode have given more information? Does RUST_BACKTRACE=1 have significant overhead? I think the answer to both these questions is 'no', but if the compiler (for whatever reason) generated instructions that your CPU does not support, those may not show up in debug mode. &gt; I have looked unsuccessfully for anything in syslog about this event - is there any other log or place in the system that would have info on what happened? You probably have to convince the program to crash again with core dumps enabled, e.g. see [here](https://stackoverflow.com/questions/17965/how-to-generate-a-core-dump-in-linux-when-a-process-gets-a-segmentation-fault), then you can fire up a debugger and investigate. It is possible, if not very likely, that your program crash was the result of a random bit flip, which is a thing that happens sometimes when your computer inadvertently acts as a very crappy cosmic ray detector. In that case you may never be able to reproduce the error.
Why is Ferris blue?
Ah yes!
Cool! While we're on the subject, where's the slack channel? Just kidding, I know we're not allowed slack.
https://github.com/bodil/im-rs https://github.com/MovingtoMars/liner https://github.com/TeXitoi/structopt https://github.com/Michael-F-Bryan/include_dir https://github.com/SimonSapin/rust-typed-arena https://github.com/ihrwein/backoff https://github.com/Matthew-Maclean/english-numbers https://github.com/kbknapp/cargo-outdated https://github.com/kbknapp/cargo-graph (shameless plug) https://github.com/myfreeweb/secstr https://github.com/myfreeweb/systemstat 
On the other hand it's pretty terrible that whether a package builds or not is dependent on the optimizer's whims...
thank you. using ecc ram so I think that would have mitigated bit flips? My suspicion is the log message buffer became too large somehow (mostly since that's new code). We will see. thanks for your help. 
thank you. stack overflow is my leading suspicion - hopefully I can track it down. 
The crate is out of date, but the list maintained on GitHub is up to date. The included crates are defacto "standard" crates for common operations. 
I'm excited about RLS, but it's been frustrating to work with thus far. It constantly crashes and is generally unstable, especially this last week it won't run for more than a few saves before crashing in VSCode.
https://www.youtube.com/watch?v=v-n1vGeVIXo
This is interesting, I think splitting the queues up may work if I have multiple consumers. Thanks for the input.
[gotta go fast](https://twitter.com/Gankro/status/882036514108182528)
as a dumdum emacs enthusiast that's new to rust and language analysis, how does RLS compare to racer? do they solve different problems?
Hi, author of xml5ever here. Preface, I am human, so I might be wrong. Pull vs push disambiguation is basically, on who controls the flow of results. - If parser doesn't do anything until you tell it, and you have to *pull* the result out it, that's a pull parser. - If parser keeps *pushing* events at you, and you need to respond to them to get anything, it's a push parser. What complicates matter is that xml5ever "hides" the events by sending them to a Sink. 
This points up what to me is the weakest aspect of the crates: you cannot tell what they are by their names, the crates.io site only seems to show them by their names, and there are over 10,000 of them, so you can't really look at them all to see what they are. Also, many of them have overlapping functionality, and there is no way to tell which ones are "better".
First off, love the benchmark. xml5ever didn't do great, but hey all the more reason to make it fast. My minor nitpicks, regarding xml5ever are: - Why is there a ? for it's namespace handling? It says it deals with namespaces. Did you find some bugs? If so please report at: https://github.com/servo/html5ever/issues/new (just make sure you mention xml5ever) - How did you test `xml:space` option? 
&gt; which is a thing that happens sometimes when your computer inadvertently acts as a very crappy cosmic ray detector. Now explain this to your users... :-)
Yep, I'm waiting on that one :)
Illegal instruction means that your CPU read a pattern of bits that it thought was an instruction but the pattern of bits did not correspond to any instruction it knew. ~~A stack overflow would not cause this as it is not a problem with memory. Rust can identify stack overflows on *NIX systems by including guard pages at the end and beginning of the stack. Once it reaches past the stack you will get a segmentation fault which the rust runtime will check if the address occurs in one of the guard pages. On windows, windows will explicitly throw a stack overflow exception. In either case rust will explicitly tell you if it was a stack overflow.~~ I could be wrong but the only four reasons you would get an illegal instruction would be, 1. In unsafe code the program was told to jump to a function pointer which was not valid, hence trying to read random data as instructions. 2. There is a bug in the rust compiler itself, leading to the compiler emitting illegal instructions. 3. You have a really old CPU which does not support some extension to the ISA. 4. Some type of corruption, either the binary on the disk got corrupted, or a bit got flipped at runtime. EDIT: Turns out you can get this error from a stack overflow. What OS and CPU are you using?
&gt; A stack overflow would not cause this as it is not a problem with memory. except... [link](https://github.com/rust-lang/rust/issues/40125#issuecomment-282804407)
Either he's trying to pass as Sonic the hedge-crab, or he's already so fast that the Doppler effect has offset his chroma.
&gt;xml5ever didn't do great But it has the best features set. &gt;Why is there a ? for it's namespace handling? https://github.com/RazrFalcon/choose-your-xml-rs/blob/master/examples/xml5ever.rs#L81 Documentation is a bit outdated, afaik, but still I have no idea how to retrieve a namespace link. &gt;How did you test xml:space option? Using data/text.xml. It's pretty simple, but enough.
It's a way to deal with macros being expanded in multiple crates, but still needing a reference point to find imports. Explained here: https://doc.rust-lang.org/nightly/book/first-edition/macros.html#the-variable-crate
I have definitely experienced Illegal instruction on stack overflow. It can happen. It was either on macos or windows 10, I don't remember &gt;&lt; And probably at least one or two stable rust versions back.
Thanks. I haven't found any opposite of this functionality thyough. I'm thinking about it, but there is a lot of hard algorithms to be involved, let alone aligning svg shapes such that it look close to the character it should morph to. 
Yep, that would be definitely nice, maybe someone can build it.
Ah, thanks so much! I read the second edition, which doesn't have a section on Macros written yet.
The `?` is always filled in prior to query execution. I'm not really sure what you are referring to there, but it definitely doesn't get computed for every row. As for varchar performance: if the text is less than four bytes, it still uses the much more expensive string comparison instructions rather than integer / word comparisons. (There's a variant of this where you join from the lookup table to an integer key that has *somewhat* similar performance profiles, if the query is planned correctly [but don't count on that], but then other things like sorting by enum value become extremely slow since you can't index the value, only the integer key).
The book is a good source for more in-depth info, you can find the modules section [here](https://doc.rust-lang.org/book/first-edition/crates-and-modules.html). 
Thanks for the kind words. AFAIK, extracting namespace link doesn't make much sense outside of treebuilder, because you need to extract the current position within the tree. Because it's tree building rules are a bit different, it doesn't follow the +1 depth on start, -1 depth on end. 
Manual implementations of `Fn` are unstable too, so if you're going to use nightly and feature flags you might as well just use `impl Iterator`.
Interesting point. One such case I can imagine is a singleton type that assumes it is not moved. If its public API only returns references to the type, and no way to construct a second value of the type (so you can't use `mem::swap` or `mem::replace`), then I *think* that there is no way to move the value using only safe Rust. But with the `take_mut` crate, you can do: let r: &amp;mut Singleton = some_crate::get_singleton_mut(); take_mut::take(r, |singleton| { singleton.do_something(); // Oops! `do_something` assumes the Singleton has not moved from its original address! singleton });
Those links are actually pointing to the Github repos not crates.io.
Or the WIP new book. https://doc.rust-lang.org/nightly/book/second-edition/ch07-00-modules.html
How much are language servers tied to using loopback IP for interprocess communication? My experience is that shared memory is orders of magnitude faster as well as more direct. 
[combine](https://github.com/Marwes/combine) is my favourite parser, I found nom to be overly complicated. I managed to write a [crappy quake level viewer](https://github.com/jFransham/quake-level-loader) with it, but I think I would have had a better time with `combine`. It's even better with `impl Trait` on nightly.
Apart from the book I'm not aware of an explicit tutorial, but what are you having trouble with? Starting with a single file program fn double(x: u32) -&gt; u32 { x + x } fn main() { println!("{}", double(2)); } We can move double into a module as follows mod my_module { // Things outside of the current module can only see things in the current module // if they are marked as `pub`. pub fn double(x: u32) -&gt; u32 { x + x } } fn main() { // Have to specify the whole path to my_module here. println!("{}", my_module::double(2)); } Instead of specifying the whole path we can use `use` to import a name into the current namespace mod my_module { pub fn double(x: u32) -&gt; u32 { x + x } } // Import double into the current namespace so we don't have to specify the whole path. use my_module::double; fn main() { println!("{}", double(2)); } Instead of keeping the source of `my_module` in the same file, we can put it in `my_module.rs` or `my_module/mod.rs`. And replace mod my_module { ... } with mod my_module;
deleted ^^^^^^^^^^^^^^^^0.8337 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/26091)
`xml-rs` and `quick_xml` can do this. You can try it on `medium.xml`, which has a lot namespaces. It will print namespace links. I have no idea about how much useful it is during parsing. My own parser, which I use in svgparser, doesn't care about namespaces at all... I don't know all namespace details, but they probably should be validated somehow, and I'm not checking it now.
&gt; how do I debug With a debugger! You should be able to at least get a backtrace that way. As someone else mentioned, `SIGILL` could come from `intrinsics::abort()`, which gets called in unrecoverable situations, e.g. if the unwinder itself panics.
Thanks. Yes, it took me a while to figure out how xml5ever works, to write an example.
&gt; I have looked unsuccessfully for anything in syslog about this event - is there any other log or place in the system that would have info on what happened? + &gt;"Illegal instruction (core dumped)"? It is not clear, did you open core, and stack trace give you nothing? &gt; The program was running in release mode. Would running in debug mode have given more information? Yes you have more info in debug mode, but you do this: ```toml [profile.release] debug = true ``` and have debug info while you still build your program with optimization &gt;I was buffering log messages and the last one is several minutes before the crash Why not send the logs into `syslog` once per second or more often? And, yes `drop` not save you because of it is more low level crash, I suppose you even can not catch it via signal handler, just crash. 
Thanks. I've added *Streaming parser* to the table. Looks like only `sxd-document` doesn't support `Read`. There is no sync/async categories for now. Maybe someone suggest a better classification. AFAIU, only xml5ever is truly async.
Yeah, documentation could be better, I guess. The examples in Github didn't help?
How would you fix any of this?
&gt; `xml-rs` and `quick_xml` can do this. True, but consider this xml: `&lt;a&gt;&lt;b&gt;&lt;/a&gt;&lt;c&gt;` What is depth at `&lt;c&gt;`? Depth can't be measured by counting Start tags and substracting End tags. You need to rely on tree builder for that info.
They solve similar problems. Racer is a 3rd party project that does it's own thing, RLS is an official project that hooks into the rust compiler. Currently racer is more complete, and a lot faster, but it has largely been superseded by the RLS in terms of ongoing development.
There are 3 methods to define a module (that I'm aware of). RustDragon shows two: * Module within a file. * Module as &lt;folder&gt;/mod.rs, where the folder name is the name of the module. The third is just creating a new file and using the `mod &lt;file&gt;` syntax So in `main.rs` mod maths; fn main() { println!("{}", maths::double(2)); } `maths.rs` in the same folder as `main.rs` pub fn double(x: u32) -&gt; u32 { x + x } 
That might be true, but loopback can still achieve thousands of TPS while being also being usable with curl, netcat, etc.
Examples are good. I've expected something like `parse_next()` method, but you have to use some kind of callback machinery, which was unexpected for me, because I never encountered XML parsers like this. Also you parse text data char-by-char(or codepoint), which was a bit misleading for me, but maybe it exactly by the spec. Also I don't understand why you reexport Token enum items to a global scope. I have no idea what is behind it.
Is it valid XML? Actually, I'm counting depth in examples. Regarding to namespaces: what exactly xml5ever do with them? I mean: - you can just split tag name by `:` - you can find corresponding link in attributes, which `quick_xml` is doing - you can collect all namespaces and scan for them when needed, like in `xml-rs` So yes, *namespaces support* is a bit misleading title.
I believe there may actually already be a fix landed for an upcoming release? https://github.com/rust-lang/cargo/pull/3970
There are a lot of crates like secstr ([1](https://github.com/quininer/seckey), [2](https://github.com/stouset/secrets), [3](https://github.com/cesarb/clear_on_drop), [4](https://github.com/burdges/zerodrop-rs)). How does yours compare to the others?
It's not. But we parse it with errors as: &lt;a&gt; &lt;b&gt; &lt;c&gt; And assign its namespaces in accordance to that.
Not the OP, but even when I worked with the (original) Book instructions regarding modules, I had a really hard time understanding the correct way to place module files in directories. I gave up at some point and resolved to come back to the topic and methodically investigate the different options, and whether they work, and then take detailed notes. Perhaps the new Book is better when it comes to addressing questions on this topic, but I found the original Book's handling of the subject frustrating. I recall having to rely on some blog postings to help me figure out what was the right way to do things.
How do those get used with something like this?
It is also inserted by `__builtin_trap` in e.g. in GCC, which emits an illegal instruction, that gdb can catch an interpret as a break point.
I've never heard of Discord. The link takes me to an invite code, but doesn't mention anything about why I should use it. Here's a Wikipedia entry for others who are also unfamiliar with it: https://en.wikipedia.org/wiki/Discord_(software)
I work at a Japanese company in Tokyo and I write Rust at work. However, our company doesn't actively seek experts on any specific language – and from my experience that is the case of the most Japanese companies. (Plus we don't have any production systems in Rust – something I wish to change in the future!) If you happen to live in Tokyo, I highly encourage you to take part to a local Rust gathering, you might get some info that way. Also, there is a Japanese Rust Slack. Send me a PM if you want an invitation.
If the stack overflows into executable memory ( rare these days due to stack smashing mitigations that kill the process before then ), then stack data could be read as invalid instructions.
Perhaps this core dump contains the bytes of the illegal instruction?
Well I might as well use an ugly implementation of `SliceTails&lt;'a, A&gt;` sooner. The point is I want `SliceTails&lt;'a, A&gt;`, I want it to be an actual nominal type because that comes with many advantages but right now to implement such an iterator you get something like: struct SliceTails&lt;'a, A&gt; (&amp;'a [A]); impl&lt;'a, A&gt; Iterator for SliceTails&lt;'a, A&gt; { type Item = &amp;'a [A]; fn next ( &amp;mut self ) -&gt; Option&lt;&amp;'a [A]&gt; { if self.is_empty() { None } else { self.0 = &amp;self.0[1..]; Some(self.0) } } } Which is just a lot more verbose, unelegant and possibly even less performant than just using `(0..slice.len()).map(|i| &amp;slice[i..])` somewhere. Ideally I would just type SliceTails&lt;'a, A&gt; = iter::map&lt;ops::Range&lt;usize&gt;, NAME_OF_CLOSURE_TYPE&lt;'a, A&gt;&gt; to return `SliceTails&lt;'a, A&gt;` 
Sadly not. `exec_replace` for windows in cargo does the same as before. You can only replace the process on unix. So for Windows some other logic would have to be applied.
&gt; I'm not really sure what you are referring to there, but it definitely doesn't get computed for every row. I was thinking of prepared statements; I thought the postgres syntax for statement parameters was the `?`, but apparently that's just the syntax in the client libraries I've been using. All I'm saying is that the enum string lookup (for the statement parameter values) can't happen at PREPARE time because the parameter values aren't known until execution time. The lookup would have to happen once per statement execution. On the results side of the coin, if query results for enum columns come back to the client in string form, presumably there'd have to be some kind of int-&gt;string lookup/conversion for each of those result values. &gt; if the text is less than four bytes, it still uses the much more expensive string comparison instructions rather than integer / word comparisons. In the schemas/queries I've worked on, I think the I/O cost would be way bigger than the int vs string comparison cost difference. But I'm sure you could construct an example where int vs. very-short-string comparison makes a significant difference. I'm too lazy to actually measure though.
&gt; I think cargo is missing a really critical feature - the ability to collect and report the licenses (and any additional notices) for the crates being pulled in. All I really need is a flag that says: do not permit any *GPL. If I try to compile and there is [LA?]GPL code in it it refuses to compile.
&gt; &gt; https://github.com/SimonSapin/rust-typed-arena So, it looks like there was [an unstable `TypedArena`](https://doc.rust-lang.org/1.1.0/arena/struct.TypedArena.html) in the nightly Rust around 1.1, but [it was removed in 1.2](https://doc.rust-lang.org/1.2.0/arena/struct.TypedArena.html). Do you happen to know why? Or rather, can someone link to a RFC, Github issue or something about it?
FYI, please see https://docs.rs/trust-dns and https://docs.rs/trust-dns-resolver for example docs on the libraries, added recently.
I think there should at least be some description of functionality indexed or tagged. Beyond that, I don't know.
Support for workspaces is highly appreciated!
Aw, dang. Thanks for clearing that up!
&gt; compared to, say, python's There are actually numerous parallels between the module systems of Rust and Python: * module `foo` in Rust is either `foo.rs` or `foo/mod.rs`, just like in Python it is `foo.py` or `foo/__init__.py` * conversely, `mod.rs` is essentially `__init__.py`: the "table of contents" for a module (package in Python) * `use foo;` is `import foo` [1] * `use self::foo;` is `import .foo` * `use super::foo;` is `import ..foo` * (similarly if you add more `super::` or dots) * aliasing uses `as` in essentially identical way in both languages * `use foo::*;` is `from foo import *` * `pub use self::foo::bar;` is the common reexport clause which is written as `from .foo import bar` in Python * all `pub` symbols in Rust are like non-underscore-prefixed symbols in Python, and they automatically form the equivalent of the`__all__` list (i.e. symbols that will be star-imported from a module) The important differences are: * in Rust, submodules have to be declared with `mod foo;`, whereas in Python they are found automatically in the file system (+/- import hooks) * Rust has the ability to define inline submodules (`mod foo {... }`) which is used mostly for unit tests and not much else * you can say `use foo::{self, Bar};` which in Python would require an `import` + one or more assignments I'm guessing the most common gotcha for people coming from Python and other languages is the first difference: the requirement for `mod` declarations. This is probably what makes the entire system seem finicky, since the compiler would at first just "not see" the modules that are "clearly there". I'm not sure exactly what was the rationale for requiring those declarations in the default case (when you don't modify the lookup with `#[path]`, for example), but maybe it will be relaxed in the future. [1] Assuming `absolute_import` is on, or if it's Python 3.
Thanks, this helped a lot
If you don't mind using the Tokio runtime, you can also try the tokio-signal [1] crate. The Ctrl+C handling there is portable between Unixes and Windows. [1] https://docs.rs/tokio-signal/0.1.2/tokio_signal/
Seconded. The pathing took me awhile to figure out via trial and error.
I found a much simpler way to get the line number: macro_rules! dont_panic { ($($x:tt)*) =&gt; ({ const X: &amp;[()] = &amp;[]; X[line!() as usize]; }) } Testing: dont_panic!(); // line 6 if false { dont_panic!(); } // line 7 And the compiler says: warning: this expression will panic at run-time --&gt; examples/test.rs:6:5 | 6 | dont_panic!(); // line 6 | ^^^^^^^^^^^^^^ index out of bounds: the len is 0 but the index is 6 | = note: this error originates in a macro outside of the current crate 
&gt;What are the likely suspects for that particular message? You executed a CPU instruction that your CPU doesn't support, or doesn't exist.. So CPU panicked, it called the kernel which force-ably killed your program. &gt;I was buffering log messages and the last one is several minutes before the crash. Lets say I had written a proper Drop impl that saves the remaining buffer - would it have run in this situation? You need to mask `SIGILL` on your primary thread at start up, and register a function to handle `SIGILL` then execute a `longjump` to a recovery trampoline and start executing a controlled crash (flush logs, close handles, kill threads). This is _really hard_. &gt;The program was running in release mode. Would running in debug mode have given more information? Not in these scenarios. There is a flag to enable core dumps (see _all memory_ and registers when you crashed). &gt;Is there any other log or place in the system that would have info on what happened? You can execute the binary in the GDB so see the local site that executed, and see what happened.
Wouldn't such singleton type be ill-behaved/unsafe by its very nature? Moving of the type's content is an ubiquitous operation in Rust, and for an object to be particularly attached to one particular place in memory it occupies doesn't seem right. (If the reason is passing long-lived pointers to some native code to be used as callbacks, for example, the data should be allocated separately on the heap). I vaguely recall reading somewhere (Nomicon?) that "good" Rust types should be moveable at any time, but I cannot find the quotation right now...
The scope is different - RLS should be able to support various requests typical for working with a language, such as 'go to definition' or 'rename symbol'. One of them is also an autocompletion feature, for which it uses Racer directly to retrieve the completion info and publishes the results to the editor via LSP. Using RLS configured with `rust-src` (just like for Racer, explained [here](https://github.com/rust-lang-nursery/rls/#step-3-install-the-rls)) should effectively give you what Racer does and more :)
Isn't this already the case? There is a description of each crate and there are keywords and categories used for indexing and tagging.
Well, the way it is now, `cargo install` can fail depending on the if the application's dependencies have updated. Preventing that is the entire purpose of Cargo.lock. From my understanding, the desicion to not check Cargo.lock into crates.io was because it would never be used, but then they added `cargo install`. For libraries, it can be ignored like it is now. Installing binaries on the other hand would read it to get the specific versions of the dependencies.
&gt; Transpile Rust to C!! If I knew more about how to do it, I probably would given that it's one of the few ways to get Rust running on RISC-V.
I think mine was first, others didn't exist when I needed that functionality :) https://github.com/quininer/seckey is pretty similar. Doesn't have optional serde or libsodium support. Has good Windows support without libsodium though. https://github.com/stouset/secrets has a hard dependency on libsodium, mine can use libsodium optionally https://github.com/cesarb/clear_on_drop is, as the name implies, clear on drop only, without the other features, but looks like it's really advanced clear on drop (with support for like, primitives on the stack?? not just vectors) https://github.com/burdges/zerodrop-rs same but "Nolonger maintained. Used ClearOnDrop instead"
 Say you have a project and it has this directory structure. src ├── lib.rs ├── stuff.rs └── things ├── mod.rs └── impl.rs In `lib.rs`: // Declare modules mod stuff; mod things; // Import 'impl' module from 'things' use things::impl; // Import everything from 'stuff' use stuff::*; The `mod.rs` file in the `things` directory is similar to `lib.rs`: mod impl; If anything is made available from `lib.rs`, it's available from anywhere within the project, even within modules.
I remember antirez being fed up with "false" crash reports in Redis that he bundled a memory testing tool inside the distribution to show people that the problem most often was their faulty RAM.
In the case of a binary, you should nail down you dependencies in `Cargo.toml` if you wish for them to never update.
According to the message, you got a core dump. So you should be able to open in in gdb and examine it. The likely problem is, you'll get just addresses and not reasonable symbols. It might be worth trying to recompile the binary (with exact same source code and exact same compiler version) in release mode but with debug symbols enabled. This *might* produce the same binary output, just enriched with the symbols, so opening it in gdb would produce better output. But save the original binary just in case it doesn't really match. If you can make it crash again, having the debug symbols will help you. These (in theory) should not slow down the program or change it in any way, they just make the binary significantly bigger.
You may not believe it, but you just blew my mind. A lot of stuff starts to make sense to me when I think about `&amp;mut` as an *exclusive reference*. And I was wondering why `AtomicUsize::fetch_add()` uses `&amp;self`. I mean it is totally mutating the value, it should be using `&amp;mut self`. But the reality is that the operation does not need exclusive access, it is using atomic primitives, therefore `&amp;self` is sufficient. This was not obvious to me at all before I read your post.
Indeed, normal Rust types must expect to be movable whenever they are not borrowed. But as you mention, FFI code might have types with externally-imposed restrictions. One example of this is the standard library's private `sys::Mutex` type. I expect any singleton like this *would* be allocated on the heap. The problem is that other unsafe code (including the `take_mut` crate) can move the singleton if you leak an `&amp;mut` reference to it, which (I think) is not possible using only safe code.
This is not the problem, we have a cross platform crate that handles Ctrl+C just fine (https://crates.io/crates/ctrlc). The problem is that `cargo run` terminates the executable it started when it itself receives a Ctrl+C, possibly pulling the rug from under the Ctrl+C handler of the child process. This only happens on the Windows platform.
I see. So again, namespaces are not that simple and saying that they are supported is a bit useless. Does XML spec has some explanation about it? Classification maybe. Like: "complete XML parser should support this, this and this".
&gt; the crates.io site only seems to show them by their names It also shows them by [keywords](https://crates.io/keywords) and by [categories](https://crates.io/categories). &gt; you cannot tell what they are by their names, the crates.io site only seems to show them by their names When you look at [one of the lists of crates](https://crates.io/categories/database), you see a brief description of each one, along with other information like current CI status, number of downloads, links to docs, and so on. That can be helpful for, say, comparing between [sqlite](https://crates.io/crates/sqlite) and [rusqlite](https://crates.io/crates/rusqlite) (of which the latter has considerably more downloads, and configured CI badges for the crate). The issues you bring up have been longstanding issues for crates.io, but there's been [a big push to address crate discoverability issues](https://github.com/integer32llc/cratesio-discoverability/blob/master/discoverability-ideas.md) this year, and a lot of progress has been made. Additionally, the libz blitz ([announcement](https://blog.rust-lang.org/2017/05/05/libz-blitz.html), [internals thread](https://internals.rust-lang.org/t/rust-libz-blitz/5184)) is also working to address this, by having one of the outputs of each library evaluation and sprint to be contributions to the [Rust Cookbook](https://brson.github.io/rust-cookbook/) showing how to use the libraries. This should mean that by the time the libz blitz is done, the Cookbook should have a list of high quality libraries and examples of how to use them, making a good number of libraries easier to discover. I don't think the Cookbook has been highly publicized yet, since it's still a work in progress, but once done should provide another way to discover some widely used crates. I think that you might be thinking of how crates.io was six months ago, but a lot has changed. If you have further suggestions, or contributions to make, for improving it further, I'm sure they'd be welcome.
Well that makes it sound like Cargo.lock isn't needed at all.
&gt; I see. So again, namespaces are not that simple and saying that they are supported is a bit useless No, no, no. See: https://www.w3.org/TR/REC-xml-names/ E.g. This isn't valid XML &lt;x xmlns:n1="http://www.w3.org" xmlns:n2="http://www.w3.org" &gt; &lt;bad a="1" a="2" /&gt; &lt;bad n1:a="1" n2:a="2" /&gt; &lt;/x&gt; Nor this: &lt;x xmlns:n1="http://WWW.W3.ORG" xmlns:n2="http://www.w3.org" &gt; &lt;bad a="1" a="2" /&gt; &lt;bad n1:a="1" n2:a="2" /&gt; &lt;/x&gt; Nor this: &lt;x xmlns:xmlns = "..."&gt; Nor this: &lt;x xmlns:xml = "..."&gt; &gt; Does XML spec has some explanation about it? Yes. &gt; This specification applies to XML 1.0 documents. To conform to this specification, a document MUST be well-formed according to the XML 1.0 specification I'm parsing as XML5, so I don't conform to 1.0 spec, so I can do whatever. In practice, I find namespaces even if document is malformed. This also means I can't grab namespaces using depth integer. I could attach namespace to each item, but that seems too wasteful. 
A key difference to remember that confused me for a while, is that paths and use statements look at a different place by default. Normal syntax, like `foo::bar` is relative to the current module, whereas a statement like `use foo::bar` starts searching from the root of the crate (`lib.rs` for a library project or `main.rs` for an executable project). If you want `use` statements to be relative to the current module, you have to do `use self::foo::bar`, and if you want normal paths to be relative to the project root, you have to write `::foo::bar`. This may seem like an annoying difference, but once you get used to it, it's actually quite convenient, they are both the behaviour you most commonly want for each situation 
Maybe I just don't see them. I'll go look harder...
[BRILLIANT](http://i.imgur.com/H9SWZ7S.png)
Ok, I'm sorry. I do see them organized by keywords and by downloads. That does give me somewhat better ways of looking through them. E.g., selecting the "time" keyword narrows it down to 36 crates, which is much easier to look through. Thanks
I actually started playing with this idea last week; using the `syntex` crate I obtained a Rust AST. You then need a visitor to walk the AST and write out equivalent C constructs.
XML is hard... I will set *namespace support* to *Yes* for *xml5ever*. Should be fine by now, I guess.
No need to be sorry, you are welcome! I think the information is mostly there, it just might not be so easy to find if you don't follow what's happening in the community closely. You have to scroll down to learn about keywords and categories. I know about them because I was around when they were introduced. If you just search for a crate, it is very easy to miss them. I also just realized that a lot of the big crates maintained by the Rust developers don't have categories or keywords.
Yeah, this mirrors my experience, Rust made a lot more sense to me once I stopped buying it and just treated &amp;mut as "the type system blocks you from acquiring another reference if you hold this one for _whatever reason_." `mut` as a keyword really spreads a wrong idea and the documentation taling about 'mutability' and 'interior mutability' isn't helping much.
[This blog post](https://manishearth.github.io/blog/2017/05/14/mentally-modelling-modules/) by Manishearth is how I got a full grip on modules and imports in Rust, it might help you too.
&gt; if rustc smart enough to find unnecessary use and suggest what use should I type to resolve unknown symbol issue, why it doesn't fix this by itself? Because it's a compiler. It doesn't ever edit/change your code.
&gt; I couldn't find a single one which also worked on Windows. As I know https://github.com/ihalila/pancurses works on windows, what is problem with it?
It took me an embarrassingly long time for me to find out about this. I remember writing `std::xxx` and the compiler complaining that it couldn't find `std`. I resigned myself to using only the main.rs file for quite some time 
pancurses is exactly this, curses style lib that works on linux and windows.
Another thing that can be confusing in Rust is that paths in `use` declarations are absolute by default, but paths in expressions are relative by default.
&gt; Because it's a compiler. It doesn't ever edit/change your code. Why? As I known java compiler is part of eclipse, it used not only for compilation, but of analyse and code refactoring. Also as I read `rls` uses `rustc` to analyse code, so why not build on top of `rustc` tool that simplify coding life? 
Funny you mention this, my wife and I were just talking about this. She would LOVE if I found a job in Tokyo, and if one came up where I could also be paid to write Rust...well that'd be the best of both worlds and may just be the secret to happiness!
The Java compiler does not do refactoring. That's the IDE doing it. RLS will enable IDEs to do refactoring. It's still not the compiler's responsibility.
If there is no `"*core*"` file in cwd of your program, then may be your OS gather core dump somewhere else, for example `systemd` have core dump gather feature.
But, as i recall, on windows it makes its own window to do it. I could be wrong.
&gt; The Java compiler does not do refactoring. That's the IDE doing it. Ok, let me reformulate compiler consist of "parsing + AST creation" plus all other sutff. Obviously IDE can reuse "parsing + AST creation" part of compiler. And several IDE do this, while other reinvent wheel and reimplement "parsing + AST creation" by itself. And after that IDE can apply some algorithms to AST to do refactoring. `rls` can (and as I read) do reuse part of `rustc` to do code parsing, why not reuse part that generate suggestions. Actually, before `rls` implement it I think about python script to parse `rustc` and remove unused `use` and add missed `use`.
RLS will bring the features you want, eventually. But it's not going to happen today.
There's no benefit to Diesel providing query builder methods for something like transaction isolation levels. You should just use raw SQL for that.
Vim has the [DrawIt!](http://www.vim.org/scripts/script.php?script_id=40) plugin and Emacs has the built-in `picture-mode` and related `artist-mode` (the latter of which lets you "draw" with the mouse). Both of these options let you move the cursor anywhere on the screen (even if there aren't already characters there) and issue special commands to draw lines, arrows, and boxes. Much of the time with Vim I don't even need to use DrawIt, you can get a lot done by selecting sections of rows or columns and using `r` (replace) followed by the desired character (`-` or `|`) to fill that selection with the character, although it is useful for those situations when you want to move the cursor freely over the canvas. This only gets you basic lines, arrows, and boxes, you'll still have to go in and round edges or whatever, but it works good for creating a rough outline of what you want. I haven't really used the Emacs options as I use Evil (vim bindings) for Emacs and it doesn't play nice with picture-mode; it also doesn't seem as easy to use (compared to DrawIt): https://emacs.stackexchange.com/questions/2428/example-of-drawing-rectangle-in-picture-mode edit: I should also mention Emacs has a [table editing mode](https://www.emacswiki.org/emacs/TableMode)
I find that this is a *MUCH* better explanation than the original book. Being perfectly honest, prior to this, I'd essentially play with `use` statements until they worked, not understanding them at all.
&gt; is there any other log or place in the system that would have info on what happened? Some distribution use systemd-coredump to manage coredump. In this case you should use `coredeumpctl` to list the coredumps. You can then use `coredumpctl info` to get more info, including a stack trace (see the man page for how to filter the output). You can use `c++filt` to demangle the symbols that start with "_Z". You can also use `coredump gdb` to use gdb on the coredump.
Hmm... Normally I'd expect the trait disambigutation syntax to work here. That's `&lt;ConcreteType as Trait&gt;::AssociatedType`. So in this case: ``` pub type BoxBaz = Box&lt;Baz&lt;&lt;Baz as Foo&lt;u8&gt;&gt;::Output = (), &lt;Baz as Bar&gt;::Output = ()&gt;&gt;; ``` But I get: error: expected one of `(`, `,`, `::`, `&lt;`, or `&gt;`, found `=` --&gt; assoc.rs:16:52 | 16 | pub type BoxBaz = Box&lt;Baz&lt;&lt;Baz as Foo&lt;u8&gt;&gt;::Output = (), &lt;Baz as Bar&gt;::Output = ()&gt;&gt;; | -^ unexpected token | | | expected one of `(`, `,`, `::`, `&lt;`, or `&gt;` here Hopefully someone more familiar with the quirks of syntax can explain why it's choking on the equals sign.
I've used the paid services before. For various reasons, they suck pretty hard. TBH, I don't expect to be shipping rust-based software in a professional product for at least another few years. I also have corporate processes and a team of lawyers behind me. I'm thinking about all the small teams, startups, and embedded type folks who have to do the same job I do, but with a lot fewer resources.
This took me a long time to realize, and made things much easier once I realized it. Hopefully this detail is made prominent in the new book.
It should get more stable soon. I've been working on some stability stuff recently. The version currently in nightly is much more stable than the previous version, so if you haven't updated, give it a try.
language servers use stdio for communication, not ip. Basically it doesn't matter, the ipc cost is absolutely trivial compared with anything a language server does (especially running the compiler) so it is not really worth optimising.
You might be interested in https://github.com/killercup/rustfix
To be fair, [`std::process::abort`](https://doc.rust-lang.org/std/process/fn.abort.html) didn't exist then (only `libc::abort`), but it does, and is stable, now.
Run a memory test, it might just be faulty RAM.
One more detail: you can `use` modules locally: fn foo() { use std::time::Duration; //... } 
Ubuntu 1604, dual v2 xeons, ecc ram
I'm new to rust. Have used python pdb but nothing else. Is there an article you would recommend on rust debuggers?
I did set the log flush more often, the issue was I only integrated logging last night! Now have adjusted. 
The disambiguation syntax doesn't work for specifying the associated types in trait references; the compiler thinks they're just type parameters, so the equals sign isn't at all expected nor valid.
RLS is still very new. It takes a while for solid tooling to be built around a compiler.
I'm somewhat concerned about non-lexical lifetimes, which would be a new axiom of Rust without adding any other libraries.
Coroutine libraries also might be a concern.
Thank you for the reminder.
The java compiler is not part of eclipse. RLS literally is the tool you are looking for -- it is built on top of rustc and does refactorings.
There is also the new `pub(crate)` which allows you more flexibility in organizing internal helper modules.
i use Eclipse, it helps me in learning Rust faster , since new test projects for small codes are generated using the Wizard. in fact i have not enabled all the features of Rust in Eclipse, as i go along and start coding myself (without copy paste) i will gradually enable them. i mean features like https://github.com/RustDT/RustDT/blob/latest/documentation/Features.md#features
Step through the code line by line using a open source IDE.
You don't need a debugger that's particular to Rust. I don't have any beginner tutorials at hand, but with GDB it would just look something like: $ gdb your-program ... startup messages ... (gdb) run ... wait for SIGILL ... (gdb) backtrace
`infer_schema!` is only used at compile time so you could have the infer_schema use the .env file and your connection function use the .env file if it exists or `app_dirs`or whatever. If you don't want to do that, you can also not use `infer_schema!` and copy/paste the output of `diesel print-schema` in your code.
By "trait reference" you mean trait object?
Hmm, like `let value = dont_panic::run(|| some_expression());` still using drop/forget hackery inside. That also makes sure the expression can't `return`/`break`/etc. to accidentally bypass `forget`.
Its really amazing to see the Silence of the Lambs about Eclipse, its been there for years with all those features but yet we need RLS, i smell JetBrains behind this conspiracy, and no it not a Theory because both exist
[Trait aliases](https://github.com/rust-lang/rfcs/pull/1733) would help, provided the issue which prevents constraining `Baz` itself is fixed. Otherwise, new syntax which would address this would be to allow Baz&lt;Foo&lt;u8&gt;::Output=(), Bar::Output=()&gt; Or, another approach: Baz + Foo&lt;u8, Output=()&gt; + Bar&lt;Output=()&gt; This would rely on allowing supertraits as additional constraints on object types, which isn't presently allowed.
This doesn't seem to allow for optimization -- `if false || false { dont_panic!() }` still warns.
[removed]
You're right. However, I found another way (sort of). Just brute force: extern { pub fn extern_panic_on_line_0(); pub fn extern_panic_on_line_1(); pub fn extern_panic_on_line_2(); pub fn extern_panic_on_line_3(); /* ... */ pub fn extern_panic() -&gt; !; } macro_rules! dont_panic { ($($x:tt)*) =&gt; ({ if line!() == 0 { unsafe { $crate::extern_panic_on_line_0(); } } else if line!() == 1 { unsafe { $crate::extern_panic_on_line_1(); } } else if line!() == 2 { unsafe { $crate::extern_panic_on_line_2(); } } else if line!() == 3 { unsafe { $crate::extern_panic_on_line_3(); } } /* ... */ else { unsafe { $crate::extern_panic(); } } }) } Then you get an error saying e.g. linking failed for `extern_panic_on_line_6`. Problem: for multiple calls to `dont_panic!()` to work, I had to remove the `-&gt; !` from the function definition. This seems like it could screw up dataflow analysis since `dont_panic!()` now returns.
I mean "any place that references a trait", both as a trait object and as a generic bound.
I'm interested in that slack room. Living in Tokyo now
interesting. i might give that a try. thanks.
&gt; it depends on how much cache pressure you’re under, but caches are larger than ever these days Caches may be larger than ever, but your L0/L1 cache is just as tiny as it ever was - and on multicore systems memory bandwidth per core is dropping real fast. It's worth trying to pack data in memory as much as practicable, at least when this can be done without undue cost to L1icache and without spending too many CPU cycles in access.
I believe the benchmark above, however crude, demonstrates that it's all about L2 or even L3 cache pressure, not L0/L1. Bitfields/bitflags performed noticeably worse in conditions where the CPU wasn't waiting on RAM, even just being in L2 or L3.
&gt; Especially interesting for me, how do you use `use`? I often use `use` locally. For example, I may write something like use glutin::WindowEvent::*; match event { // ... } or use super::ast::*;` I may also write something like let dt = ::std::time::Duration::from_millis(10); ::std::thread::sleep(dt); or impl ::std::default::Default for MidiTuning { // ... } But I plan the architecture first and then implement it bottom-up.
There's some bugs in the optimizer/LLVM notably in handling of loop(). https://github.com/rust-lang/rust/issues/28728 I hit this bug personally on OS X in rodio crate where sound would not play on my system but would on other OSes. Fixed by shockham here, https://github.com/tomaka/cpal/pull/154/files
&gt; My (admittedly weak) argument for I-U-F signatures: the use-case is building a signed message chucks at a time. I call update on each chunk as it is computed. This way I do not need to coalesce all of the chunks in a single temp buffer before signing. For big messages this is an enormous memory savings; for larger-than-memory messages it is the only way. Yes, I understand this. Interestingly, though, nobody has actually approached us with the need to sign very large messages yet. In the case of Roughenough, I only glanced at the code, but I think that any coalescing into one buffer doesn't affect performance or memory requirements appreciably. You wrote the code to use an I-U-F interface but IIUC (based on a GitHub search for `update` in the code) it would have been simpler to just make a `Vec&lt;u8&gt;`, append both two parts into it, and then sign the `Vec`. &gt; A counter argument is that digests already provide I-U-F, so hash the message chuck-by-chunk and sign that. I respond that this is not going to work in all situations as sign(hash(byte stream)) != sign(byte stream) and some situations require the latter. Right. This is why *ring*'s signature functions take the message to be signed instead of the digest of the message to sign. I think I know exactly how to provide an interface that supports an I-U-F interface (or, more likely, a similar callback-based interface) that can work for all signature algorithms we support. But we don't have any real-world demand for that yet, so we've not bothered to actually implement it. And as far as people go off and design new signature-based crypto protocols, I encourage them to avoid signing large objects when practical. 
Very cool. I am curious if you considered using https://github.com/SpinResearch/merkle.rs or a similar Merkle tree library, or if you see a particular advantage to not doing so.
Python allows function-local `import` as well.
... uh, ok. Maybe want to check which subreddit you're posting to, though. I'd say you want /r/playrust, but I'm not sure *they* accept server posts, either.
&gt; In the case of Roughenough, I only glanced at the code, but I think that any coalescing into one buffer doesn't affect performance or memory requirements appreciably. You wrote the code to use an I-U-F interface but IIUC (based on a GitHub search for `update` in the code) it would have been simpler to just make a `Vec&lt;u8&gt;`, append both two parts into it, and then sign the `Vec` Indeed. Over-engineering on my part.
Does anyone know about this dual license (Oracle|Apache)? I haven't seen it before, wondering any legal people have weighed in.
Didn't know about that implementation, thanks for pointing it out. Roughtime's Merkle Tree is extremely simple and tied closely to the on-the-wire layout sent to clients. The linked project is probably overkill in this case.
For more info on Roughtime, see my write-up [To Catch a Lying Timeserver](https://int08h.com/post/to-catch-a-lying-timeserver/)
This is really why I am cynical about "pullreq culture" where everything has to be a pullreq. Today's youth doesn't know any more how to just say "Yo there's a typo here."
Sure, but doesn't it try to manage transactions as well? Probably the only thing I want out of a transaction manager is for it to (1) default to a proper isolation level (letting me choose between read-only and read-write transactions, since that greatly impacts performance and is easy to know semantically ahead of time) and (2) handle retries on serialization failure (since otherwise requests will fail spuriously). Managing transaction isolation levels is hardly exotic, and it's very important; there are recent research papers that demonstrate that many applications that use ORMs have concurrent failures that would be fixed by just switching to a different isolation level from the default. As someone building an ORM that has the potential to be fairly widely used, you have the chance to fix that sort of issue from the getgo.
Nah, `&amp;mut` is just short for `&amp;mutually_exclusive`. The Truth. :P
Personally I find the second book a lot better to actually learn the language than the first. The first book seemed more like a sort of glossary, where you simply look up what you need just now, whereas the focus of the second book is learning the language from 0 in a logical order.
Indexing is just calling a projection function on the object using an index parameter. There you go. Now they're exactly the same thing. In mathematics it is quite natural to write [indexing as a function call](https://en.wikipedia.org/wiki/Indexed_family): &gt;Formally, an indexed family is the same thing as a mathematical function; a function with domain J and codomain X is equivalent to a family of elements of X indexed by elements of J. The difference is conceptual; indexed families are interpreted as collections instead of as functions. Every element of the image of the family's underlying function is an element of the family. 
**Indexed family** In mathematics, an indexed family is a collection of values associated with indices. For example, a family of real numbers, indexed by the integers is a collection of real numbers, where each integer is associated with one of the real numbers. Formally, an indexed family is the same thing as a mathematical function; a function with domain J and codomain X is equivalent to a family of elements of X indexed by elements of J. The difference is conceptual; indexed families are interpreted as collections instead of as functions. Every element of the image of the family's underlying function is an element of the family. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
You can't derive or implement things for individual variants, only for whole types.
Is there a way to simulate it? Maybe introducing Flush(Rank) and Straightflush(Rank) as types then implementing Ord and PartialOrd for them?
You mean like `enum Hand { ..., Flush(Flush), ... }` and `struct Flush(Rank);`? That could work.
Post to /r/playrust. This sub is about the programming language Rust, not the game.
This sub is for the programming language named Rust. You almost definitely meant to post this to /r/playrust, the subreddit for the video game named Rust.
This class depends on this class in another path... but that class also depends on this... Refactor? nahhh import in the instance method. Such a dirty way of handling circular imports. Hate it and love it. 
Oh my god. Thank you for this. I was so confused by the `self` and namespacing in general, but knowing the python equivalent this is super easy to understand now. Are there ANY notes like this in the docs?? I couldn't find them and I looked.
Really neat project. Thanks for sharing.
After looking at your gist again, I just realized your issue. `crate-type = ["dylib"]` Change that from `dylib` to `cdylib`.
I had to once prove to a customer that the 9's count up time they wanted was impossible simply because the ram wouldn't support it unless they where willing to triple protect the entire infrastructure. It was an insanely annoying discussion because that particular customer was not even remotely close to the 9's count we were all ready handling for them &gt;.&lt;
It is a bit of a dream of mine to be able to work somewhere like Tokyo. I find it such an interesting place and there's also lots of programming related meetups etc. Work hours/culture does concern me a bit. I'm sure there are a lot of good companies to work for though. Once I build my skills and work experience up, it's something I'd like to do.
I don't understand the need to conflate the social diversity aspect and the technical diversity aspect. They're different things (both important) but orthogonal to each other. Which both don't relate too much to the idea that accessibility of the language to new people of all kinds needs to be improved.
Nice troll there. Note that the IDEA (CLion) Rust integration doesn't rely on RLS, opting to implement their own analytics instead. The Eclipse extension is mainly written by one guy, and has good debugging support, however last I checked, the auto-type and jump2ref actions rely exclusively on racer. RLS OTOH implements the language server protocol, which has client plugins for VSCode (which is the origin of the protocol), Atom, vim, emacs and probably others.
I found the same symptoms trying to run the [bytecount](https://github.com/llogiq/bytecount) benchmarks on a Core-m3-6Y30 with `-C target-cpu=native`. Using `-C target-cpu=skylake` solved the problem for me.
Pretty cool that these plugins exist, though I feel like it's exiting the realm of simplicity at this point. I guess I prefer declarative text based diagrams that get fed into a diagram generator, but it's good there are tools out there for everyone.
The simplest way would be to just reorder the enum, but I guess you want to implement it the *usual* order most of the time, just use some other order in another game. You could do it by defining a newtype around `Hand`, implementing it by hand for that type, but fall back on the inner type for most of the cases. Like: pub enum DifferentHand(Hand); impl PartialOrd for DifferentHand { fn ord(&amp;self, other: &amp;Self) -&gt; Ordering { match (*self.1, *other.1) { (Flush(_), StraightFlush(_)) =&gt; Ordering::LessThan, // Typing these by hand, so the name is likely different (StraightFlush(_), Flush(_)) =&gt; Ordering::MoreThan, (x, y) =&gt; x.ord(&amp;y), // No exception -&gt; fall back to the usual order } } 
Xamarin is not a hybrid solution as in phonegap/webui. Xamarin basically embeds a mono runtime with your application to make possible you to run your .net application. Xamarin.Mac is (idiomatic) C# bindings for macOS Cocoa framework and Foundation library. There is no performance penalty on View layer since you're using the native framework for the platform. Think as like writing a GTK application in Python. Even though python is much slower than C application's ui will be very smooth. C# is a very sane language, but it is garbage collected and very highlevel language, you may not get the same performance as Rust. There is this thing called Xamarin.Forms, a cross platform solution, I'm not talking about that. So far I couldn't like any cross platform solutions. For my experience with Xamarin, I can say it was very smooth. I use it on both desktop and mobile. It's a great way to share your business logic between variety platforms. What I like the most is, since it doesn't have an abstraction layer on top of native UI frameworks (cocoa touch, cocoa mac, android) even you don't need xamarin specific documentation. You can just follow obj-c/swift documentation and port obj-c/swift code line by line to c#. Same for android. That's I think the most important point of why I like xamarin. 
It's based on curses. Rewrite everything in Rust! ^^/s
Your demo's currently #1 on the trending page of /r/InternetIsBeautiful (I forgot to provide source, I'm really sorry for that!)
Thanks u/KodrAus for sharing your POV and emotions, I totally agree with your views. This post is definitely making me feel more enthusiastic and motivated towards contributing to libz blitz initiative. 
Well, that blew up!
One other way you can achieve this is to have a wrapper `RevRank` for `Rank`, Manually implement `PartialOrd` and `Ord` for it, and then use `Flush(RevRank)` and `StraightFlush(RevRank)` in the `Hand` enum. You can see the end result [here](https://is.gd/cYSqqt). You would still pay the cost of having to construct `Rank` differently depending on the `Hand`. I would probably have just done it manually or used a macro in this particular instance.
Why don't Rust team members care about community? Look at how swift package manager was born. They started from the [community proposal](https://github.com/apple/swift-package-manager/blob/master/Documentation/PackageManagerCommunityProposal.md). Everyone could comment and contribute. Golang team is working on their vendoring solution in an open fashion, too. They even had internet polls and stuff to reach the wider audience. [Here](https://github.com/rust-lang/rust/issues/866) is how rust package manager was born: "- I think I've done it; - OK then". Nobody cared about [concerns](https://github.com/rust-lang/cargo/issues/45) of the community, core team didn't even bother to answer to the community's arguments. How come an open-source language by a non-commercial is developed like its Microsoft?
https://github.com/rust-lang/rust/issues/866 is from 2011. Back then it was a really tiny community, and the development was much more rapid and prone to breaking. So folks were totally okay with "land first discuss later", because you _could_ do that. The concerns you linked to are from 2014. They were also from a time before backwards compatibility, and this is not a backwards incompatible change anyway. The response was "We can revisit this if it causes real problems.", which is a perfectly reasonable response especially when the team is swamped with stuff. A few months before the language hit 1.0 (and stabilized), the RFC process was made front and center -- now that Rust was becoming a stable and hopefully production ready language, stuff needs [go through the RFC process](https://github.com/rust-lang/rfcs/) and get community buy in to happen. Which is very true now, most things go through RFCs. (If you want to see lowercase cargo.toml allowed, open one!) You're complaining about stuff that happened years ago back when "try things first polish later" was possible.
This fails with `unconstrained T` error: trait A { type B; } struct C&lt;T&gt;(T); impl&lt;T&gt; A for C&lt;T&gt; { type B = T; } struct D&lt;T&gt;(T); impl&lt;T&gt; D&lt;&lt;C&lt;T&gt; as A&gt;::B&gt; { } I have something similar in my code, except of course the associated type is more complex than just T. Should I not be able to do something like this? Of course I could just copy the type into the impl, but it's an associated type from another crate, so I would have to add another dependency and I would rather not do that.
On the topic of Cargo, it appears `cargo.toml` is already allowed through case insensitivity.
That would be on a case insensitive OS like mac
What kind of Wizardry is happening on that page? :D
&gt; if rustc smart enough to find unnecessary use and suggest what use should I type to resolve unknown symbol issue, why it doesn't fix this by itself? It's a reasonable question. I imagine, then, in a big project the compilation will finish off with a long transcript of all the assumptions it had to make to have the code compile... Rust then has a very big problem! The increased ambiguity of Rust code means that there will be even more compatibility issues when upgrading versions of crates and the compiler itself. It could even become very hard with a new version of Rust, to understand what the old code meant, and why the compiler in the presence of more crates and more features, can't resolve it anymore!
&gt; Rust has the ability to define inline submodules (mod foo {... }) which is used mostly for unit tests and not much else Just a nitpick, but inline submodules can also be quite useful if you do a lot of code generation, but granted that's not that common as a use-case.
You can use `rust-gdb` (it comes with rustc ). It add some pretty printing to gdb for some rust type. For example a vec looks like: Vec&lt;i32&gt;(len: 4, cap: 4) = {[0] = 1, [1] = 2, [2] = 3, [3] = 4} instead of alloc::vec::Vec&lt;i32&gt; { buf: alloc::raw_vec::RawVec&lt;i32, alloc::heap::HeapAlloc&gt; { ptr: core::ptr::Unique&lt;i32&gt; { pointer: core::nonzero::NonZero&lt;*const i32&gt; ( 0x7ffff6c28000 ), _marker: core::marker::PhantomData&lt;i32&gt; }, cap: 4, a: alloc::heap::HeapAlloc }, len: 4 } 
What an obvious Troll
Couple lines of JavaScript :-)
That's actually not so bad for a retrodenomination. Might help might farty old brain make sense of it in the future. Thanks!
At the begining of the TL;DR section it should probably be "This week in Redox"
&gt; The java compiler is not part of eclipse. You are wrong. https://eclipse.org/jdt/core/ &gt;JDT Core is the Java infrastructure of the Java IDE. It includes: &gt; &gt; An incremental Java compiler. &gt; RLS literally is the tool you are looking for I think that https://github.com/killercup/rustfix is the best match. `rls` have no such functionality at all.
Great, that is what I am looking for. I need just teach it to apply only `use` suggestions.
&gt;I don't know of any language that does this. At least IDEA can automatically fix `using`, it even fix code that you paste into some file.
`u8`, `u16`, etc have little to do with memory management unless you're either running on a memory constrained system or use a ton of memory. It has more to do with using the proper types for whatever you're working with. Many node.js projects don't need this kind of control over the types they're working with, but *some* do, so it's not really a valid assumption that node js programmers don't know about those types. I used node.js in the past and we used buffers quite a bit since we had to deal with binary data on occasion. In fact, the last time I used node.js, many types of streams operated on byte arrays (e.g. TCP streams). That being said, you're right, node.js is frequently chosen for the use cases where it works well, and there's really not much carry over except in high level concepts like futures, async I/O and other features of tokio that overlap with node.js' style.
&gt; https://eclipse.org/jdt/core/ AIUI that's a helper compiler, not the main one that gets used for normal compilation. JavaDT is not a regular compiler. In the context of editing your code, this is important. `javac` will never edit your code. (Java can mix compilers because JVM bytecode is a stable ABI. Rust does not have this) &gt; I think that https://github.com/killercup/rustfix is the best match. rls have no such functionality at all. RLS has import deglobbing, and will have more refactoring tools soon. But yes, rustfix helps here too. I meant that the "a tool built on rustc for IDE stuff" is exactly what RLS is. Yes, it doesn't have all the features yet, but it's still a work in progress.
Strings in JavaScript are passed by reference, so passing it to a function won't copy it. Like most languages, strings are immutable, so mutating a string well require a copy, so perhaps that's what your meant? Passing strings in node.js is not expensive (it's a reference), doing modification operations on strings (concatenation, splicing, etc) is, just like pretty much every other language out there.
I sent this PR in a few hours before they posted their comment. I was already in the source, so sending a PR was "click on the edit button, scroll down, type a character, click two more buttons". It's actually faster than coming back to reddit and leaving a comment. But uh, beyond that, why would this be bad?
IIRC nrc has said that this feature is explicitly planned for the RLS too, so it's more than hypothetical.
With such usage you can not from the first glance to tell "hey, this module depend on thread and time" or I see how it is implemented. But not sure how bad it is. 
Agreed, my sentiment was very vague. I should have left it off with understanding why one would choose to write in a high-level vs. low-level language. Excellent clarification however and thoroughly agreed.
As I understand this is rust 1.19 feature, at now you have to implement `Eq/PartiailEq` to use `enum/struct` in `match`, see https://github.com/rust-lang/rust/issues/36891 . So somebody have to o fix `syntex_syntax` to work with new compiler.
Switching back to the sable toolchain seems to have fixed the issue. Thanks!
I hope I can find a use for this, because the output is beautiful!
Hey, I only follow rust from the outside, so this took me a bit to get. Is the idea here that if some_expression() could panic, then there would have to be some kind of unwinding code present that would try to drop guard, and then cause a link error? And as long as some_expression() can't panic, the optimizer will see you create &amp; then forget a value that you never use, and so remove it entirely. Which means its drop handler would never get called, and no link error. Is my understanding correct? Does this mean Rust keeps track of which functions could possibly panic while it's compiling? Thanks! :)
I'm writing some networking/buffer code and I'm having a really hard time with this. Basically, I'm writing a wrapper class that buffers reads (with some features the standard one doesn't support). Part of this is that the wrapper maintains a VecDeque of "chunks" of bytes. So given some runtime variable `chunk_size`, I want to create a boxed slice: let new_chunk = Box::new([0u8; chunk_size]); But it doesn't work, because `chunk_size` is not a constant. This is **super** easy in C/C++, and I just can't find a single way to do this in Rust. The best I can find is creating a Vec and calling `into_boxed_slice()`, but this requires me to also fill the Vec with items, since it trims excess capacity (and does this also reallocate?). Really, I don't even want the memory initialized - it's a waste of time. tldr: What's the Rust equivalent for: uint8_t *new_chunk = new uint8_t[chunk_size]; Edit: Well, I have something that seems to work, but it uses unsafe code: let mut new_chunk_vec = Vec::with_capacity(self.chunk_size); unsafe { new_chunk_vec.set_len(self.chunk_size); } let mut new_chunk: Box&lt;[u8]&gt; = new_chunk_vec.into_boxed_slice(); I'd love to know if there are any drawbacks to doing it this way.
Personally, I wouldn't rely on the enum order for anything. That's a very fragile and implicit way of telling your program what to do.
For every type that can't be ranked you can just make those less than the others. 
You can use the vec! macro to initialize the vec using array syntax: https://is.gd/Rc4L2g
It is and it's not that far away. Just needs some time digging into, but there are other things on the radar if I'm not mistaken. It'd be awesome if someone wanted to contribute, as most of the required stuff is already there :)
&gt; 1) This service burns CPU ilke crazy. One core is like 800% CPU. To be clear, one core cannot be 800% CPU. You are using 8 cores. I can't really give more advice than that. You're not likely to get a ton of help by posting a thousand lines of code. If you're able to pare down your problem to some more specific questions we might be able to help, but what you posted is a bit broad for anyone to give much advice.
Yes, sorry. I slipped up while writing this whole thing out. The process is using 90-100% across all 8 cores. htop shows this as "800%" for the process. &gt; I can't really give more advice than that. You're not likely to get a ton of help by posting a thousand lines of code. If you're able to pare down your problem to some more specific questions we might be able to help, but what you posted is a bit broad for anyone to give much advice. Yeah, I thought that may be the case, there's a lot of code there. But it's the same pattern repeated a ton of times. The problem is I'm not sure what's hitting 100%. I believe it's likely my busy loops, which can be seen here: https://gist.github.com/insanitybit/98452bfc5733cfb649793130dafc2c93 I'll add this to the first post. 
I haven't gone through your code in depth, but it looks like you're spawning a lot of threads so that seems pretty expected. 
Every message gets 1 thread for the timer. This thread basically sleeps for 10's of seconds, performs a single enqueue operation, and then sleeps again for 10s of seconds. So I don't think that's burning my CPU.
This is interesting, recently I've wrote a simple SQS processor in python, though we're processing long-running single-threaded java. Looking at your code, I suspect the sleeper: Duration::from_millis(2) is too aggressive (it should be less frequent).
Well, I would ideally have no sleeping at all, but I was trying to lower CPU usage. I guess maybe this is just going to run hot? Like I'm performing no IO right now, and I'm just looping infinitely across my cores... so I guess because it's yielding constantly once I add IO in there I'll actually drop CPU usage dramatically. edit: I introduced 15ms of latency for every call to batch visibility and it's still 100% CPU. I guess that makes sense, at most that would block one thread for a meager amount, incredibly infrequently. But as I build up the rest of my services these threads should also stop running quite so hot. edit2: Urgh. I think I have to ditch the fibers crate altogether. I increased that sleep you called out and now I'm getting panics. thread 'thread '&lt;unnamed&gt;' panicked at 'not initialized&lt;unnamed&gt;', /home/indie/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-0.1.14/src/task_impl/core.rs:142 
This is one of the most awesome ways to get free conference tickets I've seen so far!
Are you running it in release mode? Seriously though, you're looking for /r/playrust.
Hmm, I'll use that for now. I wonder how much of a performance penalty you pay over a simple allocation, though. I guess if it's a problem I'll just switch to using an unsafe function (or maybe I'll write my own safe function that returns a Box&lt;[u8]&gt; using unsafe code).
I would like to be able to execute specific sequences of assembly (determined at runtime). I believe that the `asm!` macro only accepts string literals, so I really don't know how to do that… Does anyone know? Edit: I just found [dynasm](https://crates.io/crates/dynasm) that looks pretty close to what I want to do.
That Ion `@graphemes` built-in is awesome.
As I said before, you might have better luck getting help if you can condense the problem to more specific questions that don't involve 1000 lines of code
The gist I linked as 16 LOC.
uh... Are we reading the same post? You linked to https://gist.github.com/insanitybit/01e62fb40506a685c701fb477fec1bdc which is 1k LoC
That's roughly correct, although it doesn't have to completely remove the guard for this to work. But since the guard wouldn't need any data itself (zero-sized) and would only have meaningful code in the drop handler, the effect is as if it didn't exist, I guess. At a high level, you have objects and drop-destructors, but as you go lower toward optimizations and generating code, that distinction gets lost. It will be simpler like just "call `drop` with this parameter here, here, and here," no longer caring that his has to do with the life of an object. In this case, it will only insert a `drop` call in the landing pad where unwinding happens. Then optimization does its thing, including pruning unreachable code. If there's no remaining code that panics, then that `drop` call is unreachable and will be removed. AFAIK this will only work within a given function, so no, it would not keep track of which functions could panic. This will probably only work if the entire expression gets inlined, as I suspect even a single remaining function call will make it keep the landing pad. (I'm not *sure* of that though -- maybe the compiler is smarter than I realize...) So it will be a bit fickle whether this works. If the guard's drop is successfully removed, there's definitely no way to panic. But if it's kept that could mean there is a way to panic, or just that the compiler wasn't sufficiently smart to optimize it away.
I have two gists in my post.
IIRC `asm!` only works at compile time. You could call an assembler at ~~compile~~ run time though.
In golang goimports fixes up your import statements (it's an extension of gofmt) - it would be great if rustfmt could include something similar
Neat! HSV or HSL might be easier to deal with than YUV for separating color from brightness, but whatever works.
Pragmatic advice: don't be afraid to use Rc and RefCell when needed and just keep all your game state in one big honkin' object. Wrap the sharing/mutation problems behind a nicer API for manipulating your objects, you probably won't actually need many of them. Use `expect()` to unwrap all the Options and Results that should never fail anyway. Avoid trying to keep references into collections in favor of just storing the key or index and re-fetching things where necessary. Use the same numeric type everywhere. And load values that need tweaking from a config file, `serde` makes it really easy to turn json or toml into a struct.
What is it supposed to actually be? There's no description anywhere...
Slides linked in the [/r/rust_gamedev thread](https://www.reddit.com/r/rust_gamedev/comments/6lz13j/game_development_in_rust/)
Interestingly, rust has already taken some kind of stance on how this should work, since name resolution has the same issue. See this example: https://play.rust-lang.org/?gist=07570bfb60973fdbcd6506b70cbd180f&amp;version=stable&amp;backtrace=0 For the purposes of name resolution, it seems that rust assumes that erased lifetimes are static, and then errors at the borrow check stage if they turn out not to be.
With a declarative language like `graphviz` it can be a real pain to fight the layout engine to get things looking "right", it seems a better solution for large graphs where the layout isn't precisely known before-hand. I just had this problem and resorted to using a GUI diagram editor (Dia), and svgbob for a second diagram, and while both worked only svgbob allows for the diagram to be seen in-line with documentation basically as it appears when rendered which is a plus for me. edit: not that there aren't better declarative graph drawing tools out there, one I've looked at but not really tried is a Haskell library/DSL called [Diagram](http://projects.haskell.org/diagrams/gallery.html)
I don't understand how an assembler called at compile time would help executing assembly that I will only know at runtime? Do you mean that I should call an assembler at runtime?
Sorry, edited comment.
"feature" Nah m8, that's a compatibility bug.
This is a breaking change but isn't a soundness issue (as far as I can tell). Shouldn't erroring on this be reserved for an hypothetical Rust 2.0?
&gt; So dropping associated type specialization for now, where it’s relatively easy to argue for soundness, seems like the right step to take This sounds very reasonable, but it makes me sad. I've got some more type-nonsense in mind once specialization with associated types is stable. Mostly, I think I can greatly reduce the number of necessary where clauses when working with typenum.
Thanks. There was actually a pretty simple reason for choosing YUV - it's what the camera returns natively, or the video for Linux API anyway. Another format my camera supports is MJPEG, which I suppose would be more work to process. SDL2 is nice too for supporting YUV textures natively.
Nah, this happens all the time. This is a very old version of syntex. It's not expected to still compile. I however agree that Rust should be much more conservative about breaking old code. Maybe epochs will solve this situation.
Are there any at least considerations regarding moving RNG to kernel so that it's possible to implement ASLR and other hardenings?
The audio quality is horrendous. Is there an original recording that doesn't have these audio artifacts? Edit: Turns out you need to turn on HD to get the better audio quality. It is absolutely unwatchable without HD turned on.
Well, so when I started this there were no asynchronous networking libs for rust. I want to make very lightweight RPC framework were most of the things are pluggable. As in it has option to run thread per connection, tokio, whatever. I also want to make proper GCD library for rust and make this run on that too. Encryption taken from CurveCP with some fixes by zeromq author. There is no description yet because I didn't want people to look at it yet. There is a lot of backtracking during develoment.
See https://internals.rust-lang.org/t/rustbelt-securing-the-foundations-of-the-rust-programming-language/5509 for discussion.
Congratulations on finishing this; having formally proven sound foundations will be a huge deal for the future of rust. I hope the paper will not be too far over my head!
Also this works: `Foo(Bar(&amp;*b)).test();` `&amp;*b` is reborrowed but still appears to be `'static`.
Gratz and Thanks for the work! I skipped through some parts and its interesting to read but i think i need a few hours for the next days to walk through this – some parts are really dense materiel :)
Looks like this project is ending soon. This could be your last chance to own a plushy Ferris.
Something like this? fn fn1 (n :i32) -&gt; i32 { n } fn test&lt;T: Fn(i32) -&gt; i32 + std::marker::Sync&gt;(value: &amp;T) { println!("{}", value(10)); } fn main() { test(&amp;fn1); } This compiles. I think `Fn` items already implement Sync, or did I misunderstand your question?
 #[no_mangle] pub extern "C" fn create_twin_int(x: c_int, y: c_int) -&gt; *mut TwinInt { let mut new_twin = TwinInt::new(x, y); let ptr: *mut TwinInt = &amp;mut new_twin; std::mem::forget(new_twin); ptr } This is so wrong. You're creating a value on the stack (because you didn't wrap it in any sort of heap allocation and Rust is all about those value type semantics), and then you return a pointer to that value. Of course when the function returns that stack memory is now free to be reused by other functions, so even though you called `forget` to skip the destructor, the memory used by `new_twin` is still free to be overwritten by later functions. What you really want to do is use `Box` which has methods specifically for this purpose. #[no_mangle] pub extern "C" fn create_twin_int(x: c_int, y: c_int) -&gt; *mut TwinInt { let mut new_twin = TwinInt::new(x, y); Box::into_raw(Box::new(new_twin)) } And in the destructor you just turn it back into a `Box` and let it fall out of scope naturally. #[no_mangle] pub extern "C" fn drop_twin_int(x: *mut TwinInt) { unsafe { Box::from_raw(x); } } Also please switch from `#![crate_type = "dylib"]` to `#![crate_type = "cdylib"]`. The former should only be used for rustc compiler plugins while the latter is for creating dynamic libraries to be invoked from other languages.
This is a good point. I guess my mental model of that wasn't quite right. What you notably can't do efficiently with JavaScript strings (which you can do with node's Buffer) type if to take a subslice of the string without copying the underlying memory (i.e. like taking and &amp;str of a subset of a String in Rust).
That's beautiful.
Very pleased that Ferris made her Kickstarter goal. 🦀💕
In preparing to resume work on [Rust Anthology](https://github.com/brson/rust-anthology) I refreshed this master list of blog posts about Rust. I imagine this could be useful to somebody. There's a lot of overlap with [rust-learning](https://github.com/ctjhoa/rust-learning) but this is probably more comprehensive when it comes to blog posts. If there are any blog posts about Rust that you love, that I am missing, please link them.
Good reasons! :D
I'm using SDL2, and many of the things in SDL2 have lifetimes, like Texture has a lifetime. So if I want to make a Button struct like: struct Button&lt;'a&gt; { rect: Rect, src_rect: Rect, tex: &amp;'a Texture&lt;'a&gt;, } Then my button also needs a lifetime annotation, and when I put a button in my GameState struct, then GameState needs a lifetime annotation. Is this unavoidable? Is there a way to wrap the Texture in something that will allow me to not have to worry about the lifetimes? In my case the lifetimes of all of these things will always be "forever". 
I'm a rust newbie, and I was going through the "TRPL" and creating my first rust program. I just installed rustc and cargo with rustup and I created the basic hello world program. fn main(){ println!("Hello, world!"); } The file is named main.rs, and I attempted to "rustc .\main.rs" (I'm on windows) it is giving me the error error: couldn't read ".\\main.rs": stream did not contain valid UTF-8 Ninja-edit, because it was a quick fix... I feel like an idiot: whoops, I guess I made a (somewhat) obscure mistake? I echo'd the files contents in powershell and apparently that didn't encode into UTF-8. Simply copy-pasting the contents into a new file worked. Guess I'll leave this here for posterity's sake...
Is there a direct link to the submitted paper?
&gt; if I want to use something as a value type in C#, I can't Fair enough, I know what I propose requires something like the opposite of &amp; to get a 'value' from a reference. Literally if you said ```type Foo = &amp;struct{....}``` there would need to be something like ```unref&lt;T&gt;``` to make a value form a reference. ```unref&lt;&amp;T&gt;==T``` and s on. you wouldn't want to write Box&lt;unref&lt;T&gt;&gt; either, there'd need to be a shortcut to switch from the borrowed to owned form. we can't write * in the 'type language' because that already means raw-pointer. my motivation is simply reducing the number of symbols used in expressing the most common case; with Rusts inference you get values back from constructor functions without having to declare the type of the local you assign to. I realise it would be quite unusual too.. but it already 'feels different' going back to manually writing 'address-of' when one is used to C++ automatically taking the address for reference parameters. (is/was there discussion around autoborrowing? .. )
The "wrapper types in Rust" post probably belongs under "ownership"? (the original post is at https://manishearth.github.io/blog/2015/05/27/wrapper-types-in-rust-choosing-your-guarantees/, and it should have the same content as the book, but the book one isn't in the new edition of the book iirc)
Maybe &lt;https://www.ralfj.de/blog/2016/01/09/the-scope-of-unsafe.html&gt; would fit into the unsafe category? Disclaimer: I am the author.
You mean &lt;https://www.mpi-sws.org/~dreyer/papers/rustbelt/paper.pdf&gt;? It's linked in the blog post.
Thank you.
First off, really appreciate the response. Very informative, made me remember my days learning about stacks, heaps, and pointer machines. So when I send over a pointer to memory on the stack, the stack won't guarantee that the memory is sacred and not write over it. In the course of using std::cout, the stack overwrote my memory allocated in the Rust library. However, when you allocate memory on the heap, its allocation is kept track of, so it won't be deleted. Now that I think about it, that would definitely happen in C++ if I tried to pull something like this. It's pretty much returning a reference to a local variable, which my compiler catches (but not Rust!). It would be nice if I didn't have to interface with C++; I really like the usage / ownership paradigm of Rust (even though I try to break it early and often). Question on using Boxes: why not use the functions in alloc::heap? The way I see it, what you wrote would be like creating a std::unique_ptr in C++, only to call release() on it so you get the raw pointer. IMO, it would be smarter to skip the smart pointer overhead and use std::allocator directly. I suppose that the analogue in Rust would be to use the functions in alloc::heap. However, these functions are marked as experimental, which I assume means that they are not suitable for general use.
I put it under the ownership section and switched the link.
Ooh thanks. That's a good one. Added.
&gt; it would be smarter to skip the smart pointer overhead What overhead are you referring to? Does that overhead exist in Rust?
The amazing advances in the ion shell remind me of Python's [prompt toolkit](https://github.com/jonathanslenders/python-prompt-toolkit). There are several mention in this announcement of specific functions inside the shell implementation, I wonder if the maintainers intend for these to be exposed as a general-purpose "superior readline", or if they're just calling out where the community has put in impressive work?
`&amp;*b` completely legitimately uses `'static` though, so that seems fine?
&gt; It's pretty much returning a reference to a local variable, which my compiler catches (but not Rust!). Rust can't catch it in this case because you're returning a *raw pointer* rather than a normal reference. &gt; IMO, it would be smarter to skip the smart pointer overhead and use std::allocator directly. `Box` already has no overhead other than the cost of heap allocating your value. It doesn't do reference counting. It is the direct equivalent of `std::unique_ptr` from C++.
`itertools` probably has something to do with those download numbers.
you can see the dependent crates for a particular crate, and in this case itertools depends on it, which is where the downloads are coming from. it's like Result, but where there is no implication of right and wrong, only right and left.
The language matters little in the long run. It's whats common between languages that makes you an able programmer. My suggestion is to start with Python as it is, in my opinion, the easiest language to learn for beginners. 
Fun history fact: we used to have both Result and Either in the stdlib, but nobody used either so it was removed. It's still useful though, so I'm glad it is in a package.
While rust definitely feels like it has more syntax than Haskell, I wouldn't hold up the latter as a language with almost no syntax :P. There's infix functions all over the standard library.
For those curious, on Windows echoing from Powershell outputs a file encoded with UCS-2 Little-Endian, with Byte-Order-Mark.
https://www.gitbook.com/book/carlomilanesi/rust-programming-step-by-step/details I still want to learn rust and don't want to get into Py2 vs py3
Then you've made your decision. If you're ever again having to decide between Py2 and Py3, go with Py3.
Which honestly is not that difrent from right, middle, and left or just "first, second, third, fourth" If there is no implication then it might be a point to evaluate anonymous enums in the same way tuples are anonymous structs.
I've thought about having an anonymous enum type (T | U) but could never figure out a nice syntax for matching on it
When I say overhead, I say overhead like function calls - which is pretty much none, granted, but I always thought I was cheating the system by doing bitwise XOR instead of equality comparisons. You're right - there is no overhead for using a Box. Honestly, I'm tempted to unroll loops at times. Speed is a dangerous drug.
SO CUTE
Okay, let's say this: // type is required to say what the other one is. A simple number is used to indicate which slot is used. let foo : (usize | &amp;'static str ) = (1 : "Hello, World"); match foo { (0 : n) =&gt; println!("usize is: {}", n), (1 : s) =&gt; println!("str is: {:?}", s), } The type ascription of the anonymous enum could be superfluous in some cases and checked b what is done with the variable in the pattern matching which is still required to be irrefutable. 
Wild guess (might be a bad idea) Box&lt;Texture&lt;'static&gt; And maybe also: type MyTexture = Box&lt;Texture&lt;'static&gt;&gt;
ok thanks will move to py3 if i find rust too difficult
Yes, this has been considered and will be pursued in the future.
You'd be right with older compilers, but in practice LLVM is going to optimize all of that down to the very bare minimum ;) For example, on the latest Rust compiler, the correct implementation of drop_twin_int using Box::from_raw produces this bare minimum assembly drop_twin_int: mov esi, 8 mov edx, 4 jmp __rust_dealloc@PLT
Thanks. You are right. Fn items already implement Sync.
Thanks! I think this kind of effort is important. There is a lot of value embedded through blog posts that we should find ways to integrate into documentation to help new people along. I know there are some resources I found useful that I've been considering where they best fit.
I quite like pest myself.
I have a question. I backed the kickstarter, but Kickstarter has never asked me for my actual shipping adress. I'm not sure how I would even be able to get the plushie. Thanks.
I think you give it closer to when they will be shipped. That way if your address changes in the interim it will still make it to you! Not sure though, but that's how it's worked for other Kickstarters...
Thanks. I was just a bit worried.
I went ahead and checked the whole list into the anthology repo [here](https://github.com/brson/rust-anthology/blob/master/master-list.md).
Are you running overclocked? Have you updated your BIOS? If you are running a new Skylake processor there are a couple of errata that need to get patched, try turning off Hyper Threading. Try turning off TSX if you have the option. * http://gallium.inria.fr/blog/intel-skylake-bug/ 
Ion uses [`liner`](https://github.com/MovingtoMars/liner) for readline functionality, if that's what you're wondering about 🙂.
It's generally useful for abstracting over either-or situations where you don't want the connotations of Result. We use Either (well, our own variant, we didn't want to pull in a crate) in Servo in the style system to cut down on boilerplate of "X or Y" situations in CSS.
For an "anonymous enum", the variants don't have names (by definition), so in order to tell them apart, we need the restriction that every type in the enum must be unique. Then, to match you need to create a variable name for each variant: fn sum(inputs: (Vec&lt;f32&gt; | f32)) -&gt; u32 { match inputs { (v: Vec&lt;f32&gt;) =&gt; v.iter().sum(), (f: f32) =&gt; f, } } To construct a value of this enum, the compiler would have to provide a `From&lt;T&gt;` implementation for each T in the enum. To call `sum()` above with an `f32`, it might look like: `sum((0.1f32).into())`.
Yeah, the reason I ask is because I'd like to see a more highly-optimized implementation of such simple Merkle trees, and I'm wondering if people are interested in merkle.rs being that optimized implementation, at least doing what I suggest in https://github.com/SpinResearch/merkle.rs/issues/4#issuecomment-264563736 in particular. it would only matter if batching were implemented.
KS was built to fund things which do not exist yet, so time between pledging and shipping can be months or years (or infinity, KS projects can fail), and both materialised and not. As a result KS does not do shipping, rather what usually happens is the builder sends a survey when they're happy with the physical goods and getting ready to start sending them out. 
&gt; It isn't clear to me that there'll be much benefit to using a Rust re-implementation over the official SQLite library (with some Rust bindings). Building Rust projects with external dependencies is always more painful than using native Rust libs. And part of the reasons of using Rust is gone.
Wish you guys good luck and success!
You'll find jobs easier with Java knowledge. You could still learn Rust, when you are hired as a Java tester. Further, Rust is no easy language. There are known C programmers who failed to learn Rust, because of wrong expectations (you can't learn Rust within three days). Just my opinion.
&gt; it's like Result, but where there is no implication of right and wrong, only right and left. In which case, you're often better off with a custom enum anyway, with more expressive names than "Right" and "Left".
`tex: &amp;'static Texture&lt;'static&gt;` might be possible, but I'm not sure about how to set up Textures. `lazy_static` may be your only option to set it up. Luckily the ru ntime cost is quite small.
Quality
I'm trying to learn in a year No professional background in software developer but learn C++ in polytechnic
Oh yeah, I totally get it! If my Rust version was bug-free and as performant as the native SQLite library, then it'd be an obvious choice. I'm just well aware that SQLite is one of the most widely used and well tested libraries. It's going to be tough to reach that level of quality. :)
I think my 'Rustic Bits' post should be filed under 'Rust in Practice'.
Why is creating an `Rc&lt;str&gt;` hidden? https://doc.rust-lang.org/src/alloc/rc.rs.html#402-425
Well, all pointers in C++ are raw pointers, but it doesn't stop clang from issuing a warning in this case. Maybe we can also give a warning for this particular pattern.
/r/playrust.
If there is a case for anonymous product types (tuples) then the same case can be made for anonymous sum types :)
The first link, "Understanding Over Guesswork" leads to a page with https error: www.hoverbear.org uses an invalid security certificate. The certificate is only valid for hoverbear.org Error code: SSL_ERROR_BAD_CERT_DOMAIN Removing the `www.` fixes the problem.
Given the wide spectrum covered by the patches, it seems to me that getting the self-hosting will at the same time unlock a whole set of possible applications. Neat!
I wrote a quick [blog post](https://sevagh.github.io/post/mpsc/) on how this subreddit helped me implement multithreading in my music player project.
/u/aturon &gt; More fundamentally, though, it would lead to highly unpredictable behavior: fn print_str&lt;'a&gt;(s: &amp;'a str) { s.print() } fn main() { let s = "hello, world!"; s.print(); // prints 'static print_str(s); // prints 'a Print::print(s) // prints 'static } What is unpredictable there? It does exactly what I expect it would do. We type check code locally, and `print_str` is generic over `'a` (like every function that takes a reference). If the user wants `print_str` to print `'static` when called with a `'static str` then it should either choose the appropriate lifetime or it should add a specialization of `print_str` for `'static` (e.g. by moving it into a trait). That trait methods are more powerful than free functions (in particular if we add specialization for trait methods, but not for functions) is nothing new. Also, if we want `print_str` to print `static` instead of `a`, we shouldn't just add this for life-times either, but also for type constraints (e.g. if `print_str&lt;T: Foo&gt;` and `T: Foo + Bar` and there is a specialization of `.print()` for `Foo + Bar` then that should be called even though `print_str` only requires `Foo` (EDIT: I think that this works, so maybe we just need to monomorphize life-times like normal type constraints). This actually sounds to me like the right thing to do. The only argument against it is: &gt; tracking information through trans would involve a massive overhaul of the compiler, and we’d have to be very smart about coalescing code with different lifetimes but identical behavior. There’s no guarantee we could do this without making the compiler significantly slower and/or creating more code bloat. That is, it's hard, has to be done properly, and is a lot of work because the compiler isn't designed for it. This argument can be used against any new feature (ATC, incremental compilation, on demand compilation, miri integration...). To me this doesn't sound like a valid reason not to do this. EDIT2: re-reading this it sounds too harsh, if I had more time i'd had make it shorter and nicer, sorry about that. Also, I think that shipping a sub-set of specialization that we want to have anyways as early as possible is a great idea, and we should do that anyways, but if we need to make trans smarter to get the language features we want, we should still pursue it and worry about making it faster later.
I wrote two blog posts about Rust: [Little tour of multiple iterators implementation in Rust](https://blog.guillaume-gomez.fr/articles/2017-03-09+Little+tour+of+multiple+iterators+implementation+in+Rust) and [Rust merge process](https://blog.guillaume-gomez.fr/articles/2016-08-31+Rust+merge+process). Maybe they might be useful? EDIT: The second one is completely out-of-date actually.
Do you already know how to build programs? (as in, fundamental data-structures and algorithms, OO, generic programming, functional programming, imperative programming, meta-programming, etc. ?) That is, do you already know how to program, in general? If you already know how to build programs effectively, then whether you should pick up Rust or Java will depend on which kinds of problems you want your programs to solve. What problems are you interested in? If you don't know how to program, the low-level details that Rust (or any other low-level language) will bother you with will stand in your way of learning fundamental programming concepts. These are more important than learning any single language, and to learn them as best as possible you should choose a programming language that's good for the task. Python is very good for this. It is quick to pick up, has great resources that introduce you to all of the above (e.g. the Introduction to CS MIT ocw course), has great libraries, in particular for visualization which are helpful when you are starting out, and supports all those programming paradigms at least at some level. It is also great for prototyping which will let you try different approaches to software design quickly. It also has lots of science-related libraries which lets you learn programming by solving problems in some science area that you find interesting, which makes "learning programming" more enjoyable for a lot of people.
I did put the return statement inside an unsafe block, which I've heard is really a message to the compiler to trust the programmer on the code being memory safe. I'm not sure if it would make sense to raise an error given that, but it will always result in undefined behavior...
Nice post! &lt;3 the clippy shoutout. We'll continue to make it even better.
I'm using v2 xeons, none of that new fangled stuff haha. 
I am currently writing an app to learn react native and decided to also learn Rust for creating the server. Currently all I have is a simple Rocket server sending some static data to the app. I'll work on adding a DB and user auth soon. Afterwards I'll probably have to look into making the server work with websockets.
Awesome, thank you so much for explaining! :)
My favorite phrase in the paper is this: "This may sound intuitively plausible, but turns out to be extremely subtle." It is about a specific technical issue, but I think it also works well as a general comment on Rust safety.
UI objects in general. Sorry for the late reply!
You want /r/playrust
This is a naive question: Why do lifetimes need to be part of specialization at all? All the examples suggest a lifetime is adding specificity. What if you had to add eg the `+ Clone` etc and deciding which impl to use was just based on the other aspects. Then if the lifetimes of that trait impl don't work you get a borrowck error (as usual)?
&gt; ...because I'd like to see a more highly-optimized implementation of such simple Merkle trees, and I'm wondering if people are interested in merkle.rs being that optimized implementation, at least doing what I suggest in https://github.com/SpinResearch/merkle.rs/issues/4#issuecomment-264563736 +1 for a robust and optimized Merkle Tree implementation. Not sure if `merkle.rs` is that implementation. I share the efficiency concerns addressed by your proposed PR changes. The pointer-chasing due to the linked-list nature of the tree and inclusion proofs would give me pause. For a performance-oriented version I'd prefer a linear (`[u8]`) representation at the cost of code complexity. An additional generality note: Roughtime adds 'tweak' values to the leafs and nodes prior to hashing. This is not possible in [merkle.rs](https://github.com/SpinResearch/merkle.rs/blob/master/src/merkletree.rs#L62). 
I'm sorry, I'm kind of new to reddit, I could have sworn I made this post under /r/playrust.
With self hosting do they mean you can use Redox to build a new version of Redox?
Some source code would be appreciated. I'm not an expert in this stuff but maybe your cpu doesn't support a certain feature or something like that. Probably something that one of us can track down if you link it here
Yes, I believe that is the goal.
If your goal is to get a good paying job quickly java is a much better bet. I'd encourage you to learn rust if you find it interesting at all, but it wont help you get a start the software industry.
https://github.com/rust-lang/rust/pull/37132 Not sure why it's hidden and not just unstable.
is https://medium.com/learning-rust not notable, for newcomers?
Another problem is that function pointers can be higher-ranked - `print_str` can be converted to a function pointer of the type `for&lt;'a&gt; fn(&amp;'a str)`, and then in order to function "correctly", the function pointer would have to know whether its lifetime is static.
I did some recategorization and it's presently listed under 'idioms'.
Fixed. Thanks.
I added the first one. Thanks for the tip.
Oh, that's awesome. Added to the 'introduction' section.
Thanks for posting this, was able to grab 2 more ferris :)
Wouldn't fn pointers to trait methods with specialization have the same problem?
That's the idea. Ideally it should be possible, albeit potentially rather inconvenient to develop Redox entirely without using another OS.
Thanks for answering.
Why don they have names? Anonymus struct fields have names, the names are 0, 1, 2, 3, etc. Exclusing homogenous types is going to bite you, there are very much useful situations where it it can return the same type but if it's the first or second one matters; what's why it's a sum type, not a union type. Also, in your syntax, how would you create an instance of `( T | U )`
Especially in APIs you don't export and simply internally within function logic; having to define an enum just to use it internally in branching logic is exhausting. I wil say though that I do not like APIs like `Iterator::size_hint ( &amp;Self ) -&gt; (usize, Some(usize))` I would really just have a: enum SizeHint { Unbounded { lower : usize }, Bounded { lower : usize, upper : usize }, } returned from that. I in fact wouldn't mind making returning anonymous products and sums in a public API a warning.
Oh, sweet, I didn't realize that liner had the auto-suggest feature from fish built-in. I'm not sure if liner has it as a goal to be feature-rich, and I'm not sure how interested in putting features into liner the ion shell is, but I will say that the [features that pt supports](https://github.com/jonathanslenders/python-prompt-toolkit#prompt_toolkit-features) make using [CLIs that are built with it](https://github.com/jonathanslenders/python-prompt-toolkit#projects-using-prompt_toolkit) truly amazing.
Thanks for the updates, /u/imperioland.
I've never given this much thought though but it seems to me that it is possible that a closure because you use some unsafe things does not implement `Sync` automatically but in fact it is `Sync` and you know it. In that case since the type of closures is not nominal I don't think it's possible to actually do an `unsafe impl Sync for &lt;that_closure&gt;` as each closure has its own type and you can't write out those types; they are purely used in inference. You can't actually do `let closure : &lt;the type&gt; = |x| x + 1;
You're welcome. :)
Heh, that's very true. ;) Indeed we were repeatedly surprised just *how* subtle things are; may favorite examples are `{Arc,Rc}::get_mut` and `Ref{,Mut}::map`.
Do you know of an RFC that would enable the safe creation of types such as `Rc&lt;str&gt;`?
It is also the 2nd in the "Top Paying Technologies" (worldwide) category
Previous discussion: [link](https://www.reddit.com/r/rust/comments/60toqc/rust_was_voted_stack_overflows_most_loved/?ref=share&amp;ref_source=link) and [link](https://www.reddit.com/r/rust/comments/6kkgz7/according_to_stack_overflow_rust_is_the_most/?ref=share&amp;ref_source=link)
What did you expect? Rust gives you more control over operational concerns at the expense of having to deal with operational concerns all the time. That's the whole point.
Which confuses me a bit, since it doesn't appear at all in the US, UK, Germany or France categories. (Same for Clojure, Elixir and F#.) Where are all these high-paying Rust jobs? Or were there not enough of them to reach a minimum threshold in any of the single-country lists?
I agree, in public APIs, you should go to the effort of having good names.
Yeah I also don't like the `-&gt; impl Iterator&lt;Item=X&gt;` unstable feature where the returntype of a function can just be `impl Trait` as it's not a trait object and it's basically just an anonymous iterator type.
My guess would be: Rust is used in places that really do care about software quality, and are unlikely to be "pay as low as you can" shops. Also, it's popularity might be over-represented in places with higher cost of living like SF and SV.
A few of them are probably the guys getting paid by Mozilla to work on Rust.
Downside then is you don't get all the useful methods that `Either` comes with. And also, there are contexts where the type parameters to `Either` are enough to convey the meaning, and the custom names you get from a custom enum type don't actually make your API more understandable.
Well it never sleeps! :-)
Full build times are approximately the same, depending on how many templates the C++ project uses. C++ is often better on incremental compilation when you touch .cpp files, because it just needs to build that file. On most C++ codebases however touching header files will lead to worse since Rust still has course grained compilation units (crates) whereas most C++ codebases don't split stuff up that way. Incremental compilation in Rust (which is already there and experimental, and improving) will lead to much better incremental compilation times since it has the benefits of both worlds, and is smarter than header file based dependency tracking.
Mozilla has only ~10 folks working on Rust. Mozilla does have a larger contingent of folks (~50?) who use Rust at least some of the time as part of their jobs; however this is still a pretty small fraction of those using Rust at work AFAICT. In general this may be more of a Bay Area skew, however.
Unfortunately that's not possible. Fwiw, I have not encountered it a second time after making changes that would have addressed a possible stack overflow condition. 
Heh, I keep forgetting that nowadays there's actually a bunch of companies using Rust, how the times have changed over the years.
Sometimes (actually, most of the time) the fact that it's an iterator is significantly more important than the full type, which can be several lines long - and if you ever change it even slightly, that's a breaking change to your API. Similarly, in the futures crate, it's not uncommon that my `Future` types are upwards of 25 lines long, and it's really difficult to understand error messages due to the amount of irrelevance in the type. The futures crate with `impl Trait` will allow the compiler to realise that the exact type doesn't matter, it's the fact that it's a `Future` that matters, and tailor error messages towards that.
&gt; Sometimes (actually, most of the time) the fact that it's an iterator is significantly more important than the full type, which can be several lines long - and if you ever change it even slightly, that's a breaking change to your API. You obviously just give the full type a name as in you return a struct with a name that Iterats like `str::CharIndices` The problem with anonymous types is that without a name you can't write documenation or it as easily or get the size of the type as it has no name. Another problem is that you can always downcast `CharIndices` to `impl Iterator&lt;Item=(usize, char)&gt;` if you need to but never in reverse. What if you want to write a function which as argument only accepts a `CharIndices` struct as returned by `str::char_indices`; it's pretty easy to accidentally pass the wrong thing and rockets have exploded over this in the past. I'm sure there are more `(usize, Option&lt;usize&gt;)` things in the stdlib than just `SizeHint` and if you want a function that accepts _only_ size hints you can do that and you can always convert `SizeHint` the other way around if you need to feed it into something more generic.
When you just want to type check and don't need to generate code, you can use `cargo check` which should lower build times by 30-50%
There's going to be region skew, and there's also going to be skew for rarely used languages which a few highly paid guys use for fun or intrigue, and they're too important for anybody to tell them 'no.'
Any difference in linker usage?
&gt; You obviously just give the full type a name as in you return a struct with a name that Iterats like str::CharIndices This doesn't solve the problem. The problem is specifying the type of doing this: `myvec.into_iter().map().flat_map().filter().enumerate()...`. You could alias it... or you could have the compiler write an alias for you, which is much more reasonable. And the `impl Trait` spec explicitly states that this will probably be coming at a future date, via a `typeof` operator or similar.
Rust generally is able to do a lot of inlining before it reaches the link step, since the compilation units are larger. Whereas for decent inlining in C++ AFAICT you generally need to turn on link time inlining. Other than that the only difference is that in a C++ codebase you link together a million tiny things whereas in Rust you link together a moderate number of moderate-sized things.
Nightly should not break old stable code, so there is no reason to use stable for dependencies, even if you could. If it does break stable code, file a bug ASAP. If you're using nightly libraries... that's what you get. Generally speaking, I do a sweep of my direct dependencies every so often and upgrade them. The Rust compiler might complain a couple of times, I fix it and run tests.
It was just a fun experiment. Doesn't have to be shocking.
Yes, I know of that problem all too well and that it exists; I'm saying that `impl Trait` is not the best way to solve it. A newtype of some kind would be better; not an alias because an alias doesn't solve the problem of only wanting that specific struct as function argument. So say if you have: fn tails&lt;'a, A&gt; ( s : &amp;'a [T] ) -&gt; impl Iterator&lt;Item=&amp;'a [A]&gt; { (0..s.len()).map(|i| &amp;s[i..]) } It would be better to have: #[derive(inner(Iterator))] struct Tails&lt;'a, A&gt; (impl Iterator&lt;Item=&amp;'a [A]&gt;); fn tails&lt;'a, A&gt; ( s : &amp;'a [T] ) -&gt; Tails&lt;'a, A&gt; { Tails((0..s.len()).map(|i| &amp;s[i..])) } Now the return value is nominal so we can do `mem::size_of::&lt;Tails&lt;i32&gt;&gt;()` and restrict function arguments and lint based on it.
we only allow fn pointers to be higher rank in lifetimes, so if we don't have lifetime based specialization it is not an issue. 
But now in your design the definition of the struct is entirely decoupled from the function in syntax, while in fact it's very strongly coupled to the function. That obviously makes no sense. The most reasonable way to do this is to enable `mem::size_of::&lt;tails::Output&gt;()` or similar. This would also work with closures in the language as it stands today, which similarly output anonymous structs.
One option would be to store textures in a `Resources` struct or similar, and then have an enum such as `Image` that matches different textures. This has the advantage of being able to be serialized as well. 
In fact in subject he has `core dumped`, so no need to run it again, and wait several days for new crash, it has to type `gdb your-program core` and see what hapens. 
I think you don't need to add the 'b lifetime to new. You just need it to return an RClient&lt;'a&gt; and make the &amp;mut core &amp;'a mut core for it to work I think. Let me know if that doesn't work and I can dig up an example from my GitHub API library you can use that seems similar to this.
`RClient` takes a mutable reference to a `Core`, but doesn't own the struct itself. This means that when your `new` function ends, it gets deallocated and doesn't live long enough. Do you really need `core` to be borrowed? Perhaps you could have it be owned by `RClient` and do away with lifetimes altogether. A better place to ask about this sort of thing is the [#rust-beginners](https://chat.mibbit.com/?server=irc.mozilla.org%3A%2B6697&amp;amp;channel=%23rust-beginners) IRC channel.
Rc already handles unsized types; you wouldn't need an RFC, you'd need to mark this stable.
Doesn't rust took longer generating ir code?
Good. You should take a look at how OpenBSD performs early RNG seeding so that it's possible to get good entropy early.
Not really that much longer when comparing to abstraction-heavy modern C++.
Is there a good guide of when I should use link time optimization (`lto = true`) in my `Cargo.toml`?
Maybe we should add a warning about this in the book. Then again if we add warnings for all of Windows' stupid quirks the book will triple in size.
Tooling is still behind on Rust as, AFAIK, there's no support for distributed compilation whereas it's a common build speedup in the C++ world, especially Windows.
General Guideline: If you create a *thing* in a function, you can't return a reference to that *new thing* from the function you created *that thing* in. You can pass a reference, to a different function *called* within the function (your making the thing in). If your arguments take a reference (with a lifetime) you can return that reference. --- There are exceptions for when you use `Box&lt;T&gt;` or `unsafe` or some library things. But this is the _guideline_ not a rule. Basically you and pass pointers *down* the call stack, but when they go up, trouble starts. 
AFAICT for decent inlining in C++ you move things to headers, not turn on link time inlining.
Few thoughts: It would be way easier to start with blocking api, to learn rust, and then add non blocking api version. Blocking client would be much more straightforward ([example here](https://github.com/Nercury/confluence-rs/blob/master/src/lib.rs#L51) of a confluence client I wrote a while ago). Also, blocking client will be fast enough for the majority of use cases, although I admit that getting non-blocking api working can be very fun. Second, in rust, structures that use references inside are rare, and are usually reserved for temporary values, like iterator data or parser state. It is much easier to work with structs that own data. However, rust has an escape hatch here - there are ways to refer to mutable value from unmutable context (RefCell), or share value (Rc). In fact, the Core implements this very mechanism under the hood of "handles". In your client struct, make sure it owns the tokio handle, and your struct won't need references inside. You can obtain the handle by calling `.handle()` on Core and you can `.clone()` a handle to get another handle that refers to the same core. Tokio handles would allow to reuse the same core for multiple clients, which was probably what you was trying to achieve.
Thank you! I noticed that I typed it out as &amp;mut as it needed to be mutable, but my type should have just been Core in my struct. That's why it said it should have been borrowed. Changed that to not borrowed and everything worked great. And I'll check out the IRC channel. Thanks
Yes, and this greatly impacts your ability to organize code and also destroys compile times. There's only so much of this you can do. Plus, it's opt in, whereas Rust can do inlining within a single crate as much as it wants.
I think there are people who like it's opt in. In some sense, C++ gives you more control.
Right now, splitting projects into many crates (libs) yields the best result. It is not too bad, because crates in rust are easy, and you can refer to project-only subcrates by relative path.
I think opt in is a fine choice. I think that C++'s solution for inlining is terrible and is collateral damage of the greater design of how header files work, as opposed to being an explicit design choice to be opt in. Every time I need to inline something in C++ I have to often rearchitect my header files, and then deal with the fact that compilation times get destroyed since now everything is in a header file. It's not great.
&gt;This means that proportionally, more developers wanted to continue working with it than any other language.
i think its safe to say this data is wrong
If you want to check if all the values of `m` are factors of `a`, then you probably want this: m.iter().all(|x| a % x == 0)
By "tweaks," are you referring to the "\x00" and "\x01" prefixes? If so, I believe merkle.rs was changed to always use those same prefixes: https://github.com/SpinResearch/merkle.rs/blob/249234cacaf2891ee4371846b6a32bfae0743ab9/src/hashutils.rs#L62-L75 Regarding the efficiency, I don't know what their thoughts are but IIRC they at least didn't object to changing to doing things the way I suggested. Not sure what their API stability goals are though, or how much (or even if) the API would have to change to make things more efficient, as it's been a long time since I looked at it.
The problem is that there are multiple factors I have to compare. All the factors are stored in the vector.
Right, that returns `true` if all of them can divide into `a`, so that's what you can use in your `filter` closure.
I also have this implementation, but I don't find it very elegant. pub fn sum_of_multiples(n: i32, m: &amp;Vec&lt;i32&gt;) -&gt; i32 { (1..n+1).filter(|&amp;a| { for x in m.iter() { if a % x == 0 { return true } } return false; }).fold(0, |sum, x| sum + x) }
Thanks!
/r/playrust
Genius.
Is your project somewhere on github?
How did you not read an entire page of topic titles that were obviously not game related? 
Is there a way to cleanly format error dumps for release (i.e., just use the actual error message)? The end user won't get much use out of seeing a full panic message and a prompt for a backtrace.
Or sometimes to hide details you need `Pimpl` idiom in `c++`. For shared libraires this is obvious choice, but when you have to do it to only because of too long compilation time this is bad.
A more interesting solution can be done with imply types and using traits to describe `Nat` instead of an enum. Something like this: #![feature(associated_consts)] pub trait Nat { fn succ(&amp;self) -&gt; impl Nat; const as_u64: u64; } struct Whole&lt;T: Nat&gt; { prev: PhantomType&lt;T&gt;; } impl&lt;T: Nat&gt; Nat for Whole&lt;T&gt; { fn succ(&amp;self)-&gt;impl Nat {Whole&lt;Self&gt;{prev: PhantomData}} const as_u64 = T::as_u64 +1; } struct ZeroStruct; impl Nat for ZeroStruct { fn succ(&amp;self)-&gt;impl Nat {Whole&lt;Self&gt;{prev: PhantomData}} const as_u64 = 0u64; } // Here to expose Nat types hiding them. // If it where possible I'd rather use const // const ZERO: impl Nat = ZeroStruct; pub fn ZERO() -&gt; impl Nat {ZeroStruct} pub fn NatToU64&lt;N: Nat&gt;(n: N)-&gt; u64 {N::as_u64} pub fn U64ToNat(n: u64)-&gt;impl Nat { if n == 0 { ZERO() } else { U64ToNat(n-1).succ() } } fn AddU64&lt;N: Nat&gt;(n: N, u: u64) -&gt; impl Nat { if u == 0 { n } else { AddU64(n.succ(), u-1) } } pub fn Add&lt;N1: Nat, N2: Nat&gt;(n1: N1, n2: N2) -&gt; impl Nat { AddU64(n1, NatToU64(n2)) } This is still awkward to use. I don't think that's so bad, given that rust is a language where the focus is on place where a Nat probably isn't a great idea (Otoh this is useful for type based systems do there might be some benefit to make it easier to use). Still a fun experiment.
[sccache](https://github.com/mozilla/sccache) can help by having a global compilation cache. Using Amazon S3 for instance lets you speed up compilation on many machines
I'll be damned. Yup, those were the 'tweak' values I was referring to. If (when?) I get to implementing batching, I'll give merkle.rs a try and see how it fits. Thanks for the pointer and commentary. 
While this will be useful, I think consideration should be given to whether a crate should be split. I'm not familiar with the libc problems, just that there can only be one so all crates need to standardize. Could some of the c-types be split out?
Based on what?
If you submit from your home page, you type in the Reddit and never even have to look at it. 
you can substitute your own panic handler that prints whatever you want, look at std::panic
It makes your final program faster and/or smaller but compiling takes longer, sometimes a lot longer. So it's just a tradeoff, try it and see.
Here's what I don't get: Why can't I tell library users to use the same version of the dependency that my library is using? Like, I have a library, that depends on `image 0.14.0`. I want people to use a struct that is defined inside image crate. However, if I forget to update the dependencies (which usually happens), people will want to use `image 0.15.0` in their binary, which is a mismatch against my library, which will cause trouble. Why can't someone say: `use mylibrary::dependency::image` (which should use whatever version of `image` the `mylibrary` library is using)?
Lock-freedom without garbage collection: https://aturon.github.io/blog/2015/08/27/epoch/
I guess the downside is that if version 0.2 has an API with N distinct touch-points, the compatibility version 0.2.1 has an API with N-1 of them rewritten to import from 0.3, and that could be a *lot* of churn. Delete seventeen implementations and replace them with imports, but if you accidentally skip one, things break. And since the Rust pattern is to put unit-tests as a submodule inside the module being tested, "delete the implementation" means you're also deleting tests, so you can't run the new implementation against the old test suite to make sure you've got everything covered. On the upside, although it's a lot of work, it's a lot of work for one crate, instead of being a little bit of work for each of dozens or hundreds of downstream crates, with all the coördination that implies.
Thanks for writing this up. Very clever.
I had no idea that was a thing. Why is that a thing? It doesn't seem like a very good thing.
Thanks. I've added it.
oh, I don't know if this is the best answer, but it is an answer I understand! 
You hit the nail on the head. As much work as this approach can potentially be, it may still be far less work than touching hundreds of downstream crates. Even if this were more total work than the alternatives, there are two major advantages that may make it a good idea anyway: - It can be completed by one or two hardworking developers rather than coordinating version bumps across dozens of external developers and having the ecosystem in a broken confusing state for months. Servo does not own most of the libraries involved in [servo/servo#8608](https://github.com/servo/servo/issues/8608) which is a big reason it took so long. - It does not mandate a major version bump for downstream crates unless they use one of the changed types. Suppose the `url` crate had `c_void` in its public API. As `url` is so widely depended upon, they may not be willing to do a major version bump until years later when there is a more compelling motivation than a `libc` upgrade.
Casual observer here. The cited 'real bug' in MutexGuard made me wonder: is it always the case that Rust's guarantees are contingent on all parties using unsafe properly? Like, if I happen to use a crate where someone (with best intentions) has unsafe code that contradicts the declared signatures... are all bets off?
Neat idea. While that may simplify some cases, I think the trick described here is a more complete solution. What if I use two different libraries that both depend on `image`? Which one's `image` dependency is my crate supposed to inherit? If one upgrades before the other and the semver trick is not in play, I still end up in a confusing broken state. [RFC 1977](https://github.com/rust-lang/rfcs/pull/1977) is actively working toward a long-term solution along the lines of what you had in mind.
0.14.0 and 0.15.0 are incompatible versions. It would be *wrong* for you to have allowed 0.15.0 before you knew what was in the 0.15.0 release and whether it was coincidentally compatible. There are ways of specifying “depend on absolutely any version of this crate”—the easiest is `*`, but crates.io forbids it and I’m not going to tell you any other ways of achieving the same thing, because you should never, ever, under *any* circumstances use it. Literally. *Never.* It’d be reasonable, if you tested your library against `image 0.15.0`, to support both 0.14 and 0.15, via a version specifier of `0.14, 0.15`, but you should *never* add future semver-incompatible releases to the version specification. If you are exposing something from `image` in your crate, *​`image` is part of your public API*—or at least part of it. Anything public from `image` that appears in your crate’s public API should be reexported, *and becomes part of your public API as well*. `image 0.15.0` may or may not trigger a semver breaking change in your crate. Anyway, the upshot of it all is that I can’t reasonably depend on “your version of `image`”, because what that is might break my code. If you *just* reexport item `A`, then if `A` changes breakily, you’ve got semver breakage and you’ll need a new release, but if `B` from that inner dependency changes, what is that to me? It’s not a part of the API. If, however, you had reexported all of that crate (along the lines of `extern crate image; pub use image;`), that `B` change which caused `image` to bump its version will also require you to bump your version. So: the reason why someone can’t just say `use mylibrary::dependency::image` is that that would allow hidden breaking changes, and Rust doesn’t like that.
We do that with nix, where we re-export our libc dependency. The only downside to this is that it relies on more work of the user. Instead of them just using dependencies directly they have to use a dependency of a dependency. So it's a little more friction there.
Well, it's a little hacky, and requires nightly, but you can make a generic closure wrapper so you can turn them into a sync closure https://play.rust-lang.org/?gist=2450ea831e3491051c3d1ccad1b7f6f8&amp;version=nightly&amp;backtrace=0 definitely don't do this unless you know 1000% that the types you're carrying won't be accessed in an unsafe way
Pretty much, yes. All bets are off for anything that unsafe code may touch, basically. But it's usually reasonably scoped and relatively easy to track down.
If I recall correctly, in this case it's because Microsoft were one of the earlier adopters of what would later become Unicode. At the time they added support to Windows, there were only 2- or 4-byte encodings. When .Net came along, they obviously decided to stick with that encoding for its String type.
If you want the `T` of `Foo&lt;T&gt;` to be constrained to `Iterable`, you need to explicitly spell out the additional bounds for the `Iterable` trait again: struct Foo&lt;T: Iterable&gt;(T) where for&lt;'a&gt; &amp;'a T: IntoIterator; // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Here's a [playground link](https://is.gd/gaMUfq) which also demonstrates implementing the `Iterable` trait and using an instance of `Foo` in a `for` loop I _think_ the reasoning for this is explicitness: your `T` parameter doesn't have any "hidden restrictions" on your type parameter beyond what you specify in-place (although there has been some discussion in the [ergonomics initiative](https://github.com/rust-lang/rust-roadmap/issues/17) to add "implied bounds", which AFAICT would make your original example compile).
It does end up saying &gt; submitting to /r/rust &gt; &gt; The Rust programming language. For the Rust video game, see /r/playrust But uh, you see how well that works.
Sure, that would fix it for `Rc&lt;str&gt;` in particular, but not for unsized types in general right?
&gt;In Rust (as in C, for that matter), two structs are not interchangeable just because they look the same. On the contrary, two structs in C with the same name are the same due to the lack of namespaces. If their definitions don't match up despite having the same name, that is *undefined behavior*. Yes, even if the two definitions are in different compilation units, but still linked together into the same binary. Isn't C just wonderful?
&gt;Full build times are approximately the same Parity with C++ compile times isn't exactly the highest bar to clear. Over the weekend I did some benchmarking to see whether my computer was up to spec for contributing to Rust, comparing Rust to D and Go by building each one. I ran each build (without running the tests) at least twice -- once to make sure all the dependencies were pulled in and built, and then again for timing: * DMD: 45 seconds * Go: 1 minute 15 seconds * Rust (stage 1 only) 45 minutes (To be fair, I also tried building clang, but with only 8GB the OOM-killer kicked in during linking) i don't know if there is a way to do "cargo check" through `x.py`, but I do hope there are more compile speed improvements.
Thanks for the clarification. I intended to mean this situation: struct A { int x; }; struct B { int x; }; int main(void) { struct A a; a.x = 0; /* Not allowed. */ struct B b = a; }
&gt; Parity with C++ compile times isn't exactly the highest bar to clear Never said it was. The question was comparing to C++. I'm skeptical of that number for dmd's compile times, though.
Also to note is that I made sure that string slicing in the shell is also grapheme-based.
&gt;The question was comparing to C++. You're right; I guess this was a bit of a thread hijacking. &gt;I'm skeptical of that number for dmd's compile times, though. Since dmd was rewritten in D, compiling it is a snap, which I have just now repeated in 50 seconds: cd dmd; time make -f posix.mak real 0m15.055s ... cd ../druntime; time make -f posix.mak real 0m11.689s cd ../phobos; time make -f posix.mak real 0m23.438s
You are right that rust is lower level than javascript. But that doesn't mean that it's somehow more "real programming" ;) I don't think you should devalue what you have learned so far. There are a lot of new concepts, but a lot (especially the functional parts) will translate into rust. A great resource to learn rust is the [official book](https://doc.rust-lang.org/book/second-edition/). It'll also introduce all the new concepts you need to know (the stack, die heap, ownership semantics etc.) Also, when stuck, jump into the irc at #rust or #rust-beginners. The people there are very helpful and overall very nice.
I'd like to see Liner become a much more powerful and featureful prompt, but I have my hands full with Ion right now, and Liner is a separate project.
Right, I meant that I was skeptical that it's the same thing being measured. Poking at it it seems to be the same thing (assuming you didn't count llvm's compile times, since LLVM is a larger project than rustc itself). 
Is there a way to do this for tests as well? Like `cargo check-test` or something?
&gt; Though my approach at learning them (together mind you) was less than optimal from what others tell me. I basically wrote code and read stackoverflow, MDN and blog posts until I had wrapped my head around every concept. I think repeating that with rust would not be a good experience due to how much lower level the language is. I honestly believe this is the right approach in principle. You don't a learn a language by just reading a book, but by trying it out instead and getting your hands dirty -- the same way you don't become a carpenter by just reading the theory! &gt; Which brings us to question 1: Am I right in this assumption? How difficult would it be for a JS programmer to just jump into Rust, so to speak? Difficult, but rewarding! :) &gt; Since I never learned the basics of computer science and "real" programming, would Rust even be a good point for me to start? Who defines the basics and what is real programming? I wouldn't worry too much about it. Just do what's interesting for you and what you want to learn. &gt; Would a higher level language like Java or C# be better to I guess ease myself into the lower levels with? Would it be best if I just picked up some computer science books and got into C instead? If you want to have fun with pointers, then Java and C# are probably not the right languages. I would stick with Rust, mainly for the reason that it's a lot stricter and and will yell at you when you do something iffy. The problem with C is that it trusts you believes that you know what you are doing. Rust is quite the opposite and principally assumes you did something wrong and will tell you that :). 
&gt;assuming you didn't count llvm's compile times That's why I ran each build twice before timing; the first time for Rust, it was I don't how long to checkout the LLVM submodule, and about 1 hour 30 minutes to compile it.
Function pointers generic over their argument types sounds like a bad idea in general (even if they are "just" generic over lifetimes). Do they exist in any other low-level language? Forbidding function pointers to `default` trait methods that can be specialized does not look like a huge loss to me. The minimal proposal does have lifetime specialization, so unless that changes this issue needs solving. In my opinion, just because we have been able to get away with generic function pointers without specialization, doesn't mean that we should pursue them further. 
Was down with the flu last week, so I still need to fully set up my notebook. Apart from that, TWiR and perhaps some clippy work.
I am worried about the speed of the mpsc, as I am not that much aware of the performance? so how is the performance of mpsc when doing inter-thread communication? 
First (Sorry, but I feel this must be said), JavaScript programming is every bit as 'real' as programming in Rust, and let no one tell you otherwise. Second, there is no one true path to learning the language, every learner has their own style. That said, I hear great things about the upcoming The Rust Programming Language book, second edition. Also the community has a number of projects that provide mentoring to new contributors (for example [clippy](https://github.com/Manishearth/rust-clippy)) with easy issues. This is – in my opinion – the best way to learn. Edit: Oh, and if you have a question, feel free to ask (in our questions thread, on IRC or elsewhere)!
I'm calling it: [2017 is the year of the Linux desktop](https://insights.stackoverflow.com/survey/2017#technology-most-loved-dreaded-and-wanted-platforms)!
Continuing work on a Pong clone with Piston, and probably finishing a post about error handling.
What speaks against a `Vec&lt;u8&gt;`?
If you're looking for on-stack buffers, I believe that currently is not an option: Rust does not support dynamic stack allocation (whether [alloca](https://github.com/rust-lang/rfcs/issues/618) or [dynamically sized slices](https://github.com/rust-lang/rfcs/issues/1031)). The normal way would be to use a Vec and store the buffer on heap.
Using Nightly features: #![feature(box_syntax, box_patterns)] #[derive(Debug, PartialEq, Eq)] enum Nat { Zero, Succ(Box&lt;Nat&gt;), } use Nat::*; fn nat_to_int(n: Nat) -&gt; u32 { match n { Zero =&gt; 0, Succ(box n) =&gt; 1 + nat_to_int(n), } } fn int_to_nat(n: u32) -&gt; Nat { match n { 0 =&gt; Zero, n =&gt; Succ(box int_to_nat(n - 1)), } } fn add(m: Nat, n: Nat) -&gt; Nat { match (m, n) { (Zero, n) =&gt; n, (Succ(box m), n) =&gt; Succ(box add(m, n)), } } fn main() { assert_eq!(nat_to_int(Succ(box Succ(box Succ(box Zero)))), 3); assert_eq!(int_to_nat(5), Succ(box Succ(box Succ(box Succ(box Succ(box Zero)))))); assert_eq!(add(int_to_nat(5), int_to_nat(11)), int_to_nat(16)); } 
Thanks! I'm sure clippy will come in handy
&gt; I honestly believe this is the right approach in principle. You don't a learn a language by just reading a book, but by trying it out instead and getting your hands dirty -- the same way you don't become a carpenter by just reading the theory! I never read a book on javascript! When I was starting out I would just headbutt the code till it worked. &gt; I would stick with Rust, mainly for the reason that it's a lot stricter and and will yell at you when you do something iffy. As a typescript user used to having typescript yell at me, that sounds great. I couldn't even begin to count all the times it saved me from doing something absurdly stupid.
Hopefully finish enough core functionality to release a Tokio based MQTT client. Working with mutable state and Futures is proving to be very annoying, so I've got pretty bloated sub-event loop. Repo is available at [tokio-mqtt](http://github.com/proman21/tokio-mqtt)
Thanks for the book, I'll start there :)
Every year is the year of the Linux desktop for developers though - developing on Windows is still as nasty as ever, and now with Docker I can literally just load up my development environments on demand.
Currently working on generating a symbol table for a WebIDL AST, perform some semantic analysis, and hopefully get some work started on generating Rust interface bindings. [WebIDL](https://github.com/sgodwincs/webidl-rs)
You may enjoy 4chan's take on this: [rust graduates from meme language state](http://boards.4chan.org/g/thread/61285612).
For posterity's sake, here is one take on this that passes all the tests. fn sum_of_multiples(a: i32, bs: &amp;[i32]) -&gt; i32 { (1..a).filter(|x| bs.iter().any(|y| x % y == 0)).sum() }
yes, I can use vec. but the typical operation is socket.read(&amp;mut buff). now, how would I perform that operation with vec? I mean I tried using vec. but it wasn't filling up.
i tried this, but it wasn't filling vector with u8 bytes. i used socket.read(&amp;mut vec).
This thread looks like it may also have some relevant answers: https://www.reddit.com/r/rust/comments/62vfbg/dynamically_allocated_array/
How do you create the Vec? If you use `Vec::new()`, the Vec will be empty. I think you should use `vec![0; size]`, or with_capacity?
yes, I used `Vec:: new()`, I thought it will automatically allocate the memory. if I will use with_capacity, then what will happen when data will be more than capacity? will it auto allocate new space?
I think you have to initalize a Vec with a certain size, the read method will not extend an existing empty Vec. See the first example here: https://doc.rust-lang.org/std/io/trait.Read.html
Yes. `Vec` will reallocate to accomodate the data and won't shrink its allocation unless explicitly asked. "capacity" is the term for how much space has been allocated. See this and the following section for details: https://doc.rust-lang.org/std/vec/struct.Vec.html#capacity-and-reallocation
Thanks for providing binaries, that made it possible for me to try it :) Feels very snappy, good work! Two questions, though: * Ion told me, but I didn't remember the location of the config file. It's not mentioned in the docs I found, so... where is it? I assume it's a simple script that gets run, right? * There's some neat completion magic at work, but... how do I use it? Just pressing enter just uses what I type (rightly so!), but if what if I want to use the completed command?
Yep! It's https://github.com/mjkillough/rqlite It's still very WIP and is useless right now.
I love rust enums but except that you have to write out every match for the enum. I was wondering if you think it would be possible to write procedural macros to achieve something similar to the following: https://play.rust-lang.org/?gist=e097a19271be4ecae939b63f863647f5&amp;version=stable&amp;backtrace=0 
Awesome progress! Although the fact that `fg` and `bg` don't default to the last suspended job irks me a little.
The config file is `~/.ionrc`. I have the same questions about the hints on the command line, I don't know how to actually expand them.
Hmm, I don't have that file, it's not created when starting the shell? Good! Although, I'd prefer if ion defaulted to the XDG spec.
wow! I wonder how DMD achieves that speed and if those techniques can be applied to rust
I kinda agree - maybe not 'wrong' in the strictest sense, but rather misleading. At least, there's a few things that set my statistics-senses tingling. For example, SmallTalk being #2 rated, when in the wild you see almost no SmallTalk projects. So that suggests that there are a handful of people who really like it, but that' not by itself indicative of future success or popularity. What's missing in these graphs is a confidence interval based on the relative number of responses in order to correctly interpret how representative the responses are. It's not sensible to compare the average of 10 responses to the average of 1000 if you are trying to understand a wider trend!
Nope, not created by default. It used to tell you that it can't find it on startup, but it seems that's gone now.
Is this what some languages call structural vs nominative typing?
[Ages ago](https://www.reddit.com/r/rust/comments/6352r2/z/dfrlu5e) I said I was working on something that might be my final year project/diss. Unfortunately I didn't get the project I wanted, instead I got a machine learning one - boosting - so I'll be working on that. I mean, I don't actually start it until October but it's a long summer when you have nothing else to do! 
I'm a JS developer who a) learnt JS in a similar way to you. b) jumped into Rust a few months ago, skipping Java/C#. I think it's an excellent choice (I found the transition pretty painless). I actually think Rust might be easiest for JS devs, as JS also has quirks in things it's OOP model, whereas developers from some other languages just assume Classical OOP, and struggle with anything different. 1. Your assumption is half right. It more seeks to replace C++ (which of source, sought to replace C, but only half succeeded). The distinction being that C ONLY contains low-level constructs (things like pointers, and manual memory allocation). Whereas both C++ and Rust also contain high-level constructs (e.g. classes in C++'s case. Things like traits and iterators in Rust's case). 2. Yes, start with Rust. A lot of Rust's introductory material, notably the second edition of The Book (which I would highly recommend starting with) is aimed at people coming higher-level languages, and contains explanations of things like The Stack vs The Heap which are pretty simple, but won't be familiar to you coming from JS (and if the explanation in the book isn't enough, then you now have a solid idea of what to google :)). I wouldn't move Java/C# as a JS developer. As I suspect you'll find their type systems quite restrictive. Rust also has static typing, but it's type system is more powerful, which means it is more flexible (e.g. Traits in Rust allow you to do something very similar to duck-typing is JS). Come back to Java/C# after Rust, and you'll find them easy :) Expect it to take a week or two to get productive.
AFAIK testing requires codegen, so it shouldn't be possible.
Sounds like this should at least be a lint, yeah.
As a big fan of JS/Node myself, I think you're making a great choice diving into Rust. It's a well designed, modern language, and a lot of the concepts you'll learn with Rust will translate well to other languages outside of JS. I think some parts will feel surprisingly familiar, like the functional bits (e.g. array/iterator filtering/mapping). Rust can have a bit of a steep learning curve, but don't give up! It's totally worth it. And definitely feel free to ask for help here in /r/rust or over on the #rust IRC channel. The community is super friendly and loves helping newbies!
I mainly work in Java during the day, and one of my coworkers talks a ton about his admiration for smalltalk, and how great all the tools were. Maybe most loved, should also have a most missed, and smalltalk would be the top of that...
Trying to migrate rust-bindgen from syntex to syn/quote and probably some diesel work.
Mostly because the stock DMD backend doesn't do the kind of optimizations that modern GCC/LLVM do. A fairer comparison would be to LDC, the D compiler that uses LLVM as a backend, which compiles in about 12 minutes.
&gt; now, how would I perform that operation with vec? socket.read(&amp;mut vec) &gt; I mean I tried using vec. but it wasn't filling up. Right, the issue here is that a Vec is empty by default (`Vec::new()` creates a vector of size and capacity 0) so if you're using as a mutable slice (e.g. giving it to [Read.read](https://doc.rust-lang.org/std/io/trait.Read.html)), well it's a mutable slice of size 0 meaning read() fills it with 0 bytes and returns happy to have done its job. The `Read.read()` API does not know anything about the Vec part so it's not going to invoke it. In that case you need to pre-fill the vector (with zeroes) so that it has the size you want, and you can resize it afterwards in case you need to. Or you invoke `read_to_end` if that's acceptable for your use case.
FWIW `with_capacity` sets the allocated capacity but not the *size*, so the slice you borrow from the vector is still empty. Yes that has tripped me up multiple times.
Going to test a bit more the next version of [gutenberg](https://github.com/Keats/gutenberg) (a static site engine) now that I've added built-in Sass support. It's still not working on Windows though if a Windows user wants to have a look: https://github.com/compass-rs/sass-rs/issues/12 
function pointers can be generic over lifetimes because lifetimes are not a runtime construct; they have been stable since 1.0. function pointers cannot be generic over types, and there's no way to make that possible. A rule like "no function pointers to default trait methods" is insufficient, because you can always write an `indirect` function that calls the trait method.
And that's an optimized build.
and the year of arm in servers while you're at it
[removed]
I think I have a big hole in my understanding of function pointers. I always considered that any function taking a reference in Rust is a generic function, and I never expected to be able to take a function pointer to a generic function. That is, given: fn foo&lt;'a&gt;(x: &amp;'a i32); fn bar&lt;T&gt;(x: T); // e.g. where T = &amp;'a i32; I wouldn't expect to be able to take a function pointer to any generic function, like `bar`, why is it a good idea to be able to take a function pointer to the generic function `foo` ? &gt; function pointers can be generic over lifetimes because lifetimes are not a runtime construct; Neither are types. &gt; function pointers cannot be generic over types, and there's no way to make that possible. So how come that we can make them generic over lifetimes? Aren't lifetimes part of the type of references? We can't assume, in general, that we can make generic functions concrete, so we cannot take function pointers to them. Yet we assumed that we were always going to be able to coalesce all lifetime parameters into a single one before trans, and hence we allowed users to take function pointers to functions that are only generic over lifetimes. To me, it looks like specialization just points out that this assumption was incorrect. Instead of crippling specialization, we should find a way to either fix the incorrect assumption or make it only hold when specialization is not in play. BTW I would really like to know why would anybody think that taking function pointers to generic functions is a good idea, even if it can technically be done in some cases (e.g. when it involves lifetimes). To me, it sounds like a decision that was predestined to cause problems long term. Maybe my whole misunderstanding is because lifetimes are a different type of generics somehow ?
You can use the [`buffer`](https://crates.io/crates/buffer) crate. It works with `Vec`, `ArrayVec`, etc., e.g.: extern crate buffer; use buffer::ReadBuffer; use std::net::TcpStream; fn main() { let mut stream = TcpStream::connect("localhost:80").unwrap(); let size = 5; let mut buffer = Vec::with_capacity(size); stream.read_buffer(&amp;mut buffer).unwrap(); } Cargo.toml: [dependencies] buffer = "0.1.7"
FWIW, Ceylon calls these [union types](https://ceylon-lang.org/documentation/tour/types/#union_types), and does matching using its normal instanceof operator and flow-sensitive typing: void printType(String|Integer|Float val) { switch (val) case (is String) { print("String: ``val``"); } case (is Integer) { print("Integer: ``val``"); } case (is Float) { print("Float: ``val``"); } } I don't think that helps Rust much.
I didn't get it after reading it first, but the trick is that 0.2.1 (where the types are aliases for the 0.3.0 version) is semver compatible with 0.2.0. So if a crate depends on 0.2.0 or `^0.2.0` or `&gt;= 0.2.0 &lt; 0.3.0`, the dependency can be satisfied by version 0.2.1. A crate with `= 0.2.0` wouldn't work, but that's not often used. Nice!
I started [a crate to parse asciidoctor](https://github.com/antoyo/asciidoctor-rs) and generate HTML from it. I also experimented with [low-level async IO](https://github.com/antoyo/async-io-rs). You can see examples of the syscalls select, poll, epoll and the crate mio in this repository. Also, I worked on [titanium](https://github.com/antoyo/titanium), a keyboard-driven web browser. I still have issue with the new single UI process that I'm trying to fix.
Change the inner `for` to `.any()` and the `fold` to `sum`, and it's perfectly fine.
The right arrow triggers the current completion for me. This is the same as the fish shell, in case you find it surprising.
I'm a real fan of this project. It's not suitable for interactive use yet (at least compared to my darling, Fish), but I'm excited to see it improve.
You can use `socket.take(some_number).read_to_end(&amp;mut my_vec)`, I believe. I don't know what the overhead is though.
Isn't that then a breaking change from version 0.2 to 0.2.1? That's not technically a problem because 0.* versions are allowed to break anything at any time, but say libc was at version 3.0 and wanted to introduce breaking changes, so they have to upgrade to 4.0. Then, according to this "semver trick", they would create a version 3.1 and replace the implementation of `c_void` with a re-export of 4.0's `c_void`, right? But isn't that a breaking change in a minor version bump (3.0 to 3.1), which is not allowed by semver?
&gt; The right arrow triggers the current completion for me. This is the same as the fish shell, in case you find it surprising. You're right! I'm not sure it's surprising, but I'd sure hope I can configure that. &lt;Ctrl-E&gt; seems made for it imho.
`cargo check --tests`?
&gt;As I suspect you'll find their type systems quite restrictive. Rust also has static typing, but it's type system is more powerful, which means it is more flexible Indeed, when I was taking a look at Rust last night, this was one of the things that stood out, that its typing system seemed to be closer to what I'm used (Typescript) in terms of flexibility. Anyway thanks for this response. Reading that the experience of someone with a similar programming background to mine was this good is quite reassuring. If my decision to take the plunge hadn't solidified yet, this did it.
My bet is on the latter, a dev is the US has way higher "gross" salary. But I hate this comparison because the same gross salary in US and in France doesnt buy the same living conditions.
&gt; Make a proof of concept clone of GTK-rs for garbage-collected coroutines I'd love to, but I don't think I can handle the responsibility.
https://www.ralfj.de/blog/2017/07/08/rustbelt.html mentions reviewing `take_mut::take`, so I guess that one at least is fine
I recall that Apple hired former core Rust developers.
You can pry ctrl-e as eol from my cold, dead, emacs-cramped fingers. Actually, thinking about it, I guess it would probably be pretty nice.
I think there are valid use cases though when it truly has no meaning like `Iterator::unzip` where there truly is no real meaning beyond first and second as it's an abstract library function.
A [cheat list](https://is.gd/fb1R2B): // stack-allocated byte array filled with zeroes that can not grow let buffer1: [u8; 1024] = [0; 1024]; // heap-allocated byte array filled with zeroes that can not grow // but can be arbitrary size (on heap, though) let size = 1024; let buffer2: Box&lt;[u8]&gt; = vec![0; size].into_boxed_slice(); // bonus: stack-allocated byte array filled with uninitialized garbage data let buffer2: [u8; 1024] = unsafe { ::std::mem::uninitialized() }; EDIT: For many use cases, keeping a `Vec` around and re-using it would be the best option performance and usability wise. For performance, vector will eventually become as large as the maximum amount of data necessary to handle. For usability, vector has `.truncate` which can be used to make Vec appear empty while the backing storage remains the same. 
Why right arrow instead of tab o_0
Ah I'd forgotten to check back on this, so I wasn't aware it was fully-funded! So excited that I'll get my Ferris plushie :)
I am currently writing an app to learn react native and decided to also learn Rust for creating the server. So far all I have is a simple Rocket server sending some static data to the app. I'll work on adding a DB (probably postgres, not sure whether I'll use diesel or just plain SQL) and user auth next. Afterwards I'll probably have to look into making the server work with websockets. 
Good point.
I started a [github org](https://github.com/rustcod) to house some game libs I've been working on. The org aims to re-implement, at least in spirit, [`libtcod`](http://roguecentral.org/doryen/libtcod/), a well-known roguelike toolkit. Currently the org houses an [OpenGL tile renderer](https://github.com/rustcod/gltile), a [tool for defining tilesets](https://github.com/rustcod/pixset/), and a very basic [game loop](https://github.com/rustcod/looper). Everything is still very much alpha and changing, so there isn't any docs outside of some basic examples. The first order of business is porting over [this well-known tutorial](http://www.roguebasin.com/index.php?title=Complete_Roguelike_Tutorial,_using_python%2Blibtcod) over to use the above libraries, all while submitting issues and shoring up the gaps in the libraries. There already is a [great rust version](https://github.com/tomassedovic/roguelike-tutorial) of that same tutorial, so I'll be sort of cross referencing both. Anyway, still much to do! There is a [twitter account for the project](https://twitter.com/rustcod_rs) so feel free to follow if you're interested in this sort of stuff.
Is "Buffer stop" really "Poller" in Swiss german? I know "Buffer stop" as "Prellbock" in german german. And a "Poller" is more like "something sticking out the ground/soil, mostly out of concrete or steel – something to prevent vehicles to pass or for mooring ships/boats" i think the right english term is "Bollard". Anyway – thanks for the post i try to concentrate on the real topic now :)
https://crates.io/search?q=graphql
&lt;Ctrl-F&gt; works
the config file used to be `~/.ionrc`, it's now `~/.config/ion/initrc`
Contributing some love to [hyper-rustls](https://crates.io/crates/hyper-rustls). In particular, now it [accepts custom TLS client configurations](https://github.com/ctz/hyper-rustls/pull/28) to tweak root store, client certificates, ciphersuites, etc.
I don't think so. Do you have an example of code that would be broken by it?
Haven't used it yet but I think Juniper was the furthest along last time I looked into GraphQL library in Rust, https://github.com/mhallin/juniper Although I'll probably using Go for my a GraphQL production app I'm starting soon. More stable at the moment and Go is easier for my team as well. 
This is brilliant. I wish I could double-upvote this.
Btw. I didn't know you could depend on another version of the crate itself, and I think my assumption was reasonable provided that you can't depend on two versions of the same crate. Was that deliberately allowed or just a serendipitous oversight?
I have been working on my very first library called [chroniker](https://github.com/DevOrc/chroniker). It makes converting between times much easier as well as other time utilities.
I believe so, yes.
deleted ^^^^^^^^^^^^^^^^0.1854 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/28668)
&gt; For example, SmallTalk being #2 rated, when in the wild you see almost no SmallTalk projects... So that suggests that there are a handful of people who really like it, From experience, that matches for me. I know a lot of people that are huge SmallTalk fans, but don't use it at work. Also, many universities teach it, so exposure is rather high. &gt; but that' not by itself indicative of future success or popularity. But that was not the question. Maybe you are more interested these two questions and the data doesn't fit. But it's kind of clear that if you ask "do you love, dread or want to work in a language?", you won't find these two answers in there. It doesn't make the data wrong. Also, if you have a look at the "wanted" tab, you see a sharp drop for Smalltalk: everyone loves it, but no one wants it, which kind of matches a language where people learned pure OO from. You also see a less sharp drop for Rust, which matches a language that people find useful, but not quite ready yet. 
I agree strongly. While I agree opening up a github issue is amazingly rude (not just with Rust, telling someone they picked the wrong language in general is obnoxious), I've seen as more of the "C apologism task force" as I have the "Rust evangelism task force" - not a representative sample, but still a problem. Also, I *really* wish more programmers took security seriously. As it stands a lot of them don't take privacy to be a threat to people's wellbeing (which it is, for various subsets of people in the world). 
&gt; Maybe most loved, should also have a most missed, and smalltalk would be the top of that... There's a "wanted" tab and Smalltalk is not even on it.
Did you check out the nix crate? It provides safe bindings to all of that functionality and wraps it in a nice API. Would love to see if it works for you.
Hello fellow Rustacean! A gentle reminder that our official name is Rust Evagelism Strike Force™, not task force. Please see r/rustjerk if you would like to know more.
didn't know there is a bunch of company using rust. that is exciting!! I hope they stick to it though. I remember when scala is really hot alot of company adopt it but some big one end up moving away from scala.
In the Makefile, you provide the `-O` flag to `rustc`; according to the `rustc` manual this is equivalent to `-C opt-level 2`. However, the package manager `cargo`, when building using the "release" [profile](http://doc.crates.io/manifest.html#the-profile-sections), sets `opt-level` to 3, the maximum value, instead of 2. This may or may not account for some of the slowdowns seen when transitioning to iterators.
That can easily be implemented. You just have to submit a corresponding issue to mark down features that you want / are missing.
The tab key is utilized to display a list of possible completions.
I noticed you're using `int` in C and `isize` in Rust. These are not the same thing. You should either use sized integers in C, or use `libc::c_int` in Rust. **Edit**: Also, you can introduce new iterator adaptors using method calls by introducing a new extension trait.
Also, former Apple developers were hired back into Rust.
We are following XDG specifications. ~/.config/ion/initrc
You mean for the async IO? It was just experiments to learn about them. I thought some people might want to see commented examples of using them.
Great!
That's a good default, too. I might just switch :)
I'm continuing to keep up with [RoguelikeDev Does The Complete Python Tutorial](https://www.reddit.com/r/roguelikedev/comments/6l596k/roguelikedev_does_the_complete_python_tutorial/). Though, I'm going through the Rust version instead of the Python version of the tutorial. My hope is to use the completed tutorial code as a springboard to create my own small Roguelike. Also writing a Sokoban game for the terminal using [rustbox](https://github.com/gchp/rustbox). Luckily I already have a [Minesweeper clone](https://github.com/mystal/rust-mines/) for the terminal, so I can focus on the Sokoban logic. 
Ooh, I've been thinking of doing the same thing for a few weeks now. I'm definitely gonna follow the development. If I find some free time, I'll see about contributing as well. You say you want to re-implement libtcod in spirit, so I'm wondering how much you'll try to emulate its API? tcod-rs has so far been quite nice to work with!
One feature it has out of the ordinary is that Either&lt;A,B&gt; is an Iterator/Read/Write if A and B are, correspondingly. That's just a covenience over writing those impls yet again for a new enum.
Read takes a &amp;mut[u8] so yes it cannot extend the Vec, only overwrite the current range.
As retep said, building a Rust object on the stack will go poorly. Either use an ancient C idiom that is Not Great: /* C code */ TwinInt ti; create_twin_int(&amp;ti); /* ti is now initialized */ Or have Rust allocate on its own heap and hand over a pointer, or since the struct is `#[repr(C)]`, pass it by value: /* C */ TwinInt ti = create_twin_int(1, 2); // Rust #[no_mangle] pub fn create_twin_int(a: c_int, b: c_int) -&gt; TwinInt { TwinInt { x: a, y: b, } } Also, and this is probably just a reddit thing and not in your actual code, but fyi you can't have a semicolon after the intended implicit return in `fn create_twin_int`.
`cargo check --test TESTNAME` will check a single test at a time
I think you have misunderstood what a higher rank parameter is. This claim in particular is not a correct description of what we have assumed, nor what the problem higher rank parameters present to the "just allow lifetime specialization" solution: &gt; Yet we assumed that we were always going to be able to coalesce all lifetime parameters into a single one before trans, and hence we allowed users to take function pointers to functions that are only generic over lifetimes. I don't have time right now to write out further explanation, sorry. I know this is not a satisfying reply. EDIT: notriddle's reply to you contains the most pertinent info
Thank you for pointing that out, I had noticed the `opt-level=2` but hadn't realized that the cargo default was 3. However, I saw no significant difference in the results: https://gist.github.com/snoyberg/300fde162193503cbcc99ffc024fe3c4
&gt; Those languages are well suited for low level programming, but require extreme care and expertise to avoid most of those issues. And even then, we assume the developers will always be well rested, focused and careful. As developers, one of our goals is to make our own lives easier. If we find ourselves running the same sequence of commands over and over, obviously we're going to automate that. If we find ourselves writing the same lines of code, we create a function. Yet, when it comes to encountering the same vulnerabilities over and over, it seems that many developers are unwilling to solve that issue. It's all about elbow-grease, IQ, and "following the rules". That's what blows my mind. It reminds me of the [blub paradox](http://paulgraham.com/avg.html): &gt; when our hypothetical Blub programmer looks in the other direction, up the power continuum, he doesn't realize he's looking up. What he sees are merely weird languages. He probably considers them about equivalent in power to Blub, but with all this other hairy stuff thrown in as well. Blub is good enough for him, because he thinks in Blub. I've collaborated with C and C++ programmers being introduced to Rust, and that accurately describes the friction that they had with the language. Rust has all this "hairy stuff" like lifetimes and ownership thrown in. Going back to the first quote, the compiler is *always* well-rested, focused, and careful. It reviews the code holistically for potentially nasty situations that the programmer didn't account for, while they were focused piecemeal on the individual chunks. If a soundness issue is found in the Rust compiler, it can be fixed *once*, and it is then fixed for *everyone.* Developers of performance critical code have avoided migrating to memory safe, higher-level languages because they can't afford to give up an ounce of performance. C and C++ have been effectively the only game in town for decades, so it's just ingrained in the developers that C and C++ are the right choice. Rust coming along makes *everyone* uncomfortable, because it is performance-competitive with C and C++, yet it offers memory safety, algebraic sum types, first-class expressions, and a host of other things. Was everything those C++ developers worked for done in vain? Are their skills obsolete? Will Rust truly stand the test of time? Rust isn't perfect, but it is a large step forward, as you would expect from a language developed decades later with tons of research done in the mean time, and it's not alone. Other languages are coming that provide memory safety without all the downsides. It would be nice if our rotten foundations could be renewed.
Very good catch about the int sizes, thank you! This does in fact have a significant impact on performance, and degrades the C loop to about the same as the Rust implementation. Or said more positively: Rust _is_ as fast as C :) https://gist.github.com/snoyberg/d82e70a477adf59313cd71bee845de51 I'll add an edit to the blog post. About the extension traits: could you describe that in a bit more detail? I'm not that familiar with the trait system. Would that allow any arbitrary existing datatype implementing `Iterator` to use the new methods, or would they need to explicitly be marked as implementing the trait?
Tip: Animated GIFs in text are distracting.
I usually use it in signatures when I want to consume a value and still be able to mutate that value within the function. Function signatures are patterns just like `let` bindings, so all the `let` rules apply. It's not a common pattern, I'll grant.
But it does not take the selected possibility from that list and commit to it? Why not?
I love these kinds of posts! First thing I noticed out of the gate: `isize` != `int`. `int` is gonna be a `u32` here, `isize` is gonna be a `u64`. I'd imagine it's this rather than &gt; It turns out that GCC it able to optimize this into a downward-counting loop, which drastically improves the performance. My gut says this has to do with UB in C, but C's UB is not Rust's UB, and that these sizes are actually the difference. Would need more investigation of course, but if similar Rust and C don't produce the same code here, that's a bug. &gt; This means that no namespacing is necessary, but on the other hand adding a new iterator adapter would mean the new function would not follow the same function call syntax as the others. (To me, this seems like a watered down version of the expression problem.) You can, actually, trait Foo { fn foo(&amp;self); } impl&lt;A, B&gt; Foo for A where A: std::iter::Iterator&lt;Item=B&gt; { fn foo(&amp;self) { println!("Foo!"); } } fn main() { let v = vec![1, 2, 3]; v.iter().foo(); } You'd need to do some more shenanigans if you wanted it to be fully chain-able than the terminal call, but I'm not gonna bother here. &gt; the Rust iterator implementation is noticeably slower than the low level loop. I'm finding it a bit harder to follow exactly what bits at this point, but sounds like you may have found a bug! &gt; Also, this behavior of Rust is in direct contradiction to the existential we used above to explicitly hide internal state from our type signature, whereas in Rust we're flaunting it. Rust is getting a form of existentials very soon in the `impl Trait` syntax. &gt; And that hunch is that the double-inner-loop problem is kicking in. Ehhh I'd be skeptical; due to all of the stuff mentioned above with regards to stack allocation, knowing type sizes, etc, usually stuff gets inlined, optimized, and ends up pretty fast. But more than that, there is only one loop here, and that's in `sum`, which is a `fold`, which is let mut accum = init; for x in self { accum = f(accum, x); } accum which is, as mentioned in the article, sugar for a loop. This is why iterators should be just as fast as loops in Rust; they literally compile to them. &gt; This is non-idiomatic Rust, and therefore (AFAICT) the compiler is not performing any such optimizations. Yup, this is going to inhibit _all kinds_ of optimizations.
Here's an example: use std::ops::Neg; pub trait NegateExt: Sized + Iterator { fn negate(self) -&gt; Negate&lt;Self&gt; { Negate(self) } } impl&lt;It&gt; NegateExt for It where It: Iterator {} pub struct Negate&lt;It&gt;(It); impl&lt;It&gt; Iterator for Negate&lt;It&gt; where It: Iterator, It::Item: Neg, { type Item = &lt;It::Item as Neg&gt;::Output; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.0.next().map(Neg::neg) } } fn main() { for v in [1, 2, 3].iter().cloned().negate() { println!("{} ", v); } } The extension trait just provides a way to latch a method on to all existing `Iterator`-implementing types, which then delegates to the `Negate` type.
It is normal human behavior to double down on existing decisions when feeling challenged. The core tenets of Rust are themselves viewed as a direct opposition to the status quo, so even mentioning it can put people on the defensive. Many people have invested a lot of time and effort in learning C/C++ and it won't convince anyone to show up with the attitude that they are doing it wrong and/or wasted their time. Attacking C isn't as productive as selling Rust. Rust has more to offer than not being C. Don't get pulled into the mud by the "C apologism task force". Pattern matching and Cargo were enough to sell me Rust. Partners, not enemies. There are a couple ways to increase the reach of Rust * Users of GCd languages are already willing to take a perf hit for safety and programmer efficiency, they don't need convincing, but when they need to use native code, make Rust the easiest, safest thing to reach for. Reducing or eliminating the impedance between the JVM, .Net, Python, Ruby, Go, etc. Plugins for popular build systems would go a long way, the support infra is often harder than writing the code. * View Rust as an assistant to making a codebase cleaner, faster and easier to manage. Need to add a multithreaded work queue manager to a C app? Use Rust! Want to use a rock solid parser for network and file access? Use Rust! On the library front, it would be a great idea if we committed to having libs that compiled on stable and nightly as well as the point releases in common package managers. Ubuntu [16/Xenial](https://packages.ubuntu.com/xenial/rustc) ships with Rust 1.15.1, while Debian [9/Stretch](https://packages.debian.org/stretch/rustc) ships with Rust 1.14. Targeting these versions will allow your code to have the widest reach. We need Rust to instrumental in building the internals of these OSes.
It allows arbitrary extensions, although the new trait need be imported. The idea is that as the writer of `ExtendedIterator` you can write: impl&lt;T&gt; ExtendedIterator for T where T: Iterator { } And provide the `ExtendedIterator` method expressed in terms of existing `Iterator` methods. With specialization (coming), individual structures can rewrite the `ExtendedIterator` method with a faster implementation for their special cases.
I feel like it's a case of a picture inserted because some style guide somewhere says to break up blocks of text, not because it necessarily adds to the content. The context suggests it has something to do with this line… &gt; You can’t just show up into someone’s project telling them to rewrite everything. …but I've never seen that image or the (I assume) video it comes from, so I have no idea what is being depicted and how it relates to the text. A picture is worth a thousand words, but that doesn't help if you only speak English and the words are in Russian.
What are the advantages of rewriting in (or even just using for a new project) Rust given I am: * Don't need deterministic memory management * Pretty happy with my memory-safe, garbage collected language ?
Thank you for the feedback. You're right about both `int` and extension traits, I've pushed a fix for both to the post already. However, regarding `filter` and an inner loop, I'm referring to [this bit of code](https://doc.rust-lang.org/src/core/iter/mod.rs.html#1101): #[inline] fn next(&amp;mut self) -&gt; Option&lt;I::Item&gt; { for x in &amp;mut self.iter { if (self.predicate)(&amp;x) { return Some(x); } } None } Since there's a `for` inside `Filter`, and another one inside `sum`, it looks like the same situation as arises in Haskell/GHC. Again, I have no data to back up my claim that this is the source of the slowdown, other than a hunch and prior history with GHC.
*Evangelism FTFY. :)
I just submitted a [PR](https://github.com/mitsuhiko/redis-rs/pull/129) to the redis-rs library. I am planning on digging into the unit test failures. I hope this can get integrated and if not, it was still fun to use nom for something.
&gt; [Crystal](http://crystal-lang.org/) is super exciting too, because it's like Rust in many ways, but it feels even higher-level. As soon as I read this, my rational self got curious. &gt; Crystal’s syntax is heavily inspired by Ruby’s As soon as I read that, my inner petulant child started calling for Crystal's failure. (I haven't been a fan of paren-less function-calling syntax or using words like `end` to denote blocks since the DOS days when QBasic was the most advanced language I knew.) **EDIT:** In hindsight, this post was inappropriate. I leave it here both to provide context to replies and to provide an honest record of the nature and degree of my past mistakes.
The article was primarily directed at aging c code bases.
[Better performance](http://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html), for one, it's also statistically unlikely that your current programming language has all of: - a nice package manager like Cargo - sum types (called enums in Rust) which are addicting once you've used them. Imagine only being able to do math using multiplication, and then discovering the flexibility of addition and multiplication. Traditional structs are called "product types" in CS theory. "sum types" are a very useful complement to product types. - [expression-oriented syntax](https://rustbyexample.com/expression.html), so every block has a result, even [if statements](https://rustbyexample.com/flow_control/if_else.html) - [match](https://rustbyexample.com/flow_control/match.html) is also really powerful - great parallelism libraries like [Rayon](https://github.com/nikomatsakis/rayon) to take advantage of the multi-core goodness of modern processors - Hindley-Milner type inference, which means that types can flow forwards and backwards, so you get super nice type inference in ways that the simple type inference engine in a language like Go cannot provide. You virtually never need to write a type annotation inside a function in Rust. All of the above, plus a few other things make Rust an excellent language for "[making impossible states impossible](https://www.youtube.com/watch?v=IcgmSRJHu_8)", which means that you can use the abstractions Rust provides to make it impossible to run code that does the wrong thing. (i.e. a compile time error, versus a run time error) Rust isn't perfect, but it's a great language even if you don't care about managing memory at a lower level.
I actually removed Crystal from my comment a few minutes ago, 10 minutes before your reply was submitted, since it is garbage collected, which I feel doesn't factor into the conversation well. I still think Crystal is exciting, but I don't think it's relevant here. I love paren-less function calls, as seen in Haskell, but I do believe some syntax should be required for function calls that do not have any explicit arguments.
Made great progress on my project, including a new site with a working demo: https://tmzt.github.io/isymtope/. It's a reactive template language with server and client-side rendering, including Redux stores and actions. Next I'll be adding content expressions and further demos, as well as supporting initial state in server-side rendering.
Rust doesn't have *Hindley-Milner* type inference. Also if you don't need manual MM you will just fight borrow checker for no profit.
It is [based on Hindley-Milner](https://github.com/rust-lang/rust/blob/master/src/librustc/infer/README.md). Nitpicking here isn't worthwhile. You can use `Rc` and owned values to avoid references, so you will effectively never run into the borrow checker. If you want to fight the borrow checker, it is certainly ready and waiting. It really just feels like you're here to pick a fight? I tried to give an earnest and very thorough answer to your question.
opt-level 2 and 3 produce the same assembler for the code in your gist.
Most languages with big runtimes and garbage collectors will at some point need to call into native code, to use C libraries, or for performance, or to communicate with hardware. They usually end up writing C for this, but Rust is a good alternative. As an example, take a look at [Helix](http://blog.skylight.io/introducing-helix/) which provides a nice integration between Ruby and Rust.
&gt; I actually removed Crystal from my comment a few minutes ago, 10 minutes before your reply was submitted \*chuckle\* Well, that's a bit of bad luck. If you removed it 10 minutes ago, then there was only a ~5-minute window in which I could have seen it, because I'd just come back after fiddling with some other stuff and the "suspend tabs after 15 minutes" behaviour I have would have resulted in enough of a page reload to notice that it had been changed. &gt; I love paren-less function calls, as seen in Haskell, but I do believe some syntax should be required for function calls that do not have any explicit arguments. I'm not really a Haskell fan. A lot of the syntax choices it made to optimize for code density are things which my younger self got burned by after incautious use of them led to hard-to-maintain code or hard-to-find typos in other languages like CoffeeScript.
Making it easy for distributions to ship packages built with Rust is an important step
A really sexy type system. Ease of avoiding race conditions. Easier C interop. Compilation directly to native code.
ah interesting! I did not know that `filter` had that, though it makes perfect sense. So yeah, I think you might be right!
Sorry, I didn't tried to offend you or start a holy war. I'm just can't agree on using Rust as a general purpose language - but I'm agree with most of your points.
Do we have to describe the type system as "sexy"? Can't it just be "good" or even "great"?
I'd be happy to file info upstream, but I'd rather be able to prove a performance improvement with some kind of skip-style implementation first. I'm hoping that a more capable Rustacean may be able to figure out a way to make my code faster :)
 &gt; it can easily be called by C code (it can export C compatible functions and structures) I claim it is not; because Rust code can panic. Panicking inside a C callback [is, AFAIK, still UB](https://github.com/rust-lang/rust/issues/18510), i e, a security hole. You could use `catch_unwind` on every C callback, but that is easy to forget, takes performance even in the non-failure case, and a bit error prone in itself (as it relies on that whatever happens after `catch_unwind` never panics). You could also compile your lib with abort-on-panic - but only if it's acceptable to kill the host process abruptly. If it's not, then you don't really have a good option. Either way, it's really easy to overlook (and get potential security holes as a result).
[removed]
QBasic is actually a [pretty nice language](https://en.wikibooks.org/wiki/QBasic/Arrays_and_Types). It is hard to resist syntax-bigotry; always make sure to reserve hate for [comments](https://wiki.haskell.org/Wadler's_Law).
It's okay, I was just confused by your response. I don't think Rust is the answer to everyone's problems. Easy to use languages like Python are going to be around for a particularly long time, and that's great. I just worry about all of our core infrastructure being written in memory unsafe languages like C or C++.
That is, until someone writes a fully abstract compiler in Rust that does proper compartmentalization and isolates the compiled crates from each other. ;)
&gt; I feel like it's a case of a picture inserted because some style guide somewhere says to break up blocks of text It doesn't accomplish that very well because it breaks and falls back to the `alt` text if the site's JavaScript has been disabled by NoScript/uMatrix/etc. I didn't even realize there images were supposed to be there until I saw the comment you replied to.
Heads up that your Google fonts (in site.css) trigger a warning about insecure scripts, and in Chrome at least they therefore don't load by default.
https://github.com/brson/brson.github.com/pull/9
&gt; QBasic is actually a pretty nice language. Point... though that was lost on me, given that I was using QBasic from about age 8 to about age 12 and had learned from a mix of the QBasic help file and pre-QBasic books on BASIC such as "My First Book About BASIC" by Microsoft Press. (published in 1985 or 1986, if I remember correctly) (After that point, I spent a year completely enthused about this thing I'd just received a book on called Perl, then discovered Python and, while I've worked in various other languages in the 15+ years since, Rust is the first thing I've found that appeals to me more than Python when I'm implementing I/O-bound stuff where the end-user experience is the main thing being optimized.) I was more intending to say that the only time I heavily used a language with things like demarcating blocks with words rather than symbols was when I lacked the context to make a judgment because I only knew one language and very poorly. &gt; It is hard to resist syntax-bigotry; always make sure to reserve hate for comments. Fair enough. I'll freely admit that most of my problem with said syntax decisions is due to bad experiences in the past where I ended up using other languages with similar elements incautiously and producing typos that still ran but produced incorrect results or code that was hard to maintain in one way or another.
I think people also misunderstand how hard it is to sell security as a benefit. Security is viewed as a *potential negative*, it's incredibly hard to convince anyone to take steps to avoid potential negatives, relative to definite postives, potential positives, or definite negatives. Microsoft put out a great paper about this a long time ago, something to do with that dancing pigs quote but I can't find it. So when you say "Oh, but rust is safe! Your C++ code isn't" very few will care. You haven't demonstrated a definitive negative aspect of their current code, you've only stated a potential negative aspect. So I agree, sell Rust as a whole, there's way more to it than "safer than C" and I think that the safety aspect is actually one of the least convincing for many devs.
I'm a Python dev, and sometimes it might make sense to drop down to C/C++. Rust is now a great alternative where you have guaranteed memory-safety as well as high performance. If Python works, it works, but try doing something harder on your CPU like running the genetic algorithm to train a neural net. While you should be using a good library for that sort of thing, if you *were* going to write your own library for Python, you might do the bulk of the work in Rust and use the CPython API to expose it to python devs. Just like numpy drops into fortran, any other Python code can now drop into Rust. Having C/C++/Rust under your belt as a Python developer can really help out those rare times where performance absolutely needs to be improved. When I run into bottlenecks it's very, very, very rarely the language, but if it ever is, that's how you can take care of it.
If you don't mind me asking, why don't you think Rust is good for general purpose programming?
That sounds like a problem with NoScript/uMatrix/etc., because it's a plain old `&lt;img&gt;` tag. ``` &lt;p&gt;&lt;img src="http://img2.thejournal.ie/inline/3280849/original/?width=372&amp;amp;version=3280849" alt="people casually sliding into your Github issues" /&gt;&lt;/p&gt; ``` (Though this is an incorrect use of `alt`, which is for text to be displayed when images are disabled, for accessibility. You want `title`. Also they're hotlinking. I don't like this rabbit-hole and I want to go home.)
For anyone wondering, the video is from https://www.theguardian.com/media/video/2017/mar/10/bbc-correspondent-interrupted-by-his-children-live-on-air-video . But yeah, it doesn't make much sense.
Thanks a lot for all your encouraging comments. :)
Sum types where the summands have the same type are useful, though, so enforcing different types would be pretty annoying. Why not just do what's done with tuples (with `t.0`, `t.1`, etc. access) and match on first, second, etc.?
ok, ok, I removed it
Maybe the reason is just me being not very experienced in Rust, but most of the code I write don't need to be super-performant, and in this case Rust feels less productive than garbage-collected languages. Also modern garbage collectors is good enough (most of the time) to not fallback to writing C for performance-critical code.
I'm in a similar situation (Python is my language of choice and I mainly write GUI apps that spend 99%+ of their time waiting on the user or other I/O) and, if I had to summarize it as concisely as possible, I'd say that the appeal of Rust is in how much correctness verification you can foist off on the compiler rather than reinventing in a less complete manner in unit tests. While not all of the points apply to you, I wrote a [well-structured reply](https://www.reddit.com/r/rust/comments/6kkgz7/according_to_stack_overflow_rust_is_the_most/djmvslo/) 9 days ago which covers things in more detail. Here's a paraphrased sample of some of the points in it: 1. Rust `enums` mean protocol/file-format serializers that refuse to compile, rather than failing with a "can't represent that type in JSON" error at runtime. 2. A standard library and ecosystem built around `Option&lt;T&gt;` from the beginning (instead of `NULL`/`None`/`nil`/etc.) means no runtime errors or crashes relating to an unexpected empty value. (It says right there in the function signature and functions can disallow it at compile time.) 3. Rust's monadic error-handling means that you can be sure you're handling every failure case that can be meaningfully recovered from just by looking at the function signatures. (No unhandled exceptions) 4. Rust is capable of implementing session types (ie. The compiler will catch that PHP "Can't set headers. We're already streaming the response body." error if you're using [Hyper](https://hyper.rs/).)
&gt; Panicking inside a C callback is, AFAIK, still UB, i e, a security hole. UB is not necessarily a security hole. It would be nice to have documentation about what currently happens if you panic inside Rust code called from C. I believe it should be pretty well-defined in an implementation-specific sense. &gt; You could use catch_unwind on every C callback, but that is easy to forget, takes performance even in the non-failure case This is the solution that has been proposed, just never implemented, from what I've seen, so the performance impact is likely to be small. The bigger concern is forgetting to do it. &gt; You could also compile your lib with abort-on-panic - but only if it's acceptable to kill the host process abruptly. If it's not, then you don't really have a good option. libraries shouldn't panic, but if they do, what else are you supposed to do? The equivalent C code would likely have segfaulted or aborted in some other manner. You could return an obtuse sentinel value like C code loves, but you're still in a screwy situation, because something was worth *panic*-ing about. That bug still needs to be resolved. I'm not disagreeing about that. But, writing Rust code in a library is still better than continuing to use C code, no matter how you slice and dice things, in my opinion, as long as Rust supports the targets you need and can be integrated into your build system.
&gt; `isize` is gonna be a `u64` *signed* pointer-sized integer = *unsigned* 64-bit integer? 
I admit to doing a bit of conclusion-jumping based on prior experience, so it's possible something else was breaking there. However, they seem to be entirely gone now, so I can'd diagnose further. &gt; Though this is an incorrect use of alt, which is for text to be displayed when images are disabled, for accessibility. Agreed there.
whoops nice catch, i meant to just say "64 bits", and messed it up
[removed]
If you want to "read all the bytes", the standard way to do it is: ``` let mut buf = Vec::new(); my_socket.read_to_end(&amp;mut buf); ``` This should work with Mio too, since the socket will be in non-blocking mode, and it should just read everything that's currently available. The main downside here is that if you get 100 GB over the wire for some reason, you're going to run out of memory. (I don't think that's possible with Mio, since you can only read what's available in your kernel's TCP buffer, but it could happen with blocking sockets.)
Gross. There's no reason to sexualize a programming language.
Juniper is the furthest from what I've seen. Problem is it's all server not client based if you're trying to make an API wrapper.
I always enjoy these kinds of comparisons, especially since I use Haskell at work and Rust in my spare time. I really enjoyed the article! :D
;) First time something I did ends up on 4chan. ^ _ ^
&gt; dancing pigs References it on the second slide, https://www.owasp.org/images/2/25/OWASP_angela_sasse_appsec_eu_aug2013.pdf Or maybe this http://www.ists.dartmouth.edu/docs/ecampus/2010/herley_ecampus2010.pdf by Cormac Herley of Microsoft. 
Are there guidelines in the community for when you make struct fields public vs using getters/setters? Should we be thinking about this the same way we do in other languages?
 &gt; UB is not necessarily a security hole. The same goes for a lot of the accidental UBs you happened to write in C, so this can just as well be an argument for *not* rewriting things in Rust :-) &gt; The bigger concern is forgetting to do it. ...or do things outside of it, that can potentially panic. Like [this bug](https://github.com/rust-lang/rust/issues/32475) in stdlib, which I fixed a year ago. &gt; This is the solution that has been proposed, just never implemented, from what I've seen, so the performance impact is likely to be small. The bigger concern is forgetting to do it. The performance impact is enough to make [Servo](https://github.com/rust-lang/rust/issues/34727) unhappy. EDIT: I read the bug a bit closer now, seems mostly fixed (at most 8ns overhead; and perhaps even lower since that figure is a bit old by now). Inserting an abort-on-panic landing pad is not the same as a `catch_unwind`, the former shouldn't have an effect on runtime performance at all (apart from some additional code size). 
&gt; You could also compile your lib with abort-on-panic - but only if it's acceptable to kill the host process abruptly. If it's not, then you don't really have a good option. This is what we do for Rust code embedded in Firefox, for what it’s worth.
 &gt; As new nightlies and betas are published, we use the cargobomb tool to test this corpus of Rust code (as of 2017/07/10 over 13,000 crates) against both the stable release and a nightly or beta release, comparing the results for regressions. Oh, so `cargo test` is run on every crate on every nightly? Nice!
Oh right, I think the intention was to use it for a React Js/relay front end client, not to be also a client API wrapper 
This week I will be testing out the [Lattice](https://github.com/andrew-lucker/Lattice) windowing manager on a larger project. I am building a [Slash'em style roguelike/roguelite](https://github.com/andrew-lucker/Premadeath). The Lattice project is still lacking a lot of graphics and positioning facilities. For example, there is no concept of a Container object that can hold and reposition children objects. Despite this development of the game is going well and have not had any real blockers yet. So, after this game is released to all major platforms, then I'll consider Lattice for a 1.0 release. 
But Rust is inspired by Ruby! 
&gt; so this can just as well be an argument for not rewriting things in Rust :-) Using Rust *just* to avoid C's UB is like using an airplane *just* to avoid the mosquitoes you'd have to deal with on horseback. It's certainly a benefit, but it kinda feels like missing the point. Maybe that comparison is too extreme... maybe. Using Rust already allows you to avoid all of the well-known terrible behaviors of double-free, buffer overruns, etc. that you have in the C world, at compile time. That alone is reason enough to use Rust. Not to mention the expressive type system which allows you to catch lots of logic errors at compile time. Even if Rust has UBs that are security holes, Defense In Depth is a thing. Rust is clearly already providing an additional layer of safety over what C gives you, and these UB holes will be fixed sooner or later. Throwing in the towel because of one UB is throwing the baby out with the bathwater. &gt; The performance impact is enough to make Servo unhappy. And I'm perfectly fine with there being an option to have a non-catching extern function, but the default should be safe. Being able to compile with panic=abort would seemingly solve the performance problem for Servo, and I can't imagine them intentionally allowing panics to unwind across the FFI, so I don't know what else they could be doing.
&gt; Do you have an example of code that would be broken by it? Well, it would break in exactly the same way as how it breaks in the example on the linked page. If some library I'm using specifies that they want exactly version 3.0 of libc, and I upgrade my dependency on libc from version 3.0 to version 3.1, my code breaks because the `c_void` types won't be compatible. Although now that I think about it more, would this break even if the definition of `c_void` isn't altered? I don't really understand *why* the `c_void` types are incompatible across two different versions (the linked article is pretty vague about this).
I finished up my UI work with [stylish](https://github.com/Thinkofname/stylish) implementing [sliders and dropdown boxes](https://i.imgur.com/vCpnGHw.gifv) as style rules for my game. I also worked on adding some audio to my game: https://youtu.be/RvABOwwqot4 Nothing major just a few sound effects plus music. The audio is done over sdl2 with my own mixer. I have thought about trying https://github.com/tomaka/rodio but I don't know how stable it is currently.
I've been putting off writing a new game prototype in Rust and after watching the Mozilla game-dev talks recently I might just have to pick up this framework! Looked very simple and solid!
As an additional point, trying it out on Godbolt (which is very good at cleaning assembly), the "loop" version gets largely unrolled and vectorised while the "iterator" version is way smaller but does lot of conditional jumping around. Interestingly, removing the `filter` doesn't actually improve the iterator version (it remains non-vectorised). Using a `for`: pub fn rust_iterloop(high: i32) -&gt; i32 { let mut total = 0; for i in (1..high+1) { if i % 2 == 0 { total += i * 2; } } total } yields vectorised assembly but not quite the same as the original loop, possibly because it does not implement the backwards-looping optimisation? It seems to differ in far more ways than just that but my assembly knowledge is very limited especially when it comes to SSE. Would be interesting it to bench it alongside the original. Finally, iterating on the filtered range (so removing the `if` and adding a `.filter`) *does* break vectorisation. [gist with the assemblies pasted from godbolt](https://gist.github.com/anonymous/32b65375b7204488f48b5dcda3ef6719)
Also `int` would be signed, so `i32` rather than `u32`.
Rust doesn't borrow the design elements from Ruby which bother me. Crystal very visibly does. In the interest of being punchy, I neglected to mention that it was more a case of "As soon as I read that, I got a sinking feeling. As soon as I saw a code snippet, my inner petulant child started calling for Crystal's failure."
I have used Juniper and contributed a patch. It's not very actively developed, but it works and you get pretty good performance. It has Iron or rocket handlers which give you a GraphIQL endpoint out of the box for development, which is nice. I think for GraphQL in Rust to be feasible for a large/complicated backend we will need a Tokio based solution with something similar to DataLoader from Facebook, otherwise any complicated schema get's way too messy.
if `futures::task::park().unpark()` is deprecated, what replaces it?
deleted ^^^^^^^^^^^^^^^^0.5455 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/47628)
One thing I love about Rust in scenarios where performance is not relevant is error handling. I just think it's so much better than exceptions.
&gt; The bigger concern is forgetting to do it. And the bigger concern of some dangerous C feature like unchecked array indexing is forgetting to do bounds check. Specially because *sometimes* you don't need to do bounds check, just like *sometimes* you don't need `catch_unwind`. The right thing to do IMO is to define the behavior of unwinding to another languages, at least for some platforms and compilers.
No, it was a research paper, not slides. Unfortunately, because the paper title was based on a common quote it's incredibly hard to find, I haven't in years. The MS paper was significant to me because it advocated that the user was *correct* to not choose security, and I found their argument incredibly compelling and representative. These slides seem to cover something similar though - that the average person's costs are impacted more by following security advice than not.
agreed, I was saying forgetting to do it is a bigger concern than the performance implications of having it done by default.
Can you explain how to use handles? I tired using them but just got more confused. `Handle#handle` and `Handle#spawn(_fn)` return imedetly and only take futures that return something like `Result&lt;(), ()&gt;`. Which makes sense since you schedule something to run _in the background_. But then even spawning "task" on reactor doesn't turn reactor and you can't turn reactor from Handle. Which leads to the concept that Library API must just return `Box&lt;Future&gt;` (or `impl Future`) and let consumer to worry about how to run reactor. This is where I get completely lost. All usage of Handle shown in any example is just passing it down to some tokio lib owned structure, which doesn't explain how to use it. Which is okay because you mostly only need to schedule timers and sync wrappers around your async client. However how do you create complex application, say its a http server that makes database and http requests when serving clients or in the background. This where I get completely lost. 
A lot of Scala evangelism seems to have been of this kind. "Oh, so we need to rewrite this? Or add this new module? Why not just do this little thing in Scala, it'll be so much easier and it integrates nicely!". But nowadays, it seems a lot of people are tearing their hair out over that.
Yet that's precisely what Rust is for: general purpose programming. Honestly, it's perfect as a general purpose language. Borrow checking isn't an issue if you know how to write software in Rust.
Here is a partial list of things that would stop me from using Rust for internal parts of a Linux-oriented C program/library: - jemalloc - for some reason Rust doesn't use the system allocator. - small standard library - C programmers are averse to dependencies in general, for both practical reasons (licenses, build systems, stability) and philosophical reasons. So in C you would usually only see "big" dependencies, the kind a distro would have a package for. Trivial stuff is hand-rolled or vendored. In Rust, the situation is exacerbated by two things: having to statically link everything (no stable ABI), and the npm-inspired small crate philosophy. In a sense, cargo/crates.io being so convenient is a misfeature here :) - small architecture support - comparing the list [here](https://packages.debian.org/sid/gcc#pdownload) to the list [here](https://packages.debian.org/sid/rustc#pdownload), adding any Rust to a project is a big loss in portability. - impossible to handle OOM. Well, actually I personally don't care about that most of the time, but other C programmers do. - No language specification, no alternative implementations - not absolutely necessary, but makes you think twice before moving off a language which does have those things. The only test case I know of up to now is librsvg. But I am not sure it's a good example; they seem to be doing the transition in a rather haphazard manner. For example, I cannot make sense of this: https://github.com/GNOME/librsvg/blob/master/rust/Cargo.toml. Git dependencies? Lock file in a library? Where are all the warranty disclaimers required for statically linking the transitive dependencies (and Rust itself)? etc. As far as I can see, no distro has adopted a new librsvg release since they started using Rust; when that happens it would be a good indicator.
I agree and I have to admit that I see no special dedication in that direction (I package for nixos, but not Rust because it is a especially hard thing to package rust for nixos).
&gt;there's way more to it than "safer than C" and I think that the safety aspect is actually one of the least convincing for many devs. I think you're right that many developers don't find that argument convincing, but I also think that's a huge problem. As I said, security has consequences! And I think that we (programmers as a whole, not just Rustaceans) should face that head-on. 
I've never opened a github issue saying that a codebase should be re-written in rust, but I don't understand why this makes people upset. &gt; "Have you considered rewriting this in Rust?" &gt; "Yes, and we're not going to do it." End of discussion, no?
Why is ferris blue?
&gt; It is normal human behavior to double down on existing decisions when feeling challenged. The core tenets of Rust are themselves viewed as a direct opposition to the status quo, so even mentioning it can put people on the defensive. Many people have invested a lot of time and effort in learning C/C++ and it won't convince anyone to show up with the attitude that they are doing it wrong and/or wasted their time. I agree that some people have vested interests in C/C++. And that is what it is. I'm not sure how accurate it is, since I assume C/C++ experience is beneficial if you are learning Rust. What I'm specifically arguing for is that we now recognize that all C/C++ code is *insecure*, and especially that we make an effort to ensure web services stop using C (recall Cloudflare, Yahoo). I don't care if you make a video game in C++ or an image conversion library in C (and I've depended on ImageMagick in the past). Further, as web service security has real consequences for people's lives, I think it's important to stop the hand-wringing in order to be a bit more diplomatic. C does not belong in a web service, point blank.
I am not really getting how, but from reading the docs it seems to me that the way to do things is through [notify(&amp;self, id: usize)](https://docs.rs/futures/0.1.14/futures/executor/trait.Notify.html). Though it seems like there is then a bit more work, forcing us to set up an id in the struct struct PollMeNTimes&lt;T&gt; { id: tokio_core::reactor::CoreId, n: u64, answer: T } along with having handle call let id = handle.id(); and passing the id to the struct. Would greatly appreciate though if someone familiar with the up-to-date code would like to chime in. In any case I am happy to see this blog post. Futures and stuff has been a "scary" thing for me for some time.
&gt; jemalloc This is [optional](https://doc.rust-lang.org/1.15.1/book/custom-allocators.html). Currently, it requires using nightly to opt-in to switching allocators, so that's a downside, but a rather small one in my opinion. Rust nightlies are [thoroughly tested](https://brson.github.io/2017/07/10/how-rust-is-tested). For production apps, I would still want to use a stable version of Rust, of course, and honestly, jemalloc brings a lot of performance benefits with it, from what I've seen. &gt; small standard library [...] For starters, C has a small standard library, smaller than Rust's, so this confuses me. With a `.lock` file, you get pretty deterministic builds, so it's not a big deal in practice, and it's actually nice to have access to 3rd party libs so easily. &gt; small architecture support that's not really representative. Look at [this list](https://forge.rust-lang.org/platform-support.html). It's absolutely a smaller list than what gcc supports, but it's not much different from what clang/llvm supports. &gt; impossible to handle OOM You can thank Linux for that. You're unlikely to see an allocation fail on Linux, you will just get terminated. [Malloc doesn't fail.](https://stackoverflow.com/a/16674629) Rust _should_ provide some facilities to handle OOM on platforms that actually have fallible allocation APIs, but you specifically mentioned Linux, so checking the return value doesn't seem to matter there. &gt; No language specification, no alternative implementations I would love to see these happen, and they likely will eventually. &gt; Git dependencies? Cargo doesn't require you to use crates.io. Several of those libraries are on his own GitHub account, and for whatever reason he has chosen not to push them to crates.io. You can even use directory paths to specify other crates that are just in your filesystem, whether they're in your working directory or elsewhere. How you manage your project is up to you. crates.io provides additional stability guarantees because authors cannot delete their published packages from there, which removes one concern of things disappearing. There are also tools to vendor your dependencies inside your repo if you need ultimate assurance of that. &gt; Lock file in a library? If it's a library intended to be used only by a C application, there's no real downside to having a lock file on a library, and it does add a little bit more determinism to things. That lock file would be ignored if a Rust application depended on that library, of course. &gt; As far as I can see, no distro has adopted a new librsvg release since they started using Rust; when that happens it would be a good indicator. There has been precisely one release of librsvg with Rust code in it, as far as I can tell, and that was just six months ago, which is a blink of the eye for distros like Debian. Linux distros tend to move slowly on library updates, unless there is a security fix. I'm not worried about this.
If we think of developers as users we can, in my opinion, assume they'll never change to prioritize security. A long history of failures re: educating users should be evidence enough.
I should also mention that librsvg is not the only "test case". Parts of Firefox are written in Rust now, and that's not going to hinder adoption by Linux distros, although it will add some challenges for maintainers not familiar with the Rust ecosystem, and that's a very prominent package, to say the least. Once they cut their teeth with Firefox, hopefully it'll be less of an issue for other packages written in Rust.
might be a dumb question, but can you give an example of how sum type is useful in practice? also is rust match similar or more powerful than scala match? 
You may like the tooling, one of many language features, modern development practices or the fact that all those things combine into very well-designed whole. However you will find many other languages that may have equally or more compelling reasons to switch to. Also if you don't have significant issues with the language, libraries will be probably more important.
I've never used Scala, so I have no idea on that. Sum types represent the very common situation of things that could be this OR that OR that other thing. A practical example is found in [this blog post](https://people.gnome.org/~federico/news-2016-10.html#28) about librsvg using Rust. Originally, in C, the `Segment` struct had a boolean to determine which kind of segment it was. Depending on the state of that boolean, you had to know which elements you could access, and which ones were undefined. The compiler did not enforce this. With the Rust code using an enum, it is no longer possible to accidentally use invalid members of `Segment`. Sum types are useful in so many situations in real world code.
&gt; jemalloc - for some reason Rust doesn't use the system allocator. You are free switch to standard allocator, its not a problem. &gt; C programmers are averse to dependencies in general You get used to good things very fast. As for other things - yes, they aren't there. Might come with time.
Rewriting is costly and while Rust is awesome, it is a programming language, not a pixie-dust. Other people have listed potential advantages, but they will probably not be worth it on their own. No need to worry. :) The RIIR *maybe* makes sense for old C, unsafe code that is kept being reused in our infrastructure.
Yeah, not every nightly. I aim to keep it running around the clock on some recently nightly or beta, but its often sitting idle (I could use help doing cargobomb runs and triaging their results). Right now we may be averaging about 1 run per week. Most betas get a run. Also Tom Prince has been working to extend cargobomb's capabilities and offer cargobomb as a service to PR authors.
Is there any possibility of seeing compilefail available as not a nightly-only library? It's a valuable tool for anybody building a type safe API
&gt; performance-competitive with C and C++, yet it offers memory safety, algebraic sum types, first-class expressions, and a host of other things. Worth noting that Enums do have a performance overhead. &gt; Was everything those C++ developers worked for done in vain? Are their skills obsolete? I doubt their skills will be superfluous. Anyways, saying their work was in vain is a bit like saying Euclid is useless because we have algebraic geometry now. With any insight, you start from the examples and then abstract. Rust learned from C and OCaml, among others.
Syntax is pretty inconsequential to how good a language is. Or at least, it's rarely the hardest part of programming in any given language. 
I meant instead of the `libc` functions you use (`poll`, `epoll`, etc.). `nix` provides safe, thin wrapper APIs around the common `libc` functions. If you haven't taken it for a spin I suggest you try it out, as it should be slightly easier to use than `libc` directly depending on your exact use case.
&gt; Worth noting that Enums do have a performance overhead Not really. They are a zero-cost abstraction, which means it is as efficient as if you did it all by hand, without the syntax sugar and the automation. Switching on an enum's type is just as efficient as manually tagging a union in C, and then switching on the tag. The compiler is also opportunistic, and will eliminate the memory overhead of the tag entirely in some situations (like with `Option` on certain types), but it's always at least as efficient as a manually-tagged union in C, as far as I know. Now, writing correct code does have some overhead. It's possible to write C code that ignores the tag and just makes unproven assumptions, and that will perform better and cause unpredictable results better.
[This crate](https://github.com/laumann/compiletest-rs) provides such support out of tree. I don't think there's any particular movement to provide such a feature officially, though it is obviously quite a useful capability to have. Compile-fail testing doesn't fit very cleanly into the basic unit testing Rust offers today, so it would be a big effort to provide it. Interestingly though, rustdoc has a compile-fail testing feature, because it's model is to compile each example as seperate compilation units. It may still be nightly only, but there's no obvious reason not to stabilize it at some point.
For me, speed. I use primarily Haskell and it's the language I write in for work. But I still reached for Rust when I wanted [something](https://github.com/vmchale/tin-summer) to replace du for my own usage. 
what design elements are you referencing? 
&gt;so you get super nice type inference in ways that the simple type inference engine in a language like Go cannot provide. Rust's type inference is actually pretty far below that of some functional languages. For instance, you have to declare a function's type to declare a function. 
&gt;Also if you don't need manual MM you will just fight borrow checker for no profit. Well, I think you partially answered your own question. Rust can also give insight into how computers work, which is useful for any programmer regardless of what language you later pick. 
They are [octal escapes](https://docs.rs/regex/0.2.2/regex/#escape-sequences). Personally, I don't really know why I added them. I think I added it because RE2 had it, and I wasn't really thinking. `regex` is still at `0.2`, so technically, when I bump it to `1.0`, I could remove support for them and possibly produce a clearer error message that backreferences aren't supported. What do people think? Does anyone use octal escapes?
&gt; can you give an example of how sum type is useful in practice? You can use sum types to express: - [Modes of operation](https://github.com/soupi/gathering/blob/master/src/Web/Gathering/Config.hs#L141) - [A constraint subset of values you can get by parsing something](https://github.com/soupi/pursuit-client/blob/master/src/Web/Pursuit/Client.hs#L30) - States for a state machine like [views/screens for your app/game](https://github.com/soupi/ld34/blob/gh-pages/src/Main.purs#L34) - [Possible user actions in your game/app](https://github.com/soupi/guess-num/blob/master/src/Main.purs#L44) - [Programming languages](https://github.com/soupi/nyanpasu/blob/master/src/Language/Nyanpasu/LL/AST.hs#L35) and [DSLs](https://github.com/soupi/purescript-slides/blob/master/src/Slides.purs#L158) Sorry this is Haskell/PureScript and not Rust/Scala.
I'm planning on merging a nice wrapper around `termios` into [nix](https://github.com/nix-rust/nix) this week in time for the big 0.9 release, which may also happen this week. This release will include support for Android, iOS, and a few other platforms, which is a pretty big deal I think. I'm also planning to push a little more on the winapi 0.3 migration as it's still blocking a release of some of my crates.
that's not a limitation of the engine, that's a design choice to reduce the likelihood of type inference taking forever, since the scope is limited, and to make APIs easier to use and more stable. "far above" rather than "far below", depending on what you're looking for.
no one takes "expressive" seriously.
"General purpose programming" is not really well-defined. I'd not want to write a compiler in Rust rather than Haskell. I also know most scientists would prefer to continue on in python, because they need publishable results, not robust libraries. Rust *does* however expand what you can do with computers. It's easy to think "Oh, I can't make a replacement for grep, that's too hard/it's been around for years", and Rust disabuses you of that notion. 
Not true -- see your sibling posts.
The two that caught my eye immediately are: 1. Function calls without parens (I'm quite attached to the idea that syntax in the vein of `x = foo` is assigning a first-class function to a variable while `x = foo()` or `x = foo[]` or whatever is calling it with no arguments and assigning the result to a variable.) 2. Using words rather than punctuation or significant indentation to denote the extent of blocks. Both of those are things I find unappealing.
&gt;general purpose programming. Honestly, it's perfect as a general purpose language. Borrow checking isn't an issue if you know how to write software in Rust. Lots of scientists use python because you'll get *results*. The fact that writing good libraries is hard doesn't matter if you're an end user. And rust just makes some things harder. 
&gt;but can you give an example of how sum type is useful in practice? Syntax trees and error handling with Option/Result come to mind. 
Let the library users to supply the `&amp;Handle` (which you can immediately clone) when creating the client, similarly to the way tokio libraries do. And then let your library users to run returned futures on their tokio core of choice. As for how this works, I will try to explain it as I understand. When we combine futures with one another, we build gigantic state machine that can be paused when waiting for something and resumed when some part of it detects that it can be resumed. Those lowest parts that detects when they can be resumed are things that we spawn on various "cores". It can be tokio core, however, it can also be [something like this](https://github.com/Nercury/epics-rs/blob/6c49001a3998eb8cd6f2d4cebe47c396c2cb595f/lib/epicloop/src/lib.rs#L96), where two closures are spawned on two cpu pools and then result is combined. Futures are what makes all the cores compatible. You can mix parts that run on cpu pools with something like timer (used in previous example), or tokio core, or everything together ([example that spawns 10 tasks and runs up to 5 of them in parallel, uses timer, cpu pool, tokio core](https://github.com/Nercury/epics-rs/blob/0533a2f7ff783aa6250581fc79a3fdce1602079b/lib/epicloop/src/lib.rs#L92)). However, there seems to be a restriction regarding any futures that are spawned on tokio core handle: they can only run on tokio core, simple "wait()" will not work. EDIT: Ok, more about the handles. I think you may need to clone the handle you got as many times as needed to create all the necessary futures that perform library tasks. For example, connecting and then fetching data might need to pass handle reference to connect and then use its clone to start reading the response.
I won't disagree, but it doesn't mean I can't have a visceral desire that languages with syntax I dislike should fail to reach critical mass for reasons that may or may not be related. (If for no other reason than to minimize the chance that I'll be faced with having a real "Is it worthwhile to learn this?" choice down the road.)
Right, we use that crate right now. It sucks that it's nightly-only though. It means that regressions there have a tendency to slip through unnoticed.
&gt;small standard library - C programmers are averse to dependencies in general, for both practical reasons (licenses, build systems, stability) and philosophical reasons. So in C you would usually only see "big" dependencies, the kind a distro would have a package for. Trivial stuff is hand-rolled or vendored. In Rust, the situation is exacerbated by two things: having to statically link everything (no stable ABI), and the npm-inspired small crate philosophy. Leaving things to libraries doesn't have to be a bad thing. Libraries like numpy are trustworthy on their own, while the python standard library is so large it accrues a lot of stuff that's poorly maintained. Anyways, so long as cargo is good, this is a good thing imo. &gt;small architecture support - comparing the list here to the list here, adding any Rust to a project is a big loss in portability. True, but I personally have never been bit by that. I doubt many care about support for 32-bit x86 running on GNU Hurd. Admittedly I mostly make CLI tools for programmers so YMMV.
The reason I like rust for general purpose programming is that generally speaking, once you get your code to compile, it just *works*. There are way fewer opportunities to choke away something simple when the compiler handles so much for you. Also, when it doesn't compile, I usually get a pretty nice message about why -- "Hey, I need this to be a `Foobar` but you gave me a `Result`". Oops! Thanks rustc, that was very helpful. The only other language that's given me such warm &amp; fuzzies was Scala, but I have a deep-seated disdain for the JVM and maven/sbt. That might be a me thing tho
&gt; The performance impact is enough to make Servo unhappy. &gt; Servo is very tightly tied to a JS engine with a lot of callbacks running back and forth. This is a pretty abnormal use case. (Also, if you look at that bug, it's a year old and the significant perf hit was fixed.) In general catching is a small impact.
Servo needs to be able to catch and report panics. The solution here is to abort only at the FFI boundary, which can be done by instantiating a type with a special destructor on the FFI calls.
Cool. Still running `cargo test` is a big step up from running `cargo build`, which is what crater does IIRC.
&gt;"far above" rather than "far below", depending on what you're looking for. Not really. I understand Rust's developers prioritize some features and I trust their judgment, but pretending it's a bonus is silly. &gt;and to make APIs easier to use and more stable Works fine in Haskell. &gt;reduce the likelihood of type inference taking forever, since the scope is limited I don't know the specifics, so I can't comment beyond "it works fine in the functional languages I've tried." I can see it being different for Rust due to function blocks being longer though. 
[Getting there](https://developer.arm.com/products/system-design/system-guidance/system-guidance-for-infrastructure). :-)
you're not correct, though. APIs in Haskell declare their types, conventionally, so people do still write out type signatures. If you have a large program that requires type inference on every variable and function, compilation times do begin to grow rapidly. It "works fine" because others are not being so liberal with the inference engine. They're writing signatures, even if you're not. &gt; pretending it's a bonus is silly it's not pretending. It was an intentional decision by the Rust developers. The type inference engine is more than capable of determining the types of functions. I would enjoy having the option to sandbox without type sigs, but it would only ever be useful for playing around in a sandbox. Functions having signatures be mandatory is excellent for many reasons. I don't know why you're always bashing Rust, but then claiming to like both Rust and Haskell, in comments that I've seen over the past few weeks of interacting with your comments.
The Herley paper that goes with those slides is [here](http://dl.acm.org/citation.cfm?id=1719050). It sounds like what you were saying: _So long, and no thanks for the externalities: the rational rejection of security advice by users_.
&gt; but I think it also works well as a general comment on Rust safety. I think it works well generally as a comment on *lots* of things...
and those are reasons for a language to fail? I find both of those things massively appealing and extremely frustrating that other languages don't have. What's your favorite language? I'll go ahead and pick on some minor detail and wish that that language fails as well.
Ah possible. I'll check it out later. Thank you.
I'm a big fan of the Rust testsuite and the approach to only accept PRs that pass all the tests, and to also test the merge commits and not the heads of the branches. This is really great! Even greater is that the artifacts of every single such merge build are uploaded, which aids greatly in bisecting regressions (there is a [tool](https://github.com/Mark-Simulacrum/bisect-rust) for this). One thing that makes me sad however is how there are multiple git submodules in the repository and if you break something upstream, you often have to change the downstream modules as well, which leads to complications as often those downstream projects have their own CI and will only accept changes that make CI pass like Rust does. Fortunately though people of the downstream projects are very kind and accept such PRs gladly and promptly.
&gt; I love paren-less function calls, as seen in Haskell, but I do believe some syntax should be required for function calls that do not have any explicit arguments. It makes sense in Haskell, OCaml, Elm, etc. because everything is curried by default. It makes little sense in other languages, and forces you to think 'should I put parens around this argument list, or should I not?' whenever you call a function/method.
Looking at the actual assembly generated, that does seem to be the problem. The iterator-based versions end up compiling down to two nested loops in assembly: rust_iter: ; prologue, puts high+1 in rdi, total=0 in ecx, and uses edx as counter=1 push rbp mov rbp, rsp inc rdi xor ecx, ecx mov edx, 1 .LBB1_1: ; outer loop: essentially total += counter * 2 - 2. Note that it figured out that the first iteration always executes this as it starts at value 1 mov rax, rcx lea rcx, [rax + 2*rdx - 2] mov rsi, rdx .LBB1_2: ; inner loop ; if counter = high+1 break cmp rsi, rdi jge .LBB1_4 lea rdx, [rsi + 1] ; total += 2 add rcx, 2 ; store if the current counter is odd (i.e. if the next counter is even) test sil, 1 ; increment the counter mov rsi, rdx ; if the counter is even go to the outer loop, otherwise go to the inner (skip) loop jne .LBB1_2 jmp .LBB1_1 .LBB1_4: pop rbp ret Looking at this however, I wouldn't say that the compiler failed. The issue seems to be more that it tried to go down a branch of optimization that didn't pan out too well by seeming to reverse the order in which some of the loop events happen (which is why the strange `- 2` appears in the outer loop, which indicates that the counter has already been incremented there while it's still doing the addition of the total from the previous iteration. In the process it however missed the most critical optimization. Currently the `jne .LBB1_2`is completely trashing the branch prediction. If the compiler would've unrolled the loop by a factor of 2, it would suddenly have very nicely predictable branches, or it would've even been able to completely strip away the skip loop branches. Either way, this is why the filter approach has such disastrous performance. Meanwhile the loop version manages to ifconvert the branch into a conditional move. While this is still not as good as just removing the skip logic entirely, the conditional move doesn't trash the branch predictor, which is why this code ends up much more performant. (At least, when testing with isize's. when tested with i32's the compiler just straight up vectorizes the code which makes it hard to observe what is happening unless you know your AVX/SSE instructions very well) Other interesting things noted: iterating through the range is indeed faster. Even for the iterator-based approach, inserting a `.rev()`call in between speeds up the loop. One additionally interesting thing is that if you rewrite the iterator as a fold, i.e. pub fn rust_fold(high: isize) -&gt; isize { (1..high + 1).fold(0, |s, x| if x % 2 == 0 {s + x * 2} else {s}) } the compiler is able to make the conditional move optimization, but additionally, it is also able to unroll the loop several times, which would mean the conditional move prediction is significantly better. Seems like the lack of a mutable accumulator value there does actually allow for some extra optimizations. It should be noted though that when you compile with `-C target-cpu=native` on godbolt, it just straight up vectorizes both the loop-based functions and the fold-based functions into unrolled loops of 256-bit vector ops. It looks like being able to figure out that it can ifconvert the branch away is absolutely critical to this operation. Unfortunately the skip loop in .filter() seems to inhibit this optimization. You can find all code for this [at godbolt](https://godbolt.org/g/fBBMeV), where you can really easily see the effect of different optimizations on the code being executed by changing the definition of `T` between isize and i32, and by turning on or off `-C target-cpu=native`
I prefer saying 'nicely typed' these days. :D
I have a bit more time and I think I've found a reasonable, short explanation of what you're missing. What enables function pointers to be higher rank over lifetimes is not that lifetimes are "coalesced into a single one before trans," it is that they do not determine *runtime representation* at all. Types do not "exist at runtime," but they obviously extremely do determine runtime representation of the values of types; this is why we cannot have function pointers be higher rank in types (without RTTI and violating our monomorphization guarantees). But since we do have function pointers that are higher rank in lifetimes, correctly dispatching lifetime based specialization would require lifetimes to result in runtime information. This is unacceptable.
amazing work, thank you!
This was asked last week. I can still only guess. Either * Ferris wants to cosplay Sonic the Hedgehog * or goes so fast that the Doppler effect shifts the red color into blue.
Hi, my experience is mostly with Python and a bit with C++ so maybe the things I have enjoyed about Rust will be relevant to you. Cargo, which is the best package manager I've ever seen and also a build system that's as easy to use as a Python dev setup. All you have to do is list your requirements in a file and cargo takes care of the rest .You can also `cargo install` dev tools like the ion shell and clippy, an awesome Rust linter. Almost all the compiler errors are imminently helpful (and those that aren't are considered a bug). There's a steep learning curve at the beginning to get comfortable with the terminology in the borrow checker, but that only lasted only a few days for me. This makes my time writing Rust much more enjoyable than C/C++ with obtuse or vague compiler errors and Python where errors pop out at runtime. The type system does so much to help with correctness at compile time. I actually feel rather crippled writing Python now because even with PyCharm watching over me it's easy to make simple type errors that a Rust compiler will identify in about a second.
They are renamed to `current()` and `notify()`.
&gt; and those are reasons for a language to fail? You seem to have missed my use of phrases like "my inner petulant child" and "visceral desire". (Especially contrasted with phrases like "my rational self") They're reasons why, on an emotional level, I hope they fail, despite, on a more rational level, knowing that they're not that important. (Sort of a "Let's cover the world in leather rather than making shoes" approach to the possibility that I might have to learn them in the future if they become popular.) &gt; I find both of those things massively appealing and extremely frustrating that other languages don't have. While I wouldn't consider them reasons for a language to fail when looking at them as a mature adult, I do find them bothersome because: 1. I see an appealing regularity in consistently requiring the same specialized indicator to execute a function. (ie. While not ideal, even `function_ref = function`, `result = function!`, and `result = function! arg1` would be better in my eyes.) 2. I find it easier to read code if using some kind of balanced pair of tokens in function calling is done consistently, rather than merely as a way to disambiguate things like the meaning of newlines or nested calls. (ie. `func1(foo, func2(bar))`, with or without the comma, rather than something like `func1 foo (func2 bar)`. 3. Using specialized symbols (eg. `{` and `}`) or significant indentation for coarse code structuring is easier for me to skim-read than using words (eg. `end`) for the same reason that it's less taxing to skim looking for an icon than the associated word. It takes me less mental effort to scan for glyphs with unique meanings or the edges of rectangles of white, rather than for glyphs which only have the desired meaning in certain sequences. &gt; What's your favorite language? I'll go ahead and pick on some minor detail and wish that that language fails as well. You shouldn't be wasting both our time arguing about this any more than you should waste your time trying to convince me that I hate the taste of strawberries. I was sharing an emotional reaction and you can't reason with emotions.
[removed]
I understand this parts. However you can only spawn futures that return nothing in either case, that means you have you use oneshot channel to return result from your future. However Handle is only a Handle if it's on the same thread as Reactor, it becomes an Remote that can't be upgraded to Handle once you move it to another thread. That means you have to spawn futures and turn reactor on the same thread. This is where I get confused. Just calling Handle#handle will only submit task to core. Something need to turn the reactor. From what I understood I can just supply empty future that never resolve and reactor will keep turning and polling that never-ending future. This makes sync wrapper on shared core impossible. Library can't turn reactor (Core#turn, Core#run) therefore it can't guarantee that any of futures submitted to Core will ever resolve. It be nice if there was a method to run reactor until everything submitted are resolved. P.S. In example you linked cpu_pool is never used in that method.
[removed]
What language are you using? 
Ahh, so that's why it's so confusing
cargo will not resolve to multiple versions within a major version series in the same dependency graph. That is, you can never have 3.0 and 3.1 together during the same build. You *can* have 3.0 and 4.0 (and 5.0, etc), however.
[removed]
The liner crate has very basic functionality at the moment, and as it's an entirely separate project from Ion, I don't maintain or develop it.
But the library users will need to use futures to get results from your library methods. I mean, a call to your library will produce a future, and the user will need to combine that future with another one (as an example, something that performs a computation or saves result to database), and _only then_ run it all on the core. In the end, this whole setup will happen on the main thread. (Sorry for the mess in my code, it was not meant to be an example, only some experiments)
I'm currently the one running clippy on Rust (though as I've seen not the only one), and I send PRs whenever I find the time. The plan to sort of stabilize clippy is still going slowly, this would greatly ease using clippy with the Rust codebase. In fact, once that's done, I'd like to enable clippy linting by feature in all rustc crates one by one, so we can have a buildbot with clippy enabled, notifying us (and the author) if clippy found something.
&gt; You may be surprised that C is about twice as fast as Rust and Haskell. But look again: C is taking 87 nanoseconds, while Rust and Haskell both take about 175 microseconds. It turns out that GCC it able to optimize this into a downward-counting loop, which drastically improves the performance. You don't get 87 nanoseconds with a loop of over 100 thousand iterations; 87ns is only a few hundred cycles! I'd love to see the CPU that has an IPC of 1000. :) GCC has optimized the code to a closed-form expression (or possibly some kind of log(N) loop), and chances are things like the function call overhead are the majority of those 87 nanoseconds.
Thank you so much for this location :)
Why does it mention a C stdlib in the download name? What exactly is MUSL Linux?
Lots of scientists shouldn't be using Python, especially given that Julia is a much better language for that purpose. Python only results in exceptionally high resource consumption, slow simulations, and high development costs due to only being able to catch errors at runtime.
All Rust software on Linux has to use a C standard library to compile on Linux. There are two choices: glibc being the default target, which is required if you are using a library that links to a dynamic C library; or use the musl target if you have no dynamic C dependencies, or C dependencies at all, in order to get a static binary. There has been an attempt to remove the C dependencies from Rust though, with a Linux-exclusive fork of the Rust standard library: [steed](https://github.com/japaric/steed)
Knowing where to look, I now found the file, it seems to be created empty at startup. Not nice :(
Rust doesn't have them, and I've just found a bug in LLVM where someone wrote some C++ code without considering that `\02` is a whole octal escape and that it's equivalent to `\x02` (as opposed to `\0` followed by `2`). Not to mention octal is confusing because it looks like decimal but it ain't it and it also doesn't match hex - so you have to know to convert. So I'd say just throw it out.
Rust's syntax explicitly panders to C++ devs, what are you talking about?
In a world where all people really debate is the title of a post, I really wish this had been titled, "Rewrite part of it in Rust." Inevitably people will read the title but not the content, and it will just further dig the trenches.
I disagree that having to use nightly is a small downside. In order for Rust to be taken seriously by the wider programming community, suggesting that people use nightly Rust for any reason is a non-starter. People who are not specifically interested in helping with bleeding edge Rust development should never even have to be aware of the existence of anything but stable Rust.
I get that. The way you describe library doesn't need Handle at all (only to pass it down TCPStream or whatever). This is how I do this — https://github.com/Inner-Heaven/angel-whisper/blob/master/src/llsd/client.rs#L82 and usage of that client — https://github.com/Inner-Heaven/angel-whisper/blob/master/tests/system-on-tokio.rs#L114 as you can see my client isn't aware what kind of Core it's running on.
Lovely post, brson! Rust's testing getup is seriously impressive and has matured a ton since I've last contributed.
FWIW, the "is Rust's main purpose to avoid C's UB?" question was discussed in depth at the start of the year (although not phrased exactly in those terms): https://brson.github.io/fireflowers/