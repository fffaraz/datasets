It's arithmetic ffs. You're comparing it to SipHash. 
I may have to give this a shot. Is there anything you find is missing that you wish it had? I've been learning Rust with Visual Studio Code + Rust plug-in. It's been alright so far. But I'm very familiar with IntelliJ's IDEs and would love to see a good Rust integration.
&gt; easier to read Short and easy to read are orthogonal. Its very easy to just skip all the imports when reading, and then read them as needed when going through the code. Its very hard to read where the imports come from if you use wildcards. I would say use of wildcards make the source code unambiguously harder to read. If you need it to fit on a blog or something, you can make imports collapsible, but I am not sure how much it helps.
I saw the PR was merged making the new format default and removing the flag from JSON, but I couldn't find any evidence in github where those changes made it into the 1.12 beta. The only thing I found was a comment saying they should make it in. Can you point me to what I missed by chance?
#### In other news, we have been making major progress on the kernel rewrite. It supports SMP and is for x86_64 by default. It runs on real hardware and will be much closer to a microkernel. **See the new kernel here: https://github.com/redox-os/kernel** #### The reliability will be improved: - Higher half kernel mapping removed opportunities for collisions of process address space with the kernel - More services will be moved to userspace - Use of no-execute on all pages where possible - Enforcement of Write XOR Execute #### The performance will be improved, in some cases by an order of magnitude: - Context switching, due to correct use of per-process page tables - Graphics operations, including the kernel terminal, due to the removal of allocations and duplicate operations, as well as the use of the Page Attribute Table #### There are other exciting future possibilities, now that we are using x86_64: - PCIDs allow several page tables to be cached at once in the TLB, reducing context switch cost further - VT-D could potentially allow for DMA to be controlled, leading to proofs of security and stability with userspace device drivers
You can use a [lazy static](https://docs.rs/lazy_static), or just a regular static (not `mut`) if it doesn't need any runtime initialization. For mutation, put it in a `Mutex` to make it safe.
That style guide has been entirely removed, check nightly. The reason it was was because its old, not updated, and not accurate.
When was the PR merged? I'm on mobile, I can't dig into this.
I specifically meant that it can make an example (ie. 3 lines in a book or post comment) easier to read. Anywhere else its a bad idea.
If your server's interface relies on the assumption that its clients are benevolent, then the server is broken.
[This PR](https://github.com/rust-lang/rust/pull/35401), merged on August 9th.
&gt; No, they go *before* it { fn add(x: i32, y: i32) -&gt; i32 x + y } I could get used to that.
Pre-RFC: Show an ASCII version of this as "compiling" indicator when compilation takes &gt; 10s.
I applaud your effort but excuse me if I use this opportunity to ask two questions: (1) Have you considered a true microkernel such as [sel4](https://sel4.systems/) instead of implementing a hybrid kernel. Maybe even re-implementing it in Rust if you do not want to use the original implementation? (2) What is your opinion of [Genode](https://genode.org/)? Thank you for your efforts.
What are we, Lisp?
The vim plugin has gotten much better in the fast few years. A lot more of the vim commands are implemented like complex replacement. 
&gt; Name your top level Error enum, Error. This just forces the importer to rename it on import. Which just makes code maintenance harder. Don't import `Error`, import just the crate root (or owning module). I never `use somecrate::Error`, I just `use somecrate` and then type `io::Error`, `fmt::Error`, `somecrate::Error` everywhere. It's not much longer than `IoError`, `FmtError` or whatever you would rename them too. Generally I don't really like importing things directly, and rather import particular modules. In Go it's the only way to do it actually (though I think it's stupid to enforce it). But consistently using `xcrate::AType` makes distinction between internal names and external ones obvious to the reader. You can always rename any module/crate import to something shorter too, if you find it too verbose.
Then I've been following the wrong style guide! :-( How do I find the right guide, and how can I in the future tell whether I'm reading a deprecated or up-to-date one? Should there perhaps be a warning label?
&gt; Use a type from Crate A, but don't expose Crate A's type as public from your Crate. BONUS ROUND! Fall behind Crate A's version, so a user has to read your crate's Cargo.toml so they can synchronize version numbers. A good reason to use `cargo doc --open` instead of the online docs.
I am definetifely going to port it now, because my current implementation does not have any future. Is is already too hard to add in new features without breaking the engine in some way. My reasoning behind using gfx instead of glium is, that its design fits my personal preferences. I am also very curious about the in work vulkan backend. The rewrite itself will not be that hard, because from the start I designed everything into reusable components (I actually made crates out of them to improve compile times). Just rewriting the rendering related code paths may be easy enough, because of my strict seperation of logic and displaying. Maybe I will even open source some of the more generally usable components(like my spartial partitioning lib, or the in game debugging and dev console).
Looks OK to me as well, though I must disagree about the line length and space indent parts. I think monospace fonts and column counting are relics from the days of simple typewriters and punch cards, and should be exorcised from modern computers.
Note that depending on your use case you *may* get away with using atomics instead of a Mutex.
/u/burntsushi is working on some of the text searching stack with https://github.com/rust-lang-nursery/regex and https://github.com/BurntSushi/aho-corasick. It's not at the lucene level of completeness though. 
I tried both simple arithmetic hashes and pseudorandom hashes (using the same hash functions in all languages for comparison) and the results are always the same. The only hash function that apparently changes the results is the "contiguous" one by Veedrac and that only works with this specific input graph because it always maps the vertex labels into contiguous integers. Even with `i+4000*j` or `i+(j&lt;&lt;12)` the Rust is still slower because although they map to unique integers those integers aren't contiguous. 
We are building an async IO stack at Tokio (https://github.com/tokio-rs/tokio) backed by Futures. Tokio has a goal of providing everything that is needed to quickly build async clients &amp; servers as well as being able to peel back the layers of abstraction when needed. In general, we found that callbacks are not a good abstraction as they require significant amounts of overhead. You can read more about the design process &amp; decisions here: http://aturon.github.io/blog/2016/09/07/futures-design/
I'm not well-versed in Rust at all. I mostly lurk, but these aren't unique problems, as others have stated. Same exists in Java, for instance. One possible middle road I've been exploring, while making a small class importer for PHP, is a bracket syntax, which I find trides the line of brevity vs clarity rather fine, so you could basically turn using models.pie.FlavorModel using models.pie.CrustModel into using models.pie.[FlavorModel, CrustModel] 
The closest thing I know of is [tantivy](https://github.com/fulmicoton/tantivy). tantivy uses the [`fst` crate](https://github.com/BurntSushi/fst), which is one of the key data structures used inside of Lucene.
http://stackoverflow.com/users/155423/shepmaster I actually really like that someone is making my answers look great! :) He/she should get a mention somewhere on TWiR, or something for doing really good job on SO. :)
Twice as many stars as FreeBSD. Quite impressive.
Rust supports multiple imports: use std::collections::HashSet; use std::collections::HashMap; can become: use std::collections::{HashSet, HashMap}; some libraries make use of glob imports: use std::collections::*; this is bad practice for the most part, in my opinion. However, some libraries have 'prelude' modules, which include a bunch of Traits that will not be seen in the code at all. For example Diesel uses a prelude to import a lot of Traits that would not be seen anywhere directly in the code: use diesel::prelude::*; And I am not sure if I like it or not, really. 
I am aware, look who I was responding to :-)
Sounds great, please keep us updated on /r/rust_gamedev!
It should be quadratic, it's just there are strong mitigating factors. The first one is mitigated by resizes, and so only goes a "little" quadratic. The second one, the attack, is actually not the most sophisticated attack I can do, but it's dead simple which I thought illustrated the point better than a more complex attack with a higher amplification. The mitigation here comes from that the first 225k elements go in smoothly and the second 225k elements only go quadratic with respect to the fraction of elements that they cause to bunch up. I believe this means it's quadratic in half of the previously inserted quarter, but scales *that* up linearly (so half that on average), so roughly `T(n / 16) ≈ T(14000)` per insertion. That makes the 2k slowdown seem reasonable to me. EDIT: math is wrong (I quartered twice), should be `T(60k)` with the numbers I gave. This leaves an unaccounted factor of ~30. This reduces again to an unaccounted factor of ~10 if you consider the once-per-insert overhead of hashing with SipHash, which is not duplicated in the pathological case. I assume that factor of 10 is mostly because of the cache; the O(n²) behaviour is at least cache friendly, unlike random insertion. Occupancy also accounts for ~15%. It is probably worth more investigation rather than some back-of-the-envelope guesses, but even if the growth is sub-quadratic a slightly more sophisticated attack certainly is. &gt; Also, is there a reason to choose 900,000 specifically 900,000 has high occupancy, which exacerbates the attack. The attacks here scale with `occupancy * 2 - 1`, so an occupancy near 50% (1M, for example) hides the behaviour. More sophisticated attacks work regardless of occupancy.
Pretty sure you meant `Z_m`.
&gt; Any reason --mode tile hasn't been implemented? Okay, it's implemented now
Of course, the challenge is in defining what "the expression" is. You can't use the innermost expression, which would be `expr?`, since that would effectively make it a noop. The current design goes the other way, and makes it the outermost expression, namely the function body, unless you explicitly bound it in a catch expression. Swift does something in between for its optional chaining operator. Some expressions composed from an optional chaining expression will occur within the optional context, namely postfix operations applied to the optional chain, but others will not. A function call expression cannot be composed into the optional context of one of its arguments, for example. The Rust approach avoids privileging a certain class of expressions, instead allowing any kind of expression to be composed with the success case of a `Result` value. Anyways, I'm not sure looking at `?` as an error chaining operator is the most useful perspective in the first place. It's more directly equivalent to explicit exception propagation, such as `try` in Swift, with the difference being that the result of an "exception-throwing" function is a first class value in Rust; you don't *have* to propagate the error since you can handle the result directly.
This is not really limited to example. Blindcard imports are (at least in my opinion) bad style, which relies on implicit information, and is harder to read. The exception being for things like reexporting and so on, but in most moduels, it should be avoided.
May I pet it?
It's harder than I thought: () () \ / ~/\/\~ &amp;_^^_&amp; \ / or () () \/\/\/ &amp;_^^_&amp; \ / or () () \/\/\/ &amp;_^^_&amp; \ / 
`termion::event::Key::Char('\n')` exactly
This is meant as constructive criticism: This example has way too many foos and bars. You have instance variables named the same as function parameters, named the same as function names. It's really confusing. I believe I've commented and renamed it with syntactic correctness, anyway. #[derive(Debug)] //Just an int struct Foo { f: i32, } #[derive(Debug)] //Contains two Foos struct Bar&lt;'a&gt; { variable1: &amp;'a Foo, variable2: &amp;'a Foo, } #[allow(unused_variables)] //Create a Bar from two Foo inputs fn make_bar&lt;'a&gt;(bfoo1: &amp;'a Foo, bfoo2: &amp;'a Foo) -&gt; Bar&lt;'a&gt; { Bar { variable1: bfoo1, variable2: bfoo2, } } //Make a Bar from a Foo passed in, and a static Foo within the function. //Output the first Foo stored in the Bar, which is the foo that got passed into this func. fn extract_bar2&lt;'a&gt;(efoo0: &amp;'a Foo) -&gt; &amp;'a Foo { let efoo1 = Foo { f: 22 }; let efoo2 = make_bar(&amp;efoo0, &amp;efoo1).variable1; efoo2 } fn main() { let mainfoo1 = Foo { f: 11 }; let mainfoo2 = extract_bar2(&amp;mainfoo1); //This SHOULD be mainfoo1, which has the int 11. println!("mainfoo2: {:?}", mainfoo2); } 
I'm working on a library for Rust, with the idea that proofs will be written as Rust programs. It does not use the type system, because Rust is not powerful enough (yet). It runs programs to check that theorems are correct (or not wrong). This makes it flexible since you can develop theorem provers by need. It also helps development to use the same language as APIs I want to reason about. Macros makes it easy to develop custom syntax. One idea for the future is to use Rust generics to interface between APIs and a theorem prover. Perhaps this can be used to automate tests?
There isn't an official one yet; we've just formed a team to work on one.
Given how we run systems today (i.e. One database on Linux, on a VM, on Linux) unikernels are really looking like the future. Why do we need context switches? Why not just let that database run in privileged mode? Why do we need a file system? Are inodes and file handles really the most effective API for hosting a KeyValue store on an SSD? Is something like swap and virtual memory helping, or hurting Redis which is trying to be both in memory and persistent? Commoditizing/Ergonomicising[0] the ability to build custom kernels would be extremely interesting. Also cool for testing: I can't find the link but I read somewhere that https://mirage.io/ shims the API it normally exposes such that users can run their applications as a Linux app, or without the shim on Xen (Ideally, on bare metal). [0] to make ergonomic - don't worry, its a new word. I promise!
I think you're looking for [r/playrust](http://reddit.com/r/playrust). ;-)
Hahaha. I haven't noticed. :D
&gt; VT-D could potentially allow for DMA to be controlled, leading to proofs of security and stability with userspace device drivers This also needs IOMMU support in the processor and motherboard, right?
Is having a reddit bot automatically flag such posts the step two? :D
That is correct.
I think a prelude has the same problem.
It wasn't broken, there was just one subtle little bug in a complex system. You could say the same about any vulnerability.
I was trying to avoid giving too many specific details, but it was patched years ago, so I guess it doesn't matter all that much. The system in question is a Java sandbox which accepts arbitrary classfiles, rewrites the bytecode to remove any dangerous functions, and then load and executes the sanitized classfiles. The "parser" is the JVM itself, and it was interpreting the classfiles as specified, which includes a 16bit length field for string data. The problem was that the santizer was blindly truncating the length instead of throwing an error when a string is longer than 65535 bytes.
Wonder how the bot would score this post...
It wouldn't. Currently it only works on self posts :s one day tho
&gt;do we need context switches why not let the database run in a privileged mode? Rust ideally makes the MMU pointless. It gives out of the box memory safety so context switches/privilege levels are pointless. But in practice, you want an MMU. One unsafe block, or really any C code is no longer just a DOS, information leak, or worse case a low priv user RCE. It is a kernel level RCE, and all root level certs/creds are gone. Also information leaks are no longer isolated to a single process. You can leak global layout information, kernel code, ssl certs, AWS keys. Most people can't figure out how to run their DB, and webserver as nonroot users let alone encrypt their passwords. Expecting people to write fully memory hardened code, even with Rust is a pipe dream. Nonblocking system calls are about on par with acquiring a mutex. If these are your bottleneck, buy a second computer. 
Also, it comes with a command line interface that makes it easy to try it out. https://github.com/fulmicoton/tantivy-cli
It would be a little harder since slices only work for `Size` types. [I'm currently working on a list type](https://github.com/mgattozzi/functils/blob/list/src/list.rs) to avoid the issues of efficiency brought up in thread to avoid O(n) or worse problems if possible. I could try working it out in the future for it.
I find it generally a bad idea (because it's easier for everyone when everything looks familiar), but I understand that it might be useful in many cases (branding, integration with other documentation etc.) Good job for documenting it!
&gt; Why do we need context switches? Why not just let that database run in privileged mode? If you want more than one process, context switches are inevitable. &gt; Why do we need a file system? Are inodes and file handles really the most effective API for hosting a KeyValue store on an SSD? Is something like swap and virtual memory helping, or hurting Redis which is trying to be both in memory and persistent? Well, the new kernel has no idea what file IO is, that's left to "superuserspace". &gt; Commoditizing/Ergonomicising[0] the ability to build custom kernels would be extremely interesting. Well, sure thing.
[The paper can be found here](https://dl.acm.org/citation.cfm?id=2967948).
Fortunately this time it was not, I could make it work by just reordering the calls. But what bugged me more that for some time I couldn't tell why the error happened until it dawned on me. The inner block trick doesn't – of course – work if you assign a thing with a lifetime to an outer variable – the lifetime is going to extend outside the block.
I fully agree with this. In my opinion many problems are incorrectly tackled with the OOP mold, when there are far better solutions. However, GUI really strikes me as perfect candidate for OOP. The approach with a baseClass 'Widget' which defines basic properties shared by all UI elements and sub-classes which define widget specific behaviour works really well in pratice, IMO. So while I think that Rust is an amazing language, there are other languages which I would prefer for the specific task of programming an UI system right now.
What are the results for gamedev-in-rust posts?
May I steal this?
I am myself a heavy Lucene / Solr user. Tantivy is heavily inspired by Lucene's architecture and vocabulary and you should not be too disoriented. I also welcome constructive criticism about the design or the API. Facetting, dismax, or a python bindings would be great additions. I am unsure the project is mature enough to add python bindings. I haven't thought yet how to handle the facet taxonomy. I know Lucene has a global taxonomy that is "append only" but I have no idea how it handles multithreaded indexing. I am interested in how you would implement it.
Well, ZFS is a complex monster, and we chose to make a temporary alternative (RedoxFS), and postpone our ZFS implementation. It is worth noting that RedoxFS is merely a temporary replacement, allowing us to focus on more important stuff.
are you actually building rustup? Or building something using rustup/cargo? if you're building rustup itself, i'm not sure what to suggest, except to try again? i was just able to successfully build rustup `70c573357d5612166094d6800ca29553ffc9eb48`
 12:42 PM (5 minutes ago) Hi all, I'd like to announce the release of my partial zbackup clone, rzbackup. This is open source software freely available under the Apache 2.0 license. https://github.com/wellbehavedsoftware/wbs-backup/tree/master/rzbackup https://gitlab.wellbehavedsoftware.com/well-behaved-software/wbs-backup/tree/master/rzbackup https://crates.io/crates/rzbackup This is written in Rust and designed to address some of my specific use cases where zbackup doesn't perform well. For example, it can operate in a client/server mode allowing you to restore multiple backups which share deduplicated content without repeatedly loading the indexes and decompressing the chunks. Its main features are: * Rust library for access to ZBackup repositories * Supports encrypted and non encrypted formats * RandomAccess implements Read and Seek to provide efficient random access * Client/server utilities to efficiently restore multiple backups, sharing chunk cache * Command line decrypt utility, mostly useful for debugging Thanks to Konstantin Isakov for relicensing the protobuf definitions so I could include them with the Apache licence, and of course to him and all the other contributors for the prior work this project builds on. James 
How do I close `STDIN,` `STDOUT`, and `STDERR`? Attempting to write a unix daemon in Rust, and part of the *procedure* to launching a daemon is closing these, or generally it is considered good practice. I know `panic` will attempt to acquire these resources, but I'm hoping via defensive coding I can prevent panics, other then runtime ones like out of range, and null pointers. When those happen, I want my process to explode anyways.
According to crates.io, the upstream `ring` repo is https://github.com/briansmith/ring. Maybe you have a local override in your Cargo config file.
Competition is good. This way, I can keep stealing ideas from your crate and re-implementing them my way :).
It brings up pleanty of very cool insights on concurrency and fast parallelism, a subject that is a core part of Rust.
Hahahaha perfect! After this post I realized that doing uncons with iterators was inefficient so I've started to just implement a list data structure to make it easier.
The features for our classifier should be pretty tuned to the two subreddits. We did our own analysis to find common words that would in most cases be somewhat unique to the two. So something like GPU is not a good feature word because we see it in both . That should keep false positives down. That said we still need to do model fine tuning and further investigation into potential false positives.
This week I was working on a [linear programming modeller](https://github.com/jcavat/rust-lp-modeler). From an objective function and a set of constraints, the tool will produce the generic mathematical model to send to any linear solver (glpk, lp_solve, coin-or cbc, gurobi, ...). 
Not yet, but I'll make sure to put some in later!
Oh man this will make implementing my functional programming crate easier! This is awesome! :D
I believe this is possible using https://autumnai.github.io/leaf/enum_primitive/index.html
Coroutines can be used with kqueue: http://www.eecs.berkeley.edu/~sangjin/2012/12/21/epoll-vs-kqueue.html It's better than readiness-based futures. EDIT: Sorry, I lost the "patience" or something like that and end up writing the above comment. I don't know how THIS future compare to coroutines. I'd like to know, but it seems the devs of this future don't care about compare them. Therefore, I remain unconvinced.
There have been demonstrations running Piston in the browser (using SDL2 backend), but currently this requires a bit of work (emitting LLVM IR and fixing stuff). When Rust gets web assembly support this will be easier. I recommend using JS.
&gt;That said, I'm not sure actually closing those handles is the correct thing to do Sources seem to disagree. Some say redirect to `/dev/null` others say close. I guess I don't have to close. **Problem solved!** &gt; I think the `daemon()` function on Linux redirects them all to /dev/null `daemon(2)` isn't great on Linux. You are technically also launching a shell process for your process to run in, and be controlled by. Also `daemon(2)` in glibc isn't the same as `daemon(7)` in systemd. * `daemon(2)` when you get `SIGSTOP` or `SIGHUP` these are just regular old terminal signals. Because you are in fact still running in a terminal. And you have to write handlers to ignore these signals. * `daemon(7)` actually over rides the standard definition of `SIGHUP` and `SIGSTOP` to mean things specific to Systemd. &gt;Also, at a high level, I think its more common these days to rely on other tools like systemd to daemonize you To be perfectly honest I'd rather not daemonize via Systemd. You break compatibly with all other init systems. Also doing trivial things with local files/sockets requires using Systemd code in place of kernel calls/glibc. If you use double fork, you can still work in a Systemd environment. But no longer need to code explicitly for Systemd. 
Definitely! :)
&gt; Nearly 100x faster than popular frameworks using Ruby and PHP (Almost) every statically compiled language will be faster then interpreted one, without even trying hard. However benchmarking speed of sending "hello world" is only useful if you make a living of sending "hello world" (I don't), pointless otherwise. Its how fast and readable it will be when I'll 50 dependencies and 10 layers of abstactions that matters, not IO. &gt; Works With macOS, Ubuntu, Heroku, Digital Ocean, AWS, Docker, MySQL, SQLite, PostgreSQL, MongoDB, Redis And More... So do at least 100 other languages, all of them better tested, documented and polished. Swift might be a good language and vapor might be a good framework - I don't have an opinion. But it doesn't have a single thing other languages don't have, and it certainly misses many.
At first sight it's easier to use. So...
I think a better question is 'Will Swift take web app market share from Go' as the two languages are much more alike than either is to Rust. I foresee Rust being a first choice language for backend stuff, the job that Java (and now Scala and some Go) does in industry. Swift and Go (along with Python and Ruby/Rails, NodeJS) will probably remain the most popular for the all-in-one web frameworks, because if you're not separating your front and back end, you probably aren't looking for that extra 5% of performance you get from avoiding GC pauses. 
The advantages Swift has are 1. It's got a very large developer-base, almost immediately, as Objective-C developers move over 2. It's very employable, since Swift opens up iOS development 3. Consequently its users have a very firm eye on commercialising Swift, evident from the more sophisticated design of the [Vapor website](http://vapor.codes) compared to [Iron's](http://ironframework.io), which has a much more tech feel and _no link to a tutorial_. 4. Its future is secured, Apple are guaranteed to be around for at least another 15 years, and are guaranteed to support Swift for that time. 5. Its memory model has a very easy learning curve initially, though it gets much more complex as you proceed, evidenced by the need for [[unowned self]](http://stackoverflow.com/questions/24320347/shall-we-always-use-unowned-self-inside-closure-in-swift) annotations. The advantages Rust has 1. It's a smaller language: Rust didn't need to have an OOP compatibility layer 2. Memory aside, it makes many things slightly easier: for example trait-objects are [something conspicuously lacking in Swift](http://inessential.com/2015/08/05/swift_diary_9_where_im_stuck), and also the implementation of custom generators is a rather complex in Swift. 3. Its memory model makes leaks less likely: I suspect in the long run leaking memory through closures will become a Swift bugbear. 4. It has a slightly better story for parallelism: Swift has a bridging layer to grand central dispatch, which is just like Java's `ExecutorService` and little else. Rust has compile-time data-race detection, channels, and [parallel iterators](https://github.com/nikomatsakis/rayon) 5. It will beat Swift to async. Swift 4 is probably two years out. In a year, it's likely the Futures/Tokio/Hyper work will be released. Given the success of Go and Java/Undertow, I would expect Rust web-apps to outperform Swift ones at this point. 6. Ironically, given recent discussions, Rust has a much better collections API. Swift still leans heavily on the old Cocoa collections, which were all mostly mutable. Rust's mutability story is much cleaner and easy to follow, and so its collections have fewer gotchas. 7. The support for macros, particularly recent work to stabilise the diesel etc. macros, reduces the amount of code one needs to write in Rust vs Swift. In the large, it must be said both are a solid advance over where we were. While modern Java doesn't deserve the opprobrium it receives (JMX and remote debugging are _significant_ advantages), developing in either Swift or Rust seems much nicer to me. However Rust does and will face strong competition Swift. Notwithstanding the async work, Iron (or Nickel) needs to be as easy as Vapor to develop in, and needs lots of documentation. It's a bit of a tangent, but I also think the discoverability of libraries needs work: I'm not sure the average Rust starter would know about Rayon for example; indeed I fear they'd just hack something together with mutexes. Hence I'd argue _for_ a Rust batteries-included platform, even if the advice was that developers should grow out of it.
It's worth noting that at this precise moment in time: * Swift has no concurrency model. Accessing class members across fields is just a straight-up data race. You can maybe do something with pure value-only message passing? Addressing this is literally not on the docket for the year (*maybe* next year). * Swift isn't portable yet. Its "tier-1" platforms are Apple's platforms. Using Swift on Linux involves discarding all of Foundation, which includes things like, you know, Files and Sockets. Windows isn't even officially supported. * Swift is barely stable. Swift 3 (released like 3 days ago?) promises source stability but not in the sense that Rust 1.0 did. The Swift 4 compiler will understand Swift 3 and be able to bridge things into Swift 4, but Swift 4 will likely have significant differences as basic features like "conditional conformance" and "protocols that can refer to themselves" are introduced. Taking advantage of these things may involve rewritting tons of stuff. If Swift overtakes Rust here any time soon, *Rust really fucked up*.
The vast majority of the time is spent waiting for external resources anyway, even if the code executes infinitely fast it's not a huge win.
Read that as "Will Smith" Now I'm a bit disappointed.
I wish the rust people had more of an emphasis on ergonomics. Python also has an 'explicit is best' approach but being a dynamic language it wasn't noisy to begin with. Given that rust is not a dynamic language it's relatively noisy anyway -- couple that with 'explicit is good' attitude it just becomes one of the noisiest languages out there. I fear that very ergonomic languages like swift are just going to drink rust's milkshake, and push rust back into a very small corner of the computing world... :( Can you tell your rust friends to lose their obsession with 'explictness' ? I beg u
[Great timing!](https://www.reddit.com/r/rust/comments/52vb6y/animated_ferris_the_rustacean/d7o1z59)
&gt; trait-objects are a conspicuous thing lacking in Swift Swift has trait objects, they call them "protocols as types" or, internally, "existentials". The problem that is being described in your link exists exactly in Rust. Swift also provides a bandaid for this problem: the AnyHashable type which contains an existential and provides a sensible == implementation when the types don't match. Currently Swift's existentials are a bit limited because there's no syntax to specify associated types -- Rust's `Iterator&lt;Item=u8&gt;`, but this is a bug and not a design decision. There's also an absence of "impl Trait for Trait"; also probably a bug (but more complicated for Reasons). Meanwhile Swift existentials are more powerful because polymorphic compilation and the ability for the language to box when it needs to enables the following two things to be considered object-safe: * `func foo() -&gt; Self` * `func bar&lt;T&gt;(x: T)`
Ah, that makes sense. (And I'm sure you meant Key::Char('\n').) I guess I expected there to be a different member of the Key enum since there are Shift and arrow and function keys, so why not Enter?
Cool, seems like a good strategy.
I came up with this: _~^~^~_ \) / o o \ (/ '_ ¬ _' \ '-----' / 
Why use Swift for cross platform development? I thought it was a platform language.
This is awesome! How about adding support for Linux fbdev, like [fbv](https://github.com/godspeed1989/fbv)?
Look at the bottom of [this page](http://rustconf.com/) ;)
Building rustup through the ArchLinux PKGBUILD from the AUR. you can look at the script if you want. It tries to update a git repo that doesn't exist
I don't have a local override there because I never touched my cargo config file. But it might be some other reason...
All five of them? Fine, I'll upgrade my Pi to a 3, then.
See, the thing is: I knew things like that have been done before (despite not knowing the name at the time) and wanted to do something different, something that'd make my "product" not be "the next *X*", that's why the tagline is "[...], sort of" and "as best as it can" in various posts ([twatter](https://twitter.com/nabijaczleweli/status/776344057341771776), this roddit thread). That, and I don't have access to a Linux machine I could compile it against/test it on. Feel free to PR it in, though, that can just be enableable via a switch. X-posted to [#8](https://github.com/nabijaczleweli/termimage/issues/8)
Rust is awesome and has a real place in the world. But honestly neither Rust nor Swift are good web development languages. Coming from Erlang, anything besides Elixir is quite terrible.
It is, they just focus on macOS atm because it's a new language, subject to major design changes
I think there's something like this that could be done with `Into` but there might be overlap issues. The shortest way I can think to do this with the standard library is: * For the Result conversion, `.map_err(Into::into)` * For the Option conversion, `.ok_or(())`
Eventually something like Rust with GC will win. Or go with sum types and generics. Not sure if swift I'd that, I haven't looked too closely since it's an apple thing. I feel like f# couldbe top dog for web dev if it weren't a Microsoft thing.
It's not about marketing, it's about being productive with a tool, if it's easy and powerful people will use it over the complex one I tried both, swift is far far far easier to learn than rust
&gt;Swift still leans heavily on the old Cocoa collections This isn't true, Swift is independent of `Foundation` collections such as `NSArray`, `NSSet`, ect. It is considered standard and good practice to always use the native structures (`Array`, `Set`, ect.).
I'm playing with GTK-rs. Experimenting with a front end for my reverse geocoder in it.
I work at a very cloud-heavy company and while latency is a large source of performance hits it is by *no means* the case that improvements to processing of the data would not have impact. I doubt that the majority of software is truly so bottlenecked on IO so as to not benefit from substantial processing speed improvements - especially when we start talking about the trivial ability to parallelize provided by rust.
It's standard practice to have a struct that's the same name as the module itself.
for mapping one error type to another I use quick-error and that map_err mechanism which is quite succinct. normally though I find I need to specify a more specific result type than just Into to make the typechecker happy.
Yes, but you need certain scale for this to matter, and when you'll get there you will have a lot more things to consider then the ones we discuss here.
I personally choose to keep the struct in the module but provide a reexport (`pub use widget::Widget`) at the level above it. This gives you the benefit of non-repetitive naming as you describe with the added advantage of all those types being declared/implemented in the same module.
Got it. For the benefit of other readers, the following is the relevant section, as far as I understand: &gt; This RFC aims to reconcile two conflicting points of view. &gt; First: hyphens in crate names are awkward to use, and inconsistent with the rest of the language. Anyone who uses such a crate must rename it on import: &gt; ```rust extern crate "rustc-serialize" as rustc_serialize; ``` &gt; An earlier version of this RFC aimed to solve this issue by removing hyphens entirely. &gt; However, there is a large amount of precedent for keeping `-` in package names. Systems as varied as GitHub, npm, RubyGems and Debian all have an established convention of using hyphens. Disallowing them would go against this precedent, causing friction with the wider community. &gt; Fortunately, Cargo presents us with a solution. It already separates the concepts of *package name* (used by Cargo and crates.io) and *crate name* (used by rustc and `extern crate`). We can disallow hyphens in the crate name only, while still accepting them in the outer package. This solves the usability problem, while keeping with the broader convention.
Isn't Swift also llvm based too?
My take: the gist of webdev is "move fast and break things". Rust probably sacrifices too much development speed to correctness for web. Plus, good languages don't automatically make it big in web. I don't see Rust's sales pitch for web developers, compared to (for example) Node.js + TypeScript, or C#. I imagine that Rust will grow into a successful systems language, and will directly compete with C++. Web's a different story.
Thank you for idea! I knew about `map_err` but I didn't realize I could write `.map_err(Into::into)` instead of `.map_err(|e| e.into())`. In case of Option, I view it more as a natural consequence of having `AdaptErr` rather than must-have.
Yes, but that doesn't affect how the languages work in any important way. The biggest similarity they have is that both have RunTime Type Information (RTTI), so they can up/downcast between interface references and concrete types (safely, with runtime checks). Rust cannot and does not want to be able to do that: RunTime Type Information means RunTime Type Errors. Whether they're actually "closer to each other than to Rust" is debatable, though. Swift and Rust both use OS threads, while Go uses M:N threading. Swift and Rust both use reference counting as their main technique for automated garbage collection, while Go uses a precise, compacting GC. These two differences are *massive*: they're the reason Go's FFI is so slow, and they firmly place Go in the high-throughput-high-latency camp. Swift, in contrast, is designed for low-throughput-low-latency systems (user interfaces). Rust, of course, pays the complexity cost that's needed to achieve both.
&gt; Coming from Erlang, anything besides Elixir is quite terrible Can you expand upon this? Curious to know if you have experience between Rust / Elixir / Swift in a webdev environment.
Thank you can confirm it works. Tiling a small image is faster than scaling the big wallpaper. I suppose that proves image decoding is the bottleneck discussed earlier.
I wish the rust people had more of an emphasis on ergonomics. Python also has an 'explicit is best' approach but being a dynamic language it wasn't noisy to begin with. Given that rust is not a dynamic language it's relatively noisy anyway -- couple that with 'explicit is good' attitude it just becomes one of the noisiest languages out there. I fear that very ergonomic languages like swift are just going to drink rust's milkshake, and push rust back into a very small corner of the computing world... :( 
Ergonomics are a major focus of the upcoming year.
Here's an animated version: https://jsfiddle.net/Diggsey/3pdgh52r/embedded/result/
I just realized I misread - the weird colors were in tmux and not xterm. Its likely what's happening is that tmux implements the feature the same way as xterm, though, or is even doing a lossy conversion of color to the 256 color sequences because it doesn't know if your terminal can handle true color. The tl;dr is that terminals are a hot mess.
And more specifically: what do you as lacking in Rust, what could be improved?
IMHO, a language is as good for making websites as their libraries that enable you to do so. Getting something up an running with ruby (rails), python (django), C# (asp.net), Java and so on takes half an hour at worst til you see some hello world and got started on your own code. Disclaimer: My day job is web-development with ASP.NET. I toyed with Rails and some java stuff. Did small PHP projects as a teenager. Other importants factors, at least for me would be: * How can I interact with the UI from my [rust/swift/python/java/go] code? * Do I need to recompile every time I made a change in the frontend (say I'm using the templating engine of that framework. Like JSP or Razor)? * How developed is the overall tooling (including what web-servers I can use). I'm not going to debug my web-application with gdb and valgrind if I can have IntelliJ and other fancy debuggers / IDEs. * How easy are interactions with other parts of my ecosystem? I want to talk to databases, other webservers, maybe a dataware-house system, a CRM, ... Edit: For some types of websites, you can even mix- and match between languages and frameworks. An angular single page application will just grab data from any (REST-)Api and push it back there. You can use what ever language suits the needs for that API and you can provide that api with very little framework.
That's interesting. Coming from C++ I find Rust positively *lightweight*! More specifically, though, I think there's sometimes to much emphasis on terseness. Programming (professionally) is not about competing in a code golf, having more information upfront can *improve* understanding, rather than the contrary. Just configuring syntax highlighting to be able to zoom in easily on the parts of interest is really useful, though.
In that case you need to contact the package maintainers. Rustup is only supported through curl right now via the command on the website. Have you tried it that way to see if it works?
Rust is heading firmly in server space with tokio; it may not mean "web development" as in "outputing html" but it can definitely mean processing REST/JSON.
&gt; Singularity, which is a research OS from MSR does this with a type safe language (C#) and seems like a very convincing model (interestingly they had to add ownership to the language to support their IPC mechanism, so Singularity in Rust may be even better). Yeah, reading the Singularity papers with Rust as a background it seemed clear that there's a connection between Rust's memory model and the way that they eliminated process isolation. Watching the talk at the SF meet up it seemed like Tock could be adapted for that approach. Possibly you could add dynamic memory allocation _and_ a preemptive scheduler and then just have the entire system as threads running in a single address space.
&gt; it's about being productive with a tool &gt; swift is far far far easier to learn than rust You're not saying one implies the other, right? Javascript is also very easy to learn. I'm able to build things in Rust that I just wouldn't dream of ever completing in JS.
Simply awesome. Now we only need some syntatic sugar. :)
Awesome. I really like the idea, and I wish I could use a library like this whenever I need one. I glanced at the code. Some comments: * can you break the library into separate create? and add examples how to use it. * `Outcome` is very non-idiomatic. IMO it should be just `Result&lt;(), Error&gt;`, where `Error` is and `enum`. No i musisz zmienić nazwę użytkownika na "nabijaczkrat". Pozdrawiam!
Re: Java &gt; (JMX and remote debugging are _significant_ advantages) As much as I love other languages, I've been employed as a Java developer for enterprise java apps for the last decade. This is completely accurate. You can make a good point about how Rust, Go, or Swift allows for smaller, better Web applications that don't require stupidly advanced architecture. But if you have one of those, remote debugging is amazing. If rust/swift/go becomes common I don't see how someone won't build a ridiculously complicated system that's difficult to run or replicate locally. Being able to remotely debug a complicated production-similar server directly from your IDE is a huge productivity boost.
Oh absolutely, any combination of the items you listed makes Rust the better language for me, personally. That's why I'm hanging out in this sub reddit and following Rust's development. I'll admit to generalising the GC issue, that was incorrect, but that doesn't get away from the fact that Rust's focus on correctness conflicts a little with the 'move fast and break everything' development attitude that tends to make sense for web dev. I don't honestly think Go is a great language, it makes too many sacrifices in the pursuit of simplicity. 
&gt; Coming from C++ I find Rust positively lightweight! heh, compared to **C++**. Unfortunately Rust also has to compete with Go, Swift, D, etc. You could argue that Rust doesn't compete with Swift and Go, but that leaves an incredibly small piece of the programming pie left that would be Rust's arena. If Rust was just moderately less verbose I would love to use it in places where i'd otherwise use Go or Swift... 
Is this something that could be implemented as a library? Or would it have to be built into the language?
&gt; image decoding is the bottleneck I think it's scaling that's the bottleneck, not decoding
Some random stats that I found interesting but are, uh, not really related to this project per se: By my count, this is the 8th open source project on Google's Github page that Github has classified as a "Rust" project based on programming language. That's just under 1% of all the projects on their public Github. 3 out of the 8 are related to the xi-editor project. The first couple of projects date back to April, 2015 which was just before the 1.0 release in May. Kind of cool to see this interest in Rust at Google. Sorry OP for being off topic. Rustcxx looks really exciting and hope I don't derail any conversation.
Piston should be more than capable and easy to use for drawing some colored squares 
Added a [prose explanation](https://rawgit.com/nabijaczleweli/termimage/doc/termimage/index.html#prose-explanation) and an [example](https://rawgit.com/nabijaczleweli/termimage/doc/termimage/index.html#example) (links without `cdn.` because it might take a while to hit it).
Thank you for sharing this! I agree with you completely on the space in which Rust can make a big impact for ML. Being able to spin up research POCs and scale them without rewriting/overhauling the code base would be really exciting! For me, data parsing is an area that we haven't really got a good answer to yet. It seems like a steep ask to expect a user to hand-roll their own structs so that they can handle varying data types per feature - especially if they have hundreds of them. Handling input data is something that numpy, pandas, R and others make impressively easy but it is still a huge headache in Rust (for me at least). I am confident that [winners in this area](https://github.com/BurntSushi/rust-csv) will arise soon enough though. _If anyone has been using any cool tools for this I'd love to learn more!_ I'm not sure that I see the fragmentation as such a big issue either. I think it's the result of many people being excited to try Rust for machine learning. Though as somebody who's added yet another matrix library to the mix I agree that it would be nice if we put all our effort into one. For now it seems that our goals differ* and time is limited enough that progress is slow on that front. \* Sparse matrices, pure-rust linalg routines, full support for a BLAS/LAPACK interface, etc. There's a lot to work on! 
And what benefit, if any, do you think creating RustScript would have over just using other embeddable scripting languages like lua or mruby?
If Google is going to start using rust a bunch, maybe they can help out http2 support. :-)
Should anyone still be curious, there is actually [an RFC](https://github.com/rust-lang/rfcs/pull/1598) on exactly this.
I'm pretty sure this is a really dumb question.. Is there a performance penalty for using wrapper types? Would it affect the speed of computations? eg: struct Foo(i32); impl Add for Foo { .. } // impl other traits fn add_it() { let res = foo1 + foo2; } versus: fn add_it() { let res = int1 + int2; }
I've always wondered why people tout Go's static typing so much and then they go and do runtime duck typing. 
Ideally, the parser would be identical. It would just be AST transforms which add some sugar. Sugar which would have been deemed unacceptable for Rust, due to performance concerns. Also, RustStript structs would just be Rust structs, function(and method) calls are just function calls. Unless mruby is different, Ruby classes are a whole thing. You'd need to handle all the message passing, and monkey patching. Now you claim to handle Ruby, if someone ports their code over, would you need to include a GIL to run correctly? Say Ruby is 1000% more ergonomic than Rust, but ends up with 10,000% performance cost. RustScript would aim for 100% more ergonomic than Rust but end up with 10% performance cost. I don't know anything about Lua, I can't comment. Something like that.
Good point. Unlike the C -&gt; C++ transition the C++ -&gt; Rust transition does not have a clear cut path. 
`impl Trait`, `?` syntax, and non-lexical lifetimes look like some pretty big wins on this front. I'm hoping that some additional enhancements for more lifetime elision wil fill in the remain gaps. Rust's syntax is largely pretty nice. Though I think it needs to do something more about rightward drift. I think filling out pattern matching syntax by stabilizing things like slice and box patterns, allowing multiple conditions in `if let` etc, could really help here.
The other, more rusty in my opinion, way is to give library writers more dsl tools. Rust's macros are awesome, and powerful but could be a lot better.
I disagree that "unikernels are looking like the future". First, I'd like to contest your assumptions - that we run "one database on Linux, on a VM, on Linux". 1. You may run "one database", but which database varies - MySQL, Postgres, MongoDB, etc. It gets more complex with things like Cassandra, which is multi-machine. 2. A VM is in no way an essential part of the story, and it's actually becoming far _less_ common in the face of containerization, which reuses the same process scheduling systems a full OS already has After that, I'd like to challenge whether unikernels are actually even good in that sense: 1. VMs impose relatively suboptimal static resource allocation, increasing cost relative to processes 2. Scheduling VMs fundamentally requires switching more state than processes, full stop. No way around this: Virtualization must switch privileged state and unprivileged state (CPU-wise); processes only need to switch the latter. Inherent performance loss. 3. There have been a large number of containment failures based on [hardware-level bugs](http://danluu.com/cpu-bugs/) in virtualization extensions; by comparison, the "rings" system has stood the test of time admirably. 4. A well-designed kernel API/ABI provides isolation that is as good as or better than virtualization, full stop. See seL4. Moreover, I think that "the ability to build kernels" doesn't need commoditizing, because the kernel shouldn't be competing on features. Instead, the kernel should provide "the perfect primitives", allowing efficient implementations of absolutely anything on top of it. Again, see seL4.
This thread is about web development, and many developers expect that their web server will run Linux.
It would probably be polite to delete this post so it doesn't clog this subreddit.
The technologies are so radically different and this problem area fits into Erlang's model so neatly that you simply can't get close to Erlang's capabilities with Rust. Preemptive scheduling - A server receives a request. It is automatically in its own green thread (actor). You can run an endless loop in it if you wish, all other requests are still going to get processed. There is nothing that the programmer has to do to have concurrency. There are no futures or threads that the programmer has to worry about. It's all under the hood. Simple functional language - The amount of code to get something done is tiny. When you are writing code there is so fewer things you have to worry about. It's simply a breeze to solve problems in it. Programmer productivity in it is crazy. And what you get out of it is neat and short (depending on your skills of course). It is easy to read and understand. Of course what it sacrifices for that is in execution speed. An efficient Rust app is going to blow an Erlang solution out of the water. But it's going to be harder to understand and take way more effort to write.
That would be tricky - you'd have to figure out how to get it to integrate with the unreal build system, which is harder than it sounds.
The only definitive answer to that can be 'measure!', but LLVM should *usually* produce the same code with or without a wrapper.
The fact that it's opt-in means it won't guarantee e.g. data-race detection pervasively. Looking at the docs, essentially they want uniq_ptr &amp; borrow support as a special case when a heap-allocated variable keeps crossing a RC boundary (e.g. a loop calling a function). So their target is as a performance optimisation only. It's worth noting that between classes vs structs, weak or unowned or unique modifiers for variables &amp; annotations for closures, Swift's memory model does get quite complicated as you develop larger apps, but without the same payoff that comes from Rust's ostensibly complicated memory model 
so, how'd the classifier do?
RC is only deterministic if alloc and free are, and in most common allocators they aren't.
I am currently working on a library called perlin. Currently it is pre 0.1 and will only support boolean retrieval in 0.1 but the goal is to implement a fully featured information retrieval library. Have a look at https://github.com/JDemler/perlin if you are interrested. I am hoping to get 0.1 out of the door by the end of next week (it is feature complete but documentation and infrastructure need some more work)
Thank you!
Can confirm. Tiling large image is much slower.
&gt; That changes nothing about its pathological behaviour. Note that this "pathological" hash function works just fine in every other language I've tried. &gt; I wouldn't be surprised if using this hash function actually slowed F# down a little bit, because F# was probably benefiting from the overly uniform nature of its hash and multiplication isn't free. I'll check. &gt; You don't need a purely functional set - you're not even sharing data between the sets so why would you bother? I removed sharing from this benchmark when I realised it would be prohibitively difficult to do in Rust. &gt; i.e. you wrote unidiomatic code in a way that makes no sense in Rust and blamed the language. I wrote both versions in all languages and noted that Rust is the only language struggling with sharing and recursion. &gt; Just write a loop. Why would you ever write a counting loop as anything but a loop in Rust? Simplicity and clarity. A recursive solution is shorter and simpler. A loop requires unnecessary mutation. &gt; You didn't actually need them, though, so your want seems to be entirely personal wishy-washy feelings. You're assuming this Rust program solves the original problem I had. It doesn't. The original problem (just a few lines of code in many languages) is prohibitively difficult to solve in Rust. So I changed the problem to the one you see here in order to investigate Rust further. And I discovered that Rust isn't very good at this either. &gt; Rust's aim is to give you lots of tools and abstractions to write fast code. If you don't intend to use them, Rust's speed improvements will be limited. There's nothing surprising about that. Fascinating. Exactly which tools and abstractions should be used in this case in order to write fast Rust code, for example? &gt; &gt; Recursion doesn't interact well with Rust's scope-based memory management and leads to memory leaks. &gt; &gt; Aka. you want a purely functional language. Just use one. What is the relevance of purely functional languages here? None of the languages have been purely functional (yet Rust is the only one struggling). &gt; Yes, because you keep trying to write functional code in Rust, despite the fact that Rust is an imperative language. Idiomatic Rust is clearer, shorter, faster and for Rust's target audience more obvious than how you wrote it. That's why I call it unrealistic: the code you wrote would never be written by someone in Rust's target audience. We've just seen that idiomatic Rust is longer, more obfuscated and slower. &gt; If you just intend to heap allocate hundreds of thousands of objects, I wasn't. &gt; there's no point using a low level language. None of these languages are low level. &gt; Of course GC allocation and deallocation is faster than the equivalent malloc-dealloc code. None of these programs spend a significant amount of time in `malloc` or `free`. &gt; That's not the point, and that's never been the point. Agreed. &gt; The point (at least when it comes to memory management) is that these languages offer tools to control when allocations happen, and avoid most allocations altogether. That is true of all of these languages. &gt; There's a reason AAA games and real time software uses C++ (and similar languages) to avoid pauses. Is it really more likely that everyone else is insane, or that you've misunderstood something? Ad hominem. 
&gt; Swift has no concurrency model. Accessing class members across fields is just a straight-up data race. Not to say the point isn't relevant, but C and C++ were used to write code using shared-memory concurrency without a standardized memory model for about 25 years.
Hi, I've always struggled with Functional Programming, so forgive me if this is a stupid question. But why is this below example any different from what you want do with rust's standard functions such as map etc? extern crate kinder; use kinder::lift::{SemiGroup, Functor}; fn fmap_vec(vec: &amp;Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec.fmap(|x| x * x) //square all elements in a vec&lt;i32&gt; } fn append_vec(vec: &amp;Vec&lt;i32&gt;, two: &amp;Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec.add(two) } fn main() { let vec = vec![1, 2, 3]; let two = vec![4, 5, 6]; println!("{:?}", fmap_vec(&amp;vec)); println!("{:?}", append_vec(&amp;vec, &amp;two)); }
But, could this somehow be used to write proper Rust bindings, that is, a library where the user don't need to write any C++? The answer is "no"?
I've rerun the benchmarks using your latest hash function. F# is 5% faster. Rust is massively faster. Varying `n` from 1000 to 8000 the Rust is 1.6-2.6x faster than F# in every case. For `n&gt;1000` the Rust crashes with a stack overflow. Furthermore, this is the first hash function I've seen that does not appear to be specifically designed for this input graph and it makes Rust fast. Yay! &gt; &gt; Note that this "pathological" hash function works just fine in every other language I've tried. &gt; &gt; By which you mean F# and Ocaml, both of which optimize their hash maps for extremely weak hashers. .NET's `HashSet` is optimised for "extremely weak hashers"? Just to be clear: you are saying that the perfect hash function I was using (`i+4000*j`) is "extremely weak"? So you mean weak in the crypto sense? &gt; &gt; I removed sharing from this benchmark when I realised it would be prohibitively difficult to do in Rust. &gt; &gt; You said your "original program" was the one you gave in this post. The original program is from my PhD 16 years ago. I alluded to that when I wrote "My original program used the supercell from a molecular dynamics simulation." in one of my posts. &gt; &gt; I wrote both versions in all languages &gt; &gt; By which you mean F# and Ocaml, two languages geared predominantly around writing purely functional code. Is .NET "geared predominantly around writing purely functional code"? &gt; &gt; A recursive solution is shorter and simpler &gt; &gt; No it isn't. The whole algorithm can be expressed something like this: let nth nth nn n u = if n=0 then set[] elif n=1 then nn u else Set.unionMany(Seq.map nn (nth nn (n-1) u)) - nth nn (n-1) u - nth nn (n-2) u An important aspect of this is that memoization lets you share the `n`th neighbors of different vertices when you are repeating the computation for different starting vertices. &gt; &gt; A loop requires unnecessary mutation. &gt; &gt; Rust is an imperative language. If mutation makes a solution clearer, use it. Mutation doesn't make the solution clearer. &gt; &gt; Rust cannot even express the original benchmark. &gt; &gt; I just showed how you can directly translate the original benchmark into Rust code. You've only seen the version of the benchmark that I simplified to make it tractable in Rust. &gt; &gt; None of the other languages are purely functional yet Rust is the only one struggling. &gt; &gt; You're being pedantic, it doesn't help. I'm trying to drive home the important distinction between purely functional data structures and purely functional languages. C# has the former but is nowhere near the latter, for example. &gt; &gt; We've just seen that idiomatic Rust is longer, more obfuscated and slower. &gt; &gt; Now you're just ignoring what I wrote. You mean: let s0: Set = s1.iter().cloned().flat_map(nn).collect(); &amp;(&amp;s0 - &amp;s2) - &amp;s1; Isn't that building a larger set and then using set difference to remove elements? Is that idiomatic in Rust? &gt; &gt; I wasn't. &gt; &gt; I don't believe you. Ok. &gt; The pause times have to come from somewhere. Stack allocation and deallocation doesn't take that long. Avalanching destructors do. &gt; &gt; &gt; The point (at least when it comes to memory management) is that these languages offer tools to control when allocations happen, and avoid most allocations altogether. &gt; &gt; &gt; That is true of all of these languages. &gt; &gt; Not nearly to the same degree. Rust's ability to pass around mutable pointers to stack data as a first class citizen is vital to making stack allocation and proper memory management workable. That is certainly a nice feature. 
&gt; An important aspect of this is that memoization lets you share the nth neighbors of different vertices when you are repeating the computation for different starting vertices. I don't understand this part. Don't you call nth only on the same vertex u? You could even write it like this: let nth nth nn k u = let iter n = if n=0 then set[] elif n=1 then nn u else Set.unionMany(Seq.map nn (iter (n-1))) - iter (n-1) - iter (n-2) in iter k Though it's an interesting problem how to compute the nth n of all vertices in a graph, and maybe a better algorithm would share computation between different vertices.
Yes, but in terms of syntax, Swift and Rust look way more alike than Swift and Go.
Every time a Googler creates a rust-related repo on GitHub in their free time and declares it's not a Google project in the readme as the Google policies require, people here start analyzing "Google's” interest in rust. Once and for all: this is a _Googler_ using rust (in their free time most likely), *not* Google using rust. Google employs about 15k devs; some of them will play around with rust, yes. Has nothing to do with Google. It being under the Google GitHub org says nothing more than Google owns the copyright for the code because that's the easiest way to open source code while employed at Google.
&gt; Eventually something like Rust with GC will win. Rust is an excellent language on its own, but the ownership system is kind of a big deal :-)
And ask those devs how they feel about concurrent code in C/C++ :) 
Well, people sent people to the moon with the whole guidance software written in assembler. That doesn't mean you should not iterate on that.
Parts of it could be implemented as a library, but full support is required to have the compiler be capable of reasoning about memory addresses, sizes of values that the memory addresses point to, and cache levels.
This reminds me of TermKit and a stupid [8086 boot sector that draws a picture of Samus Aran](http://rm-vx-ace.de/WBB4/gallery/userImages/54/1277-540ebc63.png) thingy I programmed for university homework.
Presumably it won't (directly), you just use C++ code to do the interaction, embedding that in Rust.
Are Google employees not allowed to have their own GitHub profile?
No, it's both. See the Qt example that u/farseerfc posted above, for instance.
Thanks to the author! It made quite some people smile at RustFest today: https://twitter.com/andrewhobden/status/777097398124503040
This answer gives rise to even more questions. Like in rust code directly use instances QObject? ps: My variant of Qt5 test app with QML https://github.com/ivlevdenis/rust_qt_qml
I've found that fully automated binding generation would be very hard to do, if possible at all. The main reason is simply that the semantics and concepts don't transfer very well between languages. Additionally, ownership and borrowing is often implicit in C++, where as it is part of the type system in Rust. I would have a hard time trusting any generated bindings which tries to expose a safe Rust interface. The other way around is probably much easier in that regard. I use rustcxx by writing a safe rust wrapper which calls to C++. Without it I would need to write my rust wrapper around a C wrapper around the C++ library. All rustcxx does is help reduce the boilerplate required by generating the C part.
That's pretty dope
Those agreements are non-enforceable in California. Anything you make with your own hardware, on your own time, is yours.
Or GraphQL 🤞
I don't have anything to base this upon, but I thought that most web work happened in small-to-medium-sized non-safety-critical products.
Why no? Couldn't we have pyqt-like api for rust? (Is inheritance really an irreconcilable difference? Surely not!)
Please share the recordings later, including the videos on RustConf :(
Sure, that's true, if you were to just sum the total number of Web sites. But there's a lot of consolidation in software; for example, 20% of all Web sites are WordPress alone. When software is so widely deployed, there can be a lot of value in making it as fast and reliable as possible, because even though each individual site is small and non-safety-critical, in aggregate the benefits of reliable software are significant. (Granted, WordPress is not necessarily the best example of this.) :) Moreover, when you measure usage rather than raw numbers of sites, you see a small number of high-value sites: Facebook, Google, YouTube, Amazon, Reddit, etc. These sites, though smaller in number, benefit a lot from fast performance and reliability.
Is it good practice to include static files with binary VS reading the file from disk at runtime . I like first one because it gets rid of runtime errors. The files are html and handlebar templates.
Next step is to grab the Windows console HWND and inject images directly into it: http://stackoverflow.com/a/1937178 (Kidding, mostly) ;)
This author seems to have a very Frenchy name, might be working in Paris...
My understanding is that the Rust Conf recordings will be posted soon. They're just being cleaned up and prepped for publication, which takes some time.
This is correct.
Soon is never soon enough Q_Q
thanks! 
I would like to be able to write a function like this: extern crate kinder; use kinder::lift::{Higher, Functor, SemiGroup}; use std::ops::Mul; fn squares&lt;A: Mul, T: Functor&lt;A&gt;&gt;(xs: &amp;T) -&gt; T { xs.fmap(|x| x * x) //square all elements in a collection of numbers } But that gives me the error: src/main.rs:6:17: 6:18 error: binary operation `*` cannot be applied to type `&amp;&lt;T as kinder::lift::Higher&lt;A&gt;&gt;::B` [E0369] src/main.rs:6 xs.fmap(|x| x * x) //square all elements in a collection of numbers The type of my function constrains `A` to require the `Mul` trait - but that constraint only applies to the result type of `fmap`. I don't see a way to specify a constraint on the parameter type of the incoming value. Is there a way around this? Could this be addressed by using two type parameters for `Functor`, like this?: pub trait Higher&lt;A,B&gt; { type Target; // swapped higher type, i.e. Vec&lt;B&gt; } pub trait Functor&lt;A,B&gt;: Higher&lt;A,B&gt; { fn fmap&lt;F&gt;(&amp;self, f: F) -&gt; Self::Target where F: Fn(A) -&gt; B; } fn squares&lt;A: Mul, T: Functor&lt;A,A&gt;&gt;(xs: &amp;T) -&gt; T { xs.fmap(|x| x * x) //square all elements in a collection of numbers } Edit: formatting
What is GCD?
Note, the README says it's [owned by Google](https://github.com/google/rustcxx#disclaimer). So it's Google that assigned the code the license, not it's author. Another response says that kind of agreement is unlikely for different reasons. The author's probably better informed than I am.
GCD stands for Grand Central Dispatch, which is just Apple's marketing name for libdispatch: https://en.wikipedia.org/wiki/Grand_Central_Dispatch
Google allows employees to retain copyright on released code, but it's a different and more complicated process. Moreover I would assume you aren't allowed to use Google hardware for that, which I did. So if you don't mind about giving them ownership, then releasing under Google copyright is generally the easiest option.
Thanks for bringing this to my attention! I'll definitely look into that and see how to go about fixing it. With a little massaging I've gotten your example code closer to compiling: fn squares&lt;'a, A, T&gt;(xs: &amp;T) -&gt; &lt;T as Higher&lt;A&gt;&gt;::C where A: Mul, &lt;T as Higher&lt;A&gt;&gt;::B: 'a &amp;'a &lt;T as Higher&lt;A&gt;&gt;::B: Mul { xs.fmap(|x| x*x) } But this still errors out with: xs.fmap(|x| x\*x), expected type A for (x*x) but found &lt;&amp;&lt;T as kinder::lift::Higher&lt;A&gt;&gt;::B as std::ops::Mul&lt;&amp;'a &lt;T as kinder::lift::Higher&lt;A&gt;&gt;::B&gt;&gt;::Output. I'll definitely be looking into this carefully, the solution you've proposed seems like it could fix this issue, but it'd also require a large rewrite of the codebase, so it might take some time. I'll make sure to post updates as I work on this issue, and definitely feel free to fork the repo and hack away if you're interested!
&gt; &gt; .NET's HashSet is optimised for "extremely weak hashers"? &gt; Yes, this is actually really common. Lots of languages even use identity hashes - aka. hash(n) = n - for integers. The advantage is that not-so-random hashes are faster; the downside is that there's more overhead when you are using a good quality hash. What is the overhead? Just the modulo prime number rather than bitwise AND? &gt; Since Rust encourages using good quality hashes (SipHash even having guaranteed uniformity), it optimises for the latter. &gt; When I say i+4000*j is extremely weak, I mean it doesn't distribute evenly. I'm not talking about cryptographic properties, but just whether a hash map can reasonably expect it to act randomly on normal data. I still don't fully understand this. Your contiguous hash worked superbly well for obvious reasons. Your pseudorandom hash worked well for similar reasons. But why didn't my perfect hash (`i+4000*j`) work well in Rust? Does it really produce more collisions than your pseudorandom hash? Why would it produce more collisions with a 2^n hash table but not a prime-length one? &gt; &gt; Is .NET "geared predominantly around writing purely functional code"? &gt; &gt; .NET is a backend. It's there to compile code. By the point .NET becomes relevant, whether the code you wrote was functional is no longer relevant. I think you're confusing .NET with the CLR (which is the back end bit of .NET). The CLR is somewhat functional in the sense that it supports TCO but here I was referring to the fact that I used the standard .NET mutable `HashSet` collection which has nothing to do with F#. I suspect the results would be basically the same if you used C#. &gt; &gt; The whole algorithm can be expressed something like this &gt; &gt; You've forgotten the context, which was about a counting loop. In Rust, a counting loop is certainly clearer written imperatively. That's the claim you should be arguing against. Are you assuming we're only computing the neighbor shell of a single initial vertex? &gt; Even if we ignore the context, your code doesn't suffice because it doesn't memoize. That is the purpose of the argument `nth` to the `nth` function. It isn't recursive (yet). &gt; In Rust, the memoization machinery is almost guaranteed to be more work than just writing it as a normal loop. Really? Don't you just cache the output of the `nth` function in a hash table indexed by the shell and the initial vertex? &gt; Thirdly, that algorithm still isn't aided by functional sets, since the sets can't share any data. Oh but they do and even in the simplified benchmark we're playing with here. With purely functional sets you build up one neighbor shell from the two previous ones using set-theoretic operations (union and difference). Consequently, those data structures share a lot of their internals. Furthermore, the implementations of the set theoretic operations can exploit this to improve performance (OCaml is very good at this). For example, consider `s1={1..1,000,000}` and `s2={0} ∪ s1`. With mutable collections you'd have two completely different sets each with around one million elements (i.e. twice as much space as necessary). With pure collections `s2` shares almost all of its internals with `s1`. If you calculate `s2-s1={0}` with mutable collections it takes O(n) because it must iterate through every element but with pure collections it will cull shared subtrees and get the answer sooner (more like O(log n) time). &gt; Thus, the lack of a functional set in Rust does not matter here. Well, yes, but that is only true because I changed the original benchmark to make sure Rust's lack of functional sets didn't matter. &gt; &gt; Mutation doesn't make the solution clearer. &gt; &gt; Nay, it does, regardless of which way you go. Memoization machinery is almost always mutation oriented (Haskell's confusing variant being a rare counterexample), and the loop variant is clearer as an explicit loop. Ok. Memoization and many other things are certainly clearer using mutation. I meant specifically the loop variables in this program (and not the collections). &gt; &gt; Isn't that building a larger set and then using set difference to remove elements? Is that idiomatic in Rust? &gt; &gt; That's exactly what &gt; &gt; Set.unionMany(Seq.map nn (nth nn (n-1) u)) - nth nn (n-1) u - nth nn (n-2) u &gt; &gt; does. It's not necessarily idiomatic Rust, because idiomatic Rust cares about efficiency, but if you're just looking at it stylistically I don't see why not. The asymptotic performance of set theoretic operations on mutable vs immutable sets, as I mentioned above. &gt; &gt; Avalanching destructors do. &gt; &gt; Having avalanching destructors normally implies wild heap allocation, because stack allocations are almost always flat, and moderated heap allocations (eg. what you see in games and real-time code) are shallow. I don't think it implies wild heap allocation. The STL does (or did) an enormous amount of dicking around (like calling no-op virtual destructors because of RAII) between calls to `malloc` or `free`. I spent months trying to refactor that 50kLOC C++ code base to improve the latency and failed. The problem was essentially that I couldn't make incremental changes to get from where I was to where I wanted to be and have a working program every step of the way. I had to make big jumps and every time I tried the resulting program was horribly broken. In contrast, when I ported the program to OCaml I was able to do refactorings in a day that I had failed to do in C++. And the latency was naturally a lot lower in OCaml because there are no avalanching destructors. Granted you can say I didn't know what I was doing but I'd been programming for 20 years, knew many languages including assemblies and I was doing a PhD in theoretical physics at the University of Cambridge at the time so I don't think I'm a complete idiot. In fact, I was the last person in my research group to ditch C++ in favor of OCaml so I was the die hard. :-) 
I was doing the same example and here is my code: struct Point { x: f32, y: f32, } struct Rectangle { top_left: Point, bottom_right: Point, } impl Rectangle { fn area(&amp;self) -&gt; f32 { let Rectangle {top_left: Point {x: x1, y: y1}, bottom_right: Point {x: x2, y: y2}} = *self; let length = x2 - x1; let height = y1 - y2; length * height } } fn main() { let rect = Rectangle {top_left: Point {x: 2.0, y: 10.0}, bottom_right: Point {x: 10.0, y: 2.0}}; println!("Area of rectangle: {}", rect.area()); } 
[removed]
Thanks, I'll mark the ArchLinux PKGBUILD I was using as outdated and use a newer version. EDIT: It turns out the PKGBUILD was fine, except for adding a `cargo update` before `cargo build`
Generally, "inline asm" means that you can write some asm, in some sort of hackish syntax, and it will let you get raw instructions into your code. However, trying to cut and paste some asm from an assembler would probably fail, especially if you tried to use includes etc., pulling in full asm library definitions that weren't written for your compilers' weird inline-asm syntax. This lets you embed full-blown C++ code, as if you'd written it in another file, arranged the appropriate extern symbols/FFI, and linked it all together. Long story short, it supports #include of other libraries, in C++ -- even complex ones, like Qt. 
!!! Great! I didn't know about cpp_to_rust...
From the [video I linked earlier](https://youtu.be/_fu0gx-xseY?t=22m5s): &gt; This is what the OS expects [...] You need to build some kind of context [...] and it has to stay stable in memory. So what good libraries do? They'll combine heap allocation of a context that has to stay stable in memory, it cannot be a local variable. [...] so just on heap allocation From the opening (Rust) post of this whole thread: &gt; The space for this “big” future is allocated in one _shot_ by the task So this Rust approach might be as good as Boost.Asio (whose allocators will reuse memory in a way that I don't think it's worth detailing here). But then, the CppCon presented coroutines are _better_ than this. If you continue to watch just a little more: &gt; With coroutines it works very very similar [...] And then he will detail the "inner" workings some more until he finally concludes: &gt; There are no heap allocations. This is all inlineable And then he'll proceed the whole talk showing becnhmarks and improving BOTH versions and how coroutines are always better than any callback (even a "state machine callback" used here in Rust) and no callback model will ever have better performance than a simple coroutine. So, returning for your original question. This is what I've write: &gt; topic argues that futures only play nice with the readiness based model This is backed from the Rust post: &gt; Design the core Future abstraction to be demand-driven, rather than callback-oriented. (In async I/O terms, follow the “readiness” style rather than the “completion” style.) So you twisted my words a bit (and it was my fault as I wrote a plain wrong comment which added to the confusion, sorry): &gt; Why can't Rust's futures make use of IOCP "play nice" is different than "can/can't only...". To use IOCP, Rust would have to adapt IOCP model to a readiness model and from what I can imagine in my head, this already imply zero or one additional callback, but it would be a callback for which the state would be heap allocated. It cannot be better than the C++ counterpart. Honestly I dislike very much the attitude of the Rust community towards asynchronous programming. The mio author "justify" his arguments on "I don't care about Windows, somebody will port it for me" instead of backing his choice on good technical arguments and the "blindness" towards other options is irritating.
The first day of rustfest was AWESOME. (It's a little after 2am right now and I'm just coming home. Let's see if I can manage to be awake for most of day 2!)
What I'm missing is how you would share data at all. Maybe it's obvious, but I don't understand it. Even if you memoize the nth function and call it on many different (or even all) vertices, then it still doesn't share data as far as I can tell.
I haven't actually managed to get any terminal to display sixels yet. There's also the [Terminology](https://www.enlightenment.org/about-terminology) terminal from the Enlightenment project, which has its own escapes for displaying images.
I wouldn't be surprised if at a company like Google it's very much going to change team-to-team and manager-to-manager.
Recently released was https://github.com/google/rustcxx which looks to be really easy to use. It was just released so it might have some issues. https://github.com/mystor/rust-cpp/ is a similar solution, a bit older but works in a similar way. The other option is to make C bindings and then use those from rust. 
Does anyone know how to get a stacktrace from a panic as a string? I can get the default panic handler via panic::take_hook, but calling it just prints the stacktrace directly to output, with no way to access it.
Rust can expose functions that use the C ABI, the same thing you get with `extern "C"` in C++: * Make the Rust crate a `dylib` or `staticlib` (edit: or `cdylib`) * Define your Rust functions like this: #[no_mangle] pub extern fn foo(args...) -&gt; ret { ... } * Declare the same functions on the C++ side: extern "C" ret foo(args...); * Mark any structs shared between Rust and C++ as `#[repr(C)]` There's more information in the book: https://doc.rust-lang.org/book/ffi.html
Oh. I misread the question (sorry). You could use those tools to define a C++ function that calls a rust function... But a better way would be to [define rust functions with a C calling convention ](https://doc.rust-lang.org/book/ffi.html#callbacks-from-c-code-to-rust-functions) and then make matching header files or use https://github.com/Sean1708/rusty-cheddar to do it automatically.
Reddit formatting turned your `#[no_mangle]` into this: #[no_mangle] You might want to fix that. Try using code blocks.
Apparently IBM is pushing swift hard for some general purpose, cross-platform language
You probably need the [backtrace](https://crates.io/crates/backtrace) crate. With `panic::set_hook` you can intercept the panic and call `Backtrace::new()` from within that callback.
Thanks, I get it
I am a heavy user of elasticsearch and frankly feel there is a need for alternatives. I ll get back to you, once I get acquainted with your source code.
Ironic that Reddit mangled the `#[no_mangle]` attribute.
Looking at this video again and comparing it to the design of Rust's futures they seem to be describing the same zero-allocation state machine style functionality. The C++ proposal has that os_async_context that he extends to define his state machine and that seems to match the rust Future almost exactly. He makes his AwaiterBase and awaiter to define how to proceed through the state machine for this task just like the example Join does in the blog post. It seems the only real difference between the two approaches is that the C++ one is completion based while the Rust one is readiness based. The completion design is certainly more flexible and with it built in to the compiler you could probably avoid the allocations the blog mentions for chaining operations. However, you cannot avoid the extra overhead of having to have a unique buffer for each in flight task and he doesn't even talk about that, presumably because he is building this on Windows so you have that to do async IO there anyway. On a Linux system using epoll you should be able to get much better performance with something like Rust's Future than with the proposed C++ coroutine model because the Rust model will let you reuse buffers at the application level when reading from a socket. The readiness model also makes it easier to handle cancellation and backpressure, as pointed out in the blog. Aside from that one (major) difference they appear to have the same benefits over the traditional callback model of reducing allocations, not requiring virtual method calls, and allowing the compiler to inline the entire operation for even more of a performance win (if that would be useful in a particular situation). A future async/await implemention in Rust that desugars to this Future design would remove the one advantage the C++ version has: usability. This is a long running debate in the computing world but personally I would rather have the readiness model than the completion model. Not only does it offer more optimization opportunities it is what Linux is already using. You can emulate the readiness model using completions on Windows for a small overhead but emulating completion using readiness has a much higher cost. This means the C++ proposal will result in the most efficient possible code on Windows and (much?) slower code on Linux. The Rust Future design will offer the most efficient possible code on Linux and somewhat slower code on Windows with a fixed overhead cost. As most high performance networking happens on Linux it makes sense to optimize for that environment. Since async disk IO basically doesn't exist on Linux it doesn't make sense to try to optimize for that scenario by using a completion based design, especially not when it would come at the cost of reducing performance of socket IO. Amusingly, I actually just learned that the libaio (POSIX AIO api, what the video is showing for doing his task on Linux) library is actually just implemented under the covers as a thread pool doing regular disk IO. Sorry for the long post, didn't think it'd be this much when I started.
Swift is still a small niche programming language. PHP, Python and Ruby are huge on web development. That can not change in 3-4 years. 
The only case I can think of "shadowing" as you use it to be useful is if you're changing mutability, and in that case I recommend inserting a comment as to why you did it (i.e. prevent programmer error because it's important a variable not change) because it's not completely obvious what's going on. For example: let mut x = 5; x = 10; let x = x; // immutable because we're done initializing Other than that, I can't think of a time when you'd want to do it.
Just the same thing I showed as an option on when to use it. I just wrap it in its own context to improve readability. 
Check out [the chapter about trait objects](https://doc.rust-lang.org/book/trait-objects.html#dynamic-dispatch). First of all, you need to annotate, that you want to use `vehicle` only as trait. The problem is, that you can't really write `let vehicle: Vehicle = ...`, because `Vehicle` is a trait's type and can be used only behind a pointer. So you need to box it: let vehicle: Box&lt;Vehicle&gt; = match x { 0 =&gt; Box::new(new_car()), _ =&gt; Box::new(new_bike()), };
my question: Should I use `unreachable!()` on purpose to make sure the logic in my code is correct? My main concern is that I'm abusing the macro, not the runtime cost of checking for correctness, unless that cost is super high and I don't know? Some background: I have a tree structure where `Node`s use the index of a `Vec&lt;Node&gt;` to point to their parent/children. This tree has some rules: a `Node` is either a parent, pointing to two children, or a leaf. This rule is never broken. I decided to represent that as an enum: #[derive(Debug, Clone)] enum NodeStatus&lt;T: Clone&gt; { Parent(usize, usize), Leaf(T) } It was also an enum in the C version of the code, but that didn't prevent logic errors at all, it was done to save memory. The "problem" is that I have code like this in a lot of places: if let NodeStatus::Parent(c1, _) = self.nodes[i_s0].status { self.nodes[i_s0].status = NodeStatus::Parent(c1, i_t1); } else { unreachable!("node to be balanced is not a parent") } The `else` branch never happens, and if it does it is a logic error, but I wonder if simply splitting the `status` into two different fields would be better, I can still `debug_assert!` that `parent` is `Some` and `leaf` is `None` before doing anything.
You can do something like this: https://is.gd/rhrylF
One idea is to introduce an enum in which each variant is a vehicle type https://doc.rust-lang.org/book/enums.html If you insist that your vehicles must be structs, then the enum_derive crate provides some tools for dealing with structs wrapped in an enum https://danielkeep.github.io/rust-custom-derive/doc/enum_derive/index.html#overview
Why box? Of course `drive` needs to take `&amp;self` or `Vehicle::drive` can't be made a trait object at all. let x = 0; let (car, bike); let vehicle: &amp;Vehicle = match x { 0 =&gt; { car = new_car(); &amp;car }, _ =&gt; { bike = new_bike(); &amp;bike } }; vehicle.drive();
Thanks, you're right about lack of `&amp;self`. And the trick with local variables is really nice, I didn't know it! I often forget that you can have uninitialized variables in Rust. Anyway, when you want the trait object to escape the scope, boxing seems the only way.
There is no reason to define an enum, you can simply define variables for both values (`Car` and `Bike`), and only assign one of them.
Great crate, I think I am going to use this solution.
I see, thanks!
Where would I use an uninitialized value?
I do that too. :)
&gt; because it simplifies reading and code navigation. It depends. I'm a huge proponent of shadowing; it can make things clearer when used correctly. Especially since the borrow checker often makes you split things out into multiple statements. Take the example above: let input: String = get_input(); let input: i64 = input.parse().unwrap(); Here, we never want to use the first `input` again, it's solely for converting to the resulting `i64`. Coming up with names here is tough; you either add the type name to the name, which is annoying, or you do something else. I think this code is easier to read with shadowing than it would be without. See also /u/thiez's example below.
Learning something new every day ^^. But I bet I have used it like this several times before, and never noticed it.
I probably misunderstood. Maybe you could give me a code example? Sorry, I'm on my phone on a plane and doing a terrible job of figuring this out for myself.
I was trying to install clippy using `cargo install` but could compile clippy_lint. I'm on rustc version `rustc 1.13.0-nightly (a23064af5 2016-08-27)` and cargo version `cargo 0.13.0-nightly (88e46e9 2016-08-26)`. one of the error message is `unresolved import `syntax::ast::NestedMetaItem`
It's actually quite simple: fn main() { let car: Car; let bike: Bike; let vehicle = match ... { ... =&gt; { car = Car::new(); &amp;car }, ... =&gt; { bike = Bike::new(); &amp;bike }, }; } Some type annotation might be required, but since you initialize the variable in any branch where you use it there should be no other complaint.
Is there a clean way to have tests in a Cargo project call a binary that's built from the same project? Right now I'm shelling out to `cargo build` in my test code, but that feels pretty dirty.
/r/playrust
On an unrelated note, your code would be better written to take advantage of trait generics like so: https://is.gd/7D40fv
In hindsight thiez and matthieum are right that you do not need an enum to make the snippet work. I suggest starting out with Matthieums idea to see if it solves the problem.
So excited! Close enough for me to go.
Another thing is that shadowing is purely lexical, so it doesn't work across dynamic control flow like loops: let mut sum = 0; for x in xs { sum += x; } Trivial example that you would probably replace with `.sum()`, but the point is you can't shadow a binding from a previous loop iteration since loop iterations don't exist at the phase shadowing happens.
Any updates on those rust conf videos? I'd like to at least see the opening keynote.
You need a current nightly. There has been a change in macro handling, which we had to change clippy to accommodate.
F# has a monad notation with imperative control flow structures as well.
Bit far for me :/
Awesome! I really have to dive deeper into clap. I hope I can use the bash completion with imag (as you might remember, the imag source contains multiple binaries/commands). Same goes for suggestions, I really want to get them into imag soonish.
I'm no expert when it comes to websites, but I think you can improve this a bit by trimming down those huge images. There are two of them (intro-bg and contact-bg), they appear to be identical and weight 4.4 MB each. Merging them into one and encoding them with JPEG instead of PNG would do wonders.
Woohoo! Didn't see that coming. :)
The last time I went to Ukraine I was robbed by the border guards. That was 2 years ago. This was very common and I've actually heard the situation has gotten worse since then. 
Oh, sorry. There are many ways to express this and many related things that I was calculating. This is just one of them (that I chose because it is easy to express using mutable sets in Rust). Purely functional sets make it easy to do things like store all vertices up to and including nth neighbors rather than exactly nth neighbors. I found that useful for a variety of other things. They can obviously share and that sharing can make the differences between them easy to compute. Then you can say that the `0..n`th neighbors of `u` is the union of the `0..n-1`th neighbors of `nn u`. And so on. 
Done! Thanks! :D
Additionaly you could use tools like `jpegoptim` to reduce their size even further without visible changes.
Would I click on a video tutorial for a crate? Yeah... I probably would. I understand not everyone would but I don't think we should discourage this at all.. no matter how small the crate.
The text blocks with large icons to the left overflow the screen on my Firefox developer edition (1280x1024 monitor). Also, when I scroll down to the very bottom, the header image peeks out above "Stay In Touch" and that behaviour feels wrong.
Cool, hope to see you there.
Thanks! Here's a [GIF](http://i.imgur.com/icbrIYe.gif) (or [GIFV](http://i.imgur.com/icbrIYe.gifv)) of it being used with `rustup`
Either you can define `trait MyTrait : Sync { .. }` or you'll have to ask for `Sync` explicitly in your methods, e.g. `fn x&lt;T: MyTrait + Sync&gt;(t: T)`.
Your image link is broken :-)
Fixed ;)
This is covered here: https://doc.rust-lang.org/stable/book/traits.html#inheritance
For the helix example did you follow the readme commands in the example folder? I ran the example a while back when I wrote up the readme. [edit] I was able to run the membership and turbo_blank example using the readme commands with stable but not my version of nightly(rustc 1.13.0-nightly (378195665 2016-09-08)). I am able to run the examples with the current nightly (rustc 1.13.0-nightly (32571c05c 2016-09-17)). Sorry wish I had a better answer for you. [edit edit] I was able to build and run your rake with the nightly listed above.
How do I make a function generic over `Set` there is both `BTreeSet` and `HashSet` but they don't implement a common trait?
This is a pretty cool website, but I was surprised to see that clap supports defining the argument parser in a YAML file - wouldn't TOML be more... rustic (you know, like "Pythonic" but for Rust instead of Python).
 [I've also posted this on the github issue linked in the stack overflow post](https://github.com/rustbridge/helix/issues/20#issuecomment-247851774) and have tracked my progress there. this is what I had previously: cargo --version cargo 0.12.0-nightly (6b98d1f 2016-07-04) rustc --version rustc 1.11.0 (9b21dcd6a 2016-08-15) So old! I also learned that I need to install rustup.rs to manage rust versions. I ran `rustup install nightly` and `rustup default nightly` which gives version 1.13.0-nightly 2016-09-17 (the one you recommend) After running `rake` again, I still get the same error. :-( I must me missing something in my environment. edit: the stack trace: here is my full stack trace: $ rake cargo build --release Compiling winapi-build v0.1.1 Compiling regex-syntax v0.3.5 Compiling libc v0.2.16 Compiling lazy_static v0.2.1 Compiling utf8-ranges v0.1.3 Compiling kernel32-sys v0.2.2 Compiling cslice v0.1.1 (https://github.com/rustbridge/neon#520d8abe) Compiling winapi v0.2.8 Compiling memchr v0.1.11 Compiling libcruby-sys v0.1.0 (https://github.com/rustbridge/helix#2c057f04) Compiling helix v0.1.0 (https://github.com/rustbridge/helix#2c057f04) Compiling aho-corasick v0.5.3 Compiling thread-id v2.0.0 Compiling thread_local v0.2.7 Compiling regex v0.1.77 Compiling Inflector v0.3.1 Compiling case_transform v0.1.0 (file:///home/me/Development/case_transform) warning: use of extern static requires unsafe function or block (error E0133), #[warn(safe_extern_statics)] on by default --&gt; src/lib.rs:15:1 | 15 | declare_types! { | ^ | = warning: this was previously accepted by the compiler but is being phased out; it will become a hard error in a future release! = note: for more information, see issue 36247 &lt;https://github.com/rust-lang/rust/issues/35112&gt; = note: this error originates in a macro from the standard library warning: use of extern static requires unsafe function or block (error E0133), #[warn(safe_extern_statics)] on by default --&gt; src/lib.rs:15:1 | 15 | declare_types! { | ^ | = warning: this was previously accepted by the compiler but is being phased out; it will become a hard error in a future release! = note: for more information, see issue 36247 &lt;https://github.com/rust-lang/rust/issues/35112&gt; = note: this error originates in a macro from the standard library warning: use of extern static requires unsafe function or block (error E0133), #[warn(safe_extern_statics)] on by default --&gt; src/lib.rs:15:1 | 15 | declare_types! { | ^ | = warning: this was previously accepted by the compiler but is being phased out; it will become a hard error in a future release! = note: for more information, see issue 36247 &lt;https://github.com/rust-lang/rust/issues/35112&gt; = note: this error originates in a macro from the standard library warning: use of extern static requires unsafe function or block (error E0133), #[warn(safe_extern_statics)] on by default --&gt; src/lib.rs:15:1 | 15 | declare_types! { | ^ | = warning: this was previously accepted by the compiler but is being phased out; it will become a hard error in a future release! = note: for more information, see issue 36247 &lt;https://github.com/rust-lang/rust/issues/35112&gt; = note: this error originates in a macro from the standard library note: link against the following native artifacts when linking against this static library note: the order and any duplication can be significant on some platforms, and so may need to be preserved note: library: util note: library: dl note: library: pthread note: library: gcc_s note: library: c note: library: m note: library: rt note: library: util Finished release [optimized] target(s) in 45.65 secs gcc -Wl,-force_load,target/release/libcase_transform.a --shared -Wl,-undefined,dynamic_lookup -o lib/case_transform/native.bundle gcc -Wl,-force_load,target/release/libcase_transform.a --shared -Wl,-undefined,dynamic_lookup -o lib/case_transform/native.bundle /home/me/Development/case_transform/lib/case_transform.rb:8:in `require': cannot load such file -- case_transform/native (LoadError) from /home/me/Development/case_transform/lib/case_transform.rb:8:in `&lt;top (required)&gt;' from /home/me/Development/case_transform/test/test_helper.rb:9:in `require' from /home/me/Development/case_transform/test/test_helper.rb:9:in `&lt;top (required)&gt;' from /home/me/Development/case_transform/test/transforms/camel_lower_test.rb:2:in `require' from /home/me/Development/case_transform/test/transforms/camel_lower_test.rb:2:in `&lt;top (required)&gt;' from /home/me/.rvm/gems/ruby-2.3.0/gems/rake-11.2.2/lib/rake/rake_test_loader.rb:15:in `require' from /home/me/.rvm/gems/ruby-2.3.0/gems/rake-11.2.2/lib/rake/rake_test_loader.rb:15:in `block in &lt;main&gt;' from /home/me/.rvm/gems/ruby-2.3.0/gems/rake-11.2.2/lib/rake/rake_test_loader.rb:4:in `select' from /home/me/.rvm/gems/ruby-2.3.0/gems/rake-11.2.2/lib/rake/rake_test_loader.rb:4:in `&lt;main&gt;' rake aborted! Command failed with status (1): [ruby -w -I"lib:test:lib" -I"/home/me/.rvm/gems/ruby-2.3.0/gems/rake-11.2.2/lib" "/home/me/.rvm/gems/ruby-2.3.0/gems/rake-11.2.2/lib/rake/rake_test_loader.rb" "test/transforms/camel_lower_test.rb" "test/transforms/camel_test.rb" "test/transforms/dash_test.rb" "test/transforms/underscore_test.rb" ] /home/me/.rvm/gems/ruby-2.3.0/gems/rake-11.2.2/exe/rake:27:in `&lt;top (required)&gt;' /home/me/.rvm/gems/ruby-2.3.0/bin/ruby_executable_hooks:15:in `eval' /home/me/.rvm/gems/ruby-2.3.0/bin/ruby_executable_hooks:15:in `&lt;main&gt;' Tasks: TOP =&gt; default =&gt; test (See full trace by running task with --trace) 
There's actually and issue open to add this functionality. I'm totally open to it, but the reason it hasn't been done yet is TOML is a very flat configuration file format, which means it's harder to visually parse and more verbose to write. This makes highly hierarchical structures like some CLIs more error prone to write unless you're doing some sort of dynamic or automated generation this TOML file.
I've decided to track my progress / lack thereof on github 
I wouldn't say your first two code snippets are the same, since printing out `x` at the end would produce different results :-/ let x = 5; let x = 10; // some more code println!("x is {}", x); // prints "x is 10" versus let x = 5; { let x = 10; // some more code } println!("x is {}", x); // prints "x is 5" 
To paraphrase Nirvana: Smells Like Navel-Gazing Academia. Why aren't more people building languages in Rust? Because they are still building Rust? Build more languages in Rust and it's just going to be constantly evolving turtles all the way down. Do not Grok.
They *have* open sourced all the core libraries: https://swift.org/core-libraries/
You're the best. thanks!
I take it you are cross compiling from Linux to Windows? You need to install the mingw64-gcc (or 32 what ever floats your boat) under Linux to handle the linking. The standard GCC install doesn't like to cross compile. Kernel-sys32 is binds for what is *sort of* glibc for Windows. If your compiling for Linux it should be skipped. 
BTW. Just because the subject is similiar: https://github.com/dpc/rdedup
Does it? I thought that rustup.rs instruction does not require `rust` already on your system.
Sorry, I didn't realize that would be relevant. Would it be enough to just say it is a Python application?
Those instructions use some other install script to install a prebuilt rustup binary, not building a package for rustup.
You just kinda brush over the fact that you use `catch_unwind` for control flow here, just want to say that this is _extremely not the way to do it_. You will swallow errors that could be caught by runtime checks, such as integer overflow and OOB indexing. Also, it means the correct behaviour of your application relies on the panic behaviour being set to unwind, which you cannot rely on (it is the default on x86 but there is a compiler flag to abort on panic and it's conceivable that some architecture will have that as default in the future). Additionally, if you ever want to expose the methods that panic as an API there is no way for a user to know that they need to use `catch_unwind`. A better way that gives you a lot more control and is easier to reason about is detailed [here](https://doc.rust-lang.org/book/error-handling.html), it lets panics take down the application as they are intended to, it allows filtering error kinds and propagating arbitrary information, and it means that possible errors are expressed in the type signature. Otherwise this was a really interesting article, hope to hear more in the future.
It's just running Enjarify on a couple of test apks over and over with every possible combination of options.
I am aware that panicing is not idiomatic in Rust, but this is a rewrite of a Python application. The Go rewrite had the same issue. Also, this is an application, not a library, so API isn't an issue.
&gt; Definitely report this to PyPy devs, they consider poor performance grounds for an issue on the tracker. [I already did that last year](https://bitbucket.org/pypy/pypy/issues/2269/huge-performance-regression-in-pypy3-for). I just never bothered to follow up.
Haha, they assuredly care about poor performance, they marked the issue as major! Definitely follow up, I would love to see PyPy vs Rust! Let me know if you ever get it to happen!
That comment is supposed to represent all remaining code - edit.
Rust isn't incomplete. It is full-featured and, most importantly, stable. The best part is: it is not stagnated, it keeps improving, getting faster, better implemented, and all of that without breaking its 1.0 API.
Thanks! 
&gt; (show a C++ dev `&amp;*obj` and they'll call you a madman) `&amp;*` isn't as common in C++ as in Rust but it does have some uses there, especially with iterators (it also works with `unique_ptr` etc., though you're supposed to use `get()` for that).
I'm still looking into that bug that stops [flamer](https://github.com/llogiq/flamer) from working (help?). Otherwise, I'll do some TWIR stuff and possibly clippy some projects. During the next few weeks I'll enjoy some weeks off from both work and Rust-related stuff due to $circumstances. I am unsure if it'll be next week or the week after but I'll let you know.
I wanted it to be similar to str, so that it is easier to understand, since it's just str without the utf8 restriction. Capitalizing it makes it look like it might be a custom struct.
Good article; makes some valid points. This baffled me: struct IRBlock&lt;'b, 'a: 'b&gt; { pool: &amp;'b mut (ConstantPool&lt;'a&gt; + 'a), // other fields omitted } Can anyone explain what (ConstantPool&lt;'a&gt; + 'a) means? I thought I was getting the hang of Rust, but I'd swear I've never seen that syntax before. 
Ah, never mind. This SO answer describes it quite well: [Answer](http://stackoverflow.com/questions/26212397/references-to-traits-in-structs/26213294#26213294)
I created https://play.rust-lang.org/?gist=d481eb451e48e5a76c4fc891d6208753&amp;version=stable&amp;backtrace=1 Is there a way I can make the `two_in` function more generic? Shouldn't any impl of `Contains` be able to be passed in?
I've been making a crate, [reffers](https://docs.rs/crate/reffers) with some interesting smart pointers: * ARef, which is like [OwningRef](https://crates.io/crates/owning_ref), but abstracts the owner even further. This makes it possible to return, say, an `ARef&lt;str&gt;` and have the caller drop the owner when done looking at it, without having to bother about whether the owner is a `String`, `Rc&lt;String&gt;`, a `Ref&lt;String&gt;`, a simple `&amp;'static str` or something else that directly or indirectly maps to a `&amp;str`. * I've also made an alternative to `Rc&lt;RefCell&lt;T&gt;&gt;`. By combining both layers in one, I can make it have less memory overhead (in fact, it's configurable!), slightly more ergonomic in some cases, and I also added poisoning support. * RMBA is basically an enum over `&amp;T`, `&amp;mut T`, `Box&lt;T&gt;` or `Arc&lt;T&gt;` but stores the discriminant in the lower two bits of the pointer, meaning that you can fit that entire enum in just a pointer's size. (Given that you run a rustc without drop flags.) Go ahead and use, comment, review as you like :-)
One difference between C and Rust is that some pointers are fat pointers. I e, a `* const [u8]` is actually both a pointer and a length in one. Same for trait objects. As for Rust memory layout in general, [the chapter in nomicon](https://doc.rust-lang.org/nomicon/data.html) might be an interesting place to start.
`+ 'a` says that this object must live for at least the region `'a`. As far as I know, this syntax is only useful when your code is generic over unknown objects. In this case it's generic because `ConstantPool` is a trait (and the reference is to a trait object).
&gt; There isn't a great deal you can do about it short of using GADTs Do you mean in terms of theoretical language support? Because union types are a much simpler solution to the problem - I'd argue they're actually simpler than Rust's `enum`.
&gt; Clone Arrays are not Clone Note that this is only for large arrays. Hopefully can be fixed. We need to make `Clone` a lang item for this. &gt; Reference Syntax is Confusing It basically gets confusing when patterns are involved, because autoderef doesn't work on patterns and you suddenly have to make everything explicit. Sometimes it helps to just not do complicated destructuring: let most_common: Vec&lt;_&gt; = { let mut most_common: Vec&lt;_&gt; = narrow_pairs.iter().collect(); most_common.sort_by_key(|ref thingy| (-(thingy.0 as i64), thingy.1.cmp_key())); most_common.into_iter().take(pool.lowspace()).map(|(ref p, count)| (*p).clone()).collect() }; for k in most_common.into_iter() { narrow_pairs.remove(&amp;k); pool.insert_directly(k, true); } 
I think ZoC's repo got some stars soon after your video :)
If you wondering about `X&lt;'a&gt; + 'a` you will be delighted to check `for&lt;'a&gt;`eg. http://stackoverflow.com/questions/23859866/how-to-specify-a-lifetime-for-an-optionclosure
There are other choices, too; my own [tty-player](http://tty-player.chrismorgan.info), for example.
If you have a background in C++ and are familiar with C++ containers and smart pointers, the memory layout of Rust things shouldn't surprize you. It's pretty much the same with similar abstractions (`Vec`&lt;-&gt;`std::vector`, `Rc`/`Arc`&lt;-&gt;`std::shared_ptr`, `Box`&lt;-&gt;`std::unique_ptr`, `&amp;T`/`*const T`&lt;-&gt;`T const*). One difference that crosses my mind right now is that Rust and C++ use different kinds of hash tables in their standard container library. In Rust you have an open addressing hash table whereas in C++ you deal with hash tables with an additional layer of indirection (the array stores a pointer for each bucket which points to the head of a linked list). /u/diwic mentioned another difference: fat pointers. Suppose you have an abstract base class "ABC" in C++: class ABC { public: virtual void somefunction() const = 0; protected: ~ABC() = default; }; void foo(ABC const* ptr) { ptr-&gt;somefunction(); } Here, `ptr` is a "normal" pointer storing a single address and nothing else. The magic that makes `foo` call the right `somefunction` depending on the dynamic type of `*ptr` is inside the object `*ptr` itself. This is typically implemented using a v-table pointer inside the object. In Rust this extra bit of runtime information is moved from the object to the pointer. So, given trait ABC { fn somefunction(&amp;self); } fn foo(ptr: &amp;ABC) { ptr.somefunction(); } `ptr` is now a "fat pointer" storing the address of something that implements `ABC` as well as a v-table pointer with which the right `somefunction` can be invoked. This choice has pros and cons. If you have multiple pointers referring to the same trait object, you'd have some redundancy w.r.t. the v-table pointers. On the other hand you only pay for this extra bit of information if you really need dynamic polymorphism. This trick can also be used to deal with slices and therefore "unsized types" in general. For example, the type `[u8]` means "zero or more u8 values stored consecutively in memory". Since its size is not known at compile-time it has to be stored somewhere else if you want to refer to such a slice. A reference to such a slice (a borrowed slice) is of type `&amp;[u8]`. A variable of this type effectively stores a pointer and a length, making it a "fat pointer" as well. You might be familiar with the "C struct hack". There is a clean Rust version of it: struct Foo&lt;T: ?Sized&gt; { // T can be unsized this: u32, that: u32, wrapped: T, } fn runtime_size_info(y: &amp;Foo&lt;[i32]&gt;) { // y is a fat pointer and so is &amp;y.wrapped println!("{}", y.wrapped.len()); } fn main() { let x = Foo { this: 23, that: 42, wrapped: [1,2,3,4,5] }; // Foo&lt;[i32;5]&gt; runtime_size_info(&amp;x); // &amp;Foo&lt;[i32;5]&gt; coerces to &amp;Foo&lt;[i32]&gt; } However, if you want a `Box&lt;Foo&lt;[i32]&gt;&gt;` with some runtime-dependent size of the slice, things get *real* dirty and hacky. I'm rather comfortable with writing unsafe code but I wouldn't know how to do it without relying on too many assumptions for which no guarantees have been made.
Using `try!()` is the more standard solution, but sure, adapt_err has a use there too.
I'm really eager to get [the git hook functionality](https://github.com/matthiasbeyer/imag/pull/486) merged into [imag](https://github.com/matthiasbeyer/imag) this week. By now it looks like we get it in ... after that I will add some more store testing stuff and then we'll see... whatever is in the pipeline for 0.2.0! :-)
The shellscript command on the rustup website is not building rustup, it just downloads a binary (`rustup-init`). The ArchLinux PKGBUILD actually builds `rustup-init`, but as I just noticed, it forgets to run `cargo update`, building instead with old dependencies.
It's still not clear what you mean with a hash test. Do you mean that you run the program and then hash the output to check that it's correct?
In my opinion nowadays Ukraine not the best country for holding RustFest: https://www.gov.uk/foreign-travel-advice/ukraine/safety-and-security &gt;Be alert to the possibility of street crime and petty theft, which is on the increase in Kyiv. Foreigners may appear to be lucrative targets. Where possible, avoid walking alone late at night in dark or poorly lit streets. Keep valuables and cash safe and out of sight, especially in crowded areas, tourist spots, and public transport, where pickpockets and bag snatchers operate. https://www.gov.uk/foreign-travel-advice/ukraine &gt;Public demonstrations can flare up and turn violent with little warning. &gt;You should take great care and remain vigilant throughout Ukraine. Avoid all demonstrations and take extra care in public gatherings. 
&gt; We need to make Clone a lang item for this. Or wait for [const-dependent type system RFC](https://github.com/rust-lang/rfcs/pull/1657).
REPL destroyed Scala and now Clap will destroy Rust ? i am sure JetBrains will be clapping with both hands after that. Focus on IDE/Eclipse instead, Save Rust.
I never said that there shouldn't be a conference in Ukraine and I do not understand agitation in your reply. (but just as a guess it seems I am not the only one who raises such concerns) I am more or less fine with this location and if I get chance I will certainly visit RustConf. In my post I've just noted that there is a certain aspects of visiting this country and it's better for people who will participate in RustConf to know this info in advance. Also I understand you became quite fond of Ukraine, but using as argument only your personal experience and ignoring statistics and reports is simply... subjective. As for "standard advice" just compare it with other European countries.
I don't agree with all the content of this article, but I like it and I think it's important to read reports of porting Python code to Rust, because Python code usually doesn't contain lot of esoteric stuff, but it still cares of ergonomy and coinciseness. &gt; and other easily fixed, but extremely frequent, compiler errors. I agree that compared to Rust, if you know Python well enough, you are able to write Python code in a quite fluent way, with a lower frequence of such small mistakes. This is one of the reasons I still like to use Python. &gt;traits can’t have fields, so I had to use an accessor method instead. Perhaps this Rust limit will be lifted. &gt; And that is in addition to the names of the enum constructors, leading to a ton of repetition. In some case a feature similar to the "alias this" of the D language ( http://dlang.org/spec/class.html#AliasThis ) could help Rust. &gt; The good news is that Rust’s std::panic::catch_unwind/resume_unwind proved much easier to use than Go’s defer/recover. As others here have said, this is probably a wrong way to translate that Python code. &gt; One thing I was surprised by was how borrowck-friendly Enjarify was already, given that it was written in Python, where aliasing and mutability is natural. &gt; Once again, I was surprised by how little the code actually relied on reference cycles. I was prepared to have to poorly simulate garbage collection with TypedArenas, but as it turns out, there were already no nontrivial reference cycles in the design. In most cases well written Python code should be this way. &gt; Another issue I ran into was with a recurring pattern where I iterate over all the instructions and mutate each one while examining the previous instruction or two. This doesn’t actually violate the alias rules, but there’s no way for the compiler to know that. Instead, I came up with a workaround. I created a local variable outside the loop which stores a copy of the relevant data from the previous instruction, and updated it and the end of the loop body. If those instructions are inside an array/vec then you can often use direct indexing to aboid borrowing issues like that (there is also the slower https://doc.rust-lang.org/std/vec/struct.Vec.html#method.windows ). &gt; Part of the problem is that auto-deref means you can get by without the right derefs in some circumstances, but in others, doing the same thing fails for no apparent reason. I agree this could be confusing for new Rust programmers. &gt; The compiler pretty much tells you what you need to change, but I wish I didn’t have to go through the edit -&gt; compile -&gt; edit -&gt; compile -&gt; success cycle so much in the first place. This is the death by a thousand cuts I mentioned above. &gt; Another annoyance is that even for debug modes, compilation takes a while (around 8 seconds for Enjarify) which is a big roadblock when you’re debugging in an edit-&gt;compile-&gt;run cycle. Compiling the Rust code with "-Z no-trans" helps speed up that cycle a little. Rust IDEs will help catch some mistakes faster without a compilation. &gt;Incidentally, I find it surprising that narrowing casts do not check for overflow. It seems like an oversight, though it might just be too hard to do in LLVM or something. The worst part is that it seems to be ineffective at preventing bugs. There were several bugs in the Rust version related to integer casts. In one case, I called the wrong function, and it took a different width than the desired function. But it passed the compiler because I also forgot to cast the variables in question. Besides, when nearly every call requires a cast, people will just automatically add them to shut the compiler up, defeating the purpose. I agree, and I think it's one of the few Rust design mistakes. I face this problem with a small macro: https://users.rust-lang.org/t/safer-casting-with-a-little-macro/6773 &gt; Unfortunately, usize inexplicably fails to implement From&lt;u16&gt;, so I once again was stuck with the underscore hack. This will be added, I guess. &gt; This works reasonably well, mainly because I don’t do much string manipulation beyond slicing and concatenation. But on the rare occasions where I do, I really missed the string convenience methods, which aren’t defined for Vec&lt;u8&gt; or &amp;[u8]. Python by contrast, defines all the string methods for both unicode and byte strings. It also means that the default Debug implementation shows the values as a list of integers, rather than as strings. I agree that ASCII string methods on &amp;[u8] are good to have in the std library. &gt; On a side note, I never figured out how to make the compiler shut up about the naming of bstr, despite repeated searching. Nothing I try seems to work. This should do the trick (it's actually written in the error message the Rustc compiler gives you): pub type BString = Vec&lt;u8&gt;; #[allow(non_camel_case_types)] pub type bstr = [u8]; But it's much better to change that name into a Rust-idiomatc name like "Bstr" or similar. &gt; Obviously, subset types would make the type system much more complicated, Some persons have already suggested to introduce some subsets of enums (like in Typescript?) in Rust. &gt; For some reason, even if you match on a u8 and cover every case from 0 to 255, you still have to add a _ =&gt; unreachable!() at the end to satisfy the compiler. I’m not sure why it’s not handled, but it kind of defeats the purpose of exhaustiveness checking. With an unreachable default case, there’s no compile time protection in the event that you legitimately missed a case. Apparently, this is a known issue but can’t be fixed because of backwards compatibility. I hope this will be fixed even before Rust 2.0. &gt; A similar issue makes it impossible to initialize a large array, forcing me to use a Vec inside SplitConstantPool, even though the data has a known, fixed length. There is usually an unsafe workaround, if you know what you are doing. This is otherwise a small pain point of Rust. &gt; Likewise, it seems like Vec&lt;Option&lt;T&gt;&gt; -&gt; Vec&lt;T&gt;, removing all the None elements, would be a common task, but as far as I can tell, it still requires a verbose iter/filter/map/collect chain. There is a function for that: fn main() { let v1: Vec&lt;Option&lt;u32&gt;&gt; = vec![Some(5), None, Some(1)]; println!("{:?}", v1); let v2: Vec&lt;_&gt; = v1.iter().filter_map(|&amp;x| x).collect(); println!("{:?}", v2); } &gt; For the Rust version of the mutf8 decoder, I used custom iterator adapters. Iterators return Options, not Results, but unfortunately, there is no equivalent of the try! macro for Option. I think there are discussions to add this, making "?" more general. &gt; I wish Rust did what Python does and print out the relevant snippets of source code in the stack trace. In Python the source code is most times available, in Rust this is less often true. But in general I agree. &gt; The Index trait is broken, because it requires returning a reference to the result, limiting it to collections that actually store a corresponding element. You can’t create and return values on the fly. This means that I couldn’t overload [] for Enjarify’s custom sparse array type and had to make do with get() and set() instead. I agree.
I definitely agree that people should be aware of where they're traveling. And it is also true that Ukraine has had a lot more instability in the last few years than many places in Europe, and also that I have an affinity for it :). &gt; As for "standard advice" just compare it with other European countries. Yeah, this is what I mean. Let's also check out what the UK government says about France, to pick a random country (that I also have a lot of love for ;) ) https://www.gov.uk/foreign-travel-advice/france &gt; There is a high threat from terrorism. ... The French government has extended the national state of emergency until 26 January 2017. https://www.gov.uk/foreign-travel-advice/france/safety-and-security &gt; Take sensible precautions against street and car crime. Don’t keep your passport, credit cards and other valuables in the same place; use the inside compartments in bags where possible. Carry your bag across your body rather than on your shoulder. &gt; &gt; Pickpockets can work in gangs: one to distract you while the other one goes into your bag. Keep your belongings close to you in restaurants and bars. Don’t be distracted around tourist attractions and cash points. This is what I mean by "standard" advice. It's not that it's bad advice, but that it applies pretty much everywhere. Staying safe while in a foreign place is always important. I am not saying "nothing bad has happened to me, therefore, it's impossible for bad things to happen," I'm saying that "government travel websites are extra cautious, and for good reasons, but it's important to not let that color your opinion of an entire country."
Thank you. I learnt a lot from this post.
`clippy` should pick these up by the way, that's how I learned about `cloned`.
&gt; Let's also check out what the UK government says about France, to pick a random country With the best will in the world, France is a peculiar example to pick to show the foreign office are being over cautious and the advice on violence is "standard". 
I'm building from source, yes. I build a lot of packages on my systems from source when the repository doesn't have binaries (or if they're too old).
Note I haven't highlighted terrorism threat, which is probabilistically speaking less dangerous than travel accidents. I focused on a bad trend of petty crime rates and issues with public safety. (news from last two-three months do not look so good in this regard either) As for comparing, lets see countries near Ukraine: [Moldova](https://www.gov.uk/foreign-travel-advice/moldova/safety-and-security), [Bulgaria](https://www.gov.uk/foreign-travel-advice/bulgaria/safety-and-security), [Latvia](https://www.gov.uk/foreign-travel-advice/latvia/safety-and-security), [Romania](https://www.gov.uk/foreign-travel-advice/romania/safety-and-security), [Poland](https://www.gov.uk/foreign-travel-advice/poland/safety-and-security). Arguably they all use softer wording and warnings regarding crimes. In the end I guess we conveyed our respective opinions and should close this discussion. Good luck in conducting RustFest and I hope it will be a great event! :)
I think the AUR is out of date so you should let the maintainer know but in the meantime you can build from source [here](https://github.com/rust-lang-nursery/rustup.rs) which is where all the code is officially hosted.
&gt; In the end I guess we conveyed our respective opinions and should close this discussion. Agreed! I actually think we agree 99%. :) &gt; Good luck in conducting RustFest and I hope it will be a great event! :) To be clear, I'm not involved in organizing RustFest in any way. I attended this one purely as an attendee, and will be at the next one as well.
I'm not intending to cross-compile. I'm on CentOS, I've put iron in the Cargo.toml, example source in main.rs, and typed "cargo run" - it's that simple.
I've got clang installed. I thought Rust was based on LLVM. I can install gcc for giggles I suppose...
Ahhhh I see.
Yeah I want to give them credit of course, it's already mentioned in the readme, and for the boundary-package-merge algorithm that was developed by the zopfli authors, also in the source.
the AUR version is 0.6.3, same as the latest upstream release. The package was marked out of date because of this exact build error, and I've notified the maintainer of the fix (add the `cargo update` line). Up until then I'll use my own fork of his PKBUILD that adds this line
That'd a reasonable course of action!
So, considering the thread about hosting RustFest in Ukraine, I'd like to give a little bit of perspective: We were approached by two Ukrainian community members in 2016 about running a European conference. Considering that at that time there was already a team close to announcing RustFest in Berlin, with advanced discussions about a venue lease, we decided to adopt them into our organising team and planned back then to introduce their location as the next conf in the cycle. So the main reason for picking Kiev is: there's community interested and experienced in running a conference there. We, as the RustFest initiators, trust them to carry the name. This is not an organised project decision, but one by occasion. It is also, as mentioned, it is great spot to connect two communities. I firmly believe that programming language communities should not be focused on the US and the EU, so RustFest happening outside of this area of influence is very close to my heart. Finally, note that the spacing of about half a year between the conferences ;). You will have your chance to visit a conference if you can't or don't want to travel to Ukraine for whatever reason. Andrey and Yenna and the rest of the Ukrainian community will kick ass!
Rust is based on LLVM, but the default system linker is gcc for Reasons.
Other String types in Rust use standard Rust naming conventions: [OsStr](https://doc.rust-lang.org/std/ffi/struct.OsStr.html) / [OsString](https://doc.rust-lang.org/std/ffi/struct.OsString.html) [CStr](https://doc.rust-lang.org/std/ffi/struct.CStr.html) / [CString](https://doc.rust-lang.org/std/ffi/struct.CString.html) I would recommend changing your `Bstring` to `BStr`/`BString` to match this behaviour.
TIL: there's a Rust community in Kyiv.
&gt; Note that this is only for large arrays. I thought large was defined as &gt; 32? I used a 16 element array but it still didn't work.
Note that I totally ignored zopflipng in my port, not sure if that matters :)
Curiously enough, was my issue with Go as well. The type inference is cool, but I really missed the standard C promotion rules.
This is excellent, thank you! Rust's abstractions are concrete and it won't makeup code for you. This sounds like the preferable way. fn two_in&lt;T: Contains&lt;i32&gt;&gt;(h : &amp;T) -&gt; bool { h.really_contains(&amp;2i32) } My hope was that I could do something like fn two_in&lt;T: Contains&lt;T&gt;&gt;(h : &amp;T) -&gt; bool { h.really_contains(&amp;2i32) } and that T would resolve back to the type bounds for the concrete instance of h passed into the function `two_in` but I guess why it **doesn't** do that, is that it would an implicit type of `i32` onto the function `two_in` and that is too much magic. Rust will monomorphize a concrete implementation but it won't automatically create an implementation from the types (this could be done using the macro technique you mentioned) ? 
Huh. Looks like the impl doesn't exist at all. It only works when `T: Copy`, which isn't the case here.
&gt; (there is also the slower https://doc.rust-lang.org/std/vec/struct.Vec.html#method.windows ). Is there a particular reason that it's slow? Seems like llvm should be able to understand that one pretty well
What kind of feedback are you looking for?
The binding is decent but, now that I've started rewriting some old PyGTK (2.x, obviously) applications in PyQt5, I'm realizing how primitive GTK+'s APIs really are compared to their Qt counterparts. (Not that Qt is perfect. For example, Qt 5's support for loading icons from the XDG system theme is far inferior to GTK+ 2.x's and I had to hack around the lack of an option to upscale icons to enforce consistent sizes in a list view.)
Incremental compilation is on it's way but it is by no means stable yet. It's in the works at least.
hi, i'm using a BTreeMap. i'm trying to do the following operation : remove any element (I don't care) from the tree. my keys cannot be copied. I tried something like that (t is my BTreeMap): let key = t.keys().next().unwrap(); let value = t.remove(key); but the borrow checker complains (rightly). I did not find any way to achieve this operation except by making my key type copiable and changing the code like that : let key = *t.keys().next().unwrap(); let value = t.remove(&amp;key); which I find kind of ugly (so much cost for nothing). do you see any other way to achieve this simple operation ? thanks a lot ! 
Wrong subreddit
The right subreddit is /r/playrust
I'll be working on catching up on sleep after the wonderful Rustfest 😅 After that, I'll try to finally get back to and work on/document a few of my side projects ([cargo-edit](https://github.com/killercup/cargo-edit), [rustfix](https://github.com/killercup/rustfix), [docstrings](https://github.com/killercup/rust-docstrings), and friends).
Do you know why it has a Copy bound instead of a Clone bound?
&gt; (Or for that matter, change all unreachable-pattern errors to warnings... I don't think it would hurt that much.) Or perhaps just change *this particular* unreachable-pattern error into a warning.
Agreed—experience reports like this are invaluable. Thanks for writing it up!
`Copy` is `trait Copy: Clone`, so you need `Clone` to implement `Copy`. They've implemented `Copy` for small arrays and `Clone` was implemented as part of that.
&gt; I suppose it's rare that you match on every possible value of an integer Dont forget you can use ranges too.
The `do` notation is not "completely incompatible by default" with imperative control flow. Scala has it (although it's called `for`). It's duck-typed, and it works very well. It's just syntax sugar for calls to `map`, `flatMap`, `filter` and `foreach`, essentially.
I'm fully aware, but it's _still_ unlikely to match on every value explicitly
[tessel.io](https://tessel.io/) might be what you are looking for. Someone did a presentation on it at Rustfest and it seems to be pretty functional already.
Thanks for writing this! It's very detailed and it looks like you put a *ton* of time in.
&gt; During the next few weeks I'll enjoy some weeks off from both work and Rust-related stuff due to $circumstances Nothing bad I hope. I'm this opportunity to say that I'm very grateful for your activity within the community, especially the "newbie-friendly" part of it, which motivated me to jump into rust.
Yep, which we can then #![deny(...)]
That's what I meant by "backwards compatibility hack where rustc would warn instead of error if in this specific case". Though to elaborate, it's not clear to me that unreachable patterns deserve to be a hard error any more than other unreachable code, especially in combination with macros or `#[cfg(..)]`-gating match branches (is that possible? it should be). And the effort involved in distinguishing types of unreachability could deter adding more ways to establish exhaustiveness in the future - e.g. in cases like `match x &amp; 3`, which come up quite commonly in low-level code (at least based on my experience in other languages).
So basically an Electron type monstrosity of shipping a browser with your app. As I said, no good UI options.
Actually it's not bad at all, I'll take some time off to be with my family after the birth of my third child.
This is super awesome! I will enjoy dissecting all the responses to each one of his points. I think it is only through new eyes on moderately complex programs, that anything like Rust can quickly evolve. Make this guy happy!
Could one use a compile time macro to take a { arithmetic ops } and convert it to using a certain promotion set, automatically inserting the proper casts?
Awesome :) Congrats!
Slightly offtopic, but why use `LinkedList` instead of `Vec`?
Y'all make some nice posts and I'm happy to read them, but that email sign-up pop-up is super annoying. I can't imagine that you gain more leads than you lose with that stupid thing.
Yes, I think you want something like let last = self.prime_list.back().unwrap_or(&amp;2) Edit: Ah you're right, I meant `&amp;2`.
Maybe its my experience being predominantly in dynamic langauges but I have found the lack of a strict type system to not really affect me too much. the minimal one combined with arity checking catches alot of errors already while letting me quickly iterate without me having to check my types everytime. TLDR: It has a type system. its not as strict or deep as haskell but it does a descent job and gets out of my way more than haskell's one does. 
As far as I can see tessel just provides rust bindings for their hardware but it's not general framework.
I have now changed the `LinkedList` to a `Vec` as /u/thiez suggested and thereby reprogrammed the whole function. Similar to your suggestions I removed the `last` field and now use `match *self.prime_list.last().unwrap_or(&amp;2)`. Thanks!
&gt; aforementioned untagged unions and nonlexical borrows waiting for this =) 
Personally, I'd even go as far as to say that it's as bad of an idea as using `setjmp`/`longjmp` in C to do exception handling (i.e. it would instantly fail code review, no cuts, no buts, no coconuts). &gt; Also, this is an application, not a library, so API isn't an issue. If someone else finds a bug while using Enjarify, how are they going to send you a stack trace and/or a core dump? Your panic handler just throws the stack trace information away and, in addition, they won't be able to get a core dump, since your program will keep chugging no matter what. Also, how are you going to debug it? You're throwing away details from error results that may be helpful and grepping panic messages isn't an option, unless you have the stdlib source in handy.
If it was a `HashMap`, you could use the `.drain()` iterator, and then `mem::forget()` it so that it did not empty the rest of the map. However, there is no equivalent iterator for `BTreeMap`, or at least not yet. `BTreeMap` and `BTreeSet` definitely could use more attention from the libs team. If your key type is expensive to copy/clone, perhaps you could put it in an `Rc` to make clones cheaper, then use it with the map like normal: let key = t.keys().next().unwrap().clone(); let value = t.remove(&amp;key);
Well, remember that matches can evaluate to a value, so unreachable patterns would have to panic. Actually, they did in early versions of Rust, but it was changed because a lot of panics were being generated IIRC.
Wow, where does this come from? This looks amazing. Maybe this is the right library to implement ruby support for my imag filters library :-)
You want /r/playrust. Please look at the sidebar and subreddit before you post things.
There's a lot of discussion on how to get some sort of OOP in Rust without jumping the shark and adopting a full-on class system. * Aaron Turon's [Thin Traits proposal building on Specialization](http://aturon.github.io/blog/2015/09/18/reuse/) * Niko Matsakis' [Extended Enums proposal](http://smallcultfollowing.com/babysteps/blog/2015/10/08/virtual-structs-part-4-extended-enums-and-thin-traits/) (linked to his conclusion comparing this proposal to Thin Traits) * Niko's RFC introducing one of the core features of Thin Traits, [direct field access on the implementing type](https://github.com/rust-lang/rfcs/pull/1546) I personally like both proposals, especially having support for common fields on enums. I hope we get the best features of both someday. Edit: fixed link
`T: Contains&lt;T&gt;` doesn't really make sense if you think about it. I guess it could work if `T` is actually a cyclic datastructure (`T` can contain instances of itself) but that doesn't seem at all relevant to your use-case. What it seems you're looking for is some trait that's implemented by all integer types so that your `two_in()` function can be used with any of them. That's the purpose for creating the `Two` trait and implementing it for various integer types. You can use a macro to reduce the boilerplate, a pattern used extremely often in the stdlib and the wider ecosystem: https://is.gd/85yeMu Note that `f32` and `f64` aren't immediately compatible with either set type because they don't implement `Eq` or `Ord` (mainly because `NaN`, for correctness' sake, can't be comparable to any other number, even itself). You'd have to implement a wrapper type which adds these impls and handles the case when either side is `NaN`. However, I included them in the `impl_two! {}` line to demonstrate that the macro is not limited to integers, thanks to that otherwise redundant cast. 
I assume that in your case `T` is not a simple `Copy` type, so you don't want to do `*vec[i]`. You can try this instead vec.into_iter().nth(index).unwrap() I hope that `nth` is specialized for `IntoIter` so it shouldn't be linear (edit: it isn't :( ). But dropping vec is linear anyway :P
A C string isn't guaranteed to be 8 bits.
In the Python version, I just comment out the error handler while debugging. It's not hard. It's only a one line change. If somebody wants to report a bug, they can just send me an apk. Stack traces on their own are not very useful anyway. But in case you were wondering, it does still print out stack traces of errors encountered, it's just that it keeps going after that and continues with the next class instead of crashing.
Dangerously Fun!
Failed, this is the langage RUST, Not the game. Go to -&gt; /r/playrust
Indexing does always borrow the whole object. When you put it in a loop, though, each borrow ends before the next begins. Also, it is possible to get mutable references to multiple `Vec` elements by calling [`split_at_mut`](https://doc.rust-lang.org/stable/std/primitive.slice.html#method.split_at_mut) (potentially multiple times), and taking only one element from each slice.
Serious, when you submit something, it's writen in BIG "The Rust programming language. For the Rust video game, see /r/playrust".
Ah, I didn't think of the split_at_mut approach. Clever.
TiKV is a distributed kv engine with transaction. TiKV can work with TiDB to compose a NewSQL database. For more information, please refer to: https://github.com/pingcap/tidb https://github.com/pingcap/docs
This may be a bit more niche, but using luajit you can also call the same library fairly easily! local ffi = require("ffi") ffi.cdef[[ int32_t count_substrings(const char* value, const char* substr); ]] local lib = ffi.load("../target/debug/libstringtools.so") print(lib.count_substrings("banana", "na")) Just thought I'd share
I assumed you meant an email client as a desktop app. If you want web technologies for that, this generally means shipping a browser with your app. This is how Atom and a bunch of apps work.
I'm continuing work on [Kinder](https://github.com/KitFreddura/Kinder)! Class has been pretty busy lately so it's slow going, but I'm working on adding a Transversable trait (so currently I'm working on a Foldable trait), and then I was going to work on Profuctors for Maps! Github user aoprisan is working on a fork with a lift2! macro for types like Result, which is very exciting!
So if I understand this correctly, when returning an object from a function, rust will default to copying the memory of the return object. If we the struct has copy implemented, then it will instead use the copy method to create a copy of it, similar to how C/C++ does return by value. I am assuming this is not specific to functions and it would also apply to returning from a block? 
I've always wondered why anyone would put this kind of newsletter popup on their page. It pops up before you've read the article so why would you sign up for the newsletter of a site you don't even know? I see the point if annoying ads. You earn money with them. I see why you'd put a big "sign up for our awesome newsletter"-box on the bottom of an article. But these popups are annoying for the user and I feel like the site loses visitors because of them.
The copy does get elided with beta/nightly in release mode, at least with the playground link you provided.
Nice work!
Those usually cause me to leave the page before even seeing the content.
What about more complicated expressions? Is it the compiler's responsibility to analyse the following to see that the last case will never be hit? let x = match (x) { x if x &lt; 7 =&gt; 0, x if x &lt; 200 &amp;&amp; x &gt; 2 =&gt; x, x if x &gt;= 8 &amp;&amp; x &lt; 150 =&gt; 50, x if x &gt; 140 =&gt; 100, x if x &gt; 120 &amp;&amp; x &lt; 220 =&gt; 120 }
Your "atom feed" isn't really an atom feed btw.
It's really not. The `Drain` iterator is just designed to clear the map on-drop, so you have to prevent that somehow. Passing it to `mem::forget()` is the easiest way of doing that. What kind of type are you copying? If it's only a couple integers, a copying operation will have negligible cost.
Or in my case, I remembered that the website had the popup from last time I read an article on there, and didn't even click the article to read it...
&gt; But dropping vec is linear anyway :P If T isn't Drop, isn't dropping the vector just a dealloc/free of the underlying storage, which depending on the allocator and allocation size may just drop it back into a freelist?
Two comments: 1. If profiling reveals it to be a problem, I'd give up control of allocation. That is, permit the caller to pass vecs used to store the alignment (perhaps by creating a new `Alignment` type). The caller could then reuse that allocation on subsequent calls. In my experience, alignment algorithms like this can be called a lot, so saving allocation might be worth it. 2. A more lofty goal would be to support constrained dynamic programming. That is, if the caller only cares about alignments within a particular distance, you don't actually need to compute the entire matrix. You can stick to somewhere close to the diagonal.
Well if it's `Copy` you might as well do `vec[i]`---I think types that are neither `Drop` nor `Copy` are quite rare (`Range`-s come to mind), but let's not open the `Pod` vs `Copy` can of worms :P
There is also flatbuffers. Compared to capn, it is more widely used and stable. Usually a better choice.
I've been working on a user interface library for Rust, which currently involves writing the rendering layer in such a way that it's really easy to both create new primitives and compose new types out of those primitives. Yesterday, I finished refactoring the code in a way that makes it significantly easier to do that. The best part is that it's going to be super easy to get custom derives working so that new widgets can be created without much effort on the programmers part. Currently, it can draw text and a few colored primitives, and nest those primitives inside of each other. I'd like to create a few gradient primitives in the next few days, add image rendering, and when I'm done with that I'll probably be able to start working on creating interface primitives (e.g. Buttons and drop-downs) and getting user input to interface with those elements. 
I would say that moving the allocation out of the fuction is premature optimisation. The cost of allocation is usually constant and independent of size, so it should be negligible compared to the O(nm) algorithm. I would definitely flatten the `Vec&lt;Vec&lt;&gt;&gt;` into one `Vec` though, so the cost of allocation become in fact constant, getting better performance and cache locality as a bonus.
Thanks for these detailed replies. I guess what I really want to do is reference the trait bounds of the `Contains&lt;T&gt;` in a generic function such that fn run_really_in&lt;k: T, S: Contains&lt;T&gt;&gt;(k : T, s : &amp;S) { s.really_in(k) } where T is the same trait bounds as the `S` being passed in. Is it possible to do? fn run_really_in&lt;k: K, S: Contains&lt;T&gt;&gt;(k : T, s : &amp;S): where trait_bounds_subset(K,T) { s.really_in(k) } The use of a `2i32` was an example precisely because I couldn't make it generic enough. I will play around more with the structures you have provided. Thank you. 
Not the author of the site, but it looks valid to me. Perhaps it changed in the last 6 hours. The triple slash'd URLs do look a bit weird though.
See e.g. [`catch_unwind`](https://doc.rust-lang.org/std/panic/fn.catch_unwind.html), and [this related entry](https://doc.rust-lang.org/nomicon/unwinding.html) concerning errors in the 'nomicon.
&gt; And it's not necessarily constant time because of zeroing. If you're talking about zeroing done by the allocator, I don't think it's a case in Rust (at least when using jemalloc). I made a [test](https://play.rust-lang.org/?gist=b42e1a69933f697b7fd4f3f6d8ff35f5&amp;version=stable&amp;backtrace=0), (which may not prove anything, but I'd expect unoptimized run not to outsmart me). &gt; Based on my experience (not only with text search, but with Needleman-Wunsch specifically), removing the allocations can help, especially if you happen to be aligning short sequences (something I've had to do before). My experience is quite opposite :) I've implemented recently the local alignment algorithm (only computing score though, so just two rows of allocation), and allocation didn't show in the profile. My sequences were quite long though (~100 aminoacids each in average). &gt; It's pointless to guess at the performance impact of allocation without a profile. You're totally right! But what I'm suggesting is not to ignore allocation penalty without profiling, but rather to start profiling with a simpler (allocating) version in order to not waste time on moving allocation to the caller when it doesn't make a measurable difference. Anyway, I think the best tactics for the allocation would be to have inline buffer for small alignments, and allocate for longer ones. But that's just a random idea.
That's a nice list! One entry looks highly inaccurate to me: &gt; tomaka/glium — Graphic stack for Rust. Also, the Russian-speaking community has more channels. The game-dev one, in particular, is very popular: https://gitter.im/ruRust/gamedev 
Something like [Rustyline](https://github.com/kkawakam/rustyline)?
Not the OP, hope this comes across as sincere: do you have a source for that? Anything I've read in the past year has been pretty quiet about which is preferable or more widely used. Personally, gRPC foregoing flatbuffers for protobufs which gives me pause (about choosing another Google product that may not be maintained). That said, I don't rely on either technology at the moment.
Yes, that looks just like what I was searching for. Thanks for the fast response!
Capnp has not seen a release in a year as they only concentrate on their own use in sandstorm.
_(Disclaimer: not the author of Ruru, but I've contributed a bunch of features to it. Also, I've been watching this space since before both projects were announced - back when Helix was named turboruby.)_ The main features in favor of Ruru over Helix are: * Better documentation * More active development (there has not been a commit to Helix since 6/24, and there is still no crate for it) There are other small implementation details which I personally prefer (for example, not having to ship any [C code](https://github.com/rustbridge/helix/issues/2#issuecomment-226987158)).
I think the [`retain`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.retain) method does what you need here.
[deleted] ^^^^^^^^^^^^^^^^0.1041 &gt; [What is this?](https://pastebin.com/64GuVi2F/62599)
I'm tempted to propose reading these slides instead of a paper.. It's quite nice, if probably better with a speaker explaining in a little more detail \^\^
Damn - that ... is ambitious. Sounds cool, and - best of luck!
The other answer is good, and sorry for the late response, but I will add a bit. As the other commenter said, preludes are used exclusively as a 'glob import this' module. The reason we need this is because of Rust's trait system. I will use Diesel as an example. In Diesel, to call `execute(&amp;conn)` (i.e. `users.filter(...).excute(&amp;conn)`), we must import `ExecuteDsl` from whatever module it is in. For most every method on tables in the DSL, there is a separate trait. In user code we would never see '`ExecuteDsl`' written anywhere, we would just be using the `.execute` method, so importing it seems really weird. To alleviate this, preludes are used to house a bunch of traits (and some other items) so that way we can use the traits without having them appear in the code seemingly randomly. This is the only argument that I like for glob imports. When you use globs to import it hides what you are adding. Making it harder to know what you are importing is not good for code readability IMO. Diesel's prelude seems to include some Enums which I do not really agree with, but whatever, no one is perfect :P Hope this made sense, I am just procrastinating between classes right now.
According to [Feed Validator](http://www.feedvalidator.org/check.cgi?url=http%3A%2F%2Fthis-week-in-ruru.org%2Fatom.xml), it's valid.
While we are sharing, the FFI in LuaJIT was the inspiration for [cffi](https://cffi.readthedocs.io/en/latest/) in CPython, PyPy and Jython.
You haven't missed the difference between clone and copy here? Many types that are not copyable are still clonable: let key = t.keys().next().unwrap().clone(); let value = t.remove(&amp;key); 
I always like seeing very well documented and organized libraries on a topic I have no knowledge about. Very often they become the defacto implementation because of that advantage. Thanks for this. I also am taking a look at the code used for zero allocation parsing as an example for another project I want to write.
We're using it too, only for Boost though. It really was / is a mess. But why didn't they fix it instead of creating yet another imperfect solution?
&gt; Another fun project would be to write a pure-Rust unwinder, though there's not as great a need for it in Rust since Rust uses the system unwinder. I have plans to make such a library, but there's a couple things still to do: * Right now, `gimli` only does `.debug_frame` unwinding, but almost everything uses `.eh_frame` on x86, instead. Luckily, `.eh_frame` is almost exactly the same as `.debug_frame`, just with a few little tweaks. * Need to support (possibly outside of `gimli`) ARM's unwinding format, which is distinct. * On Windows, I'll likely just use the system libraries, (eg `StackWalk64` and all that). Need to see if the various winapi system crates already expose wrappers for this or not... &gt; A neat and relatively simple application of this would be to give the backtrace crate a compile-time option to use this instead of libbacktrace. libbacktrace is one of the few remaining components of the Rust runtime written in C. This would be super cool, once we have a proper unwinding library.
&gt; Do you have any benchmarks? It would be interesting to see where you stand vs the competition, especially since libdwarf has recently gotten some performance tuning recently. philipc did some DIE traversal benchmarks between libdwarf, libdw, and gimli, and gimli performed better than the competition. That said, this may have been before the performance tuning you mention, and it was also a one-off sanity test kind of benchmark, not super rigorous. Going forward, I'd like to do some proper benchmarking. &gt; Anyway, looks cool! It would be interesting to get a debugger written in Rust using your library :) This would be very cool :)
I notice you've been posting many of these code review requests these last few days, and I'm not sure this is the best place to do so. Perhaps it would be better to have a single post (perhaps a sticky one?) where people can request code reviews in the replies, just like the weekly easy question topic, or perhaps a rust focused code review subreddit? I would like to know what other people think about this. 
I feel a little silly having to ask this, but is there a way to check the version of an installed rust binary? So say I just installed clippy with 'cargo install clippy', what would the command be to check the version of clippy that's installed?
You likely should just use `nom` for general parser stuff -- we have a lot of funky things going on in DIE parsing + traversal that made it so that `nom` wasn't worth it for us, but I think it is probably a better choice than hand writing a parser in most cases because you get so many little building blocks already implemented for you.
To make it interesting, you should write what you think about the current code, what you would like to improve, and ask some questions.
Thanks for the pointers! RefCell was just what I was missing. Definitely it's more user friendly to hide the connect from API thats on the TODO list ;-)
It's a little unfortunate that the QotW is something that's (mildly) negative about someone's work (even if that work wasn't perfect/was itself negative!), rather than a quote that only celebrates something or is funny/informative without negativity. Maybe we could adjust it? Thanks for doing the regular TWiRs.
Missing tokio-rs!
That (albeit optional) heap allocation removed from the parsing logic looks like a nice win in both struct size and readability.
It's hard to discern what you're actually looking for. It doesn't really help that you're kind of getting your type parameter syntax a little mixed up. Try to describe in plain language what you're trying to do. Do you: * Want to take any set type that contains one specific element type, or, * Want to take any set type that contains an element type which conforms to some trait bound, or, * Want to take any set type and any element type, and check if the former contains the latter, or, * Something else entirely? 
I started a rust code review Subreddit just in case, but it has never had a post. If this becomes a huge issue, we could try to make it a thing.
I'm not sure it can be called a huge issue at this point, but OP [has](https://www.reddit.com/r/rust/comments/52x2y0/zm_number_system_in_rust/) [made](https://www.reddit.com/r/rust/comments/52y12o/zm_number_system_in_rust_version_2/) [six](https://www.reddit.com/r/rust/comments/53ht3v/graham_scan_from_algorithms_in_a_nutshell_in_rust/) [other](https://www.reddit.com/r/rust/comments/53i6r6/divide_and_conquer_approach_in_search_for_max/) [posts](https://www.reddit.com/r/rust/comments/53ji3v/divide_and_conquer_approach_in_search_for_max/) [recently](https://www.reddit.com/r/rust/comments/53ms49/needlemanwunsch_algorithm_in_rust/), and a total of 5 is the last day. If two or three people start posting the same way, soon most of the new topics on /r/rust would be links to codereview.stackexchange.com. I actually like doing code reviews, but I would prefer to keep them from drowning out other content, especially when they concern small single-function algorithms, rather than bigger applications/libraries.
That's pretty normal for an Atom feed, particularly if it's created by a static site generator (like it is in this case). So long as either the `Content-Type` is set to `application/atom+xml` or the feed reader has content sniffing capabilities (and most do), it should be OK. _(Source: I learned way too much about feed syndication in a previous development lifetime.)_
I didn't. That said, I'm pretty happy with what we've ended up with.
Yaay it is out! Now I have to fulfill my promise to contribute X(.
It was perhaps a slightly more negative week than most, with two "here's a bunch of problems I encountered using rust" posts at the top of /r/rust. How much should we edit ourselves?
Take a look at our FAQ: https://github.com/Microsoft/vcpkg/blob/master/docs/FAQ.md#why-not-nuget [disclaimer: dev on vcpkg]
Hah. It is a bit of the shame that one time that I got quoted it was a negative comment. Just to be clear: I meant no disrespect to the author of that article - I was just trying to be funny. I do think the tone of that article was misguided, but I guess it's OK to be disappointed that Rust is different than what you expected it to be. Rust community is very... well behaved and eager to accept critique, to vote that article to the top of r/rust, and then put it on the top TWiR. Anyway, I'm fine with replacing my quote with something more positive. :) Edit: And thanks for putting rdedup there. I'm very proud. :)
I was just trying to show that we can laugh at ourselves as that quote criticises TWiR (well at least that's how it looked to me). But after reading your comment, I do see how else that quote can be interpreted. Should I remove it? **UPDATE:** Removed QotW.
Now that I read again, you indeed meant _crate_ and not _quote_. My bad, sorry.
Can anybodu explain to me what likely/unlikely does? 
Hints to the compiler that the condition is likely or unlikely to occur. Processors will actually attempt to predict the result of a branch (if/else, etc) before it knows which one for sure. If you randomly generated a number between 1 and 100 and then were checking if the result was less than 95, approximately 95% of the time the result will be true. You can tell the compiler that and it can take it into consideration when compiling the program.
&gt; * [compiler-rt is dead, long live compiler-builtins!](https://github.com/rust-lang/rust/pull/35021) So happy to see progress here! Another step on the long road towards building rustc with Cargo. :)
Though one has to be careful. There's good evidence from Linux kernel experience that more often than not these do not help. Other sources: http://blog.man7.org/2012/10/how-much-do-builtinexpect-likely-and.html?m=1
&gt; I'm still hoping for a paper that captures the combined awesomeness of Rust in particular Not a paper at all, and not quite well-written either (and a shameless plug for linking my own work): https://www.reddit.com/r/rust/comments/4l4wjl/a_report_on_regions_and_linear_types/ (the discussion there and the one linked to from there might also be interesting). Good luck!
Some sample output would be nice as well!
I’ll be improving the DNS resolver part of the [domain](https://github.com/partim/domain). I found out that some home routers (in particular a certain line quite common here in Germany) don’t like requests multiplexed when on TCP. So I decided to completely re-implement the part that talks to the servers and make it much more flexible by allowing it to select a mode for each server and transport type: multiplexing requests, sequential requests, or entirely new connection for each request. Once at it, I’ll make the bit that dispatches messages (aka ‘the service’) generic over the underlying transport and will have to figure out a transport abstraction that works for both datagram and stream sockets. Hopefully, the design will be a lot cleaner as a result.
C++ support for NuGet supports source compilation, for example Boost.Python does this.
The missing links ;) Repo: https://github.com/dpc/slog-rs Crates.io: https://crates.io/crates/slog Crates containing the word slog (adapters mostly): https://crates.io/search?q=slog
I think those posts are incredibly valuable; they shouldn't be edited away.
Do you have any recommendations for people who also would like to create lazy, zero-copy libraries?
You need to change the definition of is_first to be the following: fn is_first&lt;I, S&gt;(item:I, searchable:&amp;S) -&gt; bool where I: PartialEq, S: Searchable&lt;Item = I&gt; { If you don't do this, it will try to match the generic S as "&amp;Searchable" instead of "Searchable".
It is "automatically" implemented when someone has done a `impl&lt;'a, T: Trait&gt; Trait for &amp;'a T { ... }` somewhere.
In practice, I find rustup too useful compared to installing Rust through a package manager.
I just adapted the existing macro to it: https://github.com/m4rw3r/chomp/blob/7b39f8a5a6ea70cf983c51ed01f03dbe3bbd7cc3/src/macros.rs#L415-L559 Requires that the type itself has `bind`, `or` and `skip` methods with the correct signature, but otherwise it should work for most monads which work with moving closures :)
For developing with Rust rustup is clearly better, but having it included in the distribution is much better for when you simply want to *use* some software made with Rust.
Sorry, I wasn't clear. The code can be fixed a few different ways: either like you say, by passing the value instead of a reference, or by implementing the trait for `&amp;Empty` as well: impl &lt;'a&gt; Searchable for &amp;'a Empty { type Item = u32; fn item_at(&amp;self, _index: usize) -&gt; Option&lt;Self::Item&gt; { None } } My question was about why the trait isn't automatically implemented for `&amp;Empty`, as it is in this example: https://is.gd/7MYWw1 
That is because of [deref coercions](https://doc.rust-lang.org/book/deref-coercions.html).
But how exactly will that help? You'll still have to check the result though wouldn't you?
Not trying to be rude, but do you care enough to go ahead and suggest substitution quote?
Because of autoderef.
You do not, but any Rust software needs the compiler to be packaged in Fedora before being accepted itself. Every package must be build-able from scratch only using other packages that are already available.
Ah I see :)
That's right
Putting poor Java in the middle of the (safety/control) line doesn't give it justice. They should have used a 2 dimensional graph. Safety î | Rust | Java | Python | C++ +-----------------------------&gt; Control 
&gt; Release early Also because crates.io is first come first serve. I am going to release my crate in 0.2.0 within this year even though it is FAR from finished (have to figure out how to release multi-crate-multi-root-projects on crates.io, though). I really hope that some contributors will become aware of the project then! :-)
 $ cargo install --list
Web standards nerd note: this is exactly what the `+` is about in the media type: it means "hey, this is `application/atom`, but if you don't understand that, feel free to interpret it as `application/xml`. This works because all atom documents are valid XML documents.
Java has static types, Python does not.
This looks great! We may want to use this at some point to directly test the DWARF we are emitting (as opposed to just how debuggers interpret it).
&gt; rustc helloworld.c Muscle memory! It will reconfigure in just a short month or so.
It's worth being careful about simple benchmarks like the one in the post you linked. With few branches to look at, the CPU has a better chance to be good at predicting. With a more complex program, branch prediction might behave differently.
Lots of effort is going towards a GUI from scratch idea. orbtk, conrod and others. The problem with that is GUIs will never look native. It also means it will take a long time to be stable, especially since they're on top of opengl. Going at it the other way around, by building on top of native widgets (gtk, cocoa, winapi) is a huge task and requires good knowledge of all platforms. But the payoff would also be massive. Just look at how quickly libui got popular, even though it is in very early stages (and progressing slowly). Building something like that would most likely even attract lots of newcomers to the platform as there is a real need for such a toolkit. A third option is to build on top of servo tech (webrender). Using web technologies to build desktop apps has some merit, unfortunately the execution ATM is awful. Those apps are huge hogs. 
Java gets beat up so much. Maybe it's Oracle's poor maintenance over the last 6 years? Could it be that it's "too object oriented"? I dunno. It doesn't feel right to hate it so much.
All of this completely comes apart under scrutiny. Java is pretty unsafe in that people tend to have NullPointerExceptions. Python also allows every variable to be None, but somehow people tend to be more diligent checking for it. Maybe because the dynamic typing always forces you to keep the types of things in your head, while in Java, the compiler/IDE tells you about type mistakes except for null.
For me, a problem with a toolkit like 'libui' is that it makes it really hard to do anything custom. If you have a widget that is not in the set of native controls provided by the OS, you'll have a heck of a time trying to implement a custom control across three platforms. Doing all the painting in the UI toolkit seems very complex too, but there are libraries that can help with that. Libs like Cairo or Skia (or indeed Servo?) could be used to do the actual drawing. This doesn't tie the UI library to a single backend (since Cairo/Skia can render to OpenGL, PDF, etc) but would still benefit from the massive engineering effort that has already been done for those libs. By the way, separate from the rendering aspect: I took a brief look at Google's Flutter a while ago. Its designed to write Android/iOS apps in Dart (one of its low-level libraries is in C++ though). But the concepts are quite nice, similar to React in Javascript. Basically you create your application UI by composing a tree of widgets. A lot of these widgets are custom, but are basically just hierarchies containing more (non-custom) widgets. Any change in state of those widgets cause that part of the tree to be re-rendered. I think Conrod uses similar ideas?
I like the idea of a community curated GUI crate, but I think that at this stage it's good to have several independent GUI crates exploring different ideas before people agree on the preferred design (if there is one to agree on). Also there's certainly going to be a non-reconcilable split between the people who want native-looking GUIs and the people who want more a modern and/or flexible design. I don't think there will be a one-size-fits-all solution. That said I'm happy to see the growing interest in GUIs lately!
&gt; For me, a problem with a toolkit like 'libui' is that it makes it really hard to do anything custom. If you have a widget that is not in the set of native controls provided by the OS, you'll have a heck of a time trying to implement a custom control across three platforms. From what I understand of Cocoa, it has native widgets. Then when you want to create your own, you have Core Graphics to do it with. So native widgets with something like Cairo/Skia for custom controls gives you everything.
All GUI libraries I've used make use of virtual methods and inheritance. GTK even implements them in C. If you're not using to use OOP style widgets, then I'm not clear on what you are going to use. And if you're not using to use OOP widgets, then in many ways this is a research project exploring something that hasn't been done successfully before. * What will take the place of the widget inheritance hierarchy? How do I define a new widget? * How will the widget graph work? The parent/child relationship, the tree or DAG that they form. * How will the memory management work? That's probably closely tied with how the widget graph works. * How will events work? There's lots of examples to look at. GTK, QT, wxWidgets, Swing, SWT, Windows Forms. There's also whatever OSX and iOS use. I don't know much about them, but I know they're usable from Swift.
I like the idea of using WebRender as a backend. It's a really neat piece of tech, I encourage the graphics enthusiasts around here to look at it. Also it's in its own crate and doesn't force you to use the whole web browser.
Well, since vcpkg currently builds libraries from source, the ABI will match since your code is compiled against the same version that was built. However the API can change and cause your code to fail to compile (or even worse do the wrong thing if the behavior changed without the API itself changing). On Windows every application either statically links the non-system DLLs it needs or vendors them, so DLL hell doesn't really occur.
That would be super cool!
Qt is moving away from the stricly OOP-style widget model with QML/QtQuick. Also, UIs on the web (with and without frameworks) aren't generally object-oriented either. I personally don't like the OOP-style approach to GUIs. I believe we can do better with an approach akin to the entity-component systems you can find in modern game engines.
In 100% safe Rust code you'll never see a Null Pointer. In 100% safe Java, you can see a Null Pointers. The real argument boils down too. *Are Java Null Pointers the same as Rust Null Pointers?*. The answer is yes. This is an invalid reference to *something*. Java makes handling this easy (e.g.: try/catch/final block). While Rust needs to handle SIGSEGV. 
No; [it's an issue](https://github.com/rust-lang/rust/issues/28828).
I was actually just thinking today I hadn't seen one of these articles from you in a while! Thanks for posting. 
Maybe like this? struct Worker { data: Arc&lt;Mutex&lt;Vec&lt;i32&gt;&gt;&gt;, thread: JoinHandle&lt;()&gt;, } impl Worker { pub fn new() -&gt; Self { let data = Arc::new(Mutex::new(Vec::new())); let thread = Self::spawn_work_thread(data.clone()); Worker { data: data, thread: thread, } } fn spawn_work_thread(data: Arc&lt;Mutex&lt;Vec&lt;i32&gt;&gt;&gt;) -&gt; JoinHandle&lt;()&gt; { thread::spawn(move || { loop { let mut data = data.lock().unwrap(); // Do some work on data } }) } }
And of course this could be applied to any other OS, but WINAPI iis the api I have the most knowledge with.
I like it. The only downside is the function signature would be huge with a few more parameters. In addition to your changes, I was thinking something like this would probably make more sense too. impl Worker { pub fn new() -&gt; Self { let data = Arc::new(Mutex::new(Vec::new())); let thread = thread::spawn(Self::work_task(data.clone())); Worker { data: data, thread: thread, } } fn work_task(data: Arc&lt;Mutex&lt;Vec&lt;i32&gt;&gt;&gt;) -&gt; Fn(()) -&gt; () { move || { loop { let mut data = data.lock().unwrap(); // Do some work on data } } } }
To be hones Qt and it QML does serve a good job – at least for me – for having native-looking GUIs and be absolutely flexible in the design. I can have a boring looking form and make some cool effects an the components like earthquake like shaking if one types on an input field .. animate layout changes if one checks a field and the form changes etc. I am really a fan of whats possible with QML. Sadly there is no real binding for rust. we have https://github.com/White-Oak/qml-rust and/or https://github.com/cyndis/qmlrs from which you can somewhat comfortably use QML (with signals/models etc) but you totally lack the possibility to interact with the C++ that you sometimes need e.g. QSyntaxHighlighter to fully use TextAreas or you need to "externally" activate the Webview with QtWebView::initialize() on the other side there is something like https://github.com/rust-qt but i think it lacks the possibility to connect signals ans slots (never really tried that and don't really know if its working at all) if we could have Qt/QML in rust – i would be really happy :D 
Update: I've added the link to the design (lol).
Fixed (facepalm), thanks.
How about allowing for the creation of GUIs using a markup language as GTK3 does with Glade UI files? Drastically reduces the amount of code required.
I love it when people experiment with creating GUI libraries, especially ones that can create native windows GUIs. I wish you the best of luck, and I really hope your library turns into a complete product that other projects can use.
What do static types have to do with memory safety?
I seem to be having a problem with getting the length of a mutable array. let mut array = gen_array(50); println!("{}", array.len()); This complains with "cannot borrow array as immutable because it is also borrowed as mutable"
Yeah, conflicting file extensions are a problem. They will probably be fixed in the next version where you can set and unset extensions. There quick fix is to just exclude `.f` files. ``` $ tokei . -e .f ```
You can actually return `Iterator`s without summoning one of the Great Old Ones now, which is pretty cool.
I want to personally shout out to /u/m4rw3r's "chomp" parser combinator experiments. Rather than making data types and implementing some kind of parser trait on them, impl trait lets him just combine functions together, and LLVM just makes it as performant as the data type version. That's awesome.
Great post! I'm wondering about disabling all those features. Is it *impossible* to write this kind of low level code with, for example, the multimedia registers enabled? Are they just disabled for the lowest level code? 
Maybe I'm just missing it in the documentation, but is it configurable via a config file, environment variables, etc? The only runtime configuration I see is with signals.
He actually explains this: &gt; There is a special instruction to do this: [fxsave](http://x86.renejeschke.de/html/file_module_x86_id_128.html). This instruction saves the floating point and multimedia state to a given address. It needs 512 bytes to store that state. &gt; &gt; [...] &gt; &gt; However, we won’t do it that way. The problem is the large amount of memory required. We will reuse the same code when we handle hardware interrupts in a future post. So for each mouse click, pressed key, or arrived network package we need to write 512 bytes to memory. This would be a huge performance problem. For the *absolutely lowest* level code yeah, but you could, say, compile the exception handlers with SSE and MMX disabled, compile the rest of the kernel with SSE and MMX *enabled*, and then link them together. But I doubt that would gain you much performance and it would make your compilation much more complicated. Not to mention: &gt; [...] most kernels do it this way (including Linux). ---- *I have no understanding of X86 CPUs and am just going off of what Phil wrote in the blog post*
Mainly you can return unboxed closures! Here's a funny one: fn curry&lt;A, B, C, F&gt;(func: F, arg: A) -&gt; impl FnOnce(B) -&gt; C where F: FnOnce(A, B) -&gt; C { |b| func(arg, b) } let add = |a, b| a + b; let incr = curry(add, 1); https://is.gd/Cu1IDk The more general form would unfortunately require variadic generics.
I have ideas here... If only I was Clone. Sigh.
Absolutely. The usual argument is that Rust can't add much here, so there's no real reason for a re-write. I still think it would be interesting, just repeating what I've heard.
How is gen_array defined?
QML's immature support for native widgets aside, I've always felt it was a very un-rust solution, given that the Qt Quick compiler necessary for compile-time verification and best possible performance is a paid proprietary add-on.
It would be really cool if we had direct access to Servo DOM from Rust application code, so we could apply all the experience we have from building JS UX to Rust. Kinda a pure Rust version of the Electron concept, hopefully with much better performance.
`libui` doesn't work on mobile platforms, which is a non-starter for me. (I guess I didn't put that requirement in the goals section, my b). I value cross platform-ness more than native-looking-ness, so using platform specific APIs wasn't considered tbh.
Yeah, I meant without doing it the "bad" way. 😉 But I take it compiling the kernel without these features doesn't make stuff running "on" the kernel lose the features? Like, the kernel can still run alongside code that uses the extra registers
While we always recommend the latest stable version for C/C++ developers, we use `git` to enable rolling back part or all of the tree. Essentially, instead of specifying the library version in the "install" command, we specify the library version in the portfile on disk. I'd love to have a more in-depth discussion on either rust's or vcpkg's GitHub. [disclaimer: dev on vcpkg]
Any. Each `Drain`implementation can be driving by any logic desired, potentially reusing other `Drains`s. See: http://dpc.pw/slog-rs/slog/trait.Drain.html#tymethod.log Eg. https://crates.io/crates/slog-envlogger is controlled via `RUST_LOG` environment variable just like `env_logger` was. Anyone could implement a `Drain` that would be controlled via file, or web gui, or something else. Right now mostly more generic drains are available, but their number will grow with time. One easy route is to take logic from other logging implementation and port it on top of `slog` crate as a `Drain`
On crates.io, `rust-bindgen` is yanked -- was that crabtw's? And plain `bindgen` points to Yamakaky's repo. Is servo's a continuation from there? A permanent fork? In other words, which one should I be using?
Ahh I see, that makes sense. I thought "disabling" them implied using them later on would be tricky, but it's really just about telling the compiler to not use them for this specific compilation target. I was thinking, what if you hit an exception in userspace code that used the extra registers/instructions, or the red zone? 
OK, in the PR comments I see this reference from Yamakaky: [merge into upstream! #21](https://github.com/servo/rust-bindgen/issues/21) So I guess this is under discussion.
"Hey Firefox devs (sitting next to the Servo devs), we built all this awesome safer and faster web tech!" "Awesome, let's hook it up to FF!" "Oh, gross, SpiderMonkey? We only work with Chakra." "..."
In addition to the other comments asking for the `gen_array` definition, is there anything between the `let mut array` and then `array.len()`? Can you create a rust playground link demonstrating the program?
I think that's a little harsh. I'd say Python's more like a shaped explosive device in that you have to be *extremely* careful to make sure it's a useful tool and not a death trap, but sometimes it's the quickest way to get through a wall.
Ok, this is more fair to the language. :) 
I think this is sort of a weird argument, this assumes most critical bugs in JIT/VMs comes from bad generated code, which isn't necessarily the case. I would need to actually look into the data, but my feeling from working on Spidermonkey is that bad JIT code is probably below 50% maybe much less. I think writing a JS VM in rust would be beneficial in the long term, but it's not an economical project for at least Mozilla. We already get the benefits of a safe language (JavaScript) by self-hosting more and more built-in functions. I think another big thing to consider is how to make high performance GCs work well with rust. Rust definitely needs a good answer to this before doing such a project otherwise you will probably have to rewrite a lot of code. I mostly thinking of features like stack rooting, marking, barriers etc. A totally safe GC would be awesome for reducing security bugs, although unrealistic.
Yes, un-rusty but really not a problem. When I develop qml stuff, the runtime verification doesn't bother me much at all. And performance is good enough to run the GUI nicely on a phone.
Do you see any way to work with [libui](https://github.com/andlabs/libui)?
In fact, crabtw gave me the rights on the repo ;)
I'm not familiar with libui, but with a quick glance, I can't think of a way to create a library like it that will be both safe and clean to use with rust (if it's what you meant). 
SkPicture (Skia's recording target) is nice although it is tied to Skia's rendering model, so the backend that receives the commands is somewhat forced to follow the same model since it receives rather low-level commands. I prefer WebRender's higher level display lists and how it is modeled after CSS properties (makes sense for something designed to render Web content) and does not impose a specific rendering model to the backend. I'd actually prefer a fully retained scene-graph, although WebRender also does have some retained elements like images. Scaleform has a very nice multi-threaded 2d scene-graph, there's a pretty cool Siggraph presentation about somewhere on the web but I don't have it handy (though, their scene-graph doesn't have this serialized and easily debuggable command list property that SkPicture recordings and WebRender displaylist have) .
TWiR Quote of the Week material right here.
Who all is involved in packaging Rust for Fedora? Do you have contacts with the Rust developers to keep them apprised of potential roadblocks?
Thanks so much to all the Fedora folks who made this possible! Now we need to get started on rewriting little bits of Fedora in Rust. :P
Well done! Love seeing crates that take the effort to refine their APIs and strive towards 1.0.
As a rule of thumb, even with large refactorings, you should always do them incrementally, with small steps that completely compile. I don't know of any way to get the borrow checker to check borrows when the syntax can't even be parsed.
Where do I put crate-level documentation? It doesn't seem to work when I put it in lib.rs because it gets caught in the doc comments for my `mod panda;` declarations.
Right so JITs often use RWX memory and whatnot, so memory corruptions tend to be easier to exploit, but does that really matter for rust? Isn't it in fact a prime cadidate for rust? We've certainly seen exploits in V8/ *Monkey in the past, seems like a memory safe language could be useful, especially since hardening techniques tend to be difficult/ novel. What aspect of the JIT unsafety makes it harder? I think I've seen this said before but never considered it.
By 'native UI' do you mean those inconsistent, constantly changing kludges that everyone's favorite "jump the shark" OS's have been releasing? Give it a few years and nobody will even know how to recognize a native UI.
This is a cost-benefit analysis -- the benefit side is small, while the cost is huge. A JS interpreter isn't _that_ hard to write. An optimized one that can compete with the ones in browsers today is hard. It might still be worth it, since having default safety for the non-JIT-related bits is good.
Not exactly. Java NPEs are the same as unwrapping on None in Rust. The difference is that in Rust you can choose not to use Option when you know things should never be null. You can also choose to not use unwrap and handle errors in a nicer way (which most folks do). Java only gives you the second option -- you can explicitly handle NPEs if you want, but you can't avoid it entirely for a type. Annotations help. So, still unsafe, but not exactly memory unsafe.
i have, i guess. look at the line that i commented. is that correct?
I would classify QML as object oriented, but it adds convenience in that you don't have to create a class if you just want to create one instance. Instead you create an instance of the super class and modify(add or override stuff) it there on the spot. As for getting things done it is imperative programming in JavaScript, with added basic reactive programming that you can use to define object properties. The reactive programming is a bit limited, you can't rely on it for many scenarios. Instead you then need to write imperative code in event handlers.
This is amazing work. A NACL style post quantum crypto library. 
Yes, I made reference to libsodium. :D
Glad to hear it! :)
Yep that was my thinking. Rust would also provide some nicer syntax and compiler optimizations if designed right too.
wasm requires a streaming interpreter that reads and interprets code in batches. I'm not sure if this is a spec requirement or just a performance optimization, but there was a rust talk with someone who was working on it. Maybe you saw that?
You will still want to limit the amount of draw calls and textures, though. Will you cache all these intermediate textures?
If this feature is implemented, yes, until the next UI frame, ie each thread would have their own FBO texture+depth which is reused. If no UI frames are triggered then they would have the same contents. 
Keep bottles of tiger blood on hand.
What I was trying (but failed due to lack of time, Rust and WinAPI experience) is instead of an all-or-nothing approach to native UI factor the library in the following way: * have a multiplatform, but minimal core UI library. Useful for PoC or small projects, limited to lowest-common-denominator * have platform specific add-ons that integrate with the core and allows one later to specialize the app to various native environments (i.e. advanced widgets, deeper integration into desktop services, etc.)
Is the newhope key exchange able to calculate the shared secret offline (like ecc?). As I understood ring learning with error, it had to run online, but looking at the code, everything seems offline.
[deleted] ^^^^^^^^^^^^^^^^0.5255 &gt; [What is this?](https://pastebin.com/64GuVi2F/73317)
Maybe!
If you don't use floating point or sse in the exception handler there then there will be no problem at all even if you don't preserve the registers. It's the external code that's the problem. If you call another function from the exception handler then this may alter the registers as it likes because it doesn't know of any restrictions. And when it returns the registers will have changed and weird things will happen when you return from the exception handler afterwards. Another problem with mixing SSE-disabled and normal code is the usual calling convention where float values are passed and returned in the sse registers on x64 platforms. So what happens when you call a function that returns a float from the exception handler? You get either borked SSE registers or you'll get undefined behavior even if you preserve the registers because your exception handler doesn't know how to access the return value of the called function (I have no idea where it lands when you turn off SSE in LLVM on x64). So there are only two choices: Turn off multimedia registers altogether in the kernel or take the penalty and preserve the registers everywhere. This first option is usually the best choice as a kernel usually doesn't do lengthy and complex calculations. 
I'm thinking whether I need contact to the other person when calculating the shared secret or if I can perform this calculation offline with no contact.
newhope works like rlwekex, private key requires reconciliation data to generate the shared key.
Yes, we only disable SSE/MMX for the kernel itself. Userspace programs can still use the multimedia registers (and the red zone optimization). This means that we still need to save the huge SSE/MMX state when we _switch_ to another userspace program. However, we don't need to save it for short kernel entries. For example: * Userspace program A executes * A network package arrives and causes a hardware interrupt. * The CPU calls the interrupt handler of our kernel. * Our kernel handles the interrupt. * The interrupt handler continues the interrupted userspace program A. If our kernel used the multimedia registers, we would need to save/restore the SSE registers when the interrupt handler is called (because the kernel could overwrite the registers). With SSE disabled, we can safely avoid this overhead.
&gt; IPA(key): [ʃərkəˈra] It is a sanskrit, meaning sugar. https://en.wiktionary.org/wiki/%E0%A4%B6%E0%A4%B0%E0%A5%8D%E0%A4%95%E0%A4%B0%E0%A4%BE
The name was actually chosen after the [falcon](https://en.wikipedia.org/wiki/Perlin_(falconry\)) not the noise or the computer scientist behind it. I was not aware of them to be honest.
https://play.rust-lang.org/?gist=93b1a1f79f38c7deffddaa49dc8ebe9f&amp;version=stable&amp;backtrace=0
Replied to my comment
Replied to my comment
I tend to open with the standard sales pitch ie, "fast, safe, concurrent, pick three". One talk I did had that then it sort of veered into "Rust as a C replacement". Gave a couple of examples of segfaulting C functions vs compiler errors in Rust, then an outline of the runtime (ie that there "isn't one", at least no more than C/C++) and Cargo (because it's ridiculously awesome). Then a run down of potential applications, which for the Ruby/Python part of the audience was native plugins... Rust is such a big step forward in so many areas that you can really just pick one or two and gush for 20 minutes about how great it is.
When using the MSVC x64 tool chain. If I invoke the `gcc` crate in a cargo build script commonly `build.rs`. Does it use `cl.exe` or does it configure `gcc` to output MSVC compatible object files? The answer is Yes.
Surprising pronounciation. Thanks!
&gt; "fast, safe, concurrent, pick three". Hehe. Nice one! Will definitely go in my slides :) &gt; Gave a couple of examples of segfaulting C functions vs compiler errors in Rust, then an outline of the runtime (ie that there "isn't one", at least no more than C/C++) Ah, yes. I too was thinking along the same train of thought. &gt; Cargo (because it's ridiculously awesome). `cargo` is awesome but how do I demo this? Thanks for the reply! :) 
The world is full of name clashes, there's so much stuff out there and we manage. See also [Rust (disambiguation)](https://en.wikipedia.org/wiki/Rust_%28disambiguation%29)
Well, you could segregate which pool a component is place into, allowing us to only redraw those components which have changed. So, most of the intermediate textures+depth would be reused. But let's not prematurely optimize this. :) As a reference point, the web does just fine as is currently.
Nice to see you are actually pushing Rust in production! Looking forward to use `rouille` myself ;) As for the article, I think expanding on why you find the existing frameworks unacceptable would be helpful - with some concrete code and more detailed analysis of those frameworks.
I'd argue that this case is worse than Rust's. Google "rust lang", "rust programming", or "rust libs" and you'll get what you're looking for. If you google for "perlin library", you'll get libraries for generating noise. All I'm saying is that a little rename would greatly benefit the discoverability of this project.
Adding more features to [watchexec](https://github.com/mattgreen/watchexec) and still thinking a lot about the UI. Rust is so good for stuff like this.
GetMessage blocks. You may have it confused with PeekMessage.
I'd drop JS from the stack, and rather would prefer manipulating DOM with Rust if it could be made ergonomic. Sadly Servo does not expose any interfaces to manipulate page content, that piece being strictly encapsulated in the scripting module. This is something I hope could be changed in the future, I have some ideas myself on how to do it, but its fundamentally bit hard problem. 
This projects has been live for about 6 hours now. Of course you wont find anything on Google. There are [very few](https://en.wikipedia.org/wiki/List_of_information_retrieval_libraries) information retrieval libraries in general, so I do not really worry about discoverability.
That's because I have not yet implemented the actions for this (which is a trivial thing to do btw). I wanted comunity feedback before going further.
Only in python with tkinter. Haha
I though about this too. There could be two way: Implementing `ControlTemplate` on a custom widget and handle the event directly. That's unsafe though. Another way would be to create "Canvas" widgets ex: (GdiCanvas, GlCanvas, etc) and expose a a `draw` callback to the user. I don't plan roll up my own drawing wrapper though, so again there will be unsafe code.
&gt; As for the article, I think expanding on why you find the existing frameworks unacceptable would be helpful - with some concrete code and more detailed analysis of those frameworks. I didn't elaborate on that because it's been a long time time since I looked at them in details, and because other people have the same kind of problems that I have had. Take [Iron's example in their README](https://github.com/iron/iron#response-timer-example) for example. Looks a bit complicated with that middleware stuff, doesn't it? In rouille, it's just that (which I wrote without even looking at the docs): extern crate rouille; extern crate time; use rouille::{Request, Response}; use time::precise_time_ns; fn hello_world(request: &amp;Request) -&gt; Response { Response::from_text("Hello world") } fn main() { rouille::start_server("localhost:3000", move |request| { let before = precise_time_ns(); let response = hello_world(&amp;request); let delta = precise_time_ns() - before; println!("Request took: {} ms", (delta as f64) / 1000000.0); response }); } I really don't see any advantage in Rust in using a complicated system over just including the equivalent code in a single function. It *is* an advantage in Javascript with all that callback-based stuff, but it isn't in Rust. 
Regarding Rust itself: In addition to the usual "fast, safe, concurrent" points that others have pointed out, I have been trying to argue in [recent presentations](http://pnkfelix.github.io/presentations/rust-marcomms-agency-day-2016/slides.html#open-and-accessible-to-all) that Rust is breaking new ground in introducing systems programming to developers who have had no experience in the space. Regarding presentations: a big piece of advice I can offer is to figure out the 2 or 3 most important points you want the audience to hear, and make sure you cover them (at a very high level) at the very beginning. Then you can spend the rest of the talk getting into examples, or interesting details, or providing proof of the claims you made at the outset. But hit those points before diving in deep. (And, optionally, summarize them at the end if you have time. Some people would in fact probably argue that summarizing at end is just as important as doing as at the beginning.) One reason to do this: If you mispredict how much time you'll spend on various parts of your talk (e.g. if the audience is encouraged to interrupt you), then covering the high level points at the outset may be the only time that you get to it. (Another big reason to do it is that you want to show your audience the cool stuff up front, or at least hint about it, so that they are motivated to pay attention to what you are saying throughout the whole talk.) I have not always managed to follow the above rules, but the times when I haven't, I have usually regretted it. You can see slides and videos for my talks (most on Rust) here: http://pnkfelix.github.io/presentations/ and the source for most of them is on various branches of the repository here: https://github.com/pnkfelix/presentations/
Thanks. I guess I could try musl.
I really hope apple starts to make more idiomatic cocoa bindings for swift. Binding to idiomatic swift seems like a much simpler task compared to trying to graft objective-c into rust, given how close swift and rust's type systems are.
I think there are benchmarks in several of my crates, sorted roughly from less complex to really complex: [`byteorder`](https://crates.io/crates/byteorder), [`memchr`](https://crates.io/crates/memchr), [`csv`](https://crates.io/crates/csv), [`utf8-ranges`](https://crates.io/crates/utf8-ranges), [`aho-corasick`](https://crates.io/crates/aho-corasick), [`snap`](https://crates.io/crates/snap), [`suffix`](https://crates.io/crates/suffix), [`regex`](https://crates.io/crates/regex) You may also be interested in [`cargo-benchcmp`](https://github.com/BurntSushi/cargo-benchcmp), which lets you compare benchmarks across multiple runs.
&gt; let's not Fair enough :D
Do you happen to know if there's currently a (basic) example of running WebRender separately from Servo?
I am following the ArcadeRS shooter tutorial (https://jadpole.github.io/arcaders/arcaders-1-8) (I know it is a little outdated) and I'm having trouble with loading the background images. The method used for loading the background PNG files returns a None option. I know the path to the PNG file is correct and tried it with a relative and absolute path. If anyone could point me in the right direction it would be highly appreciated! pub fn load(renderer: &amp;Renderer, path: &amp;str) -&gt; Option&lt;Sprite&gt; { renderer.load_texture(Path::new(path)).ok().map(Sprite::new) }
Wrong sub. You want /r/playrust
Was that how early design decisions were taken?
You're running into the issue with borrowing and expressions. Right now, borrowing lasts for either a statement (for borrows in an expression) or a block, for borrows declared a blocks. There is a plan for 'non-lexical borrows' that will fix this. See [RFC-811](https://github.com/rust-lang/rfcs/issues/811)
&gt; It would be so much nicer to check the SQL code’s validity or even the data types at compile-time in order to avoid risking an error 500 if you make a mistake and forgot to write a test for something. Again, procedural macros could come to our rescue. Diesel does all of this. It'll even check against your DB schema if you give it a DB connection at compile time (using a proc macro). Even if you manually specify your schema, it still does a fantastic job of checking the SQL is correct.
I'm also a huge fan of japaric's [rust-everywhere](https://github.com/japaric/rust-everywhere) project. It has all the pieces you need to setup CI that produces static executables using musl and Github releases.
For what it's worth, when I was reading the title and saw "perlin" the first thing I thought was "oh cool, someone made a perlin noise implementation in rust"
Rust's not so bad -- it's one of the top results in DuckDuckGo. Try Googling for Go, and you won't find it anywhere unless you refer to it as Golang.
[rdxsort](https://crates.io/crates/rdxsort), features a whole set of performance tests for different data types with radix sort, standard lib sort and quicksort. You may want to use a subset of the benchmark set. Ah, and you need the nightly compiler since it uses the unstable `Bencher`.
Hard to know what the problem is here because the Ok() call is likely throwing away the error. Can you rewrite it like this: pub fn load(renderer: &amp;Renderer, path: &amp;str) -&gt; Option&lt;Sprite&gt; { match renderer.load_texture(Path::new(path)) { Ok(t) =&gt; Sprite::new(t), Err(e) =&gt; panic!("Error: {}", e) } } 
Thanks, I have not noticed this typo. By the way, I've just noticed that my OpenSSL benchmark is not correct. It does not give the same result as the other one. I updated it to use the `-nopad` option but I don't know how to specify the `-nosalt` options using the `openssl` crate. **Edit:** now it gives the following for the OpenSSL crate (which is more what I expected because it was around 5-10 slower according to my other benchmark): test bench_decrypt ... bench: 237 ns/iter (+/- 3)
I'm delighted to see stuff happening in Rust web land! I have two projects which might help with deployment: 1. [rust-musl-builder](https://github.com/emk/rust-musl-builder), which can be invoked as `rust-musl-builder cargo build --release` to generate statically-linked Linux x86_64 binaries using libmusl. It even supports OpenSSL, which was annoying to get working. This is used for actual production work. 2. [heroku-buildpack-rust](https://github.com/emk/heroku-buildpack-rust), which works with Heroku and Heroku-compatible deployment systems to automatically compile and run a Rust executable. This has some great recent updates from binarycleric. And as you can see in this thread, there are several other options for deploying Rust applications easily.
Why HC as symmetric encryption? That's a pretty unusual choice.
That's not even the point. JavaScript code needs to run in a sandbox, where it can't read arbitrary parts of your filesystem or look through your webcam without asking. Rust doesn't even make that kind of guarantee. Besides, there's no existing Rust JIT, or Rust GC. And rustc is not hardened for untrusted code.
&gt; I encountered a problem which is that the built executable couldn’t find OpenSSL’s shared libraries on the production server (yes, OpenSSL again). I abandoned this idea. Read up on RPATH and/or use patchelf
They made some things more "swifty" in Swift 3, but it's still not as idiomatic to use from Swift as from Objective-C.
[Procedural macros 1.1 RFC ➡️ stable serde and diesel](https://github.com/rust-lang/rfcs/pull/1681) [Macros 1.1 implementation](https://github.com/rust-lang/rust/pull/35957)
Maybe one way to do it would be to have two traits for Signal, where SignalImpl could be raw, and Signal could be parametric, such that for MemSignal and IpcChannel, you could specify that since it contains an item that implements Signal, then it implements signal through the instance that implements SignalImpl. `trait SignalImpl { fn await_signal(&amp;self); fn send_signal(&amp;self); }` `trait Signal&lt;T : SignalImpl&gt; { fn await_signal(&amp;T); fn send_signal(&amp;T); }` See, in the general case with rust, I can't specify that, for all such structs that wrap a particular trait implementation that the struct doing the wrapping should transitively implement the trait for which an instance it wraps implements. There's no ability to reference traits as a sort of meta-programming capability; I would need the ability to express inductively what I want, and I can't program the trait rules in order to do that. And at least one problem with this example though, is that there is no way to reach the contained instance that implements a trait through the type T. T is just a type, so I can't express that SignalImpl is implemented for all types that contain a type implementing SignalImpl. The next step is to add another struct and trait, as a pair, consider the `SignalImpl` trait to be the base case, and operate over the next pair as though it were inductive. `struct Signal&lt;T : SignalImpl&gt; { sig : T }` `impl&lt;T : SignalImpl&gt; SignalImpl for Signal&lt;T&gt; { fn await_signal(&amp;self) { self.sig.await_signal(); } fn send_signal(&amp;self) { self.sig.send_signal(); } }` But then I'm forced to wrap my types, `MemSignal` and others, in a Signal&lt;MemSignal&gt;, which causes some turnover. Is this the correct way to do it though?
I ended up just linking against the runtime library of my language (where said functions were already implemented in), had to call a function of it though to get rustc to actually link it in.
Yeah, right now that whole process is awful. I'm trying to get something working with Diesel and Serde, but it's not happy. I'm probably going to end up ditching diesel for now until it builds on stable without Syntex since my current schema is so simple (I only have a few tables with only a few columns each).
So you by chance know if there's a preliminary release notes for beta packages? It's a little difficult to know what is likely to land, which makes it hard to plan for. I suppose I could look through the logs for things mentioning release notes, but it would be sweet to see something a little prettier.
You can use "cargo build -v" to see how crates are being compiled. You'll have --cfg test on the binary that runs the test but not on the libraries that are dependencies of it.
Adding to sanxiyn point, the compiler may also arrange the instruction blocks differently: putting the likely block first and pushing the unlikely one at the bottom of the function. This means that the instructions for the likely cases are sitting close, and thus more likely to be pre-fetched in the L1 cache.
Sorry, that's my fault. The line should be: Ok(t) =&gt; Some(Sprite::new(t)), 
There's regular writeups on internals.rust-lang.org and This week in Rust has a section "Approved RFCs", "Final Comment Period" and "New RFCs", which are the cornerstones of the process you should look out for. Once Final Comment Period ends (towards the end of a release cycle), stuff is either in or out, and then moves towards release. I recommend subscribing to the issues of interest to you.
Everything that we use codegen for has stable equivalents. `infer_schema!` -&gt; just use [`table!`](http://docs.diesel.rs/diesel/macro.table!.html) directly. All of our custom derives have macros that just take the struct body. ([`Queryable!`](http://docs.diesel.rs/diesel/macro.Queryable!.html), [`Identifiable!`](http://docs.diesel.rs/diesel/macro.Identifiable!.html), [`AsChangeset!`](http://docs.diesel.rs/diesel/macro.AsChangeset!.html), etc)
Yeah, I agree. That's why I said `further iterations`. 
I've been doing some WebDev the past few months with Rust as well. While I have not been doing this for work I would consider it due to the fact that it has been _very_ smooth for me (granted I don't have many requirements, so for 'simpler' webdev it should be sufficient, nonetheless I see no problems so far to make it more complex). I use Iron and am very satisfied with it. I do not agree that Rust does not tend to it, it might get a bit boilerplate-y but that is nothing a good macro couldn't fix. [Here](https://github.com/TheNeikos/furratoria.space/blob/master/src/main.rs) is how my routing looks. I do not see any sort of lifetime or static issues you mentioned. Regarding Templating I use Maud as it allows me to specify my templates in Rust code which is something I enjoy quite a lot. It also allows me to write things like [this](https://github.com/TheNeikos/furratoria.space/blob/master/src/views/components/form.rs). Once I grasp plugins/macros a bit better I plan on writing a library that would allow one to specify a form and then parse a request that would spit out a struct based on that specification or the form code itself (based on some template). So for now I just generate Forms with it, like [this](https://github.com/TheNeikos/furratoria.space/blob/master/src/views/user.rs#L23). For Postgres I am using Diesel which just works, however I am just using basic CRUD operations so far, I haven't tested it with complex joins or otherwise more complicated queries.
After the next release we'll be on Macros 1.1 so we should work on all future nightlies. EDIT: And you should expect that release in the next 2 weeks or so
If you do want a perlin or simplex noise implementation, there is the [noise crate](https://crates.io/crates/noise)
&gt; It would be nice to enable it when compiling in release mode and disable it in debug mode, but there’s apparently no clean way in Rust to do that. `cfg`ing based on `debug_assertions` seems alright? E.g. the lookup function could have `if !cfg!(debug_assertions) { /* check the cache &amp;return */ } /* load from disk */`.
This is perfect. Thank you so much!
I use the nightly compiler, so that's fine. Let me just check, is there any way at all to use `cargo bench` without using nightly? I thought there weren't.
It looks like it is. I am not sure developers are all familiar with the terms "information retrieval". It might be a good idea to write "search engine" somewhere.
I'd suggest looking at this one https://github.com/glennw/test-webrender (which you can also find in the search query posted by /u/frequentlywrong )
It was not clear at all to me from the documentation. The example in the Rust documentation looks like a search engine, and the Wikipedia article on Information Retrieval gives some search engines as examples.
Maybe mention specific software-related disasters, and how the use of Rust would have prevented them. Heartbleed (buffer overrun), Therac-25 (race condition), etc.
HC is eSTREAM Portfolio one, I would like to not be a bad choice. Considering the grover algorithm, I chose hc256 instead of hc128.
If you turn off SSE and try to compile rust with floating point values anywhere it fails to compile. [#26449](https://github.com/rust-lang/rust/issues/26449). Hence there are patches for libcore to disable floating point: https://github.com/phil-opp/nightly-libcore
This is the subreddit for the rust programming language. You are looking for /r/playrust
I just want to point out that I read the README twice and still didn't understand what `bus` does until I read /u/deficientDelimiter's comment. Another way to say this: Please add some examples to your README.
You might already know this, but `unwrap` does what you're looking for: `Some(Sprite::new(renderer.load_texture(Path::new(path)).unwrap()))`
https://github.com/servo/servo/issues/13373
Still no rustconf videos. I'm starting to think we'll never see those.
We expect to upload them on this channel, so having it set up is a necessary prerequisite. :)
Probably you should look through [`RELEASES.md`](https://github.com/rust-lang/rust/blob/master/RELEASES.md). Rust the language is considerably stable, but many new features have been implemented so far. Rust the game is... hey /r/playrust people would be probably more knowledgable than me.
Macros 1.1 are just custom derives, not full procedural macros
Wonderful, thanks :)
You just invented RefCell&lt;T&gt;. :)
What you're after seems to be dynamic/runtime borrow checking like `RefCell`. 
If you can do it during compile time, then you should be able to handle it with just standard borrows and proper scope.
please share the links on Reddit and FB as well :) 
I think your idea is similar to the idea of a pointer and capability system. In such a system you can have unlimited pointers to the same thing flying around all over the place, but none of them can be used without the one-and-only *capability* value for that bit of memory. In pseudo-rust this might look something like this: let (head: &amp;Node, head_cap: Cap&lt;head&gt;) = allocate(Node { prev: None, next: None }); let (tail: &amp;Node, tail_cap: Cap&lt;tail&gt;) = allocate(Node { prev: Some(head), next: None }); let new_head_cap: Cap&lt;head&gt; = write(head_cap, head.next, Some(tail)); As you can see they're kind of unwieldy to work with, and I'm honestly not sure how you'd write a generic API for adding to a doubly linked list in this style. Several of the papers I've seen go as far as erasing the capabilities at run time, so "storing" them in a data-structure wouldn't make sense... Some links: http://www.ccs.neu.edu/home/amal/papers/linloc-fi07.pdf http://gallium.inria.fr/~fpottier/publis/fpottier-ssphs.pdf
1. Know your audience. identify the pain points they're experiencing while programming and show how Rust tackle those or show an elegant solution in Rust for a problem you all know. 2. Don't try ro oversell and prepare for questions on what you are talking about 3. Watch good presentations from other people to get ideas and see what works and what doesn't Good luck!
Well, currently, it is only "all that are on youtube". Infoq videos are out, for example. :(
&gt; As far as I can see, it would be easy for the borrow checker to guarantee correct usage of references here I don't think so. It's hard to reason about this statically. You have possibly multiple "unusable references" to the same thing in different places and the compiler doesn't know where or when one of those unusable references have been promoted to shared or mutable borrows. Keep in mind that the analysis in the compiler is intentionally function-local only. With dynamic borrow checking this ends up being `RefCell`, basically. &gt; So I thought, what's different in other languages? A linked list in C does not immediately lead to errors, You can use "unsafe pointers" in Rust as well, specifically `*mut Node&lt;T&gt;` or something like this. This together with a safe interface for the whole list abstraction would be the preferred solution. 
&gt; the compiler doesn't know where or when one of those unusable references have been promoted to shared or mutable borrows But the compiler already tracks immutable and mutable borrows, doesn't it? So this wouldn't actually introduce something new to track, because a "promotion" can just be handled as "a new (im)mutable borrow". &gt; You can use "unsafe pointers" in Rust as well. Sure, I didn't spell it out, but it was meant to help staying within the safe part of rust.
Hm, I thought I'd keep silent on this, because I do appreciate that people that were absent want to see videos of both RustConf and RustFest. Here's the thing, though: literally every thread about any of those confs has someone asking about the video release date and the tone is getting more and more negative. Even the one announcing that RustFest is ongoing. It's not even 2 weeks since RustConf. There's a reason why the linked thread contains a paragraph about video release dates. To avoid someone asking that question again. Seriously: proper conf video production takes time. Videos have gaps (e.g. the RustFest videos need all slides hand-inserted, because we couldn't access the screen video signal!), they have to be synced, audio needs to be checked and finally rendered. This is all no magic, but time-consuming and it means that the video production team needs to find the time for that. Sometimes they have clusters of events, so they batch that work. It's not like they spend all their time in the office on a computer like we do - often, they also just spend time on a plane or a car. Video production takes _at least as long as the whole material_. RustFest had &gt; 10 hours of programme. Now, I'm as eagerly awaiting the videos as you do. Because, guess what: I haven't seen any of the talks of my conf in full either - because talk time is the time I make sure the next speaker is ready, the toilets are okay and the caterer is on time. Before that, I have spent &gt; 100 hours to make sure the whole thing runs. Currently, we wind down and have to make sure the accounts are in order, because we handled a lot of money. I'm not checking every minute what my videographer does. How about "oh, cool, let's give those people a break for at least two weeks?". Constantly spamming the threads of RustConf and other confs about the videos not being released yet is - quite frankly - exhausting. Videos are gonna come. As soon as humans can make it possible. In good quality. Period. Note that this is different from asking whether videos are gonna be released at all - those people have just missed the message. Conf video is an extremely nice service: basically, you get the stuff for free that others have paid quite a lot of money for (travel, tickets). Videos are still a _huge_ chunk in any conference budget, which we - or sponsors - are happily paying. They are a great community service I believe in, because many people can neither afford travel nor tickets, or making free time. It's one of the only ways to make sure the content reaches a global audience. Just, _please_ don't act all entitled and negative about it. I really hope people will be all 🎂🎉🎇 when the videos are released. How about instead, a thread in URLO to discuss which videos were the best and greatest? 
&gt; compiler know it's safe to convert an unusable reference to some other type By checking which (im)mutable borrows exist and if the conversion is allowed. It's just the same as declaring a new borrow right now. &gt; What if this tracking is only possible at runtime because the unusable references are formed based on outside input Isn't this the same for the already existing borrows? If you can only decide during runtime, you need runtime tracking, i.e. RefCell or something similar. But the compiler can check code paths, so if you have something like `if (user_wants_node_changed)` the compiler should be able to check if you can have a new mutable borrow in that case.
I tried to figure out how to get it correct but now that I see the answer it makes sense. Using your suggestion I got a more descriptive error. Turns out that something went wrong when I downloaded the PNG files. They were corrupted and thus not the right file format. Everything compiles and the game runs again! Thank you for you help!
Yes, that sounds similar, though more verbose. Thanks for the links, I'll check them out.
This should come with rustc, like rust-gdb.
Totally agree with everything you said! Good things take time, and thank you for helping to organize these awesome events! But just incase it helps: Putting something along the lines of "Videos will be posted &lt;insert reasonable number&gt; weeks after the conference has ended." on the official website would be nice. Knowing there is a timeline answers 3 or 4 of the FAQs at the same time and the negativity would be differed at least until past the due date. 
Not strictly about Rust, but interesting nonetheless what the Rust designer is up to those days.
aaaaand he's working for Apple.
`#[derive(Identifiable)]`. Interesting that you say lack of documentation is our biggest problem. It can always be improved of course, but Diesel is usually recognized as one of the best documented crates in the ecosystem. I'd be very interested on specific issues that you've had with it so we can improve them.
When you cast an `&amp;unusable T` into a `&amp;mut T`, what exactly do you want to happen? Let's look at the options. 1. Do you want the cast to be checked at compile-time? — In that case, your `&amp;unusable T` is basically equivalent to the existing `&amp;T`. (It's a reference of which there could be several copies, just like `&amp;T` is.) 2. Do you want the cast to be checked at runtime? — In that case, your `&amp;unusable T` is equivalent to `&amp;RefCell&lt;T&gt;`. 3. Do you not want the cast to be checked at all, making it an unsafe operation? — In that case, your `&amp;unusable T` is equivalent to `&amp;UnsafeCell&lt;T&gt;`. You've answered that you want the checks done at compile-time, and sure, the compile-time checks could be smarter. For instance, doing `&amp;mut *x` on an existing `&amp;T` could be allowed under certain circumstances. But proving that there are no outstanding `&amp;T`s to the same object when you do the `&amp;mut *x` is not doable in general due to the halting problem. There are probably very few areas where this would end up being permitted, for instance if the first borrow and the mutable re-borrow occur in the same scope. Unless we start doing full program analysis and thus get exponential compile times. Adding a new pointer type wouldn't solve any of this.
&gt; By checking which (im)mutable borrows exist and if the conversion is allowed. It's just the same as declaring a new borrow right now. This can't be checked without whole-program analysis. For implementing a linked list or something you'd be storing it and accessing it from different functions, and handling that needs whole program analysis. Rust doesn't want to rely on whole program analysis for safety. Even if it did, it wouldn't be much of an improvement because of all the runtime information that the compiler doesn't know, so you *still* wouldn't be able to implement a linked list with this without getting random compiler errors when you try to do something with it.
But the local checking is sufficient right now, isn't it? There would be no new checks involved as far as I can see.. I'll try to type something up in the playground, I guess I'm missing something.
I don't know their code, but I assume that they do transformations on the IR (that's what I did when I wrote llvm based obfuscation some years ago). The issue is less llvm ir compatibility but llvm API compatibility, as you can build IR transformations using the c++ API as dynamic libraries you can load at runtime. However the apis are less stable than the IR. This approach has some limitations anyway, as you can't build illegal control flow graphs (conditional branches to somewhere random that are never taken). And static obfuscation can be circumvented usually easily through dynamic analysis, so you also need anti-debugging and then you could also use dynamic stuff based on VMs instead. In the end in my opinion usually more than stripping binaries and some string encryption is too much hassle 
How does one use `cargo-benchcmp`? Are the arguments `old` and `new` supposed to be filenames?
Hi! Even if you do these regularly, a tip would be to always have a one-sentence explanation of what imag is at the top. You never know how far these newsletters reach. The best people do it: https://this-week-in-rust.org/blog/2016/09/20/this-week-in-rust-148/
Hi. Yes, this is a gold suggestion. I will add one from the next iteration onwards!
Great. Now, I just have to figure out how to create such a benchmark file. :-) Is it simply `cargo bench &gt; old; cargo bench &gt; new; cargo benchcmp old new`? That seems to work, but I don't know if that's the intended way.
[deleted] ^^^^^^^^^^^^^^^^0.3185 &gt; [What is this?](https://pastebin.com/64GuVi2F/06334)
[Graydon on Swift](https://graydon2.dreamwidth.org/5785.html) (2014). It doesn't seem to me that [Stellar](https://www.stellar.org/) (what he is currently working on) does not use Swift, so it'd be a pure hobby.
Ah, yeah, that bites me occasionally. 
As far as I remember, the stable compiler recognizes `#[bench]` and cargo also knows the command, but the Bencher from stdlib isn't stable, so you cannot implement the right interface (you cannot even declare the right function signature, since you cannot import the required type). Long story short: it's impossible with the stable compiler. 
Ah, that's right.. Hadn't thought about that. Thanks! But consider this scenario: if the mut ref that I want keep around for later use is to an inner part, e.g. an element in a container, then I can't temporarily convert it to a immutable ref to the container. So the mutable ref to one element blocks even read-only access to the whole container as long as it exists. Any way around this problem? I keep running into this situation so many times. I think both my idea here and OP's idea would both solve this problem.
There's also some hassle even if you let others do the obfuscation: you can't debug your release builds anymore
What does the name mean? "Rest in peace, grep"?
Excellent work, and as usual a just as excellent writeup. Thanks burntsushi!
awesome! How did you make it faster than grep?
&gt; ripgrep will still work on ASCII compatible encodings like latin1 or otherwise partially valid UTF-8 latin1 / ISO-8859-1 is not really a subset of UTF-8. They only both have ASCII as their common subset. Is ripgrep working on the whole latin1?
I actually didn't mean it that way. I meant it as in, "it rips through your files." /u/neutralinostar pointed out the alternative meaning to me last night. I'm not unhappy with it. :-)
The answer to that question *is* my blog post. :-) You might skip to my [conclusions](http://blog.burntsushi.net/ripgrep/#conclusions) where I summarize.
When I saw the title, my first reaction was that the name is way too long for that kind of tool. Then I read the blog post and find out it is invoked by `rg`. So the name is really short and it turns out to be the perfect name, in France, for a tool to find information : [Renseignement Généraux](https://en.wikipedia.org/wiki/Direction_centrale_des_renseignements_g%C3%A9n%C3%A9raux)
 fn oops(x: &amp;unused i32, y: &amp;unused i32) { // since you can create multiple &amp;unused references x and y might alias let a: &amp;mut i32 = &amp;mut *x; let b: &amp;mut i32 = &amp;mut *y; // oops: a potentially aliases b =&gt; undefined behaviour } Ok, thanks for that, there goes a basic assumption of mine. Seeing that this works: fn main() { let x:i32 = 5; let mut x = x; x = 6; println!("{}", x); } made me assume that this works, too: fn br(x: &amp;i32, y: &amp;i32){ let x:&amp;mut i32 = x; let y:&amp;mut i32 = y; } Now I see that's unsafe, and it does not work indeed. Thanks for pointing that out! I don't really know what rule makes the first one work (but it's safe alright in that case), but yeah, that's my mistake right there. Thanks everyone for the comments, I learned a lot today :)
This article is awesome. I love all the small details, like * evaluating [obscure SSE instructions](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=PCMPESTR&amp;expand=786) just to see they don't actually improve things * going through the source code of multiple other tools in different languages for comparison, and even including the exact commit hashes you built them from * porting an unpublished SIMD algorithm that is basically black magic from [c](https://github.com/01org/hyperscan/blob/master/src/fdr/teddy.c) to [rust](https://github.com/rust-lang-nursery/regex/blob/master/src/simd_accel/teddy128.rs) * starting with simple benchmarks and long explanations why ripgrep only performs similar to some other tools, then escalating to slightly more complex cases where ripgrep *immediately obliterates* any competition * spending what looks like months of work on optimizing *text search* which I would have thought to be optimally solved decades ago Minor nitpicks: 0. The `Ctrl+F write up` paragraph appears two times in the article. 1. Ignoring disk reading completely for these benchmarks. I often search for text in a directory that is at most partially in cache. On spinning disks the file read order matters significantly; I noticed this a while ago when I benchmarked duplicate file finders, only to realize that recursively reading and sha1ing *every single file* in the order `find` reports them is faster than any optimization like only comparing files of the same size. ([reference](https://www.reddit.com/r/tinycode/comments/2ek3mi/fast_duplicate_file_finder_in_100_lines_of_c/ck0s9wk?context=10)) 2. The results are pretty hard to read, bar graphs would be great. Anyway, I just set `alias ag="echo use rg!"` :)
A recent tweet I made will be cited in an introductory talk by /u/regexident (he told me): &gt; Coding in Rust is like parkour suspended from strings. You'll look a bit ridiculous, but can do crazy fun moves without hurting yourself. Whatever you do, try to rehearse the talk in front of someone you trust beforehand.
There's a rust crate for it: https://github.com/alexcrichton/rustc-demangle This is "mostly" equivalent code to what Rust already uses for its backtraces and is MUCH more accurate than what the few Linux tools do atm. ~~I have yet to do a Pull Request to this library with the updated symbol demangling that I did to the Rust compiler itself though, so~~ it's not as good as Rust's Backtrace code atm: https://github.com/rust-lang/rust/pull/36059 Update: I made a Pull Request to that Library to update it to match the Rust Compiler: https://github.com/alexcrichton/rustc-demangle/pull/4
(setq counsel-ag-base-command "rg -i --color=never %s") 
There's a simple implementation of a binomial heap in [binomial.rs](https://github.com/bhickey/binomial.rs), which includes benchmarks. The benchmarks demonstrate how a nominally fast data structure doesn't hold up under a real-world model of computation. (I did speed things up somewhat by using arenas, but the asymptotic behavior of the binomial heap never improved.)
Love the extensive breakdown of the reason _why_ the benchmarks have results the way they are. I'm definitely going to be using `rg` from now on. Really like the defaults, especially the output format! And of course the fact that it's in Rust :)
Why can't you put your `toRC` implementation in the trait implementation? That way the compiler will know that it's an Rc and you won't have to use any unsafe code. If you really have to do it this way, you should: 1) Mark the trait as unsafe, since otherwise it could be implemented for another type and break your invariant 2) Use pointer casts rather than transmutes, since these are not bound by the same size restrictions.
&gt; Why can't you put your toRC implementation in the trait implementation? Because it only should exist when the phantom type is `RCPhantom`. There will be other variants with other values for the phantom type, which definitely should *not* implement `toRC`. &gt; Mark the trait as unsafe, since otherwise it could be implemented for another type and break your invariant So does that just mean that nobody can `impl` it without marking the `impl` as unsafe? &gt; Use pointer casts rather than transmutes This sounds like it's probably what I'm looking for. Thanks!
Transmute copy looks like exactly what I want. &gt; refactor your design to avoid needing this amount of unsafety This isn't really possible, since I'm using Rust as an intermediate language in a compiler. I need extreme generality, since I don't have control over what programs are coming in. Mopa looks promising, but it's still doing a bunch of runtime checks. I *know* these aren't necessary, because I could completely eliminate this problem if I had Higher-Kinded Polymorphism, I could just have a Type Constructor variable, and instantiate it with `Rc`. But I can't do that now, because `Rc` is a type constructor, not a type. I really want to be able to do this: trait MyTrait&lt;T, TCon&gt; { fn unwrap(self) -&gt; TCon&lt;T&gt; } Then no unsafety would be needed. So I *could* use mopa, but I'd be constantly checking the `.is` value, when it would *literally always* return true.
The syntax is trait MyTrait&lt;T, Phantom&gt; { fn toRc(self) -&gt; Rc&lt;T&gt; where Self: MyTrait&lt;T, RcPhantom&gt;; }
Ahhh, that helps! This will probably make things much easier. I didn't know you could refer to `Self` as a type!
&gt;&gt; Mark the trait as unsafe, since otherwise it could be implemented for another type and break your invariant &gt; So does that just mean that nobody can impl it without marking the impl as unsafe? Right. It's a "Simon says" mechanism.
Well, I have just read documentation and must apologize -- it is really well documented. My issues with documentation were more connected to macros. For example, what is [Identifiable](http://docs.diesel.rs/diesel/associations/trait.Identifiable.html) for? `#[derive(Identifiable)]` is used in many places, but I still am to discover what it is for. (Well, I guess it's only used as a requirement for associations, but it's not stated on `Identifiable` page, unfortunately). All in all, what I would really appreciate is to see more examples of using Diesel on http://diesel.rs (especially considering that [diesel/examples](https://github.com/diesel-rs/diesel/tree/master/examples) has only examples from getting started section). Other than that I admire diesel and all the work that has been put into it. (I used to say, that Rails is the magic. Now I say, that the diesel orm is the *real* magic. Though, compile-time magic, as opposed to Rails).
Did this post get flagged or something? 5 hours ago and 146 upvotes (94%), and yet i can't seem to find it on the front page. Multiple devices, multiple platforms (api/web), and i can't find it. Oddly enough, if i'm not logged in i see it. Perhaps it's a "feature" of reddit, hiding it from me. /confused
Ah good point about Identifiable. I'll document it when I get home. Its mostly used for associations but there's a few blanket impls for updates and FindDsl that use it as well. More guides are definitely something we want to do, just hard to find the time. I've been trying to write the module level documentation in a bit more of a guide format (see associations mod for example). 
&gt; combines the usability of The Silver Searcher (an ack clone) with the raw performance of GNU grep Isn't GNU grep slower than ack and Ag though?
I tried to use it, but it didn't see to work. For my use-case, a sed script was easier.
Nope, not hidden - unfortunately. Really strange. Appreciate the effort :)
This looks wonderful. Is it possible to add the ag arguments "-l" an "-L" for returning just the name of the files that contain and don't contain the search term?
I'm integration testing a command-line utility, but even though I can force the command to output terminal colour codes, running the command through [`Command`](https://doc.rust-lang.org/std/process/struct.Command.html) seems to swallow all the colour codes.. Is that the behaviour of `Command` or have I found a bug in my own code?
People are downvoting, but this is true. Decades ago you SIMD wasn't so ubiquitous for example.
&gt;`--vimgrep` Bless you, this automagically works with [ack.vim](https://github.com/mileszs/ack.vim). Just if executable('rg') let g:ackprg = 'rg --vimgrep' endif 
thanks! `vimgrep` is terribly slow... Edit: `Unknown flag: '--vimgrep'` with `rg` 0.1.15 :( Edit2: It's been introduced in 0.1.16.
What would it mean to handle paging better than ag?
Just in case you didn't know already: vim can read from stdin if you pass a single dash as the last argument. I think the pandoc example is really the strongest one and should go first.
Minor point: As I understand it, yaourt has security issues that aren't being fixed. If you're going to show an example with using an AUR helper, pacaur would be better.
I didn't know already, thanks.
But will it be faster to deliver a mild electric shock when I accidentally search my entire home directory?
"ripgrep is faster than (grep|ag|git grep|ucg|pt|sift)" * shows benchmarks where it is in ballpark range for average cases and smokes everyone else in worst-cases.
i love rust
This looks like an absolutely top-notch solution... to the wrong problem. Well, more accurately, to only part of the problem. What I mean is: I want a grep-like tool that transparently builds and uses a [trigram index](https://swtch.com/~rsc/regexp/regexp4.html), because that drastically increases the speed of repeated queries against a large codebase, which is a very common pattern in practice. Sure, it's nice if a tool can push the CPU to its limits and churn through a file in record time, and from my experience with ag, on many codebases this approach is fast enough that there's no real need for a replacement. But there is a limit: I will always have to wait while it greps through, say, `/usr/src/chromium`, while with an index most queries can be answered instantly. I could use Russ Cox's tool, but I really don't want to deal with the hassle of manually creating, updating, and (to save disk space) deleting indexes. It should be possible to do this automatically. I'd also like a frontend command that acts more like ack and friends, e.g. searching the current directory by default and dealing with gitignore. These are relatively boring usability problems, but solving them is necessary to unlock the power of a better algorithm. Thus far I've been using a simple script that uses macOS's Spotlight to find potentially matching files (via mdfind), then runs ack on the results to produce final results. Perhaps surprisingly, Spotlight is capable of finding strings that aren't full words and thus works okay for this. My script is a total hack: it doesn't really parse the regex, just splits at symbols, so complex expressions will produce false negatives. Also, Spotlight is fairly slow, and does not guarantee results are up-to-date or complete - they are from whatever happens to be in the index at the time - which is a fundamental downside to using Spotlight at all and removes my motivation to improve the script. (My ideal solution would, if asked to search a directory that contains unindexed or out-of-date data - there would have to be a daemon to detect changes - prioritize indexing that particular directory and wait until it's finished.) And yet despite all that, it's pretty fun to grep `/` and get results in a few seconds...
I've been playing around with [`piston_window`](https://github.com/PistonDevelopers/piston_window) for the past few days. I noticed that the [`MouseCursorEvent`](http://docs.piston.rs/piston_window/input/mouse/trait.MouseCursorEvent.html) uses `f64` (floating point - double precision). I'd always thought that pixel coordinates were a pair of integers before coming across this library. So what does it mean when my mouse cursor is located at point `(10.5, 2)`? edit: Maybe I should page u/long_void
Sure. Currently it's called [tint-rs](https://github.com/Osspial/tint-rs), but I'm probably going to change the name at some point.
You are a programming monster!
That's a very selfish and self-centered sentiment.
I can say exactly nothing, sorry. :-( My experience with platforms other than x86 is so incredibly limited that I can't say anything useful. I've never even tried `ripgrep` on anything other than x86. Actually, I think I can say that `ripgrep` probably compiles on ARM. Hopefully it runs too. :-)
Yes, this is something that has occurred to me, but it wasn't tenable to do this up front. You really need to start with the single threaded approach, because now one can *experiment* with parallel single file search. Namely, there are several features that seem quite tricky to get right on parallel file search like supporting contexts. Even counting lines will require a touch of thought. But with the basics working, we can do things like "use parallel search only when the file is 1GB or bigger and none of --foo, --bar or --baz were set." But yeah, it's critical to do this in a way that doesn't harm usability. Out of order printing would be a non-starter.
&gt; but on a single massive file GNU grep runs circles about pretty much anything else. *ahem* :-) Note that my benchmarks aren't only good news for `ripgrep`. `ucg` performs quite well too (but has a few downsides). Even `sift` does better than GNU grep in most cases for simple literals. (cf. The `subtitles_literal` benchmark.)
That's not overly simplistic and the thought has occurred to me. There's just not enough time in the day to do all the experiments! One practical problem with this, and one thing I've been learning more about today, is that the performance of memory maps differs across platforms. For example, I'm about to disable memory maps for single file searching on Mac because the incremental reading is faster. The reverse is true on Linux. Windows is different too, but I forget the details. So... we'd actually need a per-platform strategy here. Blech.
&gt; What I mean is: I want a grep-like tool that transparently builds and uses a trigram index, because that drastically increases the speed of repeated queries against a large codebase, which is a very common pattern in practice. My day job is in information retrieval, and I built the [`fst`](https://github.com/BurntSushi/fst) crate, so perhaps one day you may wake up and tell me that I've solved the right problem. ;-) We have to start somewhere. (The actual challenge to building what you've described isn't really technical. Most of the pieces needed to do that are now already there, for Rust at least. Frankly, `ripgrep` was the last piece of the puzzle. The core challenge is building out the user interaction, and is in turn kind of dependent on how "smart" you want the tool to be.) I might actually like to pick your brain somewhat on what an ideal user flow would be. If you ever feel like writing something up with a bit more detail on what you do (and what you'd like to do), I can promise you that it will be put to good use. &gt; But there is a limit: I will always have to wait while it greps through, say, /usr/src/chromium, while with an index most queries can be answered instantly. Have you tried `ripgrep` on chromium? Chromium was actually one of my go-to test cases in development, but it seemed more difficult to use in a published benchmark if I wanted others to be able to reproduce it. (A key part of the benchmark is building the Linux kernel, which is dead simple.) On my system at least, I get `0.653s` for `rg Openbox` and `1.690s` for `ag Openbox` in the root of the chromium repository (which isn't built, because I don't feel like doing it). OK, so it's not quite instant, but it's pretty fast. It clocks in at `0.378s` if you do `rg -u Openbox` (which doesn't respect `.gitignore` files).
Wow, well, clearly I haven't been following along! I've been using `yaourt` for maybe 7 years or so now, and haven't even thought that maybe I shouldn't be. Hmm, the [comparison table](https://wiki.archlinux.org/index.php/AUR_helpers) does look pretty damning...
Well, a simple newtype does not change memory representation, i e, there is no difference between `u8` and `pub struct MyType { pub val: u8 }`, they both look the same in the computer's RAM, there's no extra indirection or such. That's all I can help with, really. I think something else is wrong. Maybe valgrind or something like that could help debugging this issue. 
I think it'd be cool if `ripgrep` can customize colors of the matches. For example have a yellow background with black text.
`cargo install ripgrep` in my case :) And I do enjoy every commandline program that gets terminal coloring right on Windows! 
&gt; (setq counsel-ag-base-command "rg -i --color=never %s") It doesn't correctly parse rg output, so the navigation doesn't work. **EDIT**: adding `--no-heading` fixes that. Can't use Ivy occur though (it displays 0 candidates regardless of how many lines are matched).
It's an Internet argument, what do you expect? He tried to be smug over something I never even asked for. This thing just blew way out of proportion.
&gt; Convert an Rc&lt;T&gt; to a Box&lt;T&gt;, It's tricky. These types have no overhead, so the size of an allocation corresponding to a `Box&lt;T&gt;` is known from the type information only, and the same way for `Rc&lt;T&gt;`. And the sizes will be different. So we can't simply exchange one pointer for the other. To make a Box from an Rc, first of all the Rc must be unshared (have only a single handle). Then we must formally reallocate it, to ask the allocator to shrink its allocation so that it fits the one for Box. Totally depending on `T`, this might or might not incur actual reallocation and copying. For some size classes, it'd going to be cheap, for some it's going to be equivalent of just deallocating the Rc and making a new box.
My bad! Read the blog post this morning. Awesome post thank you!
&gt; Rust appears not to shuffle additions optimally (despite them being commutative) Can you expound on what "optimal" means in this case?
The level of thoroughness here is unbelievable, as always. Blog posts like this are the gold standard of accessible technical writing. Well done! :)
What I find frustrating is that as far as I know there's no way to persistently configure a pager other than than a bash alias. If the resulting lines are short enough to not require paging, it would be nice if ag was smart enough to exit(0), rather than leave me in the pager. As far as papercuts go, this is pretty minor admittedly. 
Huh, there you go, I've been using `yaourt` for aeons too. `pacaur` sorts by popularity too, which some thing I've always wanted in `yaourt`.
Wow thanks for another great tool, and writeup! In order to pipe the results to less and retain default tty settings you have to use $ rg --heading --color always -n [other arguments] | less -R (on a Mac at least) Edit: Ah I just saw there's already a shortcut: $ rg -p [other arguments] | less -R
As I said on HN, awesome work. As a side note, I'm really surprised to see the, er, passions that seem to get raised by grep tools. You've taken some completely unjustified heat over this and remained admirably cool about it. I really hope it doesn't dissuade you from doing more stuff like this in the future - I hope you at least know that the large majority have a huge amount of appreciation for your work.
It isn't only that, memory latency, prefetching, and out of order execution changes how to get the most speed out of a program.
A tree-like `((a + b) + (c + d)) + ..`, instead of the more linear `(((a + b) + c) + d) + ..` let's the CPU use its pipelines more by reducing dependencies.
I tried that, but it surprisingly ended up slower on my machine. Maybe I did something wrong? I'll investigate later.
Strange. I'm pretty sure I did, including looking at the output of `perf` and confirming `POPCNT` was being used. Ah well.
I'll look into `perf annotate` once I find the time.
That’s associativity (A + (B + C) = (A + B) + C), not commutativity (A + B = B + A).
Hi everyone! I'm working on creating a web scaffolding application in Rust as a side project to make starting new web projects faster and I have run into the issue of not knowing how to include a directory with a finished binary installed using "cargo install." This type of functionality is present in the Java ecosystem by putting a "resources" folder inside of the source directory and compiling. But I cannot find any methods to do the same in Rust other than include! but that is not maintainable if the template changes. Is there any other way to accomplish this? EDIT: I looked at the RFC for "cargo install" and it actually states that the subcommand was not intended for the purpose I set out to use it for. I have decided to write my own installer for this application.
I think we can help you more effectively if you provide the context. What you are trying to do?
Chromium uses multiple Git repositories (which are nested inside the main repository); to get them all you have to use their `depot_tools` wrapper scripts. Also, 15GB is the raw result of `du -sh` without excluding .git directories, since I'm too lazy to figure out how to do that for all the repositories. The actual source is somewhat less; `src/.git` alone is 6GB. About RAM, hrm... I should have realized you'd be able to fit it all into page cache and I wouldn't be. Duh. In theory, however much of the tree is actual source should be able to fit into my laptop's 16GB of RAM; but the OS seems to have better things to do with the RAM, because according to Activity Monitor, repeated runs of `rg` end up reading from disk anyway. I made a clone of just the `src` repository (of the random version I have on my machine), which is 2.3GB *without* `.git`. This is indeed small enough to fit into page cache, so repeated runs of `rg` take "only" 1.8s. Which is still a lot of overhead compared to Linux, but not two orders of magnitude anymore :) Of course, I would rather search the full repo, and an index would avoid having to read the entire tree from disk.
First Gankro, now Graydon? Well, first Dave Hyatt, I guess, but that was a long time ago...
Would it be better to handle this in the pager rather than in every application? (asking earnestly)
You are an inspiration to me.
Yeah quite possibly, that's a good point.
Sounds useful. And maybe a more general (takes any value) variant of count_bytes()/count_codepoints() in a memchr-like library?
Thanks for trying to help! But so far I have never had a situation where split_at could help me. Instead I have done other work arounds, like extra scopes and looking up same value in hashmap multiple times within a function. Two years of programming with rust and I'm still really struggling to be productive with rust. Much of my code is just so damn ugly that I keep rewriting over and over again. Haven't given up yet though!
For the difference between borrow and asref please refer to the documentation https://doc.rust-lang.org/book/borrow-and-asref.html
What does RMBA stand for?
No it is not. The resulting option will be bigger than the enum itself. This has to be solved on language level.
[Input events from certain devices, such as drawing tablets, can definitely reach subpixel precision.](http://stackoverflow.com/a/11179583)
Nice! So it can replace `sed` for my use cases as well.
&gt; a Rust program can never: &gt; &gt; (...) &gt; &gt; have a memory leak. That's not true - while managing memory with a ownership discipline prevent leaks, Rust lets you opt out of that - you can just call `mem::forget`, put data in `Rc`s with cycles, etc. [Here](https://doc.rust-lang.org/nomicon/leaking.html) is a discussion on this on the nomicon. Anyway, it's important to stress that the rest of this list is true only for *Safe Rust*, which is the subset of Rust in which most application code is written.
Ah, not actual utf-8 then? Shame; would have liked to start using it.
Note that safe Rust doesn't prevent race conditions, just data races. [Here](https://doc.rust-lang.org/nomicon/races.html) the nomicon talks about it. By the description [on Wikipedia](https://en.wikipedia.org/wiki/Therac-25#Problem_description) (which cites a published investigation) about the Therac-25, I can't tell if this is actually a data race: &gt; The accidents occurred when the high-power electron beam was activated instead of the intended low power beam, and without the beam spreader plate rotated into place. Previous models had hardware interlocks in place to prevent this, but Therac-25 had removed them, depending instead on software interlocks for safety. The software interlock could fail due to a race condition. The defect was as follows: a one-byte counter in a testing routine frequently overflowed; if an operator provided manual input to the machine at the precise moment that this counter overflowed, the interlock would fail.[2] Anyway, Rust synchronization tools could fix this race (as would tools in many other languages). Rust would have the advantage that, once the correct code is written in a module, misuse of its API would lead to compile-time error, not a run-time error (except when a thread panics - then a lock may be poisoned, which will be detected at run-time).
Yes that split function would be really useful. You could maybe combine it with this "idiom" for how to unroll a loop without bounds checks in safe code. Credit to [cristicbz](https://users.rust-lang.org/t/how-to-zip-two-slices-efficiently/2048/2) on the user forum. 
Good point, grep USED to run circles about pretty much anything else, but the competition has improved :)
&gt; Does the summary help? That also doesn't have any graphs, right? I mean something more like this: https://plot.ly/~phiresky/1.embed?share_key=J1VdbhoQOHBj1PGPj6OhcL So you don't have to manually compare numbers ;)
This discussion is about boundary cases. Utf-8 is fully supported. (More generally, Unicode features are supported, enabled by default and are fast.)
&gt; Also, I have 64GB of RAM. Even if it were 15GB, it would all be in page cache after a few runs. Instead of relying on the page cache a better way to force files to always be in memory (at least on linux) is to copy them to a tmpfs (e.g. /dev/shm, which defaults to half the RAM size) and then disabling swap. On macOS it is possible to create and then format and mount disk images only backed by memory (no idea if that disables redundant caching), and I remember seeing something similar for windows.
I assume "Ref, MutRef, Box, Arc".
Thanks so much for this! I'm working on a search library for Rust, mostly just for fun and learning. I haven't finished reading your discussion yet, but there is so much useful detail and fascinating analysis in there. 
See the comment on the linked issue. A FB group would be a different thing for a different purpose, but yes, why not.
It isn't working for me in neovim (it is in vim). Do you think this is on your end or on ack.vim's or neovim's end. Thanks for the awesome sw.
&gt; You have only 2/4/6 PSHUFB's rather than 20 PCMPESTRI's, right? I mean, yes, but it's also only one scan through the search text, and it can stop when it finds a match. With the PCMPESTRI route, you have to always search every pattern. The entire strategy is completely different. The two approaches don't really seem comparable, unless I'm misunderstanding something. &gt; Do you have any ideas about speeding up regex matching that is not grep-like but more lexer-like? In that case you wouldn't be skipping over large amounts of text, but rather trying to determine tokens for the whole text. Do you think that jit compiling a DFA (or NFA?) would help? The core regex engine is about 500 MB/s on my system (this happens when all optimizations fail). There are big picture experiments worth doing. Things that come to mind are bit parallel NFAs, multiple small DFAs and as you say, a JIT. Remember, PCRE2 uses a JIT, and as these benchmarks show, it's fast, but it doesn't obviously surpass Rust's regex engine. (This is a very coarse claim, and I do have more granular benchmarks, but as hard as it was to do the benchmark on search tools, it's even harder to do a benchmark on a regex engine, because I need to become very familiar with the competition. Understanding PCRE2's JIT would take months, I expect.) Some of those things, like a JIT, are so galactically different that I may in fact never actually get to it. Bit parallel NFAs, one-pass NFAs and multiple DFAs are more likely to be experimented with in the next few *years*. Some of these things don't necessarily speed up the core regex engine, for example. They might speed up resolving capturing groups, which right now requires a slow NFA. Check out the Hyperscan project from Intel. They are doing some of these things I'm talking about.
actually after some more testing the same happened with primitive types; in general seems there is some undefined behaviour when you keep raw pointers to other data behind some complex type which you happened to box::into_raw() too, when you cross the boundaries of Rust at least (although that sounds random). in general i would say it was a bad idea. I managed to work around it working with enums, although is quite more verbose also is way more type safe (and memory safe) and it works perfectly fine (only the raw pointer to PyTuple is passed, but the internal data is completely managed by Rust). So all in all it's better this way.
It's on the first Wednesday in October at Chaos Computer Club Cologne. Feel free to RSVP on [meetup.com](http://www.meetup.com/Rust-Cologne-Bonn/events/233897575/). It'll be an open space meetup: Some people (why not you?) suggest topics/ideas/problems/projects and others who are interested in working on that join them. Code review outside of Github? Getting racer to work with neovim? Finally checking out what this futures thing is all about? It's all possible. See you there!
It could be that a particular custom allocator will support this, once that exists.
An excellent opportunity for using the useful memmap crate! Split the file into num_threads views, each View starting at a newline I guess. Then let the threads synchronize line numbers in order after searching. EDIT: Actually if instead giving each thread the full view of the file and a start offset, they could search for context (for -C) individually. I'm using rg now, it's great!
Any chance to make this into a crate, and also use this for newline detection in general? Say, I have a huuuge file, and I would like to skip the first 5_000_000 lines, and then read the next 5 into a buffer or dump them to stdout? Something like `sed -n 5000000,5000005p ./path/to/file`, but faster?
I was thinking you can run the 20 substring searches in parallel. You read in 16 bytes, run all 20 substring searches on those bytes, then go to the next 16 bytes. PCRE2's JIT uses a backtracking algorithm, so perhaps a DFA or NFA based one could do better. I will look into Hyperscan. 
I'm not sure how I do it either. For the most part, certain things tend to consume me, so I end devoting a huge portion of my free time to them. (For example, I don't think I slept more than 4 hours per night this week. I pushed really hard to get this out before the weekend. If I dragged it out much more, I'm pretty sure my fiance would kill me for neglecting wedding prep. :P)
Is it a price worth paying though? You mention yourself that this isn't a hot path in either project.
Just to clarify, I don't mean parallel in the sense of threads, but just that you substring search all 20 patterns on the 16 bytes before going on to the next 16 bytes, instead of searching pattern 1 on the whole string, then pattern 2 on the whole string, etc.
Even better, /u/Veedrac sent me a PR with an improved algorithm ('hyperscreamingcount') that absolutely smokes anything else, especially on larger samples. I'll put this into a crate and send you another PR to use it when I get around to it.
Great work! Is this being used anywhere yet? :)
Thanks. I might just do that this week.
I _really_ like that logo.
Then I conclude that your superpower is working without sleep :)
Haha. I don't recommend it. Sleep is good. Actually, I meant to say, I haven't slept more than 4 hours *per night* this week. 4 hours for the entire week... Now that'd be something. I'm not that awesome. :P
Getting racer to work with neovim, des that'd be nice... I'm not in the area anymore on 5th though, so have fun, sadly I can not attend this time. 
Very likely if the number of patterns is 20, but if it's 2 or 3? Or does that not occur?
I love that `secure` is just a column, with no explanation. And that `Optional` is an answer. Because who doesn't love optional security?
I couldn't find you on the twitters so this seemed like the best place to say this -- Your demeanor on the HN thread and just in general around this has been inspiring. As a full time OSS maintainer, I wish I saw more people like you in the community. 
How do I make a macro that accepts any arguments are compiles to nothing? I'm trying to make my dependence on the log crate optional. Here's my full code: 177 #[cfg(feature = "logging")] 178 #[macro_use] 179 extern crate log; 180 181 #[cfg(not(feature = "logging"))] 182 #[macro_use] 183 mod compile_out_log { 184 macro_rules! debug ( ( $($x:expr),* ) =&gt; () ); 185 macro_rules! info ( ( $($x:expr),* ) =&gt; () ); 186 macro_rules! warn ( ( $($x:expr),* ) =&gt; () ); 187 macro_rules! error ( ( $($x:expr),* ) =&gt; () ); 188 } Things I've tried so far: macro_rules! warn ( ( $()* ) =&gt; () ); This hangs rustc forever, uses up all available RAM, then is killed by the kernel(!). wat. macro_rules! warn ( ( $($x:expr),* ) =&gt; () ); Produces the error: error: expected expression, found `&lt;eof&gt;` wut?
Thanks for your kind words! I am not technically full time on OSS, although if you counted the hours, it's probably close. :-) (And yeah, I've no interest in Twitter.)
Is there any equivalent of Vec::retain that passes the index to the callback?
&gt; Rust integer arithmetic has side-effects Not on release.
Ah. TIL
fgilcher is correct :-) Feel free to suggest a better name!
To be more precise, in Rust's case rustc itself combines bitcode files, whereas in clang's case, clang emits bitcode into .o files and then gold (or lld or ld64) combine these and generate code. If you build everything with -flto then no machine code (or assembly for that matter) is generated until link time at all.
You'll want to look at /u/Veedrac's improved implementation first. It's going to be hard to beat.
`retain` doesn't actually use any `unsafe` code or any internal `Vec` APIs so you can easily create a version that passes the index as well: https://is.gd/xNOtze If you want method-call syntax, you can implement it as an extension trait: https://is.gd/yqhJvh
Could it be used for fonts?
Yay!
I know that the two-core modules in recent AMD CPUs share a floating point unit so, that might have an effect? It certainly made a mini kerfluffle about optimizing OS schedulers when they first came out.
Calling conventions is a hard beast to tackle; you'd probably be best off being able to just define which calling convention you want to use, and query which one the OS is using.
I'm using `map` on an `Option` quite often. Not sure whether that qualifies.
Your '// Hypothetical addition to the standard library' should return T, not Point.
Another alternative is to avoid taking self by reference (EDIT Quxxy already mentioned this) fn age(mut self, a: i32) -&gt; AnimalBuilder { self.age = a; self } https://gist.github.com/12fe54b7af5993e05a8cdc63bbc1720f 
I think the iterator-ness of `Option` is convenient mostly in combination with `IntoIterator`. If you have something iterable that yields `Option&lt;T&gt;` or `Result&lt;T, E&gt;`, e.g. `Vec&lt;Option&lt;T&gt;&gt;`, you can quickly and easily perform a `.flat_map(|id|id)` to get an iterator of all the `T` elements (have I mentioned I want the identity function in the prelude yet?). Or if (for whatever reason) you want to iterate over all valid unicode characters, you could do something like this: let all_chars = (0..) .take(std::u32::MAX as usize) .flat_map(std::char::from_u32); Or with `Result`: let some_iterable_of_strings = vec!["hello", "5", "some", "10", "numbers", "3", "here"]; let the_numbers = some_iterable_of_strings.into_iter() .flat_map(str::parse) .collect::&lt;Vec&lt;u32&gt;&gt;();
I think that you could also use filter_map https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter_map
I use `Some(x).into_iter()` [here](https://github.com/diwic/dbus-rs/blob/master/src/tree/leaves.rs#L324) and [here](https://github.com/diwic/dbus-rs/blob/master/src/tree/leaves.rs#L331). If you have something like `fn foo&lt;T, I: Iterator&lt;Item=T&gt;&gt;(i: I)`, and you want to have `x` as your only value, then `foo(Some(x).into_iter())` would be the simplest way to do that (AFAIK).
I was wondering, if we disable the red-zone for the kernel, then we should disable it too for programs in user-space too, or they might have some variables corrupted by the stack frame. But when i create a program for Linux, i do not disable the red-zone (well, i don't think so). So how is this handled by Linux and all other kernel ? Thanks, and sorry for my english :3
I think it's better to use [`iter::once()`](https://doc.rust-lang.org/std/iter/fn.once.html) for this purpose.
You could, but I think `flat_map` is strictly more general. Perhaps `filter_map` is a remnant from before the `IntoIterator` trait?
Map is a [direct method of Option](https://doc.rust-lang.org/src/core/up/src/libcore/option.rs.html#383), it's not going through iterators or anything, so probably not.
[deleted] ^^^^^^^^^^^^^^^^0.4095 &gt; [What is this?](https://pastebin.com/64GuVi2F/40808)
If overflows are set to trap, then it's not associative.
I think that runs into trouble with blanket impls. If you say Foo will never implement MyTrait, but Foo does implement AnotherTrait, it's now a breaking change for the authors of those two traits to add `impl&lt;T: AnotherTrait&gt; MyTrait for T`, even though they might have no idea that your Foo exists?
&gt; Is a runtime panic the right way to do this sort of thing? Well I don't know. Is it valid to not have a field set? Is there any reason not setting a field *isn't* a mistake in the program? You can turn this into a compile-time error, but that's so hideously verbose, I *really* wouldn't bother. &gt; Does this example rely on the order of method calls? No.
Cloning involves additional cost and rust wants to be explicit about it. There's also the difference that `clone` could panic. ~~For example, Clone is implemented for arrays of Copy types but not for arrays of Clone types. The reason is that `clone` could panic and thus leave the array in a half initialized state.~~ This can't happen if the type is Copy. Edit: I was wrong, see the answers below.
AMD Piledriver arch isn't great. 2 core*(s)* modules [share a decoder and FPU](https://s21.postimg.org/70693zzyv/Bull_Dozer.png). So you want to spawn threads but then set affinity in the order `0,2,4,6,1,3,5,7`. So parallel threads don't automatically end up sharing compute resources with one-another. This will require checking CPU-ID on start-up. You'll want to do this for all AMD CPU's from Bulldozer to Bristol Ridge as they're all *more or less* the same arch internally. The stats for `std::sys::process::spawn` seem to conform that AMD's interrupt instruction is indeed 2x as slow as Intel's. This shouldn't be a *big* deal, as it is only paid once on start up. 
Ok, this makes a great deal of sense. You've taught me something. Thank you so much!
I mean an expression like `-50i8 + 100 + 50`, which doesn't overflow, but does if you add parentheses. Of course it doesn't matter in release mode, since it wraps, which is where you'd do this optimization anyway.
Oh, gosh. I don't exactly love this. I think I see what you're saying about it being a PITA.
A macro can't really help in non-trivial cases, because you start to have things like needing to convert from multiple possible input types, managing borrowed *vs.* owned inputs, mutually exclusive settings, methods that set/alter multiple fields... I mean, you probably *could* write a macro for it, but it would be so huge and complicated, it'd be faster to just write the damn code by hand. Or, you know, use `Option&lt;_&gt;` and just not have to deal with it in the first place.
&gt; I don't want you to be able to hide the cost of a heap allocation. Hmm, I can understand that you don't want this but saying that it must remain impossible sounds weird to me. What if I document this feature and it also makes the library ergonomic? 
The negative `impl !MyTrait for Foo` would almost certainly be safe if it appeared in same crate as `MyTrait`. If `impl !MyTrait for Foo` appeared in a different crate than `MyTrait`, but in the same crate as `Foo`, the question becomes more complex and there might be issues. I'd have to think it through in some detail. Certainly `Foo` would have to be very careful about what other traits it implemented at minimum, but that might not be enough. You might only allow `Foo` to depend on traits which are declared as `!MyTrait`, for example. But do I really wish the `std` crate could specify `impl !Display for PathBuf`. This seems to be safe, and it would vastly simplify a lot of code that has to work around the decision that path-related types are never `Display`.
I agree, but I couldn't think of anything else.
Like censored_username says, dynasm-rs will be much prettier for this use case. But if you want to stay on stable Rust or you want to use the glorified macro assembler ability, you can do, at runtime: let mut asm = Assembler::new(); asm.cpuid(); asm.ret(no_arg()) let machine_code: Vec&lt;u8&gt; = asm.dump().code; 
Ok, having thought about it a bit more, if we end up having an `AutoClone` marker trait, I'm going to write a lint that alerts me wherever I use a type that `impl`s it, telling me where clones happen. That way, you can have your auto-clone and I can avoid it. Win-win.
If you imagine two conflicting "pick me" directives, I think you get the same forward compatibility problems you get with overlapping traits? All the problems where upstream adds an impl today, might come back in the form of upstream adding a "pick me".
Have you considered basing it on ndarray, rulinalg, nalgebra or other matrix packages? I'm the main author of ndarray, so of course I'd love if it in particular were put to use in more places, what's it missing there for you to use it? Maybe it's too complicated to be part of something simple?
Perhaps with specialization `flat_map` could (today? eventually?) be specialized for `Option` and `Result` to also have the correct upper bound. I rarely collect so a missing upper bound doesn't affect me at all. I doubt `filter_map`'s implementation is significantly better than that of `flat_map` when you compile with optimizations, but I haven't benchmarked that, it would be interesting to do so.
I think flat_map's impl is relatively messy due to the way it supports being double ended. So my thought is that the compiler can't solve that, but maybe it can.
&gt; below error You forgot to post the error.
Are you perhaps doing something that works well with the (probably as you didn't mention generation) larger L1 cache on the intel cpu and is cache hostile on the amd? Edit: Turns out L3 is pretty similar on both and L2 is bigger on AMD. L1 is smaller on AMD, but even if you are hitting L2 cache every cycle it's still only a factor of 20, so this seems unlikely. 
Where is the Easy type from? Also that's what I thought. `write_function` forces a certain lifetime upon your closure that you can't guarantee here. It's probably either the `easy`'s lifetime or 'static. If it's 'static, you need some kind of smart pointer like `Rc` which allows you to share ownership of the `result` or maybe a channel / promise / future to send the `result` once it's available. It really depends on what the Easy type and its methods actually do. If the lifetime of your closure is just bound to the lifetime of the `easy` binding, you can easily just enclose the whole thing in a block and do your return outside of that block, so that the `easy` binding goes out of scope and the `result` binding outlives it.
Yeah... I plan to make my library extensively customizable (i.e. you can customize pretty much everything you want about your plot) but I don't think that's a feasible feature to have if the configuration is not well-documented.
I want it to be easy to use, idiomatic to the way things are done in Rust, require a minimum number of lines of code to do quick and dirty plots for experimentation, and I want it to eventually support both 2D and 3D plots. matplotlib is nice enough, but I'm not really looking to make it API compatible or even API similar. What is your library? I'd like to take a look at it. Depending on how similar the goals are, we might be able to work together, I dunno.
The difference is only on a semantic level. `Borrow&lt;T&gt;` means you want to have a `T` that can be created by borrowing. `AsRef&lt;T&gt;` means you just want a reference to `T`. Maybe it is easier to understand if you look at the types for which the traits are implemented. So is `Borrow` implemented for every `T` (e.g. `impl&lt;T&gt; Borrow&lt;T&gt; for T where T: ?Sized`). Such that we can use for example both `String` and `str` in the get Function of `HashMap`. On the other hand we have `impl AsRef&lt;[u8]&gt; for str` without a corresponding `Borrow` implementation because you don’t borrow a `&amp;[u8]` from a `&amp;str`. That’s just a conversion.
I just started on it so it doesn't even do anything yet! :) I'll likely put it up on my Github soon once it has some reasonable output. My goal is to create highly customizable plots of different varieties, but that means the way that I'm setting up configuration is definitely different. Once I have it out there we should see what can be consolidated, if at all, since it's best to not have too much fragmentation if at all possible.
One way to cut the dependencies is to pass in a generic parameter using `G: Graphics` from piston2d-graphics. In general, piston_window is not meant to be depended on, it is just a window wrapper for easy setup.
Wonderful! The community team would be happy to help out. We have a number of [resources](https://rust-community.github.io/) you can use. We got a [repo](https://github.com/rust-community/talks) dedicated towards organizing events you are welcome to use, we're building [rustbridge](https://github.com/rust-community/rustbridge/) tutorials and workshop materials, and we're just about to start working on a [meetup starter kit](https://github.com/rust-community/team/issues/4). Finally, we have our channel #rust-community on [irc.mozilla.org](http://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust-community) where we can directly help out with things. One simple starter thing I'd recommend investigating is just hosting a dinner meetup to figure out who is in your neighborhood, and what kinds of things they'd be interested in. We then can help out to coordinate speakers for you. Please let us know if there's more we can help out with, or if there's anything we're missing that'd help you out!
What I was commenting on was the idea of intersecting impls, which leads to problems in downstream crates. I don't know the right way to solve this, but put forward some ideas that are less considered, but tightly connected to other problems of usability in Rust (closed families of types, which could improve ergonomics of generic code). Cargo already links crates that vary in the first non-zero major version, so we already have cases where different impls are used. This "solves" the problem of keeping a large codebase working. Named impls would just be a refined control over this process. Named impls would use the order to resolve the scope, so it doesn't lead to an UESD. Intersecting impls on the other hand has this problem because depends on the assumption "import traits to put methods into scope". I also pointed out some of the weak points of this assumption when building software. As background motivation for extending the relations between traits and types, one major feature is OOP. I think that programming languages are overly focusing on methods as a way to build abstractions. There are not that many languages that try to solve seemingly practical applications, such as making generic code both more robust and easier to understand. We need to think of these things as a kind of topology of abstractions, and figure out the optimal way to deal with it. The reason to move away from method-thinking is because it does not provide the full benefits of what OOP was originally meant to be, so thinking outside the box might be an alternative, now that Rust does not use run-time types. I am proposing a different line of thinking, because the old rules-of-thumb might not be optimal.
I used it in [this part of rustup.rs](https://github.com/rust-lang-nursery/rustup.rs/blob/master/src/ca-loader/src/sys/unix.rs#L65-L82) (experimental `rustls` support, which requires explicit CA cert initialization) to present an uniform interface across Unix variants -- some have a single file with the cert bundle, some a directory with separate files for each cert. It's a very good fit for that purpose.
Also, I did not mean named impls to be totally absent of coherence rules, only as another way to put methods into scope. How you deal with edge cases like intersecting impls is an open question, and I do not know the right way to solve it.
While this is true, the "grammar of graphics" approach that Matplotlib and ggplot(2) take is excellent. It depends on what kind of user you're ultimately hoping to attract, I suppose.
I started working on a library for conrod for this exact thing! I haven't gotten very far yet, but it is possible to draw a dynamic amount of widgets. At least, the already existing widgets like `PlotPath` and `List` indicate so. Conrod still needs some documentation.
You're in the wrong place buddy. This sub is about the programming language rust not the game.
I'm in Tampa, and I'd definitely show up. Good luck to you!
Awesome! Been perusing this a bit (AKA I now have 2 dozen tabs open). 
Thank you! Look forward to seeing you there. How does Oct. 11th work? 
Fantastic! I was actually writing a minimal version of this for use in a controls (as in control theory) library I'm experimenting with. My need is for fast, live 2D plotting. I did the same research you did for a windowing + GUI library that could easily support arbitrary drawing surfaces, that didn't take over the entire program the library was to be embedded in. I came up empty, so I'm currently writing directly with https://github.com/tomaka/glium + https://github.com/tomaka/glutin (the rust OpenGL stack). You can see the mess I'm playing with at https://github.com/ClintLiddick/rust-controller-framework, in `main.rs`. All that said, I'll definitely be following your work! We have to use matplotlib all the time, so an ergonomic Rust alternative would be very welcome! And I'm sure the'd be something to learn from the coding choices you make.
Doesn't look like it, based on [this issue](https://github.com/PistonDevelopers/conrod/issues/740)
Yeah, I thought it might forward to `self.iter().map`.
The L1, L2, and L3 are much larger with the AMD processors. My Intel mobile CPU (i5-2410M 2.3GHz): L1d cache: 32K L1i cache: 32K L2 cache: 256K L3 cache: 3072K My AMD FX CPU (FX-8120 @ 3.8GHz): L1d cache: 16K L1i cache: 64K L2 cache: 2048K L3 cache: 8192K 
Named impls still do not allow for true OOP - late binding. Everything is still early bound with named impls. Late binding in Rust requires function pointers or trait objects.
Productivity always starts when the Internet is down. That's precisely what had me start development on the Parallel application. That Rust and Cargo doc makes it easy to get API docs beforehand in the case of lack of Internet makes it much easier to work with.
Thanks for the pointers! Very interesting, I will definitely read into them.
Super jealous that you made RustConf. Also, hi from me on twitter @ycason.
Perl, being interpreted like Python, is rather slow compared to C or Rust. Taking into consideration that I'm attempting to perform as much optimizations as I can, it should be no surprise that the Perl version is consuming 200x more CPU cycles and 4x as much memory. The Perl implementation of Parallel still does a lot more than mine though, so that also adds to overhead. However, I'm confident that I can implement the entire feature set of the GNU one without raising performance costs. I still have room for improvement to make this even faster, so I'll try to see if I can reduce memory consumption and required CPU cycles even further with my next release. However, sadly, I have noticed that AMD hardware is not benefiting from these performance optimizations at all. AMD hardware is performing 3x slower with my Rust implementation compared to the GNU one, and I'm at a loss as to why. It would seem that the Rust compiler, or LLVM, is generating code that causes AMD hardware to perform at levels worse than an interpreted language.
It was easier with the tickets not being as expensive as a normal conference. I wouldn't have made it otherwise.
 &gt; The file-storage iterators can certainly encounter errors, and shouldn't just panic when they get them. A common pattern, used here, is to create iterators of Result objects. The iterator next methods return Option&lt;Result&lt;_&gt;&gt; objects. The iterator framework isn't aware of this, so iteration doesn't stop when we encounter errors. Maybe I don't understand the issue, but I think I was able to get an Iterator over Results to break early without much effort (no break). https://doc.rust-lang.org/std/result/enum.Result.html#method.from_iter
More code would help to understand why you're having issues. Is the start method taking the Runner by self or by &amp;self? Why would Rust think that the runner will not outlive the thread that it creates? Can you design your program so that the start method returns a RunnerHandle with an append and wait/stop method?
Looks like a bug to me, it's clearly anormal for 1 file, std only, no macro code
Looks like [this issue](https://github.com/rust-lang/rust/issues/22204). And it compiles in a few seconds for me using nightly. Is that an option for you?
Yup nightly will be fine to use. Part of me was waiting to see what the output would be. But if the issue is already known/solved I can just end it and move on.
I would go with wjat /u/mmstick suggested. Create a builder struct like `RunnerBuilder` that you can add closures to, and then a `RunnerHandle` that you can use to continue interactions. If you had the thread own all the data, and then give the `RunnerHandle` channel to communicate. Something like (Not actual code): struct RunnerBuilder { tasks: Vec&lt;Box&lt;Fn()&gt;&gt;, } impl RunnerBuilder { fn start(self) -&gt; RunnerHandle { let (tx, rx) = mpsc::channel(); // Spawn thread to run tasks.... RunnerHandle { channel: tx } } } struct RunnerHandle { channel: Sender&lt;Box&lt;Fn&gt;&gt;, } impl RunnerHandle { fn add_task(&amp;mut self, function: Box&lt;Fn&gt;) { self.channel.send(function).unwrap(); } } But as /u/mmstick noted, more code would be helpful. Why not just add all the functions at the beginning? Edit: Fixed formatting.
If you want some bikeshedding, I actually believe that `Option`'s boolean-like methods should be directly promoted to the overloaded operators. That is, `x &amp;&amp; y` should be desugared into `x.and_then(|_| y)` and so on. The trait would then have a default implementation for `and_then` and `or_else` which refer to `and` and `or` respectively.
Hmm interesting, thanks (I added some code above). I see some issues though. For example, how would the runner thread tell the RunnerHandle that it is done executing the tasks sent so far? I originally wanted to implement a thread pool and for that I need to know which threads are idle.
What's the problem with it? I enjoyed reading it.
Absolutely! ;)
I had a piece of code that took an iterator of `String`s like so: // sorry didnt feel like cleaning it up for Reddit. pub fn allowed&lt;'b, I&gt;(possible_regions: I) -&gt; AllowedResult&lt;HashSet&lt;&amp;'b str&gt;&gt; where I: Iterator&lt;Item=&amp;'b str&gt; { So it took a Iterator of `String`, but in one piece of my code, I did not want to give it a `Vec` because I only needed a single `String` checked. So I just did `Some(to_check).iter()` to make it iterable. Other parts of the code gave it an actual `Vec`, but when wanted to turn a single value into an `Iterator` I just wrapped it into an `Option` then called `iter` on it... I'm not sure if this is super idiomatic, but it is what I did.
You can do that using a channel to communicate with a completion signal. I do something similar with my Parallel application which uses a thread pool to run processes in. When it is communicating back to the main thread, it sends either a `State::Processing` or `State::Completed` signal.
Ugh, you are right. I would never use such a code but it's currently valid. Meh.
FP Complete. There's some division in the community between them and the community and what the vision for Haskell's future will be, mostly for how to onboard new people. Mainly stack vs cabal.
With shadowing that wouldn't matter, but we could just call it `identity`.
Yeah the whole book follows that kind of tone overall. It comes across as elitist and petty at times.
The whole book has a deliberately silly tone. The part you're referring to is at worst an ill-judged joke. "LYAH is really nasty" has been stated a few times here recently, and as far as I can tell it's entirely due to taking a single example too seriously.
I don't have a snippet on hand, but I think I've used Option to chain a value to the end of an iterator once.
Communities are destroyed by those who are blocked silenced and banned , since all we have to do is come back for revenge with another id and if we are powerful we can make a difference by taking out that evil community, which dont respect freedom. Freedom Fighters fight back.
Have you seen [this](https://www.sciencenews.org/article/scientists-watch-bacteria-evolve-antibiotic-resistance)? I was wondering whether you could make a version that divided the area into different "difficulties" the cells had to overcome, so they make mutations or something.
That's cool ... will study your implementation! I am not actually translating to bits - only a String of 1's and 0's. It started out as a translation of this code in Python: http://www.openbookproject.net/py4fun/huffman/huffman.html.
Decoding binary data can be done *massively* faster if you hard-code the Huffman table and handle the data one byte at a time, generating enormous lookup tables. When I was doing an HPACK (HTTP/2 header compression, a large part of which is Huffman coding) implementation some time back I looked at the `hpack` crate (used by `solicit`, which was used by `hyper`) and it was using a couple of levels of hash maps and some allocation instead. (No idea if it still works this way.) [My HPACK Huffman decoding implementation](https://github.com/teepee/teepee/tree/63dff50897ae215f60a2cb4c668e7f83fd0da88e/src/http2/frame/hpack) was rather more verbose but [10–50× as fast for a bitwise approach](https://github.com/teepee/teepee/blob/63dff50897ae215f60a2cb4c668e7f83fd0da88e/src/http2/frame/hpack/huffman-bitwise-iterator-decoder-for-reference-only.rs) (basically going a bit at a time with lots of nested match expressions), and twice as fast again for [a bytewise approach](https://github.com/teepee/teepee/blob/63dff50897ae215f60a2cb4c668e7f83fd0da88e/src/http2/frame/hpack/string.rs#L62) (with its [massive generated lookup table](https://github.com/teepee/teepee/blob/63dff50897ae215f60a2cb4c668e7f83fd0da88e/src/http2/frame/hpack/string.rs#L394)). I’m curious if SIMD instructions could speed it up more; it wouldn’t be feasible to simply do what the single-byte form does but for eight bytes at a time; the lookup tables for that would be prohibitive. But maybe *something* could be done with it. For someone wishing to implement Huffman decoding, I think this, especially the bytewise approach, is useful material.
The idiomatic Python frequency function looks more like this: def frequency(str): freqs = defaultdict(int) for ch in str: freqs[ch] += 1 return freqs 
While I agree with SPJ's email, I was struck by the supposedly "hilarious exchange" that he referenced where a troll entered the channel using foul language and a variety of epithets. The Haskell IRC community opted against kicking the troll, instead responding with e.g. &gt; I don't blame him, I'd be this angry to if I had to write javascript all day too. and &gt; You're offtopic right now. This is a Haskell help channel. Do you have Haskell questions?" By the end of the exchange, the troll claimed &gt; i can't believe my attempt to troll has actually got me convinced that i should give haskell a go So, here is my problem: I am pretty sure that the Rust moderators (and I'm speaking as one of them) would opt to immediately kick such a participant based on the language used by the troll. Why? Because its more important to us that we maintain an inclusive welcome environment for people at large, even if that means that we end up turning away individuals like this one (and apparently, also satisfying their desire to be kicked for fun...) I don't know what I'm trying to accomplish by pointing all this out. I guess I just see all the gushing over SPJ here, and I wanted to say that if that exchange had occurred in the Rust channels, I would not have picked it out as representative of our values as a community. 
&gt;Communities are destroyed by those who are blocked silenced and banned Yet every thriving reddit community uses the banhammer heavily. &gt; , since all we have to do is come back for revenge with another id Takes much less effort to ban. 
FYI /u/nikomatsakis, some typos: /s/consder/consider/ /s/intoduce/introduce/ /s/supkply/supply/ /s/precict/predict/ /s/interseciton/intersection/
The idiomatic Python frequency function should look more like: from collections import Counter def frequency(str): return Counter(str) Batteries included, you know. :)
Naturally, I am interested `ndarray`'s datastructures being used, since that would give me a chance to simply use `ndarray:Array`s directly, which I use pervasively throughout my code. :-)
There doesn't seem to be a lot of activity :( Why is that?
Yeah, I thought about doing that, but I kind of wanted to write something that could be used to compress data. In one of my units at University we had to write a Huffman coding algorithm in C#, where we just coded it into a string of 0's and 1's. It was interesting. EDIT: I think my issue might be that I need to keep a counter of how many bits the actual message is, as the rest of the message is padded with 0s at the end, to keep the number of bits as a multiple of 8. I don't know if that's the only problem, that's just my first guess.
I guarantee some former Ruby programmer has `some_bool || return Err("Failed :(")` somewhere on GitHub.
I am not the biggest fan of matplotlib, but you absolutely do not have to use matplotlib's global variables. In fact, it's much more convenient to construct figures and axes via `matplotlib.pyplots.subplots()`, and then work on them directly. The global figures and axes are just used for convenience functions to make plotting quicker.
&gt; which I don't expect before 2018. I was going to say that wasn't very fair, then I realised that 2018 is less than 3 months away now. 
The return on investment for not kicking people like that is going to be pretty low. Depending on how well-trained a community is, they can stink up the atmosphere pretty badly.
My Python is a bit rusty :) Yes, that's indeed a cool way to do it in Python!
I haven't seen a nightly rust for OSX in about 5 days (4f9812a59 2016-09-21). I'm curious rather my multirust is acting up or if there hasn't been a release recently (or I'm doing something dumb). Maybe it has something to do with https://github.com/rust-lang/rust/issues/36673 ? This isn't a big deal, just curious since it's an anomaly.
An alternative is the ability to breakpoint the err case in a specific `try!` macro. Either with IDE support (yeah, about IDE support _dreams_...) or a special purpose `try_break!` which will trigger a bp which can then be debugged. This is still a bit awkward since the first time you don't know which `try!` may be propagating errors and replacing all `try!` may cause a lot of noise...
I scrolled down to the comment section just to leave a comment like this.
"Freedom" does not mean "freedom from consequences".
Hey! I'm really glad to see this update - when I was looking around for a heroku buildpack there were a few things that put me off using yours and it looks like you've solved pretty much all of them! At the time I ended up making my own (which is far less feature complete then yours). One real deal-breaker for me was/is the need to add a new file to the project. Honestly this might just be my personal preference - but it feels like unnecessary clutter and just one more thing to worry about (what if I move away from heroku later?). Now that you've posted maybe I can ask what the motivation is? Why not just use the environment variables that you can seed from heroku? Again, great work! Even if I need to add a new file I will probably switch just to take advantage of the caching and better version control!
OK, so if I understand correctly, for wrappers, `Borrow&lt;T&gt;` is more appropriate. Am I right?
[I created a create to decode huffman-encoded data.](https://crates.io/crates/huffman) [In the tests, I have a function to create a huffman tree.](https://github.com/antoyo/huffman-rs/blob/153228bd91f3ccb63b80f844741c7301b6562610/src/lib.rs#L129)
[Image](http://imgs.xkcd.com/comics/free_speech.png) [Mobile](https://m.xkcd.com/1357/) **Title:** Free Speech **Title-text:** I can't remember where I heard this, but someone once said that defending a position by citing free speech is sort of the ultimate concession; you're saying that the most compelling thing you can say for your position is that it's not literally illegal to express\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/1357#Explanation) **Stats:** This comic has been referenced 3631 times, representing 2.8320% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d82lrg5)
Do things like `error_chain!` help in mitigating this problem? It's a fix by convention, though, if it is not used at the point of raising the error, you have a problem. But that's the case for std::error::Error, too.
SPJ has such a wonderful way of answering any question that makes the question asker both enlightened and happy for having asked it.
Truly magnificent! I can't wait to show my friends who constantly tell me C++ is crap b/c there's no GC. And the best thing is this is better than GC. It collects sockets and resources too if it has to. It's truly leak-free.
Herb Sutter is one of my favorite CPP speakers, have learned a ton from him. Thanks for the link, gonna watch now.
&gt; And the best thing is this is better than GC. It collects sockets and resources too if it has to. It's truly leak-free. But it's not dangling-pointer-free: you can take references into the GC heap and then free the heap with those references still around. Or you can take references into the individual objects and then free them with those references still around. The latter is especially pernicious, and I don't think it can be solved.
Does that mean that generic functions can be inlined across crate boundaries?
The heap keeps list of all pointers into it, including those into individual objects. Upon destruction of the heap all of them are reset.
Crud I guess I'm a whale, I don't look it though.
As someone who doesn't know what this means, can you point me towards some more academic explaination of this?
That's great info. Thanks for your help.
Sorry, i fear i might be missing your point. *I said that, yes that's what karma is used has, a method of sorting content by relevance.* Someone being *absolutely absurd*, i would define has, something like someone linking to child porn or worst. *Something that requires immediate attention.*
Happy I could help! :D
Speaking for me personally (also as a mod), I agree with you. Honestly, I didn't actually look at that link too closely. I'm otherwise fine with gushing over SPJ. :-)
There's [this](http://logs.glob.uno/?c=mozilla%23servo&amp;s=21+Jul+2016&amp;e=21+Jul+2016&amp;h=servo+sucks#c485760) [pair](http://logs.glob.uno/?c=mozilla%23servo&amp;s=22+Jul+2016&amp;e=22+Jul+2016&amp;h=servo+sucks#c486581) of exchanges on `#servo`, but I think half the reason he didn't end up banned is because he left before we had a chance to kick him the first time, and didn't really do much the second time. The other half is the important similarity between Servo and Haskell, and the Rust community doesn't share it: they're not popular. This means trolling attempts on the channels are rare, and the communities are close-knit enough to shrug it off. If `#rust` just let every troll leave on their own after demonstrating an inability to trigger them, it would be overrun. In other words, the only problem I have with responding to trolls like that is that it doesn't scale.
the error comes from https://doc.rust-lang.org/error-index.html#E0038 I think the issue is: "Method references the Self type in its arguments or return type" It is caused by the def of Clone: https://doc.rust-lang.org/std/clone/trait.Clone.html fn clone(&amp;self) -&gt; Self; But I may be wrong.
/r/playrust
`Clone` isn't object safe, so you either have to avoid cloning trait objects (e.g. using `Rc` instead of `Box`) or introduce a special method for cloning trait objects: https://users.rust-lang.org/t/solved-is-it-possible-to-clone-a-boxed-trait-object/1714/6
Is someone working on a deferred heap library for Rust? Sounds like it could be even more useful in Rust where the Graph problem gets even more annoying due to lifetimes and mutability.
Haskell isn't pure because of dogma, it's out of necessity. There are lots of easy to use ways to work with state and concurrency in Haskell. When possible, it's also common now to avoid directly using the IO monad. The code layout then looks more like an Erlang program, or like something written with an MVC framework. The community embraces this. As long as library interfaces are pure, everybody's happy. 
His talks are amazing too. He is a great presenter both in terms of content and in terms of delivery.
The post text is invisible for me because Google's version of Fira Sans is broken: https://github.com/google/fonts/issues/10
Not if they're `&amp;` references. It's impossible to keep track of all outstanding `&amp;` references in C++.
Great! If you have any ideas of topics or even would like to speak let me know. I'm thinking of something along the lines of giving and introduction to Rust, "fast, safe, concurrent", getting rustup installed on everyone's machines and then hacking away on some intro problems. There's a big population of students from the University whom I would love to introduce to rust!
I must say I'm surprised to see this article recommending the use of `dylib`, which is the old `dynamic_lib` module removed from the stdlib, and didn't really receive any love since then, while on the other hand, [libloading](https://crates.io/crates/libloading) is a much more active, more used, and safer lib for this task.
FInish writing the main docs for wayland-client and wayland-server, and finally release the 0.7 version, after _once again_ a full rework of the libs. Hopefully this time I got the right design. Then, I'll upgrade the glutin/winit wayland backend to use them. Then, I'll try to make [servo work on wayland](https://github.com/servo/servo/issues/9558) (hopefully before next year ? I can't be sure to be honest).
Huh, I actually didn't know about `libloading`. I may need to look into that.
Crate visibility is really a reccuring issue... :-/
The destructor of `app` wouldn't run until the new DynamicLibrary was already constructed. 
I haven't tested any of the low power mode items yet, so I would expect it to exactly match the power usage of native Arduino code. If there are sleep mode bindings exposed (should be, I haven't verified), then they should be usable, though it would be good to verify there aren't unexpected side effects. If you get a chance to experiment, please do let me know!
I wonder how does Herb's deferred heap compares to an arena in Rust. And another question I have is - what would be the equivalent in Rust to Herb's usage of raw pointers to signal non-ownership. I know Rust has raw pointers as well but those are unsafe. Is there safe way denote such semantics?
Maybe the language of "purity" isn't great, but Haskell, like Rust, has some absolute design requirements, and its 'purely' functional style is the result of those requirements, just as Rust's 'impure' style is the result of _its_ design requirements. There may be an attitude of zealotry among some Haskellers, but that doesn't flow necessarily from Haskell's design.
Yes :-)
Here are a couple of projects you might be interested then: [Piston: Game Engine in Rust](http://www.piston.rs/) - It also seems to have some other libraries which contain items for playing music and the like [here](https://github.com/PistonDevelopers) [Amethyst: Another engine!](https://github.com/amethyst/amethyst) I think Rust can easily do these kinds of things it's just that there isn't much in terms of library support at this point in time. There are some good libraries to do some of these things like [Vulkano](https://github.com/tomaka/vulkano) a wrapper around Vulkan. I would say if you're looking for something that does all of these things I'm sure there are some people who would like that. Heck even basic crates that do some of these things really well would be a boon. Later on they can be pulled together in a really neat modular framework. I haven't seen anything that pulls it all together. Maybe something in this [list](https://github.com/kud1ing/awesome-rust) is what you're looking for?
Yay! Here is my contribution, which did use Zinc on the same hardware: https://www.reddit.com/r/rust/comments/4mx60d/blinkenlights_on_a_teensy_31_powered_by_rust_zinc/
The general idea is to have some fun messing with the troll until someone with kick rights arrives. #haskell doesn't really have people on constant standby.
[removed]
I *think* (someone will correct me if I'm wrong) that this tomfoolery is not necessary unless you want to reload dylibs at runtime. If you have a regular crate with `crate-type = ["dylib"]` then it'll handle the loading and such. If you want the standard libraries to also be linked dynamically you need `rustc -C prefer-dynamic`. The issue with packaging is that there isn't any ABI stability for the foreseeable future so you'd need a different stdlib dylib for every compiler version. This isn't too different from the situation on windows where there are 100 different MSVCRTs installed, and it wouldn't be too much of an issue for OS package managers where they can (hopefully) build all Rust packages with the version of rustc that they ship.
This is all still in planning, but I imagine the first meetup would be more introductory, and if all goes well the following meetups would be about more advanced topics, hopefully with guest speakers!
I'm in Berkeley and would come to at least some of these.
Not only real politics, but fictional/alternative/ what ever politics. Such has policy decisions within the community. If a large part of the community cannot agree, high karma posts are getting removed by mods, it's going to split the community into multiple parts, reducing the over all progress we make.
I don't know if it's that interesting to mention but I've used basic_dsp + serial to build a kind of arduino-controlled instrument which allows to play samples with a pitch shift. I've also used basic_dsp + kiss3D for a simple demo which consists of making a worm dance to sound. I found the experiences really satisfying, it was as easy than making things with Processing and cargo helped me test different libraries quickly in order to find the right one
I wonder if there is an opportunity to increase the "safety" of the unsafe transmutation by generating that code via a `build.rs` script. Then if you ever change your dylib, you don't have to worry about the main app going stale, or problems arising from typos
I'm learning Rust for my OOP class. I'm a very inexperienced programmer. Would I still be welcome? I'm in Sacramento.
There is some discussion in [this thread](https://www.reddit.com/r/rust/comments/54dowq/amd_performance_crippled_by_rust_compiler/) already.
I haven't read the book in a long time, so I can't say I have a real opinion on this, I'm just pointing out a thing I hear a lot of people say often. It might not be a big deal to some people, but to others it's a signal. I know of multiple people who have bounced out of Haskell because of that example, not because it's particularly heinous, but because they see it as a signal.
I didn't know this crate but I will definitively use it, thanks !
&gt; awesome! Processing is definitely in the same family of frameworks I'm interested in. The only disadvantage of using Rust instead of Processing is compiling time which blocks rapid prototyping (I hope Incremental building will improve that). &gt; That's great to hear that you've been working with microcontrollers and Rust. I've heard that Rust is actually great for embedded work, no? The arduino controller didn't use any Rust code, it was pretty dumb. I'm looking forward to test Rust on some Photon boards but I'm lacking time recently. Programming a state machine in order to parse messages sent from the arduino and it was really easy, easier than java or Python
From a [comment on that thread](https://github.com/rust-lang/rust/issues/36705#issuecomment-249713259): &gt; `[always] madvise never` for me by default. &gt; &gt; `# echo never &gt;/sys/kernel/mm/transparent_hugepage/enabled` helped, it is fast even on AMD now. Thanks! No, not "no one knows why". It was a difference in kernel settings on the two systems, nothing to do with Rust; in this case, this benchmark happened to just be doing a lot of forking, and that particular setting appears to have made forking much more expensive.
It is solved by not using raw references where you want shared ownership semantics. Just like you wouldn't use raw malloc where you want new semantics.
Gentle reminder that, in this sub, downvotes are "only for content that does not contribute to the discussion." If you disagree with someone, either move on or leave them a comment explaining why you disagree.
&gt; mend_slices is super cool but doesn’t have a real use case, so we pawn it off to crate odds along with slice-focused tools Stride and StrideMut. I like `StrideMut`, but I see no sign of it in `odds`... :sadface:
Thank you very much for rustup patch! It made everything else possible. :-)
These iterator adaptors are so cool. I'm totally blown away by `tuple_combinations()`. That use of polymorphic return is really neat and interesting.
Ah, interesting. We currently have forked libloading to do these things but we'd prefer not to do that. Hmm... only supporting Fn(i32)-&gt;i32 is a non-starter :-) 
I wish there was a more rust ish way of accomplishing dynamic loading, something akin to osgi. I'd like to create an app where you can implement a trait and drop it in a folder and have the engine dynamically load it.
&gt; I use different algorithms I'd like to know the actual algorithms - very frequently, this is a tripping point in password storage. &gt; Notably, I'm storing nonces alongside the hashed/ encrypted passwords - which I think should be fine, but it's been a while. This is 100% normal - note that the standard way of doing this isn't even just to store them "side by side", it's to encode them into a _single string_. However: &gt; hashed/ encrypted Clarity in exactly what is being done with passwords _really_ matters. The Dropbox post is all about defense-in-depth: 1. The AES with pepper layer is so that anyone who exfiltrates the database must also exfiltrate a (likely HSM-protected) key, in order to be able to operate on it 2. The bcrypt layer is used as a password hashing function (although the debate rages on regarding whether bcrypt or PBKDF2 is a more sensible choice - bcrypt's assumptions about what can be efficiently accelerated aren't as true today as they used to be, and it's an ad-hoc modification of a cipher whose creator said [ten years ago](https://www.computerworld.com.au/article/46254/bruce_almighty_schneier_preaches_security_linux_faithful/%3Fpp=3) he was astonished people still used.) 3. The SHA-512 layer's purpose is really more of a sanitization pass than a security feature, aside from how it's used to paper over potentially-flawed bcrypt implementations. PBKDF2 would not even require this at all - simply require the client perform the first iteration. Conflating hashing and encryption in password storage worries me - historically, that's led to breaches. &gt; I also generate salts by generating a random string (with ring) That's wonderful, and exactly the right thing to do! &gt; and then XOR'ing it against the SHA512 output of a user's username. That's pointless, and gains you nothing. Fundamentally, the password hashing security model presumes salts are public. (The Dropbox post confuses the issue by having the cleartext random value be called "pepper" and the encrypted random value be called "salt"). Anyway, trying to make your randomness "more random" in an ad-hoc way is almost always a mistake. It's never necessary unless you already failed, it almost never actually adds randomness, and it's far too easy to screw up. &gt; I only enforce a salt of 16 bytes, even though the output of SHA512 is 32 bytes SHA-512's output length is irrelevant here - your salt doesn't even need to be that long. The point of a salt is to foil rainbow tables, and ensure that no two users with the same password have the same hash. 16 bytes is entirely sufficient, and you really should just get 16 random bytes straight from *ring* rather than trying to be clever with it. Now for your actual code. First: Good news! You're using Argon2i, which won the Password Hashing Competition. Bad news! There's still [a whole lot of ongoing research](https://eprint.iacr.org/2016/875) on the topic of data-independent memory-hard functions, and it's leaving Argon2i behind. It's also very young. Generally speaking, cryptographers tend to recommend caution with adopting new crypto. Second: You've copied the SHA-512 thing Dropbox did, but slightly missed the point. The reasons Dropbox used it are: 1. Avoiding a known issue in some bcrypt implementations. This does not apply to you. 2. Avoiding DoS attacks where the user sends a very long password. Your code actually does not defend against this. The reason your code doesn't defend against (2) is that it applies it on the server side - by that time, the user already submitted their 4GB password and the DoS succeeded. The SHA-512 layer Dropbox uses should be applied on the _client_, so that a fixed-size value is sent over the wire. Third: You're using an AEAD with a random nonce. Depending on the size of that nonce, you may hit a collision far earlier than you think. The vast majority of AEADs are [catastrophically insecure if a nonce is reused](https://eprint.iacr.org/2016/475), which could make that layer entirely useless. If you really want to encrypt the stored passwords, I'd strongly suggest using a misuse-resistant AEAD like AES-SIV, putting the hashed password and (hash) salt in the message, giving it a random nonce, and putting the username in the associated data. Since a misuse-resistant AEAD leaks absolutely nothing unless _all_ of them are the same, and if the _are_ all the same leaks only the fact that they are the same, this is considerably more secure. Fourth: You _must_ use a function for comparing the slices that takes the same amount of time regardless of whether they match. Anything else opens you up to side-channel attacks. In short, my recommendation: Using a nonce-misuse-resistant AEAD `E`, a strong password-hashing function `P`, and a DOS-preventing pre-hash `H`: The server has a key suitable for use with `E`, which I will call `site_key`. The server stores `(user, nonce, encrypted)` tuples. - `nonce` is a random value from *ring* of the correct size to be used as a nonce for `E` - `encrypted` is the result of the computation `E.encrypt(key=site_key, nonce=nonce, ad=username, message=verifier)` - `verifier` is a tuple `(salt, hash)` - `salt` is a random 16-byte value from *ring* - `hash` is the result of the computation `P(salt=salt, pass=prehashed_password)` - `prehashed_password` is the result of the computation `H(user_password)`. When a user logs in: 1. The user computes `x = H(user_password)` and sends `x` to the server. The only purpose of H is to prevent DoS attacks that involve sending long passwords to the server. Applying any kind of salt or pepper here is actually counterproductive. 2. The server computes `(salt, hash) = E.decrypt(key=site_key, nonce=nonce, ad=username, ciphertext=encrypted)` 3. The server computes `y = P(salt=salt, pass=x)` 4. The server compares `hash` and `y` for equality in a constant-time manner. 5. If they are equal, login succeeds. If they are not equal, login fails. I would personally build this with `E = AES-SIV`, `P = PBKDF2-SHA-512`, and `H = SHA-512`. All of these are quite thoroughly studied, and the number of primitives needed is small - as a result, there are fewer ways for it to fail. Once the research on memory-hard functions has settled down, I would likely replace `P` with the result of that research.
pinging /u/tomaka17
I didn't file a bug yet, because of several things: * I know I've configured the font metrics incorrectly, so kerning is wrong because of this (notably the 'W' and 'o' overlap). I just don't know how to do it correctly. * I'm using a text draw function that tries to space out the text to fit the rectangle it's filling. I could play around with finding a better rectangle size, but I think the abstraction here is wrong. * The black boxes are because I forgot to set alpha blending. I'm currently trying it with conrod. I have the example in the conrod repo working and now I'm trying to learn more about it.
I am trying to use hyper on both OS X (with the native tls support) and Linux (with the openssl-devel package installed). This boils down to the set of features to use for Hyper. The Cargo.toml file below attempts to use target dependencies. It works on a given OS if I comment out the dependency for the other OS. If I leave both target.*.dependencies `cargo build` fails. On Linux it tries to compile the native OS X libraries and on OS X it requires openssl to be installed. What is the correct setup here? [gist](https://gist.github.com/winding-lines/a310e9cd7a9b7d0ea2fa8a25e93379a9)
In that cae I can help volunteer (pending actual time of the meet) since I'd say I'm a more advanced user. I was considering implementing rust for work but I thought that there was too much flux in the recent async io/futures (of which I would be mostly working with) to go with right now. I'm a recent uc Berkeley grad at a startup so it actually is viable to introduce that at some point, which is exciting.
I used to use openFrameworks quite a lot before one day landing at rust-lang.org after seeing it at the top of HN. I was sucked in by the headline, gave it a go while visiting the IRC channel and couldn't bring myself to go back! IMO rust has excellent potential for creative coders (especially oFers and Cinder users) - it gives you the raw power of something like C++ while enabling you to feel far more confident and less fearful while experimenting, not having to worry about segfaults, linker errors, etc. I'd say the main difference when I started (~0.9) was the maturity of the ecosystem, though while there's still a way to go, the situation has improved quite rapidly since. I think both oF and Cinder are still pre-1.0.0 though they have been around for quite a while. Either way, rust's FFI + rust-bindgen makes it quite trivial to bind to existing C libraries if necessary. There isn't really a singular large framework like oF or Cinder in rust, however rust's standard package manager Cargo makes it as easy to add arbitrary rust crates to your project as it is to add ofxAddons to your oF project (probably a lot easier come to think of it). As a result rust projects tend to be a little smaller and more modular than C++ ones from what I've seen. I'm using: - [conrod](https://github.com/PistonDevelopers/conrod) for GUI stuff ([here](http://blog.piston.rs/2016/09/13/this-year-in-conrod/)'s a recent blog post about it if you're interested) - currently using [piston_window](https://github.com/PistonDevelopers/piston_window) for the window and graphics context as it's very easy to set up, though will likely move to using [glutin](https://github.com/tomaka/glutin) for the window and [glium](https://github.com/tomaka/glium) or [gfx](https://github.com/gfx-rs/gfx) for the graphics in order to gain a bit more flexibility and speed. - [rust-portaudio](https://github.com/rustaudio) for raw audio I/O. It works well for me on OS X though requires installing portaudio and some users seem to have trouble on windows and some linux distros. I foresee myself switching and contributing to [CPAL](https://github.com/tomaka/cpal/commits/master), though there's a lot of work to do (no audio input yet and in turn no duplex streams or device API, etc). - [sample](https://github.com/RustAudio/sample) and [dsp-chain](https://crates.io/crates/dsp-chain) for DSP and building my audio DAG. - I made some quick bindings to [liblo](http://liblo.sourceforge.net/) for OSC about a year ago so that I could send audio data over the network to a friend's visual software. I don't have them online as it was a quick hack just to get things working at the time, but we managed to generate the bindings, wrap what we needed and get it working in about an hour. These days there might be better pure-rust OSC libs floating around, though I haven't checked. As a side note, I remember seeing arturoc (one of the main oF contributors) around the rust community every now and then pre-1.0.0. I heard on the grapevine that he was experimenting with some rust-oF ideas which had me super excited, though I haven't heard much since and I'm not sure what came of it. Would be cool to ask him his thoughts some time. Anyway, good luck with your rusty experiments! I'd recommend visiting irc.mozilla.org's #rust and #rust-gamedev if you need a hand with anything - both are very welcoming and useful channels. Edit: fixed list formatting
Ah my apologies. I couldn't find any examples and I've noticed you normally provide examples.
It does provide some metrics, but I've had a really hard time figuring out what needs to be scaled and by how much on top of mapping concepts between all the libraries.
Instead of using `font.layout()`, you should probably call `font.glyph(c.into())` instead (where `c` is the character). If the function returns `None`, return a dummy value with all 0s. Then call `.scaled()` on the glyph to get a `ScaledGlyph`. Then `h_metrics().advance_width` is the `x_advance` and `h_metrics().left_side_bearing` is the `x_offset`. As for `y_offset` I don't know, but it's probably in the bounding box. 
Thanks for the advice. Heading to bed right now, but I should try that (and turning on alpha blending), as soon as I get a chance.
weak pointers These have an overhead, but parent pointers being used that way are unsafe in C++ with any amount of static checking.
Generic symbols, the example is Fn(i32)-&gt;i32 
[deleted] ^^^^^^^^^^^^^^^^0.1712 &gt; [What is this?](https://pastebin.com/64GuVi2F/29169)
Is it a bug that `tuple_combinations()` is restricted to Clonable types only? Because from a consumer point of view, an element is present only once in a tuple so we can't run in a multiple mutable borrowers issue. To make it clearer, here is the example that doesn't work: extern crate itertools; use itertools::Itertools; struct Entity; impl Entity { fn collide(&amp;mut self, other: &amp;mut Entity) {} } fn main() { let mut v = vec![Entity, Entity, Entity, Entity]; // This should work as a and b are always different. // At any time, only one mutable borrow exist on a particular element // of the Vec. for (a, b) in v.iter_mut().tuple_combinations() { a.collide(b); } } Would be nice if it did.
It's not a bug and relates to something you'll see in many mutable iterators: mutable references need to be unique in the whole iteration, not just one element of it.
\o/ Cool to see this going forward. Even nicer that work on it happened around and at RustFest. :)
&gt; and confidentiality (hugely relevant in this case). &gt; loses security catastrophically if a (key, nonce) pair is ever repeated. My understanding is that this is only relevant when dealing with replay attacks where an attacker can reuse the nonce over and over again. However, in this case, the attacker would be on the box and would have direct access to the nonce. If I were to use a static nonce, in the scenario I have where all input is guaranteed unique (to a degree), I don't know what the attack would be. Could you explain? I want to make sure that I understand what I'm protecting against so I don't end up doing something incorrect. Just trying to understand further, thanks for the help so far. For what it's worth I intend to generate a random nonce of 16 bytes, which I think should be more than enough (and is the limit imposed by the cipher afaik).
The iterator train mandates that it's not the case and that makes sense to me since the iterator is just an intermediate for the traversal of a collection (it often is), which the borrow should be tied to.
Thanks for the clarification (and sorry for the noise). I was so happy and so frustrated at the same time when I saw your blog and tried to use the `tuple_combinators`. I keeps forgetting how `Iterator`s are designed.
Yeah but for loops will already sometimes have a .iter() or .iter().take(n).chunks(y) etc Seems a lot more natural to have this follow the same pattern.
&gt; Still, if you use anything beyond a deferred_ptr to refer to something in the heap... then... you are on your own: "Be careful when you do that", which applies to both C++ and unsafe Rust (e.g. want to store an unsafe pointer somewhere? Think thrice about it). I was with you right up until you drew an equivalence between unsafe pointers and references. Even *just calling a method* in C++ involves using an unsafe pointer, because `this` is unsafe. Most methods in C++ take references, so you're effectively using unsafe pointers there too. You use unsafe pointers all the time in C++, whereas you use them only rarely in Rust.
&gt; how would I implement in safe Rust Herb's examples in the video, e.g. the doubly-linked list or the tree with parent pointers? Use the petgraph crate.