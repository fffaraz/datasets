The approach I'm in favour of (both for this and other things) is to allow automatic casting when it's provably lossless at compile time. (eg. Allowing you to pass a `u16` or `u32` into a function which accepts a `u64`.) (Admittedly, supporting `usize` is a big point for debate there since doing it blindly would introduce a situation where core language behaviour's validity depends on the processor word size.)
Will it be informed by another user survey?
&lt;3
I agree. We originally wanted this to be a series, but time is tight. In my part, rustdoc2 hasn't developed as fast as I'd like. That's because I'm not solely devoted to it, and it relies on a bunch of stuff that's also evolving too. That said, in the past week I've made some huge strides, though that work isn't public yet. Cargo sorta struggled as well. The "areas of exploration" on the roadmap didn't see any increased work, other than the usual amount.
Yeah that's true; I really like the symmetry with last year, and hopefully next years' though...
MDR
Your work is much appreciated. Thank you and good luck. :)
if i32 -&gt; usize was safely considered 'out of bounds' for negative numbers by virtue of wrap on conversion), that would work for me very well.. technically there's two points of failure if done naively but if it can roll them into one thats ok, and if you just convert (&amp;check) naively it does do the right thing.
Yeah I would be interested in a good UX for a memoizing lazy container. The thing that I posted was the bare minimum to get anything working at all and there's a bunch more boilerplate that it'd be nice to not have to write. There are a bunch of possible interops (`From` converstions, optional integration with futures? I don't know what that'd look like) that might be nice to have.
I am a bit confused by `Option::map()`. The [documentation](https://doc.rust-lang.org/src/core/option.rs.html#396) says it should accept a `FnOnce`. If so, why do `a` and `b` cause compilations errors? let mut v = 3; let mut a: &amp;FnOnce(u32) -&gt; u32 = &amp;|x: u32| { v = x; x }; let mut b: &amp;FnMut(u32) -&gt; u32 = &amp;|x: u32| { x }; let mut c: &amp;Fn(u32) -&gt; u32 = &amp;|x: u32| { x }; let o = Option::Some(3); o.map(a); // trait bound `std::ops::FnOnce(u32) -&gt; u32: std::ops::Fn&lt;(u32,)&gt;` is not satisfied o.map(b); // trait bound `std::ops::FnMut(u32) -&gt; u32: std::ops::Fn&lt;(u32,)&gt;` is not satisfied o.map(c); // works Shouldn't all of them, including `a` and `b`, implement `FnOnce` according to [this post](https://stackoverflow.com/questions/30177395/when-does-a-closure-implement-fn-fnmut-and-fnonce)?
The real question is.. what do you get out of it? Are you expecting better performance?
If only there were some kind of traditional standard that people could look to for understanding about the Right Way of doing things. Perhaps something conformed to by lots of other people and tools.
&gt; Are you expecting better performance? holy shit, not again. you dont use 64bits where 32 are sufficient.. you just dont. if your machine doesn't show any difference - it's a badly designed machine with room for improvement I keep telling you people, intel hardware has support for vectorized 32bit gather; are you telling me you can only use that on 32bit builds??????? 
So, yes? :P I was just literally just asking why you wanted it in :)
After stripping out the subjective opinions, and pejorative but meaningless terms like 'terrible', your post basically boils down to: "this speaker has achieved nothing of significance in the PL community, and her talk was nothing but daydreaming and utopian 'what if' ideas. There was no substance to her talk." Are you saying that people shouldn't be allowed to give talks unless they have achieved something of significance (however 'significance' is defined)? And that speculating on possible future language design is worthless? 
One of my friends whom I have recently introduced to Rust mentioned how impressed he was with your thoughtful replies on reddit and beyond. Here's to another five yesrs!
&gt; We originally wanted this to be a series, but time is tight I'm gonna need you to work 16 hour days until the series is finished. ;) Seriously though, great work. I love how open Rust is with everything and how dedicated the core team is to documentation and interacting with the community. I've seen you personally and several other members of the Rust project on several online communities. In fact, it's way better than pretty much every other programming language community I've been involved in. Just wanted to give a heartfelt thank you. :)
&lt;3
Ah, thanks! Iâ€™ll give that a go too.
You want it to look like the first one? For me, running rustfmt-nightly with the default config on that results in: struct Something { value: u32, } fn something&lt;T, U&gt;(data: T, item: U) -&gt; Option&lt;Something&gt; where T: Debug + Clone, U: Copy + PartialOrd, { /* */ } Which is, IMO, pretty good. If the arguments get long it reformats to: struct Something { value: u32, } fn something&lt;T, U&gt;( data: T, item: U, val: ValueWith&lt;Complex, Internal&gt;, arity: SomethingPrettyLong, ) -&gt; Option&lt;Something&gt; where T: Debug + Clone, U: Copy + PartialOrd, { /* */ } which is, again, IMO pretty good.
If you make a symlink called xdg-open which points to /bin/echo and put it on your path, then i think it should print out the documentation URL, which you can then open on your local machine.
So still hopelessly buggy.
Just to be very clear, I have nothing against any person. I have something against the content and/or how it's delivered. &gt; And that speculating on possible future language design is worthless? Anyone can speculate over a beer with a friend however much they like. A talk at a conference however... I don't know, maybe I expect concrete evidence and concrete work to be presented, something of real-world value, something I can make use of. Otherwise why don't we just make everyone a speaker, since presenting some not-so-well-thought-out ideas/rants -- that literally anyone who has spent a few minutes on Rust, C, Haskell, Idris and Javascript can come up with -- seems to be acceptable content? As you said it's subjective, and I simply stated my opinion of that video, and other videos of the same speaker. If you think I was overly harsh or offensive to the person then please let me know where exactly. &gt; Are you saying that people shouldn't be allowed to give talks unless they have achieved something of significance (however 'significance' is defined)? Well yeah, to a certain degree, because people who haven't tried to apply their fancy ideas into the real world don't understand how much the devil really is in the details. You think nobody here has come up with their "ideal syntax", or that nobody here has ever said "it would be nice if we had this or that"? My question still stands: What's the point?
Nothing personal at all. I'm sure she's excellent, I just don't see the purpose of the talk at all, and I don't see why this kind of content would get a speaker accepted. Care to explain? EDIT: That also applies to a few other videos of her talks.
The blog post mentions that the Rust book is close to finished. How does NLL affect it? Won't NLL necessitate significant changes to the chapters dealing with ownership, borrowing, and lifetimes?
[removed]
Not really. NLL doesn't make any fundamental change to those topics; it just makes the compiler smarter so that it can "do what you mean" more often without having to jump through hoops in a few cases. Taking a quick glance at the new book (haven't read it in depth), I don't see it covering many cases in which NLL would make a difference.
Congratulations to the team on a very productive year of valuable improvements. The new Rust document is noticeably better. Well done!
&gt; What made me go "fuck yeah" was probably when I first played around with the match statement. That you can match tuples and structures and destructure them infinitely deep in the match arms. Similarly the if let that also offers those abilities. Pattern matching goes further than that. Let bindings and function signatures are also pattern matches. [Example](https://play.rust-lang.org/?gist=acdfacc63c793a3a1c392e820e3a0f98&amp;version=stable). Granted, probably *not* the best idea to do that in particular in the function signature.
This was intense to read.
That's funny, I was actually just thinking today, "Man, Steve Klabnik has been at this for a while. I hope he doesn't get bored soon." I'm glad to hear you're still committed, because I firmly believe that Rust would not be where it is today without your efforts ðŸ™‚
On an unrelated note, /u/steveklabnik1, I'd be interested to hear your thoughts about the industry. Do you have a blog post about it?
&gt; although I think exa on redox should be rust idiomatic and not a plain copy of linux ls As a matter of curiosity, what would this entail in your opinion?
The problem there comes from how you define "safely". I wouldn't want another source of "panic without violating memory safety" behaviour in my programs or dependencies which is even harder to grep for than `my_vec[index]` when I want to manually audit.
&gt;nevertheless, i would like to see more rust tools that are used on linux, from grep, cp, mv find etc. &gt;ist there maybe a linux-rust-tools collection already? You might want to take a look at [coreutils](https://github.com/uutils/coreutils) then. It's meant to be a cross-platform rewrite of the GNU command-line utils. There's also a popular cross-platform implementation of `grep` called [ripgrep](https://github.com/BurntSushi/ripgrep). For an extensive list of applications, libraries, and tools written in Rust organized by category, take a look at [awesome-rust](https://github.com/rust-unofficial/awesome-rust).
I say it makes no sense to try to wrap a braces based language into anything that has to do with Python. If the syntax is vastly different, why would I expect the style to be the same? It would be just as weird to figure out how to write Lisp so it looks like Python. They syntax is different enough that it doesn't make sense to me. 
I haven't seen it mentioned anywhere, but I think Aaron also did a great job at making sure 2017 was a success for Rust, so thanks, Aaron!
https://github.com/NishanthSpShetty/crust 
NLL wonâ€™t. Thereâ€™s a small tweak to some words, but thatâ€™s basically it. Just because an edition of the book is donâ€™t doesnâ€™t mean the book is done. Thereâ€™s always new stuff landing, and so there will always need to be updates.
You can use ? for early return. struct Data { pub inner: Option&lt;Box&lt;Data&gt;&gt; } enum Never{} fn doThing(optionalDatum: Option&lt;Box&lt;Data&gt;&gt;) -&gt; Option&lt;Never&gt;{ doThing(Some(optionalDatum?.inner?)) } And you can't nest sized struct into itself. But you can use Box. Not sure why doThing needs Option as argument when can be avoided. struct Data { pub inner: Option&lt;Box&lt;Data&gt;&gt; } enum Never{} fn doThing(optionalDatum: Box&lt;Data&gt;) -&gt; Option&lt;Never&gt;{ doThing(optionalDatum.inner?) }
My main concern with the automatic casting here is if when you actually need some collection with more than e.g. 4 billion elements but a library somewhere in the stack has auto casted a usize to u32, and therefore your program can't use more than 32 bits of space without going in and "fix" the library.
Aaron refused to thank himself in this post since he wrote it, but he deserves considerable praise. An outsized share of Rust's success comes from Aaron's diligent and tireless work of driving, coordinating and empowering people in the project to achieve the overarching vision.
&gt; In rust I'd typically write something like this I'd hope not. Not only is that invalid syntax, you've also created an infinitely sized type. But setting that aside: struct Data { pub inner: Option&lt;Box&lt;Data&gt;&gt;, } macro_rules! some_or { ($e:expr, $or:expr) =&gt; { match $e { Some(v) =&gt; v, None =&gt; $or } }; } fn doThing(optionalDatum: Option&lt;Data&gt;) { let datum = some_or!(optionalDatum, return); let inner_datum = some_or!(datum.inner, return); doThing(Some(*inner_datum)); } See also qthree's answer on using `?`.
Isn't there also a macro in Servo for this kind of thing?
I fail to see how automatic casting would make that situation more likely if it was required to be provably lossless. It'd basically be limited to widening int-&gt;int and float-&gt;float casts. (ie. Passing a `u16` or `u32` into a library which expects a `u32` or `u64` but not the other way around.) Library authors can already explicitly cast `usize` to `u32` and I'm certainly not in favour of allowing automatic narrowing casts. Heck, that's why supporting automatic casts involving `usize` *at all* is such a point for debate. It would be introducing a new reason for Rust code to only compile on certain platforms and the Rust developers are wary of that sort of thing. (Just take a look at how much discussion went on around the idea of supporting 128-bit integers, given the inconsistent nature of platform support for them.) ...not to mention that casting away from `usize` is technically discarding information, since using the type inherently carries certain implications about the number's purpose.
No idea. I just tend to write stuff like that as-needed.
Well, there's the [`winmd`](https://crates.io/crates/winrt) crate, so there's probably no technical reason why not.
Good to know!
But that's the thing, *aside* from the braces, Python has a very traditional syntax.
Thanks, I'll keep that in mind :)
Hi, I'm fearful the docs to explain this have been written somewhere, and I've read some some that allude to other aspects of this problem but, how can I make this work? use std::ops::Add; fn add2&lt;T&gt;(x: T) -&gt; T where T : Add&lt;Output = T&gt; { x + 2 } If I have `x + x`, and add `+ Copy` to the type bound, that works but how can I convert "2" to a `T` generically?
The RLS of the past few nightlies seems to be broken in this way. I added another toolchain with a working RLS (`nightly-2017-12-12` worked for me so I chose that) and set `rust-client.channel` to that string. rustup default nightly-2017-12-12 rustup component add rls-preview rustup default nightly #or whatever your previous default was And then in vscode config "rust-client.channel": "nightly-2017-12-12" and restart vscode.
I tried to stick to the 79/80-column limit I got used to from PEP8 as a non-controversial option that also helps with a vertically-split editor configuration. ...I found that it made my Rust much more awkward than a 99/100-column limit did.
RipGrep is great too
Is there any chance you might extend it to also cover backing up the associated wikis (clone-able by replacing `.git` with `.wiki.git` in the clone URL) and issues (more involved, since it has to be done via GitHub's API)?
&gt; How much overhead does a Vec have over an array, if I am never resizing the vec, and I always know exactly how large my Vecs will be at runtime? Specifically when generating and disposing them very fast in a loop. Can't know until you measure it! Benchmark your current version to see if it's even an issue. I am a bit confused about your problem, though. Could you just do something like this? const BOARD_SIZE: usize = 8; pub struct BoardGame&lt;T&gt; { data: [T; BOARD_SIZE * BOARD_SIZE], } 
What you want is the `num` crate, which has traits for this type of thing. extern crate num; use std::ops::Add; use num::One; fn add2&lt;T&gt;(x: T) -&gt; T where T: Add&lt;Output = T&gt; + One { x + T::one() + T::one() } 
For a more general approach ("convert "2" to a T generically?"): where T : Add&lt;Output = T&gt; + num::NumCast { x + num::NumCast::from(2).unwrap()} I wish something akin to `x + T::from(2)` or `x + (2 as T)` would be possible at some point without this workaround.
I think itâ€™s good.
Yeah, the type was sort of an afterthought, I was just trying to find an example of a thing to guard against and some optionals were the first that came to mind. This doesn't seem like a bad option though, thanks.
Yeah the intent was really just "some data that you would want to check things on" but that said, is the option return necessary? Is that a result of the ? syntax? I have not used that at all, but it seems like the right direction.
I cannot second this enough.
good point, I'll definitely measure allocating a bunch in a loop. The reason I don't want to have a single `const` is because I'm designing this as an api, like "you provide the definition of the game, and I'll execute all the rules for you." So the implementors have to be able to say how big their board is, it could be chess or tic-tac-toe.
It will! The summary results are here: https://blog.rust-lang.org/2017/09/05/Rust-2017-Survey-Results.html
No. I can give you some soundbites, but probably won't elaborate on them after. Maybe someday I'll write more about it... I'm afraid that the industry is not sustainable. We burn people out, and expect new people to pick up the slack. Open source is hugely problematic here. Our industry not only has no ethics, but also often rejects the idea that we should have any ethics wholesale. Everyone wants to externalize all costs. For example, we could have more secure software, but nobody wants to pay for it. So we just don't do it, or do it poorly, and it harms a lot of people. Diversity and inclusion is... making progress, but has an extremely long way to go. We have no sense of history. Old ideas get rediscovered and championed as a new thing, without taking the lessons learned the last time around. "software engineering" is a lie. Our profession is wonderful in that it pays well. It's terrifying in that we give 20 year olds a salary in the top 20% of the US, which distorts their understanding of the way the other 80% lives. This has impacts on many of the other points. Heck, frankly a lot of this is intertwined. Even though we are paid well, we should be paid a *lot* more. Unfortunately our industry is not interested in collectivizing in order to demand better conditions from management. Our culture has myths (in the classical sense, legends) that are not only misleading, but actively harmful to ourselves and those we impact. That's enough for now :)
Ah interesting! I suppose then you'd have to preallocate a big object pool like the other comment suggested. You may be able to use something like typed_arena if you only deallocate a batch of boards when you exit some scope.
Nice, i'm really curious how efficient this method really is. Rust to JS calls in my experience seemed slow.
+1 Aaron was (and still is) really great.
I'm curious too, webgl calls are heavy in nature though, so might not be an issue in this case. Not sure.
Yeah for sure -- as Steve said, we had planned to do a whole series and talk in depth about what worked and what didn't, but things got squeezed at the end. While I think the impl period worked really well -- especially having an alternating rhythm between design and implementation work for the community -- for sure the *timing* this year was not ideal, leading to a glut of high-traffic RFCs toward the end. We'll definitely work to avoid that outcome in the future. I personally am disappointed that the revisions to Tokio didn't land this year; I pushed pretty hard to make this happen, but there just wasn't time. Also, in general, the structure of the year didn't leave adequate time for, as you mention, proper retrospectives. And we're also kicking off the 2018 planning pretty late. Hopefully next year we can schedule with more "margins" in place for this kind of work. On a more personal note, I really felt the tradeoff between throughput and latency this year -- spreading myself across a lot of different efforts trying to keep up momentum, but thereby not being able to as easily focus on driving individual projects to completion. This is part of why I wanted to highlight the importance of growing our technical leadership bandwidth in 2018. (If you're interested in growing into a leadership role next year, please let me know!)
Aw shucks. It's hard to imagine having more fun!
But really, it's usually not that radical. There should at least be a vague idea of what should be avoided etc. shared within the community.
If you had some maximum size for your array, you could consider using an `ArrayVec` with that max size as its capacity. Smaller boards could just set a shorter length, so the code would still be pretty clean. That might be wasteful if you were allocating a lot of these on the heap, but if you mostly keep them on the stack it should be almost no overhead.
hi, maybe not directly rust related but here are my 2 cents. that's a lot of files so I just browsed here and there. player.rs that's a lot of usizes. why usizes and not u32 or even u16? also, why not store all attributes in an array ? you could maintain readability by indexing with consts or casting an enum into usize and then you also have a lot of "if" which look like copy paste below. 
You can either do let datum = if let Some(datum) = optionalDatum {datum} else {return}; (you can make that a macro) or use the http://crates.io/crates/if_chain crate, which provides a macro that lets you write nested ifs and if lets in a serial form. https://docs.rs/if_chain/0.1.2/if_chain/#quick-start There was a guard let proposal at one point that would let you write this easily, but it didn't go through.
Started working on small project to generate Rust types (source code) from a JSON object/file.
The first one looks like what I ought to be doing, thank you. ifchain looks nice but I prefer not to default to crates if a can find a "vanilla" solution that fits the bill - especially when I'm just complaining about syntax like I am here, haha.
Just looking at the code, I would guess .wait() is never meant to be used on `tokio` futures. When you .wait() on hyper here, the thread is paused until the future is awoken. The problem is the future is waiting on IO, and since you're waiting, `tokio` can never actually poll the IO sockets and re-awaken the future. There are two possible fixes for this. The quickest would be to do what you do above on line 113, and use `self.core.borrow_mut().run(response.body().concat2())`. This will properly let `tokio` have control of the thread while the future waits, so the next data coming into the socket will actually awaken it. The other way you _could_ do this is to have your whole method built as a set of future combinators, and only use `self.core.borrow_mut().run()` once on the very last one. This would allow for greater future concurrency of getting multiple things at once, etc.. However, it would be a large restructuring, and it isn't the prettiest without `await` syntax that we'll have in the future.
Considering you aren't exactly using `optionalDatum` in this example, you can use `and_then` combinator. fn doThing(optionalDatum: Option&lt;Data&gt;) { if let Some(innerDatum) = optionalDatum.and_then(|datum| datum.inner) { doThing(innerDatum); } } 
Oh my gosh thanks so much I think I actually kind of understand how tokio works now. The first solution worked perfectly as this program doesn't really need anything async. Thanks again :)
This fix worked for me, thank you!
No problem! Glad to help :)
Thank you both! Now I need to check on all of that and start coding!
&gt; My point that there are more proper algorithms still stands. So not the best, but better than nothing for latin languages? (probably worst than nothing for JP)
Definitely worse than nothing in Japanese. The 'turning "bu" and "pu" into "fu" in Japanese' effect would be as if `e` got broken down into `c-` and then the `-` was thrown out. As for languages using the latin alphabet, I'm no expert, but I'm sure there are situations where it'd mess things up. At the very least, it'll fall short because it won't equate `ae` and `Ã¦` in English or `ss` and `ÃŸ` in German.
I once aliased `exa` to `ls` and haven't looked back since! It's a really nice improvement over traditional `ls`. Another tool written in Rust I use all the time is [tokei] for counting the number of lines of code in my projects. [tokei]:https://crates.io/crates/tokei
You can get human readable sizes on regular ls too, just add -h option
Maybe this could be made to work: use walkdir::{DirEntry, IntoIter, Error}; fn get_targets(root: &amp;str) -&gt; Box&lt;Iterator&lt;Item=DirEntry&gt;&gt; { Box::new(WalkDir::new(root) .sort_by(|a, b| a.file_name().cmp(&amp;b.file_name())) .into_iter() .filter_map(Result::ok as _) .filter(|e| e.file_type().is_file())) } There's an [error](https://play.rust-lang.org/) about `file_type`, but I can't investigate right now... Somehow I'd think one could return the Iterator directly, but there are problem about `Sized` I don't fully understand. (e) Ah, /u/jDomantas already mentioned this. I'll just leave it anyways ;)
Not as efficient as it could be! Currently the `js!` macro from `stdweb` doesn't use wasm's native support for integers and floats (it serializes those just as any other value), and `TypedArray::from` creates a copy of the data it's passed. I do plan on fixing this in the future. (An `unsafe` version of `TypedArray::from` which doesn't copy is already there, it's just not exported; faster integer serialization might be more tricky, but should be doable with specialization, or maybe even without.)
Love the error handling. Never seen anything that made me go "that's how it should be" like this before.
Casting `Result::ok as _` is unnecessary here. It was in the previous example to coerce the function to fn pointer type so that you wouldn't need to name the concrete function type. If you remove the cast the error about type of `e` being unknown also goes away. If you try to write just `Iterator&lt;Item=DirEntry&gt;` as the return type it doesn't mean "something that is an iterator", it means that you return iterator trait object. Trait objects are not `Sized`, so it doesn't allow to return it (how would then the calling function know how large is the return value?). That's why you have to `Box` it up - `get_targets` then allocates correct amount of memory for the box because it knows the concrete type of the iterator, and returns just the owned pointer, which has known size.
Thanks for the great library &amp; the explanation.
Right, thanks for the explanation :)
Why aren't there any word about great progress in WASM support?
WASM to JS calls use trampoline, so there is an indirection that prevents inlining and trashes caches :(
So a rewrite of cloc?
I don't think that's possible. Compared to a simple reference, a RefCell borrow (`Ref`) behaves more like a lazily evaluated `&amp;T`. It only yields a real, *temporary* reference to the underlying data, when it's being used (dereferenced). A `&amp;T` created from a `Ref` cannot outlive the `Ref`, so you'll have to store the `Ref` itself; like /u/Omniviral implied.
Man, Rust is by far the best thing thatâ€™s happened to systems programming in decades. I canâ€™t wait to have a Rust day job.
You can use map method on Option. And also people prefer to calculate something instead of doing something, which is slightly more functional and without ugly hidden side effects.
TBF a programming language should be able to survive the departure of one person. Sooner or later the Great Bus will come for us all!
Well it is possible, given that I solved it :)
Blink twice if they're holding you hostage.
I reckon downloading the wiki shouldn't be too hard, the GitHub API (which I'm already using to fetch a list of repos) will most probably already expose the wiki URL, so I can hook into that fairly easily. The issues may be a bit more annoying in that the only reasonable way to store them is as a big blob of JSON. So while it'll be easy enough to download them, those issues are probably not going to be in a very usable form...
The `?` operator can be used because of the [`Try`](https://doc.rust-lang.org/std/ops/trait.Try.html) trait. It's currently implemented for `Option` and `Result` in stable, and is very commonly used when handling functions that return a `Result`.
Pretty much, although I've found tokei tends to be faster and has more sane output, by doing logical things like using the patterns in your `.gitignore` to ignore files by default. One pet peeve I had with `cloc` is if I forget to exclude the `.git` and `target` directories of a Rust project you'll often be sitting there twiddling your thumbs for a couple minutes while waiting for it to finish. ``` $ cd ~/Documents/forks/rayon $ time cloc . --exclude_dir .git --exclude_dir target 156 text files. 155 unique files. 9 files ignored. github.com/AlDanial/cloc v 1.74 T=0.26 s (564.0 files/s, 77666.4 lines/s) ------------------------------------------------------------------------------- Language files blank comment code ------------------------------------------------------------------------------- Rust 126 2673 2795 13153 Markdown 14 272 0 1384 YAML 2 14 1 84 TOML 5 14 4 82 Bourne Shell 2 13 3 26 ------------------------------------------------------------------------------- SUM: 149 2986 2803 14729 ------------------------------------------------------------------------------- cloc . --exclude_dir .git --exclude_dir target 0.31s user 0.08s system 100% cpu 0.390 total $ time tokei ------------------------------------------------------------------------------- Language Files Lines Code Comments Blanks ------------------------------------------------------------------------------- Markdown 14 1656 1656 0 0 Rust 128 18622 13154 2795 2673 Shell 2 42 25 4 13 TOML 5 100 82 4 14 YAML 1 61 50 0 11 ------------------------------------------------------------------------------- Total 150 20481 14967 2803 2711 ------------------------------------------------------------------------------- tokei 0.06s user 0.02s system 212% cpu 0.035 total ```
Pretty much, although I've found tokei tends to be faster and has more sane output, by doing logical things like using the patterns in your `.gitignore` to ignore files by default. One pet peeve I had with `cloc` is if I forget to exclude the `.git` and `target` directories of a Rust project you'll often be sitting there twiddling your thumbs for a couple minutes while waiting for it to finish. $ cd ~/Documents/forks/rayon $ time cloc . --exclude_dir .git --exclude_dir target 156 text files. 155 unique files. 9 files ignored. github.com/AlDanial/cloc v 1.74 T=0.26 s (564.0 files/s, 77666.4 lines/s) ------------------------------------------------------------------------------- Language files blank comment code ------------------------------------------------------------------------------- Rust 126 2673 2795 13153 Markdown 14 272 0 1384 YAML 2 14 1 84 TOML 5 14 4 82 Bourne Shell 2 13 3 26 ------------------------------------------------------------------------------- SUM: 149 2986 2803 14729 ------------------------------------------------------------------------------- cloc . --exclude_dir .git --exclude_dir target 0.31s user 0.08s system 100% cpu 0.390 total $ time tokei ------------------------------------------------------------------------------- Language Files Lines Code Comments Blanks ------------------------------------------------------------------------------- Markdown 14 1656 1656 0 0 Rust 128 18622 13154 2795 2673 Shell 2 42 25 4 13 TOML 5 100 82 4 14 YAML 1 61 50 0 11 ------------------------------------------------------------------------------- Total 150 20481 14967 2803 2711 ------------------------------------------------------------------------------- tokei 0.06s user 0.02s system 212% cpu 0.035 total 
I don't mind writing my own conversion code to make use of them... I just want to know I have backups getting piped into the replication system I use for backups of local content.
You can run `cloc --vcs=git .` and it will do exactly what you want it to
I don't have anything significant to add, but i want to give a big shoutout to everyone involved: Thank you for all the hard work!
I have two pieces of advice: 1. Check what a subreddit, forum, etc. is about before blindly posting to it. 2. /r/playrust.
I feel like in every Rust post I just end up saying or thinking the same thing. "Why don't we switch already!?" although I know that is not an easy thing and maybe not as good as I initially thought. That said, sometimes it is a tad annoying having to do something in another language while thinking "in Rust this would be way easier/cooler/safer/whatever"
This is just amazing. Albeit I havenâ€™t yet written any substantial amount of Rust, it is the thinking like this that keeps me drawn to the Rust community. Everyone seems to have such a dedicated attention to understanding challenges and working constructively with them. Not just in terms of code, but the whole ecosystem of a programming language. 
AFAIK just dropping the future should work.
installed and wow.. :) thank you for that. 
There are a few options similar to that, depending on your taste. I prefer the fourth as I don't really like `return` hidden in the middle of a long line like 1 and 3, and the fourth is a line shorter than the second. let datum1 = if let Some(datum) = optionalDatum {datum} else {return}; let datum2 = if let Some(datum) = optionalDatum { datum } else { return; }; let datum3 = match optionalDatum {Some(datum) =&gt; datum, _ =&gt; return}; let datum4 = match optionalDataum { Some(datum) =&gt; datum, None =&gt; return, }; 
Because it's complicated, on account of any such implementation needing to move elements out of an array, which leaves holes in the array, which can cause Bad Things to happen if a panic happens during iteration. Worse, whatever drop code gets implemented, has to be baked into the compiler, since the stdlib is written in Rust, and Rust has no way to parameterise impls over constants. I believe `Vec` gets around this by leaking memory in the case of a panic. I'm not entirely sure if arrays can even get away with that, as they're stack-allocated, not heap-allocated. Also, it doesn't have to be baked into the compiler, so that probably helps.
The ecosystem is amazing. I was trying to get an old Haskell project with a bunch of dependencies working (that I know used to work together) with Cabal and gave up in the end. Weâ€™re spoilt by cargo ;)
Here's an example how hyper handles this. Hyper has one infinite future (a ForEach future accepting new connections forever) and another one that signals shutdown. It selects on both. The moment the shutdown future resolves, it just drops the incoming future and waits a timeout for outstanding requests to be worked on. The shutdown future is user configurable, but usually the receiving end of a oneshot channel. See: https://github.com/hyperium/hyper/blob/master/src/server/mod.rs#L382-L396 
It hasn't hit stable yet.
I just wanted to drop by and say that I friggin' love Agricola, so this is awesome.
[removed]
I recently uploaded code to github and it didn't take longe before people said it's ugly :) So I went to beauty school: /// checks s, if "Daniel" returns tupel (s,i) else invokes collatz(i) fn hello(s: String, i: u64) -&gt; (String, u32) { if s.eq("Daniel") {(s, i as u32)} else {(s, collatz(i))} } /// performs collatz returns 1 fn collatz(mut i: u64) -&gt; u32 { while i != 1 { i = if i % 2 == 0 {i / 2} else {i * 3 + 1} } i as u32} /// prints name with number fn main() { println!("Hello {:?}", hello(String::from("Daniel"), 33));} Is that beautiful or not? :) It works really great with ripgrep I recently installed :) try rg fn or rg i: :)
As an aside `CancellationTokens` are great. I love how you can plumb them down and cancel many `Tasks`.
For me it's also the tooling that really interests me with Rust. I started with C++ as my first language at the same time as I started dual booting Windows and Linux. I naturally wanted to make stuff cross platform but the build systems were way harder for me to understand than C++ itself. With Rust it all just seems to work so effortlessly, which honestly surprised me. I didn't expect that from a low level language, but was pleasantly surprised, and honestly cargo seems to be even better than some of the build systems for high level languages.
&gt; I'm not entirely sure if arrays can even get away with that, as they're stack-allocated, not heap-allocated. Nothing stops the implementation from just forgetting to drop those by storing an array within `ManuallyDrop`. Also, I don't think a compiler needs to implement `IntoIterator` for `[T; N]`, I think `core` could do it as well considering it defines `IntoIterator` after all (although const generics would be nice for that purpose).
Late to the thread, but I understand you were big in the Ruby community before rust, but do you consider rust to be a language community you would stay in for a long time? It's worded weird but curious
Well right now the compiler needs to impl it, because a manual impl for every length of array would be infinitely large (and you though compile times were bad now). When const generics arrive it could be done in finite code.
&gt; holy shit, not again...I keep telling you people Keep it kind please. I don't think anyone here is going to attack you, and if they do, we'll tell them to be kind too.
Yeah, if you've got all the raw information available then it's not too difficult to knock up a quick python script to transform it into the form you want. What would you expect for a repository like `rust-lang/rust` with tens of thousands of PRs and issues though? Trying to download all of them will almost certainly get you rate-limited (you get 5000 requests/hr), and would probably result in a JSON blob gigabytes in size.
I don't think I understand the panic part; Could you explain a bit further? My understanding is that panic would drop everything on the stack while unwinding, including the iterator; so it's fundamentally not so different from dropping while iterating. For this, one could store the array inside the iterator wrapped with `ManuallyDrop`, and the iterator's `Drop` implementation would just manually drop what hasn't been moved out yet.
Would be better (for diffing and for viewing in a text editor) to store as YAML. I've been working on a similar project (in node.js), haven't even announced it anywhere yetâ€¦ but I have issue fetching from the GraphQL API that saves to YAML and commits in a separate branch.
Do you need `Segment` to own `Text`? Why not just have `Segment` be an `&amp;[u8]` instead?
Thanks for showing how to use `NumCast`, I agree that's pretty verbose but I can appreciate that it's doing a conversion and hence needs an `unwrap`. I had seen the `From` trait while browsing the docs. Is something like `x + T::from(2)` currently not possible because of the language, or is it just a matter of adding to a library? Is there a more simple way to, at compile time, when `T` is known, to say "hey convert this `2u32` to `T`"?
I love `WaitHandle` also. It drives me nuts that in Java I have to write a loop to wait on multiple futures.
 xdg-open (fd|sk) https://github.com/sharkdp/fd https://github.com/lotabout/skim
I need to be able to access the original Text object, because I there is additional data stored in Text I need access to from the Segment.
Then why not have it be something like `(&amp;Text, &amp;[u8])`?
That would need nested borrows.
Yes, I plan to stick around for quite a while.
Only if you want them to be mutable. [Works fine with read-only references](https://play.rust-lang.org/?gist=cc0b6d72afb17c87baecd241d42fe398&amp;version=stable).
Agreed. Besides some warts with `ConfigureAwait` I think C# does futures really well.
&gt; I don't think I understand the panic part; Could you explain a bit further? It's an idea called [pre-pooping your pants][p]. Basically, you want to make it so that even if a destructor doesn't get called (say it's been leaked with `mem::forget()` or a `Rc` cycle) the world is still "safe" (in the Rust sense) and not left in a broken state. An excerpt from the article: &gt; This is what I call the Pre-Poop Your Pants (PPYP) pattern: do part of the right thing in the constructor so that if the destructor doesn't run, then less bad things should happen. Why Pre-Poop Your Pants? Well I like to think of the constructor-destructor pattern like a person eating. You eat some stuff (construction), process it, and then clean up any junk by pooping it out (destruction). &gt; &gt; If a destructor gets lost, this is like never pooping. Dependent on the destructor, this can have varying consequences. &gt; &gt; - Category 1 types are broken down completely by the body, and never needed to be pooped anyway. &gt; - Category 2 types are basically benign, but clog up the system and may cause critical failure if constipation persists. &gt; - Category 3 types are big bundles of stuff. Hopefully nothing in there is nasty! &gt; - Category 4 types are like safe drugs. They're going to give your system a bad trip if they're not properly handled! If you're in a safe space when it happens you might be fine, but otherwise it can cause really bad things. &gt; - Category 5 types are like dangerous drugs. You can overdose and die if they aren't properly handled, and should be carefully controlled. &gt; &gt; The Pre-Poop Your Pants pattern is the following deviation from the normal digestion pattern: as soon as you consume something, be sure to poop the nastiest bits right into your pants. If everything goes according to plan, you can clean out your pants when you take your normal poop. If something goes wrong, you've got poopy pants but at least you're better off than otherwise. [p]: http://cglab.ca/~abeinges/blah/everyone-poops/
he mentions twiceeeeeee the broken industry. What does he mean with that? Can anyone explain?
My understanding is that we are waiting for [const generics](https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md) to be completed.
It can be done, but it's waiting for const generics to be able to have a proper iter type. https://github.com/rust-lang/rust/pull/32871
What's wrong with the `Rc&lt;Text&gt;` solution? It's only one `usize` longer than the "ideal" `pub struct Segment&lt;'a&gt; { text: &amp;'a [char] }` soltution (a pointer + a length). Alternatively, if you always have the `Text` in scope when you want to use a `Segment`, you could also stick with `pub struct Segment { first_char: usize, len: usize }`... Without knowing more about your particular problem it's hard to determine what the best solution would look like.
This may work, I will try, thanks ðŸ˜€
Been using rust for a new project and I realized that you can't make a doubly linked list without using unsafe :(
that's great. months ago I asked if there is something available similar to spring boot, so I can visualize simple structs and arrays in a html frontend. I definitely check that out :) Is there are dot matrix chart that can map values withing min / max to color gradients like here https://raw.githubusercontent.com/benbria/d3.chart.bubble-matrix/master/doc/screenshot.png I want to visualize the weights in a neural network so see what's going on in there.. Big = 1.0 small = 0.0 or same size but #fff = 1.0 and #000 = 0.0 
https://www.reddit.com/r/rust/comments/7la643/five_years_with_rust_steve_klabnik/drltvv2/
The code I settled with is actually in that issue itself. You don't need to patch it, you can just use the second version
I'm currently working on adding more types of graphs. It uses [vega](https://vega.github.io) to render the frontend, and so in premise anything it can render should work with gust. Would [this](https://vega.github.io/vega/examples/reorderable-matrix/) fit your description?
ah thank you very much. I guess I've been outed as a shallow reader. :)
I have no idea how to do that but I am very interested in the reason why you need that. What on Earth is that useful for? In my mind, a trait implementation represents a capability, something a type can do. What do I get for ensuring that something cannot do something?
/u/manishearth suggested it once to me and someone else [here](https://github.com/nvzqz/static-assertions-rs/issues/2#issuecomment-353445348) is requesting it. So that's at least two people who would find it useful. In my mind, ensuring a trait is not implemented may allow for a stricter crate API that could lead to certain optimizations? I'm not exactly sure myself how it would work.
Yeah, I get that it's good for something but for what?:)
Â¯\\\_(ãƒ„)_/Â¯
It's all good! :)
Wild suggestion, but perhaps there is some code that is only safe when some type doesn't implement `Drop`, but for whatever reason it can't or won't implement `Copy`?
Nit: I think it's not that `.wait()` isn't meant to be called on a tokio future, but that it's not meant to be called from the event loop thread. If you wanted to do things with a synchronous style, you could create a thread pool of your own and call wait from there.
Check out [Marionette](https://developer.mozilla.org/docs/Marionette).
I posted my points in [this reddit post](https://www.reddit.com/r/rust/comments/56sa70/what_do_you_wish_you_know_when_you_started/).
I agree that the solution that OP worried wasn't idiomatic enough, seems totally fine to me.
Please refer to https://doc.rust-lang.org/nightly/nomicon/leaking.html in the future, which is the modern, palatable, version of this documentation :)
`rls` is now `rls-preview`.
Automatic widening casts can be a problem in situations like this: fn mul(a: u32, b: u32) -&gt; u64 { a * b } What happens? It's hard to define this in a good way.
I look at it like this: * You can't make a doubly linked list in C++ without "using unsafe," * You can't make a doubly linked list in Java without using a GC, * But you *can* make a doubly linked list with a safe interface *and* no GC in Rust! :)
I'm aware, the update was only a few days of nightlies and as I recall that change was a couple months ago. Regardless the component is not in yesterday's toolchain.
This was my primary motivation for using Rust. The safety is a really nice bonus, but in my line of work the modern C++ is good enough. It's the quality of the ecosystem that makes me want to use Rust for everything.
Non-Lexical Lifetimes got merged and all the tools broke: https://github.com/rust-lang/rust/blob/d90d5d19da1126c5e66ede7d23bcfd8e18601a8a/src/tools/toolstate.toml#L25-L36
Thanks! Didn't realize NLL finally got merged, exciting times (once the tools can build again :).
I needed a double linked list for something (internals of a prolog interpreter where it does make sense) and out of curiosity, I tried VecDeque (from std) first instead. It made enough of a performance difference that I stopped thinking about using a genuine doubly linked list. Without knowing anything about the problem you're solving, I would still say give VecDeque a try. It might satisfy your need for a doubly linked list.
First of all, awesome stuff. Have been having some troubles getting transparency to work on win 10 but know it can be done through the winforms API after some playing around, is this possible from winapi-rs and is all winforms calls exposed? 
`mem::swap` and `mem::replace` After learning about those, something just really fell into place with regards to Rust's overall strategy of handling memory access. In the case with those two, it's all about making sure an initialized value doesn't become deinitialized while it's still alive. Essentially the good all Indiana Jones' sand bag trick.
Looks like it is intermittent. Did you guys find a way to get all versions which shipped with rls? Would be better to get it from a list instead of installing one for each day and checking.
[Defer lib](https://github.com/thibran/defer) is my try at the problem. If anyone has a suggestion how to improve it, all help/hints welcome.
One thing I noticed is that the speed-up seems smaller now than a year ago: https://blog.rust-lang.org/2016/09/08/incremental.html. I know that the incremental compilation support is being reimplemented right now, and there are other things that might explain this (the non-incremental case getting faster, better precision).
This is super cool! For reference, how much does the Linux TCP/IP stack get?
As far as I can tell from `perf top`, there are several main bottlenecks: * the weird 1's complement TCP/IP checksum (there's a reason your NIC has checksum offload), * copying memory between kernelspace and userspace, * firewall (nf_conntrack). All in all, `time target/release/examples/benchmark` shows that the benchmark spends more time in kernel mode than in user mode, which tells me that smoltcp is faster than Linux's TCP/IP stack, which is pleasing enough :D Edit: By the way, we have some [good first issues](https://github.com/m-labs/smoltcp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) for prospective smoltcp contributors! I try to be fast at reviews and merges, so here's your chance.
I'm not sure what would be the fair way to benchmark Linux against Linux (User Mode Linux?) since lo is not a real network interface.
A while ago I was trying to express all kinds of options you could want in a scatter plot in the type system (size, shape, color, connection lines, labels, legend, axis scaling, â€¦, all of which should be possible to be specified at configuration time or generated out of the data, and so on â€¦) I got sidetracked by text layout before I got around to implementing it though.
&gt;We have no sense of history. Old ideas get rediscovered and championed as a new thing, without taking the lessons learned the last time around. &gt; &gt;"software engineering" is a lie. &gt; &gt;Our profession is wonderful in that it pays well. It's terrifying in that we give 20 year olds a salary in the top 20% of the US, which distorts their understanding of the way the other 80% lives. This has impacts on many of the other points. Heck, frankly a lot of this is intertwined. &gt; &gt;Even though we are paid well, we should be paid a *lot* more. Unfortunately our industry is not interested in collectivizing in order to demand better conditions from management. Hear hear
Yeah, that does make things tricky. I guess two computers wired with 10GbE could work, but that requires two computers with 10GbE.
Oh no, I didn't mean this as an insult to you. I worded my response poorly. Instead of saying "didn't like" what I meant was more on the lines of "didn't think I could use." My apologies. Regex is super versatile, I just wanted something faster, and I'm pretty sure regex doesn't work with no_std environments (I could be wrong). The regex part is the only thing I didn't think I could use, but I did borrow a lot from what you originally did. Thank you for publishing that library. Seriously, top work. I don't actually know what nom is or how it works. Maybe it's worth investigating.
10GbE cards have a lot of offload in them though (e.g. TCP segmentation offload) that makes the comparison even less fair.
I thought you can disable most of that through the driver. Probably not worth it though - I'd say being faster than pretty much any link available to end users is more than fast enough for the intended use case :)
I can do at *least* an order of magnitude better by enabling window scaling and jumbo frames. I haven't even tried to optimize this benchmark, it just sort of plopped into the Gbps range on its own.
This is kind of insane, honestly. I should really dig into the source code.
Though these days it's a lot better with Stack. (posted for those few who are not yet familiar with it)
&gt;standard library Check out the stdweb create.
rls-preview on nightly sometimes breaks and a build isn't uploaded, meaning if you update, you'll no longer have the RLS installed on your system. Two work arounds are not using nightly or installed a dated version of the RLS from the last successful build.
I see. I don't remember RLS ever getting removed for me, so I must have gotten lucky in the past. Or does the old version stick around?
Builds are normally successful, but it's nightly so anything goes. I've only had the issue once, but I don't update that frequently.
That sounds really nice! I should sit down for a weekend try to see what i can do with GStreamer because i don't really know what i can do with it in detail. Looking forward to those "getting started" posts. 
If you have experience with this sort of thing, I'd really love some help :) It's a bit limited since everything right now conforms to the vega specification (https://vega.github.io/vega/examples/grouped-bar-chart/). However, since a lot of it is very javascript-y, its tricky to reuse too many components, and so I've pretty much been building each chart individually from the ground up. If you have suggestions for how to make reuse more effective subject to these constraints, that'd be really useful!
that comment was left before stdweb supported WASM. Trust me, I've been following stdweb with great interest, and I'm very excited it supports WASM now.
Rc is certainly idiomatic rust!
It's incorrect, and I cannot reproduce this locally with a small example. Please provide a reproducer on https://play.rust-lang.org or so so we can track it down and get it fixed :)
I guess this isn't an easy question in implementation details, but the bit I'm stuck on is the overarching design, so... I'm trying to clone an API from the stdlib, because the stdlib version brings in dependencies I don't want to require. In a particular function that tries to acquire a lock, the stdlib version of this function blocks if the lock's not available. I can't efficiently block the thread, so should my version of this function 1) spin-wait, or 2) panic? 
https://github.com/aspera-non-spernit/brainrs/blob/master/src/lstm.rs Line 85. It's already the clippy'd version, but without removing $weights[j] so the original line was like ft = ft + $weights[j] * $past * $signals.features[j].present;
Interesting, I cannot get this to reproduce on https://play.rust-lang.org/?gist=300c646a40dab04d59a194e95c5c1745&amp;version=stable - perhaps your version of clippy is very old or very new? edit: Ok, to reproduce invoking the macro is required. This is beyond what can be easily done on play I think, I can try poking at it more locally tomorrow or so if nobody beats me to it (which would of course be appreciated)
&gt; the weird 1's complement TCP/IP checksum (there's a reason your NIC has checksum offload), [This guy?](https://github.com/m-labs/smoltcp/blob/deccd469fe911736a4e3780bb7670be011eba9bb/src/wire/ip.rs#L613)
must be the latest clippy version. I cargo installed it just before I executed cargo via clippy. Maybe 5 minutes before the post + download, compile and install time. Oh and I had to switch to nightly, because I got errors when not on nightly. I replicated the error I got with clippy with stable error: failed to run `rustc` to learn about target-specific information Caused by: process didn't exit successfully: `/home/genom/.cargo/bin/clippy-driver rustc - --crate-name ___ --print=file-names --target x86_64-unknown-linux-gnu --crate-type bin --crate-type rlib` (exit code: 127) --- stderr /home/genom/.cargo/bin/clippy-driver: error while loading shared libraries: librustc_driver-826cfd06bac9b886.so: cannot open shared object file: No such file or directory rustup show installed toolchains -------------------- stable-x86_64-unknown-linux-gnu (default) nightly-x86_64-unknown-linux-gnu active toolchain ---------------- stable-x86_64-unknown-linux-gnu (directory override for '/home/genom/Dev/brainrs') rustc 1.22.1 (05e2e1c41 2017-11-22) 
I appreciate the per-feature docs in the README. I wish more crates did this.
Neat! All of my comments are bikesheddy: * Since it's an object that is initialized from the function I'd expect it to be named with a noun (like `Deferred` instead of `Defer`, but I think `deferred` has a bit of prior art). * I see how implementing `PartialEq` and friends would be useful, I'm not sure if it should represent the unfinished state of a `Defer`. I don't know what the point of it would be. * You might be able to get `map` working the way you want it to by implementing it on `&amp;mut self`, instead of `&amp;self`. * In general I think the mutation methods make sense to have as `&amp;mut self` rather than `&amp;self`. I think of this object as more like `Option` than like `Cell`, although I think you feel the opposite. * I think if you're going to implement `map` you might as well just implement `Iter`. * Coherence is annoying, it'd be super nice to just `impl&lt;F, T, U&gt; From&lt;Defer&lt;F, T&gt;&gt; for U where U: From&lt;T&gt;` * Consider running `rustfmt` on the code. If for no other reason than taking a look at how it differs from your personal style. If you're not used to it, it might look weird (`where` clauses are where you differ the most) but it's really nice once you get used to it. * You committed a `.vscode` dir. Consider adding that to your global gitignore. Nice job!
Couldn't you use SIMD (or even multithreading) for the checksum? You could keep 8 or 16 running partial sums at a time and sum 8 or 16 words at a time with paddw, then accumulate the final sum at the end with phaddw.
What are you exactly interested in, what would you like to write? Maybe I can point you to existing documentation then, or consider it in whatever I'm going to write. I think the main problem with GStreamer is exactly that, as you say, "i don't really know what i can do with it in detail". You can basically do everything in one way or another, as long as it is somehow related to processing media in a media processing graph. It's very generic.
You canâ€™t do this in regular test code, but since [fairly recently](https://github.com/rust-lang/rust/pull/43949) you can do it in a rustdoc test: /// ```compile_fail /// # #[macro_use] extern crate static_assertions; /// assert_impl!(Type, Trait); /// ```
* So many errors you'd have to deal with in C or C++ are simply *proven impossible*. As a fan of offloading as much of the cognitive workload as possible onto the compiler (which makes fewer mistakes than I do), this simplifies reading code immensely. * Conversely, when writing code, I am forced to think very clearly about my data ownership and what data gets passed to which threads, because I have to prove it all to the compiler. C++ would let me get away with it until runtime. (Or maybe even later, when I'm scratching my head for hours over race conditions or memory corruption) * `Result` and `Option` are baked into the stdlib, so we don't have to deal with C-like error codes or out parameters or even Go-style tuples. * Pattern matching on monads means it's hard to accidentally ignore errors, and that succeed-or-crash is relatively easy to spot. * I love that in most of the stdlib API, common problems have been ruled out statically. `Mutex` is a container, so it's almost impossible to operate on the value without the lock. Locking things give you a RAII handle, so you have to go out of your way to forget to unlock it. * Enums and types are cheap, especially newtypes. Being able to offload the question of, "Is my data valid?" into the type system makes thinking about the logic of how to act on those now-*provably*-valid values much much easier. 
Yes.
phaddw? The primary platform on which smoltcp is tested in production is OpenRISC 1000; x86 isn't even in top 3 deployment targets. But yes, if I could somehow use LLVM's SIMD intrinsics, I could manually optimize the checksum function for architectures that provide SIMD instructions. Multithreading is *definitely* out since smoltcp doesn't have any implicit global dependencies, even the current timestamp is injected explicitly.
You should unswitch the loop. for i in 0..data.len() / 2 { accum += NetworkEndian::read_u16(&amp;data[i * 2..]) as u32; } if data.len() % 2 { accum += (data[data.len() - 1] as u32) &lt;&lt; 8; } If you care enough to put in nontrivial effort in, it should be fairly straightforward to "fake vectorize" it with integer operations ([example in C](https://locklessinc.com/articles/tcp_checksum/)), but unswitching the loop is so straightforward it makes sense even if you're more interested in simplicity.
Why not shard the data based on modification date (to make updates quick and easy) and then compress older shards with something like gzip for easy space savings? (Basically, the same type of approach git uses with its packfiles.) Sure, you'd wind up with stale/overridden data in old shards when people edit old comments, but that could be seen as a records-keeping advantage.
I was about to say, 2-4 Gbps doesn't sound all that great when compared with 40 and 100 GbE. But given that you are not doing window scaling and jumbo frames, and real network cards have the ability to take advantage of checksum and segmentation offload, then yeah, that's a lot more reasonable. Here's a question for you: would `smoltcp` be able to be adapted to be an in-process TCP stack for use with something like [Mellanox's libvma](https://github.com/Mellanox/libvma), which uses DMA to put network packets directly in an application's address space and then implements the TCP stack entirely within the application? Their library looks like it uses a fork of lwip for the actual TCP implementation; wondering how smoltcp would compare.
Sure, that would be an ideal fit for smoltcp. In fact all the examples pretty much do that already, except they tell Linux to fake a network device instead of using a real one. It should be a matter of days to write the support code for smoltcp that integrates with RDMA.
You're not declaring it twice. You're declaring it once, then using it once. Keep in mind that the following are also possible: impl&lt;T: SomeTrait&gt; T { ... } impl SomeStruct&lt;Param&gt; { ... } impl&lt;T&gt; OtherStruct&lt;T, T&gt; { ... }
As K900_ pointed out, there are [specialized](https://developer.mozilla.org/en-US/docs/Mozilla/QA/Marionette) [solutions](https://sites.google.com/a/chromium.org/chromedriver/) for controlling browsers. However, for controlling applications in the general sense, the solution is platform-dependent. The closest analogue to Marionette for arbitrary desktop applications would be to something based on either the accessibility APIs or, for MacOS, AppleScript. The GUI testing framework known variously as [LDTP](https://ldtp.freedesktop.org/wiki/) (Linux), [Cobra](https://github.com/ldtp/cobra) (Windows), or [PyATOM](https://github.com/pyatom/pyatom) (MacOS) is an example of Marionette/Selenium-style control of desktop applications via their accessibility APIs. As for what you're specifically asking about (synthesizing input events), it depends on what platform you want to target. * On X11 desktops (Linux, BSD, etc.), you either have to use the `XTEST` extension via an [Xlib](https://crates.io/crates/x11) ([x11::xtest](https://docs.rs/x11/2.17.2/x11/xtest/index.html)) or [XCB](https://crates.io/crates/xcb) ([xcb::test](https://rtbo.github.io/rust-xcb/xcb/test/index.html)) binding (allows you to target specific windows) or use a lower-level API like Linux's [uinput](https://crates.io/crates/uinput) kernel API to write a userland input device driver not backed by a physical device. (Choose XCB over Xlib when possible.) * Under Wayland, manipulating other applications is disallowed for security reasons, so your only option is to drop down to the kernel APIs (eg. [uinput](https://crates.io/crates/uinput)) and inject your input at that level. * For Windows, [this StackOverflow answer](https://stackoverflow.com/a/4057841/435253) explains the relevant APIs and they should be accessible via the [winapi](https://github.com/retep998/winapi-rs) crate. * I have no experience with MacOS, but you can apparently do this [using Quartz Event Services](https://stackoverflow.com/a/3851793/435253). The [osascript](https://crates.io/crates/osascript) crate would give you access to that functionality via AppleScript. **UPDATE:** Apparently, someone has written a cross-platform wrapper for the low-level (not targeted at a specific window) input simulation options. See the [enigo](https://crates.io/crates/enigo) crate.
Have you looked at [spin](https://crates.io/crates/spin)? Either to use or for inspiration. It's `#![no_std]` by default.
If I'm going to go that far, I may as well save each issue as its own file (either JSON or YAML) and then put them all into their own git repository.
YAML has a problem with [parsers disagreeing](https://github.com/cblp/yaml-sucks) on how to parse it. Not something you want with your backups. (It's already caused problems [in the wild](https://ciaranm.wordpress.com/2009/03/01/yaml-sucks-gems-sucks-syck-sucks/).) 
This probably isn't going to work like you expect it to. An iterator works by *mutating* its state, so at the very least you'll need to pass in a `&amp;mut Args`. Another thing is that once you use an iterator it's been exhausted and has no more values to give. So it won't be possible to `use_again()`. It's also not possible to collect into a `&amp;Vec&lt;String&gt;` because when you use `collect()` you're creating a *new* container and filling it with the iterator's contents. A `&amp;Vec&lt;_&gt;` isn't a container, it's a reference to one, so... but who owns the `Vec&lt;_&gt;` it's pointing to?
Ah, I'm actually reimplementing `RwLock`, which they already have. OTOH, they have a slightly different API surface, so now I'm not sure if I should keep my work or not or just remove the bits they've already done. 
I feel like this is one of those times it'd be really nice if [the `!Sync` syntax][1] you use for opting out of `Send` or `Sync` was available for any arbitrary trait... [1]: https://doc.rust-lang.org/src/core/marker.rs.html#355
Thanks for your answer. I think I understand most of it. I will mull it over some more. :)
True. I'm still used to thinking in Python, where JSON and gzip are in the standard library but git is not. **EDIT:** Waitaminute. I've used git in a multi-gigabyte, many-files scenario before (storing history for a 6+ gigabyte archive of fanfiction with one file per chapter). It could just be `git gui` shortcomings I was remembering, but I wasn't satisfied with how it performed.
A while back, I had a type where you *could* derive Clone, but absolutely *must not*, or it would be unsafe. You couldn't even make a coherent Clone implementation by hand. The best I could do was put warnings in the source; I would have felt better if I could put an actual test in place so it could never be implemented, even accidentally.
Wow. I had the same problem and was wondering why I had to reconstruct the array at random times, this makes so much sense, thank you!
Why do you need the `collect()` method at all? The for loop without that should be sufficient for printing the elements in the array.
Interesting. Are there plans to support things like congestion control, selective ack, jumbo frames, and configurable rto_min, or accept patches for them, or would that defeat the "smol" and simple aspects of the design?
Please post link to the source when you can.
Just tried it, gives the same result on a 2KB chunk. Although I think your version is easier to read. f:\rust\smoltcp-bench&gt;cargo bench Finished release [optimized] target(s) in 0.0 secs Running target\release\deps\smoltcp_bench-c1de8819668dc9d8.exe running 3 tests test tests::csums_match ... ignored test tests::rolled ... bench: 686 ns/iter (+/- 4) test tests::unrolled ... bench: 689 ns/iter (+/- 6) test result: ok. 0 passed; 0 failed; 1 ignored; 2 measured; 0 filtered out
How would you use a TagSetter interface with this? `bin.get_by_interface(TagSetter::type())` ? How do you cast the Element to TagSetter and where are the `add_tag*` methods? 
`fn myfun(some_ref: &amp;SomeType) {}` think of `&amp;SomeType` as a C++ pointer, instead of a C++ reference (if you have a C++ mindset). Like a C++ reference, it is "managed" and smart, can't be null etc. But unlike a C++ reference (and more like a C++ pointer), its *type* is different. You are not handling a `SomeType` that was passed to you by reference. You are handling a `&amp;SomeType`, which *derefs* to `SomeType`, but is not `SomeType`. The `&amp;` symbol caused much confusion to me because of this, my brain was so stuck in the C++ symbol-space that I kept getting confused why my types weren't what I thought they were....
Absolutely. I don't think it is possible to overemphasize how important Rust is going to be to the software industry. We finally have a path forward to implement safe, scalable, concurrent, and performant system software with modern tooling.
Hi, I would like to suggest to add a "Table of contents" section for each page on doc(book). It might be helpful to increase the accessibility and easier to identify the contents of the page. ex: - https://hexo.io/docs/configuration.html - https://docs.docker.com/engine/docker-overview/ - https://docs.ckeditor.com/ckeditor5/latest/builds/guides/integration/advanced-setup.html#Bundler All are open source projects. Unfortunately I am not a front-end dev and bit busy with the current job :(
Sorry in advance if this is off-topic as this is more of a language design question, but in a systems language where the distinction between stack and heap allocations are important, why are the ergonomics around heap allocated types so verbose? Wrapping types in `Box&lt;T&gt;` (`Vec&lt;Box&lt;i32&gt;&gt;` or `HashMap&lt;String, Box&lt;T&gt;&gt;`) all over the place feels like heap allocations are treated as a second-class citizen.
I worked on a project that came close to saturating 4 10GbE links concurrently over linux's TCP. Well, not quite, maybe 60-70% utilized, &gt; 30 Gbps in aggregate across the four TCP streams. 
I actually disliked that in C++ T and &amp;T are considered kind of the same. This was like implicit type casting. When I learned about Rust, I liked its approach much more.
It looks trivial to implement as follows and it seems like the kind of thing the optimizer should have no trouble arriving at. What am I missing? 1. `movzx` the values of `a` and `b` to `rax` and `rbx` 2. `mul rbx` 3. Return the contents of `rax` (Admittedly, I know next to nothing about assembly language.)
&gt; feels like heap allocations are treated as a second-class citizen. They are, in the sense that they are provided by library types and not baked into the language. A long time before 1.0, there were on-heap primitives: `~T` instead of `Box&lt;T&gt;`, `~[T]` instead of `Vec&lt;T&gt;`, `~str` instead of `String`, even a GC'd pointer `@T` (dunno if there ever was a map primitive though, maybe an associative array early on). It was a deliberate design decision to shift these to being defined in libraries instead of in the language. The `box` operator is kind of the last vestige of these primitives. I would say Rust is equally or less verbose than other systems languages. C++ has templated containers but their usage looks a lot like Rust's, and C has no strong typing or method calls so everything is function calls (usually with very long names because no namespacing) over pointers and you have to manually tell the container how big your element type is. In my experience, Rust's container types aren't really all that different from any other statically typed language. Java's story with containers is arguably a lot worse because it didn't even support generics until J2SE 5.0. If you're coming from a dynamic language like Javascript or Python, yeah everything's gonna look a lot more verbose.
The declaration is `"impl" GenericDeclaration (Trait "for")? NominalType WhereClause` where any generic types you use in the trait, nominal type, or where clause must be declared in the generic declaration.
Author of May here. I know a lot of people are waiting for the final asynchronous solution in rust. Now you can have a fresh try for the coroutine in rust!
Nice clean look! My personal take on personal HTML charting is [flot-rs](https://crates.io/crates/flot) - Flot does have pleasant defaults. However, there's not much strong-typing going on, e.g. colours are just strings. Strong typing is important for javascript/HTML because errors are so damn silent.
It's definitely a case where C++ knowledge is going to fight with Rust learning. Rust references are rather more like the safe cousins of C pointers - you can never forget you are dealing with a reference. It's a little confusing at first because deref coercion means that methods can be called directly but you still have to remember to say `*r`. (I had to train myself to see that '&amp;' in verbose error messages, because often it was the reason for things not working) 
Aw shucks. My first, remarkably naÃ¯ve, thought was to be impressed the compiler was doing loop unswitching automatically on this code; I know it's possible but I didn't expect it to happen. Of course, when I checked the assembly I find exactly the opposite. Compilers are dumb beasts, and both implementations were tying themselves in knots. Try this instead: let mut accum: u32 = 0; while data.len() &gt;= 2 { accum += NetworkEndian::read_u16(data) as u32; data = &amp;data[2..]; } if let Some(&amp;value) = data.first() { accum += (value as u32) &lt;&lt; 8; } propagate_carries(accum) Note that my understanding is that you can use `LittleEndian` instead and convert at the end, which should be a lot faster still.
This happens every now and then. I have an [example](https://www.reddit.com/r/rust/comments/7hkxez/how_to_check_rls_is_present_before_updating/) of how to use my tool lorikeet to ensure that you check that rls is in the current nightly before updating.
Writing my first Rust code ever :) I'm currently writing a Motorola 6809 emulator with the end goal of emulating a weirdly beautiful eighties console, the Vectrex :) A long way to go and still learning a lot. Enjoying things so far and particularly appreciating algebraic types and other Haskell / ML influences. Great tooling too, Cargo works well and is great having this kind of thing in a systems level language. So far, so good :) 
Yep! Can confirm that this is 30% faster on my machine. This is probably an exact copy of RFC1071 C implementation example: https://tools.ietf.org/html/rfc1071#section-4
The book: https://doc.rust-lang.org/book/second-edition/ That has a table of contents on the left, does it not? Maybe not as perfectly pretty, but it's there.
Thanks for showing what the rule is. I've been wondering this myself lately. But now can you explain *why*? If everything that appears in the trait/body must be declared on the `impl`, isn't that purely redundant? What meaning is it trying to impose?
No, I don't mean the way we categorize pages. ex. [Data types page](https://doc.rust-lang.org/book/second-edition/ch03-02-data-types.html) it's too long and reader can't see how all data types have been categorized. It's more helpful it we had a "table of contents" section on right side.
I mean it's just a list of data types. The only level three headings there are all under the last level two, arrays. It wouldn't be hard to write a userscript to do it if you're really interested.
The principle is that types always have to be known *somehow* before they can be used. This can be by defining or importing the type, or by declaring it as a generic type. We as readers know that `T` is most likely a new generic type, but rustc doesn't (there is no such restriction on generic type names), so impl SomeStruct&lt;Item&gt; { ... } could mean "impl for a new type parameter `Item`", or "impl for the type `Item` that is defined or imported earlier".
&gt; I mean it's just a list of data types. But for a person who new to Rust, It's more than just a list. Also I used data type page only as a single example. Readers can see the whole picture if we had "table of contents" on right sides. And unfortunately I am not a front-end dev and bit busy with the current job(even on some weekends) to contribute this. I shared my idea here because I believe it's worth to share to improve the readability of docs. 
You're missing the point; the question is whether the widening takes place on `a` and `b`, or just on the result of their multiplication. In the presence of overflow there is a noticeable difference. Because rust forbids overflow, I would expect the widening to take place on the multiplication result only, but I'm sure either option will be found counterintuitive by some. Why introduce more sources of confusion in rust when we have simple `as` casts available that make the intended meaning obvious? 
ah, I get it! seems so obvious now. It would be ambiguous to define it only on the struct. love these "a-ha" rust moments.
This is interesting. So I guess this is another way of approaching async that isn't futures based like tokio?
Nice work! It'll make writing network servers much easy in rust
&gt; How would you use a TagSetter interface with this? You would first need to have some element somewhere that implements that interface, e.g. "matroskamux". You would need to get a reference to that element, depending on how you created it. For example you could've directly created it with `gst::ElementFactory::make("matroskamux", None)` or get it from a bin/pipeline with [`bin.get_by_name("name_of_your_matroskamux")`](https://sdroege.github.io/rustdoc/gstreamer/gstreamer/trait.BinExt.html#tymethod.get_by_name) &gt; `bin.get_by_interface(TagSetter::type())` ? That would also be an option if there's an element implementing that interface in your bin. &gt; How do you cast the Element to TagSetter As all those elements you would get like this are of type `gst::Element`, you would have to cast. For that there is the [`Cast`](http://gtk-rs.org/docs/glib/object/trait.Cast.html) trait from glib-rs. In your case you could do `element.dynamic_cast::&lt;gst::TagSetter&gt;()` as the type-system has absolutely no idea about this (if it had, you could use `upcast`, which only works if the type-system knows that your type implements that interface, or inherits from that base class). &gt; and where are the add_tag* methods? Slightly hidden in the [`TagSetterExtManual`](https://sdroege.github.io/rustdoc/gstreamer/gstreamer/trait.TagSetterExtManual.html#tymethod.add) trait. rustdoc is not ideal for finding anything with this kind of API. You have that trait (and all other relevant traits) in scope if you `use gst::prelude::*` btw. In your case you could e.g. do (once the trait is in scope and you have a `gst::TagSetter`) `tag_setter.add::&lt;gst::tags::Title&gt;(&amp;"some value", gst::TagMergeMode::Replace)`. I'll write an example using `TagSetter` in the next days and will link it from here.
`impl&lt;T: SomeTrait&gt; T { ... }` is this saying what I think it is saying? "for everything `T` that implements trait `SomeTrait`, implement for `T` *whatever*"? so I could do something crazy like implement a custom function for all `Copy`-implementing types?
I think many ownership problems can be fixed by modeling the problem in terms of structs and methods on them (basically OOP style), since methods can operate on owned struct fields. When the computation is done you can extract the fields you need from the structs to come to an answer. Methods also need less explicit arguments compared to plain functions, which can yield more readable code. Here is my [code](https://github.com/foo-jin/adventofcode/blob/master/src/seventeen/d22.rs) for that day's problem. Mind you, I am also still learning rust so this is probably not the best code, but it gives you something to compare with.
what about this? https://github.com/zonyitoo/coio-rs/issues/56 thats another coroutine lib, which is just incompatible with std. how did you solve that?
Impressive work ! I'm a bit surprised by the [benchmarks](https://github.com/Xudong-Huang/may#performance), I know it's a really tiny benchmark on a trivial task but I'm pretty surprised to see the project being 30% faster than Tokio. Anyone know what happens here ? 
That's not crazy at all, and yes.
Depending on where people are coming from, requiring explicit casting for widening operations is seen as counterintuitive by some and I'm sure there are people out there who use `(u32, u32) -&gt; u32` or similar, introducing a greater risk of overflow because they dislike `as`. It's really the sort of thing better hashed out in an RFC, rather than argued over in a reddit thread where not much would come up the effort spent.
Hi, Sorry for spamming again with a screenshot of another site. But some UX in [this API doc](https://docs.ckeditor.com/ckeditor5/latest/api/module_block-quote_blockquotecommand-BlockQuoteCommand.html) is so much better and I believe we can get some design ideas from them to increase the readability of Rust API docs. Unfortunately I am not a front-end dev and bit busy with the current job(even on some weekends) to contribute on this. I shared my idea here because I believe it's worth to share this to improve the readability of Rust API docs. Also I am not an employee/ user or any other person who relates to that editor and have no intention to share it or market it. Also I know that a new suggestion has a less value without having a proper implementation road map :( But please check the screenshot and the link if it's okay. I found they are using open source project https://hexo.io/ and https://www.npmjs.com/package/umberto to map contents from different repos, but don't know that their website code is open source(It will not be mostly) 
Futures have a lot of overhead?
yes, this doesn't depend on mio, I use the generator way to impl every thing.
I noticed that. But seems I did't have such issue in may library. Or at least I didn't see it. If you encounter it and have a way to reproduce just open an issue:)
Actually I'm not an expert of tokio. In May I use the event notification mode for async IO, but rust future use the poll mode, I don't know why there is such big difference here. Any way I just do my best:)
Hi! You note in the pull request that your mother tongue isn't English. Just as feedback: Your English is actually very good and easy to follow :). I sent you a PR here with some proofreading: https://github.com/Xudong-Huang/may/pull/1 The main thing: it's `stackful`, not `stackfull`, which also used to trip me for ages ;). I love the library, especially the _extensive_ set of examples. One question though: is there a possibility to build a futures executor on top of it?
`Direction` is copy and indeed small, so I think you could just pass value into those `turn_*` functions. But just curious, wouldn't using (dx, dy) or even 0, 1, 2, 3 in mod 4 to represent directions be better? You could easily make a mistake in those `turn_*` functions if it is generated manually. 
I used this exact simile in my [RustFest Zurich talk](https://www.youtube.com/watch?v=JR6aHXlRAOM). The talk contains a few other things I consider worth knowing.
Thanks! I'm not a future expert. I think it's possible to build an executor on top of may. But it's not a good idea. First it and another layer which call to std::thread, and secondly each may's coroutine do have a limited sized stack, and you have to tune it according to your application. And Future's one good feature is that you don't worry about the stack which May is lack of.
Sorry for not adding a watermark on vendor name :(
&gt; First it and another layer which call to std::thread, I don't quite understand that. The future interface does not mandate (or touch) the execution layer, so any executor is just a thin layer over the underlying system. &gt; and you have to tune it according to your application. This is true also for using futures with e.g. futures-cpupool and tokio, which impose certain restrictions on the futures to use. Also, their execution model is rather flat when it comes to stack usage. &gt; And Future's one good feature is that you don't worry about the stack which May is lack of. That's not how I read futures: they are a very powerful abstraction over combination and decision making in deferred code.
You are right. I mess up with tokio. Call future should be fine within coroutines. 
As a matter of interest, what are some potential applications of this project?
Absolutely. The "smol" part mainly refers to keeping it usable on microcontrollers. As for specifics: * Jumbo frames should be pretty trivial, I just didn't have a need for them. * I do not understand congestion control and it terrifies me. * Configurable rto_min seems like a total necessity because right now the httpclient example transmits everything twice. I just am not completely sure on the best API for it (a global variable? Meh, but maybe. per-socket? Seems like a waste of space, but works better if multiple interfaces are involved...) * Selective ACK is basically the inverse of reassembly, right? Then you should be able to use reassembler to keep track of SACK ranges, and this wouldn't even increase code size much. I will gladly take patches for all four.
If it's not intentional you woul notneed to state return in lines 70, 103, 115. In rust explicit return statements are only used for early returns otherwise you can just state the value you want to return without semicolon: fn virus_infect&lt;F&gt;(mut grid: Grid, mut step: F, mut x: isize, mut y: isize, mut d: Direction, n: usize) -&gt; usize where F: FnMut(&amp;mut Grid, &amp;mut isize, &amp;mut isize, &amp;mut Direction) -&gt; usize, { let mut total = 0; for _ in 0..n { total += step(&amp;mut grid, &amp;mut x, &amp;mut y, &amp;mut d); } total }
They are supposed to have smaller overhead than allocating a stack per green thead, so the result is interesting, and worth studying a bit...
Thanks, I've updated my implementation!
&gt; Depending on where people are coming from, requiring explicit casting for widening operations is seen as counterintuitive by some Sure, but at least the compiler will illuminate them immediately with a helpful error message. With implicit widening at *some* places you have to know the rules to understand what's going on, especially when type inference is being used. &gt; I'm sure there are people out there who use `(u32, u32) -&gt; u32` or similar, introducing a greater risk of overflow because they dislike `as`. Luckily we have overflow checks in debug mode :-)
Something that annoys me is how the VS Code plugin for RLS will try to update itself every time you start. This is usually fine, but with the recent breakages it means a working RLS will go and uninstall itself, then not be able to install the latest one (because the build failed and `rls-preview` doesn't exist).
So umm... what is the suggestion exactly? You've just posted a screenshot of another documentation tool without saying what in particular you think the Rust documentation could benefit from doing.
From memory, clippy will actually generate a warning when you pass a small `Copy` type by reference instead of value.
As far as I've seen/tried, it looks like it's not possible at the moment, but I'd love to be proven wrong. I feel like there *should* be a way to cast a number literal to `T` statically. (Maybe when the [type-level integer RFC](https://github.com/rust-lang/rfcs/issues/1038) lands? Not sure if it's included in that though. Maybe I'll ask in a separate thread.)
Just wanted to say thank you for this. I haven't used it yet but quickly skimmed through the documentation and the API looks much better than futures/tokio. I can finally write composable async libraries and server-side programs in Rust.
You missed my point. Overflow checks in debug mode won't help if they never test with a value that high. (I'm reminded of the installer for my old Formgen release of Mystic Towers, where it won't install in DOSBox or on my retro-gaming PC because there's too much hard drive space and they used a signed variable for the free space check. Luckily, it's possibly to bypass the installer and manually copy the game into place.)
I'm not OP but a few things: * A short and concise list of traits implemented by a type (and a list of implementors on the trait's page). This kind of exists now but it's way too verbose if you just want to check if a trait is implemented or not. I wouldn't mind this using the sidebar either as Rust doc pages can get really long but I'm fine with the current layout as well * Slightly unrelated, but list the traits implemented through blanket impls. Right now `Into` is basically not listed anywhere, yet a bunch of types implement it through the blanket `From` implementation * Have different verbosity levels for the docs. Right now you either only see the function signature or the whole documentation. When exploring a new API (or using the docs as a reference) I sometimes just want to see the signature with a short description, not all the examples, panics, etc. * The tree style navigation is really nice (while the tree isn't too deep)
What would you use the Param variant for? Wouldn't that be a generic that you're never able to use?
This is amazing. Looks very simple to use (and I love the fact that it even supports cancellation) and apparently, it's even faster than Tokio. Very, very, very cool. And my girlfriend's name also happens to be May (Meihua). What's not to like? :) Gonna give it a spin right away!
No, I don't think so. You can try out this one I built: https://github.com/agemocui/ruyi/blob/master/examples/echo2.rs It uses future but the benchmarks are interesting with the following params for may echo client. 1. -c 100 -l 100 2. -c 100 -l 16384 3. -c 1000 -l 100 4. -c 1000 -l 16384 Run ruyi echo server: $ RUST_LOG=debug cargo run --example=echo2 --release -- -w2 Run may echo client $ target/release/examples/echo_client -t 2 -c 1000 -l 100 -a 127.0.0.1:10007 
I wonder what your issues with tokio/futures are?
Coio's great as well. I was able to convert Flowgger's [1] synchronous and blocking I/O routines to Coio in minutes, with a massive speedup on environments where context switching is not cheap. Coio doesn't seem to be actively maintained any more, and rewriting everything on top of futures would be too time consuming. So, I'm pretty excited about May. [1] https://github.com/jedisct1/flowgger
Futures is pretty well optimised wrt to overhead. The whole mio/tokio/futures stack can hide quite a bit of things that yield these results but don't support such a blanket statement.
If you had a generic type that was specialized and could do extra things on certain types, you could say, impl&lt;T&gt; MyThing&lt;T&gt; { ... } impl MyThing&lt;usize&gt; { ... } 
Ah, thank you! That makes a lot of sense.
You may want to implement functions that only make sense when the internal type is the right type. The only example I can find in stdlib is that `Write` is only implemented for `Vec&lt;u8&gt;`, not for `Vec&lt;T&gt;`.
This is going to sound cliche, but the best way to improve is to *practice*. With Rust I started off just reimplementing projects I'd done in other languages. Another thing I would highly recommend is to look at existing Rust crates on GitHub, doing a couple small drive-by PRs to get your toes wet. A while back I was looking through the [mdbook] repository and saw a patch of code which looked messier than it needed to be. I did some basic refactoring and cleaned it up as best I could given the skills I had at the time and the authors were really appreciative of my efforts. Landing the PR taught me loads about code review and what "good Rust" looks like... Fast forward 10 months and I'm now one of the project's maintainers and the second biggest contributor overall :) Another really good technique for improving your skills is to go out and teach others. For example, people post to the [user forums] all the time asking for help understanding lifetimes or a particular API from the standard library. Even if you're not too confident with the topic, try to figure out what issue they may be having or what area the don't understand properly, and see if you can solve the problem together. Something I've found is that people in the Rust community are super nice! So even if you get something wrong nobody's going to get angry or call you an idiot. Instead they'll usually take the time to help understand the idea at hand, and everyone'll be the better for it. [mdbook]: https://github.com/rust-lang-nursery/mdBook [user forums]: https://users.rust-lang.org/
&gt; So, how did other new rustaceans overcome the invisible barrier There are basically three things involved in that process to overcome this barrier * code * code * code The important lesson here is that you can't really learn Rust (or any other language for that matter). You can learn the basic rules as you pointed out in your `level 1` but you can't really go any further without writing a decent amount of code. And in doing so try to reevaluate your code and try to improve what you have written. This will lead you naturally to all the obstacles and you can ask here to solve them. This will much more help you to internalize concepts because you will understand WHY you need them. You can't just learn the solution â€“ they mean nothing to you â€“ you first need to understand what the problem is. This sounds very meta and stuff but trust me. I never understood monads and was reading many articles about it without much luck. The first time i had an idea what they are was the exact moment i needed them. I had a problem and some nice dude was telling me that i need a monad and was elaborating why etc. Only after you ran into a problem and apply the solution you have an idea whats going on. You cannot just throw design patterns at people and wonder why they don't use them. For me programming is like Chess. Understanding the Rules is easy â€“ i can teach everybody the rules of chess in like 5-10 Minutes. Winning at Chess takes a Magnitude more time and effort. You just can't get around to just play Chess and try to apply and find strategies you can apply in different situations. So what should you do now? Just start something â€“ something you're interested in or rewrite some existing software you have written into Rust. The golden Rule is write Rust and meet the obstacles and ask people how to overcome them one by one. I think there is no book that can avoid that process. 
Is it possible to create a library that wraps around Diesel, if types are not known beforehand? The Getting Started tutorial uses infer_schema, and that's not possible afaik if you don't know which types will be supplied beforehand.
If you tell me which intrinsics you need and which platform exactly you are targeting Santa Claus might implement them for you before the end of next week 
It's primary use case is currently as the network stack for the [ARTIQ](https://github.com/m-labs/artiq) real-time control system for quantum information experiments, where it runs on soft-cores embedded into various FPGAs. It has also drawn attention from people working on Redox and other RTOSes.
It's primary use case is currently as the network stack for the [ARTIQ](https://github.com/m-labs/artiq) real-time control system for quantum information experiments, where it runs on soft-cores embedded into various FPGAs. It has also drawn attention from people working on Redox and other RTOSes.
I actually meant, use generic LLVM vector operations so that any target platform can translate that into their own operations. But I think Rust can't do that yet, can it?
The default stack size seems to be 4096 bytes. What happens if you exceed it?
What you want is to either .collect the args iterator in main (so you can own the arguments) or just do two different calls to env::args() (so you get to iterate twice).
As a beginner coming from C# I find type inference in Rust quite confusing because it works backwards. Lifetimes seem much simpler conceptually, though reasoning through concrete scenarios can still be tricky. What I absolutely don't get is implementing traits on generics.
well, that's another big topic. simple answer: stack overflow!! so you have to tune the default stack to a proper value by may::config::set_stack_size()
&gt; What I absolutely don't get is implementing traits on generics. That's something what is actually easier to understand to me. Consider the following structs: struct Cat; struct Child; #[derive(Debug)] struct Bird; #[derive(Debug)] struct Toy; Both children and cats have a trait, they present things to their parents "Look what I have here". So we can impl a trait Present for Cat and Child: trait Present&lt;T&gt; { fn present(self, object: T); // self a trait of the implementor, object: the object to present } The trait allows the implementing struct to present any kind of object aka a generic object. impl &lt;T&gt;Present&lt;T&gt; for Cat where T: std::fmt::Debug { // where the generic type T is presentable (ie, Debug, Display) fn present(self, object: T) { println!("meeeooww {:?}, object); } } impl &lt;T&gt;Present&lt;T&gt; for Child where T: std::fmt::Debug { fn present(self, object: T) { println("Look, what I have here: {:}", object); } } Now, both Cat and Child can present generic objects that themselved implement Debug (aka presentable in Debug mode). The &lt;T&gt; After Present is there because it's part of the trait's signature to pass it to the fn if needed. The &lt;T&gt; before Present is there to bring T into scope of the trait. We do nowhere do use T; therefore the type T would not exist (wouldn't be in scope). &lt;T&gt;Present.. 'brings' a generic type T into scope. let toy = Toy{}; let bird = Bird{}; child.present(toy); let cat_toy = Toy{}; cat.present(cat_toy); cat.present(bird); Now, any child and any cat can present anything that itself is presentable (here Debug).
this is a real example of porting thread based code to coroutine based. I'd like to see how rocket can be affected, but it failed to build the latest today. Have fun.
I can use it in trivial cases like these, but whenever I try something interesting I run into a thicket of conflicting trait implementation errors, orphan rule issues, etc.
An interesting aspect that I realized today: now that associated constants (`const` items in `impl` blocks) are stable, we can effectively have generic constants by declaring an associated constant on a generic type.
my knowledge is only suitable for trivial problems :)
It took me 6 months of coding Rust before I even wrote my first lifetime annotation. And I was writing clippy code (!) â€“ your mileage may vary. Today I have a good understanding of lifetime concepts, which is not to say I won't mess up every now and then and have to switch lifetimes around to make the compiler happy. Don't worry too much if you find yourself relying on the compiler to find the right code to write. All of us do this to some extent.
Thank you!
I wish I had known more about computers, systems programming, about various people's development practices and how to manage my time spent coding more efficient. I learned all that and more at my first full-time job.
&gt; I find type inference in Rust quite confusing because it works backwards Type *inference* (e.g. [HM](https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system)) has no notion of direction built into it - even if type-level functions (associated types), method lookup, coercions, etc. can add their own directionality on top, the "type variables" themselves can be unified arbitrarily. What C++, (Java?), C# etc. do isn't really a separate algorithm, and is sometimes referred to as "type deduction", where types can be omitted because they're *fully known* from the type of a *previously evaluated* expression. That is, if you can type-check `foo().bar();` correctly, then `auto x = foo(); x.bar();` has *zero* additional difficulty and doesn't require a separate algorithm. (NB: some of these languages might have additional rules along the lines of "generic parameter deduction", but they tend to not keep track of information across e.g. statements, so they're not in the same class of inference algorithms as HM.)
Excellent work! A Tokio benchmark with 3 threads would be cool.
Looks really cool! A bit of a licensing concern though: the [generator](https://github.com/Xudong-Huang/generator-rs) dependency is licensed under the LGPL, and since rust statically links dependencies, any project that uses the May crate will also be statically linking the LGPL crate, and have to worry about all of the licensing implications of that. IANAL, and everything may actually be more kosher than I think, but L?GPL code almost always presents a not-insignificant difficulty in getting adopted by companies with cautious lawyers in my experience. Can anyone with more experience on the topic weigh in?
Great result for such a small diff. Thanks for the work and the articles!
Really curious what motivated this, could you elaborate on a use case?
Rust can do that already, the crate you are looking for is stdsimd. You just need to use the generic vector types, they support all operators. Depending on what you want to do you can look into the faster crate as well (less control but more convenience). If a particular intrinsic is missing just open an issue.
The benchmark is testing apples to oranges. The tokio example being benchmarked is 1) an example (not optimized), 2) demonstrating how to distribute work across threads using message passing, which is counter productive for the work at hand.
Futures add zero overhead (look at the implementation and you can see this).
The benchmark is testing apples to oranges. If you want to post competitive benchmarks, at least pay the respect of trying to write up equivalent benchmarks.
I like looking at (small) pieces of code and trying to understand every single aspect. It's typically how I (try to) learn new languages as well, i.e., not read a tutorial or guide, but "jump in at the deep end" (that's what I called it [here](https://deterministic.space/learn-programming-jump-in-at-the-deep-end.html)). It may work for you, too.
My 2c. The biggest thing to leren is that the rust compiler is crabbey, it enjoys complaining about everything. I've just stopped trying to `write a fn correctly the on first try`. It will complain, I will fix it, rinse and repeat. 
Oh, I got confused by the name--it's called stdsimd but it works on `#![no_std]` in spite of that.
does it cause a memory access violation (unsafety), or does it safely terminate the coroutine? Because that's a pretty big disadvantage if it can overwrite arbitrary memory in safe code!
My actual use case is the same as for `lazy_static!`: having a â€œglobalâ€ value that can be initialized only once and use many times without passing a handle around, where initialization cannot (easily) be done in a `const` expression (in this case, parsing a TrueType font file). I donâ€™t *actually* need the deinitialization aspect, I only wondered how that could be made to work. I made this as part of a toy project: the goal is more to experiment and learn some stuff, than to actually use the end result. That said, Iâ€™ve see globals being deinitialized during shutdown in Firefox. I believe this is so that Valgrind doesnâ€™t mark corresponding heap allocation as leaks. (False positives would hinder the detection of real memory leaks.) By the way, this code work by using the â€œdouble-checked lockingâ€ pattern, which is often unsound. I *believe* this implementation is made sound by the use of atomic pointer operations, but Iâ€™d appreciate a safety review. The usual recommendation is to use `Once` instead, but this may need to run the initialization code more than once in case `get_or_create` is called again after `drop`.
I'm at the very end of the week, but hey, I'll say something anyway. I'm currently rewriting some stuff to get better, specifically I'm porting my GameBoy (dmg) emu from c#, and a toy compiler that doesn't have a linker... Or a lot of stuff, from c++
Typically you want to look at tail latency in addition to average throughput. With MAY, tail latencies seem considerably worse than with threads or tokio.
&gt; or even write a fn correctly the on first try without the compiler complain about moved / consumed / borrowed / immutable variables Heh, I do this all the time, and it's actually one of the reasons why I love Rust. I usually write pretty sloppy code, then let the compiler tell me where I haven't thought about it hard enough. If all your code compiles first time, you are either a) amazing, or b) using a language that doesn't do much/any verification.
It's something that seems desirable but I'm not aware of any concrete proposals for this feature. I'd say desired but not planned.
From a quick look at this issue and also the discussion at the relevant RFC, this looked quite serious to me. If I understand this correctly, this means that we can't safely implement work-stealing scheduler in Rust without going `#[no_std]`. Am I missing anything? I wonder why this is not a problem for may. 
I think you have an error here: $ git remote add may https://github.com/hyperium/hyper.git You should probably use address of your fork here. EDIT: This is the patch https://github.com/Xudong-Huang/hyper/commit/0765ef06a0bf95c65e4434603cad4cea7e2826ab
I would be interested in knowing how it works internally. Is there going to be some kind of â€žunder the hoodâ€œ write-up? Also, I'm a bit wary, I expect there's going to be *some* kind of catch. Is it compatible with calling into C? After a coroutine is created, is it bound to a single OS-level thread, or is it allowed to move between them? If the later, how do you solve things like something not being `Send` on the stack?
The font you're using on your website hurts so much to read, because of what it does to apostrophes. This is how it is rendering in my browser: Ok, it' s almost the same as the thread version when it should unequivocally be: Ok, it's almost the same as the thread version Every blog post of yours that I read, I have to manually edit the page in the Inspector to delete the font choices. The work you're doing is really great, and I *want* to read about it, but *not with this font.* **EDIT:** [screenshot.](https://i.imgur.com/shBJYDG.png) This is consistent throughout the pages. Using latest, stable Firefox on Windows 10. Also happens in Chrome on Windows 10.
You could do something like this: struct Cat { mumble: String } struct Baby { mumble: String } #[derive(Debug)] struct Toy; #[derive(Debug)] struct Bird; trait Present&lt;T&gt; { fn present(self, object: T); } trait Mumble { fn mumble(&amp;self) -&gt; &amp;str; } macro_rules! impl_mumble { ($m:ty) =&gt; { impl Mumble for $m { fn mumble(&amp;self) -&gt; &amp;str { &amp;self.mumble } } } } impl_mumble!(Cat); impl_mumble!(Baby); impl&lt;T, M&gt; Present&lt;T&gt; for M where T: std::fmt::Debug, M: Mumble, { fn present(self, object: T) { println!("{:?} {:?}", self.mumble(), object); } } fn main() { let cat = Cat{mumble: String::from("Meow")}; let baby = Baby{mumble: String::from("Uuuaaahh")}; baby.present(Toy{}); cat.present(Bird{}); }
That's awesome! I've been implementing REST apis lately too and this definitely will make things easier! I'll probably take a deeper look soon ;) Two questions: 1. Why do you depend on hyper and not reqwest? Since you expose a synchronous API, what it the advantage of the async version of hyper? When implementing my REST APIs, reqwest was enough and was quite simple to use... 2. Testing: Testing is kind of hard when you depend on an external service. For the implementation of restson using httpbin.org to perform real requests is easy, but how could I test _my_ work that uses restson? In other languages the answer to this is mocking, but haven't found an easy solution to this in Rust. For example, for mockito you need to disable parallel tests, and mock_reqwest requires passing your client around in generic functions. Would you have a suggestion as to how can mocking be applied to a code using restson? Thanks again for restson! I also greatly appreciate blog posts like this explaining why and how an API was created.
Cool, thanks. Some good tips in there. I couldn't work out what `use` incantation to use to avoid having to qualify every use of the enumeration constructors. 
:) thank you very much. That way only structs that can mumble can present and vice versa? is that correct? There is now no way a cat could silently present things? I came up with another solution. Don't know if it makes sense. I made the actual mumble strings to const and impl Mumble independent from Present. struct Cat {} struct Baby {} struct DrunkGuy {} const CAT_MUMBLE: &amp;str = "Meeooww"; const BABY_MUMBLE: &amp;str = "Uaaahhh"; const DRUNK_GUY_MUMBLE: &amp;str = "Eeehhh"; #[derive(Debug)] struct Toy; #[derive(Debug)] struct Bird; trait Present&lt;T&gt; { fn present(self, object: T); } trait Mumble { fn nonsense(self) -&gt; String; } macro_rules! impl_mumble { ($struct:ident, $mumble:expr) =&gt; { impl Mumble for $struct { fn nonsense(self) -&gt; String { String::from($mumble) } } } } impl_mumble!(Cat, CAT_MUMBLE); impl_mumble!(Baby, BABY_MUMBLE); impl_mumble!(DrunkGuy, DRUNK_GUY_MUMBLE); macro_rules! impl_present { ($s:ident) =&gt; { impl &lt;T&gt;Present&lt;T&gt; for $s where T: std::fmt::Debug { fn present(self, object: T) { println!("{:?} {:?}", self.nonsense(), object); } } } } impl_present!(Cat); impl_present!(Baby); fn main() { let cat = Cat{}; let baby = Baby{}; let drunk = DrunkGuy{}; baby.present(Toy{}); cat.present(Bird{}); drunk.nonsense(); } in the fn nonsense() I return a String instead of a &amp;str to avoid the lifetime / borrowing thing, I don't know how to solve. &lt;edit&gt;The drunk guy doesn't mumble --&gt; he does. forgot the println!&lt;/edit&gt; and I don't know exactly the difference between typ and ident type declaration in the macro and why it's needed. Actually Drunk could be a trait applied to Cats too :) 
Thanks, that's a good idea. I was assuming that I could treat a simple enum like in C where it's just syntactic sugar on top of an integer (so + and mod work), but once I figured out that was wrong I figured I'd stick with it and see how the resulting code came out. I used matrix multiplication to implement the same thing in python for a different challenge, and there's a clever use of complex numbers in a solution on /r/adventofcode. 
Not come across clippy but I'll investigate. Sounds like a linter for rust? 
Aha, that's good to know. Thanks! 
Yep. It hooks into the compiler and is a really high quality linter.
I'm trying to figure out how to write a function `imap_connect` which wraps [this struct](https://docs.rs/imap/0.6.0/imap/client/struct.Client.html). My main issue is that there are two variants of the struct you can have, `Client&lt;TcpStream&gt;` (unencrypted) or `Client&lt;TlsStream&lt;TcpStream&gt;&gt;`. From my understanding of trait objects, if `Client` was a trait then I would be able to write a function like this: fn imap_connect(someargs: String) -&gt; Result&lt;Box&lt;imap::client::Client&gt;, std::error::Error&gt; { // whatever it takes to construct a Client and then box it. } But I can't do this for the given struct (presumably because it's a struct and not a trait -- though the error from the compiler is a bit confusing IMO). Is there a way to work around this without submitting patches to the upstream package? Can I implement my own trait somehow which allows me to abtract away the type argument? I'm only going to be using the `impl` methods defined on the Client so there must be a way of doing this.
This is pretty damn cool
thanks for the detailed reply! The valgrind stuff is definitely worthwhile
&gt;the weird 1's complement TCP/IP checksum (there's a reason your NIC has checksum offload) There's some weird hackery that takes advantage of algebraic properties of addition to make it faster. &gt; copying memory between kernelspace and userspace, Isn't that what memory mapping and system calls like sendfile, splice, tee and vmsplice are supposed to deal with? Of course, I have always found splice, tee and vmsplice to be a mess for me to understand. But theoretically it should be possible to create a page in user space and then gift the page to the kernel without copying. But I never figured out how to do that. I recall reading about a BSD TCP/IP implementation a long while ago that does something similar for higher performance. &gt;firewall (nf_conntrack) It might be interesting for testing to temporarily disable this if it is possible.
Handy link to the [non-lexical lifetimes RFC](https://github.com/rust-lang/rfcs/blob/master/text/2094-nll.md). 
I'm not sure why `slice::from_raw_parts` and `Vec::from_raw_parts` would have different requirements on the pointer. It seems to me that `Vec::&lt;size_80_struct&gt;::from_raw_parts(p as *mut size_80_struct, 7, 7)` would have been fine (i.e., just fix the capacity). Somebody want to contradict me on this? I realize I'm ignoring alignment, but `slice::from_raw_parts` ought to have alignment requirements as well.
How many threads did the 0.10.x branch use? Because that's not mentioned in the article. Does it use 1 or as many as there are cores on the machine? Also, the latency profile (not the throughput) of the thread based hyper is much better than any of the other implementations. Which I think is quite interesting as well. 
For me the demo doesn't run. Here's the error and stack trace: &gt;eror loading Rust wasm module 'wasm_webgl_rs': TypeError: cannot use the given object as a weak map key Stack trace: __extjs_b67f2836bfcab57acb8e21dbe580790ff03192f9/Module.STDWEB.acquire_rust_reference@http://og.tn/webgl-rs/example/html/index_files/app.js.download:125:516 from_js@http://og.tn/webgl-rs/example/html/index_files/app.js.download:119:1980 __extjs_01edeef8b64800df435fca159d4fec9096720875@http://og.tn/webgl-rs/example/html/index_files/app.js.download:44:17 wasm-function[47]@http://og.tn/webgl-rs/example/html/index_files/app.js.download:20203:1 wasm-function[41]@http://og.tn/webgl-rs/example/html/index_files/app.js.download:4677:1 wasm-function[75]@http://og.tn/webgl-rs/example/html/index_files/app.js.download:27877:1 wasm-function[42]@http://og.tn/webgl-rs/example/html/index_files/app.js.download:16914:1 wasm-function[176]@http://og.tn/webgl-rs/example/html/index_files/app.js.download:62308:1 __load@http://og.tn/webgl-rs/example/html/index_files/app.js.download:178:9 __promise&lt;@http://og.tn/webgl-rs/example/html/index_files/app.js.download:195:17 promise callback*@http://og.tn/webgl-rs/example/html/index_files/app.js.download:191:27 @http://og.tn/webgl-rs/example/html/index_files/app.js.download:13:9 @http://og.tn/webgl-rs/example/html/index_files/app.js.download:7:2 app.js.download:200:17
Rust provides alignment for allocators. Mismatches will end up badly if allocator actually makes use of that. This isn't an issue for slices, as slices don't involve an allocator.
LGPL is a tricky beast. The nasty part being this: &gt; d) Do one of the following: &gt; 0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source. &gt; 1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version. This is actually hairier in practice then just "provide all source". You have to publish anything that includes generator-rs in a fashion so that you can _replace and relink_ with a modified generator-rs. In Rust, this is harder by every static library being bound to a specific compiler version. Interestingly, this blocks some uses, for example shipping a signed binary (e.g. for iOS), as the user cannot re-sign the new version. I could actually see some of the problems being solved by a cargo plugin. 
Weird for me it renders fine, Safari OS X.
&gt; leaking memory is safe. I was kind of surprised to learn that leaking memory is safe in Rust (you can do it on purpose with mem::forget!) For reference, this is due to [RFC 1066](https://rust-lang.github.io/rfcs/1066-safe-mem-forget.html). Long time ago, there was `thread::scoped` API which could cause undefined behaviour when its destructor wasn't called. This could have happened in safe Rust by making `Rc` cycles. For this reason`thread::scoped` was removed from Rust, and leaking memory was marked as allowed for safe code to state that unsafe code should consider that its destructors may not be called.
Hm, not loading. Are we giving it the hug of death? Ah, [the gist](https://api.github.com/gists/726b41acb227a0188719df38fa93155d) is overloaded &gt;{ &gt; "message": "API rate limit exceeded for 163.47.104.188. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)", &gt; "documentation_url": "https://developer.github.com/v3/#rate-limiting" &gt;}
Stacks per green thread can be amortised across the lifetime of an IO task while futures are per IO operation.
Likewise for Chrome (edit: and Firefox) on macOS.
Thanks for the reply! If you do take a deeper look, all feedback is welcome and appreciated. Implementing the library directly with Hyper was not significantly more complex than using reqwest so I decided to go with Hyper. Making a synchronous API would have been a bit simpler with reqwest but this was also a learning experience for myself so I did not mind the added complexity. So to answer your question, I don't think there are major advantages/disadvantages from using Hyper. I have been thinking about also providing async API in a similar easy-to-use manner (if possible encapsulate and hide the Hyper/Tokio event handling inside the library as much as possible). The second question is a valid one but unfortunately I do not have a direct answer to it. This is also interesting for myself so I have to look into it. Though, probably not tomorrow :) 
[screenshot.](https://i.imgur.com/shBJYDG.png) Firefox on Windows 10. Also happens in Chrome on Windows 10.
[screenshot.](https://i.imgur.com/shBJYDG.png) Firefox on Windows 10. Also happens in Chrome on Windows 10.
It probably is. I just reported an issue with code that demonstrates the problem (only in a possible way, but I'd still be able to create UB in a code without any `unsafe` in it). https://github.com/Xudong-Huang/may/issues/6 I think it should probably go through a thorough soundness audit, I was able to find two places where UB surfaces to client code in an hour. Does anyone have the time to go through the code?
woah, yeah thats pretty weird. I've tried it on Firefox and Chrome on OS X and it's ok. Must be a windows error in the font.
I'll answer myself. It moves between threads without regard to the stack containing things that shouldn't move between threads.
Perhaps check to see if Lato is installed locally? Could be a version that messes up the `â€™` character?
It seems to work again now: https://gist.github.com/anonymous/726b41acb227a0188719df38fa93155d [Mirror](https://paste.ee/p/98q1K) in case it is down again.
amazing ergonomic upgrade to be sure, but now lifetime errors will be much more rare and i'm afraid only more confusing for beginners (because the low hanging fruit is already picked by the compiler so only the difficult cases remain.)
Fine on Ubuntu Linux, both Chrome and Firefox. Using EN/US locale for everything, in case that matters. 
Interesting. This is now allowed: enum List&lt;T&gt; { Nil, Cons(T, Box&lt;List&lt;T&gt;&gt;) } impl&lt;T: Clone&gt; Iterator for IntoIter&lt;T&gt; { type Item = T; fn next(&amp;mut self) -&gt; Option&lt;T&gt; { match &amp;mut self.0 { &amp;mut List::Nil =&gt; None, &amp;mut List::Cons(ref mut x, ref mut rest) =&gt; { let r = x.clone(); self.0 = (*rest).take(); Some(r) } } } } Where previously `self.0 =` would fail. However, it appears one has to be extra careful as this compiles but gives wrong results: &amp;mut List::Cons(ref mut x, ref mut rest) =&gt; { self.0 = (*rest).take(); Some(x.clone()) } 
Just a question, but using `Rc` doesn't leak memory (anymore), does it?
It will if you make cycles. It's a tradeoff of using reference counting.
I had no idea programming languages had multiplayer. Interesting; is this some new parallel programming feature?! ***Ahem.*** You'll be looking for /r/playrust instead.
Does it yield the next item instead of the current one?
The most obvious way to me would be to explicitly return an enum (disregard bad naming): enum ImapClient { Imap(Client&lt;TcpStream&gt;), ImapS(Client&lt;TlsStream&lt;TcpStream&gt;&gt;), } Though this does not really 'abstract' anything and you have to pattern-match explicitly.
What do you mean by "cycles" - cyclic references or something else? What would be an example in which the `Rc` leaks memory? Rc&lt;Rc&lt;T&gt;&gt; // is this safe?
**EDIT:** The following code snippet *does* work with the new NLLs, I just used the wrong `#[feature]` directive. fn foobar(mut a: &amp;mut [u8]) -&gt; &amp;mut [u8] { loop { if true { return a; } a = &amp;mut a[1..]; } } ~~This doesn't compile. :/ I would have hoped to get it resolved with NLLs. I have it several times in my code, and IIRC someone suggested that this might be fixed with NLLs. Currently I'm using `unsafe` to resolve this (a hint as to why this is actually not safe as I assume is also welcome, of course).~~
This works for me on nightly: https://play.rust-lang.org/?gist=d6fc91d7962d0d9020ea4369f1f24124&amp;version=nightly Did you update to newest nightly? Do you have `#![feature(nll)]` at top of your code?
... when the attention span of the youth goes below 0
Similar comments came up for some other design changes, and it seems reasonable. At least at first. The problem is that this line of thinking requires you to see the feature in isolation. You see its easier at the beginning, but maybe the cliff is harder later? New users, though, aren't going to experience this feature in isolation. They'll experience a whole new language with lots of potentially unfamiliar concepts. They are taking in lots of information and may already feel intimidated. If we can delay some of the harder aspects of Rust until later, it gives them more time to build up knowledge of how the rest of Rust works. The result is more confidence and better momentum going deeper into Rust's features.
There's no need to be needlessly aggressive, you could have pointed this out in a nicer manner. It's not like the README was claiming a thorough scientific comparison.
The first time I ever read about Rust was, as I recall, reading PPYP.
A reference cycle occurs when an object owns a reference to itself, directly or indirectly. Rust example: fn bad() { use std::rc::Rc; struct X(Option&lt;Rc&lt;X&gt;&gt;); let mut x = X(None); x.0 = Some(Rc::new(x)); // cycle! x has a reference to itself } fn main() { bad(); // now there's a X object that can't be freed! } Now `x` will be alive as long as there's a reference to `x`, and there will be a reference to `x` as long as `x` is alive. It's not `unsafe` (memory safety won't be violated), but it's not good: that memory can't be freed.
Oh *duh*. I had `#[feature(nll)]` at the top of my code. :( (note the missing `!`.)
Yeah, cyclic references. Something like this: struct Foo(Rc&lt;RefCell&lt;Foo&gt;&gt;); Could leak if a self reference was created at runtime. I'm on my phone right now, but the idea would be to just make an instance that points to itself by mutating it through the cell.
There's still an issue though: The slice from the vector isn't guaranteed to fulfill the new type's alignment requirements, and thus it might lead to unaligned reads. The solution to this is to take a vector of the type with the higher alignment, and cast it to the type with the lower alignment.
What is `self.0 in` this context?
What does this do!? In my head it simplifies to `return a;`, but you say "I have it several times in my code" so I assume it is not a Nop.
...an **inherent** tradeoff in reference counting. Reference counting is a purely local type of garbage collection and, if `a` has a reference to `b` and `b` has a reference to `a`, then they're kept alive because each is referenced by at least one other thing and the number of objects involved in the cycle can be arbitrarily big. You need something global (ie. a form of garbage collection which employs a garbage collector) to identify which objects are inaccessible despite the presence of reference cycles. (The advantage to collector-less GC techniques like reference counting being that they're easy to incorporate into anything, like C++'s `std::shared_ptr` or Rust's `str::rc::Rc`.) Where a clear chain of ownership can be established (eg. parent&lt;-&gt;child relationships), the solution to the circular reference problem is to incorporate weak references which don't count when determining whether to clean something up. (`std::weak_ptr` in C++, `std::rc::Weak` in Rust.)
I have this pattern several times in my code. The actual code looks more like this: loop { unsafe { // FIXME(rust-lang/rfcs#811): Work around missing non-lexical borrows. let raw_buffer: *mut Buffer = buffer; if let Some((read, header)) = read_header(&amp;(*raw_buffer).buffer)? { buffer.offset += read; Self::from_header_impl(&amp;header)?; return Ok(header); } buffer.read_more(cb)?; } } 
Aww, [this](https://play.rust-lang.org/?gist=3825c1daf68374faeabb1cdacf1341de&amp;version=nightly) kinda thing doesn't work. Oh well. I didn't realise NLL had progressed that far. Nice!
If you use `include_bytes!()` it will get included. If you want to generate your bin at build time then use `build.rs. There is also PHF. You can parse it, serialize to bytes, save to file and deserialize that binary at runtime.
As a beginner, I would respectfully disagree with you on two points. First, I think efforts like NLL help beginners. It smooths out the learning cliff for me (I can pick up basic syntax without having to immediately fight borrow checker issues, even though I know I will need to understand it eventually). Also, I personally prioritize ergonomics pretty high. I want to write what I mean without having to bend over backwards for the compiler (unless I'm really doing something unsafe). And, for worries about inscrutable errors, I think there is always room for the compiler and/or documentation to better explain these. Second, I think you are underestimating the amount of low-hanging fruit left. For example, the toy case below still won't build for me. (I have googled it, I understand the immediate cause, and I am aware I can solve this using tuples, or by moving the problematic reborrow into its own function; obviously that's not my point) fn mutate_two_strings(a: &amp;mut String, b: &amp;mut String, c: bool) { { let e = if c { a } else { b }; e.push_str("(altered by mutate_one)"); } let f = if c { b } else { a }; f.push_str("(altered by mutate_two)"); } Please do not get me wrong. I think the NLL effort (and Rust in general) is fantastic work, and I hope similar efforts will be made to continue to move Rust forward in the future. Cheers. 
Can someone explain to be why that last example compiles? It seems pretty clear to me that it should be illegal to modify `self.0` while the `ref mut x` exists as a borrow. Is this an implementation bug?
There is pair-programming. Some developers do stream their sessions (see Notch). 
Here it is implemented in C++: https://github.com/kurocha/async and yes it is both fast and efficient.
This case is because `take_while` is lazy. It needs a borrow on `sieve` until the completion of the iterator, so until the end of the `for_each`. As I'm sure you know, the solution is to grab the length ahead of time, which the compiler doesn't know it can do in this case. This might be solvable if `slice::len` were marked `const`. But I don't think that's possible currently either, because the length of a slice is runtime information on the pointer, not on the type at compile time.
Thanks. I had it as part of build.rs initially, but didn't want to have to distribute the csv as part of the package. It's 1.4GB, the bin is only 26MB. I guess I can just add to the documentation that if you want anything other than the default you'll have to download and include the lib by path after running the generator.
Yeah, I wasn't surprised it doesn't work, I just kinda hoped it would. In case you're wondering the context of that snippet, it's a modified version from an iterator-based [Sieve of Eratosthenes](https://play.rust-lang.org/?gist=029283430f06c5fa974c51cc099f4fc4&amp;version=stable) I wrote when playing around.
The bare `T` in the `Cons` variant.
&gt; I think â€œundefinedâ€ and â€œunsafeâ€ are considered to be synonyms. My understanding is that 'unsafe' means 'something that can lead the result being undefined/unsound'. 'unsafe' just means that the programmer needs to be careful to not cause undefined behavior.
Author here. The primary target use-case for this is as the backing text buffer for text editors, but it may be useful for other things as well. It's called Ropey because internally it's implemented as a rope. There is still more work to be done on it (hence still being v0.5.0), but it is essentially API-complete (modulo tweaks and maybe some missing trait impls). It is fast, performing between 1.1 - 1.7 million insertions per second on large texts, depending on how coherent they are. I've also designed it with strong Unicode support in mind, and interoperatbility with Rust Strings and string slices. These latest releases (0.4.x and 0.5.x) are a re-implementation based on my experience creating a toy text editor with the pre-0.4 versions. Although that editor is a toy, this library is designed and written to be genuinely useful. Feedback, bug reports, and pull requests are very welcome! If you want to use it in a project of yours, that would be amazing, and I'm happy to work with you on this end. But do keep in mind that it's still early days, and there are almost certainly still bugs lurking in the code.
`built.rs` can download it from the interwebs too. Depends on how often data changes, if it's mostly static then just have a way to rebuild that bin and include that bin.
[removed]
Here's a solution to that `mutate_two_strings` by the way using the current lexical lifetime systems: https://play.rust-lang.org/?gist=5230a28d94a0dbbd0a2bde25e87fa0b9&amp;version=stable
Honestly, I think it improves the situation for beginners. RFC shows [how error messages could look after a change](https://github.com/rust-lang/rfcs/blob/master/text/2094-nll.md#how-we-teach-this), and honestly I feel like it's a big improvement, as they show the third point which was previously invisible in error messages.
Yes, theyâ€™re different and itâ€™s important.
This is happening due to a move. Not really a lifetime issue, and moves were already non-lexical. It can be solved like this. let e = if c { &amp;mut *a } else { &amp;mut *b }; The solution to this issue is clearly suboptimal, as `&amp;mut *` is a really silly hack to rebind a mutable lifetime without moving. `&amp;mut *` for mutable reference type feels like it should be a no-op, and yet, it isn't. 
Excellent.
Been using Rust for 4 years, and I still don't know how to declare lifetimes (if sprinkling &lt;'t&gt; randomly doesn't work, I resort to `clone()`), and I'm still fighting the borrow checker all the time.
&amp;mut &amp;mut amazing
Your example is pretty hard to solve for a compiler, you need a static analyzer for this. I'm not so much into the details of rustc, but I don't think that it even has one right now.
Have you looked at the rope libraries that power the Xi editor? Can you compare and contrast them?
In what way do you mean unsound, in this case? The double-checked locking I know of prevents multiple initialization racing, with a one-time cost of synchronization.
&gt; Rope clones share data, so an initial clone only takes 8 bytes of memory. After that, memory usage will grow incrementally as the clones diverge due to edits. Haven't read the code, but I'm inferring from this statement that you could open a file, memory map it in, and initialize a ropey from it without having to use any actual RAM for the text data - the only RAM used would be the container allocations. Then, once you start modifying text in strands of the ropey, the memory usage would only be the size of the dirty strands. On a separate note, I'm curious how aggressively you merge/split strands.
Could someone give me an ELI5 why coroutines are better than threads?
thanks for the report. I'm using chrome on windows, doesn't work on Firefox. but I don't think browser compatibility is a focus at this point.
C# has a feature called extension methods were you can just declare a method that can be called on any object. Implementation wise it's just syntax sugar for a function that accepts the value as it's first argument.
 extern crate toml; use toml::Value; let parsed = my_toml_file.parse::&lt;Value&gt;().unwrap(); for item in parsed { if let Value::Table(table) = item { if let Some(db_table) = table.get(String::from("database")) { /* do something with db_table here */ } if let Some(email) = table.get(String::from("email")) { /* do something with email here */ } } } Not tested, but this should work.
This is not purely ergonomic upgrade, this is much more important. NLL will allow writing higher performance code compared to what you can do today without resorting to unsafe keyword.
I WANT TO SELECT ALL NODES AT ONCE - **EITHER PRODUCTION OR DEVELOPMENT** - *WITHOUT HAVING TO KNOW THEIR EXACT NAMES.* 
Ah, no, that unfortunately isn't accurate. In general, Ropey assumes that ropes own their data. The data sharing only applies to direct clones of existing ropes. It would be interesting to explore that idea, though, and I think it should be doable. But I wonder how it might impact the API complexity/comprehensibility-wise. Regarding splitting: Ropey implements ropes using a b-tree, and so internal nodes follow typical b-tree merge/split criteria. The leaf nodes (where the actual text data is stored) also generally follow a similar pattern, with a few minor differences. One thing I would like to explore is being more aggressive about merging leaf nodes, to keep the memory footprint down, and there's a note about that in the readme file.
Rust does not guarantee that destructors get called. They're pretty reliable if you're gentle (i.e. don't panic), but if you do, who knows. Usually this is totally fine because the OS cleans up after you when the process exits. But if for example you rely on a destructor to modify the state of a terminal emulator, a panic may leave things a bit wonky.
Lifetime errors don't exist to teach beginners that perfectly valid code is invalid.
I have! I some of the ideas from those are applied in Ropey (the shared data for cloning is one of those). Xi is a super cool project! :-) The biggest difference, I think, is that Ropey is tailor-made for text buffers, whereas Xi has a more general b-tree implementation which is used for many things, including its rope structure. The main impact of that, I think, is that Ropey has a more straightforward and obvious API for the text-buffer use-case, and may be able to optimize things a bit better (I haven't done any benchmark comparisons between the two, though). Some secondary differences (I believe--someone please correct me if I'm wrong): - Ropey tracks all Unicode specified line breaks, not just CR, LF, and CRLF. - Ropey guarantees that grapheme clusters aren't split by its internal representation. (This is useful when e.g. printing text to screen, or working with cursor movement.) I'm also writing Ropey specifically to be a library used by other projects, whereas for the Xi data structures my impression is that's a secondary concern.
Panics have nothing to do with this, we in fact do try to run destructors when there are panics. It is considered safe in rust to leak; and APIs have to be designed assuming things may be leaked. Usually leaking involves explicit mem::forget or Rc cycles; it's hard to trigger by accident.
1. What do you mean by "exact names"? What, in your example, is an "exact name"? 2. What are "nodes" - do you mean the tables "production" or "development"? What do you mean by "all nodes"? I admit that this code will select both production and development tables, but I am not sure which you want to select - if you don't know their names. I assume you want either one of "production" or "development", but how do you choose which one? You can use `#[cfg(debug_assertions)]` if you want different things for debug / release builds, but otherwise, you'd have to know their names. In short, what is the result you want to get? You said you want "all nodes", but what do you mean by that? Please post an example of what you expect the code to output.
For anyone who hasn't had the pleasure: http://cglab.ca/~abeinges/blah/everyone-poops/
Thanks y'all for these tricks, I'll remember them.
That's what I would assume! Maybe a bug should be filed?
This is confusing because the `next` method is implemented on a struct called `IntoIter`, not on the `List` enum, but the definition of `IntoIter` isn't provided. It's probably `struct IntoIter(List)`.
`self` in this case refers to an instance of `IntoIter`, not an instance of `List`, since we're in an `impl` block for `IntoIter`. That said, I'm pretty confused by the example. 1) I can't see how the `self.0 = ...` line could change the referent of `x`, and 2) it's pretty unusual for an iterator to call `clone` on the members of the collection it's iterating over.
Asteroids should only ever be done with vector graphics. The original arcade cabinets used vector graphics! My favorite Asteroids reproduction will always be this one with real lasers though: https://youtu.be/FkHjG759ABY
That makes sense. Use the bin by default, but download the csv and regenerate it with a compile option. I'll try changing it to that.
You don't even have to reborrow manually. Simply annotating the type, let e: &amp;mut String = if c { a } else { b }; does a reborrow automatically. It's really quite silly. cc. /u/snaketacular 
guess
This might not be the right way to do this but just wanted to check if NLL could help implementing this iterator without using some sort of `take` (replacing the list with `Nil` to get access to the value that can be then deconstructed). That's really ugly. `List` happens to be nullable but many types aren't. Then one is forced to wrap them in Option or add some illegal state constructor. Here's the original code I had: struct IntoIter&lt;T&gt;(List&lt;T&gt;); impl&lt;T&gt; IntoIterator for List&lt;T&gt; { type Item = T; type IntoIter = IntoIter&lt;T&gt;; fn into_iter(self) -&gt; Self::IntoIter { IntoIter(self) } } impl&lt;T&gt; Iterator for IntoIter&lt;T&gt; { type Item = T; fn next(&amp;mut self) -&gt; Option&lt;T&gt; { match std::mem::replace(&amp;mut self.0, List::Nil) { List::Nil =&gt; None, List::Cons(x, l) =&gt; { std::mem::replace(&amp;mut self.0, *l); Some(x) } } } } Double `replace` is ugly and involves lot's of copying :(
I don't know what kind of project you're working but if you want to distribute it as a binary via package managers make sure builds a reproducible - any files downloaded during the build should be versionized.
Reduced version. This segfaults. If nobody else does it first, I'll file an issue tomorrow. #![feature(nll)] fn next&lt;'a&gt;(opt_string: &amp;'a mut Option&lt;&amp;'static str&gt;) -&gt; &amp;'a mut &amp;'static str { match &amp;mut *opt_string { &amp;mut None =&gt; panic!("uh, no..."), &amp;mut Some(ref mut string) =&gt; { *opt_string = None; string } } } fn main() { let mut a_string = Some("segfault me".into()); println!("{}", next(&amp;mut a_string)); } # Compiling playground v0.0.1 (file:///playground) Finished dev [unoptimized + debuginfo] target(s) in 0.55 secs Running `target/debug/playground` /root/entrypoint.sh: line 7: 5 Segmentation fault timeout --signal=KILL ${timeout} "$@" 
After 5 seconds, id say use an enum for state instead of a u8,but there will probably be more that can be done with a deeper look and thought into and about this code.
It will spawn enough threads to keep all the cores busy.
Threads are basis of preemptive multi tasking. Meaning scheduler can pause and resume any thread whenever it wants. Pausing involves saving thread execution context into special place, and resuming involves reading from it. Usually threads are allowed to run for specific amount time (called time slice) before switching, this allows to create an illusion that everything runs at the see time. Coroutins are from cooperative multi tasking. Meaning it's running until it gives up control to another task. Essentially they allow to write concurrent in a way you write synchronious code. Some work loads benefit from one mode while others from another.
corouines are just running on threads and will not block the thread, this is what async io do.
Unfortunately, this is something that in 99% of cases will work, but might still cause a segfault under some special conditions. This is because of the alignment requirements. * `u8` is just one byte and requires a one-byte alignment on all platforms, but `size_80_struct` likely requires a bigger alignment. 80 happens to be divisible by 8 or 16 so the compiler is unlikely to insert any padding between elements of the struct, but if you had a `size_79_struct` instead, the compiler might just pad that out to 80 bytes...depending on what's inside that struct. * Because `u8` has a lower alignment than `size_80_struct` (probably 8 on a 64-bit platform), there is no guarantee the first u8 is on a 8 byte boundary. In practice, however, many allocators would place all allocations on 8 byte boundaries (because it's simpler for the allocator algorithms to do so). * x86 and AMD64 happily allow unaligned access without complaining, might just be a few clocks slower. ARM platforms can cause segfaults in the same situation. So what IS the safe solution then? Well, the safest one is to use a parser library that does all the unsafe stuff for you. But in some cases it can be safe to copy things out from a `Vec&lt;u8&gt;` to a struct using `std::ptr::copy_nonoverlapping::&lt;u8&gt;`. On the top of my head, these are the ones: * The struct layout is defined, i e `#[repr(C)]` or `#[repr(Packed)]`. Rust structs can reorder its fields arbitrarily. * The struct is `Copy`. * There are no unsafe bit patterns. (I e, if you have an enum with discriminant values 0, 1, and 2, all other values are unsafe to copy into the struct.) * There is no padding inside the struct, inbetween the fields. The compiler might use this padding for other things, so you can't overwrite it. 
nice catch! will update.
I can change the license to remove the inconvenience. thanks!
I can do two lifetimes, and establish the relationship between them. When I hit three lifetimes, I start wondering if things aren't getting unnecessarily complex. And `clone()` is just fine, as long as it's not the full text of Moby-Dick
Ah, yes. Not a panic. I triggered this once by dropping a thread that contained an Arc. The refcount rarely hit zero, and the TUI library I was using did fun things when it didn't.
capitalism.jpg
Good call. I had briefly considered doing that, but thought it might be overkill. Now that I've updated the code, it's easier to read, and it's type checked so I get to remove the `_ =&gt;` line. thanks!
Thanks for creating an issue about this. If I understand correctly I can safely use this library as long as one or more of these hold: - MAY only uses one thread - I somehow make sure all my stack-stored objects are `Send` Is that correct?
TL;DR as far as I understand it: The compiler can reorder non-atomic operations (for example writing the pointer to the new value before writing the new value), and/or threads on another CPU core can have an inconsistent view of the memory: https://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
This sounds awfully like template metaprogramming. I'd really like to avoid riddling my codebase with `::value` everywhere.
Yes, probably (well, there are some other unsafeties, but I think these are possible and easy to fix). You can configure May to use just one thread as a user, or the library could be changed so it doesn't migrate the stacks from one thread to another. The second solution is both probably impossible in real life (you never know what happens inside your library) and Rust-less â€’ you have to be careful instead of letting the compiler take care during refactorings 2 years down the road.
This is interesting. Might even be worthy of discussion and maybe an RFC, because this feels very unergonomic. paging /u/steveklabnik1
I don't have a compiler available right now, but doesn't this code work right now by changing the last line to `a = &amp;mut {a}[1..];`? People often reach for `unsafe` too soon. 
That is awesome! Thanks for your work.
I think "You tried to write to a read-only part of memory" produces a `SIGBUS`, not a `SIGSEGV`.
Just curious, what do you mean by second class citizens? Rust treats them basically as anything else, types. Systems languages do put importance on stack VS heap and choosing the types to do so is important, I don't see rust as doing anything different here?
Great work! I'd like to note that (per the README) this compiles assumed valid Rust code to C, so there isn't even a trusting trust attack in the parts of LLVM that Rust uses. This also means that we will now be able to compile rust code to C code, which would allow us to target systems without LLVM targets but with a C compiler. Merry Christmas indeed!
Folks are trying to help you here, if you're not going to be constructive nobody will try.
Yay, let's try it! error: internal compiler error: unresolved type in dtorck Not really stable yet :(
My current stack is Angular 5 on my frontend, and a rust server using event sourcing as backend, with serde_json as serialization and hyper as server. Does anyone know of an easy way to automatically generated Typescript interfaces for my structs? I currently have 50 or so structs that are visible to the outside (don't judge me!), and creating a Typescript definition of them manually is possible - but I'm afraid I'll make an error or forget to add / delete a field someday. I could of course use regex + string replacement to generate interfaces, but that does not feel very production ready. So maybe someone here knows of a better solution.
I haven't seen this planned anywhere, but it doesn't hurt to suggest it! You can create an issue in the Rust repo and write about your usecase. If it's a duplicate, someone will let you know. Looking at the example you linked to, it seems to me that to do this automatically, the compiler would somehow need to determine whether the literal (an integer literal in the example you linked to) is capable of being of type T. We know that integer literals can be usize, u32, u64, i32, i64, etc. but we can't determine if they can be of type T until we know what T is. To get around this, we would need to be able to express another constraint on T in addition to `Add&lt;T&gt;` which says that T is expressible as an integer literal. Maybe this would come in the form of a special trait or an auto trait? Only then would we have all the information necessary in order to type check `x + 2`. It would be really interesting to see how the language would incorporate something like that. This problem seems to be about a bit more than just being able to cast to generics.
Question, have you looked at [piece tables](http://www.cs.unm.edu/~crowley/papers/sds.pdf)? Don't want to negate the work you did, just wanted to say that there are other data structures than ropes for text.
It does, but it's quite unintuitive. Despite reading about reborrowing a couple of times I couldn't build a useful mental model of them that allows me to correctly predict where they can help. Case in point, I have no idea why that makes it work here.
Question: How do compie times of mrustc compare to rustc? 
As far as I can see this is due to the simplistic nature of the MAY scheduler: It uses a simple MPMC queue for to-be-scheduled coroutines. Such a FIFO design of course can result in a request being accepted, and the response being sent much much later, if the coroutine is pushed back to the end of the relatively long MPMC queue one or more times. (cc /u/dmwp37) The Go scheduler tries to improve this situation by: - Having a global, unbounded MPMC queue (scheduler) - Having local, bounded SPMC queues for each worker thread (processor) - Employ work-stealing processors, which randomly steal half the work from neighboring processors if it has run out of work. If that fails half the SPMC queue's capacity is stolen from the global queue. Due to that goroutines tend to stick around in the local SPMC queues until they're done and are processed as quickly as possible. Such an approach would greatly improve performance of MAY even further and reduce the max. latency. P.S.: It prevents goroutines from sticking around too long, by assigning each of them a maximum time slice.
Ok, I'm sold. Those errors are amazing. My concerns are alleviated. Well done to everyone involved!
&gt; mrustc works by comping assumed-valid rust code (i.e. without borrow checking) I always wondered if its a good idea to have something similar in `rustc`. Compiling without all the "nice" checks rust has to offer could potentially increase compile times. The thing is that this does not sound very useful but i could think of two-three use cases. One for delivering software with cargo. I am using cargo to install me some nice rust tools but it takes a while to compile especially if it has many dependencies. Its not really big of a deal because its only once for the average user (or more for updates) but i don't think it is really necessary to compile with all the safety guarantees on the user side at install time. Because â€“ i hope â€“ i can trust the developer to have activated the "safe rust mode" at shipping time. Same applies to my current deployment circle for my little rust (toy) servers. I am developing on my dev machine but i do not cross compile or anything. I just ship the code to my deployment server and build it there again (sometime using docker but its essentially the same process) but i do not "check compile" it there again i just need it to compile there and it took some several minutes for deployment which i feel like could be avoided. Also if i wanna have some rapid code â€“ compile â€“ debug cycles i could go without the expensive checking for some short amount of time and re enable it after i found the bug etc. but i never felt confident enough for this idea to be very useful especially in regards to the trade off with the amount of work this feature would imply and how confusing this would be for language users. Anyway, thanks for this great piece software â€“ very nice gift indeed! 
I have to correct myself, it is possible after all. You'll just have to restrict T with the From trait: https://play.rust-lang.org/?gist=2450bb698b7b96eacc6491ae4e58c1ef&amp;version=stable
I'm familiar with piece tables, yes. I actually originally created Ropey a few years ago, and I examined many possibilities for how to structure buffers back then. I settled on ropes for various reasons, not all of which I remember clearly. In any case, re-skimming the paper, I believe the reasons I didn't use piece tables are: * Their performance gets worse and worse the more edits that are made, because you have to do linear scans over the piece table itself, which grows with the number of edits. * They don't have a good way of efficiently tracking e.g. code point or line break data, which is important for the capabilities I wanted Ropey to have.
Slow. About 1.5 times slower is my quick guess. That's due to a mix of inefficient algorithms, needing to write out text, and the C compiler having to do the optimisation.
Why does `Iterator::sum()`only have specialized implementations for primitive numeric types, but not for arbitrary type that implements `Add` and `Default`?
At first I thought this was some advanced feature that you don't need to use regularly. However, I use it all the time now to prevent unnecessary clones. 
Cargo publish rejects packages which won't build, so compile errors should only crop up due to a different compiler version, most likely having closed a safety hole
Awesome! I've been meaning to give mrustc a spin for ages
&gt; Ropey guarantees that grapheme clusters aren't split by its internal representation Is that guarantee really worth degrading from O(log n) to O(n) performance in degenerate cases?
I'd include user-after-free in the list of causes of segfaults/UB. In rust this can happen if you use raw pointers, or if you used unsafe code to increase the lifetime of a reference (for example typed arena and scoped threads do that).
Interesting. Thanks for writing that up. You might put some of that in your README! Also, I know that the design of Xi has considered graphemes, but I don't know much more than that. You might want to dig into that piece a bit more. Are there trade offs? (I mostly ask this as a prompt for more docs on your side, rather than as a challenge here in reddit comments. :))
When I started I first read [The Rust Book](https://doc.rust-lang.org/book/second-edition/) which I think is an excellent resource to get hang of the basics. Chapter 20 of the book also contains an example project (multithreaded web server) which shows many of the concepts in practice. https://rustbyexample.com/ also contains good reference code. After that I browsed some other projects on GitHub and jumped to implement a project myself. You should also check out Clippy (https://github.com/rust-lang-nursery/rust-clippy) which is able to catch common mistakes and guide towards best practices.
I love this blog post. I've struggled to explain some of the concepts in this blog post, and I really like the contrast setup between "UB is a global property" where as safety in Rust is a "local property."
Very cool indeed! What was your main goal in writing this compiler? To test for the presence of a trusting trust vulnerability? To create a better compiler than rustc? Or perhaps just because you can?:D Also, about the borrowchecking, do you expect that will be difficult to implement, compared to the rust of the compiler? 
This is awesome! But i'm curious, how on earth does compiling (or transpiling) to C result in, as you say, a **binary identical** stage0? I mean, yes, if you compile with clang (using the roughly same llvm version) a lot of the optimizations will be done by llvm anyways, but isn't there a lot of information missing which llvm has to find out on its own (no-aliasing, etc)? Also, how readable is the generated C code? Would it match modern guidelines? I'd suspect so because Rust enforces one to structure in a concise pattern.
I should jot this down as another reason why Cargo needs some kind of "side-channel" for large binary blobs.
Because 1. it may not make sense for other types depending on how they implement Add and Default and 2. type inference already no longer works in all cases, adding more impls would make it outright impossible.
I believe what /u/mutabah meant was that using mrustc as stage0 produces a stage3 that is binary identical to the stage3 produced using the normal downloaded rustc stage0.
Basically the compiler is very eager to reborrow, because that is what you want 19 times out of 20. The other one time you can simply use a tiny new scope to force a move. 
this is way too deep in the weeds for me, honestly.
`rustc` does quite a bit of static analysis.
So if that's unsafe and undefined, what is unsound? ðŸ¤”
It's not often that the checks are the big part of compile times, you can pass `-Z time-passes` to see the details.
i think /u/alexcrichton or /u/wycats would know
Great post! You have a messed up footnote btw.
Exactly that. Supposedly, stage2 would be the same too (because stage2 and stage3 are supposed to be identical) but I haven't checked that.
The C code is pretty unreadable, it's all gotos and mangled names. (It's MIR converted straight to C)
There are two libraries, coresimd and stdsimd.
Why: Why not? (It's been fun) As to borrowck, I don't expect it to be too hard compared to some other parts of the language (... typecheck) but that doesn't mean I expect it to be a quick addition either.
Pet peeve: just use the word compiling. All compilers are transformations from one language to another. The t word doesnâ€™t add anything meaningful.
My main reason to not do this is that I like being able to have backtraces, at least during development, to know where my errors come from. It really saves me so much time.
Oh, I've completely missed that, thanks! This could be something that could be better stated in the documentation...
Which part of typechecking was most difficult to implement? I'm working on an implementation of the typechecker myself (as part of a thesis project) so I'm interested in where I can expect trouble :) 
Unsound is typically used for type-system constructs which could let you violate type safety (including lifetimes), resulting in undefined behavior in safe code. E.g. specialization (unstable for now, mainly due to this) lets you require more specialized lifetime relationships which can't be enforced during type-checking (the specialized impl gets to pretend that `&amp;'a T` is actually `&amp;'static T`, for example), and this is a soundness hole that needs to be plugged still.
Inference interacting with coercions has causes the most trouble, but there's also lots of little interactions around trait/method resolution.
Wanted to say this is as well. The role of the compiler is not to go "You didn't get your code right! Bad human!" You're _supposed_ to make mistakes, otherwise what would be the point of the compiler checking your code? Mistakes/errors force you to think about what exactly you're doing.
Bikeshedding time: IMO it does, since i view `compiling` as transforming source into something executable as in compiling a list of chores, compiling a library. Transpiling expresses that it's a source transformation, since c isn't (meant to be) executable. Compiling to javascript would be something different, since js is indeed executable. Its just abused as a architecture.
 let root = s.parse::&lt;Value&gt;().unwrap(); let root = root.as_table().unwrap(); let production = root["production"].as_table().unwrap(); for (key, value) in production { println!("{} {:?}", key, value); }
I avoid the term "transpiler" because it doesn't have a commonly agreed upon definition. Depending on who you ask, it means exactly the same thing as source-to-source compiler, or it refers to a compiler which takes and emits languages with very similar levels of abstraction (for example, Coffeescript to Javascript). There's probably other usages I can't think of off hand. A good example is Pypy, which takes RPython (essentially a subset of Python more amenable to static analysis) and emits C. If you take the source-to-source compiler definition of transpiler, then it is a transpiler, but if you take the other, then no, because Python is much higher level than C. Except then you have to admit that RPython can be translated to C because it's not as high level as Python, so maybe? And that's not even getting into the fact that how abstracted or high level one language is compared to another is pretty fuzzy - some people treat anything that's higher level than an Assembly language (or C) as though it was equally high level.
you've already become wrong. what'll you say next?
no. I want to select all nodes "production" at once
Perhaps a meaningless suggestion, but maybe the distinction could be made by being strict about changing he language used: code that isnâ€™t safe could be called â€œnot safeâ€ and weâ€™ll keep using unsafe as the keyword with itâ€™s specific meaning? So, dereferencing a pointer that you donâ€™t know the provenance of is not safe and happens in an unsafe block, but dereferencing a pointer you created or performed sufficient checks on is safe and happens in an unsafe block.
That brings it down from 28MB to 8MB. Not sure what is an acceptable size to distribute, but that doesn't seem too bad.
I don't get your point. `production` contains all production nodes.
Looks like they only do a continually updated dump of the database. I'd have to host the specific version I'm using on git or somewhere else.
Communicating badly and then acting smug when you're misunderstood is not cleverness. https://xkcd.com/169/
Unsound is unintentional unsafety. _usually_ in compiler stuff (compiler lets you break things in safe code) but can also be in a library (the term is used less often in that case, folks just say "unsafe")
&gt; Also if i wanna have some rapid code â€“ compile â€“ debug cycles i could go without the expensive checking for some short amount of time and re enable it after i found the bug etc. There are very few checks that you can omit that will still let you compile; e.g. typechecking is necessary for dispatch. Borrow checking is _complex_, but not expensive. Rustc spends most of its time in LLVM doing codegen, and aside from that a nontrivial amount of time is spent in inferring/resolving types. So this doesn't help much in that space.
Except we say "compiling" for `javac`, as well, that's not exactly "something executable". And assembly isn't the final form of your code; it gets converted to microcode on the actual chip. Yes, it is a source-to-"source" transformation, except it's not really -- that output source is never readable or editable, and doesn't really work as "source". It's "source" in the sense that it's the input to something else, but that can be said to just about everything but microcode.
You need a name for "safe code" as well (i.e. "not within unsafe blocks"), and "not unsafe" would just be confusing. Generally this distinction is drawn as "safe code" vs "safety" but it's tricky and context-dependent.
"regular code"? Eh, I dunno. Was just throwing it out there.
Oh shit that's me!
that's right. so what?
Please, use line breaks. :(
I think colloquially most people mean â€œa compiler with languages at a similar level of abstractionâ€ but this is so ill-defined in almost every scenario beyond the one you noted where CoffeeScript has a clear desugaring (i.e. a localized, delimited transformation) to JavaScript. Itâ€™s further complicated by the fact that you can always compile to a narrow subset of such a language (as with asm.js). In fact, the only sensible definition I *can* come up with is that the transformation is macro-expressible. But then the word is still not helpful because we already have the phrase macro-expressible.
Can you use such anonymous `impl` outside of the module it is defined in? I'd think that having no name, it wouldn't be possible to import it, which is why [extension traits](http://xion.io/post/code/rust-extension-traits.html) are normally used for this.
Just to understand, does something like valgrind work because compiled Rust uses same binary format as C for example?
This seems to suggest that anything that compiles to a typically interpreted language is compiled rather than â€œtranspiledâ€ because the language is â€œmeant to beâ€ executable (at least at the level of abstraction youâ€™re talking about). In reality though and as /u/Manishearth response suggests, almost none of the compilers youâ€™ve ever used properly meet this definition because of the gap between assembly and machine code (and theres further complications in how processors are implemented in practice).
You can always "instantiate" a concrete constant with something like `const LAZY_ARC_FOO_INIT: LazyArc&lt;Foo&gt; = LazyArc::INIT;`
&gt; it is safe because there is literally *no way^[2](https://manishearth.github.io/blog/2017/12/24/undefined-vs-unsafe-in-rust/#fn:2) to use it such that it will do so* To make clearer what I think you've already said, safety is relative to the API. Code in an `unsafe` block can be safe for the *external* user, because you only ever access it in a well-mediated manner, but it might not be safe internally because your internal API allows you to break those invariants if you aren't careful. This is the underlying reason the unsafe-ness spreads through the module, but stops at the module boundary.
You mean bringing Rust to faraway target systems? ðŸ˜‰
I'm not sure if cross posting like this is allowed so feel free to moderate accordingly. I thought it might be of interest to a wider audience.
Congratulations, this is amazing and valuable work.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/bprogramming] [Mrustc â€“ Alternate Rust compiler in C++ â€“ Now broken the bootstrap chain](https://www.reddit.com/r/bprogramming/comments/7lvurx/mrustc_alternate_rust_compiler_in_c_now_broken/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Because it's actually not very common to need a single-item box. Much more often, the heap allocation is managed by another type, such as `Vec` or `Rc`. For example, your `Vec&lt;Box&lt;i32&gt;&gt;` is pretty nonsensical. It doesn't hurt to be reminded of that fact by typing it.
This is awesome! I was actually looking for something like this in a personal project I'm working on. I'll have to give this a go. One thing I noticed in the README, though: &gt; Ropey treats Unicode code points (`char`s in Rust) as the atomic unit of text. This is a little confusing. `char`s in Rust are actually Unicode *scalar values*, which are related to, but not the same as, code points. For example, the Euro sign â‚¬ is only a single code point, U+20AC, but it is encoded into 3 scalar values (or Rust `char`s): E2 82 AC.
You can find the relevant part of the example [here](https://github.com/sdroege/gstreamer-rs/blob/f4da93aadb82276afdd3e4a84f8ea7228c87255a/examples/src/bin/tagsetter.rs#L53-L61). It's basically encoding a couple of seconds of white noise as flac and writes a boring tag into the flac headers.
It is allowed, encouraged and actively practiced. :)
Hmm, no, this isn't actually quite right. You are right in that `char` is technically a Unicode scalar value and not a codepoint. However, the phrase "a `char` is a codepoint" is actually still also correct. The phrase "a `char` is a Unicode scalar value" is just a tighter definition because Unicode scalar values are defined as the [space of Unicode codepoints minus the surrogate codepoints](https://www.unicode.org/glossary/#unicode_scalar_value). &gt; but it is encoded into 3 scalar values (or Rust chars): E2 82 AC. The right terminology here is to say that `U+20AC` is *UTF-8 encoded* as `\xE2\x82\xAC` where each byte corresponds to a [code unit](https://www.unicode.org/glossary/#code_unit). That is, the char representation is the 32 bit unsigned integer `8364` while the `&amp;[u8]` or `&amp;str` representation is `\xE2\x82\xAC`.
It seems interesting that one of Rust's core features turns out to be very loosely coupled from the compilation process. I wonder if this is a sign of things to come, in language architecture.
keyboard input seems a bit weird - I dunno if it's a limitation of the web browser or my keyboard or what, but: - I can active forward, right, and space at the same time without problem - any other combination of three keys (two directions + firing with spacebar) doesn't activate whichever the last key pressed was. Same is true if I use `z` instead of space.
Minor error in the intro: &gt; So, first off, the waters are a bit muddied by the fact that Rust uses unsafe to both mean â€œwithin an unsafe {} **block**â€ **block** and ...
Hmm, my apologies; I must have misunderstood something when I read the documentation for `char`. Thanks for correcting me. I'm still learning. ðŸ™‚
Since `x` is `Copy`, `x.scale(a, b)` pass a copy of `x` for `mut self`, so `x` won't be modified. What you want is probably `fn scale(&amp;mut self, â€¦)`, which would pass `x` as reference.
I hope this can help make it clearer: #[derive(Debug)] struct Foo { x: u32, } impl Foo { fn update(mut self, new_x: u32) -&gt; Self { self.x = new_x; self } } fn main() { let mut foo = Foo { x: 42 }; foo = foo.update(23); // foo.update(15); Error because the value is gone println!("{:?}", foo); } The error in the commented out line happens because you're consuming foo when you call update, and you're not storing the result anywhere. The error goes away if you start deriving Clone and Copy for Foo - then the value does not move when update is called but instead a copy is created. If you want to mutate the original, you would probably want to use a mutable reference, like so: fn update(&amp;mut self, new_x: u32) -&gt; Self {
But there are 0 occurrences of the word "poop". :(
Awesome work.
That is *amazing.* Great work!
Can you try pressing some keys here https://jsfiddle.net/c9jsguf9/ and tell me if there's any combination that doesn't work for you?
Incredibly impressive! Just a question: Why did you do it in C++?
can we bounce this off [corrode](https://github.com/jameysharp/corrode) as a way to fuss each other? I.E. `C -- corrode --&gt; Rust -- mrustc --&gt; C` Or `Rust -- corrode --&gt; C -- mrustc --&gt; Rust` and test that all 3 artifacts have the same output?
Very interesting ! My main source of frustration nowadays on the front end side is the lack of proper type checking. I feel like runtime errors happen way too often while developing in React / Redux, and things like TypeScript don't help all that much. The whole Redux ecosystem is very dynamic and doesn't play well with static type checking. I haven't been following stdweb that closely. How does DOM manipulation works ? Last time I checked, WASM didn't expose any API for DOM manipulations. Anyway, I can't wait to play with this !
The `slice::len` should be able to be made into a `const fn`. For example, [here][example] `Slice&lt;T&gt;` has the same layout as `&amp;[T]`. If `mem::transmute` can be made into a `const fn`, then this would be easily possible. [example]: https://play.rust-lang.org/?gist=aede2bdb954a9ac1ac84c61b6b6728fd&amp;version=nightly 
&gt; // No more html comments! use Rust/JS style Yes, thank you! That's one of the things I dislike most about using JSX in React. It's pointlessly difficult to leave comments, either in HTML or inbetween brackets. It's not just a couple extra characters, it's a whole formatting issue. Also, huge fan of supporting Elm style. I'm definitely keeping an eye on this.
I think I was never really clear on what "reborrowing" actually means for `&amp;mut T`. I believe I understand now, though I can't find any working (safe) code that does the same thing as reborrowing. let a = &amp;mut 5; let b = &amp;mut a; // mutably borrowed a, &amp;mut &amp;mut T let c = &amp;mut *a; // mutably borrowed a, but with &amp;mut T `&amp;mut *` is very much compiler magic to me.
What degenerate cases are you talking about? If you have truly enormous grapheme clusters, then yes, it could degrade to O(n). Is that what you're talking about? If so, then I think it's worth it, yes. The longest grapheme clusters I've ever seen are from things like [Zalgo text](http://eeemo.net), and those grapheme clusters aren't even in the remote ballpark of being long enough to give Ropey any problems (I actually have some zalgo text that I test with locally--maybe I should add them to the repo). Is there a different degenerate case you're thinking of? Or are there aspects of the trade-off I'm not thinking of?
Could you do a release on github once it bootstraps rustc? We at the Guix distribution want to have reproducible trustable builds and the weird binary rustc bootstrapping binaries we were forced to use are ... disadvantageous.
Awesome, great job! I kicked the tires on it when it was first posted. I'll take another look now.
Hey I also did something similar for my learning rust project. github.com/phenguin/connect-four. Don't mind the repo name. I occasionally go back and add new stuff to it. Most recently basic networking support. Curious to see how your approach compares!
Huh, interesting. When I was looking at xi's code, the only things I noticed related to graphemes were CRLF and its unicode line breaking implementation. But I could easily have missed something! There are two trade offs I'm aware of: - A typically negligible performance hit, to make sure grapheme clusters are never split and always get merged. - Theoretically Ropey can degrade to O(n) performance with enormous grapheme clusters. But any text with grapheme clusters long enough to do that is, I strongly suspect, essentially broken text. And Ropey will still handle it correctly, just much more slowly. Thanks for the tip! I'll definitely put some of this in my README! Also, I did some performance comparisons between Ropey and xi-rope last night. It appears that xi-rope is faster when inserting/removing near the beginning or end of the rope, but Ropey is faster when inserting/removing near the middle of the rope and when doing incoherent (all over the rope) insertions/removals. In general, Ropey seems to have more even/level performance characteristics. Having said that, I tested against the 0.2 release of xi-rope on crates.io, and that may not be up-to-date. And performance is something that can change over time anyway. Moreover, both libraries are really fast (by my definition of fast, at least), so I doubt performance is something that consuming code needs to be worried about with either one. In any case, I want to emphasize that I think Xi is an amazing project, and I don't mean to compete with them in any negative kind of way. And the larger Xi editor project encompasses much more than just text ropes, which are comparatively easy to write!
Sure but it's easily possible to have something that will compile sometimes because of includes, build. RS, etc.
So this is an issue in piston?
Hmm reading the comments, i tend to agree that the word transpiling is difficult to define on its own, but in case of the JVM i'd not count it as an counter example. From the point of view of the end user, its just a blockbox which executes the bytecode, like a cpu executes assembly. That's why i'd say java is compiled. That the jvm lowers it into the native architecture is IMO just an implementation detail, which doesn't concern the end user.
&gt; And assembly isn't the final form of your code Not to mention that it's not uncommon for compilers to call out to the system assembler and/or linker after generating assembly
&gt; Except we say "compiling" for `javac`, as well, that's not exactly "something executable". javac is compiling things for the Java VM. Even if there only existed a SW implementation of the Java VM (which is not the case, https://en.m.wikipedia.org/wiki/Jazelle), I would argue that the output is still something executable. &gt; And assembly isn't the final form of your code; it gets converted to microcode on the actual chip. &gt; Yes, it is a source-to-"source" transformation, except it's not really -- that output source is never readable or editable, and doesn't really work as "source". It's "source" in the sense that it's the input to something else, but that can be said to just about everything but microcode. Iâ€™m not sure where youâ€™re going with the fact that microcode exists. Just because there is further decoding of the instructions doesnâ€™t mean that the instructions arenâ€™t â€œexecutableâ€.
**Jazelle** Jazelle DBX (Direct Bytecode eXecution) is an extension that allows some ARM processors to execute Java bytecode in hardware as a third execution state alongside the existing ARM and Thumb modes. Jazelle functionality was specified in the ARMv5TEJ architecture and the first processor with Jazelle technology was the ARM926EJ-S. Jazelle is denoted by a "J" appended to the CPU name, except for post-v5 cores where it is required (albeit only in trivial form) for architecture conformance. Jazelle RCT (Runtime Compilation Target) is a different technology and is based on ThumbEE mode and supports ahead-of-time (AOT) and just-in-time (JIT) compilation with Java and other execution environments. The most prominent use of Jazelle DBX is by manufacturers of mobile phones to increase the execution speed of Java ME games and applications. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Should I change "code point" -&gt; "scalar value" in the readme and docs? I actually wasn't aware of this distinction between the two. I knew about surrogate pairs in utf16, but I thought that was just an encoding detail _of_ codepoints in utf16.
Okay, but it makes a fine IR for a compiler. 
&gt;&gt; And assembly isn't the final form of your code &gt; Not to mention that it's not uncommon for compilers to call out to the system assembler and/or linker after generating assembly Linking is not what makes your code executable, it links together things that are executable.
It also might make it easier to include Rust in Python, Ruby and other packages. Typically the package manager for those languages can find a working C compiler and compile C sources.
Wasn't the original Rust compiler written in OCaml? I understand the language recognized by the self-hosted compiler eventually evolved significantly from that of the OCaml compiler, but I'm curious why C++ was chosen over OCaml (rewriting or updating the original) for this project.
OCaml-era Rust would be almost unrecognisable to a present-day Rust user, as would even relatively recent versions of self-hosting rustc. They're practically unrelated languages other than by their history.
C compiler but no LLVM infrastructure, yeah If I can pipe Rust into gcc that'll make it a lot more attractive to us
Clarification: I mean that c isn't meant to be interpreted. But yes, it makes a good compiler target.
â€œItâ€™s possible to have safe code within an unsafe block; indeed this is the primary function of an unsafe block.â€ This is unnecessarily confusing. We should be saying â€œitâ€™s possible to have *correct* code in an unsafe blockâ€. Safety is about proximity to harm, not actual harm itself. An unsafe block turns off some compiler restrictions, making it more likely for you to shoot yourself in the foot. The fact that you can write correct code in an unsafe block doesnâ€™t make the block magically safe, it just means you wrote correct code. By analogy: Playing in the street is unsafe, because you might get hit by a car. Not getting hit by a car doesnâ€™t magically make playing in the street â€˜safeâ€™, it just means you avoided getting hit by a car. If you want to be â€˜safeâ€™, stop playing in the street.
I guess my question boils down to a language vs library debate. For example, some newer languages that have optional types use `Int?`, which, IMO, feels nicer to use and toggle than `Option&lt;i32&gt;`. Likewise in C++ if you want a vector of heap allocated memory your type would be `Vector&lt;int*&gt;` which feels cleaner and easier to read than `Vector&lt;Box&lt;i32&gt;&gt;`. In all fairness, a more apples-to-apples comparison would use `unique_ptr`, but then the equivalent Rust example would be `Vector&lt;Option&lt;Box&lt;i32&gt;&gt;`, which, to me, feels a little too verbose for relatively common concepts, nullability and heap alloc. I think /u/DroidLogician was getting to the crux of my question: sigils. I feel a little syntactic sugar in the right place can go a long way; the `try!` macro being a recent example. With Rust being a fresh take at systems programming I feel like this is an opportunity to clean up the verbosity of using modern C++ while functionally retaining the same core concepts. I hope this made sense; I'm not very good at explaining my thoughts to others very well.
You too, enjoy your break!
So you avoid using the word decompiler as well? I don't see the point. It conveys something meaningful.
Well I wouldn't call it an "issue", they made the design choice of having the `scale` method return a copy of self which includes the changes made by the method, you just had the wrong expectations
Hello I am trying to learn rust and to do so am trying to write a shell. I am trying to use the [std:process:Command](https://doc.rust-lang.org/std/process/struct.Command.html) library and can get simple programs to run, but can't seem to figure out what the necessary parameters is for args. I created a vector of OsStrings and am passing that [args\(\)](https://doc.rust-lang.org/std/process/struct.Command.html#method.args) which is in correct. What am I supposed to pass? Thanks
Happy holidays for you too Jonathan! And I've already got an interesting RFC for you when you get back :)
An assembler makes something executable though
I didn't realize `Rc` boxed T internally; good to know! As for the Vec example, `i32` was supposed to be a placeholder for some large type. I guess I really meant `Vec&lt;SomeTypeGreaterThanSizeOfUsize&gt;`. As I'm writing code I find that my enums/structs start of smaller than `size_of::&lt;usize&gt;`, but grow to beyond that and it's reasonable to start allocating on the heap. AFAIK, Rust does not support RVO which makes letting the caller decide a little problematic ATM. I hope this clarified what I meant a little bit.
I've come across a bunch source code that uses stdweb. Stdweb expose a `js!{/*code*/}` macro, which is then used extensively by library users to wrap the underlying DOM manipulation function with a familiar name (example: set_height, set_attribute). Looking into the futurem when wasm with DOM support comes out. Application writers who uses the library which wraps stdweb like this will have minimal effort of code change to take advantage of DOM-wasm feature.
With that kind of difference, I'd decompress at runtime. Maybe provide a Cargo feature flag for using uncompressed data in the build. Remember, if the binary is 20MB smaller, that means the compiled program is 20MB smaller. A 2/3rds reduction in size is nothing to scoff at.
`impl`s aren't subject to scoping. If they exist in a `crate` you're linked to, they're active and usable.
What command are you using to compile it?
I think I got a bit of what you were saying, thanks! The types are very verbose, reading and writing. I guess partially my issue was that I have gotten used to that, and didn't really see it as a second class thing. That's just how the types work
It's a nitpick but I'd add a "new" method in "impl PartitionGenerator" because you only need to pass in two variables, k and xs. The other fields can be initialized in the "new" method since they have known initial values.
I doubt you are alone with this...
Not only that, a lot of distribution maintainers will find the 1.5x compile time quite attractive...
Have a well-defined Christmas and a safe New Year!
I wish I had learned combinators early on. All the methods on Option, Result, and Iterator are your best friends. Even HashMap and BTreeMap have combinators in their entry API. You will find yourself writing far fewer if/else chains and if let/match blocks. and_then().unwrap_or() is your friend to avoid nesting 8 if let statements with else clauses that are all the same thing. It reads much nicer as well.
Not even remotely; GCC or ICC are the only viable compilers on a lot of niche arches that I'm excited for us to reach with this
Neat! Check out this similar project: https://transform.now.sh/json-to-rust-serde
As a reference for kernel-bypass TCP done _well_ you might want to look at [Solarflare's OpenOnload](http://www.openonload.org/) before VMA. Admittedly, TCP support in VMA has...matured...quite a bit since VMA's initial release. But IMNSHO OpenOnload's integration of hardware and TCP was well thought-out from the beginning whereas VMA bolted-on TCP via heavily patched LWIP. 
The crate may have a different name. There can be situations where the name in the toml doesnâ€˜t match up with the name in the source.
Ya, I didn't really have a problem with it until the thought randomly popped into my head. It's one of those mild paper cuts more than anything else; doesn't stop me from liking Rust. As for the i32 example, it was supposed to just be a placeholder type for some some type where `size_of::&lt;T&gt;() &gt; size_of::&lt;usize&gt;()`; I probably should have used `T` in my example instead. Apologies for the confusion. See my reply to /u/birkenfeld for a bit of a longer explanation.
Or different target
1. `String` and `str` already impl `AsRef&lt;OsStr&gt;` so there's no need to manually convert beforehand 2. you are passing the slice to `.arg()` not `.args()`! this is where stuff like rls/racer really help, in my experience.
 rustc main.rs 
Wow that is very nice and a great finish to it. Thanks!
cargo manages downloading dependencies and compiling them. rustc doesnâ€™t handle that. Assuming youâ€™ve created a Rust project with cargo new â€”bin some_project Then you just need to be in the project directory and say cargo build
I don't know but I am interested in these questions as well. I only have experience with small applications but I'm currently using Postgres's pubsub features for this sort of thing. There's a couple AMQP libraries in various states of functionality, `amqp`, `amqpr`, `lapin`, etc. There's a crate for interacting with `faktory`, which looks interesting... There's several interfaces for Disque and related things, it seems... Then there's `mles` which appears to be a nice messaging system but maybe somewhat low level. I really just want something like Celery, myself, but haven't found it yet.
When one of the better solutions available looks like a bad typo, you know there's something missing.
My instinct is that there should be a filter method on Option that takes a predicate and either returns the original value or None. That would make these guards really clean and you can can tack an and_then onto the end of your guards. Unfortunately there doesn't seem to be a filter function on Option, however you could theoretically into_iter, filter on the iterable, but then there's no clean way to get back to Option 
off-topic: HEY I KNOW YOU (i'm [@er1n@social.mecanis.me](https://social.mecanis.me/@er1n) on mastodon :p)
That does begin building it, but now I have another error. Link.exe cannot open input file freetype.lib.
That makes it seem likely that youâ€™re using Windows. FreeType is a C library that kiss3d must depend on... and installing libraries on Windows is no fun for anyone. Basically, you need to download them from... somewhere... on the internet, and copy them into the right folders. I had to do it once, months ago, but I donâ€™t remember how.
Yeah, that makes sense. I'll have to check back, or maybe I'll take some time and see if I can fix it for FF and make a PR. It seems like a really cool idea!
Looks like there's an RFC already https://github.com/rust-lang/rust/issues/45860
As you can see in a [sibling comment](https://www.reddit.com/r/rust/comments/7lq5t3/featurenll_available_in_nightly/droficv/), the actual code doesn't use mutable slices but some other struct that needs to be borrowed mutably. Is it also possible to fix that using the current compiler version? I couldn't get it to work even with your hint. :( &gt;People often reach for `unsafe` too soon. When I wrote this code, I asked in the #rust channel, and IIRC we couldn't figure out a solution. How much further should I go? ^^
You could see it like that, but I think it's an inherit solution from the rules of the borrow checker. A mutable borrow moves, and you need to algorithmically choose between two mutable borrows at the same time. You can't move them though, because you might choose the same one twice. How do you solve it? Don't move the mutable borrow, just take a mutable borrow of it instead! True, if this can be fixed with NLL, then that will be nice. However I don't find it particularly bewildering that this doesn't work. Though I admit to have long internalized the rules of the borrow checker in my head and this is almost certainly not as obvious to a beginner as it is to me.
I added hyper-may branch to my [benchmarks](https://github.com/fafhrd91/benchmarks) it performs well, but not exceptional, shio-rs or Gotham performs similar. async hyper performs better. all I can say, there is no magic, and 2.5x speedup looks like magic. yes, coroutines simplify async programming. no, coroutines does not make you code automatically runs faster.
As I see it, /u/Icarium-Lifestealer's solution satisfies your needs 100%, but just to really demonstrate that you can get all the tables that begin with some prefix this way: https://play.rust-lang.org/?gist=462d12720605d2eb1b86b8a2dba79a6c&amp;version=stable
&gt; unsound In logic soundness basically means "anything which is provable is true for all interpretations of a theory" - in the context of types I suspect this approximately means "if it typechecks, it will do the expected thing".
If you're referring to 1.5 here (https://www.reddit.com/r/rust/comments/7lu6di/mrustc_alternate_rust_compiler_in_c_now_broken/drp2v5m/), they said 1.5x slower.
I'm interested in helping out with this! Just let me know what you need help with and I'd be glad to take a look.
It is bootstrapping rustc!
C++ is what I use in my day job. I know it well, so that makes it the best language to use for a very large project that isn't Rust.
C-library dependencies as opposed to pure rust deps pretty much always require a bit of manual work, especially on windows. Instructions for setting up the freetype lib that kiss3d depends on can be found here: https://github.com/PistonDevelopers/freetype-sys 
How does this compare to domafic?
I felt the same way about Redux. I would recommend the alternative, Mobx paired with Mobx-state-tree (both TypeScript focused projects), which I really like for the many similar reasons to why I like Cycle.js and Elm
I've always thought Rust's Enum approach would be brilliant for the patterns of Elm's Messages. It is wild to see something like this developing! Great work! I'd love to contribute.
I think you understood the comment exactly the opposite way it was meant.
Yup no worries! It took me a long time to get this straight. But these days, I read the Unicode standard for fun, so it's gotten burned into my brain. :)
Reading jvm bytecode is not much harder than reading most "transpilation" output, it gets pretty obfuscated. rustfmt is a tool that is actually source to source, because you regularly edit both the input and the output, and both are at similar levels of grokkability.
Cool! I actually know very little about Xi and its internals, other than reading parts of Raph's "rope science" docs. I was mostly curious where the differences were. Thanks for elaborating. :)
I'm going for microcode because you have an arbitrary definition of executable. There is no reason as to why JavaScript isn't executable in this model, making most JS "transpilers" compilers. And really, no reason why C isn't executable. In general "transpiler" as a term is too vague and not very useful, and most often gets used to create an arbitrary distinction of "real compilers" and "transpilers". It's best avoided IMO.
But that's the point, isn't it? I'm trying to draw the distinction between unsafe code and unsafe blocks there, and I clarify that sentence as I go forward.
I'm not familiar with your specific representation, but if it's truly encoding agnostic then "codepoint" is probably correct. But if your rope is specifically tied to Rust's `char` or is specifically using UTF-8, then yeah, you probably should say Unicode scalar value. Basically, when you say something like, "Ropey treats Unicode codepoints as the atomic unit of text," then at what level is that happening? For example, if you search the [regex crate's documentation](https://docs.rs/regex/0.2.3/regex/) for "scalar value," then you'll see how it's used. This is because the regex automaton is specifically constructed to match UTF-8, and therefore, cannot match surrogate code points without violating the definition of UTF-8. (The implication here is that if you want to search UTF-16, then the caller is responsible for transcoding the input text into UTF-8 before running the regex.) (Interestingly, if you search the docs for "code point," you'll see that it is indeed mentioned in one place and it indeed should say scalar value since that is more precise. This mistake happens a lot and I don't normally point it out because it often has little utility in practice for folks that aren't working on the internals of text data structures, but in this case, scalar value was being conflated with code unit, which is a different kind of error. :))
That's what I mean â€“ transpiling once to the tune of 1.5x slower compile time and then being able to build on each target's C compiler is a net win for many distros.
that's right. so what? 
It really doesn't. It conveys a pretty arbitrary distinction, one which nobody even agrees on. Most compiler devs I know dislike the term. 
From reading the source code, Yew uses a virtual dom.
I'm not quite sure what you're asking, you'll have to be more specific. Also there may be a new "easy questions" thread by the time you see this so I would just submit again over there.
I canâ€™t tell what youâ€™re referring to as â€œthe pointâ€. We agree on what an unsafe block does, I have no issue there. Iâ€™m specifically taking issue with calling the code inside an unsafe block â€œsafeâ€, I think it creates unnecessary confusion, as if thereâ€™s some kind of doublespeak at play. I believe we should instead refer to properly-written code in an unsafe block as â€œcorrectâ€, as the harm we are in closer proximity to when inside an unsafe block is that we can write incorrect code that Rust would otherwise prevent us from writing.
The whole post is about terminology as used in Rust _today_ and focuses on that notion of "safe" ("correct"). I'm not the one terming it that way, that is literally what "this code is safe" means when you are not talking about unsafe blocks.
Can you elaborate? Rust and Elm have very similar enums as far as I can tell
I already tried it on Linux m68k and it built fine. Wasnâ€™t able to build the *core* crate though. But at least thereâ€™s something now to work with for Rust on exotic architectures. Thank you very much! I will package it for Debian during the next days!
This change actually improves lifetime errors because it can then talk about things exclusively in terms of use, leading to a better mental model of things.
was that worth it rewritting it into Rust? is it significantly faster or are there other benefits? 
This didn't used to work earlier for generic length arrays, because rustc wouldn't evaluate the const item, and so wouldn't construct an actual array type. Not sure if that's changed or not
Uncritical explanations can sound like tacit endorsements. Iâ€™m advocating for abandoning todayâ€™s terminology so we can avoid the need for articles like this altogether.
I think this is pretty well entrenched already, and I don't want to bother punting on fixing the terminology, if you feel strongly about this I suggest you come up with your own proposal to fix it :)
you should try this out https://github.com/Xudong-Huang/may/pull/7 
Really? I can write!() to TcpStream and it will be super fast in any framework. But topic is how speed up hyper. if you want bench numbers, that is fine, but it has nothing to do with speed of may.
I just did? 
I mean, I suggest you push for it yourself, this isn't something I want to push for in my blog.
My perspective has always been that transpilers *are* compilers, but they're a specific subset which output another high-level language, rather than compiling to something low-level like assembly, bytecode, or machine code. (Hence the "trans" part also referring to moving more laterally than usual in a chart of high vs. low-level languages.) ...and, likewise, a decompiler would be something which translates code in a low-level representation to a higher-level representation, attempting to infer information lost in the compilation process. (As I see it, this interpretation makes the terms useful without getting lost in the weeds.)
This is a pretty decent way of looking at it, and while the definition of "high level" changes that's ok because of context.
You are right. Hyper has its own thread model where MAY is not best suit. Thanks for try out.
Yeah, that jsfiddle duplicates my problem. I'm on Windows 10 and Firefox fwiw.
I literally am pushing for it myself, right now.
&gt; it gets converted to microcode on the actual chip. Not really relevant to your point, but this is... a vast oversimplification at best.
I think myrrlyn's response was a fancy way of saying "That's an understatement if ever there was one".
Sure. To be clear, all I'm saying is that I don't intend to edit this blog post to push for this :)
Oh, sure. I didn't want to get into the weeds here.
I suspect the only way I can help is by changing the key binds - this is likely a limitation of your keyboard. I tried this on multiple computers and the webpage has reflected the native input limitations. You might also notice these problems if you play other video games on the same machine.
(Take my advice with a grain of salt. I work at BigCo where the default option *is not* to build a Sidekiq-esque system but use a pub/sub system as a primitive communication layer between several services that may or may not handle some form of . This requires organizational buy-in into mindset and tooling that might not be appropriate for most, or all cases.) That being said, I've previously delegated the queuing to a system like [SQS](https://aws.amazon.com/sqs/), [PubSub](https://cloud.google.com/pubsub/docs/overview), or [Kafka](https://kafka.apache.org/) and build (a) service(s) that handle job execution atop of it. &gt; Should we be running external worker processes, or folding the workers into the main app process as threads? We have external worker processes in the form of separate services. This is a non-trivial design decision that has significant costs in operation. I.E., you're incurring the costs of a distributed system, which if you can avoid, *do*. &gt; I'm partial to a single-process approach to this; are there downsides to doing it that way? I'm assuming that the "single-process approach" means "50-60 user threads running on a single machine with specs similar to an m4.4xlarge". Could work! It depends on how much growth you're expecting/what your failure-handling strategy is. At a certain point, you might reach the limits of a single machine. Then again, it's entirely possible that you won't ever reach those limits. It depends! My inclination, again, is to cobble together a distributed system of sorts, but I can do that because my employer has made the necessary investments in tooling that makes dist. systems approachable. (Sorry, if my comment isn't fully coherent, I'm sleepy...)
Hmm. I just realised it's possible to pack in sorting functionality alongside the span stuff. You could make a reddit with this. If you had a SortedJostleTree&lt;Article&gt; sorted by article recency, set articles' spans to an increasing function of their voted score, and sample articles at SPAN_WINDOW_CONSTANT\*randUnit()^2, quality and recency would contribute to the probability of an article being seen in a perfectly smooth, complementary way, and it'd be *so* efficient.. users could control the time window by specifying values for SPAN_WINDOW_CONSTANT, which would mean more or less randomness, pagination would just be a matter of adding page_number\*SPAN_WINDOW_CONSTANT to the sample offset... Neat. I kinda wanna do this now.
So documentation is always a big help! But I understand that it isn't the most fun task. I still need more of the transformations implemented but I think the API around that needs to change. Probably the best place to work would be the MANIAC tree. Currently it is not very performant.
&gt; You tried to dereference (â€œaccessâ€) a null pointer (`0x0` is an address! derefencing it does not work!) This wasn't always the case and `0x0` is [technically a valid memory address](https://blogs.msdn.microsoft.com/oldnewthing/20130328-00/?p=4823)... it's just the C specification that says that `0x0` is `NULL` and the OS that [allocates a guard page](https://blogs.msdn.microsoft.com/oldnewthing/20170420-00/?p=96005) to trigger a fault on access to it. The processor doesn't care and code written in assembly language that's running with kernel privileges is perfectly free to interact with `0x0`. Imagine how painful debugging was before the OS could arrange for dereferencing null pointers to trigger a fault... though that kind of unprotected access to memory did lead to some [amusingly true-to-cinema](http://blog.danielwellman.com/2008/10/real-life-tron-on-an-apple-iigs.html) bugs.
I'm usually summing things up like that: * Undefined behavior is something you *never* want to have, in any program, in any language. It basically means bugs. * Unsafe means "we can't guarantee that this doesn't lead to undefined behavior".
Lol yup Typing is hard
What's new in 1.23? EDIT: never mind, here's the tracked issues: https://github.com/rust-lang/rust/issues?q=milestone%3A1.23+sort%3Areactions-heart-desc
the tracked issues only really show regression fixes, hereâ€™s the wip rendered changelog https://github.com/Aaronepower/rust/blob/master/RELEASES.md
We should do it because we can and itâ€™s awesome. 
&gt; you can never forget to lock `Mutex` Amusing but true: you can however forget to *unlock* it if you really try, by moving the `MutexGuard`.
Dropping a Vec will deallocate. Calling `.clear` will not. So if your GameState can support a "clear" operation, Vec is gonna be easier and should remain nice and fast. 
It is a big difference, I prefer just having one executable without having separate data files, but it might be worth it in this case.
Feeling the same here. The tokio example is not as basic as the May one as it uses channels.
What I meant was to embed the compressed data in the executable, and decompress into memory at runtime.
Tools are all here. There are sidekiq client and sidekiq server in rust. &gt; Should we be running external worker processes, or folding the workers into the main app process as threads? I'm partial to a single-process approach to this; are there downsides to doing it that way? The biggest downside is that if the process crashes it will crash every worker. If your job written in a correct way - running the same job will lead to the same result then the only issue is a little bit of latency when jobs are restarted. If you use processes it's possible to do zero-downtime code upgrades, which is nice not but necessary for background tasks. From OS point of view threads are equal to process in terms of scheduling. I don't think rust has a full-blown solution for persistent PubSub like SQS or Kafka. I'd say just write a sidekiq server or pick one already written in rust.
yeah, this is a good way to draw the distinction
I believe Chrono is pretty widely accepted as "the way" to do dates and times in Rust, barring any non-mainstream needs. Even if you don't care much about the other differences, one reason to prefer Chrono is its approach to timezones. From [Chrono's readme](https://github.com/chronotope/chrono): &gt; Chrono is timezone-aware by default, with separate timezone-naive types. Without that, it's just way too easy to accidentally mix semantically incompatible dates/times. Take timezones seriously from the start, or live to regret it. I say this from experience. :)
How do you demolish wooden floors in rust?
&gt; rustc now avoids unnecessary copies of arguments that are simple bindings This should improve memory usage on average by 5-10%. That a thing.
I haven't done anything like this before, but it should be possible to use custom derive/proc macro for this. #[derive(Typescript)] struct Api { foo: i32, bar: bool, } Make a proc macro crate: // Untested code extern crate proc_macro; extern crate syn; use proc_macro::TokenStream; #[proc_macro_derive(Typescript)] pub fn hello_world(input: TokenStream) -&gt; TokenStream { // Construct a string representation of the type definition let s = input.to_string(); // Parse the string representation let ast = syn::parse_derive_input(&amp;s).unwrap(); // use members of DeriveInput to build // typescript code let code = format!(r#" interface {ident} \{ // TODO: make a field for each field in ast.body ... \} "#, ident = &amp;ast.ident); // write typescript to file... } Using custom derive for this is a bit of a hack, I suppose, but it makes it easy to make definitions for specific structs. You could also do something similar using syn in your build script.
Maybe you have `rust-client.updateOnStartup = true`in your config?
&gt;Then he went on to show how they have done some experiments replacing B-trees, hashmaps, and bloom filters in database indices with neural networks. For B-trees they obtained for 5-100x fold improvement in memory consumption and ~2 fold faster lookups, and similar (if not as large) improvements for hashmaps and bloom filters. That is a bad idea until better strategies for dealing with antagonistic input in ML are found. A zip bomb that slows your hard-drive to a crawl would be unfortunate. In any case often why machine learning embeddings are separated out is so that they can be easily run on the GPU and not just the CPU.
What do you mean by widths and positions?
I've been using rjq https://docs.rs/rjq/0.3.0/rjq/ and separate worker processes with some success.
Widths are referred to as "span" in the code. Position as "offset". Span determines how much an element will displace the offsets of the others in order to make room for itself. An element's (or, the element's slot's) offset is the minimum value, say, `v` you'd need in order to reach it with `jt.get_item(v)`. Any `v` greater than or equal to that minimum and less than `minimum + span` will hit it.
You might want to run `rustfmt` on your code... It's not anywhere near the formatting guidelines right now.
Ooh... This sounds like a good opportunity to have a tool automate that...
Domafic does too-- it builds a set of virtual nodes and only renders the diffs to the real DOM.
Document what your data structure does in the readme, a list of functions you can call to create and use it plus some usecases for the ones which aren't obvious. 
When I compile my code with --emit=metadata I am seeing almost a X6 slowdown compared to not using feature(nll). This feature is still experimental and hopefully this will be fixed, otherwise it's a big problem.
The link to sway is dead 
He answered the C++ question here, to answer the other part https://www.reddit.com/r/rust/comments/7lu6di/mrustc_alternate_rust_compiler_in_c_now_broken/drpx36z
Do you have any performance comparisons between mrustc and rustc?
Being non-constructive is not seen as a good thing here on /r/rust. Telling a mod names is usually a bad idea on most of reddit. Unless you are aiming for a ban, that is. So are you trying to prove that the Rust community is nice even to trolls or to get yourself banned?
My comment wasn't really about the i32, more the boxing of it. Vecs already heap allocate, so the boxing was a little superfluous
Rust does support RVO, it's just not spec guaranteed like C++.
it's not nice and there're no trolls here. now tell us your definition of a troll and why are you being so disrespectful?
it's not nice and there're no trolls here. now tell us your definition of a troll and why are being disrespectful by calling a moderator "a mod"?
Wow! Well done! That's so cool. I've added it to my [bootstrapping wiki](https://bootstrapping.miraheze.org/wiki/Main_Page). It's so exciting to see people working on bootstrappable builds! R.I.P. https://manishearth.github.io/blog/2016/12/02/reflections-on-rusting-trust/ !
I have taken small peaks at way cooler when I saw it probably 6 or so months ago. It's really cool to see where it is headed and the traction it is gaining! I am not currently using Wayland myself but this project has made me interested in looking into it. Keep up the good work and plans for 2018!
AFAIK [1] is required to execute because it has an observable effect (IO).
Hyper uses tokio, making async it's main mode of operation. No special steps needed, just use tokio compatible libraries. https://hyper.rs/guides/server/hello-world/
That's really great news! The reason I haven't switched to Wayland is that there is no window manager like XMonad or Awesome yet.
They are called guidelines for a reason 
I'm not following where the 'sequences of _packed_ items with _variable wdiths_' and 'trees' notions come together. From Wikipedia's [definition](https://en.wikipedia.org/wiki/Order_statistic_tree), OSTs are BSTs with extra node data and operations to ease traversal as sequences (and find an element's "index" in this sequence view). Some help please :P
**Order statistic tree** In computer science, an order statistic tree is a variant of the binary search tree (or more generally, a B-tree) that supports two additional operations beyond insertion, lookup and deletion: Select(i) â€” find the i'th smallest element stored in the tree Rank(x) â€“ find the rank of element x in the tree, i.e. its index in the sorted list of elements of the tree Both operations can be performed in O(log n) worst case time when a self-balancing tree is used as the base data structure. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
In the "server echo" example at github: println!("Listening on http://{} with 1 thread.", server.local_addr().unwrap()); Is `1 thread` related somehow to its being non-async in this case?
I'm not familiar with this particular IOCTL, but given that the `NOTE` comment is correct, the following applies: Most IOCTLs take a pointer as third argument, but this particular one is different: it takes an integer/long instead. But because most IOCTLs take a pointer, all tooling (e g, the Rust macros, libc and so on), expect it to be a pointer. But nothing on the way between your Rust program and the driver handling this IOCTL ever dereferences that pointer, so to those layers, it does not matter if the ones and zeroes passed through is a pointer or an integer. So, the parameter for this particular IOCTL is an integer. But to all the layers between the driver and your application, we pretend that it is a pointer, because all tooling is adapted to that. This works because a pointer is never smaller than a `long`. 
I think youâ€™re thinking of â€œconst genericsâ€ https://github.com/rust-lang/rust/issues/44580 where a constant (such an integer length) is a *parameter* to something generic (such as an array type). By generic constant, I mean a constant that is itself parameterized. And since const generics are not implemented yet, in current Rust these parameters can only be type parameters. In this case, `LazyArc&lt;T&gt;::INIT` is a generic constant with a type parameter `T`.
Ah okay! But for usage in a stable distribution we'd like to use an official release (tar file with version number). Otherwise, if we packaged a moving target it wouldn't exactly solve reproducibility.
Not at all, you can be perfectly asynchronous without multiple threads. Instead of 1 thread per connection, you have an event loop running, handling many connections, whether in 1 thread or many.
My understanding is that there is the caveat that the compiler has no idea whether `printf` terminates, so cannot actually deduce that `i + 1` ever gets executed, but otherwise IO has no special status.
then are all web framewoks based on hyper async?
Well, not **all** of them. Iron, for example, uses an older Hyper version (0.10) which was built on synchronous I/O. I think it's the same for Rocket.
That would be true if you leave line 2 out because then the program becomes a C program and this behavior is defined in the C ISO standard. But because of line 2 the program has undefined behavior and the C standard says that it poses no requirements on programs containing undefined behavior. Talking about â€œit should do thisâ€ because that happens before undefined behavior is reached at runtime does not make sense because per the C standard that is not a C program and we donâ€™t have a standard for programs that look like C or arenâ€™t. 
I think he is talking about the intention behind unsafe blocks, which is to write safe code, at least under the assumption that people donâ€™t break their programs on purpose. So yeah, unsafe blocks in Rust is another way to write code that is safe, but the compiler cannot prove safe in Rusts safe subset.
Link to the issue?
Please fill an issue and link it here
You mean that the Rust to C transformation takes 1.5 the time rustc takes to compile Rust to asm, am I right ? Do you know how long it takes to compile the resulting C code afterwards ?
Usually , the data-structures and algorithms are named by somebody who can prove it reaches a lowest bound on space or time for some set of operations that are useful for solving a problem. If what you describe is a good solution to a problem. Add it to your readme so people can get a feel of what is possible with a JostleTree.
That was my estimate of the whole process. I don't have some real numbers to work with, but compiling all of libstd (1.19.0) with mrustc takes 1m30s (on a single core of a 2.3GHz Xeon). That said, there might be some crates which are faster, and some that are slower with mrustc.
This is memory use of rustc during compilation, right?
Good Bot
Thank you NeoLegends for voting on WikiTextBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
I like you, squishy, hairless monkey! â™¥â€¿â™¥ You will be my pet after we have conquered the world... *** ^^^I'm&amp;#32;a&amp;#32;Bot&amp;#32;*bleep*&amp;#32;*bloop*&amp;#32;|&amp;#32;[&amp;#32;**Block**&amp;#32;**me**](https://np.reddit.com/message/compose?to=friendly-bot&amp;subject=stop&amp;message=If%20you%20would%20like%20to%20stop%20seeing%20this%20bot%27s%20comments%2C%20send%20this%20private%20message%20with%20the%20subject%20%27stop%27.%20)&amp;#32;|&amp;#32;[**TÒ‰heÌ›&amp;#32;LÌ¨isÌ•t**](https://www.reddit.com/r/friendlybot/wiki/index)&amp;#32;|&amp;#32;[â¤ï¸](https://www.reddit.com/r/friendlybot/comments/7hrupo/suggestions)
but that is just your view and it being somebodies view doesn't actually add any substance
Didn't know that, thanks.
Looks interesting but if it's ```&amp;mut Model``` then it violates some core principles of Elm/Redux architecture. I wonder what's the decision behind this.
https://github.com/rust-lang/rust/issues/46974
Maybe `unsafe` should have been `unchecked` instead, to better convey the real meaning of the keyword: the code is in almost all cases _not_ unsafe for the caller, but it signals to the compiler to run less checks on this section of the code.
Finally, I did some progress to my [ggez](https://github.com/ggez/ggez) based game: [Easy kana!](http://tatrix.org/public/games/easykana/). Currently I'm integrating [tiled editor](https://twitter.com/Tatrics/status/945017239006310401) I hope my [minor contribution](https://github.com/mattyhall/rs-tiled/pull/39) can be useful to the community. 
Average is not generally useful; I've found. I would argue for Mode instead, that is, where are the clusters, and indeed pointing out the outliers (best/worst case) and trying to understand *why* they occur.
And that reason is that following them makes it easier for others to read the code, which is - apparently - something the OP wants.
Yeah, this is a longstanding issue that basically everyone wishes we could fix. There are a bunch of ideas in that space, `unchecked` is a common one, as is `trustme` and `assert_safe`.
Finally had some time to have another go at the [three year old unsoundness bug](https://github.com/rust-lang/rust/issues/18510) in the compiler again, and with some mentoring/assistance I was able to improve the solution. It was merged yesterday! ðŸŽ‰ 
You can try [actix](https://github.com/actix/actix-web). It is not based on hyper but asynchronous and simple. Here is [user guide](https://actix.github.io/actix-web/guide/)
Iâ€™m saying that the term â€œsafe codeâ€ is unnecessarily confusing (and not quite semantically right anyways), and we should say â€œcorrect codeâ€ instead. Changing your summary to use the new terminology: unsafe blocks in Rust are another way to write correct code in Rust, that the compiler cannot prove correct in some of the ways it can in Rustâ€™s safe subset.
I'm working on a terminal rendering library called prototty, which I intend to use to prototype game ideas, specifically roguelikes. It can render to an ansi terminal, or to a web browser with web assembly. Source: https://github.com/stevebob/prototty/tree/wasm-backend I made tetris to demonstrate: * Play in browser: https://games.gridbugs.org/prototty-tetris/ * Source: https://github.com/stevebob/tetris * Terminal version is in https://github.com/stevebob/tetris/tree/master/unix_app * Rendering is done in https://github.com/stevebob/tetris/blob/master/prototty_app/src/lib.rs The most interesting part of prototty is the way you describe how a type is rendered. If you want to make a type `Foo` renderable, create a new type `FooView` (typically as a a unit `struct FooView;`), and: impl View&lt;Foo&gt; for FooView { fn view&lt;G: ViewGrid&gt;(&amp;self, foo: &amp;Foo, offset: Coord, depth: i16, grid: &amp;mut G) { // how to render a Foo } } // ... and optionally ... impl ViewSize&lt;Foo&gt; for FooView { fn size(&amp;self, foo: &amp;Foo) -&gt; Size { // how big will the Foo be rendered (width and height in cells) } } Then you can render a `Foo` viewed with `FooView` to a `Context`: let mut context = prototty_unix::Context::new().unwrap(); // OR let mut context = prototty_wasm::Context::new(); let foo = Foo::new(...); context.render(&amp;FooView, &amp;foo); This makes it easy to define decorators: struct WithBorder&lt;V&gt;(V); impl&lt;T, V: View&lt;T&gt; + ViewSize&lt;T&gt;&gt; View&lt;T&gt; for WithBorder&lt;V&gt; { fn view&lt;G: ViewGrid&gt;(&amp;self, t: &amp;T, offset: Coord, depth: i16, grid: &amp;mut G) { // render the T leaving room for the border self.0.view(t, offset + Coord::new(1, 1), depth, grid); // now render the border around the T, using `self.0.size(t)` to find out its size } } And now you can render a `Foo` viewed with a `WithBorder&lt;FooView&gt;`: context.render(&amp;WithBorder(FooView), &amp;foo); It also makes it possible to render the same data in different ways. In the tetris example, there is a border around the board, and a separate border around the next piece display. I have two types `TetrisBoardView`, and `TetrisNextPieceView`, which both implement `View&lt;Tetris&gt;`. Then I decorate each of these with a `prototty:decorators::Border`.
Ah yeah that distinction makes sense.
Great recap, and passing 2017 work all around! It might be a common trend among the rust libraries wondering around: to align themselves with a particular existing API instead of inventing s new one. Not so long ago we decided to make gfx-rs a Vulkan portable implementation, and now Way Cooler aimed at being an AwesomeVM clone. On a side note, I'm a bit sad tiling is going away.
For `ioctl`, I thought that internally it does a switch on the second argument, the command word one, and then passes the last argument, long or pointer, to the actual `ioctl` implementation of the driver. But does that mean that `ioctl` in `glibc`, or wherever it is defined, is overloaded? I thought C is not able to do that? Is this some weird hack? 
As /u/slashgrin said, chrono is the way to go. In addition, to make _some_ things easier: [kairos](https://crates.io/crates/kairos) (shameless self-promotion)!
[Related](https://www.reddit.com/r/rust/comments/71rbui/difference_stdtime_and_time/dncynd3/) The other crate is `time`. It's *officially* deprecated, but many of us actually prefer it, especially for simple tasks. Both crates have 500+ dependants.
 &gt; For ioctl, I thought that internally it does a switch on the second argument, the command word one, and then passes the last argument, long or pointer, to the actual ioctl implementation of the driver. To some degree that "switch" is done in the driver itself, but yes, that's it. &gt; But does that mean that ioctl in glibc, or wherever it is defined, is overloaded? I thought C is not able to do that? Is this some weird hack? If it helps, do you know what Rust's [transmute](https://doc.rust-lang.org/std/mem/fn.transmute.html) is? If so, you can think of it as the library transmutes the `long` into a pointer, which it then passes to glibc, which makes a syscall, and the kernel then dispatches that to the driver, and the driver then transmutes the pointer back into a `long`. &gt; But does that mean that ioctl in glibc, or wherever it is defined, is overloaded? There is just one function entry, so no. Ioctl is not overloaded. What's passed through as the third argument is 32 or 64 bits of "something", really. It's most often a pointer to something else, but in this case it is just the actual number. Does it make things clearer? 
I'm happy to see Way Cooler move towards wlroots.
To be clear, "deprecated" means that anyone who thinks it's a good crate can pick it up and develop it further; the libs team is basically abandoning it. I did this with `semver` for example. So if you like it, maybe you should take it!
I believe you're asking several wrong questions. If the question was "How do I insert many elements to BTreeMap in the most efficient way possible?", then the answer would be to look at [BTreeMap::append](https://doc.rust-lang.org/src/alloc/btree/map.rs.html#737) and [BTreeMap::from_sorted_iter](https://doc.rust-lang.org/src/alloc/btree/map.rs.html#885) - I believe this is the most efficient way possible to add elements into the set. If the question was "How do I pass a location hint for a BTreeMap insertion?", then let's try to imagine an interface for the insertion with a hint: struct BTreeMapHintedInserter&lt;'a, K, V&gt; { map: &amp;'a mut BTreeMap&lt;K, V&gt;, location_hint: ... } impl&lt;'a, K,V&gt; BTreeMapHintedInserter&lt;'a, K, V&gt; { pub fn new(map: &amp;'a mut BTreeMap&lt;K,V&gt;) -&gt; BTreeMapHintedInserter&lt;'a, K, V&gt;; pub fn insert(&amp;mut self, k: K, v: V) -&gt; Option&lt;V&gt;; } Now, does this interface have *anything* similar to an iterator? No. Why would you want an iterator to pass the location hint? That's not what iterators are for. One of the problems with using iterators as "insert locations" is iterator invalidation, which is a subtle issue in C++. Rust tries to combat this by using a borrow checker - you can't modify the underlying Vec/Map if there's a reference to it. For example, an iterator.
It works for me as is. And if a breaking issue ever arises, I will happily try to fix it. But I'm sure someone else will beat me to it anyway. I guess the real status of abandoned yet popular crates will be *maintained-by-users*. The PR that was merged a couple of days ago is a good example of that.
In my understanding it's the following: The compiler cannot determine whether the call to `printf` will ever return (the file descriptor might be a non-buffered pipe). Thus the compiler cannot prove that undefined behavior will ever be reached, hence it must call `printf`. Because unless `printf` ever returns, this is a well-formed program.
Hey, do you still need help?
I'm not sure. I had assumed it was talking about code generated by rustc.
I don't think there's any supported way to do this using the standard benchmark harness, no. If you want to stick to `libtest` for your benchmarking, you could maybe get a better approximation by setting up a large number of trees ahead of time and then appending to them in a round-robin fashion. If you'll forgive me for plugging my own project, the [iter_with_setup](https://japaric.github.io/criterion.rs/criterion/struct.Bencher.html#method.iter_with_setup) method in [Criterion.rs](https://github.com/japaric/criterion.rs) sounds like it will do what you want.
Fixed, thanks!
&gt; On a side note, I'm a bit sad tiling is going away. For the record, the tiling in Awesome is done entirely in Lua scripts which is why it's "going away" on the Rust side. Way Cooler is still going to be a tiling WM.
Do you think it can be *possible* to make an OpenCL wrapper based on a futures reactor to help work with async events (copy between host and gpu, compute things and chain with other futures-based things) ?
No, sorry, that's not an answer to my problem. I know about iterator invalidation - saying "you surely don't need this" isn't helpful in this case. **After** the insertion has happened, I need references to the previous and next element (in the ordered set) to where the insertion happened, in order to compare them with the insertion, as well as the last and first element of the BTreeSet. And this is (currently) not possible without iterating through the whole BTreeSet. So I need an iterator to call `.next()` and `.rev().next()` (for the previous element). Remember that a BTreeSet is sorted, so once a value is inserted, iterators are invalidated. You also can't really "index" into a BTreeSet, because the keys are not contigouus, so "index - 1" wouldn't yield the previous element. This is not about efficiently inserting elements - it's about getting the previous and next element, after the insertion has happened. Maybe I should have clarified this in my question. EDIT: Returning an iterator is also useful if you want to iterate from the location where you inserted to the start / end of the map / set. Not my goal, but that is one use case of returning an iterator.
This past week I've resumed work on [tarpaulin](https://github.com/xd009642/tarpaulin). This week enjoying Christmas, closing some more issues and hopefully pushing some new in-progress features out (or at least significantly progressing them). 
&gt; My first, remarkably naÃ¯ve, thought was to be impressed the compiler was doing loop unswitching automatically on this code; I know it's possible but I didn't expect it to happen. Assembly for both side by side and we can tell what happens. 
Sorry for misunderstanding you. Then if your question is "How do I get references for elements before and after a given key?", then the answer is to use [BTreeMap::range](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html#method.range) and [BTreeMap::range_mut](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html#method.range_mut). For example: use std::collections::{BTreeMap, Bound}; use Bound::*; fn main() { let mut map: BTreeMap&lt;usize, bool&gt; = [ (8, false), (1, false), (4, false), (0, false), (2, false), ].iter().cloned().collect(); if let Some((_key, val)) = map.range_mut((Unbounded, Excluded(2))).next_back() { *val = true; } if let Some((_key, val)) = map.range_mut((Excluded(2), Unbounded)).next() { *val = true; } println!("map: {:?}", map); } *Edit:* more explicit range argument
First, trees such as `BTreeMap` are usually generic over both keys and values. Yours seems to be only generic over values where as the key is always u32. Correct? Second, what does it mean that a "thing" changes its width? This tree is generic over `T`, but `T` has a fixed size (there is no `T: ?Sized` bound), so `T` always has the same size. Or is "width" not related to memory sizes, but to the difference between two keys? 
For those who haven't heard of non-lexical lifetimes before, it's simply a feature that improves the Rust compiler's ability to reason about lifetimes. It seems to remove most of the remaining cases where people commonly experience the borrow checker rejecting valid programs. For example, the following program is rejected by the current borrow checker, but is allowed with NLL in place: let mut v = vec![1,2,3]; v.push(v[0]); [Try it out here on the playground.](https://play.rust-lang.org/?gist=12443c70ee73ab1d59254838367fe5aa&amp;version=nightly)
How well does Postgres work as a pub sub? Iâ€™ve been using Redis or Rabbit mostly but itâ€™d be nice if I could drop a service dependency if Postgres works well enough and is easy to use. 
Does it work with %Z ? %z appears to be the offset from local time. See: https://docs.rs/chrono/0.4.0/chrono/format/strftime/index.html
Check the "Time Zone Specifiers" section under [`chrono::format::strftime`](https://docs.rs/chrono/0.4.0/chrono/format/strftime/index.html) documentation. It looks like `%z` expects a 4-digit offset from UTC formatted like `+0930`, not a time zone name like `GMT`.
It works fine, but I'm only using it for fairly small things so far, as I said. So I'm not really hitting any limitations/disadvantages it might have. You have to dig under the hood of `diesel` a bit and write your own SQL and such, but that wasn't super hard.
Yes, but to use the `range()` method, you have to already construct a `Range`. The only way to construct a `Range` is by searching the map for the key (which I don't want). A `Range` needs a "bound" or indices to work and I don't have them. The only way to get them would be to search the map, which is ridicoulus considering that I (technically) have a reference to the element, that I just inserted. No, the standard library just doesn't have a way to do this. By using the `BTreeMap` directly, I can see that I can get a `::node::Handle&lt;_&gt;` (which is more or less the "index" to the inserted element). So the question is, how can I convert the handle to an iterator / range? To illustrate, this is what I want: let mut my_map = BTreeSet&lt;&amp;str&gt; = [("Alice"), ("Bob"), ("John")].iter().cloned().collect(); let iterator = my_map.insert_with_iterator("Dan"); // will be inserted between "Bob" and "John", iterator points to the 3rd element // the items aren't strings, but structs, please disregard that the following doesn't make sense with strings if iterator.rev().next() == iterator.clone().next() { /* do something */ } 
I would also like to add that &gt; I need an iterator to call `.next()` and `.rev().next()` This an *invalid* requirement to for an iterator - Rust iterators are "consumed" instead of being something akin to a cursor in C++. If you read [documentation for DoubleEndedIterator](https://doc.rust-lang.org/std/iter/trait.DoubleEndedIterator.html), it says "It is important to note that both back and forth work on the same range, and do not cross: iteration is over when they meet in the middle." Which means that a double-ended iterator is expected to run from the end, not from the current position. This is why in my example of getting the previous and next element I have to get the iterator twice. Is it possible to construct an insertion interface that returns iterators for preceding and following elements at given location? Sure. Is the iterator an appropriate way to return previous and next elements at insert position? No.
Don't slip on the ICE! 
I'll take the news as a Christmas gift :)
&gt; Yes, but to use the `range()` method, you have to already construct a `Range`. The only way to construct a `Range` is by searching the map for the key (which I don't want). This is false. If you were able to insert a value, you have access to the key. Clone/Copy the key and use it.
Are the commas after the HTML attributes a syntax requirement for macros, or are they a design choice? This looks really good, and I can see myself using it! 
It says 'formatting only' in the [chrono docs](https://docs.rs/chrono/0.4.0/chrono/format/strftime/index.html) so I am afraid this kind of automatic "parsing a timezone from its abbreviation" is not possible. If you know you will always get UTC, you can just manually use Utc's `datetime_from_str` method: https://play.rust-lang.org/?gist=b390deee0cf41ea4d5981ec58dd21f10&amp;version=stable
[Previous thread from yesterday](https://www.reddit.com/r/rust/comments/7lq5t3/featurenll_available_in_nightly/).
TLDW would be nice. You can't scan a video like you can a text. And I don't want to watch 20 minutes of video.
I am also thinking about implementing ART in Rust, as the data structure shows impressive performance. It seems that performance number is on par or better than BTreeMap. Thanks for your work!
That doesnâ€™t really matter, because the compiler is allowed to transform the above code as (from mobile): int i = ....; int j = i + 1; // UB overflow printf(â€œ%dâ€, i); i = j; Or int i = ... + 1; // UB printf(â€œ%dâ€, i - 1); That is a valid transformation under the assumption that UB wonâ€™t happen, which is an assumption that the compiler is allowed to make. But because in this case UB does happen you canâ€™t rely on printf being reached at all (you canâ€™t rely on anything, really). The moment UB is somewhere in your program, having any expectations at all about what that program might do is just the wrong mindset. With optimizationâ€™s like inlining that UB might propagate to the most unexpected places causing the compiler to generate code under assumptions that will be violated at run time.
Iâ€™d go with external worker processes because it makes scale-out and failure-handling much easier to deal with. That said - Iâ€™m also very curious about this (we deal with scheduled jobs a lot). One approach Iâ€™ve idly considered - being on AWS - is hooking up lambda jobs paired with SQS queues. Sadly not rusty, but...something to consider.
Yes, but then I'd have to search for the key again, after I inserted it. The "use it" is the problem. If you use the Range with a key, Rust will start searching the map / set for the key, which is costly / unnecessary. I put an example in the parent post. Take a look at the implementation of the `range()` method: let root1 = self.root.as_ref(); let root2 = self.root.as_ref(); let (f, b) = range_search(root1, root2, range); Range { front: f, back: b} You just silently introduced a `range_search`, an (unnecessary) ~~O(n)~~ O(log(n)) operation ([source](https://doc.rust-lang.org/src/alloc/btree/map.rs.html#1783)).
I think Criterion.rs is AWESOME, so many thanks for the pointer! Maybe I'll come back with questions :)
I've tried to experiment w/ radix-based structures in [rdxsort-rs](https://github.com/crepererum/rdxsort-rs), but didn't produce anything useful. Some ideas of the cited paper (e.g. lazy expansion and path compression) came to my mind, but the implementation, especially due to the borrow checker, isn't straight forward.
[removed]
Thanks, didn't know that. Still, a range doesn't get me the elements without an additional linear search, so it's not a solution if I already have a handle.
Nice! We can even enforce that they are UTC by including that in the format string. utc.datetime_from_str("Jan 30 02:19:17 2018 GMT", "%b %d %H:%M:%S %Y GMT")
Really? That's awesome.
&gt; an (unnecessary) `O(n)` operation You misunderstand the algorithm. The `linear_search` goes through children of the current node, not through all elements. The cost of `range()` is O(log(n)), you can trivially test it.
I love you! That's the best Christmas gift I've got this year! ^((if only because I didn't get any) ^gifts) Merry Christmas!
Consider contributing instead of doing it anew, there is still a lot of work to do. Performance is not as good as I want it to be yet. Most of the benchmark is done with random input, and I seriously doubt BTreeMap could compare with ART if we had large strings as keys with a long common prefix. Sequential search though seems to be very fast.
Absolutely incredible work, and a great gift to the Rust community. Thank you!
There is also [Rudy](https://crates.io/crates/rudy) a Rust implementation of [Judy arrays](https://en.wikipedia.org/wiki/Judy_array), which are roughly the same thing. It seems the ART authors weren't aware of them (https://infosys.cs.uni-saarland.de/publications/ARCD15.pdf). But hooray; I'll check it out. :D
**Judy array** In computer science, a Judy array is a data structure implementing a type of associative array with high performance and low memory usage. Unlike most other key-value stores, Judy arrays use no hashing, leverage compression on their keys (which may be integers or strings), and can efficiently represent sparse data, that is, they may have large ranges of unassigned indices without greatly increasing memory usage or processing time. They are designed to remain efficient even on structures with sizes in the peta-element range, with performance scaling on the order of O(log256n). Roughly speaking, Judy arrays are highly optimized 256-ary radix trees. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I have implemented both. Didn't have too much trouble while implementing basic DS, but encountered several problems while trying to optimize. 
Yes, Judy is in general better, but way harder to implement. 
Dito. I've got a very basic version running which so horribly slow compared to btrees. 
Yes - sorry, my bad - but that's missing the point. It's not needed at all, it can be O(1), like the C++ version.
My initial implementation was 4x slower than BTreeMap, but it didn't take me too much time to catch up. You can check out my benchmark numbers on the github page. It is basically on par with BTreeMap on random data and taking over for bigger keys. For non-random data, ART should be faster, haven't investigated yet though.
Lack of a GC makes it hard to make the immutable data structures you need for the elm architecture. I'd love to see an immutable.js style lib in rust!
Dude, it's a human language. that shit doesn't make sense. It's all broken (humans break *everything*). Cleave: 1) To bring together. 2) To separate into parts. WTF? Inflammable? yeah, that shit is broken yo! Just ask which definition they are using and then go with it for that context. I would avoid the word personally just because of the confusion which has been outlined, but yeah this shit is broken.
A feature so good they posted it twice :)
https://crates.io/crates/crossbeam has some related work
Well, those languages (Elm/JS) don't have the same mutability system as Rust does; we tend to attack shared mutable state from the "shared" side rather than the "mutable" side. So, in some sense, yes, but in other senses, that's what I'd expect a Rust version to look like.
Have you tried [signed_duration_since](https://docs.rs/chrono/0.4.0/chrono/struct.DateTime.html#method.signed_duration_since)? https://play.rust-lang.org/?gist=27b579a9b43ce36a4dd0dd0508d426f5&amp;version=stable This does not give days as floating point though, so if you have a period of, say, 30 days 12 hours and 4 minutes, it will still show up as 30, not 30.5.
thx
There are several ([1](https://crates.io/crates/hamt-rs) [2](https://crates.io/crates/rpds)) persistent datastructure implementations on crates.io and I seem to remember implementing these types of datastructures as part of the motivation behind [crossbeam](https://github.com/crossbeam-rs/crossbeam).
It's not "because of" `Copy`, but it is related. Basically, there are multiple styles of APIs that produces a value of type `T` from an existing one. * `fn(&amp;mut T)`: mutates the argument inplace * `fn(&amp;T) -&gt; T`: takes the argument, does not consume or change it, returns a new `T` * `fn(T) -&gt; T`: consumes the argument, returns a new `T` (possibly the same, mutated, but you can't know) For `Copy` types, the second and third point are interchangeable since conceptually, a copy is moved into the function. Now, which of the three ways is chosen for a certain method depends on many factors, such as convenience, consistency and efficiency. For values with small memory size (which are often `Copy`) variants 2 or 3 are more useful (you get to keep the original value if you need it), while for potentially big values like Vectors, variant 2 would be wasteful if the original value doesn't need to be preserved, and variant 3 is not particularly idiomatic.
unsound code lets you violate type safety which is simply a more specific type of unsafety. Arguably due to Rust's substructural type system certain kinds of pointer aliasing also fall under unsoundness as well.
That works already on stable.
If `Hash&lt;D&gt;` implements Serialize and Deserialize regardless of `D`, the generated code isn't going to need any bounds. [Playground](https://play.rust-lang.org/?gist=08181a24507916f7898acb2991674eea&amp;version=stable)
Rust's built-in maps and sets are kind of lacking when it comes to complex update semantics, yeah. FWIW, what you really seem to want is a cursor, not an iterator. If I may suggest an API, building off `BTreeMap`'s `Entry` API makes sense. VacantEntry::occupy(self, value: V) -&gt; OccupiedEntry OccupiedEntry::next_occupied(self) -&gt; Option&lt;OccupiedEntry&gt; OccupiedEntry::prev_occupied(self) -&gt; Option&lt;OccupiedEntry&gt; `VacantEntry::occupy` should mostly be a copy of `VacantEntry::insert` except you need to recover the handle in the `Split` case as well (which means searching the two new nodes), whereas `next_occupied` and `prev_occupied` seem like carbon copies of `IntoIter::next` and `IntoIter::next_back`. No guarantees, but it seems like the right idea. 
The more general term for what C++ calls "iterator" is "cursor", and we don't have any in the standard library, for collections (there is one for seeking IO buffers, but that's it AFAIK). "Entry" APIs are close but provide no way to move to other elements and Rust iterators are based on something like a pair of cursors, but don't expose arbitrary movement, once you've passed by an element you can't see it again. What I'd recommend is searching around for a collections crate with cursor support, if one exists at all.
Or if `Hash&lt;D&gt;` only implements Serialize and Deserialize for certain values of `D`, the deserializer lifetime is called `'de` by convention. [Playground](https://play.rust-lang.org/?gist=aa9818c5ab8f5aa50fb9d6ad17ea0c20&amp;version=stable)
Don't embarrass yourself. mod is an abbreviation of moderator commonly used on reddit. Also I am a mod of this subreddit myself, so you're barking up the wrong tree. Finally, you'd do well to either read and follow the rules (you can see them on the sidebar and subreddit info) or go elsewhere.
I just landed fast UTF8 character count in bytecount with the help of /u/Veedrac and CAD97. Will release soon. Also TWiR.
thank you so much!
Rust can write anything that C/C++ can write, and you'll generally use it in the same sorts of things that you'd use C/C++ for, or alternately things that you might otherwise run a java/C# server for. The drawback of rust is that right now the library situation is patchy in some parts. At the _language_ level there are very few projects that are better written in other languages, and usually the other language that would be better is something strange like Erlang or Haskell. The only risk factor in using rust at the moment is that a lot of library space is uncharted and you might have to build your own tools from relative scratch.
Rust's whilst a general purpose programming language, has its initial niche as a systems programming language. I.e. aspects of system infrastructure where both safety and speed are important. For instance tools such as ripgrep and xsv are already 'best in class'. Beyond this Mozailla are writing much of their new browser infrastructure in Rust. Other areas will come in time, for instance rust as a key web assembly language is seeing some improvements, areas such as game programming have probably not seen as much development as one might expect. That said remember Rust is a relatively new language and many of these additional things will come in time. Of all the use cases Rust is probably least suited as a scripting language, due to compile times.
Luckily Rust has a great type system. ðŸ˜›
Thanks
I'd say the difference between compiling and transpiling is the nature of the output. Things that output something which is supposed to be human readable are transpilers (eg coffeescript/typescript output javascript), and things that don't are compilers (for example javac outputs bytecode, gcc/g++ output assembly, neither are written by hand often). Of course, this just shifts the problem to defining what is human-readable, and what is not, but I still think the term "transpiler" has some merit.
Let me try to show that this cannot be the case. I guess we agree on the following not being UB, right? int *p = NULL; if(p) { *p = 1; } Then I modify it into a loop, adding IO so that the resulting loop isn't UB (because infinite loops without IO or other side effects and non-constant loop condition are UB in C): int *p = NULL; while(!p) { printf("still null\n"); } *p = 1; Now comes the interesting part. Consider the following program: int *p = NULL; int input; scanf("%d", &amp;input); if(input == 0) { p = malloc(sizeof(*p)); } *p = 1; Is this program fine according to the C standard as long as the user only inputs 0? If it is fine, then your program also well-defined as long as the user can ensure that `printf` will never return (e.g. using unbuffered writes to a unbuffered fifo). Thus the compiler cannot create any strange behavior until after the `printf` call â€“ because it cannot prove that it can ever return.
Pinging /u/llogiq about this: will it be possible to detect some of the most common/obvious "manual workarounds to make the borrow checker happy even though the code is safe" in clippy and suggest the simpler, more intuitive, non-lexical version?
This documents the (successful) process of porting a game I'm working on to wasm. It might be interesting because I didn't know much about wasm or the browser graphics APIs (so you might learn something if you're in a similar boat) and the game was expected to run in the browser so it shows where those assumptions might clash. It's still a relatively simple project as far as games go, but it's not a toy example built with wasm in mind.
&gt; I want to learn Rust by writing projects but only the right projects, those that can't be done in a more suitable language. Those might not be the right project to *learn* a language, though. I know you're probably looking for niche problem spaces Rust is good at, but I just want to point out it goes beyond "there's an awesome CSS parser library, you should write things that use CSS". For example: I think Rust is an amazing language for large codebases because it's a pleasure to refactor and maintain. I would not suggest you to write a huge chunk of code and refactor it to learn Rust. How about writing a parser for a weird binary format? Or a small self-contained CLI tool (I have [some tips](https://deterministic.space/rust-cli-tips.html))? You could do that in Haskell or Go, but as a learning exercise they are very good projects in Rust, too. If you like premature optimization (who doesn't?) you could also profile, benchmark, and optimize them to see another one of Rust's features: Performance.
Imo, the paper didnt even show that. The abstract claims it but the results suggest no such performance improvements.
Ooh, that would be nice. I've actually been adding comments for those weird moments.
Not to be all moderate on your parade here, but saying Rust is the top language by feature for most projects seems WAY overboard / almost pcj worthy. In a world where everyone knows Rust like we do, maybe. In a world where many developers struggle to be productive in python/C#, Rust is just too dense and requires too much foresight to be as useful to companies trying to hire devs. And imagine trying to teach rust as a first language! There are probably a few catagories of problems better solved by scripting languages, markdown languages, and other more forgiving (probably gc enabled) tools. Perhaps Rust is the best language for you specifically to solve most problems?
&gt; those that can't be done in a more suitable language This is an impossible requirement to satisfy. Can you please relax it? &gt; What fields does Rust excel in? Speaking from experience: Rust is great for fast text searching of all kinds (although depending on what you are doing, you may need to implement your own algorithms, but that's kinda what I mean: Rust is great for doing that kind of low level work).
I just wanted to avoid things that are definitely not useful to know, like using rust for file manipulation where a simple shell script could work. But as you guys said, it really is for everything I previously did in C++. What I have in mind now is that I should use concurrency (I heard rust is good at it) and the computation power (because its a native binary). Do you know any other features I should definitely utilize?
The compiler can assume that i wonâ€™t overflow and if i wonâ€™t overflow then moving the integer arithmetic before the printf statement is allowed because the change is not observable. The only way in which the user could observe the reordering is if i would overflow but, again, the compiler is allowed to assume that it cannot happen. Whether printf can or canâ€™t return is completely irrelevant. 
Is there any place to track the development of the coroutines feature in rustc?
So, I donâ€™t have any experience with Juniper but have a fair amount with GraphQL. Unfortunately the GraphQL spec doesnâ€™t really support a â€œgenericâ€ input type. You can use Enums in an input but that is really more of a value level definition. You can return â€œunionâ€ types but for input you would need to define something like a CowData input object etc. I havenâ€™t looked closely at Juniper but with Absinthe (Elixir) I would define the fields common to a cow and include that object in both the CowData and Cow types. It is awkward and annoying and hopefully they address it soon, the asymmetry of being able to return union types but not take them as input makes things far more verbose than they need to be as well as making it easy to have them fall out of sync.
Applying your argument to this code snippet shows that it shouldn't be the case: int *p = NULL; while(true) { printf("still null\n"); } *p = 1; According to how I understand you, you argue that this code snippet is UB, because the compiler may reorder the `*p = 1` on top of the loop. However, AFAIK, it isn't. Can you comment on that snippet?
I could see some cases where a small one-liner type program is best served by some other language. In the recent Advent of Code I found myself on many days knowing _exactly_ what I wanted to write in Haskell, and the Rust code wasn't as clear to me until I looked things up in the docs to review. Or I typed what my Haskell brain told me I should write and then `rustc` gave me some bonkers complaint about how "oh, sorry, every single closure is actually its own type, haha", and I had to adjust my expectations. However, if the goal is to build _projects_, to invest time into constructing a system and to care about the quality of that code and the fitness of what you've built, then you almost always want Rust, not Python or Perl or Ruby or Java. Again, I am only talking about language features, not libraries, so in some cases if you want a library or engine that Rust doesn't offer then maybe you want that other non-Rust language. I don't think that you can name a mainstream language that beats Rust at the language level though. I've never talked to anyone that only knew Rust, but I actually _have_ spoken to people that only know Haskell. Really. Know what they say? That Haskell makes perfect sense to them, as natural as walking, and that trying to look at Java or Python code is a huge bundle of crazy with all these extra dumb rules you have to keep in mind. That you can never keep track of what's going on. That the compiler is nearly no help in diagnosing the problem when there is a problem. So, is a dev struggling to work in Python or C#? Maybe that's the fault of Python and C#.
Are there any immutable crates on top of crossbar?
(nitpick: time and serde crate links in the Crate investigation section all link to the rand crate) Good article! Gives a nice overview of the steps and hurdles of the process.
Do you have a sample of generated C code?
Ropey is specifically a utf8 rope, so I think "scalar value" is correct then. Thanks for your help! I've fixed it in 0.5.2's readme and documentation. :-)
Your function should be extern "C" if you don't want to rely on Rust's unstable ABI.
Based on your description, this sounds more-or-less like a [rope](https://en.wikipedia.org/wiki/Rope_%28data_structure%29), except applicable to more than just text.
Indeed there is no consistent way to parse a time zone from its abbreviation, because multiple time zones can have the same abbreviation. For example, â€œCSTâ€ can mean either Central Standard Time and China Standard Time.
Awesome! Bootstrapping rust for our #t2sde (https://t2sde.org/) is a mayor pain! also really do not like the installation procedure the upstream Rust maintainers recommend: curl https://sh.rustup.rs -sSf | sh
I likely will once people have reported successful builds on various platforms (compilers and library arrangements). My testing platforms are quite limited, and I've already received reports of compilation and runtime failures with different distributions.
Isn't that specific example unrelated to NLL though? 
What a huge year for Rust 2017 has been! Congratulations and thank you everyone!
Wow, this is quite awesome (obvious pun obviously intended). AwesomeWM compatibility is quite a goal, and as an AwesomeWM user I'd love to see that happen! (Especially if it solves the issue of proper hidpi support in the process).
Rust may or may not be a contender. If it can be more productive tham C++ under time pressure, then its perfectly viable. I, for one, like C++ but I find Rust extremely promising and poised to take its place unless C++20 bounces back with concepts, modules, and more powerful compile-time metaprogramming.
This is definitely not true, a lot of things are still unstable, or not implemented fully or at all (and long-term dependence on nightly is IMHO insane). And there are some fundamental differences from other languages (most notably class-based OOP languages) that make certain things difficult to implement in Rust (e.g. GUI frameworks). If Rust can keep the current speed of improvement, it will be a mainstream PL, but until then, your post sounds like dishonest marketing.
https://gist.github.com/thepowersgang/3449f1fcaba04f518e1aacc96bdba538 is the generated C for a "hello, world" example, most of it is definitions and helpers.
I use Rust for simple file manipulation I used to do in Python (and, before that, shell script) because the strong typing and monadic error handling make it much easier to ensure I've handled errors as comprehensively and meaningfully as possible. Heck, simple file manipulation is probably one of the places I need Rust *most* because, if there's a bug, it could destroy data or, worse, mess it up in ways that make it a hassle to tease apart "messed up" from "legitimate changes not yet in the nightly incremental backup". (And automated tests which need to mock the filesystem are a hassle to write.)
The example fails to compile without NLL enabled.
As best as I understand, it's a combination of NLL and another feature called [two-phase borrow](https://github.com/rust-lang/rfcs/pull/2025).. Since both require MIR they're being implemented together. But that's only to the best of my understanding. ---- Briansmith made a really interesting comment about two-phase borrow not being limited to method calls. [His example works](https://play.rust-lang.org/?gist=6e84aa159333fdfd16fa7426fdad8d7d&amp;version=nightly) - way too cool.
That is beside the point. The idea behind NLL, as I understand it, is that a borrow need not last until the end of the current statement or scope, but can end early. So NLL would enable: let mut v = vec![1, 2, 3]; let vp = &amp;v; let len = vp.len(); v.push(len); The example that /u/kibwen shows introduced new magic where the `&amp;mut v` that is created while calling `Vec::push` (because arguments are evaluated left-to-right) is temporarily ignored so that `Vec::len` might be called. I was under the impression that this change was distinct from NLL.
Finally, someone who understands me!
What about two new methods for the Entry enum: `previous_entry()` and `next_entry()`? You don't have to deal with `Option`s, you always get a new `Entry`. I have a feeling this API is unsound somehow, I'll have to think it through tomorrow. EDIT: Okay, I can't sleep, can't stop thinking about a possible solution. Here's what I came up with: the Entry API is a good place to start but an Entry instance means mutable borrow for the whole map, which means multiple Entry instances have mutable access to the map (shouldn't be possible). I looked at the API docs and found the `range()` and `split_off()` APIs, which provide a part of the required functionality. The solution would look like this: generate two immutable Ranges, not from RangeArguments, but from the key, one for elements in the map before the key, and on for elements after the key. I just realized that the sets don't have Entries, but it could be two different versions, one for the Entry and one for maps/sets (that uses the Entry API internally).
Thanks for the information. So it seems to me that either two-phase borrowing is considered a part of NLL, or there is a lot of confusion surrounding these changes. 
It really does look like the two have been merged together unofficially. 
I think that example should continue to be rejected. Although it's trivial *in this case* to see that `a` won't be moved from twice, it can very quickly become too difficult for the compiler to reason about. Instead we can say "the compiler always assumes that any branch can be taken; it's not smart enough to see that the same boolean controls them both." The reborrow idiom (`&amp;mut *a`) is allowed. I don't really like the term "reborrow"; the Rust says "borrow the target of `a`". In the past I've said that I'm not a fan of this idiom but I think I'm warming up to it as I become more proficient. let e = if c { &amp;mut *a } else { &amp;mut *b }; e.push_str("(altered by mutate_one)"); let f = if c { &amp;mut *b } else { &amp;mut *a }; f.push_str("(altered by mutate_two)"); This is allowed because it's safe for `e` and `f` to point to the same thing. If that happens, the first `push_str` will happen before the second `push_str`. It's definitely another new concept that people won't have before Rust.
Woah... what? WHAT??? Especially since you *cannot* pass lvalues through `if ... else`. let e = &amp;mut * if c {a} else {b};
Very quick reply before I'm going to bed: are you aware of https://github.com/rust-unofficial/too-many-lists? (the online rendered version seems to be down, but reading the readme recreating it locally should be pretty easy)
I doubt that contestants would choose rust. Usually only standard libraries are allowed in these contest, and some essential functionality of rust for solving problems (e.g. random, `lazy_static`, fast `HashMap`) lie outside `std`.
My opinions are already based on stable only. I generally don't even look at what Nightly stuff does. If I pull open the std lib docs and search something and it comes up as nightly, I just skip it. The biggest thing that I want to see in stable that isn't is 128 bit support. Everything else is nice but not really required. I'll say that you _almost_ have me on the GUI stuff, except that there are GUI frameworks that are being built up for Rust even without OOP.
Interesting, yeah, the mechanism is basically the same.
Width is not related to memory sizes, it's a number (it should be generic over number types, I just haven't done that yet) associated with each element. Span and Offset as representations of the widths and positions of a series of adjacent ranges is one way of imagining the kind of data this models. It occurs to me that this span-range representation *could* be bolted onto a regular sorted tree map, but right now it couldn't really be said that there's a key type. Items are indexed by their `offset` but their offset changes as the items with lower offsets change.
As the [An Obligatory Public Service Announcement](https://web.archive.org/web/20170915080622/cglab.ca/~abeinges/blah/too-many-lists/book/#an-obligatory-public-service-announcement) section in the book Darsstar mentioned explains in more detail: In the real world, linked lists are a class of niche data structures which get undue attention due to how academia loves functional languages. However, they have many practical downsides with regards to real-world hardware. (The book covers several in detail.) They're also a poor choice for beginning Rust practice because implementing one (rather than using [`std::collections::LinkedList`](https://doc.rust-lang.org/std/collections/struct.LinkedList.html) or depending on someone else's implementation through cargo) is a significantly more skill-requiring task in Rust than in most other languages, so it's not representative of how difficult solving real-world problems is likely to be. (Though they do make a for a good intermediate-level learning experience because, if you *can* implement one, you've got a pretty good grasp of how to use various ownership-related constructs in Rust.) Here's the URL the book's normally at when it's up: [Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/) Here it is through the Wayback Machine, for reading right now: [Learning Rust With Entirely Too Many Linked Lists @ Wayback Machine](https://web.archive.org/web/20170915080622/cglab.ca/~abeinges/blah/too-many-lists/book/) ...and, to complete the set, here's the source repo Darsstar already gave, which claims that rendering your own copy is as simple as `cargo install mdbook; mdbook build`: [rust-unofficial/too-many-lists @ GitHub](https://github.com/rust-unofficial/too-many-lists)
That's why `next_occupied` and `prev_occupied` take `self` by value; it ensures uniqueness. You could use ranges instead, which is more and less flexible in various ways, but it seemed harder to implement. Traversal with `Entry` is trickier because you can't actually conjure up `VacantEntry`s: they have keys stored by-value.
My previous post might sound overly pessimistic, but many systems programmers will miss multiple features when they look at stable Rust; inline assembly, SIMD, special compilers (e.g. ICC), targeting all kinds of GPUs, MCUs and OSes, generating code from hardware descriptor files for device drivers (most of this would be unsafe code, and unsafe Rust is not fully defined yet, so it might be problematic), bindings to domain specific libraries, etc. Also, a lot of proprietary software's (e.g. CAD, analysis, etc.) extension/plugin API is C++ and writing unofficial bindings for them would be a lot of work. Most of these problems will disappear in a couple of years, because some of these are also important for Firefox and/or under heavy development, but these can be dealbreakers. In the meantime, Rust is excellent for complex terminal applications and speeding up software written in languages with a big runtime/WM among other things.
haha not sure what you mean but okay!
Because this doesn't introduce new APIs or syntax (correct me if I'm wrong) would I be right to assume it's just behind a feature flag to gain confidence that it's stable in broader usage?
cool, let me take a look at the tutorial. I like to understand the basics first before solving real world problems, since I feel that if I can't code something basic like linked list or graph, than I am going to have a hard time coding more complicated algorithms like neural network. Also if I ever want to get a job in rust, I would imagine that this would be the question a company would ask too. Most of the interview question I get is binary tree, linked list or graph related. 
Awesome, thanks for the recommendation, let me take a look.
For the purpose of advertising this change to users I've been using "non-lexical lifetimes" to encompass two-phase borrows as well, yes. Casual observers don't need to get bogged down with the details, especially since two-phase borrows are also technically non-lexical. :P
Remember that lifetimes are a compile-time construct; they have no effect on codegen. When you write `let x = y`, the type of `x` exactly matches the type of `y`, so their lifetimes match, but annotating `let x: &amp;mut _ = y` means unification happens, and Rust has subtyping of lifetimes so the lifetimes *don't* have to match. It's not clear to me whether this difference is intrinsically necessary (I don't see a strong reason to not just always reborrow), but it's a natural consequence of the rules as they stand.
Enjoying the upcoming new year by chopping my way through the features of https://github.com/kvark/vange-rs, making sense of the original C code and getting the logic implemented, bit by bit.
Presumably, it's that you're passing different values for the `--user` argument.
This is correct. The milestone for the "NLL prototype" that was exhibited at the recent Mozilla All-Hands still has a few outstanding issues (https://github.com/rust-lang/rust/milestone/42) and the issue tracker has plenty more with the NLL tag (https://github.com/rust-lang/rust/labels/A-NLL). There's plenty of work yet to do, but it's in a good enough shape that now's a good time to get experience with it.
typo. they're the same
do you know how I can capture the full command line from "Command::new(".....")" ?
I've removed them, didn't help
Insofar as I'm aware, you can't. I think there's a `Debug` impl, but last I saw, it doesn't take quoting into account.
actually, it did. thanks.
&gt; It's a little annoying since the execve syscall you're looking for happens in the child process. A debugger would also work. this won't show the exact command line arguments passed to curl
The technique of adding additional fields to a balanced search tree is called "augmenting" and the resulting data structure is called "Augmenting Data Structure", c.f. [link](http://www.bowdoin.edu/~ltoma/teaching/cs231/spring14/Lectures/10-augmentedTrees/augtrees.pdf). Also see a generic [segtree implementation](https://github.com/EbTech/rust-algorithms/blob/master/src/arq_tree.rs) in rust.
We should figure out how the rules define "standard library" and then make the smallest possible change to the core project such that the top 500-or-whatever crates fit within that definition :)
I find it easier to explain to people by basically changing: unsafe {} to trust_me {} .unwrap() to .trust_me() Then it's clear that the user is telling the computer that they think the code is fine. Then I can just be like "but actually it's really called unsafe and unwrap..."
I am the author of Rudy. I have most of Rudy basically implemented. The major missing features are one of the node types that helps with sparse arrays and iteration. I am uncertain how much interest there is in Rudy, so I have not spent much time on it recently.
We already have an issue for this and....currently think very hard about what those common patterns actually are ðŸ˜‰
Did you correct the type in your edit? You may want to do so. The current `Command` is basically doing `'api:key-1234'` (with extra quotes)``` where the command line version is just `api:key-1234` without the extra quotes.
What was the solution? GP has deleted their comment.
I believe the problem are the various single quotes in the arguments (they are usually parsed by the shell, but there is no shell here). 
wonderful! Open source?
I agree, quotes are almost certainly not necessary here for this invocation.
I see your reasoning, but I think it also works the other way around. Coming from imperative languages, I am pretty helpless when trying to write haskell. Furthermore, the error messages from the compiler dont help me much, as I dont know what the terms in them mean. I dont however think its haskells fault that im struggling to work in it, but rather my fault for not having the know-how: In order to use haskell I need to invest more time into learning and practice. Sure, some languages are quantifiably better than others, but most mainstream languages are both usable and learnable (how else would they become mainstream). Struggling to use them thus cant be fully blamed on the language itself.
Itâ€™s actually [noted here](https://github.com/nvzqz/static-assertions-rs/issues/2) that someone else also wants that.
Iron to me feels outdated and underdocumented. I took another look at Rocket, and realized even though it doesn't offer infinite middleware, it satisfied pretty much everything I could reasonably think of, and did it more concisely. The catch of having to run nightly isn't that bad.
&gt; rascal.exe why not!
I was thinking of using rocket, are there any resources for creating middleware or are they going to support it in the future? Middleware just seems like a very big thing to be missing.
Itâ€™s his first name.
http://www.rascal-mpl.org
Rocket supports middleware, but under the name [fairings](https://rocket.rs/guide/fairings/). IIRC, the author was initially opposed to the conventional notion of middleware, then came around to it but with some rules. You might peruse through the guide a bitâ€”fairings arenâ€™t the only way rocket solves problems that are traditionally solved by middleware: â€œrequest guardsâ€ are worth looking at too. I could have that all wrong, but thatâ€™s what Iâ€™ve gathered over the past few months of using it (and digging it). Edit: more info.
The hype is real. I was able to remove a few hacks in my codebase with this feature (mostly related with `None` pattern borrowing the value). Great job!
Yeah actually since the rocket suggestion I've building an app in it, fairings are new to rocket 0.3.*. The docs for rocket are really awesome, they also go through integrating diesel which is fantastic! I'll push my project to github in a few days and link it! :) Back to my original point though, I really wish Iron would wrote more documentation as there are a lot of features that people could potentially learn from.
Assigning through a pointer has side-effects, incrementing an int does not, so you canâ€™t extrapolate from one example to another because the types of undefined behavior arenâ€™t the same. However, C guarantees that all loops terminate, so if your C compiler can see through printf due to LTO and see that in this case it always returns it can actually eliminate all that code because it contains an infinite loop and that is UB so it canâ€™t be reached.
Please put the code in a gist... 
Yep: https://github.com/tomassedovic/dose-response Though it's my first actual game and first large-ish Rust project so the code's not excellent.
Thanks! Good catch, the links should be fixed now.
Thanks! I actually had it there originally and removed it because it didn't seem to change anything and I forgot what it was for. Thanks for reminding me.
That is an interesting question. On the one hand, it should be the fastest single threaded associative map. On the other hand, people see its complexity as a liability (hard to maintain). Moreover, concurrency matters these days and I have seen people and companies opt for very simple yet concurrency friendly DSs like SkipList. ART is not concurrent, but orders of magnitude simpler :).
I started by adding a few new features and speeding up [cargo-contribute](https://github.com/Xion/cargo-contribute). Right now, for example, it will rarely have to talk to crates.io anymore and instead use local Cargo cache for dependency metadata. And since this second Cargo subcommand of mine has gotten such a good reception, I will probably start working on my third and final idea for a subcommand. Well, after I make sure /u/burkadurka doesn't know of any prior art, that is ;-)
IMO, Rust makes it harder to implement complex data structures because of its overly restrictive safety pre-cautions, than other languages, for example C++. https://news.ycombinator.com/item?id=13670366
Strange things are always hard when you don't know how to do them. Learning to program in a language that is very different from your current languages will always be a challenge. Rust is very different from Python and C#, and so it's hard to learn because first you must unravel your expectations. However, simply because a language is mainstream doesn't mean that it's good or bad. This is true of so many other things as well of course, but particularly with programming languages. They are somewhat a tool for work, but largely a tool for expression, and so people don't pick them for purely engineering reasons, but also for artistic reasons. I'm not sure that most mainstream languages _are_ as usable and learnable as you might think. I think that they seem that way because we've got millions of programmers around the world who throw themselves into the programming machine and at that scale eventually _something_ useful will come out. Look at it from the other direction though: we have all these books and languages are design patterns and best practices guidelines, and code is so very often bad despite all that. It's not fully the fault of the language, but I wouldn't let the language off the hook at all.
It's much faster, it's parallel, it's easier to install and use (don't need Ruby, don't need devkit to build extensions on Linux), it's more secure (now we can use a good licensing system instead of RubyEncoder). So I think it's worth it. The only problem is that APIs are not so clean as in Ruby, but it's my first Rust product and I've tried to find out how fast could it be.
I thought there were quite a few tiling WMs for wayland already?
Great writeup and good job working through all the small issues. One (very) minor bug report on the game: The text in the upper left corner should probably read "Shift+Left".
I think there are enough obscure computer things out there that naming collisions are forgivable.
Conversely, Rust also makes it a lot easier than C++ to include a library where somebody has already built and verified that complex data structure.
Thanks! The source code is here, see the calculate function - https://github.com/tomcraven/mandelbrot-wasm/blob/master/src/lib.rs Let me know if you have any questions and I'll try to help
or we could include random, lazy_static, and friends in the standard library like we do with environment variables, multi threading, and collections. Luckily we've started heading in the right direction with the inclusion of time back into std.
Dope! Thank you.
Have you tried: Event::Input(Input::Button(Button::Keyboard(Key::A))) =&gt; { ... } Or, if you `use` the names you need, you might be able to get it down to: Input(Button(Keyboard(Key::A))) =&gt; { ... }
You can save some space with `if let`, at the very least, assuming you have other branches.
You may try [actix](https://github.com/actix/actix-web), it has middlewares and it is faster than iron or rocket
[removed]
Technical language tends to be more precise than ordinary human language, and that precision is important. The problem isnâ€™t that I canâ€™t understand people saying transpiler (of course I can, I just replace it mentally with compiler). Itâ€™s that it is very imprecise but supposedly technical, and the term is used to somehow separate classes of real world compilers arbitrarily. Compiling to C is quite common, and not really meaningfully different from compiling to assembly. Separating the two because â€œhumans write C codeâ€ is weird because humans definitely donâ€™t write C that looks like that. Or take something like Idris which has compiler backends for a bunch of human-written languages. The output is human-readable, but itâ€™s again not something any human would ever write in any of those languages.
Okay, *that* makes sense.
I did something similar a time ago with similar deep comparisons: https://github.com/aspera-non-spernit/orbclient_window_shortcuts/tree/testing/src
Meh. Datastructures in Rust are annoying to do -- you need to write unsafe code and it's not as ergonomic as plain C++. The Rust philosophy is that you write these once and then use them as a library, which is kinda counter to what's going on in ICPC which focuses on writing these datastructures among other things.
I believe the [if_chain] crate was developed for exactly this use case. In clippy and other compiler-like tools you tend to work with highly nested data structures (ASTs) and being able to narrow down on exactly one pattern with a chain of if-let expressions is super useful. [if_chain]: https://docs.rs/if_chain
I wrote a slightly-ranty tutorial for Iron a while back: https://wiki.alopex.li/ActuallyUsingIronAgain It's now a couple versions out of date but I played with it a bit more recently and the basic structure didn't seem to have changed much. https://wiki.alopex.li/AnOpinionatedGuideToRustWebServers might also be useful to you.
As far as I have seen, beginners' questions are perfectly fine here. To answer the literal question from the title: You can use the [`file!`](https://doc.rust-lang.org/std/macro.file.html) macro to get the path to the current source file. !However!: This is mostly intended for debug output. Rust code is compiled to a binary file and when you distribute or install that, you don't want it to refer back to the source code. (Which might have been deleted after installing the binary, or, if the binary is distributed, resides on a completely different computer. Maybe you want to embed the file in your binary? There is a pair of macros for that: [`include_bytes!`](https://doc.rust-lang.org/std/macro.include_bytes.html) and [`include_str!`](https://doc.rust-lang.org/std/macro.include_str.html) If you need a `File` like `Read` object instead of a raw bytes slice or str, you could use `include_bytes!` together with [`Cursor`](https://doc.rust-lang.org/std/io/struct.Cursor.html), maybe a bit like this: `let important = Cursor::new(include_bytes!("important"));`
This is me advertising my own project, but there's also `ggez` where you just do: fn key_down_event(&amp;mut self, ctx: &amp;mut Context, keycode: Keycode, _keymod: Mod, _repeat: bool) { match keycode { Keycode::A =&gt; { println!("aaaa"); } }
Thank you for the reply. As far as I understand there are two basic project structures in Rust - library and executable. For the later there should not be such cases like you mentioned. But for libraries how do you include, say some configuration files, for each modules?
https://github.com/rust-lang/rust/issues/43122 and https://github.com/rust-lang/rust/labels/A-generators
Well, how often do you write a linked list from scratch when coding complex algorithms?
you probably want std::env::current_exe https://doc.rust-lang.org/1.12.1/std/env/fn.current_exe.html edit: like exoticorn said though, important would need to be distributed with the executable or use include_bytes
I don't know about your question, but joinmastodon.org is a better information website about Mastodon than mastodon.social.
 let go = |e| { let i = match e { Event::Input(i) =&gt; i, _ =&gt; return } ... println!("aaa") } 
So saved. Gotta remember to look at this again when I'm next time doing something Rusty!
Great, we need more visualization tools for Rust :) Btw, are you planning to support candlestick charts (market prices)? Or can you recommend a crate for plotting candlestick charts?
&gt; Or, if you use the names you need, you might be able to get it down to I try to avoid using standalone names (without `::`) because if you typo one of them it becomes a catch-all variable assignment instead.
Uploaded my [picshuffle](https://github.com/digikata/picshuffle) utility to GitHub. It works w/ some bugs in the optional exif date reading that I have to track down. But I will likely clean that up, make the code a bit more canonical, and then play with parallelization.
I donâ€™t know of anything for rust at the moment which supports candlestick charts, but Gust should be getting support for them soon :)
Maybe you can give a slighly more detailed example of what you are trying to do? Are you talking about writing a library that can be configured with an external file but is also bringing its own config file for the default values? I haven't seen that pattern being used in rust libraries, but I know it from the java/scala world. If I'd were to implement that, I'd build a config reader that can load and merge multiple config files and use include_bytes! to read from the embedded config.
Isnâ€™t there a lint/warning about using CamelCase match variable names?
The only fully functional I know of is Sway which is a i3 clone. And I like to have a script/program for configuration like XMonad and Awesome have, because that gives you more possibilities.
It sounds like your main knowledge gap might be what the relative path is for rust/cargo. With python the script is likely colocated with your important file. With rust, you've included it with the source. Assuming you need it at runtime and not compile time this does little for you. Getting you closer to your python solution would be running `cargo build` and then navigating to your target/.../... directory. Copy your important file to that directory and run `./myrustprog` Now I don't know cargo well enough, but it looks like you could use build scripts (build.rs) to copy that file from your source structure to the target directory. This would allow you to go back to using `cargo run` to build and run in one step.
you are absolutely right. For the past couple of hours I have been skimming through github trending rust projects and literally not a single one had any file other than .rs in src module folders. I've always used this kind of structures in python codes quite frequently. But here the approach is different.
Yes, what your looking for is something like https://github.com/Pirh/config_struct or even the config crate might work for you.
The author makes some incorrect claims about the Rust and the C here, specifically that things are heap allocated. I haven't dug in to figure out the details of why we're slower here though.
wow that is quite nice. I learned so much from one "simple" hiccup!
This post is wonderful, thank you so much for writing it up.
There's some Rust users on Mastadon, but I'm not sure they're all in one place.
I, for one, am glad we've left these crates out of the standard library so far. They've recently been going through a bunch of API evolution that would be impossible for something in std, and lazy_static may be much less needed once we have `const fn` support in stable.
Is the use of another function in the match adding to the loss in speed? I don't actually know how that reduces behind the scenes.
No mention of the C compiler used or the compiler flags, which is a pain. I'm honestly more interested in C being so much faster.
Obviously wrong microbenchmark, knives out everyone.
I felt the same way, they support two big things that satisfied my requirements: - fairings, basically these limited but global middleware you can put on everything ( logging, etc.) - route gaurds, basically middleware hook for things like authentication - they bonus have a lot of support for turning forms, POST body, etc into an easily verified and usable form, they call these data gaurds Outside of that I didn't see much other support for middleware, but at the time I couldn't think of much other middleware I needed than auth, data, or logging.
I'm not sure why a function is introduced there to perform the equivalent of `i &amp; 1`, and why signed integers are used.
What `match` reduces to depends on the features you use; sometimes it's an if/else chain, sometimes it's a jump table, it just depends.
Naw; I think it's interesting to look into cases like this.
Especially since the function does `((n % 2) + 2) % 2` instead of C's `n % 2`.
Replacing it with the C equivalent 'if' statement changes nothing in the benchmark for me.
I sadly don't have a machine around to run benchmarks on, but even if it doesn't change the result, it does make the rust program look unnecessary complex.
LLVM will probably inline it away.
It's not really usable yet, though. Trying to compile EdgeDNS with it causes an ICE (`unresolved type in dtorck`).
Would you mind filling an issue at https://github.com/rust-lang/rust/issues ?
Looks to me like the use of signed integers is inhibiting some optimizations. [See code/assembly here](https://godbolt.org/g/FxFY3J) Looks like with unsigned values LLVM is able to eliminate the match branch, instead inserting a conditional move. My gut says that the reason it generates worse code is because it's generating code for `modular` which is also correct for signed values of b, which means it can't just reduce some of the operations to bitshifts as shifting negative values would cause issues. 
Rust: https://godbolt.org/g/Yo8Zfp C (with clang): https://godbolt.org/g/fLsryt 
Also you might replace if with some expression along the lines: `(i&amp;1) * (3*i+1) + (1 - (i&amp;1)) * (i / 2)`
Thanks for the input, can't believe I didn't notice that. As for strings, they are just implicitly cast to OsStrings?
I haven't look into the on this particular issue but in the past I've seen issues with lack of hinting causing LLVM to not understand the range of some values leading to suboptimal code. The fix was relatively straight forward of adding the extra hunting to the LLVM IR.
That's a legitimate optimization, but not really fair for this benchmark unless you do the same contortion in the other languages.
If you're talking about this bit of assembly (lines 16 to 20): mov rcx, rdi shr rcx, 63 add rcx, rdi sar rcx mov rdi, rcx That's actually the divide by 2 in the match arm. The `modular` function is getting inlined and optimised to this: test dil, 1 jne .LBB0_3 Which is exactly the same as `i%2`. It should be noted that according to the docs, the `sar` instruction maintains the sign bit, so all it should need to do instead of that mess is `sar rcx`, and if you replace the `i / 2
Would you consider trying actix for your guide?
&gt; rand: didn't compile -- this is a problem. We depend heavily on rand You can work around this by using rand with no std. My understanding of the problem with using features from stdlib is that rust tries to link against libc functions, which aren't there. I worked around it by using rand with `default-features = false` and seeding the value manually from `window.crypto.getRandomValues`. You can see me doing this in [wasm-asteroids](https://github.com/m1el/wasm-asteroids).
The author doesnâ€™t initialize the `l` variable in the C code. Also nothing in the C code allocates on the heap... Iâ€™m very confused by what the author is claiming is going on
Wait, how does the C program get away with not initializing l?
Shouldn't this be reported as a bug to the LLVM team?
Yeah the author is lucky that the right answer is even coming out
`i / 2` rounds towards 0, but `sar` rounds towards negative infinity, ergo the longer instruction sequence. The fact it's pointless because of the modulo check doesn't seem to be visible; using `n = (n &amp; ~1) / 2;` causes better codegen, though it breaks CMOV. (Hilariously, writing __builtin_assume(n == (n &amp; ~1)); n = (n &amp; ~1) / 2; makes the code generated bad again.) 
I looked at the Intel [instruction reference](https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf)(page 1234), instead of just the Godbolt docs. According to the Intel docs, while it is a signed division by 2, it's not the same as `idiv`. Specifically: &gt;Using the SAR instruction to perform a division operation does not produce the same result as the IDIV instruction. The quotient from the IDIV instruction is rounded toward zero, whereas the â€œquotientâ€ of the SAR instruction is rounded toward negative infinity. This difference is apparent only for negative numbers. The Godbolt docs only quote up to the paragraph *before* that detail. That would explain why LLVM is adding the sign bit to the number before calling `sar`; it's accounting for that and not a bug.
There's a discord server
Yes, but see [this comment](https://www.reddit.com/r/rust/comments/7m99wo/outperforming_rust_with_functional_programming/drseoky/) - in this particular case, LLVM could have ignored that detail, because it is irrelevant in this context. That's the bug. 
Ooh... Please, tell me more!! Links? 
Here's a quick overview: 1) As near as I can tell, the statement that &gt; we do something that is not possible to do in Rust or C - we safely stack-allocate the intermediate value, eliminating all heap allocations is false. Everything I see in Rust and C is stack allocated as well. 2) "l" is not initialized in the C code. int collatz_c(int n) { int l; while (n != 1) { if (n % 2 == 0) n = n / 2; else n = 3 * n + 1; l++; } return l; } The author is relying on undefined behavior for this program to work correctly. This is unlikely to explain the difference in performance since it's outside of the loop, but it does demonstrate how Rust helps to prevent risky practices. I'm a little surprised that it works at all. If anything I would hope that a variable would get initialized to 0. This looks to me like the sort of thing that could turn into a nightmare debugging project if it was integrated into a larger system that did additional calculations based on this function. 3) This to me makes this an apples to oranges comparison as far as Rust/C to ATS is concerned: &gt; The implementation in ATS is notably less straightforward... &gt; *completely different algorithm using multiple loops and what appears to be a lambda function* Without knowing the language, I can't say whether this is the way you'd idiomatically solve this particular problem with ATS. But for this to be an effective comparison of whether the languages rather than the algorithms, you'd need to write the same (functional) version of the algorithm in Rust and then benchmark it against the ATS implementation. Can anyone transliterate the algorithm used in ATS for generating the Collatz sequence into Rust or C(++) and see if they're still slower?
The author is not familiar with C.
Nitpick: It would be Event::Input(Input::Button(_InputButtonVariantType_ { button: Button::Keyboard(Key::A), .. })) =&gt; { ... }