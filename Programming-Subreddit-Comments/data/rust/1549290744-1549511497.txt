I'm not very experienced with macros... I have something like the attribute above the struct, e.g. ``` #[jupiter( url = "https://sheets.googleapis.com/v4/spreadsheets/{spreadsheet_id}/values/{range}:append", method = "POST", ty = "self::append::AppendValuesResponse" )] ``` The attribute-like proc macro receive it as `TokenStream` (and it is only the attribute, without the rest of the tokens). I was trying with deriving `FromMeta`: ``` #[derive(Debug, Default, FromMeta)] struct Opts { pub foo: String, #[darling(default)] pub bar: u32 } #[proc_macro_attribute] pub fn baz(attr: TokenStream, input: TokenStream) -&gt; TokenStream { let attr = syn::parse_macro_input!(attr as syn::AttributeArgs); let opts = Opts::from_list(&amp;attr).expect("Meta"); } ``` and for this: ``` #[baz(foo = "FOO")] fn qux() {} ``` it works fine. Anyway, for this: ``` #[baz(foo = "FOO", bar = 42)] fn qux() {} ``` It says ```Error { kind: UnexpectedType("other"), locations: ["x"], span: Some(#0 bytes(121..122)) }``` 
I'm using the `unsafe_textures` feature (naughty, I know, but the lifetime semantics were killing me initially, it's a choice I want to roll back). Thanks for the advice re: `&amp;Vec&lt;T&gt;`. The pattern is taken from one of the SDL2 examples, but its also very similar to how I handle things in C++ so I adapt it slightly. I'm going to take your advice and go back to the drawing board and just keep things simple until I find my feet a bit more with Rust. I'm sticking to SDL2 for now though because learning too many new things at once will kill any progress and it "just works" everywhere I want to build this for now. All I really need to achieve with my abstractions at this stage is hiding the SDL2 dependency from the main game code. 
\&gt; The idea of splitting formatters as actions is intended to allow others to pick and choose How about a \`.gitformatters\` (or another name) configuration file in the root? Another detail of the formatter I linked is that what formatters to run are controlled by attributes so that files can easily be ignored or included if they'd otherwise be included or excluded by the default tool behavior. \&gt; Coming back to cherry-picks, if the fix had been done as a PR with a single commit, and the formatters created following commits, it seems to be possible to pick the change with git cherry-pick -m 1. That may not be the first idea when selecting the commit that fixed the issue when using cherry-pick. Not all changes are that simple :( .
I don't know D so I can't speak for it. If they did something good with variadics, then kuddo to them!
Well.. forget it... Right after posting this I've changed `bar = 42` to `bar = "42"` and... it works fine V( o _ o )V
I want to take an enum and a list of operators and find a way to represent a logical expression (as a syntax tree?). Like with a `derive` simiilar to `Debug` would be awesome to automagically generate tokens that represent the different values of the enum with the name itself. pub enum AnimalQuality { Biped, Canine, Leopard, Feathers, } enum Op { // this is an oversimplification, operands should really be something like Expr And(AnimalQuality, AnimalQuality), Or(AnimalQuality, AnimalQuality), Not(AnimalQuality), } Bonus if I can somehow nest `enum` types. I want to make an AST from strings like "`Feathers &amp;&amp; Biped`" (EDIT: arbitrarily complex nesting, not as simple as presented here). They don't have to be infix operators if that makes it harder to implement, they just seem easier to read. I am kinda a newb and I don't quite know how to represent an AST but it would probably be lots of `Rc`'s of `Animal` and `Op`? Does `nom` or `pest` seem like a good library for this use case? Ultimately I want to convert the AST into a predicate or a set of predicates that could be evaluated but I'm hoping that will fall into place without much difficulty once I have the AST.
Well, that is one thing I had to fight - darling is very picky about how it parses attributes. And the error messages aren't the best (though they aren't bad). I did end up implementing some custom `FromMeta` stuff for some added flexibility there.
?
Excuse my ignorance but I'm really certain what interactive means. The command will accept some input and return some output. So yes, I'm basically looking to test the return of the app for some input. And that return should be deterministic.
The readme sounds very serious and ambitious, however the name itself is very toyish, not-so-serious. Cool project otherwise:)
&gt; The problem is described in the background section. That seems to be describing low-level implementation details rather than a concrete problem. E.g. "more efficient than traits" isn't a good problem statement (what if traits are already as efficient as possible?). It looks like your link predates "impl Trait" which I think resolves the performance concerns, at least in theory (e.g. the compiler *could* do the C++ trick of "the child vtable is also the parent vtable"; whether it actually does I don't know, but if not then it could be implemented as a compiler optimization without requiring a language change). 
Can't you just store the raw pointer?
Trying to clean up and add features to a [frontend WASM framework](https://github.com/David-OConnor/seed)
If you want your code to run on a raspberry pi or similar, then rust for sure. Haskell takes a lot of resources for compiling on arm, and cross compiling is trickier. Last time I used haskell arm cross compile wasn't working but that may be fixed by now.
My ad blocker is blocking them (uBlock: Origin) on Firefox. I guess it blocks links to localhost from appearing? but I'm not sure why it would block that image too.
We aren't going to touch raspberry pi or it's brother's and sisters :p
Nice! I will refactor some of my code with `darling`, then. I was writing some dirty parsing code that will be gone :P
The problem with all these methods is that you can no longer run \`cargo test\` to run tests or am I missing something?
Web development is a broad field, but some observations from Rust: - There are no Rust server frameworks that are as robust and batteries-included as Django or Rails. There are several nice Flask-like equivalents, but these will require writing your own code, or tieing in addons for things like ORM, auth, admin, email etc. This is applicable for building websites, but may not be for your use case. - Rust is a flexible language that supports many programming styles. Haskell is purely functional, which you might find nice, or too-restrictive.
No because there is no way to extract the vtable and put it next to the object (or somewhere else).
I had in mind that this worked ‚Äúper project‚Äù, and not for multiple independent projects. But I‚Äôll have to check myself!
I think you're totally right. I remember them putting us on a 2.0 beta and it being terrible. I'm glad to hear they fixed it. We need better CI systems. 
Wow! Glad you found it helpful.
Thanks! I agree with your comment about the toyish name.. I should think of a better name.
This post is about how poorly code manipulating pointers to stack values performs when compiled to Wasm. This is extremely important for a language that heavily leans on stack pointers such as Rust. I would wager that Rust programs have more pointers-to-stack-values than code in almost any other language.
&gt; You can also use as to cast to ints. Yes, but you can't cast an int to a simple enum. Being able to say `7 as EnumType` is kind of a thing if you're hoping to use enums as a representation of bit values in low-level code. There are crates for a lot of this. I haven't looked at them in a long time, and should review them again. The ones I tried last were kind of awkward and heavyweight, and I just gave up and represented numbers as numbers and used hand-built symbolic constants. Having said all that this was /u/8igg7e5's complaint, not mine. I can live with Rust's current enum state; it's just more awkward than the corresponding C or Java sometimes.
Working on [RFC2235](https://github.com/rust-lang/rfcs/blob/master/text/2235-libc-struct-traits.md). Things are looking pretty good for that implementation, but the question of enforcement has come up. I'm now looking into [adding lints for missing trait implementations](https://github.com/rust-lang/libc/pull/1217), but it's an open question whether these lints belong in Rust or Clippy. I'd like to merge the RFC changes and sort out the answer to that question this week.
The systems programming thing is a misnomer, and a marketing mistake in my view. In practice, Rust is about as high-level as Swift Haskell has complexities of its own: types vs kinds; String vs Data.Text; eager vs lazy; and the constant worry about text encoding in libraries , which is haphazard. Rust performance is also much better. I‚Äôd say it‚Äôd be easier to get a Hello world working with Haskell and Servant, but you‚Äôll find it easier to finish a product with Rust and either Rocket or Actix. I‚Äôd read https://www.arewewebyet.org and check out the sample code for the Texhempower benchmarks I can‚Äôt speak to GraphQL. 
You might also consider Go. It has its own set of issues, but has some nice support for webapps. For me the determinative factor would be long-term maintenance. As somebody who has written a reasonable amount of Haskell code and played with the web frameworks, I have found that it's easy to write a ball of Haskell that's hard to work with and maintain. Skilled Haskell developers are probably harder to find than skilled Rust or Go developers at this point, and developers of ordinary skill can have a hard time with Haskell's fancy-type-centric approach and inferred program flow. Fixing laziness performance bugs in the face of higher-order monadic types is its own thing: I know literally dozens of people who can do it. Michael Snoyman (/u/snoyberg), founder of FP Complete and author of one of the most well-developed Haskell web frameworks (Yesod), seems to be doing a lot of Rust now. You might ask him for an opinion.
I'm the maintainer of \`darling\` - sorry that you're having some difficulty with error messages. I'm actively working on this, starting with moving errors to point to the right locations in your source code so you know which attribute is causing problems. If you run into any sharp edges, please go ahead and file issues in the GH repo and I'll look at them as soon as I can. 
Unfortunately, I don't think \`bar = 42\` was valid Rust back in 1.18; I use \`syn::Meta\` to do the parsing of source code for me, so I'm beholden to what it accepts. I'll check if that's changed, and if not then I'll add an example on how to collect numbers.
&gt; Is it valid design decision in general? Perhaps not. Maybe your queue consumer should be a separate application that calls _this_ application via an endpoint? Also, why `rabbitmq`‚Äîyou might that using Google's PubSub/AWS' SQS/Azure's Service Bus to be less costly and simpler to operate.
I'm not sure why it's deciding the location of that error is "x"; that seems like a bug on my side.
No you are not missing something. `cargo test` doesn't support that. What I do is create a function that calls a `Once` to initialize everything and call that function before every single test. It may be a hassle but it's what we got for now (you can create a proc_macro to abstract that call for you, but it probably wouldn't help much).
Just some thoughts about why writing a book made me a better programmer and got me deeper into Rust. Feel free to ask any questions :) 
Take a look at http://rocket.rs
On Clippy lints, you can use `#[allow(Cippy::clippy-lint-name)]` to ignore a lint. This also works for normal warnings. 
Irreducible control flow is not necessarily inherently more difficult to optimise, V8 just doesn‚Äôt use an arbitrary CFG as its internal representation. All other compilers immediately convert the control flow into an arbitrary CFG without reducibility information.
&gt; In practice, Rust is about as high-level as Swift There is still room to improvement regarding memory management productivity. In Swift, most of the time one only has to worry about breaking cycles. They have very explicitly stated that the ownership improvements introduced in Swift 4 and being improved for Swift 5 are always going to favour ergonomics. Then again, Swift won't ever significantly grow outside Apple platforms.
Thank you! I will check them out.
Sorting by multiple things is easily done with a tuple. Your hypothetical example code: foos.sort_by(|v| v.max_possible) .then_by(|v| v.bounds()) .then_by(|v| v.bounds.distance_from(origin)) .sort(); can actually be written as foos.sort_by(|v| (v.max_possible, v.bounds(), v.bounds.distance_from(origin))
Welcome new rustacean.
I've been working on [thin_main_loop](https://github.com/diwic/thin_main_loop), which is a cross-platform main loop abstraction, which could form the base for native GUI applications (to compare with Mio/Tokio, which was designed for highly scalable servers). It's still quite early days for the library but I have a good feeling about it, if it can gain some traction and interest from people other than myself.
Deref would work if you want the ItemList to be indexed by usize, but if you want it to only be indexed by ItemIndex I think you need to manually implement the indexing trait.
I'm working on porting my ray tracer [lucis](https://github.com/shaunbennett/lucis) to compile to Web Assembly. The goal is to have a text editor on a webpage where a scene can be modelled in JavaScript (instead of lua in lucis) and then have the scene rendered right in the browser. Hopefully it will be a fun toy to play around with hierarchical modelling and raytracing. I have a shell so far at [lucis-web](https://github.com/shaunbennett/lucis-web), I need to cut the lua and multithreading out of lucis so I can compile to wasm, then work on the javascript bindings
Breaking cycles in Swift becomes pretty complex in the case of closures: the difference between the unowned(self) and weak(self) annotations requires a lot of mental work
Regarding text parsing, there's the `scan!` macro in [serde\_scan](https://docs.rs/serde_scan/0.3.2/serde_scan/macro.scan.html): let line = "#1 @ 555,891: 18x12"; let parsed = scan!("#{} @ {},{}: {}x{}" &lt;- line)?;
&gt;The standard library provides a max-heap. I regularly needed a min-heap. I got one by making a custom struct with custom compare function. &gt; &gt;This was a bit of a pain. It could have been easier. Priority queues with customizable compare functions seems like a useful thing that would exist. I'm probably doing it wrong? `std::cmp::Reverse` is a newtype that reverses the sorting order, so e.g. `BinaryHeap&lt;Reverse&lt;i32&gt;&gt;` will effectively be a min-heap of `i32`. Having said that, it would indeed be nice to be able to have a heap with a custom comparison function, and iirc there are plans for that.
We have both Haskell and Rust backends and I love them both. Rust uses a bit less resources and is quite fast though working on nightly because of rocket has some pain points. Our backends aren‚Äôt extremely complex and the code doesn‚Äôt feel too hard to manage since the compiler is usually helpful. Haskell can take some getting used to but since we use Elm on the front end it isn‚Äôt as hard as long as we don‚Äôt go to crazy into the powerful features the language/compiler has. We use Yesod as a web framework as it has as close of a batteries include feel but also you can just get rid of what you don‚Äôt want to use. The framework is just packaged that play well together. I had expected the Haskell code to not perform super well since it‚Äôs so high level but so far it hasn‚Äôt been bad, but most of our code is more IO bound then CPU bound. Personally I would pick Haskell to do a project in but that‚Äôs only because I‚Äôm familiar with it. Honestly I think y‚Äôall just need to do a small project in both and then decide because who knows what would work best in your specific case. HTH
Oooo someone else recommended std::cmp::Reverse but it wasn‚Äôt clear to me how it was used. Seeing it wrap the type and now I get it. Interesting! That should *totally* be part of the example for BinaryHeap documentation.
I would actually use JavaScript components. Rust and JavaScript integrate well, even in syntax flow. And wasm can be used for better memory management of JavaScript components and events. I feel like frameworks exist for that. Something bare-bones would be an interesting rust GUI project. Like it or not, JS is the standard for GUI after all these years
Btw. do you think that integer on this position should be acceptable? I see that at least `rocket` attributes accept integers.
Something like electron? I want a minimal and lightweight approach without struggling with javascript or other web techs.
I've written web APIs using rust for personal projects and for school assignments using both Rocket and Warp. I can't comment on Haskell, as I haven't used it. 1. As someone who has had prior experience in imperative languages, but acknowledges that functional style has its uses, Rust is ideal. Using Warp, most of the code I write is functional, but the ability to drop into imperative style aids in clarity in some places. 2. Libraries are still WIP, with very few 1.0.0 stable dependencies to choose from. That said, there are a lot of frameworks to choose from (Rocket, Actix-web, Warp, Tower-web, etc..). There is Diesel for ORM stuff, and other libraries for communicating with databases. Async is largely absent from the ecosystem at the moment, which some consider to be a problem (this is getting better quickly though). 3. I find with Diesel + Warp or Rocket, its exceptionally easy and fast to set up and query db tables and then set up endpoints to attach to those queries. 4. There was a significant ramp time to becoming productive with these libraries. Diesel and Warp produce obtuse error messages (Diesel mostly) if you do anything that doesn't fit into their type arrangement. A lot of these web frameworks don't come with all of the batteries included, so if you want to use JWT authentication, you will probably end up stitching other libraries together to make that work with your chosen framework. It takes a while to get everything you need in place, but once you get set up, productivity is pretty good. 5. All well-used libraries tend to have ample API documentation, many have examples directories that show off common use cases, and a few like Diesel and Rocket have dedicated guides that explain how to use their library. I usually can figure out what to do by looking at examples + docs for a few minutes. 6. Build times are terrible. Anecdotally, a school project with Diesel and Warp takes about 6 seconds to build about 3000 lines of code that supports about 15 endpoints. This usually isn't a problem, with utilities like cargo check allowing verification if your current code will compile, and the general cargo ecosystem making it easy to split your app into multiple crates to bring down incremental build times. 7. I had fun learning how to do this in Rust. I think it took longer, but offers better results than .Net, Spring, or ember.js. &amp;#x200B; 1. Rust brings overhead in the form of the developer having to understand its borrow system. Coming exclusively from JS, that may initially be a problem, but once you understand the borrow checker and learn to leverage the compiler's strictness to force yourself to write "better" code, that overhead mostly disappears. 2. JS is closer syntactically to Rust than it is to Haskell, and I would hazard a guess that its easier to pick Rust up than Haskell coming from JS. But this is about what you and your company want. Node, Rust, and Haskell will all allow you to accomplish your task. But other requirements come into play: will this need to be supported after you leave the company? Is there someone else on your team willing to learn Rust or Haskell? Would the system have to be rewritten or reverse engineered back to Node.js once you leave if it needs further development? The safe bet would be to continue using what the rest of the system/team uses. If your team is Ok with you using an alternative language/ecosystem, I would heavily weigh the input of your team members when deciding. If someone else wanted to learn or already knows Haskell, I would choose that over Rust. 3. I haven't used it, but [https://github.com/graphql-rust/juniper](https://github.com/graphql-rust/juniper) seems to be the graphql library with the most support for Rust.
Oh yes, I will say 2.0 had some teething problems, and required you to adjust how you thought a little.
The first standard version was C++98. Or 15 years after its "birth".
Yes I agree. Was thinking of a crate that efficiently translates the various JavaScript object parameters into a GUI rendering. It‚Äôs a huge project if you go for completeness (the JavaScript standard is enormous), but something bare-bones would be achievable. perhaps copying segments from open-source nodejs (written in c++). 
Sorry, I thought question was about integers. Most of data I feed to gpu is made of `f32`
Some big boy input i see
&gt; When Rust ships custom build profiles I'd love to see a MaxSpeed config. RipGrep doesn't ship binaries with LTO. I think the lack of custom profiles is causing a lot of shipped Rust code to leave free performance on the table. I'm the one who added the [lto option to Cargo](https://github.com/rust-lang/cargo/commit/74bd3c3be3b93e20970f228289c814b9a518065d), so for ripgrep at least, the lack of LTO is not born out of ignorance. :-) LTO increases compile times significantly, last time I checked. And I could hardly observe much performance difference in common tasks anyway. Of course, it's been a while since I checked, so perhaps it has changed. But compile times are already too long. I could cause the release process to patch the `Cargo.toml` file to enable LTO only there, but I'm generally not a fan of making the release configuration different from the configuration that I use personally, because dogfooding. (This isn't a hard line stance. I could be convinced otherwise, but I'd want something compelling in exchange.)
I've actually started taking a look at rust for web backend development recently and warp seemed pretty cool. You mention using Diesel with it, but since warp is built on tokio, aren't you blocking the event loop by using it? As I understand the best way would be to use an async driver for db queries (like tokio-postgres) but there are no nice ORM abstractions for that :(
...and after you've done this, you'll need to set up a source map. The build location can be seen in the header of disassembly dump. To find out local path use this command: echo `rustc --print sysroot`/lib/rustlib/src/rust/src Not sure it'll help your debugging much, as libstd is built in release mode.
You can‚Äôt, in general. Slices are contiguous, whereas the results of your filtering are not. So, you can `.collect()` your iterator into a `Vec` of string references.
Can you fill an issue about the BinaryHeap docs?
Don‚Äôt you have to cfg_attr on ‚Äúcargo-clippy‚Äù?
Ah okay, that makes sense. Is there a trick to doing that? I tried here: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=313f790a0f56822d57ee9a80280ae37d And ran into a bit of a trouble, as the compiler complains about moving. I would have thought perhaps lifetimes needed specifying, but the error doesn't seem to imply that. Thoughts?
&gt; I had expected the Haskell code to not perform super well since it‚Äôs so high level but so far it hasn‚Äôt been bad, but most of our code is more IO bound then CPU bound. Haskell is actually relatively fast, if you need to squeeze every cycle out of your code, you probably want to go with Rust or something comparable, but Haskell code will often run within an order of magnitude for C in compute bound applications, and in IO bound applications can achieve nearly the same performance with far less unsafety.
FTFY: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e1747be4b810fd62a2ae9327208def79
Still working on https://www.github.com/KenSuenobu/rust-pushrod ... trying to figure out a good way to genericize the `Widget` library. I saw how Conrod did it, but that seems waaaaaaay too complicated for my taste. I'm trying to keep things simple. Any suggestions would be great. :)
&gt; the lack of LTO is not born out of ignorance I know. My RipGrep link was purposefully to a discussion thread on it to help make that clear. &gt; I'm generally not a fan of making the release configuration different from the configuration that I use personally, because dogfooding I flip-flop on this. It's a good argument. It also leads to the majority of projects not using LTO and similar options. It comes down to how much a few percentage of points is worth and how much it costs. I'd be curious to know how many LLVM projects turn off LTO after it causes problems (other than compile times).
All your seven pointers point to Python and/or PHP and/or JavaScript. Rust is a lower-level language, Haskell has non-web roots. PHP, Python, JavaScript, all have excellent facilities for creating web applications and have immense ecosystems supporting this task. Rust and Haskell would be quite overkill in my opinion, unless you do really work with immense data and humongous calculations relatively often. Then again you could use FFI to call Rust code from Python for instance. If you are looking to learn Rust or Haskell alongside creating this application, then by all means try out both, they're great and will look good in your CV in the future if you manage to learn them well. Just make sure your company is ready to invest in your learning expenses and that you get a blessing to make mistakes along the way. But as I said, some tools are better for certain jobs, and regular web apps are better made with PHP or Python in my opinion. Have you considered who will be maintaining the application after you leave your company?
Interesting, thank you! Thought they coerced automatically when the type was specified. Need to look at those coercion rules it seems. Thanks!
TIL about serde_scan. This is really cool for small ad-hoc programs. Thank you! 
I know that there was a talk on porting Redox OS to ARM, but it's not listed here.
I don't know l, I haven't. But then again, I always run clippy instead of carco check.
Done. [https://github.com/rust-lang/book/issues/1799](https://github.com/rust-lang/book/issues/1799)
I have a vector of numbers. Each number is range of [50, 125]. I want to convert this vec&lt;u32&gt; to vec&lt;char&gt; by taking the corresponding ascii value (like A -&gt; 65 .. and so on). and then also concat all of this vectors to form a string. It is mainly a solution to projecteuler.net #59. Here is my initial approach at https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=357fac1941ba531fe9b6700dc1c876cb i am getting error ``` only `u8` can be cast as `char`, not `&amp;std::vec::Vec&lt;u8&gt;` ``` but in this case it compiles successfully https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e06597112d48fc49e0134d661ef48d9e
That's going to eagerly evaluate the last two criteria.
I would recommend starting with the official Rust Book: https://doc.rust-lang.org/book/ It doesn't assume any more knowledge than you will have from python :)
Yes; see [issue #52](https://github.com/TedDriggs/darling/issues/52) filed this morning on that point.
You can implement Mul for references :) impl&lt;'a&gt; Mul for &amp;'a MyFoo { ... }
Basically yeah, but it can be mitigated by using a pool, so you don't immediately end up in a non-concurrent situation. As I understand it, you will starve your connection pool if you get too many requests requiring db access. This isn't immediately a problem, as there is a timeout period where the pool will still try to get you a connection if it becomes available. So it still takes a pretty high sustained load before it starts throwing errors, but it becomes a problem as your queries take longer or if your traffic has spikes. Async is on the way for Diesel, abit [slowly](https://github.com/diesel-rs/diesel/issues/399#issuecomment-457313516). For school projects that I'm using it for, it isn't even close to being a problem, but I would hesitate to use it in production without stress testing it to see where the pool begins to starve.
First time I've heard of serde_scan, that is awesome.
Not a guide but you could look into existing libraries like [libui](https://github.com/andlabs/libui) (written in C) or [wisegui](https://github.com/yupferris/wisegui) (Rust). Latter one is much simpler so I would suggest starting there.
IMO projects should not have LTO enabled if they don't at least have a few benchmarks in the code base. If enabled, the measured gains should be documented, either in README or straight in Cargo.toml next to the LTO switch. Justification should include environment info and Rust versions used for ulterior comparison.
Thank you so much. 
Still no love at the generic level. lib.rs 85 44 error E0369 binary operation `*` cannot be applied to type `&amp;S` (rust-cargo) lib.rs 85 44 info E0369 an implementation of `std::ops::Mul` might be missing for `&amp;S` (rust-cargo) The problem is that this is a generic library, for which an implementation of the `&amp;S` type will be provided later; there's not supposed to be an implementation for these yet, so how do I tell the compiler it's okay that there be an *abstract* implementation of Mul for references?
Search for ‚Äúinto rust‚Äù, it‚Äôs a collection of videos which explains the basics of how the language works If you‚Äôre not an experienced developer, learning rust might not be the best use of your time if you want to get a job, as it‚Äôs very different than other languages. So for that reason c/c++ are good as writing code in those languages is more similar to other languages like Java or even python/ruby But if ur looking to have a hobby and learn some cool stuff, or you‚Äôre experienced in other languages, Rust is perfect!
Experiences vary, but for me typically a copy&amp;paste is a smell for architecture problems, lack of good abstractions and poor code structure, and not as much problem of copy&amp;paste itself. On the other hand I've seen plenty of cases where people invent amazing things, just to avoid having two lines that look the same in two different places, obscuring code readability, splitting logic into multiple ad-hoc functions / classes that don't serve any purpose other than satisfying DRY zeal of the developer. In the comment I was responding to: having `println!` repeated 4 times, locally does not affect anything in the bigger picture. Depending on performance requirements having this `println` redundant might be OK or not. Using `Cow&lt;'static, str&gt;` just like in https://chrismorgan.info/blog/rust-fizzbuzz.html is the best way (because of the nicer functional-programming abstractions and avoiding allocation), but overall there's nothing to split a hair over here. So it's IMO a matter of scale. If people copy&amp;paste long sections of code it's probably a problem, but trying to shave off 4 words is probably not worth it for it's own sake.
I am not really looking to get a job that requires c/c++ i am trying to learn as a hobby so I can build stuff. But like i said I have worked with other languages like python
Is there a good way to see what‚Äôs changed in the rust book since the dead tree edition? I‚Äôve noticed a couple of small errors in my print copy that are fixed in master, and I‚Äôd like to find a list of all of the errors fixed since publication because I‚Äôm sure I‚Äôm not catching everything. Thanks!
Nothing like that exists, but you could check out conrod and azul
Maybe requiring `where S: Mul + Borrow` will help with this?
Was in Another room: https://fosdem.org/2019/schedule/event/microkernel_written_in_rust/ 
Grr, need to reply with the correct button ;) This was in another room: https://fosdem.org/2019/schedule/event/microkernel_written_in_rust/
Thank you. I now use `hashbrown` which reduced the time to a 1/3. Plus another optimization on my side, the total time is now down from almost 30 mins to &lt; 2,5 mins.
I think you are running iter on the Result, instead of the Vec. What you should do is unwrap the Result first.
Thanks. I now use `hashbrown` which reduced the time to a 1/3. Plus another optimization on my side, the total time is now down from almost 30 mins to &lt; 2,5 mins. I wish there was some HashMap/HashSet benchmarking game to give people some indication what is out there.
Not \*really\*, but you could diff the \`nostarch\` directories from previous releases. &amp;#x200B; The most major change is the modules chapter. Moving the macros chapter from an appendix to be part of a chapter is the second biggest change.
It should have asked you if you wanted for it to add it to your PATH for you. If it's working, then you're good.
Nice! I think different implementations are going to perform better under different assumptions (lots of reads vs lots of writes, big keys vs small keys, random input vs pathological/adversarial input, etc), so it might be hard to capture all that in a simple set of metrics. But yeah it would be cool to read :)
Nice. The Rust language has rules in place that don‚Äôt allow dangling references, but the online Rust Book doesn‚Äôt do us the same courtesy üòÇ
Another person chiming in to say I love when people shout out small useful crates with stuff like this. Hadn't seen that before but I like it a lot.
Docs.rs does a really good job of highlighting when you're looking at something that isn't the latest version, and then linking to latest. I wonder if it would be a lot of work for The Book or other official docs to do something similar.
I created a [browser extension](https://github.com/srishanbhattarai/trpl-redirect) to somewhat intelligently navigate to the right page on the new book. You‚Äôre welcome to try it out until the Rust folks come up with a fix! 
Shameless plug: Might wanna try this [extension](https://github.com/srishanbhattarai/trpl-redirect). Works half decent for me. 
This week the [voladdress](https://github.com/rust-console/voladdress) crate came out, the best crate available for handling volatile access. I'll add a bit more soon in a 0.2.2 update, but otherwise it's pretty much stable and done until the nightly feature it depends on (marker trait bound in const fn) becomes stable (hopefully later this year).
Shameless plug: created a small [browser extension](https://github.com/srishanbhattarai/trpl-redirect) to fix this annoyance until it‚Äôs solved by the Rust folks. 
I've been thinking about this a little. You could have a struct that contains the original type and a few closures, but that has a memory usage cost. You could have a macro that makes a newtype struct, and then makes an Ord instance for it but I'm not sure what it'd actually look like to call - you'd have to convert your collection elements into the wrapper somehow. Maybe just a foos.sort_by(&amp;[|v| v.foo1(), |v| v.foo2()]) would work?
Please stop using electron.
&gt; `foos.sort_by(&amp;[|v| v.foo1(), |v| v.foo2()])` I guess not, since `foo1` and `foo2` would need to have the same type (i.e. return the same key types).
Just tell the compiler exactly what you want: `where &amp;S: Mul`. If that doesn't work you need to post more code so people can help.
Yeah, sadly they never advertised it was essentially a proof of concept. Check their bug tracker if you don't believe me.
Good point. foos.sort_by(&amp;[|(v1, v2)| v1.foo1().cmp(v2.foo1()), |(v1, v2)| v1.foo2().cmp(v2.foo2())]) would work, but now we're looking at unpalatable amounts of boilerplate. What about something like haskell's [Data.Function.on](https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Function.html#v:on) so it'd look like foos.sort_by(&amp;[on(|v| v.foo1()), on(|v| v.foo2())]) Where `on` is something like pub fn on&lt;A, O: Ord&gt;(f: impl Fn(A) -&gt; O, a1: &amp;A, a2: &amp;A) -&gt; Ordering { f(a1).ord(f(a2)) }
Umm, the "proper" way to do it is: foos.sort_by(|a, b| { a.foo1() .cmp(&amp;b.foo1()) .then_with(|| a.foo2().cmp(&amp;b.foo2())) }) (not tested)
Nice text! I‚Äôm also considering to write a book, but in my mother language. Portuguese speakers dev communities lack of good readings about programming, specially those targetting embedded systems.
It is still easier than dealing with lifetime annotations, specially in structures and internal mutable fields, or trying to avoid having Rc&lt;RefCell&lt;T&gt;&gt; until the compiler is happy. Also doing classical CS datastructures in Swift is relatively easy.
I ultimately wimped out and abandoned the use of std::ops entirely, instead declaring that I would have my own implementation of a Semiring, and provide my own definitions for the four major operations (zero, one, mul, and add). It worked well, performance is where I expected it (yay for zero cost abstractions), and it's actually nice and easy to read. I'm starting to think the initial pointers I had to "implementing [Algebras We Love](https://kubuszok.com/2018/algebras-we-love/) in Rust" were leading me in the wrong direction for practical implementations.
Thanks Steve! You and carol knocked it out of the park with this one btw, I‚Äôve been telling people for weeks that I think this is the best technical writing I‚Äôve ever read. Good stuff on every page!
How about using [`Ordering::then_with`](https://doc.rust-lang.org/std/cmp/enum.Ordering.html#method.then_with)? Like this: foos.sort_by(|va, vb| va.max_possible .cmp(&amp;vb.max_possible) .then_with(|| va.bounds().cmp(&amp;vb.bound())) .then_with(|| va.bounds.distance_from(origin) .cmp(vb.bounds.distance_from(origin))))
Could someone ELI5 what this means (for a noob) in this example?
Ah, gotya. I did not click on the link. Thanka for clarifying!
Thank you! &lt;3
With the newer `cargo` changes (increasing the number of CGUs and enabling incremental compilation), some apps, especially small benchmarks took a significant performance hit. It kinda' makes the language look bad in tests. Of course, you might argue that's not a large problem, but I feel this is going to be the new "did you build it with `--release`? It also makes benchmarking changes harder and less deterministic. So a build profile which avoids these surprises would be really nice.
If `v1.max_possible &lt; v2.max_possible` then you don't need to look at the other properties. The if-else solution (as well as `then_with` solutions mentioned in the comments) in the article only call `v.bounds()` when the two `max_possible` fields are equal. The tuple solution _always_ calculates `v.bounds()`, and `v.bounds.distance_from(origin)` even when they're not needed for the result.
I just tried out this library. It's pretty nice. I don't love how you need to use nightly though. It makes it a hard sell at big companies.
There's also the Tantivy talk from the Search track. https://fosdem.org/2019/schedule/event/deepdive_tantivy/
u8/usize. Working with raw bytes or indices.
I don‚Äôt think `impl &lt;S&gt; Mul for &amp;S` is allowed due to the orphan rule.
Define "from scratch". How scratch are we talking?
There is an offline mode that allows you to use Cargo without a network connection: https://doc.rust-lang.org/nightly/cargo/reference/unstable.html#offline-mode AKA "airplane mode" since you often don't have a network connection on an airplane. It is currently only available on nightly, and at some point we want to stabilize it. But I think it still has some rough edges.
Not a real ELI5, but I can try it if the explanation below isn't enough. `sort_by_key` takes a function and uses it to compare pairs of the vector. The OP wants to compare elements by multiple criteria, as in "sort by name, and if the names are equal, by age". The solution above builds a pair (tuple) of `(name, age)` and uses it as a key, because tuple comparison does exactly what you want. But imagine you're not comparing some attributes of the values, but something that takes a lot of time to compute, like say the rating of a restaurant and the driving distance to it. You want to get the restaurants with the better rating first. But if the ratings are distinct, there's no need to compute the driving distance. So `restaurants.sort_by_key(|r| (r.rating(), r.driving_distance())) will not only compute the distances even if they aren't needed, but do it on each pair that gets compared (N * log(N) comparisons on average). Of course, in this case you'd rather want to cache the keys, so it's still fast even if all ratings are equal, but that's another story (`sort_by_cached_key`, which is still unstable). --- Sorry, I'm not sure that was a good explanation. :-(
Indeed, as the language grows we'll need localized content since there are a lot of people who don't feel comfortable enough with their English. Go for it :) 
It's fascinating to see so many attempts at bringing Redux to Rust - I too have [my own version of it](https://github.com/brunocodutra/reducer). I had a look at the source code and it's very intuitive, feels almost like a direct translation from Typescript, well done! One thing that seems very limiting to me are stateless Subscribers - there's not much one can do with those besides `println!`. The approach I ended up taking was to denote subscribes through a [trait](https://github.com/brunocodutra/reducer/blob/master/src/reactor.rs#L58), which allows cool things such as providing built-in implementations for containers of types that also implement that trait, like tuples, slices, smart-pointers. This helps a lot with composability. The same kind of composability benefits reducers by having them implement a similar [trait](https://github.com/brunocodutra/reducer/blob/master/src/reducer.rs). In fact _combining reducers_ becomes a trivial `(reducer1, reducer2, reducer3)`, entirely obviating the need for a macro. 
I've taken a quick stab at refactoring to make it a bit more data-driven and come to this: use crate::math::Rectangle; pub struct Renderer { // ... } pub enum RenderCommand { Clear { r: u8, g: u8, b: u8, a: u8, }, // ... } impl Renderer { pub fn new(window: sdl2::video::Window) -&gt; Renderer { // ... } pub fn process_commands(&amp;mut self, commands: &amp;[RenderCommand]) { for command in commands.iter() { match *command { RenderCommand::Clear { r, g, b, a } =&gt; { self.canvas .set_draw_color(sdl2::pixels::Color::RGBA(r, g, b, a)); self.canvas.clear(); } // ... } } self.canvas.present(); } } This is definately better, but I'm still not sure it helps with the texture creation issue. As in `main.rs` I have the following: let mut renderer = Renderer::new(window); // ... let render_commands = game.draw(); renderer.process_commands(&amp;render_commands); Since both `process_commands` and `create_texture` require `&amp;mut self` I still don't have a way of passing the renderer around for texture loading without wrapping it in a `RefCell` - it could just be that I can't see it rather than it not existing. I appreciate all your advice and help so far /u/RustMeUp - I've refactored a bunch of other stuff today and I'm starting to feel the benefits of it now.
Coming from C#, ownership and borrow checking were why I wanted to learn Rust. Memory safety without garbage collection, concurrency without data races, and abstraction without overhead sounded intriguing. Immutability by default, first-class enums, and no null pointers were also big selling points. I like that the compiler makes you think about what you're doing with your data. It's strict, but it catches a lot of the stupid shit I try.
Continuing my contemplation of `Any` (read: staring at the relevant page in the docs), I've noticed that there's no way to downcast an `Any` to an owned value. The best you can do is to `swap` or `clone` it (or its contained field). Is there a fundamental reason to disallow it? Would it be possible to implement such a method for an (owned) `Any`? (potentially pointless speculation follows) If it is possible, it would likely lead to very awkward code - or at least the way I see it working, there are probably better ways. The return type of hypothetical `downcast&lt;T&gt;(self)` can't be an `Option&lt;T&gt;` - you'd only get one shot at figuring out the right type, if you get `None` it's over, the `Any` is consumed anyway. So it has to be `Result&lt;T, Self&gt;`. With `downcast_ref` and `downcast_mut` I'd probably go for a number of `if let`s to handle specific cases. With `Result` it, however, gets rather awkward, because reusing the original `Any` is impossible, and handling the cases degenerates into a sequence of `.map_?`s and `.or_else`s which I can't quite envision. The fact that the whole chain will have to boil down to a single success and a single error type further complicates things. So I'm not sure that (even if it's doable) having this is actually worth it. But it seems like a decent conversation material.
&gt; I don't want to go down the ECS route as it's a bit overkill for the simple game I'm making. Especially for simple projects, you won't have time to deal with countless problems resulting from non-data-oriented approach in Rust. You don't have to go with ESC, but at very least I would store data in bigger collections and use their Ids (Indices, etc.) as references, instead of trying OOP approach. OOP always leads to pain eventually, but in rust this eventually is actually "really fast". https://dpc.pw/the-faster-you-unlearn-oop-the-better-for-you-and-your-software 
Not on the newest compiler versions 
Is there an open issue for tracking this? Thank you for creating the extension, but that fact that the extension exists is indicative of a larger problem ‚òπÔ∏è
I'd recommend if let Ok(xor_data) = afterxor { let result: String = xor_data.iter().map(|&amp;x| x as char).collect(); } instead of an `.unwrap()` call
You can't directly own an `Any` value, anymore than you can own an `Add` value directly (vs a type that implements `Add`). What you can do is own a `Box&lt;dyn Any&gt;`. And if you do own a `Box&lt;dyn Any&gt;`, you can try [downcasting to an owned value](https://doc.rust-lang.org/std/boxed/struct.Box.html#impl-2). The reason you probably didn't see it in your reading is that the method is part of `Box`, rather than `Any`.
I sent a pr to clean up over a dozen dead links on the old website and it never got accepted. 
How can I return a &amp;'Vec&lt;char&gt; instead of clonning the vector in the following example: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b9d93d7425b624a981074af8734c48c4](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b9d93d7425b624a981074af8734c48c4)
Unfortunately [docs.rs](https://docs.rs) has the same "issue" in that it sends you to the root of the docs when you click "latest version" or whatever the text reads. Would be lovely to get that fixed, too!
I think it should redirect you to the version you're compiling against instead of the latest one.
You will need to make sure your docker container has certificates installed, so that the library has a list of trusted certs to verify against.
&gt; Doesn't this make it really hard for you to tell where code is coming from? In VS and VS Code you can just F12 to the definition. 
Of _course_ it's on the `Box` page, not `Any`. I did realize that the `Any` trait object has to be boxed in some fashion, but thought for whatever reason that they'd put the `impl`s in the trait's module (where they don't belong, because they are not `Any`s methods now that I think of it). It is kind of amusing to see the return type on the `Box`s `downcast`. Do you happen to know any examples of this in the wild? Curious to see how people deal with checking for multiple possible types.
Clippy is stable now, along with the namespaced lints, so nope.
In the first one what you're doing should actually be written as `fn disp(err: &amp;dyn Error)`. The syntax you used is confusing, it was updated to make it more googleable to what was happening. The short version is that the first using Dynamic Dispatch, which has a performance overhead. You are passing a "Trait Object", and there is a lookup that has to happen at runtime to find the function in your codebase. In my limited experience, you pretty much only want this when you have a collection of types that share a trait, and you have several different types. The second is "Generic" over all structs that use the Error Trait. Which means at compile time it is generated just as if you wrote the function for every struct that implemented that trait. This means there is no performance overhead. You generally want to use the second one.
- compile times. Why does it feel like `cargo build` tries to recompile every dependency? - the module system. When I reference a crate in Cargo.toml I can use any public item anywhere in my code. Why do I need to jump through extra hoops to reference items from my own crate? - RLS is still very immature. Java and C# equivalents allow me to stub out an interface implementation with a click of a button and manage to reason about my code even when it's terribly broken. - some error messages are very helpful, but some are extremely confusing, dropping six-line long generic types on you. - automatic derefs are so common right now that I am surprised when I have to write that asterisk. 
Yep, that was the problem. Didn't even cross my mind that OpenSSL trusts no certificate by default. Thanks a lot, learned something new!
Amazing explanation, thanks. Just a couple things. What is the purpose of the dyn keyword? It compiled fine without it. And also, why do I have to pass a reference to it (&amp;dyn Error) vs just a (dyn Error)? Thanks again.
Amazing explanation. Thanks.
The dyn keyword was added for Rust 2018 edition. Basically the language designers recognized that the old syntax was confusing, and now they want everyone to use the dyn keyword to actually show what is happening. The having to use reference Trait Objects have no known size, and things passed 'by value' are passed on the stack, which has to have a known size. A reference is a pointer with a known size, so it can be passed. A lot of this is outlined in the book: https://doc.rust-lang.org/book/ch17-02-trait-objects.html https://doc.rust-lang.org/book/ch19-04-advanced-types.html#dynamically-sized-types-and-the-sized-trait And just in case you didn't see my edit: EDIT: And for the record, in case your code isn't just used as an example: ToString is implemented on everything that implements display. So you should just do err.to_string() if that's what you want. 
Might get that lisp checked out, and i'm not talking about the language.
Yeah I saw the edit, it‚Äôs just an example. Thanks for the trait object explanation
Got my Clippy lint [merged](https://github.com/rust-lang/rust-clippy/commit/6ce78d1257ac6fd77f245730fcfbadd536a173eb) last week; going to spend this week putting it to use scratching the itch I made it for. Also going to write a lot of async code (mostly using Actix, but some manual futures) as part of the same project and hopefully try out Yew!
Its nice to see these kinds of libraries pop up. I am a bit confused about your choice to implement the integration as traits on the primitive types, though. Have you considered doing something like this: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3e4876224537e2c647844883362cb726](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3e4876224537e2c647844883362cb726) &amp;#x200B;
Idk, I wouldn't call ~6 recent memory/segfault bugs, actively being worked on, a powderkeg of memory corruption and segfaults, or something to avoid at all costs. Bugs exist, they're being fixed. [Rust itself has more and worse](https://github.com/rust-lang/rust/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A%22I-unsound+%F0%9F%92%A5%22) than that, should we avoid Rust as a powderkeg of memory corruption and segfaults? [my pet peeve in particular is empty loops crashing safe code but also being widely used in examples and documentation especially for embedded](https://github.com/rust-lang/rust/issues/28728) that and [repr(packed) being UB and segfault-ey to use despite being stable](https://github.com/rust-lang/rust/issues/27060) [float to int UB for casts is fun too](https://github.com/rust-lang/rust/issues/10184) [extern scope not existing and causing UB is interesting](https://github.com/rust-lang/rust/issues/46188) [as a windows user this sounds fun](https://github.com/rust-lang/rust/issues/35836)
Hmm, I worked on my rust project on the plane with no network just fine. The only things I can think wouldn‚Äôt work are the install and publish steps off the top of my head. And you obviously need dependencies to build. So what good is the airplane mode?
For blocking code, you can use [tokio_threadpool::blocking](https://docs.rs/tokio-threadpool/0.1.11/tokio_threadpool/fn.blocking.html) to offload to another thread quite easily.
Ahhh this makes sense, thank you!
Cool stuff as usual. The fact that the demo run in the browser thanks to WebAssebmly is neat. By the way, it is not difficult to make the FEM volume hang. You just have to grab the soft "noodle" and try to make it across a rail. Usually, it got stuck and the whole thing is unresponsive. I guess the physic engine is stuck in a loop trying to the get the "noodle" out of the rail.
Oh nice, didn't know you could make the library search paths arch-specific like that. We use \`cargo lipo\` at Embark as well and that was one of the main use cases. Also do use \`cargo lipo --xcode-integ\` for the build script to get the right config in debug/release but guess that could be set up with a branch in the shell script itself.
I hear this a lot. We don't sell user data. We have never and will never plan to sell user data. In addition to being very well funded via VC, we also make money from a freemium model: https://discordapp.com/nitro Additionally, we are also launching a game distribution service: https://discordapp.com/store Also, you know, our privacy policy literally states we don't sell user data: &gt;The Company is not in the business of selling your information. We consider this information to be a vital part of our relationship with you. (There is more after that though - that I suggest you read. We do share information in certain situations - but it's either in aggregate, or with integrations you specifically opt into via our API/SDK [oauth2 scopes, etc...]). But of course, people still say we sell data. Meh :(
Nice work S√©bastien! I see that you‚Äôre planning on expanding continuous collision detection support. Awesome! If I may toot my horn slightly, I‚Äôd recommend checking out my own rust crate MGF if you need a reference for some of the trickier algorithms. To my knowledge it includes the only correct exact implementation of moving capsule/capsule and moving capsule/polygon collisions online, and additionally all of the reference material I have found (Such as Real Time Collision Detection) omits such algorithms, and some of the descriptions for continuous collision algos are wrong (such as sphere/poly algo in RTCD). Just a thought, I know first hand how hard it is to find references on these algorithms and I‚Äôd be happy to allow unlimited use of the code in MGF. 
Awesome!
i wonder if there's a user script version
Well, it must be at least as easy (since it's a subset), and some backends would struggle to handle it, so it seems fair to call it strictly more difficult. I'll accept that most compilers won't mind, though. On the other hand, it does introduce inefficiencies, so getting to optimal code is probably harder.
You can use `cargo docs`.
Indeed but that's unrelated to the live docs.rs site functionality.
The old website had no clear ownership. For the current one, things are actually merged quite fast, so we would be glad about the contribution if you would try again :).
saw a thread about it and it's installed now! curious to see how it works out!
- How would a browser extension know? - What happens if you have multiple Rust versions installed? - Book versions don't line up with Rust versions. There's no "Rust 1.15" book to go to if you use that version (afaik). How would you map Rust versions to book editions?
You may use `rustup doc` for that.
No problem! I'll take a look at the texture loading issue tomorrow.
Normally I'd tell you where to find what you're looking for, but this is super sketchy and like 90% a scam preying on children so you can just fuck right off. Hope you get your account banned from Reddit.
Using https://briansmith.org/rustdoc/ring/signature/index.html#signing-and-verifying-with-ed25519 as a guide: ``` let pkcs8_bytes = signature::Ed25519KeyPair::generate_pkcs8(&amp;rng)?; let pkcs8_bytes: &amp;[u8] = pkcs8_bytes.as_ref(); ``` Then write the `pkcs8_bytes` to a file.
**That would've saved me a lot of time. But at least now I know :)** 
Are you using the wrong buffer variable name? mut_buffer
what do you mean? 
what I shared is pretty much the entire code so i cant be using a wrong variable name because theres no other variable named buffer
Why is it best to use usize for indices? The book mentions it's a good idea but not why.
The problem is how can i implement later the complex numbers? ```Self``` for the types helped me more than ```Self``` for the functions, i can literally put: ```Inte::numint(T, T, &amp;Fn)``` and ```Inte::numint(Complex&lt;T&gt;, Complex&lt;T&gt;, &amp;Fn)```
Ah I see. And that's eagerly evaluated because it's a tuple?
That was an amazing explanation. Thank you so much! Totally understand now =)
FYI, the link to Part 3 from Part 2 seems to be broken.
&gt;The second is "Generic" over all structs that use the Error Trait. Which means at compile time it is generated just as if you wrote the function for every struct that implemented that trait. Only for the types that are actually passed to that function somewhere in your program, I assume?
I've always assumed it's because usize is always required to be positive and has sufficient size to index all possible blocks of memory.
Still working on [Eko](https://github.com/ravernkoh/eko), a simple scripting language written in Rust. I‚Äôm in the middle of reworking the interpreter to use bytecode instead of walking the tree.
there are definitly parts missing. 
I don't like it.
Let me know if you need any help -- sou fluente na portugu√™s! :)
this is the entire function ![](https://i.imgur.com/M36X5V5.png)
It's a current limitation of the Iterator API: you can't return references to the struct. https://stackoverflow.com/questions/30422177/how-do-i-write-an-iterator-that-returns-references-to-itself The TL;DR is that usually you can do let a = iter.next(); let b = iter.next(); But in your example this would not compile, since `b` cannot be created because `self.result_buffer` is already borrowed by `a`.
If I understand correctly, the error you are getting is `error[E0716]: temporary value dropped while borrowed` at the line where you define `str_val`? If so, the problem you are having is that `trim_end()` borrows the `String` to return a slice (`&amp;str`), that is a reference to part of the `String` you create. However, the `String` you create is only used for `trim_end()` and is only temporary, it's not even bound to any name, so as per RAII, the compiler tries to get rid of it immediately. It can't however, because you want to keep `str_val`, the reference to part of the `String` in question. To fix this, you can turn the `&amp;str` into an owned `String` using `(...).trim_end().to_string();`. You can also bind your temporary `String` so that it can outlive the reference to it: let string_val = String::from_utf8_lossy(&amp;buffer[..num_read]); let trimmed_val = string_val.trim_end(); // use trimmed_val 
`pub fn from_utf8_lossy(v: &amp;'a [u8]) -&gt; Cow&lt;'a, str&gt;` returns a `Cow` because the bytes you provided could already be a valid utf8 sequence and potentially no allocation is required. `let str_val: Cow&lt;'a, str&gt; = .. ;` has a lifetime tied to `buffer` because that is where the bytes came from; That would be fine by itself, but it looks like you are doing this inside of a loop or something that we can't see, and the `str_val` escapes that loop or block. However, buffer is no longer valid outside the loop/block and that part of the stack could be reused for other data, who knows what str_val would be pointing to! Something like this should get it working: `let str_val = String::from_utf8_lossy(&amp;buffer[..num_read]).into_owned().trim_end();` 
unfortunately, as soon as I pass the `strval` into `serde_json::from_str()` it gives me the same error
I can't really follow what you are trying to do but: &amp;#x200B; fn handle_connection(conn: &amp;mut UnixStream) -&gt; Result&lt;String, std::io::Error&gt; { let mut buffer = [1024]; conn.read(&amp;mut buffer)?; // quit fn if error Ok(String::from_utf8_lossy(&amp;buffer).trim_end().to_string()) // trim and return string } fn main() { let mut stream = UnixStream::connect("/home/genom").unwrap(); match handle_connection(&amp;mut stream) { Ok(res) =&gt; println!("{}", res) , Err(e) =&gt; println!("{}", e) } } Something like this should work. Do the serde stuff outside.
\&gt; The nest best abstraction layer that i would consider technically correct is [mio](https://github.com/carllerche/mio). &amp;#x200B; A typo: The next best
This statement strikes me as far too general. There are projects where you know that - Your iteration test cycle is far greater than the amount of time spent compiling (with or without lto). - Tiny performance increases in hot loops can have large impacts on final runtime - Your code will be run many more times than it will be compiled For projects like those you might as well turn on LTO, turn off the extra codegen units/incremental compilation/anything else that might impeed optimizations, and keep going. Benchmarking is only useful when it might tell you *not* to do something. LTO is very unlikely to hurt runtime, so you might as well turn it on by "default" for some projects. Maybe benchmark later if you become suspicious it's inlining things it shouldn't.
You could use from_variants and an enum error in that case.
Take a look at (this playground)[https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=53bd835bce1c57b568f7ec8a1b082036] and see if either fix the issue.
No extension should be needed if the compiler points you to the correct one. There's no book for specific versions, but there is a point where the new book works better. But I've received enough downvotes to see it was probably a bad post and it is too late for me to try to understand why. Good night everyone.
The code is pretty readable, so converting it to a user script wouldn't be too hard, I'd imagine.
Nice, I'll certainly be using this.
Is the \`dyn\` keyword optional? If so, should I use it or not? I've read a little about it (discussion about \`dyn\` seems to involve \`impl\` too?), but I had a hard time getting a quick overview of the situation and why people felt it was worth discussing or debating.
You should have gotten a message from a bot (Dyno it seems) with instructions. The Discord was having some issues earlier in the year with spammers joining, so this is a measure to reduce that. If you didn't get a message, try typing `?talk` in the welcome channel (`#welcome`); according to the mods, that should work as well.
Given that you are interested in `async` without noise, perhaps you should take a look at https://github.com/socketry/async - even though it's Ruby, maybe it gives you some ideas. Additionally, I make a simpler syntax wrapper on top: https://github.com/socketry/async-await - I think it's good for users because it keeps things conceptually simple.
It's entirely optional, but I would recommend using it. To clarify, there is no situation where adding `dyn` or not will make a difference in code, it's purely for clarity. `Box&lt;Trait&gt;` is exactly equivalent to `Box&lt;dyn Trait&gt;`, except the latter makes it clear this is a "trait object" while the former leaves it inferred. When a trait is used somewhere a concrete type is expected (like `Box&lt;Display&gt;` or `&amp;Display` rather than `Box&lt;u32&gt;`/`&amp;u32`), the compiler treats it as a trait object. This, rather confusingly, means the compiler compiles the following two quite differently: fn takes_generics&lt;T: Debug&gt;(x: &amp;T) { ... } fn takes_object(x: &amp;Debug) { ... } These are pretty similar, and if you weren't used to generics in rust you might think they're identical. But the one taking `&amp;Debug` generates one function body and uses a vtable at runtime to find the debug function, while the one taking `&amp;T` where `T: Debug` generates one function per type - "monomorphizing" it. To fix this, the Rust team decided to add in the `dyn Trait` syntax. It's optional, but may become mandatory in a future edition. The above functions are now written as: fn takes_generic&lt;T: Debug&gt;(x: &amp;T) { ... } fn takes_object(x: &amp;dyn Debug) { ... } And there's more clarity. `dyn Debug` tells us for sure that there's a dynamic trait object here. Still not 100% clear without rust knowledge, but at least there is some indication that something different is happening. --- This relates `impl Trait` only because it makes it easier to explain the difference. `impl Trait` has two functions: it acts like generics when used as a function argument, and allows anonymous concrete return types when in return position. The return position functionality is entirely new, while the argument position functionality just makes it nicer to write functions. With impl trait, the above two functions can be written as: fn takes_generic(x: &amp;impl Debug) { ... } fn takes_object(x: &amp;dyn Debug) { ... } This is equivalent to the last snippet, but now the difference `dyn` vs. `impl` is highlighted. This makes it easier for things like the book to teach the difference between monomorphization and dynamic trait objects by having very similar things allowed with the only difference being this keyword. It also allows some nice contrasts between fn returns_anonymous() -&gt; impl Debug { } which is allowed to return exactly one concrete type, and fn returns_object() -&gt; Box&lt;dyn Debug&gt; which can return any number of different types at runtime, but they need to be boxed with vtables. 
Agreed. I have over a decade of Python experience and only a couple of one-semester university courses worth of C++ and I found the book to be a great way to get started with Rust.
I was unable to use the \`.sum()\` method so had to use the \`fold\` variant. Can I use the \`.sum()\` method? The problem is that the vector is of \`u8\` and but the sum is of type \`u32\` \`\`\`rust if let Ok(xor\_data) = afterxor { let text: String = xor\_data.iter().map(|&amp;x| x as char).collect(); let common\_words\_exists = \["in", "the", "and", "to"\].iter().all(|&amp;x| !text.find(x).is\_none()); &amp;#x200B; if common\_words\_exists { println!("{}", text); &amp;#x200B; //let mut result: u32 = xor\_data.iter().sum(); let mut result: u32 = xor\_data.iter().fold(0 as u32, |acc, x| acc + (\*x as u32)); println!("{}", result); &amp;#x200B; } } \`\`\`
It is good that you are providing a solution for embedded devices. However, I found the article off putting. I would suggest spending more time showing us what your library can do instead of criticizing another library.
Brilliant, I can't believe I didn't think of this before. (5 star review)
Thank you! Please also star the repository if you found it useful :) 
I like the idea of this. I'd have to write some code to get real feeling on how ergonomic it is, or not. In one project I used a certain crate that embraced the whole Tokio/Futures thing and I got so annoyed at all the cruft I had to write to use it that I eventually just wrote a synchronous wrapper around it and was much happier. I feel like it leaked way too much C++-template-astronaut-ish complexity into my dumb, straightforward code. And I did use an early version of async/generators and still wasn't content. Now, I haven't really been following the async thing at all recently (my latest project just uses Mio/threads/crossbeam-channel and I'm happy with it), but if its moving as slowly as it seems, perhaps the best (ambiguous I know) abstraction/approach that will make people eager to use/develop it hasn't been chosen. (Don't take offense, I know people work hard on this stuff.)
I tried changing a function into a macro and broke some things. Could anyone point out where things went wrong? pub fn clamp&lt;T: PartialOrd&gt;(val:T, lower:T, upper:T) -&gt; T{ if val &gt; upper {upper} else { if val &lt; lower { lower } else {val}} } macro_rules! clamp(($x:expr, $a:expr, $b:expr) =&gt; (if ($x) &gt; ($b) {$b} else { if ($x) &lt; ($a) { $a } else {$x}})); I feel like there's enough parentheses that each expression would evaluate correctly in order in pretty much any case...
Thank you! 
I did think of this while writing it - that it will no longer be useful once the Rust team fixes all the link references. But, then I thought that I would still be too lazy to click on the ‚ÄúGo to new version‚Äù even if it had the right page so went ahead and did it üòÇ
I agree that Tokio is far more complicated than many people would like, but it's not at all clear from this blog post how Osaka handles multiplexing connection-oriented sockets, which is the main reason most people want to use an async I/O library in the first place. Is it even possible, or is this library only for using with connectionless protocols like UDP? The example in the blog post seems like it would be simpler and cleaner if it just used regular old synchronous I/O, and your DNS implementation (aside from being kind of rough to read due to the pervasive and unnecessary use of `unsafe`) also doesn't support DNS-over-TCP, so I couldn't look there either.
Awesome. Thanks a lot.
Hope you find it useful!
How come `ring` can generate `Ed25519` key pairs and `Ecdsa` key pairs but not `Rsa` key pairs?
The main reason is that lists have a bigger maximum size in 64 bit programs than in 32 bit ones. In 32 bit programs, you can access at most 4GiB of memory addresses, and thus that is a hard cap on your arrays. usize/isize are the types in Rust that do change size depending on the architecture. Specifically, they're _designed_ to be list indices, and not much else. i32/u32/i64/u64 are designed to be used for data. If you aren't using usize or isize for list indices, you'll have two problems: - your program could potentially run into error cases on 64 bits which don't occur on 32 bit programs - you'll have a hard time interoperating with all other rust software which uses usize/isize for indices As for usize vs. isize, usize is recommended because negative indices often don't make sense. It's alright to use isize if you're doing calculations where negative indices would make sense, but other than that usize is more robust.
RSA key generation is much harder than ECC key generation. We removed most of the C code that implements the math needed for RSA key generation when we did a bunch of C to Rust conversion and we haven't had a chance to do the Rust version of the math that's missing yet. 
Looks cool. Having multiple different way to handle async is def important. Rust's success is still way too tethered to tokio at this point in time and things are still too difficult for more mainstream use cases. async/await could change that but the jury is still out. Tokio is an abstraction over the concurrency primitives, yet people find find its api unweildy enough that they want to build abstractions over it. How long until its abstractions on top of abstractions on top of abstractions and performance is out the window? Unfortunately for osaka and other tokio alternatives, tokio is largely embedded into the ecosystem as a whole. Even if you could somehow prove that osaka is objectively better for all use cases, more ergonomic, more performant, or whatever, people will have to give up so much interop between crates to use it. Imo if you want people with to use this outside of embedded you would have to make it another abstraction over tokio so people still have the interop. If you managed to get real adoption(like what something like actix has now) then you could swap out the backend(tokio) from under it. I don't see any other way of pulling it off. Your api does look clean tho. I will def play around with osaka at some point.
I disagree about level triggered sockets. Especially for the generic library use case. Level triggered sockets assume you are always ready to read/write when the socket is ready to read/write. Also level does not work on [windows](https://github.com/carllerche/mio/blob/master/src/sys/windows/mod.rs#L120). 
I appreciate the sentiment that rust should have multiple async implementations besides from tokio, but I think the tone of the blog is a bit inflammatory. There are people at the other end of these projects and using language such as `bikeshedding monster trashfire` does not breed understanding or healthy debate. It's just not a nice way to communicate.
Unfortunately, both are probably overkill, and you‚Äôd probably do more for your company choosing a dime a dozen language like Java or NodeJS. You‚Äôre right that Rust is a systems language. It‚Äôs well suited for building HTTP servers, but it‚Äôs noting special for building HTTP applications, which are quite frankly so popular you can spit in any direction and land on someone‚Äôs web framework that will suit your businesses needs just fine. If you were an experienced Haskell developer you could probably write some types and derive all the code your business needs, in a quarter of the LoC you‚Äôd need in Rust, and it would be as solid as any mathematical proof. But chances are it isn‚Äôt going to be that easy for you, and even if it was it would be incredibly terse, difficult for your coworkers to read and work with, and hard to hire for. It would be about as efficient as the equivalent Java program, and much less efficient than a Rust version. At the end of the day, I don‚Äôt think it sounds like your don‚Äôt anything interesting, and you want to spice it up and make it interesting. That‚Äôs common. But the honest to goodness truth is that you aren‚Äôt going to do anything for this project that‚Äôs going to make stable, boring, ugly Java or JavaScript a worse choice.
The function itself is generic over all types (in that any type that implements the trait *could* be passed into it), but yes, code will only actually be generated for type combination that are actually passed into it somewhere in your program.
Ah, I wasn't sure if it was going to be that kind of reason, or because there was some reason why people shouldn't use RSA anymore.
Yeah, my bad, I meant the generating part in particular.
Not familiar with this project. Why would you ever ‚Äúneed‚Äù to use nightly? Do they not have releases?
I don't understand some of the criticisms. They might be certainly valid for tokio, but not for futures0.3/async_await. E.g. in async/await there is no implicit executor either. It has to be explicitly created and managed. People have been building futures0.3 executors for embedded platforms, e.g. [here](https://www.reddit.com/r/rust/comments/akg42h/simple_alloconly_futures_executor_for_embedded/) and [here](https://github.com/Nemo157/embrio-rs). It's not ideal yet for super resource constrainted platforms because of the need for dynamic memory allocations in the task executor, but osaka seems even worse there because it requires even more `std` types. A huge benefit of the futures 0.3 model (which btw doesn't mean one needs to include futures0.3 in the project) is that actually futures from various sources are **composable**. We can wait at the same time on one future that represents a network operation which uses mio under the hood. And at the same time wait for a timer which gets completed by a timer thread or is even triggered through an ISR handler in an embedded system. We can also build futures that get fulfilled by other IO sources, e.g. through LWIP as a network stack. I don't see that composability in the given osaka examples. It seems to only work on top of a single mio loop and propagates the knowledge about that loop to all subtasks. What might be a valid concern is binary size. All the type composition and monomorphization in Rust certainly leads to more generated code than what a type-erased version or plain threaded code would require. I think that's an area where we would need to get more experience with.
Have a look at how I represented expressions in my AST: [Expression](https://github.com/Lehona/Parsiphae/blob/master/src/types/exp/expression.rs) and (e.g.) [BinaryExpression](https://github.com/Lehona/Parsiphae/blob/master/src/types/exp/binary_expression.rs) They're a bit more complex (just because I have more different types of expressions), but I think you will get the gist of it :)
&gt; Work on discord is wasted time, providing no value to anyone but discord's owners. Right, because all the people who choose to use the service (myself included) couldn't *possibly* be benefiting. And no one who's choice set has been expanded by the existence of a free communication platform has benefited either. Private work exists because of the benefit it provides for the customer. It doesn't do anything for some amorphous "social collective," but it does create value for all the parties involved, otherwise they wouldn't voluntarily agree to the exchange. The profit motive is not a mark of impurity.
Nightly version of the rust compiler, not nighrly rocket.
What would the return type of parsed be? Does it only work with numbers?
Counterpoint: When I read that particular phrase I had a good chuckle and was reminded that if I'm taking the whole thing too seriously I just need a break. I mean it's so over the top - I'm reminded of big-personality YouTubers like Colin Furze and AvE. Salty? Yes. But I mean, there's such a long-running history of salty hacker swagger, that I don't think we *should* throw it all away on the altar of Just Play Nice. Yes, I would agree that it's important to avoid excluding people with too much heat - and that sort of thing *does* lead to verbal abuse easily enough. But at the same time, can't part of the solution be helping folks appreciate a good jab? I mean, it *isn't* a personal attack - the process is a mess and a lot of folks are wearied of it, and probably a lot of folks *are* thinking "wow, what a dumpster fire." I'm not participating, not because people need to be nicer, but because I don't have the ability to form an opinion about async that fires me up. *I don't believe I'll be forced to use it* so it's not a threat to me. I'd actually feel a little bit better if it were a brilliance-measuring contest - some forms of British debate come to mind. If people are legitimately arguing between prefix and postfix notation, that tells me that the more substantial decisions have either come to consensus or the idea-havers have left to code what they care about. This is a good point for me to start brushing up on the design and, again, decide whether to learn it or avoid it.
Note that in *ring* you can load an existing RSA key and generate RSA signatures with it, and you can verify RSA signatures. The part that is missing is generating new keys.
Yes you'll need to render HTML, much like you'd need to render a man page. Both w3m and lynx will allow you to output to stdout quite easily, and given the typical size of an application's documentation they should render fairly quickly.
I made a [`comparator`](https://crates.io/crates/comparator) which could be helpful here.
This site is legit, I‚Äôve gambled on it myseldning, gamblef my skins lost and won. They sponsor nigger YouTubers and i dont think they would risk their carrier iver some fake shit.
Hi! Please, consider contributing your crate to the unic project! We should unify all into work around cldr/Unicode/ICU 
Do your PR literally anywhere else. Why are you pushing all this facebook integration, asking for phone numbers, and refusing to implement e2ee? Why is your desktop app process logging other applications?
Well you can .map to u32s and then .sum, but it should just work for iterators of item u8. Relevant Docs: https://doc.rust-lang.org/std/iter/trait.Sum.html#impl-Sum&lt;u8&gt;
&gt; The puzzle inputs allow solvers to ignore cells which touch the border. I think the puzzle author should have been mean. He should have forced us to find finite areas that cross the input bounds. Can't be done, I think. [Theorem and Proof Sketch](https://www.reddit.com/r/adventofcode/comments/a3kr4r/2018_day_6_solutions/eb7axrw/)
The similarity heuristic that the extension uses can be used relatively simply to find the new link for each of the second edition links. Piping that output through awk, xargs and sed to fix all the links in the markdown files should be simple enough if anyone wants me to do it. (Or do it themselves)
As a new Rustacean I commented recently about the combinator style being totally unreadable, interesting to see I‚Äôm not alone. It just looks like trying to build a less well supported language inside the language.
The subscription method isn't very useful indeed. I will work on a stream based method when I have some time to do it. As for the reducer, defining it as a trait seems like a good idea. Thanks for your feedback!
So, as you may or may not know, GUI isn't a completely solved problem within the Rust world, and there are a lot of different projects with different approaches looking for the best way to solve the problem. You can find several of them on the [Awesome Rust](https://github.com/rust-unofficial/awesome-rust#gui) list, and track the state of the art over at [Are We GUI Yet?(http://areweguiyet.com/). If you are trying to create a GUI system completely from scratch, all you need is a surface you can draw on, and a way to receive input events from peripherals (mouse, keyboard, touchscreen, etc). If you have access to these two things, everything else, more or less, can be built on top through various layers of abstraction. From there, it is up to you to decide how you would like to structure the logic and code, but there are a few common patterns which are implemented almost universally throughout any GUI system. At the core is some form of event loop - not much unlike what you would expect to see in a video game. Except, while a video game's core loop is going to be optimized for throughput, a GUI's core loop will ideally be optimized for power consumption (doing as little work as it can get away with). This event loop will process raw input from the peripherals such as key events, mouse positions, as well as signals from the operating system, and translate them all into messages which the application can respond to. Most GUI toolkits have some concept of a "widget" or "control." These could be anything from a label, to a button, to a slider, to a cat video, to a row, column, or table where you can arrange multiple buttons, sliders, and cat videos. Typically there is a hierarchy of widgets which can optionally be subdivided into other widgets, and this hierarchy is used to define a scalable layout - defining the ultimate size and location of each widget within the surface. This layout is then applied for two purposes. First of all, to determine which widget is the recipient of a mouse even given some arbitrary coordinates - and second - for walking the tree of widgets and telling them to render themselves at a particular location with a particular size when necessary. From that point, there are a lot of different directions you can take things depending on your tastes.
I'm building a GraphQL server in Rust. I haven't used Haskell much, so the other languages I considered were Go and Crystal. Decided against Go because, well, generics and performance primarily. I have used Go for some large-ish projects before and I just don't like the language that much. Crystal is very interesting, but just not production ready. Initially, I didn't want to go with Rust. First time I've used Rust was in 2015 or 2016 or something, when it was still in alpha. Then I didn't use it at all for 3 years, and I unlearned most of it. It is hard to learn, so I strongly disliked the idea of using it again. Well, initially. The obvious pros for Rust were: * Performance. This was a big one, my project is a public API that should be able to handle lots of requests per second. * Macros. In Go, pretty much all GraphQL implementations use codegen, which feels dirty to me. Rust has a great alternative in the form of macros and also compiler plugins. Because there was no better language that could give me these things, I started writing my project in Rust. I had to fight the borrow checker a bit, but I quickly got over that. Definitely worth it in exchange for not having GC. Then, I rediscovered why Rust is great: * Safety. In Rust, you almost never have to debug, because Rust does not allow you to ignore errors. In other languages, you often make little mistakes like "forgot that value X can be null". Rust doesn't have null, so you'd use Option&lt;T&gt; and explicitly check if it has a value every time. Or, if something unexpected happens, you'd explicitly trigger a panic. Most of the code I've written is in JS, which makes it very easy to screw up, so I totally love this about Rust. You'll spend a bit more time writing it, but a lot less time debugging it. * Cargo &amp; the ecosystem. Cargo is the best package manager ever, and the ecosystem is in a pretty good state as well. * Tests in the same file as the code it's testing. Makes it really easy to notice when there aren't tests for a component. I'm using Juniper as my GraphQL library and it's been great so far. I initially wanted something that is schema-based, but now I actually prefer the code-first approach.My only issue with Rust is the build times. My CI build takes 10 minutes, and my project isn't even that big yet. Every change takes about 20-30 seconds to build on my (Skylake dual core) laptop. Cargo check is a bit faster, but still, that's a lot. Is Rust overkill for a project like this? Absolutely not. Rust was simply the best choice for my situation. I didn't choose it because I liked it, quite the opposite, I didn't want to use Rust. But it worked out well. Safety is always important for any networked application, and so is performance.
It is sad that a community so invested in diversity is so obsessed with prescribing language to other people, even though that language is what makes these other people diverse in their own right. Frankly, me as a person who cares about equality and respect, but who was raised understanding that a good quip is nothing to be offended about, feel increasingly afraid to express myself in "my own, natural manner". Why is it that for all sorts of trolling here people will go at lengths to accommodate for the off chance that that person wasn't trolling, while for a perfectly interesting post the highest ranked comment gets worked up and judgemental on the author's humor. *This * is what's making me feel I should exclude myself here. 
Like others, I agree with the sentiment, I‚Äôm glad you wrote something simpler and lighter weight. I will check it out for sure. But I also agree with others that it could be a little more formal and less complainey But again, the content was a good read, thanks!
I don't know the quality of osaka.rs but at least i can read the sample codes and reason about. I'm totally lost with tokio/futures etc. Last month I was helping my friend with his server code using futures, and it was a terrible experience. My favorite async implementations are both golang's and erlang's. Both of them are very easy to reason about. There are only very few simple abstractions you should learn before trying to read a piece of code / flow. I'm not sure if rust will ever have this. I was very excited about rust in the beginning (still I am) but I'm afraid it is going to be another overly complex language / ecosystem.
That's why people are so excited for async/await syntax. I think it really helps prevent abuse of combinators. Overall I do like combinators for iterators and such, although I get annoyed sometimes by the things the people from functional programming languages really like (such as using `Option` as an interator).
Based on the naming, i thought this was a new crate as a part of the `num` family. Pretty misleading imo.
&gt;Why are you pushing all this facebook integration So you can find your facebook friends and suggest them to you as contacts? &gt;asking for phone numbers For anti-spam purposes if our system detects something suspect - and if you want to enable 2fa using SMS authentication. &gt;refusing to implement e2ee? Because e2ee doesn't make much sense in a mass gamer group-chat application? Breaks core product features like search, device sync/persistence, etc... Not that we don't want to, but it's not really justifiable at this point. Also there are plenty of other awesome e2ee messaging apps. &gt;Why is your desktop app process logging other applications? For [game detection](https://support.discordapp.com/hc/en-us/articles/217960107-Games-Detection-101). 
I'm kind of baffled about the 60% binary size reduction he's claiming. Is rustc really that bad at optimizing futures code right now?
Hrm. You know what, I think you're right. If the distances were euclidean then a non-infinite area could extend over the border. However for manhattan distances that's not true. Well dang. Now I feel silly!
No need to feel silly: it took me a day to work it out. :-)
You probably just need to put a bound on your generic parameter. pub fn serialize&lt;T: Serialize&gt;(arg: T) -&gt; Result&lt;()&gt; { &amp;#x200B;
Bad spelling, bad grammar, negative and unpleasant tone. All I see is complaining, sorry, this blog post looks like a simple rant to me.
I don't think the language matters that much, but some when learning system fundamentals, you need to refer to the various C (Posix) api to get a feel for what goes on under the hood. For instance open() close() select() mmap() - pthread , mutexes etc. So that even if you learn Rust, you can see the connection to the underlying OS, and appreciate how Rusts abstractions can make many things safer and simpler.
Eh, look at your sibling comment. There's both kinds of reactions. It's a balance that seems to work fine.
Wrong sub, try /r/playrust
Wrong subreddit.
On the other hand, if you're not multiplexing connection-oriented sockets then there shouldn't be any overhead associated with that if these are "zero cost" abstractions. Anyway, I don't agree with the post 100% but I do agree with the general idea that object code size is an issue that seems to be neglected. More generally, I think if we can look past the tone we can see there's a valuable idea here, namely that these things might scale *up* really nicely but maybe we can improve how they scale *down*. I myself have had people bring up strong concerns about object code size when trying Rust for embedded use cases so I do think it is worth investigating.
There are so many typos and grammatical mistakes in the text that I‚Äôm not sure whether the author really cares.
I searched for something like this yesterday and stackoverflow directed me to the [separator](https://docs.rs/separator/0.4.0/separator/) crate. I ended up not using it because it wasn't all that important and completely undocumented. This looks much nicer :)
To me it seems they both agree that the "dumpster fire" comment was in good humor and the parent comment of "don't use such language" was judgemental.
Hi! I'm the author of that post (and sort of new to reddit). Thanks for the post brian :) Yes, that dns implementation is pretty raw, it doesnt even have A records ;) multiplexing of higher level streams is currently done like this: &amp;#x200B; &gt;\#\[osaka\] &gt; &gt;fn foo(poll: Poll, a: Stream, b: Stream) { &gt; &gt; loop { &gt; &gt;let again = match a.read() { &gt; &gt;Ready =&gt; do something &gt; &gt;Again =&gt; a &gt; &gt;}; &gt; &gt;match b.read() { &gt; &gt;Ready =&gt; do something &gt; &gt;Again =&gt; yield a.merge(b) &gt; &gt;}; &gt; &gt; } &gt; &gt;} &amp;#x200B; &amp;#x200B; since its level triggered, there's no additional complexity needed to ensure all streams are polled. The only disadvantage versus tokio here is that it's a much more explicit style, requiring you to define which resource is polled first and handling non-ready explicitly. That is intentional. It could be abstracted away with a macro (osaka::select or something), but that depends on feedback from others i guess. &amp;#x200B;
Have you tried just letting it go out of scope? Iirc, drop is called automatically...
sorry if that offends you. I'm not good at writing and my style of speech tends to make more sense when you see me on stage laughing ;)
Try using (map\_err)\[[https://doc.rust-lang.org/std/result/enum.Result.html#method.map\_err](https://doc.rust-lang.org/std/result/enum.Result.html#method.map_err)\]. &amp;#x200B; Here's a new (version)\[ https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b3014a1661e84c7ced0a3b27beafbf82 \].
Happy cake day!
I think ideally `Locale` enum should support getting current locale from OS.
&gt;Maybe your queue consumer should be a separate application that calls &gt; &gt;this application via an endpoint? Via REST API call or maybe gRPC? Does it really matter in this case?
Aside from whether *this post* deserves the criticism it did get, your argument here really falls flat for me. "I feel excluded because I can't be professional and empathetic". Forgive me for being harsh, but: yes, that is the point, you are meant to feel excluded. The intention is to exclude people who aren't nice and don't play by the rules. It's an explicitly exclusionary policy. The thing is if we don't explicitly exclude anyone we are implicitly excluding other people, because they're not going to feel comfortable participating, as you seem to be. So the choice is not between "exclude people" vs "don't exclude people", as the latter is simply impossible. The choice is in *which* people to exclude. And personally I vote for excluding the not nice people, rather than excluding the nice people. Think about this in a professional setting. If you are a teacher you can't stand in front of your class and make off-color jokes. If you are presenting something in front of a potential client, you probably can't make your "good quips" either. Certain settings require decorum, they require you to adjust the way you talk and communicate, otherwise you might face consequences. You can't expect to express yourself in your own, natural manner in every conceivable context. This community is no different. Can this whole thing go overboard? Yes. Do we sometimes err on the side of "playing nice" too much? Yes. But we're all people and people have to make hard choices all the time and we're not going to get it right all the time.
Thank you so much.
\&gt;&gt; Unfortunately rust isn‚Äôt focused on embedded Never a truer word spoken! 
Hey this is really cool, I was looking for something exactly like this. The other formatting crate only supports groups of 3 which is disappointing. I'm assuming it also supports asian formatting of groups of 4?
That's not what one necessarily wants. I.e. I have a locale of en-us but I might want to switch between kr and en! Etc.
There is, and I‚Äôve been asking for help. Oh well.
It looks like Reddit ate your formatting. For multi-line codeblocks, prefix each line with four spaces (and leave an empty line before and after the code block), then it'll come out like this: use ws::listen; listen("127.0.0.1:3012", |out| { move |msg| { out.send(msg) } }).unwrap() Apart from that, I'm not sure what you're asking. Could you provide a code snippet where you put, e.g., `&lt;HERE, execute closure held in variable x&gt;` where you want to execute the call?
I've meant that it would be nice to have an associated method for `Locale` enum which will return `Locale` value corresponding to a current OS locale, i.e. programmer will still have to choice how program should behave, should it pull OS locale or use locale pulled from some config, command-line argument, environmental variable, etc.
I case you're interested: I took the approach outlined in this post and created a Docker image that cross-compiles for ARMv6: [https://hub.docker.com/r/mdirkse/rust\_armv6](https://hub.docker.com/r/mdirkse/rust_armv6)
I reproduced this in the plasticity demo too.
I didn't know about Summed Area Table either but I ended up with a similar structure. My structure didn't continue to build up the sums between rows though. Making it continue seems obvious now, like with many good ideas. Still have 7 problems left to solve, so much fun problems!
Channel \`Sender\`s aren't sync.
If I'm understanding you correctly, newpavlov, what you would like to do is currently possible. If you would like to use the system locale, just pass and `Environment` instance (as opposed to a `Locale` instance) to the various formatting methods (e.g. `to_formatted_string` or `write_formatted`). Instantiating an `Environment` instance with its `new` constructor will get you your system locale data. Since all the formatting methods take a reference to a type that implements the `Format` trait, you can pass them either a `Locale` or an `Environment`, both of which implement `Format`. In other words, take a look at the `Environment` struct. I think it does what you want.
&gt; It's so slow that when I want to find out if a change is correct, I can Alt+Tab to a terminal, type cls &amp;&amp; cargo check, wait, read the results, Alt+Tab back to my editor, and RLS still won't be finished. I believe that running cargo check will actually block RLS as they both lock the crate, but don't quote me on that. I see your point though...it is pretty slow.
semanticssemiotics - It currently supports three types of "grouping": "standard" (e.g. 1,000,000), "Indian" (e.g. 10,00,000) and "posix" (e.g. 1000000). That's because those are the only types of "groupings" that show up in either the CLRD database or the locale files I have installed on my computer (macOS). If there are other groupings that exist and I should support, I'd be happy to include them if you can point me in the direction of documentation for them. In any case, since the two source mentioned above (CLDR and my locale files) were the only sources I had to work with when writing the crate, I wasn't aware of any other conventions besides the three that are already included.
And also barely works! Don't recommend putting in the effort right now, I was pissed I went through all of it myself but that's mostly because I like being able to mouse over things to view their types. &amp;#x200B; Not shitting on the dev or anything though. I'm sure it will be good soon and it's an extremely laudable effort.
But it‚Äôs so free and legal 
And it isn‚Äôt fake and shit either.
Also - I guess I should mention that if you want to pull your formatting from a config file or command-line argument, you can use yet another type that can be passed as input to the formatting methods: `CustomFormat`. This is the third built-in type (in addition to `Locale` and `Environment`) that implements the `Format` trait. And as a final, final option (in case none of the three built-in types implementing `Format` work for you), you can always implement `Format` on your own type that does something completely different than these three.
Ah actually I wasn‚Äôt thinking earlier. Korean numbers are still written with groups of 3s, same as western style. However when spoken they actually are grouped in terms of four. I‚Äôve been working on a korean numbers learning app so I had it mixed up in my head. It would be nice to be able to choose it in terms of 4 zeroes but I ended up using a React lib instead (since it‚Äôs a wasm app).
Good idea, thanks. I achieved the one liner by creating a new trait, [custom\_err](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=a1e40ae349f041844467e482d2d722fa). The only downside to this is that I have to implement custom\_err for each error type that I want to propagate. But I can live with it.
&gt; I believe that running cargo check will actually block RLS as they both lock the crate I'm starting `cargo check` *after* RLS has already starting checking the crate. Insofar as I'm aware, if RLS was locking the crate, it would block `cargo check` from running, but it doesn't (or perhaps: it does, but it doesn't say anything, and `cargo check` finishes first *anyway*).
Taking these 3 words out of context is kind of unfair in this situation. &amp;#x200B; In any case I think it perfectly describes the async/await fiasco/issue.
A "custom" grouping as you describe (even if not standardly used anywhere) might be a cool feature. I'll add it to the issues and see if it might be something we can add in the future.
Building a wrapper around SDL2 to make platformers
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/raspberry_pi_zero] [Cross-compile Rust for the Pi Zero (W)](https://www.reddit.com/r/RASPBERRY_PI_ZERO/comments/anczob/crosscompile_rust_for_the_pi_zero_w/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
For real you don‚Äôt think it‚Äôs legit? I‚Äôm not sure but I believe that sarcasm right there, lol. Big youtubers support this site, I do it because you earn 0,2$ from every referral you get. 
I'm imagining him trying to pick up the trap to drop it again :D
Thanks for sharing your efforts! Although I think the cross project already maintains a armv6hf docker image that can be used to build binaries for the RPi Zero: https://github.com/rust-embedded/cross
It's all good, I'm not offended by what was written. I can understand that it can be hard to convey humour especially if english is not your primary language. I'd just know that if that style was used in a professional setting, it'd definitely raise a few eyebrows! I am glad that there are alternatives to tokio that are being created like this library and [fibers](https://crates.io/crates/fibers). It's definitely a better use of the `async/await` syntax when it lands if there are multiple libraries to choose from, rather than just the one.
It's handy to have a helper function in built for the common case, and for some this will be the common case.
Dude, have you ever even taken a look around this subreddit? This subreddit has absolutely nothing to do with the game Rust. It's about the programming language called Rust. But besides that I still have a question anyway: Should I really take gambling advice from someone who cannot even identify the correct subreddit? I say no! ;-)
Japanese (and I think Chinese and Korean) don't use 4-digit grouping with commas. You wouldn't see "11,7764" as a number. You would see it written "11‰∏á7764" where ‰∏á=10,000. It would certainly be useful to convert numbers to that form, or entirely into Chinese numerals (‰∏ÄÂçÅ‰∏Ä‰∏á‰∏ÉÂçÉ‰∏ÉÁôæÂÖ≠ÂçÅÂõõ), but I think that's outside the scope of your library. It's inappropriate to present numbers that way in many contexts, and the standard Western form (e.g. 32,653) is very common online and understood by everyone.
Lol, not sure how I missed that. Thanks for the tip!
&gt; Aside from whether this post deserves the criticism it did get, your argument here really falls flat for me. "I feel excluded because I can't be professional and empathetic". Forgive me for being harsh, but: yes, that is the point, you are meant to feel excluded. The intention is to exclude people who aren't nice and don't play by the rules. It's an explicitly exclusionary policy. I agree, and the exact same point was made in [this clip](https://www.youtube.com/watch?v=dIageYT0Vgg&amp;t=545) where Emily Dunham refers to how Rust's enforcement of a strict code of conduct drives away people who "would try to resolve their differences via a fight rather than via moderation".
I could see that. 
Yes, I've noticed this... Very helpful
Interesting... thank you!! 
Thank you very much :)
Oh no ... What does that mean?
Ohh... you mean bc peeps who are more experienced/ program much more often might choose something not so jarring on the senses..? I could see that :)
We literally made a whole team have a whole effort over the last year for embedded. That‚Äôs basically the highest level of support the project can give, structurally speaking. 
I posted in all Reddit‚Äôs that had anything to do with rust.
Well. This subreddit has absolutely nothing to do with Rust though. Yet you posted here anyways.
In my experience, developer time is generally worth more than machine time. With Rust builds already being slow and LTO doubling that, CI cycles on fast-moving teams can become problematic. For those cases where machine time _is_ valuable, I think it's worth taking the time (ah!) documenting how, when and what is being made to increase performance at the expense of complexity. But of course, this all depends on the project and process.
r/rust in my eyes it says rust. People that play rust are a part of this subreddit.
There are definitely areas that don't work yet: inferring some types, resolving some things from the standard library but those are being worked on (in fact one commit https://github.com/rust-analyzer/rust-analyzer/pull/742 just helped a lot with std support). File bugs!
Haha. I will explain this to you again: This subreddit is for a PROGRAMMING LANGUAGE that is called Rust. This is NOT about the Rust game. 
I have to take some issue with: &gt;And rust clearly is intended for web and desktop apps rather than embedded devices. While the story is not yet complete for embedded, Rust was largely designed originally for just this use case, and is more and more becoming a viable option.
The point is, phrased in the terms that you are using, that sometimes that developer time is more heavily influenced by machine time than compile time. CI cycle times are dominated by runtime costs not compile time costs. Type errors and such that can be caught before runtime aren't affected by LTO and other code gen settings anyways. Documentation is nice and all, but it takes a fair bit of developer time to make.
People that play rust are apart of this group, at least I got automatically subscribed by making my account and then writing something about rust in the registration.
If I understand it right, you have a value (a closure) that implements a trait (`Handler`), a value of a type that implements `Handler`, and you want to call it like a closure. That won't work. But you can look at the `Handler` implementation for closures: impl&lt;F&gt; Handler for F where F: Fn(Message) -&gt; Result&lt;()&gt; { fn on_message(&amp;mut self, msg: Message) -&gt; Result&lt;()&gt; { self(msg) } } And call it with `handler.on_message(msg)`.
I've seen CI build times jump from 10 minutes to 20 minutes because of LTO. Granted, the build produced multiple binaries, hence was link-time heavy. My point remains, LTO being opt-in, having a non-zero cost but possibly near-zero benefit, I think it is good practice to explain why it is being turned on, so that an unknowing future coder doesn't turn it off to speed up the build. I'm not talking master-thesis documentation level, just a few commented lines above the switch. Hell, even a screen scraped `time my_app` LTO vs non LTO would do. 
The reason the iterators (and closures) are preferred is because they're lazy and this is a case where you want that laziness.
lolwut? Now you're just trolling!! ;-)
And I've seen them jump for 5s to 10s... If in the future build times are enough of an issue that someone is thinking about how to speed up the build, they *should* just try and turn it off. Maybe things have changed. Any measurements I make right now aren't going to be relevant anyways.
Yeah, derive macros are probably heavyhanded for this case, but a declarative macro (`macro_rules!`) will most likely help. macro_rules! impl_mytrait { ($struct_type:ty) =&gt; { impl MyTrait for $struct_type { // ... } } } Then you can invoke it once for each struct type and get three impls without typing it 3 times. 
The [combine-language](https://crates.io/crates/combine-language) crate exports a bunch of extra parsers specifically to help parsing programming languages. For this use case you would want to look at [expression_parser](https://docs.rs/combine-language/3.0.1/combine_language/fn.expression_parser.html) which lets you define such a parser from an atomic expression, an operator parser which specifies the associativity and a function which combines the results of the two parsers.
Sounds interesting, but seems the server side is actually a Node.js application. It's only the client core that is Rust unless I'm missing something.
Thanks, it's working like a charm !
This approach is used a lot in the standard library. E.g. [the implementation of `::std::ops::Add`](https://doc.rust-lang.org/src/core/ops/arith.rs.html#102-117).
Aren‚Äôt OS level locales a fucking disaster? 
&gt; Now I'm suspecting that rustc installed with rustup use his own llvm or something like. It does, you can find the compilation instructions in the repository, though I don't know an answer for your specific question. There is a `llvm.assertions` option in `config.toml`, but I'm not sure whether it would help in your case. Sorry for not being able to help you, but your project seems cool. Please post again if you manage to do it.
...I thought that using `alloca` was generally a bad idea anyway and shouldn't be done, since it can sometimes fail with a stack overflow and there's no good way to detect it?
My understanding is that any time you have generic code instantiated with distinct types (such as Future&lt;T&gt; for different T), you need to make a fresh, slightly tweaked, copy of the code behind Future&lt;...&gt;. Countering this is one of the main benefits to using trait objects even when you don't otherwise need to.
Interesting question! 16-bit x86 isn't listed on the [platform support page](https://forge.rust-lang.org/platform-support.html) and afaik no support for 16-bit architectures is intended in rustc. There's some old messages on [the LLVM mailing list](http://lists.llvm.org/pipermail/llvm-dev/2014-January/069344.html) that suggest that LLVM knows how to produce 16-bit x86 code, at least. So I bet this is squarely in the realm of "should be possible maybe but never been tested". ELF executables are not what you want anyway; I don't even know if they support 16-bit-only code. The easy options for DOS exe's would probably be COM files (just a flat binary iirc) or MZ, but I don't know how to make LLVM generate them. Must be a way though, since I believe LLVM supports both. Maybe it's possible to make a 32-bit Rust program and run it under DOS using a memory extender or something...? I have no idea how that sort of thing actually works though.
How would I collect the regex captures from captures_iter into a vector? Collect()? I tried to using For cap in re.captures_ iter() { Cap_vec.push(cap) } But I am not sure of the type for the vector. Thanks for any help.
&gt; I dont know what to do next. Identify the `rustc` case that caused the problem. Ideally: try to reduce it to a minimal representative case. Report it as a bug to the rust team. &gt; should I try to gdb rustc? Yes, that's probably a good idea, in order to find a workaround. Using `gdb` or repeated `gstack` could tell you whether it's just spinning over the same block of code. Sometimes just looking at the name of the function/feature it's using will help identify a workaround.
&gt; Forgive me for being harsh, but: yes, that is the point, you are meant to feel excluded. The intention is to exclude people who aren't nice and don't play by the rules. It's an explicitly exclusionary policy. That's not what's really going though. I think it's a common myth that probably even the mods believe that the Rust community prescribes nice. It doesn't. It prescribes a _certain specific kind_ of nice. This is something I have personally struggled with since pre-Rust 1.0. Many of the things Rust mods consider non-nice are perfectly fine to me and many of the things they say or do are quite not nice from where I'm standing. It seems the Rust leadership/mods kind of consider their notion of 'nice' to be a just and universal one, applicable to anyone from any culture, but that's not the case, their notion of 'nice' is culturally specific and is not at all without its own cultural bias, for better or worse. Perhaps this is beacuse I'm european. If I had to guess I'd say that /u/arvidep is european as well. To me it's really silly (and like the comment above said, sad) that someone would get coloured as "not nice" just because they didn't pick the Right words or picked some Wrong words but overall meant well. Yes, there was a lot of criticism in that post, big whoop. 
I don't think the compiler supports segmented architectures, so a COM file would be better.
This might be a termion question rather than a Rust one but I'm just trying to print a colored box: use termion::color; fn main() { println!("{} \n {}", color::Bg(color::Blue), color::Bg(color::Reset)); } but the background color continues to the end of the line on the second line like: jamie@jdm-pc-mhp:~/test/clr$ cargo run Compiling clr v0.1.0 (/home/jamie/test/clr) Finished dev [unoptimized + debuginfo] target(s) in 0.14s Running `target/debug/clr` ######## ################################################################ (with octothorpes in place of blue spaces). Any nudges in the right direction? :)
&gt;Maybe it's possible to make a 32-bit Rust program and run it under DOS using a memory extender or something...? It is a great idea! It should work. IIRC dos extenders get program in LE (Linear Executable file format) and pack it with some payload code into DOS MZ. It should be possible with linker script to create LE from ELF, or maybe directly link program into LE. I'm afraid that 64Kb per segment might be too little for rust. But if it so, I could try dos-extender. 32-bit segments is enough for everyone.
As for the smarter import you can write `msgs::geometry_msgs::{Point, Quaternion, Pose};`
\&gt; Perhaps this is beacuse I'm european. I'm European too, I really don't think that has anything to do with it. \&gt; It prescribes a *certain specific kind* of nice. What exactly would the alternative be? Prescribing an unspecific kind of nice would be pretty much useless. As you yourself mentioned, what is considered nice varies wildly from culture to culture, and even from person to person. "Just be nice" is a useless set of rules. Yes the notion of nice is specific. It has to be. \&gt; It seems the Rust leadership/mods kind of consider their notion of 'nice' to be a just and universal one \[citation needed\]
&gt; afaik no support for 16-bit architectures is intended in rustc :-O Not at all, or just not x86? I mean, what about microcontrollers?
that fixed it, thanks!
great, thanks!
He's being snarky and saying the current async solutions are really only suitable for desktop and web servers. They're too big for embedded.
Is he right in that current async support is too bloated for embedded?
Actix is pretty simple and makes sense once you get it. But you have to use actors. It makes sense to me for webservers because each connection gets an actor. But again probably Overkill for embedded. The websocket support is really nice to think about. Just start an actor and start chatting...
I see I totally agree with you
I don't know if that's the case for this specifically, but there are quite a few node modules written in rust.
Actor model is awesome yet it is pain to write in rust, my friend was using actix actually, but actix can‚Äôt help about the usage of futures. Actix is solves the structural problem, but futures are the building blocks. With error handling and other futures combinators just for the sake of running in the combinators makes the code too noisy. Even though your logic is only ~10 lines you can end having &gt;20-30 lines. 
Right, so there's the C function `alloca` which is (usually) used with a dynamically-sized argument. This `alloca` is generally frowned upon. `alloca` in the more general sense is considered fine - as long as you use it with a compile-time constant there's no issue. This is also true in C - `alloca` with a compile-time constant is fine - but you never see a compile-time constant as the argument to `alloca` because in that case you might as well use a fixed-sized array.
Sorry for offtopping but 'dos.json' is like the most anachronistic file name I've ever seen .
I think it's just a matter of choice. It says Into implements for free if you implement From. It also says that Into cannot infer the type you want to convert into, therefore you need to declare the type like you did in the num2 example. If you don't know the type you can only use the num1 example, since it allows to infer the type for num1 from the argument for the from fn. In num2 30 cannot be passed easily as a variable. Some things are just a result of human language being more flexible than computer languages. I guess that's why they did that. I am flying from x to y is the same as I am flying from y to x.
Do you happen to know why `&lt;- line` instead of `, line`?
Into is implemented where From is. You can implement From on your types but only Into for your types to make foreign types. Always trait bound on Into to allow maximum flexibility. Implement From where you can and otherwise implement Into. If you have to, make wrappers around foreign types that implement From as needed.
\&gt; During a heated [twitter debate](https://twitter.com/arvidep/status/1090639300960665600), i made it quite clear that building a new async ecosystem for embedded is justified because the alternative is not using tokio. The alternative is nothing, since tokio does not and will never work on embedded. &amp;#x200B; Author made *absolutely nothing* clear, because they had no argument to present either on Twitter or on their blog post, apart from conflating futures and tokio all along.
The example on [this page](https://doc.rust-lang.org/std/convert/trait.Into.html) is a *bit* clearer. I don't think the example could work the same way using a `From` vs `Into`. fn is_hello&lt;T: Into&lt;Vec&lt;u8&gt;&gt;&gt;(s: T) { let bytes = b"hello".to_vec(); assert_eq!(bytes, s.into()); } let s = "hello".to_string(); is_hello(s);
&gt; You can implement From on your types but only Into for your types to make foreign types. Ah that helps!
Me too, I don't know why you are being downvoted. 
You probably want print!, not println! (which finished the line). Haven't worked with termion, though, just an educated guess.
Thanks, I tried this but then the background color continues on the next prompt line like this: Running `target/debug/clr` ######## ########jamie@jdm-pc-mhp:~/test/clr$ ######################################### &amp;#x200B;
Using neon, one can compile rust to node modules. 
&gt; IIRC dos extenders get program in LE (Linear Executable file format) and pack it with some payload code into DOS MZ. Yeah. The Open Watcom C/C++ Programmer's Guide (`pguide.pdf` in the PDF versions) goes into more detail on it. (Open Watcom is a great way to play around with that sort of thing, because it comes with three DPMI extenders (DOS/4GW and two drop-in replacements with different strengths and weaknesses, PMODE/W and DOS32A) plus a similar offering for writing 32-bit protected-mode Windows 3.1 applications using an extender called Win386.) ...no clue why they don't also bundle [CauseWay](http://www.devoresoftware.com/freesource/cwsrc.htm), what with it having been released into the public domain. It also supports generating binaries for FlashTek's DOS extender and Phar Lap's 386|DOS-Extender according to the docs, but those aren't included. (I believe a version of Phar Lap's extender was included with Visual C++ 1.x similarly to how DOS/4GW is a special **W**atcom bundle version of DOS/4G.)
I'm assuming "`std` on 16-bit architectures" is what was meant to be said, given that there is already Tier 3 support for MSP430 microcontrollers.
I immediately got the plasticity demo to crash by wrapping the deformable solid around one of the black bars. The backtrace says that an `expect` in `update_dynamics` failed.
Hi! Thanks! Note that the deformable demos I've put online so far are based on the finite-element (FEM) formulation of deformations. Because of that, severe deformations will cause the solver to crash (because some matrix will become non-definite-positive). This is a limitation with the underlying physical model more than a limitation of the implementation. FEM-based deformation are more suited to controlled environment more than free user interactions. I've not taken the time to put demos based on mass-spring systems online yet (those are much more robust but less realistic).
Thanks! Will take a look at your work! Though I will start by implementing a generic solution before optimizing for special cases like capsule/capsule and capsule/polygon.
&gt; What exactly would the alternative be? Prescribing an unspecific kind of nice would be pretty much useless. Well, from my point of view, since I'm usually having a hard time understanding the mods' concept of 'nice', it might as well be unspecific ¬Ø\\_(„ÉÑ)_/¬Ø However, I'm not advocating for that here, I'm just advocating for being more mindful of the fact that people come from very different backgrounds and might have troubles understanding the particular concept of nice or that there might be general cultural misunderstanding. They might be excluded from the community not (only) because of not being nice enough but because of a failed mutual understanding of what is nice. This issue is, of course, mutual / reciprocal. &gt; [citation needed] Well, look [here](https://github.com/rust-lang-deprecated/rust-buildbot/issues/2) for an example. That replacement probably makes sense in the US, but in Europe - especially in Germany and surrounding states (I was born in Czechoslovakia) - much less so. This issue specifically doesn't really bother me - I couldn't care less whether the concept is named master/slave or whatever/whichever. My point here is the discussion - it's an illustration of how the people involved think strictly only inside their cultural reference frame and the issue with other historical contexts or translations of the word `leader` doesn't seem to even cross their minds. That is, except for one guy with a polish name to whom no one seems to have paid any attention. 
Yes, as mentioned in my answer to /u/codec-abc above, this crash is expected because deformable bodies based on the finite-element method are not designed to sustain heavy deformations. What is recommended to use for applications like video games or applications involving sever deformations are deformable bodies based on the mass-spring model (for which I have not yet uploaded any online demo).
Can you link to it? Is it just conditional links for when there's a valid page in the second?
This is technically not answering your question (i.e., how to accomplish this with `combine`), but I have found that to solve this problem it is much easier to implement a [Pratt Parser](http://journal.stuffwithstuff.com/2011/03/19/pratt-parsers-expression-parsing-made-easy/) (e.g., [https://gitlab.com/jrop/rust-calc/](https://gitlab.com/jrop/rust-calc/)). This also scales incredibly well when adding new operators.
I don‚Äôt believe so. There‚Äôs so many inaccuracies in the post it‚Äôs really hard to tell. I haven‚Äôt personally built a project with an embedded executor and looked at code size, but working on embedded well was a hard constraint on the design, and people have been doing it.
When I return a value of type `impl TFoo` I have to explicitly bring `TFoo` trait into scope before I can call methods on it... Why? Example: [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4cc80db89d1cb3743ef758f234829400) mod foo { pub trait TFoo: Copy { fn foo(self) -&gt; i32; } impl TFoo for i32 { fn foo(self) -&gt; i32 { self } } pub fn fooify(i: i32) -&gt; impl TFoo { i } } use foo::fooify; fn main() { println!("Hello, {}!", fooify(42).foo()); } I am trying to create a 'wrapper' without actually wrapping anything by using `impl Trait` syntax. I was hoping that the returned type would not need to explicitly import the trait as this value can hardly support any other methods. Compare with trait objects and generic constraints allow you to use trait methods by virtue of that being the only sensible thing without explicitly importing the traits: [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=73a3011cbb802060916563b662cf9dd0) mod foo { pub trait IFoo { fn foo(&amp;self) -&gt; i32; } impl IFoo for i32 { fn foo(&amp;self) -&gt; i32 { *self } } } fn foo_generic&lt;T: foo::IFoo&gt;(val: T) -&gt; i32 { val.foo() } fn foo_tobject(val: &amp;foo::IFoo) -&gt; i32 { val.foo() } fn main() { let a = foo_generic(42); let b = foo_tobject(&amp;13); println!("Hello, a={} b={}", a, b); } Why does `impl Trait` require an explicit use of the trait, while these other cases do not?
There‚Äôs a few things in a few places, and I‚Äôm on my phone so links are hard, but there have been several Reddit threads on this topic in the last week where I left a ton of comments. There‚Äôs also been internals posts and issues opened beyond that. There‚Äôs a few different interconnected issues that have various tracking issues and open questions. Better links are one of them, for sure. That‚Äôs a matter of sending in the PRs. In general the docs team is trying to do a better job of surfacing this kind of stuff, but the edition *really* wrecked us, so it‚Äôs tough. Feeling positive about the future, though.
We use Spark at my day-job. For me to start trying out DataFusion for our use-case, I would need to see a couple of things: * More SQL queries (specifically `ORDER BY` and `JOIN`) * DataFrame support * Distributed cluster support * More data-source options would be great. We use a lot of Apache Kafka. DF seems like such a cool project, I love watching it develop. The choice to rewrite it with a more slow and measured approach was I think a good idea. I take a look at DF and other similar projects (timely/differential dataflow) with interest. I hope to see a lot of good things come from Rust in this space!
Could you provide an example of unreadable async Rust code, I'm interested as I'm rather a novice at Rust too, but so far I wasn't tripped up about async, more about trait-fu. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/privacy] [Discord PR rep tries to defend discards horrible practices.](https://www.reddit.com/r/privacy/comments/anfodx/discord_pr_rep_tries_to_defend_discards_horrible/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
| I'd say that [/u/arvidep](https://www.reddit.com/u/arvidep) is european as well. yup, I'm of Italian descent, where insults sound a lot harsher than they really are, and i agree on the cultural bias of the "niceness". But i don't agree on the impact that has. It's not like you get thrown out of an event for calling shit code literally shit code. People get offended, sure, but hey that's just normal human behavior.
That is similar to what I currently do, using a lazy\_static. I was looking to see if there are better ways. Thank you.
There isn't a problem here, that's just how floating point works. If it's a problem, don't use floating point. 
There are Decimal crates that converts a float into an "i128" on those you can do operators that should be more precise. The d128 uses, I think a vector of i64 or so and executes the Math step by step. Haven't tested, and I am not a scientist.
Now this is note-taking. That said it's gonna be hard for me to get away from a syncthing folder with markdown files.
Was it the same problem I ran into under the "SSL certificate problems" section or something different? https://www.fpcomplete.com/blog/2018/07/deploying-rust-with-docker-and-kubernetes
Welcome to the wonderful world of floating point numbers, your first lesson: they have finite precision. If you really depend on having "infinite" precision, either use a decimal number library or create one of your own (the arithmetic isn't particularly hard)
I think you misspelled org mode :P
Instead of using floats, use integers instead. Instead of going from start to stop with step (where start, stop, step are floats) , use an integer i from 0 to n and calculate `start + i*step` at each iteration.
Rust mod here (not an r/rust mod though): &gt; Well, from my point of view, since I'm usually having a hard time understanding the mods' concept of 'nice', it might as well be unspecific ¬Ø\(„ÉÑ)/¬Ø The code of conduct outlines our standards of behavior: https://www.rust-lang.org/policies/code-of-conduct In particular, I'd like to emphasize a few excerpts from the "Moderation" section beneath the CoC: &gt; In the Rust community we strive to go the extra step to look out for each other. Don‚Äôt just aim to be technically unimpeachable, try to be your best self. &gt; &gt; ... &gt; &gt; And if someone takes issue with something you said or did, resist the urge to be defensive. Just stop doing what it was they complained about and apologize. Even if you feel you were misinterpreted or unfairly accused, chances are good there was something you could‚Äôve communicated better ‚Äî remember that it‚Äôs your responsibility to make your fellow Rustaceans comfortable. Everyone wants to get along and we are all here first and foremost because we want to talk about cool technology. You will find that people will be eager to assume good intent and forgive as long as you earn their trust. If you have questions or want clarity on specific parts of the CoC, please email us: rust-mods@rust-lang.org
Yeah, that's great, I know what's going on under the hood, as I mentioned, but here in the real world we need to make 1.1 + 2.2 = 3.3 sometimes.
I can't org mode from other devices. I can edit markdown from my phone tho.
I can org mode on mobile just fine
emacs within termux or something? Is it easy to copy paste, edit text, etc?
That would be a wonderful 64KB segment. Most likely only with core support.
I adapted the example in chapter 12 and 13 of "the book" to use string slices instead of cloning the strings. I had to use some lifetimes, which were easier than I expected, but I'm not quite sure what the code I came up with means. Here's my code: pub struct Config&lt;'a&gt; { pub query: &amp;'a str, pub filename: &amp;'a str, pub case_sensitive: bool, } impl&lt;'a&gt; Config&lt;'a&gt; { pub fn new(args: &amp;'a Vec&lt;String&gt;) -&gt; Result&lt;Config&lt;'a&gt;, &amp;'static str&gt; { if args.len() &lt; 3 { return Err("not enough arguments"); } let query = &amp;args[1]; let filename = &amp;args[2]; let case_sensitive = env::var("CASE_INSENSITIVE").is_err(); Ok(Config { query, filename, case_sensitive }) } } What does it mean for the \`impl\` line to have lifetime annotations? Is it just a technicality? As in, the struct is a \`Config&lt;'a&gt;\` so I have to \`impl Config&lt;'a&gt;\`, and then declare the lifetime annotation leading to the full \`impl &lt;'a&gt; Config&lt;'a&gt;\`? I view the \`impl S { ... }\` block as being just a container of functions, a syntax thing, underneath it's all just functions. What does it mean for a syntax construct to have a lifetime? I also saw I can do the following instead, and I'm wondering which one is better?: pub struct Config&lt;'a&gt; { ... same as above ... } impl&lt;'a&gt; Config&lt;'a&gt; { pub fn new&lt;'b&gt;(args: &amp;'b Vec&lt;String&gt;) -&gt; Result&lt;Config&lt;'b&gt;, &amp;'static str&gt; { ... same as above ... } } In this case I didn't even use the \`'a\` lifetime anywhere, why do I have to declare it?
I would say part of the issue is the element choice. Linear tets are infamous from suffering from strain locking. Linear hex elements are a bit better, but if you really want to deal with large deformations you need to use higher order elements. Next, the system itself will start to misbehave more due to nonlinear geometric effects (large body rotations is an example) coming into play at larger strain values. Honestly as someone who does research and development work in computational solid mechanics/nonlinear FEM, I'd say, after quickly looking at your FEM code, anything past 5% total strain (elastic and plastic strain) is unrealistic based on the additive decomposition being made for the deformation. I won't get into the realistic behavior of the actual models being used because that doesn't belong in a discussion about a real time physic engine, since those models are nowhere close to being able to be run in real time hahaha. Also, I hope I don't come off as devaluing anything you've done. I just wanted to make some observations from someone who works in the FEM field. I actually really enjoy seeing these online type demos that use some aspect of FEM under the hood. It's not an easy task to get a 3D model up and running that can also be interacted with and run in real time, so congrats on the great work!
You might be able to compile to C with mrustc (https://github.com/thepowersgang/mrustc) and then use a DOS C compiler...
That doesn't exist though, does it? You can find special use case libraries for handling numbers, but generally speaking (to my knowledge) no one uses floats if that's a problem. You cannot do `1.1f + 2.2f = 3.3f`. So the solution to the "problem" first needs to have the problem identified. Ie, the way i perceived your linked issue is that visually, you do not like the result. This indicates to me that it is a display problem. Ie, handle how much precision you want to render. If however the math is needed to be precise, then this isn't a display issue. You'll simply need to use explicit integer math, not floating precision math. Ie, `(110 + 220) / 100` where the actual math is done on integers, and the final division returns your floating human-friendly value. I've not seen any Rust-specific solution to this problem yet, so my handling of similar issues has been the same as with any other programming language.
You can't represent 1.1, 2.2, or 3.3 with a IEEE-754 floating point number (neglecting the decimal floating point numbers that they define). The solution is to use [arbitrary precision arithmetic](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic). 
I am using mobile org, the app and sync my files never thought on using emacs on mobile - great idea!
Unic project is currently working on the introduction of Locale API which will be an underlying layer for a lot of higher level apis. (I maintain intl-pluralrules API which also needs a locale negotiation strategy)
If you are only concerned about Linux, you could do it with GTK. It's possible to draw transparent windows with it. There's rust bindings for it. Here's an example in C++ https://stackoverflow.com/a/3909283
Yep, the same issue. I think I even stumbled upon this page, but it was before I ran into the certificate issue. Should've read the whole thing, would've saved me some time.
I see. Does `std` contain many things that have to be done in a platform-dependant way? (For others who, like me, wasn't aware, [this is what "tier"](https://forge.rust-lang.org/platform-support.html) means.)
Pinging /u/arvidep (the author). --- &gt; I think that's an area where we would need to get more experience with. The `Future` trait is object-safe, last I checked, so you should be able to avoid the monomorphisation by using `Box&lt;Future&gt;`. I am personally looking forward to const generics, which would allow `InlineBox&lt;Future; 240&gt;`: a type with a stack buffer of `240` bytes, which can contain any type implementing a `Future`. This would avoid having to choose between monomorphisation and allocation. *Note: unclear if it can be implemented today with the following API: `InlineBox&lt;Future, [u8; 240]&gt;`.*
And banned.
&gt; instead of criticizing another library. Actually, I do find it important to explain *why* said another library is not a good fit. It's important to contrast the two, and explain the consequences of the choice. Where I disagree is with the tone. When I read this article, I read: "I know what I am doing, those others are idiots", and that's very off-putting. I would much prefer a more neutral tone: I did that, they did that, advantages/disadvantages, judge by yourself which you prefer.
Hey guys, I'm writing a test to catch a specific error. My code is below. It is functional. However, I'm using two nested matches with one if statement. I wonder if there is a nicer way to do it. let db_conn = establish_connection().unwrap(); let trade = get_trade(&amp;db_conn, 1); match trade { Ok(_) =&gt; assert!(true), Err(err) =&gt; { match err.kind() { ErrorKind::DieselResult(result_type) =&gt; { if result_type == &amp;diesel::result::Error::NotFound { assert!(true); } else { assert!(false); } }, _ =&gt; assert!(false), } }, }
That's the problem with sarcasm: it's hard to communicate over a written medium, especially to non-native speakers.
Perhaps asking in /r/playrust might help?
I don't think you need `lazy_static` to use a `Once`
Hum... seeing as it extends `num` with formatting capabilities... it kinda is, no?
It's because in your second example you're writing the location of `IFoo` explicitly. IE, when you write `foo::IFoo` the body of that function knows _this type is implementing_ `foo::IFoo` In your first example you're returning `IFoo` from something scoped in `foo` but at no point in the outer scope do you actually declare you're using `IFoo` Consider this example: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=bc979234df9fcab69e07712378d9afc4 Which foo would it use?
If you're in /r/rust, you don't have to wave around the CoC. You still have to deal with the fact that calling others 'retarded monkey' or other ableist slurs isn't going to win you any points.
I want a solution to the common problem with Rust that development/debug builds are too slow to run and release builds are too slow to build. The solution is *not* making release builds faster by sacrificing their optimizations (increasing codegen-units, enabling incremental, etc.). I want my actual release build that I publish to be as optimized as possible. The solution is to have separate profiles for unoptimized debug builds and optimized development builds. I want to be able to do this: [profile.debug] opt-level = 0 debug = true lto = false debug-assertions = true codegen-units = 16 incremental = true overflow-checks = true [profile.opt] opt-level = 3 debug = true lto = false debug-assertions = true codegen-units = 16 incremental = true overflow-checks = true [profile.release] opt-level = 3 debug = false lto = true debug-assertions = false codegen-units = 1 incremental = false overflow-checks = false I am not saying these settings should become the new defaults (particularly enabling `lto = true` for release builds by default seems like a bad idea); I am just giving an example of what I would like to do in my own projects.
More or less. `core` contains the parts of the standard library which make sense on bare metal while `std` adds on the bits which conceptually assume access to or a willingness to reinvent OS services.
I really liked the frank tone of the author. It's freshing to have a real discussion without people walking on eggshells that someone else might get "offended" by code/implementation/design criticisms.
I guess that depending in the use case it could be better to "cheat". By that, I mean to detect when a matrix become non-definitive-positive and somehow reset/fix it. From a user experience I would rather have something that start to teleport in some cases than freezing the entire web page/program.
Awesome. Once you get a chance, let us know where it's being tracked so that our future efforts can start being directed at the source. It'll probably also merit a post here and all the various mediums so that the community can see that we can contribute to it üëç
TIL about shunting-yard algorithm, which is a very clever way of solving this. Thank you for the link! 
&gt; What does it mean for the `impl` line to have lifetime annotations? It means that you're doing an `impl` for `Config&lt;'a&gt;` for all lifetimes `'a`. The `impl` itself has no lifetime, it's just introducing a generic variable that you're implementing over. You have to do the same thing when you do a type generic (like `impl&lt;T&gt; Foo&lt;T&gt;`.) &gt; In this case I didn't even use the 'a lifetime anywhere, why do I have to declare it? Mostly so you don't have to declare `'a` on each method, since any method that takes a reference type of `self` would need that `'a` value. Meaning you can do: ``` impl&lt;'a&gt; Config&lt;'a&gt; { fn foo(&amp;'a self) -&gt; bool {...} fn bar(&amp;'a self) -&gt; u32 {...} } ``` ...though technically, you don't have to use that `'a` in the lifetimes, you can introduce your own lifetime.
You might find this post interesting, it talks about some of the bare minimum assumptions Rust makes: https://gankro.github.io/blah/rust-layouts-and-abis/#the-anatomy-of-a-platform &gt; For Rust to support a platform at all, its standard C dialect must: &gt;* Have 8-bit, unaligned bytes (chars) &gt;* Have a boolean be a byte, where true = 1 and false = 0 &gt;* Have integers be two's complement &gt;* Have IEEE 754(-2008?) binary floats, if they exist (e.g. we're comfortable with just disabling floats) &gt;* **Be at least 16-bit (just in terms of pointer size, I think?)** &gt;* Have NULL be 0 (although things may be mapped to 0, but that's messy since references can't be NULL) 
If you need precision and determinism, convert to integers entirely using some precision multiplier (i.e. 1.1 + 2.2 ‚âà 3.3, but 11 + 22 = 33), or use fixed point (a few crates exist but I have no personal experience or recommendations).
You can do: ``` let caps = re.captures_iter(...).collect::&lt;Vec&lt;_&gt;&gt;(); ```
I'd do something like this: let db_conn = establish_connection().unwrap(); if let Err(err) = get_trade(&amp;db_conn, 1) { if let ErrorKind::DieselResult(result_type) = err.kind() { assert!(result_type == diesel::result::Error::NotFound); } else { panic!("Expected ErrorKind::DieselResult"); } }
that's really cool, thanks for sharing. I'd like to write a game engine in Rust one day and resources like that is really useful
It appears that this is simply a choice made by the rust team (perhaps an oversight)? The compiler knows which `Foo` trait is being used because you call `foo::fooify`, not `bar::fooify`. If you were to `use bar::Foo` you would not be able to call methods on an `impl foo::Foo` type, despite having the same name the compiler has other means to identify the exact trait being referred to.
lol I like that you haven't banned me or anything. but as a sanders supporter of past, I can tell you this policy is what ruins communities. Ban on first offense (stupidity being an offense too...)
Thank you!!
Let's hope this doesn't follow the saying of "Apache is where good projects go to die".
The shunting-yard algorithm is also used by typenum for its `op!` macro. It's a great algorithm; makes this much easier. https://github.com/paholg/typenum/blob/master/build/op.rs
It's funny - of course right after I say this, I come to a point where I'm like "okay.. THAT went from 0 to 60 real quick. Slow down!!" And I'm wish it would explain *more*!! Bad part is, I'd already *gone* through this section (I'm making a "guessing game") and it almost seems *more* difficult this time! Oh well, back to it.
I've been keeping DataFusion on my radar for a while now. I really hope this will put DataFusion on the map and hopeful encore contribution. There are still a lot of missing features for me to consider it for real use cases.
Thanks! That was useful knowledge.
 let captures: Vec&lt;_&gt; = re.captures_iter("").collect(); [`captures_iter()`](https://docs.rs/regex/1.1.0/regex/struct.Regex.html#method.captures_iter) returns a [`CaptureMatches`](https://docs.rs/regex/1.1.0/regex/struct.CaptureMatches.html) which implements `Iterator&lt;Item=Captures&gt;`, so you'll end up with a Vec of [`Captures`](https://docs.rs/regex/1.1.0/regex/struct.Captures.html).
&gt;&gt;* **Be at least 16-bit (just in terms of pointer size, I think?)** Does that mean that small, 8-bit microcontrollers are out of the question?
The lifetime is another type parameter that you're making your type generic over. You can also have `impl`s for specific lifetimes (which, AFAIK, just means 'static). For instance, you could also have impl Config&lt;'static&gt; { pub fn new_from_strs(query: &amp;'static str, filename: &amp;'static str, case_sensitive: bool) -&gt; Self { Config { query, filename, case_sensitive } } pub fn static_query(&amp;self) -&gt; &amp;'static str { &amp;self.query } } which adds a new constructor method for creating static `Config`s, plus a new method for getting the query as a `&amp;'static str`. I'd favour not adding new lifetime parameters if you don't need to. If you don't introduce `'b`, you can replace `Config&lt;'a&gt;` with `Self` in the return type.
Thank you
Thanks
First, I get the impression that you might be misunderstanding what lifetimes are. When you specify lifetimes on a function, you're not defining a new relationship. You're telling it which of the possibilities it already figured out on its own is correct... so `'auto` is dead on that point alone. Second, Rust tries to be a minimally magical language. There's a very strong burden of proof needed to justify adding magical things and you *certainly* wouldn't see a lifetime annotation magically changing the defined type of an argument. There was drama enough when someone proposed the idea of having special function types which transparently added `Ok(...)` for you when returning `Result`s. Third, there are various other problems with applying your idea to Rust, but you caught me just as I'm winding down for bed, so I'm going to stop here for now. I seriously doubt you'll get traction on this idea beyond the `std::borrow::Cow` that we already have. 
This doesn't need to be a feature. Just implement it as an enum. I see absolutely nothing preventing that.
That is my hope too, that this will encourage more contributions and help accelerate progress so we can all start using Rust / DataFusion for real work use cases. I'm in the same boat.
The macro is totally unnecessary; what are you trying to accomplish?
Regarding your final statement, I wonder how it compares to the number of pointers-to-stack-values in Go. Before they are moved on to the heap (if they are detected to leak from the local scope), almost everything there is stack allocated. Of course, your point is well taken. I just think that it might be interesting to see how the two languages compare.
Why wait? You can implement this using an extension trait. (`IteratorExt`)
It's a macro, so the syntax is just however the macro itself is defined. It's not a part of rust's syntax at all.
There's a couple different things I want to address. I understand that lifetimes aren't about relationships, I'm not saying that `'auto` makes sense when interpreted as a standard lifetime. Instead the intent is for it to represent self owning entities whose lifetimes aren't limited by any other data but that aren't `'static` because they aren't contained within the binary. In this system `'auto` would be a sensible lifetime to assign to owned trait objects. Magically changing the data format of types that are annotated `'auto` is definitely sketchy and is probably not a great pattern and I'm definitely open to other solutions. One way or another finding ways to communicate that complex structures can live as long as they are needed because they own their own dependencies feels like it would be very valuable. Additionally, I grant that minimal magic is a noble goal and that other languages which go to far with that run into problems, especially with readability. This is definitely not a final proposal but a discussion I want.
What are your problems with the Rust fuse library? For me, I didn‚Äôt want to link to the c library and the pure rust version had some limitations around using it with Tokio. Didn‚Äôt end up finishing the side project I wanted to use it for.
Is it useful for this crate to convert "11 thousand and one hundred fifty-five" too?
Stumbled upon [https://blog.subnetzero.io/post/building-language-vm-part-01/](https://blog.subnetzero.io/post/building-language-vm-part-01/), could be useful to anyone reading this.
Interesting work. Can you detail how many run you made and the compilation options?
Once again, I'm pretty impressed by the performance numbers of Julia. Other than that, I find the difference between C and Rust is intriguing: +36% is pretty big. Have you checked where the difference came from? Different data-structures maybe?
&gt; I understand that most of what I'm talking about can be accomplished right now using things like Cow, I just haven't found solutions that feel idiomatic. What you have described very much sounds like `Cow` to me indeed, so unless you specify why `Cow` does not feel idiomatic to you, or how different from `Cow` your think is, it's a bit complicated. Also, you have touched on how this feature would work with mutable references: I suppose you intended to restrict it to shared references?
I have a few questions that are rather rhetorical. Will your book be more targeted toward specific tasks, or a general language use? If general, will your book be a better learning resource than the current rust docs? 
Specialization has good reason for it at least
Many (most?) 8-bit micro controllers use 16-bit addresses.
Why u32 and not u64 or u128?
Rust says it has an affine type system. However, when I check out the wikipedia page on the subject (substructural type systems), there is a lot more detail on linear type systems than affine ones. What is the difference between the two?
Sure, but there are both 8-bit and 32-bit ones as well.
You can check out the qt bindings for rust https://github.com/KDE/rust-qt-binding-generator Or just use rust as a back-end to an electron app. 
In linear type systems, every value must be used *exactly* once - so for example if a function takes something as a parameter, then it must either move it to somewhere else, or return it. In affine type systems values must be used *at most* once - so you can just bind something to a variable and then never use it (so basically like letting it `drop` on its own without consuming it explicitly).
I'm still trying to figure out the exact details, which is part of the reason why I decided to create this survey; so that I could see what the community would be best suited by and desires. Whether it will be a better or worse learning resource than currently available resources will be a subjective matter that people don't yet have the ability to judge. I believe that I will be able to create a book that fills a niche that isn't currently being filled -- and don't necessarily imagine it as competing directly with anything else, but rather as complimenting the available educational resources. The exact way things will play out remains to be seen, and it is important to know the characteristics of the community planning to learn Rust -- particularly those who will start doing so a handful of months from now, as that is how long I predict it will take me to finish. However, I will every month starting from today post an update, and will eventually be keeping nearly real time track of development through my website -- similarly to a beta book. I actually plan to implement much of my website with Rust, and am holding off on particularly developing it in the meantime as I will possibly be documenting my development of it as a narrative through my book -- if the survey indicates that is an appropriate strategy. I believe that I will dedicate the first chapter of the book not to Rust at all, but rather to techniques for learning to sort of set the groundwork for learning in subsequent chapters. I also will be applying the numerous pieces of literature I've read regarding skill acquisition, learning theory, and the like, to designing the material. Things as simple as changing color accenting from chapter to chapter, or suggesting things as simple as taking a break to do a little tangentially related exercise at a strategically appropriate time, could make slight differences that add up to more rapidly learned material. So, one thing that my intuitive sense believes is the correct course of action, is paying more indirect attention to the process of learning (the how) than things like the standard documentation (which focus on the what of what is being being learned). 
For being in limbo? What would those reasons be?
To be clear, we didn't post this. Someone took it upon themselves to post it here after they saw we had an opening (which I think is awesome -- thank you OP!)
It's using trait objects for the function `f` when it should be using generics. I would take these performance numbers with a grain of salt.
It's less that projects go there to die, more that projects go there to go on life support. At least we still get security patches.
See https://os.phil-opp.com/freestanding-rust-binary/. Those functions are missing, and you have to provide an implementation. You can use empty ones to get started, but I think there are some crates that have appropriate implementations, or you could try `panic = abort`.
Rust is apparently fastest, so a trait object seems like it's an acceptable choice there.
I'm really frustrated by that argument. I have shipped hundreds of thousands of embedded devices in my career, yet the pain we go through to get rust onto these boxes weights zero against anecdotal references that it should work in theory.
Most people agree that Rust has a *steep* learning curve ‚Äì you (have to) learn a lot with little effort. This is made awesome by Rust's learning tools: Starting with the compiler that will give you great error messages (also with clippy you will get even more useful warnings), the very thorough docs, the helpful community, mentoring, etc.
I've been interested in a cross-platform solution for this. I hope that somebody knows!
&gt;InlineBox&lt;Future; 240&gt; i like this, but for Fn :) | but not for futures0.3/async\_await. Future is not the correct abstraction layer, because it imposes a specific style of execution engine that isn't very universal. Of course you can implement anything as a Future eventually. You can run doom in a browser too. | I don't see that composability in the given osaka examples. absolutely correct. composability is a non-goal because futures does that extremly well for its intended use case (web servers) and will hopefully give us a big ecosystem like rails/rack eventually. osaka's intended purpose is for explicit arrangement of external polling in systems where you know all the resources. specifically, on embedded devices. Both could be cased on generators or monads or whatever you call Fn-&gt;Fn if rustc authors wanted that. &amp;#x200B; | it requires even more std types no, thats the mio set. You can use a different one, that's the idea. That code is also the work of a single author, so it's not all thaaat great anyway ;) &amp;#x200B; &amp;#x200B;
The benchmark game doesn't quite get compilation correct either. For example, I see at least one benchmark (regex-redux) that uses -march=native for C, but -C target-cpu=core2 for Rust. Whether that makes a difference or not of course depends on what cpu is being used to run the benchmarks.
Ah! Then I think GJK and Minkowski sums will do nicely for you :-) 
Here you go https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.msi
IIRC the benchmarks game actually ues a `core2` CPU, but I wasn't suggesting that it was perfect, only that it was better (e.g. it actually has output validation, it measures all benchmarks in the same way, etc.).
do i just download it &amp;#x200B;
Sorry, that was a joke. You are probably at the wrong sub.
yes
i downloaded it what do i do im confused
&gt; _Note: unclear if it can be implemented today with the following API:_ InlineBox&lt;Future, [u8; 240]&gt;. This is what utils like SmallBox, SmallVec, and SmallString do. However SmallBox requires nightly in order to support DSTs (and zero-sized types?). 
my fan just went nuts lol
I'm far from meeting any of the requirements but it's nice to get an idea of what people are looking for in a rust developer
Ok, thanks for the offer, although I was just trying to sort of defend OP here since he seems like a nice guy to me... 
install, you must
Thanks, this looks great. I may try implementing that algorithm myself for the extra practice, but I'll take a look at that crate to see if anything else in it is useful or educational!
Well, how would you implement it? You could replace the "size" field in the vtable with a pointer to a function that calculates the size, but this complicates the simple cases
You should read The Book so you can grasp the basics of the language.
Pretty cool! Let us know when it is done! I always loved ray tracers :)
my computer crashed &amp;#x200B;
wdym
https://github.com/maps4print/azul IOS is probably out of the picture (for any cross platform rust gui crate), w.r.t. how they handle their ecosystem. But you can still use your rust code as a library. 
For sure. &lt;3
Neat. I wonder if this (or something like it) could be a piece to the cargo sandboxing story?
I rarely need the range provided by u64 / u128- most of the stuff I've written in rust deals with numbers that are counted up in smallish increments. Things rarely exceed 1,000 and very rarely exceed 1,000,000. u32 strikes a good balance where I don't really need to care about the range, but it's also pretty small.
It is also hard to tell what your anecdotes are about when you get so many facts wrong.
This strategy was directly inspired by Rust: Rust's approach to `&amp;mut` crystallised what Swift wanted from its `inout`. 
I've had it running on a nrf52840, and helped out with the nrf52 series crates although I haven't had a chance to look work on it for a few months some am a little rusty. I seem to remember there was an optional feature on cortex m rt called abort on panic that gave some default implementation for those language features. But I could be wrong. The nrf-rs org is pretty friendly I would ask a question there. 
proficient in C# here. He is using the top quality .net benchmarking tool there, so that is good. I can only achieve moderate speedup (dropping ~20ms) without resorting to tricks he isn't using in Rust. Comparison seems pretty fair there. 
Not the author, but I've ran into similar issues as both of you. I've had to fork and implement features such as notifications (invalidation, deletions, cache operations) and exposing iovec interfaces. Regarding the C library dependency--that's only used during the mounting process and I don't see a reason to not do that through Rust. It's caused a lot of pain for me while cross-building. I've been waiting for the async/await story to get a bit better before spending more time on a tokio-compatible fuse library. Instead of handling fuse operations in a thread, they would go through tokio with a RawFd. It would definitely clean up large parts of my application if you didn't have to go through a fuse&lt;-&gt;tokio boundary.
I thought the game used to say what machines were used, but I can't find it any more.
I appreciate the reply. The "exactly" once vs. "at most" once thing was discussed on wikipedia and in other places. Does this refer to any use or just the equivalent of borrows? For instance, if I just declare an int and assign some number to it, then it is used once. I just then have to make sure it's not used any more than that before it is destroyed. I also get confused if it also applies to const borrows. In Rust, if I have a mutable variable, I can have unlimited const borrows and still mutate the original variable or I can have one mutable borrow and then i can't mutate the original variable during the borrow. In a linear type system would this also be the case?
&gt; He is using the top quality .net benchmarking tool there, so that is good. FWIW he is using a top quality benchmarking Rust as well.
You're in the wrong subreddit. You want /r/playrust.
GTK bindings are here [https://gtk-rs.org/](https://gtk-rs.org/) or a more rust idiomatic attempt is [https://github.com/antoyo/relm](https://github.com/antoyo/relm) .
In the future: don't blindly install `.msi`s strangers on the internet link. This one is benign (it's a programming language's development tools), but it could easily be malware and you've no way to tell.
I believe this is actually what Graphene did as well. Though references to that project seem to have become rather hard to find since browserhtml was retired.
&gt; \[*Noob conclusion*\] -&gt; So, if the premise is correct instead of building completely an app would it be possible to strip down what powers Firefox (if I got it correctly it's built on top of Gecko/Servo), and build an **app** or a **pwa** on Gecko/Servo or Firefox itself **like Electron** but hopefully with more efficient results? Yes, this is certainly one way to do it. Most modern GUI systems support some form theming functionality, and some (GTK+) are using CSS and SVG for that already. I think as time goes on, we will continue to see convergence between web technologies and native user interfaces, as there is a large overlap in problem-space. That said, I'm not happy with the "wrap a browser and call it an app" approach whatsoever. I think individual components should be reused where it makes sense, and many of the others should be left out. Cut the JavaScript, and cut the WebAssembly entirely. Take the CSS engine for style and layout, take the renderer, and build an GUI library which interfaces directly with those components natively. This is the approach being taken by [Azul](https://azul.rs/).
They say they support Windows: https://readme.drone.io/install/agents/windows-1803/ and *BSD needs docker support (so Docker or OCI needs to run on *BSD for Drone to support it) https://github.com/drone/drone/issues/2431 
Very. Cool. Great writeup. Great example of tools I want to see written in Rust. Great use case too!
`winit` can open a transparent window with no title bar for you to draw on. Though after that you have to handle the drawing yourself for the most part.
I'm glad someone said this-- just going by the general tone and the fact that OPs username has "2004" in it, we're dealing with someone who *probably* doesn't know better. It's one thing to jokingly link to the rust binaries and another to go "wait shit, you actually installed that?" and explain some basic browsing safety.
I would recommend not going the trait route. Using a macro would be far simpler and more clear, even if the message changes. Otherwise, using a plain map_err is the most idiomatic, IMO
&gt; 'auto lifetime for a reference that indicates that it manages the lifetime of the underlying data by itself The way I'm interpreting this is "you are free to move this value around as you please throughout the program". If that is the intended effect, then this is the exact behavior of `'static`. Something not immediately obvious (at least not to me when I struggled with this) is that every value in Rust has a lifetime. In trait-bounds it's usually expressed as `+'static`, meaning that the target value itself must not be constrained by any particular lifetimes (such as it would if it contained inner references with more narrow lifetimes) and therefore support being moved around until the end of program. So, `usize` have the lifetime `'static`, as does `Rc&lt;T&gt;` where `T: 'static`. Your `'auto` lifetime sounds suspiciously much like `'static`. For you use-case of supporting both `&amp;'static str` and `Rc&lt;String&gt;`, you can easily be generic over both (and many more similar structures) using the trait-bound `AsRef&lt;str&gt;`.
I really hope that performing more language specific optimizations in rustc will reduce the amount of work LLVM has to do and make compilation time much shorter.
I was a bit confused about `'static`, I thought I had read in the book that it was only meant to be used for things that were literally encoded into the binary or were otherwise guaranteed to never go away. &gt; One special lifetime we need to discuss is `'static`, which denotes the entire duration of the program I have been forced to use `'static` in places that I felt weren't consistent with this idea to represent the lifetime of owned data. I think part of what I was looking for in the idea of `'auto` was to generalize the idea "immutable data encoded in the binary" to include "owned data that can be relied on until dropped." As I understand it implementing this would involve relabeling some things that would currently be considered `'static` to `'auto`. To some degree this portion of the discussion is entirely about "what should static mean" and is more philosophical than practical.
Does LLVM short circuit, as in do less work, when the input code is already partially optimized?
What is the purpose of Docker here, and why is it needed?
I've experienced the pattern of changing the architecture to improve performance, but then after the huge refactor, performance is the same or worse. He gives some good explanations for why that might have happened in this specific case. In other cases, I think software developers really love the idea of having the "perfect architecture", but don't always go through the steps to prove that the new architecture allows specific performance improvements that couldn't be implemented as easily on the old architecture. SoA/microservices are often culprits since the main benefits are about having independent development processes. In fact, decoupling software in this way can often inhibit performance improvements. Caching is another one where developers try to introduce it before figuring out what the hit rate and read/write costs are. E.g. expected_cost = read_cost + (1 - hit_rate)*(write_cost + base_cost) It only makes sense to cache if `base_cost &gt; expected_cost`. You can tweak the formula a bit for async writes, fast misses, storage, or latency vs. throughput. But many times people don't measure or estimate any of the variables, don't do the calculation, and then are disappointed when the cache didn't improve performance.
I think the idea is that we could reduce the quantity of LLVM IR that we pass LLVm in the first place if we can optimize that code away.
Aren't we just doing what LLVM already does then? We shouldn't reimplement portions of LLVM.
I think I'm definitely going to mess around with that. Do you mind if I message you questions I run into?
LLVM won't have all the context that the rustc compiler has when emitting IR, it just has the IR. It doesn't have to include any re-implementation, just feeding sane input to LLVM.
I think it's more accurate to say that Rust supports affine types (= types that are used at most once), but most types can still be used more than once. Anything that is `Copy`/`Clone` is not affine, and this includes things like shared references. Section 1 of https://gankro.github.io/blah/linear-rust/ may be helpful.
Tsk, not even any standard deviation listed? It's decent work, but your analysis needs more attention.
An excellent example of San Francisco bubble marketspeak. I wish you the best, but expect nothing useful.
I think it's a matter of "generate large amounts of stuff we can generally count on llvm to handle" vs "not bothering to generate it when we don't need it". The latter is often cheaper, performance-wise.
I see, thanks for the explanation.
You might find this post useful: [https://ricardomartins.cc/2016/08/03/convenient\_and\_idiomatic\_conversions\_in\_rust](https://ricardomartins.cc/2016/08/03/convenient_and_idiomatic_conversions_in_rust)
fuck &amp;#x200B;
it was bad &amp;#x200B;
You typically need a compiler toolchain for the target architecture when cross compiling rust (mostly for the linker). Those can be a pain to install on some systems. The cross docker images have everything installed for the target architecture, including a couple of common libraries such as OpenSSL, so it typically makes the process as simple as `cross build ‚Äîtarget the-target-triple`. Personally on a [recent Raspberry Pi Zero project](http://www.wezm.net/technical/2019/01/linux-conf-au-rust-epaper-badge/) I skipped docker/cross and just installed the [arm-linux-gnueabihf toolchain from the AUR](https://aur.archlinux.org/packages/arm-linux-gnueabihf-gcc/) and using plain `cargo build ‚Äîtarget` worked fine. 
Made me think of a post a while ago about using formal verification + super optimization in modern compilers, can't find it now though
I see I would definitely start learning rust
Ah. I think it's better to avoid the allusion to PyPy.
this is a good place to start, in case you do want to put that thing that's not the rust game that you just installed to good use: https://doc.rust-lang.org/stable/book/ or maybe this: https://stevedonovan.github.io/rust-gentle-intro/readme.html though to be honest i'm not actually sure how well rust fares as one's first programming language
in some of the langs he is using benchmark tools that figure out how many runs to do to get reliable data. if you run c# or rust examples you will get lots of statistics
the rust and c# examples at least list that when you run them. maybe the others too
This is the subreddit for the Rust programming language. You're looking for /r/playrust
Haha damn sorry. I bet y'all get this alot.
This is generating a lot of interest. The project is trending on GitHub. It's *neat*, and there's no need to justify its existence since it's not a paid business venture, but why the fervor? What do all these people see in this project that I do not? I'd love some insight. 
it's also a very unsatisfying development, that MIR didn't enable better non-LLVM bound backends. that's horrible in it's consequences! :( -- e.g. for embedded system development, where GCC based toolchains are still supporting a much wider range of architectures. rust would be the perfect choice for this kind of development otherwise, but we still not able to use it e.g. for the very popular ESP32 ¬µCs in an acceptable efficient manner because of this unalterable LLVM blinkers.
Yes. Yes, we do. :) 
Starting week 2 of a new job, which is pretty fun so far even if it doesn't involve any Rust (YET). Spending far too much time and energy thinking about how to make an LED turn the right color. Not much work on [`ggez`](https://github.com/ggez/ggez) lately, unfortunately. I really need to get another release candidate published soon, 'cause there have been some very nice bugfixes contributed by Ozkriff and various others. I did get a thing done I call [`isildur`](https://crates.io/crates/isildur) but, as useful contributions to humanity go, that's not really high on the list. I want to work on writing up a revised web framework comparison, but again, got a little sidetracked.
It‚Äôs an interesting thought, and definitely reduces code overhead. However on the other hand we lose another good property of futures: they use exactly as much size as required for their ‚Äûstack‚Äú. If we always waste some bytes of the 240 (or whatever the number will be), we might waste a lot of memory in deep call chains in memoryconstrained environments. However it is at least better with async/await than with combinatory, since the future for the generated state machine requires less nesting. 
I'm trying to use the RLS with Sublime Text 3. I've installed the appropriate components and plugins and it appears to be working. However, autocomplete doesn't seem to show anything. e.g. if I add 'use std::' at the top of the file nothing shows up except things already in the file which is not very helpful. Is there a way I can fix this to show the normal autocomplete you'd see in VS Code?
Thank you, I didn't realize there was an LLVM intrinsic for `alloca` as compared to the C function. I did discover however that Rust actually makes it quite difficult to allocate an array on the stack, `alloca` style, *without* its length being a compile-time constant.
&gt; This would not be such a problem except that Rust by itself has pretty much nothing in the standard library and you need to get many crates via Cargo for even fairly simple programs. As compared to, say, C? Anyway, long term I'd rather build C with cargo than build Rust with something from the C ecosystem.
I understand that this means rustc can optimize some code away cheaper because it knows more than LLVM can ever know through IR.
Under the hood actix uses futures. I haven't had to see them directly at all.
Serde allows me to serialize and deserialize anything I want into JSON or YAML. But that's an overkill for what I need. Is there a way to simply convert an enum into string or from string into enum? Something like #[derive(Serialize, Deserialize)] enum A{ b,c,d,e } And I can go from "b" to `A::b`, or from A::b to "b"?
The LLVM phase is by far the longest part of Rust compilation. Since many other languages compile more quickly on LLVM, Rust is somewhat unique in generating so much IR code that it notably slows down LLVM. 
is there a way to access to std::alloc::raw\_vec in user code? working on a custom collection type, but would prefer to just have direct access to its buffer allocation API than jumping through hoops with Vec's clunky unsafe API.
Case in point: the relatively small (~10k LOC) project I have open right now is feeding 123MB of LLVM bytecode into LLVM. That makes the slow compile times not that surprising...
A number of people in the Rust community have expressed a wish to see more guidance on how to structure medium sized projects, given that Rust is not a tradtional Object Oriented language. A book that walked through a relatively xomplex project like this, and explained the design choices could be popular 
In the sentence "Use async when you just have I/O stuff to avoid making threads", I would like to put the emphasis on "**when you just have I/O stuff"**. When you're not doing I/O(or don't need it to be async), one does not have to avoid threads. One of the points I'm trying to make in the article is that threads give you isolation for a "component", which can run it's own internal event-loop using channels, without the need to use locks for any of its internal state. &amp;#x200B; A problem I have encountered with async is that it's harder to get this isolation. So I find it great to perform "computations" inside an isolated component(where any "state" of the computation is ephemeral and can just be moved into closures), I find it harder to use to model an entire system consisting of logically separate components that are "long running". &amp;#x200B; In fact, what I have seen is that when people use async for everything, meaning an entire system that logically consists of separate stuff, they end-up having to resort to locks(because they're running a bunch of components with state on a threadpool). &amp;#x200B; The point is made more clearly in [https://swtch.com/\~rsc/talks/threads07/](https://swtch.com/~rsc/talks/threads07/). Where their mention of the problems with "top-level select loop" are essentially applicable to doing "async with a threadpool". 
for user facing stuff you can always `impl std::fmt::Display` or `#[derive(Debug)]`, but you'll have to hand write your own deserialisation for those. if you want it to be fully automatic then you better stick to Serde, although you could try out some of the different encoding schemes available.
r/lostredditors
Rust is only an affine type system in the sense that _some types can be made affine_. Specifically, if you have a type like this: struct X { ... } impl X { pub fn new() -&gt; X { ... } pub fn use(self) -&gt; ... { ... } } `use` can be called only once per instance created by `new`. Immutable and mutable borrows don't play into the affine aspect of the rust type system. For this reason, I'd say rust's type system has affine types rather than all rust types being affine.
Why are the screenshots photos of screenshots?
Awesome! This will be useful for adventures in audio with Rust 
Gave it a reply. I have a light amount of tutorial writing under my belt, feel free to contact me if you want someone to bounce ideas off of.
Is there any comparison in that quote? I don't believe so. Cargo is pretty great, and I like it. However if you are deploying apps on GNU/Linux, being compatible snaps and flatpak helps a lot. I'm not sure if flatpak supports cargo yet. I could be wrong.
Any chance to see function attributes to count for an entire function?
Do you have any example code?
`Rc`'d types already have a `'static` lifetime, so long as they aren't references. `Rc&lt;String&gt;` is `'static`.
They're normal screenshots.
The same reason for docker in most places. It provides a way to distribute a consistent and repeatable environment. &amp;#x200B; ...and depending on who you ask, massive security holes. But that's not really too much for a development environment
You can use [strum](https://crates.io/crates/strum) for exactly this. It derives FromStr on the enum.
&gt; I understand that lifetimes aren't about relationships Actually, relationships are all they *are* about. That's the point I was making. When the compiler sees multiple inputs that could delineate the lifetime of an output, you have to use an explicit lifetime annotation to tell it which input the output is dependant on. The point is that a lifetime annotation can't "keep something alive longer". Things live as long as they will and lifetime annotations just tell the compiler how long you *intend* for them to live so it can check your work. &gt; Magically changing the data format of types that are annotated 'auto is definitely sketchy and is probably not a great pattern and I'm definitely open to other solutions. To be honest, it sounds like what you want is the `@` and `@mut` garbage-collected pointer types that were [deprecated in Rust 0.9](https://www.i-programmer.info/news/98/6837.html) and removed before 1.0 as part of a larger push to prune down the core language and put the heavy or specialized features in libraries instead. (eg. `Box&lt;T&gt;` [used to be written as `~T`](https://github.com/rust-lang/rust-wiki-backup/blob/master/Sigil-reference.md).)
Do you mean `#[count_alloc] fn foo() { .. } =&gt; fn foo() -&gt; (usize, usize, usize) { .. }`? Or `-&gt; X =&gt; -&gt; ((usize, usize, usize), X)`?
I don't follow?
Yes, great point. It was the tone that got me. The criticism was not constructive.
I quit my day job this year, and I am considering putting significant time to Rust. MIR-based GCC backend is among top ideas. Others are JS backend like Scala.js, improving WASM backend, and working on security of Rust ecosystem (I am especially interested in fuzzers).
Some alternatives: - if you have a known lower multiplier like 0.01, use integers and multiply by that for presentation - round the floating point representation: println!("{:.05}", 1.1 + 1.1 + 1.1); This will print `3.30000` and ignore further digits - should be relatively good. - use decimals rather than floats - a library like https://github.com/paupino/rust-decimal could be helpful 
In my experience (as a novice hobby programmer w/ a little over a year of total programming experience), one can learn quite a bit on their own simply by fighting the borrow checker. After a few months of that, I'm not afraid of it anymore, and now it's my friend. What I still have trouble with - to the point of avoidance - is traits and lifetimes. Perhaps the 2018 edition will make lifetimes easier to teach than before. Anyway, my point is that there's plenty of info out there on types and borrow/move semantics, but not as much on traits and lifetimes (at least not material that I've been able to easily absorb). 
It looks like they were also inspired by FORTRAN SUBROUTINE Swap(a, b) IMPLICIT NONE INTEGER, INTENT(INOUT) :: a, b .......... END SUBROUTINE Swap https://pages.mtu.edu/~shene/COURSES/cs201/NOTES/chap07/intent.html
You can create `'static` references only from data literally encoded into binary. But owned types all have `'static` lifetime as well.
The general solution I know of if you're still using floats is to just trim out digits. Like, printing `println!("{:.5}", 1.1f64 + 1.1 + 1.1);` will give you some extra `0`s but not enough to show the floating point inaccuracies. I'm 90% sure there's a way to throw out digits that matter too little when compared to the biggest digits, but I'm not sure how to do that in Rust. 
You never want to allocate memory on the RT thread in audio, and reusing buffers for codecs is a must. I have profiled Lewton and its reliance on Iter::Collect&lt;T&gt; is a real perf killer.
Then yeah, this would help :)
I assume this has some overhead. A feature to effectively disable the crate, and hopefully remove all overhead, would probably be handy for release deployments.
Will you share some sort of anonymized version of this data with the community? A lot of us are creating (free) learning resources and this data would be a great contribution to everyone's work. 
Those two sentences feel like they describe very different semantics for `'static` on one hand it's a strict notion about data originating from the binary and on the other hand it's a very loose label for anything that has no lifetime in its declaration.
Exactly. I‚Äôd be interested in helping out, when you start. 
You're responsible for setting the global allocator. You can feature gate the #\[global\_allocator\] yourself to completely remove the cost.
/u/raphlinus has written and talked on this a bit, his talk on the synthesizer-io project a few months back covered more ground. Essentially one thing we're dealing with in the audio world is the concept of "the audio thread waits for nothing." Low latency and high performance audio code has to be rigorously wait and lock free. It runs in a callback that is a glorified interrupt service routine, meaning if you have any blocking code and you miss a deadline you get artifacts in the audio playback, which is audible as clicks and pops. However, user interaction that manipulates the audio processing happens on a different thread that is free to lock/wait all it wants. The trick is getting data from user interaction into an audio callback without forcing the callback to lock or wait, and minimizing the work the audio callback needs to do other than processing audio. Most allocators are not lock or wait free. However, several well known algorithms and data structures (like the textbook implementation of a Michael-Scott queue) assume that they are lock/wait free. So when it comes to your crate here, the application I see is to use it in a test bed to help develop and evaluate lock/wait free data structures for communicating with an audio callback. We can (hopefully) provide stronger guarantees in friendlier APIs for a variety of audio utility crates, with the ultimate goal of creating a solid ecosystem in Rust for developing audio software. In my professional opinion it's tools like this that make Rust an excellent candidate for future work in multimedia software. 
If you're referring to the name, I guess that's a possibility, but the thing of particular interest here is the behaviour.
If you combine #\[no\_alloc\] with #\[no\_panic\] from [https://docs.rs/no-panic](https://docs.rs/no-panic) you \*can\* get compile-time guarantees that allocations don't occur. This only works in release builds (or debug with opt-level=1) and if the compiler does figure it out the allocation branch is unreachable.
Is something like `#[cfg(feature(foo), my_attribute)]` possible right now? (Not sure on syntax). I think that can be done with a procedural macro even if `cfg` doesn't support it. 
Excellent. To go along with /u/Cocalus's comment being able to feature gate this would be great, I personally use cargo-check more often than build to get such guarantees since it's quicker than compiling a release build. 
One of the screenshots has a really odd gradient that is easily mistaken for a reflection on a screen. 
Confirmed, it works.
Er, no. You're after #\[cfg\_attr(feature = .., my\_attribute)\]
I thought you were trying to say that lifetimes don't convey the relationships that exist between data inside of a struct, which is part of what my last part about structs was saying. When you're talking about inputs and outputs are you talking about lifetimes as a tool for relating function input lifetimes to the return? The point that a lifetime annotation shouldn't change the underlying data format has definitely been well made.
Thanks, I don't use cfg that often 
`#[global_allocator]` `#[cfg(debug_assertions)]` `static A: AllocCounterSystem = AllocCounterSystem;` and/or `#[cfg_attr(debug_assertions, no_alloc]` `fn foo() { .. }` 
Or even ‚Äûdos.jsn‚Äú?
You're probably already aware of https://github.com/rust-fuzz , but you may also be interested in https://github.com/AngoraFuzzer/Angora , one of the newer fuzzers with the interesting characteristic of being written largely in Rust, but not (yet) targeting Rust.
Yup, I am aware of both, and am subscribed to [Secure Code WG issue for Rust/Angora](https://github.com/rust-secure-code/wg/issues/17).
At NEAR, you have the chance to become an early member of a world-class team with significant upside. The opportunity we are tackling will completely reshape the landscape of the web for decades to come and potentially affect billions of people. We are funded by the top names in the industry and have attracted the best technical minds in the world. &gt;This is a performance culture where you‚Äôll be working alongside -- and learning from -- other top quality teammates executing at their best. We value boldness, ownership, transparency, curiosity and experimentation. We offer all employees competitive salary and benefits plus a fully stocked office within walking distance of BART in SOMA. &gt; &gt;If you‚Äôre excited by the challenge of winning in the fastest moving ecosystem as a member of its best team, this might be a good fit. Hyperbole ‚â° red flag &amp;#x200B;
Hey, I took a shot at modernizing the nrf52dk crate a few months ago. I ended up being able to build a working blinky example, but got stuck debugging the example that used the softdevice. If I remember, there was some call to some NVM / config component that failed, but I got busy with other projects. Branch is here, and I think I had a rust toolchain file so it could be a good starting point. https://github.com/kamathba/nrf52dk-sys/tree/crate-updates?files=1
Yeah, eh_personality missing sounds like a no_std problem to me, and ideally some combination of the cortex-m-rt and nrf52 bsp crates would address that specifically.
That's not exactly true - see for example [Box::leak](https://doc.rust-lang.org/std/boxed/struct.Box.html#method.leak). The key is that lifetimes don't have to work backward in time -- if you create something that can potentially live for the rest of the program's execution, it's `'static`. Event if it doesn't come from the binary and was created after startup.
Cargo has been able to take care of most actions along the development process...is it crazy to think that it one day might capture CI as well?
Th√© purpose of the question was to have more details than this _unscientific_ benchmark 
&gt; gillesj &gt; Interesting work. Can you detail how many run you made and the compilation options? &gt; icefoxen &gt; Tsk, not even any standard deviation listed? It's decent work, but your analysis needs more attention. I will send the questions to the author
Major soundness issues
The "data from the binary" isn't true anyway, see my reply to the parent. Lifetimes aren't so much about "how long, exactly, does something live" but "how long, potentially, can I use it" without accessing freed resources. The object may live longer, that's not a problem. So a type without references in it has a potentially unlimited lifetime, since if you take control of it, *you* decide how long you keep it alive, and that's encoded by saying it's `'static`. (So yes, I would agree that `'static` is not named very well since it evokes slightly different semantics from other languages. Something `'unlimited` would have been easier to grasp.)
I think it'll be more useful to be able to mark an entire function as `#[no_alloc]`, either unconditionally or in debug-only (while a release build could have *less* allocation due to optimization, it's unlikey to have *more*).
&gt; When you're talking about inputs and outputs are you talking about lifetimes as a tool for relating function input lifetimes to the return? Yes. On a purely conceptual level, any return value which references data it does not own must not outlive what it references. That means that, regardless of what the compiler actually implements, a function cannot return anything which references variables which exist in the function's scope. To avoid massively complicating both the programmer's mental model and the compiler's model of the code, `rustc` rules out allowing function calls to modify the lifetimes of variables in higher stack frames. That means that you've only got two options when you return something that doesn't own everything it references: 1. You're returning a reference to one of the function's arguments or something within it. 2. You're returning a reference to something like a global which is not an argument but outlives the function. Lifetime annotations are a way of telling the compiler "Of the inputs you see, *this* is the one that the output depends on" so it can either confirm or deny that the code is correct. That's why lifetime elision works so well. (In early versions of Rust, you had to provide lifetime annotations for *everything* but, in modern Rust, the compiler will do it for you in the unambiguous cases of functions with only one argument or functions where the return value completely owns everything it touches.) ...but they were conservative in implementing the version of lifetime elision released with Rust 1.0, so, last I checked, it's still an open question whether it should cover structs as well as functions. As for relationships between data inside a struct, things like self-referential borrows (eg. structs that both own something and contain a separate reference to some part of it) are a Hard Problem‚Ñ¢ that has been on the Rust devs' radar for years. (ie. The more you try to design a solution, the more you realize how many hidden challenges there are.)
Foolish me - I guess it already does with crates like cargo-make
DOS.JSN
I think you are confusing the lifetime of a value, with the lifetime of a variable. usize is 'static, since it's not tied to any particular lifetime. It can be used and moved around freely until the end of 'static. Now, if the lifetime of a value represents "how long can we keep using this value", what lifetime do Rc&lt;T&gt; have? The lifetime of T, of course. And if T is 'static (I.E. usize), so is Rc&lt;T&gt;. However when you take a reference to a _variable_ (for simplicity, let's say on stack), with lifetime 'a, that reference can not outlive the lifetime of that variable, so it itself is assigned 'a. The reference is also a value, but it's a value associated with a lifetime. It can be moved around freely, but only until 'a ends. The reference is not referring to the value, it's referring to the variable. And only static variables have the 'static lifetime, hence only references to those can themselves be 'static.
Does this provide an option to override the c allocator as well?
I feel like this is a little inflammatory and uninformed. On the surface, you have a metadata format that is well defined and easily serializable (Cargo.toml), a registry with an API (crates.io), and if you already have an algorithm for resolving dependencies and compiling/linking targets it shouldn't be that hard to deal with. build.rs throws a wrench in the gears, but it's a piece of duct tape for resolving dependencies that aren't covered by Cargo. So the perfect gap for Meson to fill. 
This is really useful! I am used to staring at mangled symbols in cachegrind. How is it different from [https://github.com/bspeice/qadapt](https://github.com/bspeice/qadapt)
Hi, First of all, don't start with xargo, it's too early. Start with rustc generating a single .o or .ll and check what you get. Correct the target specification and try again, until you get a valid object file (.o) for DOS. Then try no_std, but if I remember correctly, you'll have to write compiler support routines for your target (the ones in compiler_builtins). 
That's the translucent tree view in Xcode showing blurred things behind it. Current GUI fashion.
&gt; it's also a very unsatisfying development, that MIR didn't enable better non-LLVM bound backends. That's not true, MIR has enabled the cranelift backend.
[I reviewed QADAPT here.](https://github.com/bspeice/qadapt/issues/8) TL:DR: I reviewed QADAPT and wrote this in response as a better design, fully approved by bspeice who has/will archive QADAPT in favor of alloc-counter. QADAPT only supports enabling and disabling the panic-on-alloc, it doesn't enforce scoped isolation nor provide counters for non-panic users. It always backs off on debug builds; whereas alloc-counter puts that decision on the consumer. If you want it to only be enabled on debug builds, you decide that. If you want it in production builds, no problem.
What C allocator? If you mean the System allocator, yes, you may use any allocator you want. `#[global_allocator]` `static A: AllocCounter&lt;YourAllocator&gt; = AllocCounter(YourAllocator);`
The overheads are just that of a thread\_local Cell&lt;usize&gt; and a check if you're currently panicking (to avoid double-panics thus uncatchable aborts).
Interesting crate! I guess one would mostly use this for benchmarking and profiling right? Would you suggest using this in tests as well?
Benchmarking, profiling, tests, and even in production. It is very cheap and I might add an optional feature to log the allocation events with function scope information for more in depth analysis and real-time information. No roadmap or time schedule for that. Patches welcome. :)
&gt; but only Into for your types to make foreign types Pretty sure you can impl `From&lt;YourType&gt; for ForeignType` without violating the orphan rule.
&gt; I'm not sure if flatpak supports cargo yet. I could be wrong. I had a look and it seems like there isn't any official support for Cargo (or any other language-specific package manager). https://github.com/flatpak/flatpak-builder/issues/15 That's not to say you can't use Cargo when building a flatpak. [Someone has already done it](https://github.com/eyelash/xi-gtk/blob/flatpak/com.github.eyelash.xi-gtk.json).
Positively speaking: "ownership" is one of their core values.
Does it have the same level of peer-review as the other crates in the num organization ? 
Lifetimes are fairly simple, Traits have seemingly unlimited potential complexity.
I'm not sure why I should care. That's maybe callus, but if Meson dies off in the face of Cargo because Cargo can do everything you need from a build system, then that's a success. The author is invested in Meson, and so can have an interest in making sure it keeps existing - but if they want that to happen, they should give a reason for it to exist, and why to not just use Cargo other than it harming their livelyhood.
Maybe they're referring to the allocator of C code staticly or dynamically linked into Rust via FFI? Seems like it'd be hard to replace that but a reasonable question.
No I mean the allocator used by c code. The equivalent of the unprefixed_malloc_on_supported_platforms feature on jemallocator
My thoughts as well. LLVM is pretty good so it would be pointless to reimplement its optimization passes. However, it would be useful to reduce the MIR size so that LLVM has less work to do and thus speed up compilation.
| like who the author of futures was, etc. glad we agree it's minor. I did not do a proper analysis of who the actual author is because i don't think it's a relevant fact. It's roughly the same group. Sorry if someone feels blamed. Not the intention here. | It‚Äôs talking about futures 0.1 and tokio yes. about about async later on, but i haven't provided concrete evidence of how that is broken, you are absolutely correct. I can in full length as i already have numerous times, but If the only allowed evidence is theoretical language construction, my argument will always be weak against "It can be done in theory" Hence i feel the better alternative is to move along and maybe agree to not throw too many stones in each others way. One of the low hanging fruits is not emitting a future from async but a generator. It'll reduce design dept and make async universal. 
Yeah, for a language that focuses heavily on scientific computing and has performance as a somewhat secondary goal, these numbers are really impressive.
Please include location in the title. Mods, is this something we should enforce?
No. I'm not doing any linker overrides to become the allocator in C-land.
It is a useful feedback, but it could have been more useful if you addressed which parts specifically looked like a hyperbole. When it comes to the team we actually do think we have a world-class team. If you check [the composition](https://nearprotocol.com/) you will see that the majority of our engineers are at the minimum ICPC/TopCoder finalists or have 10+ years of experience and worked at companies like Google or FB. This is far better than what is in Google or FB, and no one has problems with Google calling their engineers the best of the best. Regarding the investors, our investors are top in crypto. Crypto ecosystem is also notoriously fast-paced. And our office is indeed one block away from BART, it is not an exaggeration :) &amp;#x200B; &amp;#x200B;
i don't want to devalue all those more experimental an rather interesting alternative approaches, but just reusing well established compiler infrastructure -- i.e. also supporting GCCs GENERIC/GIMPLE IR instead of still depending mainly on LLVMs IR --, would make rust a much more attractive solution in the case of embedded software development. sure, cranelift and other more native rust based alternatives could perhaps open very interesting alternatives in the long run, but right now, they are definitely no solution for this particular purpose... it's really a shame, that rust, which would otherwise a fantastic language for this kind of work, doesn't handle this obvious issue in a more satisfying manner.
That was insightful. I wish cache could be as easy as a macro or annotation in rust and in debug mode calculate this automagically.
It is in San Francisco, USA. Sorry, reddit does not allow me to edit the title.
My last project failed: I tried to extend my Python Portal with rust libs, but this ended in a frustrating story of ‚ÄúUpdating of my python version don‚Äôt work with my current or rebuilt rust extension‚Äù. After a few thoughts and making a game and some stuff in plain C i decided to rewrite my Portal in plain C. After a good developement state i remembered rust and his GUI Situation. So i will let the GUI in plain C and will exclude the application parts into rust. First steps is becoming familiar with the FFI to c and find best practices. So i write a lots of tests to check each FFI Situation i need.
See [my answer](https://www.reddit.com/r/rust/comments/anikw6/looking_for_rust_network_engineer/efuwm7d) to [firefrommoonlight](https://www.reddit.com/user/firefrommoonlight).
It‚Äôs still early days. This is the best lace to get up to speed on the status quo: http://areweguiyet.com
Are you suggesting that Meson should ship its own implementation of cargo?
Azul looks cool. I'm thinking it might get hairy to use when you need to either create a new view or style it with bells and whistles on the css or both (say, add tween animations to a ribbon component or something), but that's a question of examples...
It's a build system, should it not be able to build the languages it supports? And like /u/Holy_City said, meson basically only needs to download from crates, it should already be able to manage dependencies. In my uninformed opinion, yes they should include it if they want to build rust, maybe as a plugin
DOS~01.JSO
&gt; Anyway, long term I'd rather build C with cargo than build Rust with something from the C ecosystem. What about other languages? Should cargo fcompile, locate, link, download, dependencies from other languages? I don't see how this kind of duplicated effort would benefit any language project.
I dunno about the Meson people, but I would hate to have to maintain an independent reimplementation of something non-standardized like Cargo, I'd have to constantly reimplement every new feature or change they release to maintain compatibility, or else get yelled at for fragmenting the ecosystem or whatever. I'd be the IE6 of build systems.
Have you seen [salsa](https://github.com/salsa-rs/salsa/)? There's a really good video introduction [here](https://www.youtube.com/watch?v=_muY4HjSqVw).
[John Regehr](https://blog.regehr.org/archives/1122) has done a lot of work on this subject.
&gt; This however then would mean that the master branch (prior to 1.0.1 being released) would contain a more up to date version than crates.io. Why would that be an issue?
Make a dev branch. Make pull requests against the dev branch. When the dev branch is merged to master, update the version number and push to crates.io.
Could we say it is the 2nd language that implement explicit ownership model after Rust?
I don't really know about cargo, but to me it seems like building a language should be standardized if it's very complicated. Maybe I am missing something, but assuming Meson has to call rustc anyways, they can use it to do the heavy lifting. It's not like cargo.toml files are extremely complicated. I suppose build.rs is a real problem, because it can do stuff the build system cannot anticipate. If it's written with only cargo in mind, it will not run. This would result in crates that meson cannot build, thus fragmentation. A big problem indeed...
&gt; Switching to Meson for a configuration language sounds fine for that. As long as it doesn't hurt the usability of the common case Cargo currently implements.
It's not a **huge** issue, and for some maintainers common practice to directly make a commit incrementing to the next version after publishing a release. However it creates the problem that you can't patch a dependency in your Cargo.toml with a git dependency that e.g. has a bug fix that is not released yet by the maintainer, as cargo only allows patching with the same version number. There has been more than one occasion where I had to fork and decrement the version again to be able to patch a dependency.
Is there a compiler flag to find out how much bytecode my project is generating when it compiles? I am curious to see. 
build.rs is really hard to deal with since many use for building c dependencies or linking against system version of those dependencies. Choosing how to deal with these non-rust dependencies is something that is pretty well defined in a meson build file.
That's why there is `links` key in `Cargo.toml` and cargo supports overriding output of build scripts and supplying your own information about how to link to C libraries. Meson could do the same, i.e. if it sees `links = "bzip2"`it would not run the build script and resolve this dependency itself.
But is that used extensively or somehow enforced?
Alright, I didn't look into it carefully. But it seems to me that a build system that wants to be 100% declarative will have problems encode `build.rs`. Another misuse of `build.rs` is to generate new rust code that will be compiled (potentially from other input files). How will meson know these dependencies so that it knows when to rerun the `build.rs` executable?
Salsa looks good for apps that need it (because it - attempts to - solve the 'minor change alters cached 'larger struct', everything is to be thrown away). Yes, salsa would 'probably' avoid the need to care about this formula unless you're doing something completely silly like cache a generator and your miss rate is 100%. It's also much more complex than slapping a cache annotation on a immutable or copy value.
Thank you. I played a bit and found the following way. I think this gives the same outcome right? let db_conn = establish_connection().unwrap(); let error = get_trade(&amp;db_conn, 1).unwrap_err(); assert_eq!(error.kind(), &amp;ErrorKind::DieselResult(diesel::result::Error::NotFound));
I think this would be a great contribution to the Rust ecosystem if it doesn't exist already.
What I would do is determine the precision from the float_by argument, and then use it when outputting the count. Something like this: fn precision_of(s: &amp;str) -&gt; usize { let after_point = match s.find('.') { // '.' is ASCII so has len 1 Some(point) =&gt; point + 1, None =&gt; return 0, }; let exp = match s.find(&amp;['e', 'E'][..]) { Some(exp) =&gt; exp, None =&gt; s.len(), }; exp - after_point } Inside `main`: // Load the CLI arguments let opt = Opt::from_args(); let precision = match Opt::clap().get_matches().value_of("count_by") { Some(count_by) =&gt; precision_of(count_by), None =&gt; 0, }; Inside the loop printer: env::set_var("COUNT", format!("{:.*}", precision, actual_count)); That way, if someone calls with `--count_by 1.1` they get 1.1, 2.2, ..., and if they call with `--count_by 1.10`, they get 1.10, 2.20, ...
Why do you call it a misuse? What is wrong with using `build.rs` for generating code? I did it in my projects and always assumed that it is proper usage of build scripts. &gt; How will meson know these dependencies so that it knows when to rerun the build.rs executable? Probably the same way cargo does. Correct build script should output `cargo:rerun-if-changed` for every input file that it reads.
I've always found it annoying (and I'm not talking about Rust specifically) that there at all is an assumption that the compiling machine also is the running machine. Is the reason that compiler toolchains would take up too much space if they included support for all architectures? &gt; including a couple of common libraries such as OpenSSL Are crates not downloaded and compiled on the fly, as they usually are, when you cross compile? Nice display project!
OK, sure it is the intended use of `build.rs`, but I call it misuse because it is opaque to other build systems. If cargo where a proper build system it would contain a line saying `x.rs.in -&gt; x.rs` that could be parsed without execution permission. Instead of implementing the typical use cases in a declarative way, cargo allows the build definition to contain and execute arbitrary code. This is insane.
&gt; runtime checking
any offline but up to date copy of the rust book I can download for offline reading? I can see a lot of outdated links for older versions, and I know you can pay $40 for an e-copy at no-starch. I'm learning the language in my spare time at the moment with no real concrete goal (except to level up as a coder) so a bit hesitant to do so. Also on a sketchy web connection and my efforts to install rust and mdbook and clone the git repo so far have not been that successful, heh.
I‚Äôve only seen this for json documents not structs yet. An alternative would be to use structurally shared persistent data structures like im.rs which would give undo/redo for ‚Äúfree‚Äù. 
I've been somewhat frustrated by the S3 FUSE implementations (there are two of them). I wish there was a Rust option, but I'm familiar with neither S3 nor FUSE, so I'm probably not going to try it.
&gt; Is the reason that compiler toolchains would take up too much space if they included support for all architectures? We're talking about C compilers here. It would be hard/strange for Rust to distribute a C compiler for many architectures along with itself. As far as Rust goes adding a new target is as simple as `rustup target add arm-unknown-linux-gnueabihf`. &gt; Are crates not downloaded and compiled on the fly, as they usually are, when you cross compile? Yes Rust crates are cross compiled fine. It's C libraries like OpenSSL that some crates depend on that aren't so easy. These need to be cross compiled/available for the target architecture too. 
Check out r/programminglanguages
Meson lags behind CMake in building C/C++ projects. It‚Äôs difficult to expose build artifacts for them to be consumed later in the build. Meson has a worse cross-compilation and build generation and is not supported by the major IDE‚Äôs. Adding dependencies is actually easier in cmake, and with the mew package managers (conan, vcpkg), it even becomes easier. Meson provides it‚Äôs own DSL which although being not turing complete and more palatable than cmake, is prone to the same issues as cmake. So that‚Äôs why cmake is becoming sort of the de facto standard build system at least for C++. The thing that irks me with modern C/C++ build systems (cmake is relatively old) is that they try to support several languages out of the box. Meson and build2 come to mind. Why not improve the situation for the main language you‚Äôre supporting rather than targetting other languages which have an established build system +/- package manager. If Meson doesn‚Äôt want to support crates, they have no business targetting Rust. Cmake makes no such claims and is still better for building C++/Rust mixed code bases! Sorry for the counter-rant!
If you use Rustup and have the `rust-docs` component installed then the Book is already on your system. You can run `rustup doc` in the terminal and it'll open [the docs index](https://doc.rust-lang.org/) (of your local install) in your default browser. If you don't like Rustup then it should be in [the standalone distributions](https://forge.rust-lang.org/other-installation-methods.html#standalone) as well. Inside those packages it should be under `share/doc/rust/html/book`. Historically it was actually possible to navigate https://static.rust-lang.org/dist and find tarballs of the individual Rust components so you didn't have to download an entire distribution just to get docs. However since the site revamp it 404's instead.
Oh, C! Then, I must not have fully understood the situation. So, would the problems be out of the way if support for all architectures were included in the C toolchain (or could be added as easily as in Rust)?
If you want to see if a crate is in the num family, the way to do it is to check the table in the README of the num crate. There already are other num-* crates, num-cmp and num-digitize, and some num_* crates, num_alias, num_cpus and num_enum, and there's bound to be more as time goes by as num is such a broad qualifier.
&gt; So, would the problems be out of the way if support for all architectures were included in the C toolchain (or could be added as easily as in Rust)? Yes üëç
Not actually. I've spent around 1 hour to make `azul` work. It was quite easy, but it lack very basic functional, such as `make window topmost`. Lack of documentation and mediocre support/issue resolution are here as well. Then I spent around 6 hours to make work `gtk-rs`, but it's not here. I had multiple issues with linker, that was fixed by manipulating env variables and cargo cache. Now I just can't run it because of issues `entrypoint in dll not found`. Devs support is much better here, I've got a solution for the first part of the problem - i.e. "doesn't build", but it's not working yet. No other popular GUI frameworks found. So, my experience here is "No UI yet". Maybe it's just my windows-only problems, as I see that most of help is "apt-get this and that.. Oh you have windows? Sorry, no support can we provide then". But they are here.
Neat, thanks for sharing!
&gt; Instead of implementing the typical use cases in a declarative way, cargo allows the build definition to contain and execute arbitrary code. This is insane. Historically, using `build.rs` to generate code was a workaround for procedural macros not having been stabilized. Now that procedural macros have been stabilized, the same ability to run arbitrary code at compile time exists within the language itself to implement things like generating code from schema files or other DSLs. How is using `build.rs` to rewrite source files any different from how various C and C++ projects call tools like Flex and Bison, aside from Flex and Bison having enough seniority to have predefined helpers? Do you really want your build system bloating out with logic to understand Flex/Bison definitions, the various flavours of XML and JSON schema definitions, API definitions like WSDL, and every other domain-specific language that a project might want to generate code from? Heck, the fact that `build.rs` only runs *before* the code is compiled is one of the reasons that people wrap Cargo up in Makefile-like tools like [just](https://github.com/casey/just) and [cargo-make](https://crates.io/crates/cargo-make)... so they can run arbitrary stuff *after* build.rs.
The rust-qt binding generator (mentioned elsewhere as well) works great for Windows (at least, I had no big issues, I also think it's quite nice, even if you're not a Qt/C++ nerd)
See https://www.reddit.com/r/rust/comments/7zexrs/programming_language_written_in_rust/. It's a tad older, there certainly are some newer things, like https://www.reddit.com/r/rust/comments/9eprbz/programming_language_parser_and_interpreter/. I also remember someone writing a python interpreter, and some sort of org or website where those things are collected (but I can't find it anymore :().
The way I've always heard people say to implement it in other languages is, rather than diffing to derive undo/redo after the fact, define operations as bidirectional "commands" and then just apply the command to both the working data and the undo/redo stack rather than operating on the data directly. (ie. Modify the data by "redoing" something that has yet to be undone, rather than trying to retroactively describe the modification.) That said, I can see why that would be somewhat unintuitive for a graphics editor, which needs to be performant and to batch up incremental changes from something like dragging a brush around.
Thank you so much.
Thanks for an idea. I'll try that.
That's one of the things I wanted to work on. But I'd really like to have fuse-tokio first.
You can use `rustc --emit llvm-bc` for that. Something like `cargo rustc --emit llvm-bc ...`
I only know the saying as "Apache Incubator, where projects go to die". (Because Incubator projects are not protected by Apache and often don't build enough steam to be adopted. Like Google-&gt;Apache Wave.)
The reason for that is that this issue is much less obvious than you think. First of all this is already enabled by MIR, but integrating with GCC is a ton of work nobody has stepped up to do. There seems to be relatively little interest in obscure architectures that only GCC supports (other than the occasional "this would be nice I guess" comment). Then there might also be licensing issues because GCC is GPL - can you even use it as just a backend, or would you effectively have to reimplement the entire Rust frontend (parser, macros, type/lifetime inference, borrow checker) in GCC? And finally, it would probably be more useful to write more LLVM backends for those architectures instead of messing with GCC. That way all languages using LLVM would benefit. (regarding specifically ESP32, people were talking about an official LLVM backend being worked on, but there only seems to be a proprietary one now? anyways, the fault here lies with Espressifs unwillingness to communicate, because I'm pretty sure I'd have started to work on a backend if people weren't talking about a backend already being worked on)
Cool :-). &gt; Also the big issue with doing a good S3 fuse filesystem is caching and cache invalidation. `s3fs` "caches" the full files: if you read a 1 MB chunk out of a 1 GB file, it creates a 1 GB file, downloads the requested chunk, writes it to disk, then returns it to the application. It was fun debugging why reads were failing and why `/` got full.
One thing that makes getting these numbers tough is that this feature has been moving quickly, so nightly-only stuff gets outdated. tracking it all down is tough. Regardless, I've done it. So, [https://github.com/polachok/fahrenheit](https://github.com/polachok/fahrenheit) is a toy executor that's trying to emulate tokio. It's still not really written for size; it assumes you have an OS, so it's also a bit bigger than you might do for true embedded. Its server example ends up being \~300k. I also tried [https://github.com/mgattozzi/async-await-class](https://github.com/mgattozzi/async-await-class), which doesn't work with the OS, and is pretty basic. It's a bit outdated (\` nightly-2018-08-18 \`), also not written for size, but it ends up being \~155k. Finally, [https://github.com/Nemo157/embrio-rs](https://github.com/Nemo157/embrio-rs) is explicitly for embedded. Its "hello" example weighs in at \~38k. So, yeah.
And what does it do to invalidate that cache?
&gt; Having a build system whose only task is to call a different build system is useless. It is nothing but bloat that should be deleted. No!!! This is great! Not useless! I like how [Fractal uses Meson](https://gitlab.gnome.org/GNOME/fractal/tree/master), all the Rust code is just built with Cargo, and Meson does everything else. And there's a lot of that. Checking dependencies, building gettext localization, gtk resources, gsettings schema, templating desktop and appdata files, installing the whole thing into the system.
The [interpreter book](https://interpreterbook.com/) walks through writing an interpreter for a new language called Monkey in Go. I (as well as many others) have gone through the book implementing the code in Rust instead of Go, which I really enjoyed. The repo containing my resulting code is below, but I'd highly suggest buying the book and working through it yourself. https://github.com/JoshMcguigan/monkey
&gt;and some sort of org or website where those things are collected Could that be http://craftinginterpreters.com/?
In case of writes, or when it expires? I haven't look into it, the files in that bucket were read-only. I would prefer it not to cache anything than save everything to disk. That behaviour is not even documented: &gt; -o use_cache (default="" which means disabled) &gt; local folder to use for local file cache. &gt; -o ensure_diskfree(default 0) &gt; sets MB to ensure disk free space. This option means the threshold of free space size on disk which is used for the cache file by s3fs. s3fs makes file [sic] for downloading, and uploading and caching files. If the disk &gt; free space is smaller than this value, s3fs do not use diskspace as possible in exchange for the performance. &gt; &gt; If enabled via the "use_cache" option, s3fs automatically maintains a local cache of files in the folder specified by use_cache. Whenever s3fs needs to read or write a file on S3, it first downloads the entire file locally to &gt; the folder specified by use_cache and operates on it. When fuse_release() is called, s3fs will re-upload the file to S3 if it has been changed. s3fs uses md5 checksums to minimize downloads from S3. &gt; 
&gt; nobody has stepped up to do True. I am considering stepping up myself. &gt; can you even use it as just a backend Yes. There is no need to reimplement Rust frontend. &gt; it would probably be more useful to write more LLVM backends I disagree. Writing one LLVM backend for one arch doesn't help at all for other archs.
I think the biggest question here is "how to create a programming language", using Rust would be more of an implementation detail. I've personally learned a great deal from this course: http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=Compilers. It suggests using Java or C++ as language to implement a compiler in, but you should be able to follow along using other languages as well (I did this in Haskell). Another awesome resource is programming language zoo: http://plzoo.andrej.com/
Maybe see https://github.com/Rantanen/intercom?
I think this thing is called *bitcode*, in the context of LLVM
i think its a great path they did a great thing on Lucene and i hope they do the same
Oh, awesome. Thank you so much. 
Sorta; if you don't use it, you may run into errors and/or bugs. So in practice, people do use it. At least as far as I've seen.
I've been slowly porting the [rust foundationdb bindings](https://github.com/bluejekyll/foundationdb-rs) to use the new futures API. The new API is [so much more straightforward as a result](https://github.com/bluejekyll/foundationdb-rs/compare/master...josephg:master#diff-cbbc248ddbadc0abad2be764b7018068). I still don't have range queries or watches working yet. And there's a huge lump of unit tests that I haven't touched yet, which need some lovin'.
How much you know about building languages? And about rust? To compile to what?
Hey guys, If you are using sqlite to store a local database, what would you choose as the location for that file? Same location as the binary? User directory? Documents?
I was thinking that there was something rust-specific, but might be mistaken.
&gt; Historically, using build.rs to generate code was a workaround for procedural macros not having been stabilized. Now that procedural macros have been stabilized, the same ability to run arbitrary code at compile time exists within the language itself to implement things like generating code from schema files or other DSLs. This is fantastic. &gt; Do you really want your build system bloating out with logic to understand Flex/Bison definitions, the various flavours of XML and JSON schema definitions, API definitions like WSDL, and every other domain-specific language (and implementation thereof) that a project might want to generate code from? flex/bison and similar code generation tools are actually feasible to describe with build rules. You first describe how to compile the compiler and then you describe which files are generated and optionally what the source files are. You don't actually have to *execute* anything at configure time to figure out which files will be created and the ordering. It is very hard to define what the program built from `build.rs` will do because it does arbitrary things. Obviously flex/bison could do arbitrary things as well, but in practice it does not. It follows a nice workflow that is easy to integrate into existing build systems. &gt; Heck, the fact that build.rs only runs before the code is compiled is one of the reasons that people wrap Cargo up in Makefile-like tools like just and cargo-make... so they can run arbitrary tooling and code after compilation build.rs. IMHO any mature project should have tooling that calls cargo (like rustc and servo). I'm afraid that we are going in the wrong direction of adding features upon features to cargo. I don't think it is a good idea to let cargo handle for example creating releases of code. I would like rustc/cargo to integrate better into regular build systems because I think that will increase adoption in multi-language projects.
Niko made a few videos walking through how salsa works if that helps. &gt; - [How Salsa Works](https://www.youtube.com/watch?v=_muY4HjSqVw&amp;t=471s) &gt; - [Salsa In More Depth](https://youtu.be/i_IhACacPRY) (thanks @**Jonathan Turner** for this)
Here you can find C based example using LLVM: https://llvm.org/docs/tutorial/ And I have the playground https://github.com/jinnzest/rust-llvm-sys-playground to generate an executable using rust binding to LLVM. 
&gt; I think as time goes on, we will continue to see convergence between web technologies and native user interfaces, **as there is a large overlap in problem-space.** from a design perspective, absolutely, but not in terms of runtime. browsers have to be generic enough to render any conceivable ui layout, whereas 99% of user interfaces (including websites lol) have fully prescribed layouts and state graph and barely need any magical layout engines or anything. i can't speak for azul, but imgui and dom in the same sentence seems like a massive contradiction to me. at least you can use it without javascript i guess. full disclosure: i think the web is a disaster and am disturbed by the trends set by electron and vscode etc, despite limited experience working with said technologies. i have all my eggs in the wasm basket so i can just write guis using manually computed coordinates relative to the window size. in my experience, as soon as you try to do something remotely non-trivial with a layout api you end up fighting it far more than it helps you. 
In line with 'I think the biggest question here is "how to create a programming language", using Rust would be more of an implementation detail', I'll suggest a couple more links: * https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours * https://compilers.iecc.com/crenshaw/
Swift does a lot of static checks too. The runtime checks are in ~essentially the same place as Rust requires them (although those cases come up more in Swift): shared ownership with reference counting, global variables, etc. (I believe the one major place that's different is closures that escape up from the stack frame that declares them.) The [first two examples](https://swift.org/blog/swift-5-exclusivity/#examples) demonstrate static compiler errors.
Might be just me, but I could not get any of the videos on the Stanford course to play. 
Not only does rustc have access to more information, but it can also optimise generic code before monomorphisation, meaning optimising/improving it once, rather than having to redo basic work for every instantiation. It's true that it's still important to optimise after monomorphisation, but there's likely lots one can usefully do before (e.g. sections of generic functions that aren't using the generic parameters directly).
&gt; they try to support several languages out of the box. This. Everyones wants the perfect universal One True Build System For Everything‚Ñ¢ and it just never works well, and I doubt it even can, and it just makes everyones lives harder. Poorly supporting a bunch of languages instead of properly supporting the target language. Theres nothing wrong with having a good system for your languages ecosystem. Can't cater to everyone's needs.
Yes I will 
ah, that's it! thank you, i'll bookmark this :)
the details are all in the github link. the benchmarks are pretty solid, you are just complaining about what he posted directly on reddit. 
To explain why this article is relevant to this sub, since it's not immediately obvious from the title, `reserves` appears to be a command-line tool written in Rust for generating and verifying proof-of-reserves for funds in the Bitcoin network.
I wonder if this technique could be combined with `sizeof` to avoid this problem?
@sanxiyn if you are really willing to spend you time on this challenge, it's definitely worth the efforts! it's indeed not just the missing support in LLVM for a few exotic platforms, but more the freedom to choose your preferred tool. in case of software development for embedded solutions, most professionals will most will prefer the GCC toolchain, even in cases, where LLVM would be available for the given platform as well. being forced to change essentials tools and mature development environments, just to be able to use a nice programming language like 'rust', should be seen as similar unacceptable as nobody wants to be forced to change his accoustomed IDE or build system just because of more or less 'political' constraints. nevertheless i see the actual progress -- e.g. all the recent refactoring and code cleanup in librustsc\_codegen\_\* --, which looks indeed promising in this respect, although many public debates and individual development efforts still do not pay much attention to essential pragmatic requirements like this. 
I wish these attributes will be added to the Rust itself.
From what I can tell, there are only 2 sentences in that description that I wouldn't count as "hyperbole", so it's probably shorter to list those: &gt; We value boldness, ownership, transparency, curiosity and experimentation. We offer all employees competitive salary and benefits plus a fully stocked office within walking distance of BART in SOMA.
I agree that cargo should integrate better into larger projects with their own build systems. I just think that expecting everything to be fully declarative is a fool's errand that leads in the direction of academia-esque solutions which have beautiful theoretical properties, but few people use them because they line up poorly with what they actually need to do efficiently and effectively.
`#[no_panic]` doesn't seem to work, there is a bunch of panics that it didn't catch. Some fake that it did. I think it needs more polishing before actually becoming useful.
Cargo can't do everything you need from a build system unfortunately, which is why e.g. https://github.com/rust-lang/rfcs/pull/2136 exists. Specifically, Cargo is not very useful for anything that needs to integrate with other build systems yet, that needs to build code from other languages (`build.rs` is not really a scalable solution here), that wants to run other tools (e.g. a documentation tool for a C API that is exported from a Rust library), or even just anything that needs to install more than an executable (data files, libraries, ...).
I also agree that everything cannot be fully declarative because yes that is not flexible enough. I think Meson is in the class of "pragmatic" and not "academic" solutions. It has "custom" outputs and it has "generators" as core concepts which gets you pretty far. You basically need to define the program to run and what the output/inputs are. Since it also has support for arrays, dictionaries and for-each loops you can create pretty powerful build definitions. Meson also has a modular `dependency` resolution which means it can find dependencies using `pkg-config`, `cmake` or from other ecosystems. I understand that for the vast majority of the rust projects cargo with build.rs is good enough. But I think it is a real problem that `build.rs` calls out to `pkg-config` with hard-coded flags and does arbitrary things. It is not a friendly behavior towards the overall community. It is a problem that meson depends on python, but on the other hand I also believe that that has enabled their super fast evolution, so it is a trade-off.
Are you compiling with \`cargo build --release --target thumbv7em-none-eabihf\` ? If yes, can you provide a full repo that you are trying to compile?
Hi again, I just published the current unfinished prototype that I have currently in my hands: https://github.com/rask/saha
I would use [https://crates.io/crates/dirs](https://crates.io/crates/dirs) to choose a particular directory; probably [https://docs.rs/dirs/1.0.4/dirs/fn.data\_dir.html](https://docs.rs/dirs/1.0.4/dirs/fn.data_dir.html)
Trying to fix the HTTP signature verification in my program 
Great! Thanks!
I was thinking more that it would print the name of the function and the counter stats on return.
Me neither, and from looking at the page source it includes a `swfobject.js` script, so it might require Flash to work
My crate [inkwell](https://github.com/TheDan64/inkwell) is a safe wrapper over LLVM and may be helpful too
&gt; Theres nothing wrong with having a good system for your languages ecosystem. Well there is if you ever want to do multi-lang projects, like replacing C with Rust in an iterative manner. Now you have to trust that every single language does dependency tracking, caching and everything else properly instead of using a common framework for this. There seem to be a lot of reinventions of the wheel. &gt; Can't cater to everyone's needs. Why not though? It is not like cargo is doing anything new. It is simply a reiteration of other similar tools.
Maybe https://docs.rs/treediff/3.0.1/treediff/ can help. It does support merges. Disclaimer: I am the author.
C is easier to learn and get started with, but difficult to master as it's littered with undefined behavior, and a weak type system. Requires quite a lot of typing as well, since the number of features are limited. Rust is harder to learn, but easy to master. There's strong guarantees about code that compiles. The many modern concepts enable more flexible and expressive abstractions.
It's funny you ask that. I'm a TA for a course where the students do just that. Here are some of the resources that the professor recommended: * Modern Compiler Implementation in ML, Andrew W. Appel (Appel shows everything in ML, but the concepts still apply) * Types and Programming Languages, Benjamin Pierce * [Crafting Interpreters](http://www.craftinginterpreters.com/contents.html) This is for a compiler specifically, however a compiler is basically just an interpreter with extra steps. If it's for educational purposes, I'd definitely go this route since you can learn a little more from it than from writing an interpreter. &amp;#x200B; This field is a very popular one, so information is pretty easy to find, especially once you know the vocabulary. As others have mentioned, there are courses from various universities that were made public that go over this topic as well.
&gt; - i.e. also supporting GCCs GENERIC/GIMPLE IR instead of still only targeting/depending on LLVMs IR --, Quoting myself: &gt; That's not true, MIR has enabled the cranelift backend. That is, there is a rustc backend that uses Cranelift, and has been enabled by MIR. This backend does not use LLVM. If you want to lower MIR to GCC GIMPLE, write a backend to it. Just keep in mind that GCC GIMPLE has been designed to make doing that as hard as possible, which is why nobody wants to work on that. 
*Hey just noticed..* it's your **5th Cakeday** matklad! ^(hug)
What is the etiquette for writing in these dirs?
I'm not sure; I don't think there's any special rules.
Yeah... I succeded with creating an object file and 16 bit mode doesn't work. $ objdump -d main.o main.o: file format elf32-i386 Disassembly of section .text.main: 00000000 &lt;main&gt;: 0: 50 push %eax 1: 66 c7 44 24 02 00 00 movw $0x0,0x2(%esp) 8: 66 8b 44 24 02 mov 0x2(%esp),%ax d: 66 c1 e0 01 shl $0x1,%ax 11: 66 89 04 24 mov %ax,(%esp) 15: 66 8b 04 24 mov (%esp),%ax 19: 66 89 44 24 02 mov %ax,0x2(%esp) 1e: eb e8 jmp 8 &lt;main+0x8&gt; All this 0x66 prefixes... it is 32-bit code working with 16-bit values.
It‚Äôs unclear to me how to get started. That is, what are the highest-value tickets to work on, etc. This is _absolutely not_ a knock against you: project management is very different from writing code, and it‚Äôs a lot easier to build a system if you‚Äôve done similar-ish things in the past. For us newbies it‚Äôs all somewhat overwhelming :)
Update: in the meantime, people have sorted it out, see [https://github.com/rust-lang/book/pull/1803](https://github.com/rust-lang/book/pull/1803) and [https://github.com/rust-lang/book/pull/1804](https://github.com/rust-lang/book/pull/1804) That's of course only for this particular issue, but it's still great!
Hi, I'm the writer of the benchmark codes. For different languages, I try to utilize corresponding mainstream benchmark methods (except C). You can check out [https://github.com/astrojhgu/adaptrapezoid\_benchmark/blob/master/Benchcmd.md](https://github.com/astrojhgu/adaptrapezoid_benchmark/blob/master/Benchcmd.md) for detailed benchmark framework. &amp;#x200B; For C, I'm not sure which benchmark lib is suitable, so I simply let the program loop 100 times and use terminal command `time` to time the elapsed time. &amp;#x200B; The number of loops is determined by the corresponding benchmark lib automatically (except Rust, for the default loop number need too much time). &amp;#x200B; About the compilation options, I tried my best to use the most optimized options. Taking C as an example: -O3 -ffast-math is used. If you know any options that can improve the performance please tell me. &amp;#x200B;
What are the plans for sandboxing on Windows? Is there really any way to accomplish something similar? The tools are different, but I know there is similar functionality with layers like FS Filters and other windows middleware.
Almost all of it; it's also vague.
Thanks
I see thanks for the advice 
https://github.com/Nemo157/embrio-rs Why would you link to a repo with not one word in the readme? 
It's available here, https://lagunita.stanford.edu/courses/Engineering/Compilers/Fall2014/about You need to register an account to use it. I am currently enrolled. 
My standard metric: If nobody would ever say the opposite, it's useless. - "Early member": people might say "well-established", so this is useful - "world-class team": people never say "join our shitty team", not useful. - "significant upside": you won't advertise "neglible upside", "significant downside", not useful. - "completely reshape the landscape of the web for decades to come and potentially affect billions of people": "the status quo for a few days, affecting dozens of people", not useful etc. 
Great point. You can see the current list of Rust issues here: [https://cwiki.apache.org/confluence/display/ARROW/Rust+JIRA+Dashboard](https://cwiki.apache.org/confluence/display/ARROW/Rust+JIRA+Dashboard) They are not prioritized because different people care about different things. My personal motivation right now is expanding DataFusion but others are working on low level optimizations to the core Arrow code (which is great, because DataFusion will automatically leverage those optimizations). The best thing is to join the developer mailing list I guess and start asking questions.
Talking about culture, specially talking about expecting performance, and those hyperbolic characteristics is a huge red flag, it's probably extremely forced propaganda bullshit and micro-managed. Is it bold to stay after working hours without pay? Is aderal good enough for performance? &gt;The opportunity we are tackling will completely reshape the landscape of the web for decades to come and potentially affect billions of people. This seems like my fake company slogan for the entrepreneurship class. I'm sorry if I'm being rude, but seriously, it's such a sad state of affairs. Not only for the doublethink and abuse of workers rights (I've seem people be ironically applauded for leaving at their hours at places like these).
Thanks for giving giving context to satisfy the on-topic rule. :)
No, the reason you can't is because From and ForeignType are both outside your crate.
OK, I managed to solve this by tokenizing the `TypeGenerics`, and parsing it as[`AngleBracketedGenericArguments`](https://docs.rs/syn/0.15/syn/struct.AngleBracketedGenericArguments.html), but this is not super efficient and feels kind of hacky so I'm still looking for a better solution.
Just as a comparison, I wrote a python version that uses [numba](http://numba.pydata.org/) (jit compiles python using LLVM). The pull request to the repo is [here](https://github.com/astrojhgu/adaptrapezoid_benchmark/pull/7). With minor changes, I get 228 ms for the sorted solution and 130 ms for the unsorted solution, which is comparable to the Julia generic solution on my machine, which gets 237 ms and 91 ms, respectively for the same two calculations. Numba is more limiting that Julia or another language since it only handles a subset of python, but can produce fast code when dealing primarily with vector and scalar calculations. 
Regarding #1: In the case that you need a `String` as the parameter (like when you need to store it in a struct that owns it), You have two good options. Option 1 is to only accept a `String`, and if the caller wants to pass in a `&amp;str`, they just have to call `.into()` or similar on it. This is the simplest option. Option 2 is to accept `Into&lt;String&gt;`. When you do this, passing a `String` will just pass it normally, and passing a `&amp;str` will clone it just like the caller would have had to do in option 1. This is a little more ergonomic, but comes at the cost of simplicity and also will increase your binary size a little from monomorphization. I wouldn't recommend using `ToString` for this purpose. `ToString` is implemented for all types that implement `Display`, meaning you could pass all kinds of things into those functions, like `f64`, `char`, and error types. On the other hand, `Into&lt;String&gt;` is for things that are already text representations, which is mostly just `&amp;str`.
&gt; Now you have to trust that every single language does dependency tracking, caching and everything else properly instead of using a common framework for this. There seem to be a lot of reinventions of the wheel. I can flip that around the other way. "Now you have to trust that build system X has a mature understanding of the relevant ins and outs of every single language." &gt; Why not though? It is not like cargo is doing anything new. It is simply a reiteration of other similar tools. Because any given project will have finite resources and "get people from project Y to deprecate it and start helping on project X" generally isn't a feasible option. The more languages you try to support, the more your resources get divided.
Not an answer: how many of these enums are you going to have? I would assume that all the strings you are going to have will consume much more memory than your hypothetical reduction in size of the enum will save...
[winapi-rs](https://github.com/retep998/winapi-rs) contains some COM interface declaration. It seems to use multiple macros for the definitions (e.g. RIDL: https://github.com/retep998/winapi-rs/blob/58cd7e507874f5b4e947bf4f3e1e8aa8d077c8f4/src/macros.rs#L131). An example of how the declarations are written: https://github.com/retep998/winapi-rs/blob/master/src/shared/d3d9.rs
&gt; The attribute is useless in code built with panic = "abort". There goes the main use case?
I think one of the reasons LLVM and the language server protocol are successful is that you have a lot of code reuse between projects that would have been unrelated otherwise. As long as you are compiling to machine code and don't have a runtime the problem is not impossible. It may be hard and have a lot of edge cases, but in the end it should be possible.
I don't know how many I'm going to have, I just did some digging around and thought "gee this looks really wasteful, I'm using 16 bytes to store size information that i could easily store with just 8 or potentially 4 bytes." And wanted to see if anyone had a solution to this, as strings are a pretty common thing to use and we could get a memory savings of between 30% and 50% on all uses of it
I have not yet had the time to check the difference in generated codes. However, the C version is written almostly bottom up from malloc. So the difference may exist in the memory management (e.g. the strategy of how a vector is expanded).
How have I not run across the Programming Language Zoo before? This is an awesome resource! Also another good one: [Crafting Interpreters](http://www.craftinginterpreters.com/).
I didn't list the STD in the results*.md, however for some of the languages, e.g., Rust, C++, Scala, (maybe Julia, I forgot), the stds are displayed if you run the codes.
I don't agree that using two build systems side-by-side is a problem. We do that for the mosys binary on ChromeOS and it works just fine: https://chromium.googlesource.com/chromiumos/platform/mosys/+/master/README#43 In some ways it is actually better: IDE's see Meson and Cargo as to separate projects by the two separate language-support plugins (C/C++ and Rust) in the IDE of the author's choice. 
That's true. It's not uncommon to use u32 or u16 as a count type in specialize implementations for data structures. My assumption is, that for Strings, it's not very important. As I wrote, you could put your two-string variant into a box, so that it lives on the heap and not on the stack. If that's not enough, you could try to work with CStrings I think. They should be null-terminated.
For an internal tool that will likely never need to support super high loads or lots of concurrent connections both Haskell &amp; Rust seem like overkill to me. I love them both, but using your company's time learning a new langauge and ecosystem just to spice up your development seems... wasteful? If you don't care how long it will take, then by all means pick whichever one floats your boat. Personally I love Haskell. Otherwise, you're just building a CRUD app and you're probably better off using typescript &amp; node instead of a radically different language.
* [thin-vec](https://crates.io/crates/thin-vec) is a replacement for `Vec&lt;T&gt;` that is only a single pointer wide. This was written for use in Firefox. [mediumvec](https://crates.io/crates/mediumvec) is a vector that uses `u32` for its length and capacity, so on 64-bit targets it is only two pointers wide. (Disclaimer: I wrote this crate but have never actually used it for anything, so it should be considered untested. Testing and feedback welcome!) Either of these could be used as the basis for a "ThinString" type. Some other work I've done on containers optimized for reduced inline width: * https://crates.io/crates/smallbitvec (also used in Firefox) * https://github.com/mbrubeck/smallbigint/ (still in development, very incomplete)
If you'd like some sample code, I'm building a scripting language called [Eko](github.com/eko-lang).
You can always copy raw_vec.rs out of rustc and use it. It's unlikely to ever be stabilized.
But how important is improving the build time for optimized code? Isn't it much more important to improve the build time for unoptimized, i.e. debug, code, such as with cranelift?
I could use Box&lt;&gt; to reduce the size on stack, but I still think it'd be inefficient and cause reads to take longer when iterating through a vector. Typically, when memory is read, if it's not in the cache, the OS will bring in an entire cache line from memory into the L1. With String's current implementation, you can only bring in 2.66 `String`s per read from main memory (48 bytes) whereas if we were to shrink it from 24 to 16 bytes each we would be able to pull 4 whole `String`s into the L1 before needing to go back to main memory. (all assuming the cache line size is 64 bytes)
I never thought about the idea to directly make a commit right after the release, thanks for that insight. It does however feel kind of ugly (in my opinion) to have separate commits just to represent that a certain version has been reached. I feel Git Tags should be used for those kind of things. But I guess its something we just need to deal with! I do really like Cargo and how easy it is to load other dependencies.
I am not sure if this is what you are after, but I very successfully used [zeromq](http://zeromq.org/) for process communication both on the same computer and over network. In my case it was used in python but it seems like there is a rust crate named rust-zmq ([github](https://github.com/erickt/rust-zmq), [crates.io](https://crates.io/crates/zmq)) that seems fitting... I hope this is what you are after or is at least somewhat useful...
I think this is one of the best ways to deal with everything! Then you could even squash the commits when merging them into master and rename it as a certain version.
You may also be able to do some tricks to reduce the space used by the enum discriminant. For example, if you replace the enum with this struct: struct Foobar { foo: Box&lt;str&gt;, bar: Option&lt;Box&lt;str&gt;&gt;, } then the size is reduced to 32 bytes.
That's not exactly the same. `unwrap_err()` will panic if there isn't an error, but the original code would not.
I actually considered that. The problem is: 1. If you don't pick C for the shared components, you'll have an uphill battle to get projects written in non-C, non-C++ languages to accept adding a new language as a dependency. 2. If you do pick C for the shared components, you'll have an uphill battle to get non-C, non-C++ projects to accept a new component written in a language with such archaic/nonexistent affordances for build automation and such a memory-unsafe language. 3. Either way, you'll have trouble convincing projects to drop the code in their language of choice, which they already know how to easily extend whenever they need a new feature.
Nice article! One small nit: You don't need to`collect()` a range into a `Vec&lt;_&gt;` just to call `into_iter()` on, as ranges are already iterators. So you can avoid the allocation, make your code simple and faster by removing it.
\&gt; That is, there is a rustc backend that uses Cranelift, and has been enabled by MIR. This backend does not use LLVM. the historic truth tells us, that some people spend a lot of development efforts into the development of GCC compatibility long before MIR saw the light of day (e.g.: [https://github.com/redbrain/gccrs](https://github.com/redbrain/gccrs) and [https://users.rust-lang.org/t/rust-front-end-to-gcc/11601](https://users.rust-lang.org/t/rust-front-end-to-gcc/11601)). one of the main reasons, why this efforts couldn't establish satisfying results resp. stability, has to be seen in the unsatisfying specification and never ending flux in the intermediate layers of the reference implementation (see: [https://internals.rust-lang.org/t/mir-specification/6111](https://internals.rust-lang.org/t/mir-specification/6111)) sure -- optimizations of MIR may seek for various different goals, but better compatibility with already existing popular compiler frameworks resp. stabilization of interfaces for IR exchange should at least not completely neglected as one plausible candidate/interest in this game. IMHO it doesn't make much sense to only increase the number of very strange academic experiments and ego-projects, which may indeed look impressive from a deattached isolated point of view, but don't help to solve all those very simple practical needs, which are necessary to make a programing language more useful for real world work resp. serious \[ex-\]change. 
Oh that's way better, thanks! I'll clean that up. 
&gt; the historic truth tells us, that some people spend a lot of development efforts into the development of GCC compatibility GCCRS is not an effort towards GCC compatibility, but towards the implementation of an incompatible Rust frontend built on top of GCC.
There are a couple rust-&gt;C compilers. I prefer this over direct IR compilation (though it's not a perfect replacement for it), since you can use whatever compiler you want (at most any version you want). [mrustc](https://github.com/thepowersgang/mrustc) is the most well known, and I believe it acts as an actual rustc backend like cranelift. There's also this one, but I haven't looked into it much. https://github.com/uwplse/crust
The problem is that you have to consider up-front costs from the perspective of a developer. For example, When I look at [rust-qt-binding-generator](https://github.com/KDE/rust-qt-binding-generator), there are two details, either of which is a deal-breaker on its own: 1. I'd still have to write C++ for the actual interfacing with Qt, so what's the point? 2. **I'd have to learn CMake or some other C++ build automation.** For all its flaws compared to Rust with C++, I find using rust-cpython to access PyQt a much more appealing alternative for using Rust in Qt GUIs. You're going to have a hard time convincing Rust developers that it's worth the effort to bring in a whole new build system when "it works well enough for me" Heck, as ugly an ideas as it is, I've sometimes pondered preserving the simplicity of `cargo install`for tools that need odd post-processing by having `build.rs` launch a long-running process which uses `inotify` to watch for the file handle on the build artifact to be closed, then runs post-build tasks. I don't do it, but that should give you an idea of some of the pitfalls involved in getting buy-in for your idea.
Rust MIR optimizations are mostly about improving build time in general. They primary aim at reducing the amount of MIR. This in turn results in less code that needs to be processed by Rust backeds (Cranelift or LLVM).
Ah makes sense
I wonder if const generics could enable exposing these as parameters on the std String type...
\+1 for all works Appel and Pierce
You probably know about [mrustc](https://github.com/thepowersgang/mrustc), I imagine. I've thought about creating an "automatic MIR-&gt;C compiler generator", which would essentially run a bunch of different MIR structures through the existing LLVM backend, and decompile into C (with something like [llvm-cbe](https://github.com/JuliaComputing/llvm-cbe)) to generate templates for later use. Then those templates can be used in combination with a general translation function as a rustc backend to output C code, which can then be passed into whatever compiler you want. But perhaps it would just be easier to use rustc-&gt;llvm-&gt;llvm-cbe-&gt;[cc] directly.
It has limited scope. "It only works in release builds (or debug with opt-level=1) and if the compile does figure out the allocation branch (or other panic branches) are unreachable."
Thank you so much. 
If you're not capturing panics, you can still [use the technique it uses with any branching condition.](https://godbolt.org/z/X75LdV) 
It would be better with a full effects system. No indication of that coming to Rust any time soon. :(
I would love to see a detailed plan of attack for this problem.
As I've mentioned above, I am considering to add some more options. Such as printing the function name and its counts. But no roadmap or timeline. Might happen today, or in a month. I'm working on other projects. Patches are welcome. :)
I wouldn't worry all that much about data on the stack. For most circumstances that you should care about, "on the stack" roughly translates to "almost certainly in cache memory that's relatively close to the CPU". Unless you have a particularly problem that warrants it, it's better to keep things like the length of a string on the stack than hidden away on the heap behind a pointer indirection. That said, I too am curious about why these types require just so much memory. My guess would be "alignment". Let's take the enum example you gave: One variant is 2 strings, so this type should be able to store at least 2 strings. Each string is at least a pointer + a length, which on x86_64 means 2 64-bit numbers (i.e: 16 bytes). For 2 strings, that's 32 bytes. Now, you're going to want to factor in space for the enum discriminant. Assuming that the compiler gives this some ridiculously relaxed alignment - i.e: 8, that's another 8 bytes which brings us up to 40 bytes. What is the rest? I have no idea and I'd like to know.
No problem, hope it helps.
ipc-channel was what I had hoped would become the, but last I checked they don't support Window's named pipes. If you're using mio (or tokio) in my experience it's not too burdensome to use the respective support crates for named pipes and domain sockets.
It's possible, but at a cost. If you use a `Or` combinatorial with left: 80 bytes and right: 80 bytes, it's a different monomorphization than left: 60 bytes and right: 100 bytes. In this case, I'd suggest picking the maximum for each side, independently, to get a single monomorphization.
&gt; Typically, when memory is read, if it's not in the cache, the OS will bring in an entire cache line from memory into the L1 Nit: The CPU handles that, not the OS.
Well, the trick is obviously to pick the lowest number possible (ie, the max of any actual concrete type passed). It should be possible to get a compile-time error if the type passed is too big to fit, so you can be aggressive.
You're looking for /r/playrust
Thx!
Hi! Newbie here. I've found myself wishing for a tool that would allow me to put my cursor over an expression in Rust code and see the type of the thing I'm looking at. So in code like this: let x = vec![1, 2, 3]; x.iter() .map(|&amp;num| (num, num * num)) .last() .unwrap() I'd like to be able to put my cursor over the \`vec!\[1, 2, 3\]\` part and see that it's a Vec&lt;i32&gt; or whatever. I'd also like to be able to inspect more fine-grained parts of the expression, so eg I want this tool to let me put my cursor over the &amp;num in the closure and see its type, and put my cursor over the .last() and see that it's an Option or Result or whatever it is, etc. Does a tool like this exist? If not, is that because it's impossible, or just because it would be a lot of work and hasn't been built yet? Thanks!
But why not enum Foobar { Foo(Box&lt;String&gt;), Bar(Box&lt;String&gt;, Box&lt;String&gt;) } then, which is 24 bytes?
You're welcome. By the way, [clippy](https://github.com/rust-lang-nursery/rust-clippy)'s `needless_collect` lint would have caught it and give you the same suggestion. If you want this and more cool tips right in your compiler output, don't wait! Try clippy today! `&lt;/advertisement&gt;`
What are the current best tools to dig deep into stack and heap allocations, L1 &amp; L2 hit/miss rate, etc? Are people just using LLDB in the terminal?
Done in alloc\_counter 0.0.2
Done in alloc\_counter 0.0.2
The main distinction between imperative programming and functional programming is control flow vs composition. Where imperative would write if/else, functional would write map+map_err. In a pure functional programming language like Haskell there is no ‚Äúcontrol‚Äù flow, only data flow, and pretty much everything data is iterator. The advantage is it places the burden of lifting code that operates on one datum to operating on a collection of data on the author of the collection. The downside is it necessarily gives control of the lifting to the author too, and because the lifting is abstracted, it is necessarily opaque. This is also how ‚ÄúMonad‚Äù came to be such a hot thing in functional programming. It is a concept that abstracts over enum (e.g. boolean, comparison), collection (e.g. array, list, map), context (e.g. IO, network), and more. It also layers upon itself, e.g. an Array of Maybe is a Monad of Monad. Almost a be-all end-all abstraction for data manipulation. That‚Äôs why to unexperienced eyes functional code may look like ‚Äúabuse‚Äù of combinators. They are used for everything. But that‚Äôs because they are facilitators of composition, and again, composition is to functional what control flow is to imperative. There is even a computing model consisted solely of combinators equivalent to the Turing model. Combinators in that model are so abstract they are only named by single letters, e.g. the famous Y combinator is responsible for infinite loop. 
For what it's worth, if you can borrow those strings you'll end up with a 40 byte enum. Not great, but certainly an improvement.
That's true, but the actual string data is still scattered on the heap, so you're going to have a cache miss when reading the string regardless of the data structure (pointer and capacity) being in the cache. And depending on the size of the string, the CPU prefetching strategy etc. you might have more than the first one one while you're iterating. Either way, I have a gut feeling that actually reading/processing the data is going to be your bottleneck. That of course depends on your use-case --- can you share what you're trying to do?
Noted. That's quite interesting indeed. Thanks buddy!
There‚Äôs a trade-off. `Box&lt;String&gt;` is narrower than `Box&lt;str&gt;` but has an extra layer of pointer indirection.
&gt; Reddit user askalski posted their hyper-optimized SIMD solutions on Reddit and GitHub. It runs in a mindblowing 41 milliseconds. &gt; &gt; Talk about an ego buster. His solution set is more than 10x faster than mine! &gt; &gt; I can't claim the speed crown. I can briefly talk about my approach and experience. This post will also serve as a mini-review of AoC. I like that you did a little research and you're humble about your achievement. Yet another example of why I love the rust community. Unfortunately I can't do much more than skim the article, as I want to solve these puzzles in rust myself at some point. The visuals look great, though! :)
I actually use clippy frequently! It's very nice. One part that isn't totally obvious to me is when each lint is and is not enabled -- how do I know which lints are on by default, and which ones need to specifically turned on? 
But you own `YourType`, so it's allowed. See [this playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7b0c0bc8e0fc65d30b313899a0d7480c) for an example.
You could try the RLS plugin for VSCode. It does this pretty well, though sometimes it doesn't work right. You could try rust-analyzer, which is meant to replace the RLS, but I'm not sure if it has this supported yet. You could try the rust plugin for IntelliJ. I had it works very well.
Oh, interesting, I have been doing it wrong for quite a long time then. Thank you for the snippet.
See the other comments on the thread. Seems you actually can implement From on foreign types to make your type. Always implement From and trait bound on Into.
&gt; The GCC project does not want the Rust frontend (or any other language fronted) to use GCC as a backend. Last time I checked, GCC had support out of the box for Go, C, C++, Ada, Fortran, Objective-C and D. Out of tree there are also Modula-2 and ISO Extended Pascal compilers. GCC project wants that any language fronted that lands on upstream to be properly maintained and abide by their license rules. Anyone that does so is welcome.
Hi! I'm looking for a way to extend the lifetime of a bunch of string slices Right now, I'm doing this: - add String object to Vec - get reference to the String's slice back from the vec using some unsafe trickery - drop(vec) after I no longer need the strings This kinda works but I feel there must be a better way. The library I'm using requires that the string slices live until I am no longer using the struct that's referencing them. Essentially this would be runtime interning, but I haven't found any crates for doing that.
ping /u/arvidep : Nice experiment on generator. I disagree on the ‚Äúless magic‚Äù part. To me osaka just trades magic of Future for magic of Generator. It‚Äôs clearly a good trade in this use case for the binary size benefits, and verifies the viability of the generator design. 
I'm not sure what point you are trying to make. I said (emphasis mine): &gt; The GCC project does not want **the** Rust frontend (or any other language fronted) to use GCC as a backend. That does not mean that GCC does not want **a** Rust frontend, only that it does not want that frontend to be the current one. 
Your assertion is false given how many language frontends are shipped with GCC as backend.
Which frontend that's not exclusively part of the GCC project is shipped with GCC as a backend? You mention C and C++, but gcc and g++ are part of GCC, you can't easily use clang with GCC as a backed anymore. You also mentioned D, but the gdc frontend that's part of GCC does not use the DMD frontend.
It appears to already be installed. There must be some way to point VSCode at it?
Yes I agree it is an unfair comparison to LLVM. But I think the comparison to LSP is OK. Imagine "I just have to create a module in {BUILD_SYSTEM}" and then I will have cross compilation support, dependency tracking, module management, interop with many other languages, etc. I think such a project could be successful. Regarding language, I think also that python is a good compromise language to write such a system in. It runs on most platforms because it is written in C. It is a well known scripting language. Many ad-hoc build systems (x.py, mach) are built in python.
Would MIR -&gt; GIMPLE be more feasible? Also, a Rust -&gt; C (in any way possible) also makes targeting rare platforms pretty simple, we just need a toolchain to glue it all together in a easy way and almost everything becomes basically a 1st-tier.
Glad you asked: we have [docs](https://rust-lang.github.io/rust-clippy) that show the different lint groups and default levels for all lints.
Yeah, I feel like I should've held my tongue - I just finished building a "guessing game," and wow, was *that* a heckuva jump in complexity. But pretty cool, nonetheless :) It just went from 0 to 60 in one chapter. I suppose it's just illustrating the fact that it's not a "beginner's book" on programming, or something. Which leads me to another question: how did many of you begin your foray into programming? Anyone just "picked a language and started learning it," and became usefully successful? Or is that rare? Do most people go through a broader introduction to programming "in general"?? 
&gt;[rustc: Implement RFC 2091 re implicit caller information](https://github.com/rust-lang/rust/issues/47809) \- make 'unwrap' report a useful line number. This is great! I'll finally be able to write quick-and-dirty code without having to write unique `expect()` calls in place of `unwrap()`.
&gt; Should cargo compile, locate, link, download, dependencies from other languages? No, but I think we should have robust libraries for handling other languages within a build.rs. There are already several. The cc crate is a good example. There's some work that could be done to improve this workflow. A "cmake-to-build.rs" type of utility could go a long way. 
&gt;GCCRS tried to implement **a** Rust **frontend** from scratch for the GCC project, and AFAIK due to librsvg requiring Rust, there are still ongoing efforts to implement a Rust frontend for the GCC project. you really shouldn't mix up the *historical* order in such a cynical way! :( when gccrs was written, it simply has to be done in this particular way, because there was no other realistic possibility available, but all the other more recent attempts to realize GCC integration, just wanted to utilize MIR but also suffered because of missing specification stability and cross-implementation unfriendly design decisions.
This sounds like my crate [simple-internerner](https://crates.io/crates/simple-interner). &gt; A very simplistic interner based around giving out (wrapped) references rather than some placeholder symbol. This means that e.g. strings can be interned in systems based around `&amp;str` without rewriting to support a new `Symbol` type. &gt; &gt; The typical use case for something like this is text processing chunks, where chunks are very likely to be repeated. For example, when parsing source code, identifiers are likely to come up multiple times. Rather than have a `Token::Identifier(String)` and allocate every occurrence of those identifiers separately, interners allow you to store `Token::Identifier(Symbol)`, and compare identifier equality by the interned symbol. &gt; &gt; This crate exists to give the option of using a simplistic interface. If you want or need further power, there are multiple other options available on crates.io. I return a `Symbol` that is basically just `&amp;str` with `Eq` being `ptr::eq`, and it provides a trivial way to get back to `&amp;str`.
Well, you pay a cache-hit if you're actually reading the strings, right? You aren't actually pulling any strings into L1 or any cache, just their pointers. Every pointer you dereference will be another cache miss. &amp;#x200B; Locality for pointers doesn't seem like much of an optimization (assuming you are dereferencing them) - I'd be curious to hear if someone has a reason to disagree.
They actually want to get rid of allocations, and there is already some work in that direction: https://github.com/RustAudio/lewton/issues/40 Feel free to jump in and eradicate some allocations ;)
&gt; I agree that cargo should integrate better into larger projects with their own build systems. Consider that cargo is (or *can* be) just as useful as a distribution system (via `cargo install`) as it is a build system or dependency management system. Several crates ignore this, either by tying their crates to one platform with something like pkg-config or by mixing build systems which makes them nearly impossible to install. I hope that these problems are addressed and that everything converges using only cargo. And I hope that the Rust team doesn't do anything to compromise this aspect of cargo's usefulness. 
There's also [rustig](https://github.com/Technolution/rustig) that does the same thing
Oh you absolutely take a cache-miss for reading in a `String`, but having it take up less size on the stack or in a heap-allocated structure (like `Vec&lt;String&gt;`) will also reduce the number of cache-misses. Take `String`'s implementation which is a 24 bytes on the stack vs. one that uses `u32` for size and capacity. Assuming a cache line size of 64, we can pull in 2.67 `String` structs per cache miss and then 1 cache miss for fetching each `String`s data. This gives us an average cost of 1.375 cache misses per `String` read. If the `String` were 16 bytes, we could pull in 4 structs per cache miss, and again have 1 cache miss for reading the data of each `String`. This gives us an average of 1.25 cache misses per `String` read, so having a smaller struct on the stack / in a vector would result in a 10% reduction in the number of cache misses when reading a large collection of `String`s.
So why not build on libp2p?
I began when I was really young playing scriptable video games honestly. I didn't even realize I was programming at the time. My dad (who was a programmer, ironically) told me and I made a direct jump to C++ which was maybe a mistake at the time. It's funny because I had a similar experience as you just did where I thought I knew the language really well simply because I didn't know what I didn't know. I don't really think there is an ideal "first language". Just stick with it. It's gonna suck at first (it did for me at least) from the standpoint of not understand things, but things will click. Don't expect to be largely useful for a little while but don't let that discourage you from trying to do things and failing.
perf or VTune works well
\&gt; we can pull in 2.67 structs per cache miss and then 1 cache miss for fetching each &amp;#x200B; Isn't it more like "Pull in N pointers to cache, dereference pointer, cache miss, repopulate cache for next N pointers" ? So the 2.67 structs in your cache are irrelevant because you'll deref the first struct, immediately lose the other 1.57 cached pointers, and have to pay the cost of pulling them back into cache anyways? Maybe they would hang around in L2 or L3 idk, I may be wrong about it, but I think that pulling the pointers into cache won't matter since a deref will flush them out of the cache.
Thanks /u/WellMakeItSomehow, that fixed the eh_personality problem. /u/urbeker, you mean at github, or do they have any other discussion/chat site? /u/MayanApocalapse , thanks for that link, Ill take a look! Thank you /u/TeXitoi , that was the final piece. It seems like I have been "changing too much" between each try, always ending in a combination of settings that will not end up compiling. I've added the tiny setup to git [here](https://github.com/trondhe/rust-embedded), and Ill continue to add basic stuff to it. I have yet to test in on the actual harware, that will be up next combined with making some led blink.
From the [gitlab](https://gitlab.com/dessalines/torrents.csv): `Torrents.csv` is a *collaborative* repository of torrents and their files, consisting of a searchable `torrents.csv`, and `torrent_files.json`. With it you can search for torrents, or files within torrents. It aims to be a universal file system for popular data. Its initially populated with a January 2017 backup of the pirate bay, and new torrents are periodically added from various torrents sites. It comes with a self-hostable [Torrents.csv webserver](https://torrents-csv.ml), a command line search, and a folder scanner to add torrents, and their files. `Torrents.csv` will only store torrents with at least one seeder to keep the file small, will be periodically purged of non-seeded torrents, and sorted by infohash. ![img](https://i.imgur.com/yTFuwpv.png) To request more torrents, or add your own, go [here](https://gitlab.com/dessalines/torrents.csv/issues). Made with [Rust](https://www.rust-lang.org), [ripgrep](https://github.com/BurntSushi/ripgrep), [Actix](https://actix.rs/), [Inferno](https://www.infernojs.org), [Typescript](https://www.typescriptlang.org/). 
The standard I see most crates is having the version as `1.0.0` throughout the development of 1.0.1, in PR branches and master branch, and after PRs have merged. Then when all of the separate PRs have individually been merged, and tests pass for the merged result, the maintainer makes one more commit "Release 1.0.1" which simply changes version numbers wherever appropriate (Cargo.toml version, docs link urls, etc.). The release is very rarely, or never, done via a PR since it's the decision of the maintainer alone when to release, and no code is changed. This is somewhat similar to your second situation, but it has the advantage of keeping a "working master" which already has PRs merged that are known to be good. This way more pull requests can be made against the master _which already contains some features/fixes_ before the release: this is useful for avoiding merge conflicts, but also for splitting up big features into multiple PRs which can be reviewed separately, but depend on eachother. I very rarely see rust projects use a `dev` branch or a `-dev` release tag, but those are also good alternatives if you're a huge project. For regular libraries this is overkill, but having a `dev` branch where all PRs are merged keeps the master at "release" all the time. It has the advantage of having a single branch which is always release-quality, but several disadvantages including: - crate could look unmaintained if a lot of work is done on dev branch before a big release, and it's been a long time since a commit on master - new developers need to realize this dev branch exists so they don't accidentally make their pull requests against master (which contains old code) - releasing things is more work (need to: merge dev into master, make release commit on master, do release, then merge master into dev) The last strategy I know of is '-dev' version tags. This is essentially what I described first, but with the additional step of making a "Prepare for next development cycle" commit directly after the release commit which changes version from `1.0.1` to `1.0.2-dev`. This means there's exactly 1 commit where version = `1.0.1`, so that's an advantage for ambiguity reading history, and it makes sure people know that the master branch is dev-quality, not release-quality. I learned this first in java projects using maven, but it could be applied to Rust ones as well. It's main two disadvantages are: - no one in the rust community does this, so seeing a '-dev' tag will be slightly out of place - cargo and allows patching dependencies with git versions, but only if the `version` tag in the git branch matches the version released. `-dev` prevents this - more work when releasing versions (need to make an extra commit)
This might work! I'll have to try it tonight.
This criticism of async/await seems very off to me, /u/arvidep. Maybe you can clarify some things for me: &gt; what the futures.rs authors decided to do: cram futures.rs into std and add an entire language feature to hide the bloat behind a code generator. First the post establishes a problem with `futures-rs`- combinators and their associated binary bloat. Then in this quote it assumes that the bloat will carry over to the std-based futures used for async/await, even though the express purpose of std futures and async/await is to *replace* combinators with exactly what the author wants- generators. &gt; the rust equivalent would be slightly uglier, because generators can‚Äôt have continuation arguments The next thing it assumes will carry over is the thread-local executor. (Or that's how I read it; it's a little unclear- maybe it means tokio's default executor?) But this complaint *also* doesn't apply to async/await, which has converted to an executor argument (`Waker`) to `Future::poll`, which is again *exactly* what the author wants- a continuation argument to the generator. &gt; its in no way fancy, but has two significant advantages over async/await. Firstly, it emits a generator without a specific execution engine. &gt; &gt; Secondly, Result just works as intended. There‚Äôs no need to wrap it in FutureResult because no such type exists. &gt; &gt; Lastly, sync is just: [polling and yielding in a loop] so anything that returns Ready/Again is sync. There is no task queue, no singleton, nothing ‚Äòmagic‚Äô. These last criticisms of futures and async/await are the most baffling. Futures have *always* been agnostic to the execution engine, even in the 0.1 form, to a greater degree than osaka. The std version of futures *already* just uses `Result`. And `await` *is* just polling and yielding in a loop. The task queue is osaka's `yield poll.again(..)`, the (actually non-)singleton is osaka's `poll` argument, and the "magic" is that `poll` is only accessible from manual `Future::poll` impls. This all seems like *very* surface-level stuff to me- hardly something that would prevent anyone from using async/await on embedded. The execution model is identical. I saw [this tweet](https://twitter.com/arvidep/status/1092818400798298114) saying "you cannot create a push based executor" and [this tweet](https://twitter.com/arvidep/status/1092823133390864385) saying "any [Future-based] alternatives need to look like tokio," but it's unclear to me exactly what you're changing here, compared to existing embedded uses of async/await.
&gt;Also, a Rust -&gt; C (in any way possible) also makes targeting rare platforms pretty simple, we just need a toolchain to glue it all together in a easy way and almost everything becomes basically a 1st-tier. no it's not much more then a very questionable work around -- e.g. for debugging this isn't a desirable solution. there are good reasons, why nobody wants to work on C++ compilers anymore, which just act as C preprocessors. ;) it's nice, that we have mrustc/llvm-cbe etc., but they can/should not conceal a much more significant shortcomings. supporting more of this serious popular toolchains instead of of just hacking add hoc glues for on particular solution, would also help to strengthen and refine the software. if rust really want's to be an attractive programming language and not only imitate the most stupid shortsightedness of monopolist business management and their typical one-and-only reference implementation approach, it has to cooperate a little bit more open with other competitors in the free software scene.
AFAIR, the implementation of lifetime restrictions (which address the soundness issues you mention) hinges on chalkification. So it's hopefully just a matter of time.
You can search for files too, by using the dropdown selector on the top right. For example, "frasier s04e06".
use crossbeam-channel. They are not only better, but much faster. std also has [https://doc.rust-lang.org/std/sync/mpsc/fn.sync\_channel.html](https://doc.rust-lang.org/std/sync/mpsc/fn.sync_channel.html) , but its bounded.
I still don't get your diatribe against GCC, mixing the terms programming language with compiler frontend, asserting that GCC doesn't want language frontends when there are several to choose from, many of which outside of main GCC tree. GNU Compiler Collection (GCC) has **frontends** for several programming languages. If you want a GCC frontend for some language, you make it generate the intermediate code in GIMPLE format, as expected by the GCC backend. D language developers are able to plug their compiler frontend into three compiler backends, dmd (digital mars), ldc (LLVM), gdc (GCC). Go language developers are able to plug their compiler frontend into three compiler backends, Go (official compiler), gollvm (LLVM), gogcc (GCC). GNU Modula-2 is an out of tree project, pluggable into any existing installation of gcc-8.2.0, gcc-6.4.0, gcc-5.2.0, gcc-4.7.x and gcc-4.1.2. See a pattern here? Don't blame a compiler infrastructure, without which there wouldn't be any Linux eco-system to start with, with *several existing language frontends* to the lack of resources that the community has placed into GCCRS.
Look, I *like* functional programming. But I also like being able to visually distinguish code that operates on "0..1" elements from code that operates on "0..n" elements, because usually that is a really important clue that can help you understand at which level data is being processed in a really long method chain. After all it's possible to implement arbitrary computation using just a [single instruction](https://en.wikipedia.org/wiki/One_instruction_set_computer#Instruction_types), but we don't do that because we prefer code whose shape gives a clue to what it's doing and what kind of data it is operating on.
Perfect! Really appreciate it. :)
For my case it's a bit of overkill to use a full-featured messaging framework for a simple IPC between processes. TCP can't be used either, IPC channels must be named.
Ideally I would like to have something like tokio-ipc with top-level abstractions like `NamedIpcClient`, `NamedIpcServer`. Underlying implementation can rely on tokio-named-pipes, tokio-uds and tokio-fs (for pipe client part). I am thinking on starting to work on that.
This is awesome. Thank you :)
&gt; still don't get your diatribe against The OP complained that we don't add a GCC backend for the Rust language frontend (rustc). I mentioned that GCC does not support that, requiring instead a different language frontend to be implemented as part of the GCC project. &gt; D language developers are able to plug their compiler frontend into three compiler backends, dmd (digital mars), ldc (LLVM), gdc (GCC). AFAIK DMD is the D language frontend, and it also has its own backend. LDC uses the DMD frontend and an LLVM backend. GDC does not use the DMD frontend, it implements a different one, and it uses GCC as a backend. &gt; Go language developers are able to plug their compiler frontend into three compiler backends, Go (official compiler), gollvm (LLVM), gogcc (GCC). AFAIK [gofrontend](https://github.com/golang/gofrontend) is a C++ frontend for go that only works with gccgo: "As of this writing it only supports GCC", while [llgo](https://github.com/llvm-mirror/llgo) is a different frontend written in Go. &gt; GNU Modula-2 is an out of tree project, pluggable into any existing installation of gcc-8.2.0, gcc-6.4.0, gcc-5.2.0, gcc-4.7.x and gcc-4.1.2. What do you mean by pluggable? Does it use GCC as a library? Or can it be plugged and build together with GCC ? &gt; Don't blame a compiler infrastructure, The OP is blaming us for not having a GCC backend in rustc, and argues that MIR is bad because it does not allow that. Yet (1) rustc has multiple backends, (2) it can dynamically link against both and you can choose which one to use at run-time, and (3) I don't know of any project that is able to do that with GCC. 
That was a really good episode.
That's awesome. This will be a great asset for the Ion shell, since allocations are among the biggest challenges. We've been trying to eliminate as many of them as possible. Profiling tools don't give the full picture, but this should make it more obvious where the most allocations are happening.
Can you not use `std::alloc::{alloc, dealloc}` instead?
Awesome. Please tell me how well that goes! :-)
This is the subreddit for the Rust programming language. You're looking for /r/playrust
Not the same thing, it appears to do it much better! Using the call graph for analysis is much cleaner than abusing the linker and optimizer to detect reachable code paths.
Related, I wonder whether anyone's considered putting the discriminant on the back of the enum, which would let things pack better. Rust's structure ordering puts biggest things first (because they have strictest alignment), except in the case of enums. For example if I have a struct of u32 and an enum, then if the enum required alignment of 8, e.g. (usize plus u8 discriminant, total 9 bytes, padded to 16), but had the discriminant on the back, then we could fit the u32 after the discriminant, total size 16. As it is right now, total size would be 24. In code: \`struct X { a: u32, b: Y } enum Y { A(usize), B(usize) }\` 
I guess I was just a little too late for last week, so "this week"? [New crate alloc\_counter. Count allocations, reallocations, deallocations. Allow, deny, or forbid allocations on an expression or function basis.](https://www.reddit.com/r/rust/comments/anmjgs/new_crate_alloc_counter_count_allocations/) Featuring `#[no_alloc], #[count_alloc], count_alloc(|| { closure }), {allow,deny,forbid}_alloc(|| { closure })`. `#[count_alloc]` writes the counters along with the function's name. Oh and these work on `async fn`s too!
&gt; build.rs throws a wrench in the gears, but it's a piece of duct tape for resolving dependencies that aren't covered by Cargo. So the perfect gap for Meson to fill. build.rs gets used for much more than that; they can generate code, compute features, and other things besides. See https://doc.rust-lang.org/cargo/reference/build-scripts.html.
What I am proposing is basically MIR-&gt;GIMPLE, so yes. I am completely uninterested in working on compile-to-C implementation, because I believe it will result in subpar quality. Sorry about that. Someone else can work on it.
Try what the other person said about the plugin, this is for further reference. If you want a way for the compiler to tell you what it thinks the type of an expression is, you can try assigning it to the unit type, e.g. let x = vec![1, 2, 3]; let y: () = x.iter() .map(|&amp;num| (num, num * num)) .last(); will return an error `note: expected type () found type std::option::Option&lt;({integer}, {integer})&gt;`
Great idea, does it allow sorting/filtering, and displaying more than 10 torrents at a time?
No, mrustc is not a rustc backend like cranelift. It implements its own independent frontend.
I'm also still trying to understand the async subsystem of Rust, so take this with a grain of salt, but .. According to this https://boats.gitlab.io/blog/post/wakers-i/ the waker should be used as a callback by the event source. So in case of a sleepy future the waker should make its way to a timer wheel, that is usually part of the executor main loop, but the waker API makes it possible to separate these (across as many threads as people would like). See also https://boats.gitlab.io/blog/post/wakers-ii/ 
Your correct. I remembered that after making this comment.
I am actually also interested in contributing to this. I read up the original WAM paper on how to make it and was considering making one at one point. I can PM you, but I am definitely interested in this!
You could add these as issues to the gitlab repo. The back end already support showing more than 10 torrents at once, but I couldn't find a non-obtrusive way to add that to the front end. The sorting I did by default by popularity and size, and it's cached that way to sqlite. The queries would get extremely slow if it had to sort by other things. What would be the use case for sorting by not anything but seeder counts, on the other fields?
I feel like code generation should be soft deprecated in build.rs... that's what procedural macros are for, right? 
Except that when speaking compiler developer's language, rustc is the Rust *reference compiler*, not the Rust *language frontend*. Yes in the absence of alternative compilers, one can be mixed up with the other, which still doesn't make them the same. D uses the same frontend for all compilers, they just tend to lag behind dmd due to the integration work. All compilers used to share the same frontend written in C++, now the frontend is written in D. GDC took the decision to keep using the C++ frontend for GCC 8, with the D frontend being adopted into a later version, due to bootstraping issues. GNU Modula-2 compiler is pluggable, because it has been designed in a way that multiple versions of GCC can be used as backed. 
Looks pretty good. I think `f64::from(2)` can be written `2f64` or just `2.`
You keep saying that you want to work with raw generators instead of futures, but never why. What makes generators universal but not futures? It's unclear what you would gain by using generators directly- in fact, [this has been discussed before](https://internals.rust-lang.org/t/pre-rfc-await-generators-directly/7202) and the current design was chosen to enable composition between multiple uses of generators. It's also unclear what you want to get rid of from `Future`- it's very similar to the `Generator` API, but with a smaller surface area. This lets it be stabilized sooner, while remaining forward-compatible with other uses of generators later. All this "I've shipped hundreds of thousands of embedded devices" talk is fine, but if you can't articulate what you've learned by doing that then nobody can even begin to understand what changes you want made.
On the topic of finding edge cases, you should probably give generative testing a whirl. [Proptest](https://github.com/AltSysrq/proptest) or [Quickcheck](https://github.com/BurntSushi/quickcheck) are the most popular options for exploring the input space in a way that is type-system aware and integrated into the standard Rust test framework. [Fuzzing is also pretty accessible in Rust](https://fuzz.rs/book/introduction.html), for a different tack at finding edge cases.
Also, as long as you don‚Äôt escape a reference to the enum LLVM is able to reduce or eliminate the stack allocation. This is performed by the `mem2reg` and SROA (scalar replacement of aggregates) passes. 
&gt; AFAIK DMD is the D language frontend, and it also has its own backend. LDC uses the DMD frontend and an LLVM backend. AFAIK GDC does not use the DMD frontend, it implements a different one, and it uses GCC as a backend. Is that so? In any case, for DMD and LDC, one does not interface with dmd and choose different backends from it, but one needs to use either dmd, or ldc (and ldc will pick up dmd as a frontend). You are wrong. Quoting from the [D compilers page](https://wiki.dlang.org/Compilers): GCC D compiler The DMD compiler front end coupled with the GCC compiler back end. Fast and open source. 
Thanks for your reply! I'll just stick to the first approach then :)
Finding rare songs: https://torrents-csv.ml/#/search/file/radiohead%20gagging%20order/1
Thanks. So the first blog post starts with this: &gt; At the ‚Äútop‚Äù of the program is the executor. The executor schedules futures by polling them when they are ready to make progress. &gt; At the ‚Äúbottom‚Äù of the program, the futures depend on event sources. (in the case of async IO, the source of IO events is often called the reactor). So it separates the "executor" from the "reactor", but why? Isn't the event loop both the executor and the reactor? (I've edited my post above slightly to add "reactor".)
We went down that route initially but had trouble reconciling neon/turtl-core/msys/gcc so eventually gave up and went the way of ffi instead. I'd like to use neon eventually, but it wasn't an option when we first embedded the core into electron.
&gt; Yes in the absence of alternative compilers, one can be mixed up with the other, which still doesn't make them the same. Arguably, rustc is the Rust _reference compiler_ and the Rust _reference language frontend_, which is why I was specifically talking about _the official frontend_ vs _a frontend_. --- I think the main difference between rustc and other compilers like DMD is that rustc supports pluggable backends (LLVM, Cretonne, emscripten, etc.). LDC and GDC use the DMD frontend to build new compilers, but the DMD compiler itself does not support plugging a different backend in. 
You're right, the core is the only rust component. That said, probably 85% of the app's logic (frontend and backend) lives in rust because the app is so client-heavy.
&gt; The GCC project does not want the Rust frontend (or any other language fronted) to use GCC as a backend. This is a bold claim. Do you have any reference to back it up? Or is it just a hunch?
From part 1 of that series: &gt; At a high level you can think of the executor as managing the program‚Äôs compute resources (scheduling futures that are awake and ready to be polled) and the reactior as managing the program‚Äôs IO resources (waiting on IO events and waking futures when they are ready). The executor and the reactor form the two halves of what in most asynchronous computing systems is called the ‚Äúevent loop.‚Äù One of the great things about Rust‚Äôs design is that you can combine different executors with different reactors, instead of being tied down to one runtime library for both pieces.
Haskell is pretty much the kind of expressive type systems. I'm not saying you should learn haskell to program in Rust, but there was definitely somr inspiration from Haskell.
Isn't the String on the heap still taking as much space as it would in the stack? If so, Box&lt;String&gt; would actually be larger than Box&lt;str&gt;.
Would it be possible to designate a file descriptor to write to, so that a log could be written to the disk?
Stallman itself says it: https://lwn.net/Articles/629293/ That's an exchange between Emacs developers (a GNU project) wanting to use GCC to do auto-completion, and asking Stallman if he has changed its mind about making GCC reusable. In 2015 he mentions: &gt; My hope is that we can work out a kind of "detailed output" that is enough for what Emacs wants, but not enough for misuse of GCC front ends. In the 1990s, making GCC hard to reuse was a goal to prevent companies from wrapping it up and selling it as a product. 
looks cool\~
`Box&lt;String&gt;` takes more total memory, but less memory inline in the struct or variable where it is stored. * `Box&lt;String&gt;` takes up one word inline. This word stores a pointer to a three-word heap allocation that stores the `String`. The `String` contains a length, a capacity, and a pointer to a second heap allocation that stores the string data. * `Box&lt;str&gt;` takes two words inline. These store a length, and a pointer to the heap allocation that stores the string data.
&gt; Anyway, long term I'd rather build C with cargo than build Rust with something from the C ecosystem. Why? There is also other languages than C.
&gt; Note: unclear if it can be implemented today with the following API: InlineBox&lt;Future, [u8; 240]&gt; Be careful with that, because you have to worry about alignment, too. 
Now that I have the time to look at a bit your code I have some more things to say about that. Think of `&amp;str` as read-only text. This should be used in cases like this, where you only need to read the contents. pub fn upload_url_for(&amp;self, upload_id: &amp;str, file_id: &amp;str, part: u64) -&gt; Result&lt;GetUploadUrlResponse, WeTransferError&gt; { let path = format!("/{}/files/{}/upload-url/{}", upload_id, file_id, part); self.requester.get::&lt;GetUploadUrlResponse&gt;(&amp;path) } Since you can turn a `String` into a `&amp;str` just by referencing it (putting a `&amp;` on it), this makes your function very accepting. And if you were to instead require a `String` or an `Into&lt;String&gt;`, then if you wanted to pass a string you were borrowing, you'd have to clone it. Right now, I see tons of `to_owned()` calls in your code which is doing tons of allocations that shouldn't be needed. I won't get into too much detail, since googling "rust str vs string" will give you lots of information. On another note, you should look into [PathBuf](https://doc.rust-lang.org/std/path/struct.PathBuf.html) as the recommended way of creating paths like in this function. Something I'd really recommend is setting up [clippy](https://github.com/rust-lang/rust-clippy), which will warn you of a bunch of common things. It can be run on your code from the command line or set up with your editor to highlight code.
That Stallman quote has nothing to do with the claim that GCC does not want to allow use of the Rust frontend. Stallman is objecting about about making output from GCC's frontends usable by other software that could be proprietary. In the case of the Rust frontend it is the other way round, GCC would use the output from the Rust frontend, so I cannot see how that issue is relevant to the claim that GCC does not want the Rust frontend. There is no strategic reason for GNU to not use the Rust frontend, which is free software using GNU's definitions.
&gt; Passing an `Arc` around. The problem: I have no way to know for sure everything will be dropped. My first try resulted in a missing reference somewhere and the destructors still not running. I would expect that to work -- assuming you don't leak the value, you stop all threads and the `tokio` `Runtime`, and so on.
I keep getting killed solo üôÑüôÑ
You probably want /r/playrust. This is the subreddit for the Rust programming language.
Ah sorry, no that's a general feeling. If you read, e.g., the last link I posted, the GCC project is not only motivated by technical decision, but also (and mostly) by political ones. There is a fear that including too much too freely-licensed code will extinguish GCC. For example, nothing prevents GCC from including LLVM and releasing it under the GPL, yet in the case of emacs there is a lot of resistance to allow emacs plugins that use clang to do auto-completion in tree.
I don‚Äôt know if it would help with all of your ownership issues, but I think if you used https://docs.rs/tokio/0.1.15/tokio/runtime/struct.Runtime.html#method.block_on instead of the run function you could have the runtime peacefully return control back to your main function when the future you pass to it finishes. Then all of your shutting down could happen in your main function after all your futures have been dropped. If you were to create one of these: https://docs.rs/futures/0.1.25/futures/sync/oneshot/fn.channel.html, you could pass the sender around wherever as your cancel button, and then use https://docs.rs/futures/0.1.25/futures/future/trait.Future.html#method.select to race your main future with the receiver. Your main future now finishes when that send method gets called, and therefore the call to block_on will block until one of your futures calls the send method. 
That's actually a very smart solution, thanks a lot! I'll look into that for sure.
This was actually caused internal tokio panics for some reason. 
Oh
Can somebody gift me rust
Probably, yeah.
Hopefully. It's been a while since I looked into this. I can ask Niko and folks tomorrow if you want
/r/playrust
This could probably be done without additional overhead, because String already does overflow checking on every allocation
What differentiates this from the search provided by torrent trackers themselves?
There we can agree: GNU motivations are more political than technical, and the GNU project is willing to choose a technically inferior solution if that helps its goals that all software be free software. But I believe GNU is ready to (using GNU terminology) use permissively licensed code, especially if using it will make its copyleft software more competitive with other permissive software, specifically I think GNU would be fine with using the Rust frontend as it will only make GCC more competitive with LLVM.
They called me a retard and sent me viruses
&gt; So it separates the "executor" from the "reactor", but why? Isn't the event loop both the executor and the reactor? Not necessarily. It's possible that you might not even *have* a "reactor." I'm working on some embedded async abstractions and I handle the `Waker` by making it accessible to the interrupts that trigger when, say, the USART peripheral is readable/writable. My very WIP code for async embedded serial is [here](https://gitlab.com/polymer-kb/firmware/stm32f103xx-futures/blob/wip/src/serial.rs). /u/Nemo157 also has some similar things [here](https://github.com/Nemo157/embrio-rs/blob/master/embrio-nrf51/src/uart.rs). For your timer example, you could do something like thread::spawn(move || { thread::sleep(&lt;however long you want&gt;); waker.wake(); }); return Poll::Pending; It's pretty naiive and not very efficient, but that's the essence of what needs to happen. Something somewhere needs to be given the `Waker` so that it can call `wake` at the appropriate time.
You can't search a torrent tracker. You can go to a *torrent site*, which indexes torrents, but none of those are open source, or let you see the data. This provides both the source data (which is &lt;~200 MB) for all popular torrents in existence, and several ways to access that data, such as a webservice, command line search, etc. And it can all be done locally too. I suggest reading the linked gitlab repo to learn more.
&gt; futures does that extremly well for its intended use case (web servers) Futures are intended to be general-purpose; they are not intended for web servers any more specifically than they are intended for actor systems. For example, the original comment cites executors for embedded platforms.
If nothing else, searching is stupid fast. How scalable is it to keep the csv under version control? Are there frequent large changes?
Would it be possible to support multiple possible strings?
Fair point. Yes, certainly possible. BTW: If you look at the sample output above, it is counting the allocation costs of the `eprintln` of the inner-calls, so ideally the logger will have a reasonable fixed-size buffer so it doesn't allocate when printing information. Should it use the log or slog crates or be standalone? Slog supports no-std and that is desired. The only thing blocking alloc-counter from no-std currently is the use of thread\_local! but that is an easy fix: either forbid multithreading or use atomics or emulate thread\_local depending only on alloc.
Always a good laugh when I see those weird machines. I feel you, I have difficulties decoding the combination of combinators too. In the end a lot of coding is like writing, with so many preferences available. Just wanted to share my 2 cents. 
&gt; How scalable is it to keep the csv under version control? Very, esp considering it's small size (~80 MB currently). I've had to do a few big prunes (removing torrents with no seeders), and some large updates, which I've occasionally cleared the history for that specific file fully, but my commits / line additions are usually pretty small. So far I haven't received a PR for actual torrent additions, but I've tested it and its very clean to collaborate on adding new torrents to this, and it could easily be a universal torrents DB. I've been adding torrents daily to it, and the diffs are small enough that it's manageable.
&gt; But I don't think this is what's expected from a "production" executor as well, or is it? I think it is. Not sure though. On the subjects of Timers. The level you are operating at is very much 'no-batteries-included' and the same goes for timers. Timers are a feature of the event loop. Depending on the flavor of your event loop it is probably running at 100% in a loop, or is triggered by events who may or may not be on a timeout. The big question is: How and when do you expect to weave the timers events(create/delete/trigger) into your event loop. 'Simple' solutions are things like using some OS specific API and/or a dedicated timer thread. Here is a blog post introducing some of the things Tokio does: https://tokio.rs/blog/2018-03-timers/
You are going to need to provide more information. You also probably wanted r/playrust.
I think you can more neatly calculate the sum of your BitVec by writing `bv.iter().filter(|&amp;x| x).count()` 
In the case of the system shell, it has raw control over stdout/stderr , so you wouldn't want it to log anything stdout or stderr. Things get trickier when subshells are spawned, via forking, where stderr may be a null FD. Usage of the log crate may work, since I could set up fern to divert all logging events to a file. It's also the standard convention. no_std isn't import for the ion shell, since it already heavily relies upon various std features.
I‚Äôd suggest first, reading The Book, then trying to use whatever piece is new to you in a toy project. Also look for blog posts on that topic. For traits, you might want to look up something along the lines of ‚Äúcomposition vs inheritance‚Äù 
Am I seeing it correctly that you are [formatting your own SQL query strings with minimal escaping?](https://gitlab.com/dessalines/torrents.csv/blob/master/server/service/src/main.rs#L90-95) That sounds like a recipe for disaster.
Why would you ever need a string longer than 4GB? This is not a common use case. It's just a waste of 8bytes per string for no good reason.
Thanks for taking the time to answer such a embarrassingly obvious question. 
&gt; I'm being sent a &amp;Waker (actually &amp;LocalWaker) when I'm polled, but I'm not sure what I should do with it. There's just a Waker::wake, not a Waker::wake_at_instant (or a Waker::wake_when_fd_is_ready etc). You should register the waker with the future, and at the instant the future is actually ready you call the `Waker::wake` method. Calling this will trigger another call to `poll`, and when that happens you return the actual result (Which will probably be `()` if its a simple timer). You can either register the waker with the executor (Eg some `Executor::wake_at_time(timer_endtime, waker)` function) or you could attach the waker to the future, and then register the future with the executor. Blog posts always seem a bit hand-wavey to me. I think the best resource at the moment for learning about futures is through reading through the (futures-preview)[https://github.com/rust-lang-nursery/futures-rs] codebase. They have executors, `AtomicWaker`, some IO code, a whole lot of future combinators and lots of tests which show how futures are used through `block_on`, `async {}` and `await!`.
Apologies, what should be done there? Also, would a prepared statement be necessary since these are new SQL LIKE queries every time? I couldn't kill it with your query.
Hi! I am very slowly getting my feet wet with Rust, and over the past few months I wrote a chess engine to see how it might compete (in terms of speed) against one that I wrote in JavaScript (using comparable algorithms, etc). Currently, I use my JavaScript engine from an express server, where a client makes HTTP requests for the best available move given a string representation of a chess board. I want my Rest API to instead use the Rust engine I wrote. When developing locally, it works swimmingly; my express server takes the request, extracts the board representation, and executes my rust binary with the board as an arg. It then takes the output and returns the result to the client. Unfortunately, I'm having difficulty getting this to work on the AWS EC2 instance that I host my server on (it's Ubuntu 16.0.4). The binary that is compiled on my mac will not execute in Ubuntu, and I've had a relative nightmare getting rust set up on the VM (at least a version of rust that supports some of the features that my engine apparently requires). Soo... my questions: 1) Is there a way to compile a binary from one OS while targeting another? 2) Does anyone have any experience installing Rust on an EC2 instance (my understanding is that the newer Amazon Linux 2 boxes support Rust much better, but for purposes of this question, let's assume I cannot use a different box)? 3) Does it make sense that a Rust library is not backward compatible? 4) If you can spare a moment, is my \[project\]([https://github.com/robtaussig/rust-chess](https://github.com/robtaussig/rust-chess)) reasonably set up? Finally, completely unrelated, but here is the UI I would like to use to send the requests: [https://robtaussig.com/chess/](https://robtaussig.com/chess/). Scrolling on mobile causes some issues with drag and drop, so I would avoid it if you are using mobile.
This has reignited my will to keep learning Rust. Definitely came across the problem of following the compiler guidelines too religiously and ending up down a rabbit hole.
Oh cool, thank you! I'll use this trick in the future!
Hey, that is very exciting! Please feel free to reach out if you get stuck on anything; I'm easy to find on the internet and I like helping. :) 
This sub is about the low level programming language known as Rust. In no way is it related to the game. You're best bet to get the game is to buy it.
You're right that your current way of doing it has a subtle and painful memory bug. Thank borrowck for saving your ass! One option to work around this is to use `mem::replace` to pull out the socket you act on, do your work (with the slab now free to be reborrowed), and then put it back when you're done. This is kind of annoying though, since (1) you have to take care to put it back at every return point (even uses of `?`!) and (2) you need a placeholder value. To solve (2), since you're storing an enum anyway (`Service`), you could just add a `None` variant. If you don't like this, perhaps it's okay to use uninitialized memory (`mem::uninitialized`), but as with any use of unsafe, be very careful. To solve (1), RAII would be the rusty way to do it. (Buzzword-free version: make a sentinel object that, on drop, restores the service to the slab.) *However*, I don't see a way to acquire the &amp;mut on slab *only* once the sentinel is dropped. I think this is an unfortunate (but understandable) limitation of the language. Another option is that you might be able to structure your error-handling such that you don't have any early return points at the same time you need to manually clean up a resource. For example, you could have `process` do the work of extracting the service from the slab, then pass both off to a private `process_inner` with most of the interesting logic, and then, back in `process`, you clean up after having extracted the service from the slab. It's messy, but tolerably so, I think.
https://medium.com/learning-rust/rust-beyond-the-basics-4fc697e3bf4f might be helpful for you.
They aren't treated specially by Rust. They are simply matched verbatim (i.e. the first definition would match a `graphql_object!(@generate, meta = { lifetimes = ['a], name = String }, blah blah)`.
I'd cross compile with musl. That's what I've done at work (I've written a pretty complex git server side hook in rust). It's dead simple if you have docker for mac on your laptop: https://github.com/emk/rust-musl-builder
macro_rules! is a grammar writing machine, in some ways. As such you can kinda write DFAs where each state represent some bit of precompiler execution. This macro specifically must take the input and throw it around other internal macros until it generates that final form. AFAIK, those @generate and friends have no magic behind them, they are just the syntax the macro expects to find at that position.
You're in the wrong subreddit. Please actually read before posting, you want /r/playrust.
Update: here's a [playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=c4764cb81e45a5970719f56cfa313e2a) implementing one version of what I did above. Like I said, it's clumsy, and I didn't make much effort to format the code well, but hopefully it conveys the point well enough.
/r/rustjerk
I'd recommend reading [the little book of rust macros](https://danielkeep.github.io/tlborm/book/) to get a super understanding of macros. In particular, @generate and meta seem like tokens for what TLBORM describes as internal rules: https://danielkeep.github.io/tlborm/book/pat-internal-rules.html. Essentially, the macro can call itself recursively and this `@generate meta = { }` pattern is how it differentiates data it's passing itself recursively from data given by a consumer.
ain't no education potential in a game with Nudity. gotta be Frank with you man sorry
Kinda like the rule against rolling your own crypto, try not to write your own query formatter. This one's for SQLite: https://docs.rs/rusqlite/ There's also https://diesel.rs but that would be a much bigger change.
I'm working through the Rust Book, and I ran into this exercise: &gt;Using a hashmap and vectors, create a text interface to allow a user to add employee names to a department in a company. For example, "Add Sally to Engineering or" "Add Amir to Sales." Then let the user retreive a list of all the people in a department or all the people in the comapny by department, sorted alphabetically. I tried this problem a few months ago (and even posted about it on this subreddit!) but hit a point of diminishing returns, and gave up. Now I'm a little more experienced with the language, so I figured I'd give it another shot. But still, I can't seem to write viable code to solve this problem. I keep running into strange errors, and I think I'm making it more complicated than it needs to be. Did anyone else try to solve this exercise when they were going through the rust book? What was your solution. At this point I think it would be more beneficial to me to see someone else's solution than to continue scratching my head over this problem. Any help is appreciated!
Looks nice! Regarding the double crate issue described in your GitHub readme: you can \`pub use enum\_from\_str\_derive::FromStr;\` in your outer/non-derive crate. Then users won't need to explicitly depend on your derive crate. You might look into incorporating this with \`serde\`, which might make this more useful. See [https://github.com/serde-rs/serde/issues/1019](https://github.com/serde-rs/serde/issues/1019) for thoughts/context.
It's using `rusqlite`, and in the example docs you linked, they're also using raw sql with `conn.prepare`, exactly what mine is currently doing. So I'm unsure what changes to make.
*Hey just noticed..* it's your **5th Cakeday** jrheard! ^(hug)
Awesome. I‚Äôve used the previous version a bit. I‚Äôll check it out. 
I'd appreciate any feedback! All tutorials I found felt somehow outdated and I don't have the board used in the embedded Rust book. So I wanted to document how I got some Rust code running on the BluePill
Not entirely sure the borrow checker is okay with this, since I'm no good at Rust, but something like: let stmt_str = "select * from torrents where name like '%' || ?1 || '%' limit ?2 offset ?3"; let conn = Connection::open(torrents_db_file()).unwrap(); let mut stmt = conn.prepare(&amp;stmt_str).unwrap(); let torrent_iter = stmt .query_map(&amp;[ &amp;query, &amp;size, &amp;offset, ], |row| Torrent { infohash: row.get(0), name: row.get(1), size_bytes: row.get(2), created_unix: row.get(3), seeders: row.get(4), leechers: row.get(5), completed: row.get(6), scraped_date: row.get(7), }) .unwrap(); ```
I wonder how hard it is to build for x32, which gives 4-byte pointers and 4-byte usize. That just gets around the whole "usize is huge" problem.
You are only looking at the memory overhead to point to the string. Look at the overhead for each of those strings on the heap. If you are managing a lot of strings, it pays to use more efficient methods of heap storage.
Thanks! I'll try this out and write a unit test for it. https://gitlab.com/dessalines/torrents.csv/issues/44
Setting up CI on Azure Devops for our [C to Rust translator](https://github.com/immunant/c2rust) while waiting for permission to open source version that is installable from crates.io. The Azure folks are super helpful and let us run up to 10 builds in parallel for free :)
I took a MOOC related to Scala; that's where I really cut my teeth.
\*Your FTFY. :)
&gt; Also, would a prepared statement be necessary since these are new SQL LIKE queries every time? Under the hood, every [unique query made by the sqlite api creates a new prepared statement](https://www.sqlite.org/c3ref/stmt.html). 
&gt; Haskell is pretty much the king in expressive type systems Haskell only goes so far though. For other interesting work, see languages like Coq, Agda, Idris, Liquid Haskell, F*, etc. Haskell is far from the end of the story when it comes to interesting type systems.
The problem is not using raw sql, it's naively generating raw sql using rust's string formatting, using statement parameters (the `(?1, ?2)` in the example) built into sqlite. SQLite is [insanely well tested and statically verified](https://www.sqlite.org/testing.html), honestly much more than your average Rust program, and does a much better job preventing [bobby tables](https://xkcd.com/327/) incidents than using the rust `format!` macro to in effect give the user to call arbitrary sql code.
We _should_ have examples soon, but https://github.com/awslabs/aws-lambda-rust-runtime/issues/77 is a decent starting point.
If I have values that tend to be within range of a i32 or i16, but I will end up right away adding them to i64, does it make sense to just make them i64 from the get-go? Are there performance penalties in converting, and are there performance benefits to keeping it small?
[Here you go](https://rustup.rs/)
Vala also had, and has, an ownership system. It's not as good as Rust's, but writing `(owned) value` transfers ownership of a pointer. Effectively, it calls `gpointer g_steal_pointer (gpointer gp)`.
The text in each of the screenshots are abnormally blurry. Not the kind of quality I'd expect from a screenshot.
Because in 64 bit systems a word is 8 bytes, and an int is a word. That's the price of using 64 bit architecture.
The optimization manuals are free downloads from AMD or Intel or Agner Fogg. There's some wonderfully surprising things there. For example, on my processor (*not* cutting-edge, K10) u16 and u8 are only faster in the context of SIMD. Scalar operations (in particular load/store) don't implement all optimizations for the small types. Reimplementing something like `String` as a `(usize, u32, u32)` tuple is totally doable and should be educational. I wouldn't expect it to be much faster. Note that the size of `(usize, u16, u16)` is 16, not 12. It has to be a multiple of the alignment, an assumption baked deeply into how the std allocator works. --- There's nothing wrong with having an amateur or incomplete picture of the hardware's performance characteristics - that's where I am. But I really encourage you to keep learning. For instance with my old-ish computer I have ~20KB per microsecond of memory bandwidth to play with (shared over 4 cores). Actually utilizing that requires really good prefetching. Following many pointers in a short span of time is very bad for memory bandwidth utilization. If 2,500 `String`s were 16 bytes each instead of 24 bytes, I could save a microsecond of memory utilization loading from `*const [String]`. But after 2,500 unpredicted cache misses to L3 or system RAM I don't think I'll notice an extra microsecond. In a nutshell: wasted SDRAM cycles because none of your cores know what they'll need next is *far* more likely to be a performance bottleneck than fetching extra cache lines because of a little structure bloat. I still think a smaller `String` would be cool and a nice touch, but it's unlikely to really matter. I mean, a situation where it could matter is if you have, say, a bunch of `[&amp;str]` which all point to the same buffer which fits in L2. AMD actually *does* encourage 32-bit indexes, and "fits in L2" strongly suggests "32-bit indexes." And the savings is potentially quite a bit more substantial: 8 bytes vs 16 bytes. 
Huh
Alternatively, I wonder if there have been any attempts to describe the String/Vec/RawVec type in a way which makes certain bit patterns invalid so that the enum discriminant can be encoded inline instead of as an extra field, much like how `Option&lt;Box&lt;T&gt;&gt;` occupies just _one_ pointer instead of a pointer + discriminant. For example, maybe Vec could be encoded in a way where the data pointer is guaranteed to never be zero (but must not be dereferenced if capacity is zero). Then the enum type could be 6 pointers wide: ``` enum Foobar { Foo(String), Bar(String, String) } // Foo(String) is represented as [ (data_ptr, capacity, length), (0, *, *) ] // Bar(String, String) is represented as [ (data_ptr, capacity, length), (data_ptr (non-zero), cap, len) ] ``` i.e. the data pointer of the second string could be used as the discriminant. I think the layout of Foo could actually be changed to move the discriminant to the front, as well. ``` enum Foobar { Foo(String), Bar(String, String) } // Foo(String) is represented as [ (0, *, *), (data_ptr (non-zero), capacity, length) ] // Bar(String, String) is represented as [ (data_ptr (non-zero), capacity, length), (data_ptr (non-zero), cap, len) ] ``` No idea without digging up the implementation of `RawVec` if this would have any perf impacts elsewhere. I'm impressed that discriminant encoding can be so much fun though :-)