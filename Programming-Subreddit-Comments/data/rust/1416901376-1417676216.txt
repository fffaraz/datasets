You can certainly have cyclic references in a safe and optimal way. In fact, I went ahead and implemented them for this type.
There is a third option not covered by the other comments, which is to use plain references. https://gist.github.com/pythonesque/e0662ace5d23cbdf236b is an implementation with parent pointers in this way. This should satisfy your requirements. The key is to recognize that &amp; means shared reference, not immutable. There are types with inherent mutability, like Cell, that allow you to mutate aliased memory where it is provably safe to do so.
You do not need Rc for this. Rc provides nothing over references when it comes to forming cycles (and really, you should try to avoid Rc cycles anyway, since you need to explicitly break them).
This is a good alternative indeed. If you ditch `Box` and stick to the scoped allocation instead then you can do lots of things safely. My original comment only referred to the case when you should keep `Box` (i.e. forced affinity).
Thanks for the suggestion. We are not familiar enough with Ada to know if that extension is possible. If you would like to discuss further, you are welcome to start a thread on /r/gngr.
This XML file uses CRLF as line endings. With one change: "&lt;/asset&gt;\n" to "&lt;/asset&gt;\r\n" this programs prints "find: 42".
thanks, got it. Now i use `line.trim()` instead of `line.trim_chars()`. Things work better.
1.0 is about backwards compatability. You can add more features, like named parameters, later on.
Is this orthogonal to namespacing below the username, or is it an alternative?
I think it's an effect of my old hacky body implementation. `Request` did have a body field, but it was always an empty string when I started building Rustful, so I just made a quick fix.
But the standard library won't make use of these new features. I'd say Java is evidence of the downsides that this has.
If I am reading it right, it is an alternative. I would be curious to hear the reasons that using the username as the namespace wasn't proposed.
&gt; If I am reading it right, it is an alternative That's what I thought &gt; I would be curious to hear the reasons that using the username as the namespace wasn't proposed. Me too. It seems to work nicely for Docker.
I... just think it's strange that I can accept certain types (speaking at the preprocessing level, where types means :ident, :ty, etc.) as inputs to a macro, but yet somehow I can't return something of those types. The mathematical space which Rust macros live in doesn't support an identity function (where I take in something and return the same thing), and I think much of what "feels wrong" in macros today is a direct result of this. I suppose I could submit an RFC, but I've never done that before.
Perhaps we should file a bug to remove it, then?
That comment should probably be interpreted as a facetious "I don't want to install through MSYS, so clearly anyone who does is a loony". I mean, the real solution is to distribute Cargo with Rust, which I assume is going to happen sometime in the near future *anyway*, at which point it'll be somewhat moot.
Great to hear it!
*Feeds Gankro a nice VP-Tree, with festive sprinkles on top.*
Github ran a gem host next to the official rubygems.org once, which allowed things like `skade-my-rails-plugin` It also automatically built the gems from your repository. It lead to a terrible mess of incompatible forks of libraries, as everyone would just publish and run their patch of the library. Finding out what the canonical version was, was horrible. The move did more harm then good and it took a few years to properly recover. (many of them were subsequently pushed into the main rubygems) Forks should be renamed, not namespaced.
How about Cursors (non-threadsafe fail-fast iterator alternative that is able to remove elements, analog to java iterators, or even to change values at the current position, see C++/D). Also packed bit sets (like e.g. [RoaringBitmaps](https://github.com/lemire/RoaringBitmap/)) seem like a nice addition.
Cursors are on the docket in one form or another. I'm probably going to fork std's DList temporarily to prototype it out. Every other data structure is totally of interest. But I only have so much time in the day! If you think you can implement any, please do!
Hey, I just wrote a sliding window iterator for Rust the other day! It's MIT licensed, but I'll gladly re-license it into whatever Rust needs. https://github.com/slapresta/rust-iterslide
I've started a small tutorial about it. Anyone interested? https://kintaro.github.io/rust/window-manager-in-rust-01/
[I want a streaming iterator *soooo* bad...](https://github.com/BurntSushi/rust-csv/blob/master/src/reader.rs#L662-L676)
&gt; We want to support "special" or "blessed" packages Can somebody motivate this for me? Why not just define one or more "official" namespaces or group names or whatever it ends up being?
[Flat containers!](http://www.boost.org/doc/libs/1_57_0/doc/html/container/non_standard_containers.html#container.non_standard_containers.flat_xxx)
So this is where the mad rust scientists dwell. Good to know. 
Well, obviously. PHP package manager "composer" calls them "vendors": https://packagist.org/explore/. Has no special "blessed" status, actually we are required to use vendor name, and that worked out pretty well. In fact, vendor name is kind of advertising - people learn to identify what vendors are quality ones.
I'm gonna need help reviewing all this code! Anyone want to help out officially?
It would fit into a library for persistent collections, no?
Please see rules 1 and 2. ---&gt;
The Guide is already 90 pages... ;) (I don't know if anyone else is working on anything, but I hope so!)
Do theses count?
thesis.
I agree, a library of persistent versions of the collections would be awesome, and my memory of Okasaki is not failing me finger trees are an awesome building step for some "fundamental" structures (such as a queue).
"Seven Lifetimes in seven weeks"
I'd like to borrow that book and inline some notes.
"A Lifetime to Learn, A Lifetime to Master"
I think the official guide and Rust for Rubyists are of the size and quality of a book.
I think the Guide is already pretty verbose. What kind of book would you like to see?
ATS?
"The Rust Programming Language", to keep in line with the classics of C and C++ :)
Does it not work to simply expose Hyper's `Request`? Or take the `Reader` from it to use in place of the `body: Vec&lt;u8&gt;`?
&lt;3. R4R's quality has deteriorated, I need to actually fully deprecate it.
If this were implemented, all that'd happen is that everyone would register their username as a group for all their projects, to get away from the global namespace and also to fork other users' projects under the same name under their own namespace. If the idea was not to have user namespaces for some specific reason, this will negate that.
I made the experience that I learned new languages best with a printed book in my hand that explains not only the syntax and the "how", but also the "why". I think this is extra important for languages that introduce new concepts (borrowing, lifetimes...). Also getting my hands dirty solving some exercises after each chaper helps me a lot. A very good example is "Erlang Programming" by Francesco Cesarini, Simon Thompson. I worked it through from the front to the back solving all excercises, digging deeper and deeper.
&gt; Cursors are on the docket in one form or another. Pre or post 1.0? Edit: Since I could not find an RFC, but just this "pseudo-RFC" by you, I'm guessing it's not planned for the 1.0 release. http://discuss.rust-lang.org/t/pseudo-rfc-cursors-reversible-iterators/386
I have a lifetime problem
Can't data-flow analysis figure this one out? If there's no read from an uninitialised value, the zeroing can be optimised away. That, or make the whole thing explicit like in ATS, yes. Somethingsomething annotate the lifetime with "uninitialised", making it write-only until it's completely written to. Which is, as so many things, infeasible in the general case^1 but something usable shouldn't be too complicated. ^1 e.g. it's hard to figure out whether an arbitrary piece of code writes to every element in an array. It's easy if it's obvious, it's hard to impossible to decide either way if it isn't.
What about marking types that don't need to be zeroed on Drop? Not zeroing, in general, is a security issue: after you are done with some sensitive data (such as cryptographic keys) you should zero is ASAP to prevent a data leak to reveal it. Rust isn't immune to bugs like Heartbleed - many code will use C libs, and even if not it has `unsafe` after all. The compiler can't know which data is sensitive and which isn't, and without further hint it should be conservative.
You haven't missed the [Modules and crates guide](http://doc.rust-lang.org/guide-crates.html), have you? As of whether I think it's good or bad: It has a few quirks, e g, that "use" statements always start at the root of a namespace where everything else starts at the current module namespace. Also when using "external crate" imports from within a module, that crate is inserted into the module's namespace, potentially causing problems with macros: the log crate's macros don't work unless you import it in the crate root, e g. (I suspect that the proposed macro reform with solve this, but I have not looked.) Also you suggest that it should be possible to define a module across several files, and that you would implicitly insert "use super::*" into every module. I haven't used Rust long enough to tell if this is good or bad, maybe the more experienced ones can answer that.
"The problem of a lifetime" 
The reason drops cause zeroing is not related to security. When you need such a thing, it should be easy to define your own `Drop` implementation that does the zeroing, instead of making every single type pay for it.
Ah, but there IS a read from that zeroed array! When the drop flag is checked later, it has to read out the zero. Therefore they can't optimize it away. It might be an llvm bug, it might be a rustc bug. All I know is that the generated code is terrifyingly bad.
"Learn Rust in 1 lifetime." Like all those "Learn &lt;language&gt; in 24 hours" books.
Why does it zero on drop?
To avoid double-drops. It can be better: https://github.com/rust-lang/rfcs/pull/320
&gt; Hidden bits are bad: Users coming from a C/C++ background expect struct Foo { x: u32, y: u32 } to occupy 8 bytes, but if Foo implements Drop, the hidden drop flag will cause it to double in size (16 bytes). Wow.
I have a concrete example in the intro: http://doc.rust-lang.org/nightly/intro.html#ownership In the example, a mutable, aliased pointer causes undefined behavior in C++, but is prevented at compile time in Rust.
I'm at least thinking intensely about it.
(2) and (3) are both special cases of use after free (in C++, at least), and the reason for this is that you can actually call `drop()` through a `&amp;mut` reference (as long as you can acquire at least one instance of the actual type, which is almost always the case, and perhaps always in existing Rust code). It is only recently that mutation through an *instance* was disallowed if you had a `&amp;mut` pointer to the instance. This was done to provide an additional benefit--safe mutation of data rooted in another thread's stack. Rust's standard library does not provide support for this yet, but the knowledge that `&amp;mut` references are actually unique means that you can never have a data race through one, which allows you to write high performance parallel code safely. If you know data can be restricted to a single thread, and the type you want to share fulfills `Copy`, you *can* have safe, shared mutable state through a `Cell`. And in theory Rust could have a type of reference that allowed mutation of shared state as long as mutation of the state did not change the data representation (basically, the same restriction a garbage collector enforces), but it's beyond Rust's current type system to express this safely.
Is servo much faster on OSX than linux?
"Effective Rust" Actually, let's skip that one.
Which stuff? I think if you want to learn about very specific and advanced things, *less* verbose resources are more useful (like Rust by Example or the standard library documentation).
The rust community is actively setting up a user forum (probably at discuss.rust-lang.org and moving that existing one to internals.rust-lang.org) to provide a non-reddit place for the community to. I agree with you in general, and would prefer not to post here. (Though I do still read several subreddits) http://discuss.rust-lang.org/t/distinction-between-user-and-developer-audiences-for-this-forum/925
Did you have a look at Rust by Example? It is the opposite of verbose, but it is nice for getting your hands dirty.
ATS is a programming language. See this blog post for a description of how ATS handles uninitialized memory: http://bluishcoder.co.nz/2013/01/25/an-introduction-to-pointers-in-ats.html In general I think ATS is a very under appreciated language. It has the same goals as Rust, namely safe low level programming. Unlike Rust it has a strong basis in type theory. This makes it a lot less familiar to C language family programmers, and in some cases less convenient, but its type system is much less ad-hoc and far more powerful (many of the programs that require unsafe in Rust can be expressed in safe ATS). I think it would be great to base a language like Rust on a simple core language where higher level features are provided as syntactic sugar over e.g. linear capabilities.
Do you have any links to such hateful comments in /r/programming? I can't recall seeing any in recent times, and the ones I have seen are always heavily downvoted.
Any place popular on the Web eventually gets filled with such trolls. See any traditional news media corporation's site, forum/BBS, and you'll see the same kinds of... "unenlightened" comments. Continually running just seems to avoid the problem, though, and with Reddit's up/down-voting it's much easier to manage the communities/discussions.
(Disclaimer: I haven't coded in ATS) I suspect that, in the way that Rust is somewhat lacking in the type system department compared to ATS - which might necessitate more unsafe blocks because the compiler can not verify the safeness - ATS might be less ergonomic when it comes to more run-of-the-mill GC-less code (which would validate your point about how high-level features could be offered above a more advanced underlying system). Low-level (no GC, etc.) code in ATS seems harder to get working (ie, be definitely memory safe) than in Rust. It's interesting to note that Rust started out with a fair use of GC pointers, but that that simply fell by the side as the whole "manual" memory management got pleasant to use (or at least that's my impression). And though GC/RC-code is probably still easier to write, it seems that there really is no culture for it - people would rather use the "manual" features. On the other hand, ATS' creator seems to prefer himself to use the GC whenever he can get away with it. It might be that that's just how he likes to work - I guess an ML language might be more pleasant to use that way - but I wonder if run-of-the-mill performant (assuming no-GC is more performant) code is more of a bother to write in ATS than in Rust? This is all pure speculation on my part, though. Anyway: something that would be interesting is if some people would write whatever code that would have to be marked as "unsafe" in Rust in ATS; make all the necessary proofs to make sure that it is safe according to Rust's definition of safe, compile to C, and call the generated C from Rust.
In Java, sandboxing is implemented almost entirely in the libraries. It had little to do with the runtime. The Rust standard library *could* do the same, but (1) isn't. Also, rust allows, an an exceptional case, (2) unsafe code and (3) the invocation of C routines. So, I agree with you that Rust is not as safe as Java, but it's generally fairly safe from most programmer mistakes. To the get same safety as Java, Rust would need to remove the 'unsafe' blocks and modify much of library to implement sandboxing. Of course, sandboxing could be bolted on externally, like how Chrome (or Sandboxie) works. Considering all of the sandbox exploits that have happened in Java, that might be a better choice.
Is calling people "nazis" an example of the behavior you bemoan? Or are these individuals to whom you refer actually part of the National Socialist Party?
I was under the impression that the primary motivation for forbidding mutable aliasing was fat pointers. For example, a slice is actually a pointer + a length as a primitive type. If you allow mutable aliasing; or indeed, any borrow of an element that is mutable, then you potentially end up with a situation where you have the incorrect data in a fat pointer. I'm not 100% sure what the 'fat pointer' type in rust are, but there are a few I think?
https://yourlogicalfallacyis.com/composition-division I think you're being too general. Yeah, Reddit has some jerks. But that's a consequence of being such an open platform. Anyone can speak their mind and do what they want here without fear of retribution in all but the most extreme of cases. And some people simply aren't aware that they might be coming on too strongly for others. The biggest feature of Reddit is being able to custom-tailor your experience, and IMO, people who have developed opinions like yours simply haven't taken full advantage of this. I know that's horribly generalized but I've found it to be true in many cases. After I unsubbed from most of the defaults excepting /r/AskReddit (because a lot of the threads there are pretty entertaining) and a couple others, I noticed that my perception of the quality of posts and comments on Reddit improved dramatically. I think having a sub for Rust improves its exposure and thus, perceived accessibility. It also serves as evidence of the community's friendliness and openness: every day, the frontpage is filled with people getting help with the language and showing off their pet projects, and I think that really resonates with passerby. You're gonna come across jerks no matter where you go. A Rust-specific forum would still need moderators to deal with trolls and assholes, no different than any subreddit does. And it lacks the benefit of being connected to a massive internet community, even though, as you've shown, that's a double-edged sword. Don't misunderstand me, I'm not arguing against a Rust community forum. Some people just don't like Reddit, others just come here for Rust and would rather have a place specifically for it. I just don't want Reddit to be totally discounted in the discussion, because I actually kind of like it here. But I think we should choose one specific place to promote as **the** community forum, because fragmentation means people in one place will miss important discussion in the other if they don't frequent both, and I don't think that's good for the community as a whole.
I agree almost entirely with you. I don't get this part however: &gt; It had little to do with the runtime I thought the fact that the runtime is interpreted majorly helps sandboxing out JNI calls. And preventing JNI is important, because native code can then subvert the sandbox. Indeed, the libraries make the actual call to `AccessController.checkPermission()`, but I believe the edifice is held together by the runtime and its characteristics. 
I always forget the `self` stuff and get confused with the errors... and the fact that you have `use foo::Bar;` for importing and `::foo::Bar(..)` in the body of your code is kinda inconsistent... :(
Right, but exposing the underlying Hyper objects directly is problematic with the current design, which simply copies out the necessary information.
Java doesn't exactly sandbox JNI calls. It would be terribly inefficent if it checked every low-level method call. Instead, it sandboxes `System.loadLibrary()`. You can't call a native JNI method if you can't load its library. Again, that's just a library call. Also, Java is not necessarily interpreted. Heavily used methods are converted to x86 by the JIT by Hotspot.
I actually don't like the forum layout very much, the lack of thread-iness really makes it hard for me to follow discussions over 8 or 10 comments. Most threads seem to be short enough for it not to matter, but that's just cause the community there is fairly small.
Is there an issue with simply using Hyper's `Request` object, or wrapping it instead of consuming it? I get that you want each middleware to be able to access the request body, but that might not be feasible with large request bodies such as what multipart handles. And it would be empty with `GET` requests, so accessing it then isn't necessary. Perhaps another middleware, `Transformer`, that runs before `BeforeMiddleware` and is expected to process the request body, then make the actual `Request::body` field polymorphic: pub enum RequestBody { Empty, Data(Vec&lt;u8&gt;), Typed, // Somehow do this without generics. Trait object or phantom type? } If there's no `Transformer` middleware, the body is read and passed to `BeforeMiddleware` as `RequestBody::Data`. Since it's assumed that a `Transformer` would read the body to EOF, Iron would need to choose which one to call out of several, perhaps letting the `Transformer`s decide for themselves and choosing the first one that confirms it can process this request. For example there could be a `Transformer` that reads the request body to JSON if the headers specify that, or a transformer that reads the `multipart/form-data` entries, keeping a map of fields and saving the files to a temporary directory, or another one that reads it as XML. 
Have you looked at the video [Introduction to programming safe and efficient systems in Rust](http://vimeo.com/111852387)? It may save some preparation time for you.
&gt; Instead, it sandboxes System.loadLibrary(). You can't call a native JNI method if you can't load its library. Again, that's just a library call. Thanks; I stand corrected. That was a hole in my understanding. So JNI is protected through *link-time* sandboxing. In JVM, the linking is done at the time of execution, so it could be loosely said that this is *runtime* sandboxing. Is there some other permission that requires the runtime? I ask because in the parent post you said "little to do with runtime", which leaves a slim possibility open. &gt; Also, Java is not necessarily interpreted. Heavily used methods are converted to x86 by the JIT by Hotspot. True, but the translator could first check for JNI calls and special case them. But anyway, since JNI calls are not being checked, my point is moot.
I believe so, due to X things, but I'm not sure.
That...would actually make a lot of sense. I like that term better than lifetimes, in fact.
FTR, I didn't downvote you, and you are right. But proving such things is non trivial, and in any case is a change of the language specs. Which means, every library used by the project will have to be written with the altered specs in mind. Edit: One example where expressibility can be sacrificed: if (debug) { logToFile() } where `debug` is a mutable variable with some non-local scope. Expressing this won't be possible in the hypothetical language.
&gt; Is there some other permission that requires the runtime? No.
The JVM security policy is separate from the OS's policy. The JVM's security manager is for the app's *internal* sandboxing, say for example, when running third-party code. To the OS, it is just another app and when it receives a call, it applies its usual policy. I hope that clarifies things.
Interesting. This and the /r/golang/ are the only ones I frequent. So far, so good.
I personally find @-style attributes very ugly, and my pre-Rust programming experience is mostly in Java. Are there any other benefits to this other than the appearance of attributes?
If I get time I might try writing one, but this would have to be post-1.0, and mostly an ebook.
I really don't care if we use `@` or `#` as the sigil, but the braces have to go. They're what make the attribute syntax the second-ugliest thing left in Rust.
What would you say is the most ugly thing in Rust? :D
I like it. But at this point it might be better to stick to "lifetime". There's a whole bunch of tutorials and articles out there that use the term "lifetime", so people new to rust will almost certainly encounter that term at some point. It wouldn't be ideal for them to have to learn two terms for the same thing.
The `::&lt;&gt;` syntax for type specification. Thankfully type inference means you don't have to see it exceedingly often. And no, switching to use `[]` instead of `&lt;&gt;` doesn't resolve the ambiguity. Sadly, my suggestion to fix this by nixing the less-than operator `&lt;` and replacing it with `!&gt;=` was not met with much enthusiasm.
Oh, yea. That black sheep that confuses new people because it's not standard with how all other type arguments are passed... They should just nuke the `::` part from orbit, for the LOW LOW price of arbitrary look-ahead in the grammar parsing. What's the worst that can happen?
&gt; for the LOW LOW price of arbitrary look-ahead in the grammar parsing. What's the worst that can happen? I'm not too familiar with grammar parsing, but I sense sarcasm. Will it really increase compile times that much?
`::` is my least favorite part of the syntax. I like the look of it, but I find that those extra characters discourage heavy use of namespacing; even a few characters matters. It's too bad a TOP modifier key didn't stick around. http://en.wikipedia.org/wiki/Space-cadet_keyboard#mediaviewer/File:Space-cadet.jpg
No, a "lifespan" would be a measure of the *length* itself; it's not anchored to anything.
I'm so glad there's no books so far. Programming books are horrible.
On my fairly fast laptop it takes about 0.22 seconds.
Is "lifetime" anchored to anything? I thought both terms refer to the lifespan (or lifetime) of the reference pointer itself. They should make no difference here.
 [fc:~/rusttest/src]$ time rustc main.rs rustc main.rs 0.25s user 0.33s system 6% cpu 8.732 total [fc:~/rusttest/src]$ rustc --version rustc 0.13.0-nightly (395901393 2014-11-24 00:46:30 +0000) Thanks. I think it's my system's issue. I have 2.8GHz i7 on OSX 10.10, guess I need to reboot..
Does it *always* take 7-10 seconds? Or is it just the first time?
 #[foo(bar)] fn something() { .... } #foo(bar) fn something() { .... } @foo(bar) fn something() { .... } I quite like `#foo(bar)`. It doesn't look like @attributes from Python and Java, which is a good thing, because they are fundamentally different.
I did try a few times to test, and most of them took 7-10, not just first one. Some did compile in &lt;1s though. so I guess it's just my system's issue. 
Arbitrary lookahead isn't suffucient - the grammar becomes context sensitive so you actually need something like C's lexer hack: http://is.gd/9PM6Un
Indeed, the change cost is a valid concern. (But we have made many changes so far, haven't we? At least it's not too late before 1.0 is out.) I made the suggestion because lifetime is such an important concept yet also stumps many newcomers. If changing the term makes the concept (and the language) more accessible, I think it's worth consideration. 
I would say `'a` lives longer than `'b`, but the "longer" here has some 1-dimensional spatial perspective.
Yes I've seen it and it helped me sometimes when I had syntax problems. The advanced parts are unfinished but still one of the rare resources on these topics. It's not that convenient to learn a language by reading blogposts, API-docs, Videos and RFCs. I think every language needs a "The XXX Programming Language" book as solid foundation for getting started and reference.
+1 for less typing. With a German keyboard, you have to use AltGr+8 for [ and AltGr+9 for ], so it's 4 additional keystrokes. I don't care much about the sigil, though the precedent set by Python and Java will flatten the learning curve by just a bit. 
`'a` *outlives* `'b`. Edit: Or survives. Whatever you like best.
&gt; I personally think the module system makes a lot of sense. The only thing that IMHO doesn't make that much sense is that 'use' have to be before 'mod' statements, but 'extern crate' statements have to be before 'use' statements. For me the most intuitive order would be: extern crate ... mod ... use ... 
Sure. use std::mem; struct T { a: i32, b: i32 } fn main() { let t = T { a: 0x1234, b: 0xabcd }; println!("{} bytes of T:", mem::size_of::&lt;T&gt;()); let view = &amp;t as *const _ as *const u8; for i in range(0, mem::size_of::&lt;T&gt;() as int) { print!("{:02x} ", unsafe {*view.offset(i)}); } println!(""); println!("10 bytes of main"); let view = main as *const u8; for i in range(0, 10) { print!("{:02x} ", unsafe {*view.offset(i)}); } println!(""); } This is highly unsafe, obviously. Also, if you try to modify some value through a slice of the type different from the original, you might end up with the incorrect code *thanks* to the type-based alias analysis (TBAA). Be very careful.
Thanks. Yes, I understand that most of the time this is not a good practice in doing anything that should work, even in C. But there are projects that need this kind of data manipulation, i.e. some kind of hot-patching mechanism, self-modifiable code, stuff that executable protectors/packers perform, etc.
I still want to hear your use case. Are you going to create a self-extracting executable, for example?
It wouldn't no. However it also wouldn't be sufficient to fix the ambiguity, as pointed out by /u/Rothon in a sibling comment.
I've got a pretty optimized version of a [HAMT](https://en.wikipedia.org/wiki/Hash_array_mapped_trie) here: https://github.com/michaelwoerister/rs-persistent-datastructures It currently is very much out of date and won't compile but with a few hours of work it should be fine again.
I'm afraid my case is rather boring, because I don't have any particular project in mind. I simply like to inspect generated code under a disassembler. Viewing calling conventions used, methods for accessing stack variables, code that is being generated. At one point of time I was really interested in software reverse engineering, which (among others) included modification of a compiled executable to load a custom DLL, which performed additional functionality inside the program. That required reversing the binary layout of the structures used by the program. All of this I was doing by using Assembly and C(++). Rust is said to be a system language, capable of doing low level things like that, so it feels natural to me to be able to perform operations like this.
Or just allow any order. That would enable me to keep things in a logical order that would make the most sense, e g: mod foo1; use foo1::bar1; extern crate foo2; use foo2::bar2; mod foo3; use foo3::bar3; 
There's a rustier way to do it -- using std::slice::from_raw_buf and you'll get a byte slice.
@ -&gt; at -&gt; attribute. Can't believe I hadn't spotted that till now *facepalm* I like it
If trolls turn up, they can troll a "user forum" just as well as reddit. The answer is to have a positive community that doesn't tolerate them and downvotes them or whatever. Also Rust is technical in a way that really doesn't encourage trollish comments, because it is hard to derail discussions when most people are pretty aware (e.g. ranging from "cleverer than you" to "way way cleverer than you").
It is referenced indirectly from home page &gt; All docs &gt; Guides &gt; Crates and modules
Reddit tolerates the shit, though. Thefappening wasn't taken down immediately. Jailbait was around forever. That is enough to make me not want to touch the site and be associated with that.
There are sysems who are partly designed to run without allocating memory and working within a predetermined (usually small) chunk of memory. Think about database error handlers, bomb disposal robots or programs for launching stuff into space.
&gt; Hongwei has said that their style is write code using GC to get things working quickly then use linear types and other features of ATS to not use GC and have the compiler complain about where things need to be fixed. From what I've read, his style is to first write the code using GC and then - if it is needed - cut out the GC by using linear types etc. I have had the impression that for him, using GC is an acceptable overhead in a lot of cases. *shrug*
like feces and fecis
Rust by example gives this warning on its pages now: &gt; WARNING! This site is currently unmaintained, and has been like in that state for several weeks/months now. The content is likely to be outdated and misleading, proceed at your own risk! I highly encourage you to read the official guide instead. - The Author
&gt; let view = &amp;t as *const _ as *const u8; " * const _ " ? Never seen that one before. As you might know, in C "void * " pointers are allowed to be cast back and forth to other types. Is this something similar, and would it make things easier for callbacks from C, if the userdata pointer was a " * mut _" instead of a " * mut libc::c_void"?
Programming language syntax is not suitable for non-english keyboard layouts. I've switched to programmer dvorak to avoid RSI and type faster. The bad part is that I need three keypresses for localized letters (using a compose key). It has been worth it several times over though, and I would recommend giving it a try.
huh. Then why does pointer aliasing (eg. via unsafe code) result in undefined behavior? Or doesn't it? 
I'd like to add to what cgaebel said: zeroing on drop absolutely not sufficient security-wise if you don't want to have copies of sensitive information lingering on the system when they're not needed anymore. If the pages used to store the sensitive data were swapped at any moment, zeroing them out at deletion won't do much good. Especially since it's easier for an attacker with physical access to a machine to dump the swap rather than the live RAM. If you want to do that correctly you have to instruct the OS never to swap those pages and then you can consider zeroing the pages out (and you have to be careful doing that to make sure the memory is actually zeroed, it's the kind of stuff that can be optimized out at several levels, in particular compilers can decide to remove memset calls when the values are never read from again).
1. Pretty much any time that you can't return a `&amp;str`. If the string didn't exist before your call, it'll probably have to be a `String` just to return ownership. 2. Assuming they're all unique, that sounds fine. If they're mostly unique, still reasonable. If you're really, pathologically afraid of allocation, you could always pass a pre-allocated buffer *into* the function, and write the result into that. There's `std::borrow::Cow` that can help here: return a slice where possible, fall back to an owned `String` otherwise. 3. [std::str::StrPrelude::split](http://doc.rust-lang.org/std/str/trait.StrPrelude.html#tymethod.split). `Prelude` = automatically imported from the standard library for you, `Str` = applies to `&amp;str`. And since `String` auto-derefs to `&amp;str`, you should be able to use it on either a `&amp;str` or `String`. 4. It's reasonable enough. `collect` will take advantage of size hints, if available, to pre-allocate enough space. Also, `connect` computes the amount of space needed, so it only allocates once. AFAIK, there's no guaranteed single allocation method for that.
With the advantage that the unsafe `offset` is no longer needed, and you can use normal iterators and indexing. Rust will do the bounds checking to make sure you aren't reading beyond what you have
I like `#[foo]` better, though I'm not opposed to the `@` syntax. What makes me -1 this is that it will break virtually every crate out there (again), and this time for an aesthetic change, with impromevements that are subjective at best :(
&gt;&gt; What's the worst that can happen? The worst is you could be back to C++'s parsing issues. I'd much rather type ::&lt;T&gt;; thankfully the inference means you don't need to type it often. 
Thanks for the quick feedback. I'm not gonna bother with passing a buffer, I really like how I don't have to think about this in the current version. For some reason, I seem to have missed _StrPrelude::split_. `sort_str.split(',')` works the same as the Regex variant, thanks for mentioning it.
I do more thinking and less coding to avoid RSI. And I type fast enough even when typing stuff like angle brackets, even with QWERTZ (or sometimes QWERTY).
Unrelated hint: .map(convert_one_sort_str_field_to_sql) .filter(|input| input.is_some()) .map(|input| input.unwrap()) Can be simplified as: .filter_map(convert_one_sort_str_field_to_sql) 
Seeing as the amount of research done on register allocation is so extensive and there is some concept overlap, do you think it would be possible to "borrow" some of their results. Pun is bad and I feel bad.
"Lifescope".
Thanks! That reduces the boilerplate code for dealing with the `Option`s quite nicely!
I don't think lifespan makes any more sense then lifetime, except for that lifetime is far more common for non-english speakers then lifespan. In the end it sounds to me like the same thing but more uncommon to say. Also humans may not see time. As i said in the discussion i would prefer some wording around the borrowing/leasing concept (aka lease or whatever makes sense) but in terms of lifetime vs. lifespan, it means the same thing only that lifetime seems to be more common. Go for the more common word, imo.
It should be possible to provide a tool to convert code automatically, so breakage should be minimal.
You want to test how will your app will react to bad network or just the network?
maybe, agree this relates more to a spatial region of the programs' nested (scope) structure, whilst lifetime sounds more like a dynamic concept
This might be a bit horrible, but in my UDP library, I spawned dummy receiving sockets in a separate task as part of the unit tests.
Coming from a C++ and C# background, I find I also prefer the brackets for the reasons you cite.
I want to test how the library will react to network failures.
I'm in!!!
Any chance of running it around FOSDEM? I know quite some people into Rust that will there (including me) and Amsterdam is a convenient hop.
FOSDEM is early this year (31st Jan and 1st Feb), so that would fit your bill.
But it's so basic! I don't really have much to show for my two years of yak shaving :(
I don't understand why people are fine with this purely cosmetical change when proposals replacing the generic '&lt;&gt;' by something else were panned as "bikeshed". I don't really care either way but seems very arbitrary and it breaks a lot of code for the sake of breaking a lot of code IMO. 
The underscore tells the compiler to infer that part of the type. It's the same as if they wrote &amp;t as *const T as *const u8 It just saves space if 'T' was a longer type
Oh it is! Hadn't checked yet. I'll be there in any case :-)
Oh yes! \o/ Although I can't get it to work: note: /usr/bin/ld.bfd.real: cannot find -lclang collect2: error: ld returned 1 exit status This is Ubuntu 14.04 with rust nightly. (I tried installing some llvm development packages, but it did not help.) And with the following in Cargo.toml: [dependencies] rust-bindgen = "~0.13.0" I also tried the git repo but had the same problem. 
Oh no! /o\
If you want to expand your horizons further, you should look [Ocaml](https://ocaml.org/) (the language Rust was bootstrapped from). It's pretty normal to leave pretty much all the type annotations off and the compiler happily figures it all out. It looks like a scripting language except everything is actually built on one of the soundest static type systems you can find.
&gt; or even to change values at the current position Rust already has iterators over mutable references. The iterators in Rust are the same concept as ranges in D.
The `struct` representation was changed from using the C representation to being implementation defined, with `#[repr(C)]` to use the C representation. If it was defined, the benefits from moving towards the current implementation defined representation would be lost. It was done to permit optimizations and fuzzing / hardening via field order randomization. Thanks to features like private fields, generics and `type_id` there is little hope of writing a true Rust API with a stable ABI anyway. You already have to limit yourself to a C-like ABI and at that point you can use `#[repr(C)]` and `extern "C"` for a well-defined, stable ABI.
Rust doesn't zero on drop. It zeroes on *moves* out of types with destructors. It is not an implementation of data sanitization. For example, vectors certainly don't zero out the old data when they move the elements to a new allocation.
Rust doesn't zero on drop. It zeroes on *moves* out of types with destructors. It is not an implementation of data sanitization. For example, vectors certainly don't zero out the old data when they move the elements to a new allocation.
Don't worry. You're currently at 20 upvotes. That means there's probably 19 other people who didn't notice this until you pointed it out.
You also have to consider `#[test("foo")]` wherein among the three matching sets of delimeters the brackets contribute only noise.
Thanks. :) That's the best thing an author can hear.
&gt; I simply like to inspect generated code under a disassembler. If you didn't know... steve@warmachine:~/tmp$ cat hello.rs fn main() { println!("Hello, world"); } steve@warmachine:~/tmp$ rustc --emit=asm hello.rs steve@warmachine:~/tmp$ head hello.s .text .file "hello.0.rs" .section .text._ZN4main20h690a1620a7395274eaaE,"ax",@progbits .align 16, 0x90 .type _ZN4main20h690a1620a7395274eaaE,@function _ZN4main20h690a1620a7395274eaaE: .cfi_startproc cmpq %fs:112, %rsp ja .LBB0_0 movabsq $136, %r10 EDIT: aaaaand as I read downthread, I see this is already mentioned. :)
https://github.com/rust-lang/rust/issues/10052 is the relevant ticket. You can see all the activity! :p
Well, some rust dev did have to accept the patch I suppose. I know I shouldn't expect stability pre-1.0 but I thought that change in particular was a bit... brutal, especially since deprecation would have been trivial to implement. An other point to consider is that those "cosmetic" changes break a lot of existing rust code that'll probably never get updated, stuff like stack overflow answers and the like. Again, I'm not arguing for a complete freeze of rust syntax of course, just that in the case of @ attributes it seems like a very arbitrary and subjective change with not a lot of obvious benefits.
I apologize if it seemed as though I were denigrating your efforts, my point was only that interpreting the handling of the panic change as some grander strategic policy of immediate deprecation is uncharitable. :)
&gt; And since String auto-derefs to &amp;str, you should be able to use it on either a &amp;str or String. How so? The best thing I can get is: #![feature(slicing_syntax)] fn main() { let string = "xxx".to_string(); let str: &amp;str = string[]; } 
Every Stack Overflow question ever answered already cannot be trusted and should be considered as good as broken. Just because a given answer compiles doesn't mean the underlying semantics haven't silently changed. In the run-up to 1.0 I, personally, intend to quit my full-time job so that I can spend all day every day doing things like updating *every* SO answer for Rust 1.0 and adding automatic regression tests for each so that we can automatically detect when answers need to be updated. In fact, I *encourage* such breakage at this stage, because it makes it ever more obvious that outdated code is suspect when the syntax is immediately recognizable as incorrect. I have considered submitting an RFC that does nothing except deliberately break code in pursuit of this goal, while providing an automatic tool in the compiler (the beginnings of a rustfix, as it were) to automatically alert people to the existence of pre-1.0 code and offer to update the syntax for them. This sounds extreme, but I personally believe that dealing with outdated third-party documentation is going to be one of our biggest challenges in the coming year.
No no, I read it as saving me from criticism &lt;3 In general, I strongly believe that the deprecation strategy is correct. I had just gone on a rant on Twitter about not breaking the universe earlier that day, actually. There's just always exceptions...
I was hacking on some rust code that implements a simple parser. I found closures in Rust to first of all not work the way they wanted to because the way they consume local variables makes them hard to use, and then I found it unnecessarily annoying to re-factor my code to use functions rather than closures. Has there already been any talk about implementing these factorability ideas from Jonathan Blow in Rust?
This "time" output looks very weird to me, it seems most of the compilation time has been spent doing nothing. Could your filesystem be very slow? Are you running on NFS or something? Maybe running strace on rustc might give you an idea about what's blocking exactly.
I like it. But I would generalize this further. In any context where a value could be a specific enum type, name resolution should peek inside that enum's namespace. That would let you do this, as well: fn stooge_call(stooge: Stooge) -&gt; { ... } fn main() { stooge_call(Larry); // &lt;-- implicitly look inside Stooge and find Larry } You couldn't do this, though, because the context is ambiguous: let stooge = Larry; So you would do this: let stooge: Stooge = Larry; Or this: let stooge = Stooge::Larry; 
Sounds like it to me.
This is a bug: https://github.com/rust-lang/rust/issues/19293 And it should soon be fixed: https://github.com/rust-lang/rust/pull/19317
I actually can't pronounce (or hear) the difference between *f* and *th*, so I tend to avoid the word *theses* in conversation. This doesn't take too much effort. :)
I thought "Effective X" was more of a thing for old enough languages where the warts have been discovered and are being actively avoided by the initiated, and such a book is a guide to avoiding them? ;)
I'm not aware of the existence of any *proposals* to switch generic syntax from `&lt;&gt;` to e.g. `[]`. I'm aware of people posting on reddit that they wished it was that way, but that's far from a proposal. Has anyone actually gone through and fully specified the proposed changes to the grammar, the interaction with indexing syntax, slicing syntax, array literal syntax, etc?
Does anybody know if anything along these lines has landed in the standard library yet? 
Previous discussion about the same video: https://www.reddit.com/r/rust/comments/2hjrrm/jonathan_blow_declarations_and_factorability/
It depends. The primary rule of resolution is that if ambiguity there is, you ask the writer to disambiguate :) In the case of nested enums such as `Option&lt;Stooge&gt;` you would need `Some(Larry)` anyway, and therefore `Some` has to be matched in `Option` and once that is determined peeking inside `Some` reveals that `Larry` has to be matched in `Stooge`. In the case of overloads (via traits), then you would require disambiguation.
Yes, I had written the RFC to switch them up. However, it's primary purpose was to get rid of any ambiguity but I overlooked certain aspects of the syntax and it resulted in purely being a bikeshed for a different syntax.
Not sure what you mean, since C++ doesn't have lifetimes. I just meant that coming from a background where's there's no such thing, I still think the concept was easy to grasp.
Agreed. But in this case, I'd say we just don't bother searching enums for you. If the context is unambiguously an enum type, then we search. If it is anything else (including a generic type parameter), we don't.
&gt; I found closures in Rust to first of all not work the way they wanted to because the way they consume local variables makes them hard to use It's very rare that a closure is used where a function is used or the other way round.
I'm sorry. I used to use a keyboard where / could only be accessed by AltGr+q, and it was annoying, so I remapped the "menu" key to /.
I am not aware of any difference in regard to fat or thin pointers there. The same rules apply for both.
&gt; Functions and closures are very different in terms of cost. Not really. The difference between `fn` and `&amp;Fn` is one extra pointer. Different, yes, but not *very*. And incidentally, any closure with a `'static` lifetime is equivalent to an `fn`. The more fundamental problem (as far as I see it) if we were to attempt to entirely replace `fn` with `&amp;'static Fn` (which we don't want to, because of the one extra pointer), is what happens to associated functions (methods) in traits, especially given that `Fn`, itself, is a trait? Is it turtles all the way down?
There's two separate things here: 1. fn -&gt; closure: cheap, easy. as you say, `'static`. They even coerce for you! 2. closure -&gt; fn: not cheap, not easy. While there is only one more pointer to the type, it points to an environment...
:confetti_ball:
i think thats just the nature of them .. you can choose to take advantage of the convenience of capture , it does more good than harm compared to rolling a function object in pre c++11. he's got a problem with twiddling the |x| syntax but again, it allows other conveniences, I haven't seen if he's going to have inferred return types in his language.. the inference in arguments &amp; return of closures in rust is seriously nice IMO. he seems to want -&gt; to construct functions/closures alike, is it always going to have to be (args)-&gt;return_type &lt;expr&gt; ? I have wondered if simply (x){...} could be a lambda but it seems like asking for trouble somewhere. I agree with the points he makes about refactorability generally but most of his hostility is toward C++ (I personally bitterly hate the asymmetry between functions and methods classes)
Yeah, if it had been capable of getting rid of `::&lt;&gt;` I would have been for it. But in the absence of that I think we're better served by the C++ precedent of `&lt;&gt;`.
Is `move ||` going to go away once we have better inference? Or is it staying forever (or until we decide to change the name)?
Kudos to the rust team!
Usually `-lclang` means it's looking for `libclang.so` or something similar. Try: sudo apt-get install libclang-dev That should fix that issue.
If the environment is empty, it should be possible to cleanly "upgrade" an unboxed closure to a function. They're already zero-sized anonymous types. Give it a name and pass it by value, and you're golden. (I am only tenuously aware of the plans in this area)
There used to be a `{:?}` format specifier that would print the raw type and contents of any struct.
Yup, that's true.
Example output: // collections::vec::Vec&lt;uint&gt;{len: 2u, cap: 4u, ptr: (0x7f4231c26000 as *mut ())} println!("{:?}", vec![0u, 1]); println!("{:?}", &amp;[0u, 1]); // &amp;[0u, 1u] println!("{:?}", box 0u); // box 0u println!("{:?}", &amp;0u); // &amp;0u Check this [playpen link](http://is.gd/4g7N6r)
Thank you all for your help, I'm immensely grateful :) It all did clear some things for me.
`move ||` actually allows for much greater inference than the original unboxed closures plan, which required separate constructs for different kinds of captures: `||`/`|&amp;mut:|`, `|&amp;:|`, and `|:|`, as well as `ref` versions of each of these. I think `move ||` is a pretty good place to have ended up at, as it gives us a nice balance of power and usability. See https://github.com/rust-lang/rfcs/blob/master/text/0231-upvar-capture-inference.md for the full rationale.
Coming more from Haskell, I don't see the problem with calling it lifetime. Is there some sort of stigma attached to the word, since it's become such a bikeshed issue? It just makes me feel as if there's some sort of language barrier that I don't understand (I'm not a native speaker of English). The word seems fine to me.
Well you have been a large part of helping build up the rust gaming ecosystem. Which we are all grateful for, and is an impressive achievement.
&gt; used to be Meaning it's gone since [#18064](https://github.com/rust-lang/rust/pull/18064)
Awesome! :) Is the feature gate for unboxed closures still around, or creating tasks now will be de-facto feature-gated (due to needing UC)?
No offence taken. As part of my usual workflow I use a lot of keybindings, some of them custom. I also use some templates I added to my IDE of choice.
Last time I tried playing with unboxed closures I needed `|&amp;mut:|` to be able to change captured variables (say, a closure that increments a variable every time it is called). Has that changed now?
This PR does not touch the feature gate.
There's only four ICEs on the tracker: https://github.com/rust-lang/rust/issues?q=is%3Aopen+is%3Aissue+label%3AA-closures Did you file yours? Just curious.
&gt; Has that changed now? Yes, `|| {}` is sort of a generic closure now (think of it as a generic literal: `0.0`), and it gets "coerced" to the right kind of unboxed closure if used as an argument to a function that expects some `Fn*` implementor: #![feature(unboxed_closures)] fn call_twice&lt;F: FnMut()&gt;(mut f: F) { f(); f(); } fn main() { let mut x = 0u; call_twice(|| { x += 1; print!("{} ", x); }); } Outputs: `1 2 `
More precicely, in Java different annotations have different "retention" behaviors. Some annotations exist only at source code level, some are kept in the generated bytecode, and all others are available at runtime.
All of those capture types are obsolete, but I hear that unboxed closures still has a week or two left to bake so who knows if that syntax has been removed yet. All this confusion during the transitionary period is why I've just been telling people to avoid closures for the past month. :P
Verbally, how should one read `move || { ... }`? It kind of feels like I should be reading it "move or &lt;something else&gt;", but that seems strange.
I'm a french programmer, and I use both a qwerty and an azerty keyboard. I sincerely can't say one is better than the other for coding.
Thanks. So I'm a bit further, I managed to figure out that you need to modify rust-bindgen/build/Makefile: LIBCLANG_PATH=/usr/lib/llvm-3.4/lib ...but after using LD_PRELOAD to work around [issue 89](https://github.com/crabtw/rust-bindgen/issues/89), my build fails with this error: error: can't find crate for bindgen #[phase(plugin)] extern crate bindgen; Not sure why it can't find the crate...?
There's a mistake in the usage. `[dependencies.rust-bindgen]` in the Cargo.toml I believe builds the crate as `rust-bindgen`. So try changing that to `[dependencies.bindgen]`.
Love it. I was asking about this (lack of inference) problem on another thread, and this is the perfect solution, imo.
I came across http://np.reddit.com/r/pics/comments/2nc5qc/walgreens_looted_and_on_fire_in_ferguson/cmdvb9a today, so I figured I'd just mention it as an example.
Is he correct that using local lambdas in C++ is a performance hazard? I've never heard that.
The `{:p}` format specifier will print the address of a pointer instead of its contents: http://is.gd/qSCaVx There's also a thread on Discuss for a `ReprShow` formatter which would give you something more like `y:box 6`: http://discuss.rust-lang.org/t/repr-formatter-with-showrepr-trait/926
I don't believe it is possible to infer `move ||` vs `||` in all cases, but at least for the case of the argument to `spawn` we ought to be able to infer it in the not-too-distant future.
This blog post gives more background: http://smallcultfollowing.com/babysteps/blog/2014/11/26/purging-proc/
Actually, it does weaken the feature gate in some respects, so that it is not necessary to opt in quite as much. I expect to author another PR to finalize the feature gates (which will be both somewhat stricter and somewhat looser than what is currently enforced, I think).
Ugh, I missed that part of the diff, then :( But :) to removing the gate sometime soonish!
why cant people just add one use statement to the top of the block? enum Stooge { Larry, Curly, Moe, Shemp, None } fn stooge_call ( stooge : Stooge ) -&gt; &amp;'static str { use self::Stooge::{Larry, Curly, Moe, Shemp, None }; match (stooge) { Larry =&gt; "Nyuk Nyuk", Curly =&gt; "Why I oughta", Moe =&gt; "Get outta here!", Shemp =&gt; "Pay me", None =&gt; "Who you callin a stooge?", } } fn main () { println!("{}",stooge_call(Stooge::Curly)); } this is much more explicit, and explicit is better than implicit in Rust. If this change happens, Id rather it be post 1.0 at the very least.
&gt; However, we dont currently have support for invoking `fn(self)` methods through an object, which means that if you have a `Box&lt;FnOnce()&gt;` object, you cant call its `call_once` method (put another way, the `FnOnce` trait is not object safe). We plan to fix this  possibly by 1.0, but possibly shortly thereafter Could you elaborate on this a little? (What would passing a DST by-value "mean", and how would it work?) In particular: what's the difference between `fn hof&lt;F: FnOnce()&gt;(f: F)` and `fn hof(f: FnOnce())` and `fn hof(f: impl FnOnce())` from [RFC PR 105](https://github.com/rust-lang/rfcs/pull/105)? (I still believe that an `&amp;move` reference type would be the right solution here...)
I wrote a long comment about default integer: http://discuss.rust-lang.org/t/default-integer-type-should-be-handy-on-large-heaps/956 Spoiler: it should be called `int`, have size `sizeof(void*)` and integer literals should fall back to it, because it should not overflow when working with arrays of size &gt; 2G.
(And two of them are tagged `needstest`, i.e. fixed and just need a test checked in to be closed.)
If you're working arrays, wouldn't type inference give you the required integer type anyway? My understanding is that fallback to i32 wouldn't mess with type inference at all, because it only occurs when the types are unconstrained.
You can even use globs: use self::Stooge::*;
This seems like the right way to do it if you really don't want to write the namespace for the individual enum variants. Perhaps making it clearer that this is the best way would be a good thing.
Example code that won't work correctly on large heap: struct MySuperDuperHashTable&lt;K, V&gt; { table: Vec&lt;(K, V)&gt;, } impl MySuperDuperHashTable { fn get(&amp;self, key: K) -&gt; &amp;V { // assuming hash() returns default default integer, which is 32-bit let pos = key.hash() as uptr % self.table.len(); &amp;self.table[pos].1 } }
I worked on a team that did an analysis of its collection types, looking at exactly this. Should we use intptr (int where sizeof = sizeof(void*)), or int32 for array indices, etc.? The result was that using int32 was significantly cheaper, and that we could honestly not find a single legitimate use of a single collection with more than 2^31 items in it. By the time you're dealing with datasets that large, your normal collections just don't cut it, and you want something bigger, and you want the index types to be uint64, not int/uint.
&gt; In the future, Id like to make it possible to pass object types (and other unsized types) by value, so that one could write a function that just takes a FnMut() and not a &amp;mut FnMut(): &gt; fn foo(hashfn: FnMut(&amp;String) -&gt; uint) { ... } Whoa. Would this work with traits as well? e.g. would the following work: fn print(show: Show) { println!("{}", show); } Would this then always be preferable to boxed traits?
I don't understand why anyone would want to fall back to `int`.
Assuming there are following integer types: * u32 * uint (which is going to become 32-bit I'm afraid) * u64 * uptr Which type would you choose for hash? Now I'd choose `uptr`, but it is possible that some other developer chooses 32-bit `uint`, because it is "default" type, and nobody really needs a hashmap of size more then 100Mb.
&gt; and you want something bigger, and you want the index types to be uint64, not int/uint It is not that simple. First, you should chose size which is equal to sizeof(ptr). Second, if you develop some code (generic network transport library, for example), you often have no idea, how large would be data size in a year in a program developed by people in another department.
&gt; First, you should chose size which is equal to sizeof(ptr). You're starting with the conclusion, rather than offering reasons and then reaching a conclusion. &gt; Second, if you develop some code (generic network transport library, for example), you often have no idea, how large would be data size in a year in a program developed by people in another department. Which means that I should make a decision now which is reasonable, rather than writing code which will behave differently on 32-bit and 64-bit systems. For example, if I were writing a network stream processor, I would *unconditionally* use uint64, not uint, regardless of whether my code was running on a 32-bit platform or a 64-bit platform. 
[RFC to rename `int` and `uint`](https://github.com/rust-lang/rfcs/pull/464) [RFC to change the fallback type](https://github.com/rust-lang/rfcs/pull/452)
Sure, but that's the fault of the developer.
Well, my point is that properly chosen "default integer type" can prevent that fault.
Good stuff!
&gt; For example, if I were writing a network stream processor, I would unconditionally use uint64, not uint, regardless of whether my code was running on a 32-bit platform or a 64-bit platform. If variable is to store message size, it is OK for it to be 32-bit on 32-bit systems, because program cannot allocate that much memory anyway. Anyway, you are saying that choosing 32-bit is a programmer's fault. That's true. But the same can be said about every crash of program written in C++. Rust type system prevents memory overruns, and similarly properly chosen integer types can prevent integer overflows.
I would choose a type whose meaning did not depend on the target platform. I think uptr is a mistake. Either u32 or u64 are the better choices.
So code like this would work: for i in range(0, 10) { println!("{}, i); }
This is great. Piping downloaded commands to a root shell rightfully frightens many people. As easy as this is, maybe it should be made even easier by adding the extra steps you described in the installation script.
[Previous discussion](http://www.reddit.com/r/rust/comments/2bhwgc/what_does_rusts_unsafe_mean/).
What's up with all those `continue` statements and the labelled `loop` and `break`? `match` is not like a C `switch`, you don't have to worry about fallthrough. ~~You can just use `for cap in reiter` rather than using `loop`.~~
Sorry, I should clarify: Why fall back to `int` instead of a fixed size type?
Okay, but at 23:20, Blow says &gt; One reason I might not want to do a lambda like this in C++ is performance is now questionable. I've heard reports that a lot of compilers will do a heap allocation to store the data for the variables that that lambda captures, like "&amp;people"... where people is a vector in the example code. My question is why would compilers do a heap allocation to store a reference? Should I not use local lambda functions in performance critical code?
I probably don't need the continue statements, I couldn't get For .. in .. to work since I needed to advance the iteration, and then take that next position and continue on in a recursive manner; hence the loop and next() statements. I really wanted the For to work tho, it's way cleaner looking. I labeled the loop and break for the heck of it, I figured someone (non-rusty) might consider that it only breaks the Match and not the loop-- I'll probably remove it now that you mention it. Thanks!
I'm slower at keys per minute at programmer Dvorak still after four years. That's not the point. You will not have to put your hands in awkward positions, which means a lot more than you think! Typing {} (or sumilar) thousands of times with a single easy-to-reach keypress rather than a strange wrist-wrangle is the difference between decades at the computer rather than years. This is a long term choice, and it's simpler to change your keyboard layout than popular programming language syntax. It's great that you feel it's a non-issue, but I felt the same way until it suddenly became a problem (after 20+ years).
I'm a big fan of how in Swift, you can write switch/match statements like: switch topic.children { case let .ContentItems(contentItems): return contentItems case let .Topics(topics): return concatMap(topics) { self.flattenedContentForTopic($0) } default: return [] } and similarly call functions like so: let lastHyphen = filename.rangeOfString("-", options: .BackwardsSearch) while omitting the namespace.
Have a look at another threads in this topic, where I explained, why default should be platform-dependent, and another thread, where 0xdeadf001 explained, that default should be fixed.
Hmm. Trying out `unboxed_closures` using the current feature gate on the [playpen](http://is.gd/szNroW), I can't seem to get this to work: #![feature(unboxed_closures)] #![feature(overloaded_calls)] fn main() { let closures = [|&amp;: x: int| x * x, |&amp;: x: int| x + 1]; for closure in closures.iter() { println!("{}", (*closure)(10i)); } } I get: &lt;anon&gt;:5:40: 5:57 error: mismatched types: expected `closure`, found `closure` (expected closure, found a different closure) &lt;anon&gt;:5 let closures = [|&amp;: x: int| x * x, |&amp;: x: int| x + 1]; ^~~~~~~~~~~~~~~~~ If I take out the types in the second element, to let it infer the type, I instead get: &lt;anon&gt;:5:44: 5:45 error: the type of this value must be known in this context &lt;anon&gt;:5 let closures = [|&amp;: x: int| x * x, |x| x + 1]; ^ Or if I just leave the `int` in, I get: &lt;anon&gt;:5:40: 5:54 error: mismatched types: expected `closure`, found `|int| -&gt; int` (expected closure, found fn) &lt;anon&gt;:5 let closures = [|&amp;: x: int| x * x, |x: int| x + 1]; ^~~~~~~~~~~~~~ Am I doing something wrong, or is the feature not quite baked enough in the version running on the playpen?
You are correct abouth the `for ... in` thing, I overlooked that you use `reiter` in recursive calls.
Cheers for organizing this. Will be there!
This needs to go into the docs.
I disagree. A hash function is designed to produce a hash, not a platform-dependent value. The hash function allows you to convey a certain amount of information / entropy. It's up to the algorithm to decide whether 32 bits is the *right* amount, or 64 bits is the *right* amount, and this decision should not be forced by what kind of CPU is underneath your algorithm.
Huh, I didn't realize this had been posted before. Looks like the URLs shifted around in the intervening months, so reddit didn't detect the repost. If enough people complain then I'll be happy to delete it.
But... that is already the semantic of `mod`! :) That s, all declared items are automatically in scope, including modules.
Yeah, I adjusted the URL structure a little. (I have no problem with the repost personally, but Im a little biased. :p )
Oh, sorry, my mistake. Of course you can't do this, the closures are unboxed so you can't possibly construct an array from them, you would need to have an array of references to closures. Sorry, having spent so much time working with garbage collected languages in which everything is boxed has made me sloppy in my thinking about the distinction.
I don't get the difference beetween "|:|" and "move ||". Don't both capture by value thus are safe to be moved out?
We do have http://doc.rust-lang.org/reference.html#unsafety , though there's always room for improvement!
Awesome post.
So... you don't actually want to import the module, but all its children instead. I don't really see the point there, as that's identical to not having a nested module at all. :P
It would work for any unsized type, which includes slices and trait objects, of which FnMut and co are a special case.
No, he is not correct. He seems to confuse C++ lambdas with the library facility `std::function`.
&gt; I don't really see the point there, as that's identical to not having a nested module at all. Not from outside of the whole thing.
 for those with the relevant Unicode coverage.
If you think your hash table will ever contain more than 2^32 items in it, then I don't think the default platform hashtable will ever do the job for you. General-purpose collections should cover the 95% case. If you are building a hash table with more than 2^32 items in it, you will no doubt need to take advantage of the particular problem domain that you are working in, and build a hash function that is appropriate for what you are doing. And it is likely that the choice of hash function (and hash size) are *independent* from the target CPU architecture that you are working with. Which means that the hash function and hash type need to be parameters that you can vary, *independent* of the CPU architecture. A good large-scale hash table implementation should allow you to provide both the hashing function as well as a type parameter of the hash. Again, consider a hash table that is expected to hold more than 2^32 items. You will likely *not* be using a flat Vec as your backing store, because the cost to expand such a store is very, very high. There are are lots of good ways to design such collections, but I think it's a bad idea to tie this class of data structure design with the default in-the-box hash table that is provided for general-purpose computing. Consider the *counter*-example, too, where you want a much better hash function (and a much larger hash) than u32, even on a 32-bit CPU architecture. Not because you expect to put an enormous number of items into your collection, but because the cost of hash collisions is so much higher for what you are doing. (Caching the results of very expensive computations, typically.) So it is reasonable to want your hash size to be u64 (or u128, or whatever), *even* on 32-bit CPU architectures. They really need to be independent quantities.
I'm repeating my comment on discuss: If there's no `uint` type, then the most widespread type in the standard library and in external libraries will be `uptr`. So, then: * `uptr` and `iptr` names are unusual, while there are nice and popular names `int` and `uint` familiar to every C/C++/Java/C#/Python/Haskell/Swift and so on programmer (BTW, in C/C++/Haskell/Swift int is platform dependent, and in Python int is big integer; int is fixed size in Java/C#, and both Java/C# has problems with that: max array size is limited to 2^32). * falling back to `i32` instead of the most widespread integer type is counter-intuitive
What do you mean by "default integer"? Is it just a convention? Falling back to `int` is useless for arrays, because you have to cast to an `uint` anyway. &gt; * uptr and iptr names are unusual, while there are nice and popular names int and uint familiar to every C/C++/Java/Python/Haskell/and so on programmer The problem with `int` and `uint` is that the names are familiar. It makes people use them when they should not be using them. &gt; * falling back to i32 instead of the most widespread integer type is counter-intuitive Are you saying `int` is the most widespread integer type? I'm pretty sure it is the *least* widespread one. (`uint` is widespread, but not `int`. We don't want to fall back to that, because it is unsigned.) I think most of the issues you are discussing are orthogonal to what the fallback is.
On the other hand they introduce platform dependent behavior. So your program might work on 64-bit but not on 32-bit. It can also cause integer overflows.
There is no silver bullet. Having "default integer" to be platform-dependent does not prevent all errors, but decreases amount of them.
Since quite some people support this idea, I will summarize and submit an RFC. Thanks for all your inputs!
The question is which is worse: overusing `int`/`uint`, or underusing `iptr`/`uptr`. Overusing `int` is worse because it makes code somewhat slower, and work different on 32/64-bit systems when `i32` overflows. Underusing `iptr` is worse, because integer overflow may lead to incorrect behavior on large objects (&gt; 2 Gb) on 64-bit systems: from prohibition to data corruption.
I like enum namespacing. The namespace is just the type of the enum. The type of the enum can be inferred, there's no need to type it out. Enum namespacing means that Option::None isn't Stooge::None. That's good. If we're matching on a potential stooge, Option::None isn't valid. That's type inference, imho.
&gt; First, it's odd to me to frame this as the programmer guaranteeing something 'to the compiler'. The compiler doesn't care. It's going to compile the code regardless of whether that code actually maintains the required invariants, and it doesn't suffer the consequences when those invariants aren't maintained, because it's a compiler and has no agency. I hereby move to rename `unsafe` to `invoke_infallibility` to clear that one up.
What's wrong with this suggestion? Your suggested solution just adds to the boiler plate you have to write. I think this suggestion, although implicit, is rather useful, as it allows for less boiler plate to be written.
Not necessarily. `|:|` only makes sure that `self` is taken by value which makes it a `FnOnce` (in general). And `move ||` only makes sure that the environment is captured by value. You can actually have one without the other. Of course, to replace the old `proc` you need both of these properties. Well, capturing by value (`move`) is important for passing it across thread boundarires. The `FnOnce` version just conveniently lets you consume the environment in the function body of the closure. HTH
Yes! Assuming this `MadeIterator` (which I'm sure has problems, but works as POC): struct MadeIterator&lt;'items, Item&gt; { func: Box&lt;FnMut&lt;(), Option&lt;Item&gt;&gt; + 'items&gt; } impl&lt;'items, Item&gt; MadeIterator&lt;'items, Item&gt; { fn new&lt;F&gt;(f: F) -&gt; MadeIterator&lt;'items, Item&gt; where F: FnMut() -&gt; Option&lt;Item&gt;, F: 'items { MadeIterator { func: box f } } } impl&lt;'items, Item&gt; Iterator&lt;Item&gt; for MadeIterator&lt;'items, Item&gt; { fn next(&amp;mut self) -&gt; Option&lt;Item&gt; { self.func.call_mut(()) } } It's possible to create this factorial iterator without defining a struct: fn make_factorial() -&gt; MadeIterator&lt;'static, u32&gt; { let mut index = 1; let mut value = 1; MadeIterator::new(move || { value *= index; index += 1; Some(value) }) } (note the `move`. Without it, we wouldn't be able to return the resulting `MadeIterator`, because `index` and `value` would still be on the stack frame of `make_factorial`. With `move`, they are moved into the closure itself) And then, we can use it as any other iterator: fn main() { let factorial = make_factorial(); for num in factorial.take(10) { println!("{}", num); } } [On the playpen](http://is.gd/XsKE9A) (with some modifications to account for the old `rustc`).
Hmm...this seems to work already...: [playpen](http://play.rust-lang.org/?code=%23[allow%28dead_code%29]%0A%20enum%20Stooge%20{%20Larry%2C%20Curly%2C%20Moe%2C%20Shemp%2C%20None%20}%0A%0A%20fn%20test%20%28%20stooge%20%3A%20Stooge%20%29%20-%3E%20%26%27static%20str%20{%0A%20%20%20%20match%20stooge%20{%0A%20%20%20%20%20%20%20%20Larry%20%20%3D%3E%20%22Nyuk%20Nyuk%22%2C%0A%20%20%20%20%20%20%20%20Curly%20%20%3D%3E%20%22Why%20I%20oughta%22%2C%0A%20%20%20%20%20%20%20%20Moe%20%20%20%20%3D%3E%20%22Get%20outta%20here!%22%2C%0A%20%20%20%20%20%20%20%20Shemp%20%20%3D%3E%20%22Pay%20me%22%2C%0A%20%20%20%20%20%20%20%20None%20%20%20%3D%3E%20%22Who%20you%20callin%20a%20stooge%3F%22%2C%0A%20%20%20%20}%0A%20}%0A%0A%20fn%20main%20%28%29%20{%0A%20%20%20%20println!%28%22{}%22%2C%20test%28Stooge%3A%3ACurly%29%29%3B%0A%20})
Hmm, I didn't think that syntax (&lt;trait&gt; for &lt;trait&gt;) was allowed at all, in which case your error seems like the wrong one. Try: ``` impl&lt;T&gt; Show for T where T: MyTrait { ... } ``` Also `write!(...)` will return a result so you can drop the `;\nOk(())`.
This quote is the reason I fear the Rust team is rushing too quickly to get 1.0 out the door. This is not an insignificant issue, and there should be no promises of backwards compatibility until these issues are ironed out. It's not like they're delivering a product to a paying client; there is no reason to make compromises, and promises to fix, like this for the sake of meeting some arbitrary schedule. There is no reason 1.0 can't wait for issues like this to be solved.
I tried to clean up the code and make it cleaner. - Added a bench test method to evaluate performance - Removed the box, it is not required and only causes unnecessary allocations - Turned the recursion into an iteration by using a stack of expressions - Switched from loop + match to for - Restructured the if statements a little bit for better code sharing Especially the switch from recursion to iteration brought the largest performance win. Before: test bench_medium ... bench: 797373 ns/iter (+/- 46956) test bench_small ... bench: 228495 ns/iter (+/- 9405) After: test bench_medium ... bench: 513237 ns/iter (+/- 28470) test bench_small ... bench: 217787 ns/iter (+/- 9845) I uploaded the new source here: https://github.com/mkaufmann/rust-sexp EDIT: How can i make the bencher to display the throughput in MB/s again? I did set the .bytes but still only got the time without throughput?
Thank you, but I followed your advice and now I'm getting `error: cannot provide an extension implementation where both trait and type are not defined in this crate`
After some tinkering, I've got a working version: http://is.gd/QhYUgS Still don't understand why the lifetime parameter is needed though.
&gt;One downside of the older Rust closure design is that closures and procs always implied virtual dispatch. Why aren't closures represented as a `(env, fun_ptr)` pair where `fun_ptr` is a function whose first argument is the environment `env`? 
Fantastic! I was waiting for this
You may want to take a look at the rust-rosetta repo on GitHub... You can compare your code with the one we currently have for S-Exp: https://github.com/Hoverbear/rust-rosetta/blob/master/src/s_expressions.rs
DST should allow e.g. embedding an unsized vector inside the struct: https://github.com/rust-lang/rust/issues/12938 That looks like what you want.
It can be done. As far as I'm aware it is easiest to do it thus: struct Example&lt;Sized? T&gt; { normal_data: i32, more_normal_data: f64, unsized_list: T } fn main() { let example: Box&lt;Example&lt;[u8]&gt;&gt; = box Example{ normal_data: 5, more_normal_data: 1.5, unsized_list: [1,2,3,4,5] }; println!("{}, {}, {}", example.normal_data, example.more_normal_data, example.unsized_list[0]); } On the `let example` line we went from `Box&lt;Example&lt;[u8, ..5]&gt;&gt;` to `Box&lt;Example&lt;[u8]&gt;&gt;` which I believe is the easiest way to do it. Note that variable size structs must always be behind a pointer of some type. We can also use a reference: let ex = Example{ normal_data: 3, more_normal_data: -0.5, unsized_list: [true, true, false] }; let ref_ex: &amp;Example&lt;[bool]&gt; = &amp;ex; 
OP is trying to achieve full cache locality by having the elements of the vector contained in the same memory area as the struct. A `Vec` allocates separate heap space which means cache locality cannot be guaranteed. 
Yes, you are right. I didn't think about that detail...
I've seen Blow talk about languages a lot. Not actually seen any of his videos though. Is he actually insightful or just good at *sounding* insightful? 
Had no idea this existed, thx! I guess it's a bit long winded to submit to Rosetta? Not sure why it's not on the site
This is great! Couple of questions: * does this impose a potential to stack overflow since there is no boxing? I'm not sure the way I did it would help either, just curious. * why do you push an empty vec on the initial parsing? But then again on lp? * does rust not do well with recursion, or was the speed up mostly due to removing boxing? * on line 34 you unwrap, could this potentially fail if the sexp was malformed? Thx again, really appreciate the work
You can add : pub use foo::bar; You can then access the bar writing foo::bar. But you need to re write all the pub functions/struct in your mod.rs file. That's heavy, that's why I'd love public content of the file to be pub in the whole module by default.
Oh really? I will have to try this! 
Linear type doesn't seem to fit well with event-driven software design in which you are only allowed to define an event handler with access to a stateful date storage. I'd like to gain more experience in this area. (Perhaps sort of CPS helps?)
Well, the downside is that it would only work for function arguments, not for arbitrary local variables. Apart from that though, it would be the same as for sized types: Take T, &amp;T or &amp;mut T, avoid taking Box&lt;T&gt; unless special needs for it arise.
He's basically trying to unify (named and anonymous) closures with (named and anonymous) functions, by having one thing where you have one syntax: (normal function parameters) -&gt; [parameters that are closed over] {Some code block}. He's explaining this syntax around 59:29: http://youtu.be/5Nc68IdNKdg?t=59m29s
 fn main() { fn foo() -&gt; i32 { const FOO: i32 = { fn _foo() {} mod bar { pub const BAZ: i32 = 32; } bar::BAZ * bar::BAZ }; mod yodel { pub fn bananas(n: i32) -&gt; i32 { println!("hi"); n } } yodel::bananas(FOO) } println!("{}", foo()); }
* The internal databuffer for the vector is allocated on the heap, so it won't fill up the stack * The main structure during parsing is a Vec&lt;Vec&lt;Exp&gt;&gt;. The outer vector is my stack, so that during the iterations I can have multiple nested scopes (represented by the inner vectors). Insertion of elements will always happen on the last element in the stack. To simplify the logic i initially insert an empty vector which is basically the global scope. This will also get returned after succesfull parsing * I think the speedup might be associated with fewer inlining possibilities and additional cloning of datastructures. Removing boxing of the vectors barely made a difference as there are so many allocations anyway due to strings and the large overhead of the regex * Line 34 is really bad, it will fail. Instead of unwrap() ist should be .expect("Unmachted closing parenthesis") PS: When implementing the parser manually instead of using a regex, the performance is over factor 100x higher. If I have some time polishing it I will upload my version of the rosetta code in the next days. I think the version in the rust-rosetta repository is unnecessarily complicated.
oops! Sorry about that; I'm new at the reddit. I'm a bit surprised reddit doesn't catch duplicate links; seems like low hanging fruit, but maybe site-specific URL structure changes a lot...
Maybe this just shows that I don't understand all of this, but why don't we just have a fourth trait: trait FnBox&lt;A,R&gt; { fn call_box(Box&lt;self&gt;, args: A) -&gt; R }; ...and use that for the spawn stuff, instead of using the Invoke trait workaround? And in case the FnBox can't be inferred, we could add this syntax: let c = |box:| { ... }; 
I explicitly point out in my post that the path to the libraries may have to be modified (if you use a 32 bit OS e.g.). &gt; As an aside, does the script work correctly on Linux? It does work at the university computer lab with `DYLD_LIBRARY_PATH`. They use Debian there.
I'd say the "virtual dispatch" comes from the concept of actually having a function pointer, rather than having the address at compilation time. Or put in another way, every function call that is not a virtual dispatch is statically dispatched, and can potentially be inlined. If you have a pointer to a vtable which in turns points to functions, you just have one more level of indirection before you know which function to virtually dispatch.
Try [Comcast](https://github.com/tylertreat/Comcast) for the connection failure tests.
I tried it (and then I also had to change "package" in the rust-bindgen repository). But I still have the same error, can't find the crate. What I *think* happens is that the libclang.so/llvm rustc is compiled with (I get a precompiled one from rustup.sh) is different from the libclang.so/llvm I LD_PRELOAD. Which sounds like a recipe for trouble. Also, using the [Ubuntu PPA](https://launchpad.net/~hansjorg/+archive/ubuntu/rust/+packages) for rust is unlikely to help as it does not build-depend on libclang/LLVM. So, I'm back to square one, it seems like...
Issues that can be solved backwards-compatibly can be easily deferred to 1.0 without breaking existing code. It does mean that the stdlib won't be able to take advantage of the new features, however if you were to wait until the language was completely done and perfected before releasing for want of a perfect stdlib we'd be here for another two years at least. I think Rust is 85% good enough *now*, and I'm tired of squabbling over percentage points. Ship, damn it! :)
&gt; the C++ precedent And C#, and Java, and Swift, and... (hmm, are there others?). Together those probably account for something like 95% of the population of programmers who have used a language with any kind of generics. I think `&lt;&gt;` is unambiguously the right choice for Rust for this reason alone, even regardless of the `::&lt;&gt;` thing. Out of at least semi-mainstream languages with any kind of C heritage, I think it's just Scala (`[]`) and D (`()`) which make a different choice.
If its just about grouping items into submodules but importing them all in the current scope, then you can do that with something like this: use self::foo::*; mod foo { ... } It would also be possible to define a custom attribute for this: #[flatten] mod foo { ... } --- However, this is currently blocked on the accepted-but-not-yet implemented RFC that lifts the order restrictions between `use` and `mod`, and on wildcard imports working correctly. 
I think it's called `Sized?` nowadays.
Could you elaborate on what you mean? Currently, the `||` only contain parameters for the closure, so moving the `move` keyword in there is strange as the capture mode has no connection to those parameters.
I found the lifetimes extremely daunting at first, and now only on occasion and much more mildly so. I've actually had lifetime issues point to mistakes in my logic so I have found the safety gain to be real and that has encouraged me to improve my fluency with lifetime syntax so that I can enjoy the benefit without feeling like I'm slogging through syntax soup to get anything done. There is definitely a learning curve though, and it can be frustrating.
Instead of using functions that mutate self, make a function that simply returns a value calculated from some parameters. Then, calculate all fields before passing them to the struct initializer: fn new(content: &amp;str, target: &amp;str) -&gt; BoyerMoore { let pat = target.to_string().into_bytes(); let source = content.to_string().into_bytes(); let delta1 = BoyerMoore::make_delta1(pat.as_slice()); BoyerMoore { pat: pat, source: source, delta1: delta1 } } fn make_delta1(pat: &amp;[u8]) -&gt; [int, ..256] { let mut delta1 = [pat.len() as int, ..256]; for i in range(0,pat.len()) { delta1[pat[i] as uint] = pat.len() as int; } delta1 } Notice how make_delta1 no longer takes self, and instead of mutating things it simply returns the initialized value. Also, notice how pat, source, delta1 are all set up before the struct initializer.
thanks for your pointing!
I'm curious as well.
No direct way AFAIK, but this would probably work: return BytePattern { pattern: { static P: &amp;'static [u8] = &amp;[0x00u8, 0x00u8, 0x01u8, 0x00u8]; P }, mask: { static M: &amp;'static [u8] = &amp;[0xFFu8, 0xFFu8, 0xFFu8, 0xFFu8]; M }, }
Why not look at the individual PipeStream members themselves (assuming you spawn the process passing CreatePipe(false, true) for stdout/stderr, which have methods like read_at_least, read_exact, etc: http://doc.rust-lang.org/std/io/pipe/struct.PipeStream.html
`make_delta` can be improved further by iterating over `pat` directly: for &amp;i in pat.iter() { delta1[i as uint] = pat.len() as int; } To OP: By the way, why the heavy use of `uint` and `int`? Perhaps you want `u32` and `i32`?
I'm curious as to why this one doesn't work: http://is.gd/q5w1Rp\ I mean, I can see the error, but... why?
If Im allowed a tangent: &gt; Isn't arrays in C++ just pointers? No, there really are arrays in C++. I cant really fault you because this persistent misconception is tragically abetted by several language quirks, or possibly warts: - function parameters that are declared as arrays are instead pointers parameters (e.g. `void f(int p[5]);` and `void f(int* p);` are equivalent declarations), - the array form of `new` returns a pointer - arrays *decay* to pointers at the slightest provocation If that last rule alone leads anyone to conclude that arrays *are* pointers, then by that same token `int`s would be `double`s (and vice versa, as a bonus). The precise relationship is one of coercion / conversion from the one to the other. The second point is a consequence of the interaction between the need for runtime-sized arrays and the type-system; in Rust this appears as the relation between `[T, ..n]` (statically-sized arrays) and `[T]` (runtime-sized slices). Neither language has a type for a runtime-sized array proper, as it may exist in other languages (and they both defer to a resizable library-type vector instead).
I see! I'm far from experienced when it comes to C++ (I have tried to use it in a few projects and it would more often than not end in frustration) so I'm not claiming to know anything. I just assumed that arrays would be like in C. I have always been told that what you get is basically a pointer to some memory space and that the `x[n]` operator is just sugar for `*(x + n)`. I may very well have misunderstood how statically sized arrays works, but they are placed on the stack, as far as I know. Thanks for the info, today I learned :)
Yes, I was meaning `Sized?`, my bad.
Even in C, arrays are different from pointers. Take code like this: #include &lt;stdio.h&gt; int main(void) { int arr[5]; int* ptr = arr; printf("Array: %d, Pointer: %d", sizeof(arr), sizeof(ptr)); return 0; }
You can use macro to wrap the above code. Playpen: http://is.gd/lVguFR I still believe you're using "magic numbers"...
`[0x00u8, 0x00u8, 0x01u8, 0x00u8].as_slice()` should also work
Actually, the `?` is not part of the keyword, as there is no keyword here. :) The idea behind the `Trait? T` syntax is to express "might (not) implement `Trait`", that is its a mechanism to remove default trait bounds from bare type parameters `T`. Currently, `Sized` is the only trait that is implied to always be implemented for a `T` per default, and as such the only one for which this syntax makes sense to use.
Because `read_at_least` "*will continue to call read until at least min bytes have been read. If read returns 0 too many times, NoProgress will be returned*". cf. http://doc.rust-lang.org/std/io/trait.Reader.html#method.read_at_least That means that if a process returns two lines three bytes each: "foo", "bar", and we're using read_at_least (4), then we won't see the first line until the second one is ready, and *that* might take a while. It harms the interactivity invariant, in other words. Wrapping the stream in a buffer in order to get its `read_until` method isn't an option either and for the same reason - the buffer will block waiting for the pipe's output to fill its, well, buffer, preventing us from seeing the output when it's ready and not later. The only way to handle it properly without loosing interactivity and without resorting to the byte-by-byte reads is, AFAIK, using the [non-blocking reads](http://stackoverflow.com/a/1736861/257568), but I'm not sure Rust has any. P.S. What a funny feeling, when you post a solution to a problem and people look at you like you're crazy, because it turns out they aren't even aware of the problem, mhmm?
C++ have lifetimes, as about every other language. There is even a whole chapter of the specification dedicated to the concept: 3.8 Object lifetime (in n3337, C++11). There is no syntax to represent it and the compiler struggles to point out violations related to referencing objects whose lifetime has expired, which is what is making it so difficult NOT to corrupt memory/crash.
I don't really get the motivation here -- the words are synonymous. In common usage, the word lifespan refers exclusively to a span of time. If you want to get rid of the temporal connotation (something I don't think the RFC really justifies) you'll have to do something more drastic and get rid of *life*. Any connotation with physical dimensions (extent, length, etc) risks conflation with the idea of size as how much memory a thing takes up. And ultimately, aren't lifetimes intended to provide constraints on the *ordering* of certain operations? That sounds pretty temporal to me!
Or not. It's actually a bit more complicated than what you would expect. Specifically, the issue is that `[0u8, 0, 0, 0]` is a temporary in an expression and today temporaries' lifetime end at the end of said expression. If you want a `static`, then it's up to you to create a `static`.
When using `unsafe` it is very easy to fail to uphold the invariants that Rust expects and to invoke undefined behaviour. I expect incorrect unsafe code and logic errors to be the biggest source of bugs.
Unfortunately that constrains you to a fixed number of array lengths, fixed at compile time I mean. I don't think there is a way in Rust right now to express this common C++ idiom, of a variable-length tail with a fixed-length header. 
Ignoring unsafe code (there's a lot of fun API boundary issues there): Overflows are still A Problem in Rust, although all the integer types have defined overflow semantics and have checked_add/sub/mul/div methods for when you *do* remember to think about it, it's easy to forget and get some garbage results (still a problem in tons of std lib places).
Yeah, this is where I think Rust got it wrong. Checked arithmetic should be the default, and you should have to opt-in to unchecked arithmetic. I've worked on a system that contained many MLOC, all using checked arithmetic. We did many performance analyses, and the cost of checked arithmetic varied between 0% and 1%, max. In all cases, profiling points out where checked arithmetic matters, and then you examine your algorithm and judiciously opt in to unchecked arithmetic. Another key requirement is that your compiler has to do a good job on checked arithmetic. The code must be efficient. We did this on x86 using the INTO (Interrupt on Overflow) instruction, and JO (Jump on Overflow) on x64 (since the INTO opcode got repurposed in x64). I'd be really, really happy if Rust flipped this particular design decision, and decided that overflow is just as important a class of defect as null pointer dereference. Because it is. Edit: I suggest adding a whole new set of "unchecked" operators to the language. I suggest using the % as an indicator that an operator has modular semantics. So we would have "compound" operators like ``+%``. For example: let x: u32 = 42; let y: u32 = 32; let z: u32 = y -% x; // y is now 0xffffff6 or whatever 
No worries. It's actually a different link. Blow streams at twitch and tends to repost them on youtube.
There's nothing preventing a T: Show from implementing MyTrait, at which MyTrait tries to implement Show for it, and... then what? Who wins?
What do you expect to happen when I implement `MyTrait` for `u32`? It already defines its own Show implementation. But you also specify a Show implementation for it. Who wins? Does it depend on whether I import MyTrait into scope? This sort of thing *might* be possible in the future when we have negative trait bounds. Basically being able to express: impl&lt;T&gt; Show for T where T: MyTrait + !Show { fn fmt(&amp;self, fmt: &amp;mut Formatter) -&gt; Result&lt;(), Error&gt; { write!(fmt, "MyTrait"); Ok(()) } } Note that blanket impls *can* work, but you can generally only do it for implementing the trait you just defined. That is, you can write: impl&lt;T: Eq&gt; MyTrait for T {} And that should actually work today. However Rust will likely get mad if you try to write any other implementations for your trait, because it's very easy for those to conflict with others. That is: impl&lt;T: Show&gt; MyTrait for T {} will certainly conflict with the previous blanket impl. Even if there didn't exist any object that did implement both traits, Rust can't really know that for sure about everything that it might eventually see. Also it will often not let you even do some concrete `impl MyTrait for MyConcrete` because it *might* implement Eq.
But what do you do on overflow? Panic? If you aren't going to exit in a non-normal way, then you either need to add exception handling or make every function return some flavor of Result to properly consider the possibility of an arithmetic problem.
Ooooh, I see.
No problem :) just checking cos I'm not familiar with DSTs yet.
&gt;I'd be really, really happy if Rust flipped this particular design decision, and decided that overflow is just as important a class of defect as null pointer dereference. Because it is. I disagree completely. A null pointer dereference cannot make sense -- there is no logical thing to do when this happens. An overflow can be well defined and doesn't corrupt memory in any way; it might result in a logic error, but that's still memory safe. &gt;I suggest adding a whole new set of "unchecked" operators to the language. I could easily see checked operators getting some syntax sugar (~+ (maybe plus) and ~- (maybe minus)), but I think that the default being unchecked is important. Rust needs to match C/C++ speed by default if it's ever going to succeed.
Rust still has some sources of undefined behavior, like overlong bit shifts (though that's really a bug). Rust still allows for memory leaks, deadlocks, and many other types of bugs.
My experience motivates a very different conclusion. You're free to disagree, of course. But in my experience, adding two positive numbers and getting a negative number, or adding two positive numbers and getting a *smaller* positive number, utterly violates the programmer's expectations. Again, I have seen very large-scale systems built with checked arithmetic enabled by default. There is no performance cost. And we found, over and over, that checked arithmetic exposed bugs and attack vectors. Including security attack vectors. 
Yes, absolutely you panic.
Violating the programmer's expectations and violating memory safety are two very different things, except possibly in unsafe code *where the developer should be extra careful anyways*. As far as performance goes, there literally *must* be a cost in checked arithmetic. Whether or not it is significant to the application is, of course, dependent on what the application is. But as a low-level language, checking every operation is ridiculous; on micro-controllers and deep inside an operating system or game the cost is just too high. 
&gt; As far as performance goes, there literally must be a cost in checked arithmetic. I've actually done the measurements, on large-scale systems. Yes, there is a cost, but there is also a benefit in exposing and fixing serious correctness bugs. We judged the cost to be so close to zero as to be negligible. Also, since you can locally opt out of checked arithmetic, the cost approaches zero. Consider the costs from a micro-architectural point of view. We used the Intel CPU performance counters, and focused on these CPU counters (and several non-CPU counters): * HW_INSTRUCTIONS_RETIRED.ANY - Total number of instructions retired by the pipeline. Note that this does not give you any indication of the cost of executing those instructions. * L1 instruction pipeline stalls: This is the number of times the instruction pipeline stalled, due to the lack of availability of hardware resources, such as execution units, reservation stations, space in write queue, etc. * Last on-chip cache miss rate (L3 misses) * Cycles elapsed per core. * Wall time elapsed. Relevant for MP systems, since this is global time elapsed, not time per CPU. In every analysis performed, the only metric that increased was the "instructions retired" metric. Total CPU cycles did not increase in any statistically significant way, for example. We should *expect* instructions retired to increase, since we execute another JO (or INTO) instruction after every operation that could cause an overflow. Most importantly, L1 instruction pipeline stalls did *not* increase. This means that we weren't even exhausting pipeline resources, to do these checks. Consider what is happening in the execution pipeline. A math op like ADD is scheduled, and immediately after it, a JO is scheduled. Most modern x86/x64 cores always assume that a branch is not taken, if there is no information in the branch cache. This matches perfectly our situation -- we assume that overflows will rarely occur. Because the CPU always correctly predicts the results of the JO instruction, it never even modifies / dirties its branch target buffer. The JO instruction adds one data dependency in the u-op pipeline, and (most important) this instruction does not have any data dependencies on it. It has a control-flow dependency, but since the CPU always correctly predicts this dependency, the cost (from a micro-architectural perspective) is extremely close to zero. We tested many, many different algorithms, components, and subsystems. I personally focused on graphics algorithms, which are numerically intensive. We tested TCP/IP, saturating five 10 Gb/s in a single box. We tested a SQL server, serving hundreds of thousands of requests per second. In all of these cases, we did the A/B comparison between "all operations are checked" and "no operations are checked". The result was that the cost was statistically indistinguishable from run-to-run noise. In other words, it's free. And we found many, many bugs by using checked arithmetic. Serious bugs. In every case, dealing with these bugs motivated us to build a better system, with clearer semantics and invariants. So in my experience, the cost/benefit ratio of checked arithmetic is solidly in favor of enabling it. You may reach a different conclusion, of course. But my conclusion is based on years of investigation and performance analysis. I will not be swayed by "I don't like checked arithmetic". Nor will I be swayed by "it has to be too expensive", because I have done the measurements, and seen measurements done by others, and the hard data is conclusive: checked arithmetic can be done quite efficiently, with virtually no impact on performance. The key is having the compiler generate efficient code, rather than using some kind of "checked math" library. The result of our analysis showed, definitively, that checked arithmetic had a cost that was so close to zero that it was negligible. Edit: fix typo around on-chip vs. off-chip caches
If you believe Rust needs checked arithmetic by-default that strongly, you should write up an RFC.
You're right. Burn it all to the ground.
I think that's a good idea. I'm new to Rust and the Rust community, but I think that's a good idea.
If you're uncertain you can submit a Pre-RFC to discuss.rust-lang.org or just a vague issue to the rust-lang/rfcs repo. Making a full RFC is more likely to get serious consideration, though.
Yes, that part has always made me suspect that there is more behind it. Now I know :)
Actually, there appears to already be [an RFC in this vein](https://github.com/rust-lang/rfcs/pull/146)
Alright, this has definitely cleared some of the fussy parts of my C knowledge. I guess the professors didn't bother explaining it further during the C courses I took, and settled on a "good enough" level. Makes sense, considering the level of at least the first course, but it's a source of misconceptions. Thanks for the corrections!
You can use byte slices like this: http://is.gd/7JQFKd #[deriving(Show)] struct BytePattern { pattern: &amp;'static [u8], mask: &amp;'static [u8] } fn main() { let b = BytePattern{ pattern: b"\x00\x00\x08\x08", mask: b"" }; println!("{}", b); } 
and of course, *actually using* checked arithmetic is probably the only way we'll finally have hardware support for checked operations instead of having to call `seto` like savages. FWIW Rust's checked_* uses [LLVM's overflow intrinsics](http://llvm.org/docs/LangRef.html#arithmetic-with-overflow-intrinsics) which should be as efficient as can be e.g. [libo](https://github.com/xiw/libo) claims `overflow_mul(int *, int, int)` compiles to imull %edx, %esi movl %esi, (%rdi) seto %al ret which should be the minimal implementation.
for me the ability to opt-in more checks for debug builds would be interesting (or relegate the performance&gt;safety case to another compiler setting). Another way might be to select by types.. e.g. `use std::checked::{int,..}` vs `use std::unchecked::{int,...}` , and the same operators are just overloaded for them you might have profiled it at 1% for a specific case but its' going to vary between platforms. There are contexts where an error check isn't valid behaviour, because the program isn't allowed to have any sort of runtime failure, and hardware/tools are optimised accordingly. Some game consoles perform disproportionately badly with 'branchy-int' code, traded for vector-float performance &amp; GPU
Is there a compiler switch to "switch" between safe and unsafe arithmetic? I think I would like to explicitly always opt-in into safe arithmetic in some places, but I would also like to have a way to run all my tests using safe arithmetic to discover possible issues (and then handle them explicitly). 
Note that you can #[cfg] an entire module, as we do with #[cfg(test)] for all our testing modules.
&gt; Yeah, this is where I think Rust got it wrong. Checked arithmetic should be the default, and you should have to opt-in to unchecked arithmetic. FWIW that's exactly what Swift does: [checked (faulting) arithmetics by default](https://developer.apple.com/library/ios/documentation/swift/conceptual/Swift_Programming_Language/BasicOperators.html), with a different set of [overflow operators](https://developer.apple.com/library/ios/documentation/swift/conceptual/Swift_Programming_Language/AdvancedOperators.html#//apple_ref/doc/uid/TP40014097-CH27-XID_73) &gt; Another key requirement is that your compiler has to do a good job on checked arithmetic. The code must be efficient. We did this on x86 using the INTO (Interrupt on Overflow) instruction, and JO (Jump on Overflow) on x64 (since the INTO opcode got repurposed in x64). Rust's `checked_*` uses LLVM overflow intrinsics, which basically just adds a `seto` after the corresponding arithmetic operation to store the overflow flag.
As an alternative to `LD_LIBRARY_PATH` on Linux, I've had success adding an appropriate rpath to the rust nightly binaries using [`patchelf`](http://nixos.org/patchelf.html). Running the following after installing a new nightly seems to work ok (only tested on x86-64): #!/bin/bash RUST_NIGHTLY_DIR=$HOME/local/rust-nightly RUST_BIN_DIR=$RUST_NIGHTLY_DIR/bin RUST_LIB_DIR=$RUST_NIGHTLY_DIR/lib RUST_RUSTLIB_DIR=$RUST_LIB_DIR/rustlib/x86_64-unknown-linux-gnu/lib for i in $RUST_BIN_DIR/*; do echo Patching: $i patchelf --set-rpath $RUST_LIB_DIR $i done for i in $RUST_LIB_DIR/*.so; do echo Patching: $i patchelf --set-rpath $RUST_RUSTLIB_DIR $i done for i in $RUST_RUSTLIB_DIR/*.so; do echo Patching: $i patchelf --set-rpath $RUST_RUSTLIB_DIR $i done 
As a follow up, I also compile my rust projects with `rpath = true` in the Cargo profiles to avoid needing `LD_LIBRARY_PATH` to run them. This is works for me because I'm mostly just writing small programs for myself. I don't think that rpath works well for binaries you intend to distribute. 
Can you explain how the #[cfg] attribute works? For example, in C: int main() { #ifdef _DEBUG_ printf("debug string\n"); #endif } Then I can compile with **gcc program.c -D\_DEBUG_** Is there a equivalent of this in Rust? 
I get Access Denied out of this URL.
&gt; Another key requirement is that your compiler has to do a good job on checked arithmetic. The code must be efficient. We did this on x86 using the INTO (Interrupt on Overflow) instruction, and JO (Jump on Overflow) on x64 (since the INTO opcode got repurposed in x64). Hold on, doesn't that mean at that point it would be impossible to run a 32-bit binary on a 64-bit system?
[yay RFC!](https://github.com/rust-lang/rfcs/pull/146) They haven't rejected it yet so maybe there's still hope... (edit: oh, I just noticed you've already linked it)
&gt; References can only have lifetimes shorter than that of the object they point to, not equal or greater. Well strictly speaking you can do `let ref x = SomeObject::new()` and the lifetime of the reference is exactly equal to the lifetime of the object :)
Ah, that'll work. Although, does it incur a slight runtime penalty since there's conditional branch, or will it be compiled to be identical to the C version?
The issue here is that checked arithmetic is currently implemented using the `checked_$op()` methods on the `core::num::Int` trait, and those return `Option`, implemented under the hood using specific LLVM intrinsics. Having a switch to swap what intrinsics are used would add non-negligible complexity and technical debt to the compiler, as well as change the semantics of a program in unpredictable ways. 
No, not at all. When an x64 processor is running in 32-bit mode, INTO still works exactly as it does on 32-bit processors.
&gt; you might have profiled it at 1% for a specific case but its' going to vary between platforms. That is certainly true. My team also measured ARM systems using THUMB2 encodings, with similar results.
You can easily leak memory with Rc&lt;T&gt;.
Uh. Isn't this the same as `unique_ptr`? How does it leak?
Theres `#[cfg_attr(name, attribute)]`, set this attribute if the named cfg is set, which one might have expected could be used to write #![cfg_attr(core, no_std)] #[cfg(core)] extern crate core;  but unfortunately `cfg_attr` seems a bit dodgy in places and this use of it doesnt seem to be working at present. 
Pass `&amp;str` to your method, instead of `String`. As others have pointed out, the `String` that is passed into your function is moved into your function, and then it dies at the end of your function. Then, you can do one of two things. Either you can build a `Vec&lt;&amp;str&gt;` and return that. You'll need to use explicit lifetimes to indicate that the vector that you're returning contains references whose lifetime is the same as the input string that was passed in. Something like: fn to_words(text: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; { let mut words: Vec&lt;&amp;'a str&gt;::new(); ... build up words ... return words; } I'm not actually 100% sure that this compiles, but it shouldn't be far off. The downside to this is that you will always allocate heap memory, for the backing store behind the Vec. Or, second, you can implement your algorithm as an iterator that walks through a string. The advantage of this is that you don't actually have to do any allocation, ever. You just declare a struct that contains `&amp;'a str`, and provide methods that advance through the string. This works really well with the iteration methods already provided for &amp;str. 
The branch is on constant data and is optimised out.
I don't see how "lifespan" improves anything. It's the same concept. "Lifespan" is also a little harder to pronounce than "lifetime", but that's purely aesthetic. 
This is such a good example of how elegant Rust's syntax is, and how well the different elements of Rust compose. You could probably be even more succinct: return BytePattern { pattern: { static P = &amp;[0x00u8, 0x00u8, 0x01u8, 0x00u8]; &amp;P }, mask: { static M = &amp;[0xFFu8, 0xFFu8, 0xFFu8, 0xFFu8]; &amp;M }, }
If you have to do it with unsafe code, then I think that's an admission of failure. 
It is probably possible to stuff most of the secret sauce in a safe interface.
Cyclic references, and then it's also generally not a problem to leak memory even in completely garbage collected languages. You just forget a reference somewhere: Have it be logically inaccessible.
No, you don't, overflow might be desired behaviour. Saturated arithmetic might be desired, too. The solution is to have all these actually be different number types, because, well, they're actually different types. What's the default, then, is less of an argument. I'd be happy with throwing a coin, there.
Hmm, so if I am making a multiplayer game and some guy manages to get to max_gold and gains one more gold than max his game or part of it should just crash? 
If you *want* modular or saturating arithmetic, then that's what you want, and it should be easy and efficient to get it. That's why I proposed `a +% b` as a syntax for modular addition. But in the vast majority of cases, you don't actually want modular arithmetic. My team actually considered different number *types*, but that doesn't compose as well as having different number *operations*. The semantics really attach to the operation, not the type. For a counter-example, look at how rounding modes work in SSE. Originally, rounding modes were controlled by a floating-point control flag. If you wanted one specific rounding mode, then you changed the rounding mode (which applied to *all* instructions), then you performed your operation, then you restored the original roudning mode. This is inefficient, when you have to deal with a lot of different rounding modes. (And there are many very common algorithms where you frequently want different rounding modes; look at interval arithmetic, for example.) Later, SSE was extended so that you could specify the rounding mode in each rounding instruction, using 2 immediate-mode bits (bits encoded into the instruction. 
Yes. The alternative is worse -- your game continues with bogus state. With a crash, you get all the data that you need, pinpointing the exact place where your game is broken and needs to be fixed. If you don't protect against overflow, then you expose yourself to bugs like underflow, where if you can find a place where the game underflows (goes below zero), now you suddenly have 2^31 - 1 gold, or whatever. Panic is far preferable to continuing operation when you hit an unexpected overflow.
Right. Which would be a fine thing to do. You could provide a single generic type, which took type parameters H and E, where H was the "header" type and E was the "array element" type. Such a type should go into your platform libraries (like std, or the equivalent), not into user code. User code should be 100% safe, or as close as you can possibly get.
Why do you allow him to get one more gold, if he's at max_gold?
I don't get how panic is preferable to some form of error handling e.g. via exceptions or error codes.
having been through platforms where a branch out of place is intolerable.. I'd never accept something like this as a compulsory default for a language claiming to be a C/C++ replacement. If it works for your use-cases then great; but its' like "bounds-checked arrays" .. for my use cases it belongs in Debug. and indeed a Debug build can have *more* checks, and more help/info when it does fail. Once a program is sound, it should never have an out of bounds array access, so there is no case for any runtime cost in Release, and platforms themselves may be optimised around that knowledge. Embedded software can't "panic!()". (your plane can't fall out of the sky, whatever.. it has to know it's values are always in appropriate ranges) There are many implementations of ARM , with different characteristics. It might restrict your ability to apply vectorisation to the loop.. whatever - predictability and assumptions can always be exploited to make things run faster or on cheaper,more widespread hardware
FWIW, I think the unboxed closure stuff is working pretty well for at least the last week or two. I use them almost exclusively (I need them for a few things) and haven't experienced any new ICEs in some time now.
If your computation overflows, then it's a logic bug in your program, because you didn't validate your data ahead of time.
Originally this was a small part of another library but I thought it might be useful for someone else so I factored it out and provided some documentation. `Morphism` essentially allows you to chain an arbitrarily large number of type-compatible closures (e.g., `fn(A) -&gt; B, fn(B) -&gt; C`) and "compose" them in a loop at the end in order to avoid blowing the stack. I have another library in the works where I use this for a kind of "map fusion", i.e., you can represent a structure that supports `::map()` in such a way that `.map(f).map(g)` behaves like `.map(f.tail(g))` without iterators. As far as the internals go, there's probably a much nicer and cleaner way of making this work but I'm still pretty new to rust. Comments, suggestions, etc. are very welcome.
&gt; With a crash, you get all the data that you need, pinpointing the exact place where your game is broken and needs to be fixed. Yeah, well, the plane have crashed, but in the black box we have all the data we need to fix the next one. What, the next one crashed? Well, it's another bug, the first one we fixed, NP. Tell the relatives of the dead to relax. Compile-time error checking is good, runtime panics are bad. A buggy plane is usually easier to save than one whose software has been killed by arithmetic checks. The best thing, of course, would be to notice the problem and prompt the pilot if he wants to hand control the subsystem. Catching the problem to isolate the harm is the next best thing. Exceptional handling is inevitable. Only in Rust it's still more painful than necessary and I wouldn't want it to panic until the exception handling is improved. &gt; The alternative is worse -- your game continues with bogus state. To continue that anology, a game with a bogus state is *much* better than a crashed one. Users will keep playing and buying the game with a bogus state, they'll run screaming from the panic-ing one. It will be a marketing disaster. The same applies to web sites and web services, in my experience. A site which is down for several hours because of a bug in the math logic is a disaster, it's loosing a big portion of its audience forever. Not to mention their personal anguish if the site was doing something important, like managing plane tickets. It's time to drive to the airport but suddenly you can't check your ticket because somebody overflowed. &gt; With a crash, you get all the data that you need, pinpointing the exact place where your game is broken and needs to be fixed If you want a memory dump, you can make it. google-coredumper, for example. There's no need to kill the thing. BTW, here's a nice idea: Use the checked arithmetic when testing your program locally and disable it when deploying. Most of the time, that's what you want to do. There's only a handful of places where you'd want the checked arithmetics enabled always, risking the unexpected (and in Rust, hard to isolate) faults. It could be a conditional pragma which enables the checked arithmetic, say, in the current module. Then you can even add it everywhere if that's your thing.
&gt; "I'd like to know what you sell, though, because I'd like to avoid buying it." you're taking things out of context. a panic!() or equivalent runtime failiure doesn't pass certification. It rather destroys your sense of immersion in a game. What I sold wouldn't be saleable with these kind of runtime checks. If it has an out of bounds access,or overflow it should have been caught or prevented earlier, BY DESIGN. (e.g.: world data is segmented and you exploit knowledge of its precision, anything that might 'overflow' would have saturation to handle it gracefully). The lions share of the work is data fed to the GPU,which doesn't have any kind of valid behaviour if you give it out of bounds array indices.. (whats it going to draw, FFS.. you screwed up elsewhere) I've also been constrained by having to work on platforms that work as I describe - in order processors with very feeble branch prediction, requiring unrolling and instruction reordering at compile time - branches basically killed them, by their very existence. Yes OOOE is more commonplace now; but you are still asking for a runtime cost. And big AAA games *still* ship on these platforms (PS3,xbox 360). And I realise that the same efficiency (exploiting any predictable coherence in your system ) can still be exploited with vectorisation. Who knows what the future will bring with platforms ? they don't just "get faster" , they make different tradeoffs. Working with shrinking energy supplies is the story of the C21st which means squeezing more out out of less.
Does a `[T, ..0]` have the correct alignment for the allocation as it appears here?
&gt; I'd like to know what you sell, though, because I'd like to avoid buying it. likewise, I'd like to avoid buying anything you're selling because if you need indiscriminate runtime error checks, it's design is unsound and it hasn't been tested enough, and you're wasting energy, throwing CPU power at the problem when actually this world has a surplus of labour and is facing dwindling energy supplies. 
My designs are not unsound. My designs are *verified* to be sound. Your designs *may* be sound. But you don't actually know. As I said elsewhere in this thread, I have measured the effects of checked arithmetic. They are negligible. &gt; when actually this world has a surplus of labour and is facing dwindling energy supplies. This is the most ridiculous thing I've read all day long, and I've spent a lot of time on Reddit. So thanks for the laughs.
&gt; No, it isn't. Game development is absolute shit because of this attitude. I'd also argue contrary. If my game panics and crashes every time I log in, because I have too much gold, I'm going to quit the game, then and there. I'm not going to wait days/weeks/months/years until devs fix their shit. That's the reality of gaming. Your approach with PANIC ALL THE THINGS would cause too much player frustration, would result in bad name for dev/publisher, etc. Error codes are from my understanding standard practice when it comes to critical systems. Exception handling due to its nature can cause premature jump points anywhere in code and is thus avoided. Panics are reserved for extreme cases. Overflowing isn't an extreme case. 
Did someone cause overflow on Gankro's peacefulness :P ?
Virtual memory has a cost. Why don't you run all your code in kernel mode? Didn't think so. As I said you're free to make your own choices. And I'm free to think you're making bad choices and to avoid your products.
Overflowing is absolutely an extreme case. If I log in and lose all my gold because of your buggy game, I'm going to be just as mad.
I see that you're downvoting me, while I'm simply disagreeing with you. Good day, sir.
&gt; We did many performance analyses, and the cost of checked arithmetic varied between 0% and 1%, max. For clarity, I assume you mean 0% to 1% for your whole application, not on a more fine-grained level looking at the computationally intense functions. E.g. the following (which unfortunately isn't vectorised due to a small deficiency in our iterators/LLVM) is much more than 1%: extern crate test; use std::num::Int; use std::rand::{mod, Rng}; use test::Bencher as B; fn get_vec() -&gt; Vec&lt;u32&gt; { // 100 * 1000 &lt;= 2**32 so the sum won't overflow rand::task_rng().gen_iter::&lt;u32&gt;().map(|n| n % 100).take(1000).collect::&lt;Vec&lt;_&gt;&gt;() } #[bench] fn checked(b: &amp;mut B) { let v = get_vec(); b.iter(|| v.iter().fold(0, |a, b| a.checked_add(*b).unwrap())) } #[bench] fn unchecked(b: &amp;mut B) { let v = get_vec(); b.iter(|| v.iter().fold(0, |a, b| a + *b)) } Compiling with `--opt-level=3` gives: test checked ... bench: 1055 ns/iter (+/- 57) test unchecked ... bench: 753 ns/iter (+/- 27) And, it's trivial to adjust those (using unsafe indexing to get vectorisation to work) so the checked version is 7x slower and would be even more with AVX and AVX-512 etc. As you say, those sort of functions can be picked up in profiling and optimised by hand, I'm just asking for clarification about your statement.
Or, with a non-modular addition, 65535+n = 65535. The value saturated, it did not roll over. This works well for gameplay, does not require additional logic, and is the naively expected outcome in this situation (at least when the number is bounded by 2^16). 
`u32` and `i32` are not nature on 64bit machine, i think.
&gt; If I log in and lose all my gold because of your buggy game, I'm going to be just as mad. In this example, it's a difference between having a window broken in winter and having a door which can't be opened. Being completely blocked out of your house is definitely worse than suffering the weather. But the example misses the point completely. It's not about this user being able to login or not, it's about *all* users not being able to login, because your application choosed to crash.
This would be the best solution. The choice is good. Or perhaps a set of unchecked::int vs checked::int types with the standard arithmetic operators doing no checks/checks accordingly, and you opt in to the behaviour suitable for your use case.
You can still do explicit checks where you want to do something more intelligent than crash. In the case of gold you would probably saturate. However by default the program assumes that nothing reasonable can be done on overflow. So when you miss a place where an explicit check should be made, the program explicitly crashes and tells you that you messed up, instead of silently proceeding with a corrupted state.
&gt; The design and certification process for life-critical systems is vastly different from the rest of the software market. The only thing they have in common is that they deal with 1s and 0s. And that's wrong. The business have to apply similar design decisions if it cares about its customers. &gt; An attitude that catches errors during early development and testing -- which panicking would do -- would force a lot of these issues to be fixed. `Panic`-ing during development, which users don't see, I have no problem with.
Generally speaking, there's no simple solution to this. You can't have multiple mutable pointers to an object. You can use unsafe to get around this... but that will inevitably lead you to undefined behavior: bugs and crashes later. Typically if you have a situation where you have a Foo, that contains a set of Bar, and you need to operate on both the Foo and the Bar, you would either: 1) Iterate over the Bar set and update then, THEN update the Foo 2) Move the Bar out of Foo, update them and then move them back. The latter case works in your example above, but is a lot more complex in (for example) the case of a Vec&lt;T&gt;: trait Child { fn do_more_stuff(&amp;mut self, parent:&amp;mut Parent, value:u32) -&gt; bool; } struct Parent&lt;'a&gt; { child: Option&lt;Box&lt;Child + 'a&gt;&gt; } impl&lt;'a&gt; Parent&lt;'a&gt; { pub fn do_stuff(&amp;mut self, value:u32) { if self.child.is_some() { let mut child = self.child.take().unwrap(); child.do_more_stuff(self, value); self.child = Some(child); } } } fn main() { }
I've stated my motivations and the reasons in the RFC, so I won't repeat here. To those who have understood Rust's lifetime well, the new term is just a synonym, but think about the newcomers, and why the lifetime thing stumped you (if it ever did) for a while. I've been coding for 20+ years, mostly in C, C++, Java, Python and JavaScript. Maybe I'm not a talented programmer, but the fact that I'm still coding and love it at least means I'm a reasonably experienced programmer. Looking back at why the lifetime thing baffled me, I found two major pitfalls: 1. The lifetime is static. It's purely determined by the layout of the code, not the execution of the code. (Of course the execution is governed by the code layout, but really Rust's lifetime calculation has nothing to do with runtime behavior. It took me a while to realize this. That's also why a static term *scope* helped me.) 2. The lifetime is related to the references (borrowship) only, nothing to do with object creation or destruction. I think the confusion partly came from my interpretation of *lifetime* in other programming languages, which usually means object lifetimes. To me, the term was a source of confusion, and I think *lifespan* is a better term. I'm not sure if it's just my fancy, so I [asked folks](http://www.reddit.com/r/rust/comments/2nfu5r/is_the_term_lifespan_better_than_lifetime/) on /r/rust/ and found quite some people agreed with me. I know the term lifetime in Rust has been widely used and changing it has cost. In fact, once a thing exists, it has its own lifetime (no pun intended). Maybe it has grown too strong to rename/remove it. Although I've understood Rust's lifetime, as a beneficiary of the Rust programming language and its great community, I felt composing an [RFC](https://github.com/arthurtw/rfcs/blob/master/text/0000-rename-lifetime-to-lifespan.md) to hopefully help others is the due diligence I should do, hence the submission. I'm fine if the end result is the status quo.
The Rust tool-chain was not built to optimize checked arithmetic with abandonment semantics (panicking). The assembly code generated for checked_add() is not as efficient as it would be as the JO idiom, of jumping to a panic label after the addition. It's close, but I see some other things clouding the picture. My team did many targeted microbenchmarks, in order to measure the costs of array bounds checking, virtual dispatch vs. static dispatch, checked arithmetic, and many other things. *Even in microbenchmarks*, we found that the cost of checked arithmetic was in the noise. As in, [0..1]%. I personally tested a port of libjpeg and libpng. Both of them used plenty of checked arithmetic; they are both numerically intensive components. And again, the cost of checked arithmetic was *barely detectable*. And again, it is largely because our compiler toolchain understood abandonment semantics, and generated code optimized for it. I copied your code into a local file and ran the benchmark. At the same opt level, I get fairly similar results: test checked ... bench: 799 ns/iter (+/- 3) test unchecked ... bench: 536 ns/iter (+/- 1) Now let's look at the disassembly. The disassembly shows significantly more than just a JO branch (or JB for unsigned, of course) after the ADD. Here's the unchecked code for the closure. I've edited out most of the irrelevant .foo directives: .def _ZN9unchecked12closure.2052E; movq 32(%rcx), %rdx movq 8(%rdx), %rcx xorl %eax, %eax testq %rcx, %rcx je .LBB6_4 movq (%rdx), %rdx shlq $2, %rcx xorl %eax, %eax .align 16, 0x90 .LBB6_2: testq %rdx, %rdx je .LBB6_4 addl (%rdx), %eax addq $4, %rdx addq $-4, %rcx jne .LBB6_2 .LBB6_4: retq Here's the checked equivalent: .def _ZN7checked12closure.2026E; subq $40, %rsp movq 32(%rcx), %rdx movq 8(%rdx), %rcx xorl %eax, %eax testq %rcx, %rcx je .LBB4_5 movq (%rdx), %rdx shlq $2, %rcx xorl %eax, %eax .align 16, 0x90 .LBB4_2: testq %rdx, %rdx je .LBB4_5 addl (%rdx), %eax jb .LBB4_6 addq $4, %rdx addq $-4, %rcx jne .LBB4_2 .LBB4_5: addq $40, %rsp retq .LBB4_6: leaq _ZN6option15Option$LT$T$GT$6unwrap14_MSG_FILE_LINE20h64951c84312e0c04AtoE(%rip), %rcx callq _ZN9panicking5panic20h949ad8cadc75c9b1TNlE ud2 If you'll notice, there are several differences that are not related to the JB test. Mainly, the "checked" version allocates 64 bytes of stack space: `subq $40, %rsp`. Near the end of the function, this adjusts %rsp back for the caller: `addq $40, %rsp`. The stack diddling is completely unnecessary, and so this clouds the microbenchmark.
&gt; Virtual memory has a cost. Why don't you run ask your code in kernel mode? Didn't think so. &gt; er... when did I say anything about virtual memory. I'd never advocate relying on that. I've worked on products that don't even use dynamic allocators. &gt; I'm free to think you're making bad choices and to avoid your products. completely out of context... you still don't get it. If you had to work on an xbox 360 or PS3, you'd have quickly found those additional checks were intolerable. Large chunks of code had to be regularly butchered to get things to run acceptably.. making key loops *branchless* was essential. And the big games *still ship on these platforms*. You cannot make blanket decisions from your experience and say "this is the way everyone else must work". I've been through a variety of platforms and consequently wont take an inflexible attitude. There are different situations. So a good language should be adaptable. Overloaded operators? compiler switch? there are many options here.
Practically speaking from personal experience, it's in FFI from C code, and badly made unsafe wrappers. Although there's a lot of fuss about 'unsafe code' bugs, honestly I've very seldom encountered 'pure rust' bugs that caused program failure (memory leaks, sure, but not bus errors or segfaults).
That's a really good way to look at it. I think checked arithmetic is an important part of correctness, but I also think memory safety is far more important. I do hope that the Rust architects seriously consider doing a better job with checked arithmetic, but I would be happy enough (overjoyed, actually) if we are able to move systems code to Rust, even as it is now, just for the type safety and concurrency safety.
&gt; er... when did I say anything about virtual memory. I'd never advocate relying on that. It's called an analogy. Look it up. Virtual memory has a cost -- but if your perfect, beautiful code is perfect, *why pay that cost*. &gt; You cannot make blanket decisions from your experience and say "this is the way everyone else must work". Show me where I said that? I said that my team had solid data that showed that checked math had an acceptably low cost, and gave significant value. &gt; I've been through a variety of platforms and consequently wont take an inflexible attitude. That's a surprising statement in this thread. Where you have insisted that there is only one true way to do things. &gt; you still don't get it. This attitude is really grating. I have offered my experience and argued from that experience and the empirical data collected from it. But you come at me with this attitude that "you still don't get it", which basically means "Capitulate to my neckbearded always-right opinion, or gtfo." I've shipped high-performance code on many platforms. I know what the costs are, and I let the data lead me. If my analysis shows that checked math has an unacceptable cost in a given situation, then I'll believe it. If it doesn't, then it doesn't. I have never said that one single approach is the right thing, everywhere. You're the one doing that.
I understand it, that's not the issue. I understand thinking about lifetimes in C++ and the concept is pretty much the same in Rust, it's just part of the core language now. 
&gt; Where you have insisted that there is only one true way to do thing If you're finding these checks are ok in your cases then great, but... &gt; This attitude is really grating. I have offered my experience and argued from that experience and the empirical data collected from it. But you come at me with this attitude that "you still don't get it", which basically means "Capitulate to my neckbearded always-right opinion, or gtfo." &gt; well I've worked on platforms where branching is 'extremely high cost'. Also I know that parallelism generally relies on upfront predictability. This is a higher level idea, that feeds into the design of low level systems. OOOE itself is a runtime cost to achieve parallelism. For unpredictable branchy desktop software its a good choice, but I've seen it *is* possible to cost reduce a machine or parallelise more by taking more advantage of up-front knowledge. *big games still ship on the xbox 360 and ps3* . If you write some cross platform code that ends up there.. you'll kill it if you're taking experiences from x86 and even ARM and assuming that'll work *everywhere* Now, most people are lucky enough not to have to work on these platforms. But I bring them up because C/C++ gets used in such unusual situations. Rust has an ambition to be a C/C++ replacement. It would have to deal with these scenarios. &gt;&gt; It's called an analogy. Look it up. Virtual memory has a cost -- but if your perfect, beautiful code is perfect, why pay that cost. ah ok. Indeed whilst games machines have virtual memory , we were advised to set the 'TLB size' to 64k instead of the default 4k, and not rely on it so much, because of the performance penalty of TLB misses. And I personally have worked on machines without it, I know you can design systems that don't need dynamic allocation, its' a good fit for realtime software.
Yes, right now there's no way to mix capture modes, as in C++. What I would propose is adding them to the list of arguments as faux arguments. i.e.: `move y` does what it says on the tin. It isn't an argument to the closure, but it does move ownership of y to it.
&gt; So when you miss a place where an explicit check should be made, the program explicitly crashes and tells you that you messed up, instead of silently proceeding with a corrupted state. Why isn't returning a result and telling me there is an error preferable to panicking? Why not make explicit checks default? 
Can't you simulate non-blocking reads by spawning the process in a task that loops reading individual bytes and sending them over a channel? The receiving end can use try_recv and continue doing other things, therby maintaining interactivity. edit: ah yes, that seems to be what you're doing in the linked snippet. I didn't actually notice there was a solution in the original message, hence my first reply.
Same reason indexing is implicitly checked, I reckon. It's usually unexpected, there's rarely something useful to do with an overflow, and it would be too tedious to deal with on every operation.
Yeah, thanks for the tip! : ) Sorry for being vague.
While the code is quite short, I have to say, I really like your style, that is some very nicely written code, both to read and to look at. Just thought I'd comment on that. Edit: Also, thinking about it, if it were possible to tail closures recursively (so tail a new closure from inside another closure at append time), this structure could more or less be an explicit version of how a lot of scheme languages implement tail recursion - by using continuations and calling them in a loop from an outer 'trampoline' function that calls each stage. You could use the outer loop in this structure to do tail recursive functions. Though the space requirements for that are probably terrible. Just a thought.
Most programs that can't tolerate overflow should be using bigints. Defining the number size = machine word is just an optimization, and a premature one at that.
This makes me want HKT in Rust so badly it hurts.
Yeah Sounds like it's going to be coming eventually though, right? I hope so anyway :)
Any semantic error. That "if" you forgot to code, that condition you got wrong ("&lt;" instead of "&lt;=") is still going to bite you the same way. The program is going to behave badly, not perform in the expected way, etc. The nice thing is: you can handle the error conditions and fail gently. At least you know it will not crash... but implementing the expected behaviour is a completely different thing.
&gt; If Rust panics, then it doesn't get to leverage hardware support for trapping on overflow. Wouldn't hardware traps be leveragable if rust removed unwind-on-panic, as advocated by /u/mitsuhiko?
&gt; Which AFAIK exists neither on x86_64 nor on ARM. It's in response to the claim that this hypothetical feature would allow avoiding a branch on all arithmetic in the future. &gt; Also wouldn't hardware traps be leveragable if rust removed unwind-on-panic, as advocated by /u/mitsuhiko? I'm strongly in favour of removing unwinding, but I think it's highly unlikely that it would happen without a fork of Rust.
Sorry for the confusion. I've worked in a lot of different environments, and in the main one that is relevant to this discussion, yes, we terminated entire processes when abandonment (panic) occurred. &gt; There is a movement in Rust to make the panics rare and exception handling optional. Your proposal goes against that. I think there is some confusion, here. What do you mean by "rare"? Do you mean that the code contains very few places where panic can occur? Or do you mean that the frequency at which panics occur should be low? I believe that most could should have a very *high* number of checks for invariants. When checks can be performed at compile time, fantastic. When they can't be performed at compile time, then we need dynamic checks. This includes array bounds, arithmetic, null pointer checks, etc. But the frequency of the failures should be absolutely at a minimum. In a correct, debugged program, the failure rate should be (by definition) zero. My team runs production services where 100% of the code is compiled with arithmetic overflow checking enabled. Our failure rate in production, for arithmetic overflow, is zero. The reason is that we have a hardcore policy around this, so all of our bugs are caught during development and unit testing. So nothing makes it through to production. You might ask "Then why not compile out the checks?" For the same reason that the memory at page 0 is not mapped to RAM -- if an error *does* occur, we want a clean failure, with a core dump, so we can debug it. Not a silent failure that goes undetected. So to summarize: Panics should be frequently *written* (even when implicit), but should rarely (if ever) *occur*. 
I assume it does, because `\forall n . n &gt;= 0 -&gt; Example&lt;[T, ..n]&gt; can be cast to Example&lt;[T]&gt;`. Observe: let normal : Example&lt;[bool, ..0]&gt; = Example::new(); let refnormal = &amp;normal; let refunsized : &amp;Example&lt;[bool]&gt; = refnormal; It must be the case that the target of `refunsized` has the correct alignment because it wouldn't make sense to be able to create a reference to an incorrectly aligned instance. We also know `normal` wasn't sneakily moved around to get the correct alignment because that would invalidate `refnormal`. I conclude that `Example&lt;[T,..0]&gt;` has the same alignment as `Example&lt;[T]&gt;`.
&gt; It's in response to the claim that this hypothetical feature would allow avoiding a branch on all arithmetic in the future. I don't think that claim was made this time 'round, although after reconnecting with the discussion's context I'd removed that bit which didn't add any value.
Yes, I believe in the future when most architectures would have efficient checked arithmetic, we can have a major version of Rust breaking backwards compatibility and use checked arithmetic by default. For now sadly I think unchecked arithmetic is a more realistic choice *for Rust*. Swift can get away with its choice because its intended platform (modern x86-64/ARM) have better performing checked arthmetic. 
Huh, I guess there's many things I don't know about processors.
&gt;Yeah, well, the plane have crashed, but in the black box we have all the data we need to fix the next one. What, the next one crashed? Well, it's another bug, the first one we fixed, NP. Tell the relatives of the dead to relax. Doesn't Rust try to reach for that Erlang-style robustness of "crash on error and restart"? So it wouldn't be the plane that crashes, just some single task inside the program. 
I found this the other day on [SO](http://stackoverflow.com/questions/21747136/how-do-i-print-the-type-of-a-variable-in-rust) when I was looking for the same thing. [Playpen link.](http://is.gd/8ryghe)
What happens when you have a static struct with a vec tail and you need to enlarge the vec? You reallocate both the head and the tail?
If you really cared about shrinking energy supplies then why are you making computer games? Surely playing such a game is inherently a waste of energy, and much energy could be saved by making card games and/or puzzles instead? I'm willing to buy they whole 'some platforms have lousy branching and no OOOE' argument, but arguing for removal of bounds checking out of 'concerns over shrinking energy supplies' seems ridiculous, doubly so when the person bringing up those concerns appears to make computer games for a living.
Rust mostly tries a different way of never throwing at all. cf. http://lucumr.pocoo.org/2014/10/30/dont-panic/ The panic handling infrastructure remains a second class citizen. I'd like to see Rust going the Erlang way, but I don't see it happening so far. (On the other hand, Rust going the other way and ditching green threads and runtime will make it easier to embed, which is a big thing). For example, in Erlang the task is really lightweight, spawning it is no problem at all. In Rust tasks are being renamed to threads, they are native OS things which aren't cheap. In Erlang passing things into a task and out is not a problem, in Rust tasks are isolated, you might need to change a lot of logic and code just to wrap something into a task.
Is it possible for Morphism to implement FnOnce? Then it would have cool call syntax, and .tail() and .then() would be equivalent. Edit: Ok I see you concat the Morphisms, so separating .tail and .then is logical. a DList of RingBuf sounds heavy though, but it's similar to a C++ std::deque right?
My bad, it seemed to me that `Sized?` was just a replacement of the `unsized` keyword from the earlier proposal. If it is not a keyword but refer to the Sized trait, why not use the ` T : Sized?` syntax? It seem to me it would make more sense.
Pretty much, yeah.
yep, this is pretty much what I wanted to do, and there is no solution, at least for now, as I see it.
I think we can all agree that the best way to avoid integer overflow errors is to have static checks, not runtime ones. However, to be practical, this requires at least a limited form of dependent typing as found in Idris, Agda etc. I don't think Rust's type system will ever be that advanced, so probably the only viable option is some form of refinement types combined with an SMT solver similar to LiquidHaskell. This is an active research area though and at the moment maybe not ready to be included in a mainstream programming language. If you're gonna have runtime checked arithmetic (with panic failure) I definitely think it shouldn't be the default, but rather a deliberate choice by the developer. However the choice should be easy to apply on all the code in a project (either with a compiler flag or by a simple change to a type alias). It should also be simple to disable the checks for specific parts of the code.
What's the story these days on `foo.clone()` doing surprising things depending on which of the inner/outer types implement `Deref` and/or `Clone`? (Likewise any other method; `clone` is just an example.)
&gt;&gt; If you really cared about shrinking energy supplies then why are you making computer games? This is a whole other tangent. The real problem is the reliance on finite energy to multiply the carrying capacity. (fossil fuelled agriculture and trade) hence the population boom. There's a big difference between renewable and non renewable resources. ironically if we'd wasted more energy (instead of using it to raise the population) I think we'd be in less trouble. As it happens I think computer games are a relatively low energy pastime (aside from when you go overtop with top end overpowered hardware) , relative to other things like physical travel to events. Things like suburban living arrangements generally are a *much* bigger problem. Its a shame games machines (and now phones) are artificially crippled computers (I mean r.e. the OS, not the price performance tradeoffs) -that *is* wasteful - but I don't make the rules. Joe public demands a dumbed down plug &amp; play experience, and those who supply it get to make the rules. anyway as I indicated - I believe we also have a labour surplus (the productivity of a few multiplied by machines is enough), but we keep each other busy competing for these finite resources. Hence all the artificial barriers. (like game consoles that don't allow you to install an OS of your choice, etc). we have spare time, and we have a need for escapism to keep us sane. as fossil fuels run out, billions will die in famines and resource wars, nothing can stop that. Its not like fusion would get solved faster if I worked on something else instead. There's enough *people* working on it, not enough *resources* to try more experiments.
Usually you can find some arrangement of helper functions that solves the problem. #[cfg(USE_FOO)] fn f(n: i32) { foo(n); f_common(n); } #[cfg(not(USE_FOO))] fn f(n: i32) { f_common(n); } #[cfg(USE_FOO)] fn foo(x: i32) { .. }
Sorry, I understood that from your original post and didn't mean to imply otherwise. I only meant that there's a learning curve involved in using the heavy syntax that may play into the frustration. In other words understanding the concepts and feeling comfortable expressing them in the given syntax are different things. Both of the above are obstacles to new users, you're over one already, but it sounds like the second obstacle (the heavy syntax one) is frustrating you. I was suggesting that maybe once you've grown more accustom to the syntax, and become fluent in it, that it wouldn't feel like such a burden and you could enjoy the benefits. Of course you're free to disagree with that. But since you said you want to like Rust I wanted to provide one data point of someone who felt the same way in the past and got over that hump. Whether the increased safety is worth the heavy syntax, even after 100% fluency, is a philosophical question and if your answer is "No" maybe another language is a better choice for you. If I didn't care as much about the extra safety I would probably use Go (except for its current lack of generics).
This is a rather old RFC which hasn't been accepted nor rejected yet. There've been multiple discussions about overflow here recently, and each time its existence seems to have come as a small surprise to the participants (presumably because of its non-recentness). So consider this an attempt to raise awareness.
FWIW, the compiler uses hash-tables, and it's semilikely that the hash function(s) used (e.g. FNV) trigger and rely on arithmetic overflow. (Obviously this something that is easy to manually guarantee that is using overflow.)
It's been suggested that memory leaks should be supported in safe code. It's not unsafe to leak memory, it's just a (possible) bug. leaking allows you to convert a Vec&lt;T&gt; into a &amp;'static [T] for example..
We seem to be working on the same kinds of software bases (I write quite a lot of code that probably couldn't tolerate overflow checks) but I have absolutely 0 problem with checked arithmetics being the default (actually, I'd welcome it) as long as there's an easy way to turn it off for a specific operation/function/program. I mean, right now rust generates function preludes to reallocate the stack if it overflows (or panic if it can't I suppose), it does the check every time a function that needs to grow the stack is entered. Be it in debug or release builds. Is that acceptable for low level "bare metal" code? Absolutely not. Is there a way to turn it off on a function per function basis or globally? Sure. Is there a problem then? Not in my opinion. I'd wager the very vast majority of code out there could do with checked arithmetic by default without any noticeable performance degradation. And if you start noticing your code runs too slow you can profile it and remove the checks where it makes sense. And your "if you panic! the airplane falls out of the sky example" is amusing, because if your code is *that* sensitive then you probably don't want an unexpected integer overflow to go unchecked either. It's probably better to have a sensitive piece of equipment report "I'm having a problem, I can't give you the right result" rather than reporting a completely bogus result. See for instance the famous ariane 5 incident: http://sunnyday.mit.edu/accidents/Ariane5accidentreport.html .
From a pure correctness standpoint, I don't actually think adding some sort of fail-on-overflow is any real improvement. If you're designing software for the Mars rover, I don't think it makes much difference if your system crashes on overflow or simply proceeds and performs incorrectly. In either case, you're in big trouble; and more relevant to the discussion, neither case is the behavior which the programmer actually wanted. From a mathematical standpoint, the only correct thing to do is to formally prove at compile time that your program cannot overflow, which is not doable in Rust (and won't be for the foreseeable future, and probably never will be). And let me comment here that manually if-checking the overflow is almost certainly not going to give correct behavior either: let's say you do check if(!multWillOverflow(a,b)), well what do you do now? What could you possibly write in that block that would be considered correct, desired behavior and not just a sugar-coated crash? If you don't prove overflow can't happen, I don't see what difference it makes if your program behaves wildly or crashes; either way it's completely wrong. Practically speaking (systems-level engineers are typically practical people), with 64-bit integers, in most applications, the probability of this happening is quite low; if your application actually does require formal guarantees that that cannot happen, this proposal won't help you anyway -- it just makes your program fail loudly instead of silently. If you actually need formal assurance that your program is correct in this way, you're using the wrong language. As far as failing loudly vs silently goes, sure, this is useful for debugging. Maybe checking all arithmetic in debug builds is a good idea -- I wouldn't be against that; but in production, I don't see how loud failure is any more correct than silent failure except possibly in security scenarios. It's less performant, but not more correct.
Well, two reasons: First, the `T: Trait` syntax implies that there is a _bound_ on the `T`, that is you accept only those types that implement the trait, it restricts the number of allowed types. The `Trait? T` syntax on the other hand _widens_ the number of accepted types, so more types than per default are allowed. Putting something that allows more types into the syntactic position that normally implies less types just seems more confusing to me. The second reason is that `T: Sized?` reads as "`T` might have a bound on `Sized`", which is exactly the wrong intuitions to have here, as a bare `T` always implements `Sized`. In fact, `T: Trait?` would always be identical to just `T`, as you could never assume the existence of a `Trait` implementation.
Thanks for the tips, I will need some time to study this code snippet!
Except that taking the address of a static is worse than what you need, which a const `&amp;'static [T]`. Changing const items to rvalues seemed fine at the time, and I was disagreeing with /u/strncat on the matter, but dealing with the implementation in trans made me realize it's actually terrible.
I see. Well, I think its unnecessary from a pure language perspective to support such fine-grained syntax, as you can mix and match the capture modes as you like with by-value captures. (By-reference closures are purely there as common convenience). This is more verbose, of course, but I could also imagine that with an overhauled expression-attribute and macro system it would be possible to define something like this: let f = #capture[move x, ref y, clone z] || x + *y + z; That expands to let f = {let x = x; let y = &amp;y; let z = z.clone(); move || x + *y + z;} ~~(Though I realized just now that that would not work for the reference capture due to the scope. Nevertheless, the idea seems like a good basis)~~ ~~Nevermind, that works fine.~~ Okay, I had to play around with a up-to-date rustc install and the current state of half implemented unboxed closures, but this definitely compiles and should be usable as the example above: #![feature(unboxed_closures)] fn main() { let (x, y, z) = (1u32, 2u32, 3u32); let f = {let x = x; let y = &amp;y; let z = z.clone(); move |&amp;:| x + *y + z}; println!("{}", f()); } 
Of course preventing overflow is better but I can see advantages in failing instead of doing nothing : - you immediately notice something is going wrong and where. - an incorrect behavior might be used to break security(even if memory safety greatly reduce the risk) 
Error at the time of overflow is definitely preferable to unexpected behaviour - I can restart a crashing subsystem. The counter argument is to just use BigInt in the places where you need safety against overflow. Arguably the safest language design would be BigInt everywhere by default, and let the programmer drop into native machine int as an optimisation. 
Yes. The benefit would come from these areas: * Earlier and easier detection of bugs. A panic is much more obvious than silent wrong behavior, both with respect to "something is wrong" and "why is it wrong". A notable consequence is that the normal human intuition that "if it doesn't make sense for a value to be negative, use an unsigned type" becomes valid again - I think this is pretty important. In C and C++ the recommendation (sensibly, but counterintuitively) is to use signed types even in these cases, because then you can at least `assert(value &gt;= 0)`. With an unsigned type, "going below 0" merely gives you a very large positive value, which you then have no way of distinguishing from any other, legitimate, large positive value. You had meant to use the types to better express your intent, but you've only made it impossible to detect when those expectations are violated. This is rather insidious. * Likewise, it becomes possible for static analysis tools to discover overflow bugs without false positives. (Which, while it can obviously never be perfect, can still be very useful.) Potentially including warnings in the compiler itself! If there is no way to distinguish where modulo arithmetic is correct and desired from where overflow is a bug, this is a lot harder. * The same thing goes for humans as well. If a human is auditing the code for overflow bugs, her job is easier if the code itself contains the information about whether or not overflow is an expected circumstance. * This obviously depends on the particular case, but in many of them panicking may be less catastrophic than silently incorrect behavior of the program.
I don't really buy the argument, since the interrogation mark make obvious it is not a usual bound. 
Why can we readily a accept divide by zero is a runtime error, but arithmetic overflow is not? Our programs should produce mathematically sensible results or panic. Modulo arithmetic is an optimisation that shouldn't be baked in as the default.
We all hope so, there's just not guarantees.
It's not just suggested, it's explicitly behavior not considered unsafe http://doc.rust-lang.org/reference.html#behaviour-not-considered-unsafe
It did before, but doesn't really now.
(I'm not sure why people are downvoting the parent, what she/he says is quite true, as far as it goes. Checked arithmetic is an unsatisfying solution and only a "least bad" compromise.)
You mean "byte string literals", right? http://doc.rust-lang.org/reference.html#byte-string-literals Yes, this must be exactly what the OP was looking for. Both of the OP and I had a misunderstanding that byte (string) literals can only contain u8 values in the 7 bit ASCII character set. The fact is that byte string literals can denote arbitrary &amp;'static [u8] though we can use only 7 bit ASCII characters to input them on a source file (we need help from backslash escape notation.)
Example: let (x, y, z) = (1u32, 2u32, 3u32); let g = capture!(move x, ref y, clone z in move |:| x + *y + z); assert_eq!(g(), 6); And I hope that one day, should we have working expression macros, a lighter attribute syntax and free-form syntax inside a attribute, it would become possible to define the macro as an attribute instead: let (x, y, z) = (1u32, 2u32, 3u32); let g = #capture[move x, ref y, clone z] || x + *y + z; assert_eq!(g(), 6); 
I'd like to address something that bothers me in this whole thread. Many of you seem to make the following assumption: *When an overflow occurs, panicking will always lead to catastrophic failure while continuing with the bogus state might kind of work.* I really don't think that's true. A "clean" panic might actually be preferable and avoid letting a bogus variable corrupt the rest of the state, even from a end user perspective. Let's take the videogame example. You have a variable that contains the id of the level you're in. There's a bug in the game that causes an overflow on this id in a certain case and you end up in a wrong level. You can't exit the level because you're not supposed to be there (you don't have the key to open the door or something). If the overflow panics the problem is identified and the action does not complete. You can submit a bug report to the devs and restart from your last save and attempt not to trigger the problem again. If the overflow goes unnoticed you end up stuck in a bogus level. Then the game autosaves. Good job, you're stuck forever, you can restart a new character. And I can come up with a shitload of examples. You have an auth system that stores the user's privilege level as an integer. The lower the value the highest the priority. An attacker finds a way to overflow the value. I don't think anybody would argue that continuing is worse than panicking in that case.
The point is, its not bound at all. Its not an unusual bound, its the opposite of a bound, a un-bound. :P
Problem is, we both can list examples left and right. That seems to indicate that there are use cases when panicking is ok and there are use cases where panicking is definitely not ok. 
Which is worse, an error undetected or an error detected? I'm in the detected camp. And because of that, my team's product had already gone through the process of finding and fixing these bugs.
[mio](https://github.com/carllerche/mio) now has support for pipes. It is a non-blocking, poll based model. It would be trivial to set up a handle which prints data as soon as it arrives. 
&gt; Edit: Ok I see you concat the Morphisms, so separating .tail and .then is logical. a DList of RingBuf sounds heavy though, but it's similar to a C++ std::deque right? Sort of, yeah. Both DList and RingBuf used to implement a Deque trait too. Originally I was using just RingBuf and concatenating Morphisms was done in a loop: I'd pick the smaller of the two RingBufs and move its morphisms into the other so it was O(min(n, m)). But with the outer DList I can just append which is O(1).
In principle they could, but the syntax and terminology has been chosen such that they best fit the fact that there are only bounds. Allowing un-bounds there would not break the language, but ruin much of the reasoning and theory behind the syntax, and the documentation. Note also that negative bounds are not really a problem here: They are basically just an operator that produces a trait that refers to all types for which another trait is not implemented. That is, `T: !Foo` is still using a trait as a bound. In any case, this all ran through an design process many months ago, and I had nothing to do with it, so if you are still unconvinced you should probably dig for the relevant meeting minutes or talk with one of the core developers. :)
Isn't the question more if bringing the whole system down in flames is the appropriate way to detect the error? Especially if the application isn't run by a developer, but by the end user?
Many people ignore the issue. In a garbage collected language it's very easy to ignore, you don't understand why your application takes so much memory (unused variables kept too long...) but it's complicated to investigate and you have better things to do with your time... ... in bare bone languages like C++ most of the people I have seen (including most of my colleagues) will just ignore the issue entirely until their program crash, at which point they try and locate one of those mysterious guys rumored to know how to read a crash report and divine what the fix should be.
I'm not familiar enough with static numeric check systems, but I can agree that static &gt;&gt;&gt; run-time, whenever possible.
The DList is probably not needed except in some limit at very large number of morphisms or closures.
That would be true if rust wasn't opinionated on how to handle such issues. It wouldn't make sense to add those checks in C for instance. However I think it's a bit incoherent that rust enforces things like buffer overflows by panicking while ignoring things like integer overflows. Both can lead to nasty bugs and security issues. I find that drawing the line at "memory safety" is a bit arbitrary and hard to justify in this case.
Yeah, that's true. I wasn't entirely sold on the change but since we delay closure composition I felt it would be more consistent to also delay morphism composition. Once there are some large examples to test with it might be worth revisiting. I'm open to other designs too.
Actually... modulo arithmetic would be *far worse*. Imagine that you have some module to measure at which angle the plane is descending or ascending, you use `u16` to represent the possible range of values: `[-/2, /2]` (in radians, thus). Now, some guy made a mistake, and added a value in degrees, such that the order of magnitude is completely off. You are in one of two situations: - with modulo arithmetic: you basically get a random value in your interval, goodbye plane - with panic arithmetic: the static analysis tool you ran tell you that this operation could possibly panic and should be checked, and if ignored, your task is toast and the watchdog restarts it Personally, the latter just seems better.
I would much prefer to detect the error and get crash dumps from the field, *and fix the error*, than let things just march on with corrupted data. If you can't stand your customers knowing you have bugs, then maybe work on finding and fixing those bugs in the first place.
Unfortunately, a debug build checking this is impossible, and any kind of static analysis is impossible too. Why? Because since the specifications today says that overflow is *legal* and defined as wrapping, overflow is *used*, for example when computing hashes. As a result, any static analyzer would report those uses as spurious warnings and a debug build of anything using a hash would probably panic in the hash. That is why the result of opting for wrapping by default is so much of a problem.
Unfortunately, if wrapping on overflow is the defined semantic, then no static check can point it to you without triggering spurious warnings for the cases where it's intended. On the other hand, even without going so far as actually checking it, simply defining the value of the result as *undefined* and defining special wrapping operations/functions would immediately allow the compiler and static analysis to point with no false positive lots of places where there is a risk and also allow turning on runtime detection with a flag.
I could certainly be swayed to this point of view. In my experience, using operators was the clearest and simplest approach. But the type approach probably has merits, too. And I'm willing to follow it to its conclusion -- to try out writing code with the type approach, and see where it leads. I do think that the default behavior should put a higher priority on correctness than performance. Profiling should tell you where your costs are; then you can hammer down the costs by casting to the u32_unchecked type, or something like that. Saturating arithmetic is the rarest beast of them all. It certainly has its uses, but it's far less frequently used than ordinary arithmetic, regardless of whether ordinary arithmetic is checked or unchecked.
A very good point. If the hardware provided an efficient checked-ADD instruction, which would raise #OVF or something, then none of this would be an issue at all, and we could have safety and efficiency, both.
&gt; Both can lead to nasty bugs and security issues. Remember, Rust is concerned with memory safety, not 'bugs.' Many, many kinds of bugs are possible in Rust. It's hard enough to tackle one entire class of bugs like this. Hopefully a Rust++ will be able to eliminate other classes of bugs too. In a certain sense, it is arbitrary: it was chosen as the premeir style of problem for Rust to deal with, out of many others. But it's not as if what is and isn't memory safe is arbitrary: it has a pretty straightforward definition.
From the other thread though, /u/0xdeadf001 claims [that performance is rarely affected](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmeqh47) because: - in general purpose cost, the performance of arithmetic is dwarfed by other issues (cache misses, memory allocation, etc...) - in numerically heavy code and performance hot spots, you are always free to drop to unchecked arithmetic using special purpose functions This is actually a very compelling argument, which is very much a mirror of how `unsafe` can be used today to speed up a hot spot. 
Although I may not agree with the actual syntax proposed here, I do agree with the intent: opting into wrapping arithmetic should be necessary, as opting into `unsafe` is, before having access to this quirky behavior.
&gt;oops, we accidentally fired a nuclear head on New-York, please accept our apologies and be assured that...) This gives a whole new meaning to **destructors**. Maybe you shouldn't crash that task. :-)
&gt; Now, some guy made a mistake, and added a value in degrees This shouldn't be possible in your program.
I was just looking for a simple way to do simple audio playback. This couldn't have been timed better! It fails to build however, did you intend to hard-code the path `../rust-portaudio`?
I don't suppose it's actually able to forbid access to values which aren't explicitly captured, right?
I think I should point out that it is just something I hacked together so it may not be the best way to accomplish what you want. I'm quite sure it could be done better :)
Thanks for the pointer, I shall try it out! BTW, does it have a line-buffering somewhere in there? Because when you simply read what's there from stdout and stderr there might be overlaps, half-lines printed early, I've seen this happen, so line-buffering might be important too.
Ahhh my bad, I'll fix it now :) EDIT: should work fine now!
Well, the point is, that shouldn't be mandatory for everyone. Your proposal to just make all the default math operators panic, it should be optional, allowing the program author to compile his program and its dependencies without such harsh measures.
His analysis is very interesting and makes sense. However he doesn't say anything about auto vectorization and how forcing this by default would interact with vectorization annotations like, e.g., `#pragma omp simd` which basically auto-vectorizes your code (or tells you why it cannot be vectorized). I guess one would have to just try it out and see what happens. Best case it works as predicted, worst case everyone is using unchecked arithmetic in 2 years because of fear of lost performance and we are exactly where we are now. I say, let people choose, put a compiler switch so that everyone can choose until we know for sure what is the right thing to do. Anyhow this seems rushed for 1.0, lot of experience is required not only in using this feature but in the compiler backend. Maybe it is possible to implement optimizations in the rust backend (before the LLVM optimizations) that would make such a feature perfectly efficient. Who knows.
When gold hits MAX_GOLD you should detect it and do something (like cap it and warn the user). Silently resetting to 0 would be undesirable, but that's what happens with unchecked arithmetic. The argument for checked arithmetic is that failing to handle this behaviour should cause the program to stop instead of [behaving wrong](http://www.reddit.com/r/civ/comments/2k4cwn/real_life_gandhi_vs_civilization_gandhi/clhy1wj). (Edit: Oops, that's what I get for replying without refreshing the page. :-)
&gt;When gold hits MAX_GOLD you should detect it and do something (like cap it and warn the user). Silently resetting to 0 would be undesirable, but that's what happens with unchecked arithmetic. Yes, but in case of MAX_GOLD, checking for integer overflow is useless. You should check for gold overflow, with the limit you specified yourself. If you forgot this, it doesn't matter what your gold does at u32 limit, your program is already incorrect.
&gt; What I meant is, if you don't have an upper limit, then clearly neither u32 nor u64 is enough. And if you have an upper limit, what are the chances it will be the exact limit of u32 or u64? We have a misunderstanding here. I meant to say &gt;&gt; But that'd imply that you actually have this **explicit** upper limit, and don't forget to check it every time you manipulate that value. Of course, the values representable by any human-storable number is limited. We just don't have unlimited memory, so can't store irrational numbers in full precision. But... that's not what I was aiming for: Here, you explicitly set the 1000 as the upper limit, but should I need to do this for every number I use? Consider the upper (and lower?) limit every time I use a number in Rust? No, I don't think that's sane. In the case that you explicitly decide on bounds, default checked arithmetic is no substantial advantage over unchecked arithmetic, and if you never forget to check these bounds, there actually is a tiny disadvantage (speed). However, usually you wouldn't know precise bounds, e.g. for the size of a vector, the maximum amount of connections, the index in a while loop, and, for those cases, I consider it better to notify the programmer: *Hey, I think this is not intentional, because you didn't use the special operators for unchecked arithmetics*. In any case, the negative impact is minimal, but the potential advantage is substantial.
Checked arithmetic is an implementation detail you have to deal with. It's unlikely that u32/u64 limit and MAX_GOLD will be the same, but you want to try to fit into u32/u64 for performance because that's what the machine can handle well. If you don't have a limit then you should use BigInt.
A problem with that seems to be that tasks in Rust are OS-level threads, so not as lightweight as tasks (or whatever they're called) in fail-fast languages like Erlang. Or is that not a problem?
Ahhh okay, so maybe a user defined type with arbitrary bounds? I wonder if the [Bounded](http://doc.rust-lang.org/0.11.0/std/num/trait.Bounded.html) trait could be used here?
Sure, it can be used for checking, but you'd still have to use [checked_op](http://doc.rust-lang.org/nightly/std/num/trait.Int.html#tymethod.checked_add) or some other checking mechanism explicitly. Oh, and Bounded [is deprecated](http://doc.rust-lang.org/nightly/src/core/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libcore/num/mod.rs.html#1818-1823).
&gt; I'm excited that there's someone else interested in this :) I'm interested too. We're mainly a Mac shop and my wet dream is to write complete software in a strict static typed language. Including the UI stuff. Currently we're writing our business logic parts in C++11. But for the UI part we're using Obj-C (which has very nice interop with C++) because Cocoa beats any other UI framework out there. It would be great if there was a Cocoa wrapper for Rust so one could write safe UI code. I've thought about hacking something together but Rust isn't syntax stable yet and I've got too much other work to do. So yeah :(
As far as I can tell, no. Arenas have no support for resizing an allocation to them. You'd have to manually resize the allocation of a pointer with `alloc::heap::reallocate()`. This also requires you to calculate the new size of the object in bytes as well as the alignment, which I think you would set to the size of a cache line in your target processor.
So you can't have a TypedArena with a type Vec&lt;T&gt;?
I think memory safety is a fine line to draw your line! Otherwise you end up with essentially trying to write low level Coq. Buffer overflow play a much larger role in browser security than intever overflows so it makes sense to focus on that asoect.
No, because that will only allocate the struct of the `Vec` itself to the arena, which is only a pointer to the heap and two `uint` fields: pub struct Vec&lt;T&gt; { ptr: *mut T, len: uint, cap: uint, } You could re-implement `Vec` as a struct that keeps the vector within its layout and resizes itself automatically, which is basically the hack OP is trying except encapsulated in a struct (which still works as owned sub-structs are kept within the layout of their owners, and that's also why you can't have recursive types without indirection, since the memory size of the outermost struct would be theoretically infinite). You'd have to have some way of asserting that the vector is heap-allocated, since it won't work right on the stack.
Good stuff, but oh man, you are barely scratching the surface. Think about the fact that because the rust compiler is written in rust, it can compile itself! Crazy!
I like that we can link to packages on Crates now, but it doesn't exactly make for a stellar presentation, with only one line of description.
probably relevant for you: https://github.com/rust-lang/rust/issues/9898#issuecomment-64921775
I think [this](https://github.com/rust-lang/rust/issues/9469) is relevent. Is multiplying numbers in a loop a useful benchmark? Is there any reason (besides performance) that overflow shouldn't be handled similarly to DBZ?
I found a resource leak marked unsafe. http://doc.rust-lang.org/std/os/fn.pipe.html
https://github.com/rust-lang/rust/issues/19384
It's not *harsh*. It's *correct*. Rust has a strong emphasis on correctness. Thus array bounds checks, no dangling pointers, etc. The default should be correct, not fast. The "fast, not correct" viewpoint is what has made the software industry such a steaming pile of crap. Then, *when you have performance data that shows you where your performance problems are*, you make things efficient. And in my experience, with *hard empirical data to back it up*, checked arithmetic is rarely has a first-order impact on performance. In nearly every case, cache footprint dominates. 
I'm not an expert, but as far as I know `objc_msgSend_fpret` is only necessary when you have a method that returns a float; this could be necessary on certain architectures for methods like NSNumber's `floatValue`. We don't need all these different methods when wrapping NSValue in Rust, though, because we can use generics. [I did this using NSValue's `getValue:` method](https://github.com/SSheldon/rust-objc/blob/master/foundation/value.rs), which copies the value into a buffer. Since this doesn't return anything, no need for `objc_msgSend_fpret`!
&gt; It would be great if there was a Cocoa wrapper for Rust so one could write safe UI code. Yeah, that was my original motivation; I work as an iOS developer, so I was wondering what it'd look like to work with UIKit/Cocoa from rust. &gt; I've thought about hacking something together but Rust isn't syntax stable yet and I've got too much other work to do. Haven't actually had much trouble from rust changing; every week or two I update my version of rust, and sometimes there are no problems, sometimes there are a few things to fix. It can be kind of fun :P
Yeah, I had actually thought about that... I settled for the crates.io link because both the github repo and the documentation are reachable from it, but the links are kinda hidden away at the bottom of the page, so its still akward.
It's not about performance, it's about runtime safety. Correctness for the sake of correctness doesn't make much sense, it's the end user experience which matters. Returning to runtime errors is like returning to NPEs and races. The end result is always a program not working when the user least expects it to fail. Your team achieves zero-bug execution in production, you're a miracle worker, you don't need Rust. You're like fakir and blade-dancer, the danger only excites you. We, humble mortals, need the language to make our programs safer and less panicky, not the other way around.
Not to dissuade you from the original question, as I would like to hear also, but you might research embedded Erlang. http://www.erlang-embedded.com There is not a lot, but they've done some pretty cool projects with that. I think it may require a Linux os, so that might not fit your requirements. Good luck with it though and be sure to post updates if you get anything running.
Thanks! I am considering Erlang also, but the purely-functional paradigm concerns me. And yes so far attempts to run erlang on ARM have been hosted on linux. Also looking into D and Smalltalk/Newspeak which have actor-style concurrency, but D has overhead from garbage collection, and Smalltalk/Newspeak have both VMs and garbage collection. I'm assuming the overall footprint from Rust (specified as a systems programming language) is smaller than D, Erlang, or Smalltalk. 
You probably want to look at [Zinc](http://zinc.rs/). From what I understand, it's an embedded dev stack for ARM. Rust *technically* has a runtime, but it's in the process of being eliminated in favour of a layer on top of the OS's native IO interface. *However*, you don't need it to be there. Various people (myself included) have written Rust code that runs directly on hardware without an OS. `#[no_std]` removes the implicit link to the standard library, letting you use as little as you want (though you do need to provide a few intrinsics like a function that's called when a panic happens).
If you do explore the Erlang route I heard some of the same people are looking into using Elixir which might be a bit more learner friendly than Erlang.
Excellent, thank you. Glad to see there are others already working on such things. **Edit:** Also, I wonder what hardware have you used Rust on without an OS?
No. The exact same logic that applies to stay bounds checking, null pointer checking, and virtual memory applies to the correctness of something as fundamental as arithmetic. My team achieves reliability in production precisely because we don't trust ourselves to be perfect, so we use our tools to expose bugs.
If you want to expose bugs, why panic? Why not simply log the place where the integer overflow occured, complete with stack trace and registers, or even a full memory dump if you want?
Oooh, sweet.
Usually you have no real in-game reason to have a MAX_GOLD. If the player earns more gold, well, he'll have more gold. There is no reason to limit that to 1000. But there usually are technical reasons, like that it's stored in a u32. These limit can be automatically checked if you have checked arithmetic. That's a very common case. You have an amount of gold or a time span or any other number and if you add to it you expect the value to get larger. You would want unbounded values, but due to technical limitations there is an upper bound. You just hope that upper bound is large enough to never be reached. Checked arithmetic notices if it happens anyway.
That's exactly what I do, in my environment. And because the process is no longer in a consistent state (invariants have already been violated), our environment terminates the process. So I get a memory dump and a machine report, then we fix it, then it doesn't happen again.
I feel like this should be stressed a lot more. Either you *can't* handle overflows, in which case you use bigints, or you think long and hard about what size integer you need. If you have artificial limits on your data (like the `MAX_GOLD` example), you check it explicitly. I know that unexpected things can happen, but to me that seems like an input validation issue rather than a language semantics issue.
For what it's worth, this is similar to this [rfc issue](https://github.com/rust-lang/rfcs/issues/421) which deals with module scoping, not enum scoping. [EDIT] This issue is a bit old and doesn't deal with the new 'fully namespaced enums' but is still similar.
This comment doesn't seem to match [the obvious benchmark](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmezc4p) at all, e.g. `perf stat` on the checked vs. unchecked gives 33% vs. 9% frontend cycles idle and 2.6 vs. 3.2 instructions per cycle. I would be very interested in a response [to my reply](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmf5b8f) about the analysis of the compiled code. I would be quite surprised if JO was really 33% faster than JB, but maybe this is the case?
While I like Rust (and even ported it to iOS), I believe if you develop for *OS X only*, better way to go is to use Swift both for business logic and UI.
&gt; so I was wondering what it'd look like to work with UIKit/Cocoa from rust. The question is why do you want to use UIKit/Cocoa from Rust? What benefits it gives to you? 
Right, I overlooked that getValue is using a reference, I just looked at the return type ana remembered that there was something tricky
cool :) 
But bugs happen, [even to the most expensive software](http://en.wikipedia.org/wiki/Cluster_%28spacecraft%29): &gt; Specifically, the Ariane 5's greater horizontal acceleration caused the computers in both the back-up and primary platforms to crash and emit diagnostic data misinterpreted by the autopilot as spurious position and velocity data. Thankfully, the flight termination system kicked and engaged self-destruction. Still, if diagnostic data can be interpreted as spurious position/velocity, certainly one can add degrees to radians. I also seem to remember a feet/meters bug in launcher software though I can't put my hands on it.
&gt; You're mistaking the panics with the idiomatic Rust error handling IMHO. I am not. Static analysis tools can be configured to look for certain classes of errors. - panics on memory allocation may occur anywhere, thus are not worth looking for - panics on arithmetic operations can be reported without false positives - wrapping on overflow is allowed by the language, so cannot be reported without false positives 
It's unclear what compiler switch you advocate here. The way I see it working is: - move away from wrapping arithmetic, and specify that the value of an integer in underflow/overflow is unspecified - provide a compiler switch to activate runtime checks (with panic) outside of `unsafe` blocks - activate the switch by default in Debug, but deactivate by default it in Release Note that moving away from wrapping arithmetic is a necessary prerequisite, the two semantics cannot meaningfully co-exist. Also, possibly, one could retain wrapping arithmetic in `unsafe` blocks, to ease implementing hashing algorithm and other bit twiddling operations.
If you're correct about this, how is it that seemingly everyone else is getting it wrong and that the performance impact is non-trivial? Even downthread, huon refutes your analysis, and Rust seems to already produce optimal code, yet still suffers a large % impact. Please provide details. It'd be a shame if unchecked was left as a default due to some performance secret.
&gt;Still, if diagnostic data can be interpreted as spurious position/velocity, certainly one can add degrees to radians. I I'm not saying it doesn't happen, just that it shouldn't be possible in ideal. Degrees and radians should be 2 different types.
How can a compiler compile itself, when it's already compiled? :-)
I don't get it. Are you going to move every "a + b" operation into a separate thread (=task) just to make the error handing of overflow panics sane? Or are we talking some theoretical ground where Rust isn't even relevant?
It improves on the syntax but it's still a functional language in the same way than Erlang - There is still no mutable data. I don't think they would work on a Cortex M type CPU.
Of course... ... however at some point they are just integers (or floating point numbers). And therefore, even with different types, an error may occur at the boundary of the system.
I think `unsafe` blocks silently changing the meaning of code therein is likely to be a bad idea. The only effect should be to permit calling `unsafe fn`s and operations. EDIT: A semi-related thought I've had though is that perhaps we could make unwinding through an `unsafe` block abort the process by default, except where there is some kind of explicit indication that it should keep going? Obviously aborting is non-ideal, but plausibly less non-ideal than memory unsafety and security bugs resulting from `unsafe` code not having considered the possibility of a `panic` happening. A kind of harm reduction.
Out of curiosity, what syntax would you prefer?
I'm not sure. Let's say its Rust, and you have Degree(int) and Radian(int) wrapper types, with the int field being private. There is no way you can do `Degree + Radian` by mistake, unless you implemented the `+` operator to add Degree and Radian. But if you actually cared to implement the `+` for Degree and Radian types, you probably cared to convert one of them to the other type in the implementation, so that `Degree + Radian` is actually `Degree.add(Radian.to_degree())`.
1. C/C++ is usually used in low-level, UIKit/Cocoa is UI level and I believe Rust gives almost no advantages for UI level, also p.2 2. There is also Swift, which has access to almost all high level UIKit/Cocoa machinery (so you don't have to re-invent all the bindings) and it is has a lot of nicities Rust is good in low-level and it can replace C++ in that. There are only 2 questions: 1. Will it solve your problem much better than Swift? 2. If it will (for example for cross-platform business logic) - do you need to use UIKit/Cocoa from that "engine" or it is easier to use other ways of communication with it (like RPC or built-in HTTP server or C API, there is a lot of options)
The first versions were in OCaml... That compiler was able to compile rust, so they programmed the compiler in Rust and it was compiled. (See http://en.wikipedia.org/wiki/Bootstrapping_(compilers) for more information)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Bootstrapping (compilers)**](https://en.wikipedia.org/wiki/Bootstrapping%20(compilers\)): [](#sfw) --- &gt; &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), __bootstrapping__ is the process of writing a [compiler](https://en.wikipedia.org/wiki/Compiler) (or [assembler](https://en.wikipedia.org/wiki/Assembly_language#Assembler)) in the target [programming language](https://en.wikipedia.org/wiki/Programming_language) which it is intended to compile. Applying this technique leads to a [self-hosting compiler](https://en.wikipedia.org/wiki/Self-hosting_compiler). &gt;Many compilers for many programming languages are bootstrapped, including compilers for [BASIC](https://en.wikipedia.org/wiki/BASIC), [ALGOL](https://en.wikipedia.org/wiki/ALGOL), [C](https://en.wikipedia.org/wiki/C_(programming_language\)), [Pascal](https://en.wikipedia.org/wiki/Pascal_programming_language), [PL/I](https://en.wikipedia.org/wiki/PL/I), [Factor](https://en.wikipedia.org/wiki/Factor_programming_language), [Haskell](https://en.wikipedia.org/wiki/Haskell_(programming_language\)), [Modula-2](https://en.wikipedia.org/wiki/Modula-2), [Oberon](https://en.wikipedia.org/wiki/Oberon_programming_language), [OCaml](https://en.wikipedia.org/wiki/OCaml), [Common Lisp](https://en.wikipedia.org/wiki/Common_Lisp), [Scheme](https://en.wikipedia.org/wiki/Scheme_(programming_language\)), [Java](https://en.wikipedia.org/wiki/Java_(programming_language\)), [Python](https://en.wikipedia.org/wiki/Python_(programming_language\)), [Scala](https://en.wikipedia.org/wiki/Scala_(programming_language\)), Nimrod, [Eiffel](https://en.wikipedia.org/wiki/Eiffel_(programming_language\)), and more. &gt; --- ^Interesting: [^Self-hosting](https://en.wikipedia.org/wiki/Self-hosting) ^| [^Jikes ^RVM](https://en.wikipedia.org/wiki/Jikes_RVM) ^| [^Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cmg3xv0) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cmg3xv0)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I know, I was joking.
 $ cat test.rs pub fn f(a:int, b:int) -&gt; int { a + b } $ rustc --opt-level 3 --emit asm --crate-type staticlib test.rs -Cllvm-args=--x86-asm-syntax=intel $ cat test.s | c++filt | egrep -v '.*\.[a-z]+.*' f::hd47699df162bfa53daa: add rdi, rsi mov rax, rdi ret .Ltmp0: 
&gt;better way to go is to use Swift both for business logic and UI. We evaluated Swift and it had one glaring hole: It doesn't offer any immutability guarantees on instances of 'class' objects. And class instances are always passed by reference. So you can't rely on the compiler to catch unintended mutations somewhere deep down the function call chain (you can't mark refs as const). That's pretty much a KO criterium for us. So no Swift (at least not in the current language revision).
Yes, I'm seeing now that the standard library tends more towards CSP, than actor model. The wikipedia description is a bit misleading... also rust should probably be removed from [this list](http://en.wikipedia.org/wiki/Actor_model#Later_Actor_programming_languages). I guess the question which remains to be answered is whether Rust is a good fit for concurrency distributed across multiple devices, whether anyone is considering using std lib channels for inter-process communication.
I'm not familiar enough with Rust to be certain - but is this: https://github.com/mitchmindtree/sound_stream/blob/master/src/event.rs#L133 locking a mutex on the audio IO thread? Generally you should avoid anything that may block inside the audio callback thread. See http://www.rossbencina.com/code/real-time-audio-programming-101-time-waits-for-nothing where the author of portaudio explains the reasoning behind why. 
I doubt that anybody would want to write the GUI logic in rust but the low business logic fore sure. And as long Apple does not open up the language, I do not consider Swift as an Option for the latter application.
Rust itself has rules for where the files can go. If you have `mod shared`, then you need to have a `shared.rs` or a `shared/mod.rs` in the same directory, as the erorr says. This isn't configurable. That being said, if you made 'shared' its own crate, and had the two targets both use it, that would work. But within each crate, you can't get around the `shared.rs`/`shared/mod.rs` distinction. Also, as always with conventions, just know that by doing your own thing, you make it a little harder for others to jump into your project and help, since they have to re-learn your original convention.
&gt; Usually you have no real in-game reason to have a MAX_GOLD. If the player earns more gold, well, he'll have more gold. There is no reason to limit that to 1000. No... that's just one particular way to design a game. There are perfectly good reasons to limit the player's wallet size. Games like Zelda depend on it.
There shouldn't be any mutexing in that loop whatsoever, is there a line you're looking at in particular? The only locking occurring is waiting for the buffer to be available for reading/writing which is done using a binding to the native portaudio function. And yep, that article is one of my favs from Ross, was very handy the first time I tried to design my own little audio loop :)
I have no idea; my point was just that the idea mattered more to me than how it should be implemented (types, attributes, compiler flags, ...)
&gt; That being said, if you made 'shared' its own crate, and had the two targets both use it, that would work. But in this case I can not have several "shared crates" in one project because of Cargo restriction - there can be many executables (bin section), but only one library. Or did I miss something? &gt; since they have to re-learn your original convention. Maybe you are right, but there are another reasons. (Un)fortunately there are many different projects on different languages with different conventions. And when people want to migrate on Rust then it will be easier for them to use common techniques. 
&gt; Or did I missed something? You can only _export_ one library, but you can split your project up into as many crates as you'd like.
&gt; Rust's concurrency primitive is called a task. Tasks are lightweight, and do not share memory in an unsafe manner, preferring message passing to communicate. http://doc.rust-lang.org/guide.html#tasks
&gt; 1. How can Rust help in thinking, modeling and designing of such a system? Rust can help prevent _data races_ at compile time. So some kinds of errors will just be straight-up disallowed by the compiler. Others, though, still happen. &gt; 2. How can Rust help in ensuring that the parallelism will work correctly? That means: no deadlocks, running as much things in parallel as can be run (on a given machine / cluster). It can't, really. One could write some libraries _in_ Rust to help with this kind of thing, though... but Rust can absolutely deadlock, and 'as many things in paralell as can be run' is a Hard Problem in the general case. &gt; 3. Are there other some general and specific points of Rust that would be helpful in this situation? It's a little old, but you're gonna want to become familliar with http://doc.rust-lang.org/guide-tasks.html
&gt;However, imagine that you are writing the code that interfaces with an instrument driver: the driver returns an int. Of course, you immediately wrap it in either Degree or Radian to be safe... but which? The type that the driver returns. It can't just randomly return either Degree or Radian, can it? Of course, if you have to interact with a library you didn't write and you can't check what it will give you... then you are pretty much screwed, it seems to me. Unless you trust people who wrote the library to apply the same caution as you do.
All of this right after I finished writing my own IRC bot ([Cleese][cleese]) based on [Rustbot][rustbot]. I quite like what you've done here, but I'm curious about how new commands are registered. For example, Rustbot works by defining plugins which are registered with the system and then receive commands in order of registration. This is roughly similar to the way GitHub's Hubot functions. This way custom functionality can be easily added whenever you want. Is there a nice way to do this in your irc library? [cleese]: https://github.com/andrewbrinker/cleese [rustbot]: https://github.com/treeman/rustbot
&gt; split your project up into as many crates as you'd like. Could you please give an example (maybe a link to some project on github)? Can I do this in "one project" or should I create many projects with different Cargo.toml files? I don't like the second approach because all these libraries are needed only for executable and are meaningless by themselves.
Very often you want unbounded numbers and don't have an upper limit you want to enforce. That's where checked arithmetic is useful. Of course there are exceptions where you want different things, but that misses the point.
That's pretty surprising.
Thank you. Just one more thing - can I have separate dependencies for different targets? As I can see, dependencies are "package level" entity: [[bin]] name = "target_1" path = "target_1/main.rs" [[bin]] name = "target_2" path = "target_2/main.rs" [dependencies.shared] path = "shared" No a problem in my case, just curios.
Maybe one can use symlinks to work around this? I e, make a symlink project/target_1/shared that points to project/shared.
&gt; Maybe one can use symlinks to work around this? I e, make a symlink project/target_1/shared that points to project/shared. I don't like this idea because in this case one will have to do some non-obvious thing in order to build a project.
In theory, that might work, but it semes like an exceptionally fragile thing.
Any time.
Yeah sorry, I was taking a shot across the bows without knowing what I was aiming at (pot-shotting). *Blushy face* I don't know enough Rust to tell that machinery there doesn't involve a mutex or similar. Given you're aware of Mr Bencina's article I'll happily apologise and shuffle off :-) That said, if you do have a moment - what do you find Rust like for audio programming? Have you attempted any SIMD style loops for things like oscillators or FX? Any things that struck you as tricky to do the "rust" way while playing around? (e.g. thread local temporary buffers, that sort of thing, non-blocking ring buffers, atomics...). C++ is getting awfully long in the tooth these days and it'd be nice to have something a little less weighed down by history to move to in the future.
Your usage of 'data-paralellism' made me remember http://smallcultfollowing.com/babysteps/blog/2013/06/11/data-parallelism-in-rust/ , which the OP should be interested in.
Are you going to use that Sound Stream library you posted to be able to detect pitches?
There are several parts to my reply: 1) It's not clear that the assembly generated is optimal. As I already wrote in this thread, the checked closure code contains an unnecessary `add $40,%rsp` and `sub $40,%rsp` to this very short function. If you notice, there's also an odd sequence of instructions on both sides, which add and then immediately subtract 4 from %rsp. (Or maybe vice versa, I don't remember exactly.) It is conceivable to me that the combination of the extra add/sub $40 with the existing add/sub $4 is creating a data dependency on the RSP register, causing greater instruction latencies. 1b) Also, our compiler understood checked arithmetic, and used the implications of checked arithmetic to improve optimization. So for example, if you had a range check at line 10 of your code, and at line 12 you did some addition that the compiler could prove stayed within the range was checked at line 10, then the compiler could drop the overflow check at line 12. Similarly, array bounds check elimination also took into account checked math, and so in some cases a single addition could allow the array bounds check eliminator to remove a bounds check, where previously it could not. 2) Microbenchmarks and macrobenchmarks are quite different beasts. This is a microbenchmark, whose purpose is to expose the cost of one single operation. Yes, it does exactly what I would expect it to do -- it exposes one aspect of its cost. However, because this is a microbenchmark, it has virtually no mapping to real-world applications and the impact that checked math has on them. Let's do something like port libjpeg to Rust, and then measure the cost of checking every operation that could overflow. 3) As I have already said elsewhere in the thread, I believe that checked math should be the *default*, and then you should let the profiling data point out places where the cost is significant / unacceptable. I don't claim that checked arithmetic has *no* cost. What I do claim is that the cost is extremely small in real-world code, and that you should not prematurely optimize code by turning off an important correctness constraint, *until* you have done the perf analysis and have the motivating data. Then, you examine your algorithm, and do things like add well-chosen range checks *outside* of hot loops, and then you turn off checked math *inside* those hot loops. For example, I expect that the UTF-8 handling code in String / str /etc. is quite performance sensitive. It would be entirely reasonable to scrutinize the code and use `unchecked { ... }` where it makes sense. (Or to use `a +% b` or whatever the syntax would be.) But it should be clear and obvious to the reader where this form of safety has been turned off. Look, I'm not saying "black is white! black is white!" and I'm not expecting people to just jump up and down and turn on checked math and never look back. We should absolutely be driven by the performance data. I agree with the microbenchmark -- if you focus on nothing but the cost of a single operation, then *you will see the cost of that single operation*. But this example is artificial -- it doesn't actually do any other work. Modern superscalar processors really shine when you have enough u-ops active to feed your execution units. In our experience, in real-world work loads (like libjpeg and libpng, and many others), the cost of the JO was usually trivial because it was interspersed with other real work. And in those cases where it was not trivial, we did the obvious thing of examining the code, hoisting range checks earlier, and disabling checked math inside the hot loops.
&gt; there's little sense in terminating a cell phone kernel, for example, or a browser which missed a few calculations in CSS. I disagree. If you don't panic, then the bug will never be fixed. Tracking down the root cause of failures like this is incredibly difficult. I would *much* rather my browser panic a tab, and send a crash report on that broken code, than show my broken content. Because I want the bugs fixed.
Although integer overflow itself cannot violate type safety, it absolutely violates correctness. Think of a scheduling algorithm, which chooses the next thread to suspend or resume. You accidentally added two u8 values before promoting to uint or u32, and so you overflowed. Now you're only ever going to deal with the threads in the range 0..0xff, and you'll never schedule the threads that are at higher indices. Perhaps your system deadlocks, or otherwise gets into a screwed-up state. I would honestly rather get a debuggable core dump than risk have a machine in a screwed-up state like this.
Looks nice. I'll have to take a look in more detail, but I like what you've done thus far. Thanks for sharing!
You can use the #[no_mangle] attribute above your function definition and it will preserve the name BUT that means that you must ensure that there aren't any other functions of the same name...shouldn't be a problem unless you are using it everywhere though EDIT: I know it works for IR I am assuming it works for the actual assembly as well but I could be wrong
I'll also add that while I haven't employed this myself, you can also use the facilities available in [irc::data::command](https://github.com/aaronweiss74/irc/blob/master/src/data/command.rs) to do message processing a bit differently from the way it's done in both my examples.
I hope this reply helps: http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmgd3lz Again, I agree that the microbenchmark exposes the cost. That's what microbenchmarks do. The 0-1% figure comes from macrobenchmarks. Specifically, I ported libjpeg and libpng to our environment. *After doing profiling and removing a handful of checks from hot loops*, the overall perf results were in the 0-1% range. Specifically, in JPEG decoding, there are two main hot loops: 1) running the inverse DCT (which converts from an 8x8 block of 16-bit integer coefficients to an 8x8 block of either 8-bit or 12-bit integer sample value), and 2) the Ycc-to-RGB colorspace transform. There is also the Huffman decoder, which has a significant cost, but it is usually dwarfed by the IDCT and colorspace transform. For the DCT, I disabled checked arithmetic for the body of the loops, but not for the loop iteration code (i.e. foo++). I forget the exact numbers, but this gave some small but worthwhile increase in performance. For the colorspace transform, I did the range analysis on the color transformation code, and saw that it stayed in-bounds (as one would expect, for such a well-known well-tested code base). So I disabled the math checks on the color transformation code, but left it enabled for everything else in this function. This also gained a bit of perf. Substantial, but not huge. The end result was that my libjpeg port was in the 0..1% range of the perf of the original C code, and that nearly all of the arithmetic in it was checked, except where it had been locally disabled. Keep in mind that libjpeg is about 27 KLOC; the IDCT routine that I modified (jpeg_idct_islow) was about 300 lines of code, and the Ycc-to-RGB transform (ycc_rgb_convert) was about 50 lines of code. So out of 27 KLOC, checked math was used in 98.7% of the code, and checked math was disabled in the remaining 1.3% of code, while having equivalent perf to the C code. That to me is evidence that you can get the benefits of checked math as well as the efficiency you need, where you need it. You might say "But you're cheating, you're disabling it in the only place where you do any math!" Well, not quite. There are many, many areas in JPEG outside of those two hot loops that do a substantial amount of math. Things like setting up the Huffman tables, computing all sorts of loop control variables, computing the size of buffers and indexing into many buffers, etc. This is precisely the sort of stuff that I want done with checked math -- stuff where if an overflow occurs, I get really deeply screwed up results. One more thing about JPEG. If performance matters, and of course it does, then let the profiler guide you. Don't assume that the overflow check is your greatest cost, because until you've measured, you don't know. I ran ycc_rgb_convert under a profiler many, many times. One thing I noticed was that it uses byte-sized writes to output its sample values. The loop basically reads 3 sample value (Y, Cb, Cr) from different color input planes, does some linear algebra on them to compute the R, G, B values, and then writes the 3 consecutive RGB values to a single output plane. I changed this code to accumulate the results of 4 pixels at a time, and then when 4 had accumulated, I did 3 uint32-sized writes to the output plane. Same number of bytes overall written, but basically using 3 large stores instead of 12 small stores. I forget the exact numbers, but this made a big difference in the perf of this loop -- and it was a much bigger difference than the cost of the overflow checks. I also saw a big drop in the "pipeline stalls due to resources" CPU counter, which to me suggests that byte-writing algorithm was consistently waiting for space in the write queue, while the uint32-writing algorithm was not. So as one of my side projects, I'm currently working on porting libjpeg to Rust. I want to repeat my experiments in Rust, and share those with the community. Then everyone can do their own performance analysis, and we can all tear this stuff to shreds, and we can generate the perf data that will allow us to make good decisions on things like checked / unchecked math. I currently have the IDCT ported, the colorspace transform ported, and I'm working on the Huffman decoding now. When it's all working, I'll share this with /r/rust, etc. and then you and I and everyone else can see what really happens. Sound good? Edit: Honest question: Why is this being downvoted? 
My preferred technique is: * Configure your compiler to emit individual object file sections per definition. This is the default in Rust. With GCC/clang use `-ffunction-sections -fdata-sections`. * Compile the program to a relocatable object file `foo.o` * Disassemble it with relocations printed inline, e.g. `objdump -dr foo.o` Using individual sections means that references between functions (calls, etc.) will show up as named relocations. So that crucial piece of information is preserved. On the other hand, it discards a lot of details that are relevant to the assembler / linker, but not directly relevant to how the code works. Looking at disassembly is also great because it's natural to jump between there and interactively messing with the program in a debugger. You can (partially) demangle Rust names using `c++filt` which is also available [online](http://d.fuqu.jp/c++filtjs/). (If you're using this technique in C, note that turning on `-ffunction-sections` *can* change the instructions emitted. It's uncommon on x86 but common on ARM.) To be honest, I'm not sure how many other people in world would consider this a reasonable workflow It suits me and my background, anyway. By the way, there is [rust.godbolt.org](http://rust.godbolt.org/) but I think it's been broken for a long time :( [play.rust-lang.org](http://play.rust-lang.org/) can emit assembly but it's the same mess you'd get from running rustc yourself.
I believe [this](https://github.com/mattgodbolt/gcc-explorer/blob/master/static/asm.js) is the code gcc.godbolt.org uses to filter assembly output. You could turn that into a command line tool that runs in Node, or use it as the basis for your own filter script.
&gt; Yeah, well, the plane have crashed, but in the black box we have all the data we need to fix the next one. &gt; What, the next one crashed? Well, it's another bug, the first one we fixed, NP. Tell the relatives of the dead to relax. I feel like these are cheap shots and counter productive to a useful discussion. If the survival of hundreds of people in a plane depends on whether or not the programming language used has checked arithmetic or not, there are larger issues at hand! But regarding failure-resistant systems: I actually think having the process panic would be much easier to handle than attempting to carry on in an inconsistent state. If the thing dies, you can probably quickly detect that and restart something or fall into some sort of emergency back-up protocol. On the other hand, if the process tries to carry on in an inconsistent state, it might compound the damage before the self-monitoring systems detect that something is up.
Rust code doesn't seem to do much explicit arithmetic, so I don't think there will be a big *performance* issue with checked arithmetic.
If you want unbounded numbers, you should use bigints. In the trivial case where the bigints are about a word size, the overhead shouldn't be that big anyway. (Anyway, I would like to know if implementing the 31 / 63 bits trick is possible with Rust. That is, implement small bigints as a word-sized int, reserving one of its bits to identify whether it needs more words)
&gt; 1) It's not clear that the assembly generated is optimal. Fortunately we can look at the ASM ourselves and see that it is close to optimal. I would be interested in seeing the ASM you think is optimal, and then we can try to improve rustc/LLVM to generate it. &gt; If you notice, there's also an odd sequence of instructions on both sides, which add and then immediately subtract 4 from %rsp. (Or maybe vice versa, I don't remember exactly.) Nope, I don't see that odd sequence of instructions. [The ASM you gave](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmezylh) contains exactly two mentions of `%rsp`: the add/sub $40. &gt; It is conceivable to me that the combination of the extra add/sub $40 with the existing add/sub $4 is creating a data dependency on the RSP register, causing greater instruction latencies. Are you seriously suggesting that the data dependency from those two instructions matters when the inner loop [has length 10_000_000](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmf5b8f) for each call of the closure, especially since the `%rsp` register is only used once at the very start and once at the very end? &gt; 2) Microbenchmarks and macrobenchmarks are quite different beasts. This is a microbenchmark, whose purpose is to expose the cost of one single operation. Yes, it does exactly what I would expect it to do -- it exposes one aspect of its cost. However, because this is a microbenchmark, it has virtually no mapping to real-world applications and the impact that checked math has on them. Let's do something like port libjpeg to Rust, and then measure the cost of checking every operation that could overflow. I don't see how &gt; A math op like ADD is scheduled, and immediately after it, a JO is scheduled. Most modern x86/x64 cores always assume that a branch is not taken, if there is no information in the branch cache. This matches perfectly our situation -- we assume that overflows will rarely occur. Because the CPU always correctly predicts the results of the JO instruction, it never even modifies / dirties its branch target buffer. The JO instruction adds one data dependency in the u-op pipeline, and (most important) this instruction does not have any data dependencies on it. It has a control-flow dependency, but since the CPU always correctly predicts this dependency, the cost (from a micro-architectural perspective) is extremely close to zero. could mean anything but "the local cost of the checking instructions themselves is close to zero", which the microbenchmarks clearly demonstrate is not true. Furthermore, [you originally stated](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmezylh): &gt; My team did many targeted microbenchmarks, in order to measure the costs of array bounds checking, virtual dispatch vs. static dispatch, checked arithmetic, and many other things. Even in microbenchmarks, we found that the cost of checked arithmetic was in the noise. As in, [0..1]%. but now are saying &gt; This is a microbenchmark, whose purpose is to expose the cost of one single operation. Yes, it does exactly what I would expect it to do -- it exposes one aspect of its cost. However, because this is a microbenchmark, it has virtually no mapping to real-world applications and the impact that checked math has on them. and in [your other reply](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmgdtpn) to my grandparent &gt; Again, I agree that the microbenchmark exposes the cost. That's what microbenchmarks do. The 0-1% figure comes from macrobenchmarks. Anyway, as I stated in the other thread, I 100% know that most applications do not have checked arithmetic as a bottleneck and that profiling allows for reducing the obvious performance drains, I just wanted precise clarification about the context of the 0-1% number, and I'm now getting inconsistent messages, previously micro-benchmarks, now only macro-benchmarks. (To be clear, I don't have an immediate problem with checked arithmetic, I just don't like seeing statements from either side that are easily demonstrated to be false, incomplete or inconsistent.)
Memory unsafety gives you a debuggable core dump only if you're *lucky*. It can give you horrible heap corruption that's not easily debuggable, or, worse, it can just expose your private keys to the whole internet or let your computer be pwn'd (with no way to tell it happened). (I believe someone did an analysis of the fixed security bugs in Firefox, and apparently, iirc, all/most of them were addressed by the memory safety that Rust enforces.)
is the operator overloading up to making custom int types with whatever checks you want. admittedly this is 'opt-in', but imagine if it could be made a library issue rather than a language issue (imagine if i32 etc were just renamed, and a the stdlib provided i32 etc 'implementations' ). admittedly compile times might suffer.
I just went ahead now and wrote the RFC since there was very little feedback on either my experiments, the RFC "announcement issue" or the rust discussion forum. Hoping this might change this a bit now :)
&gt; I feel like these are cheap shots and counter productive to a useful discussion. I feel like you're beating a dead horse, picking on a part of discussion. We've come to a conclusion of sorts, I hope, that overflow handing is better left to the programmer. When the overflow is detected the language should give the programmer of the application the opportunity to do what's best in that particular case, be it a an application crash, starting a panic! unwinding or simply logging the thing for future analysis. &gt; If the survival of hundreds of people in a plane depends on whether or not the programming language used has checked arithmetic or not, there are larger issues at hand! In a real world, it does. There are all kind of both simple and complex bugs crashing planes and even spaceships, IIRC. No need to add to this list. &gt; If the thing dies, you can probably quickly detect that and restart something or fall into some sort of emergency back-up protocol. Rust would need a better exception handling to make that practical. Right now you would hardly even know what caused the panic!, even if you do manage to refactor your code into isolated tasks. We are discussing a concrete proposal to panic! on integer overflows in the current (and v1.0) Rust, not some ideal of what could and should be done in a perfect language. And if you check the discussion, you will see that 0xdeadf001 keeps telling that crashing the entire application every time an overflow happens is a good idea. So his plane *will* crash on overflow, by design, just to make people care about the errors. If you think this is a bad example, well, think of a better one, there are *tons* of them around. I think it is an excellent example, because plane crashing is a well-known phenomena in IT, there's a lot of literature and history around it. &gt; I actually think having the process panic would be much easier to handle than attempting to carry on in an inconsistent state. Sure, no man no problem. You do realize, that infrastructure to clone and restart a process repeating a failed action isn't always out there? And that repeating the action will most likely crash the process again, so not only the end user will suffer, but your system will be DoS-ed by the very failure recovery procedures you've been setting up? You should have realized that, if you've been reading some plane crashing literature. ;-)
101domain lets you do this with the same trick /u/Jhsto mentioned, I've used them before to get around similar restrictions for some other ccTLD. (Although it is $78 for a .rs)
To cut to the chase, I went ahead and implemented checked ints in rustc. First impressions: while slowdown on micro-benchmarks (like the one [here](http://www.reddit.com/r/rust/comments/2nlis8/which_classes_of_errors_remain_with_rust/cmezc4p)) is definitely noticeable, the difference is much smaller in "real-world" scenarios. I've tried a few apps from %rust%/src/test/bench, and in many cases there was no measurable slowdown. Looking at the generated assembly, I was pleasantly surprised to see that in many cases LLVM is still capable if hoisting checked computations out of the loops. Even when it can't, it tends to use pre-computed a flag rather than abandoning the whole optimization. Anyhow, for your benchmarking pleasure, [here's my branch](https://github.com/vadimcn/rust/tree/checked-ints). Usage: `-Zchecked-ints` : enalbe overflow checks. `-Zchecked-ints-with-loc` : enalbe overflow checks and use panic!() with source location (probably too heavy for real use). `#[unchecked_ints]` : An attribute for marking functions that rely on wrapping math. NB: At the moment this attribute does not affect nested functions and closures. 
I really, really love Rust having supported stack traces and support for other failure methods, and am considering using rust-incidents since it's so nice. On the other hand, re-writing the default failure pattern sounds weird. In particular, making developers choose if they need the `box` overhead or not for any error they throw and not supporting tracebacks if they don't doesn't sound very good. I suppose it's not a huge problem since you will only have the overhead when your code fails and in debug builds(?), but `try!` and `fail!` automatically choosing that type seems odd. Also, what is the problem with "`At present the biggest problem with this pattern is that errors have grown to very large structs which mean that results are now much bigger on the Err side than the Ok side for a lot of code.`"? It makes much more sense to me that Err has bigger overhead, since you won't be running into that path as often and the times that you do, there is a chance your application will fail. Does Result&lt;T,E&gt; have constant size regardless of which path it takes?
&gt; Does Result&lt;T,E&gt; have constant size regardless of which path it takes? I believe so. All I can find at the moment though is this /r/reddit thread: http://www.reddit.com/r/rust/comments/1z3k28/sizeof_enum_when_pointed_at/
Hey no problem, it's a good word to spread :) Re rust for audio - I'm absolutely loving it! I think Rust's more functional-esque approach and nice generics make it really easy to write nice, elegant portable stuff. I've also come from C++, and I've ended up spending the last 3 months porting all of my project across as I reeeeallly didn't want to go back. I've only limited experience in C++ with audio, only via playing around with openFrameworks and writing addons for it. I've not written a SIMD loop yet but I've noticed Rust seems to have some basic support and I've been meaning to give it a go! Performance-wise I've noticed no significant difference to C++ - Rust's debug mode is quite useless in comparison but once optimised, it feels at *least* as fast as my (limited) experience in C++. I'd go a step further and claim it seems slightly faster (possibly due to it's awesome awareness of ownership at compile time, as well as my improvement as a developer since using c++) but I don't have any benches to support this. Audio-wise I'm currently working on a generative music engine which has it's own native envelope (freq, amp) synthesis - I can run about 300 oscillators simultaneously which involves interpolating the two envelopes, calculating the waveforms etc in real-time using a buffersize of 128 before it starts to choke on my mbp and there's definitely still room for improvement :) [dsp-chain](https://github.com/RustAudio/dsp-chain) is the crate I've developed for doing modular audio. It's designed to sit on top of sound_stream and it currently has the trait (DspNode) I implement for all my generators and will also be implemented for all my effects/processing, mixing, etc when I get around to it. I guess you can think of it as a tool for constructing a dsp-graph. Oh also, I'm not using any locking in any of my projects - channels (which are basically FIFOs) make it suuper easy to communicate with the audio thread without ever blocking! I've been experimenting using fixed-size arrays with sound_stream and dsp-chain (they're generic over both Vec&lt;Sample&gt; and FSAs) so there's a stack frame always waiting with a zeroed buffer :) I'm yet to bench this against using Vec though with a dynamically changeable buffer-size. Nice to meet another audio head in rust-land :) I've just kicked off a [RustAudio github group](https://github.com/RustAudio) so if you ever have anything you want to contribute you're more than welcome!
What is panicking addition? I'm not finding references to it.
Size is determined statically as a property of the enum itself, so there's no such thing as the ability to change the size based on which variant you've chosen. Note that if this weren't true, it would be impossible to do things like have an array of enums, since it would be impossible to know the offset at which the next element could be found (likewise, it would be impossible to ever store an enum in a struct). As a result, the size of an enum is always going to be at least as large as the single largest variant, even if that variant is never used.
Yes. But you said there is no reason to have MAX_GOLD (which misses OP's point). But there are reasons...
If you're using Cargo, there are several environment variables available to you while the code is compiling. You can see [an example here in the version function](https://github.com/BurntSushi/xsv/blob/master/src/util.rs). I think this is actually how Cargo itself does it (last time I checked anyway). ...and I'm not sure if these environment variables are a documented feature or not (on mobile, too lazy to check).
Thanks for pointing that out!
The RFC doesn't need to consider the case of the enum, because its goal is to reduce the size of the enum by making the `Err` variant no longer enormous. Consider the following program: use std::io::IoError; use std::mem::size_of; fn main() { println!("{}\n{}\n{}\n{}", size_of::&lt;()&gt;(), // 0 bytes size_of::&lt;IoError&gt;(), // 64 bytes size_of::&lt;Result&lt;(), IoError&gt;&gt;(), // 72 bytes size_of::&lt;Result&lt;(), Box&lt;IoError&gt;&gt;&gt;()); // 8 bytes } Note that `Result&lt;T, IoError&gt;` is the exact definition of the `IoResult` type in the stdlib, and is mentioned in the RFC. `IoError` is pretty dang huge, at 64 bytes. When we stick it into a `Result`, even though the `Ok` variant is `()` and thus *literally zero size*, the overall size of the enum balloons up to 72 bytes because of the `Err` case. However, the size of a `Box` is always just the size of `uint`, regardless of what's inside the box. By boxing the `IoError`, we reduce the size of the type drastically.
I have not personally had a need for this (all the servers I use are UTF-8), but I'll add it right away!
A bit late, but I have to say - I am fallible enough to waste time browsing Reddit daily, including many of the default subs (and have been a member for five years), and while I've seen various forms of hate, I literally do not remember ever seeing an overtly Nazi/racist comment. I do not doubt they exist, and, well, I haven't read the comments of any Ferguson related posts on Reddit, but I think it says something about the scale of the problem if I haven't seen a single one. *shrug*
Do restrictions apply for non-Russians and .ru domains?
No. Anyone can buy a ru domain.
As of 0.3.0, different encodings are supported.
The linked example shows how to include them in your crate so you can query it at runtime. I think you're out of luck for crates unless they've been thoughtful enough to include their own version checking call.
You're not missing anything. In that example, changing from `&amp;Shape` to `&amp;mut Shape` doesn't affect the code's validity at all, and the compiler knows that. &amp;mut references can always be coerced into &amp; references.
I'd look at taking readers/writers, instead of dealing with slices of u8s
/u/stepancheg ?
Nothing to do with iterators: you're not closing the channel, so the receiver has no way of knowing it's supposed to *stop*. Note the use of `drop` below. This is only an issue (presumably) because you're not *actually* doing anything asynchronously. Normally, the child task is spawned, the parent task sends some data, then the sending channel falls out of scope and is implicitly closed. Also, the code as given doesn't compile because you're passing two arguments to functions expecting one. use std::comm::{Receiver, channel}; #[deriving(Show)] struct Stmt; fn gen_code_from_ast_sync(ast : Vec&lt;Box&lt;Stmt&gt;&gt;) { for node in ast.iter() { println!("{}",node); } } fn gen_code_from_ast_async(ast : Receiver&lt;Box&lt;Stmt&gt;&gt;) { for node in ast.iter() { println!("{}",node); } } fn gen_code_from_ast_async_generic&lt;T : ASTIter&gt;(ast : T) { ast.stmt_iter(|stmt| println!("{}", stmt)); } trait ASTIter { fn stmt_iter(self, |Box&lt;Stmt&gt;|); } impl ASTIter for Vec&lt;Box&lt;Stmt&gt;&gt; { fn stmt_iter(self, cb : |Box&lt;Stmt&gt;|) { for node in self.into_iter() { cb(node); } } } impl ASTIter for Receiver&lt;Box&lt;Stmt&gt;&gt; { fn stmt_iter(self, cb : |Box&lt;Stmt&gt;|) { for node in self.iter() { cb(node); } } } fn main() { println!("sync..."); gen_code_from_ast_sync(vec![box Stmt, box Stmt]); println!("async..."); let (sendr, recvr) = channel(); sendr.send(box Stmt); sendr.send(box Stmt); sendr.send(box Stmt); drop(sendr); gen_code_from_ast_async(recvr); println!("async generic vec..."); gen_code_from_ast_async_generic(vec![box Stmt, box Stmt]); println!("async generic channel..."); let (sendr2, recvr2) = channel(); sendr2.send(box Stmt); sendr2.send(box Stmt); sendr2.send(box Stmt); drop(sendr2); gen_code_from_ast_async_generic(recvr2); } 
Well first of all you posted the wrong code because it doesn't compile, because of those strings you pass as arguments in main, but are not used in the functions... Secondly because you keep the `sendr` variables around, the receiving end of the channel doesn't know that you might not send more, so the iterator will block. If you simply add `drop(sendr)` after the `sendr.send` statements but before the `gen_code_from_ast_async` call, you'll see that your program works better. Add a `drop(sendr2)` to the obvious place and everything works. Edit: I see /u/Quxxy was faster :( Bonus question: why are you using `Box` everywhere?
That definitely just seems like a badly worded section, your understanding is correct.
derp its always the little things...thanks...and yeah this is a massively simplified version of a project I am writing...in the real one I spawn new threads to do the work
ahhhh in the real program the work gets spun off in a function where the Sender is created and used and then that function returns and drop is called on it...forgot about that logic when I simplified it down
thanks!
No, it has been around since forever. It is very simple: fn drop&lt;T&gt;(_: T) {} That is, it simply takes ownership of its argument and then does nothing with it. Because its argument is passed by value, it will go out of scope when `drop` ends, and destructors are run.
It won't probably be random, however: - you could misread the documentation - especially if the format depends on some "mode" - and that "mode" is global and someone else changes it in a later bug fix without accordingly changing the code you had written I *am* very much a type freak; I love types; however the boundaries of your code, especially when interacting with another language, or I/O, are always a place of risks. Ideally, this risk should be thoroughly eradicated with an extensive test-suite specifically focused on that zone.
Slide 8 of [this presentation](http://www.ac.usc.es/arith19/sites/default/files/SSDAP3_ArithmeticOnTheMillArchitecture.pdf) ^warning:pdf . The result on overflow can be either: - truncated (modulo) - error (which I guess Rust would map to a panic) - saturating - double-width
Yes, /u/glaebhoerl pointed to me that this change of semantics would probably not be a good idea either. I wanted to avoid `panic!` in `unsafe` blocks because there might be holes in the memory and in case of `panic!` those holes might not be filled back before the memory is used elsewhere... which opens up the memory unsafety Rust strives so much to contain. /u/glaebhoerl pointed out that there were plenty of other ways to trigger a `panic!` in `unsafe` code and it should probably be addressed once and for all anyway.
&gt; Huh. I would have thought Result would store a &amp;E though, so it wouldn't have the overhead for the Ok case. Or even better, an Option&lt;&amp;E&gt;... That requires the error to be static or  boxed :)
Just one remark: why only support tracebacks in Debug mode? It is, I guess, a sensitive subject; however in my own experience tracebacks are actually *more useful* in Release mode. I work on server-side software, so that might influence my vision, but the thing is that during development (in Debug) we log a lot whereas in Production (in Release) we just cannot afford such a volume of logs. As a result, we need the traceback in Production much more than we need it in Debug, and that is why currently (in C++) we systematically generate the traceback for all exceptions: - in Debug: it's already slow, so it does not matter - in Release: we have very little other information, so best provide as rich an error as possible if we aim to be able to solve the issue Actually, we even do more than put a traceback in exceptions, we also have specific constructs that attach additional information to the propagating exception during unwinding! The end result is a very hefty object: - a lengthy description, capturing as much dynamic information (name of things, actual value, expected value, error details if coming from the external world, etc...) - the traceback, captured with ABI/Platform specific code, which on Linux means just an array of 50 `void*` in our case, only decoded the first time we need to print it - additional notes, each individually allocated On the other hand: - errors are rare, maybe 0.01% of all transactions or even less - errors need be investigated and fixed, and some are tricky enough as they rely on a precise timing of events occurring on an ever changing environment (try it later, it works...) The point of this rant: **Instead of arbitrarily choosing that tracebacks will be enabled in Debug but not in Release, I would instead propose to let the choice to those who are going to consume the software.** Developers can take the habit to activate support for fat errors only in Debug, or they can activate it in both Debug and Release. Hey! They could even use an A/B thing and create two Release libraries -- one with, one without -- and then direct 5% of traffic to the one with. The only requirement, in this case, is *interoperability*. Libraries compiled with and without support for tracebacks should be able to interoperate, without crashing.
No, because you might have different versions of a dependency. Besides, if the repo is cloned at depth 1, it wouldn't take a lot of space. Not sure if that is what happens now, not on my dev machine atm.
All we need is a Serbian Rust developer and they can start a little business :)
&gt; Just one remark: why only support tracebacks in Debug mode? The main reason is actually cargo. Since each library needs to propagate that information introducing more than a debug and release artifact will just balloon out of control quickly. The second reason is that with the current implementation it's super slow because it freezes in the data. I had an RFC open to allow freezing the PC only and then using DWARF data to look up the source location at a later point but that was closed. &gt; The only requirement, in this case, is interoperability. Libraries compiled with and without support for tracebacks should be able to interoperate, without crashing. That's already the case.
Yea, I really hope `if let` gets un-feature-gated soon
What the others said, but also remember that we're in a weird way in this pre-1.0 world: you won't be using nearly as many git dependencies post-1.0, too.
Luckily it will be fixed whenever someone r+'s my new ownership guide.
It's been around forever, but you rarely see it called explicitly, since it's explicitly called by Rust itself if you don't.
No it isn't. The drop glue is called by rust, the drop function is not. The drop function is merely a way to trigger the drop glue manually. 
Sugar is sweet.
Why is `if let` implemented by the compiler instead of just being a macro?
The `#[link]` attribute is not the recommended way to link to C libraries. See http://doc.crates.io/build-script.html for more details.
&gt; The #[link] attribute is not the recommended way to link to C libraries. Hmm, the [FFI guide](http://doc.rust-lang.org/guide-ffi.html) doesnt mention build scripts. In fact I had read the document you linked, but decided that it described a process too elaborate for my goals. After all, I want nothing beyond linking a shared object in the common library directory of the system. (I. e. ``-lfoo`` on the linker line.) In a case as simple (in my naive eyes) as this, is it really recommended or even required to write a full-fledged program to get the binary linked? 
&gt;&gt;&gt; Let's do something like port libjpeg to Rust, and then measure the cost of checking every operation that could overflow. &gt;&gt; I don't see how &gt; Either we modify rustc to treat all adds as panic-on-overflow, or we modify every + to be add_overflow().unwrap() (or whatever the idiom is) I compiled libpng with 1) clang, 2) clang with -fsanitize=signed-integer-overflow -fsanitize=unsigned-integer-overflow -fsanitize-undefined-trap-on-error and compared it against my platform libpng (some gcc compiled version) with pnmtopng blub.png &gt; /dev/null with each library version LD_PRELOADed. version time number of jo instructions in objdump -D some gcc 0.5s 273 clang 0.5s 1803 clang sanitized 1.0s 296 I think this shows that adhoc benchmarking is not constructive :) clang -fsanitize is not the same as compiling optimized checked arithmetic code. Also I wanted to show this just for curiosity. Can one interpret these numbers constructively?
Related: Is the **?.** (or was it .?) operator going to make it into 1.0? This would make e.g. the very first unwrapping in src/librustc/lint/builtin.rs much more readable, i.e. if e.node?.node?.node == true { ... } Well, if it ast::Expr is an Option enum. And I wonder how `ref` would work there. Furthermore, if working with a more complex Enum, could you maybe specify the type you expect/want to unpack? Say... if let language = animal?&lt;Bird&gt;.climate?&lt;Artic&gt;.country?&lt;Canada&gt;.humanLanguage
I don't see how this is necessarily true. The HotSpot JVM uses hardware traps for null pointer exceptions. There is no reason in principle why we can't do the same thing. We may need LLVM modifications to add landing pads in places where they didn't exist before and to teach the optimizer that checked arithmetic can throw, of course. (Note that this may dovetail with work that LLVM wants to do anyway, for Swift. Swift follows the same model, including making heavy use of unwinding.)
It's annoying when technical limitations get in the way :)
More than a large std, I would like a well organized and compact one. For example in go, most things are were you expect them to be. On Python is kind of a mess. I would also like comprehensive set of traits and data structures so that user libraries are interoperable. I think that a good package manager and build system (such as the one in Rust) reduces the need for a batteries included language.
The FFI guide is fairly old, and I haven't gotten around to updating it yet. While it is a bit more complex in the short term, it's significantly easier for those who come later: just depend on the foo-sys package and you're good to go. Plus, a more robust *-sys package could do even more: like downloading and building a copy of the library if you don't have it, where just -L would lead to an error.
&gt; This could plausibly be done with a macro, but the invoking syntax would be pretty terrible and would largely negate the whole point of having this sugar. https://github.com/rust-lang/rfcs/blob/master/text/0160-if-let.md 
Throwing an error (i.e. trapping) on overflow. Same sort of thing as if your program tries to divide by zero. But built-in instead of you having to check for it explicitly.
Yep, there are cases where saturating arithmetic is preferred. The problem is that there are cases where any particular thing is preferred - there's no clear "best".
What is meaning of "packed" and "unpacked"? 
Note that this is actually an HTTP server serving static files off a directory, not a [Directory Server](http://en.wikipedia.org/wiki/Directory_service) in the usual sense of the term.
AFAIK, the repos are fully cloned since a dependency can be on any commit in the full history (I believe cargo is intelligent about only having one clone and multiple checkouts, but I'm not sure).
Oh, I wasn't aware of actual directory servers. Sorry for any confusion. :/
Maybe you and html5ever are using different rustc versions?
Ive been using ISTanCo, but Im not terribly impressed with them. As noted, no SSL, and they also took three days to respond to a support request related to [lib.rs](http://lib.rs/). Incidentally, what should I put there? Anyone got any ideas? (One of the things Id rather like is moving the crates.io registry there.)
According to the "Packing" section of the Cap'n Proto [spec](http://kentonv.github.io/capnproto/encoding.html), it's a pretty simple compression scheme helps compress away some redundant `0x00` values.
no, it's too big and unsettled a feature to get in at this stage.
Was there any traction on this topic? Any next steps?
While I sympathize with the petition, I don't think Github is a good platform for this. If I put my `+1` there, I'll spam the inboxes of a lot of people.
I addressed this exact issue for audio callbacks in the Rust SDL2 bindings. [See the code I wrote here](https://github.com/AngryLawyer/rust-sdl2/blob/13e09c2e8d0c03d80e6e676ffb71293d869841e1/src/sdl2/audio.rs#L320). Basically, Tasks have the ability to be run within an existing native thread (no spawning). For more information on the Task implementation, [check out the docs](http://doc.rust-lang.org/rustrt/task/struct.Task.html). use rustrt::task::Task; // Create a new task. let task = box Task::new(None, None); task.name = Some("My task".into_cow()); // Running a task is very cheap, so don't be afraid to do this often. let new_task: Box&lt;Task&gt; = task.run(|| { callback(); }); if new_task.is_destroyed() { // A panic occured. } // Every task must be destroyed before it is dropped new_task.destroy(); For efficiency purposes, it'd be wise to reuse the same task for multiple callback invocations (if you're doing that). You can do this by sending the Task over via a callback data parameter. Because `Task::run()` consumes the Task object and you can't move data from a reference, it can get tricky. I'm too lazy to write some code for that right now, so check out my code in the first link if you want an example. Best of luck!
It's mostly superficial resemblance. If there is one thing Lisp directly had inspired Rust, it's a macro hygiene. Everything else (even the concept of macros itself) is not unique to Rust nor Lisp.
Yup, different people learn differently. I've always enjoyed Rust By Example.
Could you elaborate? I'm deliberately using ~no external dependencies until 1.0 because I don't want to wait for authors to update them when breaking language changes come out. For example, I transitioned my code to use stdout of imageMagick because both of my external .png reader dependencies broke. 
This is an exceptionally clear explanation of borrowing/lifetime semantics. I feel like some version of it should be in the official guides. Calling on [/u/steveklabnik1](http://www.reddit.com/user/steveklabnik1)!
lisps are usually garbage-collected,dynamically typed (??); whilst Rust is deterministic memory management &amp; statically typed; those are key differences IMO. Lisp inspired many features in many languages, so you can probably find some similarities anywhere. (I have heard of an experiment to do unique ownership in a lisp.. "linear lisp" ? ..and I'm no expert on the range of lisp implementations.. I gather some can do static typing?)
Having lib.rs be the new registry url is extremely sexy, you should talk to the Cargo people
I don't think so, I ensured that there is only one version in my system. I can check with strace what cargo is running.
I've always thought Github needs some primitive voting system. Upvoting/downvoting etc.
There seems to be a lot of support for this - so why does it need to happen? If people are interested in maintaining it, it shouldn't matter if it's under the rust-lang umbrella or not. Of course if there's no interest in maintaining it then being under rust-lang forces it to be maintained, but adds additional work for the core contributors.
The problem is the repo needs to move to a new owner because it appears Japaric has fallen off the radar or simply doesn't have time to merge pull requests anymore. So it's mostly petitioning him to transfer the ownership to rust-lang, since it looks like the core team is already okay with taking responsibility for it.
Yup! Going over the reference is on my 'just before 1.0' checklist.
After 1.0, things will never break, and it'll pretty much be 'just use the crates.io package' instead of directly depending on a git repo.
It was there, but it wasn't really useful, so they axed it.
`Task::run()` simply returns `self`, so no re-boxing or additional allocations happen (sorry if the name `new_task` was misleading). To my knowledge, the task is only mutated, i.e. in case of panics.
Thanks for writing this series. It's great.
Just fork the repo (the [license](https://github.com/japaric/rust-by-example/blob/master/LICENSE-MIT) lets anyone to pick up the work) Or is this not about the repo but about the ownership of rustbyexample.com?
title is a little bit misleading, could this be changed?
If you're wondering what it actually is, what's happening is: mov rsp, rbp sub rsp, 48 ; This is to make the point that there are 48 bytes ; allocated on the stack (5 longs + 1 pointer) mov quad [rbp-32], 0 ; initialize arr[0] to 0 lea rax, [rbp-32] ; set rax to the address of arr[0] mov [rbp-40], rax ; set ptr to the value of rax So that you have, on the stack, five integers (where arr[0] = [rbp-32], and arr[5] = [rbp] ), and a pointer (at `[rbp-40]`) that points to `arr[0]` on the stack. Edit: The C code that would compile to this is: long arr[5]; arr[0] = 0; long *ptr = arr;
I agree the title is misleading, and that wasn't my intention. I already changed the title in the article, and added a note at the bottom, but the Reddit title cannot be changed by me. The mods might be able to, but it's not *that* important.
What's the likely portions that might cause security issues? (Eg: could [this](https://github.com/hoeppnertill/sersve/blob/master/src/sersve.rs#L122) be a security hazard? Or, is it possible to escape its directory misusing ".."?) Also, does [this](https://github.com/hoeppnertill/sersve/blob/master/src/sersve.rs#L105) mean it is allocating a new ".." every time it's called? If you're familiar with it, I would be interested in knowing how it compares to the HTTP portion of [publicfile](http://cr.yp.to/publicfile.html) (it's supposed to be a secure static server; it's written in C, but I suppose Rust is more suited to this task)
I think it's a little bit of both. If rust-lang took over Rust By Example then it might end up under the rust-lang.org domain. E.g. rust-lang.org/by-example isn't much harder to remember than rustbyexample.com and it doesn't require maintaining a separate domain name registration. It seems like the Rust team is waiting for Japaric's permission before proceeding, even if the license will let them do it without his blessing or even acknowledgement. His name is still on the copyright notice, they might want that changed since he's not officially affiliated with Mozilla and could potentially cause a legal issue down the road (all other Rust official publications, e.g. the Guide and Reference, as well as all their repos, are copyrighted to the Rust team). They probably just want to officially transfer the ownership and copyright of the repo so there's no possibility of legal repercussions, since they do represent a large organization, and they can't do that without some effort on Japaric's part. 
I don't see why he should surrender his copyright, does Rust really require a CLA / transferring copyright to Mozilla? Even for documentation? He already released his stuff under an FSF, OSI-approved license.
I love this little bit of insight: &gt; Congratulations on that last one by the way, you've just re-invented the boolean. But it's a super-boolean because it works with all the code built for working with Results!
I'm not part of the Rust team so I can't even begin say what they're thinking. From what they've added to the discussion on GitHub, it looks like they're interested, but waiting for some sort of response from Japaric before they take any action.
&gt; What's the likely portions that might cause security issues? (Eg: could this[1] be a security hazard? Or, is it possible to escape its directory misusing ".."?) No, I made sure that can't ^shouldn't happen. If you visit [trinaryco.de/../../](http://trinaryco.de/../../), you will see it won't serve any directory above the given root directory. As for the `from_utf8_unchecked`... that 'optimization' is not worth it. (Shame on me) But, security-wise, it shouldn't change anything, as the filenames would need to be invalid UTF-8. &gt; Also, does this[2] mean it is allocating a new ".." every time it's called? Yes, that's what this means. But that isn't a terrible lot of memory, and it's freed pretty soon afterwards. The performance impact is negligible, compared to other allocations from rendering from [rust-mustache](https://github.com/erickt/rust-mustache) to a Vec. That part can surely be optimized by preallocating more memory, but I haven't yet benchmarked that part. &gt; If you're familiar with it, I would be interested in knowing how it compares to the HTTP portion of publicfile[3] (it's supposed to be a secure static server; it's written in C, but I suppose Rust is more suited to this task) I've not heard of publicfile before, but sersve likely is a lot less sophisticated. For one, it doesn't (yet) allow for https, doesn't provide ftp, and doesn't allow to resume downloads. Since I don't have a whole lot of experience with testing stuff for security, I've only guarded against a few things. That's why I asked for additional attack vectors. But I suppose &gt; publicfile avoids bug-prone libraries such as stdio. also goes for sersve, to some extent. Even though the standard library is far less battle-tested, the compiler helps proving it's sanity.
I too have started a Rust-ObjC project. I have wrapped the ObjC runtime API, added bindings for bits of CoreFoundation and Cocoa, and even managed to get a basic Cocoa .app running. It was the simplest GUI possible, but it's an .app that runs, and only has a native Rust binary. I replaced ObjC code bit by bit until it was Rust only, including a delegate. As you mention, there are *many* challenges, not the least of which is memory management and method invocation. My solution is mostly jury rigged for now, and haven't had time to work on it lately. This article comes at it from a different angle, and addresses a number of issues I kinda sidestepped. It would be good to get all the bits and pieces people have worked on all together...
Honestly, you might just be better off with Qt if you really want something more strict than ObjC-Cocoa. Having started work on my own Rust-ObjC/Cocoa bridge, I can tell you it's a) not at all easy, and b) somewhat of an impedance mismatch, and c) misses some of the aspects of ObjC that make Cocoa so good. The Rust "UI story" is still very unclear...
He was probably talking about the supported rust version for html5ever, not your local version used to build html5ever.
HKT has very little to do with it - as far as I'm aware, F# doesn't have it either! (Which was the language used throughout the presentation.) (To be more precise, HKT is needed to *abstract over* some of these patterns for any generic type, but nothing stops us from writing concrete implementations for each particular type, [and we do](http://doc.rust-lang.org/core/option/enum.Option.html#method.and_then). And for e.g. monoids, they're not even relevant.) Off the top of my head, the only thing talked about here which Rust lacks is implicit partial application of functions. But you can do it explicitly with a lambda, and you could surely write a utility function (or perhaps macro) for it as well, like ye olde `boost::bind`, if you wanted to. The critical thing you need for most of the material is just a first-class function type, and Rust has that. In fact it has *several*, due to explicit control over ownership, mutability, and memory management, but apart from forcing you to think harder in some places, this shouldn't be an obstacle. I would be *very* surprised if there were anything in here which couldn't be carried over to Rust. In fact I would consider that a major deficiency in Rust which should be fixed. To put it more directly, I think this talk is about *good programming*, in any language. And not "good programming in F# and Haskell".
Having MyClone return Box&lt;MyClone&gt; specifically might work (basically equivalent to Java again), but I dunno about Box&lt;Self&gt;. Anyway, trait objects are super coarse right now. Is the trait in-and-of-itself object-safe? No? Can't use it as a trait object. I haven't followed what the compiler folks' plans are in this space. Trait objects aren't *super* on my radar, to be honest. I'm more interested in generics than virtual dispatch. Edit: oh and the `&amp;self`, `&amp;mut self`, and `self` abstracting is tricky because it's a hard problem. :P The key part is really the Traverse trait, which takes the kind of ownership you're interested in and translates it to the "next thing" you want. In this case "some kind of RingBuf" -&gt; "The equivalent iterator". Also, I actually believe Traverse (in this design) is going to become redundant with aturon's IntoIterator trait in the pipeline.
Thanks to Thomas Young for pointing out that it's super easy to statically write an RSS feed: http://cglab.ca/~abeinges/blah/rss.xml Add that to my incredibly elaborate manual post generation system.
"I often hear that Lisp is the most powerful language." I'm going to argue that this statement means nothing. Lisp is a syntax involving a homoiconic tree structure usually with parenthesis ("(" &amp; ")") that enables powerful macro systems. There are many variants of Lisp, and they are all different, and thus have their own different levels of power. Some examples of lisps are Scheme, Common Lisp, Clojure, and Racket. These languages are as similar to each other as Ruby, Python, JavaScript, and Lua are to each other. On another subject, these days it is rare to find a language that doesn't provide automatic memory management. A slightly more interesting question is how does the language provide the feature. Most languages (or language implementations - it gets fuzzy here) provide it via garbage collection. Rust is going down its own route, blazing open the trail to a lifetime annotated system.
Perhaps because implicit dereferencing would prevent people from doing pointer arithmetic in unsafe blocks?
So, some changes. [It validates the strings for UTF8-ness now.](https://github.com/hoeppnertill/sersve/blob/b394f31e8293f262a5eeb00cc5997d69178f4406/src/sersve.rs#L151-153) "..".into_string() is destined to stay, as rust-mustache requires StrAllocating. And it can fork itself and passes Iron how many threads it's supposed to use, as well as allocating a little less for Strings.
Rust wants generators (with `yield`), but they're not implemented yet. Proposals I've seen involve entire functions being generators, but maybe it could work at the expression level too. Loops are expressions for consistency, but their value is always `()` regardless of what's inside, so using that cake is not that useful.
Thanks for this! I was looking for a feed in the last post and couldn't find it anywhere. Both posts are excellent btw :-).
That makes sense, thank you. I hope they do end up bringing in generators one way or another. They are extremely useful, especially considering how powerful things like consumers and adapters are.
It's only advised to pass it by value if the function needs to have ownership (or a copy to modify). Path is only read, not stored or modified, so cloning is unnecessary and would need an extra allocation. &gt; *If* a function *requires ownership* of a value of unknown type T, but does not otherwise need to make copies, the function should take ownership of the argument (pass by value T) rather than using .clone(). That way, the caller can decide whether to relinquish ownership or to clone.
&gt;cloning is unnecessary and would need an extra allocation The path is cloned within the File::open_mode(...) function, which is called by all the "constructor" functions. See [here](http://doc.rust-lang.org/src/std/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libstd/io/fs.rs.html#136-141).
We do however have iterators, and [you can do a bunch of things with them](http://doc.rust-lang.org/std/iter/trait.IteratorExt.html). Until we have generators, you can do the equivalent with a struct that implements [the `Iterator` trait](http://doc.rust-lang.org/std/iter/trait.Iterator.html). What would be the generators local variable and state (which of the `yield` statements youre at, if more than one) have to be encoded as struct fields. Its much more verbose, but equivalent in power.
Thanks for pointing that out. It appears to violate the guideline, but -- unfortunately it needs a ref to be able to report the error anyway.
&gt; Until we have generators, you can do the equivalent with a struct that implements the Iterator trait. Already implemented as [`std::iter::Unfold`][1]. [1]: http://doc.rust-lang.org/std/iter/struct.Unfold.html
It would be kind of awesome if loop{} could evaluate to a non-() expression, by using `break &lt;value&gt;;`. But I realize that this is ambiguous with the syntax for breaking out of a specific loop. Also, realize that for() loops are quite different from loop{} loops. for() loops work on iterators, and it is not obvious to the compiler that the body of a for loop runs even a single time, while loop{}s work on an explicit break statement.
It actually seems more clear to me. I like to know when I'm dereferencing a pointer vs. using an actual value. If you're dealing with references to small primitives (like integers), I'd suggest that you explicitly move data between local variables and the borrowed locations. I do admit, though, that the a.b --&gt; (*a).b thing is super-convenient, so I don't claim to be 100% consistent.
Couldn't the error reporting take a reference to the path that has just been moved into the function?
This could possibly be implemented as a lint, but since this is a fixed-length array it's almost trivial to get the length since it's in the type information: let v: [int, ..4] = [0i, 1, 2, 3]; However, the payoff would be low since fixed-length arrays aren't used a whole lot in Rust code, except when you need something like a [stack-allocated buffer.][1] And since the programmer knows the length of the array because they had to write it as part of the array's type, the programmer should know already if a given index is out of bounds. Vectors and slices are a little more complicated since the length is determined at runtime. Any lint that checks vector or slice access would need to follow the flow of data and actually *count* the number of inserts and removes to determine the length of the vector at the point of indexing. That's a little too complex for a logical check that the programmer can easily verify themselves. If you want an index op that won't panic, use `SlicePrelude&lt;T&gt;::get()` which returns `Option&lt;T&gt;`. However, most places where one would use indexing in Rust could instead be refactored to use iterators, which do internal bounds-checking and are designed to be very efficient. [1]: https://github.com/hyperium/hyper/blob/master/src/http.rs#L344
You're posting this in the wrong subreddit. Spam @ /r/playrust instead of /r/rust. This is the rust programming language subreddit. 
It could
The fact that `for` loops act on iterators should make it easier to have them return one, since that's what a lot of iterators already do (like in the example in my OP). End of the day it should not matter if the loops are technically different, since both do 0 or more iterations, and those iterations could yield a value (if that is how the language worked). Note that I'm not asking for the loop to evaluate to a single value, but to an iterator. I think the more logical syntax would be `yield &lt;value&gt;;` or even just `value` (no semicolon) not `break &lt;value&gt;;`, but that is another discussion.
I'd say the vast majority of methods take `self` by reference, which makes the `self` parameter something of a special case. I like that there's no auto-borrowing of parameters or implicit conversion between numeric types. That stuff gets mega confusing in C++.
Not enough people know about unfold. Thanks for spreading the word, brother.
You can't do pointer arithmetic with `+` or `-`. And raw ptrs don't *ever* auto-deref. Doesn't seem like a problem.
Yes, you're right, I overstated my case. I should have said "Rust favors explicitness over convenience".
And the RFC for the lazy: https://github.com/rust-lang/rfcs/pull/379
Yeah, it's a tough call, and I don't know if I agree with the style guideline in all cases. It's certainly a clunkier api to have to clone `Path`s before giving them to `File` methods in the usual case; however, you also want to allow the optimal path when possible. \_()_/
&gt; And since the programmer knows the length of the array because they had to write it as part of the array's type, the programmer should know already if a given index is out of bounds. One programmer may write the array, another may index into the array and a third may modify the length of the array 2 years later. Easy for these things to be missed with incremental changes in distributed development.
Well we have the power to create custom iterators, but there is no special syntax for it. Since all loops are already expressions, I expected them to be this syntax. It makes sense for loops to return an iterator, since they iterate by definition. If you want an example of how this would be useful, you could look up the `yield return` operator in C#, which does exactly this (turn a loop into an iterator).
Most errors caught by the type checker are 'trivial' in the sense that a quick inspection at that site would expose the error. The type checker rejects it anyway. I don't know how much more complex the type system would need to be to reject out of bounds access of static arrays though.
&gt; It's certainly a clunkier api to have to clone Paths before giving them to File methods I suppose it comes down to preference, but I don't mind having to explicitly clone any non-primitives which I want to re-use after passing to such a function. At worst, the compiler should give you a very straightforward "use after move" error. :)
`yield return` does far more than that. It converts the entire method into a state machine, and defines a type which holds the state. Also, it usually requires that the state-containing type be a ref type, which obligates you to allocate in the GC heap, just for the purpose of enumeration. I'd be fine with making `for` sugar for iterators. It makes a sort of sense. But it's not trivial to get it right (and I would not accept the costs that `yield return` implies), and we already have things in the language which do what we want.
If you're brand-new to Rust, then it may be a bit daunting to jump right in and try to complete 24 tasks in as many days. But there are lots of different ways to contribute, depending on your skill level, and for the maximum chance of success I'd suggest finding someone specifically with some work in mind and set about doing it with their guidance. For example, here are two metabugs with various tasks related to improving the collections in the standard library: https://github.com/rust-lang/rust/issues/18424 https://github.com/rust-lang/rust/issues/18009 Contact Gankro on IRC (see sidebar) to coordinate completing the tasks above. You should also consider contributing to Rust projects aside from the compiler: Servo and Cargo spring to mind. And even if you're not comfortable writing Rust code yet, you can do things like give feedback on the official documentation (directed at steveklabnik) or attempt to do some triage in the issue tracker. Every little bit helps!
The fact that every single F# approach in there, *other* than partial function application, has an elegant Rust equivalent is extremely promising, in terms of appealing to the FP audience. While a macro or a closure can be used, it would be worth taking another look at https://github.com/rust-lang/rust/issues/5893
Isn't there always a clone in case 1 as well? The Path parameter is moved to the File object, so a clone has to be created to be used by the update_err(...) call. So passing by moved value will result in two memcpy and a clone, where as passing by reference has only one clone. 
I think it does. I may be wrong...
`for ..{ ..break &lt;value&gt; } else { ... &lt;value&gt;}` would be nice way IMO to take the 'expression oriented syntax' further, although I gather it might be too much specific 'sugar' from some POVs. (maybe else could be worded better too, but it is analogous to an if..else construct)
I'd definitely like to contribute to projects aside from the compiler (I hope that came across). Some projects that stuck out as I did some looking around are: - rust-url (https://github.com/servo/rust-url) - cargo - rust-peg (https://github.com/kevinmehall/rust-peg) As you said, this might be a bit daunting, so I can see myself starting with documentation fixes and writing tests. 
There are a couple of syntax errors in what you wrote above: * the semicolon after Baz in `struct Baz { bar: &amp;'static Baz; }` should be a comma * the field `foos` in `static BAZ : Baz = Baz { foos: &amp;BAR };` should be `bar` * the `'static` bound on `&amp;'static Foo` is illegal; `'static` in that position is only used with traits There is also a semantic error: the type of the field `bar` in `Baz` should presumably be `&amp;'static Bar`, not `&amp;'static Baz` After fixing those things, it compiles for me. [playpen] [pp] [pp]: http://play.rust-lang.org/?code=%23%5Bderiving(Show)%5D%0Astruct%20Foo%3B%0A%0A%23%5Bderiving(Show)%5D%0Astruct%20Bar%20%7B%0A%20%20%20%20foos%3A%20%26%27static%20%5B%26%27static%20Foo%5D%0A%7D%0A%0A%23%5Bderiving(Show)%5D%0Astruct%20Baz%20%7B%0A%20%20%20%20bar%3A%20%26%27static%20Bar%0A%7D%0A%0Astatic%20FOO%20%3A%20Foo%20%3D%20Foo%3B%0Astatic%20BAR%20%3A%20Bar%20%3D%20Bar%20%7B%20foos%3A%20%26%5B%26FOO%5D%20%7D%3B%0Astatic%20BAZ%20%3A%20Baz%20%3D%20Baz%20%7B%20bar%3A%20%26BAR%20%7D%3B%0A%20%0Afn%20main()%20%7B%0A%20%20%20%20println!(%22%7B%7D%22%2C%20BAZ)%3B%0A%7D
Learning C will give you a keen appreciation for the class of problems Rust is designed to solve. Right now Rust is hard to learn for two reasons: low level programming is inherently more challenging, and Rust itself is a moving target, with spare documentation and few tutorials etc. From my experience with the language so far, Rust will be easier to learn than C once the documentation is in place. If you're not in a hurry, you might want to wait. 
I'd definitely learn C first if I was you. I done Electronic and Computer engineering in college which involved 2 years of C and alot of that was embedded programming so i was right in the meat of low level programming and I've basically given up on Rust for the minute. The 2 biggest reasons i gave up is because I'm learing Java now (i'll be doing it next year when i go back to college, giving myself a head start basically) and because the documentaion for Rust is still very much lacking compared to other languages. Like I'm already better at Java after 2 weeks than I am at Rust after (trying) to learn it for ~2 months, simply because of the mountains of online resources Java has compared to Rust. EDIT: I should say that I was learning Rust a good while ago before the updated guided. It's better now but the frustration learning it still outweights the fun of learning it :-).
Here you go, an iterator that yields Fibonacci numbers: http://is.gd/9k3xKw Not exactly Haskell's `fibs = 0 : 1 : zipWith (+) fibs (tail fibs)` but still pretty concise for systems programming, IMO. Edit: Here's a version that stops on overflow and doesn't skip the 0th and 1st Fibonacci values: http://is.gd/bIrvVN 
You can implement it with a compiler plugin if you understand the libsyntax API. 
Just rediscovered this issue and was interested in getting wider feedback. Basically it would allow for things like: let mut v = [1, 2, 3]; match v { [ref mut x, ref mut y, ref mut z] =&gt; { *x += 1; *y += 1; *z += 1; } } I think this allows strictly more programs to be written, so I don't actually think it requires an RFC, right? Just someone to actually implement it?
C is a very simple language, learning it takes 2 weeks for a novice. Mastering all the techniques idioms and pitfalls takes some years but that's irrelevant now. Understanding the basis of c, pointer arithmetic and linking will be very useful to know. Also c is the lingua franca of systems programming.
It's a bit hard to recommend anything, as the language is still changing enough that crates that were well-written last month may have rotted by now. That said, I'll nominate [html5ever](https://github.com/servo/html5ever), though be aware that it's pretty macro-heavy (which may or may not excite you).
I believe the Cargo.toml sections at the end are redundant, since they're the default values.
If I were you I would read the code for the compiler and the std lib...its pretty illuminating
I am interested what are you using this for?
C is full of pitfalls that have nothing to do with the theory behind low level programming. It's an extremely antiquated design and a frustrating tool under the best of circumstances. I think learning pointers etc. in Rust would be rather pleasant, because when you screw up, you'll get a compiler error instead of nondeterministic crashes or worse. The cost is that Rust is a more complicated language, and will throw more concepts at you. On the other hand, the deepest concepts (e.g. borrowing and ownership) are vital in C as well. Rust will require you to learn them properly, instead of muddling through and writing crashy / insecure software.
Well then I would revise my statement to the core types you would use everyday such as `std::vec::Vec`...they are used enough that they get updated daily...ps i see u created the phf crate!...thanks alot I am using it as a base for vtables in my toy language that I am also writing in rust!
I mainly created it for use in [rust-lazy](https://github.com/reem/rust-lazy), so you don't pay for locking and unlocking in `sync::Thunk`.
If loops evaluate to an iterator, how do you distinguish between loops you want to run eagerly ("now") and loops that just create a lazy iterator value? See, if you just do v.iter().map(|..| ..), the iterator is lazy, and nothing is iterated without something pulling the values out one by one. Typically, the for loop is the one to pull values out.
I think for the most part this is true, but there are certain pieces that are really nice, like a few of the smart pointer types like `Rc` or `Arc`, great examples of RAII usage like `Mutex`, and a few awesome collections like `HashMap`. I would stay away from the compiler itself, since it's some of the oldest rust code, but a lot of the new stuff in the std lib is pretty nice.
[also this discourse thread](http://discuss.rust-lang.org/t/compact-closure-args/432/20)
Cool, I didn't know that. Updated the post, thanks!
&gt; I think this allows strictly more programs to be written, so I don't actually think it requires an RFC, right? That isn't the burden for an RFC. The burden is a 'substantial' change. Seee more here: https://github.com/rust-lang/rfcs#when-you-need-to-follow-this-process
A `for` loop isn't lazy, so even if it were an expression, returning an iterator wouldn't make any sense. Evaluating the expression results in evalutating the loop. If anything, the last expression of the last iteration of the loop would be the only consistent value, imho.
Fair, though I'm still unsure if this would be regarded as substantial. What is your opinion?
Also, if the compiler can't decide that v[4] is wrong, then it also can't decide that v[3] is right, which in turn will cause a lot of bounds checking that could have been optimised away. Correct?
&lt;G: Gendered&gt; || format!("D'awww... {}'s adorable!", &lt;G as Gendered&gt;::third_person_pronoun())
If you feel the urge to learn Rust, learn Rust. I don't think you need to learn C before. C will always pop up as a comparision, but you can learn all that stuff on the go. It's not like these things are magical, there's just a lot of details. There is no point in learning them upfront. Rust has enough to stand on its own.
This is done by LLVM AFAIK, so basically inaccessible to the Rust compiler.
Im planning on writing up a general RFC that adjusts array/slice patterns a littlethere are a few other things Id like cleaned up (particularly after DST). If I end up submitting it, Ill probably include this as one of the changes, just in case it does require an RFC, if thats fine by you.
I don't get it. Why is that better than a normal mutex?
very kewl stuff kudos on your type wizardry
With little to none experience of C, I can say that learning Rust was a very good (yet hard) introduction to low-level programming.
Can we lose the "Hi ..." and just leave the logo and website on the back?
I have a sketch of a maximally efficient `Lazy` type which you might be interested in [here](https://github.com/glaebhoerl/rust-notes/blob/master/rust.txt#L596). But this is only for single-threaded use through `&amp;mut` access; how to extend it to shared, and potentially `Sync` access is something I haven't thought through all the way yet. This relies on: * The `UnsafeUnion` type described [here](https://github.com/rust-lang/rfcs/issues/371) (which presumably needs some compiler support). * A more general form of existential quantification than we currently have, where you could take a `Box&lt;Lazy&lt;F, T&gt;&gt;` (or `&amp;mut`, or whatever) and simply "forget" the `F` and pass it as just `Box&lt;Lazy&lt;T&gt;&gt;`, as a thin pointer without any kind of additional evidence (vtable, length, etc.) being stored. The idea is that the definition of `FnOnce` for `F` is only required when the `Lazy` value is constructed (the type "manages its own vtable" in the `eval` field); and if it's behind a pointer, it can be represented without knowing the identity of `F`.
An unboxed closure is a struct that implements one of the `Fn*` traits. You need to use generics and bounds `F: FnOnce()` to store them in structs (or use them in functions): #![feature(unboxed_closures)] struct Struct&lt;F&gt; { f: F, } // `F` is the type of the unboxed closure // Is an unboxed closure because `F` implements the `FnOnce` trait impl&lt;F&gt; Struct&lt;F&gt; where F: FnOnce() { fn new(f: F) -&gt; Struct&lt;F&gt; { Struct { f: f, } } fn call(self) { self.f.call_once(()); } } fn main() { Struct::new(|| println!("Hello")).call(); }
Please see this discussion from two days ago: http://www.reddit.com/r/rust/comments/2nrhrg/several_targets_and_one_shared_module/cmg5xoa
It's pretty damn good! I would love PRs to make these parts of the docs more clear.
Is it possible to use this for a closure that *won't* be called just once? With `Box&lt;F&gt;` or something?
Absolutely.
Rust's macros, while still cumbersome to work with, are probably quite sufficient to acomplish your goals. Rust as a host language for DSLs is something I'm quite interested in as well.
Maybe a dumb question, but is there a reference to explain generic constraints in rust? The guide doesn't seem to cover it. Sometimes I see them inside the actual &lt;&gt; block(i.e, &lt;F : T&gt;, sometimes after where like your example etc.
I can already hear the *"Got (the) crabs?"* jokes.
That would eliminate the ability for LLVM to do most code motion.
Both behave the same, but where clauses are a recent addition that additionally allow more general constrains.
I believe [this is the relevant RFC](https://github.com/rust-lang/rfcs/blob/091e5fabbbbd0419b8d291a6d1cd8e6e59c56deb/text/0135-where.md), look into the 'Motivation' section. edit: this is for `where` clauses specifically, not constraints in general
http://doc.rust-lang.org/guide.html#traits explains using traits as generic constraints. It doesn't explain the 'where' syntax yet, though.
The only thing preventing moving on this is time and effort - everybody has a a great deal on their plate right now.
It's actually pretty easy to have multiple library projects in a git repository, each with their own `Cargo.toml`. Just use `path="..."` to point to the other project folders. This way, each library can have its own version, dependencies, etc. For an example, see [rust-uchardet](https://github.com/emk/rust-uchardet), which is actually three related libraries.
I think that the libraries aren't there yet to make it really easy. I'm currently writing a small toy compiler and had to write the lexer by hand. By the performance I guess that regexes starting with `\A` still run over the whole string. 
Here is another one. I don't know if it is official. http://www.redbubble.com/people/eugeneoz/works/12595912-rust-programming-language-logo?p=t-shirt
I bought a rustlang sticker from redbubble, and was pretty happy with the result. Not sure about the quality of the shirts they print though.
While ability to shoot yourself in the foot is prevalent in C, I personally still find it an elagent simple language, even though yeah it can be annoying and is old; but it is still beautiful. (unlike java{which was the first language I really knew}) Of course I haven't done too much with c, so the whole int ************i deal hasn't happened to me to badly yet....
This was the way that I'd come up with that worked, but it's a bit messy because the libraries need to have relative paths between them. This means that if you want a nested structure for whatever reason - my large-scale Java project at work has 4 levels of nesting in some places! - then you have a lot of path = "../../../systemA/subsystemB/libraryC" all over the place. It seems a real shame when the libraries all have unique names anyway...
I don't believe that's true: let mut sum = 0; for x in v.iter() { sum = sum + *x + (loop invariant expression) } The optimizer should still be able to hoist out the loop invariant expression because there are no externally visible side effects even if it throws.
Rust's syntax is "more conventional" than lisp, lisp looks like cat(((((1))))))))(((((((4),3)))))(((((((2)))))))))))))))(((((((())))))))))))))))(()))), ok exagerating but seriously that syntax
Looking here, http://rosettacode.org/wiki/Sorting_algorithms/Quicksort#Racket Racket doesn't seem so bad, I probably am just having bad memories of it from the one intro to cs lab where I had to use it.
I'm not sure who coined Rustacean or decided we should be crabs, but I really not a fan of it :P. 
OK I added 8 charts for every code sample. That should help the readers.
Perhaps consider having intermediate crates which re-export all of `systemA` or `subsystemB`? There's often no good reason for `systemD/subsystemE/libraryF` to be digging around in `systemA/subsystemB/libraryC`. Alternative possibilities include: 1. A flat `crates` directory with many uniquely named libraries as children. 2. Seeing whether you can set up your own internal crates.io server. I've never heard of anybody doing this, so I'd imagine that it would involve submitting patches upstream.
There are multiple parser generator libraries. I use [rust-peg](https://github.com/kevinmehall/rust-peg). It allows easy definition of grammars and automatically generates a parser. My workflow is to put the grammar in a seperate file and automatically include it at compile time with the corresponding macro. In the examples folder of the repository you can see how to use it.
Asking why it would be better than a regular mutex is a legitimate question. A lightweight mutex fast path (no contention) involves a `cmpxchg` and then an atomic store to unlock *at best* so reducing that to a single atomic fetch is a small improvement. It's a larger improvement over the default OS mutex types due to the wider range of supported features like fairness.
I use the first option in my project (though I call the directory `libs`), and it works quite well. I can understand the desire to reflect your library dependency structure in your directory structure, but to my mind all the libraries in the project are really peers, regardless of what the dependency graph looks like. So this feels cleaner to me. And it means all of the dependency paths are just `path = "../other_lib/"`.
Our regexes are definitely fairly naive. Optimizing them is TBD. https://github.com/rust-lang/rust/issues/14029
I'd be interested in working on some libcollections stuff. I looked for an issue and didn't see one for insert or other methods on RingBuf. Is there somewhere where you track those missing methods?
Those are implementation details, not fundamental limitations of unwinding. In any case, I don't want to spiral into the "performance of unwinding" debate. The point is simply that hardware traps are not incompatible with unwinding.
They are incompatible with unwinding as implemented in LLVM, and working support for asynchronous exceptions would have a large performance impact.
My [libJIT wrapper library](https://github.com/TomBebbington/jit.rs/) is nearly stable if you're interested.
That seems to make sense. Dead code is recursive, if something is only called from dead code, then it's dead code as well.
You have a couple of `&amp;amp;` instead of `&amp;` in the code examples. Otherwise nice article :)
&gt; - ... As a consequence, Rust is no longer a useful programming language. &gt; &gt; - ...Rust is now only of academic interest, and lacks any practical applications. ???
The language is named after a parasitic fungus and the unofficial mascot is a crab. Man. Go-lang certainly has us beat in name-origin and a cute mascot. 
&gt; MaybeOwned[Vector] has been deprecated in favour of Cow. See the PR for full details of all the consequences. Is this real? 
Sample code or it didn't happen. The dead code analysis identifies things that are *transitively* dead. So referring to `VALUE` in a block of code which is *itself* dead does not keep `VALUE` from being considered dead.
It is, but it also a very specific usecase. How many times are both the bounds AND the index statically known? If it only works for static arrays and a static index, is it really worth it? Now, I would love to see something like non-type template parameters and even (why not?) dependent typing, which would provide much more generic solutions to this issue (not restricting to one compiler known type), but that's not for now :x
Iterators over slices are written in `unsafe` code to avoid bounds checking. While your proposal *might* be possible in the distant future; for now the advice is: profile and act. If the profiler does not point at bounds checking as being an issue, then just let it there, otherwise either try and use more idiomatic code or switch to `unsafe` code. Rust does not aim at being 100% safe code, it aims at being as safe as possible, and at clearly labeling (with `unsafe`) the few areas where you asked the compiler to trust you. Even in the extreme case where 10% of your code base would be unsafe, it's still 9x better than C++.
I would think so. COW here would stand for "Copy On Write". The bovine humour is just a result of Rust's casing guidelines.
Yes. It's also very, very out of date.
Those made me smile :D
Okay. Well, for a moment there I thought that they were really talking about cows. I'm glad to have this assumption be incorrect. 
&gt; Statically allocated TLS keys are now explicitly leaked. I was wondering if those would now show up in valgrind reports? Furthermore, isn't there also the issue that you are now better off NOT having anything useful to do in the `drop` method?
That would be great!
Fixed, thanks!
I wouldn't read very much into this; it means they're making it more stable for actual use. Read the links, man. It's all good. 
I should have been more precise, my intention was to say "the AN_INT code path *should be* fulfilled" etc. Regarding FooBar being undead on match: I might want that, actually. With a constant, my issue is that it obscures information. Surely we can agree that there's some different quality to the non-use of AN_INT and ANOTHER_INT? There would be that same different quality to an enum that isn't used at all vs one that's used from a dead code path, a function that is never called vs one that is called from a function that is itself dead, and so on. Where to draw that line? Rust is consistent here, that's a virtue. Constants would make a useful exception for the reason I've outlined: I truly feel they're 'used' in a different fashion from anything which may be mutated in any way. Some sort of crate level attribute allowing for conservative code warnings perhaps? It would be quite nice to only be warned about a single level of death, if one is inclined to program from the bottom up. 
Just nitpicking, but the docs call it Clone on write, as Clone is the preferred verb.
Good point. Also makes more sense in Rust's context.
When names are supposed to mean one thing and literally mean something completely different, it bothers me so much. `Arc`, `Cow`, the late `Pod` (guess I should consider it lucky we didn't take `fun` from ML)... I must be in the minority on this, otherwise we wouldn't keep ending up with them.
Btw, if somebody got problems with building Piston on windows, please open up an issue about it or post a comment here https://github.com/PistonDevelopers/piston/issues/754
I love the name -- I'm assuming Ferris is a word play on Ferrous!?
The layout of our java project at work is deep and makes sense to be so. Roughly speaking it is: Root Business Dao Impl Webapp Ui Webapp RichClient Platform Api1 Api2 So, api1, api2 and ui/webapp all use business/api to talk to the business layer. Rich client uses api1 to talk to the business layer - since it runs outside the data centre. As such, there are quite a few dependencies across large parts of the project... Now, I wouldn't be surprised if the answer here is - that's not the Rust way to do this, which is fair enough. I'm still getting to grips with all of this :-)
typo: std::**or**::unix
Chromium doesn't even give an OpenGL handle to the renderer processes. It proxies all of the rendering requests over pipes. No one complains about the performance, and it's awesome from a security perspective. It does mean that they lose in some synthetic benchmarks to Internet Explorer.
It's quite trivial to do sandboxing on Linux: https://github.com/thestinger/rust-seccomp/blob/master/example.rs It's hard to do it across platforms, especially due to the lackluster support for sandboxing on non-Linux platforms. There are equivalents to the functionality of namespaces / chroots but nothing to eliminate the massive kernel API attack surface. PNaCl provides a sandbox for native code without depending on OS sandboxing. It has a fairly small performance hit due to the portability of the IR. In Chromium, it runs within the outer OS sandbox but it's not strictly necessary. It's certainly safer than the dangerous runtime machine code generation done by JIT compilers for untrusted JavaScript in web browsers.
Check out the Rust gamedev subreddit http://www.reddit.com/r/rust_gamedev/
Ahhh okay that make since...i am still getting a weird error though and I am not sure if it is an ICE or not. impl FnMut&lt;(int,), int&gt; for MyType&lt;(int,), int&gt; { fn call_mut(&amp;mut self, args: (int,)) -&gt; int { 1 } } This gives the error "method `call_mut` has an incompatible type for trait: expected "rust-call" fn, found "Rust" fn [E0053]" ...any idea?
I'd suggest `FnMut`, then you can pass in `|&amp;:|` and `|&amp;mut:|` closures, whereas `Fn` can only take `|&amp;:|` closures, and those can't have mutable upvars. The downside is you need a mutable context to call `FnMut`.
When manually implementing the `Fn*` traits the `call` method needs to be mark as a `extern "rust-call"` function: impl FnMut&lt;(int,), int&gt; for MyType { extern "rust-call" fn call_mut(&amp;mut self, args: (int,)) -&gt; int { 1 } } 
I will most certainly post a comment about this. 
Fixed (in repo, time to sync is ????)
Rust is doing exactly what it should. The test function is a single unit which is executed from start to end; that the test should fail is a statement that *something*, *somewhere* in there is expected to panic; the body of the function is a black box as far as the test runner is concerned. If you want multiple conditions to be tested, rearrange them so that your desired condition will be satisfied, or split it into multiple tests.
I'm a little split on this, but I can certainly see the virtue of this viewpoint. If you decide that unused2() is actually dead and delete it, then you'll get the warnings on unused1(). So the information about recursively dead code is not actually lost. It may just require extra recompiles to see it. However, when you get a big cascade of dead code warnings, which all result from a single function not being called, it can take a lot of reading to conclude that all of these warnings originate from a single missing call. If it only actually warned for root dead code this error case would be easy to resolve.
I see said the blind man...thanks for that one
There's also this one: &gt;Unboxed closure captures are now **avaiable** in debuginfo.
Redesigning some of the internals of Teepees header representation scheme last week (so that getters could take `&amp;self` rather than `&amp;mut self`), I *almost* ended up with a method named `raw_cow` (but I decided to rename it just `raw`). I had the comment Moo! on it.
&gt; I had the comment Moo! on it. So cute! :D
I've come to the same conclusion. I want a well-formed tree: I'm interested in where the tree is broken, not the leaves that are unreachable from the top node.
`cmpxchg` is very slow (&gt;100 cycles + cache-line bouncing). Atomic fetches are very fast (on x86/x86-64 they are just normal fetches, only constraining compiler optimizations). 
I think "RFCs" is spelled this way, not "RFC's". Example: https://en.wikipedia.org/wiki/List_of_RFCs
While we're on this topic, is this something a testing library for Rust could cover?
Yeah, but it's not a system call - which is what people tend to think.
It sounds to me like you want to invert each condition in your test. Does this do what you want? #[test] fn five_back_failure() { assert!( 6i != five_back()); assert!( true != true ); } 
In this specific case, I think Rust is doing exactly what it should. The only way this would work as you suggest is if `#[test]` starts doing magical things with the code its passed. Better to have this in a higher-level testing library than re-using existing syntax, but dramatically changing the semantics.
That does look like a very Java way of doing things... Which is not to say it's wrong, just that it's not how I'd personally want to work in Rust. Which I guess is a long way of saying it's "not the Rust way to do this"... But I'm enough of a Rust neophyte myself that I'm not sure I'm comfortable making that claim.
I recommend comparing against `PTHREAD_MUTEX_ADAPTIVE_NP` on Linux. The normal POSIX mutex types have fair scheduling so they are significantly slower. I have no doubt that this one-time lock is a significant win over it, but it would be interesting to see how far ahead it is.
&gt; A function that can succeed after failing should be given a chance to do so Once it hits a `panic!`, there's no more "chance" it can get. The task is dead. It can't recover.
I thought about the tree, but it seems it would be difficult to output a well-formed console message which properly communicates such a tree. The limitations of working in a non-graphical medium. I think that adding a hint that a thing is only dead because another thing is dead might be a good compromise. The warnings for the root dead code should definitely come first. Though it needs to then deal with the case where something is called from multiple places, but all of those places are dead. Something like "Foo is only dead because Bar, Boof and Garply are dead." Also, if it's multiple steps down the graph, does it report it's immediate caller, or the root dead code? That is, in this case: fn unused1() { } fn unused2() { unused1(); } fn unused3() { unused2(); } Do you warn "unused1 is only dead because unused2 is dead"? Or do you warn "unused1 is only dead because unused3 is dead"?
I believe this is a distinction between "rlib" and "staticlib". If I remember correctly, an "rlib" is a static library which contains a reference to other dependent rlibs and static libraries instead of including the actual symbols. Basically, they're *intermediate* static libraries that have to be linked together at compile time by rustc so it can pull all the symbols together. A static library, on the other hand, are *final* static libraries, suitable for linking against other objects. As such, they have to include all dependent symbols. As for what else to link... dunno. Probably the standard library. Again, I don't think you're supposed to manually use rlibs; they're an intermediate format for the compiler's use, not yours. When I was playing around with bootable Rust, I just used static libraries as the output format from the compiler, which appeared to work just fine. Also, `rustc` doesn't produce `.o`s; the `Makefile` appears to just be renaming rlibs.
What Rust is doing is the only feasible behaviour for a language in its position. Instructions are executed in sequence; panicking is expressly *designed* to stop things immediately. It is utterly infeasible to design it in any other way and retain the paradigm that Rust has of executing instructions in sequence. The entire panicking mechanism cannot possibly be done in the way you are contemplating. It would require redoing the whole test infrastructure from the ground up to not use panicking. While possible, this would be a radical departure from tradition for dubious benefits and certain increases in complexity.
I think the problem here is that `#[should_fail]` is intended to test code that has it's own internal calls to `panic!()` in it, not for cases where you want to explicitly assert the inverse of something, because you can always just invert the condition on the explicit assert (that is, assert `6i != five_back()`).
Clearly, we need an RfC on how RfC should be capitalized and pluralized :P
My understanding is that the `extern "X"` syntax specifies the ABI to use on that function. What I don't understand is why you need to specify a separate ABI for something used entirely internally to Rust.
i know that unboxed closure are a fairly recent update so it could either be that this is just a rough edge that hasn't been smoothed out OR mayb this actually does require a different ABI because you are getting passed an arbitrary sized tuple of arguments instead of getting them individually...a regular c function doesn't work this way
Good point. It seems one of two behaviors is desired: sometimes you're working to link a bunch of code together, and you want to know where the breaks are. Other times, you're pruning code, and want to find everything you killed so you can delete it. This seems like it should crate-level attribute, and not at all related to `const`.
I-nominated P-high
Your wish is my command: #![allow(dead_code)] 
now... how many people in a general setting will think this is a band t-shirt?
And the logo is a bicycle chainring! I find it amusing that the name originally came from the fungus, but the rust ecosystem has jumped on board with the "industrial" theme - names like Piston, Cargo, Crates, Iron, etc.
There's already http://rustbyexample.com. And if you just want to toy with some code, http://play.rust-lang.org /u/brson just took over maintenance of Rust by Example and has started merging pull requests that have been sitting around a while, ~~so hopefully the "Not Maintained" disclaimer goes away soon~~ and has already removed the "No longer maintained" disclaimer.
Here's the relevant RFC: https://github.com/rust-lang/rfcs/pull/369
Rust now only need 'Cat' &amp; 'Dog' trait :)
These were a test to see if anyone reads TWiR, right?!?! I laughed :)
So borrowed cows get cloned when you write to them? Cool, let me get my stationery, I need to see this! (Sorry)
Thanks, that explains everything.
Without apostrophe plural, with apostrophe possessive. Unless it's "its" or "it's", where the latter isn't a possessive but contraction, hence the exception. Plurals' possessives look different still. Don't tell me to use an "of", there. Source: Not a native speaker. I *learned* that stuff.
I'm going to assume the context was "This RFC's awesome!"
If you want you could just make your own... trait FloatOrInt { } impl FloatOrInt for i32 { } impl FloatOrInt for f32 { } and just use those.
Just a small formatting issue: in the paragraph starting with &gt; In Rust we don't have the problem, because ... some of the inline code is messed up. The first one seems to be correct, but the next two are broken and you can't see the parameters to `Vec&lt;&gt;`. This is in Chrome 36 on Ubuntu 14.04. [Here's a screenshot of what I see](http://i.imgur.com/Ao8Z1Th.png). EDIT: nevermind, a hard refresh seems to have fixed it. No idea what was going on there.
IIRC making the compiler used to output `oxidize:` instead of `rustc:`.
Haha, fantastic!
Macro invocations work at the item level (where `fn`, `mod`, `struct` and `enum` definitions are items) and at the expression level, but nowhere else. Theyre based on a semantic understanding of Rust code rather than a text-based one like the C preprocessor. Thus you need to refactor things so that the macro produces the entire item. Like this, for example: #![feature(macro_rules)] fn main() { let obj = foo::Bar::new(); println!("obj.txt: {}", obj.txt()); } macro_rules! impl_string_getter { ($ty:ty, $field:ident) =&gt; { impl $ty { pub fn $field(&amp;self) -&gt; String { self.fields.$field.clone() } } } } pub mod foo { struct Fields { txt: String, } pub struct Bar { fields: Fields, } impl Bar { pub fn new() -&gt; Bar { Bar { fields: Fields { txt: "Hello, world!".into_string(), }, } } } impl_string_getter!(Bar, txt) }
Didn't you notice the title has changed to *"This Week in Haskel++"*? Basically Rust has been discontinued... forever. In other news wild unicorns spotted out of shore of Maryland.
Yes. Consequently Rust has changed it's syntax to [(Moo. Moo? Moo!)](http://esolangs.org/wiki/Ook).
How this library influences the workflow?
You seem to be representing lat and lon as `f64`. It seems to me that 32 bit fixed point would make more sense here as you may loose accuracy or at least performance when working with floats. http://wiki.openstreetmap.org/wiki/Node#Structure Also the license is a bit questionable as you seem to include gplv3 code
I can't speak for the people making these decisions, but I do know that one aspect of this you didn't mention was their concern over de-facto standardisation. It's come up a few times that CSS vendor prefixes were intended to allow vendors to prototype features, converge, then standardise. What *actually* ended up happening was that people just went ahead and deployed websites using the prefixed properties. Once a prototype reached a certain critical mass, it would become... "politically" untenable to just go and break it, as it would be *perceived* as the browser vendors maliciously breaking the web. &gt; As an aside, this is a problem Microsoft has had since forever. Applications are written to depend on broken or explicitly unsupported behaviour. User upgrades OS and programs break; who's to blame? Or more importantly, who *gets* the blame? Microsoft. I feel it would be fair to say there are some core devs that are *really* concerned about sub-standard unstable features becoming so widely used in nightlies that it just becomes completely untenable to *not* standardise them. *Even if* there's a stable version with explicit guarantees, those guarantees aren't worth the bytes they're encoded in if everyone just uses nightlies and ignores stable. So to summarise: I believe *one aspect* of the removal of functionality is to avoid de-facto standardisation of unstable functionality simply because it's been there for a while and is "good enough".
....... ?
Sure - proggit always features "the D" jokes in threads about D-lang.
This makes no sense to me, *at all*.
Pluralized R4Cz.
If something gets to the standard library, it's going to stay there for a long time. There are language features that are expected to be implemented after the 1.0 release, such as higher-order kinds, thus enabling us to implement e.g. iterator traits properly. We cannot afford implementing them in Rust as it is, because it isn't going to be compatible with the future proper implementation. Stability has its issues.
Copy And Transform Data Object Generator
Yep. The question should be asked "What is the real benefit of having a large standard library?" More often than not, there is very little benefit to it. But there is a huge downside. Once something is standardized it will be nearly impossible to get it removed. For this reason it is my opinion that the best route forward for language is to have a very small and refined standard library which only contains things useful to most applications. Anything else should be provided by 3rd party libraries. For a third party library, there is less of an issue if they decide to break their API, remove things, etc. They can just release a major version and be done with it. On top of that, they can simply make a brand new library if they decide they hate the old one and programmers can pick and choose which one does things the best. When something goes into the standard library, that is often the end of innovation for that thing in that language. Nobody is going to re-implement what is in the standard library and even if they do, very few people will use it. Any upgrade often requires a new version of the language.
Old version was wrong.
&gt; More often than not, there is very little benefit to it. But there is a huge downside. Once something is standardized it will be nearly impossible to get it removed. These are good points. I have no issues with things getting removed if they don't belong in the standard lib - it's when they're removed for stability reasons that I'm wondering about. When something's standardised it's here to stay, for sure. But if we can't include un-standardised features as experimental/unstable, what's the point of stability attributes?
C'mon now!  I'm new here but I can read an error message. What I and others in this thread would like is a [dead_code] attribute that shows where the tree structure is broken, not every item that is unreachable from main/lib. What you're offering is...turning off warnings. 
rhymes with dimple!
Not sure how common or usable these are, but in my mind: * `{` : begin * `}` : end * `;` : next * `+=` : increment * `-&gt;` : returns type * pattern `=&gt;` expr: if pattern matches return expr * ` : life-time * `&amp;` : borrow (a variable) 
I'll give an example of the types of things I'm struggling to read aloud: fn inverse&lt;T&gt;(x: T) -&gt; Result&lt;T, String&gt; { Declarations like this I find tricky to vocalize, especially around generic (and lifetime) syntax. How would an experienced rustacean read that declaration aloud?
&gt; You can add type if you want to &gt; You can leave inference behind \- *Type Safety Dance*
It's a sad experience, I can't even have posts embedded in posts embedded in posts etc. I think it's limited to two levels
I feel like there's some inside joke I'm missing out on here... -- clueless newbie
I try to read left to right, so would: "declaration of a named function inverse which is generic over type T, takes an argument x of type T, and returns a generic Result over type T and String" make sense? Is it precise enough and/or using accepted terminology (for instance I wonder about "generic over")?
Men without hats, safety dance.
I'm in no way an experienced rustacean, but I would probably read that as "inverse is a function parameterized by the type T that maps a parameter, x, of type T to a Result of type T and String". I'm fairly sure that the reference to mapping is technically incorrect because the function isn't necessarily a pure function, but it sounds the best. 
If what you want is better diagnostics, then I'm all for that. But what you initially asked for was *worse* diagnostics. =]
You could probably leave out the generic over bit. I'd probably say it like: "function inverse which takes an argument of type T, and returns a Result, T, String" But I've listened to a lot of Haskell presentations
It's not going to be possible to transition to checked arithmetic by default unless you want Rust 2 to completely break cleanly with Rust 1 kind of like Python 3 vs. Python 2.
There was a time where so many threads to the mailing list were addressed with some clever pun on "rust" "Hello fellow Rusties" "Hello Rustafarians" (my favorite :p) "Greetings Rustaceans!" (Somehow this one took off...) 
&gt; That basically requires halting work on everything that can't be done before 1.0. Why not a separate library? Why do we have to do experimentation in the standard library? It doesn't require halting work, it just requires that experimentation is done outside of the standard library. There is nothing that says an experimental library can't eventually be merged into the standard library. &gt; I can see people relying on unstable (though it's silly), but I've got no pity for anyone relying on experimental features. And yet, java has a "internal don't use these things" library in the standard library and you know what? People used them and now there are parts of the internal library that they can't change for fear of breaking some of the users of that code. If it makes it into the standard, no matter what annotation and guard you put around it, people are going to use it. Those people are going to complain, loudly, whenever someone breaks something.
When you discuss generic functions all day long (and that's me, some days), you shorten it to just "of". "List of int" or "Map of string to int" or "Vec of string", etc. Also, when you deal with T, U, V and your most common type parameters, you don't bother saying "type" any more. It's just "list of T" or even "T list".
Presuming that the implementation causes panic/abort on overflow, all Rust code which does not deliberately utilize wrapping overflow (which we presume is the vast majority of code) would compile as-is (leaving aside the thorny issue of creating many more places where panics/aborts can occur). It would be an extraordinarily less invasive change than Python 2 to Python 3. Furthermore, it could also be done with a generous deprecation period: first introduce wrapping overflow operators (as Swift does) for the people who need them, and add an attribute like `#![future(panic_on_overflow)]` so that people can selectively port their code to the new behavior. After adding these features, announce that checked overflow will become the default two years down the road. This is the same approach that Python 2 took with all of its own breaking changes, and was massively successful. Recall that Python 2.6 broke backwards-compatibility, as did 2.5, and 2.4, and 2.3... but they pulled it off so gracefully that nobody even remembers! So yes, it would be a breaking change, and via Rust's commitment to semver it would require a bump to the major version. But languages actually break backcompat *all the time*, even giants like Java and C++ and PHP, it's just that most of them don't bump the version number when they do so.
I would word it like so: I define a function named "inverse". It is generic over type "T". It takes an argument named "x" of type "T". It returns a "Result" that is instantiated with the types "T" and "String". As following: (description of the bracket contents follows...) Note that due to the usage of the bracket you're not just declaring the function but also defining it.
Yeah. I know its a bit of a mouthful, but I was trying to be as descriptive as possible, in the style of the example given in the OP. 
But if you can't type then you're no friend of mine~!
Providing field accessors that copy String objects seems like a very un-Rust way of doing things. You're working against the language and its goals, rather than taking advantage of what makes Rust powerful (such as borrowing). You might consider exposing these fields directly, rather than using field accessors. I've spent many years writing OOP code, and I find the guidance to write endless pages of trivial get/set accessors to work against the goals of performance, maintainability, and generally sanity.
&gt; "What is the real benefit of having a large standard library?" One of the selling points of Python is "batteries included". But then this is probably from a time when Python did not have a package manager.
Yep. Most new languages are coming out with package managers which significantly decreases the need for having an expansive standard library.
I think there is a distinction between a web standard and a systems language standard, in that with a web standard we have no control over what version of the standard our software is being run against. That is, the user could be hitting our website with IE8 or Chrome 32, or any number of other browsers, and we just have to try to make it work on as many of them as possible. And if Chrome 33 comes along and breaks a feature we were using, then our website will just break for anybody using an up-to-date Chrome. (The same principle applies for OS APIs, as mentioned with the Microsoft example. And in reference to the thread below, Java is similar, because you're stuck running against whatever version of the JVM the user has installed.) With a systems language standard, though, we have full control over what version we're using. If we're using an unstable feature in Rust 1.0, and Rust 1.1 comes along and changes it, nothing breaks for our users, and nothing even breaks for us until we decide to update our compiler. And we can update the compiler on our own terms. Our Rust 1.0 code won't suddenly be running on Rust 1.1 in our customer's hands. In short: Rust should be able to deprecate or drastically change features that are in active use, without causing the sorts of problems that happen when a browser does the same.
This is helpful; as a newb I'm probably being (and will continue to be for a while) overly verbose, but I think compressing the internal vocalization is a step towards starting to just intuiting the meaning of the code.
I like the "maps ... to" nomenclature, am going to try that on. Pragmatism is the name of the game for me right now.
Great post. :P One nit: &gt; It's also possible to change the delimiter (for example if you have TSV data - tab separated values) although it seems to be the only customizable syntax at the moment. I think it would be fantastic if the library allowed for different CSV dialects, as does the Python standard library. In the past week or so, I've added support for changing record terminators and quote characters. [They're in the docs.](http://burntsushi.net/rustdoc/csv/struct.Reader.html) For example, to read [ASCII delimited values](http://en.wikipedia.org/wiki/Delimiter#ASCII_delimited_text), you could use something like: let mut rdr = csv::Reader::from_file("blah.csv") .delimiter(b'\x1f') .record_terminator(csv::RecordTerminator::Any(b'\x1e')); But yeah, there aren't any bundled "dialects," but I think all of the configuration you could want is there.
&gt; Providing field accessors that copy String objects seems like a very un-Rust way of doing things. You're working against the language and its goals, rather than taking advantage of what makes Rust powerful (such as borrowing). You might consider exposing these fields directly, rather than using field accessors. Since the data structure is created by a binding to a C library, all it offers is a bunch of ``*mut libc::char`` lumped together with little discernible organizational system. I do not intend to expose those in a raw (mutable) form. There is also no way I could allow mutable borrowing because according to my tests some of the fields are uninitialized after calling the constructor (!). Thus I will have to make an informed decision for each value before including it into the export -- hence the getters. Regarding the ``*char`` fields, at this point Im not certain whether I will return owned ``String``s or borrowed refs to a memoized version that will be created from the original value on first access. Technically I prefer the latter, but its also going to take more time to implement. Thanks you anyways for your feedback, fellow denizen of the OOP hell. 
&gt; Macro invocations work at the item level (where fn, mod, struct and enum definitions are items) and at the expression level, but nowhere else. Theyre based on a semantic understanding of Rust code rather than a text-based one like the C preprocessor. Thus you need to refactor things so that the macro produces the entire item. Like this, for example: Thank you very much for your response! Indeed, my preconceptions from using C probably caused a too naive understanding of Rusts macro system. However, even considering that macros are required to expand to complete items or expressions I dont see how it follows that the previous macro expanding to a ``fn () {}`` appeared to work while the revised version expanding to an equivalent ``pub fn() {}`` failed. What about the former makes it produce a complete, valid item, whilst the second version fails to parse? 
I try to read syntactically: * `fn` = the function * `inverse` * `&lt;T&gt;` = for all `T` * `(x : T)` = taking `x` as `T` * `-&gt;` = giving * `Result&lt;T, String&gt;` = `Result` of `T` and `String` * `{  }` = is The function `inverse`, for all `T`, taking `x` as `T` and giving `Result` of `T` and `String`, is 
"Function select takes a Shape ref, float threshold, and 2 T refs, and returns one of the T refs." I mostly treat lifetimes as info the compiler needs, but isn't really part of the function signature. In this case, it tells you that the reference returned by the function is (probably) a reference to one of the two input references. I'm not sure why you'd have to have the same lifetime on the Shape reference, though.
Yah, but... they seem so fundamental, especially to figure out the right way to interact with functions that are declared with them, so I'd like to be able to vocalize them as I'm learning.
I like your method of breaking the syntax down. Very good for learning - going to give it a whirl.
Maybe ignore them and then tack on "and a, b, and the return value have the same lifetime"? For one thing, the name of the lifetime is usually garbage, and it only matters which ones have the same lifetime.
That makes sense. Also the bit about providing borrowed access to memoized versions of those strings sounds pretty sensible. I might *borrow* that idea, too.
Generics mostly make my brain hurt. Except now and then I feel like Neo, seeing through everything... And then my brain hurts again. 
Rustafarians is so good...
&gt;Rust comes with the promise of high quality code, documentation and stability. Not every library in the package manager will make that promise. True, but if we want this, we could push for Rust dev sponsored libraries with the same code quality/documentation requirements as the standard library. &gt; I have to keep trying libraries that may or may not have documentation, change all the time or be abandoned altogether in 6 months, that is a serious downside if I'm considering the language for a serious long-term project. Even if rust has an expansive standard library, this will still be a problem. New languages have a turbulent library story, that is just how it is. The thing is, everything eventually settles. Some libraries come out as the clear winners, many die off. Either way, if you are doing something non-trivial, it doesn't matter how expansive the rust standard library is, there is a good chance it won't have what you are looking for. There is nothing about the standard library that guarantees code quality or good documentation. There is also nothing that prevents a non-standard library from having good code and good documentation. I think the approach that should be taken is that we have a miniature standard library and, if needs be, non-standard but supported by the rust devs libraries (sponsored libraries?).
Thanks, sorry for missing that. I updated the article.
Futures and promises are very similar: http://doc.rust-lang.org/std/sync/struct.Future.html That said, there's been comments that our Future isn't that great... there's some improvements we could make. &gt; If this is good code and wanted by anyone, I could create a repo and crate it, let me know In general, you should just default to creating and sharing a package. Even the most trivial of things can be quite useful, and you may not even know for a bit. The most popular Ruby package I created has 1.2 million downloads, and was about 40 lines of code and 30 lines of tests that I wrote at 4am when I had insomnia on a business trip one time. I wrote it to fix a bug in another one of my projects, tweeted about it, and went to bed. Turns out one of my friends needed the exact same thing for his job that day... and it just kind of went from there.
I tend to pronounce things in my head almost exactly as theyre written (skipping most brackets), so I say tick r for lifetime names. I like pronouncing things very briefly, so that example would be function select with tick r, T takes shape, and tick r T; threshold, f sixty-four; a, and tick r T; b, and tick r T; and returns and tick r T. That said, I pronounce things very tersely in a way that is unclear and slightly ambiguous at times. A more correct/clear way of saying it would be `select` is a function with a lifetime parameter, r, and a type parameter, T, that takes four parameters: `shape`, a reference with lifetime r to a type `Shape`; `threshold`, a sixty-four bit floating point number; `a`, a reference with lifetime r to a T; and `b`, a reference with lifetime r to a T; returning a reference with lifetime r to a T.
Excelent! This is exactly what I would hope you guys would do :). I really like the idea of having a minimal std library. It makes it much easier (imo) to evolve the ecosystem.
Thanks! :D
If anyone wants to actually *read* the code, [here you go](https://gist.github.com/eddyb/7856c19e1bd47906d073).
Perhaps "function inverse which takes any T..." edit: or "function inverse from T to Result, T, String"
&gt; we have full control over what version we're using The rub is that you don't have control over what versions other people are using, which means if one of your libs needs 1.0 and one of your libs needs 1.1, a breaking change between 1.0 and 1.1 will put you in a bind. And Rust can't guarantee ABI stability between versions, so you can't necessarily just compile them separately and link them together. The compatibility promise has value only if all 1.0-compatible libraries build with all 1.x versions of the compiler (and note that this promise means that everyone using nightly, unstable versions of 1.x Rust will *also* be able to use 1.0-compatible libraries, thus easing the burden on library authors even further should they decide to target a stable version).
FWIW I got a great chuckle out of `You can no longer invoke Dark Magicks and match on an enum struct variant as if it were a tuple.` I actually attempted to do that as a potential work around for an ICE I've been having [moving fields out of a struct-like enum variant](https://github.com/rust-lang/rust/issues/19340) But hey... it was an improvement at least. I got an error forbidding such dark magics, instead of an ICE.
Uh oh.
I agree. The quality of the library is orthogonal to how it is distributed. The expectations for the standard library might be higher, but that does not necessarily mean that they will be met, or that libraries in the package manager cannot fulfill those expectations as well.
Once it's stable you can use it to provide a JIT backend for your DSL.
Sync has recently been rewritten, so I'm not sure if this is true anymore.
It's still true, as it hasn't landed. You could easily measure the performance of fair / unfair native mutexes.
This is what is *not* supposed to happen. But I guess this was more or less was bound to happen without full type inference. Compare Haskell: As laziness kept it pure, so did full inference keep the (extension-less) type system sub-turing. Not that rustc should accept functions without type declarations, but it being able to tell you, in all cases, what type it's supposed to be might actually be a very nice feature.
Some `Cargo.toml`s for the libraries in the main repo would be nice. Even if they aren't used for the main bootstrapping process it would make std-lib hacking and cross-compiling a lot nicer.
1) Proper test should have just one assertion so it make sense to fail on first assertion. 2) If you have multiple assertions, it make sense to pass only when all assertions are true. That means nothing fails. assert!( a); assert!( b); Should be same as. assert!( a &amp;&amp; b); What you want is at least one succeed. assert!( a || b); Or something like this. If a { assert!( a); } Else { assert!( b); } I think this is the same as when you are throwing error in any language. It is enough to throw one error top interrupt you don't have to throw all of them. EDIT: Assertion is there to assure condition is met otherwise something wrong will happened. When condition is there to direct flow in your code, you should use match or if.
You're right, I just found the PR. I'll see if I can get around to tweaking the benchmarks later.
https://github.com/rust-lang/rust/issues/19240
Can you explain how one can see that missing full type inference let to this weird machine?
Should add it to the template too. "If you find any issues, please submit a Pull Request!" or something.
Some recommendations: - I usually start by calling my installed `rustc` directly on the modified library (`rustc src/libcollections/lib.rs`), then workout the compiler errors. If `rustc --test` works, then also do that and fix the unit tests - Look at `make tips`. There is make command like `NO_REBUILD=1 make check-stage1-{rpass,cfail,std,...}` that runs most tests with minimal rebuilding. - Use `RUSTFLAGS=-Ccodegen_units=4` this parallelizes LLVM codegen, it makes compilation faster, but makes your executables slower (but that doesn't matter because you just wanna pass the test suite) 
It's impossible to have full inference for a turing-complete system: To infer a type you have to unify with arbitrary types of the same system, i.e. you now have to decide equivalence of two turing-complete programs, which is even harder than "just" the halting problem. Of course, if you give up on inferring types for all programs, most general types etc. you might get somewhere and still terminate. But that's generally not what's meant by *full* type inference. It wasn't *bound* to happen, only more or less so. Turing completeness is easy to achieve by accident.
&gt; I don't even have a single follower on github haha I'm following you now :P
&gt; Why not a separate library? Why do we have to do experimentation in the standard library? This is a fair point, and I'd be happy with this. &gt; And yet, java has a "internal don't use these things" library in the standard library and you know what? People used them and now there are parts of the internal library that they can't change for fear of breaking some of the users of that code. I'm not too familiar with Java's release model, but did they have a nightly/beta/stable split like Rust will? The people that will rely on feature continuity are those using Rust in production, and they won't be (or shouldn't be) using the nightlies in the first place - hence no access to unstable features. &gt; If it makes it into the standard, no matter what annotation and guard you put around it, people are going to use it. Those people are going to complain, loudly, whenever someone breaks something. While I understand this attitude, I very much hope the devs will not be afraid to play with experimental features. One difference between Rust and Java that I can see is that (at least for most of its history) Java has been managed by a company with an interest in keeping customers using Java. While the Rust community has the same interest, there isn't the same strength of influence there.
Thanks for writing that. It seems I'm over-optimistic about the amount of attention people will pay to stability attributes. I personally wouldn't make any assumptions about a feature marked experimental, but I guess not everyone thinks that way.
sweet! :D
I think it's something that needs to be handled carefully. A small standard library makes it easier to avoid de-facto standardization, but it's not without its downsides. It becomes more difficult to find a library for your project when there are multiple offering the same features. Maybe you pick two libraries to use in your project, but they use different time libraries and need to interact? In this way, fragmentation is also an issue. Some libraries use one library, others use another, etc. A mini version of D's Phobos-Tango split. I like the idea of having dev-supported/promoted crates in crates.io - it seems like that system would go along way to reduce/prevent fragmentation.
In this case I was actually trying to run doctests, but `rustdoc src/libcore/lib.rs` worked fine. The error messages it was giving me, however, were pretty much useless. It gave me `&lt;stdin&gt;:29` instead of `iter.rs:2071` where the error actually was. The context was equally useless, because it turns out the error was because of this: ///``` Instead of this: /// ```
Is this a bad thing or a good thing?
I asked this exact question [just a little while ago][1]. [Here][2] is a fixed link to the RFC /u/dbaupp posted. [1]: http://www.reddit.com/r/rust/comments/2ki4lg/whats_the_difference_between_impli_trait/ [2]: https://github.com/rust-lang/rfcs/blob/master/text/0135-where.md
Thanks! So since you asked the question... do you think it makes sense to keep both syntax's? Or deprecate the bounds? I know the RFC says keep the bounds syntax as sugar, just wondering your opinion :)
Rust is not an interpreted language, so there won't be a possibility to make the tutorial as interactive as with i.e. Ruby
Personally I would prefer deprecation as I'm not a fan of fragmented syntax and `where` clauses are much more readable, but there's a lot of inertia behind the trait bounds syntax: you see it everywhere. That kind of change would cause widespread breakage, even worse than the change to the enum namespacing behavior a couple weeks ago. Also, `where` clauses still need a little bit of work. For example, they currently don't work to add trait bounds to existing generic parameters on methods: struct MyStruct&lt;T&gt; { t: T } impl&lt;T&gt; MyStruct&lt;T&gt; { pub fn duplicate(&amp;self) -&gt; MyStruct&lt;T&gt; where T: Clone { MyStruct { t: self.t.clone() } } } In this case, you'll get ```Error: type T does not implement `Clone`!``` because the `where` clause in this case is ignored, even though it is valid syntax. You need a separate `impl&lt;T&gt; MyStruct&lt;T&gt; where T: Clone {}` block, which is a lot of extra noise for one method, IMO.
Come on guys, it only took Mozilla 10 business days to Change The World(tm) with Javascript, I'm pretty sure you can just press the "Release As-Is" button on Rust now.
What would it take to fix this? Now is the time.
Maybe as a name for a group of Rastafari rust programmers, but otherwise its just awkward cultural appropriation with racist undertones. I strongly discourage its use. The crustaceans won't mind.
So, this essentially means rustc will never be capable of fully implementing the Rust specification? Or simply that the specification itself is incomplete (in Gdel's sense)?
You mean, like this? https://gist.github.com/stevenblenkinsop/f5eea3d1a06e9d719afb
Once the where clauses actually work, wouldn't it be possible to write some kind of tool that rewrites the &lt;T : Trait&gt; syntax to where clauses? I'm in impression that the functionality of where clauses is a strict superset of &lt;T : Trait&gt;. 
They're absolutely part of the function signature and have tons of valuable information, it's just that if you screw up the compiler yells at you so you don't have to think about it :P
How about [multiplication and exponentiation](https://gist.github.com/tikue/e0a39e556bb22fe308bc)!
Yeah it's possible. That's a basic migrations utility. But you'd have to decide if it would be worth the effort to build it and make sure it works when it would only take everyone a few minutes to go through and update their code to the new syntax. It's unfortunate that Rust's syntax isn't described with a standard grammar specification, like the format that the [Java Compiler-Compiler (Javacc) ][1] consumes. Then you could autogenerate such a tool. Well, it is described in the [Reference][2], but that has to be written and updated by hand; `rustc` and its specification aren't coupled at all. [1]: https://javacc.java.net/ [2]: http://doc.rust-lang.org/reference.html
&gt; Turing completeness is easy to achieve by accident. I'm pretty sure it's also become a game for computer scientists at this point. "Hey guys, I found that this pile of logs, some twine, a paperclip, and this rock are Turing complete! Check out this 3-SAT solver I made with them..."
&gt; It wasn't bound to happen, only more or less so. Turing completeness is easy to achieve by accident. Wait a minute, a famous feature of Hindley-Milner type inference is that it is not Turing complete, and that that is very desirable to have for Haskell and many other FP languages. That particular thing isn't an accident, it's very much by careful design. You're right that Turing completeness is easy to achieve by accident in lots of other contexts.
Inasmuch as there is a Rust specification, one cannot take an arbitrary Rust AST^1 and say, "This program is (or is not) well-typed," because following the rules to type-check it, whether by machine or by hand, might require you to follow an infinite set of steps. (And in the process of following that possibly-infinite set of steps, you might happen to compute useful results. You could write a perverse Rust program that might compute any mathematical result that is computable by a Turing machine or equivalent computing device.) On the other hand, rustc is perfectly capable of implementing that infinite set of steps inasmuch as is possible. It's just that the type-checking phase of a perfect implementation of Rust might never terminate (or crash after running out of memory.) 1: I choose this phrasing specifically because _parsing_ Rust is decidableyou can still take a string and say, without fear of looping forever, "This particular string corresponds to such-and-such Rust AST," which is something that is not true of, say, [C++](http://blog.reverberate.org/2013/08/parsing-c-is-literally-undecidable.html).
why?...crates.io and rust-lang.org look great...We don't want to wait until 1.0 to have a convenient, easy, and visually pleasant way for people to play with the language! 
There was [a pull request](https://github.com/rust-lang/rust-playpen/pull/5) to improve the playpens design a while back that did get merged, but I think it got reverted for some reason (I think because the share button was accidentally removed).
Is the repo for playpen public? Maybe you can add some issues / PRs and get the ball rolling? I agree btw, would be nice if it looked a little nicer - at least as nice as the runnable code on the [rust-lang.org] (http://www.rust-lang.org/) home page itself. Oh also, I really like how I can use standard vim keys in playpen! Just wanted to mention :)
Just to clarify, you mean that since it wasn't forced to be Turing-incomplete, it was easy for Turing-completeness to sneak in?
Exactly. 
[The repo is here](https://github.com/rust-lang/rust-playpen). As I wrote in [this comment](https://www.reddit.com/r/rust/comments/2o8gt7/upgrading_the_playground/cmkr06g), theres already been [a pull request](https://github.com/rust-lang/rust-playpen/pull/5) to update the design that seems to have been merged, but reverted.
If Rust is going to have a Turing-complete type system, let's do it properly and go fully dependently typed*. Having incredibly abstruse ways of doing metaprogramming like this leads to the headaches we traditionally associate with C++ templates. \* I know this is difficult and that the Rust core team very explicitly doesn't want dependent types, that's why I think the far more sensible reaction is to close whatever loophole is being exploited here :P