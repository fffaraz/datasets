That they do. I don't see why that means all libraries you use in your application have to be written in Rust too.
&gt; gcc frontend This is exactly what i need! :) I've been pretty snowed under recently, in all seriousness my front-end i intend to finish but the last 6 months or so i found it very very hard to keep up with language changes in rust so i feel like i almost _have_ to take a step back until its closer to the 1.0 release. I implemented a bunch of stuff which isn't valid anymore basically. And since its my hobby i cant spend the time required to stay up to date fighting my own code until things get more stable.
Looks like clang in gnu89/gnu99 mode or clang++.
Actually, sorting all the transparent geometry from back to front and then drawing it in a second pass will eliminate all artifacts and make it look great. Its even easier in a voxel game because you only need to re-sort when the camera moves across a voxel boundary or when a chunk mesh changes, so you don't sort every frame. In addition, you dont have to sort all the geometry in the scene together. You just sort on a per chunk basis, and then also sort the chunks back to front. If you are already sorting the chunks front to back to get optimal early-z rejection, then all you need to do is iterate your chunk list backwards. I honestly have no idea why it took minecraft this long to do that, I was able to do this in SoA in about a day :/. EDIT: I should mention that this only works if all your transparent geometry can be drawn together, with the same shader. If your particles and water use a different shader, then particles might occlude the water or visa versa. Then it is tougher. But water should never occlude itself if you sort.
Ok, I get that, but if the player is running, and you're re-sorting on every voxel boundary, you still have to re-sort at least a couple of times per second, which means that sorting has to be fast enough to not cause a hitch in framerate. I guess my problem is, I don't really have meshes I can just re-sort easily, and I don't consider the camera when I generate them in the first place, they're one-offs until the chunk gets far enough or close enough to be meshed at a different resolution or until the geometry changes. I do still need to make the optimization where each axis can be rendered separately so I don't even pass the back faces to the GPU, I'll think about this when I get to that problem. 
Because one of the Rust's selling point is reducing the amount of unsafe code. If you write your application in 100% safe Rust, using a unsafe library add a risk of memory unsafety / datarace.
The Rust Rosetta Project is always seeking more contributors! [Link](https://github.com/hoverbear/rust-rosetta)
Is anyone working on datetime? I see datetime-rs was started but hasn't made much progress.
I'd really want a blog post that focuses on inner workings of html5ever. It's got a lot of interesting stuff I'd love to hear more about :D One such question is, what are PHF for?
The key is not to sort the meshes, but to sort the indices for the meshes. Assuming you are using indexed drawing ( you should be ). If you are sorting indices, you only need to move around the 4 byte quantities rather than the large vertex structs. With a voxel mesh, you can even sort by quads instead of by vertices. In c++, std::sort is plenty fast, and you should notice almost no fps drop even when sorting often, unless your transparent geometry is quite complex. A further optimization can be had if your mesh consists of only quads. If your index list always follows the same pattern, for instance (i, i+1, i+2, i+2, i+3, i) for each quad, then rather than sorting all the indices, you can sort a list of quad indices and then reconstruct the vertex index buffer. This is pretty much the fastest way to do it that I can think of.
the blog (not mine!) is appropriately named `mainisusuallyafunction`.
Yeah for the sorting to work you do need to keep the indices in memory. You do not need to keep the vertices though. Instead, you can store a separate buffer that just stores the positions of each quad. This buffer never needs to be sorted, and the additional memory usage for the index buffers and position buffers is quite small, especially if you compress your positions to 3 bytes as relative-to-chunk positions. Anyways good luck with the depth peeling! I was reading about that and it seems interesting.
Use this repository rather than Rosetta code website itself, as it has more up-to-date examples: https://github.com/Hoverbear/rust-rosetta I think the biggest 10 there are: LOC Filename 302 - 24_game.rs 189 - markov_algorithm.rs 187 - sha1.rs 152 - arithmetic_rational.rs 146 - md5-implementation.rs 144 - huffman_coding.rs 123 - iban.rs 107 - hamming_numbers.rs 107 - hamming_numbers_alt.rs 103 - four_bit_adder.rs (the LOCs were determined by changing the extension to .cpp and then using sloccount). 
this does not strike me as a very good idea either, since you just remove the OS (which is part of the compilation as well) from the equation. Also, on both cases, you only exercise a tiny part of the compiler if you grab such a small example - it may make parts of the compiler fit in the processor cache where it otherwise wouldn't. Ideally, you would have the same large project in multiple languages. Second-best, but also not yet realistic (or very precise), would be to compile a bunch of large projects, measure the compiler throughput in LOC/sec, then normalize it to an index like LOC/FP (lines of code per function point - kinda fluffy). There's some metrics of LOC/FP for old-school languages at http://page.mi.fu-berlin.de/prechelt/Biblio/jccpprtTR.pdf, but I don't expect you'll find that for Rust... alas... 
Thanks for sharing. I liked that all the Rust translations compared favorably, and that you were pleased with the resulting design.
&gt; Rust gives you the choice between boxing and unboxing in all circumstances Except for strings, no?
I've reserved room 203 in the CSE building from 5:30pm to 9:00pm. The room has whiteboards on all the walls and also has a projector. I'll PM you my contact info for getting in.
Cool, my 4 bit adder made it in the top 10 :)
How would that be different? I just run sed once. The intent is many different files. Not one file.
Grrreat. Thanks.
You can't do meaningful compile time benchmarks with mini-programs like these. It's like measuring the fuel efficiency of a car by driving a 1 m distance.
Good point. Any idea?
Any of the large crates in [the main distribution](https://github.com/rust-lang/rust/tree/master/src) (e.g. `core`, `std`, `syntax` or `rustc`), or one of the crates in [servo](https://github.com/servo/servo) (I don't have a good suggestion for which ones).
Ok, I may have misunderstood you. I created a file called `src/test.rs` and moved one of my unit tests into it. In the place where I had the unit test I wrote `#[cfg(test)] mod test;`. As I stated on the link above, my lib.rs has several mods, some nested. This is for a top level mod. I still can't run my tests. I get the following error: `$ cargo build` ` Compiling Shogun v0.1.0 (file:///XXX)` `XXXX error: file not found for module ` test ` ` `XXXX mod test;`
Here's the steps I went through. I don't have cargo installed yet (sadly), so I'm just using `rustc`. We start with the tests in the same module: // foo.rs fn foo(a: int, b: int) -&gt; int { a + b } #[test] fn it_adds() { assert_eq!(3, foo(1, 2)); } And move the test to it's own module, but still the same file: // foo.rs fn foo(a: int, b: int) -&gt; int { a + b } #[cfg(test)] mod test { use super::foo; #[test] fn it_adds() { assert_eq!(3, foo(1, 2)); } } And then move the tests to a separate file: // foo.rs fn foo(a: int, b: int) -&gt; int { a + b } #[cfg(test)] mod footest; // footest.rs // This name matches the `mod` above use super::foo; #[test] fn it_adds() { assert_eq!(3, foo(1, 2)); } 
I'm not sure how these benchmarks are typically done, but you might get useful numbers if you measured the compile time of a large Rust project (say, Servo), and then found the time-per-SLOC (source line of code), and then compiled some other large projects in various languages. To be as accurate as possible, you'd want to run C/C++ projects in particular through their preprocessor before counting SLOC, to get rid of dead code that isn't touched during compilation. (e.g. the Linux kernel probably #ifdefs large amounts of code to specific platforms) Even then, just keep in mind that every measurement has some degree of uncertainty. Some code is probably compiled multiple times under certain compilers, due to extenuating circumstances, and one SLOC in one language may contain more statements than another language (Haskell is an extremely dense language, each line usually doing a dozen different things). Benchmarks are fun, but cross-language ones rarely have much use that I've seen. Comparing Clang and G++ on the other hand, yields very interesting results.
Can you do that all on the GPU? I had to sort the translucent blocks furthest to closest on the CPU, I didn't think there was another way.
\#rust is good, a few people who fiddle with the project hang out on #rust-rosetta, but it's not seen much action. Feel free to pop by!
It is possible to compare them on specifics though. In particular: How does Rust's code generation capabilities compare to D's? How does Rust's verbosity compare to D's?
2 big differences i know of, [1] D only achieves safety with a garbage collector, which simplifies code at the expense of performance/determinism [2] D has more features, whilst rust seems to be going for minimalism(1 way of doing everything where possible, &amp; explicitness vs hidden magic) So it wouldn't surprise me if both made Rust more verbose than D sometimes
The main reason for Rust's current minimalism is having a stable, good core language to be able to release 1.0. There will be more scope for 'fancy' extensions after 1.0.
No, sorting is the new style, the old style just used the closest translucent face. I somehow hope there is a way to do the matrix computations for the sorting on the GPU but that might be too expensive/unrealistic.
 What I like about D is, that it feels ways lighter than Rust. In Rust I constantly fight with syntax and proving to the compiler that the way I handle data is going to be safe. The meta-programming in D rocks: it's super intuitive yet very powerful. Rust feels way more strict about everything, but gives a bigger control over data, and safety. The meta programming consists of very limited `macro_rules`, and very powerful, but complex syntax extension systems. D has a GC. That is a cardinal problem with it for a lot of applications. Another problem that I used to have (maybe it's not true anymore) with D is that it did not embrace Open Source from the very beginning. I can smell the Windows culture every time I deal with it. In Rust community everything fits my Linux world perfectly, uses Unix standards and conventions. Also, Rust made me realize how important memory safety is, and how easy it is shoot yourself in a foot with aliased mutable data. So D would be my better-Python and Rust would be my better-C. Either I'm aiming for convinced, or mission-critical tools.
I really hope that Rust will keep the "There should be one - and preferably only one - obvious way to do it" and won't go crazy adding features like some languages I could name (but won't).
I've moved the call to the test mod outside the mod and made a struct in that mod public, and it seems to work.
It's not just that. I think main issues that stop language comprehension are: A) Too much magic that makes it hard to reason about code B) Too many different language dialects 
Yeah, sure, I don't disagree. However, there are still [a lot of features](https://github.com/rust-lang/rfcs/pulls?q=is%3Apr+label%3Apostponed+is%3Aclosed) that would be nice, but aren't in scope for 1.0 (and are reasonably orthogonal).
&gt; and spawning threads for each internal request will have high overhead too. [citation needed] From basic testing rust with threads here beats the crap out of Python with greenlets.
The problem with D is the garbage collector. That kills it for a significant number of projects.
Given the runtime model, I think it is very very hard to be slower than python with rust.
&gt; Rust needs to be as usable in Windows If it is not already, it will be eventually. I don't think, Mozilla is interested in building a next-gen Servo-based browser that only works in unixy environments. 
I thought Servo was mobile only? Honest question.
There is certainly some overlap between what D and Rust are trying to achieve. Rust seems to focus more on safety (you have to opt out via `unsafe` instead of opt-in with the [`@safe` attribute](http://dlang.org/memory-safe-d.html) for functions) and for Rust (at least) this includes lack of data races. Since I hope the majority of code to be in the safe subset, the approach by Rust seems like the better choice. The "SafeD" subset is also more restrictive than "normal Rust" code simply because there is no way to annotate references with lifetime parameters. For example, you are not allowed to point into stack-allocated data structures. This is something Rust handles well via to the lifetime system. So, Rust represents a rather unique approach to memory safety without relying on garbage collection. Keeping things safe seems to be easier in Rust. For me as someone who switched from Java to C++ for performance reasons, I welcome this approach. Sometimes you really need to control the memory layout of things. Extra levels of indirections tend to increase the number of cache misses. Data locality has become very important in the age of CPUs with three levels of cache. D follows the class/struct dichotomy from C#. This is something that simply does not appeal to me. I don't like the fact that the implied indirection with class types is invisible. You have to know whether some named type was defined with "struct" or "class". IMHO this indirection should always be explicit. In Rust I would probably not create an alias for something like `Rc&lt;SomeStruct&gt;` just to safe typing because it would hide the "reference semantics" of `Rc`. Maybe this approach avoids overuse of "reference types". It certainly makes people more aware of aliasing, I think. I like the focus on "value semantics" in Rust and that move semantics in Rust is so much simpler than in C++. Move semantics in D is also simpler than in C++ but the way Rust does it is unique becase it is actually destructive. This is a good thing. It allows you to move things that are not necessarily mutable and you are not forced to invent some kind of "valid zero state" for moved-from variables of some type just so that the destructor can still run safely. Moves being destructive in Rust means that you can't use the moved-from variable and that a destructor *won't* run on it anymore. This frees you from adding a "zero" state to your type's invariants. Apart from that, I think Rust has still some way to go. I believe that D has other nice metaprogramming and usability features. But I hope that Rust will improve in these areas as well. Personally, my only pain point with Rust right now is the support for generic programming. I'm used to be able to do lots of "crazy" things in C++ to avoid code duplication and/or boiler plate code. Among other things, I relied on overloading and [ADL](https://en.wikipedia.org/wiki/Argument-dependent_name_lookup) for that. Sometimes, I feel Rust could do a better job of supporting generic programming. For example, I don't want to have to use [this double dispatch trick](http://smallcultfollowing.com/babysteps/blog/2012/10/04/refining-traits-slash-impls/) to make multiple types work well together. But I also know that this is being worked on. :)
`libgreen` was the opposite of green, being less green than `libnative`. Instead of coroutines (which generally require dedicated stacks and state switching, though Python "misuses" the word for a kind of generators), I'd suggest looking into generators and `async`/`await` abstractions built on top of them. They can be very cheap, using dozens of bytes (instead of at least a page for a coroutine stack), and state machines are generally efficient. There are some generator issues specific to Rust, around borrowing and existential input/output types, but nothing insurmountable.
The convention isn't mandatory, having the module name and the file name match is though. That's just normal Rust module stuff though. 
Pretty sure it's not. It's a research project; at best they're looking into what it would take to at some point possibly replacing Gecko.
&gt; spawning threads for each internal request will have high overhead too Maybe it's not as much as you would think? Some quick testing on my computer (Linux 3.16.1) shows ~0.01ms for spawning a rust task
As you gain more tasks spawn times should increase as system resources become more scarce. But that depends on how heavy each task is, and the resources your working in. 
Both languages aim to have the same niche of C++. But D tries to be a mixture of Java and C++ which is probably why many C++ developer don't want to use it. The default in D is GC, but you can make deterministic destructor calls too( see scoped). Where D shines is zero cost abstractions, it has many meta programming features that are actually nice to use. Personally I like the approach of Rust more =&gt; Memory safety at compile-time and GC as a library feature. It also has meta programming like CRTP, but it's not much at the moment. Also it seems like the Rust community is almost as big as the D community and the language isn't even final yet. D already has a lot of features which makes it a viable choice for many projects while Rust is still missing many nice features. I think the future of Rust looks very good, but I wouldn't start a serious project right now. 
There are actually parts of D, the C header file to D file generator for example, that aren't released because they are proprietary to DigitalMars. D may feel lighter at coding time, but you pay for it at runtime. Rust is the opposite, it feels syntactically heavier but will probably end up being a much faster language in the long run. That being said, I think comparing D to Python is a crime.
unless you use hardware GC. completely different memory layout. FPGA based parallel flipflops that dynamically configure themselves for zero latency allocation and deallocation. and super safe.
Notice that Piston does not restrict you to a specific graphics API. It tells you when to render, but not how to render. You can use opengl_graphics (2D back-end for rust-graphics), gfx-rs (low level safe 3D graphics abstraction) or gl-rs (OpenGL). opengl_graphics works with gl-rs and hgl-rs. With Piston comes also rust-image, which is a 100% pure Rust library for image formats and image processing. With a few extra lines you can fly around with a 3D camera. Check the examples for more information: http://www.piston.rs/ Piston is used in [Conrod](https://github.com/pistondevelopers/conrod) and [Hematite](https://github.com/pistondevelopers/hematite).
What kind of code do you think about when you say "syntactically heavier"? The new lifetime elision rules did help a lot, IMHO.
I think generators can assist in streaming data from a connection, but they won't be able to handle a TCP connection directly. Or maybe I'm dense and can't think of a way. Given that Linux is the only operating system I care about, the issue of green-threads is also less relevant than it was a decade ago, as Linux can now handle 100's of thousands of threads that are mostly waiting on IO without breaking a sweat. 
JavaScript will use generators to solve the callback hell issue. [Here's more info](http://blog.stevensanderson.com/2013/12/21/experiments-with-koa-and-javascript-generators/).
I don't think Go blocks for all I/O. I was under the impression that some calls blocked and required a whole thread to service, but network I/O at least was handled with select/poll/whatever, so only requires a single network thread for many many blocked calls at the Go language level: http://morsmachine.dk/netpoller
Here's the HN link btw! https://news.ycombinator.com/item?id=8243277
Sorry, I could have been more clear. I realize that under the hood it's using non-blocking calls to achieve what I'm describing. What I was driving at is that if I write a goroutine that reads from a channel, from a flow-control perspective that read call is going to block -- the asynchronous nature of the underlying call is an implementation detail.
What is the difficulty of doing non-blocking IO on Windows? It might use a different system than it does on Unix-like systems, but the async IO apis are there.
While it is true, this specific generator is legacy and not recommended to use anyway. Most up to date alternative (https://github.com/jacob-carlborg/dstep) is libclang based and Boost-licensed.
// Programmer who does D development for a living with a casual interest in Rust here. In my opinion main difference between D and Rust is base design philosophy. Rust is more purist and has good theoretical basis while D is more use case based and willing to act "dirty" if it is considered practical. D also feels easier to just jump in and start coding if your already familiar with C language family which is a big advantage for smaller projects. At the same type very strong type system and consistent rules make Rust extremely tempting for larger scale applications. I absolutely admire many of Rust features (like ownership system or traits) but using it in production does feel like a serious commitment as it is rather hard to get used to new concepts iteratively sticking only to familiar stuff initially. Quite ironically I was only able to quickly grasp some of advanced concepts only because of previous D experience of a similar flavor. Not going to call this wrong approach but this is language trade-off one needs to be aware of when thinking about marketing. Still curious how it all will look in a year or two from now though.
I hope you can excuse me for giving you unsolicited advice. It seems like a major deficiency in your library that your `draw` functions require so many arguments. One of the main attractions of IMGUI libraries is the brevity and simplicity of their client code. Although Rust lacks keyword arguments, default arguments and function overloading, I can still see scope for improvement. Many of your function arguments could be eliminated, reducing the typing burden for your user. Specifically, for `button::draw`: * I suspect that `&amp;RenderArgs`, `&amp;mut Gl` and `&amp;mut UIContext` are unlikely to change independently of one another. Could the first two arguments be lumped into `UIContext`? * It may be prudent to decompose the `Point` argument into two or three `f64` arguments. `Point::new()` represents twelve unnecessary characters which will need to be typed for every widget invocation. * Styling arguments such as framing, colors, fonts and font sizes are likely to fall back to a program-wide default in most client code. I'd recommend passing them all in as an enum `Style`, with discriminants `DefaultStyle`, `CustomButtonStyle(font, color, ...)`, and so on. Alternatively, all styling parameters could be made 'global' state in the UIContext, with a method like `with_style(new_style, callback)` to manage the current style in a straightforward way. The "before and after" could easily look like this: button::draw(&amp;screen, &amp;mut gl, &amp;mut ctx, ok_button_id, Point::new(x, y), width, height, Frame(frame_width, frame_color), Color::new(r, g, b, a), Label("Ok", font_size, font_color), callback); button::draw(&amp;mut ctx, ok_button_id, x, y, width, height, "Ok", DefaultStyle, callback);
I'll probably be there!
Yay! 
Didn't realize there were alternatives, you'd think the D community would advertise them on their wiki. (shrug) Thanks for the link. EDIT: it requires Tango, I thought phobos had kinda won the day....
Hardware GC?
What's "immediate-mode" and what difference did you expect to see between algebraic data types and OOP for UI programming?
The only reason why Tango/Phobos was an issue back in D1 days was because they used different runtimes, and you couldn't use both in the same program. When I and others ported Tango to D2, this limitation was resolved. With D's package manager, Tango is merely a large 3rd party dependency.
Immediate-mode GUI libraries are procedural, rather than object-oriented. Each widget is represented by a function call which is invoked zero or one times per frame. That function call takes a description of the widget (which generally can't change too much from frame to frame without breaking the library), and a unique ID which the library uses to manage a tiny amount of internal state for the widget. IMGUIs are mostly intended to be used for in-game level editors, which need to be trivial to implement, while still being fairly powerful. It's usually more pleasant to write a bit of almost-declarative IMGUI code, rather than trying to deal with a full-featured UI library like GTK+ or FLTK. IMGUIs are not suitable for standalone GUI applications, like word processors or web browsers.
Off topic question, how does one choose between Phobos and Tango?
Thanks for this description! I thought the lack of objects looked cumbersome, with the gigantic draw-calls, but your description makes a lot of sense. It feels kinda "plug-n-play" when you don't have to keep track of objects, and just do draw calls. 
One thing that came to my mind is [PortAudio](http://portaudio.com/) It would be nice to have a Rust library that provided audio I/O and would work seamlessly with Cargo.
http://lambda-the-ultimate.org/node/4561 has a good discussion of immediate mode guis 
You could start making Rust bindings from a C library. I don't think anybody will tell you off because the code is not perfect. Give it a try, ask for help if you get stuck and learn from the feedback :-) @long_void suggestion could be your starting point!
&gt; That being said, I think comparing D to Python is a crime. What I meant is: if I don't care (that much) about performance and predictability, and want to get something done quickly I would consider D over Rust. Especially that #!/usr/bin/rdmd makes D programs feel like a real scripts. A a side note: I really dislike Python and I think it's terrible for everything. I'm not going to argue about it. It's purely my opinion.
I used that repo. Thanks. Gathered all files in a big one. here http://pastebin.com/ugaPUqZt. Fixed a bunch of issues caused by pasting all that code. Now theres a frakton of "only irrefutable patterns allowed here". How to fix those?
I'll try to do something with it
Yeah, but that would be the first C to Rust binding that I would have made
http://www.lowes.com/images/LCI/Planning/HowTos/Legacy%20articles/compactor1.jpg
You don't need to chose, Tango for D2 is just another 3d party library used side by side with standard one.
I feel like nonblocking IO problem is only a small part of this. I mean to make a non blocking socket is trivial untill it comes to Windows. But what i want to say is that Python-3's async core is a great abstraction yielding coroutines let me write some interested feed handlers for a project in work. Green Threads are the way to go for scalability and in general, but scalability and speed are mutually exclusive and finding the balance depends on what you need to achieve.
You can use a recursive macro that evaluates the arguments into local variables before doing anything with them. It has to be recursive because there is no other way to get 'new' identifiers for each argument. The key step is having a rule like this in `memo`: (: $fun:ident ($arg: expr $(, $rest:expr)*), $store_name: ident, $($vars: ident, )*) =&gt; {{ // recursively evaluate all the arguments into local variables. // hygiene means all these `x`s are different. let x = $arg; memo!(: $fun($($rest),*), $store_name, $($vars,)* x, ) }}; (With the base case being a rule that handles `$fun: ident()`.) [playpen][pp] [pp]: http://play.rust-lang.org/?run=1&amp;code=%23![feature%28macro_rules%2C%20trace_macros%29]%0A%0Amacro_rules!%20memo_init%28%0A%09%28%24store_name%3Aident%3A%20%24args_t%3Aty%20-%3E%20%24ret_t%3Aty%29%20%3D%3E%20%28%0A%09%09local_data_key!%28%24store_name%3A%20std%3A%3Acollections%3A%3Ahashmap%3A%3AHashMap%3C%24args_t%2C%20%24ret_t%3E%29%0A%09%29%0A%29%0A%0Amacro_rules!%20memo%28%20%0A%20%20%20%20%2F%2F%20internals%0A%20%20%20%20%28%3A%20%24fun%3Aident%20%28%20%29%2C%20%24store_name%3Aident%2C%20%24%28%24vars%3A%20ident%2C%20%29*%29%20%3D%3E%20{{%0A%09%09use%20std%3A%3Acollections%3A%3Ahashmap%3A%3AHashMap%3B%0A%09%09match%20%24store_name.get%28%29%20{%0A%09%09%09None%20%3D%3E%20{%20%24store_name.replace%28Some%28HashMap%3A%3Anew%28%29%29%29%3B%20}%0A%09%09%09_%20%3D%3E%20{}%0A%09%09}%0A%09%09%0A%09%09let%20arg_key%20%3D%20%28%24%28%28%24vars%29.clone%28%29%29%2C*%29%3B%0A%09%09let%20val%20%3D%20%28*%24store_name.get%28%29.unwrap%28%29%29.find_copy%28%26arg_key%29%3B%0A%0A%09%09match%20val%20{%0A%09%09%09Some%28x%29%20%3D%3E%20x%2C%0A%09%09%09None%20%3D%3E%20{%0A%09%09%09%09let%20val%20%3D%20%24fun%28%24%28%24vars%29%2C*%29%3B%0A%09%09%09%09let%20mut%20memoized%20%3D%20%24store_name.replace%28None%29.unwrap%28%29%3B%0A%09%09%09%09memoized.insert%28arg_key%2C%20val.clone%28%29%29%3B%0A%09%09%09%09%24store_name.replace%28Some%28memoized%29%29%3B%0A%09%09%09%09val%0A%09%09%09}%0A%09%09}%0A%20%20%20%20}}%3B%0A%20%20%20%20%28%3A%20%24fun%3Aident%20%28%24arg%3A%20expr%20%24%28%2C%20%24rest%3Aexpr%29*%29%2C%20%24store_name%3A%20ident%2C%20%24%28%24vars%3A%20ident%2C%20%29*%29%20%3D%3E%20{{%0A%20%20%20%20%20%20%20%20%2F%2F%20recursively%20evaluate%20all%20the%20arguments%20into%20local%20variables.%0A%20%20%20%20%20%20%20%20%2F%2F%20hygiene%20means%20all%20these%20%60x%60s%20are%20different.%0A%20%20%20%20%20%20%20%20let%20x%20%3D%20%24arg%3B%0A%20%20%20%20%20%20%20%20memo!%28%3A%20%24fun%28%24%28%24rest%29%2C*%29%2C%20%24store_name%2C%20%24%28%24vars%2C%29*%20x%2C%20%29%0A%20%20%20%20}}%3B%0A%20%20%20%20%0A%20%20%20%20%2F%2F%20the%20user-interface%20of%20the%20macro%0A%09%28%24fun%3Aident%20%28%24%28%24args%3Aexpr%29%2C*%29%2C%20%24store_name%3Aident%29%20%3D%3E%20{{%0A%09%20%20%20%20memo!%28%3A%20%24fun%28%24%28%24args%29%2C*%29%2C%20%24store_name%2C%20%29%0A%09}}%0A%29%0A%0Amemo_init!%28test%3A%20%28u8%2C%20u8%29%20-%3E%20u8%29%0A%0Afn%20foo%28x%3A%20u8%2C%20y%3A%20u8%29%20-%3E%20u8%20{%0A%20%20%20%20println!%28%22foo%20called%20with%20{}%20{}%22%2C%20x%2C%20y%29%3B%20%0A%20%20%20%20x%20%2B%20y%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20memo!%28foo%28{%20println!%28%22only%20prints%20once%22%29%3B%200u8%20}%2C%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%202u8%29%2C%0A%20%20%20%20%20%20%20%20%20%20test%29%3B%0A}%0A
The gigantic draw calls aren't typical (outside of this library).
&gt; Although Rust lacks [...] default arguments and function overloading Are these features planned? Not having those is pretty much a dealbreaker for me.
AnyMap was just a radically simplified version of Teepee’s header representation technique; this TypeMap is basically just somewhere in between the two.
Neither are planned right now. I'm not sure what the temperature is on default arguments, but it was pretty cold towards function overloading, last I checked. Both of these changes are things that'd have to go through the RFC process, so if they're important to you, maybe work on a proposal? I don't think we've actually had one for either of those yet.
Nice. I really don't like when softwares (like postfix or apache) have huge and complicated configuration files. A much better solution (IMO) is to directly writing the behaviors of your server applications in a programming language, just like when you write a website with node. 
Perhaps he can even make some shortcut methods ctx.button(ok_button_id, x, y, width, height) .color(r, g, b, a) .label("Ok", font_size, font_color) .frame(frame_width, frame_color) .draw(callback)
I think the concern of rust devs is that, any cross-platform API cannot given the best possible performance on every platform, since the way epoll and iocp work are different. At least that is how I understood the statement in the discussion to remove libgreen, I may be wrong.
It looks so beautiful. Now some naïve questions, why is the sleep placed inside that particular state?
Thanks! I like how NodeJS allows to build custom HTTP servers by providing simple tools to do so. That's kind of what I'm going for.
The idea is that the application is suspended and not taking up CPU cycles when it does not need to render or update.
I don't agree about apache, but if you look for a good smtp server with a simple configuration and nice management tools, then I can't praise opensmtp enough.
/r/playrust
Thank you!
 ctx.button(ok_button_id, x, y, width, height) .with_color(Color::new(r, g, b, a)) .with_label(Label("Ok", font_size, font_color)) .with_frame(Frame(frame_width, frame_color)) .draw(callback) This is just crying out for default/keyword args. I really hope the language gets these, eventually. This is the kind of long winded workaround to language omissions/half broken features we're trying to escape from C++. not a priority for the core language it seems, but if Rust wants to cover the full spectrum of use cases eventually (whilst not nullifying core pillars of zero overhead abstractions etc).. it will be useful
`Vec` currently [uses a growth factor of 2](https://github.com/rust-lang/rust/blob/43c26e6041b811b322f49933e8d0f9455cb7ea2b/src/libcollections/vec.rs#L1584), fwiw.
We had one for function overloading on arity.
Benchmarking different growth factors seems to be a good idea.
The document claims that the growth factor of 2 is "rigorously the worst possible", and that GCC "staunchly" uses it, while other compilers don't. I would rather believe that there's some undiscussed reason for it, instead of assuming that the GCC developers simply don't know better. Is anyone aware of the reasons behind the decision?
They also optimize for jemalloc, probably worth looking. 
[libc++](http://llvm.org/viewvc/llvm-project/libcxx/trunk/include/vector?revision=215332&amp;view=markup#l941) also uses 2.
(Yes, I do know R, but I also want to try my hand at Rust as a language for computation and simulation)
Is joystick support a planned feature for piston?
&gt; The document claims that the growth factor of 2 is "rigorously the worst possible" Well to be fair it doesn't just claim it, it explains the reasoning behind the claim: with a growth factor of 2, as the vector grows it can never reuse its own previous allocations (ignoring fragmentation) because step n will always need strictly more memory than all its previous allocations. Vector reallocation will thus cause the vector to "creep forward" in memory and contribute to increased memory fragmentation. Here's a usenet thread on the subject: https://groups.google.com/forum/#!topic/comp.lang.c++.moderated/asH_VojWKJw However while TFA claims GCC (libstdc++)'s the only one who's remained on a growth factor of 2, clang (libc++) seems to have the exact same behavior. MSVC 7 apparently switched to a growth factor of 1.5. And in the interest of fairness, here's a recent-ish libstdc++ thread on this subject: https://gcc.gnu.org/ml/libstdc++/2013-03/msg00058.html where the sole response includes &gt; Sean Parent visited TAMU last week, and this topic was the subject of an extensive discussion. The conclusion of the discussion was that, from their measurements, any factor less than 2 is empirically worse though without providing any more information or evidence
Let's say your growth-factor is `g`. In a dynamic array this means that after `n`insertions for large enough `n`, every element was on average copied/moved g/(g-1)^2 times (measured directly after the reallocation. This means that for n = 2, every element was moved two times on average. for n = 3, it was moved 3/4 times on average and for n = 1.5 it was moved six times. If you now assume that the type cannot safely be moved and has a very expensive copy-constructor, you can quickly come to the conclusion, that a growth factor of two is a valid decision. Also: The GCC-guys apparently value binary backwards-compatibility over everything else, so this may be another reason.
There was some discussion about this on the Rust bug tracker some time ago: https://github.com/rust-lang/rust/issues/4961
[kiss3d](https://github.com/sebcrozet/kiss3d) may be what you are looking for, but I don't know if you can dump the output into an image.
What's the use case for something like `"abcd".safe_slice_to(5)`?
I'm not sure exactly what your needs are, but there's Conrod, an immediate-mode GUI lib. Here's a recent blog post about it: http://blog.piston.rs/2014/08/30/conrod-update/ On github: https://github.com/PistonDevelopers/conrod
I wouldn't call these 'safe' as that has a very specific meaning in Rust. 'Eager' is a good word, though.
Ruby also has the above behavior.
but Python will still raise an exception for the 0 stride case, which doesn't apply to rust primitive slices.
I think that this is actually a good way to go; slicing methods can *not* ignore the out-of-bound tests (otherwise they would be unsafe), and having a useful behavior instead of failing may be better both for ergonomics and performance.
I don't think empty string is a good idea, its the same reason we dont use NULL or any sigil, its not strongly typed. I would say return an Option with the 'safe' version, maybe call it `slice_from_opt` 
You need to load OpenGL functions, which sdl2_game_window or glfw_game_window does for you. https://github.com/pistondevelopers/sdl2_game_window See the piston-examples repo if you don't know how to use Piston. Alternative you can load the OpenGL functions manually, see https://github.com/bjz/gl-rs Also, talk to people in the #rust-gamedev IRC channel (link in the sidebar -----&gt;). Also, I am looking for somebody to help improve the useability of opengl_graphics.
The proggit thread suggests that this is out of date: http://www.reddit.com/r/programming/comments/2ezy59/facebooks_stdvector_optimization/ck4mgj7
You can always save the results to a file, and display with gnuplot.
Don't most malloc implementations assign memory out of pools of various sized chunks? Since the memory is allocated out of a different pool, how can later allocations re-use the previously vacated space, as different pools are not contiguous? The only way this particular optimization works is if a) the vector is already using the largest memory chunks, and b) nothing else is using large memory chunks that would risk fragmenting memory space in the largest pool. That, to my mind, is a pretty specific optimization - the kind that is usually addressed by a custom allocator. On the other hand, being able to memcopy when moving instead of copy constructing/deleting could be a huge win for specific types of vectors. That to me is the bigger optimization.
Yes. If you are interested, open up issues here: https://github.com/pistondevelopers/input/issues We are developing a back-end agnostic model of user input to be used across games and widgets libraries. It will make it easier to maintain generic controllers that support new hardware. If you want to help testing, go to the #rust-gamedev IRC channel or ask people (link in the sidebar ----&gt;)
You may want to take a look at [rutst-sfml](https://github.com/jeremyletang/rust-sfml), which it the Rust binding of the famous C++ multimedia library SFML.
That's cool with us because we can realloc since we don't have to run copy/move constructors, right?
I've never understood their discussion though. The argument that vector should not use a growth factor of 2 because it forces reallocation each time whereas sometimes 1.5 will allow expansion in place seems kinda silly. I would argue that instead you should review `malloc`/`realloc` so that when it gives you a memory block it tells you its *exact size*, and therefore whatever the growth factor `vector` would *always* use memory block to their utmost capacity, and as a result growth would always require reallocating anyway. Am I completely off-track ?
When I'm iteratively consuming a string by slicing it in increments, and then finding specific characters in the slices. I'm done when I don't find any more of those characters, since `StrSlice::find()` works on empty strings. It's in my WIP solution for this: http://www.reddit.com/r/dailyprogrammer/comments/2exnal/8292014_challenge_177_hard_script_it_language/ The alternative is manually checking the length on every input slice, which I guess I could do instead but hadn't considered.
It might be a direction we have to go in general if fail!() use is reduced. Take for example `.swap(i, j)`, if instead of fail!() it simply returns an error code or even a boolean to signify success, there will be a lot more errors passing silently. I think that's ok, but we must know what we're doing.
I tried running the examples and got some compilation errors so I decided to try it out later. I'll have a look again, thanks :)
I don't want to just plot things, I want to have some visualizations with shapes and all. if it was just a plot I would have calculated it in Rust and shoved it into gnuplot :)
See also https://github.com/SiegeLord/RustGnuplot
Rust is going through some breaking changes at the moment. * Make sure you install latest Rust nightly * `cargo update` * `cargo build`
I'm after exactly the same! The closest I found was [sdl2 bindings](https://github.com/AngryLawyer/rust-sdl2)
This was a small talk I gave recently at a Pittsburgh Code and Supply meetup, and I thought that some may find it interesting!
how about something like: enum EagerSliceResult&lt;'a&gt; { Truncated(&amp;'a str), Full(&amp;'a str) } impl&lt;'a&gt; EagerSliceResult&lt;'a&gt; { fn unwrap(&amp;self) -&gt; &amp;'a str { match *self { Truncated(s) =&gt; s, Full(s) =&gt; s } } } trait EagerSlice&lt;'a&gt; { fn eager_slice_to(&amp;self, len: uint) -&gt; EagerSliceResult&lt;'a&gt;; } impl&lt;'a&gt; EagerSlice&lt;'a&gt; for &amp;'a str { fn eager_slice_to(&amp;self, len: uint) -&gt; EagerSliceResult&lt;'a&gt; { if self.len() &lt; len { Truncated(*self) } else { Full(self.slice_to(len)) } } } fn main() { println!("{}", "abcd".eager_slice_to(5).unwrap()); } edit: updated as per /u/dbaupp recommendation
My hunch is that this will continue to be a problem until 1.0, which will help a lot by guaranteeing some degree of backwards compatibility.
You could always just fix the dependency yourself and submit a pull request. Rust should be getting closer to being stable, but there's still going to be a few major breaking changes.
And after forking and fixing it, you can probably override Iron's dependency on the package with your own. Not sure if Cargo works that way though.
Yeah, you can source a dependency from a specific branch using `branch = foo` just below `git = &lt;URL&gt;`. For the most part this has been enough for me, although yesterday the rust nightly was outdated and couldn't build a bunch of iron related projects (like rust-encoding).
1. Create folder `.cargo/` 2. Create file `.cargo/config` 3. Put `paths = [ "path/to/override", "another/path/to/override" ]`, etc. in `.cargfo/config`. 4. Run `cargo update` That should override the dependency without having to mess with Cargo.toml.
That's good to know! Editing Cargo.tool has been useful for me when building PRs upon PRs, but for local changes your method rocks.
Thanks for the shoutout. Usually I just make PRs to all of our dependencies when things break and hope for the best. Sometimes we end up depending on a fork for a bit if things take too long. Hopefully all of this will go away come 1.0 and once Iron and its dependencies become a bit more stable.
It always irritates me when someone is talking about a lack of Moore's Law. For now the doubling of transistors is still an ongoing trend, and in fact the very reason that we are seeing an increasing number of cores. We still have more and more transistors, but we can't put them to good use within a single core any longer. With that out of the way: This is a great overview of Rust's concurrency primitives. I'm particularly impressed that this seemed quite understandable to people new to Rust, while not including much of an introduction to the language itself. Well done!
&gt;We still have more and more transistors, but we can't put them to good use within a single core any longer. Is this not Amdahl's/Gustafson's Laws?
EDIT: I've changed the answer to reference the wiki (thanks /u/dbaupp !), and have it provide more generic information (which is much easier to keep updated). Thanks for the feedback :) I'm the author and "maintainer" of this answer, and although I take some time every once in a while to trawl this subreddit and find new or updated things to add, my time and understanding is limited! Notably, I have to do descriptions myself, and judge "outdated-ness" myself, which are both hard and potentially full of mistakes. I would appreciate help from library authors or contributors on this :)
They mention doing this as well (using jemalloc).
&gt; some degree of backwards compatibility we're taking SemVer very seriously, so it should be absolutely backwards compatible until 2.0. That said, we're only human, so bug reports welcome! I'm excited for the new release cycle to help with this.
EDIT: ok at least rust has decent macros that might streamline setting up param-builders. I dont take C++ as the ideal: it's a lot of boilerplate in C++, and the language still has ommisions/half broken features, and a lot of syntactic space is taken up sub-optimally Maybe default struct values would help.. then you could easily create parameter structs and just fill out what you need. But the best would be full defaults and keyword args (perhaps the complexity in the compiler can be reused between the two cases, struct initialisation and function arg block both having similar functionality)
I am pretty scared about the goal to reach absolute backward compatibility by the end of the year. I believe Rust team should be more careful, since backward compatibility mean we could have to deal with bad choice indefinitely. I believe there should be at least a 6 month delay between a release labeled "1.0 beta" (feature complete and strongly advertised) and the actual 1.0 release. IMO, it's necessary to take time to be sure nothing important is wrong and to have more feedback, especially from the numerous ones who are waiting a stable Rust to use it. The Rust team would continue working on the next version features during the delay.
Thanks for this. I like how you presented the high level concepts and goals rather than getting bogged down in the minutiae of the language's syntax and control structures. I hope things are going well for you in Pittsburgh!
Or it's a good idea for the same reason as `Iterator::collect()` does not return a `None` but an empty collection if the iteration does not yield a single value.
Seems right. The way I read it is 'box(ref a)'.
[Slides](https://github.com/pnkfelix/present-rust-ocaml2014/raw/master/rust-ocaml2014.pdf).
Overly complicated. Why not just return a slice? In &gt; 99% of the use cases of eager_slice you won't care whether the slice was truncated but if you care, you can still ask for the length…
&gt; Am I completely off-track ? Yes. The 1.5 growth factor has nothing to do with in-place expansion, it has to do with cases where reallocation was necessary: if the growth factor is 2, step n will always be strictly greater than steps 1..n-1, so even if the discarded space from previous allocations is contiguous, you won't be able to reuse it and your vectors will keep creeping forward in memory, increasing fragmentation and the amount of VM space needed. With a growth factor of 1.5 however, starting from a capacity 1 and resizing (with reallocation): * allocate 1.5, discard a, discarded 0 (discarded: sum of all discards *previous to this reallocation*, available for allocation) * allocate 2.25, discard 1.5, discarded 1 * allocate 3.375, discard 2.25, discarded 2.5 * allocate 5.0625, discard 3.375, discarded 4.75 * allocate 7.6, discard 5.0625, discarded 8.125 We have 8.125 memory available from previously discarded allocations, assuming they're contiguous the allocator can coalesce these previous blocks and provide memory without having to ask the OS for more, and the vector moves back in memory. Of course the allocator has to support coalescing blocks, but if it does every 4 reallocation can reuse previously allocated memory. Expansion is a different concern, also but separately covered by integrating into jemalloc.
In many, many uses of this functionality I actually *care* about truncation. I'm explicitly running off the end, but know/understand Python's behaviour and so are thus avoiding the `min(len(array), ...)` rigamarole, i.e. for this code, the translation of `x[a:b]` would have to be x.slice(a, cmp::min(x.len(), b)) 
or how about 'trunc_' (truncated)
BTW, a slicing operation should be returning a slice, and it should be implemented for more than just string literals, i.e. if taking this approach the type should be: enum EagerSliceResult&lt;'a&gt; { Truncated(&amp;'a str), Full(&amp;'a str), } and the `trait`/`impl` should be trait EagerSlice&lt;'a&gt; { fn eager_slice_to(&amp;self, len: uint) -&gt; EagerSliceResult&lt;'a&gt;; } impl&lt;'a&gt; EagerSlice&lt;'a&gt; for &amp;'a str { fn eager_slice_to(&amp;self, len: uint) -&gt; EagerSliceResult&lt;'a&gt; { ... } }
It's controllable on a type-by-type basis, since `collect` is just a thin wrapper around `FromIterator`, so it's a little weird to talk about `collect` having some specific behaviour. For most types, makes *total* sense that an empty iterator (i.e. a sequence of zero things) returns an empty collection (i.e. a collection of zero things), but there are some types where it might make sense to return some form of "failure". E.g. trying to collect into `VecWithAtLeastThreeElements&lt;T&gt;` could only return the real type if the iterator had at least 3 things in it.
Learning rust I did expect an Option. With the "!" unwrap syntax this would not be too verbose. Would this hurt performance?
Thanks, updated my post. Juggling lifetimes and generics and all that is still bit much for a quick-n-dirty example for me :)
Generators and async IO are two completely independent thing. You can have generators without async IO, and async IO without generators. Generators can make syntax for async IO look nicer, but that's it. Generators are not a solution for the lack of async IO APIs.
Are we still talking about an additional truncating function or about extending `slice`? My point is: Why would anybody who explicitly calls the auto-truncating variant care about truncation? If you care about truncation you need extra logic anyway and thus can truncate yourself.
Stack Overflow really isn't very suitable for lists of things, it's really made for Q&amp;A. As you noticed, it's hard to keep the list reasonably up to date and other people can't really comment/vote all that well on the single items in your list. And if you split it up in single answers it also gets a mess. A page on rust-lang.org or so would really be a much better place for a list of rust libraries.
Not noticeably, no. I believe `unwrap` is inlined, and it's a simple function internally.
Oh I'm all about ILP, I just hope whatever it is that eventually dethrones x86 looks something like [TRIPS](http://en.wikipedia.org/wiki/TRIPS_architecture), or some variant of EDGE architecture. Mill looks pretty good too. Unfortunately I can't find any recent evidence that TRIPS is still alive.
ah, so the merge commits get the tag. I see, thanks.
Especially since a 2.0 version could easily introduce something like the python 2/3 split
Well, not usually... it should have been on the commit, but was in the pull request description instead: https://github.com/rust-lang/rust/pull/16453
&gt; C is an M1 Garand standard issue rifle, old but reliable. I venture the author is not yet acquainted with undefined behavior.
I find things such as this article generally shallow and largely entertainment for those who "judge a book by its cover" but don't actually understand languages. The illustrations are good, at least.
Agreed, I lol’d at the Lisp thing.
I don't know if it's worth changing the growth factor from 2 to 1.5 (which means, more moving around). But the realloc stuff is actually pretty easy in Rust (if it's not already done) because in Rust you don't need to ask objects to move, you just move them by copying bits. So, while in C++ moving objects might involve nontrivial operations, in Rust it's much simpler, hence, realloc should always be fine. In C++ you would first need to check whether T in `vector&lt;T&gt;` is "trivial" and if so, realloc can be used for moving.
That is the main point of [that kind of joke article](http://www.toodarkpark.org/computers/humor/shoot-self-in-foot.html), yes. They're a bit of light humor and "here's what I think of language X", not deep insight. The kind of stuff people talk about around the coffee machine—small talk. I am sorta surprised to see the post [escape /r/ProgrammerHumor](http://www.reddit.com/r/ProgrammerHumor/comments/2f2dow/if_programming_languages_were_weapons/).
The problem with moving objects in C++ usually isn't the object itself but that there could be any number of pointers and references to the object, which would have to be patched on the fly.,
Callback pyramids of doom are not only less good looking, they are also more expensive, you need an allocation for each closure.
What does it mean when a value is 0? The test didn't pass or it was immeasurable?
&gt;statically scheduled tiny threads, it seems? More or less. The compiler bundles instructions together in such a way that two bundles that are not dependent on each other will be able to run concurrently within the superscalar architecture, even so far as not to not step on each other's toes by trying to use the same functional units at the same time. Bundles can also be dependent on the result of a previous bundle, which could be cached, thus the compiler can dictionary replace any identical bundles with a cache lookup. Branches are also very aggressively speculated and predicated. They treat pipeline stalls and such like the plague. There's more tricks ofc, but that's the gist off the top of my head.
It means that (according to eulermark.rs) the execution time of the solution was less that one nanosecond. This usually means that the compiler optimized away the routine that I wanted to benchmark, and the reported time is incorrect. This can be fixed by adding a `asm("")` in C programs or a `test::black_box` in Rust programs, which prevents the compiler from optimizing away the routine under test. This "fix" hasn't been applied because I've deprecated `eulermark.rs` in favor of [euler_criterion.rs](https://github.com/japaric/euler_criterion.rs/tree/master/problems/001) (which is less outdated, but not up to date :P)
Had a really good laugh at the JavaScript sword since I have to use it daily. :)
Author here. eulermark.rs uses a benchmarking algorithm very similar to the one used by test::Bencher, which is not good when benchmarking GC languages (You can tell that from the big error bars!), and (eulermark.rs) does almost no analysis of the collected samples. I ported [criterion](https://github.com/japaric/criterion.rs) to Rust to address these shortcomings, and created [euler_criterion.rs](https://github.com/japaric/euler_criterion.rs), which repeats these benchmarks but using the criterion approach. But those two are WIP (and currently outdated), so I'd wait a little before trying to use them. --- Also, please don't draw conclusions like "Language X is faster than Y!" from looking at these plots - these benchmarks only cover a very small fraction of the standard libraries of those languages. At best you could say something like "The X implementation of the solution S to problem P appears to be faster than the Y implementation on hardware H".
Isn't it the same in Rust? I can think of a few cases where you have several pointers to the same thing.
Thanks! Does this override that dependency in any place in "dependency graph", or only in my immediate dependencies?
No; many can be solved with 32 bit integers, and most of the rest only need 64 bit ones.
you can tell from how the author says C is reliable but complains about Java NullPointerExceptions
Alright, thanks!
How does the compiler "optimize away" the routine you're testing? Would you mind explaining what it does in that instance?
All of the project euler problems can be replaced by a program that simply returns a string. That is, there is no input and the entire execution sequence is a constant. Some compilers will be able to optimize loops that produce a constant to just the constant, that is: the compiler replaces the program to just outputting a constant. As an example, consider the many dozens of optimization passes possible with LLVM. [One blog series for Haskell](http://donsbot.wordpress.com/2010/03/01/evolving-faster-haskell-programs-now-with-llvm/) focused on trying to "evolve" the best set of flags. This resulted in wildly different assembly being emitted, but ultimately with the goal of outputting a constant. With the right flags, the compiler optimized computing the sum from 1 to 100,000,000 to just storing a constant. No loop, no computation, just a precomputed value.
You better not be dissing A Brief, Incomplete, and Mostly Wrong History of Programming Languages here!!
You don't really get Cargo with 0.11, and it's becoming the de facto way to handle dependencies. Cargo.lock allows you to keep the exact commits that your dependencies were on when they compiled. That way you can keep using a certain nightly and only update when you need a bugfix/feature that came after a Rust upgrade. Also, Rust now has [breaking-change] labels for backwards incompatible commits, with instructions for fixing your code, which should make Rust upgrades less painful than ever :).
You mean something like http://arewewebyet.com/?
The timing loop looks like this: let start_ns = time::precise_time_ns(); for _ in range(0, iterations) { routine_under_test(); } let end_ns = time::precise_time_ns(); (The average execution time is `(end_ns - start_ns) / iterations`) If `routine_under_test()` is a "pure" function (doesn't do I/O, doesn't allocate heap memory, i.e. no side effects) and its return value is "easy" to compute, the compiler may replace `routine_under_test()` with its actual return value. For example, if `routine_under_test()` is `range(0i, 10).sum()` (which returns `45`), the compiler may reduce the timing loop to: let start_ns = time::precise_time_ns(); for _ in range(0, iterations) { 45; } let end_ns = time::precise_time_ns(); On the next optimization pass, the compiler will see a for loop that does nothing, so it may remove the loop too: let start_ns = time::precise_time_ns(); let end_ns = time::precise_time_ns(); Now the elapsed time (`end_ns - start_ns`) will remain constant regardless of the number of `iterations` (that's the signal that tells you that the compiler optimized the routine away), and the reported average execution time will be a very low value. To workaround this optimization, you can introduce a `test::black_box`: fn routine_under_test() { let mut n = 10i; test::black_box(&amp;mut n); range(0, n).sum() } Now the compiler can't reduce `routine_under_test` because it "can't" see what the `black_box` call is doing, that call may change the value of `n`, so now program will have to execute the function at runtime.
I think it overrides them anywhere, but I don't know the internals of Cargo, so don't take my word for it.
Why wouldn't you just spend more time rendering? 
https://github.com/rust-lang/rust/wiki/Community-libraries#web-programming
Is it possible to see the code that is being benchmarked?
https://github.com/japaric/eulermark.rs
I'd think so[0], but I'm way too lacking in knowledge on the subject so I preferred leaving it for others to discuss. [0] it is also a problem in many GC'd languages, requiring quite a bit of work when trying to switch from non-moving to moving GC.
&gt; Also, please don't draw conclusions like "Language X is faster than Y!" from looking at these plots Honest question, then why did you do them?
C++'s approach is to just let the user keep track of that. I.e. it's the programmer's responsibility to not have any pointers into the vector when it reallocates. The issue with C++ is actually the moving: an object can have an overloaded move constructor (and an overloaded copy one), so code that moves things in memory has to call that constructor if it exists, meaning realloc (which just memcpys) is invalid. (It may be that the move constructor is just updating pointers or some such, but this isn't necessarily the case.)
It requires unsafe code to be able to move an object while there is a reference to it.
FYI, there has been https://github.com/jeremyletang/rust-portaudio for some time.
What system limits did you change to get such large number of threads running concurrently? use std::io::timer::sleep; use std::time::duration::Duration; fn main() { for i in range(0i, 50000) { println!("{}", i); spawn(proc() { sleep(Duration::hours(1)); }); } sleep(Duration::hours(2)); println!("All spawned"); } This reliably fails at around 32K threads on my laptop with 4G of RAM. I tried this with 'ulimit -u 1000000' and 'echo 1000000 &gt; /proc/sys/kernel/threads-max'.
That caveat that you quoted should go on any benchmark ever, so this question is pretty moot.
So we can see that "The X implementation of the solution S to problem P appears to be faster than the Y implementation on hardware H" 
So we can stop saying that benchmarks are useless?
Benchmarks are not useless, people need to just stop thinking that benchmarks are wholly certain measures of a languages speed. 
Laptops and mobile phones has limited battery life and desktop computers are running other tasks. You set a maximum number per frames that suits the application, if you want it to render more frames then you set it higher.
Definitely. If you have git, `rustup.sh` could be modified to grab the commit history between the two git commit hashes and grep for `[breaking-change]` (preferably saving it to a file for later).
[This?](http://en.cppreference.com/w/cpp/experimental/optional) &gt; After reviewing national body comments to N3690, this library component was voted out from C++14 working paper into a separate Technical Specification. It is not a part of the draft C++14 as of N3797. 
Yeah, I completely agree! What I say is that generators are a possible solution to a different problem (the problem that you mention), not for the lack of async IO APIs.
I had assumed /r/cpp would be a better place to ask C++ questions. Any reason why you chose /r/rust?
I saw that mentioned here. If I head over there I suspect some people might want to start investigating what rust is to begin with.
Option is currently in the [Library Fundamentals TS](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4082.pdf) under the name std::experimental::optional and will be merged into C++17. Result is [proposed for C++17](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4109.pdf) as std::expected. The implementations already exist in boost and other libraries. Unfortunately they are not so convenient to use as in Rust due to the lack of language support for pattern matching and sum types.
Great talk, clarified some of my confusions about Arc (etc). "The power of Rust is that it's impossible to use it incorrectly". This. I usually say "Rust doesn't trust the programmer", but this seems like a more effective way of saying it. `Option`/`Result`/ownership/etc all seem to be centered around this — Rust will try and reduce verbosity where it can, but not where stuff is disambiguous, which gives it the best of both worlds.
have you looked at http://lamsonproject.org/ ? While a plain SMTP server is interesting to learn rust etc., something like Lamson might be useful as a library in practice. The problem with SMTP is how much configuration there is to "get it right". Removing that bloat of configuration and replacing it with code as lamson tries to do is a good goal. 
Option's advantage over null is that it's a real type the compiler can enforce - you can't plug in a None where you were expecting a T, you have to specify Option&lt;T&gt;. None is not transparently dereferenceable, which forces you to deal with the None case, as opposed to other languages with nullable values where a null deref can happen almost anywhere. Note that the proposed `!` syntax, while not completely doing away with this advantage, weakens it considerably, making a None 'deref' exactly as syntactically expensive as a possible null deref in C++ (foo! vs *foo). 
Right, it isn't nullable by user, because compiler prevents misuse. But on the machine it's optimized into null, correct? It acts as a better nullable type (like [Type?](http://msdn.microsoft.com/en-us/library/1t3y8s4s%28v=vs.80%29.aspx) from C# if I'm not mistaken). What is the propsed `!` syntax? Is that like [safe invoke operator](http://groovy.codehaus.org/Operators#Operators-SafeNavigationOperator%28?.%29) from Groovy?
given that the C++ equivalent hasn't been standardised yet... imagine how nice it would be if it could coevolve an interface as close as possible to the rust version
It's not optimized into null for all types. Most notably not for those where `0` is a valid value (e.g. int). In general `Option&lt;T&gt;` is like any other enum and requires an extra tag word. For some types like `char` you can do interesting optimizations, because not all bits are used. While `0` would be a valid value, `None` can be represented by e.g. `0xff000000`. If I correctly understood the safe invoke operator, the `!` syntax is pretty much the opposite: "Fail if it is None". It is proposed syntactic sugar for `Option::unwrap()`.
&gt; people might want to start investigating what rust is But that is good for Rust! :D
&gt; But that is good for Rust! You're also right.
It's to do with nesting: with callbacks, each callback has to be erased, which means all of its captures have to be boxed. On the other hand, nested generators are static dispatch, there's no indirection for nesting. You might want to box the top-most layer, but it's not as bad as callback hell.
Why was libgreen so heavy? And when will M:N threading come to Rust?
Ah, so this is based on coalescing storage. From [this](https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919?_fb_noscript=1) I had gathered that jemalloc implemented size classes which, as I understood, meant different pages used for different sizes of objects: - Small: [8], [16, 32, 48, ..., 128], [192, 256, 320, ..., 512], [768, 1024, 1280, ..., 3840] - Large: [4 KiB, 8 KiB, 12 KiB, ..., 4072 KiB] - Huge: [4 MiB, 8 MiB, 12 MiB, ...] Do you happen to know how coalescing works in this scenario ? I had assumed, perhaps naively, that a change of size-class would necessarily imply a re-allocation (in another page).
Note that writing this code: fn call_me(f: |&amp;: int|) { f(5) } Gives me: &lt;anon&gt;:5:15: 5:23 error: unboxed closure trait sugar is experimental &lt;anon&gt;:5 fn call_me(f: |&amp;: int|) { ^~~~~~~~ &lt;anon&gt;:5:15: 5:23 note: add #![feature(unboxed_closure_sugar)] to the crate attributes to enable &lt;anon&gt;:5 fn call_me(f: |&amp;: int|) { However adding the feature gate leads to: &lt;anon&gt;:6:15: 6:23 error: cannot use unboxed functions here &lt;anon&gt;:6 fn call_me(f: |&amp;: int|) { This leads me to think that an easy syntax is being planned but either it's not working yet or I don't know how/when to use it (of course I could be wrong, it's just a supposition). 
Huh. Me too. I have based this assumption on the limits of VMem to support the stack as well as the ulimit settings. RLIMIT_NPROC should be the authoritative answer on how many threads can exist. This value in my procs is well above 100k. I've never actually tried to go past 32k before, but there is clearly some limit. Having set pid_max, threads-max, and even setting the stack-size to 16k, I get the same result, it stops reliably at 32755, whose significance I cannot devine. Here is my test c code: https://gist.github.com/rrichardson/076f504d6c7c495d9616 &gt; clang -pthread main.c -o cthreads **edit** Found it! strace ./cthreads gave me a bit more of a clue... mmap(NULL, 16384, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0x7f2eab330000 mprotect(0x7f2eab330000, 4096, PROT_NONE) = -1 ENOMEM (Cannot allocate memory) so I looked around for values in the kernel relating to memory mapping and found: vm.max_map_count = 65530 I changed it to 262120 (4x previous value) and blammo. Worky worky. 
Just for reference, I tried doing the "same thing" with GHC using associated types, and it works: {-# LANGUAGE FlexibleInstances, TypeFamilies #-} class Fn f where type Arg f call :: f -&gt; Arg f -&gt; IO () instance Fn (a -&gt; IO ()) where type Arg (a -&gt; IO ()) = a call = ($) call_me :: (Fn f, Arg f ~ Int) =&gt; f -&gt; IO () call_me f = call f 5 main = call_me $ \a -&gt; print a This is relevant only in the sense that if GHC *hadn't* been able to infer the type, then I wouldn't expect Rust to be able to do so either, but the reverse isn't necessarily true (just because GHC can doesn't mean that Rust also should, although it would sure be nice). (Haskell has built-in closures rather than a type class, so I just mocked it as the `Fn` class which is implemented by all built-in functions, with the output type fixed to `IO ()` for simplicity. The important part is only whether it can infer a type for `a`, which it apparently can.) (Rust's `Fn` will also use associated types once we have them, and the current formulation is supposed to be equivalent apart from syntax.)
I'm not familiar with Alexandrescu 's `expected&lt;T&gt;`. I tried googling it, but couldn't find the answer to my question: Does it support compiler warnings if the result isn't used, the way rust does?
Wasn't aware of this project. Will definitely have a look :-) The Rustastic SMTP project is indeed great to learn Rust. But the primary goal is to build a robust and fast SMTP server that is suitable for commercial use (even though it will stay open source). I was thinking of providing a fluent interface for configuring via code instead of configuration objects or files. This may well become the default way to configure stuff eventually. If you have ideas on how it should work or feel, I'd be glad to hear them :-)
With GCC and Clang you can mark every function returning `expected` with attribute `warn_unused_result`, but you can't use that attribute on the type itself.
Optional was removed at the last minute: http://en.cppreference.com/w/cpp/experimental/optional
It's in his [Systematic Error Handling talk](https://isocpp.org/blog/2012/12/systematic-error-handling-in-c-andrei-alexandrescu), which is really great. 
Tests should be isolated, so there's no reason for them to be run in a particular order. They even run in parallel, AFAIK.
It's currently the syntax for two reasons: - we need *some* syntax to implement them, even if it's not the final syntax - it has to be different to what we have now so that we can migrate Furthermore, you can actually use the `|...| -&gt; ...` syntax as sugar for trait bounds: fn call_me&lt;F: |&amp;: int|&gt;(f: F) {
That's trying to write `fn call_me(f: Fn&lt;(int, ), ()&gt;)`, and does actually work when passing as a trait object properly: #![feature(unboxed_closures, unboxed_closure_sugar)] fn call_me(f: &amp;|&amp;: int|) { // (*f)(1) // #16929 f.call((1,)) } fn main() { call_me(&amp;|&amp;: x| println!("{}", x)) } [playpen][pp] [pp]: http://play.rust-lang.org/?run=1&amp;code=%23![feature%28unboxed_closures%2C%20unboxed_closure_sugar%29]%0A%0Afn%20call_me%28f%3A%20%26|%26%3A%20int|%29%20{%0A%20%20%20%20%2F%2F%20%28*f%29%281%29%20%2F%2F%20%2316929%0A%20%20%20%20%0A%20%20%20%20f.call%28%281%2C%29%29%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20call_me%28%26|%26%3A%20x|%20println!%28%22{}%22%2C%20x%29%29%0A}%0A
Tests are run in approximately alphabetical order (based on their full module path), but are run in parallel (by default) so scheduling differences means the order isn't always this. However, the testing library doesn't make any guarantees about this order and so it cannot be relied on. (You can run with `RUST_TEST_TASKS=1` to have no parallelism for tests.)
Sure why not, but I am not OK with externally testing private interfaces as if they were public. Have a test.rs for your public api, and some quick #[cfg(test)] functions for private API. Having a public test for private functionality is a code smell, in my opinion.
But wouldn't it be useful and convenient if the tests of a basic module would run first and the tests of all the other dependent modules would run afterwards? That way you would get the relevant failure first and not mixed in the middle of lots of other errors caused by it. This can also apply to functions in a module. Or more precisely: It doesn't matter in which order the tests *run*, but it would be convenient if the results were presented in order.
In irssi, you can do /network add -usermode +p mozilla to make irssi set the mode automatically on connect. Replace mozilla with the name you have for the network if it's something different.
That doesn't make sense; why wouldn't one use the normal testing infrastructure to test functions? A `#[cfg(test)]` function will just sit there unused.
For those like myself who were excited upon reading this, be warned it [doesn't support heap allocation](http://ivorylang.org/ivory-concepts.html): &gt; Ivory does not allow nullable pointers, unbounded memory access, or heap allocation. These restrictions are made with safety and security in mind.
what do you mean by "A #[cfg(test)] function will just sit there unused."? Is a `#[cfg(test)]` function not be executed by the testing infrastructure generally? I guess that makes sense because it is only a compile-time-check not special in any way. I just dont see how publically testing private functionality is a good idea in any case. Sure add `assert!()`s or what have you to make sure your data is correct before during and after the execution, but that sort of stuff shouldnt be made publicly available. Am I missing the mark here and completely wrong? Thats how I was taught to test, publicly test public APIs, and privately test private APIs. With the `test.rs` approach, you cant test private APIs because its a public file.
Even setting it to 1M doesn't help me. I also set vm.overcommit_memory. Also, after spawning around 25K threads, it takes much longer to spawn more. It also made the OS kill some other processes in the system. I guess the conventional wisdom of not spawning very large number of OS threads is a good thing to go by.
&gt; what do you mean by "A #[cfg(test)] function will just sit there unused."? &gt; &gt; Is a #[cfg(test)] function not be executed by the testing infrastructure generally? I guess that makes sense because it is only a compile-time-check not special in any way. Yes, `#[cfg(...)]` just conditionals includes that function in the final output, it doesn't do anything special beyond that. You need `#[test]` for a test to actually be created and run. &gt; Thats how I was taught to test, publicly test public APIs, and privately test private APIs. With the test.rs approach, you cant test private APIs because its a public file. Are you saying `test.rs` as a file completely separate to the main crate, i.e. one that has a `extern crate your_lib;` and has to be compiled separately to the normal `rustc --test lib.rs` for tests to run? As in, you're suggesting: - you can use in-crate `#[test]`s for anything you want - only use out-of-crate `#[test]`s for public APIs If so, this is the only thing that *can* work in Rust, since private items cannot be accessed elsewhere anyway, and, this is what I was meaning anyway. &gt; publically testing private functionality &gt; &gt; that sort of stuff shouldnt be made publicly available &gt; &gt; publicly test public APIs FWIW, it is nonsense to have a *public* test, even for public functionality. Why on earth would any external user wish to independently execute a test for another library?
.. pretty sure there's a channel mode for that, too.
Whoops my bad meant #[test] not #[cfg(test)] I know thats the only way that can work in rust, but OP has asked multiple times how to have a out-of-crate #[test] for private APIs, which I think is not a good idea. I didn't mean public test as in the same way as `pub` code, I just meant out of crate. Sorry that was misleading. Sorry for not quoting and formatting, on mobile.
Failure is very complex and involves a *lot* of code generation. The `unwrap` method and anything else using `fail!()` is a very large amount of code.
A fairly straight translation would be something like the following: (using the C# names, which aren't the idiomatic Rust names, and only giving one, for brevity) trait IVisible { fn draw(&amp;self); } struct Visible; impl IVisible for Visible { fn draw(&amp;self) { println!("I'm showing myself.") } } struct Invisible impl IVisible for Invisible { fn draw(&amp;self) { println!("I won't appear."); } } struct GameObject { // a "trait object" v: Box&lt;IVisible&gt;, } impl GameObject { fn new(v: Box&lt;IVisible&gt;) -&gt; GameObject { GameObject { v: v } } } struct Building { obj: GameObject, } impl Building { fn new() -&gt; Building { Building { obj: GameObject::new(box Visible) } } } struct Trap { obj: GameObject, } impl Trap { fn new() -&gt; Trap { Trap { obj: GameObject::new(box Invisible) } } } A trait object gives type erasure/polymorphism in a similar manner to an interface.
I don't know much at all about Rust's implementation, but I have a question about the optimizations you mention. For a simple enum (like the `Direction` one in the [enum section](http://doc.rust-lang.org/tutorial.html#enums) of the tutorial) would it be possible to perform this optimization with a slight loss in the maximum representable members, or would there be significant disadvantages to it? 
Running the tests in order was simply to appease my OCD. I strongly disagree with the not testing private methods. I honestly don't understand what's the problem with vigorously exercising a particular code path. As for me wanting to move the tests to a separate file, it's because my private tests are quite bulky - I do positive and negative and thus my `mod tests` ends up taking up more space than the business logic.
The 3D gun printing people would say that's because its still a developing technology not ready for initial release yet. Much like the rust community when ever people point out flaws in rust :P
If your stack size is at the default 8MB, then you're probably just running out of VMEM. Looking at RLIMIT_NPROC is the way to know, as the kernel will have done the calculation of available VMem for you. I intend to do some benchmarking when I have more time, but I reckon 10,000 threads on my laptop will be just fine for a busy web server. I stand corrected, though. 100,000 might be a bit much :) 
Looking at https://github.com/visionmedia/co. This looks and feels a whole lot like continuations. These are easy to implement and relatively cheap in interpreted languages, since you get to build and manage your own stacks. I'm not sure if I'd be comfortable having this be the default IO manager in a natively compiled language that is intended to be as fast as C++. This involves quite a bit of bouncing around between functions. https://github.com/visionmedia/co#thunks-vs-promises Anything that works with IO at the library level would have to return a continuation. That is a lot of closures. 
Off the top my head I think the `Option&lt;Enum&gt;` would be most valuable, `Option&lt;bool&gt;` less so, and `Option&lt;char&gt;` the least; however I still think they would be very nice to have, for zero-overhead type safety (e.g. a parser can just store `next: Option&lt;char&gt;` (with `None` meaning EOF) without any additional size, instead of storing e.g. `-1` for that purpose).
Thanks for the code! I read about trait objects but didn't know how to use them in this context. But I think you have to use some macros to reduce the code size like /u/HeroesGrave mentioned.
The coroutine/generator model I've seen also involves a considerable amount of closures. Is there some research or code (or even a conversation thread?) somewhere which demonstrates this model in native compilation? I am very interested. *\*edit\** So I found a few resources which describe this. [Mordor](https://github.com/mozy/mordor) does it in c++, but also relies on a fiber class which leverages ucontext. I am assuming that this was the way it was done with greenthreads. [C#](http://blogs.msdn.com/b/ericlippert/archive/2010/10/28/asynchrony-in-c-5-part-one.aspx) seems to have the best implementation of async/await to date. 
Generally `/mode` doesn't stick. But your IRC client likely has ways to make it stick.
&gt; Green Threads are the way to go for scalability and in general They're really not, they offer little to no advantage over native threads in terms of scaling in Rust. Either way, the stack is by far the most significant resource consumed by a thread and neither is lighter. The green scheduling mode just avoids the overhead of context switches except during an explicit yield. At the same time, that means using any code with loops or recursion is broken because it blocks the scheduler. A language with first class M:N threading support would use segmented / relocatable stacks by accepting inferior performance and very slow FFI. It would also insert yield points at function preludes and loop edges to avoid the need to do it all by hand, which makes libraries unusable. Rust isn't that language.
It gets slower at the same rate for green and native threads. The only important resource is the large slab of memory used as the stack.
You would run the tests in parallel and then serialize their results in the order the user wants to see them. For what order that would be: For the tests in a single file, the obvious choice is the order in which they are declared. The order of the test results/failures would correspond directly to the code in the source file. If there are multiple modules, it would be useful to run the test for any dependencies first. I don't know enough about rust testing to say if the module dependencies could be easily determined by the testing tool.
rust-bindgen should build this as well as wrappers for the rest of the leveldb interface. No need to do this manual work by hand, you can put your effort into building the higher level, rust-like interface. 
Uh ? Typing `impl X for Y {}` is not so much more wordy than adding `: public X` to `Y` so I don't quite see what you are complaining about...
That's not the issue I have. My question is what a high-level interface above this API could look like.
I haven't had a great experience with rust-bindgen there. In any case, it's precisely that high-level interface I am interested in, which neither approach provides.
This is a culmination of the work I've been doing with Rust for the past year or so. I'd love feedback of any form, whether it's the API, style, documentation, implementation details, or anything else. I'd also appreciate it if a Windows developer could take a look at it - it's currently horribly broken on Windows and I can't figure out where it's going wrong.
As somone who wants a Garand, loves C, and wants to be Rusty: I propose the author has hear of "Garand thumb". http://garandthumb.com/what-is-garand-thumb/ In both c and m1 messing up can be painful. Btw, our .gov is actually still seeling surplus m1-s to us civvies, I could post a link.
It is currently using WinPcap for layer 2 - https://github.com/libpnet/libpnet/blob/master/src/datalink/winpcap.rs. The issue is it doesn't seem to be able to receive packets - I haven't been able to figure out why yet.
Ah good - I missed that. I see a call to set a NDIS_PACKET_TYPE_PROMISCUOUS flag, is this the same as PCAP_OPENFLAG_PROMISCUOUS?
I used to have that compulsion ("everything must run in order!") But since I learned haskell its become "As many things as possible must run in any order possible!" Referential transparency is a hell of a drug.
rust-bindgen has some quirks, I think it is a net gain, though. Either way I recommend having two function. The direct literal translation of the extern "C" to rust, then the high level rust-ish function, which presumably would take a closure as a callback. 
The definition is here - http://octarineparrot.com/assets/libpnet/doc/pnet/macro.pfor!.html. It's not defined in Rust, it's a custom macro for libpnet.
Thanks! I'll ignore the warning, if you are okay with that ;). https://github.com/andrew-d/leveldb-rs/blob/master/src/lib.rs#L10
It has the same effect - it is not the same though. There are three parts to WinPcap - the driver, a wrapper library, and libpcap. I'm using the wrapper library directly to minimise dependencies. The former constant is part of Windows[1], the latter is libpcap specific (and I assume will trigger NDIS_PACKET_TYPE_PROMISCUOUS usage later on). [1] http://msdn.microsoft.com/en-us/library/windows/hardware/ff569575%28v=vs.85%29.aspx
I probably should have just used libpcap - it would likely be working if I had. No fun if all the work's already been done though!
Hah, yeah, that's there more as a "just in case" warning. The issue is that the `leveldb_comparator_t` can't be dropped until we're no longer using the comparator, which pretty much means that we have to tie it to the lifetime of the database itself. I do this by storing the comparator [within the database options](https://github.com/andrew-d/leveldb-rs/blob/master/src/lib.rs#L171), and then storing that [within the open database](https://github.com/andrew-d/leveldb-rs/blob/master/src/lib.rs#L917), so I'm pretty sure things are OK. Still, the warning remains for now ;-)
I don't know much about this kind of stuff, so I'm wondering how this is different then the networking functionality in std::io. Is it simply access to a lower level of network programming, and if so why would you want to work at that level as opposed to the TCP/UDP level of abstraction? Either way, thanks for contributing to the community!
No, it is a custom macro: [Here](https://github.com/libpnet/libpnet/blob/43797a51479a2075a4ec20743328d1146e8c7426/src/lib.rs#L69) is some documentation and [here](https://github.com/libpnet/libpnet/blob/43797a51479a2075a4ec20743328d1146e8c7426/src/packet/mod.rs#L37) is the definition.
You are correct, it is a lower level of network programming than what is offered by std::io::net. Most programmers will never need to work at a lower level - the transport protocols are there to give you guarantees which raw IP does not give, and allow multiple applications to work on the same machine. There are quite a few uses for working at lower levels, I've picked out a couple: * Developing new transport layer protocols: UDP and TCP are great, but there are a lot of things that they don't do or which could be done differently. Typically if you're developing at this level you go one of two routes - write it in C; or write it in a scripting language. While scripting languages are great for rapid prototyping of new ideas, you don't get the performance needed for a real network stack (ok, that's debatable... I won't go into it here). Alternatively if you use C, you have to be very careful to write bounds checks everywhere, make sure that threading is done correctly, and all the things other things you probably know about since you're using Rust. Hopefully libpnet provides a happy middle ground. It should perform equally as fast as an equivilant C implementation (I'll be working on benchmarks in the coming weeks), while having the high level abstractions which make implementation easy in higher level languages. * Implementing tools such as ping or traceroute require doing interesting things with the network/transport headers - using TCP/UDP directly won't work. * Raw packet capture/filtering: If you want to go even lower level, you can look at packets exactly as they are "on the wire". This is useful for things like network diagnostics, firewalls, traffic shaping etc. Thank you for your questions!
Very informative reply, thank you, and best of luck with your project!
Rust doesn't have copy / move constructors. It's already doing in-place reallocations whenever possible. The pools used by jemalloc for size classes are divided up into *spacing classes* and AFAIK it can often expand in-place even for small memory allocations.
I think this is possible by using `extern "C" fn` wrappers to call into a trait. trait Comparator { fn name(&amp;self) -&gt; *const u8; fn compare(&amp;self, a: &amp;[u8], b: &amp;[u8]) -&gt; i32; } extern "C" fn name&lt;T: Comparator&gt;(state: *mut libc::c_void) -&gt; *const u8 { let x: &amp;T = unsafe { &amp;*(state as *mut T) }; x.name() } extern "C" fn compare&lt;T: Comparator&gt;(state: *mut libc::c_void, a: *const u8, a_len: uint, b: *const u8, b_len: uint) -&gt; i32 { slice::raw::buf_as_slice(a, a_len, |a_slice| { slice::raw::buf_as_slice(b, b_len, |b_slice| { let x: &amp;T = unsafe { &amp;*(state as *mut T) }; x.compare(a_slice, b_slice) }) }) } extern "C" fn destructor&lt;T&gt;(state: *mut libc::c_void) { let _x: Box&lt;T&gt; = unsafe {mem::transmute(state)} // let the Box fall out of scope and run the T's destructor } fn create_comparator&lt;T: Comparator&gt;(x: Box&lt;T&gt;) -&gt; leveldb_comparator_t { unsafe { let state: *mut libc::c_void = unsafe {mem::transmute(x)}; leveldb_comparator_create(state, // specialise each function to this T destructor::&lt;T&gt;, compare::&lt;T&gt;, name::&lt;T&gt;) } } Adjust signatures (&amp; how they are wrapped) to taste. (Preferably the return value is wrapped in a struct with its own `Drop` impl, that calls `leveldb_comparator_destroy`.)
This is cool project, and I'm excited to see all the things people are doing with Rust. I'm mainly interested in why you chose to work on this project and what made you interested in Rust development. Does it relate to work you have previously done on other projects, or something else?
Please add this to the readme file
Most libraries out there track the nightlies. You won't be able to do much without the latest Rust. Also, it's hard to remember the changes since v.11, so it is hard to help you out if you are using it.
Thank you. You sound like a bright fellow. I'm pleased to see RW locks available too. I think the question raised about calling `lock()` on the `Arc` was valid - I'm much more comfortable with a more explicit notation of accessing the innards of the Arc, e.g. `a2.value().lock()` ... otherwise it, `a2.lock()`, looks too much like sorcery to my novice eyes. The other question I had was about refcounts and `drop()` - I presume the refcount itself is an atomic integer - or is this protected using mutexes underneath? Finally, the `spawn` command: I see it used over closures (`proc()`) but can people write functions and `spawn` them? I haven't seen this done in example code (maybe I haven't looked far afield enough). With the tech space moving towards channels (which Rust supports) I was wondering if mutexes had all but disappeared - so comforting to see them. I wonder if others have read Java Concurrency in Practice. One of the interesting take-aways was that some Java collections have built-in mutexes - e.g. ConcurrentHashMap - which actually has multiple mutexes built in to speed up concurrent access. Are there plans for such libraries in Rust? Thanks - sorry for all the questions!
&gt; I think the question raised about calling lock() on the Arc was valid - I'm much more comfortable with a more explicit notation of accessing the innards of the Arc, e.g. a2.value().lock() ... otherwise it (a2.lock()) looks too much like sorcery to my novice eyes. Accessing the internals of an `Arc` is really really cheap, it's exactly the same as accessing the internals of an `&amp;T` or `Box&lt;T&gt;`, just following the pointer. &gt; I presume the refcount itself is an atomic integer Yes. &gt; Finally, the spawn command: I see it used over closures (proc()) but can people write functions and spawn them? I haven't seen this done in example code (maybe I haven't looked far afield enough). A closure is a function, so what does this question mean? (If you mean `fn foo() { ... } spawn(foo)`, it works now, and anyway, you can always call a function inside a proc: `spawn(proc() foo())`.) &gt; Are there plans for such libraries in Rust? Anyone can write such a library which others can use via `cargo`, since 'external' libraries have the same sort-of low-level power required for efficient data structures; in any case, there are issues about this: [#10580](https://github.com/rust-lang/rust/issues/10580), [#13166](https://github.com/rust-lang/rust/issues/13166).
Thanks. The issues regarding concurrent collections were interesting to read - clearly that is a problem being worked on right now. &gt; Accessing the internals of an Arc is really really cheap, it's exactly the same as accessing the internals of an &amp;T or Box&lt;T&gt;, just following the pointer. Isn't it better to dereference, then, like in C++? e.g. `(*a2).lock()` rather than magical `a2.lock()`?
&gt; Isn't it better to dereference, then, like in C++? e.g. (*a2).lock() rather than magical a2.lock()? Meh, it's a matter of taste, but it is rather noisy for an operation that is essentially zero cost.
&gt; Rust itself provides facilities for configuring sections of code based on the platform. We plan to support per-platform configuration in Cargo.toml, including platform-specific dependencies, in the near future. Will be particularly welcome for cross platform dependency builds when it lands.
Is libgreen being dropped for real? Does this mean that rust isn't going to get a builtin N:M scheduler for it's tasks? (edited)
Yes, it will.
There's lots of stuff that Cargo doesn't yet do. https://github.com/rust-lang/cargo/issues has lots of things that aren't just bugs. Off the top of my head, some cool things in the future: 1. Central repository for projects. Lots of other cool possibilities with this too... 2. Profiles. 3. better support for C dependencies 4. `cargo install`. 5. better debugging support (`-g` is disabled for now)
Thanks, the video is pretty informative about Alexandrescu's `Expected&lt;T&gt;`. A minor nitpick: I think Alexandrescu is wrong that Haskell's `Maybe t`/Scala's `Option[T]` is the equivalent of his `Expected&lt;T&gt;`. A closer analogue is Haskell's `Either t e`/Scala's `Either[T,E]`, which encodes a value, or an error message why the value could not be computed. https://hackage.haskell.org/package/base-4.2.0.1/docs/Data-Either.html http://www.scala-lang.org/api/2.9.3/scala/Either.html Either is a bit more general than error handling (it's used whenever a function can respond in two ways), and Scala added a "special case" of either specifically for error handling, `Try[T]` which seems to match `Expected&lt;T&gt;` almost exactly: http://www.scala-lang.org/api/2.10.2/index.html#scala.util.Try
&gt; Is libgreen being dropped for real? Well, the RFC hasn't been accepted yet, but the removal of libgreen is one of the less controversial parts, so... &gt; Does this mean that rust isn't going to the a native N:M scheduler for it's tasks? I'm not exactly sure what you're asking here. Rust already uses 1:1 threading by default.
We have some basic support for SIMD in intrinsics. Rust 1.0 is not going to block on it. I think it's a good time to reiterate that asking "will X be in 1.0" isn't the right question for features that can be backwards compatibly added (like this one). We will continue to add stuff to the language post 1.0 on a rolling release model.
Yes, agreed!!!!
Hi /u/dbaupp, as always: that's very helpful and fits what I had in mind (turns out, I had a similar implementation attempt, but I couldn't get the callbacks right). I have questions: How does memory management work in that case? The Box is never held by another structure somewhere, but still you handle it in the destructor. Also, I used the `Options` struct to wrap `leveldb_options_t *` and optionally `leveldb_comparator_t *` and handle the whole creation process there (I'd prefer users to not fiddle with that too much). But if I call destroy in `Drop`, the program crashes. Is there something obvious I am missing or what would be a good way to debug that? https://github.com/skade/leveldb/blob/master/src/database/options.rs#L55-L73 
libgreen is moving into Cargo. For more details, see the RFC: https://github.com/rust-lang/rfcs/pull/219
Are there any plans for a kind of equivalent to `virtualenv` for Rust? Something that says, test this library with Rust 2.0 or Rust 1.0?
It started as a university project to implement [DCCP](http://en.wikipedia.org/wiki/Datagram_Congestion_Control_Protocol) using Rust. While I never managed to achieve that, I did manage to build the beginnings of this library. There's a lot of work that went into it which you don't see here - lots of weird little hacks and workarounds etc. There were also lots of changes required to librustrt/rustuv/native so that it integrated nicely with the rest of Rust - since libgreen is being removed from the standard distribution I've been able to remove a lot of that code, which has dramatically simplified things. At the end of the (adademic) year, my supervisor offered to employ me over summer to continue work on it - so that's what I did. I've spent the summer adding missing functionality, making sure it works correctly cross platform, cleaning up lots of the hacks which previously existed. I finally managed to get it into a (hopefully!) usable state in this past week or so, and here it is. As for why, I originally wanted to do some kind of networking project using [D](http://dlang.org/), since I've been using that for years now. When I approached Colin (my supervisor), he suggested using Rust instead, which is how this all began. I'm very glad I learned Rust - I believe it is far better suited for networking than D due to the many safety guarantees you get; there are a huge number of issues which I encountered which caused compilation to fail with Rust, which would have gone unnoticed in D.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Datagram Congestion Control Protocol**](https://en.wikipedia.org/wiki/Datagram%20Congestion%20Control%20Protocol): [](#sfw) --- &gt;The __Datagram Congestion Control Protocol__ (__DCCP__) is a message-oriented [transport layer](https://en.wikipedia.org/wiki/Transport_layer) [protocol](https://en.wikipedia.org/wiki/Protocol_(computing\)). DCCP implements reliable connection setup, teardown, [Explicit Congestion Notification](https://en.wikipedia.org/wiki/Explicit_Congestion_Notification) (ECN), [congestion control](https://en.wikipedia.org/wiki/Congestion_control), and feature negotiation. DCCP was published as RFC 4340, a [proposed standard](https://en.wikipedia.org/wiki/Internet_standard), by the IETF in March, 2006. RFC 4336 provides an introduction. FreeBSD had an implementation for version 5.1. Linux also had an implementation of DCCP first released in [Linux kernel](https://en.wikipedia.org/wiki/Linux_kernel) version 2.6.14 (released October 28, 2005). &gt; --- ^Interesting: [^Transport ^layer](https://en.wikipedia.org/wiki/Transport_layer) ^| [^Stream ^Control ^Transmission ^Protocol](https://en.wikipedia.org/wiki/Stream_Control_Transmission_Protocol) ^| [^Transmission ^Control ^Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol) ^| [^User ^Datagram ^Protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ck7ujdt) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ck7ujdt)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
What happened to `cargo test --nocapture`?
Done - hopefully I covered everything!
`type` is just a type alias. It's pretty useless. What you're looking for is newtypes. Eg: `struct Miles(int);`
No, `type` is just a type synonym, it is much like `type` in Haskell: it just defines an alternative (probably shorter) name for the *same* type. You need to use "newtype" structs: struct Miles(int); struct Hours(int); fn main() { let m = Miles(5); let h = Hours(5); let nonsense = m + h; // won't compile } However, if you want to add `Miles` to `Miles`, you will need to implement `Add` trait manually for it, as underlying types' traits are not implemented automatically for wrappers. #[deriving(Show)] struct Miles(int); impl Add&lt;Miles, Miles&gt; for Miles { #[inline] fn add(&amp;self, other: &amp;Miles) -&gt; Miles { let (Miles(a), Miles(b)) = (*self, *other); Miles(a + b) } } let m = Miles(5); let n = Miles(6); let s = m + n; println!("{}", s); [Playpen][pp] [pp]: http://play.rust-lang.org/?code=%0Afn%20main()%20%7B%0A%20%20%20%20%23%5Bderiving(Show)%5D%0A%20%20%20%20struct%20Miles(int)%3B%0A%0A%20%20%20%20impl%20Add%3CMiles%2C%20Miles%3E%20for%20Miles%20%7B%0A%20%20%20%20%20%20%20%20%23%5Binline%5D%0A%20%20%20%20%20%20%20%20fn%20add(%26self%2C%20other%3A%20%26Miles)%20-%3E%20Miles%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20let%20(Miles(a)%2C%20Miles(b))%20%3D%20(*self%2C%20*other)%3B%0A%20%20%20%20%20%20%20%20%20%20%20%20Miles(a%20%2B%20b)%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%0A%20%20%20%20let%20m%20%3D%20Miles(5)%3B%0A%20%20%20%20let%20n%20%3D%20Miles(6)%3B%0A%20%20%20%20let%20s%20%3D%20m%20%2B%20n%3B%0A%20%20%20%20println!(%22%7B%7D%22%2C%20s)%3B%0A%7D
(As an addendum to the other answers, note that "newtypes" `struct Miles(int);` and normal structs `struct Miles { dist: int }` have the same representation at runtime (unlike the newtype vs. not-newtype distinction in Haskell), and the latter is often more convenient to work with at the moment, with due to field access; so don't feel forced into using a one-field tuple struct just because some people happen to use a special name for it.)
&gt; We have some basic support for SIMD in intrinsics. Rust 1.0 is not going to block on it. &gt; [1] will you freeze features or block changes (like SIMD) not required for 1.0 out of pragmatism for achieving the stable 1.0 release. [2] if [1]=yes - how feasible would it be to continue evolving SIMD support in parallel, maybe behind a feature gate or in a branch [3] another question is what data types we can eventually count on being SIMD-able (post 1.0): [T,..4] vs (T,T,T,T) vs Vec4(T,T,T,T) vs Vec4{x:T,y:T,z:T,w:T} . these options all have their own strengths and weaknesses; Is it safe to pick any of these representations and count on eventual SIMD ability in a future version of the language
&gt; [1] will you freeze features or block changes (like SIMD) not required for 1.0 out of pragmatism for achieving the stable 1.0 release. No. Rust is being very careful about how 1.0 will be cut and what be stable. We have and are currently using a lot of infrastructure to allow us to not stabilise everything, e.g. feature gating (using the `#[simd]` attribute to define your own SIMD types is feature gated) and stability levels (the SIMD types in the stdlib are marked `#[experimental]` and thus will give compiler warnings/errors by default).
I will probably check your implementation later tonight. I was also planning on adding "callback stream" support [to my binding](http://www.rust-ci.org/sellibitze/rustaudio). Looks like there are now three PortAudio bindings and none of the authors wanted to join forces. (I didn't like the first one because it wasn't as safe as it could have been).
Doesn't work: `$ cargo test -- --nocapture` `Unknown flag: '--nocapture'` `Usage:` ` cargo test [options] [--] [&lt;args&gt;...]` 
Interesting. But `--nocapture` *was* an argument for the test binaries. Maybe this changed. I would run the test binary with `--help` to see all the options.
Definitely no `--capture`. And that was a very handy parameter to have.
lol, down voted again.. 7."Downvotes are for bad information or rudeness, not casual disagreement" was I rude? did I give bad information? I just asked some questions. 
Precisely, I understand most of the reasoning in the RFC but having green threads as a second class citizen will have its costs.
It is OS X compatible, e.g. [it is built and tested on OS X on Travis](https://travis-ci.org/servo/servo/jobs/34263083). That looks like a rustc version mismatch rather than anything platform specific.
Thanks!
Ok, I've tried to abstract this away because I want to use it on a lot of my types. I want to separate my countable types from identifier types (for example, miles + miles should make sense, but userId + userId should not, despite both being ints). Here's what I've come up with: #![feature(macro_rules)] macro_rules! identifier( ($inp:ident) =&gt; { #[deriving(Show)] struct $inp(pub int); } ) macro_rules! countable( ($inp:ident) =&gt; { #[deriving(Show)] struct $inp(pub int); impl Add&lt;$inp, $inp&gt; for $inp { #[inline] fn add(&amp;self, other: &amp;$inp) -&gt; $inp { let ($inp(a), $inp(b)) = (*self, *other); $inp(a + b) } } impl Sub&lt;$inp, $inp&gt; for $inp { #[inline] fn sub(&amp;self, other: &amp;$inp) -&gt; $inp { let ($inp(a), $inp(b)) = (*self, *other); $inp(a - b) } } }; ) countable!(Miles) countable!(Hours) identifier!(UserId) identifier!(LocationId) It seems to compile and work -- I just want to make sure I'm doing this correctly. Am I thinking about this the 'right way'? edit: formatting
I've just `rustup.sh`. There are definite differences in the installer but `--nocapture` still doesn't work. I guess it's a question of waiting a bit more.
Hmm I think I missunterstood s.th. about newtype. In my use case I have three structs having the exact same representation (for now), but I want to treat them as different types. Currently I have this implemented as #[deriving(Show)] pub struct Predicate { pub name: String } #[deriving(Show)] pub struct Constant { pub name: String } But when I try to use newtype #[deriving(Show)] pub struct Predicate(Constant); #[deriving(Show)] pub struct Constant { pub name: String } I get this: error: structure `Predicate` has no field named `name` Does newtype only work with primitive types?
N green threads and M native threads 
Yeah, looks fine. That's exactly what macros are for.
(Your original question has some extraneous words, which make it hard to interpret its exact meaning, hence /u/steveklabnik1's confusion.)
No, tuple structs work with all types. You will need to show me your whole code because that error doesn't correspond to either snippet. I have a strong feeling you just forgot to update one construction site, i.e. leaving it as `Predicate { name: ... }` when it should be `Predicate(...)` with a tuple struct.
The file properties.rs was removed in favor of properties/mod.rs, but a 'git fetch &amp;&amp; git merge --ff-only' will not delete the file. You can just rm properties.rs and it should all work. Sorry for the inconvenience!
Github needs a way to voice agreement with comments without empty posting. I don't want to have to go down the thread commenting "I agree" on each of /u/strncat's comments.
Sorry, I just noticed it.
It's worth noting that having them as first class citizens (e.g. inserting yields into loops and function preludes) has its costs too.
On this note: What about say a mille times a mile? That should have type mile^2... Wouldn't that be awesome to define types in meters and seconds and whatnot and have rust do the unit checking in equations?
I'm not suggesting voting on a way forward. Just that giving the people most able to present their arguments an idea as to what the people reading the discussion think would be useful.
I guess I don't get why you feel compelled to comment on this each time. If you haven't noticed, it's incredibly common for every comment in a thread to initially get 1 or 2 downvotes. (Trolls? Bots? Just a couple very negative people?)
Any particular reason why the two constructs aren't unified similar to Scala's primary constructors, i.e. the parameters are also fields (or vice verse)?
I [asked about this](http://www.reddit.com/r/rust/comments/1vutg0/rust_newbie_questions_regarding_types/cex11i6) at some point; apparently it would need a more powerful type system. I think you basically need the ability for types to be parametrized by numbers. So you can tell it that e.g. a velocity has 1 power of length, -1 power of time; then define a generic multiplication operation that returns a type with the appropriate dimensions. From what I can tell that definitely won't happen before 1.0, and I'm not even sure that HKT would be enough. (But I'm very much a noob when it comes to this stuff.)
You can't have your cake and eat it. But maybe in this case it would be better to decide for "no cake" (that is, no support for green threads) rather than a cake that tastes bad (mostly-useless green thread library).
First of all, I don't think it's useless, because green threads still spawn much faster than native threads, message passing is faster, and they reduce OS resources. But also adding yields into loops and function preludes wouldn't really address the major problems: Go for example doesn't yield on loops, just on function preludes, and it doesn't seem to cause problems. The biggest issue is that our stacks are non-relocatable.
I agree that it could be more powerful than it is.
Not necessarily, but for convenience you probably want integrals. In C++ you can indeed to so with integrals, there are 7 dimensions to the SI units, so the base unit would be `unit&lt;1, 0, 0, 0, 0, 0, 0&gt;` (shuffle the `1` around a bit for other dimensions) and then you can multiply and divide units (do you allow rationals ?). Doing so in Rust at the moment would be fairly annoying, though is not impossible. The trick would be to go down the route of Peano numbers: 0 is `Zero`, 1 is `Succ&lt;Zero&gt;`, 2 is `Succ&lt;Succ&lt;Zero&gt;&gt;`, ... combine with a `Neg&lt;...&gt;` wrapper to get negative numbers and you have all integrals. However, the diagnoses would be pretty complicated to read. Already in C++ with 7 dimensions it's sometimes a bit hard to pick whether the number you are reading is the 3rd or 4th, here it would get ridiculous.
In `&lt;module&gt;/f.rs` you can do `use super::g::ctype` to get at `&lt;module&gt;/g.rs`. `super` allows you to go one "up" in the module hierarchy, so in this case brings you to the "&lt;module&gt;" layer, where you an access g. To build as a library, you can use cargo or if you want to do it with rustc only, just do `rustc --crate-type=rlib &lt;module&gt;/mod.rs` though convention for the file which is supposed to be targeted by rustc is `lib.rs` not `mod.rs`.
thank you. i know the "super" i example code. the mod level not too deep. if ::a::b::c::d::e::f::g::h want ref ::a::b the only way i could do is " super::super::super::super::super::super::super::b " ? is there anyway use "use" syntax like what we do in java? in ::a::b::c::d::e::f::g::h just import ::a::b ?
Correct me if I'm wrong, but this would also take away the keyword status of `type`, right? In my experience, that keyword often collides with the most apt name for certain variables/fields. It's a bit irritating.
I always find it surprising that `::a::b` works when directly using types or functions, but not for `use` declarations. Edit: Actually just `use a::b;` works! Could somebody explain how path resolution in `use` differs from paths in the code?
This sub has very werid downvoting at times, I wouldn't worry about it unless you stay significantly in the negative for a while.
Is that the dev who used to work with all the SIMD stuff in Rust?
I have also wondered this in the past. With respect to [`use` declarations](http://static.rust-lang.org/doc/master/rust.html#use-declarations) the language reference says, &gt; Also note that the paths contained in use items are relative to the crate root. The grammar of `use` is defined as, use_decl : "pub" ? "use" [ path "as" ident | path_glob ] ; Although I haven't been able to find a definition for `path`, the reference [defines grammar](http://static.rust-lang.org/doc/master/rust.html#paths) for `expr_paths` and `type_paths`, expr_path : [ "::" ] ident [ "::" expr_path_tail ] + ; expr_path_tail : '&lt;' type_expr [ ',' type_expr ] + '&gt;' | expr_path ; type_path : ident [ type_path_tail ] + ; type_path_tail : '&lt;' type_expr [ ',' type_expr ] + '&gt;' | "::" type_path ; It's not at all clear to me why these behave so differently. It would be nice to hear the rationale for this from someone more familiar with the evolution of the language. 
Yes, he actually implemented almost everything I could possibly want from SIMD, but then the RFC process stopped it dead in its tracks, IIRC.
I don't like this solution. Aside from the readability issue of always having 7 fields, regardless of how many you're using, it doesn't address the fact in every field besides physics, these are almost certainly not the units you're looking for. Maybe you want apples, or people, or vespene gas, or test scores, etc. These are all examples of where units should be used, but there isn't any SI unit available. Unfortunately, I'm not really seeing a way to do generic algebraic products in Rust (i.e., where I could multiply unit A * unit B, then later divide by unit B and end up with a result in unit A). It's easy enough to code each instance you need with tuples, but the general case just seems a bit beyond what's expressible in the language (though it's possible I'm just too dumb to see how to do it). EDIT: After further pondering, I think this is sort of doable-ish with hash maps over std::intrinsics::TypeId like in reem's TypeMap, but there will be no compile time correctness assertion, which is kind of what we're after here I think.
Alternately: match s.unwrap().as_slice().trim() { That will remove spaces as well as line feeds (`\n`) and carriage returns (`\r`).
What about parametrised type aliases though? Do you think those could be integrated into `use` as well? Like: pub use IoResult&lt;T&gt; = Result&lt;T, IoError&gt;;
Correct. Although sometimes keywords are preemptively reserved just in case they might be useful in the future.
`type` here almost sounds like it should be called `alias` then.
That isn't the first time that has happened and IMO the Rust core team should _seriously_ go out of its way to ensure it doesn't happen again. There needs to be some better communication with people making such changes or maybe a slightly different dev process that accommodates experimental code that's still shipped but turned off by default (sounds like feature gates, doesn't it?). All of freaking _Chrome_ is developed like this (_hundreds_ of developers!); just because a feature isn't ready to be released into the wild doesn't mean the people who are working on it can't check in their code and develop it in-tree if it's understood that the feature will ultimately be launched at some point. The code can also always be deleted if the decision comes down to not launch the feature after implementation experience shows it to not be worth it. This current situation where feature X is desirable, the RFC for it is widely considered good but the core team wants it for post-1.0 results in effectively saying "go away" to people working on the feature. People end up doing exactly that. It's really sad and results in unhappiness and wasted resources for absolutely no benefit. This is a people management issue and the core team needs to step up and fix it.
I've commented [further down the thread](http://www.reddit.com/r/rust/comments/2fbyxu/simd_support_in_rust/ck8ns7c). I'd like to hear what someone from the core team has to say.
I would hope such a feature would never, ever be needed.
Even if this instance of the `type` keyword were removed from the language, its imminent use in [associated items](https://github.com/rust-lang/rfcs/pull/195) is all but guaranteed.
Right.
For that case, there is also `.trim_right()`, producing `" q"` or even `.trim_right_chars(&amp;['\n', '\r'])`, producing `" q "`. This is something I learned from Java: make the standard lib work for you.
Isn't it depends on what you want to benchmark? I mean, you could just copy the asm code from C++ implementation, paste it into rust with `asm!` and get perfect 1 to 1 parity.
Judging just by the PR number, that must've been like an year before aatch.
Why not? I can definitely see some people being stuck on 1.0 due to company policy and makers of software not wanting to leave them behind. Sure, in an ideal world you don't need it, but that is probably not what will happen, either. 
I'm not very experienced with Rust - would it be possible (using macros maybe?) to implement something like Haskell's `GeneralizedNewtypeDeriving`, where the compiler will automatically generate the instance of a typeclass for a newtype if the base type is already an instance? As an example, I could do: {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Miles = Miles Integer deriving (Num, Show, Ord, Eq) and all of the relevant instances would be created, allowing: λ: Miles 2 + Miles 4 == Miles 6 True
Ok, [I did a straight translation of the C++ into Rust](https://gist.github.com/huonw/9822c83dc37875509c0f) including keeping their... inconsistent choice of integer types (I also retained the variations of `reverse` from above). `perf stat -r 3` gives these averages: | | Time | |----------------|------| | clang++ | 1.18 | | g++ | 1.20 | | Rust (`doener`) | 1.16 | | Rust (`idx`) | 1.18 | | Rust (`ptr`) | 1.19 | | Rust (`iter`) | 1.61 | | Rust (default) | 1.33 | | Rust (original) | 1.80 | So we just need to get the safe `reverse` functions down to the unsafe ones.
http://play.golang.org/p/QHk3upAS_X ;-)
Could you add the time for the reverse implementation I gave below? Thanks! Edit: Preferably with inlining enabled for the `reverse` function, as it didn't make a difference for me, when inlining was disabled (for whatever reason...)
Rust does have newtypes, that's just not what the `type` keyword does.
Did anyone look into the Nix package manager as an alternative to Cargo?
[Added](https://gist.github.com/huonw/9822c83dc37875509c0f/242bbdc82cbdc6c8aad32a63f05b5698cab830fd#file-fannkuchredux2-rs-L129-L136); it does seem that it is a few milliseconds faster.
Yeah, the difference is more significant if you run it with 12 instead of 11.
i guess i'm taking it too personally ..ok . i'm happy to see my post got up voted since 
Thank you. I feel silly, but I also feel less crazy which is good.
Would it be worth having two rusts on the shootout? Rust safe (which disallows non-standard-library unsafe code) and rust unsafe (which does not). I think there's a potential marketing issue if we fill the shootout with unsafe code, in that people may use it to state that rust needs to use unsafe to achieve good performance.
Ah! That's quite a different issue indeed. The SI system I described is indeed for physics, the main advantage being that all physical units can be expressed with it. For apples and oranges, I would use the term quantity. This is much simpler, you just need a phantom type: Quantity&lt;i32, Apple&gt; would be a quantity represented internally as `i32` and counting apples. Now, I would remark that it generally does not make sense to sum quantities of different objects. I could however see ratios in bartering systems (3 bananas for 4 apples). This can be encoded in the type system as well: either with this type or a dedicated `QuantityRatio`; in any case: `Quantity&lt;i32, Ratio&lt;Apple, Banana&gt;&gt;` is perfectly valid as type.
Sure. I don't expect that function to go into a standard lib like that. It's just the fastest I could come up with and a point to start from. The safe version using `len()` maybe just needs some tweaking to be as fast, not sure, and I didn't want to spend all day tinkering with it ;-)
Thanks, this is awesome. I think I'll check in /u/doener's as `shootout-fannkuchredux.rs` and rename the current safe version to `shootout-fannkuchredux-safe.rs`. Anybody disagree with the strategy of 'cheating' the shootout be writing `unsafe`? I wonder if we could make up some of the ground by specializing the stock `reverse` for `Copy` (and of course whether we should - we don't specialize anything right now).
&gt; perceived as comparing only languages The website states repeatedly that the comparison is between programs and the comparison is between programming language implementations.
Did he leave the Rust project or something? I remember him being very active on irc a while ago. 
If this is still unclear, check out [Niko's talk about memory safety](https://air.mozilla.org/guaranteeing-memory-safety-in-rust/).
I see. I misunderstood what I was looking at. Yes, I'll replace the existing wyth your translation.
If only good benchmarks were the point! Sadly, with the shootout, good PR is the point (the benchmarks are notoriously bad).
People are interested in rust for the safety. That's its selling point. And of course people are interested in benchmarks that show how fast it is *with that most important feature enabled*. Doing an unsafe benchmark and then using that as a PR tool to claim "We are safe. We are as fast as C." would be very dishonest. Rust is already surprisingly fast with all the safety features enabled, there would be more to lose than gain by "cheating" in the benchmarks. A benchmark for unsafe code can be marginally interesting as an "for the really, really time critical parts you can even disable the safety and get this amazingly fast result". But generally people want the safety, and want to know how that will perform and how much performance that safety will cost them. All together, I believe the best solution (as in, most useful for a potential user of the language) would be separate rust listings in the shootout for safe and unsafe code. It would show very clearly how much (or little) you pay for the safety, and how much performance you can get in case you really need to.
But don't you have to explicitly define each algebraic product (in this case, ratio) here? The reason I don't like that it's just hardcoding something which is strictly part of a well-understood abstract principle. For any two cardinal (quantitative) types T1 and T2, T1 x T2 *always* makes sense from a logical perspective, and the result of T1 x T2 / T1 will *always* be of type T2. I shouldn't have to write in my code Ratio&lt;T1, T2&gt; to use that product any more than I should have to specify "enum uint {0, 1, 2, 3}" to gain access to the integers 0-4. In practice, it's probably not actually that painful to specify each needed type product, but still, it just doesn't feel quite right.
Notoriously bad compared to what?
That would be my preference as well. I think it's analogous to having separate entries for, say, Lua interpreted and LuaJit. Sometimes you want one (simple implementation, easy embedding) and sometimes the other (high throughput), depending on requirements. Rust users will be in the same boat. That said, there's plenty of precedent for, well, I don't want to say "cheating," I'll say "creative interpretation of the rules," for other languages. For example, a lot of the fast floating point C and C++ entries actually use x86 specific intrinsics (spectral-norm in particular). These are not written in anything resembling portable C, but on the other hand do represent how people use C in the real world for performance-critical applications.
Hi, Isaac. Didn't mean offense. Not compared to anything, but the shootout is widely considered to be unrepresentative of language performance, and more of library performance, e.g. use gmp and win; the rules for what different languages are allowed to do are often considered arbitrary and unfair. Obviously this as a problem to greater or lesser degrees with all competitive benchmarking.
Did you have the chance to test with a patched slice::swap? I think that using a custom reverse function is a bit of "cheating"
Also, I apologize for saying the shootout is 'bad'. I could have chosen my words more carefully.
PR is up: https://github.com/rust-lang/rust/pull/16999
This got stuck in the spam filter due to the is.gd playpen link, feel free to delete and resubmit; see the end of the sidebar for a work-around, in this case it could be [playpen][code] [code]: http://play.rust-lang.org/?code=struct%20A%3C%27a%3E%20{%0A%20%20%20%20c%3A%20||%3A%27a%2C%0A%20%20%20%20counter%3A%20uint%2C%0A}%0A%0Aimpl%3C%27a%3E%20A%3C%27a%3E%20{%0A%20%20%20%20fn%20inc%28%26mut%20self%29%20{%0A%20%20%20%20%20%20%20%20self.counter%20%2B%3D%201%3B%0A%20%20%20%20}%0A%0A%20%20%20%20fn%20iter%3C%27a%3E%28%26%27a%20mut%20self%29%20-%3E%20I%3C%27a%3E%20{%0A%20%20%20%20%20%20%20%20I{a%3A%20self}%0A%20%20%20%20}%0A%0A%20%20%20%20fn%20counter%28%26self%29%20-%3E%20uint%20{%0A%20%20%20%20%20%20%20%20self.counter%0A%20%20%20%20}%0A}%0A%0Astruct%20I%3C%27a%3E%20{%0A%20%20%20%20a%3A%20%26%27a%20mut%20A%3C%27a%3E%2C%0A}%0A%0Aimpl%3C%27a%3E%20Iterator%3Cuint%3E%20for%20I%3C%27a%3E%20{%0A%20%20%20%20fn%20next%28%26mut%20self%29%20-%3E%20Option%3Cuint%3E%20{%0A%20%20%20%20%20%20%20%20self.a.inc%28%29%3B%0A%20%20%20%20%20%20%20%20let%20c%20%3D%20self.a.counter%28%29%3B%0A%20%20%20%20%20%20%20%20if%20c%20%3C%205%20{%0A%20%20%20%20%20%20%20%20%20%20%20%20Some%28c%29%0A%20%20%20%20%20%20%20%20}%20else%20{%0A%20%20%20%20%20%20%20%20%20%20%20%20None%0A%20%20%20%20%20%20%20%20}%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20mut%20a%20%3D%20A{counter%3A%200%2C%20c%3A%20||%20%28%29}%3B%0A%20%20%20%20for%20n%20in%20a.iter%28%29%20{%0A%20%20%20%20%20%20%20%20println!%28%22item%3A%20{}%22%2C%20n%29%3B%0A%20%20%20%20}%0A%20%20%20%20println!%28%22len%3A%20{}%22%2C%20a.counter%28%29%29%3B%0A}%0A
It does seem like reverse can use an unsafe swap to fix the perf issue in the safe code. fn reverse(self) { let mut i: uint = 0; let ln = self.len(); while i &lt; ln / 2 { unsafe { let pa: *mut T = self.unsafe_mut_ref(i); let pb: *mut T = self.unsafe_mut_ref(ln - i - 1); ptr::swap(pa, pb); } i += 1; } } Am I missing a reason that doesn't work?
The much easier way is to use the `from_str` function. It works with anything that implements the `FromStr` trait which int does. It does return an `Option` so you need to handle the `None` case if the string is not valid. (i.e. 'vf' is not an int) fn main() { let x: int = from_str("32").unwrap(); println!("{}", x); } &gt; $ rustc foo.rs &gt; $ ./foo &gt; 32
I think you can do something like let num : Option&lt;Int&gt; = from_string("10".as_slice()); ...but I'm really new myself, so not sure. Edit: Compared to other answers, it looks like I'm wrong. You don't need as_slice(). =) But I don't think you should just call unwrap. I would use a match statement, since you don't know if your input is actually all digits. Edit again: Example match num { Some(num) =&gt; num, None =&gt; println!('Failed cast') };
Precisely, some sort of "not bounds checked" swap, so the stdlib can achieve performance comparable to @brson version.
I normally wouldn't, but the &amp;str comes from `regex!(r"^[1-9]\d*$")`... though I suppose it could [fail if the number is too large](http://is.gd/HU9E0f). Maybe I'll check it anyway.
The `as_slice` is not needed because a string literal is already a `&amp;str` (a string slice). If one was trying to parse from a `String` then it would be required.
It allows you to do low level programming without your programs crashing. So it's sort of a bridge between Haskell (very safe, but sometimes has performance issues) and C (very unsafe, but has excellent ways to tune performance)
Can you define "low level"? I hear that thrown around a fair amount
&gt; better support for C dependencies Then it should have a way to extract those C dependencies to build packages for Linux distributions. Ideally, a script should be able to turn the entire Cargo database into distro packages without manual intervention.
One point is memory management. Most languages let you to create variables at will (say, store the contents of this file in the variable my_file, etc). Those variables live in RAM of course. Bad things happens when RAM is filled up, so programs that run for a long period of time need to reclaim variables that aren't needed anymore, to reuse this memory for something else. Failing to do so is called a *memory leak* and everybody hates to use programs that leak memory. Anyway all the cool languages nowadays don't require you to manually insert a "I'm done with this variable" command (that is, they don't require you to manually *free* the memory). To prevent memory leaks, they use a technique called [garbage collection](http://en.wikipedia.org/wiki/Garbage_collection_(computer_science\)). GC is very neat but imposes constraints on your system. The worst of it is latency: many GCs have a "stop the world" phase where the program freeze for some milliseconds while garbage is being collected. This is undesirable for low-latency applications such as multimedia or gaming (note that a 60fps game has 16ms to generate each frame). Relying on a GC isn't a big deal for *most* software though. The alternative is to drop GCs and do manual bookkeeping of your variables. Whenever you're done with a variable, you must free them. This is very tedious and typically lead to many kinds of software bugs, including the dreaded [use after free](http://cwe.mitre.org/data/definitions/416.html). When you say "I'm done with this variable" be sure to really not touch it anymore, because its memory may be used for something else - so if you try to change a variable after freeing it, you may change *another variable*! Rust has another approach: it tracks the [lifetime](http://doc.rust-lang.org/guide-lifetimes.html) of variables so that when the compiler is sure that the variable can't be used anymore it will free it automatically. This is done at compile time so there is no need for a GC (but you can easily still use a GC for more flexibility). This has the potential of enabling safer code for tasks that are currently performed by C or C++ programs.
A question: can a parser like this be used as the sole parser of a real world browser? What about invalid HTML, XHTML 1.0, and other stuff?
Rust is an excellent language, but honestly, if you're new to programming you will probably not be able to appreciate it. Go ahead and learn it if you're interested, but you won't really understand what makes it great until you've dealt with hairy code in other languages.
FYI your proposed tour of byte formats is excessive, "10".as_bytes() should be enough.
&gt; Is there anything about it that would make ... Rust, unlike many other 'new' programming languages actually provides the facilities to implement arbitrary tasks. You may see things like 'python is great, but I would never write a game in it' or 'go is amazing, but you're doing it wrong if you use it for a task heavily suited to generics', or 'julia is a great computing language, but its not really suited for building websites'. There's certainly something to be said for domain specific languages that are very good at some specific things, but not at others; however, it's disappointing to start learning a language, only to discover later that it once you get past 'trivial examples', it's actually not suitable for anything you want to actually use it for. Rust, I feel, falls in that sweet spot between low level verbosity (c) and high level productivity (python say) while retaining the performance characteristics to make it viable, *and* targeting both desktop and mobile computing devices. That's a very powerful combination. Some other languages (notably C++ and go) also fall in this range, but with some caveats (Go's GC, C++'s manual memory management). Some other good things about rust: - /r/rust is helpful and active - irc.mozilla.org#rust is full of people interested and willing to help even complete new comers - the tooling is free on all platforms - http://play.rust-lang.org/ lets you test and prototype code, very very handy &gt; or break my interest in learning it? Unfortunately, it's not ready yet. A quick list of downsides: - The tooling is still immature; cargo is not feature complete and has some rough edges - Specifically the windows tooling is current pretty disappointing and difficult to setup - You'll encounter breaking changes where things you've written no longer run (probably good experience though, I think, but it will make it harder as a new programmer) - Some parts of the 3rd party packages are still immature, for example, it's still rather a poor choice to write a website in, lacking useful 3rd party modules for templating, database tooling (eg. migrations), etc. That said, I think the pros are pretty compelling. Go or C++ would also be very reasonable first choices.
Then `unsafe {}` galore it should be!
As someone who's relatively new to programming I kind of agree. Rust is hard to get your head around. Often there's times I have no idea what's going on, but regardless I keep moving forward. The more I move forward the more I enjoy it. After coming from Python to this, Rust has a different feel, more on the control front. Plus this community rocks!
its good to learn some complimentary languages.. python + C++ is a good pair. what languages are you learning already? Rust aims for the same space as C++, but is a cleaner design with new ideas on safety; it borrows more from the functional world. Learning rust , a lot of the ideas will go back to C++ as others have said. 
&gt;Can you define "low level"? I hear that thrown around a fair amount With all due respect, if you need to ask that, then this language probably is not for you. Rust is not a teaching language, or a particularly easy language to learn, without prior experience in coding. Look into [Python](http://www.python.org) ([the subreddit](http://www.reddit.com/r/python)) ([the help subreddit](http://www.reddit.com/r/learnpython)) for learning to program. To answer your question, you should not learn Rust, and I am only being curt because everyone else in this thread has probably said everything about rust you could understand and more already. Maybe one day when performance does matter, and a high level language like Python or Java will not cut it, you can come to Rust :)
By 'uninteresting' I just mean very predictable, i.e. changing the stdlib reverse will almost surely give the same speed up (and... since you implemented it, we know that it does :) ).
Yes, that would be just changing the implementation of `reverse` to the faster version, which is done in [#16999](https://github.com/rust-lang/rust/pull/16999).
&gt; The tooling is still immature; cargo is not feature complete and has some rough edges On the other hand, the common case of a normal binary/library with few basic dependencies etc works very well, so if you're not doing anything too fancy, dive right in. :)
I think fallback to a int is a poor decision, since it is not a known size type. It can make code slower or overflow when switching from one architecture to another. When porting an application, it is easy to search all occurences of the 'int' type and check if there is a problem. If any declaration can be infered to int, you have to check every piece of code. I agree fallback is needed, but I think i32 would be a far better choice. It is faster on 32 bit and 64 bit architectures (the most common), and IIRC even 16bit architectures handle 32bit numbers pretty well.
/r/playrust
I think fostering an appreciation for any modern convenience by deliberately avoiding it to provide contrast seems, well... silly. Society advances. I simply don't know what it's like to walk to school in the snow uphill, and that's *good*. If a new programmer starts with Rust and never experiences the malloc voidstar world of the past, that's *fantastic*. It means society (well, programming) has progressed. That being said, as a first programming language, any systems language, Rust or otherwise, might not be the best choice -- at least not without being concurrently taught about the general system architecture.
To be fair, C++ is used as a teaching language in plenty of places. And there is a tradeoff — it's harder and less people are able to effectively learn from it, however those who do end up being better programmers. I prefer python as a first language too, but I've seen people start off with C++ and it's not that bad. But yeah, Rust is a rather bad language for those without prior experience.
&gt; however those who do end up being better programmers [citation needed] ;)
I am new to languages but I won't be learning it for a while anyways. I just always hear that C++ is a common and useful language but then I hear that rust can do the same stuff but better and the language is easier to learn. I don't expect to jump right into it, it's just a language I might keep my eye on for when the time comes.
It's a personal experience thing :) Not rigorous, I know. Come to think of it, it's not exactly "better programmers" as it is "take less time picking up new languages". 
Rust has some quirks that C++ doesn't have, mainly centered around its use of pointers. It has a great standard library; but many of the functions work with &amp;str and other &amp; pointers. The "easier" part of Rust might be outweighed by the pointer machinery, if you have to use pointers. If you can avoid them (or learn how to use them), you should be fine :) #rust on IRC is quite helpful, too, as is this subreddit.
I just posted this on the YouTube comments, but I might as well include it here: Here are the slides (updated to include minified links to all playpen examples in the demo). [slides](http://pnkfelix.github.io/presentations/rust-demo-ml2014.pdf)
That is exactly the argument made in [the RFC](https://github.com/rust-lang/rfcs/pull/156): &gt; The available ABIs are conceptually much more like members of an enumeration than like arbitrary strings. 
One persons idiomatic is another persons idiotic. Wouldn't it be ridiculous not to show *"how people use C in the real world for performance-critical applications"* ?
Yeah I figured :/. I tried putting the full URL into the link but that got cut off. Didn't occur to me to turn it into a reference, thanks.
&gt; Not compared to anything In that case, how do we know that any-better-than "notoriously bad" is more than *wishful thinking* when it comes to cross-language performance comparisons? &gt; … widely considered … often considered … The question is whether you can justify *your assessment*, not whether there's snark on the internets. &gt; use gmp If it's all about arbitrary precision arithmetic, that does seem to be a good idea! Libraries matter. Libraries matter a lot. *otoh* specialized libraries, `gmp` and `pcre`, dominate 2 tasks, and that allows comparison with programs that don't use those libraries. &gt; arbitrary and unfair By someone who wanted to use inline C in their Ruby program? :-) &gt; the shootout Please don't refer to the project or website as "the shootout". Obviously that is not what the project is named. Obviously that doesn't appear in the URI. Obviously that doesn't promote thoughtful consideration of the different programs. 
&gt; You'll encounter breaking changes where things you've written no longer run (probably good experience though, I think, but it will make it harder as a new programmer) This is especially true if you're tracking nightly (which you should). It's often frustrating when you can't find any help because your code worked yesterday but won't compile with last night's build. However, the community is great on the #rust IRC channel. &gt; Go or C++ would also be very reasonable first choices. +1 for Go. I've been messing around with Rust because I'm interested in game dev, but I really hate C++. Go really isn't suitable for that (GC, among other things). I wanted something that makes concurrency easy, so Rust was a natural choice. Then again, C++ is also worthwhile because it'll teach you a lot about how memory works. I honestly prefer C to C++, but C++ is *far* easier to find help for and there are quite a few nifty features in the newer releases that make life easier.
Well, thinking a bit about it, it might not as easy as I originally thought; it depends whether you can impl: impl &lt;I,L,R&gt; Mul&lt;Quantity&lt;I, R&gt;, Quantity&lt;I, Product&lt;L,R&gt;&gt;&gt; for Quantity&lt;I,L&gt; { } impl &lt;I,N,D&gt; Mul&lt;Quantity&lt;I, D&gt;, Quantity&lt;I, N&gt;&gt;&gt; for Quantity&lt;I, Ratio&lt;N,D&gt;&gt; { } not sure Rust would be able to select the correct impl there, (what if `N` is a Product), but I would think that with enough patterns and a clever enough Rust the "maximum" pattern could be unambiguously selected. Of course, it might have trouble extending to more than two types...
This works: [link](http://is.gd/JguawZ). The reason your fn iter() does not work is two-fold. (changed 'a -&gt; 'r for clarity) fn iter&lt;'r&gt;(&amp;'r mut self) -&gt; I&lt;'r&gt; { I{a: self} } This creates a new lifetime 'r which is unrelated to the 'a which is used in the impl, remember that the 'a is the closures lifetime, not the lifetime of the A struct. The first thing we might do to resolve this is to remove the new lifetime from 'iter'. fn iter(&amp;'a mut self) -&gt; I&lt;'a&gt; { I{a: self} } This sort of works. If we remove the last line in main we can get the program running, however after calling 'iter' the object refereed to as self in the 'iter' call is now bound to the 'a lifetime making us unable to borrow the struct again. Finally we see that the solution is the introduce an extra lifetime on the iterator struct for I struct itself. We also need to add a lifetime bound on the first lifetime to make sure that the 'a lifetime outlives 'b.
I get it work by making `I` takes 2 lifetimes. fn iter&lt;'b&gt;(&amp;'b mut self) -&gt; I&lt;'b, 'a&gt; { I{a: self} } ... struct I&lt;'b, 'a: 'b&gt; { a: &amp;'b mut A&lt;'a&gt;, } ``` [playpen][code] [code]: http://play.rust-lang.org/?code=struct%20A%3C%27a%3E%20{%0A%20%20%20%20c%3A%20||%3A%27a%2C%0A%20%20%20%20counter%3A%20uint%2C%0A}%0A%0Aimpl%3C%27a%3E%20A%3C%27a%3E%20{%0A%20%20%20%20fn%20inc%28%26mut%20self%29%20{%0A%20%20%20%20%20%20%20%20self.counter%20%2B%3D%201%3B%0A%20%20%20%20}%0A%0A%20%20%20%20fn%20iter%3C%27b%3E%28%26%27b%20mut%20self%29%20-%3E%20I%3C%27b%2C%20%27a%3E%20{%0A%20%20%20%20%20%20%20%20I{a%3A%20self}%0A%20%20%20%20}%0A%0A%20%20%20%20fn%20counter%28%26self%29%20-%3E%20uint%20{%0A%20%20%20%20%20%20%20%20self.counter%0A%20%20%20%20}%0A}%0A%0Astruct%20I%3C%27b%2C%20%27a%3A%20%27b%3E%20{%0A%20%20%20%20a%3A%20%26%27b%20mut%20A%3C%27a%3E%2C%0A}%0A%0Aimpl%3C%27b%2C%20%27a%3E%20Iterator%3Cuint%3E%20for%20I%3C%27b%2C%20%27a%3E%20{%0A%20%20%20%20fn%20next%28%26mut%20self%29%20-%3E%20Option%3Cuint%3E%20{%0A%20%20%20%20%20%20%20%20self.a.inc%28%29%3B%0A%20%20%20%20%20%20%20%20let%20c%20%3D%20self.a.counter%28%29%3B%0A%20%20%20%20%20%20%20%20if%20c%20%3C%205%20{%0A%20%20%20%20%20%20%20%20%20%20%20%20Some%28c%29%0A%20%20%20%20%20%20%20%20}%20else%20{%0A%20%20%20%20%20%20%20%20%20%20%20%20None%0A%20%20%20%20%20%20%20%20}%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20mut%20a%20%3D%20A{counter%3A%200%2C%20c%3A%20||%20%28%29}%3B%0A%20%20%20%20for%20n%20in%20a.iter%28%29%20{%0A%20%20%20%20%20%20%20%20println!%28%22item%3A%20{}%22%2C%20n%29%3B%0A%20%20%20%20}%0A%20%20%20%20println!%28%22len%3A%20{}%22%2C%20a.counter%28%29%29%3B%0A}%0A%0A
Great talk; thank you for sharing. As someone who only encounters discussion about Rust via reading, it was nice to hear the "tick ident" phrase for speaking lifetime syntax aloud. Just that one little moment will change how I read Rust source. So, again, thank you. Does anybody want to offer any discussion about the interesting question at the end (~28:03) about linear types not playing nicely with modularity in large scale systems programming? I'm wondering if I should find the given answer satisfying or if there's more to say on the topic.
Rust actually has volatile versions of `copy_memory`, `copy_nonoverlapping_memory` and `set_memory`.
Sorry I'm a week late, but the Rust compiler is itself written in Rust, so you could try compiling that.
I agree. IMO, it would have been be better to rename `int` to `index` or `intptr` but it was not accepted.
There's still a [discussion](http://discuss.rust-lang.org/t/if-int-has-the-wrong-size/454) about renaming them.
It just is a great language, it is low-level and C like with things like references. But it also encourages to use immutability and functional programming, without being all pure and strict like say Haskell. It also has traits, which are basically just Haskell typeclasses, which in turn are just really powerful interfaces. Their strength is that you can basically implement interfaces for every type you want, even ones in the standard library. As someone who has used Haskell before, I can say that it's a wonderful language. But of course there are also cons. * It may not feel intuitive at all if you've never used a functional language or immutable types. * It changes really rapidly, although this means that your code must also change very rapidly it also means that Rust will become really natural when it becomes stable. If the devs want a new feature or change something to be more intuitive, they just add it. I think that will make for a really practical language in the end.
Rust is currently premature but that is only temporary. You don't have to restrict yourself to only a single language. Try it and see how it goes. [Rustbyexample](http://rustbyexample.com/) and [the guide](http://doc.rust-lang.org/guide.html) set the barrier pretty low as far as trying things out (although, windows support may currently only be mediocre). Rustbyexample in particular because everything can be run in place (without downloading anything). Concepts in one language extend to others so learning one helps with others. --- Some of the following comments about Rust may also extend to other language like python. For example, python has a well organized and seemingly comprehensive documentation set on their main site which is a huge help. Other (newer) languages may also. If you intend to learn it using the internet only (without printed books) then C++ may not be a good choice (in my experience). This [C++ tutorial](http://www.cplusplus.com/doc/tutorial/) is pretty good but it is extremely brief compared to a book like [Accelerated C++](http://amzn.com/020170353X) which is compact, detailed, and covers much more material than the tutorial will (it may be difficult for a beginner though). Rust doesn't currently have the luxury of referring to good books for instruction. So best practices and coding conventions are being baked directly into the compiler/[guidelines pages (currently WIP)](http://aturon.github.io/)/[the guide](http://doc.rust-lang.org/guide.html). This is really convenient compared to C++ where resources are much more scattered (aside from books). Inevitably, if you try writing something though in either language, you will get confused/perplexed by something regardless of the quality of documentation. In those cases, having IRC for help is incredibly helpful. They can save you hours upon hours of mystification. The Rust IRC is extremely extremely helpful in that regard. Rust has *cargo* which makes testing new things incredibly easy. My experience with C++ is you find a new repository you want to test, you download it and spend the next 2 hours trying to get dependencies in order so you can compile it. With cargo, you often run this and you're done: git clone site:repository cargo build The C++ compiler is notoriously unhelpful also which extremely confusing especially to a beginner. It is commonly the case that it finds some issue on line 12. When you eventually find the error, it's on line 20 (the compiler points you to the wrong location). What kind of issue could it be...maybe you forgot a semicolon. In general, the rust compiler is much more helpful with regard to error messages. If there is a confusing error message, the compiler team would like to make it better. If you're still confused, there is always IRC. C++ has a lot of IDE support which is very helpful for advanced projects. Rust currently doesn't (Python doesn't seem to either). This will probably improve in the future. Also, IDE support often costs money depending on the language.
Honestly I'm getting tired of cross-posting between /r/rust and /r/programming ... It went on the [front page](http://www.reddit.com/r/programming/comments/2fi78v/how_to_zero_a_buffer/) of /r/programming and is 18 hours old. I know I'll get downvoted for that but... c'mon guys.
There are certainly patterns that are less elegant in Rust, like the observer pattern, because you need to pass in the environment. Capturing multiple references is not permitted in closures, even you know each closure will be called one by one. Since you need to organize the environment somehow, you need a think a bit about how it is structured and how to separate the input from the output. For example, in game logic an enemy attacks the player, but you can't mutate the player since the enemy is a borrowed mutable into the array of game objects, which also contain the player. This can be worked around by buffering all the attacks and summing them before applying it each object. I will say Rust puts more constraints on design, but the same constraints "gravitates" you toward a local minima. For large scale systems it might happen that you get stuck in the design process. It may not be a pleasant experience. On the other hand, you can get lucky and hit jackpot. Using Rust in large scale systems usually means the compiler and ergonomics has more influence on the design than "the right pattern" by the books. In the [Piston](http://www.piston.rs/) project we are moving away from typical OOP patterns and toward iterators, event threading, immediate mode user interface etc. We started with "normal patterns" and failed, ending up with designs that I think look dumber and dumber, but for some reason are very effective. Perhaps is this a limitation with linear types, or perhaps we are starting to explore something better? One thing for sure, I am spending less time debugging, and if it compiles, it usually works...
I suspect there's a lot of people who only visit reddit for /r/rust or at least don't follow /r/programming, and naturally stuff gets posted here if it's interesting to consider from Rust's perspective whether /r/programming has already been discussing it or not. I sympathize with not wanting to see the same link half a dozen times, once for every programming community you subscribe to that feels impacted and again for HN or whatever, but usually at least I personally appreciate the rust-centric comments even if I've already seen another discussion elsewhere.
&gt; For example, in game logic an enemy attacks the player, but you can't mutate the player since the enemy is a borrowed mutable into the array of game objects, which also contain the player. This can be worked around by buffering all the attacks and summing them before applying it each object. I wonder if we should just make a vector that does dynamic borrow checking on which elements are mutably borrowed. I think that could be made quite efficient since you could just pack borrow flags together.
It's not a stretch to create a text post that said: &gt; Hey guys! Saw an interesting post about C compiler guarantees of wiping memory... let's talk about how we can ensure that happens in Rust!?
I don't see what that buys us, it's just boilerplate text adding no info but necessitating an additional click to get to the post?
Oh shit sorry. I'll just cross post everything in /r/programming and not bother to explain the relevance.
Say someone, like OP, didn't know this function existed and wanted to go about figuring this out on their own using the [rust docs](http://doc.rust-lang.org/std/). The first place someone might look is on int (after all, other languages like C# have int.Parse() and int.TryParse()). After searching int, you get a number of results. The FromStr implementation information is on the third result. Is there a better way to go about figuring out these sorts of things on your own? Like a list of these global static functions, or are these just a thing which one needs to memorize after reading some introductory text on the language?
If discussion is not closed, my combo solution would be : - rename the current `int/uint` types to `index/isize/usize/`... or any other name. - inference fallback to a new `int` type since it is the name everyone will expect for the default type. - the new `int` type would be an alias for `i32`, since it seem to me the best fit on most architectures for a default type. - maybe (not sure it is a good idea) a compiler switch might allow to change the fallback type.
Not sure what you mean. Can you give an example of how this would look like?
Write the pointer type like `*const ST`, and use `*mut ST` for the C type `ST*`.
Am I the only person who thinks `i32` looks nicer than `int`? My preference would be to scrap the `int` keyword altogether in favour of one of the renamings.
&gt; Am I the only person who thinks `i32` looks nicer than `int`? I suspect so. :) But I agree about not keeping `int`. The premise behind having an `int` type is that there's a clear choice of type which you should use "most of the time", and then we can define `int` as that type. But I'm not sure that the premise holds. Maybe we can't escape having to think &amp; make decisions.
`objects.borrow_mut(10 /*index*/).attack(objects.borrow_mut(1 /*index*/))` where `borrow_mut` returns something that can be deref'd to `&amp;mut`. It dynamically makes sure you can't have two mutable references to the same object (by comparing indices at runtime).
(In [`std::intrinsics`](http://doc.rust-lang.org/master/std/intrinsics/) if anyone is looking.)
`'a: 'b` means that `'a` lasts at least as long as `'b`. This is required here because the `&amp;'b` reference needs to always be valid, so the data it contains needs to be valid (which is only the case if `'a` is longer).
What about `let (a, b) = objects.borrow_mut((10, 1)).unwrap();` which checks the indices in the tuple that they are not the same and returns a tuple with two mutable borrows?
sorry.typo. int update(const ST *st); i guess. const ST* is a const pointer to ST *const ST is a pointer to const ST so define them in rust , must be different , or did i wrong?
i do not realy know c language. const ST* *const ST are different, aren't they? or they are the same thing in rust language?if not , how to define these two type?
I'm going to give a different answer from others in this thread: Because Rust is a much better-designed language than almost any other mainstream or semi-mainstream language (potential exceptions: Haskell, Swift). This means that, "pound for pound", more of your time and effort spent learning will go toward learning things that are actually important, meaningful, and broadly applicable, instead of dealing with the [accidental complexity](http://en.wikipedia.org/wiki/No_Silver_Bullet) and messiness arising from problems the given language creates for itself with bad design. Learning about generics (or "parametric polymorphism") is fundamental knowledge that will stay with you anywhere you go. Learning about the particularities of C++ template instantiation is something that will only be relevant to you when you are programming in C++. A case could also be made for learning Haskell instead, which has a simpler, cleaner and more elegant core, but the correspondence between analogous features in Rust and other mainstream languages tends to be much more obvious, whereas Haskell has a totally different syntax, terminology, philosophy, and background (so the correspondences are still there, but you have to look deeper to see them).
No RFC as of yet I don't think.
That sounds reasonable, but it then needs to happen. Far too often the response to sensible requests is "but if we do X instead it will be better" and then X is never done.
Does this run afoul of any Minecraft copyrights/trademarks/ect.? For instance, does the project come with their texture pack?
Nope. You have to extract the textures from the minecraft assets yourself and add them.
I understand, and that's what is kinda drawing me to rust even if I'm not capable of learning it yet, thanks for the reply
A great example of how to present Rust to an FP crowd. Thanks!
This kind of reminds me of the [collection views](https://github.com/rust-lang/rfcs/pull/216) proposal.
&gt; In the Piston project we are moving away from typical OOP patterns and toward iterators, event threading, immediate mode user interface etc. How much do you use mutable contexts? I have personally found these extremely difficult to reason about in Rust code. I sort of get the feeling that Rust solves the safety issues surrounding mutability, but it doesn't address the how hard it can be to reason about and test. I'm starting to think that we should be focused on trying to encapsulate mutability in pure interfaces, relying on moves to ensure efficient code. Whilst mutability is important for performance in some instances, we still need to be disciplined in reducing our dependence on mutable state. `&amp;mut self` should be a warning that you are doing something that has the potential to become a burden of technical debt in the future.
I'm not a hematite dev. I just did this one thing.
Why are there so many of the benchmarks completely missing a rust implementation?
Because they haven't been implemented or haven't been submitted. See https://github.com/rust-lang/rust/tree/master/src/test/bench for all of the ones implemented (not all optimised though).
Can we at least mark those posts somehow? I.e. there should be a tag like "[non-Rust]" for posts that are not directly related to Rust but still brings something in perspective, and in which the case the poster is required to give a context and rationale.
For the curious/lazy, about the flag: http://clang.llvm.org/docs/UsersManual.html#options-to-emit-optimization-reports
The RFC has a huge section on it: https://github.com/nikomatsakis/rfcs/blob/restore-int-fallback/active/0000-restore-int-fallback.md#motivation
If your data structure is Copy, you can use the builder pattern, or type currying with nil as default. If you then inline the methods with 'self', the compiler will optimize statements while getting better expressiveness than '&amp;mut self'.
Always read C declarations right-to-left: const ST * is a pointer to `const` ST (you can modify the pointer, but not the pointed-to variable). Whereas... ST * const is a `const` pointer to ST (you can't modify the pointer, but you can modify the pointed-to variable).
Probably means we should start not failing on it on Travis, at least ;)
I've given a few talks before ([Slides here](http://manishearth.github.io/Presentations/Rust/)), but I won't have the time to prepare for a highly professional one like the above (plus I live in a different hemisphere, and I'm not an awesome Rustacean myself so there are bound to be mistakes). I don't mind helping whoever is interested, though. It would be neat if we had a well-produced video tutorial. I've seen taped talks on Rust before, but they either focus on analyzing the awesomeness of the memory model, or are a rough overview. They seem to want a more code based one ("The talks will be code-heavy and philosophy-light.")
Oh neat! I live in NYC too...
thank you very much. ps: pointer in c make me confused.
Huzzah! Try it, the Stack Exchange folks are a really fun bunch.
I don't think LLVM is the problem?
It's an argument for why compilers should implement important building blocks like nonremovable buffer zeroing -- so that it's easy, correct and portable, so that in turn the software can be safe.
It's not; I'm saying that we should start ironing out Rust's issues with LLVM3.5 (till now it seems like rustc-with-llvm3.5 is rather unstable; at least from lurking for the past few months) Just a joke.
Rust doesn't use the released version of LLVM, e.g. Rust head is currently canonically compiled against some ['random' commit](https://github.com/rust-lang/llvm/tree/rust-llvm-2014-09-01) that's actually a month or so *after* the 3.5 release was cut.
&gt; (Slides here, feel free to steal) Link seems wrong.
See the [follow-up](http://www.daemonology.net/blog/2014-09-06-zeroing-buffers-is-insufficient.html) too.
Fixed, thanks :)
Parens left unclosed. :P
cool :) By the way, what about [/r/rustcodereview](/r/rustcodereview/)?
I reserved it in case this happened so often it crowded out regular old posts. That hasn't happened. yet.
I'd take a PR for sure :)
This is quite a long read. Could someone write a TL;DR about how this changes the life of a regular rustican, and what do we need to make use of it?
Came here to say, "Oh hey, steveklabnik lives in New York..." :)
Bah :P
I'd say that today "low level" is more or less interchangeable with "cache friendly", because being cache friendly has become by far the biggest concern when you're trying to write a fast program. Any language that hides from you when objects are created, how they are laid out in memory (stack or heap) and when they are destroyed is not low level, and will probably encourage you to write programs that are slower than if written in a low level one. For me the killer feature of Rust is that like C++ it "embraces the stack" and it allows you to transparently place objects there... it's amazing how many languages in the past were designed to work like if the fastest and safest memory at your disposal doesn't exist. Anyway, all of this information is definitely a cognitive load on the programmer, so if you're starting out it's probably better to do it in a language where you don't have to worry about these details. You can learn "performance" later! 
Eventually, there will be more Commands, but you're right. Can you expand on 'automatically'? EDIT: oh, oh, the descriptions. Yes, they should be better than placeholders, which would make it non-automatic. I haven't thought of what theme I want it to be yet. :)
&gt; This is quite a long read. Could someone write a TL;DR about how this changes the life of a regular rustican The time it takes to compile will decrease at the cost of producing worse (less optimized) code. Mainly will make a difference if you have a very large project. &gt; what do we need to make use of it? Should be either some sort of flag or option to turn it on, or it will be on by default when specifying lower optimization modes.
They got back to me and they're interested, so that's pretty cool!
Ahhh thank you! I felt bad about the is_some, but I forgot about any.
If they will remain string literals, consider using `&amp;'static str` instead of `String` in the `Room` struct.
This should do it: type WNDPROC = extern "system" fn(HWND, UINT, WPARAM, LPARAM) -&gt; LRESULT; Also make sure to use `#[repr(C)]` as /u/qrpth noted, or Windows won't understand your structs.
For 32 bit WinAPI it's surely needed
There is some source in https://github.com/tomaka/gl-init-rs that might help.
I think the goal is to not duplicate parts of the API when the conversion is basically free.
Editorialized title. You might as well have written "why Rust is insecure".
I think so. But rust is a developing language, so this can be fixed.
And I probably should. Too bad there's no “edit” buttion.
If you want to rely on Rust memory allocation, then you can write create and destroy functions simply like that: use std::mem::transmute; #[no_mangle] pub extern fn Widget_Create() -&gt; *mut WidgetImpl { unsafe { transmute(box WidgetImpl::new()) } } #[no_mangle] pub extern fn Widget_Destroy(pimpl: *mut WidgetImpl) { let _drop_me: Box&lt;WidgetImpl&gt; = unsafe{ transmute(pimpl) }; } Edit: Oh, now I noticed that at the end you too had already noticed that transmuting from and to boxes is most convenient way to do it.
As far as speed is concerned, the call to `as_slice()` should be completely optimized away.
Ok, I got it, there is no cost on `as_slice`. But, anyway, there is `as_slice` all over my code. I'm not really an *OCD* person when it comes to code but it starting to hurt readability...
Good idea, I might implement the trait(s) as you suggested. I just don't get why this isn't on the standard library. Looks like something I'm going to be always using...
Yes, I came to this solution in the end, just after realizing how `Rc&lt;T&gt;` implementation works. (I should say this aspect of transmute is a bit surprising (in a good way?), because reinterpret_cast always acts on pointers/references an never has value semantics, except for "integral &lt;-&gt; pointer" conversions) The trick should probably be added to the FFI guide - of course it's always useful to spend a couple of hours digging the library and the sources at github, but there should be an easier way :) 
Yes, `transmute` is a little bit different than `reinterpret_cast` in C++, because it operates on values, not pointers. But as soon as you see `transmute`'s signature `(T) -&gt; U`, it shouldn't be surprising that it moves and consumes its argument (if you are aware of move semantics). You can add this trick to the FFI guide if you find it helpful!
It's not in the standard library because the design of the standard library hasn't been finalized yet.
i thought WinUser.h:62 =&gt; typedef LRESULT (CALLBACK* WNDPROC)(HWND, UINT, WPARAM, LPARAM); means WNDPROC is a pointer. so , i defined a new type for WNDPROC pub type WNDPROC = *extern fn(HWND,UINT,WPARAM,LPARAM)-&gt;LRESULT; but you said i should : type WNDPROC = extern "system" fn(HWND, UINT, WPARAM, LPARAM) -&gt; LRESULT; without * , WNDPOC also a function pointer? 
Rust is memory safe so the problem of buffer read vulnerabilities doesn't exist for the vast majority of code. Sanitizing data is not solving the root of the problem, only reducing the likelihood of the attacker getting sensitive data with each use of an arbitrary read vulnerability.
No, doing something for data fields and types won't accomplish anything. The safe abstractions everything is built on are not aware of sanitization. For one example, a vector or hash table doesn't zero the old memory when it reallocates. I haven't seen any actionable suggestions that are applicable to Rust. The author's proposed C language extension is out of the scope of the Rust compiler or Clang. It is up to LLVM and compiler-rt to implement a feature like zeroing the stack frame and registers after it returns.
I don't see why the Rust compiler would be more suited to implementing sanitization than Clang or GCC. The only concrete thing mentioned in the article is a way to sanitize the stack frame and registers before returning from a function, which is something LLVM / compiler-rt would need to implement, not `rustc` or `clang`.
I've been away from Rust all summer, and I'm astounded to see how far Cargo has come in the meantime! Supremely exciting that it's able to handle such a large and complex project as Servo. Congratulations to all involved!
It's nonsense either way. It doesn't provide anything when there's no arbitrary read vulnerability in the first place. C is insecure because it's a memory unsafe language without the ability to build safe abstractions. Programs written in C are going to have lots of arbitrary read / write vulnerabilities in practice, and sanitization will only reduce the chance of a single use of the vulnerability recovering valuable information. Sensationalizing an issue just means people aren't going to listen to you - I started reading the article with the impression that the author didn't know what they were talking about because I didn't realize the title was changed.
Zeroing buffers certainly does something in programming languages without arbitrary read vulnerabilities. Assuming your language has absolutely no invulnerabilities (which is a strong assumption even for Rust, unsafe code exists), things like cold boot attacks can read keys from memory. In a still running process, its also possible that pages with keys on them would be swapped on and stored on a swap device (which means a power cut could cause these keys to be on persistent storage). Cleaning up after yourself in cryptography is a concern because of system security, not programming language security. The only security feature required from the programming language/compiler is the ability to clean up after yourself (which is what this post is about).
&gt; It can only slightly reduce the probability and scope of damage from each exploitation attempt. I would argue this is more than a slight reduction, but that's just a difference of opinion. &gt; No, the post only mentions the ability to zero the stack frame and registers on the exit from a specific function. I confused this thread with the earlier post from the author about memset's getting optimized out of binaries. Regardless, I would still count clearing registers and the stack as cleaning up after yourself. The general idea of both posts is how difficult it is to sanitize after a function call. The point of my post is that regardless of the type safety features of Rust, systems running Rust programs would still be more secure if Rust supported the sanitation features suggested in the two posts. I do agree that these features are less important in Rust than in C, but they still do make Rust programs more secure. Do we agree on that?
&gt; Do we agree on that? No, I don't agree with it. It doesn't eliminate a class of vulnerabilities and would add more low-level compiler code generation and complexity to `unsafe` code. It has a far larger chance of creating a memory corruption vulnerability than it does of ever preventing one. Memory sanitization is only a weak form of damage reduction unless it's being applied in a systemic way to eliminate a specific class of information leaks. Rust doesn't suffer from pervasive buffer overflows on the stack since it can't occur in safe code and even `unsafe` code doesn't use the typical culprits like C strings. Security isn't about adding more features and complexity. It only makes sense to add a memory exploit mitigation when it can wipe out vulnerabilities that are known to be a problem in the language. I don't think stack canaries make sense for Rust either, especially since the implementation in GCC / LLVM is only a small obstacle for exploitation. It is more of a liability than a defensive measure. If people want to use `volatile_set_memory` to reduce the lifespan of sensitive data, that's fine, but adding complexity to many low-level library features and the compiler for dubious reasons is not. Zeroing the stack frame and registers as an LLVM feature makes sense, and exposing that takes no effort - just a few lines of code to add an attribute. If it turns out to be buggy, that's only a problem for LLVM and people who decided to use the feature. Adding 20-40 lines of code to `Vec`, `HashMap`, `RingBuf` and more to support sanitization would be a liability.
thank you! it works now!!!!
&gt; You can add this trick to the FFI guide if you find it helpful! While I learn Rust I regularly have the desire to fill up some missing parts in the documentation/guides, but, unfortunately, English is not my best friend, so I didn't even tried.
While I think features sound extremely useful, I don't believe that they will really solve this specific problem. The issue is that compiling for a specific build target (msys, visual studio) is a compile flag; it's like -DFoo in cmake. The features specifically state: In almost all cases, it is an antipattern to use these features outside of high-level packages that are designed for curation. If a feature is optional, it can almost certainly be expressed as a separate package. ...but having build configurations is something that *low level* packages need, not high level packages. As I see it there are two issues: 1) How do compile flags get passed to the build step (both for dependencies and rust code)? 2) How do you ensure compatible naming of build flags? Consider this dependency tree: myapp -- rust-web -- rust-template-mako -- rust-orm -- rust-orm-mysql -- rust-mysql -- rust-watch -- rust-libuv -- rust-fstat Lets say rust-orm offers various features, eg. mysql; however, the rust-mysql library has two variations; one which compiles a static mysql driver, and one which uses the system mysql library. In the former case different linking arguments are required in msys and non-msys builds. Additionally, rust-watch (a fictional package to watch for file changes) can choose to either use rust-fstat (stats files manually) or rust-libuv (fs-events) to watch for file changes. Finally, the rust-libuv package can be built in multiple different ways depending on the environment; specifically if cargo is invoked in an msys environment it needs to link the msys libraries, and if invoked in a non-msys environment it needs to the link to the native windows libraries. Trying to express this with features seems like it won't work. - rust-orm: offers mysql as a feature - rust-watch: offers libuv and fstat as features - rust-libuv: offers msys or visuals studio as features - rust-mysql: offers msys, system or vstudio as features What does myapp do? rust-web isn't offering features; it simply requires the child repositories. Or does rust-web have to wrap all possible feature combinations or all dependencies in it's own cargo.toml? Notice that rust-libuv and rust-mysql offer differently named features, so the application level configuration for rust-web will have to then offer features: mysql, libuv, visual studio, vstudio, msys, fstat (ie. all possible combinations of child features) 
I actually thought about some marking of data that then gets passed as metadata to llvm. But I wonder how this would work out with pointers and how hard it is to find out which data is actually sensitive.
From a different angle :What are you doing that requires so many string manipulations that as_slice is getting in your way?
Most are renames, but our code review tool just marks them as delete-creates. I had to wade through that yesterday while reviewing. Not fun. :P
So it looks like `as_slice` is going to be renamed `as_str` - that's good, I think it clears up the confusing terminology.
Yes but since rust allow unsafe block and FFI. This kind of problems can happen in rust programs so it is not fully off-topic
And then Travis has to come and crap on the party. 
I got to thinking about how this could potentially be generalized: First, maybe we could pass in any number indices in an array, and get the same number of mutable borrows back. But without compile-time parameterization over `uint`s, I don't see a way to do this without allocating. (But maybe you could pass in a slice, and get back some kind of iterator?) This would also be a really good use case for HKT and a `Functor`/`Foldable`/`Traversable` hierarchy. You'd pass in any data structure of indices, `DataStructure&lt;uint&gt;`, and get a `DataStructure&lt;&amp;mut T&gt;` back which has the same "shape", but with borrows where the indices were. The part of this where you compare indices at runtime and then get a "proof" of their inequality which you can carry around also doesn't feel like it should be `Vec`'s business; it could be split out as separate functionality. With just two indices, you'd have something like pub struct InequalIndices(uint, uint); // priv fields pub fn test_inequal(a: uint, b: uint) -&gt; Option&lt;InequalIndices&gt;; pub fn get_indices(two: InequalIndices) -&gt; (uint, uint); // vec.rs: pub fn borrow_two_mut(self: &amp;mut Vec&lt;T&gt;, indices: InequalIndices) -&gt; (&amp;mut T, &amp;mut T); But isn't there some kind of data structure which guarantees inequality of elements by construction? Oh, right... pub fn borrow_many_mut(self: &amp;mut Vec&lt;T&gt;, indices: &amp;HashSet&lt;uint&gt;) -&gt; HashMap&lt;uint, &amp;mut T&gt;; But that again requires allocating, and to avoid it you'd probably again need to be able to compile-time parameterize on the number of elements. So... this is just brainstorming, and most of it doesn't appear to be actionable in the current language. But I hope it's food for thought, and maybe someone else can come up with clever substitutes for the missing things.
Will this eventually connect to Minecraft servers? If so, I love this.
It provides a really pleasant, extensible infrastructure for writing commands (`mach build`, `mach wpt-test`, etc.) that wrap your regular tools (so we get `mach wpt-test path/to/test.html` instead of `WPTARGS=--include=to/test.html make check-wpt`, for instance). It's also got structured logging baked in and a host of other goodies.
&gt; From a different angle :What are you doing that requires so many string manipulations that as_slice is getting in your way? For me it's parsing. 
If you are beginner, better start with something like Ruby or Python.
You need to specify a lifetime bound on the `RuntimeCommand` trait within `Box`, e.g. `Box&lt;RuntimeCommand + 'static&gt;`. This tells Rust that the concrete type in the box either does not contain references, or only contains static references. If you want to be more flexible you can parameterize it over a lifetime variable as usual, but this is not necessary for your example.
Built this after looking for the same functionality as `ghc-pkg dot` and not finding anything. I'm pretty new to rust so I'm looking for code review. I'm especially curious about whether or not I'm handling Strings in an idiomatic way because I had a lot of trouble with them.
The post is about mitigating damage from environmental attacks: - compromise OS + kill/stop your program + access the program memory/stack/registers/file handles/... - power off the computer + try to do a cold read of RAM state, ... I don't think a programming language can solve these problems, but the discussion on the post is about mitigating damage in case these attacks happen. The approach of marking a function with an attribute such that the compiler automatically generates code to sanitize the stack and the registers used by that function does _help_ to mitigate the damage. How does Rust approach to memory safety help to mitigate the damage or solve the root issue of the problem?
Thanks for the reply. Just to be clear, the 'static lifetime lasts for the program duration right? For this example, that would be fine, but I'm picturing a scenario where many commands are being created throughout a relatively long program duration. In that situation, I probably would want the lifetime variable. Do you think it would be bad practice to introduce an argument to the function simply to control the lifetime of the return value? I actually like it, but it does seem a bit strange. I updated the gist to include this unused "lifetime anchor" variable. I actually like it because it led me to grok lifetimes, but it also leads to an unused variable warning. I wonder if there's a better way.
It doesn't make sense for a function type not to be a pointer, so the * is redundant in C and I have no idea what it would do in Rust other than maybe make a double pointer.
I looked at doing that, but I couldn't find anywhere that defined a way to load the .lock files, only produce them, though I probably just missed it. I am using the same toml library that Cargo uses, though.
This is pretty awesome, nice work! If you distribute this as `cargo-dot` executable then other projects can actually invoke this by just running `cargo dot` (or some similar name). This is certainly a popular request, as we've had a [tracking issue][issue] for some time now! [issue]: https://github.com/rust-lang/cargo/issues/460 As dbaupp mentioned, you may be interested in using cargo as a library. You should be able to depend on cargo directly and use something like this: 1. [Load a path source][1] 2. [Load the lockfile][2] 3. (maybe) [Generate a lockfile][3] 4. Have some awesome [cli flags][4] [1]: https://github.com/rust-lang/cargo/blob/b3a9dee814af4846267383c800999a42b295e0d2/src/cargo/ops/cargo_generate_lockfile.rs#L22 [2]: https://github.com/rust-lang/cargo/blob/b3a9dee814af4846267383c800999a42b295e0d2/src/cargo/ops/cargo_generate_lockfile.rs#L96-L109 [3]: https://github.com/rust-lang/cargo/blob/b3a9dee814af4846267383c800999a42b295e0d2/src/cargo/ops/cargo_generate_lockfile.rs#L31-L39 [4]: https://github.com/rust-lang/cargo/blob/b3a9dee814af4846267383c800999a42b295e0d2/src/bin/generate_lockfile.rs#L9-L19 Also, if you feel like `toml-rs` is letting you down, feel free to let me know if there's anything I should add!
&gt; but you're greatly exaggerating the gains it can offer. I don't think so. If I zero a buffer and the compiler removes the operation because it determines that it is a no-op within the ideal machine model assumed by the C/C++/Rust spec I would like a way to override that decision and force the compiler to execute my code since in the real system my program runs on that code could have very visible side-effects. The author tried a ton of hacks in his other posts and although he comes pretty close it is still not working. The whole situation is just nuts. &gt; There's a huge difference between a language lacking a damage mitigation half-measure and the language being insecure. C and Rust are not "insecure" because they lack a way to guarantee that registers are zeroed after certain calls. Sensationalism and misinformation doesn't help to get people on board with the idea of implementing more mitigations like this. There are far more valuable exploit mitigations like address space layout permutation that are not implemented by any mainstream compiler / linker, but yet no one is going around calling them insecure because that would be foolish. What are you talking about? Who says that these languages are insecure? Someone writes a post about compiler optimizations ignoring real-world side-effects and you seem to take it as an attack on Rust or C (who if you would follow his blog the author is a huge advocate of). &gt;Nope, it doesn't do anything to mitigate damage in the scenarios you've presented. There are cases when it can mitigate damage (arbitrary read vulnerabilities) but you're greatly exaggerating the gains it can offer. How does security against read vulnerabilities matter here? The program used to read the memory can be written in any language so it doesn't matter in which language the target program is written on.
This is brilliant! [This](http://i.stack.imgur.com/wkHTy.png) is what Servo looks like. -sys crates are all C bindings, and the _traits crates are just to decouple crates for cleaner compilation. Some things might be missing, but it doesn't look like it. I'm surprised it worked, actually, since both rust-deps-graph and Servo cargoification are quite new and probably unpolished. If it worked for Servo, it probably should work for everything else. Kudos! :D
I'm puzzled. It uses ` extern crate graphviz` but the crate isn't declared or used on Github. So, I'm puzzled where it is?
It's actually a default crate bundled with rust: http://doc.rust-lang.org/master/graphviz/
Solid musical tastes.
I added cli flags and then I tried using the `cargo` lib directly, but I ran into a weird issue. In order to load the lockfile, I need a SourceId, which I would presumably have from the Cargo.toml file, but it's not used for anything of relevance to my code so I just used a dummy value: https://github.com/maxsnew/rust-deps-graph/blob/cargo-lib/src/main.rs#L33-L37 . I only want to have to look at the lock file (in case the person doesn't have the Cargo.toml file in the same place for some reason). It's working though so I guess it doesn't matter too much :) Also, how do I get rid of the "warning: using multiple versions of crate `docopt`"?
&gt; In order to load the lockfile, I need a SourceId This is actually the SourceId of the *root* package, and it mostly just affects the output. You're right in that you probably don't have to worry too much about this if all you're doing is building the dependency graph. &gt; Also, how do I get rid of the "warning: using multiple versions of crate docopt"? Ah it looks like we're using burntsushi/docopt while you're using docopt/docopt (and cargo compiles both separately). We should probably change what cargo points to!
The compiler wants it to be able to print the control flow graph (via `--pretty=flowgraph`).
Let's discuss this issue from a cryptographic point of view and discuss the threat model, since there seem to be some misunderstanding about it. Let's assume that we have a cryptographic network protocol with forward secrecy. I.e. a protocol that somehow generates new ephemeral keys for each connection, and where an attacker that gets their hand on a key at a certain point of time cannot decrypt older traffic. The threat model is that an attacker intercepts all network traffic (but cannot read it, since it is encrypted). At a later point in time the attacker gains access to one of the computers running the protocol. This would, as many have pointed out in the comments, lead to a complete exposure of all current and future information transmitted. However a system with forward secrecy shouldn't allow an attacker to use the current information (keys, data, etc. ) to decrypt the old captured information. The problem is that since there might be traces of old ephemeral keys, the attacker might be able to use those to decrypt the old captured traffic. I.e. a problem that cryptographic protocol set out to solve reappeared because of implementation issues. So the reason the blog post argues about a flag for sensitive information is to solve that specific problem. 
Also, for anyone looking at those slides: The Cell example in the slides is imperfect because it uses an enum with two int variants rather than an `A` with code and a `B` with data. (I was too focused on fitting everything onto one slide when I made it.) This is a better illustration of the point I was trying to make; or at least, it forms a closer analogy with the earlier examples of aliasing: http://is.gd/njNMj8 or better still, this: http://is.gd/ULWnOX
Nice. This looks like a classic graph with labelled edges when the rooms are connected. Might be easier to store the plan in a 2d array or even just a TreeSet with a `(room_id, room_id, direction)` tuple if the rooms are connected. Checking then is slightly more involved, e.g. for i in range(0i, n_rooms) { map.contains((current_room_id, i, North)) || map.contains((i, current_room_id, South)); } Very sketchy but you get the idea: store the graph edges globally instead of as part of each Room :)
Perhaps this should be posted as a question on Stack Overflow with the [rust] tag, then self-answered, so that it would be easier for people to search?
Once [Raw PtrTys](https://github.com/rust-lang/rust/pull/16788) land in our compiler version in the next upgrade, I plan on creating [a new attr that does this](https://github.com/servo/servo/issues/2774). The general solution is [this RfC](https://github.com/rust-lang/rust/issues/11813)
Hey thanks! This is super great.
Yep, thanks! That or if you look at my updated gist (the original link in the post), I can also use an argument to control the lifetime of the boxed commands explicitly. I still think this will cause them to live longer then I'd like, but it will certainly compile and work. 
Where's the worse performance? Also, note that that is a direct 1-1 translation of the C++, it's not particularly idiomatic.
If you need to make modifications to a single structure, you don't need to define traits and then implement them for that one single structure. You can directly add methods to a structure like this: pub struct YourStruct { some_data: int } impl YourStruct { pub fn get_data(&amp;self) -&gt; int { self.some_data } } 
&gt; My problem is that I have to create the directory before this tests run (and deleting it later would be nice too). Use [TempDir](http://doc.rust-lang.org/std/io/struct.TempDir.html) to create a temporary directory per unit test. At the end of the scope of the unit test (or if the unit test fails) the temporary directory and its contents will be removed (RAII!). That's how I'd do it. Also, chances are that you don't want multiple tests to use the same file/directory, since the tests are executed in parallel.
I imagine you'd need editor support or some sort of `rustfmt` tool.
&gt; It's not a big deal, I could do a script for this scenario; but I admit that doing just cargo test is sexier. I, uh, am not exactly in the habit of referring to software as 'sexy', but there is a good reason to not do this kind of thing with a script: there's advantage to sticking to the standard commands. A custom testing script means that you need to have something different in your README, you need to change the `script` command on Travis, etc. There's tons of advantage for having every singe project use the same command.
I guess that is the way to go. ty for pointing out TempDir, I only knew std::io::fs::mkdir.
This posts are amazingly good.
Rust-Graphics was the project that Piston (http://www.piston.rs) was originally developed for. The idea is to have a 2D graphics API with the lowest number of dependencies possible by design, such that all code written for it can run on everything that can render triangles. gfx_graphics is a back-end for rust-graphics, using another back-end agnostic API called "Gfx" which abstracts over 3D devices in general. This means you can use gfx_graphics to combine 2D and 3D graphics safely.
This was actually due to the CSP headers of Rust-CI, and is now fixed (both at the Rust-CI end, and, at ours, by making the docs more resistant to this failure mode), see [#17035](https://github.com/rust-lang/rust/pull/17035).
\o/ Awesome to hear!
&gt; They can be accessed in libc::funcs::posix88::unistd. Everything in the `libc` crate is [reexported at the top level](http://doc.rust-lang.org/master/libc/#reexports), and this is the (strongly) recommended path to use them, i.e. `libc::execv`.
Thanks, fixed that.
Have you looked at [Rust for Rubyists](http://www.rustforrubyists.com/)? I'm not sure how up-to-date it is these days, but it seems like exactly what you'd want. I think generics and static types can be learned in Rust as well as in any other language, and their value will probably be apparent to someone with a Ruby background. Static types save you from obnoxious type mismatch errors at runtime, and generics help recapture some of the flexibility lost when you give up dynamic typing. However, I suspect lifetimes and ownership semantics would seem unrecognizable as desirable features without a prior background in systems programming. Languages like C hand you `malloc()` and `free()` and leave you to fend for yourself. Once you've experienced all the myriad ways you can shoot yourself in the foot, and learned all the various idioms and best practices to reign in the chaos, Rust will seem like such an obvious crystallization of those principles that you'll wonder why nobody came up with it until now. Satisfying Rust's stringent ownership, lifetime, and aliasing rules are much less of a headache than being responsible for the next [Internet security meltdown](http://heartbleed.com/). This is why the language immediately clicked for me, and I think many others feel the same way. You can probably push through and learn all of Rust's features within Rust, but some of them might seem arcane or needlessly baroque without having some C or C++ under your belt.
&gt; I also want this, but it feels clunky to be part of cargo. FWIW, Cargo actually just uses the default built-in test harness, i.e. just invokes `rustc` with `--test`.
For what it's worth, I've found py.test's [injected and parameterized fixtures](http://pytest.org/latest/fixture.html) to be significantly more modular and flexible than xunit-style setup and teardown.
First off, thanks for the reply. Yes, I have read Rust for Rubyists. So you are recommending that I learn some C before diving into Rust? 
No, no, no, no, no... you're not actually creating a method called `spawn` that will confuse simple syntax highlighters because a language keyword is also `spawn`!? Noooooo.... --- Update: my mistake, `spawn` is a function loaded by `std::prelude`, not a keyword as I first assumed.
If you are using [this](https://github.com/PistonDevelopers/blog.piston.rs/commit/64489327a7557ddb51a23d1df51a009350ab635f) technique you can turn off word breaking for code blocks.
`spawn` is not a language keyword, it is just a reexport in the prelude of the freestanding function `std::task::spawn`; furthermore, since this `spawn` is a method on a specific type, the meaning is clear from context (IMO anyway).
I don't think learning C is necessary or very helpful. 1. Doesn't help explain generics 2. You don't call malloc() and free() in Rust code 3. Syntax is different in many places I think if you want to learn Rust, you should learn Rust. I don't know much C, but I know a little bit of Java. That does help understand generics, but that's about it. I don't think Rust is that challenging.
I've been underwhelmed with Cargo's testing functionality - I think I'm likely to stop using it as I gain more tests. To properly test my code it needs to be tested under different environments and with different --cfg options (all of which varies per-OS) - simply running `cargo test` isn't enough. For example, some tests (depending on OS): * need to be run as root (or some other user) * need to be run with `RUST_TEST_TASKS=1` * need capabilities to be set on the binary in between compiling and running * need certain --cfg's to specify which group they belong to I think supporting all of this is probably out of scope for Cargo, but it's a shame that users can't just type `cargo test` like with every other Rust project.
Depends how new you are I think. I am sceptical of a new programmer learning Rust. The ownership / borrowing / lifecycle concepts, while enabling a unique combination of performance and safety, do add non trivial cognitive overhead for the programmer relative to a garbage collected language. I suggest first learning (some of) an ML (Standard ML, Ocaml, F#) or ML-like (Scala [1], Swift), which share much in common with high level Rust. However for the experienced programmer it is always good to learn languages that change the way you think: they will make you a better programmer even if you do not use them in production. Rust will force you to think about ownership in a way other languages will not. Coming from a predominantly JVM background it has also given me greater consciousness of memory layout and pointer indirection. [1] There's a good Scala course on Coursera, almost all of which is applicable to Rust too https://www.coursera.org/course/progfun Edited to clarify that the above course teaches functional programming. Scala is just the choice of implementation language. This is absolutely the way to learn to program: learn styles, techniques and concepts rather than languages.
Personally do not recommend C, it will make you cry. I had to learn some C before I was able to escape to Python. It's annoyingly crammed full of weird leftovers from its decades long heritage (header files???). Rust (from the small amount of experimentation I have done with it) successfully solves most of C's problems and general arcane-ness while keeping all the advantages (fast low level programming, low overhead memory handling). You will need to learn about strong typing etc (if you have any experience with Java that's probably a much lower barrier introduction than C) but those are good things to know anyway. Best of all it's C-compatible, you can even use compiled Rust libraries from Python through ctypes (and I suspect you can do the same using whatever mechanism is equivalent in Ruby). I think there's a lot (too much?) documentation for Rust for C/C++ people because that's the target audience, but that doesnt mean you need to be a C coder to try Rust.
The idea behind the `use` keyword is that it will import identifiers, not everything. Also, enum variants in Rust are at the same level as their enum type (so `Some` is not `Option::Some`). It's possible to even declare a variable of some type without importing that type with type inference, or use an object of some type without importing it. For example: mod stuff { pub struct Foo {...} impl Foo { pub fn baz(&amp;self) -&gt; {...} } pub fn bar() -&gt; Foo {...} } use stuff::bar; let a = bar(); a.baz(); // I can use this without importing Foo Basically, `use stuff::bar` says "every time the token `bar` appears, resolve it to `stuff::bar`". It already knows that `bar()` returns `stuff::Foo`, and I don't use the token `Foo` in the outside mod, so I don't need to `use` it. `use` is more of an alias than an "import" (you can also custom alias via `use Foo = bar::baz::Quux;`). So when you `use` an `enum` `Things`, you are just saying "when I use the token `Things`, I am talking about the type `path::to::Things`". You have to import the variants as well (`use path::to::{Things, Variant1, ...};`) to use the token `Variant1`. In many cases, you only have to import the variants you are using without importing the enum type itself. Glob imports exist though, you can `use stuff::*` if you're lazy (feature-gated though, but that's easy to bypass). The only case where `use` is an _import_ is when it comes to traits. Importing a trait means that you can use the trait methods on any object that implements the trait. For example, to use `to_ascii_lower()` on `&amp;str`, I have to import [`std::ascii::AsciiExt`](http://doc.rust-lang.org/std/ascii/trait.AsciiExt.html) use std::ascii::AsciiExt; "".to_ascii_lower(); In this case I don't need the _alias_ "`AsciiExt` is `std::ascii::AsciiExt`", but I need to say that "The trait `AsciiExt` is imported and all implementors should now have its methods"
May I suggest you fix the formatting of the answer? It becomes much easier to read.
You could write a test runner (i.e. thing with a `main` function) that lives in `tests/foo.rs` and then have [[test]] name = "foo" harness = false which will compile and run that as a normal binary during `cargo test` (with "test fail" vs. "test pass" based on the exit code). You could even reuse the `test` crate to get the nice test pass/test fail behaviour, e.g. [html5ever](https://github.com/kmcallister/html5ever/blob/f7b29f19515e390d0cff937d7f289b054bbb49b7/tests/html5ever-external-test.rs). (In my mind, this binary would end up "shelling out" to compile/run the tests with the appropriate configuration.) If you don't need *complete* control, you could also look at [cargo's tests](https://github.com/rust-lang/cargo/tree/master/tests) (entry point `tests.rs`), which have a large number of `#[test]` functions that create directories and shell out to cargo etc. etc. (this would be omitting the `harness = false` line).
I second this motion. Don't use ``` just use . . . . (where . are spaces).
Or click on 'formatting help' just below the textbox (on the right). :)
Fixed, sorry :P I know the differences between Reddit/Github MarkDown, but I'm accustomed to the GH one and sometimes use stuff from it accidentally :P
This *test runner* solves my problem. I'll able to write it in steps (which is what I need), no shell script involved plus just `cargo test` to run. This behavior (using main as the test function) is all due the `harness = false`? I didn't find much on *harness* but, for what I have read ([source code](https://github.com/rust-lang/cargo/blob/444dc1e28e7cfc19500f8f98aa49e426ec9067fd/src/cargo/ops/cargo_rustc/mod.rs#L350)), it just omits `--test` when compiling the test. So is safe to say that by doing this I'm also not running any `#[test]` functions that'll be in code (right?).
I can see projects become pretty verbose. I still believe that importing an enum should automatically import its values.
I think that the variants should be scoped under the enum instead.
How would that syntax look like?
Interesting, I should try that!
Perhaps after 1.0? Remember, you can only appear on the sidebar for one half-year. If we choose Jan 2015 (by when I'm told we probably should have 1.0), one of the major put-offs for Rust will be gone. :) I wonder if Servo would work there. It's not something that can be *used* by programmers as such yet, though, maybe we can rope in contributors and feed them to the DOM? :&gt; Oh, also, other sites have this too, eg on [Programmers.SE](http://meta.programmers.stackexchange.com/questions/6315/community-promotion-ads-2014). But the goals of the ads on other SE sites* are a bit different, and it only makes sense if the community is interested in it. I'll ask Chris or Yannis or one of the other Programmers mods about that. *Stack Overflow's ads are for promoting open source tools and miscellany (languages included). The ads on all other SE sites are for promoting more community-esque things, though tools are also allowed. On Physics.SE (which I moderate), we have a couple of LaTeX tools in ads, but most of them are for sites like ArXiV.
Rust for Rubyists tracks the last release, so it goes back and forth between being up to date and a little out of date. It's been a few months since 0.11, so it's a little out of date right now.
&gt; (and I suspect you can do the same using whatever mechanism is equivalent in Ruby). Yup. The second production deployment of Rust is actually a Ruby gem.
That makes way more sense, thanks for the help! 
`*` works in `use` statements? Like `use std::io::*`? Didn't know about it. That's handy.
Yep. You might have to put a `#![feature(globs)]` at the top.
Wouldn't that be just as verbose? Right now you can do `module::variant`, this switches to `type::variant`.
You probably want to post this in /r/playrust instead of here.
Then sorry, I can't reproduce it. Your server from the original post and updated client work fine together. I'm on Linux though, I will only be able to test it on Mac tomorrow.
Thanks. Yup, the secondary guides are now my focus, they are intended to solve exactly this: deeper dives in a particular topic.
Well it's not about saving typing. It's to make the enum and its variant one logical unit; importing Option would give you access to Option::Some, etc. C++ introduced enum class which is in this style, and maybe Rust should learn from it.
&gt; ...now if I could only wrap my mind around the borrow checker :P Since you're a new comer, I'll say this (and I've said it before): *it is OK to get pummeled by the borrow checker*. If you've never been exposed to linear typing before, it is a very weird twist. I know I spent a few hard full days just trying to get something by the `rustc` compiler. But after writing a couple thousand lines of code, it definitely starts to get easier. I'm hovering at around 10,000 lines now, and the borrow checker has become completely normal to me. If you don't have prior experience with pointers, then I'd expect that the pummeling might last a little longer. :-)
Some adjustments were made to the resolution algorithm (specifically, you cannot have both `mod foo { ... } enum foo { ... }` in the same scope) which allow adding this backwards compatibly.
That's fine. It can be added after 1.0.
Awesome! I can give a talk about gfx-rs architecture.
I think this was a deliberate decision. Again, `use` is meant to be more of an alias than an import. I personally find top-level variants to be much better than adding the extra scoping. How would you handle name clashes in this system? With aliasing it's fine if you have two enums which share a variant name. With imports, trying to use both enums will break it. Besides, it makes the code easier to track. If you see a token `Foo` used in the code, you can look at the top and you'll know where it came from. With enums importing their variants, you have to resort to more roundabout methods.
Nice! Anyone planning on comming from the Waterloo area?
Added to the calendar https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com&amp;ctz=America/Los_Angeles
Who was elected corkmaster?
Say what?
https://github.com/japaric/serial.rs seems not too old, though Travis says it's failing. And it hasn't been updated to Cargo. It's very small, though, ~300 lines.
The link doesn't work :(
I'm still learning Rust so sorry if I have some messed up code or something. I was messing around and wanted to make something with a real-world application, and thought of making a library for interacting with SOCKS proxies like Tor. This is the result. It only supports Socks4a right now, but within the next few days I hope to add support for Socks4 and Socks5. Each version will have their own modules to make things easier and cleaner in the code. How it works is you feed it information like the address of the SOCKS proxy, and the domain you want to visit. You then call build() and it sets up the con nection and returns a TcpStream for you to work with. Let me know what you think.
That's weird. It's not set to be private or anything. I just created the github repo a few minutes ago, will it take some time to work for everyone else or something? (I've never seriously used github for anything other than messing around before) Also just noticed my name is 404'ing. WTF?
This is actually safe without the `&amp;mut`/`&amp;` borrowing (which already happens now, i.e. this is actually a different problem), since it can be written as loop { match iter.next() { None =&gt; break, Some(i) =&gt; { let mut iter2 = iter.clone(); for j in iter2 { ... } } } } It is just a question of the the compiler knowing it works. This is true because the `Iterator` object is divorced from the yielded value, i.e. there's no lifetime connecting them in the definition of `next`: trait Iterator&lt;A&gt; { fn next(&amp;mut self) -&gt; Option&lt;A&gt;; // ... [#8372](https://github.com/rust-lang/rust/issues/8372) covers this. There was a 'fix' for it, but it ended up being incorrect and unsafe, so it had to be reverted.
That's a fair point, but that's not the only time you could want this. For example: fn main() { let mut vector = vec![2u, 0, 1]; for i in vector.mut_iter() { // fails as vector mutably borrowed *i += vector[*i]; } } It's the `vector.mut_iter()` that borrows in this case, and a naïve rewrite will be both uglier and slower.
Ah, well. That example there is slightly different, since you're not just trying to get access to the iterator. It's not safe to allow access to the `vector` *directly* while the iterator exists, since this can allow for aliasing a `&amp;mut` and mutation of a `&amp;`, which is memory unsafe (it allows one to invalidate references in safe code). What can be allowed is accessing the remaining elements of the iterator as a `&amp;mut [T]`, e.g. let mut iter = vector.mut_iter(); for i in iter { *i += iter.as_slice()[*i] } but this has very different semantics. It does seem like it could possibly be made safe, but an extension like that should really only be done when there is a formal model so that it can be proved to be correct.
&gt; since this can allow for aliasing a `&amp;mut` If the `mut_iter`'s `&amp;mut`-reference to the vector was lexically changed to `&amp;`, like my suggestion, this would not be a problem as you can't get a `&amp;mut`-reference when there's already an outer `&amp;`-reference. Of course, this requires the compiler to assert that inside this subsection of the `&amp;mut`'s lifetime the data isn't mutated and thus can be safely treated as if it were a constant reference.
This is what my last paragraph refers to.
Wow, that was soon after the announcement. &gt; Swift version 1.0 is now GM. What does GM mean?
Ah, I misunderstood.
Golden Master, equivalent to the GM printing of a CD
Anyone know if there've been meaningful language-level changes since the original announcement, or if it's just the implementation that has matured? &gt; You’ll notice we’re using the word “GM”, not “final”. That’s because Swift will continue to advance with new features, improved performance, and refined syntax. "refined" is kind of ambiguous, but I'm guessing they do intend to maintain source compability from this point?
&gt; #[allow(dead_code)] Why are these needed?
You can bind the port twice. On the client side just use the function `connect` to connect to the given socket.
Tried submitting this yesterday but there was a problem with the repo and no one could see it but me, but Github support has told me it's fixed now and everything should be working fine. I'm still learning Rust so sorry if I have some messed up code or something. I was messing around and wanted to make something with a real-world application, and thought of making a library for interacting with SOCKS proxies like Tor. This is the result. It only supports Socks4a right now, but within the next few days I hope to add support for Socks4 and Socks5. Each version will have their own modules to make things easier and cleaner in the code. How it works is you feed it information like the address of the SOCKS proxy, and the domain you want to visit. You then call build() and it sets up the connection and returns a TcpStream for you to work with. Let me know what you think
FYI, you only have to write the type suffix after one of the values in a `vec![]` macro and it'll type-infer the rest, saves some keystrokes and preserves some readability!
&gt; a bit disappointing that you chose not to stand in front of a train A death wish like this is completely unacceptable. (I assume you were also responsible for the now-deleted thread on [a previous submission of /u/DroidLogician](http://www.reddit.com/r/rust/comments/2d6twy/my_first_major_project_in_rust_imgdup_a_tool_to/) that had personal (and GPL) hate.)
It seems surprising unless the functions aren't being called. Are you sure that they are? (see `--pretty expanded`.)
Cool library! I have some (possibly crazy) things I want to do with reddit from Rust: my loose sketches a while ago involved shelling out to httpie for each API interaction... this looks much, much nicer.
The fixed length vectors and "obvious" `range` loops means there's no bounds checking.
Ah, that makes sense. From your other comment I'm guessing that's an LLVM-level optimisation, is that correct?
Yes, all non-trivial optimisation in `rustc` is done by LLVM. (We do do some very simple dead branch trimming for constant things like `if true { ... }`, but that's all.)
&gt; Stronger encapsulation for dependencies. One of the reasons why dependency hell is so insidious is the dependency of a package is often an inextricable part of its outwards facing API: thus, the choice of a dependency is not a local choice, but rather a global choice which affects the entire application. Of course, if a library uses some library internally, but this choice is entirely an implementation detail, this shouldn't result in any sort of global constraint. Node.js's NPM takes this choice to its logical extreme: by default, it doesn't deduplicate dependencies at all, giving each library its own copy of each of its dependencies. While I'm a little dubious about duplicating everything (it certainly occurs in the Java/Maven ecosystem), I certainly agree that keeping dependency constraints local improves composability. But there's an enormous flaw in NPM's approach: if two libraries expose objects of different *versions* from within their respective dependency chains, then the whole thing falls apart. In theory, anyway. Perhaps JavaScript is flexible enough to avoid this as an issue. I'm kind of new to Rust, but I've been following some of the Cargo related news lately. Does Cargo recursively (static) compile dependencies like this in order to sidestep this issue? I've noticed that the lockfile stores several levels of dependency graphs, rather than unifying the whole mess into a single, flat, dependency list.
Cool, thanks :)
Just fyi, it's 'Cargo' not 'Crate'.
How does it compare to the C++ one on your machine?
Wouldn't it make sense to have it be `#[fixture(bar)]` and `#[mut_fixture(bar)]` considering immutability is the default in Rust?
That is a neat trick. I could very well see myself using it at one point. Really, why don't you add it to the FFI guide? Don't worry about your English. After all you are here discussing the whole thing :) 
&gt; if two libraries expose objects of different versions from within their respective dependency chains, then the whole thing falls apart. Then those two libraries must be said to be incompatible anyway. This case isn't special or notable, it's just there. They would also be incompatible if package-localization *wasn't* a feature of the package manager in use; when it is a feature in use, then a bigger superset of package can be compatible with eachother.
You should always open an Issue. Yes, there's a lot of them, but it's the only way things get fixed. I personally read every comment and every issue. I'm on my phone, so I'm not sure what's going wrong here, but return types are generated from the source, soooooo...
You are using the older version (maybe 0.11) of Rust, where [`TempDir::new` indeed returned an `Option`](https://github.com/rust-lang/rust/blob/0.11.0/src/libstd/io/tempfile.rs#L64). Indeed, there is a [documentation](http://doc.rust-lang.org/0.11.0/std/io/struct.TempDir.html#method.new) for 0.11 which says the correct thing. In general though, you SHOULD NOT use the point release since everything else including the non-versioned documentation and third-party libraries assumes the development version. &gt; My first thought was the issues on github, but the thing is so crowded that maybe I'll be doing more harm than helping (there is nearly 2000 open issues there). Oh, please don't mind to post issues! Never reported issues are always worse than reported but ignored issues. If your issue is not really an issue (like this one), hopefully someone will give you a correct solution and mark it as "invalid".
I was using using `rustc 0.12.0-pre-nightly (5419b2ca2 2014-08-29 22:16:20 +0000)`. Just updated to `rustc 0.12.0-pre-nightly (325808a33 2014-09-08 20:51:14 +0000)` and now it's working. Should have thought of that... ty.
I wonder about this usage as well, however a quick look at the benchmark reveals the use of a plain array of struct. I do not see any memory allocation in the main loop, so I would guess there may be some allocation overhead *before* it, but that should not influence the cache much.
Doesn't make it any less safe. All safe functions use unsafe at some point. 
RAWR is cool. Is it being used for anything? I might just go ahead and rename the repo. Though you could almost read the current name like RAWRS.
I don't think you are able to tag them yourself, so no, you don't need to do that :p The core developers keep track of issues and tag them.
Cargo is awesome, and the people working on it listened to and solved every concern we had in the [Piston](http://www.piston.rs/) project, including the future ones. We used it since the first Alpha version came out! * You can override dependencies locally * You can postfix a string in the semver version to use it as Alpha/Beta * You can decouple dependencies and maintain tiny libraries * You can replicate a build of a binary * You can have multiple crates in one repo
I just realized that a language with structural rather than nominal typing would evade those problems gracefully. Off to the drawing board!
When working with releases slightly behind, I have found most useful to use the *local* documentation rather than the online one. If you download the tarball, it comes with the documentation already generated and you can browse it as well as you can browse the online one.
Re: 1, the type of fibcache needs to be `Vec&lt;uint&gt;`, not `[uint, ..100]`. The latter is a fixed-size array of length 100. Re: 3, you can't. Rust doesn't allow a struct to be partially initialized. You can make a field an Option type and initialize with None. Re: 4, you can't. In Rust mutability is inherited; for any given object it's either entirely mutable or entirely immutable.
1. You were on the right track with `Vec&lt;uint&gt;`. It's what you want to use to get a growing vector. The `vec!(...)` macro takes the elements and not an array, like this: `vec!(1, 1, 1)`. 2. You would have to make a static mutable variable, which is unsafe, so I don't recommend it. Try to pass a `Fib` object around instead. You may be interested in an [`RefCell`](http://doc.rust-lang.org/std/cell/struct.RefCell.html) within an [`Rc`](http://doc.rust-lang.org/std/rc/struct.Rc.html) to make copies of a `Fib` share the same cache, if it's impossible to just borrow the same `Fib`. 3. Like you did with `Fib::new()` or by implementing [`Default`](http://doc.rust-lang.org/std/default/trait.Default.html). I think a constructor (`Fib::new()`) works just fine. 4. You can't, but keep them private and they will be "safe". You can make methods for reading them if you don't want anyone outside the module to touch it. I hope this answers your questions. :) Edit: 3. If you meant to make an empty struct and not a default constructor, you have to write something like this: struct MyStruct; //Defines an empty struct fn main() { let my_struct = MyStruct; //Instantiates it }
Please don't feed the troll. This isn't the place for pointless bickering.
Meh, it's worth a read, but I don't particularly agree with him or the linked article. Distrobution software package management is a whole different beast than development package management. I *DO* agree that it should be possible to narrow down the dozen software package management systems (pacman, apt, yum, etc.), because they all essentially do the exact same thing. All trying to fill the exact same role, for the exact same problem set. Package managers for different languages however are something else. I don't think you could have a universal way to manage modules across different languages because each language is so different in their use and requirements. In my mind it comes down to better versioning (as mentioned in the first article), if I can rely on a third party being acurate in their versioning so that I know at a glance whether this new version breaks existing APIs or not, awesome! tl,dr; I don't have a problem with a different package manager for each language, only when it's multiple managers for the same problem set (i.e. software managers, or multiple Python module managers, etc.).
Rust label [documentation](https://github.com/rust-lang/rust/wiki/Note-tag-label-names-and-definitions)
I'm not sure if this is what you're looking for, but you can lazily initialize variables by doing this: let x: Dummy; Later, you'd still have to fully initialize it before using it: x = Dummy { value: 0 }; You can also provide a default constructor method, as /u/SirOgeon mentioned and as you already implemented: let x = Dummy::new();
Thanks for the tip, though apparently it even works without the suffixes on any of the values. Don't know why I thought I needed them. Anyway, the code has been updated.
On an i7-3770 I get C++ g++ #8: 4.03user 0.00system 0:04.03elapsed 100%CPU (0avgtext+0avgdata 2612maxresident)k 0inputs+0outputs (0major+115minor)pagefaults 0swaps Rust: 4.97user 0.00system 0:04.97elapsed 100%CPU (0avgtext+0avgdata 2592maxresident)k 0inputs+0outputs (0major+130minor)pagefaults 0swaps Edit: Same C++ sources compiled with clang-3.5: 4.13user 0.00system 0:04.12elapsed 100%CPU (0avgtext+0avgdata 2212maxresident)k 0inputs+0outputs (0major+101minor)pagefaults 0swaps
&gt; RAWR a package manager for jruby. I think your okay.
I've found that trying to set up a Nix VM is tricky at first. There's a *lot* to learn if you want to understand how your configuration works, and while it's easy to tell what's there, it's not so easy to tell what isn't. That said, the longer you use the system and make changes, the better Nix is, because you'll have all the changes in one easy to see spot, instead of having the understand what each file in /etc is and remembering whether you've changed it from its default or not.
Re: 3: use [`Default`](http://doc.rust-lang.org/core/default/trait.Default.html). If all of the child fields implement the `Default` trait (most stdlib ones do), then you can do the following: #[deriving(Default)] struct Foo { bar: uint, baz: Vec&lt;uint&gt; } And if you run `let foo: Foo = Default::default();`, it will initialize it with default parameters. 
That's 1.2x! Much better than the 10s -&gt; 21s (~2.1x) on the website
Just to confirm what everyone else said, don't worry about tagging them, we can handle that. :)
Small conventions note: `vec![]` is slightly preferable to `vec!()`.
Yes, I was about to write that, but I felt like it would be best to keep the vector related things simple and "familiar" to other languages, for now. `vec!` works like a function, so I chose to keep it like a function. Edit: But thanks for the notice :)
I'm running on nix and I really like it. There are issues though, e.g. if you follow the main &lt;nixpkgs&gt; repository then stuff breaks a lot (see e.g. [1] for how many failures there are). The local dev envs (I use nix-shell) set up environment variables instead of the symlinks so you're not really getting the same as a normally linked system. That may just me being stupid though. There are well-known problem such as a change in libc requiring a rebuild of all dependent packages (almost everything), which IMO isn't that much of an issue in practice. Another problem is that the project is seriously understaffed (i.e. rough edges) but then which project isn't :) [1] https://nixos.org/wiki/Zero_Hydra_Failures
Did you compile the C++ with the same flags as the shootout?
Yes, copied the command from the website (for g++, obviously). g++ is g++ (Debian 4.9.1-13) 4.9.1 rust is rustc 0.12.0-pre (8eee1b44b 2014-09-10 09:20:39 +0000) 
Looking at the g++ #8 code and it seems incorrect for systems where N in the advance() function ends up being odd. The that would cause accesses past the end of the `r` array. That `r` array is probably something we could try to replicate though (except for the incorrectness :-p). It allows to calculate two steps at once, making better use of the xmm registers. The code for the squared distance currently looks like this: movupd (%rsi), %xmm4 movupd (%rbx), %xmm3 subpd %xmm3, %xmm4 movsd 16(%rsi), %xmm3 subsd 16(%rbx), %xmm3 movapd %xmm4, %xmm5 mulsd %xmm5, %xmm5 movapd %xmm4, %xmm6 unpckhpd %xmm6, %xmm6 mulsd %xmm6, %xmm6 addsd %xmm5, %xmm6 movaps %xmm3, %xmm5 mulsd %xmm5, %xmm5 addsd %xmm6, %xmm5 The x and y deltas are computed in parallel, then the z delta is computed. So far so good, but then the values have to be moved around and unpacked for the multiplication and addition to happen. Serializing all the steps. By doing all the delta computations beforehand, the C++ code can make better use of the xmm registers like this: movsd -48(%rdx), %xmm5 movsd -40(%rdx), %xmm6 movhpd -16(%rdx), %xmm5 movhpd -8(%rdx), %xmm6 movsd -32(%rdx), %xmm4 movhpd (%rdx), %xmm4 mulpd %xmm5, %xmm5 mulpd %xmm6, %xmm6 addpd %xmm5, %xmm6 mulpd %xmm4, %xmm4 addpd %xmm6, %xmm4 Instead of having x and y in one xmm register and z in another. Each register has either x, y _or_ z, but for two consecutive iterations. So the distance computation is fully vectorized. I'll probably not have time to try to get the rust code to get the same (or better ;-)) assembly, but maybe someone else can make use of the above.
Oh, strange mismatch between the website and locally. Maybe rustc has got significantly faster.
&gt; But not only does it not do that, but it needs the lifetime declared twice. Why? I don't really see it as being declared twice. It's declared *once* with `impl&lt;'a&gt;`. The `SCGI&lt;'a&gt;` part says, "Here's an implementation for `SCGI` with an arbitrary named lifetime `'a`." In other words, the `impl&lt;'a&gt;` brings the lifetime `'a` into scope. It can be used elsewhere in the `impl` block. For example, if you have a function that returns something inside of your `SCGI` struct that is bound to lifetime `'a`: struct SCGI&lt;'a&gt; { foo: &amp;'a Foo, } impl&lt;'a&gt; SCGI&lt;'a&gt; { fn get_foo(&amp;self) -&gt; &amp;'a Foo { self.foo } } And the `'a` refers to the *same lifetime* throughout the `impl`.
&gt; It's declared once with impl&lt;'a&gt;. The `impl` is always tied to the declaration of the `struct`. Why not use that lifetime? It's what the programmer would want to do.
Will there be stickers? Pleeease!
Actually I initially had this, but actually using the attributes in my tests I realized that you virtually never want an immutable fixture; these aren't setup and teardown functions; it'd more like injected dependencies for a test. Having an immutable value is actually not very useful at all; you can't perform any meaningful operations on it. Can you think of any useful reason you'd often want to inject an immutable value? (a gist of the sorts of tests that might involve would be really useful)
Ah right. I get it now. And the `&amp;a` is an immutable reference which gets assigned to `__arg0` so there's no change of ownership there either. That sound about right?
If you attack people for their opinions, I don't know how you expect yours to be taken seriously.
Exactly. 
I will go to the end of the Earth to make it so
Super, thanks!
This syntax comes from the syntax for generics. struct Foo&lt;T&gt;{...}; impl&lt;T&gt; Foo&lt;T&gt; { // ... } impl Foo&lt;String&gt; { // ... } The compiler needs to know here that `T` is a generic parameter, not a type (in the latter case, `String` is a type, and only `Foo&lt;String&gt;` objects will get the goodies from that impl), so all generic parameters are declared after `impl`. [Here's][1] an example of this being used for lifetimes. [1]: http://play.rust-lang.org/?code=%23!%5Ballow(dead_code)%5D%0A%23!%5Ballow(unused_variable)%5D%0Astruct%20Wrap%3C%27a%3E%20%7B%0Aptr%3A%20%26%27a%20str%0A%7D%0A%0Aimpl%20Wrap%3C%27static%3E%20%7B%0A%20fn%20foo(%26self)%20%7B%0A%20%20%20%20println!(%22only%20statics%20can%20call%20this!%22)%0A%20%7D%0A%7D%0A%0Afn%20main()%20%7B%0A%20let%20stat_w%20%3D%20Wrap%20%7B%0A%20%20ptr%3A%20%20%22this%20is%20static%22%0A%20%7D%3B%0A%20stat_w.foo()%3B%0A%20let%20owned%20%3D%20%20%22this%20is%20static%22.to_string()%3B%0A%20let%20local_ptr%20%3D%20owned.as_slice()%3B%20%20%2F%2F%20local%20lifetime%0A%20let%20local_w%20%3D%20Wrap%20%7B%0A%20%20ptr%3A%20%20local_ptr%0A%20%7D%3B%0A%20%2F%2Flocal_w.foo()%3B%20%2F%2Fwon%27t%20work%0A%7D%0A
Thanks, I'll look into it
Update for people paying attention to this: * Ryman ( https://github.com/Ryman ) cleaned up some of my code and added error handling. * I added support for Socks4 I'll be adding Socks5 soon, though of course anyone else is welcome to fork the library and do it themselves first.
You're "allowed" to say whatever you like, as long as it satisfies our rules (see the sidebar), specifically: don't violate our [code of conduct](http://www.reddit.com/r/rust/comments/1nvsdh/a_note_on_conduct_please_read/) and be constructive. This comment refrains from abuse and direct personal attacks (for the most part...) and thus doesn't deserve a ban, unlike your other comments (at least, it doesn't deserve a ban if it were viewed in isolation). Disagreement is fine, but it should *never* descend into abuse and death wishes.
As per /u/Sinistersnare's suggestion, I have changed the repo name to RAWR.
I imagine that is our large runtime, which is loaded into memory statically (I.e. the memory use is not due to significant dynamic allocation). This will be dramatically improved with the [removal of our current runtime design](https://github.com/rust-lang/rfcs/pull/230) (which involves trait objects and indirection, inhibiting dead code removal). It should place us essentially equal to C++.
I wish I understood some of this more deeply to know how useful some of this is, especially your notes on dynamically sized types being replaced with existentially qualified types (which sounds awesome). Nonetheless, super interesting to read through a lot of this. Thank you for posting these.
Based on your comment history and the way you write, it sounds like your a teenager with little to no experience writing real software. Be happy he releases the source code at all, and deal with it. If you think its bad to use GPL then make your own version of GPL'd software and license it WTFPL or what have you. Don't criticize others for when they are smarter than you, and have the understanding of what licenses mean and when to use them. Also you have -100 comment karma and many many comments. Maybe its best not to voice your opinion on Reddit.
This should hopefully resolve a lot of the disconnect between the language designers and the community.
Could someone who knows more about rust than me explain the comparatively large memory footprint?
See above: http://www.reddit.com/r/rust/comments/2fzjxa/benchmark_improvement_nbody/ckf109j
Indeed, the [Lifetime Elision RFC](https://github.com/rust-lang/rfcs/blob/master/active/0039-lifetime-elision.md) includes rules for eliding lifetimes in `impl` headers. This isn't implemented yet, but it's planned. See [issue #15872](https://github.com/rust-lang/rust/issues/15872).
Wouldn't then be more appropriate (with move semantics in mind) for `println!` macro to take `&amp;a` instead of `a`?
But shouldn't the compiler go look for the struct that it belongs to and infer from that?
This makes sense.
I think it would be neat to get a build time confirmation that every string resource is defined in every supported language. Maybe not bake it into the code, but fail the production build if eg the German translation for a new ui component is missing.
One executable per language would be annoying if you want to have more than one language available on a computer. For example, a shared computer where different users prefer different languages, or for single users who prefer working in a language other than English, but occasionally switch to English because documentation, online guides, help forums, etc. explain how to use the software in English and the translation for terms in the software is not always direct/obvious.
Ugh there's an RFC there asking `type` to be renamed to `typedef` (a very good idea and in line with C/C++) - but someone refutes the idea saying everything should be done the Haskell way. Isn't Rust more like C than Haskell? Why is the language being pushed away from its C roots?
Time to mark the tuple traits deprecated.
Why not `typealias`?
infer what? The identifier used? That doesn't make sense, what would you as a programmer type when you want to use the lifetime? There's no symbol for "insert inferred lifetime here". The `'a` here is a lifetime _parameter_: struct SCGI&lt;'a&gt; { foo: &amp;'a Foo, } It's not a real lifetime, it's a placeholder. Let's say this was in a different file, and also it was defined like this struct SCGI&lt;'x&gt; { foo: &amp;'x Foo, } Now, you want the compiler to infer it in the implementation: impl SCGI { fn get_foo(&amp;self) -&gt; &amp;'x Foo { self.foo } } But wait -- you used `'x` there, because it was defined as `'x` elsewhere and the compiler inferred the identifier. So you still have to look up the symbol used. When you make a struct def, all generic parameters (including generic lifetime parameters) are locally defined. Remember, these are _parameters_, not the actual lifetimes, so inferring them doesn't make sense -- the only thing you can infer is the symbol, and that's silly. What would happen if generic types were inferred the same way? (see my other comment below) For example, if there was inference of that kind, you would get code like impl Add for Foo { fn add(&amp;self, rhs: &amp;RHS) -&gt; Result {...} } Where both `RHS` and `Result` are the generic type parameters defined in [`core::ops::Add`](http://doc.rust-lang.org/core/ops/trait.Add.html). That's a bit arbitrary, having generic type params scoped locally makes more sense -- instead we do impl Add&lt;A,B&gt; for Foo { fn add(&amp;self, rhs: &amp;A) -&gt; B {...} } or whatever other symbols you want. ----- To be fair, since `impl` blocks can't take lifetimes from the outside _except_ for `'static` (which is a keyword anyway), it actually wouldn't break anything if this was allowed: struct SCGI&lt;'x&gt; { foo: &amp;'x Foo, } impl SCGI&lt;'a&gt; { fn get_foo(&amp;self) -&gt; &amp;'a Foo { self.foo } } unlike generic _type_ parameters, as far as impls go you cannot pull in concrete named lifetimes from the surroundings; i.e. this isn't allowed: fn foo&lt;'a&gt;(x: &amp;'a str) { impl SCGI&lt;'a&gt; { fn wololo() { } } (though the same works if we were using generic type parameters, since they're a bit more mobile). There's only one concrete named lifetime which you can use in an `impl`, and it's `'static` (which is a keyword so it can't be used for generics anyway), so this sort of sugar doesn't break anything. Note that this isn't _inference_ of any kind. It's just some sugar. However, lifetimes, being part of the type system, get almost identical treatment as type parameters when it comes to generics — I personally don't feel that this is reason enough to change that.
Oh, that's a nice idea. What can be done is to have a syntax extension that compiles in various modes (this is easily done) -- one where it checks everything for dev builds, one where it gives a binary for a single language, and one where it provides a language switcher.
&gt; Why is the language being pushed away from its C roots? It's more like OCaml than anything. The syntax may be C-like but the language constructs are much more like the MLs (with Haskell's type classes thrown into the mix).
There was a talk at Fosdem this year on Nix and NixOS. https://archive.fosdem.org/2014/schedule/event/nixos_declarative_configuration_linux_distribution/
I *like* it!
I wonder how crazy it would be to extend this to *fixed-size* arrays (they are, after all, equivalent to homogenous tuples). That you means you could index them with both `a.3` and `a[3]`, but the difference is that while the latter is checked at runtime, the former is checked at *compile time*. This means that, as with tuples and structs, you could do both `&amp;mut a.3` and `&amp;mut a.2` at the same time, because the borrow checker can tell that these are disjoint loan paths.
While interesting that's not often what you want to do, is it? More often you want to access both `a[i]` and `a[i+1]` for runtime value *i*, and this is the case you want the borrow checker to "get".
Believe me, I'm not a formal type theorist either, I just try to read papers sometimes. And so can you. :) I wish I understood it more deeply as well, especially the parts in Greek. But I understand much more of it than I used to. [This is very helpful](http://logitext.mit.edu/logitext.fcgi/tutorial) for being able to read typing judgments, for example.
i was interested in this, supposedly there's a separate RFC to make (T,T,T) [T,..3] synonymous ?
&gt; I don't actually think I've ever seen that being a useful thing to do. There was recently a reddit where exactly this solution was needed: http://www.reddit.com/r/rust/comments/2fk03l/how_to_return_iterator_from_a_struct_with_a/ Edit: as advised below.
The Rust compiler supports lint plugins, which should make this easy to do. I can't find any documentation on them atm, though.
Actually I think `alias` is better than `typealias`. 
Well, Rust doesn't have C roots. It rooted in Ocaml, and had its first compiler writen in Ocaml. *Then* more and more good ideas from Haskell/C/C++ snacked in. 
I *also* like it!
And why would I have two different implementations for the same struct?
Stickers are now imminent and unstoppable.
It might be an implementation of a struct with a specific type param; eg `impl&lt;T&gt; MyStruct&lt;T&gt; {}` is your generic impl, and later on you decide to supercharge it if it contains a string (`impl MyStruct&lt;String&gt; {}`). Or you use traits (`impl&lt;T: FromStr&gt; MyStruct&lt;T&gt; {}`). Especially if the type or trait you wish to `impl` it on wasn't defined in the originating crate for `MyStruct` (I think if it's otherwise it won't let you do it without newtypes).
`alias` and `typealias` are great names for type aliasing, but it don't fit for associated types
This is so damn cool. Good job!
Rust takes its lifetime syntax from OCaml, and a lot of other conventions are similar, though I'm not sure about what exact ways ML is different than OCaml.
Yay, my `Tuple*` traits can be killed at last!
I'm all for the possible simplification of the type system, but an interesting point here is the memory layout guarantees. While for general tuples it would better be unspecified, for arrays (and for homogeneous tuples if they become desugared arrays) it is unacceptable and they should be a special case.
Yes, it was also noted that this issue appeared with other types of collections (vectors, maps, etc...) where you legitimately wish to get *several* elements at the same time. A generic way of specifying that several *non-overlapping* elements should be borrowed mutably at once would be great. I would expect something akin to: fn get_slice(keys: &amp;[&amp;Key]) -&gt; [&amp;mut Value]; with a dynamic check that the keys/indexes do not overlap, and a guarantee that the returned array has the same length as the passed-in array... though unfortunately the latter cannot be verified at compile-time at the moment.
Feel like trying out a few variations on [ptr::swap](http://doc.rust-lang.org/src/core/home/rustbuild/src/rust-buildbot/slave/nightly-linux/build/src/libcore/ptr.rs.html#142-155)? I bet it's possible to write one that doesn't use a large scratch space, at least when you know the regions are non-overlapping.
To quote, don't use the bar, use the `&gt;`, see formatting options for how to use reddit markdown
Earlier this year we met with somebody from the Fx i18n team and they said 'no': 'serious software' (Fx at least) needs to do it at runtime, mostly for reasons of flexibility I believe. Like /u/mbuhot suggests though, there likely *is* a large role for compile-time plugins in i18n regardless though.
is there a general algorithm that would allow optimisation/packing, and make (T,T,T) arranged the same as [T,..3] . eg. "first sort by size of type, then by order of appearance" 
? There's a 'a syntax in OCaml but it's used for polymorphic types, as far as I know, like in ML.
yeah, i’m perfectly happy with using pacman for my python packages. 5 minutes to wrap some average PyPI package in a AUR package, and your AUR helper of choice is able to install it. i‘ve never encountered a package needing some specific version (as opposed to one needing a specific *minimum* version)
Another update: * I've added support for Socks5. Copy/paste from a comment I made on the commit: If possible, I'd appreciate it if someone could review this code and/or fix any errors I made. In the process of debugging I swapped things around a few times and made some changes and while I think everything's fine now, I'd like to be sure. Also, authentication hasn't been tested, so it would be nice if someone with a Socks5 server on hand that requires authentication could test this. Note as well that IP addresses are not supported, only domain names. I couldn't think of a clean way to integrate domains, IPv4, and IPv6.
In this case the scratch space is just an `i32` i.e. 4 bytes, and the asm ends up being reading the two elements into registers and then writing each register back to the other location (i.e. no extra space is being used).
absolutely.
If you look at the source, you'll find that the default implementation of `ne()` looks like this: fn ne(&amp;self, other: &amp;Self) -&gt; bool { !self.eq(other) }
So my next question is: if you can override a default implementation of a trait - can a child trait override the default implementation if the parent didn't? e.g. pub trait PartialEq { fn eq(&amp;self, other: &amp;Self) -&gt; bool; fn ne(&amp;self, other: &amp;Self) -&gt; bool { !self.eq(other) }; } pub trait Eq : PartialEq { } impl Eq for CustomStruct { fn eq(&amp;self, other: &amp;Self) -&gt; bool { ... }; fn ne(&amp;self, other: &amp;Self) -&gt; bool { ... }; } And can a child trait provide its own default implmentation? e.g. pub trait PartialEq { fn eq(&amp;self, other: &amp;Self) -&gt; bool; fn ne(&amp;self, other: &amp;Self) -&gt; bool { !self.eq(other) }; } pub trait Eq : PartialEq { fn ne(&amp;self, other: &amp;Self) -&gt; bool { !self.eq(other) }; } 
Great news. Does this include a 64-bit version of Cargo, or would I have to install that separately? Honest question: roughly what percentage of packages, say listed on http://rust-ci.org (or is there a better listing?), have ever been installed or tested on a Windows machine? The choice of Dwarf on Win32 (though I guess not new) is a bit puzzling to me - quoting from http://stackoverflow.com/questions/15670169/what-is-difference-between-sjlj-vs-dwarf-vs-seh, &gt; In win32 mode, the exception unwind handler cannot propagate through non-dw2 aware code, this means that any exception going through any non-dw2 aware "foreign frames" code will fail, including Windows system DLLs and DLLs built with Visual Studio. SJLJ is slower, but Dwarf's inability to deal with exceptions from 3rd-party DLL's is pretty bad.
Great! Have you considered adding other Unicode properties too?
Thank you for that example. Interesting that you can define the child trait and overriding function - but it's not until you try to use it that you get "multiple methods in scope" compile-time errors.
There are a lot in the [`unicode` crate](http://doc.rust-lang.org/master/unicode/), most of which are accessible as methods on [`char`](http://doc.rust-lang.org/master/std/char/trait.UnicodeChar.html). But no, I'm planning to keep this library focused and simple for now, unless there's some gains to be had from combining them together.
&gt; I was under the impression that traits were like interfaces in Java - abstract function prototypes without implementation. Note that Java 8 introduces default method implementations in interfaces.
A note: for some reason the installer defaults to "C:\Program Files (x86)\Rust", rather than "C:\Program Files\Rust".
&gt; GCC and LLVM don't support SEH on 32-bit. I know. That's why I didn't mention SEH when discussing Win32. &gt; Interaction with third party libraries doesn't matter As someone who calls into a huge number of third party libraries, I very strongly disagree. No sane way of dealing with failures originating in FFI? Edit: nevermind, I was unaware of the specific meaning of "failure" in the context of Rust, or exactly what the exception handling facilities of the chosen MinGW implementation are used for.
It's not really inheritance as in class and super class, as I understand it. It's more like saying that something has to, for example, implement `PartialEq` to be able to implement `Eq`. It gives `Eq` the properties of ` PartialEq` because of this dependency, just like inheritance, but you have to implement the whole chain instead of just collecting all the necessary functions under the child trait.
&gt; As someone who calls into a huge number of third party libraries, I very strongly disagree. No sane way of dealing with failures originating in FFI? Failure doesn't exist in foreign libraries. It's a Rust-specific feature that's not compatible with exception systems in other languages like C++. It's not possible to catch failure in Rust anyway, it wipes out the whole task. It's not used to handle runtime errors, only logic errors. There's no reason to do failure handling via a mechanism with a significant runtime cost in the non-exceptional path.
&gt; &gt; Interaction with third party libraries doesn't matter &gt; As someone who calls into a huge number of third party libraries, I very strongly disagree. No sane way of dealing with failures originating in FFI? I think he just meant that this does not work anyway.
You can't catch failure in the safe subset of Rust, and there are a lot of caveats if you're doing it manually via `unsafe`. If it was memory safe, only allowing it to be caught at task boundaries wouldn't make any sense. We have nearly all of the drawbacks of exceptions right now (slow compile-time, binary bloat, many exception safety issues) without the advantages. Regardless, throwing failure across a boundary into foreign code is undefined behaviour, as is throwing a C++ or SEH exception into Rust code.
Okay, strange. Sorry for using wrong terminology then, the lack of win64 support has kept me from actually trying to use rust yet. I work with large data sets and call into 64 bit dll's frequently. Though the number of other MinGW toolchains on my system with very particular configurations means I'll be waiting a little longer until https://github.com/rust-lang/rust/pull/16957 lands into the nightlies. If the only thing that Rust uses libgcc's exception handling for is the Rust-specific notion of failure, then I suppose it makes sense. But when almost every C and C++ library that I want to use is built with either SJLJ (in the 32 bit case) or MSVC's SEH, Rust pulling in an incompatible runtime library will probably make life difficult.
There will be support for using Rust without failure in the future. It's already possible if you only use the low-level libraries. It's not a very useful feature anyway.
Okay, I see now that it's undefined to throw a C++ exception across an `extern "C"` API. So hopefully this shouldn't happen if libraries are exporting proper C API's and being disciplined about it. If Rust ever attempts a C++ FFI, this seems like it would be an impedance mismatch.
Just installed it. rustc --version worked fine, but compiling hello-world errors out because there is no gcc linker. I don't see any instructions in the getting starting guide for installing anything else, so... not sure. Freebsd ports has __morestack failure. So I'm still unable to do more than toy programs on web compilers :( One day, Rust, we will be together.
AFAIK Rust on Windows still requires MinGW. Easiest way to get set up (in my opinion) is via the [msys2 installer](https://msys2.github.io/). Install it, update packages via msys2_shell.bat (may need to do this a few times, re-opening msys2_shell.bat at each iteration), then use mingw64_shell.bat for regular development. Though it's entirely possible I'm wrong and your problems are due to some packaging error. EDIT: Looks like MinGW deps were added a few hours ago. https://github.com/rust-lang/rust/pull/16957 , so this may not be necessary in the future. That said, many packages use GNU make or shell scripts at the moment, so it may be prudent to keep an MSYS install around.
Also please goto http://toprustservers.com/ and look for "burned x3gather/no dura/fall/tpa &amp; home" (no.21 server) and comment Admin abuse, thanks!
You want /r/playrust
I think you're confused. This subreddit is about Rust the programming language. What you want is the subreddit for the game Rust at [/r/playrust](/r/playrust)
Can't the library simply avoid using functions that can fail and just return Result?
oh shit hahaha
[Probably not](http://logs.glob.uno/?c=mozilla%23servo&amp;s=11+Sep+2014&amp;e=11+Sep+2014&amp;h=android#c109402). Not sure, though. Certainly is cool :)
Templates and object inheritance are other impedance mismatches. I don't think usable C++ FFI is realistic. Even if Rust had object inheritance, it wouldn't map to how it's implemented by C++ compilers.
How about a syntax extension that automatically generates a tuple struct filled with `n` fields of the same type, and automatically implements `Deref` for it? So that `tuple_array!(OneHundred, 100)` gets converted roughly to this: struct OneHundred&lt;T&gt;(T, T, T, ..., T); // one hundred `T`s impl&lt;T&gt; Index&lt;uint, T&gt; for OneHundred&lt;T&gt; { fn index(&amp;self, idx: &amp;uint) -&gt; &amp;T { &amp;match *idx { 0 =&gt; self.0, 1 =&gt; self.1, 2 =&gt; self.2, ..., 99 =&gt; self.99, } } } That means that `OneHundred` could still roughly be used as a fixed-size array (with dynamic indexing), but also could be used as a tuple struct, meaning that multiple mutable borrows could be taken out so long as they are different, compile-time indices. I agree that this would be nicer if it were built-in to the compiler. I just don’t know how useful it would be. (I have never wanted such a feature, and I rarely use fixed-size arrays in any case.)
Failure catching is an extremely useful feature for fine-grained task isolation without process overhead. How do you propose that, for example, Servo recovers from an index-out-of-bounds exception during image loading and displays a broken image icon without exceptions? By spawning a whole process per image (unacceptable overhead)? By crashing the whole tab (much worse UX)? That said, I agree that we should fully support no-exceptions libraries for those who don't need the feature. But I dispute that the feature is not useful.
&gt; You can't catch failure in the safe subset of Rust You can catch failure in the safe subset of Rust. By using a task. &gt; We have nearly all of the drawbacks of exceptions right now (slow compile-time, binary bloat, many exception safety issues) without the advantages. We don't have exception safety issues like C++ does. &gt; without the advantages. The advantage is that you can recover from things like out-of-bounds array access without the runtime killing your whole process dead.
&gt; The choice of Dwarf on Win32 (though I guess not new) is a bit puzzling to me My understanding is that SEH on 32-bit is not zero-cost via table-driven unwinding, which is pretty painful from a performance point of view. &gt; Dwarf's inability to deal with exceptions from 3rd-party DLL's is pretty bad. We will clearly need some way to catch SEH exceptions from 3rd-party DLLs. Proposals are welcome :)
The whole inheritance saga will end in tears. Even most OO people now preach "prefer composition over inhertiance" and admit that it creates a very close coupling, not to mention that people often do not get the Liskov substitution constraint right. But hey - in Rust we need this form of "code reuse" and couple it with subtyping. Back to the nineties, yay!
I'm still in favour of the OCaml or Haskell Keywords for ADT's ie "type" and "data" respectively. Probably Haskell's as we use type elsewhere. I agree that enum violates the expectations of people coming from C/C++
I agree, however I don't know if maintaining two pieces of similar documentation is possible/desirable.
Well... there's also the opinion that removing certain tools which are notoriously toxic is good: a prime example is mutability. When I first started learning C++, I remember seeing the "const" keyword for the first time and to me it just seemed so stupid: it literally does nothing other than limit what I can do. Why would I ever want to do that? Everything I could have written with const I could write without it and have more functionality. What a stupid idea. And yet I now spend my time in languages like Rust and Haskell. I had no idea how much I wanted somebody to take those tools away from me until I experienced it. Availability of a tool is not better than unavailability simply because one can "choose not to use it."
How about "tagun"?
I don't personally think "data" makes a lot of sense, as someone not familiar with Haskell could rightfully ask "Isn't a struct data as well?"
Well yeh, "data" would work best with one of the proposals that unifies struct and enum.
Thanks! &gt; All that is guaranteed is code compiled now will be supported on iOS 7 and Mac OS X 10.9 and newer. To clarify, if ABI compatibility is *not* maintained, how will this be accomplished? iOS/MacOS will ship multiple versions of the libraries/runtimes, and older Swift programs will just use the older ones? (Maybe they already do this for Objective-C; I wouldn't know.) I wonder why it is that Mozilla/Rust consider source forwards-compatibility from 1.0 to be of paramount importance, and Apple/Swift do not.
eh, it's okay to have many books on the same topic.
You can find it on the wayback machine. The tutorial has many, many problems, and isn't worth keeping around without putting a lot of work into it, and then you just end up with the guide.
Not when one of the books is wrong and out out date...
Does it really violate expectations from C? afaict, it can do all the same things as a C enum. Coming from C, all I expected from enums was a logical grouping of things, that because of limitations had to have a number associated with them even if the variants didnt logically want numbers). To me, it just seems like a much better and more flexible C enum.
We can't just not solve this problem on philosophical grounds. Not only does Servo need this to create the DOM, the need for something like inheritance comes up again and again (such as entities in a game world, display lists, etc). We can make it feel and work like composition instead of inheritance, we can restrict subtyping to coercions, and we can also have closed instead of open inheritance, fixing a lot of the fragile base class problem. I would like to emphasize again that we cannot just wish these problems away because object-oriented programming is out of fashion. We didn't want to add something like inheritance because we love OO. We needed it because there is literally no other way to solve some serious engineering and performance problems that we had when developing software in Rust.
Of course. I have a lot of respect for his work in general, and you can see how well he responded in the comments in this case.
i can't help but feel *very* dubious with regard to a notion like "no other way but inheritance"
The fat objects approach would be a nice-to-have *option* for DSTs in general, I guess. For example, I thought about implementing a [skip list](https://en.wikipedia.org/wiki/Skip_list) using struct Node&lt;T&gt; { value: T, next: [*mut Node::&lt;T&gt;], } when DSTs are fully supported, but then I realized that all the pointers would be fat, effectivly storing the "height" of a node redundantly in every next pointer. It seems better to have this meta information about the length of the last DST field being stored directly inside the node object, perhaps via virtual struct Node&lt;T&gt; { value: T, next: [*mut Node::&lt;T&gt;], } where `virtual` would make the compiler move the length of `next` inside the node object and keep all the pointers thin. I believe, the Servo DOM issue is similar to this.
&gt; Not only does Servo need this to create the DOM, the need for something like inheritance comes up again and again (such as entities in a game world, display lists, etc). Are we a little biased maybe? I think this claim needs some serious data to back it up. As it stands I do not buy it. And I would have preferred a much more detailed motivation before reaching this conclusion instead of every RFC just copying the bullet points of the previous ones.
The motivation has been detailed many times—the Servo DOM needs to be fast. There is plenty of data that has been presented in the various RFCs.
What is your concrete suggestion?
I have a Pi, sounds like fun :D. But I don't have anything to automate; not allowed to rewire my room :P
So rust can compile to the raspberry pi now? I never realised..
Yeah, but they need to be vigorously maintained by different people. Is that really going to happen here?
You must be looking at a different set of RFCs then. The ones I have seen (linked to from the meeting minutes) all repeat the same mantra: &gt; Supporting efficient, heterogeneous data structures such as the DOM or an AST (e.g., in the Rust compiler). Precisely we need a form of code sharing which satisfies the following constraints: &gt; &gt; - cheap field access from internal methods; &gt; - cheap dynamic dispatch of methods; &gt; - cheap downcasting; &gt; - thin pointers; &gt; - sharing of fields and methods between definitions; &gt; - safe, i.e., doesn't require a bunch of transmutes or other unsafe code to be usable." or better even &gt; Provide an alternative to adding virtual structs and virtual functions that provides the same ability to express Java-like OO, but keeps the good design of current Rust and extends the current Rust primitives instead of adding a parallel system. Which is about as vacuous as stating *"We need OO-style inheritance because we need it!"* Do you honestly think a powerpoint-style list of constraints counts as "plenty of data" and motivates the proposed "solutions" well enough? Surely you are joking Mr. Walton.
Looks like another instance of the [xy problem](http://www.perlmonks.org/?node=xy+problem): &gt; What's the rationale behind it? [...] &gt;&gt; It's about the fact that there's no good way to model classical OO in Rust right now. [...] 
Wit.ai team here. Yes, as you can see on witd's README (https://github.com/wit-ai/witd), compiling for Raspberry Pi is very hacky right now. Compiling a Rust cross-compiler is fairly simple, but then you need to make sure the linker can find all the necessary libraries. We ended up mounting the Raspberry's filesystem remotely on a Debian host - and we also had to trick Cargo into passing extra options to the linker (by specifying a custom linker in .cargo/config).
Quoting myself here: http://www.reddit.com/r/rust/comments/2g6hus/weeklymeetings20140909_fnmut_sugar_assoc_types/ckg4zxm
Hi Everyone, I've been learning Rust and built a somewhat working CQL driver! Hopefully the community finds this interesting, I'd like to evolve this into something more than just a pet project for learning. This is my first project in Rust and also my first time writing a driver for a DB. It's been challenging, but I think I'm finally starting to get the hang of this stuff. Thanks to all the contributors to the Rust docs!
Don't disagree! I think the other thing to remember is that Java-style references are actually `box`s in Rust - shared pointers that only "garbage-collect" when no more references are made to that object. For a Java programmer the idea of a "borrowed pointer" is quite alien.
Nah, they're not joking, they're just focusing on the wrong things: they need trait objects that store the vtable inline in the data instead of having it next to the data pointer; and they need cheap fixed-layout field access through trait objects. They are both optimizations. The latter can build on *composition*/DRY-targetted associated fields in traits, and in my (not yet written all down) plan, the former uses the latter + a `#[repr(inline_vtable)]` annotation. The DOM (and a web browser in general) have horrible design details, getting into conflict with idiomatic Rust, but that doesn't mean we can't provide orthogonal features that many other usecases benefit from.
Servo needs to model the DOM, and thus needs to access fields of supertypes without virtual method calls, and also needs to store pointers to nodes without duplicating their vptr everywhere. Here's an example in C++ of what needs to be modeled: https://gist.github.com/jdm/9900569 That's impossible with traits as they currently are. One solution is plain old inheritance, but if you look at how inheritance is implemented, it's actually a bunch of features that are useful on their own.
I don't see why anyone would be biased in favour of a particular language feature, in particular a specific form of inheritance. We know we have a problem and that problem has performance constraints. We want to solve that in the best way possible. The bullet points come from the Servo team's experience implementing the DOM. To some extent we must trust that they haven't made these up, but I'm comfortable that they have not and have measured performance to get these constraints. So far the best ways possible have all been variations on single inheritance. We are open to other ideas, otherwise we would not be going through this process.
Traits do not allow thin pointers to trait objects nor fast access to fields.
That list of constraints *is* the data. It boils down to "we need a fast, maintainable Web browser engine". If you don't think that writing a fast, maintainable Web browser engine is a worthy goal of Rust, then there's no use arguing.
I agree that Rust needs to solve these problems. I just wish there weren't so many proposals essentially just adding inheritance, and there were more emphasis on finding better, more orthogonal solutions.
Yes. The list of bullet points all describes areas in which traits fall short (thin pointers, field access, etc.)
No. A browser engine *must implement the DOM to be a browser engine*. We have to implement X to do X.
We prefer 'composition' over inheritance too, but sometimes you have closely coupled 'classes' (i.e., within a single crate or module) where performance is more important than flexible re-usability. If you imagine a spectrum from performant to flexible, enums are at one extreme and traits at the other. Efficient single inheritance is somewhere in between. Note that none of the proposals tie inheritance to subtyping, only to coercions which is the same situation as trait inheritance today. We want to take the best bits from various paradigms, whether old or new. That is better than being a slave to fashion and taking only features that are 'cool' now.
Do you have the build-essential package installed?
And don't forget the FUD about some now-expired (I think?) Borland patent. 64-bit is a more important target in most cases, unless you have a lot of closed-source 32-bit binaries that you need to call into. Thanks for your work upstream BTW, getting (64 bit) SEH into LLVM will be really useful for Clang and Julia (what I'm mostly using right now) and other projects. Does cross-compiling a Windows version of Rust from Linux work yet? https://github.com/rust-lang/rust/issues/12859 In my experience with Julia it's a much faster way of getting the binaries built than compiling natively from MSYS.
This is the wrong subreddit. Go to /r/playrust
There's [`std::sync::atomics`](http://doc.rust-lang.org/master/std/sync/atomics/), you'll have to manually put padding in, and the reference stuff is mostly easily modeled by just restricting it to `Send` types and taking ownership (i.e. taking the argument by-value) which automatically stops the sender being able to keep a reference around.
thanks for working on this. These types of libraries are badly needed in the Rust ecosystem
I did not, and installing it resolved the problem. Thanks.
You should interpret it as node.js troll.
`test` and `cfg` act very similarly to macros, fwiw. `cfg` could be implemented as one (possibly with a few small adjustments), as could the test harness as a whole (again with some small adjustments). &gt; There's a macro in every Rust 'Hello World'. use std::io; fn main() { io::println("Hello world"); } This isn't a spurious example, a non-insignificant proportion of Github hello-worlds use it: - [macro hello-worlds](https://github.com/search?utf8=%E2%9C%93&amp;q=println!%28Hello+world%29+language%3ARust&amp;type=Code&amp;ref=searchresults) - [non-macro hello-worlds](https://github.com/search?utf8=%E2%9C%93&amp;q=io%3A%3Aprintln%28Hello+world%29+language%3ARust&amp;type=Code&amp;ref=searchresults) &gt; Perhaps the post refers to postfix !, but the only language that I know of that does that is Ruby, and it does not use it for error reporting Swift uses it for error handling. (Also, many lisps use it in a similar way to Ruby, and some seem to use it for "dangerous" operations.)
The most critical thing to respond to out of that comment was the trivially minor literal inaccuracy of a rhetorical statement supporting the true assertion that macros are very common in Rust?
&gt; How can I do this in Rust without offending the lifetime analysis? There are concurrent readers and writers on the same backing array, though the consumer only writes to it's sequence and the producer only writes to it's sequence and the backing buffer. I don't think there's a way to give concurrent mutable access to the same array from different threads, except for the good old `shut_up_compiler_i_know_what_im_doing` keyword, usually shortened to `unsafe`.
Can you elaborate on how this is the diamond problem? This seems like a single-inheritance hierarchy. Is it because for some `trait B: A` doesn't entail a parent-child relation, and impling B is the same as impling two traits separately?
&gt; Or rather, what is the fundamental interaction which gives rise to exception unsafety? Interacting with invalid objects: in the common case, running destructors on partial data, but for "task" code, data can be left in an invalid state after the other "task" unwinds. The `Send` bounds stops this happening because there's no sharing without `unsafe` (and then types with `unsafe` internals that can be affected like `Mutex` need to be careful to poison themselves). &gt; And in particular, Send seems too restrictive to me. I have a hard time believing that &amp; references could cause any issue here. Which then suggests that the only real problem is &amp;mut. But then do we add a new built-in trait above Send which allows &amp; but not &amp;mut? What is its precise definition, what's the intuition behind it? Is forbidding &amp;mut a necessary and sufficient condition for exception safety? I think `&amp;RefCell` has similar problems to `&amp;mut`, so it seems that allowing this would require `Freeze` at least.
&gt; but without any real reason for requiring it. This is not true. It simplifies the grammar a great deal, which helps with tooling.
&gt; Perhaps the post refers to postfix !, but the only language that I know of that does that is Ruby, and it does not use it for error reporting. This is true, `!` refers to something that is 'dangerous', just like in the blog post. &gt; Part of the reason for this is that ! has a long history of being the sigil one uses to indicate something dangerous or surprising. 
&gt; This is true, `!` refers to something that is 'dangerous', just like in the blog post. My impression was that it designated in-place modification. Is that not the case?
I'm genuinely surprised that people reach out to `io::println` instead of the macro. That said, I am going to blame Steve for this, as Rust for Rubyists used to show that as (in my opinion) decidedly unidiomatic 'Hello World' up until a few months ago: https://github.com/steveklabnik/rust_for_rubyists/commit/84c97b13e4fba90da57946cfd0125cbff7075e73 &gt; Swift uses it for error handling. It seems very un-Rustic to me to start treating what a new, unproven language does as a good precedent. Can we at least give it a year before we start calling every syntactical decision it made a success?
Some might consider that potentially dangerous :)
This doesn't explain why does it require the reader's attention.
You will be just as easily able to filter out macros with a unified syntax, because decorator macros will be unambiguous. If you can lex token trees, you can lex macros by just looking for an at-prefixed identifier and parsing a token tree.
That is a common misconception. That is only one kind of danger. For example, in Rails, many methods will return nil, and their ! variants will throw exceptions instead.
Use the serial connector. You get output from the boot loader very early on.
`!` is used in lisps as far as im aware to show bad things happening. I know in clojure there are functions defined as `do-a-thing!` which mean "this function does something bad!" http://stackoverflow.com/questions/20606249/when-to-use-exclamation-mark-in-clojure-or-lisp I am sure other languages do that, I have seen it, but it escapes me.
They're not dangerous, they are hygienic and the type check like the rest of Rust.
I'll probably get one of those cables the next time I give it a try.
Don't straw man. He's obviously claiming X is implementing the DOM and Y is saying inheritance is necessary in the language, and as silly as naming something "the xy problem" is, this is absolutely an instance of it. When I first saw inheritance mentioned here, my initial response was "... why? Did someone from Cambridge publish a new paper on the virtues of inheritance that I'm not aware of?" Then I come to the comments and see "Oh. Someone close to the committee wants it for their project. Right." I'm not for or against inheritance one way or the other; I'm just pointing out the clear politicking going on here which you seem to be asserting is a mere technical issue.
That is not the case, since it would be unparsable by the Rust compiler too (unless we intertwine (limited) name resolution and parsing, and provide some way for macros to "chomp" arbitrary source after their invocation), i.e. it is a proper AST item. I think it's fairly clear that a *massive* change to how attributes/macros work (i.e. removing the source distinction between arbitrary-token-trees and proper Rust) would not part of this syntax renaming proposal; generalising attributes to take arbitrary token trees inside their `()` is a very natural extension of what we have now.
I was referring to the Ruby standard library convention's of using `!` to denote methods which mutate rather than return a new object.
I don't think anybody is calling `&amp;mut self` dangerous. This conversation was about Ruby's use of `!` and in Ruby, like most languages not called Rust, mutation can be dangerous.
Why does having a different definition of danger mean that comparisons to Ruby are invalid?
Assuming https://github.com/rust-lang/rfcs/pull/194 lands, all that would be required to make `cfg` a syntax extension is the ability for syntax extensions to parse non-items like match arms, fields, and variants.
Macros and functions are different. Their evaluation rules and the places where they can appear are different. It helps when we can easily tell macros from functions (or from attributes, for that matter). But the exact syntax to be used is open for bikeshedding. Of the three alternatives in the blog post, I prefer the third. EDIT: oh you were not saying that we did not need to differentiate, what you meant was that `!` was too much. Personally I am fine with `!`, but I think `@` is fine too. 
Of the three alternatives in the post, I prefer the third. But the other alternative proposed in the comments by Sodel_the_Vociferous can be better: `@foo`: macros; `@foo:`: outer attributes (the colon signifies the fact that an outer attribute applies to the following entity); `@!foo`: inner attributes. 
Sorry for the late reply. Here is a quite relevant piece of discussion, which starts in [this mailing list post](https://mail.mozilla.org/pipermail/rust-dev/2013-November/006550.html). 
Steve, I think the main doc.rust-lang.org page has regressed in the last few days. The link to the reference manual is completely gone. The link to the standard library "lost the importance" and was moved somewhere to the end of the document and hidden under "here" in the middle of a sentence.
It would be nice to not get confused by the error message each time you forget a ';'. That is such a killer of programmer productivity. Ideally, there should be just one error message and it should include "did you forget a ';'?" in the error message.
Thanks for another great blog post explaining the rationale behind these changes. I was at first unenthusiastic but this changed my mind. I would say let's try out option 2 and see how bad the ambiguities are in practice.
Skimmed through the docs and source. Very nice! This could be useful for game assets!
I agree that using `!` to distinct potential failures at first sight is a good idea. But there is a huge problem : `!` is used for negation too. To solve this problem you would need to change `!`/`|`/`&amp;` to `not`/`and`/`or`. It might be a good thing : it would allow to distinct references and closures at first sight too. Having only one purpose for each sigil make the code much more readable. It might even allow future changes that would have been impossible because of ambiguities. (IIRC, with the old `do` syntax the keyword was necessary to prevent ambiguities between the "closure pipe" and the "or pipe") But i am pretty sure it is far too late for such a change and a lesser "C like" syntax would not please everyone.
There's a fix in the queue to re-add the manual link, it got lost in a rebase.
dammit, every time i want to code something in rust that’s missing, someone else has already done it. should have finished that XML parsing library back when nobody else had one.
&gt; Interacting with invalid objects: in the common case, running destructors on partial data Well then, what causes invalid objects and partial data? :) Anyways, I've both thought about this [a lot more][notes] and partly [moved past it][notes2] since this comment was written. But if you have any further observations about the linked stuff, please share! [notes]: https://github.com/glaebhoerl/rust-notes/blob/268266e8fbbbfd91098d3bea784098e918b42322/my_rfcs/Exceptions.txt#L135 [notes2]: https://github.com/glaebhoerl/rust-notes/blob/268266e8fbbbfd91098d3bea784098e918b42322/my_rfcs/Exceptions.txt#L546
That was my inspiration actually. The missing writer and some API trouble had me decide to write something my own, but maybe it can be merged later on
Yeah, there was a while where I wasn't heavily involved in Rust day-to-day, where either the macro was confusing or I was confused about it (I vaguely remember the macro not being able to print just a string literal), plus not wanting to talk about macros right away.
Great to see how quickly key libraries are being created! Also impressed by how well lessons have been learned from previous languages - brand new stuff but already you have clean looking docs, easy install from a package manager, the works...
Thanks for summarizing the two main issues that arose in that HN thread. I agree that the guide is good. I am an programmer with 6 years of C, Objective-C and Java and found the material to be just about right in depth and scope. I've also found Steve's writing in the past to be a bit distracting in its informality but I didn't get that feeling at all this time around. I think he's starting to find a voice that works well with this kind of technical writing. It's conversational enough to keep me engaged and interested, but not so much so that I become distracted or annoyed by it. I found the guide to be a really good read!
Thank you.
This RFC makes immutable variables more immutable and less against programmer intuitions. ~~2. makes guaranteeing (scoped or whatever) lifetimes of movable values easier;~~ ~~3. makes NoisyDrop/QuietDrop from RFC PR 210 unnecessary.~~ EDIT: The second and third points are invalid because guaranteeing lifetimes can be done *today* with explicit drops *alone*. 
&gt; I don't see why anyone would be biased in favour of a particular language feature, in particular a specific form of inheritance. I can tell you why: &gt; We want some way to have code sharing which is more efficient than traits (in terms of time and space) [...] [1] So clearly it has been recognised that traits as a language feature already provide the desired functionality. If the implementation isn't efficient enough then ... well, make it efficient. Also it isn't obvious at all to which use of traits the author is alluding to--(i) traits as a vehicle for ad-hoc overloading (aka type-classes) or (ii) "trait objects" (aka existentials) and their "late-binding" behaviour. &gt; and more flexible than enums. [1] Another vague statement. Why would enums not be flexible enough? No data. I assume that this is related to: &gt; You can't use enums, because there's no way to extract base fields without a pattern match (which is too expensive and LLVM does not optimize it, and is also just plain ugly) [2] where the mention of "base fields" shows the very same bias I was referring to. And of course the tools (i.e. LLVM) are blamed and used as an excuse why there just isn't an alternative (like for example improving LLVM). &gt; We want to solve that in the best way possible. Then make pattern matching efficient (which you should do regardless) and fix your existentials. Otherwise tell me why a DOM can't be modelled as a sum type with type-classes for ad-hoc overloading. What I read from Rust people is that "something like inheritance comes up again and again" [3] which shows an obvious modelling bias and yet big (and efficient) projects are done without it (I wonder how [GHC](http://www.haskell.org/ghc/) could ever be done without inheritance). --- [1] http://discuss.rust-lang.org/t/summary-of-efficient-inheritance-rfcs/494 [2] https://www.reddit.com/r/rust/comments/1p52tj/a_draft_proposal_for_single_inheritance_in_rust/ccz92hb [3] https://www.reddit.com/r/rust/comments/2g6hus/weeklymeetings20140909_fnmut_sugar_assoc_types/ckgcrod 
:D
&gt; but sometimes you have closely coupled 'classes' [...] where performance is more important than flexible re-usability. What do you mean by "classes"? And why can you not use sum types? It would seem quite natural to me to define the various node types and make them instances of relevant type-classes (which are statically resolved) and define a single sum type whose constructors contain the individual types. To deconstruct this type use pattern matching. &gt; That is better than being a slave to fashion and taking only features that are 'cool' now. I do not understand what you are trying to say here. 
I don't think GHC cares about the memory representations of its internal structures down to the byte; it certainly doesn't have the same sort of performance goals as a modern browser engine.
I've been rolling over how to write this in my mind, even outside a concurrent context. This almost certainly is going to need some unsafe code, or a pile of Rc/RefCell indirection, maybe both. It seems plausible to write a reasonable safe interface to it, though. Some questions about your usecase: * Is it necessary or desirable to be able to consume/hold mutliple values from the array at once? That is, would it be satisfactory if the consumer *had to* discard the last value it got before it could access the next one? * Is it necessary or desirable that the producer can add elements while the consumer has an element (I assume so, but I just want to make sure I'm not misunderstanding). * Is the ring buffer fixed capacity? If so, how is the producer/buffer expected to behave when it is full? Drop values? Block? Hard fail? * Is it necessary that the consumer gets elements out of the buffer by-reference? Is by-value satisfactory? And if it can get the elements by-value, does it matter anymore if the consumer can hold onto the values? * Is it necessary or desirable for the producer to be able to read the buffer? E.G. if the buffer was hidden from both at the interface level, and all producer could call was "push" and all the consumer could call was "pop", would that be acceptable?
&gt; Note that none of the proposals tie inheritance to subtyping, only to coercions which is the same situation as trait inheritance today. Well, coercions are one way to define subtyping and when I read: &gt; All names of variants may be used as types [...] Fields in outer items are inherited by inner items [...] Inner enum values can implicitly coerce to outer enum values [1] then this looks very much like subtyping to me. --- [1] https://github.com/nick29581/rfcs/blob/virtual2/0000-virtual.md
This is not implemented yet, but according to the spec you can also set an is UTF-8 flag. This could break compatibility with very old clients, but since it is in the spec, it should not cause too much trouble.
I remember doing some testing, since I didn't really trust the spec.^1 I believe I tried WinRAR, 7z and InfoZIP; WinRAR mangled the filenames whilst 7z and InfoZIP could both create archives with Unicode filenames, but they couldn't correctly read each other's archives. I know one of them was storing UTF-8 directly without any kind of flag, the other was storing it in a separate bit of metadata somewhere. I know neither conformed to the "official" UTF-8 spec which was several years old at the time. Zip's only real advantage is how widespread support for it is; I'd personally be *extremely* conservative with breaking compatibility. At the time, I concluded that Unicode support was impossible under that philosophy. --- 1: Things may have changed since I did this, but I remember big holes in the spec that had to be filled by inspecting the output of other programs and libraries. For example, it never specifies what number to start multi-disk archives on: 0 or 1. This is why I only vaguely trusted what it said. That said, this was several years ago.
The sum type solution is currently used in `rustc`. Because enums are the size of their largest member, it wastes a lot of space and uses a lot of pattern matching which could be faster in some cases. The optimal solution would let each type use exactly the space it needs, and allow for faster dispatch when it makes sense to do so- traditional inheritance with virtual methods is one way to do that, although some pieces of the current RFCs on the topic provide other solutions.
&gt; Because enums are the size of their largest member, it wastes a lot of space [...] Why don't you use pointers?
HN is especially weird, because many people can upvote but not downvote. (There's a karma minimum, and assuming it's anything like reddit, many more people vote than actually comment/participate.) Since many more people will vote on the top comment than on ones further down, I think this makes the first somewhat popular post (no matter how inane) quite sticky. If you read the discussion, most folk seem to disagree with the "too casual" claim.
&gt; There's a lot of talk about the Guide being too long/informal for a C/C++ programmer who just wants a quick diff between Rust and C++ - specifically the feedback would like a much shorter Guide that doesn't explain things in detail. I think people interested in a "minimal" guide should have a look at [Rust by Example](http://rustbyexample.com). It is very condensed. It would be great to have something like [Rust for functional programmers](http://science.raphael.poss.name/rust-for-functional-programmers.html) for C++ though.
`rustc` does in fact use pointers. It would be impossible not to, since they're building up an AST of an arbitrary source file. That doesn't mean the value being pointed at is only the size of its current variant... ...which might be a possible optimization, but it would be quite complicated and only applicable in some circumstances. A better solution is just to let the programmer specify what they want directly.
Have you given any thought to how to implement something like the DOM using just traits and/or enums? It becomes immediately apparent that they are not a very good way to express a solution to the problem, regardless of their performance. A simple example from the DOM: You need a bunch of different node and element types that all share several fields- a parent pointer, a first_child pointer, a list of attributes, etc. They also all share several methods that use those fields. Now, given a reference to some element (you don't care which, because you're in a default method in the Element trait or something that just needs those common fields), how do you access those fields? With the traditional solution of inheritance, any reference to an Element parent class already knows the exact location in memory of those fields. In current Rust using traits (e.g. impl Element for HTMLImageElement), any default methods in Element don't know about HTMLImageElement's fields, whether it has an ElementFields member or just puts them in there directly or whatever. So you have to use a virtual accessor method and implement it over and over for every child "class" of Element. Even worse, now every single reference to an Element has to store its vptr next to the actual pointer. In current Rust using enums, you have a bunch of enum cases for every child class of Element, all with an ElementFields plus the rest of their contents. They all take the same size in memory as the largest one, and you still can't just access the fields from Node and Element without writing an extra match arm for every single child "class". No amount of optimization or "fixing" LLVM is going to change any of this. The tools to express the DOM in a sane way simply aren't there. The quick and dirty way out is to just add inheritance, but at the lowest level we just need a few different features: * the ability to access fields shared among different DOM node types without virtual accessor methods or a giant match * fat objects/thin pointers, or trait objects with their vptr stored next to the object itself * the ability to safely check if a reference to a Node is actually one of its children, and extract a pointer to that child, without making all children the same size That looks an awful lot like the list of requirements in those RFCs.
&gt; The top comment on this post is blatantly disagreeing with this decision, and nearly every other comment not tagged with the orange "Mozilla" flag is at least questioning if not outright disagreeing with adding inheritance. &gt; The fact that your project dictates what goes in the language even when the community totally disagrees is what makes it politics. We don't, and shouldn't, make decisions based on what the majority thinks; our decisions are based on technical merit. I haven't seen any valid arguments against adding inheritance; they've all been philosophical arguments about the merits of OO versus FP (for example, the top comment in this thread). Being opinionated about OO versus FP, when doing so leads to worse performance, safety, and ergonomics, is not in line with Rust's goals.
You still have the overhead of fat pointers, and you still have no way to access shared fields without a giant redundant match or a virtual call.
&gt; You still have the overhead of fat pointers What do you mean by "fat pointer"? The ones I was referring to are certainly "ordinary" pointers and the type information w.r.t. T1 and T2 is known statically, hence no virtual calls. &gt; you still have no way to access shared fields without a giant redundant match or a virtual call. Do you mean another type X which is used by T1 and T2? Like so (excuse the pseudo notation): datatype X = X { commonStuff :: Int } datatype T1 = T1 { x :: *X, foo :: Bool } datatype T2 = T2 { x :: *X, bar :: Int } let a = C1 (new T1 { new X {0}, True }) match a of C1 t1 =&gt; println (t1-&gt;x-&gt;commonStuff) C2 _ =&gt; return () I do not see a "giant redundant match" nor a "virtual call".
&gt; I haven't seen any valid arguments against adding inheritance 1. The close coupling it creates between base and derived classes. 2. Related to 1. the [fragile base class problem](http://en.wikipedia.org/wiki/Fragile_base_class) 3. A redudant mechanism for virtual dispatch (which is available through "trait objects") 4. A redundant mechanism for code re-use (which is available through traits as type-classes) while also being weaker (single inheritance vs. multiple traits). 5. That you will most likely end-up with subtyping (via implicit coercions) and all the [LSP](https://en.wikipedia.org/wiki/Liskov_substitution_principle) violations will cause lots of trouble. 
You cannot statically know the type of the entire DOM from a document loaded from the network. When you put a TheSumTypeIWasTalkingAbout in a struct, vector, etc. somewhere, as you must when actually implementing the DOM, it will have to contain a tag next to the pointer. You also can't make a two-variant enum and then claim the problem of the giant match is solved, when the actual DOM would have far more than two variants. Every single node type would have its own redundant arm in the match, just to access a common field (in your case x).
This seems to be the same discussion we already have in [this other thread](https://www.reddit.com/r/rust/comments/2g6hus/weeklymeetings20140909_fnmut_sugar_assoc_types/ckh4t8i).
Thanks; saved for future reference. Are there any parts in particular you might want to recommend?
(1), (2), (4), (5) are all philosophical arguments about the merits of OO vs. FP. (3) has been addressed in the various RFCs; the biggest problem with virtual dispatch via trait objects is that they are two words, not one. Also, we already have subtyping in the language via the lifetime system.
1, 2, and 5 are reasons to avoid inheritance in your program and reasons to limit the available forms of inheritance, but not to remove it completely, as things like the DOM are impossible to implement efficiently without it (or some variation or lower-level implementation of it). 3 and 4 are invalid because virtual dispatch and code re-use can still be through trait objects (indeed, some of the RFCs do it this way).
&gt; philosophical arguments I am curious what definition of Philosophy you have in mind here? &gt; we already have subtyping in the language via the lifetime system I would have expected that the reference to Liskov makes it obvious that I was talking about behavioural subtyping which--being undecidable--opens a whole new class of problems which currently do not exist in Rust.
&gt; I am curious what definition of Philosophy you have in mind here? Arguments that object-oriented programming as a discipline is worse than functional programming. Those arguments are irrelevant in my mind when the very nature of the problem demands that you have the ability to model, for example, a tree of objects with a subtyping relationship. The DOM as an *idea* suffers from (1), (2), (4), and (5). But you have to implement it, or else you aren't a Web browser. Similar arguments apply to the kobject tree in the Linux kernel, the Qt widget hierarchy, or the Objective-C model. When the nature of the systems programming problem demands an object-oriented style, we might as well make it as nice to implement as possible. I am of course sympathetic to the idea that our solution should fit in well with the rest of the language, and even sympathetic to the idea that our solution should favor functional programming over object-oriented programming, though I think the latter may well be more of a question of the libraries.
Is [this](https://github.com/rust-lang/rust/wiki/Rust-for-CXX-programmers) acceptable?
&gt; This is the fat pointer problem I see. And profiling has shown that pairing a pointer with an int results in bad performance? So bad in fact that you are willing to trade a (hopefully) sound and decidable type-system for something fast but broken. &gt; Every DOM node has some common fields, and you have some functions that need to operate on a tree of these nodes. So you define a type-class `class Common a where common :: a -&gt; TheCommonNodeType` which allows functions that operate on common data to be constrained accordingly, e.g. foo :: Common a =&gt; a -&gt; ... foo n = let c = common n in ... Writing an instance *once* for TheSumTypeIwasTalkingAbout to extract the common component out of every branch is surely not too much to ask for.
Yes, traits (trait objects) provide the functionality, but not the performance required. "make it efficient" is not possible in the general case. One (maybe two) of the proposals are for making trait objects more efficient in specific cases. To be precise on the problem here - trait objects are implemented as fat pointers, this is an unacceptable memory overhead for the DOM since each node can store multiple pointers to other nodes (parent, siblings, children). The solution is thin pointers with the vtable pointer part of the object pointed to, but how exactly to do this is what a lot of the discussion is about. Enums are not flexible enough for two reasons: you can't ergonomically change behaviour depending on the variant (so data changes, but behaviour does not, in contrast to traits). Secondly, the implementation of enums requires they have the size of their largest variant. That is not acceptable for very heterogeneously sized 'enum's. 'base fields' is not a functional requirement. We wish to have values with the same data and to have fast (O(1), and basically two instructions) access to it. Whether that is present in the language via base fields or something else is up for debate. No bias there. I hope that fills in some of the details for why fixing existing structures is not good enough. You have to trust us when we say that we have considered that and are only adding features to Rust extremely reluctantly. To your last point, GHC has very different performance constraints to Servo, it is not really possible to compare.
&gt; Arguments that object-oriented programming as a discipline is worse than functional programming. Those terms better get a clear definition too before I would even begin a debate. &gt; The DOM as an idea suffers from (1), (2), (4), and (5). That is impossible. You are assuming that subtyping and inheritance are the only ways to model the DOM which is obviously false. In fact your arguments are mostly about performance characteristics. Anyway, given the choice between something fast but broken and something slow but sound, I would choose the latter. It seems we are on different sides on this issue. Good luck then!
By "'classes'" I mean groups of values, whether they are enum variants or instances of inheriting structs or sum types, just a general grouping of values. The problems are in the details - performance and size, mainly. Without specifying in detail how such things can be implemented we can't make an assessment. Perhaps there is a way is like this, but we have not found one. The point of this process is for people to suggest strategies like this, so we are receptive if there are good ideas (but that means working out the implementation details). Nothing is set in stone yet and we will go with what is best. I am trying to say that OO inheritance seems to be unpopular with some people right now, and there are some good reasons for that (over-abstract OO/design patterns-ey architecture being first in my mind) but there are also some powerful and useful ideas - it has not been the dominant paradigm for the last 20+ years because of luck and it would be crazy to discount those ideas where they are useful just because the paradigm as a whole is not cool any more.
&gt; The producer interface is more like: i) Get a sequence number that is certainly not being read by the consumer, ii) Convert sequence number to an index within the backing array, iii) Mutate fields by writing whatever you want into the struct. iv) Publish the sequence number with a store-store barrier so that when the consumer reads the sequence number (with a load-load barrier) it sees the newly written data. This strikes me as pretty poor encapsulation. Why do the consumer and producer know about the backing array at all? Surely you can skip all the index cruft and simply yield a ptr to the index of interest (blocking until possible, of course). I was picturing something like: /// Backing buffer, not to be used directly struct Buffer&lt;T&gt; { buffer: *mut T, capacity: uint, start: uint, length: atomic::AtomicUint, } /// A consumer's interface into a Buffer pub struct Consumer&lt;T&gt; { buffer: *mut Buffer&lt;T&gt;, } /// A producer's interface into a Buffer pub struct Producer&lt;T&gt; { buffer: *mut Buffer&lt;T&gt;, } /// Handle to a valid consumable location in the buffer pub struct ConsumerHandle&lt;'a, T&gt; { consumer: &amp;'a mut Consumer&lt;T&gt;, } /// Handle to a valid producable location in the buffer pub struct ProducerHandle&lt;'a, T&gt; { producer: &amp;'a mut Producer&lt;T&gt;, } /// allocates the buffer, and builds the consumer and producer /// For simplicity, assume that it allocates the indices with Default. fn make_SPSCRing&lt;T: Default&gt;(capacity: uint) -&gt; (Consumer&lt;T&gt;, Producer&lt;T&gt;) { // ... } impl&lt;T&gt; Buffer&lt;T&gt; { /// Get a reference to the first element in the buffer /// unsafe because the length of the buffer might be 0 /// or the memory might be uninitialized. Also offset is unsafe unsafe fn first(&amp;self) -&gt; *T { &amp;*ptr.offset(start as int) } /// Get a mutable reference to element *after* the last /// one in the buffer. Unsafe because this might be the first /// element if len == capacity. Also offset is unsafe unsafe fn last(&amp;mut self) -&gt; *mut T { ptr.offset(((start + length) % capacity) as int) } } impl&lt;T&gt; Consumer&lt;T&gt; { fn next&lt;'a&gt;(&amp;'a mut self) -&gt; ConsumerHandle&lt;'a, T&gt; { // first block until buffer.length &gt; 0, somehow ConsumerHandle{ consumer: self } } } impl&lt;T&gt; Producer&lt;T&gt; { fn next&lt;'a&gt;(&amp;'a mut self) -&gt; ProducerHandle&lt;'a, T&gt; { // first block until buffer.length &lt; buffer.capacity, somehow ProducerHandle{ producer: self } } } impl&lt;T&gt; Deref&lt;T&gt; for ConsumerHandle&lt;T&gt; { // totally safe based on how we get a handle, modulo // concerns discussed in producer's deref_mut fn deref(&amp;self) -&gt; &amp;T { unsafe { &amp;*(*self.consumer.buffer).first() } } } impl&lt;T&gt; Deref&lt;T&gt; for ProducerHandle&lt;T&gt; { // See deref_mut for details fn deref(&amp;self) -&gt; &amp;T { unsafe { &amp;*(*self.producer.buffer).last() } } } impl&lt;T&gt; DerefMut&lt;T&gt; for ProducerHandle&lt;T&gt; { // This is actually unsafe if you allow the buffer to have // uninitialized or garbage data at any point. I'm assuming you don't // for simplicity. Otherwise, this should yield the raw ptr, or expose // a more complex interface for manipulating the ptr fn deref_mut(&amp;mut self) -&gt; &amp;mut T { unsafe { &amp;mut *(*self.producer.buffer).last() } } } impl&lt;T&gt; Drop for ProducerHandle&lt;T&gt; { #[unsafe_destructor] fn drop(&amp;mut self) { let buffer = &amp;mut *self.producer.buffer; // atomic flags stolen from Arc buffer.length.fetch_add(1, atomic::Relaxed); } } impl&lt;T&gt; Drop for ConsumerHandle&lt;T&gt; { #[unsafe_destructor] fn drop(&amp;mut self) { let buffer = &amp;mut *self.producer.buffer; let cap = buffer.capacity; buffer.start += 1; buffer.start %= cap; buffer.length.fetch_sub(1, atomic::Release); } } Which is used like: // ... Some common area let (prod, cons) = make_SPSCRing::&lt;uint&gt;(1024); // send these wherever you want... // ... Somewhere in a producer's code { let next = prod.next(); // blocks until we can get the next value *next += 1; } // next goes out of scope, and only then does the buffer's length increase, // guaranteeing that consumer can only access the updated value when we're done // with the value. Any dereference of next also can't "escape" next, because // lifetime is tied to it. // ... Somewhere in a consumer's code { let next = cons.next(); do_stuff(&amp;*next); // can't call cons.next() again until we drop `next` // so we can only "have" one reference at a time, even if // the buffer totally has multiple "ready" elements. // Otherwise, we'd need a substantially more elaborate design } // next goes out of scope, buffer's length decreases, // and the consumtion head shifts forward. Also, the reference // can't escape as discussed above This is of course just a sketch of the overall design, there's tons of details omitted, and probably a pile of syntax/type errors and missing Sends or something. Edit: One obvious massive omission is that the producer/consumer need to collaborate to free the the buffer in their destructors. Maybe an Rc or something. Not the interesting part of the design IMO.
Yes, I thought the tone was spot on. Formal in the right places but then with a conversational element every so often to keep you interested. I wasn't intending to read the whole thing but I did for some reason. 
Could you get around that by storing a list of vectors of different types? You'd also gain efficiency in being able to store unboxed vectors and remove *two* indirections. It should be noted that this is how game developers do it. Storing a vector of pointers to a list of children and iterating over that is much slower than storing multiple unboxed vectors each pointing to a single type. Many game developers take this even further, and may decompose a vector representing instances of a type into multiple vectors of primitive types. This optimization is how unboxed vectors in Haskell are automatically optimized for tuples. A `Vector (a,b)` becomes a `(Vector a, Vector b)`. This leads to the greatest performance and efficiency in allocation. Right now, if you have a vector of a supertype, you can't store an unboxed child without violating the open world assumption. It also seems like it would make compiler code for dealing with unboxed values in general much more complicated.
I think that debate over `enum Gender { Male, Female }` was unnecessary. It's not exclusionary if you clarify that gender is whatever the person identifies him/herself as. I think that's pretty strongly implied to begin with. If you try to include all the possibilities you just overcomplicate things. As a cis-man--I think that's the right term--I may not have the most unique perspective on the issue at hand, but I think a petty debate over a single line of code would turn away more people than the few who might feel excluded by a simple omission. I know it made me reevaluate my interest in the project. 
True, but you also lose the ability to store unboxed vectors of a type that might have subtypes without violating the open world assumption. That is, you can create an unboxed `Vec A` if and only if you can statically determine the maximum size of all children that subtype `A`. That is a pretty bad deal for Rust, because boxed vectors impose substantial costs in memory fragmentation.
It seems this cost also has an extremely large price: it will no longer be possible to implement unboxed vectors of a type for which a subtype may exist that alters size. That's a huge cost, and it seems like the Servo team may be pushing for features in Rust and pursuing a local maxima, unstead of a global maximum for performance. Unboxed vectors are key to efficient sequential operation. It's a well known optimization to improve data locality and efficiency with working with arrays of children by decomposing a single vector into multiple vectors, each of a more primitive data type. That means turning a vector of `Foo` and `Bar` (each a subtype of `Qux`) into a vector each of unboxed `Foo`s and unboxed `Bar`s. And it can go further. If `Foo` stores an integer and a string, *that* vector may be decomposed into two vectors, one of of integers and one of strings.
Most likely we'll have closed inheritance, so this sort of optimization will still be possible. But I doubt this sort of thing is the kind of thing a compiler could do automatically anyway.
&gt; in which case every variant would be the same size, namely that of a single pointer. It's actually a pointer + the tag byte (and any alignment padding). Furthermore, adding in a lot of pointers can lead to slow performance and increased memory use, due to poorer cache behaviour and malloc "slop"/metadata.
&gt; Anyway, given the choice between something fast but broken and something slow but sound, I would choose the latter Almost everyone just wants a good web-browser; they don't care how type-theoretically sound the implementation details are, only the external characteristics like being fast and not using a lot of memory.
&gt; I think that making the example more complicated for the sake of being politically correct is terrible This would only be valid if the only other possibilities were actually more complicated, which is definitely not the case, e.g. the suggestion of enum Height { Over6Ft, LessThanOrEqualTo6Ft } struct Person { // ... height: Height } seems reasonable. As does just removing the custom enum and having something like struct Person { // ... likes_chocolate: bool } or struct Person { // ... age: uint }
Lots of people actually don't recognize that, which makes it even more important.
what about people under 4'8" who would prefer to be called dwarfs, or over 7' who would prefer to be called giants? Its so disrespectful to those people! Or the people who hate chocolate with such a passion that if someone mentions it they must automatically go take a nap and relax? I'm not trying to be a dick, I'm saying that you will never please everyone, can we please just drop the BS and write good code instead?
There, made the `Height` example totally objective. Gender/sex is a sensitive topic (and is nearly unique in this regard), especially in the currently unbalanced field of programming, and it's rather easy to just choose something that won't be exclusive. I strongly encourage you to stop digging this hole for yourself.
Wow. I'm just glad that Rust is so good that eventually there'll be multiple community discussion locations and not all of them will be infested by SJWs. Every time this comes up (by 'this' I mean a completely benign and well meaning Rust users *attacked* for not being PC to the max) I have to give a bit of thought as to why I even contribute to this community. But since me and DroidLogician are the 'default', excluding us doesn't matter.
I know, I'm deeply involved with many people of 'non-regular' gender, and all of them don't give a care about male/female because its the world we live in. It's always the 'regular genders' that make a big stink about it. OK I will accept the rest of the downvotes now and continue writing code instead of bickering about things that don't have to do with writing code. Sorry for stirring the shit.
There's no attack here. I don't think anyone in this thread is a bad person. Well, your usage of 'SJW' is a bit suspect, but I don't think the author of this PR is a bad person for using a bad example, nor my parent for not understanding that this matters to people. My comment is no different than any comment on the guide, suggesting that an example is substandard. &gt; But since me and DroidLogician are the 'default', excluding us doesn't matter. Sorry, why are you 'excluded' by using a non-gender-based example?
As a programmer you should pay attention to macros. Take the following code snippets: let x = bar; foo(x); and let x = bar; foo!(x); Can `x` be something other than `bar`, given that it is not declared `mut`? It can in the second version, but not in the first. The `!` reminds you that macros have a very different set of invariants compared with functions.
Smart pointers are no different from references: `Box&lt;Fat&lt;Trait&gt;&gt;`, `Rc&lt;Fat&lt;Trait&gt;&gt;`. It's basically RFC PR #9- arbitrary DSTs can move their size information behind the pointer. I'm not sure I understand you about upcasting though- how are the mechanics of upcasting a `Fat&lt;Trait&gt;` any different from upcasting a `#[repr(inline_vtable)]` Trait?
The DOM semantically needs to represent a heterogeneous list of child nodes (e.g. text node, image, image, div, text node). That would be at least as complicated to represent with separate vectors as the currently used hacks, although it is a nice optimization for several other situations.
Yes, doubling the size of every pointer in a data structure and then doing a virtual call or pattern match to access all common data results in bad performance vs single pointers with constant offsets. Your `Common` typeclass is equivalent to the Node base class we've been talking about the whole time, with its `common :: a -&gt; TheCommonNodeType` equivalent to either a virtual accessor method or a method with one big giant match on all the variants, both of which are the exact problems the inheritance proposals were created to solve.
Rust already has unboxed vectors and none of the inheritance proposals change that. It's a systems language- you have to opt in to unboxed anything by using a pointer of some kind.
My resistance to inheritance is that most of the proposals feel very tacked-on. They add whole new features that do almost the same thing as traits, or they make existing features a lot more complicated to reason about. I just wish there were more focus on the ideas for making trait objects more efficient, since they're a lot more orthogonal and flexible, and they strongly avoid traditional OO concepts- fat objects, traits to specify common fields, etc.
Looks cool. Though I have to admit that after reading the title I though this was a library for [functional zippers](https://en.m.wikipedia.org/wiki/Zipper_(data_structure).
It is something that Haskell is able to do with its unboxed vectors, but is probably beyond the scope of the Rust compiler right now.
I would be interested in seeing an example where moving from a non-mutable variable actually causes a problem because I don't think that it does cause a problem at all and I would hate to have to put `mut` in front of every binding just to be able to move it somwhere else. In fact, I really like how things are right now and I consider it an improvement over move semantics in C++ which kind of requires mutability because in the C++ world moves are not destructive. But if a move is destructive (and it is in Rust), I don't see the harm in allowing moving from non-mut bindings. It's just more convenient.
&gt; Perhaps the post refers to postfix !, but the only language that I know of that does that is Ruby It's also used for [templates in D](http://dlang.org/template), which is an argument both for and against keeping it for macros, since it's a similar operation, but not exactly the same (Rust already has generics with different syntax). In this case, it also doesn't denote any kind of error. I would prefer keeping the current syntax because it feels similar to D's compile-time templates and Rust's macros are also expanded at compile-time. &gt; I find the motivation that attributes are often macros a bit dubious I do too. @ for attributes is a *very* common syntax, @ for macros is non-existant as far as I know.
You probably want /r/playrust subreddit.
Does Conrod auto-handle state for you? I'm thinking of something like a toggle button where I don't really want to have to keep that state around myself. By the way, congrats on your progress so far, I'm really looking forward to making stuff with this!
No sorry, I can understand why you might want it, but the general design aim has been to try and encourage generating the GUI's state out of the app data wherever possible. Here's a [toggle example](https://github.com/PistonDevelopers/conrod/blob/master/examples/all_widgets/all_widgets.rs#L237) where you pass in the initial value and it provides the new value through the callback. I think it'd be pretty trivial to add the option of self-contained state - perhaps it could be added as an `.auto_state()`method, or even just as a separate widget? EDIT: come to think of it `.auto_state()` wouldn't work, but perhaps an enum would: `uic.toggle(uiid, Value(b))` or `uic.toggle(uiid.toggle, Auto)` Or even removing the value from the signature, so that it can just be passed as an optional builder method like everything else? `uic.toggle(uiid).value(b)...` *finishes throwing around ideas*
Interesting. When I was playing around with immediate-mode gui in Rust my main use case for auto-state storing was for things like slider bars where you want to store the mouse position so that while the internal code might be cluttered, your user isn't burdened with storing 4 or 5 mouse-related details.
Thanks! Giving simple, illustrated, explanations of the concepts is the most important thing on a user guide IMO. I hope that if something is not clear people will let me know so that it can improved.
I've been porting over a huge personal project from C++ over the last couple months - I'm finally just about finished yet somehow my Rust is about 30% of the size that I had before. NOTE: this probably has just as much to do with my development as a programmer as it does Rust. Also Rust's meta programming has helped a great deal (I wasn't doing any in C++). Not having to write header files has definitely been a huge plus.
Good question. Rust is definitely not as succinct as functional languages like Haskell or Ocaml, but from what I hear, it is quite decent when compared to C++. The lack of header files, class boiler plate, and the expression-heavy programming style with pattern matching can make things much more succinct. Do note however that for a number of reasons Rust does suffer more from rightward-drift.
What metaprogramming techniques did you use? How much?
I think that it was a mistake to resolve macros during parsing, instead of after type checking. You can automate a lot more patterns if you have type information available to macro system. Thus reducing LOC and repetition. Still massive improvement over c/c++, whose macro system isn't even turing-complete.
Yeah... you are right. My mistake. I am still right about macro system not having access to types being a bad idea.
I'm porting some old C (with minimal ++) project to Rust and for now Rust code is about 80% of the previous size. But I'm not sure it's representative since my Rust code is not very idiomatic yet (no iterator chaining and similar functional stuff) and have to interact with C code and keep binary compatibility. So, instead of headers I have FFI cruft, interoperation with C strings and unsafe stuff, which is really noisy in Rust. Amusingly, mandatory curly braces in Rust have noticeable impact on the line count. Also, lines are often longer in Rust because of various as_slice's and similar things. But in the end, when all the FFI will be dropped, the code will be significantly smaller. In general, my impression is that there wouldn't be a big difference between idiomatic modern C++ code and Rust code in terms of number of lines. Except for headers, but C++ modules are on their way already.
 .callback(|| /* do stuff */) What's this `||` operator (that looks like a logical OR operator)?
It's a closure with no arguments. The arguments would normally go between those pipes. [More here](http://doc.rust-lang.org/rust.html#closure-types).
I don't see an attack. I see Steve thoughtfully explaining his position. I also don't see any exclusion here - I see absolutely no way in which changing an example from 'Gender' to, say, 'BinaryNumber' harms anybody, and it is apparently of benefit to someone, so why not take this _zero-cost_ opportunity to help someone?
Then you misrepresented `Fat` as being a library-only abstraction, requiring zero compiler support. That is how my `Vtable` and `HasVtable` work, the rest building upon existing, upcoming or needed-anyways features. [Btw, I managed to put together a thin pointer version of the dom.rs](https://gist.github.com/eddyb/69e4d2c14c3610f14ea3)
On the other hand, `macro_rules` is very experimental with a few annoying bugs and definitely scope for change (possibly arbitrarily large). Despite this, macros rule.
I actually thought about that and indeed with views you could get two methods relatively easily: fn split_mut(&amp;mut self, index: Index) -&gt; (View&lt;&amp;mut E&gt;, View&lt;&amp;mut E&gt;); fn pick_mut(&amp;mut self, index: Index) -&gt; (View&lt;&amp;mut E&gt;, &amp;mut E, View&lt;&amp;mut E&gt;); And starting from there getting two (or more) borrows is easy as long as the views also implement those methods. I think it could be done generically over any type implement `Index`: a simple proxy over the container with the "allowed" range (may get tricky for maps, because the range only make sense if you know how to compare); however it would probably be more elegant if done with associated types so that the views propose the same set of methods as the original type.
**Edit** Ugh, sorry for the link to the fork page. Here's a [direct link](https://github.com/shepmaster/sxd-rust). Hey all, I'd really appreciate any feedback. This is my first Big Thing in Rust, but I've been working on this project off-and-on for *4 years* (that's a lot longer than I realized...). It originally started as a C library until I realized I was recreating C++, so I switched to C++. Then a few months ago I dumped all the C++ into a Rust file and compiled until the tests were green. I'm certain there's lots of things I've done wrong or could do better, so I look forward to any insights you are willing to share!
A big +1
It only needs compiler support for core::raw::TraitObject, which is already there, and then it can be in a library: https://gist.github.com/krdln/08764f70a1e5aeda2338 (not my gist, but linked elsewhere in this thread). Even with compiler support I prefer it for its flexibility.
If you are interested in Ai behavior trees, you should read the blog post in this issue https://github.com/PistonDevelopers/rust-event/issues/73 In Rust-Event we are going a step further by allowing parallel processing of events. Yet we hope this is going to be common way of programming game logic! This is fresh design and we appreciate all feedback you can give us.
Me too.
Yes, `as_slice()` will be `[]` soonish. RFC was merged, don't think it's been implemented.
Sorry I feel like we are going out of the scope of the original question which pertains to implementation and not design decisions. The consumer does not need access to the backing array - hence the onItem(T* item) interface. The producer does not need to know about the backing array either. It uses the get() call to get a T* item as well. The producer can wait till a slot is available, or alternatively use an API like tryNext() to figure out if a slot is available and drop data/buffer if not. Dunno why you thought either of them need to know about the backing array. They also need different cursors (to prevent sharing) on different cache lines (to prevent false sharing) that are monotonically increasing (to be able to use cheaper barriers). To see why ring buffers are designed this way please take a look at the java project Disruptor if you haven't already - https://github.com/LMAX-Exchange/disruptor. If you look at the example usage I linked, it's very similar to what you came up with. The reason the more low level calls on both the consumer and the producer side need to be kept around is to satisfy the use cases where the producer or consumer need to be in control of their run loops. For e.g. in a server, the producer thread could be accepting connections, maintaining timers, and putting complete requests on a ring buffer (to be processed by worker threads). In such cases a callback based interface doesn't work. Further it could want more than one slot at a time. Imagine again the server use case. If you use a flyweight wire protocol like Cap'n'Proto you might want to put (validation + plain ole memcpy) variable sized objects onto the ring buffer and in that case T is really a byte and the objects themselves can all be different. The producer can fill a buffer (even one from the ring buffer by requesting N slots), read the data into it (or memcpy it if the buffer wasn't from the ring buffer), validate it and then publish a sequence. 
[this](http://imgur.com/zWuYT6J)
Wonder if something like that will ever be available?
I know that macros exists in Rust, although I also know that they are somewhat crippled by not having type information.
It's [RFC 58](https://github.com/rust-lang/rfcs/blob/master/active/0058-slice-notation.md), if anyone wants to know the details.
&gt; The DOM as an *idea* suffers from (1), (2), (4), and (5). But you have to implement it, or else you aren't a Web browser. This is makes me angry to read because it's so true. There certainly is a right way to build something functionally identical to a web browser without using inheritance, but the problem is you have an existing "solution" forced upon you by having a standard DOM that you *must* conform to to be relevant in the world. You don't get to design your own solid development framework for use in your browser -- you have a load of crap forced upon you by the same brilliant engineers who thought it was a good idea to make a standard &lt;blink&gt; tag. Honestly, it just sucks that everyone has to suffer for bad decisions of yesteryear's engineers, in this and a thousand other cases. The best I can hope for is that Rust comes up with a novel way to handle inheritance to make it conducive to good design in the same way it came up with a novel way to handle pointers and enforce their correct usage. Lemonade from lemons, so to speak.
Not an.answer to the question per se, but perhaps LOC isn't a good metric. Rust offers a measure of safety and predictability that makes it, in principal, easier to maintain or compose without undesired explosions. So even if it's same LOC (other answers suggest it's not), it's probably still faster to develop in; fewer gotchas to worry about.
It depends. Here’re aggregated statistics from my in-progress projects (an LALR parser generator, an LLVM wrapper that simplifies existing LLVM libraries, and a compiler for an esoteric programming language). I’d describe myself as an advanced C++ programmer and I’ve been programming Rust for 18 months. While I’ve worked on programming complexity research, I’m writing from a humanistic, rather than analytic or empiricist perspective (i.e. I won’t compare denotational or operation semantics and formal grammars). **Statistics** * Lines of code: C++ 100%; Rust 106% * Column width: C++ 100%; Rust 118% **Summary** * C++11 and Rust are semantically incomparable. * C++11 and Rust are syntactically similar (the difference between the two is subtle). I suspect a distribution of an infinite number of identical C++11 and Rust programs would show that they’re equivalent from a complexity perspective. **Comments** Rust’s semantic complexity is a feature. Discounting implementation and portability issues (i.e. issues that I expect to be resolved by 1.0.0), most of Rust’s complexity is related to memory and type safety rather than C++’s complexity that’s related to generic programming (e.g. templates) and undefined behavior. Rust’s syntactic complexity is painful. I don’t enjoy reading the language. And I especially don’t enjoy writing the language. Most of Rust’s constructs are denotationally and operationally gorgeous, but syntacticly unpalatable. It’s a shame. If Rust was syntactically bolder, I’d use it for everything. I’d even consider using it in papers rather over the standard languages in my fields (C++, Haskell, and OCaml). I daydream about writing a compiler-compiler that adds significant syntactic sugar to the language, but I’d dread rocking the boat. Especially when I understand and appreciate the trade-offs that were made. Nonetheless, I’m disappointed.
&gt; Yes, doubling the size of every pointer in a data structure and then doing a [..] pattern match to access all common data results in bad performance vs single pointers with constant offsets. No argument about the *relative* overhead of a "fat pointer" vs. a "thin pointer" (regarding the pattern match see below) but rather if it matters in practice, i.e. are there any empirical results to back this up? &gt; [...] equivalent to [..] a method with one big giant match on all the variants Yes, it was meant to illustrate the "helper method" you alluded to. Related to the previous point I would like to know why you think that pattern matching would be slow? Surely it is a constant operation (w.r.t. the number of constructors). Is this due to LLVM? Did you measure this? If pattern matching is not performant, who do you expect to use it then? In short--I would like to see some benchmarks (incl. sources) which back up your claims. 
Rust macros are less powerful (that is, have more restrictions) than C macros, by design. This is why they're more usable: it's harder to get horrible behaviour from them. E.g. hygiene means its harder to accidentally change arbitrary local variables. 
&gt; If it is polymorphic reuse you object to, then do you have a problem with trait objects too? Not quite sure which kind of "polymorphic reuse" you have in mind as there are many forms within Rust which I have no issue with. Regarding trait objects in particular I would prefer to see proper existential type support. Not only are trait objects artificially limited by their direct correspondence to a particular trait but the implementation is also slow due to virtual method invocation. Proper existential types do not suffer from these limitations. I would suggest you read chapter 21.1 of Robert Harpers [Practical Foundations for Programming Languages](https://www.cs.cmu.edu/~rwh/plbook/book.pdf) and study GHC's implementation.
&gt; Almost everyone just wants a good web-browser; they don't care how type-theoretically sound the implementation details are, only the external characteristics like being fast and not using a lot of memory. It seems you are in this game for the wrong reasons. You would be better off to write it in C++ and make it as fast as you like.
If you want type info, you can work with syntax extensions instead (though you don't get types directly -- you have to force an inference)
This behaviour is perfectly possible with macro rules; just call trait methods/trait-bounded functions (this is what syntax extensions which appear to be typed do).
[One down](https://github.com/rust-lang/rust/pull/17259), `Span` to go, and countless tests abusing `Gc&lt;T&gt;` i.e. `@T` since time begun.
I'm not aware of any Servo-specific benchmarks, but maybe someone on the team could chime in. This is the entire point of any of these proposals, so I'd assume they've run into problems. In general, doubling the size of your pointers destroys the cache and significantly raises memory usage, which absolutely is a problem that a systems language needs to solve, even if it doesn't apply to this exact situation. The naive generated code for pattern matching here would be to switch on the tag and then access the fields. That switch may or may not be constant depending on whether a jump table or if/else chain is faster, both of which are much slower than simply accessing a constant offset. As an optimization, the switch on the tag can be eliminated only if the undefined struct layout of Rust happens to place the common fields at the same offset in every variant, so at the very least we need a way to specify enum variant layout.
Rust is targeting C++'s niche. If it can't compete with C++, it's not fulfilling its purpose for existence.
How do you plan to implement these proper existential types without virtual methods in a systems language?
Windows uses the PE executable format (.exe), OS X uses the Mach binary format, and Linux uses the ELF binary format. Mach allows you to build binaries that contain support for multiple architectures, so you only need one OS X binary (you might need to do something special to actually create such a binary, though), but for Windows and Linux you have to distribute both 32bit and 64bit versions. You'll have to compile each of these. I don't know how well rustc handles cross compiling, so I'm not sure if you'll need to run rustc on each platform, or if you can just spit out binaries for all platforms from a single installation.
I haven't tried it with Rust myself, but the usual problem with cross-compiling binaries is not the actual compiling, but getting the external libraries your binary depends on and figuring out how to ship them in a way that the resulting binary can find them. It tends to be a different solution for each platform, but at least you only have to figure it out once. I recommend: * build 32-bit x86 binaries, not 64-bit. 64-bit Windows and Mac OS run 32-bit binaries just work. * It's usually a better use of your time to let Linux users compile from source than to build a binary for every distro. * build natively on each platform * look at the Rust compiler nightlies for an example of how to package a Rust-built command-line tool. 
I agree with all this except that I think 64-bit binaries are a good default for Mac OS X. All Macs manufactured in the past 8 years have 64-bit processors, and current versions of OS X don't even run on old 32-bit-only Macs.
Do windows rust binaries still link to the MSYS c runtime libraries? I haven't looked at it in a while, but I thought that this was the case; and it would certainly be something of hindrance to distributing windows binaries. edit: on the other hand, I guess you wouldn't really be using a command line utility on windows without msys...
That's nonsense. C++ has a wide variety of memory safety issues, and thus, IMO, should be regarded as unacceptable as a tool for modern software that requires security (like a web-browser). There's no particular reason that Rust can't be equally appropriate for these sort of domains (and, it already is, for many), except there are some memory-safe behaviours that are currently very hard to encode without `unsafe`.
Why is it important to clean up the tests? Does it slow them down? 
If we're gonna remove `Gc&lt;T&gt;` from the language, the tests can't use them.
Executables don't depend on MSYS in any way, insofar as I can see. I believe it should be possible to introduce such a dependency, but why you'd want to do so, I can't imagine. As for your edit, the only time I use MSYS is when some inconsiderate person makes their build process depend on make or sh.^1 :) --- 1: Well, that and when I genuinely need a proper POSIX shell and it's not worth doing the automation in Python.
That's really not that greatt. I appreciate the extra safety afforded by Rust, but would have thought you'd be able to reduce the LOC count much further.
WTF with all the deleted posts? Can't get any context.