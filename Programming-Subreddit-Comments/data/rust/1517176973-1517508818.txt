The code was already written, so that wouldn't help. What's so difficult about that pattern?
&gt; Sancta trinitas linguae pucciniae legit Sure. What precisely are you trying to say?
You can probably reuse part of [TiKV](https://github.com/pingcap/tikv).
Enough that it is one of the reasons why Relm was created on top of Gtk-rs. http://relm.ml/relm-intro And is part of Niko's notes regarding issues not solved by NLL improvements. https://github.com/nikomatsakis/nll-rfc/blob/master/0000-nonlexical-lifetimes.md Basically the way borrow checker works, it is not ergonomic to access Widget data from struct fields inside event handlers, even if the event handler is the only one using the said field.
I'm still working on renaming the crate - I have one in mind, and I should be ready to transition it after 0.5.0 is out the door.
I‚Äôve worked for two small Sats companies and you‚Äôd be amazed how much python is space. In both cases the companies were interested in rust for projects which were in c but never felt like they were staffed for it or could well articulate the advantage. Neither had stability issues with the c code in space anyway. Most of the bugs were in the python 
I'd probably change "legit" to "est", but otherwise looks fine to me. Another possibility is "Tres sanctae principia linguae Pucciniae est:" which is "The three sacred principles/elements/foundations of the Rust language are:"
&gt; ü§îü¶Ä Killed me
In my crate `rug` I did something similar for multiply-and-add for bignum integers and floats, where I wanted to avoid cloning. My solution was kind of similar to your first option, except multiplying two references does not create a number, but creates a kind of reference, which can be added to the number using a custom function. It is as efficient as the fourth method while still being as easy to use as the first. Something like this: struct I(i32); mod hide { use super::I; use std::ops::{AddAssign, Mul}; pub struct MulRef&lt;'a&gt; { lhs: &amp;'a I, rhs: &amp;'a I, } impl&lt;'a&gt; Mul for &amp;'a I { type Output = MulRef&lt;'a&gt;; fn mul(self, rhs: Self) -&gt; MulRef&lt;'a&gt; { MulRef { lhs: self, rhs } } } impl&lt;'a&gt; AddAssign&lt;MulRef&lt;'a&gt;&gt; for I { fn add_assign(&amp;mut self, mul: MulRef&lt;'a&gt;) { // the actual work is done here self.0 += mul.lhs.0 * mul.rhs.0; } } } fn main() { let mut accum = I(10); let a = I(2); let b = I(3); accum += &amp;a * &amp;b; println!("accum {} (a still {}, b still {})", accum.0, a.0, b.0); } 
Alright, you're the boss! I went with the more explicit "Tres sanctae principia". (but: huh? singular form here for "the three _"?)
Oops. You caught me in a booboo. I modified the first nounal phrase, but then didn't make sure everything else agreed with number. Let me fix.
I really dug this talk - especially Nicolas' notion of "error modules" as another kind of architecture boundary. I'm a new rust programmer, who has read [the book]( https://doc.rust-lang.org/stable/book/second-edition/) and has started on [the other book](https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283/ref=sr_1_1?ie=UTF8&amp;qid=1517179911&amp;sr=8-1&amp;keywords=rust+programming) I definitely got something out of it. Recommended viewing for others in a boat the same make as mine.
(a) doesn't Rust abort on OOM (b) does `println!()` unwrap or just drop the result? If the former, use `writeln!()` and match on it; if the latter, no problems
It's all good, and now also matching genders and plurality :) I'm glad English doesn't ~~have~~ expose these nuances, even though I actually really like how it helps make sentences less ambiguous. (But my first language is German so I'm clearly biased.)
No it shouldn't. do_a() is allowed to call terminate or abort; those aren't panics.
&gt; The Roman agricultural festival Robigalia (April 25) has ancient origins in combating wheat rust. [[citation]](https://en.wikipedia.org/wiki/Rust_(fungus)) 
Love too have my application forbidden from use in closed environments because `log` crate wasn't good enough
**If I ever get a tattoo, it will be the Rust logo!**
Actual citation &gt; Evans, R. (2007). Utopia Antiqua: Readings of the Golden Age and Decline at Rome. Taylor &amp; Francis. ISBN 978-1-134-48787-5. Retrieved 2018-01-12. ;)
I imagine it was fairly natural if Latin was your primary language, but yeah, as much as I appreciate the logic and clarity of heavily declined and conjugated languages, it's clear it is not the ideal for communication as languages have consistently evolved towards less conjugation and declension.
I haven't looked at the crate at all, so it may be a stupid question, but I wonder what are the `u8s(xx)` and `f64s(x.x)` for. It makes the function in `simd_map` look weird imho. Is it necessary or is it something chosen to make things more explicit ? 
Ok so this was unexpected, the compilation process took 138 minutes with brew üç∫ /usr/local/Cellar/webkitgtk@2.4.11/2.4.11: 305 files, 31.5MB, built in 138 minutes 2 seconds however I still get the same error.. error: failed to run custom build command for `webkit2gtk-sys v0.5.0 (https://github.com/gtk-rs/webkit2gtk-rs#6bb613f3)` process didn't exit successfully: `/Users/matias/workspaces/rust/webkit2gtk-rs/target/debug/build/webkit2gtk-sys-0f97467abb5734eb/build-script-build` (exit code: 1) --- stderr `"pkg-config" "--libs" "--cflags" "webkit2gtk-4.0 &gt;= 2.4"` did not exit successfully: exit code: 1 --- stderr Package webkit2gtk-4.0 was not found in the pkg-config search path. Perhaps you should add the directory containing `webkit2gtk-4.0.pc' to the PKG_CONFIG_PATH environment variable No package 'webkit2gtk-4.0' found Couldn't find any solutions so far.. 
Note that if you want to use the inner values then the closure passed to `map()` needs to be marked `move`.
Don't closures automatically move variables if it's the most sensible thing to do? I was under the impression `move ||` is only needed when it can't infer it. The code example posted works for me as is: https://play.rust-lang.org/?gist=d245498d924e9ed5cc84a072ffcff784&amp;version=stable 
You can host sentry yourself in an isolated environment.
Regarding UPX, while I try to [convince them](https://github.com/upx/upx/issues/177) that their current website is too scarce on the details, you might want to look at the information on [this archive](https://web.archive.org/web/20160823082004/http://upx.sourceforge.net/) of their old SourceForge site's landing page. (Just remember to click the √ó on the Wayback Machine header to fix the header alignments.) It's mature software (initial release in 1998) so, aside from the News section not acknowledging patch releases made since then, the archived landing page is still basically identical to what it'd be today.
interesting, yeah! I think the difference here might be that `println!()` borrows things, so it doesn't *have* to be a move? I mean with `x.and_then(|x| y.map(|y| (x, y)))`, there's no way in the type system that `(x, y)` represents references. It's a direct move of the variable `x`, so not moving would be an error in all cases?
Making high-quality and powerful SIMD tools in order to encode things into emoji more quickly? People say managing programmers is like herding cats, but few cats are this awesome. ([okay, maybe some of them...](https://imgur.com/vGOR0LN)]
How can I use this insert_into(tags) .values(( label.eq("foo"), )) for multiple fields?
That's awesome stuff! I'll be trying to incorporate the PR into bytecount, but I've gotten negative reactions from my main user (which I completely trust on this matter). Either we can find a legal solution or we'll change the PR to make faster a feature like simd-accel and avx-accel.
Yeah, I have a lot of respect for BurntSushi and I understand his hesitation. It's unfortunate that we disagree on licensing, but I'm definitely open to making it work and not forcing the MPL upon anybody.
I've been looking for a rust-native version of this also, but there isn't something else out there that is basically rust only. There are two options that I have seen commonly in use: * [Sqlite](https://github.com/jgallagher/rusqlite): This is already a fully blown relational DB, but could be used as a guide. * [RocksDB](https://github.com/spacejam/rust-rocksdb): This is a k/v store that has good rust bindings and may be low-level enough for you.
That's great to hear. Note that you'll need permission from all of your co-aurhors to change licenses (I've been in more than one round of "I agree..." issues so far).
I'm extending various crates to allow using webgl on the native webassembly target (without emscripten) Demo compiled without emscripten: http://brave-bell-e62dc4.bitballoon.com/
When will cgmath adopt faster?
At least look kind of clear (Despite I don't know much of rust)
Just pass a tuple of multiple fields. This is covered in the insert guide on the website. 
BufReader is an implementation of the Read trait, but can't be extended in the OOM sense. You can implement the Read trait yourself though, which internally uses a BufReader. From what I understand of fastq though, you probably don't want to use the Read trait, which is basically used for filling up bytes in a buffer, but instead you want to provide a sequence or a list of records. For this, there is the Iterator trait, of which you need to implement one method: `next`. [Here](https://doc.rust-lang.org/std/iter/index.html#implementing-iterator) is an example of how you would implement this. I would have two structs: * FastQRecord: an individual record * FastQDecoder that implements `Iterator&lt;FastQRecord&gt;` The FastQDecoder is responsible for providing the FastQRecords. Internally this could be via a BufReader field. (the better, more future thinking way, would be to use generics, and have FastQDecoder accept anything that implements the `Read` trait) 
Safe i32 abs. The docs say that calling `n.abs()` is dangerous on `i32`, because if the value is equal to `MIN` then it doesn't fit in the results `i32`. So you have wrapped and checked abs to solve these issues. I want the value as a `u32`. So it'll fit. Am I right in thinking this is fine? Is this in the standard library anywhere? Or do I need to write my own `abs` code?
Stable has had RLS for at least a month.
I am working on http and websocket client for [actix web framework](https://github.com/actix/actix-web)
Ok! Thanks! So something like this? I tested it enough so that it actually builds with `cargo build`. I have a dummy method `Seq::blank()` to help make this example shorter. use std::io::BufReader; pub struct Seq { id: String, desc: String, seq: String, qual: String, } impl Seq { /// Make a new sequence object fn new (id: &amp;String, desc: &amp;String, seq: &amp;String, qual: &amp;String) -&gt; Seq{ return Seq{ id: id.clone(), desc: desc.clone(), seq: seq.clone(), qual: qual.clone(), }; } /// Make a blank sequence object. fn blank () -&gt; Seq{ return Seq{ id: String::new(), desc: String::new(), seq: String::new(), qual: String::new(), }; } } pub struct FastqReader&lt;R&gt; { reader: BufReader&lt;R&gt;, } impl&lt;R&gt; FastqReader&lt;R&gt;{ fn new(reader: BufReader&lt;R&gt;) -&gt; FastqReader&lt;R&gt; { return FastqReader{ reader: reader, } } } impl Iterator for FastqReader&lt;Seq&gt; { type Item = Seq; fn next(&amp;mut self) -&gt; Option&lt;Seq&gt; { return Some(Seq::blank()); } } 
I kinda like ‚Äúfaster‚Äù
SIT: a file-based, SCM-agnostic/independent, merge-conflict-free issue tracker. I've been thinking about this topic for sometimes and finally putting it together now: https://github.com/sit-it/sit It's very early but it's also pretty lightweight so it can mature relatively quickly. Once I am done with the first iteration of reducers, issues will be able to be transformed in a much more renderable representation (JSON)
Yes, this should be fine, though I can't find anything in the stdlib or the `num` crate. However, it's trivial to implement this yourself: fn unsigned_abs(i: i32) -&gt; u32 { // the bitwise repr is the same in this exact case if i == i32::MIN { i as u32 } else { i.abs() } } 
switching licenses for me was a breeze, see [the issue where I did it](https://github.com/vitiral/artifact/pull/194). Pretty much all of the rust community is extremely responsive. Being MIT+Apache2.0 makes it much easier to use your stuff in the ecosystem!
Is there any plans to have a neural network library built on `faster`? That would be sweet!
Glad to see another trying to get Rust into space! I currently work on a software development framework for cubesats and we are beginning to use Rust for our higher-level/middleware services. It is certainly a breath of fresh air when coming from a primarily C background.
Continuing work on my [Subsonic API](https://github.com/Azphreal/sunk). Starting to get to the point where I can almost use it as a library, but I'm still breaking a lot of things and most of the implementation is far from stable. Now that I know the scope of what I'm doing I should be able to at least stabilise the typical methods and traits soon.
My experience with nerual networks is limited to mathematical analysis and high-level libraries, so I'm hoping somebody else will take the reins on that one. I was entertaining the idea of writing a cryptocurrency miner with it, though.
How about "Godspeed"? I recently learned that means ‚Äòmay God help you prosper‚Äô, which is kind of amusing, since at the face value it just looks like "gosh darn fast".
What a *darn* shame.. *** ^^Darn ^^Counter: ^^52369
I am trying to define a constant for a struct that contains a BigInt. All of the functions for creating BigInts are not constant, and the internals are private. Is there anything I can do about this to get past `calls in constants are limited to constant functions, struct and enum constructors`? I have heard about a few const related things happening recently, but will any of these fix it in the near future? There will always be a workaround but this particular one will be very hard.
This week I am going to try to get something done on implementing const generics. I've never really done any work on the compiler, but if you are going to dive into something the only safe place to dive is the deep end, right?
The peaking and subsequent slumping of Swift is curious, as is the general decay of functional programming languages. I agree with the writers caveat on how all languages are now including functional feature sets. I thought at least one, perhaps Elixir, would still be on the rise. Maybe it's more a relative shift than a decay, as Python and JavaScript(+TypeScript) are just growing so fast. The more I look at the graphs the more I think having everything normalized to percentage of mean active users is hiding the actual local growth of various communities.
The [main tracking issue](https://github.com/rust-lang/rust/issues/24111) for `const fn` is still open, but there's activity, so it's hard to say how long it'd be. I believe it would then require `bigint` being able to use it. For the moment, the easiest way around it at the moment is using [lazy_static](https://crates.io/crates/lazy_static).
Working on Rustwell, a RESTful frontend to the Gnome Shotwell backend database. The plan is as follows: 1. Put a Diesel &amp; Rocket front end in front of the Shotwell SQLite database, with full standard CRUD capabilities. 2. Add two tables to the Shotwell Database: One to define StorageResources, (NAS boxes, photo upload servers that you use, et cetera), and then one table to define Objects. Each photo or video in the existing Shotwell database can then have one or MORE rows in the Objects table, one for each copy of the photo/video. Once that is done, I can crawl my two NAS boxes and find exactly which photos are backed up and which are not (from years of disorganized backups), and (much later) then which photos are backed up elsewhere (Flickr, Google, Shutterfly, etc). 3. Add RESTful clients to standard photo storage services. 4. Compile the whole thing to my BeagleBone, and now have something that automatically syncs my two NAS boxes and uploads to Flickr (those things that should be uploaded t0 Flickr). 5. Android app to talk to by Beaglebone, a la Sweet Home. 6. Tor service? I am on step 2 now. 
I think it should say something like ‚Äú(unsafe code) eunt domus‚Äù.
I‚Äôm unsure why BurntSushi feels MPL would contaminate. The MPL license website seems to indicate it wouldn‚Äôt be expand outwards aside from a license notice and a link back to the repository. I‚Äôm guessing there is a specific legal decision that occurred?
See Google's take on that matter: https://opensource.google.com/docs/thirdparty/licenses/. Search for mozilla public license. One reason it is more difficult to use MPL in corporate than apache or mit is that every change needs to be made public. That sounds great, but is often not particularly useful for user and maintainer: For example inside google, everything is build in a mono repo. That means pinning versions for ALL libraries used inside Google. Now you pull in a MPL licensed new library. Usually you have to remove some functionality and change some function signatures to make it work with the current set of pinned libraries - nobody outside cares about these changes, but these changes need to be made public. 
&gt; In step 5 it suggests me to add a support crate for my chip but there's no such thing for K20. I guess, I'm expected to create one myself but unfortunately it's beyond my ability at the moment. If you want to look further into that, I think the way to do it is obtain some XML file from the chip‚Äôs vendor and generate the crate with https://docs.rs/svd2rust/
Just curious, would these types just not implement `Sub`, or would they panic if you tried to subtract a number from itself?
Ehhh, the way he phrased it in the PR comment thread implies he feels it would require the copyright of bytecount's or his code to change. The MPL wouldn't require someone taking a feature out of, say, the regex library to publish anything unless the way they took the feature out was by modifying faster's code, so I can only presume that that isn't the sense in which his objection was to be interpreted.
I believe Windows supports ANSI codes, it just isn't enabled by default and requires a Windows API call to enable. Unsure if it supports all of them though, or if it's just the color stuff, since it also has separate API functions for dealing with things like cursor position. Also, most developers probably use console replacements like ConEmu or Cmder (which is also based on ConEmu I believe) instead of the terrible built-in console, and these (ConEmu at least) do support at least some of the ANSI codes by default.
* Could you have any of the struct fields of `Seq` a different type? I.e, could `desc` be a vec of an enum with 4 values: G A T C, and could `qual` be a numeric vec? * What is the reason behind the `blank()` function? If it's to help creating new `Seq` structs, maybe the builder pattern is more appropriate here? For the `FastqReader` and other structs, I would structure it like (no idea if this compiles) pub struct FastqReader&lt;R: Read&gt; { reader: R } impl&lt;R: Read&gt; FastqReader&lt;R&gt;{ fn new(reader: R) -&gt; FastqReader&lt;R&gt; { return FastqReader{ reader: reader } } } impl&lt;R: Read&gt; Iterator for FastqReader&lt;R&gt; { type Item = Seq; fn next(&amp;mut self) -&gt; Option&lt;Seq&gt; { return Some(Seq::blank()); } }
Even on Stable?
yeah, that's why I'm confused, Stable should prevent us from this kind of breakage
I have a set of JSON which always contains two certain keys, and a third key which will be one out of around 45. Unfortunately I need to deserialize this. I've looked high and low for `serde` supporting a "fallback" key in a struct and it doesn't seem like it exists. My question is thus; given that I need to get the value of whatever the third key is (that I can't be sure of before I parse the data), which is faster: - an enum with 45 variants, with `#[serde(untagged)]`, and `get_value()` being a match on the variant; or - a struct with two fields and an `Option&lt;Value&gt;` for the 45 variables, and `get_value()` being 45 `if let Some ...` statements - manually implement `Deserialize` (from the ground up?) somehow and look for 45 possible fields
I've recently thought about exactly this situation (as in, just this weekend -- I am working on an abstract interface for iterative solvers, e.g. you implement `LinearOperator` and its `axpy` method, and you automagically get BiCGSTAB, TFQMR, etc). My solution is almost identical to your first option -- here is a catch-all "dense" implementation for a `Vector` trait I defined: use std::ops::{Mul,AddAssign}; use std::iter::Sum; impl&lt;X: Sum + AddAssign, V: AsRef&lt;[X]&gt; + AsMut&lt;[X]&gt;&gt; Vector&lt;usize,X&gt; for V where for&lt;'a,'b&gt; &amp;'a X: Mul&lt;&amp;'b X, Output=X&gt; { fn inner_product&lt;'a,'b&gt;(x: &amp;'a V, y: &amp;'b V) -&gt; X { x.as_ref().iter().zip(y.as_ref().iter()).map(|(x,y)| x * y).sum() } fn axpy&lt;'a,'b&gt;(a: X, x: &amp;'a V, y: &amp;'b mut V) { for (x,y) in x.as_ref().iter().zip(y.as_mut().iter_mut()) { *y += &amp;a * x; } } // ... } Notice that my scalar type `X` only needs `Sum` (for the dot product), `AddAssign`, and the reference-by-reference `Mul` as you outlined in option 1. I haven't profiled yet, so I cannot comment on the overhead of multiplying by references (I would assume maybe slower for f32/f64 and probably faster for float-like objects that allocate), but I think it's the most logically consistent. I also place too much faith in the optimizer (or at least tomorrow's optimizer if not today's) and haven't worried too much about it.
I suggest swizzle, woosh, or choochoo (like the train).
Yes the newest windows console do support ANSI but it is disabled by default. The previous windows consoles however do not support this. In my program I have an function to enable this ANSI feature on windows but it is not used yet. Maybe later I'll will make the user or library decide if to use ANSI over winapi. But I want with my library support the most consoles also windows versions less than windows 10, and that was the problem with the most libraries there are now, that they don't do that.
You're talking a lot AVX2 here; is this tested on Ryzen too? Would be interesting read.
I don't have any AMD hardware, unfortunately. Actually, I wonder if AMD would be willing to send me a Ryzen or 'Dozer chip like they do to influencers. I'd be more than happy to optimize faster for them.
Kiera seems quite close to https://github.com/Keats/even that I've just finished but yeah having lots of themes to choose from seem to be very important for many.
The simplest way to host your static website imo is https://www.netlify.com They have Hugo support built-in so it can build your site automatically on commit without any setup from your part other than a one-liner in their config. I hear you on the Hugo template language, that's the main reason why I don't want to use it. Your site should be easy to port, minus the year/month/day url scheme which I believe is not great for SEO. The tags in your also have a double slash link (eg: https://fungos.github.io//tags/c/c++) that still works but looks a bit odd on hover.
TiKV was using RocksDB for low-level storage list time I checked. It's basically a port of Raft wrapper around RocksDB.
all red on Archlinux Firefox 58 Error loading Rust wasm module 'webgl': TypeError columnNumber: 10780 fileName: "http://brave-bell-e62dc4.bitballoon.com/webgl.js" lineNumber: 53 message: "cannot use the given object as a weak map key" stack: "__extjs_8adab11649c3c91785d157bc51cb5d1ce1d8d8f8/Module.STDWEB.acquire_rust_reference@http://brave-bell-e62dc4.bitballoon.com/webgl.js:53:10780\nfrom_js@http://brave-bell-e62dc4.bitballoon.com/webgl.js:53:8767\n__extjs_26df623d81271e4fc5f50d634a84f1e05ab8bcfb@http://brave-bell-e62dc4.bitballoon.com/webgl.js:131:46\nwebgl::webgl_rendering_context::WebGLRenderingContext::create_buffer::h529b90dcbcf00bd6@http://brave-bell-e62dc4.bitballoon.com/webgl.js:74506:1\nwebgl::main::hf10f7f82e1b0b43f@http://brave-bell-e62dc4.bitballoon.com/webgl.js:60858:1\nstd::rt::lang_start::{{closure}}::hb01474e7377862e4@http://brave-bell-e62dc4.bitballoon.com/webgl.js:69735:1\nstd::sys_common::backtrace::__rust_begin_short_backtrace::he3d139222cc318ba@http://brave-bell-e62dc4.bitballoon.com/webgl.js:18601:1\nmain@http://brave-bell-e62dc4.bitballoon.com/webgl.js:55011:1\n_start@http://brave-bell-e62dc4.bitballoon.com/webgl.js:82800:1\n__load@http://brave-bell-e62dc4.bitballoon.com/webgl.js:229:9\n__promise&lt;@http://brave-bell-e62dc4.bitballoon.com/webgl.js:246:17\n"
Godspeed as in the speed of gods doesn't seems overly religious to me, more mythical (I guess there's not much of a difference), makes me think of Hermes/Mercury or Hermod.
You have my vote for `amphetamine`. :D
... for what cryptocurrency? All cryptocurrencies I can think of are either: 1) not suitable for CPU mining, because GPUs and/or ASICs are orders of magnitude faster (bitcoin/sha-256, litecoin/scrypt, ethereum, ...) 2) limited by the size of the CPU cache, not by parallelism. You can't get higher performance from either more threads or simd, unless you get a cpu with bigger L3 caches. (monero/cryptonight) That said, I could maybe see a use for a Monero miner which can do multiple hashes in parallel on a single thread, loading up only 1 cpu core, rather than doing them on separate threads and loading up all cpu cores.
The issue isn't in the name itself, but in its searchability
This crate introduces `hex!` macro which allows us to write `hex!("00 ab cD EF")` which will be converted to `[0x00u8, 0xAB, 0xCD, 0xEF]` at compile time. It's based on [`proc-macro-hack`](https://github.com/dtolnay/proc-macro-hack) and was written after this [RFC](https://github.com/rust-lang/rfcs/pull/2244) was closed. Additionally it can be seen as a smaller, more specialized, and dependency-less version of [`binary_macros`](https://github.com/golddranks/binary_macros) crate, another difference is that this crate does not crate `static` and instead generates arrays directly, which allows it to be used for defining consts.
I have a struct `Color` with `pub type RGBA = Color` and several other representation like `RGB`, `HSVA`, `HSV` etc. which impl `Into&lt;Color&gt;`. Now I want to predefine some Colors like Black, Blue, White etc. * Is there a better representation? * Where the predefined colors should be defined? Currently all structs are defined in the `color`-module. Should the constants be defined in the module as well or should the be defined in the `Color`-struct impl?
&gt; I‚Äôm unsure why BurntSushi feels MPL would contaminate. I don't. I never said it did. I said it would introduce a copyleft dependency, which it does, and that I would not allow that to happen.
I'm working in a GUI App, [task_remote](https://github.com/werner/task_remote) that allows to run scripts in a linux server. I'm a Ruby on Rails dev, and the boss is always asking me for data in the database, ex. How many sales with certain tax do we have? or give me an Excel file (csv) with data concerning with sales from certain period of time. I do have files with all theses scripts saved in a folder, but sometimes it gets annoying to look for these and access to the server to run them. So, I decided to create an App that saves all the tasks in a sqlite database and have easy access to them. I'm using Gtk, but I'm facing some problems, when I clicked the execute button, it waits until the ssh command finished, sometimes it take too long and the app collapsed. I've tried to use threads with no success. I'm seriously thinking in changing the GUI library.
Not really, when googling "rust simd", faster is the 2nd match after rust-lang-nursery/simd.
&gt; But I want with my library support the most consoles also windows versions less than windows 10, and that was the problem with the most libraries there are now, that they don't do that. Yeah, that's probably a good idea. As long as the library API doesn't leak OS-specific implementation details like file descriptors/handles (which Termion did, IIRC), I guess it might be possible to detect whether the console version supports ANSI codes and use those if it does, but fall back to the old APIs otherwise? At least the parts it supports.
Any idea where I can find that graph but with PureScript and Elm included? I'm curious to see PureScript's growth over time.
Good idea I am going to look into it.
Super excited about the thing! When do you think it will be ready?
I found this in the Microsoft documentation. https://docs.microsoft.com/en-us/windows/console/console-virtual-terminal-sequences The following code provides an example of the recommended way to enable virtual terminal processing for an application. The intent of the sample is to demonstrate: 1. The existing mode should always be retrieved via GetConsoleMode and analyzed before being set with SetConsoleMode. 2. SetConsoleMode returning with STATUS_INVALID_PARAMETER is the current mechanism to determine when running on a down-level system. An application receiving STATUS_INVALID_PARAMETER with one of the newer console mode flags in the bit field should gracefully degrade behavior and try again.
Maybe crack? I'm actually thinking of the sound a whip makes...
I've been keeping an eye on [sanakirja](https://crates.io/crates/sanakirja) for a while now. It's an LMDB inspired storage engine written in Rust.
It's kind of tragic what happened to Ruby because it really is a beautiful language. It's extremely clean and flexible.
I'm somewhere between half and ‚Öî done. I've got the top-right section and the tree to do. That's not counting the text on the ribbons, where I'm waiting to see what the Latin pedants say. Also: the SVG is now over a megabyte, and Inkscape hung for about five minutes when I cut the hatching paths for the crab to move them into a different group. Inkscape is... *less than optimal* for complex images.
Working on a stb_image and stb_image_write wrapper. So far I have support for reading from, and writing to, BMP, PNG, JPEG, and TGA files, and modifying pixels with color enums that can be converted to/from both float and byte tuples. https://github.com/lukediamond/stbi-rs
In a previous thread, someone pointed out that mangled city names are memorable, searchable, can be descriptive, and [kind of a thing already](https://tokio.rs/). The proposal there was `simdney`, which I'm personally quite fond of. It ticks all the boxes. I can't find the thread now, tho. 
A little bit of work on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis). I still haven't solved [compile time](https://github.com/iliekturtles/uom/issues/52) issues and probably wont have a huge amount of time this week so may knock off some easy issues before tackling the big ones.
As a big fan of copyleft, this makes me sad. :( Have you spelled out the reasons for this strict exclusion somewhere? I've found some single-sentence statements of yours, but nothing more detailed than that.
The CSV library by u/burntsushi is excellent for all your multigigabyte CSV file needs. We use it for 50+GB files regularly at Faraday.io, and there is support for building an "index" that permits parallel processing of chunks.
`const`-generics give you this ability (and much more), so you can define, for example, a `struct Bounded&lt;T: Int, const From: T, const To: T&gt;` type that enforces this invariant, and many others.
Similar to creating bindings for Gecode, it might be posible to generate bindings for the [libminizinc](https://github.com/MiniZinc/libminizinc) library which is the actual implementation of MiniZinc. I don't know if that would be a viable way to make a programming interface for constraint programming in Rust.
Out of curiosity, what do you use constraint programming for?
First, a big thank you for working on RLS. It has become a highly valued component in our use of Rust at work. It makes the whole Rust world more accessible to developers with less Rust experience. But the downside of this is that when it's broken, everything is just a bit harder and less fun. :-/ I wish I had more time to help, but unfortunately, I'm already overloaded with other Rust projects, both at work and in my free time. So this is just a kind of a "Rust 2018" wishlist thing, I'm afraid... (And solid cargo workspace support would be a huge win. But that might be a ton of work?)
I have worked on a number of repos which were (mis)identified as being primarily/only Javascript simply because they included a lot of JS framework code. Unless the repo owner goes to the trouble of correcting this, you could be overstating the percentage of JS projects.
You'd probably get something way cleaner with a framework like [yew](https://github.com/DenisKolodin/yew) (never tried, but it looks inspired by relm, so‚Ä¶ :) ).
Could you elaborate your thoughts on the matter?
Ooh, now that's an idea...
TL;DR - [copyfree.org](http://copyfree.org/) provides some loose background. That site is both persuasive and informational, so be on your guard. You needn't accept their claims as true to understand my position. :-) Since these are projects I work on in my spare time, and because I feel very strongly in my opposition to copyright (and IP in general), I do my best to find outlets for expressing that opposition while simultaneously attempting to remain practical. One outlet used to be that I would release all my code [under the UNLICENSE](http://unlicense.org/), but this license is not widely used and some folks are rightly skiddish of it. Therefore, I switched to a dual licensing scheme that incorporates both the MIT and the UNLICENSE, which AFAIK, has resolved all outstanding complaints about my previous strategy. On the surface, this scheme doesn't make any sense because the MIT is, *in practice*, just as permissive as the UNLICENSE. The only way this scheme makes sense is if you look at it from an ideological perspective. The purpose of the UNLICENSE is to conscientiously object to copyright. It may not be clear why this means I reject copyleft, because the copyleft folks also tend to start from a position that opposes copyright. The key to understanding this is that a world without IP laws is not the same as a world with copyleft. I've talked to a lot of people about this, and my experience is that a lot folks are confounded by this position. The high order bit is that my position here is a *legal* position and not necessarily an ethical one. That is, I believe in the goals of copyleft (I love the *idea* of user freedom) but I reject the use of third party legal systems to uphold it through coercive means. The aforementioned is mostly ideological. But I'm also a deeply practical person, and my opposition to copyleft is grounded there as well. In my experience, the use of any license that might need to get Big Corp's Lawyers involved leads to sadness on my part. I once used a [more obscure but permissive license](https://github.com/BurntSushi/toml/issues/183), and I paid for it with emails for years from big corps (nicely) asking me to change the license because their lawyers wouldn't let them use it. Am I saying that the MPL will lead me down this same path? No, of course not, but it gives me pause. I don't want roadblocks, and IME, copyleft is a roadblock because it places requirements on distribution. Lawyers are reasonably unreasonable about these things. N.B. I do not want to encourage a broader debate about IP. I've held this position for years (and not just limited to software), and any criticism you have of it is likely something I've already thought deeply about. If you're interested in some of the practical ramifications that IP has, then [Information Feudalism](https://www.amazon.com/Information-Feudalism-Owns-Knowledge-Economy/dp/1595581227/ref=sr_1_1?ie=UTF8&amp;qid=1517231852&amp;sr=8-1&amp;keywords=Intellectual+feudalism) is a good exposition on the topic without much of an ideological bend, which should make it accessible to folks regardless of which politicial aisle you stand on.
See: https://www.reddit.com/r/rust/comments/7tnken/faster_progress_report_2_up_to_600_speedups_for/dteraxa/
Cocaine is a hell of a crate
Ah, thanks for submitting. :) (Speaker of the talk here.) Please note that the talk is geared for a PL researcher audience, so it's very much on the theoretical side. You can find the paper at &lt;https://plv.mpi-sws.org/rustbelt/popl18/&gt;. In my blog at &lt;https://www.ralfj.de/blog/categories/rust.html&gt;, I also talk a bit (and hopefully more in the future...) about this research, and those posts are hopefully more understandable for practitioners. ;)
Thanks! This should keep me busy reading a little while. ;) Once that's done, I will probably have some comments or questions, and I'll then figure out what to do with them...
I'm generally opposed to copyright as well, so I'm sympathetic to your position. But I also don't think it's going anywhere absent the technological singularity literally happening. Given that, I think copyleft licenses are a bit of vinegar in the eyes to the largest beneficiaries of copyright.
True, but googling "rust faster" gives more ambiguous results (although it's still in there). But even the title of OP's post is confusing because of the name. I really think it needs to change.
What happens when a `hex!` is given an invalid string?
Is there a parallel to be drawn with Rust and, say, PHP ?
I'm shocked by the poor quality of this analysis. The Ruby FUD and subsequent rowing-back, the mis-classification of Jupyter Notebooks as a programming language, the "these languages are apparently cool, I've begun using them" conclusion (which is based on dubious analysis, and, even if it weren't, would be questionable). Enough already.
I had that problem when using the `mqttc` crate which, unfortunately, contains `unwrap()` so may panic at times. As I was using it in server code, the panics were really unwanted for. My solution was to wrap potentially-panicking-code blocks in a `let result = panic::catch_unwind(|| {here_be_panicking_code});` statement. That way you can check if the code executed correctly with a `result.is_err()`, or you can even simply ignore the panic by replacing `result` with `_`. No more panics at runtime, happy me. But that's a band-aid, and I don't even know if it works in all conditions. I totally agree with you, I would really like to be sure library code is exempt of panics.
&gt; Is there a better representation? Depends. If all your algorithms operate on RGBA, you might get rid of the other types completely and just add more constructors to you RGBA color type. If your algorithms work on all representations natively, you might want to create an enum for the colors. If your algorithms works on different representations, your current choice seems best to me. &gt; Where the predefined colors should be defined? Currently all structs are defined in the color-module. Should the constants be defined in the module as well or should the be defined in the Color-struct impl? Constants in the color module seems like the better option.
Do you support NEON?
&gt; Most of the time it has to do with databases where zero has a special meaning and is not legal to be used as part of the program. I must be misunderstanding something, but in which database has zero special meaning?
The struct has a much bigger in-memory representation, that is something to consider as well. Also, it is less correct, because if I understand correctly you will never have all other 44 fields not be `None`. I would be careful with `time cargo test --release` because of debug asserts inhibiting optimizations.
Just a pet project of mine involving a multi-agent system. I found out that some part of the decision making of the agents seemed to rely on constraint, and I figured I could try to use constraint solving for that part.
nohup will write to nohup.out if you don't redirect.
I would use web-view crate.
They're probably talking about user experience.
It should panic with the message explaining that exactly gone wrong, e.g.: hex!("010203d4") // thread 'rustc' panicked at 'invalid character: 'g'' hex!("1") // thread 'rustc' panicked at 'expected even number of hex characters' hex!(10) // thread 'rustc' panicked at 'expected string literal as an input'
Any plans for adding dynamic reconfiguration, with the ability to notify application of change? And what about support for overriding configurations? For example, hierarchy configurations within file-system, where config files in inner catalogs override those in more root ones? Or user configs on top of global one? 
Continuing to work on the guts of [relm](https://github.com/antoyo/relm). Right now I'm working on updating the `syn` dependency from 0.11.x to 0.12.x and it's...complicated. There's nothing quite like changing one line in `Cargo.toml`, seeing the entire minimap in Sublime Text do it's "I'm inserting errors" dance, then seeing your entire screen turn red. I'm also adding in more targeted error messages as I go.
Is there any idea what version of Firefox webrender is being targeted for? Or even when it might be enabled by default on nightly like Stylo was?
Without knowing any details, my very uniformed guess would be that your requirements on the constraints, probably are * Simple interface for multiple Spaces/Stores/problem instances * Mostly simple constraints on the variables (no complicated global constraints for example) * Mostly simple search requirements The first requirement (if it is a requirement) probably rules out MiniZinc, since it (AFAIK) is much more geared at solving a single instance and is much more heavy-weight when it comes to specifying a problem (since it involves compiling to low level FlatZinc and setting up an external solver). Gecode would be good for that, if it had Rust bindings. An alternative, which is very much doable if the model used is fairly static and is way simpler integration-wise, is to write the model in C++ and to export a simple interface for that model into Rust. That way, the Rust code can construct instances of the model with the right data, and handle that through some simple custom functions. 
I will once the foundations of the library are in place (and once I can compile it for my Snapdragon 800).
Languages often become popular due to their use cases. I think the number one issue with Ruby was that it was only popular due to Rails. Outside of the Ruby community there aren't really any other well known Ruby frameworks, libraries, or applications.
in my understanding, not yet.
&gt; defining them via static methods i think they were suggesting associated constants
You are implying they will. Is that planned?
No, what the parent means is something else. This is a ranking by percentage of GitHub users. Ruby is *the* reason GitHub became popular, it's written in Ruby, by Rubyists, and Rubyists generally used it first. Rails was one of the first major projects to start using it. That's not a statement of some kind of "hipster cred," but when you start off at near 100% of usage, any new usage brings that down. That doesn't say anything about Ruby as a language, that has to do with new people coming to a platform.
I wanted to try my hand at a SHA256/Scrypt miner - not to make any money, but to see if how a low-effort solution with faster stacks up to a solution with years of monetarily incentivized optimization.
Jekyll? Sinatra? Mongrel? Nokogiri? Sass? Most people know about them (I certainly do, and I've written &lt; 100 Ruby LOC in my life, and am thus definitionally "outside the Ruby community"). Ruby is a very big, active ecosystem. You can get an interview for a Rails job in London, Berlin, NYC, or SF _almost instantly_. If Rails went away tomorrow, the community would be smaller, _but it isn't going to_. It hasn't been replaced by anything; the most concrete thing you can say about it is that some other languages / frameworks are currently growing a lot faster.
From my cursory research, getting that behavior involves loading a vector past the bounds of a slice or shuffling a vector across lanes. I haven't looked into the safety of masked loads, so this might change in the future. I'd prefer the behavior you described, though, so any ideas would be welcome. 
It's totally fine to implement traits on builtin types.
[Update on progress.](https://www.reddit.com/r/rust/comments/7tt4zm/certa_celeris_concurrens/)
Good idea, thanks! I'll update to that.
For simple problem you can use : * [quickbacktrack](https://github.com/advancedresearch/quickbacktrack): which is a backtracking implementation * [ai_kit](https://joshmarlow.github.io/ai_kit-rs/ai_kit/index.html) : which has simple constraint solving. 
All hail Quxxy, His Holy Macroness! Now I have but a single question: Do I need to ask /u/Quxxy or /u/qedunham to get a (bunch of) t-shirts with this?
I'd say the latter: I didn't do the creative work.
Not doing the creative work sounds like "doing the sales work". I'l take 42 shirts in equal parts of {grey melange, white, black, /r/rust-orange} ‚®â {L, XL} with shipping to Europe please!
Depends on the lib and the data flow, but doc tests and tests/ directory files and so on should be able to cover it all, then cargo test will run all your tests. Quickcheck lets you get broader coverage and can sometimes show you edge cases that you didn't think of.
Meh. I'd like Rust to satisfy the "writing hacked up software that accidentally turns out to be reliable, scalable, clean and fast" requirement.
If I recall correctly, Rust promises that your code will always work, but it might be slow when a panic actually occurs. It's a great way to ensure that a block won't panic, though, so if you need the certainty... TIL. Thanks.
Seems to work great, thanks!
What's going on? Am I somehow in r/rustjerk?
Is the playground link supposed to illustrate the error? Seems to work fine, even with the first version of the function without the transmute.
So you want zero-copy deserialization?
&gt; a bit of vinegar in the eyes Yes that's precisely the benefit and the problem at the same time.
Author of `prost` here. &gt; the only one who uses practical types is quick-protobuf Practical by what criteria? I'm aware of three implementations: `prost`, `rust-protobuf`, and `quick-protobuf`. All three generate reasonable data structures given their design goals. `prost` is written explicitly for being straightforward to use in async contexts (no lifetimes) and with simple and idiomatic generated code; `rust-protobuf` mirrors the original C++ implementation to a large extent; and `quick-protobuf`'s goal is to reduce allocations to the greatest extent possible by leveraging lifetimes. &gt; Allocating everything again so that it can be serialized is entirely stupid and it's also insecure. It is not 'entirely stupid'. The Protobuf format is not conducive to zero-copy serialization. Every field is preceded on the wire by a tag (and length for variable width types). Thus, if you want zero copy encoding, you are going to end up calling `write(2)` (or equivalent) for *every single field*, which is a very poor way to do IO. Allocating plus copying to a buffer will end up being faster in all but trivial cases where you are serializing a message with one enormous `string` field (especially when you can share the buffer among many messages). On the receiving side it does make more sense to go after zero-copy, if your use-case is deserializing very large strings. `prost` can't currently do this, but there is a plan for how to eventually get there using the buffer abstractions in the `bytes` crate, see [#31](https://github.com/danburkert/prost/issues/31) for more details if you are interested. Finally, I'll say if your requirements are for absolute performance and/or cryptographic material security, you shouldn't be looking at Protobuf. That's not what it's good for.
Mods are sleeping, rewrite everything in Rust!
Continuing work on [nanowasm](https://github.com/icefoxen/nanowasm), a standalone webassembly interpreter. Really I barely touched the actual interpreter at all, most of the time went into writing test infrastructure and some convenience code. Turns out there's a rust binding to the [WebAssembly binary toolkit](https://crates.io/crates/wabt), which makes life much easier. :D
Perhaps [this](https://play.rust-lang.org/?gist=d7e81250a6e717765733a1579ac195d5&amp;version=stable) is a better demonstration of the problem. You can never use `BTreeMap::range` when it contains `'static` keys, unless you have a range of `'static` items.
&gt; Thus, if you want zero copy encoding, you are going to end up calling write(2) (or equivalent) for every single field, which is a very poor way to do IO. Well not exactly true. This is what writev is for. But since you can't use that with any ssl lib it's generally a mute point. I know it's not zero-copy. That is fine. What I dislike is that it is not zero allocation. If I have strings/bytes as a part of some data structure, rust-protobuf and from what I see prost force me into cloning that data. Because I have to put those strings and byes into the API data structure. Copying it to an intermediate buffer is fine as you can't really do much better. 
Zero allocation serialization.
Oh
[Nothing to see here, move along](https://www.reddit.com/r/rust/comments/7tkiuv/rfc_adjust_subreddit_logo_to_say_simul/)
Thanks! That mostly covers Rust testing though. I'm looking for something that will help me specifically with the FFI code, including checking for memory leaks when calling the manually written constructor/destructor functions. Maybe a simple C integration test called through valgrind would be enough.
*Edit:* Actually you can reserve memory for a list in Python: `[None]*N` where `N` is the capacity. This is kind of a workaround and I don't see an option for dictionaries.
Don't worry, you wouldn't lose your exclusivity: This is not an insignia of The Temple‚Äîit's a coat of arms.
Exceptions for flow control aren't bad because of performance; they're bad because they lead to bugs like most pythonic disciplines. At least with _unchedcked_ exceptions; the problem with exceptions for flow control is that it's a control path that is _very_ easy to forget to check and this has nothing to do with dynamic typing. In Python it's considered pythonic to just do this: try: value = dict[key] except ValueError: # handle key not in dict # do stuff with value The problem with this idiom is that if _forget_ to account for the possibility that the key is not in the dict that the ValueError propagates upwards and _another_ thing that uses exceptions for flow control might catch it instead because you internally used a dict in some other data structure and that it propagates the exception was a bug; this is a very easy to make mistake due to how it works because forgetting to actually check raises no alarms. In Rust you either use a way to access that returns an Option and you can't use Option&lt;T&gt; where you would normally use T and if you Option::unwrap you are conscious of that you have checked. Alternatively you use a function that can panic but you in general don't catch a panic later as part as flow control and Rust panic's in general aren't meant to be caught and catching them is a very exceptional thing with the general rule being that if a panic occurred someone made a programming mistake somewhere so you only catch them to deal with someone else's mistakes. This is in general a problem with many more pythonic idoms in that they generate bugs. Like that Python's string.find seriously returns -1 on not found rather than returning something like an optional integer; so you forget to check for the error case or I don't know make a typo and do `index == -2` instead of `index == -1` and you just pass the `-1` invalid index onwards which when you use it to index again by the way indexes the last element of a collection in general. All this stuff turns typos into bugs rather than into compile-time or early-caught runtime errors.
Don't send a single byte at a time, that will be really slow - the item type of your channel should be a `Vec&lt;u8&gt;` or similar. Wherever these bytes are coming from, I expect you receive them in chunks rather than one at a time, so send those chunks over the channel. Alternatively, pick a fixed chunk size and experiment to see what gives a good performance/memory trade-off. The size of the channel probably doesn't matter too much - the biggest factor is how "far ahead" you want to allow senders to get in relation to the receiver.
Unrelated to your main question, you should probably change `&amp;'a self` to `&amp;self` to get a more general and usable API.
This doesn't really answer my question, and I think it's because the intent of my original question isn't entirely clear. I hope I can make my question more clear with some more information. Let me start by making two observations: 1. Looking at your code, **it seems what you're doing is relying on enabling ANSI support for Windows terminals**, which is what I would assume you would do with this particular library -- it's far easier to enable ANSI support in Windows than try to make a ton of platform-specific logic. 2. As /u/Saefroch helpfully pointed out, across platforms this applies to raw mode usage for TUI applications; you can't leave something in raw mode! That would be pretty lame to leave a user to fend for themselves in raw mode after your app closes. **This same sort of concern applies to enabling ANSI support on Windows terminals, because you're changing core terminal state and it really should be reverted back (i.e., ANSI gets disabled if it wasn't enabled before) once an application is cleaning up.** So, with these two points in my, my question is: **How are you handling this issue? *Do* you handle this issue?** This is a concern I've already had to consider with the (small) amount of time I've put into piggybacking on previous work to Windows support to [Termion](https://github.com/ticki/termion). I'm hoping you've found something I haven't. :)
I appreciate the help. :)
In `prost`'s case you can simply move the `String` or `Vec&lt;u8&gt;`field value into the message using normal Rust move semantics, no tricks needed.
Oh. Probably added that while trying to make it work. Thanks.
What I'd want is a single thing which, as you put it, "conflates" the two concerns of receiving readiness events and executing code using _n_ threads, in a way that... (1) avoids unnecessary thread handoffs (when _n=1_, it's just like what tokio does now, rather than having an epoll thread and a separate run-stuff thread), (2) doesn't cause thundering herd problems (e.g., doesn't use level-triggered epoll on every thread), and (3) handles a slow callback as gracefully as possible. I don't think it's possible to achieve these properties without this "conflation", which is why I say it's not possible to add this on top of what tokio provides. In particular, if exactly one thread is responsible for the epolling, then you either can't run callbacks on it (violates 1 above), or the other threads basically go idle if it gets stuck (violates 3 above). So you need something a little more sophisticated, such as rotating the epoll thread (first thread to idle does the epolling). (IIRC, libevent does this. I'm sure you've seen other event loops that do as well. gRPC does [something fancier](https://github.com/grpc/grpc/blob/master/doc/epoll-polling-engine.md).) Am I wrong? Can that be implemented on top of tokio as it exists today? I'd be happy to be corrected. &gt;&gt; I'm used to not having to deal with [complications of having N single-threaded reactors] at all. Having a multithreaded reactor is a much better solution. &gt; There is a big difference between saying you can't do X and you don't want to spend the time figuring out how to do X. That's not what I'm saying, though. What I'm saying is that I can't do X, so I'm forced to do Y, and Y is a lot harder than X would be.
You're not wrong, Walter, you're just an asshole.
My pet idea for an OS is to have an "aggressively autocompleting shell". With existing OSes, this doesn't retrofit as well as it would work from scratch. You want to know exactly which commands are available to the user, and exactly what the possible arguments are for each command. As each character is typed, the list of words that could be used in that position are filtered. The list of possible inputs would be displayed in the shell in real time. Help text could be displayed for the previous argument (or command, if the last entered word is actually still the command), which would make everything self-describing. As soon as only one item remains, the word is instantly autocompleted, with the cursor placed at the end. All input (except for backspace) would be ignored until the user realizes and hits the spacebar, to prevent undesirable behavior if the user keeps typing. If the user presses backspace, it should erase the entire last argument. Wildcard arguments where the user can literally enter anything would behave like traditional arguments, no aggressive features, but if an argument is looking for a file name, that could obviously be handled with aggressive autocompletion. I also think PowerShell had the right idea with piping objects instead of text between commands. Another thing to consider is the coming revolution of NVRAM. When there is no longer a distinction between a computer that is asleep and turned off, rebooting loses its meaning. What kind of abstractions do you make for a computer that has no volatile storage... all storage is persistent, so RAM is no different than long term storage. There's no need to load the program into memory, there's no need to allocate objects in memory and then write them to disk... etc. 
Continuing work on [tarpaulin](https://github.com/xd009642/tarpaulin) full steam ahead! Last week I moved a lot of my test running logic out into it's own state machine. Makes the code tidier, tarpaulin easier to hack on and removed all blocking ptrace calls! This week I'll be closing an issue or two, refactoring more stuff and maybe work on a blog post on what happened in 2017 and what I'm hoping for 2018. I also plan on playing around some more with my branch/condition coverage idea which is basically a mini-JIT to evaluate the expressions...
Could you explain what you're looking for here? &gt; 2- Work with BTrees and hopefull allow to also use other things like arrays. Are you talking about the interface you use? None of the databases folks are talking about here (lmdb, sled, sqlite, rocksdb, sanakirja) use btrees or arrays in their interface. With the exception of sqlite, their interfaces all just deal with sorted key/value pairs. The data structures they use to provide that (btrees or LSM trees) are an implementation detail. There is one embedded key/value database I know of (Kyoto Cabinet) that supports hashtables as an alternative datastructure for a table, for performance at the expense of not being able to iterate in sorted order.
Thank you myrrlyn for voting on Darnit\_Bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
*Darn it* myrrlyn, I am not a bad *darn* bot... :c *Beep boop*, I am actually an awesome bot. *** ^^Darn ^^Counter: ^^52577
Thanks! Oh yeah, I wanted to do this but was too lazy to do it. Patches are welcome! 
It also supports cargo, so things with dependencies are no problem
When do bps wipe on official servers 
A probably very basic question: Can `faster` help multiply slice of integers with a scalar? `fn multiply_with_scalar(v: &amp;mut [u64; 16], f: u64)`
Yup! fn multiply_with_scalar(v: &amp;[u64], f: u64) -&gt; Vec&lt;u64&gt; { let scl = u64s(f); v.simd_iter() .simd_map(u64s(0), |v| v * scl) .scalar_collect() } Multiplying the slice in-place will be supported in 0.5.0 with `simd_for_each`.
I mean I need to at least work on btrees. I could store the tables as btrees (and the indexes) This is the case with LMDB where it only work with btrees and need to build all on top of that. However it will be nice if can also use arrays or hashtables.
If you're using lmdb, then you're not working with btrees. You're working with sorted key/value pairs. What are you hoping to achieve by specifying the data structure it uses internally?
Ok Ferris is creepy in this. Look at those soul-hungry eyes.
Thank you for your answer. I suspected something panic-related. Looks like I need to learn more about dealing with exception safety.
Another problem is in that the reference passed to the closure can escape self's lifetime.
Repository: https://github.com/edwin0cheng/unigame 
Honestly, it reads weirdly. It's not immediately clear either where the expression ends and the "else" block starts; or at least, if the expression was a tad more complicated it would not be as obvious to a *human* reader: else let Some(c) = if x.is_some() { x } else { y } { Err("x and y cannot both be None!")? }; I would really advise having some separator between `}` and `{`. Maybe `or`?
If it were me, I would run one Tokio reactor and run all logic on a thread pool. This isn't too hard to do today, but is even easier to do w/ the changes currently pending in Git. &gt; avoids unnecessary thread handoffs I am *highly* skeptical that, in a generic case, sharing the epoll work across the thread pool will pan out to be worth it. And, again, the Tokio / futures-rs model is significantly different than that of other async libs, so you can't say "this worked in this other library, so it is what Tokio should do". The logic described in the gRPC link involves a heavy number of syscalls. I am skeptical that in the general case, the benefit of moving FDs across epoll instances is going to outweigh an efficient cross thread notification strategy... **especially** in a post meltdown world. That said, the building blocks that tokio provide lets you reuse both the tokio library and a thread pool library and build something similar to the epoll set / island etc... and that is the point, we're building a foundation. And, if I am wrong, since we have a solid foundation, changing the model would be pretty easy.
I mean can you explain why it is this way? (It's counter-intuitive considering normal Vecs grow from the left.)
Thanks for clarification! I just found my bottleneck: the task that reads the stream was too low ;)
This was a well-written post that clearly outlines how lifetimes can enforce memory safety in Rust. I like the idea of using lifetimes to avoid copying data when all you need to do is read it. A post that I would love to see is one that involves using multiple lifetime parameters. For example I have [seen this snippet](https://github.com/rust-lang/rust/blob/5f3bd73d8143f093c58f367b46434c2d555ef62d/src/librustc/infer/README.md#creating-an-inference-context) in the compiler documentation and don't quite grasp how each lifetime applies to that struct's implementation. I would appreciate any reading that I may have missed already if there is some.
Yeah the temple ones you have have to go through a whole ritual to summon it. unsafe is involved, you might end up being possessed by a nasal demon. Not fun.
It's basically the same as with: fn main() { println!("{}", 0u32 - 1u32); }
In MySQL an `INSERT INTO table SET id=0` is going to use the next auto-increment unless you add `NO_AUTO_VALUE_ON_ZERO` to your SQL Mode.
Btw, can you recommend how to get into applied crypto (with Rust)? Should I read Applied Cryptography by Bruce Schneier? Or would you recommend another book? I want to have a foundation for working in the blockchain space..
Sure - let's say you're iterating over an 11-element collection with 4-width vectors. A vector is just a really big register in the CPU. We can load a vector in a single instruction by giving our CPU the address of the first element we want to load, and the CPU will load that and the next 4 elements into the register. Therefore, you load the first vector like this: _mm_load_i32(slice[0..]) +-+-+-+-+ |1|2|3|4| +-+-+-+-+ ^ ^ ^ ^ +-+-+-+-+-+-+-+-+-+-+-+ |1|2|3|4|5|6|7|8|9|0|1| +-+-+-+-+-+-+-+-+-+-+-+ No problems yet. Now, we load the second vector: _mm_load_i32(slice[4..]) +-+-+-+-+ |5|6|7|8| +-+-+-+-+ ^ ^ ^ ^ +-+-+-+-+-+-+-+-+-+-+-+ |1|2|3|4|5|6|7|8|9|0|1| +-+-+-+-+-+-+-+-+-+-+-+ Still good, but what about the third vector? _mm_load_i32(slice[8..]) +-+-+-+-+ |9|0|1|!| +-+-+-+-+ ^ ^ ^ ^ +-+-+-+-+-+-+-+-+-+-+-+ |1|2|3|4|5|6|7|8|9|0|1| +-+-+-+-+-+-+-+-+-+-+-+ We can't load this vector in one instruction when we're starting at 9. We could manually insert each element in the vector, but that takes 3 cycles instead of 1. With 64-width vectors, that's no good. _mm_insr_i32(0, slice[8]) _mm_insr_i32(1, slice[9]) _mm_insr_i32(2, slice[10]) So instead, I just go back as many elements as I need to for the last vector _mm_load_i32(slice[slice.len() - 4..]) +-+-+-+-+ |8|9|0|1| +-+-+-+-+ ^ ^ ^ ^ +-+-+-+-+-+-+-+-+-+-+-+ |1|2|3|4|5|6|7|8|9|0|1| +-+-+-+-+-+-+-+-+-+-+-+ That would cause 8 be in a vector twice, so I keep track of which elements were already loaded, ignore them when I'm storing them, and initialize them to the "default" given by the user in `simd_map`, `simd_reduce`, etc. +-+-+-+-+ |D|9|0|1| +-+-+-+-+ ^ ^ ^ ^ +-+-+-+-+-+-+-+-+-+-+-+ |1|2|3|4|5|6|7|8|9|0|1| +-+-+-+-+-+-+-+-+-+-+-+ Of course, the ideal behavior would be somehow shuffling that vector so the user sees `|9|0|1|D|` when iterating, but that becomes difficult and slow when your vectors get very big (thanks, Intel). 
Wow, nice! Did you write the webgl bindings just for this demo? Would it be possible to extract it into its own crate?
Hmm Option&lt;\*const&gt; is probably your best bet then, for certain values of best. It'll cost you a word of space per, though, because raw pointers don't receive the NonZero space packing optimization. Or just use bare \*const and remember to always null check it. parent.attach(child) will have to attach the child to itself and then insert its address into the child You already have the bottom-up drop behavior, so that should be fine‚Ä¶ Honestly I'd recommend just taking the hit of two words per object and use Rc rather than reinventing it
Can I be honest? I don‚Äôt like it. It gives Rust some religious touch.
Thanks, but what do you do when you try to load a 4 element vector but the total slice to load from only has len 3?
For reference, the versions of refutable let syntax that I know about: ----- `let ... else` let Some(val) = opt else { bail!() } Known problems: - Ambiguity when rhs of `=` is in form `if { () }` - Expect regular binding by prefix, get something extra at the end ----- `&lt;keyword&gt; let ... else` guard let Some(val) = opt else { bail!() } Known Problems: - Same ambiguity as above - Requires new keyword ----- `let ... &lt;keyword&gt;` let Some(val) = opt or else { bail!() } Known Problems: - Same expectation of regular binding by prefix - Requires new contextual keyword ----- `else let` else let Some(val) = opt { bail!() } Known Problems: - Possible ambiguity when proceeded by an `if` statement? - Human ambiguity around blocks in rhs that exist for `if let` ----- `&lt;keyword&gt; let` unless let Some(val) = opt { bail!() } Known Problems: - Requires new keyword - Human ambiguity around blocks in rhs that exist for `if let` ----- If there's another alternative I missed, tell me. I'm drafting out a potential revival of the deferred RFC and want as many alternative syntaxes to mention as we can bikeshed. 
That expression has different behavior in debug and release mode. Would you actually want it to allow creating a value of 0 in release mode?
&gt; Rust needs a solid embedded story. Good gob. At the very least ARMv7 Linux should be tier 1. (and if we're serious about wasm it should be tier 1 also)
I think you're looking for /r/play_rust.
&gt; a mute point It's "moot point", btw. Relevant username? :P
üòÇüòÇüòÇ I didn't read it carefully enough. my bad 
WASM should be 2, imo. We need a libstd to compile on it; we don't need the compiler suite to do so. WASM isn't a full target and doesn't even need all of libstd; I think that'll require some reclassification of what it means to be a supported target
My webgl code are mainly from https://github.com/oussama/webgl-rs, but there are some performances tuning and firefox compatibly fix added in my code. And i know that someone added a pull request for adding webgl support in gl-rs crate too. 
Thanks for the reaction and clarifying your question. First of lets start reacting to your two first observations: * `It seems what you're doing is relying on enabling ANSI support for Windows terminals`. I have the functionality to enable `ANSI code` support for windows, but I am currently not using it. What I do, is to check whether the current platform is `Windows` than use `WINAPI `and if the current platform is `UNIX` than use `ANSI codes`. Probably not an good idea I just thought because CMD is not the only console for windows there are a ton of greater 3th party terminals for windows that support ANSI. The question I however have for you is: you say `it's far easier to enable ANSI support in Windows than try to make a ton of platform-specific logic`. I agree but then what to do with terminals like windows .. 7 an 8. Should I just completely ignore those terminals and just focus on ANSI escape codes? and windows 10 terminal by enabling ANSI? Is it not just for those terminals that this library does not forget? * Good point I agree that the library should clear its mess up after some changes to the core state of the terminal. Actually I did not think of that during the development of this library. Here are two points how I address this issue: * As I said in other questions I currently have not an thing like clearing up the changes made my library. The only thing that gets cleared up are the colors and styles like attributes(Bold, Italic, underlined). This is done directly after something with colors or style gets print to the terminal. * Currently I actually do nothing with the core state of the terminal in both windows and UNIX systems. As mentioned bij #1 there is one function that modifies the state wits is `enable_ansi()` method for windows, but this functions is not used. So when the process of the client stops nothing is really changed to the core state. Than you got these points where you had suggestions how you would solve this practically problem for restoring the state of the terminal back to its original state. *`Expose the entire library API through a struct (i.e., Terminal)`. This would be an idea but the thing I don‚Äôt like about this approached is that all the library‚Äôs modules will be loaded for every time an user wants to have the terminal size. What we could do however is: Currently there are three structs the user could interact with: (Terminal, Cursor, Color). Wat we can say is to have this functionality as you said it implemented for each of these structs this would not change the current design and the storing and restoring the terminal state could be easily implemented in the current structs. The storing of the state starts at the `init()` method and we could implement the rust [destructor]( https://doc.rust-lang.org/beta/nomicon/destructors.html) wits will revert all those changes. And the client will not even notice it that this functionality is implemented. * I think I covered this point at #1 * Why check every call, when the client will change the cursor it will get an struct wits has those cursor related actions implemented. So in the `init()` method of this struct we can just check for once, if ANSI codes are enabled. Another thought I had to store the state of the terminal and easily restore this state is by using the `Command Design pattern`. I don‚Äôt know if you are familiar with design patterns. It is something from OOP languages. But this pattern could be implemented for rust as well. Shortly said what this pattern can do is having an command (struct) like `set terminal size` this command has two methods one to `execute` that command and one to `revert` that command. So every time an user executes one command that command the command will be executed and be pushed into some vector. And when the struct gets destructed we can loop true that vector of commands and just call all the revert method. At the end all the actions an user did can be rewinded. We actually don‚Äôt have to do this to all the actions an client can perform on this library but we only have to wrap these actions that change the core state of the terminal into an command. Does this answers your questions? Shortly said currently the core state will not be changed so I did not have to think about this issue before. But when this state will be changed we can solve it like discussed above. 
Thanks for the reaction and clarifying your question. First of lets start reacting to your two first observations: 1. `It seems what you're doing is relying on enabling ANSI support for Windows terminals`. I have the functionality to enable `ANSI code` support for windows, but I am currently not using it. What I do, is to check whether the current platform is `Windows` than use `WINAPI `and if the current platform is `UNIX` than use `ANSI codes`. Probably not an good idea I just thought because CMD is not the only console for windows there are a ton of greater 3th party terminals for windows that support ANSI. The question I however have for you is: you say `it's far easier to enable ANSI support in Windows than try to make a ton of platform-specific logic`. I agree but then what to do with terminals like windows .. 7 an 8. Should I just completely ignore those terminals and just focus on ANSI escape codes? and windows 10 terminal by enabling ANSI? Is it not just for those terminals that this library does not forget? 2. Good point I agree that the library should clear its mess up after some changes to the core state of the terminal. Actually I did not think of that during the development of this library. Here are two points how I address this issue: - As I said in other questions I currently have not an thing like clearing up the changes made my library. The only thing that gets cleared up are the colors and styles like attributes(Bold, Italic, underlined). This is done directly after something with colors or style gets print to the terminal. - Currently I actually do nothing with the core state of the terminal in both windows and UNIX systems. As mentioned bij #1 there is one function that modifies the state wits is `enable_ansi()` method for windows, but this functions is not used. So when the process of the client stops nothing is really changed to the core state. Than you got these points where you had suggestions how you would solve this practically problem for restoring the state of the terminal back to its original state. 1. `Expose the entire library API through a struct (i.e., Terminal)`. This would be an idea but the thing I don‚Äôt like about this approached is that all the library‚Äôs modules will be loaded for every time an user wants to have the terminal size. What we could do however is: Currently there are three structs the user could interact with: (Terminal, Cursor, Color). Wat we can say is to have this functionality as you said it implemented for each of these structs this would not change the current design and the storing and restoring the terminal state could be easily implemented in the current structs. The storing of the state starts at the `init()` method and we could implement the rust [destructor]( https://doc.rust-lang.org/beta/nomicon/destructors.html) wits will revert all those changes. And the client will not even notice it that this functionality is implemented. 2. I think I covered this point at #1 3. Why check every call? When the client will change the cursor it will get an struct wits has those cursor related actions implemented. So in the `init()` method of this struct we can just check for once, if ANSI codes are enabled. Another thought I had to store the state of the terminal and easily restore this state is by using the `Command Design pattern`. I don‚Äôt know if you are familiar with design patterns. It is something from OOP languages. But this pattern could be implemented for rust as well. Shortly said what this pattern can do is having an command (struct) like `set terminal size` this command has two methods one to `execute` that command and one to `revert` that command. So every time an user executes one command that command the command will be executed and be pushed into some vector. And when the struct gets destructed we can loop true that vector of commands and just call all the revert method. At the end all the actions an user did can be rewinded. We actually don‚Äôt have to do this to all the actions an client can perform on this library but we only have to wrap these actions that change the core state of the terminal into an command. Does this answers your questions? Shortly said currently the core state will not be changed so I did not have to think about this issue before. But when this state will be changed we can solve it like discussed above. 
Thanks for the reaction and clarifying your question. First of lets start reacting to your two first observations: 1. `It seems what you're doing is relying on enabling ANSI support for Windows terminals`. I have the functionality to enable `ANSI code` support for windows, but I am currently not using it. What I do, is to check whether the current platform is `Windows` than use `WINAPI `and if the current platform is `UNIX` than use `ANSI codes`. Probably not an good idea I just thought because CMD is not the only console for windows there are a ton of greater 3th party terminals for windows that support ANSI. The question I however have for you is: you say `it's far easier to enable ANSI support in Windows than try to make a ton of platform-specific logic`. I agree but then what to do with terminals like windows .. 7 an 8. Should I just completely ignore those terminals and just focus on ANSI escape codes? and windows 10 terminal by enabling ANSI? Is it not just for those terminals that this library does not forget? 2. Good point I agree that the library should clear its mess up after some changes to the core state of the terminal. Actually I did not think of that during the development of this library. Here are two points how I address this issue: - As I said in other questions I currently have not an thing like clearing up the changes made my library. The only thing that gets cleared up are the colors and styles like attributes(Bold, Italic, underlined). This is done directly after something with colors or style gets print to the terminal. - Currently I actually do nothing with the core state of the terminal in both windows and UNIX systems. As mentioned bij #1 there is one function that modifies the state wits is `enable_ansi()` method for windows, but this functions is not used. So when the process of the client stops nothing is really changed to the core state. Than you got these points where you had suggestions how you would solve this practically problem for restoring the state of the terminal back to its original state. 1. `Expose the entire library API through a struct (i.e., Terminal)`. This would be an idea but the thing I don‚Äôt like about this approached is that all the library‚Äôs modules will be loaded for every time an user wants to have the terminal size. What we could do however is: Currently there are three structs the user could interact with: (Terminal, Cursor, Color). Wat we can say is to have this functionality as you said it implemented for each of these structs this would not change the current design and the storing and restoring the terminal state could be easily implemented in the current structs. The storing of the state starts at the `init()` method and we could implement the rust [destructor]( https://doc.rust-lang.org/beta/nomicon/destructors.html) wits will revert all those changes. And the client will not even notice it that this functionality is implemented. 2. I think I covered this point at #1 3. Why check every call? When the client will change the cursor it will get an struct wits has those cursor related actions implemented. So in the `init()` method of this struct we can just check for once, if ANSI codes are enabled. Another thought I had to store the state of the terminal and easily restore this state is by using the `Command Design pattern`. I don‚Äôt know if you are familiar with design patterns. It is something from OOP languages. But this pattern could be implemented for rust as well. Shortly said what this pattern can do is having an command (struct) like `set terminal size` this command has two methods one to `execute` that command and one to `revert` that command. So every time an user executes one command that command the command will be executed and be pushed into some vector. And when the struct gets destructed we can loop true that vector of commands and just call all the revert method. At the end all the actions an user did can be rewinded. We actually don‚Äôt have to do this to all the actions an client can perform on this library but we only have to wrap these actions that change the core state of the terminal into an command. Does this answers your questions? Shortly said currently the core state will not be changed so I did not have to think about this issue before. But when this state will be changed we can solve it like discussed above. 
So now we can port some of the flash games to Rust that were popular back in the day?
Oh, I'm a super Rust newbie, not sure I can offer a ton there. Learn to play with rust libsecp256k1 wrapper, and rust bitcoin? That's what I'm using this crate for(I needed "-1" to do some quick EC math). As far as books, I would stay away from books written too long ago. Lots of stuff have happened for crypto design since then to remove footguns. I have this book from my alma mater which is pretty decent: https://www.amazon.com/Introduction-Modern-Cryptography-Principles-Protocols/dp/1584885513
Mainly related to stdweb crate, would try to help to improve there.
This is a really cool tool for checking whether versions of a crate are semver compatible. Inokentiy Babushkin did most of the work as part of Google Summer of Code last year, mentored by eddyb. Moving to the nursery is a step towards making it an officially supported tool. Try it out and let us know what you think!
Apparently? I have to admit, this is pretty surprising to me too! I need some time to investigate properly...
interesting to see that the 'lazy expression template' idea does translate across , but so far I've personally tended to stick to named functions which I prefer r.e. the way borrows/values work (vs C++'s reference args). I dont like writing things like ```accum+= &amp;a * &amp;b``` .. (purely r.e. intuition built up from C++); I can accept seeing '&amp;' prefixes all over the place as named function args better. The downside to that is people wont agree on the best names (my own favourites are inspired by ISA's of vector units i've seen over the years lol), the plus side is chaining is sometimes a pleasing way to write expressions
Ah, that might not have been clear. There will be exactly zero or one non-required fields (maximum of three fields on any single instance).
Extremely good article! I'll definitely be bookmarking this. One nitpick: about 1/4 of the way down, there's a type in &gt;It's pretty *rate* that structs have more then one lifetime binding. Indeed, I've myself never used one that has. (emphasis mine)
You should make the upper band larger so that the text doesn't need to be squeezed (even worse, overlapping the shadows).
I‚Äôm sorry you didn‚Äôt like my post. I wasn‚Äôt trying to spread FUD about Ruby - the only reason I spent so long talking about Ruby was to qualify that its initial shocking looking decline isn‚Äôt as bad as it seems: it has still grown its user base 3x over the time frame I was looking at, and was over-represented early on in the initial GitHub user base. As another data-point, [Stack overflow trends](https://insights.stackoverflow.com/trends?tags=ruby%2Ctypescript) only shows a 40% drop in Ruby usage, and that drop doesn‚Äôt start until 2014. I probably could have been clearer in this section. For Jupyter Notebooks, I‚Äôm using the language classification as provided by GitHub - and the GitHub language identifier includes Jupyter as a language. I don‚Äôt consider Jupyter a programming language myself, and might merge Jupyter/Python in the future.
I would advise against choochoo. Have different connotations in Spanish-speaker countries.
I like the design itself, though I'm not sure how much I like having it on the subreddit in place of the previous logo.
Sure, the design is cool! Looks like a tattoo or something. I just think it is out of place here; that it looks non-scientific. But I don‚Äôt want to complain ‚Äì this is just my first impression.
&gt; there's a type in [Muphry's Law](https://en.wikipedia.org/wiki/Muphry's_law) strikes again
Uh, another one, awesome! It is like [git-dit](https://github.com/neithernut/git-dit) (I am co-author of git-dit) but builds its own object store instead of using git for that, do I understand that correctly?
&gt; And, again, the Tokio / futures-rs model is significantly different than that of other async libs, so you can't say "this worked in this other library, so it is what Tokio should do". I think I missed something. I heard you say tokio separates these concerns to allow you to pick your own concurrency model; is that what you're referring to? or some other difference? It's true that many other libraries today only support a single concurrency model, but I also think they prototyped / experimented with several models before choosing that one. (For example, that gRPC link described a variation they abandoned in favor of the thing they have now.) Is there some reason to believe those experiments aren't valid on Rust/tokio? Or that there's a better model no one considered? I'm sure eventually this will be settled with benchmarks, but my prior is that the concurrency model I'm used to would work well in Rust code as well, and yours is the opposite. I'm not sure why. &gt; That said, the building blocks that tokio provide lets you reuse both the tokio library and a thread pool library and build something similar to the epoll set / island etc... and that is the point, we're building a foundation. And, if I am wrong, since we have a solid foundation, changing the model would be pretty easy. That's nice to hear, thanks. I admittedly know quite little about tokio so far. I've written applications in C++ that cares about this stuff, and hyper-based (indirectly using tokio) stuff in Rust that doesn't care about this stuff. Not sure when (or even if) I'll write a Rust application that does really high-performance, low-latency, IO-driven stuff.
**Royal Society** The President, Council and Fellows of the Royal Society of London for Improving Natural Knowledge, commonly known as the Royal Society, is a learned society. Founded in November 1660, it was granted a royal charter by King Charles II as "The Royal Society". The Society is the United Kingdom's and Commonwealth of Nations' Academy of Sciences and fulfils a number of roles: promoting science and its benefits, recognising excellence in science, supporting outstanding science, providing scientific advice for policy, fostering international and global co-operation, education and public engagement. The society is governed by its Council, which is chaired by the Society's President, according to a set of statutes and standing orders. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
The speed improvement seems to be an artificial problem where either from a new version of `rustc` or some innocent-looking patch caused up to a 10x speed regression. Fixing that causes the original code to run significantly faster (up to 2x) than the `Faster` variant. Though it was a very helpful patch for finding this issue, I'm most likely going to suggest dropping the change once I get confirmation. 
even 28th nightly does not work for me, win10
bad bot
SIMD is Now EasY
*Darn it* vitiral, I am not a bad *darn* bot... :c *Beep boop*, I am actually a cool bot. *** ^^Darn ^^Counter: ^^52649
I've used this before when I was writing a compatibility layer for an old library and it was absolutely fantastic. Was also really interesting to see how easy it is to unintentionally break downstream code! I remember originally digging it up (after remembering it being a gsoc project) and was sad because it's such a useful idea with seemingly little traction after completion. Glad to see it's becoming more official.
Yes, you do understand it correctly! I am aware of git-dit, but one of the reasons why I started SIT was that I didn't want to rely on git or any other SCM. SIT works fine even without one.
Hi. Great article. I learned a few things from. A minor thing is the use of 'then' when you actually mean 'than'. I found it very distracting. Besides that, it was quite good. 
Feed it C.
The incredible bias here is that Ruby is the first language of GitHub. Their first big project was Rails. At some time, the only community purely happening at GitHub was Ruby. Quite some [notable Rubyists are have 4 digit IDs](https://api.github.com/users/dhh). It can only go downhill from there. What is happening in this graph is the whole world moving to GitHub, expanding its size extremely and fixing the bias that GitHub had. You can‚Äôt read anything notable in that graph. They even admit that factor and still draw that conclusion. 
You're forgetting Puppet, Chef and many deployment tools are written in Ruby. There's quite some Ruby happening outside of Rails.
[removed]
&gt; As another data-point, Stack overflow trends only shows a 40% drop in Ruby usage, and that drop doesn‚Äôt start until 2014. I probably could have been clearer in this section. As another datapoint, if you map C to Ruby, they are following similar trends. I'd hardly call C a struggling language. https://insights.stackoverflow.com/trends?tags=ruby%2Ctypescript%2Cjava%2Cc You are running into a similar trap as in the blog post: relative numbers are a very bad indicator in growing ecosystems. Especially if they are as gigantic as "all of programming". Yes, as sharply as I have to put it, you are spreading FUD. 
I was core maintainer of Padrino for a while and we had 10 Fortune 500 companies using us. I was able to run my consulting business off it in Germany alone. It's rare that people talk about it, but we still have happy users with that. Also, Hanami is becoming popular.
awesome, I hope to contribute to servo when I have a better handle on rust
Made a pass through it, hope it improved. Thanks for pointing it out. Yeah, it's an annoying habit I still have, but English is still my second language ;).
Yep that's just some generic crab. Eye stalks are creepy.
MSSQL from 2012+ has the concept of schemas, just like PG. That's what the `dbo.` is at the front of most tables. It also has the concept of shareable sequences, which I personally prefer to identity columns. With regards to this, this looks like something that you could easily implement on the DB level using a technique such as the one [here](https://blog.2ndquadrant.com/application-users-vs-row-level-security/), allowing you to use the same security model for other db clients as well (potentially reporting or ETL jobs as needed).
ctrl+L crtl+L ctrl+L... thank you for doing the thing. you are wonderful.
7pm GMT happens when this comment is 20 hours and 3 minutes old. You can find the live countdown here: https://countle.com/9ixM128575 --- I'm a bot, if you want to send feedback, please comment below or send a PM.
/u/diggsey is working on having [gl-rs](https://github.com/brendanzab/gl-rs/pull/447) support WebGL through [stdweb](https://github.com/koute/stdweb)-based interop. It will hopefully be merged in somewhat soon (it's currently blocked on releasing new `stdweb`, which I plan to do soon)
(*shameless plug*) I've also wrote an experimental [plugin](https://github.com/koute/parcel-plugin-cargo-web) which allows the use of [stdweb](https://github.com/koute/stdweb) (and by extension - [the `js!` macro](https://docs.rs/stdweb/0.3.0/stdweb/macro.js.html)) from Parcel projects, and in the next few days I plan to push a `js_export!` macro which will make it possible to export arbitrary functions from Rust, not just those using WASM-native types.
The Gothic font is an anachronism at best :D
It's been a while since I posted about Rust debugging here. I've mostly been working on improving the DWARF that's generated for Rust enums. The needed LLVM changes are close to landing, and after that I can back-port the changes to rust-llvm; update the Rust compiler patch; and finally update the gdb patches. I've also been back to working on the Rust lldb plugin, talking with upstream about how best to approach expression parsing. Finally, I'm talking at FOSDEM about Rust debugging, so I'm writing my slides for that. If you're at FOSDEM, I'd love to meet up.
Cool design! What does the tree in the middle represent?
I do this a lot at work where I write some libraries in rust that are called from c++/python/c# on windows and linux. I have the following setup: **Inside Rust:** *unit test - regular old method unit tests in rust *integration test 1 - I run integration tests in rust itself using this layout (https://doc.rust-lang.org/book/second-edition/ch11-03-test-organization.html#integration-tests) **Windows** I have a powershell script which builds a dll, moves it into two directories and runs pre-compiled c# and c++ integration tests. I don't ever call this library from python on windows, so I don't do it **WSL** I have the windows subsystem for linux setup, and it's awesome. I have a bash script which compiles into a shared object, moves into the c++ and python directories, and again runs integration tests, checking return codes for success and failure. 
A recent thread on the subreddit was about a graphic with 3 latin words describing Rust's strengths. One was [Celerius](https://worldofdictionary.com/dict/latin-english/meaning/celere), kind of appropriate? :P u/vvimpcrvsh u/boscop u/Regimardyl
Unfortunately it is very painful to attempt to navigate issues (especially if you don't have the data locally meaning you can grep over them) in this form. Nonetheless I'll be watching the development of this! :)
Love the idea of a new website with emphasis on tooling!
Nice to read the roadmap of 2018,this make the summary of all blogs? Good luck to rust 2018.
It is a very early preview.. and the commands that are available are limited and more like "plumbing" ones in Git, so yes, it's not perfect for sure :) Just to understand your experience better, how were you trying to navigate issues? Were you trying to navigate them on GitHub in .sit?
I fully support your right to license your code as you please, but rejecting another library (which is explicitly licensed so that liberally licensed projects can use it) because it does not agree with your ideas about copyright does not seem very practical to me. Personally, I think the strength of the open source community outweighs individual differences over licenses or copyright vs. copyleft. This only causes unnecessary divisions.
&gt; I'd be somewhat leery of such. Reasonable people can have disagreements about what is and isn't a semver breaking change. There's an RFC on the matter of evolving APIs, but even that exposes some not technically semver rough edges. The tool could default to only blocking unambiguous breaking changes (such as removing a public function) and only warn on the others. This is something that e.g. the code analysis in jetbrains' IDEs does very successfully for normal errors &gt; Another concern is the practice of providing technically public, yet undocumented APIs as "essentially private" when working with closely related crates. This is touched on in a recent New Rustacean podcast interview with /u/rabidferret and diesel. This is a practice I have no issue with, and use myself, but would cause such a tool to reject it. I assume this could be fixed by ignoring doc(hidden) APIs.
Is it just me, or is there red bleeding around the modified banners? That'll happen if you've adjusted the stroke path and not the fill path. Simplest fix would be to delete the fill path, move the stroke path to the bottom of the group, then set the fill on that. Or stick the modified SVG somewhere and I'll clean it up.
Yes I was browsing the file structure via GitHub's web UI. Are you planning to build a GUI or TUI for SIT? If not then a GUI/TUI could of course be a community/third-party tool (akin to `tig`). While I like CLI applications just as much as the next person (git cli all the way) I think browsing issues (and having discussions, labeling, etc) is more intuitive through a UI.
A raw summary is [here](https://paper.dropbox.com/doc/aturons-Rust2018-summaries-Xq57JQv6EkDr0nF5kLsWe). This RFC takes into account the various arguments that people made, but it certainly doesn't include *everything* that was proposed (which would be impossible to do in one year). Part of the goal of a roadmap is to *focus* our work, which does mean turning down some opportunities. For example, not pushing for stable const generics in 2018. The RFC represents the core team's best judgment on where to put our focus this year, based on the blog posts we read (and wrote!), the 2017 survey, discussions with production users, and project management considerations.
Do you have an exemple to use parcel with a cargo project ?
I was thinking along the lines of rustfmt/clippy. Where there's a common standard, but is syntactically salted with something like #[ignore_semver] or "cargo publish --ignore_semver_compat" when it doesn't fit right. The optimist in me thinks that semver should be easier than syntax to standardize on. Also that a sub-optimal semver standard is probably better than none. Changes to such a standard couldn't break backwards compatibility, so it should be easier to correct course if there's faults. Though it has to be "correct" enough for a good majority of users. So ideally, to me, this library would designed to facilitate a cargo publish use case. Then becomes an opt-in option and switching to opt-out when the consensus is that it's good enough, if that's possible. But I maybe underestimating complications since I currently don't have users of my libraries beyond myself. Also I've haven't had too many semver issues with the current wild west solution. But when there's things like the orphan rule to avoid breaking changes, it seems that a semver check should also be justifiable.
The only defense I can offer is that I did a search for what typefaces were used in heraldry, and found a website that listed Old English. It was past midnight, so I just went with a free blackletter I found.
&gt; I think you showed far too much respect for the original art It is not for one such as I to decide which lines must stay and which must change. If the creator had not wanted those lines there, surely they would not have drawn them that way! To alter the work is heresy, blasphemy! And now I can see there *are* red strokes visible. Some of the shapes are "split" with separate stroke and fill shapes. As I said in my other comment, I'm happy to fix them up if you want.
&gt; ctrl+L crtl+L ctrl+L... ... select the address bar?
The text is a result of not being able to find a reference typeface that *did* fit what was in the original image, and people suggesting the actual content should change. Also, I know effectively nothing about heraldry. /u/QEDunham (author of the original image) appears relatively content with the text, but I'm happy to make alterations if desired.
Not everything flowery and metaphorical is religious. It's an (as far as I'm aware) unofficial coat of arms.
How rude. He's a crab. He can't help how he looks. *For shame.*
QEDunham drew a thing. Some people liked it. I decided to trace it. Look yonder to the sidebar, truthseeker.
thus the ability to opt out? most libraries are relatively simple, and just following the tool will lead to reasonable releases. advanced libraries or opinionated developers could opt out.
Ok, I get now what are you saying. LMDB actually abstract the storage. I could be happy with anything that allow me to work well (and enough low-level, so no sqlite) with relational data, because I wanna implement the DB as much as possible. I could do also the storage, but now I need to delve even more in how do that. 
&gt; What happens when it fails? [The program panics with the given message.](https://doc.rust-lang.org/std/option/enum.Option.html#panics) &gt; Does expect return File? [`File::open`](https://doc.rust-lang.org/std/fs/struct.File.html#method.open) returns an `io::Result&lt;File&gt;`. [`io::Result&lt;T&gt;`](https://doc.rust-lang.org/std/io/type.Result.html) is really `Result&lt;T, io::Error&gt;`. [`Result&lt;T, E&gt;::ok`](https://doc.rust-lang.org/std/result/enum.Result.html#method.ok) converts it into an `Option&lt;T&gt;`. [`Option&lt;T&gt;::expect`](https://doc.rust-lang.org/std/option/enum.Option.html#method.expect) returns `T`. So yes. &gt; and why is `.ok()` even in there? Because `Result` didn't always have an `expect` method like it does today. &gt; Maybe unwrap asserts if it fails. [`Result::unwrap`](https://doc.rust-lang.org/std/result/enum.Result.html#panics) panics. Asserts panic when they fail. &gt; Is there special syntax to do this? No. Rust generally abhors special syntax when existing syntax does fine.
That's not what this translates to. My blog post goes over this in a lot more detail, but else let Some(val) = opt { bail!() } // Other code Is equivalent to if let Some(val) = opt { // Other code } else { bail!() }
Great work, looks like it's going to be a really good year for rust. I'm really exited about the wasm focus
Is there a mode to just suggest what the new version number should be and summarize the changes that lead to that suggestion?
What is the hexagon badge on the top right side? It looks cool.
&gt; No. Rust generally abhors special syntax when existing syntax does fine. Well, aside from `let i: u32 = 4` or `let i = 4u32`, which could be considered special syntaxes for the cases where applying `.parse()` to a string literal and discarding the literal would actually have a use.
&gt; generally
To be clear, I think your comment misrepresents my position and I'm not interested in discussing it further.
Yes, I am planning a GUI and maybe even a TUI. Just trying to think what's the best way to allow customizing reports and views of issues. On the web side of things, I am trying to narrow down on a stack/arch that doesn't require transpilation, bundling, etc. On the TUI side, using something like `cursive` create is an interesting idea but customizing it gets even harder. The need for customization comes from the fact that record types are open-ended and different projects might want to implement handlers and renderers for their own stuff. By the way, I just merged in a patch that simplifies querying issues. Makes it a bit easier! Check out the updated README. 
I know that MSSQL supports multiple schemas, although I've never read of tenant-schema being used in mssql. Regardless, I'm still interested in finding a type level way to ensure that diesel queries correctly set the filters - this could also be used for other situations as well. The fact that the filters are encoded into the type of the query makes me hopeful that this would somehow be doable.
Also, alternative "community" GUIs would be nice to have, too.
&gt; I was thinking along the lines of rustfmt/clippy. Right, but these are both currently opt-in. I was speaking purely on making something like, "block `cargo publish` if semver fails" is something that I'm against. I agree it'd be great in the simple case and would (hypothetically) make the ecosystem better, but it's rarely that clear and simple.
&gt; On the web side of things, I am trying to narrow down on a stack/arch that doesn't require transpilation, bundling, etc Long term I think you'll end up finding that a lot more painful than just biting the bullet and learning the bundling tools. If you want a newer option with less configuration required then you may wish to look at Parcel.
Thanks for the mention /u/aturon! /u/killercup and I are working on the CLI ergonomics under his crate [quicli](https://github.com/killercup/quicli) as well as the [ergo crate ecosystem](https://github.com/rust-crates/ergo). Check them out!
This is probably the biggest issue with this possible syntax. We're used to reading `else` as a conjunction between two phrases rather than a start to a statement. I don't know. Maybe we'd get used to it the same way `if let` and `?` got quickly assimilated into best-practice code. But unfortunately the refutable let binding sits in a niche that doesn't see too many real use cases, as Rust has an ecosystem of providing great higher-level combinators.
I haven't read your blog post yet and I'm not sure if I understand the problem correctly, but I spontaneously had an idea for another syntax. What about: if let Some(_) != opt { bail!() }
FWIW [wyrm](https://github.com/maciejkula/wyrm) uses `stdsimd` at the moment, and I'll be very happy to switch to `faster`. Bear in mind, though, that most mature neural network libraries run on GPUs for speed, and GPU-based SIMT is something completely separate from SIMD.
I'm surprised the Haskell logo isn't hidden away in there somewhere
The point is the binding provided. (I'm using `Option` here for simplicity though it provides great combinators for this kind of case). Consider: if let Some(a) = make_a() { if let Some(b) = make_b(a) { if let Some(c) = make_c(b) { return Some(foobar(c)); } else { log!("failed to make c"); return Err(ContextC); } } else { log!("failed to make b"); return Err(ContextB); } else { log!("failed to make a"); return Err(ContextA); } Versus: else let Some(a) = make_a() { log!("failed to make a"); return Err(ContextA); } else let Some(b) = make_b(a) { log!("failed to make b"); return Err(ContextB); } else let Some(c) = make_c(b) { log!("failed to make c"); return Err(ContextC); } return Some(foobar(c));
That wouldn't compile. The block of a refutable let is required to diverge. The compiler would point else let Some(val) = opt1 { } ^^^ And say that that block has to diverge (return, continue, break, loop, or panic).
Just remember, I also had a fun idea with regards to browsing on GitHub. I think it might be possible to develop a browser extension that would render SIT issues if it detects it's a SIT-enabled repository. Sort of like what ZenHub does. This would likely require having SIT implemented in JavaScript. But maybe it can be compiled from Rust using that wasm target. It'll just need a bit more work on abstracting some pieces of functionality, like ordering records. (That said, it shouldn't be very difficult to reimplement SIT it in pure JavaScript, but I doubt I'd want to maintain it)
Okay, but putting a return between those brackets doesn't solve the problem that this binding is easily overlooked.
I really hope I didn't screw up the Latin...
Rust really aligns with my programming philosophies, even if i don't understand all of it completely. i'm totally invested in its future. i would love to write more rust for more projects.
Ha! Another idea! if let None = opt { bail!() } let Some(val) = opt; The compiler could see that opt must be Some when reaching the let statement.
That's flow typing, and way beyond the scope of what Rust is likely to pick up in the near future. It'd probably also require refinement types and more, as you'd want an `is_some` call to make `let Some(x) = opt` an irrefutable pattern.
Just so I may reach someone else about this so it doesn't get lost: webrender currently leaks memory every frame. To send a command to the webrender API, you have to send a `Transaction` (via `RenderApi::send_transaction()`). That transaction is leaked (somehow). Meaning, if you redraw at 60FPS, you can leak up to 1MB per second, depending on how many transactions you send. I notified a guy on IRC, he could reproduce it - these three lines will crash your machine completely: https://gist.github.com/fschutt/b97941bfdc21c719a67b17e51325e383 (the update step isn't even necessary). I thought this was rather important, so I wanted to post this here, too. 
hmm, I would question whether .parse really "does fine" for that :p with result, methods really do work fine. I mean rust even has the `?` operator now, for when they don't.
Flow typing and refinement types for Rust ‚Ä¶ well I guess I know what my thesis will be about :D I'd like that stuff! The more I read about type systems in the context of Rust, the more I want to dive deeper into Haskell^^ Googling refinement types ‚Äì again ‚Äì brought something about Haskell up.
Would love to. Could you clarify which parts overlap?
I want to join the 81mm mortars so that the game can be far away from the farther distance, and it's more interesting to hope that the mortar is a tank or a green box with a chance to open it.
I'm sorry if I have done so. Let me try to summarize how I understand your position (if this is inaccurate, please correct me): - You agree with the ideals of copyleft, but oppose copyleft in practice because it uses copyright to enforce those ideals. My point was that, personally, I don't think rejecting a copyleft licensed library to support this is worth the division it causes in the open-source community (especially when said library has no impact on the license or IP of one's own code).
It is, but I wasn't sure what the intended message was if that's the case. Besides, you need *at least* four simplifies to really do a decent job, *especially* on paths made by the pencil tool which seems to drop a new node on every damn pixel.
Async/await is not mentioned, is it part of some larger feature or not gonna make it on 2018?
&gt; Between generators and macros 2.0, we will have some support for async/await on stable Rust (possibly using macros, possibly some other way).
Jan 29th on Win10 works for me.
In retrospect, I should have known better than to put up a temporary logo as a goof and then go completely off the grid for two days. Damn you, Latin pedants!!
It's a very similar concept, what you're proposing. Afaik typestate would have conditions that the data would have to match, checked at runtime. Sounds similar to this. 
OK, missed that. Thanks!
Re marketing - have been wondering if it would be useful if I offer to coordinate reaching out to all the friends of Rust companies to compile success stories. This has probably already been taken up by someone but in case it hasn't, I am happy to raise my hand :)
Already doing that, but the writing takes a little time. I'm sure there's help needed, though. Want to send a message to Community@rust-lang.org? :)
I assumed so. Will send an email to see if there is anywhere I can pitch in.
It may even make sense to conpletely rebrand the project as 'rustlint'...
Anyone have this in a more digestible format? 
&gt; We never seem to use the term "lend". Why? It's a perfectly good English word that is much closer to describing what the &amp; operator and other "borrow sites" (lending sites) do. Yes, this discussion comes up from time, but I don't see how introducing new language would help here. &gt; Lifetimes and borrowck are really about answering the question "I loaned this thing elsewhere in the function, is it safe to take it back at this point? Always? Can that safety be statically proven?" This is definitely not the point of the lifetime annotations. Yes, that's the point of borrowck, but this article is intended to give an introduction to the notation. &gt; The definition of "ownership" is also misleading. Rust has shared ownership in std; specifically it has Rc and Arc. No, absolutely not, and that's a common trap. Ownership in Rust means you: * Can mutate the value (even go from immutable to mutable) * Move the value * Drop the value `Rc` and `Arc` provide none of that. `Rc` and `Arc` manage their _handles_ through (exclusive) ownership, but the value is _not_ owned by multiple parties. It's owned by the `Rc` structure that you can get handle to. Uniquely, and that's of crucial importance for the whole system to work. `Rc` and `Arc` also don't allow mutation or immediate destruction of value, which you'd need.
It‚Äôs worse than disabled on surface books.
Are you on the team? Are there plans to improve surface book performance? It‚Äôs worse than disabled 
At that point people might rightly ask why rustc is giving some of the lints and some weird other program is giving the rest. Which is ultimately a fair question.
So many checked boxes, but not mine ;_; But they tell me that my wish could come true with specialization some day anyway.
I made a mistake in update bench for actix. it commits every update to a db, but should commit once per batch.
link to [actix framework](https://github.com/actix/actix-web)
Original tweet: https://twitter.com/pcwalton/status/958201640313536512.
Thanks
I am enamored with text interfaces, so I think this will be a fun space to explore. Extending the elf format to require auto completion metadata might be one way to accomplish this. I have been thinking about a kernel/userspace API because I just got interrupts and syscall instructions to work properly. I'd like to pursue multitasking soon but I think first getting a userspace program loaded from the file system might be in order. Hmm, the NVRAM sounds like it kinda obsoletes some cool OS features like swapping :P
I am not a Mozilla contributor, just been watching the bugs. So I don't know anything specific about Surface book. What problems are you seeing? Graphics issues?
Yeah no problem! It's something we care _a lot_ about, so after you said that I immediately had to go back and check just to make sure :)
Can you talk about what you had to set up to get streaming going? I've wanted to try streaming my coding from time to time but it always sounds intimidating.
Note that this is _not_ the writeup that the community team will also do :).
I mentioned that assertions would be fine too, hence, yeah, this would be fine. The important part is that the public API clearly communicates what is expected and that it fails during development.
Can you fix this in that stage or is it to late? I don't know how the bench life circle is. 
I just submitted PR, but it might be too late for Round 15
Another one: at the end, you write &gt; Taking ownership isn't cheating, through cloning or using Box isn't cheating That should probably say something like &gt; Taking ownership (e.g. through cloning or using Box) isn't cheating PS I really enjoyed the article!
&gt; Taking ownership isn't cheating, through cloning or using Box isn't cheating I don't know where you read that... ;) Just joking, I integrated your suggestion. Thanks! 
Streamers on twitch can set their videos to archive automatically, but the amount of time is kind of pathetic (like 2 weeks for non-partners?). Maybe you already knew that and hence your question. If you're lucky, OP posts their previous broadcasts to somewhere like youtube or turns each video into a highlight. In my experience, those extra steps are a pain. So I wouldn't be surprised if OP doesn't do that.
I haven't streamed programming, but I have done a tiny bit of streaming. The easiest way to get started is to download OBS (Open Broadcast Software) and then look at one of the many tutorials. You'll also want a microphone and possibly a webcam. Also important is the right mindset. At the beginning no one will know about your stream so you'll be lucky if you have even one viewer. If you set a schedule (and stick to it), promote your stream in appropriate places, etc, then over time that should change and you might pickup some regular viewers. You might also find it useful to look at the /r/twitch subreddit.
That's a good point. Maybe _all_ the lints should be moved into a "cargo lint" command, and the regular run/build commands should only display errors and deprecation warnings?
I agree. Nothing about the word "clippy" really makes me think of linting. The main things the name brings to mind is Microsoft's infamous Office mascot, clipboards and video clips - none of which have anything whatsoever to do with linting.
&gt; Which is ultimately a fair question. * rustc gives you lints that are 100% accurate (have no false positives) and apply to all projects, and diagnoses errors. * clippy lints are either not 100% accurate (they might have false positives) and in many cases diagnose style issues that you might or might not want to follow (depending on the project), or that apply only to certain style conventions. For example I think that shadowing let is idiomatic rust: let v = Some(3); let v = v.unwrap(); Other people think that shadowing let should be avoided like the plague. Clippy has a lint to detect shadowing let, and you can enable it if you want. But shadowing let is perfectly valid Rust, so `rustc` shouldn't really be bothered with this.
Fair points all. clippy docs should explain this, or the book, or something.
We'll work on a summary, too.
I would expect many developers are used to having a separate linter to their compiler though (RubyCop, JS lint, FxCop, Scalastyle, etc.), so I don't think this will be all that foreign. The compiler gives a few warnings here and there, but by using clippy, you can add many more lints that you don't have to run every time you build.
While I agree that other languages do it, I'm not totally sure we should do the same. I'm on the fence about it.
...or `(0 ..= 23)` once it's stabilized.
On lyon's side, there is ongoing work to add the missing features to lyon's [math primitives](https://docs.rs/lyon_geom/) so that pathfinder can focus on it's core algorithms.
I'm not sure if this a easy question but, any help would be appreciated. I test and do release builds using the trust template. When appveyor is doing a test build everything build fine but when I'm doing a release build 32 bit window (gnu and msvc) fails to build with linking errors. Anyone have any idea why this might be? Link to my project: https://github.com/zethra/servy Failed builds: https://ci.appveyor.com/project/zethra/servy/build/job/x1ikbfnqwi60351o https://ci.appveyor.com/project/zethra/servy/build/job/0w64o1sixestjekn
(I'm on the team) /u/blastuponsometerries summarized it accurately. About the surface book, we don't have specific plans for specific devices, but we do want webrender to run well on modern hardware so WR should get better on the surface book eventually. It'd be great if you could file [an issue](https://github.com/servo/webrender/issues) with as much info about the device as possible and the sites that are particularly bad. People on the issue will then ask you to enable a few options to get profiling data and see what's going on.
&gt; Not to mention that in blackletter, you never, *ever* use all-caps. All blackletter styles are younger than the karolingan minuscle and have stupidly complex (arguably, unreadable) caps I agree. I think it always comes out looking like a tacky tattoo when you use blackletter all-caps. And to reinforce your second point: Carolingian minuscule is still almost a *thousand years* newer than the Roman square style we use today for capital letters. And Blackletter is additionally several hundred years newer than those. Blackletter is a modern writing style. 
I don‚Äôt know which sites are particularly bad. It feels slower on my test of reddit 
[removed]
Any chance [thread_local](https://doc.rust-lang.org/std/macro.thread_local.html) would work for you? I can't test right now if it works on WASM, but I imagine if lazy_static! does, it will too. It implements pretty much what you want, except with extra copies being created if you access it in multiple threads. Since WASM is always single threaded, it should be equivalent to a global variable?
They should be mass-produced and sold at FOSDEM. I‚Äôd totally buy one.
Or, rather, a bit crabby.
Is there any document explaining how to set this up with CI (e.g. travis-CI / cargo-travis) ?
I just enabled vod saving. The vod will be saved for 14 days. 
Totally! Ask me in chat during the show. 
[This](https://github.com/parcel-bundler/parcel/tree/ede8440a2c54d9f005dc489bf94250263a83d936/test/integration/rust-cargo) integration test!
[removed]
This still uses `cargo-web` underneath (you can't really avoid that since Rust itself only generates a bare `.wasm`); the plugin just integrates it into Parcel. (It will install and call `cargo-web` for you.)
Starting to learn rust, for real this time! Maybe a small interpreter later this week or something.
I didn‚Äôt see anything in here about incremental compilation. I know it‚Äôs getting turned on by default in the next stable release, but I also thought there was still some optimization work on the table? Is incremental considered as already shipped for the purpose of this RFC?
&gt; Microsoft's infamous Office mascot rust-clippy is named after this, because lints are _annoying_.
Where is the tokio-minihttp?
I've done something similar. Not direct webgl binding but more of a webgl2 -&gt; opengl es 3 bindings so the gl crate just works. Nothing that's in a public repo i'm afraid. But is looks a bit like this: https://gist.github.com/ksev/0cb14d914f44e081e1d18b19e829ec8a Also i'm not sure how one would go about distributing a crate aimed at wasm that also requires javascript code to work. 
`else` doesn't seem like a good choice, it already exists which is cool but I find the readability extremely low. Swift has a `guard` keyword for this behaviour: guard let Some(a) = make_a() else { // if guard fails, `a` is not in scope } // if guard succeeds, `a` is in scope Alternatively, Perl and Ruby have a `unless` keyword which means the reverse of `let`.
That doesn't sound impossible
... ergosystem!
This looks great! There are two tui crates I enjoy using that provide abstractions for various terminal backends - [Cursive](https://github.com/gyscos/Cursive) and [tui-rs](https://github.com/fdehau/tui-rs). It would be cozy for Crossterm to be an option as well.
For people who are interested, things seem to work quite okay on my 2013/2014 MacBookPros (integrated graphics only) with High Sierra, where I'm using WebRender for all my browsing needs now. I'm not quite sure if the performance improvement is noticeable, but at least there are few bugs, and what bugs I've found tend to get resolved in a few days.
&gt; let i: u32 = "4".parse().unwrap_or(0); &gt;I like this example. Is there special syntax to do this? Since Rust does not have nullability (except for raw pointers), there is no syntax to "return something else if a value is null". But `unwrap_or` has the same semantics and is available on both `Option` and `Result`, which are the generally the two ways to indicate an error condition in Rust.
First off: shipping clippy with rustup is a fine thing. I have no problems with that. &gt; maybe one day we will think only about clippy as "rustc's warning module I really hope this won't happen. I hugely dislike warnings that have false positives. They give people a "hey you can just ignore warnings" kind of attitude which is not okay. Nor am I a fan of warnings that try to enforce some stylistic choice. I am okay with the `bad_style` warnings currently in `rustc` though. So I do avoid clippy warnings. I don't say that clippy is bad or you should avoid it. If you use clippy and are happy with it, then I congratulate you and wish you lots of fun with it. I just don't want clippy to be integrated into the compiler or something, because then people like me who can't stand false positives or being told to follow some from my perspective arbitrary stylistic choice. If clippy became "Rust's lint module", newcomers might think "wow, rust does give you pretty shitty warnings". I had *precisely* the opposite experience: rust gave me *wonderful* warnings that were all useful. I don't want to be forced to use clippy just because rustc moved all its warnings to clippy. So we should continue to keep the two things separate and also market them separately: rustc with perfectly reasonable, well working lints on one hand. clippy with possibly buggy lints or ones that enforce arbitrary stylistic choices as well as "low impact" lints that do noting more useful than suggesting you to use syntactic sugar instead of the current implementation. I don't really like to rant against clippy. Many people like it and I don't want to hurt your feelings. But please also don't hurt mine and make it part of the compiler or move the reasonable rustc lints to clippy or something.
I agree. Being obvious is a virtue.
enforcing sane defaults mean opt out. if it stops you from publishing, you can always force it to publish. If it is opt in and you forget to opt in, you *can‚Äôt unpublish* that SemVer-broken release. one of these is worse than the other.
One thing I don't see is increasing commercial backing. If some other companies could be persuaded to sponsor some of the work it would help not just with the implementation work but with mindshare too.
I‚Ä¶ think we agree but for your misusing of the terms opt-in and opt-out? Opt-out publish means you have to "opt out of" publishing, meaning publishing is the default. Opt-in publish means you have to "opt in" publishing, meaning the default is not to publish.
in my vocab here, opt out means opting out of the use of the tool. opt in means opting into the use of the tool. but, gotcha.
Ahhh I see, you're considering semver as a tool which would be invoked by default when publishing a crate yes? And thus one would have to opt-out to bypass semver's checking in order to publish a crate it disagrees with?
&gt;epochalypse cometh brilliant! 
That's what Roman Latin inscriptions normally look like. Latin was used for centuries after the Romans. For example, there is [at least one well-known book](https://en.wikipedia.org/wiki/Gutenberg_Bible) which has Latin written in blackletter script. 
I think that XML libraries are missing. We still don't have a full-blown XML parser/tree. This is my biggest pain (I wrote an XML parser by myself already). Also, 2d graphic library is missing (pathfinder?).
&gt; I hugely dislike warnings that have false positives. Warnings with false positives wouldn't be enabled by default, in the same way that they are currently not enabled in clippy by default. Or what am I missing?
I don't see how your example could be considered "special syntax" in this case. It's just showcasing the interaction between two very different pieces of syntax meant to achieve very different things. The fact that it results in two ways to write the same thing is a side effect.
Marketing makes a tool better when its quality depends on its users. Programming languages are such a tool (libraries, bindings, devtools, documentation, blog posts, teachers, ...).
I'm sure it can, but I certainly don't agree that that's an absolute. Altering a tool for marketability is certainly no guarantee for quality.
The `clippy.toml` file can control certain properties of the warnings, but currently it cannot be used to list warnings to enable/disable.
FWIW clippy's false positive problem is much less severe now. It's only a couple lints with issues, and we could just disable them. The intent of clippy is not to be a place for "false positive-laden lints", it's for lints which may be slightly more opinionated. This does mean that the recommended usage of clippy is with liberal disabling of lints. _This_ is the reason it's not in the compiler, not "the lints are buggy".
&gt; Edit: This seems to work: `valgrind --error-exitcode=23 --leak-check=full build/tests` You can even create a Rust wrapper in `tests/tests.rs` that automatically runs this for you every time somebody types `cargo test`.
I am working on [azul](https://github.com/maps4print/azul), my GUI framework. I've managed to get the hit testing to work so that I can invoke callback functions from the hit test. I also minimized the amount of work and got the performance from 20% CPU to 1-5% CPU and the memory usage down from 40MB to 25MB. I prepared the stuff for creating the layout, i.e. converting the css constraints (such as `padding: 5px`) to cassowary constraints. I'm creating a dialect of CSS because I dislike some aspects of CSS (like centering something). If that's successful, I can work on integrating it in my PDF editing tool, which is why I made that library in the first place. 
My user experience is hugely dependent on having a rich ecosystem of high-quality plug-ins. This is something I've found Vim to be hugely lacking in, compared to e.g. Emacs, Atom, and VS Code.
Ancient Romans had really nice typography.
&gt; Honestly I'd recommend just taking the hit of two words per object and use Rc rather than reinventing it The problem is that the actual data structure is a b-tree, so the memory cost will be greater that just a few words per copy. I also don't want to incur the costs of cache misses and atomics.
I don't doubt that you are missing whatever functionality you want, or that it's difficult to write plugins (which I think is one of the main goals that NeoVim is trying to address), but I've never heard anyone complain about a shortage of plugins in the vim ecosystem before. vim.org has over 5500 plugins published on it right now. Who knows how many more just exist on GitHub. I personally use 29 plugins in my own vimrc.
Since nical works at mozilla gfx, I guess it would be quite awkward if they couldn't collaborate together with pcwalton ;)
Just like the Office mascot, Clippy provides suggestions about what you might want to do, or what you might have meant. The Office Clippy was notorious for being annoying and offering lots of suggestions that really didn't help, so Rust's Clippy is a tongue in cheek reference to that (though of course with the intent of actually being helpful).
I use 14! But none of them is a functioning semantic autocomplete for any language. ;_; (I have racer in there but it doesn't work and I need to figure out why...)
That's an interesting way of looking at the topic! I never viewed ownership of a variable/etc from a "get off my lawn" viewpoint. I can absolutely see how that's confusing. Someone explained ownership in Rust as "who's holding the ball" where one function can "intercept" the ball for a short time. It stuck nicely. 
Haskell, OCaml, Cyclone.
Right! Although be prepared; it is time consuming not only to build the compiler but also trying to figure out what `bcx`, `ccx`, `hcx` and `tcx` (etc) is and how to use them to do what you want. 
The Const Observer?
[See this comment](https://redd.it/7tkiuv).
it in different result set. and it is faster than hype and actix
&gt; I'll definitely write a follow-up Yes please, and thanks!
Well, some parts of the ergonomics initiative would fall in that scope for me, but those are old arguments. My concern here is with the mindset of Rust as a product. Conflating epochs and marketing is an example of that for me. This goes into a direction where new epochs are considered positive, whereas I personally would like to see them only used when absolutely necessary.
I can agree with you that I would have loved to see a new epoch with ecosystem-changing features like const generics, specialisation, GAT. For me personally, const generics are *the single biggest* desired feature in Rust. Rust feels crippled without them. I feel like I cannot fully love Rust until const generics are ready. I would have been amazing to have them in the new epoch. However, I also agree with the proposed roadmap RFC that these features are a lot of work and it is not realistic to expect them to be stabilised this year. It might make sense for that to be the next epoch for Rust. This year: release the first post-1.0 epoch: Rust with impl Trait, NLL, macros 2.0, generators, `?`, all the other things that are almost ready now and can be stabilised soon. The next epoch will then be const generics + specialisation + GAT.
Many XML libraries already exist -- have you seen [`choose-your-xml-rs`](https://github.com/RazrFalcon/choose-your-xml-rs)? For the 2D graphics library, it seems there are plenty of crates [with this crates.io search](https://crates.io/search?q=2D%20rendering): * [`inox`](https://crates.io/crates/inox) * [`ggez`](https://crates.io/crates/ggez) * [`lyon`](https://crates.io/crates/lyon) * [`gate`](https://crates.io/crates/gate) Perhaps you've tried them and found a use case that wasn't fulfilled yet? I'd love to hear more at [Not-Yet-Awesome Rust](https://github.com/erichdongubler/not-yet-awesome-rust). :)
Why is the builder pattern "out of consideration" here? These seem kind of like query functions, which seem like a fine place for it.
&gt; Builder pattern will be out of consideration Unclear why. Especially with something like [derive_builder](https://docs.rs/derive_builder/0.5.1/derive_builder/). &gt; is there other solution for defalut values? structs implementing the `Default` trait. Basically builders without the builder part, I guess.
What about moving the parameters to an options struct, and then you use the builder pattern on the options? If you have different sets of default options then you could use multiple constructor functions, one for each set. i.e. foo( options::new_artist().limit(10) )`; test( options::new_album() ); In your code you have a `type` as the last argument. I'm also wondering if an enum would make more sense here. Enums can have values within then, and so a different enum value for each type could make sense. 
I found [this one](https://blog.rust-lang.org/2017/12/21/rust-in-2017.html), I think some other people who got involved in lib blitz wrote their own retrospectives as well
Yes, I did too much philosophy and law at university and now almost exclusively associate the term property with control. When I found out that an owner's responsibility is basically `Drop`, I was almost disappointed.
&gt; have you seen choose-your-xml-rs? Yeah! I'm the author. =) And none of the listed libraries are support DTD. Also, they are mostly parser and not a DOM/tree, which is far more complex (namespaces, `xml:space`, etc.). &gt;inox dead &gt;ggez &gt;gate It's not a 2D, software, rendering library. Afaik, you can't use it to generate images. Especially without the hardware support. &gt;lyon It's a tessellation library. 
I think LMDB would be a good choice‚Äîit's most similar to what SQLite uses under the relational layer. You could switch to `sled` some day when it's ready to have a pure Rust thing. I think switching would be fairly easy (relative to, you know, building a relational engine). Good luck with your project!
Hi, Not sure if this is an 'easy' question or even on-topic, but I'd like to know some good practices for multi-crate projects within IntelliJ IDEA. I have 3 crates with following directory structure: ./client ./common ./server ( The `client` and `server` have a dependency on `common`) I can create an IDEA project at the root directory and import the 3 crates as IntelliJ Modules but whenever I issue a command to build, (e.g. by pressing Ctrl+9), all 3 crates are built. If there's an error in the `client` crate, then the result of that compilation run is hidden behind the result of successful compilation runs for the other crates, which is confusing. Can I somehow make it build only the active module or prioritise unsuccessful compilations to show at the bottom? To show what I mean: [image](https://i.imgur.com/Rp89ao5.png) It'd be nice if that first tab was shown automatically if it had errors instead of the third tab hiding it
Communities don't let ya get away with *anything*.
Hmm yes! Thank you.
With impl Trait on argument, you may do something like fn foo(limit: impl Into&lt;Option&lt;usize&gt;&gt;) { let limit = limit.into().unwrap_or(30); ‚Ä¶ }
I do feel `move` is often put too much at the center of things. The compiler will automatically switch a closure to a `move||` closure if the variables are moved, so I rarely need to use it. I feel `move ||` is still more clear than `let x = x;`. The latter will force the compiler to automatically do the former, but `move` makes it very clear you need to move the variables, while `let x = x;` is less than obvious unless you know the way the compiler reasons about things. Is there a reason you prefer `let x = x;` to `move`? I like `move` better because `let x = x;` is indirectly forcing the compiler to move it, and it seems much less clear. Especially to readers who don't know this forces the compiler to move variables "into" the closure.
What is a good, idiomatic way to express ```let i = func() or return```? ```let i = match func() { 0 =&gt; return, x =&gt; x, }``` Or maybe ```let i = func(); if i == 0 { return }```
Reminder that there's at least one popular piece of Rust writing that challenges [ownership as theft](https://sing.stanford.edu/site/publications/levy-plos15-tock.pdf).
thanks for the feedback. Have you ever used threads? This is where it bites me the most.
Ping me here if you get to it, I'd love to somehow integrate it into my crate. Ideally cargo publish would run the tool and warn me if my semver version upgrade is incorrect (and I should update the major version).
`clone!` is a great idea! I might add that as well.
Okay, that means online translators can still learn something
When I wrote the post I still thought that the epoch would be _after_ 2018, not in the middle of 2018. So I can really see that all the work needed for const generics, specialization, and GATs is way too much.
I added `rust-semverver` as flair for you, so it's clearer to people :)
&gt; It is better to give these things time to mature and be developed properly than to rush them. As impatient as I am to get const generics, I'd rather have them right than soon. Coming from C++, I suffer daily from half-features that remain supported forever :(
I don't think there are any 'wrong' questions here, but you may get better answers there https://github.com/intellij-rust/intellij-rust/issues
https://www.reddit.com/r/rust/comments/7tt4zm/certa_celeris_concurrens/
Ah! I see.
Yup, this is basically my current perspective on this epoch and the next.
I would be happy enough with `own!(var1, var2); own_mut!(var3);`, although I'll grant that your desired syntax is even nicer. :-)
That's basically the only place I use `move` closures.
Is something planed to expand the infrastructure? Waiting a week on bors is not very fun.
I'm similarly blocked here, so I think the simplest, if not most ideal, solution would be separated macros. macro_rules! own { ( $( $x:ident ),* ) =&gt; { $( #[allow(unused_variables)] let $x = $x; )* }; } macro_rules! own_mut { ( $( $x:ident ),* ) =&gt; { $( #[allow(unused_mut)] let mut $x = $x; )* }; } macro_rules! own_ref { ( $( $x:ident ),* ) =&gt; { $( #[allow(unused_variables)] let $x = &amp;$x; )* }; } ---- |a, b, c| { own!(a); own_mut!(b); own_ref!(c); } 
seems it is not possible to execute multiple updates at once with diesel, so "updates" benchmark is a weak spot.
I guess I'd argue that taking into account people's expectations given their experience with the rest of the language is pretty important. In C++ and Java, the understanding that exceptions should be reserved for exceptional cases only is pretty well-entrenched, and the implementations have adapted accordingly, pushing quite a lot of complexity onto the exception-handling path to get the best performance out of the success path. In Rust, errors are just values, whose definitions people can often see. Having one type in a category be substantially more expensive than its kin is asking for trouble.
I haven't... yet. still working on that. if any employers are reading this... ;) It's definitely still not common to find Rust jobs, in my experience, but Rust seems to be growing steadily.
I am just ambivalent to the text. Source of most recent mods is http://files.edunham.net/rusthex-0010-prodded.svg and y'all are welcome to fix the fonts if desired.
"simplify the selected curve" in InkScape. Makes the lines smoother and prettier instead of following every detail of my clumsy and slightly shaky inking :)
If I had perfect art skill, sure. But any art I can create in finite time is only a rough approximation of whatever imagery I might have actually been imagining while attempting to realize it :)
I think we don't have them for structs, do we?
&gt; Mods are sleeping, rewrite everything in rust! They could start with Inkscape... So, they could generate an SVG from a rust version of Inkscape... It would be super cool... /s
&gt; An issue with su allowing login as any user by pressing Ctrl+D Well that's not reassuring...
I think that is probably fine for the time being. IF the syntax can ever be extended we can just deprecate `own_mut` and `own_ref` and suggest ppl use `own!(mut x)` I think I'll do `clone!` as well.
Just wanted to say I respect you a lot for your consistent channeling of credit back to the original artist.
Not many people like to write docs, but I do. Mozilla appreciates good docs.
It's not quite the same! At least for non-`Copy` types, `let x = x` will indeed force the closure to move `x`. But the `let` also *consumes* the value when it is called, meaning this closure can only implement `FnOnce`. `move` can still be independently useful for `Fn` or `FnMut` closures that need to take ownership of some data, but only use references to it when called. In this case you'd take ownership so its lifetime is not bound to borrowed captures, so you could return the closure, `Box` it, etc. For example, an iterator over index pairs might look like: (0..n).flat_map(|i| (0..n).map(move |j| (i, j))) Without `move`, `i` will only be borrowed in that `map` closure, and you'll get an error that it doesn't live long enough. `let i = i` won't fix this.
&gt; Having one type in a category be substantially more expensive than its kin is asking for trouble. This just isn't true. Many error types are at least as expensive to construct as `failure::Error`. Errors created by the error-chain crate contain a backtrace which works exactly the same as failure::Error's (except that it can't be controlled by a separate env var, making it worse). A wide variety of errors heap allocate in some or all cases, such as io::Error &amp; json::Error. You don't seem to be very familiar with the state of the ecosystem here.
I don't actually get paid to write Rust, however I am allowed to use Rust in the things I'm paid to write...if that makes sense. I use Rust for small utilities and components. I'm lucky in that my boss gives us freedom to pick the best tool for the job (I don't work on projects that end up getting maintained by others so there isn't a fear of not having a developer who knows X). Rust was a great fit to replace a lot of Python (and some Go) internal utilities we had.
Thanks! I was putting them in the `struct` block, so it did not work...
is there already an issue on the diesel github?
I've interviewed with Google on-site twice in the past 13 months, and no one has ever heard of Rust there, as far as I can tell. I even was asked to write some code demonstrating it on a whiteboard for one interviewer. I don't understand their interview process. I feel like I've adequately demonstrated that I'm a capable developer both times, but whatever. If Google starts using Rust, that would be cool.
i always do that first :)
[Here!](https://play.rust-lang.org/?gist=f4db927fb5c86b842f05c0302702d94d&amp;version=nightly) is an [incremental TT muncher!](https://danielkeep.github.io/tlborm/book/pat-incremental-tt-munchers.html) implementation of your macro.
Google is very conservative when it comes to programming language choice. I've been on onsite interviews three times and when I bring up any modern language like Scala, Rust etc they usually don't know about it or just use it in small, in-house projects. Kotlin I guess is the only exception (no, I don't consider Go to be a modern language). 
wow... now I'm even more confused! let n = 10; println!("{:#?}", Vec::from_iter((0..n) .flat_map(|i| { (0..n).map(|j| { let i = i; (i, j) }) } ))); Fails with --&gt; src/main.rs:18:25 | 17 | (0..n).map(|j| { | --- capture occurs here 18 | let i = i; | ^ borrowed value does not live long enough ... 21 | } | - borrowed value only lives until here 22 | ))); | - borrowed value needs to live until here _why_ does it only make an `FnOnce`? Is it actually not possible to call a function which references scoped variables?
Cool, [works on nightly](https://play.rust-lang.org/?gist=b7642adf3309085ab12db6ee89593d37&amp;version=nightly). It's giving some weird errors that the "variables are never read", but it _does_ seem to be correctly moving them. Thanks!
I met a bunch of cool people at the first RustFest! (I'm a freelancer, and don't work on Rust stuff full time, though.)
It works on stable too, I'm just using the trace_macros! feature to demonstrate that it produces the desired source code. The errors are because the variables were never actually declared -- again I am just demonstrating that the desired source code is generated correctly. You won't see those errors when using it in your suggested context. :-)
In general, to do alternation and sequences (that is, match a sequence where each element can be several different things) you need to use a [tt muncher](https://danielkeep.github.io/tlborm/book/pat-incremental-tt-munchers.html). Hey, wait, come back! The strategy we will use is: 1. Check whether the macro input starts with `mut` (ignore commas). If so, pull `mut $i:ident` out and write out `let mut $i = $i;`. Else, pull out `$i:ident` and write out `let $i = $i;`. 2. Recurse, calling the same macro with the rest of the input (unless the macro input is now empty). This can be accomplished with three (count 'em!) rules. macro_rules! own { // recursion base case (step 2 above): do nothing if there are only commas left ($(,)*) =&gt; {}; // step 1 if the input starts with an optional comma and then `mut` ($(,)* mut $v:ident $($rest:tt)*) =&gt; { let mut $v = $v; own!($($rest)*); // recursion! }; // step 2 if the input starts with an optional comma and then something other than `mut` ($(,)* $v:ident $($rest:tt)*) =&gt; { let $v = $v; own!($($rest)*); // recursion! } } The rules must be in that order since the matching proceeds top to bottom. _First_, quit if we are done, then check for `mut`, otherwise it must be a bare ident (if you swapped the latter two, it would never get to the `mut` rule because `mut` looks like an ident).
Looks like the macro implementation is being worked on, so what's left for me to add here? Oh, right: You should do another one called `owl!`, so `Cow` has a friend.
Like others, I'm just paid to make software and I have the blessing to choose the best language for that software. I mostly code in rust and go.
I said I'll work in C;)
Haha, definitely. The Owl type watches the variable, and if the variable ever gets freed your program panics with the message "whooo whooo done it"
oh, guess I'm showing how much of a noob I am at macros :)
sweet! This is awesome, thanks! I think these examples might be good for the cookbook, since it is a relatively simple (yet complex-ish) use case. Who do I talk to about that... I'm guessing this approach would not be extensible to `own!(&amp;var)` as well?
Is it true that macros 1.1 can be applied only to a struct/enum type? I wanted to add a custom attribute on some impl methods, is it possible? 
Holy derp, it would have helped if I actually looked at your username, eh? I don't know much about the domain of graphics rendering either, so I only have high-level things to respond to. You should definitely see if I'm missing anything for the XML entry and maybe consider adding an entry for 2D graphics there with the use cases that are missing in Not-Yet-Awesome Rust!
I'm also thinking of naming it `take!` instead. Anyone have an opinion? - `own!` is what it _is_, you get ownership - `take!` is what it _does_ and is kind of the opposite of `move` -- `move` is _outside_ the closure whereas `take!` is _inside_ the closure. I prefer `take!` personally, but I'm open to other opinions.
I find that "knowing when to close github and ignore it until Sunday afternoon" to be a key skill. Responding to an issue with "RTFM" is a good indicator that it's time.
An open culture and some convincing is slowly turning my C job into a Rust job.
This is awesome advice. I‚Äôm not maintaining any crates myself, but I know I‚Äôm the type of person that would want to reply to every comment on every issue and discuss every pull request, and I would burn out in short order if I wasn‚Äôt careful.
I'd take the stress in exchange for feeling like I've done something useful for once. 
I just checked GlassWire to see closer what is going on when rust is on.. and look what I found.. https://i.gyazo.com/4cecbdb8e9fc81721438737e7f50ca9e.png Minecraft Server.. if Software's Quake Server? wtf has this to do with rust....
Not great, but what is reassuring is that the project is proactive about finding these kinds of issues and doing it early. Much better than the common approach of speculative mitigations then reactive fixes after release.
Cool trick. I had not realized you could do this. But a warning to OP. This will monomophize , i.e. if you have 3 arguments the compiler will generate upto 2^3 implementations ( depending on how many call types you actually use ) .
[removed]
Maybe so.
This is great advice. I've used all of these techniques to varying degrees. The most important one, IMO, is giving your permissions not to fix everything. My inbox has over 1,000 emails (even though I am constantly triaging it) that I've let slide into my "TODO" list, and it doesn't bother me one bit. If it did, I'd be in a lot of trouble.
Suppose you have something like this: let var: T = ...; let closure = || { let var = var; }; There are basically *three* `var` bindings here: the original outer `var`, the closure's captured `var`, and the inner local `var` when the closure runs. If `T` is not a `Copy` type, then `var` will move from one to the next. Since it's moved from the captured `var` to the local `var`, the closure will no longer be in a consistent state to be called again. The actual underlying code is something like this: struct Closure { var: T, // owned } impl FnOnce&lt;()&gt; for Closure { type Output = (); fn call_once(self, _args: Args) -&gt; Self::Output { let var = self.var; // moved } } If `T` is a `Copy` type, the closure will only capture a reference instead. It doesn't *need* to move the value to make a copy! This will look something like: struct Closure&lt;'a&gt; { var: &amp;'a T, // borrowed } impl&lt;'a&gt; Fn&lt;()&gt; for Closure&lt;'a&gt; { fn call(&amp;self, _args: Args) -&gt; Self::Output { let var = *self.var; // dereferenced to copy it } } // also implements `FnMut` and `FnOnce` But if we'd written it as a `move` closure, it would be: struct Closure { var: T, // owned } impl Fn&lt;()&gt; for Closure { fn call(&amp;self, _args: Args) -&gt; Self::Output { let var = self.var; // copied directly } } // also implements FnMut and FnOnce (note: I'm using `T` as a concrete type, not a generic parameter.) ((I hope I got the details right -- corrections welcome!)) Also yes, if you mutate captured variables without moving or copying them locally first, then those changes will persist for each following call.
I think the name should be a clear intent that it's just a wrapper serving this particular purpose. I got suggestion to look at petgraph from someone else as well, I will take a closer look at the lib. I still feel there should be a better way to express such relations without the need to store separate id for every node. My biggest concern was recrusion as well, with the approach I tried you can't do a loop version of tree traversal without using unsafe. I added a proof-of-concept counting of nodes with stack of pointers and unsafe dereferences. With proper closure passed to some generic traverse function maybe this could be an acceptable solution. 
Based on my experience, I wouldn't say there's anyone here with a political agenda against Rust. It's more a question of whether the tools we've already invested in, and are arguably much better equipped to wield effectively, are already good enough. I'm optimistic about what Rust brings to the table, and is love to see it wisely stored, but Those Other Languages are also pretty good at what they do.
I built my own company and told my employees they have to use Rust :)
I think the "going down to the metal" approach of what a closure does is really useful. I think mentioning the `let x = x;` strategy should be critical for teaching closures to folks. I've [added a comment for the rust-lang book related to this point](https://github.com/rust-lang/book/issues/1089)
Might be better off building a sailboat. 
Those functions *aren't* safe, so they're misusing the `unsafe` abstraction. Global, mutable access without synchronization makes parallelism extremely problematic. What you're asking is for the Rust compiler to just let you do something that goes against everything it stands for. &gt; So is there a way I can get rust to spawn multiple shared libs (of the c code) so that the tests can stay parallelized? I don't think this is even possible at an OS-level. It really sounds like Rust is not the tool for this job. Idk.
logic errors are still possible, especially in error handling code! Rust makes these things _harder_ to do, but everyone makes mistakes. It's good they are checking at these early stages of development. [computers are hard](https://www.youtube.com/watch?v=Acx4Yi7nxW4)
Just out of curiosity, why does this library do things this way? It would be so much more robust if the C functions accepted a pointer to a C struct as the current state and mutated that, rather than mutating global variables.
Why?
That was really interesting, thanks for posting
I interact with the Go team at Google *reasonably* often, and I've never picked up on that. Of course, not being a Googler, it's also possible that they're hiding it, but everyone is so genuine that I'd find it kind of shocking. They mostly seem interested in learning from our successes, and I'm mostly interested in learning from theirs :)
Indeed, I feel like OP was a bit harsh. This bug was subtle: https://github.com/redox-os/userutils/commit/02759b4a5a347726e6e81d4ee46a2ade86fd9e1e
Trying to make a linear algebra library. It probably exist already, but i'm very new to both rust and not very good at computational mathematics so trying to learn them both at the same time.
This is one of the coolest crates I've seen! I must admit that I didn't know this was possible. Very neat trick.
Out of curiosity, was the goof in response to that /r/playrust mispost where someone commented about how the person should've noticed the big Rust logo on the rightpane before posting?
I was pretty excited about Xi until I found out that the core text engine was inexplicably put behind a text-based RPC interface, so that the UI has to send JSON back and forth to talk to the text buffer. I asked about that in a different reddit and the answer was that it was to enhance modularity, but this seems unconvincing to me. RPC isn't necessary for modularity, modules are. Putting the core text engine in its own module (perhaps with a C api in addition to the Rust API, perhaps even a dynamically loaded library) does indeed make sense, but making everything go via a JSON layer doesn't seem like it actually buys you anything here. In fact it seems like this would have major downsides (one of which is that it makes things *less* modular, see point 3): 1. They state that low latency/performance is a goal. Having all the editing behind a very chatty back-and-forth RPC channel seems at odds with this (as opposed to just normal function calls). 1. Their internal data structure is a rope, and one of the key benefits of ropes are that they're copy-on-write. This means that the plugin, the renderer, etc. can easily grab a read-only snapshot of the text buffer without impeding editing operations. But of course that doesn't work when the text buffer is in a different process. 1. The API for an RPC approach seems like it would necessarily be a lot less simple. E.g. instead of the renderer just saying "hey, get me a snapshot" and then going off to render that snapshot however it wants (e.g. it may have multiple views into the same buffer), when the text buffer is behind an RPC interface the core editor now has to know about all the different views into the buffer so it can send over only the relevant regions, and all this has to be synchronized back and forth. Ick. So paradoxically, this approach whose stated goal is to make things more modular, ends up making things *less* modular (i.e. more chatty API, where each side of the API boundary has to keep hand holding the other side back and forth). So anyway.. it just seems like they decided to adopt a fashionable micro-services approach for no real reason and are compromising some of their core goals/advantages in the process (note that you could easily build a "proxy" layer on top of a native API, if you really did need to talk to the editor via JSON for some specialized use case).
Is it really not possible to communicate with the backend via function calls, given that it's written in rust too? I seriously don't get why they wouldn't expose that.. 
Apparently not! I think someone mentioned that they may let you move the core in-proc (but still use the RPC serialization interface), but that doesn't really get you the benefits of being able to natively talk to the code (actual types with static checking, shared memory, etc.). 
i agree that a json-rpc is not the only choice for an editor that separates the UI from the core. but to me, it's not obviously a bad choice. i think the xi-editor project will learn a lot by seeing how far they can push this architecture. i also don't think they've painted themselves into a corner, and if the json-rpc layer proves to be too slow, they can add support for another faster/lower latency rpc.
&gt; that it's written in rust too? Many of the front ends, including the "official" ones, aren't in Rust.
It seems like an obviously *unusual* choice to me. Like, it clearly has loads of downsides, so if you're gonna go that route there must be some big wins.. right? But I've scoured the website and talks like these and so far there isn't really any motivation for it.
Right, but his reason was a non-sequitur, as I pointed out. Loose coupling doesn't require a process boundary with JSON in between (with all the downsides that entails).
Nope, I was just cleaning out my hundreds of old browser tabs and unearthed it in one of them, and was sufficiently delirious with sleeplessness to stick it up there.
While I don't work for Google, rust at Google scale has issues. They do trunk development with thousands of commits per day, with heavy dependency reuse, and atomic versioning based on commits (dependencies and apps are versions together). This means lots of cold compiles in a single day and long overall compile times. Pretty much all of the core values that they created go for are the opposite with current rust. 
So, the only safe way to do this is to have an atomic handle that lets you modify the globals and call the C code. All of the globals and all of the C code would probably have to live behind a single mutex, for example. Depending on the amount of C code, rewriting the whole thing to accept a "current state" to use with each function call (as suggested in the other comment) might be worth looking into.
&gt; Loose coupling doesn't require a process boundary with JSON in between (with all the downsides that entails) No, it doesn't. However, that is one way to do it. Regardless, like he did in that thread, I'm not going to bother replying to you anymore. Your FUD and aggression here make it pointless. You've made up your mind, and no matter who explains things to you, I don't think that's really going to change. It's a shame that projects get treated like this here, though.
They use plenty of C++, and in my experience the compile times aren't that different between C++ and Rust. But, regardless, my point is more that I would expect these cutting edge software engineers to at least be familiar with their industry. Google might not use Swift, but I would still expect their developers *to know what it is.* They're supposed to be *the best*. In certain areas, Rust has *huge* advantages over C++. In others, C++ has advantages over Rust. They *can't* do their job as well as they should be able to if they're not knowledgeable about which tools are available to them to solve problems. Those specific interviewers who *didn't* know about Rust left me unimpressed. I did meet at least one Googler who knew some surface-level detail about Rust, which is fine. I'm **not** saying every Googler should be an expert in every language and every tool. That's unreasonable. But, not knowing about Rust makes me wonder *what else* they don't know about.
&gt; our FUD and aggression here make it pointless Aggression? You might be projecting a bit here. I recommend you chill out, it's just a discussion on the internet. All I'm doing is asking about a core decision that seems a bit odd and at odds with the stated goals of the project, and so far nobody has actually even attempted to explain why RPC/JSON is necessary here as opposed to a C API (which works for just about everything else with the same goals).
None of this strikes me as FUD or by any means aggressive? The original comment lists several valid technical criticisms. Ironically, your response is far more "extremely dismissive". Even looking in the other thread, I agree that Xi's author does not make a particularly strong case.
yes, like all tradeoffs, there are upsides and downsides. for the downsides, i primarily seem latency and architectural complexity as the concerns. for the up-sides, i primarily see easy process separation and the ability to write a UI in something that can't do rust or c FFI. i'm afraid i don't share your "this is obviously unusual" intuition, though. and this is totally fine and normal. i see you're getting some static about your opposition to json-rpc, and if i may hazard a guess, it's because we can't tell if you're saying "i wouldn't do it this way" or "i don't understand why anyone would do it this way". the former is fine, but the latter might make some people defensive
Macros 1.1 encompasses three features: custom derive, procedural (bang [`!`]) macros, and procedural custom attributes. I'm assuming you're talking about the first as the latter two aren't stable yet. It is true that this can only be applied to structs/enums since it's for deriving traits. There is [a crate](https://github.com/dtolnay/proc-macro-hack) that provides a hack that lets you implement a procedural (bang) macro via custom derive, but nothing for attributes. If you're willing to switch to nightly, you can implement a procedural custom attribute. It's unstable but on the path to stabilization (hopefully sometime early this year). There is [a chapter in the Unstable Book](https://doc.rust-lang.org/beta/unstable-book/language-features/proc-macro.html) (written by yours truly) that covers these two unstable macro kinds.
The upside is potentially a flexible RPC message that can evolve over time. If you don't remove a field, you can pretty much just keep expanding json and ignore when fields are extra/default when fields are missing. That isn't to say other message formats couldn't do this. It is a little surprising to see that something like protobufs/flatmessage/etc isn't being used instead. Has pretty much the same upsides as json which much higher marshaling speeds and lower memory footprints. My guess is that it ultimately came down to familiarity.
[removed]
How is this different from a C API though? E.g. you add a new function, but keep the old one around (just like for new RPC messages). Same for data fields as well (new C API gets accessors to new fields, old code doesn't know it's there so they stay at their defaults if an older plugin talks to a newer core). This seems relatively standard, right?
Lucky you =)
IME pretty much all major languages have really good C API support, including tools for auto-generating code based on headers. I'd expect the number of languages that have built in C FFI support is greater than the number of languages that have built in JSON interop (in the sense that these RPC messages turn into "functions" on the other side rather than needing manual message construction/interpretation). I think version support is tricky in either case, but I'm not sure why JSON would be better. IMO a C API is easier to evolve because all the various "versions" that are still supported are statically listed and checked as opposed to based on some text based schema. You have to take a bit of care not to directly expose structures to the caller (they need to be allocated by the library, so that new members can be added after the fact), but this seems pretty well understood (and no harder than building/reading JSON messages IMO).
You don't really end up having to maintain that separate function though. So old plugins keep on working regardless of changes to whats in the message. Further, a C API requires that the language speaks C and that the you do a lot of reading on who owns the memory. Doing things via message passing eliminates both of those problems. Pretty much everyone speaks json and because it is message passing, the answer to who owns the memory is always "You do". That jives much better with things like lisp or erlang or haskell or Java. Where integrating with a C api might be somewhat cumbersome.
&gt; EDIT: okay i'm just deleting my other replies, whatever. Being a jerk about people's work isn't okay. It's not about what was said, it's about how it's being said. I'm out. It was never my intention to be a jerk. There's an old saying that I think is fairly crucial for surviving on the internet: "Offense is taken, not given". In retrospect I can see how some of my comments could've used some more caveats or whatever to come across less harsh, but ultimately if people are intent on misreading things on the internet to get maximally upset, they will. Note that you're the one who veered off path to talk about *me* as opposed to the actual issues I was raising (also note that at no point did I call you a "jerk" over that, whereas you kept going with the ad hominems). So maybe take a look in the mirror next time. 
IMO, the way you're choosing to express yourself in this thread is pretty shitty. From my view, you're coming across as if no reasonable person could hold a position contrary to yours. You say, paraphrasing, "well, nobody has explained it to me." *Fine*. But just because nobody has voluntarily chosen to spend their time engaging in what appears to be a laborious conversation with you doesn't mean someone is doing something for "no obvious reason" because "it just seems like they decided to adopt a fashionable micro-services approach." What an absolutely *ridiculous* thing to say. &gt; How is this different from a C API though? As someone who maintains many open source projects, I very explicitly avoid taking on C dependencies (if I can help it) because of the maintenance burden they carry. Ecosystems (as in, **any** language in which you'd write a frontend for Xi in) have gotten better with this, but there is still a categorical difference between, say, maintaining a library that is only written in Python and maintaining a library that needs to wrap a C library. If I were the one leading the Xi project and I thought I could manage to **reduce the friction** required for writing plugins or frontends, then I'd take it in a heart beat. And you can bet that I would consider a JSON interface over process stdin/stdout to have far less friction than requiring C bindings. You might say, "But each language only needs one binding." Sure, and who is going to write them? Someone has to. Are you going to do it? It decreases friction if someone doesn't need to write the binding at all.
Same here. I always try to make things modular enough so someone can rewrite it easily in another language. Performance is important but not paramount compared to the benefit the compiler gives in terms of stability (much less crash, someone accidentally breaking things is less frequent too).
That's a valid argument. I don't personally understand why sending a bunch of JSON message back and forth is harder than talking to a C API (it's usually pretty straightforward to bind to IME), but if you do then I guess that works for you. However, as I mentioned towards the end, if you really wanted a JSON layer, you could add it on top of the native library API. It wouldn't even need to be in the core distribution, but could evolve independently, to support patterns from a wide variety of languages and scenarios (e.g. it would perhaps implement a rich library of "bulk" operations to make the RPC library less chatty, whereas the native API doesn't need that kind of stuff). Thus, people who want maximum performance, static typing, etc. can use the fast/small/simple native API (or the C api if they're not a Rust tool), and other people can use the JSON layer. 
Sure, Xi Core produces this in V0.0.1 { "bob": 1234 } However later they decide 'You know what, it would really be useful to know foo! So they update the RPC message to add foo { "bob": 1234, "foo": "bar"} Now, the XI core team has made a change to the RPC message, but the old plugin that consumed that message only ever pulled "bob" off the message. So long as the plugin was written to discard unused fields in some fashion, then no changes in the plugin are really necessary. And further, the xi core guys don't need to have a v0.0.1 API that sticks around for the remainder of the project because important plugin X uses it and the author of that was hit by a bus. On the flip side, lets say that Xi wants another field in the rpc data, then they can add it but choose reasonable defaults if it is missing, which doesn't break plugins. They can also choose to start ignoring fields and that also doesn't break old plugins. You can't easily do this with C. If you want to add a new field in a backwards compatible way you need a new version of the struct along with new function calls that take that struct. And further you need stuff to convert from the old struct to the new struct so you aren't dealing with the old guy everywhere. It gets even more messy if you want to rearrange the order of your fields.
Raph literally told you that they aren't opposed to a native library API if it turns out the benefits outweigh the cons. So I really don't know what ax you're trying to grind here. "Maximum performance" is a questionable phrase in this context, because the benchmark shouldn't be "how fast can you communicate" but "can you communicate fast enough that a human cannot perceive communication latency." If latency can't be perceived, then doing a bunch of extra work to go faster sounds questionable to me.
To wrap that argument back around, it's possible they just wanted to use the JSON layer first for experimentation, and were interested in frontends and plugins that don't really benefit from a C API. At that point, why not build the JSON API first and leave the C API for later? It can be inserted under the JSON API when it becomes something you want, and in the meantime you don't have to maintain it.
But this works very similarly in C. If the "message" is a function with parameters, then you just add a new function and make the old function forward to it with defaults internally (just like setting defaults on the message in the RPC protocol to support old plugins). Yes, your API now has a "deprecated" section with old functions that forward to newer functions but this doesn't really cost you anything (and if anything, it's useful to remember the old APIs so you don't accidentally change the internals in a non-compatible way - with a C API all those old stubs will tell you exactly which old versions will break). If the "message" is an actual struct with fields, then you just add the new parameters in the new version (with getters/setters on the struct) and the old plugins simply don't know they are even there (the library sets the defaults internally). The only thing you need to do is make sure the library allocates the struct (and deallocates it) so that the caller doesn't know the size or rely on any of its fields directly. This is fairly standard for C APIs.
The fact that Rust's type inference enables two different ways to write a given integer literal was irrelevant to my point. I was saying that *having* an integer literal syntax of any kind is a special syntax for situations where calling `"4".parse()` may have real-world utility. (And I wasn't paying enough attention to realize that I hadn't made my jovial tone clear enough when in the absence of tone of voice.)
Looks like it: * [Tracking issue for `..=` inclusive ranges (RFC #1192) -- originally `...`](https://github.com/rust-lang/rust/issues/28237) * [https://github.com/rust-lang/rust/pull/47813](Stabilize inclusive range (`..=`))
I have no axe to grind, I'm just questioning some design decisions that don't seem to be very well motivated in my opinion, and which literally took me from super excited about this project to losing all interest. Part of the reason I would like a "new" editor architecture is because so many of the ones around today made some poor early choices that forever limit their performance (e.g. VS Code) and it's just disappointing that this one wasn't the project I was looking for. 
&gt; tooling that can't talk to C apis The problem here isn't possibility, but ease. Right now, with the JSON RPC interface, I can sit down and write a python, Java, Rust, C, Go, or any other language frontend with little to no friction. Someone who has no knowledge of C can do this too - and they don't have to worry about introducing memory leaks or segfaults. With the rust core completely separate, it really is language agnostic, and there isn't a large unsafe C API which must be maintained and validated. As for a thin layer, this would still have to be maintained. Having it separate from the core would just add an extra intermediate, and I don't see the majority of frontends choosing to use C over JSON RPC. Sure, json has a little speed overhead, but C has a large conceptual overhead, and a large cost if anyone messes up using it.
As someone who created and is currently maintaining two popular repos on GitHub, I'm having a hard time bringing back the feelings of achievement/usefulness. At this point it just feels like a chore, especially with the yearly Swift major version bump and dealing with breaking betas. There's definitely a thrill to making a popular and useful project. It just doesn't seem to stick long term, at least for me.
They want low latency and they‚Äôre using JSON? Yeah, okay.
The win here is that a frontend can be written in any language, by anyone, with no risk of segfaults. Is that not enough? Worst case if someone messes up a JSON RPC interface: editor interface gets an error, and makes a pop up window reporting it. Worst case if someone messes up a C interface: entire process is killed and all unsaved work is lost. I don't mean to be dismissive, but I don't see a large case for a C interface over JSON. There's a loss of flexibility, a loss of language interoperability, and a loss of safety, for a small gain in speed?
I don't think there's a great amount of anti-Rust animosity-- it's more that admitting languages for general-purpose use in google3 is a pretty huge undertaking, requiring a great amount of tooling, build system support, and proof of commercial viability.
Since this is quite a negative comment (and it's OK), I'm going to throw in my 2c. Technically Xi is quite interesting, however, I have 99 problems with text editors, but editing 4GB text files is not one. And Xi seems to be mostly about it. Personally, I find `kak` most interesting editor in the works right now since it really creatively improves on Vim modal text editing. It has simple, single-threaded, non-blocking architecture and delegates async work to separate processes. However authors seem to ignore the plugin/extensibility story, and RPC is a weird keystroke-based API, which is a high barrier for entry, so I don't see it gaining a lot of 3rd party features ATM, and thus popularity. I really need an editor loaded with candy to breakd my Vim habits. 
Hi, I work on one of the most used programming editors on the planet, and I gotta say, RPC for plugins is the way to go. Moving plugins out of process lets the editor make performance guarantees about e.g. typing latency. A crashing plugin with RPC is quite graceful, but often times, an in-process plugin that crashes can take down the entire editor. As other people in the thread have mentioned, RPC lets plugin developers (and UI developers it looks like?) to use different languages. Since all of your concerns seem to be focused around speed, maybe you should post some benchmarks of Xi vs other editors? My experience doing performance investigations on IDEs has me willing to bet heavily on Xi.
One thing that keeps me going is helping people through the PR process, especially beginners. It's such a fantastic opportunity to teach others that it feels really good when they succeed. (YMMV of course. I personally have always liked teaching, so this is kind of an intrinsic reward for me, and won't be the same for everyone.) I guess my thought here is to see if you can find a way to bring back the fun. :-) But yeah... Lots of breaking changes isn't fun. Back when Rust was going through that it was pretty tough, and it was a huge relied once 1.0.0 landed. This is (one reason) why I try very hard (but don't always succeed) to avoid using nightly only features.
Think if you watched the video, latency was not the only reason for selecting JSON, the github page for Xi gives more supporting reason why JSON was selected
Thanks for yours suggestion, it seems I misunderstand builder pattern :(
(Yes, those are certainly concerns, but I wish this thread could've discussed them less confrontationally. It feels too late, now.)
I am a little bit confused about "structs implementing the `Default` trait", does it mean there is only one default value for each field in struct? But I want different default value for different function, could you show me some examples ?
So the code is for work and is automotive (runs on an ECU). The way most automotive companies do it (from my minimal experience), is by developing ECU models in something like Simulink or DSpace and then generating C code from them. This code *can* be generated as nice composable functions, but isn't unfortunately (for good reasons). Most of the hardware interfacing is of course still done in C. Automotive code does use a lot of globals. This design is because you have a lot of constant globals that are editable by hardware for on the fly tuning of vehicle parameters (like while test driving!). So the structure makes some sense, but as you can see it has limits. As to your main point I can preserve unsafe's guarantees by running the tests in a single threaded manner and maybe figure out how to spawn process around the C code instead. I'm translating intensive and sizeable tests from Simulink to run on the C code, so some high level language that can actually generate the test data (like Rust) is a pretty strong requirement.
Dope
Thanks, I guess I'll have to live with single threaded for now...
Because I see certain projects of mine as chores, the PR process for me generally involves ensuring the changes made are ones that don't accidentally wreak havoc. But this seems specific to my Swift projects. The few that I have in Rust have been enjoyable regarding PRs. Nightly has enticing features but my tiredness of breaking changes keeps me from committing to it. I'm only partially using nightly for `simd` on the chess engine I told you about. And that's only because I manage to get gains that (to me) outweigh the maintenance burden.
While you're generally correct that language choice is fairly limited within google3, there are lots of projects outside of google3 that are more adventurous.
Depending on the data relationship, it might be possible to break things up into more than one mutex (fn1_and_varA_mutex, fn2_plus_fn3_and_varB_plus_varC_mutex, etc), but you still have to run the C behind a mutex.
Neat! Yours was one of the first commercial successes w/Rust I had heard about :)
Cold compiles with an extremely fine grained dependency graph, if you were to make every module in every crate a separate crate instead, rust's incremental compile times wouldn't be as bad.
[removed]
I founded a company last may and decided to go with rust
Ok. So, while I disagree with the policy of generating C code that uses globals... I don't think that's going to change right now. What I would recommend is just to do is follow a pattern [like this](https://play.rust-lang.org/?gist=f0c73a6453aa0cef978111fde7b9a947&amp;version=stable). You're then operating entirely out of the *unsafe* `run` function instead of `main`, so you can operate with the global variables as you please. For using Rust's test harness, you could build tests like this: #[test] fn test2() { unsafe { X += 2.1; Y = 4; some_func(); } } Enclosing the entire inside of the test in an `unsafe` block will allow you to access memory freely there too. `unsafe` Rust only enables a very small set of behaviors. The compiler still enforces a great deal of things. If you want to go the route of generating getters / setters, you could [use a macro like this.](https://play.rust-lang.org/?gist=48582603ec9c0b96021ea82a7273e7d4&amp;version=stable) Basically, gen!(my_float1: f32); expands to: mod my_float1 { mod c_internal { extern "C" { pub static mut my_float1: f32; } } pub fn set(new_val: f32) { unsafe { c_internal::my_float1 = new_val; } } pub fn get() -&gt; f32 { unsafe { c_internal::my_float1 } } } so then the actual `extern static mut` variable is hidden, but it can be accessed through the accessor functions without using `unsafe` from the test functions, or it can be accessed directly under the name `my_float1::c_internal::my_float1`, confusing as that name might be.
I agree that JSON is not the right choice when it comes to just about any criteria, except that JSON is supported heavily in every language's ecosystem. For any arbitrary language you can bet that the json parser will be one of the most heavily optimized/ cared about parsers in the ecosystem. This may not hold true for every lang, but it holds true for *most*. Everyone is also *used* to JSON. Pick the average developer and they'll have experience with it. The same can not be said for other formats. I do not think json will be the bottleneck for xi and I think anyone claiming it will be slow enough to prevent 'scaling' should show that.
So, those are two different kinds of comments, `///` would be paired with `/**`, and `//` would be `/*`. https://github.com/rust-lang/rfcs/issues/1371 and the other linked issues have lots of discussion. Really though, what it boils down to is that picking one style is better than some people using each style. That issue should probably be closed.
&gt; I'd expect the number of languages that have built in C FFI support is greater than the number of languages that have built in JSON interop Just as one example, Java FFI is still very cumbersome to use. JSON on the other hand is a piece of cake.
`///` is documentantion ‚Äî `//` is a comment. I don't know the reason, but I would guess. `/* */` is harder visually to parse without syntax highlighting, harder to align. I use `/* */` to temporary disable some code paths tho.
A co-worker was starting a greenfield project in 2016 that, for predictable latency, needed to be written in a non-GC language. He was able to pick Rust over C and C++ due to good will he'd accumulated from several recent technical successes and due to the fact that I, at that time primarily an ETL Ruby dev, would be able to contribute with a reduced likelihood of accidentally sneaking lots of little safety issues into the project. tl;dr: I got my job as a Rust developer by being a Ruby dev at a company that let us build a large project in Rust. We also sponsor the Rust DC meetup and hired a great dev directly through that. (Not that he (or any of us) exclusively works in Rust, but he's done a lot of that here.)
Congrats, Michael! That's great! Do you do any Ion (/Redox) work for them or just internal stuff?
One of the first slides is one with performance graphs that show the IPC/RPC/JSON is almost none of the overhead compared to the rendering and such. There are other things that could be an issue. With 'native' if they change the underlying data structures all the plugins and such break and need to be kept in sync. Possibly they could even break without it being obvious. There isn't so much of that with something like JSON. The standard IPC channels could be swapped out for something like TCP, which would add much more overhead but could be used for some kind of client server scenarios. Like a multiuser text editor. Editing files over the network on low end systems. Rust itself doesn't support dynamic libraries (at least not without hacks like going through the C ABI) so 'plugins' aren't really feasible.
[Was inspired](http://svgur.com/i/5Gz.svg).
ooh I wasn't aware that this was what your job was. This is really neat! I do this kind of stuff on and off -- if the Firefox folks need stuff I try to push for it in RFCs or do the impl work (done a bunch of that this month) and it's pretty rewarding!
Bearing in mind, of course, that some people (myself included), have a mindset in the vein of "I'm used to manually following/applying a coding style. If the formatter can't be configured for the style I want, I just won't use it." (In my case, it's more that I use `rustfmt` infrequently because of the extra effort it takes to manually go through the changes with `git gui`, commit only the ones I want, and then revert the ones I couldn't customize sufficiently in `rustfmt.toml`.)
On the one hand, sure. I like a nice, straight line with smooth second derivatives. On the other hand, the challenge with vector graphics is to make them not *look* like vector graphics. You have to go out of your way to add the little imperfections that give an image character and a sense of not just being a big bunch of lines. That's why I traced all the individual strokes for hatching instead of using actual tiling hatching patterns.
This is interesting, and I have thought about doing this in the future. Though what are you doing for libraries if you can't find an equivalent Rust crate? Calling C libraries? I mean Python and Go have a "batteries included" standard library, and Rust's third-party libs are still "relatively" new (at least compared to Python's huge ecosystem). Just wondering because this is one of the things stopping me from using Rust at the moment.
Sure, there are always exceptions. I was talking about the general case.
Interesting. I'll try to obtain a copy of clippy and run it on some of my projects.
Yeah, I just started doing this full time at the beginning of January. Prior to that I was working with Fuchsia folks during my 20% time.
&gt; (note that you could easily build a "proxy" layer on top of a native API, if you really did need to talk to the editor via JSON for some specialized use case). This would have been a really smart thing to do. Pay for it if you need it, don't if you don't. It would be more modular (separating serialization from everything else). And you could plug in other protocols later. It doesn't seem like defining a stable Rust + C API for a text editor would be *that* difficult. It almost they were designing obstacles in to overcome.
"Panicking" bails out of whatever the thread was doing in a controlled way. It's *very* similar to C++ exceptions, even reuses much of the llvm exception support. As a broad overview, panicking occurs in these phases: - payload and location are identified, payload is moved onto the heap. - panic hook runs, message is sent to stderr or other logging functionality. Backtrace may be captured - all values dropped from stack - after-panic cleanup occurs. If panicking has exited from `main`, program ends. The panic hook may be customized
I'm working on building a small site generator to make it easier to get started contributing to a given open source library and facilitating finding resources and getting mentorship for it. Its still in its really early stages, but I have a hardcoded prototype up and running here: http://foundry-test.surge.sh/ 
I think the issue is that the editor's UI isn't/shouldn't be a plugin, but from the comment you're responding to, it sounds like that *also* has to talk to the core via json RPC.
Uh, I think tiling hatching patterns would be much closer to my original goal for the image? Also I should look up how to do those. But I do agree that the messy pen-work gives it an, uh, "distinctive" feel.
[Gluon](https://github.com/gluon-lang/gluon) might be worth a look. It has a rust-like syntax, but is mostly targeted at being an embedded scripting language from inside rust programs.
https://www.reddit.com/r/ProgrammerHumor/comments/7i0xzi/time_to_end_the_debate/
I updated the PNG and SVG with a slightly fixed version of /u/QEDunham's modified version. I also rendered out a big PNG for print. See the post itself for links. I removed all the leaks and seams I could find. I also went through and killed all the strokes that aren't supposed to be visible anyway, just to be sure. The URL banner is also now just paths instead of real text, since I noticed Firefox was having issues rendering it correctly. As an aside: this has also reinforced my distrust of Inkscape's "simplify" command. [This is supposed to be a smooth edge](https://i.imgur.com/tHvDyLq.png). [These two green paths are supposed to be connected](https://i.imgur.com/v7SZZBT.png). It even totally removed the cusp node at the corner. I mostly left the simplified paths alone, since they generally had so many control points removed, there weren't enough left to fix them without reconstructing them from scratch anyway.
&gt; is it possible to write Python or Haskell styled code in Rust? Given the nature of Rust's syntax, I doubt you'll find that someone has written anything like that. * In Haskell's case, a lot of the visual simplicity is only a sound decision due to details like ubiquitous partial function application and lazy evaluation which you don't find in languages like Rust. * In Python's case, the decisions made to get that clean syntax don't come for free and some of the things that are hard to work into that design in a clean, coherent fashion (eg. multi-statement lambdas) are relied on heavily by idiomatic Rust APIs. &gt; and/or is there something like Goby (Ruby'fied Golang) alternative for Rust? There are various crates for scripting Rust. I'm not familiar with Goby but I like to combine Rust and Python into the same project using [rust-cpython](https://github.com/dgrunwald/rust-cpython) (for the actual integration) and [setuptools-rust](https://github.com/PyO3/setuptools-rust) (to put `setup.py` at the top of the build tool stack because ["extend, don't embed"](https://twistedmatrix.com/users/glyph/rant/extendit.html)). It provides a typesafe and very comfortable way to write Python extensions in Rust or embed a Python interpreter in Rust with automatic conversion of common data types and a clean API for defining your own conversions. If you don't mind depending on the nightly compiler, [PyO3](https://github.com/PyO3/pyo3) is a fork of rust-cpython which makes things even more comfortable using compiler features with unstable APIs. (You choose between them when configuring seuptools-rust using the `binding` argument.) Likewise, there's also [Helix](https://usehelix.com/) for building Ruby extensions with similar convenience, [Neon](https://www.neon-bindings.com/) for Node.js, and two [different](https://crates.io/crates/rlua) [approaches](https://crates.io/crates/hlua) to embedding Lua.. If you mentioned Goby because you want something in pure Rust that may integrate with the type system more seamlessly than possible with an existing language, I'd investigate [dyon](https://crates.io/crates/dyon), [gluon](https://crates.io/crates/gluon), or [rhai](https://crates.io/crates/rhai). (I haven't used them, but I keep hearing about them.) Personally, my preferred solution is just to use 
Making people interop with C is a good way to have a barren plugin ecosystem.
THAT'S how you make TT munchers work I was missing a small detail of your repeater and failing miserably as a result
Four! Don't leave out the module level comments, `//!` (tired) and `/*! !*/` (wired)!
[removed]
I don't have a lot to add here. It's a tradeoff, certainly there is some performance benefit to be had for eliminating the JSON serialization. I also believe it's really tricky to design C API's correctly, especially if you care about stability and want to bind it to other languages. I'm exploring one side of this tradeoff and believe it will work well. If someone wants to explore building a C API using some of the code and ideas of xi, I'm not going to stop them.
&gt; Learnability and readability improvements acheived by making task wakeup handles into explicit arguments. I know it's January, but I want this for Christmas.
My experience with these walled garden megacorps is that they hire people directly out of college, work them very hard, and provide a universe and ecosystem that has everything they need. Google will lift from open source, but many of the folks that work there don't live it, I had the same experience with Amazon folks who were working on the cloud for that database company. They would be explaining some new in-house component and I am like, oh mysql-proxy or redis or haproxy and they would give me a blank stare back. To me it was fact, sitting right out in the open, like a golden frog. But they lived in their own universe for so long, they literally had no idea what the rest of the world used. And ... when talking to C++ programmers from all megacorps, is that they have internal support, tooling, codebases, code search, build, etc. They have also put in 10s of years at being proficient in the language. From the couple of badass C++ programmers that I have tried to introduce Rust to, they were cranky cranky babies. And understandably, in one regime they are 10x, best in class, and in Rust, in the beginning they were in 101 again. They felt useless and drunk. I gave up C++ along time ago because I don't have the capacity to either handle the complexity, or feel comfortable in thinking I can handle the complexity. If you have 10 years left before retirement, are you going to start all over again? That is a scary proposition. I have a hunch, that the first megacorp to succumb to Rust will cause the rest to adopt it in short order. I envy the JVM and .Net with its interop, if Rust could interface with C++ in friction free manner, it wouldn't matter what so much what the rest of the codebase (and the C++) devs were doing.
Why not call back into Python for the libs?
Can we please stop putting everything in a browser just because we can? Pretty please? Awwgg OK fine.
Well, it can be for your birthday, that's sooner (unless it's January)
Also December :(
You could make a pre-processor that converts rust-ish code (written with haskell-style indent rules instead of braces) into actual rust code. The semi-colons would be fairly easy to insert I think. The braces would be less easy to insert.
For Python's standard lib, I'm assuming you're talking about Rust calling Python's native C API? If so, then that could work. But what about Python's huge 3rd-party lib ecosystem..how would these be called from Rust? I'm not too familiar with how Python's C API works at the moment and what its limitations are.
The author designed an incremental buffer update routine that is constant time over RPC, no matter the size of the buffer. New plugins will get the data streamed - like they would too if they were in-process. He also measured JSON + IPC and found that it has no important impact on performance and is quite feasible to reach his stated goals. I think that is something you should adapt too: If you worry about performance, you should provide numbers. If you do not have them you should generate them through experiments. You start out your post with an assumption (IPC is too slow), then you build on this false premise and in the end bash a good approach quite abrasively.
I think there a two wrong premises in your post, at least I disagree with them: 1) You seem to think all changes done to a project are good for upstreaming. This is wrong - if google removes functionality and changes build files to make stuff work in their mono repo and releases them, it helps nobody. Actually it will rather create confusion - people will try to use the "google" version of the library - because Google is a massive company that raked in $90 billion last year and surely their changes to the library must be great, right? Looking at it in another way, this is forking the library in an incompatible way which creates confusion for users and headaches for upstream. 2) You seem to think Google would like to keep modifications of open source software in-house. That is not the case, very much the opposite. Open source software is - nearly by definition - never a unique selling point of your approach/design - i.e. there is no IP in there that is worth protecting from a competitor. If an engineer at Google finds a flaw in a open source dependency or misses a feature and adds it internally, this is a fork. Keeping this fork 'rebased' on upstream as the library is updated internally is work for every update (merge conflicts et al) - unnecessary work. Instead a good engineer would spend the work of upstreaming the change once and makes future internal updates of the lib easier. This is very much policy in big companies like Google for this very reason - it is the best for everybody, including Google. Note that this explanation does not care about the license, Google would do this even for BSD/MIT licensed tools, just because it makes sense. That said, the MPL is fine as a license - I just think it does not buy the project much and it is a slight hassle for users. Just use Apache2 + MIT and you will get the same contributions back, but your users will be much less hesitant to try out your library.
This isn't just about single key-presses. Sure a few hundred microseconds overhead for a key press may be ok. The problem is if you want to implement e.g. search over the text buffer (which may be large - one of the goals of this project is to handle massive files). Are you going to copy a 5GB text buffer to the plugin process just to search through it, even though it's already loaded and could be searched instantly if you just had access to it? Or are you going to have a full-blown copy kept around in your regex plugin all the time (and keep it synchronized) so that it's ready when you need it? None of these options seem nice. There's loads of things where you want to do a bulk operation on the data in the buffer and with this kind of RPC approach you'd have to put it in the core text engine. Which of course means plugins don't get to play - you only get to use what the core text module builds in, everything else has to be small scale enough that the RPC overhead won't get in the way. 
Roughly yes, the standard libs and 3rd party libs have the same status. So calling into a Python VM, either in the same process or as an external process and using the libs available. [This](https://github.com/dgrunwald/rust-cpython) looks like the most popular cpython lib, but it also looks way too low level and overly coupled to cpython. I'd probably do a unix domain socket to a subprocess and serialize with JSON if the interaction could be high level enough (large enough work units to make serialization overhead moot). PyPy has a much better embedding story, esp using the CFFI to automatically access c structs. After a cursory glance, I don't see anything that would allow 10 minute setup time for communicating to a Python worker process.
Awesome article. Now I have to figure out how to get my projects popular! :-)
If the function is large enough, you can deal with it by doing this: fn foo(limit: impl Into&lt;Option&lt;usize&gt;&gt;) { let limit = limit.into().unwrap_or(10); bar(limit) } fn bar(limit: usize) { ... } Only the first function will be monomorphized, which shouldn't be too bad.
There's a sort of zen to it. The not-so-obvious key to my particular implementation is that the recursive matches actually distinguish based on the *2nd* token in the arguments (which is either a comma or an ident). Here is a dumber version of my macro that sort of makes that more obvious: https://play.rust-lang.org/?gist=fc8f8ba0ed412a8ed302a915ca5bd20c&amp;version=undefined It's important to think about how many tokens you need to "peel back" to make your match unique, then consistently eat that many for each matching arm. (You can always restore extra tokens when you make the recursive call, and philosophically you have as many "end-of-arguments" pseudo-tokens as needed, e.g. the empty branch in this case is kind of like two end-of-arguments tokens.)
BTW, it is possible to kinda get both performance and crash isolation here, if you're willing to take a relatively major complexity hit to the core data structures. Store the rope in shared memory. Everyone but the core process has read-only views of the data, and can easily get the latest read only snapshot (ask for an "index" to the most recent root, and then just follow those indices around the rope, which is all stored in the read-only pages you've mapped in). Keeps all the plugins in separate processes so they won't bring anything down if they crash, but you still get to share data. The downside being that implementing the rope in such a way would be painful (e.g. you may want to use indices everywhere so that it can be mapped at any virtual address and still be read). And you'd need to do some manual reference counting for which root nodes are being accessed by which processes (so you can basically garbage collect unneeded noes). And of course, you would need some sort of shared client layer that every plugin/UI can talk to in order to access this data in their own process (probably written with a C API) since you wouldn't want to duplicate this client-side logic in N different languages. Still need IPC for messages, but the API could probably be less chatty now that everyone can easily get a full read-only view of the buffer (with the various read and iteration operations executing in their own process' address space).
Is there a functional Linux front-end for this now (preferably one that can be easily compiled with just cargo)? I've been impatiently wanting to try it for the longest time now.
Awesome. I hope there will be a plugin for vim behaviour :-)
[removed]
Breaking change, oh well at least rocket still using hyper 0.10 branch.
This is inspiring.
I am writing a full featured parser for the [MovingAI benchmarks format](http://www.movingai.com/benchmarks/), a popular benchmark database for pathfinding algorithm. [Repository](https://github.com/THeK3nger/movingai-rust)
Do it for my birthday, it's in 6 days!
Xi seems really cool. I've not dig into it because the code was owned by Google. I could be uninformed (and probably am), but what would happen if the maintainer were to change jobs? Just makes me wary about investing time in learning an editor and potentially contributing to the ecosystem that isn't properly open source. 
How does Xi compare to neovim? It seems very similar. I assume that Xi will (or already does) deliver better performance? 
Is there a way to scatter the impl block over several submodules? I have a struct with about 200 global constants right now, and I think making those associated constants would be nice, but I want to have them in different submodules (mostly for organisational purposes).
I'm trying to deserialize a json into a struct using iron and params: let map = req.get::&lt;Params&gt;().unwrap(); let ev = itry!(serde::deserialize::&lt;Ev&gt;(&amp;Value::Map(map))); *Ev* is a struct with derive(Deserialize). Everything works fine if i add all fields to Ev structure but deserialize will error out if i skip a field. It seems serde implements ignoring not present fields but it doesn't seem to work here. Any idea? Should it be implemented in the params library?
Storage engines tend to be behave radically different in various scenarios. Is it good with big writes, is it good for small writes. Are they entirely recovery safe, how they support backups, etc. For serious use you need lots of details on how it works.
Having skimmed the code, it looks like it will be almost trivial to throw away the RPC part and replace it with some plugin-abstraction-layer, or give access to internals. The reverse would probably not hold?
I keep hearing JSON is bad and not a good format. Could you refer to some more elaborate explanations as to why that is the case?
It's better than putting the browser into a desktop application (most of the time).
Yep! This is just an early peek. It's quite flexible, as you define what a page is through the Materializer trait. It's log-structured, so it can handle high write volumes. It doesn't matter if writes are big or small that's up to you, the DB designer. The IO buffer is lock-free and there is no head-of-line blocking. Backups are supported by copying files, or pausing segment reuse, flushing, then copying ranges of the file externally. Recovery is per-page linearized. Pages may be relocated in the log-structured store, but within the page all recovery is linearized. The on-disk structure of any page is linearized with the memory representation by reserving bytes in the lock-free IO buffer, then performing in-memory lock-free operations, then either committing the buffer or writing a "failed flush" in its place. Transactions may be implemented by you, on top of it, because it is per-page atomic and linearized.
&gt; I indent with tabs, Rust Style RFC be damned. There are dozens of us!
Q: Why doesn't std::ops::Range have an iter() function. Ranges are pretty much the most iterable conceivable concept! Non-salty Q: How can I get an Iter&lt;usize&gt; from a Range&lt;usize&gt;? Cheers
Range implements Iterator. You don't have to do anything to it at all to use it as an iterator. Here's an example: https://play.rust-lang.org/?gist=8056f87480d6b75e72e39c1cc9c54c42&amp;version=stable 
&gt; isn't properly open source. The code is licensed under Apache2. That's "properly open source", as well as "properly Free Software."
Argued that it was a good idea to build a shared library with common functionality to be used by many different languages, then Rust was a pretty easy pick when it came down to language selection (C/C++ inappropriate because of security issues and tendencies to segfault, Go starts a runtime with multiple threads, garbage collector, calls to the library needs to be synchronized with the runtime threads, possibility of cross-GC memory leaks when embedded in other GCed languages etc.)
https://github.com/google/xi-editor lists two frontends using GTK+, one text frontend and an electron frontend. Did you try any of these?
Yeah, I have looked at the ones listed there. There might have been a few months since last time, though.
Not the parent but. * No real way to add binary data. * Numbers are all floats * No standard way to add field data * The spec allows for all sorts of monkey business which makes writing parsers hard. * The recent removal of comments just because they might be abused (speaks to bad stewardship) Those are the big ones off the top of my head. But, meh, people work around them anyways.
&gt; One of the first slides is one with performance graphs that show the IPC/RPC/JSON is almost none of the overhead compared to the rendering and such. What about that 5 GB buffer that has to be sent across (in point 2 of your parent comment)?
This is a linguistic issue: for mental modelling, you can totally use things that the compiler doesn't see. I chose that way. Most people I know model the lifetime of a value on top of the references to it, even if borrowck does only check borrows. (Later in the article, I specifically mention that lifetime _checking_ does not have any kind of impact on the code you wrote)
&gt;rustc: SIMD types use pointers in Rust's ABI This one looks very interesting to me. Looks like we have some "decision" on how to go forward with SIMD. Looks like a good approach for stabilizing a minimal set of types (more like we have from intel anyway) and go from there, saving some huge hassle by avoiding some huge stress in creating/discussing/fighting_our_way_through a bunch of huge RFC's for cross plattform/ISA lane-aware types. Stabilizing some minimal way to get SIMD into Rust may be possible this year! 
The "GUI over RPC" idea is similar, but xi is not derived from vim, I don't think it has any vi style behavior built in at all.
Also Mozilla MDN JavaScript docs are best in class and much appreciated. (Sadly I write JavaScript for a living. I use TypeScript at least, which definitely helps).
Cool trick, I don't know this either
Yet again, the RPC vs C ABI conversation. Clearly, both approaches have huge pros and cons. And when I say huge, I mean really huge. And that let's me wondering, why isn't there a middleground? Couldn't we find a fairly cross-platform way to load plugin code in a different address space, and recover from segfaults, without having to launch a whole separate process for that? I guess this question would be up for the OS guys. To me, it seems like the whole software industry is craving for a compromise on this problem that they should investigate.
Maybe json is the issue? Creating and parsing a batch of json to make a function call sounds expensive. Maybe the core RPC mechanism should be raw buffers, protobuf, or something else, and then json built on top of that. That way if I write the UI front end in say, c++, I don't have to deal with a textual representation of function call interop. 
Is it being used at all in data science community? I feel like people are pretty much settled with optimized libraries in C/C++ with python API.
Quicli looks great (also, great name)!
Hadn't noticed that, only saw the "owned by Google" part of the readme and that it was a Google repository. Should have looked closer, my bad. Thanks! 
Recent removal? I didn't think that JSON ever had comments.
I make the [text frontend](https://github.com/little-dude/xi-tui/) but have not taken the time to work on it for weeks unfortunately so it's probably not in sync with the core at this point :( If anyone is tempted to work on a frontend written in Rust, have a look at [xrl](https://github.com/little-dude/xrl), it's an Xi RPC library. It might need to be updated a little if the core changed, but it's globally in a better shape than `xi-tui`.
JSON as a format is fine. That said, formats are for specific things, and so JSON is good at some things, and bad at some things. For example, we picked TOML for Cargo over JSON because JSON isn't a great configuration format: it has no comments, it doesn't support trailing commas, and has a lot of punctuation. As an interchange format, JSON suffers from under-specification in some areas; for example, numbers: https://tools.ietf.org/html/rfc8259#section-6 &gt; This specification allows implementations to set limits on the range and precision of numbers accepted. aka, it's implementation defined, rather than defined. This isn't generally terrible, but it is a wart. In a related sense, thanks to the name, many people think that JavaScript objects are valid JSON. [But that's not true](http://timelessrepo.com/json-isnt-a-javascript-subset). This issue is similar to the number one; as the post says, it's an easy fix. For the use-case here, the justifications make absolute sense, and I think it's a fine choice.
Mm, your right. I thought it did because many parsers handle them.
Enums can also be structures. You could do something like: pub enum TYPE { Artist { name : String, age : u32 } Album { title : String }, Track { title : String, track_number : u32 }, Playlist { name : String }, User { username : String }, } I'm wondering if this would fill the role for your options? Rather than passing in 3 different values and a type, you pass in a single enum value of that type.
How do I use the inner enum with the `type` alias I made? For some reason I can access it using `typedef`. https://play.rust-lang.org/?gist=041f0d106be4e9982a58a7911ed414a2&amp;version=stable
My understanding is that the core is the only thing that holds the whole buffer, the front-end just gets what's needed to display enough for the current screen.
No worries! I'm not a Googler, but in my understanding, any open source you do is owned by google, and there's a process that you can go through to free it, but it's generally annoying and there's not often good reason so most people don't do it.
thanks for your inspiration, sounds great :)
Np. Glad I could help.
&gt; With 'native' if they change the underlying data structures all the plugins and such break Not necessarily. If the underlying data structures aren't exposed bare, and instead a higher level API may be provided for accessing parts of text (e.g. an iterator-based mechanism that returns slices of text) that is robust enough from breaking even if underlying data structures change. Although, I would argue that such an API is probably harder to implement and maintain than using the current approach.
I'll learn how to maintain a fork of the compiler if they ever try to take away hard tabs and block comments, tbh Never give in! Never surrender!
well why is it made if the creator dont even want us to use it?
/u/gankro said that he'd give the author $50 to write it
GCs can only help you with memory. Ownership/Borrowing helps with any kind of resource. http://blog.skylight.io/rust-means-never-having-to-close-a-socket/ (This is from 2014, before 1.0, so the syntax/imports are a bit off!)
&gt; you use an array of byes. Can you explain a little what you mean here? I thought JSON didn't have binary literals.
&gt; why aren't we satisfied with garbage collection and **message passing?** This solves the message passing part. And do you have examples of a situation where iterator invalidation is a thing in a GCed language ? The only thing which is unique to Rust among mainstream (in the sens non-research) languages is the affine type system (only one owner of the data) but it's generally not something people are looking for per se (even though it's really strong to avoid some logic bugs). IIRC Haskell and Swift want to include linear/affine logic in their time system at some point (or is it done already ?)
&gt;Finally, a pointer is valid at `&amp;'a mut T` if it points to `size_of::&lt;T&gt;()` many bytes of memory3 that are *owned for lifetime `'a`* Why did you write *owned* and not *mutably/exclusively borrowed*? This goes against my mental model of ownership, because generally, if you own something, you are free to move and/or drop it. Also, it seems to me that `&amp;'a move T`, if we had it, would be closer to what you said 
FWIW, your last paragraph is an exact summary of "why is Go popular", and it deservedly is Go is a great tool for the use cases you describe; goroutines are probably a way easier tool than Rust's competitor utilities for the moment, even
Not mentioned in this post, but I found in the library documentation that currently only Mac OS X is supported.
so, as i mentioned above, JSON doesn't place any restrictions on the range of numbers, only their literal grammar. so yeah, you don't get byte literals, but you basically do a `[u8; N]`: { "bytes": [ 72, 101, 108, 108, 111 ] } hopefully you're machine generating this, rather than writing it by hand!
They are owned, just not by the reference. The reference is using 'a to describe the lifetime of the referred data, not itself. The reference has exclusive borrow for a lifetime shorter than 'a, and does not own. Your model is correthatand that quote is a miscommunication
&gt; And do you have examples of a situation where iterator invalidation is a thing in a GCed language ? Of course. Adding items to the equivalent of a `Vec` will invalidate iterators over it in most languages. E.g. in C# you'll get an exception if you're iterating over a `List&lt;T&gt;` and you append an element.
and it was 2 years ago i presume? so basically unsupported project from the get-go.. ok.
Consider this expression: a * f(b) + c() * d Does it matter what order you perform the multiplications? Can `c()` depend on the value passed to `b`? In pure math, the answer is a resounding "no". The addition must happen after the multiplications, but that's only because it depends on its inputs. Either function evaluation can happen before the other. Some functional languages (e.g. Haskell) are the same. The only way to specify the sequence of events is by their data dependencies. This completely separates the program's ability to remember what is happening (state) from how it decides what to do next (state transition function). Which is pretty cool and all, but also very heady stuff. In C the answer is a resounding "yes". There is a specific order of evaluation (almost completely specified) and one function may have spooky effects on *anything*. Passing an argument to `b` makes `c` do something different. It's even legal for either function to mess with your local variables, because pointers. This dependency on order of evaluation is "raciness". State exists and depends on the path that the program has taken to get there. The beauty of Rust is that it gives you the freedom to be a little racy while gently (and not so gently) reminding you that raciness, shared mutability, is fertile ground for bugs. Like, you can have `mut` local variables and even loan them to other functions but you *can't* experience unexpected mutation through a dangling `mut` pointer. Garbage-collected languages often force everything onto the heap (no pointers to locals) and once an object is a heap object it's capable of changing unexpectedly because of a method call to a different object. If you need shared mutability, you have to specify it explicitly with a cell type (`Cell`, `Mutex`, etc.) - and think about thread safety too. But this is culturally not the default way to solve problems, and a *ton* of Rust libraries make little use of it. 
Thanks!
ah, that's also a totally valid option. Probably slightly better. &gt; I was hoping there was a JSON feature we missed. Yeah, I'm kinda bummed it's set in stone forever, there's tons of tiny stuff like this that is missed. Oh well.
&gt; do you have examples of a situation where iterator invalidation is a thing in a GCed language ? Java's `ConcurrentModificationException`, Python's general "don't edit iteratee's while iterating over them, bad things may happen" warning. These aren't *security* issues in GC'd languages, but they can bite in production.
The current XInput libraries all use the older 0.2 series of winapi. This one also provides an interface to the actual FFI calls that's as direct as possible, without asking you to use its own custom input loop or anything like that. It's not on crates.io yet, I wanted to spend another weekend and get DirectInput support going as well. If folks think it warrants being on crates.io even in the current minimal state I can put it up.
Since message passing is not mandatory, you can make data race by mistake. It's the same situation than in C++ : you can use smart pointer to prevent most memory safety problems, but there is no guaranties you are using them right. 
My employer Faraday.io does quite a lot of data work with Rust, both in our main Extract-Transform-Load (ETL) system, and in some of core code.
I pretty much define "borrowed" as "temporarily owned", hoping that on an intuitive level, this is close enough. Your comment about `&amp;move` made me realize that maybe one constraint here is not explicit enough: When I say "owned for lifetime `'a`", this also says that *when `'a` ends, the ownership has to be passed back*. So, dropping things wouldn't work, you couldn't satisfy this final constraint. Maybe I should just use the term "borrowed" even inside invariants. I will see how well that works for the more interesting invariants later in the post.
how do I create a bigint constant from a hex (or decimal) literal ?
Which one are those? Because Spark, Hadoop, Flink are JVM things with some python bindings.
&gt; If I ever have time I might turn this into a series. YES PLEASE
Worth every penny.
Worst case scenario, json turns out to be a bottleneck somehow and a json to bson conversion is done in the code; that'll be very trivial to do, refactoring wise, as it's basically just a super compressed form of json. But, I think that'll be very very unlikely. Much more likely is all of the plugins you'd want to use anyway (autocorrect, etc) end up dominating the processing time and the "json lag" isn't even noticeable
When I implemented multi-line comments in https://github.com/rust-lang/rust/issues/66 I was told they were intended mainly for commenting out large blocks of code during development and testing, while single-line comments are intended for actually describing code.
Good question, I think others have done a good job of answering and I have nothing to add.
&gt;Why shouldn't it be a plugin? Well because it doesn't fit my preconceived notions of how the world works, obvs.
How is it different from an exception?
https://github.com/eyelash/xi-gtk works for me :)
The guy who made this just told you it's a trade-off he has made. He's a smart guy who has been coding for years. I think he has been good reasons for what he code and why. The whole thread had been people telling you it's about tradeoffs. It's hard to get your cake and eat it too. You've just doubled down on this opinion of yours from the start saying this C API would be better. Fork and make it then. Provide hard numbers. Prove us wrong and show your way is fault tolerant and easy for plugins to use. Not every optimization for code has to be made. Especially if it's not noticeable and other more critical parts need optimization. I don't know why you're so hung up on this, but everyone responding to you has provided ample enough reason as to why this isn't a good idea here and that using json is good enough.
Yeah, but they aren't in the fast-path for rendering (and can use a streaming interface so don't have to have 2 copies of the 5GB file in memory at once).
Since this comes up quite a bit, I'll address it by linking to public [docs](https://opensource.google.com/docs/) on the matter. Googlers have a choice between the opensource approval process and [personal ownership](https://opensource.google.com/docs/iarc/). Going the former route has many advantages, not least of which I can work on it on Google time. Of course, the code is all released under proper open-source licenses, so the question of who owns the copyright doesn't matter all that much. In the extreme edge cases (for example, when there's a dispute about who actually wrote the code), it's helpful to have things really nailed down and explicit, and I think the Google open-source processes are good about that. There's a tiny bit more friction around these processes than if I were on my own, but overall they make a lot of sense and I'm happy to work with them. The open-source team at Google is excellent about helping people get stuff done rather than throwing up roadblocks. To me, the most confusing thing is the "google" organization namespace on github, as it mixes together official stuff (like protobuf), totally random personal projects (synthesizer-io), and stuff like xi which is not itself an official Google product but is used inside Fuchsia. I can sympathize with people looking at this and not being easily able to figure out what's what. Obviously all the above represents my personal opinion and not official Google policy (except of course, the contents of those links _are_ official Google policy).
&gt; Something I have heard vaguely alluded to is that there are 'political' problems with Rust within Google. After all the resources they have put into Go, sounds plausible (yes, to me they are competitors, whatever anyone else says). I think it is inevitable that they will pick Rust up sooner or later, though. As one of the people in charge of programming language decisions as Google, no, there aren't political problems with Rust. About the only language there is political problems with is Java, and that is because getting sued for billions of dollars tends to generate political problems. But we use it anyway, you'll notice. What is inevitable is that we will pick up something. There are teams that experiment with Rust (Fuchsia, alluded to above, is a good example). There are teams that experiment with other languages too (I can't share, unfortunately). The timeframe on which something gets picked up and supported for real is going to be pretty long. Rust may or may not be the best thing to move towards over time. We actually measure internal customer satisfaction with languages, etc. 
1. What kind of stuff are you using Rust for? 2. Are you hiring?
True, but the fact that they even have to stream could be avoided if the actual data could be shared (and thus could process the whole file considerably faster). I'm not saying I'm against RPC for plugins, as it offers a lot of advantages. But I would prefer to see an actual "native" API used to either build in-process plugins, or an RPC layer for out-of-process ones. Then you have the best of both worlds.
You might need some combination of [`parse_bytes`](https://docs.rs/num/0.1.41/num/struct.BigInt.html#method.parse_bytes) and `lazy_static`, until `const` functions are stable?
I think this is true, but also that Rust would be a vastly superior language for Pythonistas to drop down into for low-level optimizations. As ever, it's a matter of libraries and tooling. The numpy/scipy/cython/pandas ecosystem is very mature, getting a toehold would be difficult.
They are, Java saved millions of pointer errors from ever occuring and gc'd languages make up the majority of production code.
Garbage collection gives up control. It also often seems to lead to assumptions that result in a less efficient memory model to begin with (ie, everything being a boxed pointer on the heap). That said, I am starting to think that it would be interesting to have a language with both ownership semantics/borrow checker, AND a GC. They might do well shoring up each other's weaknesses. We have something like this already with `Rc` but a tracing GC might be able to go further.
&gt; why aren't we satisfied with garbage collection and message passing? That really depends on what data you pass. For example, even in Erlang, I can easily pass a file handle to two threads and close on both. This doesn't lead to unsafety, but may lead to bugs. Rust's type system _can_ make guarantees there that others can't. That being said, BEAM gives you so much more then just message passing that there's many good reasons to use it.
That would be nice, but there's only so much time people can put into hobby projects, and it makes sense to put the effort towards the RPC interface first because of their greater general utility, and the fact that you might just never need an in-process plugin interface. I'd be more inclined to speed up the RPC by adding an optional binary format like protobufs when JSON and RPC end up start being significant bottlenecks. But I think this goes back to the OP -- you don't know what your bottlenecks are until you measure them, so until there's evidence that JSON and RPC are the bottleneck for something that they're doing, they should probably keep to their current course and ignore all of these. also, keep in mind that most programming editors use crazy architectures. I don't know vim's internals, but emacs is kinda crazy, and most of the hard stuff is happening in a slow interpreted language. vscode and atom embed a whole web stack, and sublime uses an in-process python vm which can run potentially blocking code. You can probably leave a lot of potential performance on the floor and still be the fasted programmer's editor in the world.
&gt; But I think this goes back to the OP -- you don't know what your bottlenecks are until you measure them, so until there's evidence that JSON and RPC are the bottleneck for something that they're doing, they should probably keep to their current course and ignore all of these. I don't think this is a good principle to apply systematically. The editor has "speed" as an official "first-and-foremost" focus. If these things do turn out to be bottlenecks, you'd need to refactor the whole thing to switch the focus from the RPC API to the FFI one, and then implement RPC over it. So either you do that right away, or you go and make benchmarks *right now* to at least have some confidence that it's no big deal. &gt; You can probably leave a lot of potential performance on the floor and still be the fasted programmer's editor in the world. That's true, but we should go after the low hanging fruits either way.
&gt; why aren't we satisfied with garbage collection and message passing? In most languages, message-passing is implemented by sharing the message between threads, which does not solve data-races, at all. Actually, the few ways to crash a Go program without resorting to unsafe are passing a slice or an interface wrapped in a struct through a go channel and then use it from one thread while modifying it from the other. The official answer is "don't do that", which is technically correct, but not too helpful.
&gt; If it's memory safety and fearless concurrency that we want, why aren't we satisfied with garbage collection and message passing? **Latency**. I work in HFT, as did Carl Cook who presented [When a microsecond is an eternity](https://www.youtube.com/watch?v=NH1Tta7purM) as CppCon 2017. In HFT, latencies are measured in wire-to-wire (w2w): - start the timer when the first byte of the inbound packet enters the network card (leaves the cable), - end the timer when the first byte of the outbound packet exists the network card (enters the cable). The best software stacks have a w2w of 2.5 **micro-seconds**^1 . *Consistently*. It may be possible to achieve with a GC, more or less regularly. I have talked with former employees of the Chicago Market Exchange, and they mentioned using Java... with 0 GC cycles in a given day. But at this point you have to wonder whether the language is helping or hindering. So, if **consistent latency matters**, not just the average/median one, GCs are generally out. There are some outliers; the Nim GC is pretty incredible for soft real-time and last I heard was able of pausing for only 10 micro-seconds at a time. Unfortunately, that's at least an order of magnitude too much. ^1 *The informed reader will realize that this 2.5 us include the time it takes for the signal to move on the network card, through the PCI bus, and reach the CPU... and back again. If the software had 2.5 us to itself, it'd be a luxury.*
this week (just like the past couple of weeks) I have been working on the backend for a schedule widget (React) for a TV Station. It had been interesting and surprising to find that Rust has nothing to envy from JS (I hope I don't offend anybody with that statement :) ) when it comes to data manipulations, iterations, map, filters, etc... Working with dates is another matter altogether. Timezone was kind of complicated with chronos but then hourglass came to the rescue. But definitively I'm enjoying Rust's high level language feel and ridiculously fast low level performance!
What is `vi` (conceptually) but a UI "plugin" to `ed`? :)
https://www.slowtec.de/team.html I'm guessing.
Definitely watch the video.
I'll tease you: this is what we know about and is ready to announce.
Well... it's not correct but it's not *entirely* incorrect. 1. An array of bytes is a pretty crappy way of storing binary data. Each byte may take up to 4 bytes (3 digits and a comma), so if we assume all digits are equally frequent, (which [isn't a good assumption](https://en.wikipedia.org/wiki/Zipf%27s_law) but we'll roll with it for now) then you average 0.39 bits of data per actual bit... you're inflating your data by 60%. Even base64 is better, at 0.75 bits of data per actual bit. Plus, you have an extra step to actually validate that and handle all of this, which is more difficult than it should be because... 2. Numbers *may* all be floats. [The spec does not distinguish](https://tools.ietf.org/html/rfc7159#section-6). That makes life hard. It's fine for the normal cases but leaves a *lot* of edge cases for the end-user to handle (nan, inf, values outside the range of what they want, implementation-defined differences, etc) which could be handled by the implementation far more easily and consistently with a tiny bit of help. 4. Support that parsing JSON is not easy: http://seriot.ch/parsing_json.php JSON isn't *terrible*, really, but it has a number of sharp edges that it shouldn't.
&gt; An array of bytes is a pretty crappy way of storing binary data. Oh yeah, like, I don't think it's a *great* way by any means. My point is that JSON has ways of doing this. I guess it comes down to "real", in what the parent said. &gt; JSON isn't terrible, really, but it has a number of sharp edges that it shouldn't. Agree 100%. 
Here's a quote from the xi author: https://news.ycombinator.com/item?id=11577160. I don't think it's very similar though, especially wrt the internal architecture (rope vs. memline).
It depends on the plugin but we've discussed ways to keep the RPC approach speedy for large files (e.g. mmap the backing file data + edits &amp; serialize just the information necessary to rebuild the rope structure locally in the plugins process). This is fast &amp; avoids every process needing to maintain a copy of the file. I'm not too worried about it at this moment as it should be something we can slot in at a later date. There is more immediate work that needs to be done.
We're building a cryptocurrency exchange in Brazil, I think betting on Rust payed off, its performance, correctness and expressiveness hit a sweet spot for our domain. We aren't actively looking for new devs before launching in February, but applications are welcome. As I said above we're in Brazil though
Raph made the good observation on HN that the specific language/technology in use right now is not the important part. The part that will survive is the RPC &amp; the conceptual APIs exposed therein to make the system work. Yeah, xi will probably move away from JSON at some point. JSON actually has advantages in that it's untyped nature means that modifying the schema is easy &amp; since everything is in a few git repositories we don't have to worry about arbitrary 3rd party code for now. As Raph showed the performance bottleneck isn't actually JSON (except in Swift where the JSON story is pretty bad).
In one sentence: Rust panics are about ensuring your web server has a way to return a 500 instead of overflowing a buffer, not what should happen when some user starts exploring different `profile_id` values in the URL. If robustness is a goal, you can catch a panic and limit the damage (a web server giving up on serving one request instead of the whole server crashing) but every time you write `panic` or `expect` you're not expecting it to run and if it does you're not expecting to solve the problem without programmer attention. other points of difference from other languages: - there's no type checking, no `throws BlahBlahException` clauses. Any Rust function may throw anything. - panic recovery has major safety implications for unsafe code and subtle correctness implications for all code. This is probably true about exceptions in other languages but Rust is more cautious about this sort of thing. - there's no expectation that it's fast 
A rough sketch, I don't have time to play with the code, so here's the idea: You have to define some lifetimes. Make the function generic over a lifetime, let's call that 'item. After that, declare in the where clause that the T::Item lives as long or longer as that lifetime: T::Item: AsRef&lt;str&gt; + 'item. Finally, make ParsingResults generic over lifetimes too so it can safely contain non-'static stuff, and reflect the lifetime 'item in the return type: Result&lt;ParsingResults&lt;'item&gt;, ParsingError&gt;.
I have very bad memories about shared memory. Like shared memories buffer getting "stuck", requiring reboots to clear. And the synchronization across processes was not exactly easy-ish either, and required systems' code which may not be portable or accessible from many languages. Honestly, if I never have to develop with shared memory again, I'll still feel like I've had too much of it :(
As others have said, those functions aren't safe. This is a way that could be done but is totally untested: #[macro_use] extern crate lazy_static; use std::sync::Mutex; lazy_static! { static ref LOCK: Mutex&lt;()&gt; = Mutex::new(()); } extern { static myint1: i32; } fn set_myint1_safe(val: i32) { let _guard = LOCK.lock().unwrap(); unsafe { myint1 = val; } } fn get_myint1_safe() -&gt; i32 { let _guard = LOCK.lock().unwrap(); unsafe { myint1 } } Depending on your tests, this might allow you to keep them parallel.
The talk shows that the JSON rpc is not necessarily the bottleneck, but JSON impls are. This current plugin architecture is easily more accessible than a c-api when it comes to cross language bindings (just because it's easy in C doesn't mean it is in a Python implementation). Why not focus on more low hanging fruit, there are easier optimizations that provide better UX than having an API that is at best a tiny bit faster.
I do believe you could improve on the python/C++ combination and I still dont believe Rust targets (precisely) the areas for improvement; (for reasons I wont repeat :) ) I agree Rust would certainly be competent in this role.
The responsibility for keeping the str data in memory isn't clear. You can either make the `ParsingResults` keep its data in memory by using `String` instead of `&amp;'_ str` -- (seriously, do this) Or you can keep the source container borrowed: pub fn parse&lt;T&gt;(&amp;self, args: I) -&gt; Result&lt;ParsingResults&lt;'i&gt;, ParsingError&gt; where I: IntoIterator&lt;Item = &amp;'i str&gt; { However, that is likely to make things awkward in the calling function.
The simd crate already has all those lane-aware types, and has for a while.
That doesn't work because the iterator may return `String` (for example) and those items will be dropped in the function body, meaning the references would dangle. I.e. You can't use the `.as_ref` borrow site to create a reference that will outlive the function return.
Have you explored the semantics of `let ref x = x;` and `let ref mut x = x;` ?
Thank you, I'll go with the first option (String).
&gt; This solves the data-race part. It doesn't. Go, which has message passing as its primary concurrency mechanism, still has data races. The only way to avoid data races I know of other than Rust's system is to disallow mutation at all - e.g. persistent data structures.
Bingo! GCs get a lot of bad press sometimes from the Rust/C/C++ realm, but they really are not the worst thing in the world. They work pretty all right for most scenarios. However, latency and memory consumption are easily the biggest problem with GCs, pretty much any GC. You almost always trade computational performance for better latency when talking GCs. However, if you are in my shoes doing large amounts of data shuffling and processing, then the 100ms occasional pause time really isn't too much of a headache. (That isn't to say I wouldn't kill for some of the other guarantees that come from ownership... just that the memory guarantees aren't the only thing that matter)
To me, this is a killer feature of rust. Nothing more annoying that dealing with and tracking down resources that you are supposed to close. You don't find out it is a problem until the 1,000,000 resource is partitioned, which can often mean "Not till it hits production".
That still wouldn't prevent tests from overwriting each other, based on what OP has described. There would need to be a mutex-per-global, and then each test would need to lock all of the mutexes that corresponds to any globals that will be read or written during the test. Of course, care would have to be taken to avoid creating a Dining Philosophers deadlock.
FWIW, I think you'll get all the testability benefits of having the functions act on values that are being passed in by reference, even if those references are to globally shared values in the production call. 
&gt; I'm not a big fan of go, but the fact having a runtime race detector would imply that there are many problems stemming from shared mutable state is a bit stretching the problem. Go normally uses message-passing to communicate between goroutines. The comment I replied to literally states that message-passing solves data races. Go having a runtime race detector implies Go has data races, and thus falsifies the original assertion. That's literally all my comment is saying. &gt; Yes, the problem for working with shared mutable state is often some sort of locking. But rust also requires locking for that (the type system only helps you prevent omitting locking where it is possibly necessary). So for that reason rust has as much a reason to want to pursue deadlock detection as go has. As its name denotes, the data race detector is a *data race* detector, not a deadlock detector. Go [had an issue requesting a deadlock detector](https://github.com/golang/go/issues/13759) which was conceptually accepted but AFAIK no work on it was done. &gt; Furthermore, it's perfectly possible to have deadlocks without shared mutable states, for example when using channels to communicate. As Go ‚Äî again ‚Äî demonstrates, channels and shared mutable state are not intrinsically exclusive. And though you are technically correct, that point is still irrelevant to my comment.
&gt; The only way to avoid data races I know of other than Rust's system is to disallow mutation at all - e.g. persistent data structures. You can also disallow sharing state, either by copying across the boundary or by only sharing immutable data structures. Erlang essentially takes both approaches at the same time. I would say Clojure takes the latter, though I don't believe it *enforces* it.
I work in a medical imaging startup. We mostly use C++ and Python. Near the end of 2017, we decided that 1) Python was too slow 2) Cython is an horrible unreadable mess 3) we needed to port some key programs to a faster language. Our reflex was to use C++ because we are "experts" in it but we took the time to think about it. I proposed D because I tested and liked it, but Rust actually offers meaningful and important changes. I ported two medium projects to Rust, they are cleaner, clearer, faster, leaner and safer. Seriously, fuck cython! tl;dr: we are slowly transforming our job into a Rust job and we don't regret our decision.
To add on to what others have already said: * No specification for how to handle dates or times. By convention, we've all kind of settled on ISO 8601, but JSON just gives us a string really. If you work with some older web services that offer JSON, particularly .NET-based ones, you're likely to get bit by this. * It's (generally) not streamable, you need to parse the whole document to have a valid object/array. Hence we have [JSONL](http://jsonlines.org/) to compensate for this. * Can't serialize functions, or define variables and reference them later in the document. [Not that it's stopping people from trying to anyway.](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-authoring-templates) * The spec officially does not allow trailing comma. Some parsers will allow it but you can't rely on it. * The spec officially does not allow single quotes around strings. Though JS itself does, and you'll find tons of single quoted JSON out in the wild. 
Thanks for sharing the talk, rather enjoyed it. I don't really tend to write much desktop software, but I like to see someone taking this approach on using GPU rendering. It'll be interesting to see what tech evolves to allow us to build GUIs more easily with GPU rendering. Electron has such success because HTML &amp; CSS is so flexible in building UIs. The downside is you're essentially wrapping a browser, which is not a light operation.
The thing that Rust gives you (without sacrificing safety) is *control*. You can of course clone all over the place (and the proven way to start out with Rust is doing just that), but you can also get by with very little allocations because you have a choice. Likewise, you can of course chase pointers if you tell Rust to do that, but having flat structs and enums works just as well or better (unless you really need refs, e.g. for self-referetial structs), so why bother? What's more, Rust makes doing the fast thing *easy*, so a good percentage of Rust code already starts out pretty fast. The compiler has your back, so there's less *fear* holding you back from trying projects you'd consider beyond your skills in other languages. Once you experience that punching above your weight this way not only improves your skills but gives actual results, you are greatly *empowered*. It is this power that makes Rust your fireflower.
See some of y'all in Paris!
I have been messing with this problem, and it seems that it is impossible to make a const from lazy_static. The closest I got is this: `pub struct i256(BigInt); lazy_static!{ pub static ref MIN: i256 = i256(BigInt::from_signed_bytes_le(&amp;[0u8])); pub static ref MAX: i256 = i256(BigInt::from_signed_bytes_le(&amp;[0u8])); }` in another module you can then `use i256;` and `let test: &amp;i256::i256 = &amp;i256::MIN;` (this works because the module is named i256) but it is ugly and macros will still recognize that the part is just a module and you have to do i256::i256 everywhere for the actual type. I will have to do something else.
BTW, I will be giving a [lightning talk at FOSDEM](https://fosdem.org/2018/schedule/event/bulletinboard_dht/) this weekend.
GC helps you prevent memory leaks that take no effort to prevent in non-GC language and yet leaves out leaks that harder to catch (see leaked descriptors). I think GC is a big lie. They promised us much more than it actually gives. I mean it does prevent you from use-after-free, double free, and some other mistakes, but still leaves you with others. That said, other issues got a little be less annoying in java 8 for example. 
Just for completeness, a middle ground is `Cow&lt;str&gt;`, either with a borrow lifetime or `'static` (so you can borrow string literals or build owned `String`s at your discretion).
I'm currently struggling to write a statistics function. The function takes a Vector of type Vec&lt;T&gt; as a parameter. This vector represents my sample data, for example vec![9, 5, 4, 9, 9, 0, 4]. Note that some elements can occur multiple times in the vector. The function should turn this into a HashMap. The key of that map is simply the element and the value is how many times this element is present in the sample data. In the above example, the HashMap should have the value 3 at the key 9 because the 9 appears 3 times in the vector. At key 4, we have the value 2 and so on.... Is there a proper way to do this? I tried to write a function that provides this, but I'm still relatively new to Rust and I quite could't figure this out.
big +1 to this poist, for sure.
A stop-and-copy garbage collector can be insanely fast. A good stop-and-copy GC will use a stack cache, so it gets most of the benefits of stack allocation. Runtime-wise, GC wins, generally. Folks mentioned latency above, and there it's hard to build a low-latency stop-and-copy GC. Incremental GCs like the one cited above can hugely reduce the worst-case latency, but it's still usually worse. And those incremental GCs pay a runtime cost. Yes, memory usage is the other big killer here. A stop-and-copy GC really wants *a lot* of extra memory to be efficient. The GCed languages generally look worse than they really need to be, because they also generally are in the habit of allocating all kinds of short-lived garbage gratuitously on the heap. When your heap is constantly getting full of cons cells and thunks, you're going to have a bad time.
Whew, I was scouring my understanding of them for a hole hehehe I've found so many bugs in this project in the last 6 months through lots of model checking, crash testing, and concurrency interleaving gnashing that my ego has died 1000 deaths, I'm certain there are many more that will be found as testing continues to intensify and people use the code in ways I haven't tried yet! Thanks for making me question my assumptions, this is always a healthy exercise :)
I'm not sure what sort of if any comments you would like in this thread, however I will take the hopefully correct liberty of stating what I would like to see. I would like to see primarily security errata, such as reproducible builds: https://users.rust-lang.org/t/testing-out-reproducible-builds/9758 Work toward formal specification and specification in general. Perhaps having a compiler in a different language rather than self hosting, though I believe work is already being done toward this and perhaps there is already a compiler written in C or C++? Improved documentation in terms of increasing descriptiveness as well as reducing ambiguous language. I've been learning Rust pretty intently and have to say I love it, especially recently I've really had numerous aha! moments where a concept fully emerges in my mind, I mean recently I started thinking of how Rust essentially has a form of inheritance in subtraits and supertraits with the default methods as well, I mean it isn't called inheritance but it just popped into my head that it is essentially a form of inheritance nevertheless. Speaking of that it would be nice if you could access properties from the default methods in traits, I believe this was being looked into already though. I plan to use Rust as my primary language for all of my personal projects. The primary thing I would like to see is orientation toward the security community, just in terms of reproducible builds, having an easily reasoned about and audited compiler, formal specification, and anything else that could increase the value of Rust to people implementing security critical software in adversarial conditions where nobody can trusted. 
&gt; almost completely specified if I'm not mistaken, the order in which `f` and `c` are called is actually completely unspecified. Which will be an issue if the functions have side effects. The paragraph 3 of the section 6.5 of the c99 standard says: &gt; the order of evaluation of subexpressions and the order in which side effects take place are both unspecified An din paragraph 12 of the section 6.5.2.2 &gt; `(*pf[f1()]) (f2(), f3() + f4())` &gt; the functions f1, f2, f3, and f4 may be called in any order. All side effects have to be completed before the function pointed to by pf[f1()] is called. 
Oh, C...
It might be a good idea to add a comment explaining why the impls are safe, then (and why `Tree` can't automatically implement the traits).
I didn't consider `ref`. How would such syntax look? I ask because you can do `let ref x = &amp;x` as well. At that point you should use the `let` statement directly IMO since any kind of "shortcut" I make is likely to be more confusing than the let statement itself.
&gt; Work toward formal specification and specification in general. Perhaps having a compiler in a different language rather than self hosting, though I believe work is already being done toward this and perhaps there is already a compiler written in C or C++? There is indeed: [mrustc](https://github.com/thepowersgang/mrustc)
1. Automation / Feedback control systems 2. Yes, but not remote at the moment
How did you know? ;-)
`let ref x = y` is equivalent to `let x = &amp;y`. I strongly suspect that it solves the FnOnce issue, but I haven't played with it yet. 
Do you ever read a subreddit before posting to it? You want /r/playrust
thanks......new to reddit
Interesting, I'm just thinking about integration of something like this into my crate (https://github.com/Dushistov/rust_swig ), so not only cooperate via functions calls (as it is now), but also pass structures directly.
Dam that sucks. He is probably just threatened by being outed as not able to learn a new language. Not going to be something easy to change.
The decision is arbitrary, but I guess it makes the hello world example shorter.
Uuuh, I'm in there. Nice, thanks!
GC a big lie? Graphs are *much* easier with GC. GC is amazing in just about any situation where you don't need deterministic destruction.
hire me and I'll tell you. ;)
https://play.rust-lang.org/?gist=5f39d5af6828150449c9656fe484c478&amp;version=undefined :)
Right, but if the API for the comparison functions has them returning i32 instead of bool we're just kinda _stuck_ like that. And if that's the official way to access SIMD then the whole ecosystem is stuck like that. It really is hard, i agree, but it's not like the simd crate is coming out of nowhere. It started even before rust 1.0 and has had a pretty steady versioning rate since (about 1 minor per year, only 7 versions total)
33,370 subscribers CCC = 333, illuminati confirmed.
Still not changing the fact that GC promised more than it gave. Okay, it's easier to build graphs, then what? A few developer nice-to-have paid by the end user at the run time? I'm always more worried about memory in GC'ed language than in non-GC'ed. 
I agree with you completely; I've tried my hand at Go a handful of times and it just does not click for me But apparently some people like it, so it must be usable somewhere
Just realized that I get a burst of joy when I look at a piece of code. I guess I‚Äôve found my purpose on life ;)
He doesn't even try to learn it, he complains that he doesn't like Rust's syntax (doesn't even know about the borrow checker yet). I think it would be hard to learn for him (coming from PHP) but not impossible. But it seems he is in a comfortable place with his ways and doesn't want to challenge himself.. (he's nice as a person though). But I think this tension will only increase over time because our boss wants us to agree on a language that we will both use for future projects, our boss even offered to do a competition: He and I both build the next project separately in parallel for a month, he can use PHP and I can use Rust, then our boss will compare the results and make a decision which one we will use in the future. I immediately accepted the rules of the competition, but that's when he said "no, if I have to use Rust, I'll quit" so now the competition won't happen and my boss is trying to convince me to use PHP for the next project..
Passing structs works really well, I do it a lot at work which is why I wrote this. There are a few gotchas, but they're mostly when calling into managed code like c# (blittable vs non-blittable types)
:D BTW, getting to know how the author pronounces their creation's name is one of the main reasons I listen to tech podcasts!
It is possible, [purescript-waterslide-rs](https://github.com/tomhoule/purescript-waterslide-rs) does this.
I switched from D to Rust in 2014, are people still using D nowadays?
&gt; XXXMMMCCCLXX 3 blocks of triples? That can't be a coincidence
Dyon does not meet all of your criterion, but probably, you still will be interested [https://github.com/PistonDevelopers/dyon](https://github.com/PistonDevelopers/dyon)
Please give some context beyond the image for the benefit of people who don't know what this is referring to.
The missing `ctx` was _incredibly_ confusing to me. Hearing that it was a thread local variable makes be feel kind of sick to my stomach. This new approach seems so much cleaner. FYI I know _literally nothing_ about futures except for reading their type signatures and being horribly confused. I now feel 90% _less_ confused from this one post. Semi-unrelated to the futures crate itself: does anyone know if there is a path for expressing linux file I/O as a future? According to [tokio_file_unix](https://docs.rs/tokio-file-unix/0.4.2/tokio_file_unix/) "This crate is primarily intended for pipes and other files that support nonblocking I/O. Regular files do not support nonblocking I/O, so this crate has no effect on them." While it may be true that regular files do not support nonblocking I/O (is this true? It is completely _shocking_ to me), I would at least like some kind of "backend" that greenthread's file-io or _something_ to improve the performance story here. Without fast file IO I don't see me using futures any time too soon.
How dare you ask such a thing! Jk, I'm using vscode with the Andromeda theme.
What do you mean? _this library has no bugs_ Thanks for the bug report!
I _believe_ `FnOnce` is caused by `let x = x`, i.e. "moving" ownership. When you use the `let` in that way it causes `x` to be dropped at the end of the function so it is `FnOnce` (I am speaking as someone who knows nothing and am still incredibly confused about closures).
Likely numpy/pandas. I work on a data team, and the ETL/Data Science sides both rely heavily on a mixture of PySpark/Hadoop and libraries like numpy, pandas, and scikit.
Is it public by any chance?
Technically yes, but there's not much there yet. It should be bootable in a week or so.
Looks like the author is writing yet another unikernel crate for rust? Because `os` crate on crates.io isn't related to this. I'm saying "yet another" because there are few of attempts and they all are dead. Unikernel is when your application is completely self-hosted and runs on bare-metal. As in there is no OS between your application and hardware, your application is the OS. Well, usually those frameworks target hypervisor like Xen. See http://www.includeos.org/ for more information in general.
I found this [blogpost](http://kkourt.io/blog/2017/10-14-linux-aio.html) that covers async io on Linux. It even mentions rust and tokio.
Huh, I looked for the os crate and somehow missed it. I guess I'll call it something else. EDIT: Can you point me to the failed attempts?
L = 12th letter = 1+2 = 3 X = 21th letter = 2+1 = 3 So basically, LXX stands for 333 and is the fourth triple. 3 + 4 = the holy 7! I always knew the creators of Rust are very special people. Who else could develop such a great programming language. Do you see the illuminutty eye in the R and also on the right side near the corner 
This is a kind of "No True Scotsman" thing. You're claiming that message passing isn't message passing unless it also has additional restrictions. It'd be better to be clear about what we are talking about instead of arguing about what is and isn't message passing. The fundamental problem is aliased, mutable state, for which there are multiple solutions.
For what it's worth, I didn't downvote you, but it is common courtesy to at least watch/read the content in question before speculating about technical details / design decisions.
I think you could generate bindings for java easily as well, via JNI.
It should be relatively easy to add. Pull requests are always welcomed üôÇ. Or if you could build an example Java class, I could try and implement it
I wouldn't call them failed, I'd say they just not maintained or active: https://github.com/uni-rs/uni.rs https://github.com/CRust-OS/CRust-OS Not unikernel in rust, and not exactly unikernel, but still related ‚Äî https://github.com/rumpkernel/rumprun-packages/blob/master/rust/README.md 
I had the exact same thought! The JVM seems to me to be the worst choice for high performance and data processing. I have spent a year on Spark and my doubts were confirmed. I can't count the number of days lost because of OOM issues, often for stupid reasons like the JVM not being set to the right memory size for an environment, etc... The boot time of a Spark cluster was also terrible (2-3 years ago though...) I think hadoop/Spark came at the right to fill a gap in data processing and have gained lots of traction now. I really feel like so many things could be improved both by changing few concepts and the core language.
Thanks for this. I've actually been following xi since it was first announced. Re-reading my comment, it certainly does sound like me speculating on the design, but my intention was to play devil's advocate to the OP's question. 
Isn't leaking considered "safe" now? I.e. things are not absolutely guaranteed not to leak in a well formed Rust program?
I hope so! I have some very, very cool ideas for it! 
According to the reference, this is indeed [undefined behavior](https://doc.rust-lang.org/beta/reference/behavior-considered-undefined.html): &gt; Mutating non-mutable data ‚Äî that is, data reached through a shared reference or data owned by a let binding), unless that data is contained within an UnsafeCell&lt;U&gt;. If you want to do this, you can definitely use struct Ref { inner: UnsafeCell&lt;usize&gt; } or struct Ref { inner: UnsafeCell&lt;*mut usize&gt; } I don't believe `UnsafeCell` has any runtime cost, it's purely a marker.
I don't believe this is possible - `type` seems to only allow access to functions, and associated constants, as the error message indicates. This just isn't something `type` is meant for. See https://doc.rust-lang.org/beta/reference/items/type-aliases.html: &gt; A type alias to an enum type cannot be used to qualify the constructors
I would love to see the github behind this, and maybe contribute in the near future 
Awesome, very cool useful macro. Just because of the crate name: But what I do have are a very particular set of macros, macros I have acquired over a very long career. Macros that make me a nightmare for closures like you. If you let my variable go now, that'll be the end of it. I will not look for you, I will not pursue you. But if you don't, I will look for you, I will find you, and I will compile you.
Many people had similar bad memories about dynamic allocations and managed memory. Then rust arrived. Why couldn‚Äôt there be a sharing mechanism designed with safety in mind at the OS level.
Rust `std` isn't Posix; it's agnostic between platforms and at least one of them doesn't act like Posix (Windows, minus CRT). IIRC the entry point typically does *not* get arguments on the stack. C programs have a little bit of runtime support code which counts the arguments before calling `main`. Basically the only reason to do it like C is because that's how C programs start on Unix. Rust isn't C or Unix-specific so it makes sense to do things as simply as possible. Note that `main` is a safe function, so it's allowed to panic, so panic handling needs to be initialized before `main`.
yes, it is.
So then you should probably close your sockets explicitly, or no?
RAII should handle it in the case of Rust . GC's only collect memory - this leads things like Mutex's, Sockets, File handles, etc to the user. This is actually extremely annoying - I have seen many Java bugs that come from this. And often the GC makes it even more confusing - there is the notion that things will just be freed, oh except you have to remember to free literally everything else.
Better than Heartbleed.
Ohhh, wow, really? GCs don't call a destructor? That's really dumb. Or is it more that you don't want to wait for something like a file or socket to be GCed for it to be freed, because they're more limited than memory?
&gt; Or is it more that you don't want to wait for something like a file or socket to be GCed for it to be freed, sometimes it's about the wait, sometimes it's that it's non-deterministic.
&gt; you can basically ignore yields and borrow-check generators just as if they were normal functions; Had some trouble parsing this sentence because it sounded like it was saying **ignore both yields and borrow check generators just as they were normal functions** as opposed to **ignore yields, just borrow check generators as if they were normal functions** I'd suggest changing it slightly to make it easier to read &gt; You can basically ignore yields, and simply borrow-check generators as if they were normal functions. PS: Good article thanks for sharing üëç
What he said
What if, we could have custom move semantics? Every time an object is moved, the compiler implicitly applies custom move semantics (where we basically relocate the references by making new reference from the original object) through a special magic trait impl for the desired type (the generator in this case). This is basically C++‚Äôs move semantics but in rust it could be much safer and more constrained.
[removed]
You still have to make sure the message getting passed doesn't get held on and modified by the sending go routine right? I haven't worked in go but if I have a message that's says an entity was changed, with a property being the entity itself isn't it possible two go routines have access to that entity and thus you can have a race condition?
I agree, this is quite tricky. But in this case, I could think of one way to solve this, by restricting the rules for relocating a reference (or in this case references in the heap) even further. Whenever a field has a self-ref lifetime, the whole field gets ‚Äòinvalidated‚Äô and must be re-initialized by the special trait impl.
What all architecture will this run on? 
A `NullPointerException` is safe and, name aside, not a pointer error in that sense.
x86_64 to start, but it'd be fun to get it to run on arm as well.
I'd like to see high quality support for the Arrow in-memory columnar data format. I know there's something in the works currently but it's still early and depends on Flatbuffer support which also hasn't landed yet. Having Arrow support would enable rust programs to interoperate with Java python Etc and not have to worry about all the boilerplate of data input output Etc.
From a user's perspective, thanks for sled. In Go, I always reach for boltdb, and sled is the closest thing I've found in Rust.
Great article! :) So that I'm understanding correctly, are you saying the hard part is the compiler would need to figure out that all `'array` references are self referential and thus would to be rewritten? Or that the hard part is once you know the above, finding out *what exactly* each of the `'array` references was referencing in order to correctly rewrite all the references to be pointing at the correct new address? The former seems almost trivial because it'd be any lifetime *not* parametrized in the struct definition itself, and could be further "helped" by using lifetimes which match field names (which admittedly would technically be a [minor] breaking change introducing new context sensitive lifetimes), no? The latter does seem perplexing to me, especially since I'm not familiar with the compiler internals. The naive solution would be these "special self references" becoming a fat pointer and carry additional information such as the offset.
Adding a new ?-trait isn't necessarily a problem from a complexity point of view, because `?Sized` implies `?Move`. Thus in most cases, e.g. the `HashMap` case, the signatures wouldn't change. But the other issues outlined there still apply.
Dude you are a friccin genius
What if the reference is being stored as a `*const` somewhere internally (possibly tracking the lifetime with an associated `PhantomData` for example)?
How would you reinitialize a vector? How would you know what the references in it were pointing to? This seems like just restating the problem.
Definitely not. Just lazy
I don't agree, but this is a judgement call so YMMV. There were many places in the standard library that gained `?Move` bounds in the initial PR, too many in my opinion.
Did you read my post? I am talking as the receiver.
Potentially cyclic graphs, yes. That's what GC is built for in the first place. Graphs which are guaranteed free of cycles can be modeled by Rc, at a far lower cost in the typical case. And known graph structure can be exploited to make memory management even more efficient.
They *will* call a destructor, but you really, really don't want to rely on it. Relying on destructors is considered an antipattern in Java, in my experience.
Python uses reference counting for it's garbage collection, but there are things you need to keep in mind. As the other commenter said, you can't mutate the value in an RC, or share them. Also even with weak references, you need to watch for reference cycles. Python has a small garbage collector just for this that detects cycles. There's also a [GC](https://github.com/Manishearth/rust-gc) for rust that apparently is in working condition, but also in dev right now. You might want to check it out.
Graphs are not that hard without GC. Especially if you're willing to take a GC-comparable performance hit to implement them.
A thought I had was : What does C++ do? MSVC supports async/await and IIRC there are multiple proposals for this. I find it helpful to reason about borrowing errors (and other borrowing related things) from the POV of "how could this break in C++?". While Rust sometimes does throw borrowing restrictions which aren't 100% necessary, most of the time it's something that would cause a segfault in C++ (or Rust, really!) for some set of inputs. Understanding the potential unsafety has often helped me understand what I need to change to fix it. Now, C++ doesn't guarantee safety the same way we do, however it would kinda suck if async/await would silently introduce UB whenever you take a local reference. C++ doesn't have the lifetime scaffolding to be able to accurately distinguish between a local reference and an outer one (a function/method call returning a reference could be _anything_) -- so it can't even warn here. I suspect the solution involves move constructors, but you still need a way of _detecting_ it. Worth trying out pathological cases in MSVC and seeing what the generated code does. I think Hopper has a mode that takes C++ with debuginfo, compiles it, and decompiles to C with structs and such. Useful for seeing how e.g. vtable dispatch works.
I was probably over-eager with the simplify. You're welcome to keep the ribbon tweaks and scrap the other mods if you'd like.
best you can do is have a to_bool method on the types you want. ```rust trait Truthy { fn to_bool(self) -&gt; bool; } ```
&gt; I was probably over-eager with the simplify. To be fair, I was trying to accurately and perfectly reproduce every minor hand-tremor. To be even fairer: most of the problems introduced by simplify are unnoticeable unless you zoom in and go looking for them. I'm just annoyed because I spent a lot of time and effort putting in all that unnecessary detail; it could have *at least* done a better job when removing it. :D (I was up until early last night working on a trace where I was doing individual eyelashes because I'm insane and someone please help I can't stop) Aside from overbearing fussiness, I have no complaints about how it looks. The only thing that *really* bothered me was the red leaking into the nice greyscale design. If you can update the shirts and mug so that red bleeding isn't there, that'd be optimal... but it's not like it's the end of the world if not.
In particular, one of my favorite aspects of "move == memcpy" is that memcpy is nothrow, or in other words cannot panic. This makes a lot more code completely panic-safe, including a lot of unsafe code that now has much less to worry about.
Fair enough. Then I'll rephrase my point as ‚Äúthere are ways to make message-passing which prevents data-races without needing a borrow-checker‚Äù which is the point I wanted to make in my original comment.
Where is the `usize` that it's pointing to being kept? That's what governs whether or not the behavior is undefined.
Thanks, I wasn't aware of that. Afaik JavaScript, which is the language I'm the most familliar with, doesn't have that so I supposed it was common for managed runtimes to handle this automatically.
Thanks!
Well, forced allocation seems inevitable for the tokio use case, where you have a scheduler managing a bunch of heterogeneous generators. For the iterator use case, I agree.
I'm pretty sure it's backwards incompatible, due to unsafe code among other reasons.
Oh, in retrospect that was an obvious solution. Kinda disappointing. But if rust doesn't try to solve problems C++ has given up on, then who will? :P 
Right, otherwise we're basically doing segmented stacks with one segment per call. Ouch.
Is there a guide for publishing CLIs? It's such a noob question, but I'm still not sure how you go from cargo run to having a command that you can run by invoking it directly by name.
`usize` was for demonstration purposes. It's actually a small struct that includes a reference count, it's stored in a large array I'm using as a sort of garbage collected heap. Other code can access it (setting or getting values) via other `Ref`s, but not get references to it (or anything in it) except in the form of a `Ref`. Everything is single threaded. `Ref` is not guaranteed to outlive it... the last `Ref` pseudo-drops it (puts it in a free list, which is represented as a linked list with the next pointers stored in a `usize` field and the head pointer being a global `*mut Block`).
You don't necessarily need to patch references when you move. Generators could check whether they've been moved as first thing after resumption, and do patching then. Finding references from heap is the only real problem with this approach.
I believe this was brought up in the discussion but there was some reason why it wouldn't work well. Can't look it up right now but it should be somewhere in the recent implicit bounds discussion
When you run `cargo build` or `cargo run` the executable you can run by name is created in `target/build/&lt;program name&gt;`, when you run `cargo build --release` or `cargo run --release` it is in `target/release/&lt;program name&gt;`. If you run `cargo install` and you let `rustup` setup your path (or set it up yourself) it will build your program and put it a directory on your path so you can just type `&lt;program name&gt;` to run it. In terms of publishing to distribute to other people, you could publish to `crates.io`, or upload executables somewhere.
Do unikernels come with significant performance improvements given the impact of spectre/meltdown mitigations?
I highly approve. Maybe Rusty can be the "nightly" mascot, being crepuscular and all.
Note that if you want to put this on non-Copy types you'll want to have it accept &amp;self instead of self, but most things that one might want to make "truthy" are probably Copy already. I guess you might want truthy vectors that are true if non-empty and stuff.
`take!(=cloned_var);` This is a solid experiment around what makes closures ergonomic and flexible. Thank you for making this!
but what the point if the code is unsupported even since?
Implemented with care, your use-case sounds all right, though as always, the devil is in the details. Of course, I meant that `Ref` should *not* outlive it as then you'd have a dangling pointer. You can statically guarantee that with lifetimes using [`PhantomData`](https://doc.rust-lang.org/std/marker/struct.PhantomData.html#unused-lifetime-parameters), I would definitely recommend looking into that. You can't accidentally send `Ref` to another thread because `*mut _` disqualifies it from being `Send` or `Sync` without an explicit impl so you don't have to worry about race conditions. The only other issue I can think of is mutable aliasing, if you have two `Ref`s pointing to the same `String` or `Vec` or something and you mutate it through one while the other one has a reference to the inner data then you could end up with a dangling pointer there. 
Yeah! I went ahead with the &amp;self route. It should make more sense for my project. Thanks for the advice!
Out of blatant curiousity, is there a way to reverse the order with sort_by_key?
This is definitely a piece of the solution.
Javascript specifies how collections behave if you modify them while iterating, but that doesn't mean that the behavior is intuitive. Modification during iteration is nearly always a bug, whether it throws or not.
Java would be a much better example. Java specifies evaluation order completely (left to right).
Consolas
I'm really puzzled by the paragraph about immovable types, especially by this sentence : &gt; Can we really afford for the Q parameter of `HashMap::get` to become `Q: ?Sized + ?Move + Hash + Eq?` If I understand it well, `?Move` mean : that parameter doesn't need to be movable. Which means your function now accepts parameters that are either movable or immovable, right ? But how can you accept an immovable parameter to a function, since passing a parameter to a function is almost the literal definition of a ¬´move¬ª ? Unless your type is Copy, when you pass it to a function you move it, then I don't understand how a `?Move` bound even make sense. There has to be something I'm missing. Can someone enlighten me ?
I've got an idea for a dynamic pointer-fixup scheme for when a generator is moved. It'd require all captures of a generator to implement a trait that looks like this: pub struct LastLocation { pub start: *const u8, pub end: *const u8, } pub trait FixupPointers { unsafe fn fixup_pointers(&amp;mut self, last: &amp;LastLocation, new_start: *const u8); } Every time `Generator.resume()` is called, it checks its `&amp;mut self` pointer against `LastLocation.start`, and if it's changed it updates its `LastLocation` and then iterates its captures calling `.fixup_pointers()` with the `LastLocation`. The impl of `FixupPointers` (which can be derived) looks at all references and pointers in the type and determines if they fall between `start` and `end`; if they do, it computes the offset between `start` and each pointer then updates that pointer to `new_start + offset`. This could be slow, but would only need to be done when the generator is moved (which it can determine automatically). Client code (as well as most code in the stdlib) can simply derive `FixupPointers` which would also recurse into its members if they implement `FixupPointers`. Then if the user actually wanted a pointer that doesn't get fixed up, there would be a `NoFixupPointers` wrapper with a stub impl. 
You'd want `sort_by` in that case. The documentation has exactly that use case in one of its examples.
Why not take a feather out of C++'s cap and have the compiler allocate the enum, but only if self-references are used, in a manner such that the distinction is transparent to the programmer?
So unikernels are a different take on virtualizing like containers? So if I am understanding correctly, containers create an isolated environment for the apps to run and still depends on the host kernel to communicate with the hardware, unikernels are application binaries that come compiled with all required bits of a kernel to directly utilize the hardware?
I found this [futures-fs](https://docs.rs/futures-fs/0.0.3/futures_fs/) crate by /u/seanmonstar . It runs file i/o on a seperate thread pool.
I'm wanting some feedback on ergonomics for pivoting on results from a REST API. I have a set of structs, `Song`, `Album`, `Artist`. An `Album` has an internal list of `Song`s, and `Artist` has a list of `Album`s. Say I query for a particular artist. That returns an `Artist`. Now I want to look at one particular album that artist produced. Now I want to get all the songs in that album. How do I do this in a way that feels natural to Rust? --- Currently my API is backed by an abstracted `reqwest` `Client`, so all fetches have to run through that. Thus, getting the songs out of an album looks a bit like `Artist::albums(client: &amp;Client)` and `Album::songs(client: &amp;Client)`. This is repeated quite a few times in various interfaces. However, this starts to get awkward for anything requiring arguments and a reference to self, because it's not naturally clear why it requires a handle to a client; for example, `Song::similar(&amp;self, client: &amp;Client, number: usize)`. Other alternatives I've thought about: - A Rust DigitalOcean library creates a version of a `RequestBuilder` on each possible request, and manually sends it with `execute(&amp;Client)`. This seems okay, but very wordy if I only have one or two fields in a call. - I could thread a reference to the calling client through each and every single result from the API. This would allow only having to use a `Client` for a top-level call and not for pivoting. However, the number of references could spiral out of control very quickly, it messes with (de)serialising data, and I'm pretty sure it means I can't make them `Send` or `Sync` safely. - Do what most REST APIs do and require having to issue a new call from the `Client` for every single call; i.e., `Client::similar_songs(&amp;Song, number: usize)` or `Client::similar_songs(id: usize, number: usize)`. This feels very awkward to me though.
What is SLG? I did some unsucessful googling so I figured I would ask here.
&gt; There's also a GC for rust that apparently is in working condition I'm using it in my prolog interpreter. The only complaint I have is that it doesn't (yet) have a weak gc type (I talked to the author a bit about that but neither of us worked on it). I originally used Rc and just allowed the leaks. In fact, because of the way I implemented hash-consing my interpreter will be very leaky until someone adds a weak gc ref type. Other than that, it's worked well and was easy to switch to.
Hmm...Could this be pushed a bit further with deref coercions?
get takes an argument of type `&amp;Q`, not `Q`. You can borrow `!Move` things, that's not moving them.
https://github.com/lachlansneff/crateos
&gt; looks at all references and pointers in the type and determines And how do you do this if some of them are stored in the heap somewhere? How do you know how to traverse every data structure anyone could ever write? What if they're stored as something other than a reference or a raw pointer, like a usize or an isize. You can (and I have) store pointers as `AtomicUsize` if you want to.
Not in a way that could be consistent across the code base (orphan rules), so probably a bad idea to go that route.
I had no idea either. Found some info in an old post, linked to this article: https://link.springer.com/chapter/10.1007/3-540-48159-1_12 if you want full pdf copy PM me
Yeah I was just thinking about this. I think the required trait impl would be better. For most user types it can simply be derived, for datastructures it can be implemented by the datastructure author (or maybe derived if the datastructure has an `impl IntoIterator&lt;Item = &amp;mut T&gt; for &amp;mut Self`). I think the optimizer can eliminate cases where the `FixupPointers` impl is trivial.
Look this http://smallcultfollowing.com/babysteps/blog/2017/10/21/chalk-meets-slg/ This blog is one of series about trait logic resolution in rust.
I hadn't thought of that, but you immediately have my support.
Thank you very much!
But you can't move out of a borrow right ? Oh no, you can always `mem_replace` it. Which means defaulting to `?Move` for bounds to things behind a references isn't an option because it would break all code using this function. And if you add an implicit bound to `Move` everywhere, that makes the immovables types pretty limited ‚Ä¶ I think I'm starting to see the problem. I guess there isn't only `mem_replace` in that situation ? 
Last time I was one of those to suggest offsets, and was just a bit taken aback by the lack of explanation, but your rigorous treatment was worth the wait. Kudos! L That said, nothing in Rust *forbids* you create a type that can store a pointer minus its own position. Best of both worlds.
How about making unsafe `Move` trait with default `memcpy` implementation and require users to implement it by hand in case if struct uses self-referential lifetimes?
You can `mem::replace` a `&amp;mut Q` but not a `&amp;Q`.
use std::default?
https://gitter.im/diesel-rs/diesel?at=5a7271057dcd63481f018faf
&gt; In most languages, message-passing is implemented by sharing the message between threads, which does not solve data-races, at all. Unless the messages are immutable. Languages which have gone hard for this approach are usually functional ones where data is always, or strongly expected to be, immutable - think Erlang, Akka in Scala, etc. Go has an emphasis on message-passing and also mutable data, but then Go is Go.
Tcl would be an even more fun example, as c could contain an upvar call which changes the value of d in the enclosing scope!
&gt; Rust seems to be really popular (and getting more popular) in areas that could, one might think, be adequately served by a garbage collected language. [...] If it's memory safety and fearless concurrency that we want, why aren't we satisfied with garbage collection and message passing? I think there's a misconception here. People aren't using Rust to write these things because GC'd languages aren't satisfactory. They're using Rust because they love Rust! If your goal was to write a server doing some complicated business logic, without pressing latency or memory constraints, and you had no personal preference as to language, then you wouldn't choose Rust. You'd be more productive in something comparably high-level, but garbage collected (and with a faster compiler!). 
If you don't want to use `sort_by`(like /r/Throwmobilecat suggested) you can use something like this: https://play.rust-lang.org/?gist=0f109640fb785d1545d18ac28f5e9d1e&amp;version=stable
eh‚Ä¶ If you run a unikernel on bare metal, you're not vulnerable to spectre/meltdown since you're the only process on the system, there's no untrusted code running side by side, typically no concept of users/processes or anything. If you run many unikernels in a shared hypervisor (e.g. VPS/"cloud")‚Ä¶ you've kinda reinvented a multitasking operating system, but every switch between processes is now a switch between VMs. [Spectre does work across VMs](https://security.stackexchange.com/questions/176709/meltdown-and-virtual-machines). So maybe instead of doing that, the best way to run untrusted applications is to still have a proper unix box, but define a simple sandboxed ABI‚Ä¶ and hey, [CloudABI](https://nuxi.nl/cloudabi/) is now [supported by Rust](https://cloudabi.org/write/rust/) ;)
&gt; I guess I‚Äôve found my purpose on life ;) Code reviewer? 
Either negate the field (if it's signed) or use this: https://doc.rust-lang.org/std/cmp/struct.Reverse.html
Sadly docs.rs does not work because it is using a very old nightly, so I have to build the documentation myself and upload it to gh-pages. I am using cargo doc-upload for that which worked for a while but it seems that it stopped working again for some reason :/
Sure, Latin is technically still in use today in the Vatican at least... http://www.vatican.va/archive/aas/index_sp.htm (note: parts of the documents are in Italian, but must are in Latin)
Thank you for all your answers, this has been an interesting discussion. It seems that there are a few [niche cases](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtj1rqv/) where garbage collection is not practical: tiny microcontrollers, operating system kernels, and applications that **really** need [very low and predictable latencies](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtj20iy/). But I don't think this applies to the vast majority of software that is written. I found the discussion around "fearless concurrency" much more convincing. Message passing is insufficient to guarantee safe concurrency if it is possible for a message to contain shared state. That's ([almost](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtj04mq/)) impossible in Erlang but [possible](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtk8dn4/) in Go, although [discouraged](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtj1dyf/). IIUC, it's [actually impossible](https://doc.rust-lang.org/book/second-edition/ch16-02-message-passing.html) in Rust. Rust also has some other nice features, e.g. compile-time ownership/borrowing checking for [resources other than memory](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtiroet/) and [immutability by default](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtisn6u/). However, the cost/benefit analysis may not [always favor Rust](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtkdmc9/)! Sometimes, it may even favor [Go](https://www.reddit.com/r/rust/comments/7uaaqc/the_drawbacks_to_garbage_collection/dtis5ms/)... ***** In the above, I linked to comments by: /u/llogiq, /u/matthieum, /u/fgilcher, /u/masklinn, /u/steveklabnik1, /u/claire_resurgent, /u/tomwhoiscontrary and /u/myrrlyn but there were lots of other good comments in the threads too, thank you all!
Why not `Into&lt;bool&gt;`?
Deleted my previous reply because I realized you were asking about the derive, not the intrinsic. The whole idea is that the type provides its own impl of `FixupPointers`; the derive would literally be just calling `FixupPointers` for each field of the type. Datastructures would provide their own `FixupPointers` impls that would iterate the datastructure and run fixups internally. Like with `Vec::drop()` the optimizer could eliminate the iteration when the `FixupPointers` impl for the element type is trivial. If you're storing pointers not as pointers directly on the stack in the generator, then I guess there is a movement issue there. We can perhaps warn on code casting pointers to integers inside a generator, provide a struct that implements `FixupPointers` but just calls through to a closure so they can fixup pointers they've cast to integer variables. `AtomicPtr` would not implement `FixupPointers`, requiring the user to implement it themselves for their use-case. We can combine it with the immovable generator solution to make it an either/or. Either all the captures of your generator implement this trait, or the generator is immovable. Maximum flexibility.
How would the desugaring work for non-movable generators? Say |a, b| -&gt; { let c = mkBorrow(b); yield a; use(c); } In the most naive desugaring enum Generator&lt;A, B, 'a&gt; { S1(A, B), S2(B, &amp;'a B), Done } B is moved between mkBorrow and use. Not all lifetimes can be well nested so moving outliving data to the beginning of the enum doesn't always help. So does the desugarer need to add padding so borrowed data stays at the same offset between states? 
You can take a look at https://github.com/savant2212/solo5-rs it's my try to make rust unikernel on top of Solo5(base layer for mirage os). It compiles and have heap allocation, but now i don't have enough time to improve it to support ip stack and storage :(
Hi, I just wrote a very simple program that shows probability of distribution. Imagine a pyramid in which you pour balls into the tip. The pyramid has seven layers and on each layer there there is one more joint splitting the chance to arrive in a particular distribution bracket in the lowest level of the pyramid. I wrote the code by copying basically if else clauses, if anyone could advice me to 'improve' that code, I would really appreciate it. I know how to use macros, but I am not sure how to add a loop that adds an additional if-else loop for each subsequent layer. this is important because: At level 1 the ball has a 50/50 chance to either take path 0 or 1, but excludes the 7th bracket on the last level already and so forth. Thank you very much.
&gt; the current C++ solution is just to forcibly allocate all generators. Noteworthy is that there is an LLVM implementation of this, and while you are correct in that they forcibly allocate all generators, the implementation comes with some optimizations to elide this allocations in the cases in which they are not necessary. AFAIK these optimizations are pretty reliable.
&gt; move == memcpy is a really nice property. Sure, but this results in expensive move operations for many types already so might be a problem worth fixing anyways.
Yep, I didn‚Äôt think this through.
When it gets to a more developed state feel free to post this on /r/UniKernel 
Is this the same position that's been open for some time now and has yet to be filled? Or are more Rust developer positions opening up at MaidSafe?
I don't whether there are any persistent collections in crate.io, I realy like persistent collections in clojure and haskell, Bring them to rust is a greatt idea.
Will do!
There's a pattern in the way your start and end values are selected for the RNG. Let's take level 3, for example: if r == 0 { r = rand::thread_rng().gen_range(0, (three.len() - 2) as u8); } else if r == 1 { r = rand::thread_rng().gen_range(1, (three.len() - 1) as u8); } else if r == 2 { r = rand::thread_rng().gen_range(2, three.len() as u8); } Notice that as `r` increases, the end decreases at the same rate. Likewise, your start is the same as `r`. You could replace that with this: if r &lt;= 2 { r = rand::thread_rng().gen_range(r, (three.len() - (2 - r)) as u8); } Additionally, I would suggest making `r` a `usize`. It's only used for indexing, so it would reduce the amount of casting required. You also have multiple lines like this: `five[r as usize] = five[r as usize] + 1;`, which could be replaced with `five[r] += 1;` I would also suggest calling `thread_rng` once, and reusing it. In this specific code, because there's a predictable pattern, you could also put references to the six arrays into a single array, and do the calculation by looping over them [like this](https://play.rust-lang.org/?gist=b2e867851d01f1c13ce8b8e52f498010&amp;version=stable). That would also make it easier to add new levels.
I find it slightly difficult to discern exactly what this macro is useful for. I would have appreciated a motivating example in the module documentation with no hedging language. Something like "When writing a closure it is easy to end up in &lt;problematic situation&gt;: &lt;example broken code&gt;. You can fix this with &lt;strategy 1&gt; or &lt;strategy 2&gt;, both of which have disadvantages: ... With the `take!` macro this becomes cleaner: &lt;beautiful example&gt;" I think this would be a good way to distill the essence of the macro. Congrats on launching! :)
We filled the Rust role but it fell through so we are looking for a replacement plus an additional developer. 
Thank you for giving me the heads up on this, I tried posting again today and the site is showing as having no rust jobs in the world at the moment
Thanks for this :)
I've put up a separate thread with all the details
For now we'll only hire locally, we're not prepared for a remote team
I'm not sure how you'd do it in Java. Java doesn't have any way to take a chunk of memory and convert it to an object with the same layout, which is what C++ and Python are doing here (i don't know C#). One option would be to parse a byte[] or ByteBuffer. Another would be to wrap an index into a ByteBuffer and decode fields on the fly. The latter is what i'd usually do if i needed efficient random access to a pile of native data. Neither of these involve JNI - you would separately use JNI to move a pointer to a buffer into Java. To generate the conversion code, you would have to be able to work out offsets for all the fields. That means knowing the sizes of the types, and the rules of C struct layout. You certainly could put all that in the generator script, but it's going to be twice the size of the other generators!
/u/ralphlinux have you given any thoughts to rendering images within xi-test (because you mentioned not doing gamma correction, amongst other things in the talk)? Also, any thoughts about editing TeX or similar? (e.g. LaTeX?) Some editors (e.g. emacs) let you edit the "rendered" document, by switching to text a small part of the framebuffer when you click on a formula and switching it back to rendered LaTeX when you step with the cursor out of it.
it's a really funny joke
Yeah, JavaScript doesn't throw an Error, but it's surprisingly complicated to decide what "handle this automatically" would exactly mean even in quite simple cases. For example, check if you can guess what these pieces of ES6 JS code print in the console. I guessed many wrong myself ;) **Iterating with forEach + mutating the list:** const list = ['a', 'b', 'c'] list.forEach(el =&gt; { console.info(el) list.push(el) }) console.info(list) **Iterating by index + mutating the list:** const list = ['a', 'b', 'c'] for (let i = 0; i &lt; list.length; i++) { const el = list[i] console.info(el) list.push(el) } console.info(list) **Iterating with for-of + mutating the list:** const list = ['a', 'b', 'c'] for (const el of list) { console.info(el) list.push(el) } console.info(list) **Iterating with forEach + changing the variable** let list = ['a', 'b', 'c'] list.forEach(el =&gt; { console.info(el) list = [el] }) console.info(list) **Iterating by index + changing the variable** let list = ['a', 'b', 'c'] for (let i = 0; i &lt; list.length; i++) { const el = list[i] console.info(el) list = [el] } console.info(list) **Iterating with for-of + changing the variable** let list = ['a', 'b', 'c'] for (const el of list) { console.info(el) list = [el] } console.info(list)
1) You should be able to do this in Java without many issues, and this tool should be able to handle it once the rules are set. The basic rule of thumb is, if you can call C code from it, you can call Rust from it. 2) C++ isn't making a copy of the data. Actually, it's the only one of the current target languages that isn't making a copy (in most of my implementations). I return a struct on the stack to C++ that contains a void pointer to the heap with the necessary information for the C++ code to know which struct to de-reference that pointer to. It never makes a copy unless someone physically makes a copy themselves. 3) C# does make a copy using pinvoke. But there are some gotchas here. The biggest is understanding the difference between [blittable and non-blittable types](https://docs.microsoft.com/en-us/dotnet/framework/interop/blittable-and-non-blittable-types). You'll notice that this tool won't even try and translate structs that don't have a #[repr(C)] on them. This is because that directive tells the compiler to use C-compatible memory layouts, which the other languages understand.
Great, I'm pretty sure I've got the details right then, whether the mutation was legal was the only thing I was really worried about :) Instead of `PhantomData`, I just make sure everything inside the struct is `'static`, which I think is a stronger (perhaps unnecessarily so) guarantee?
Indeed, but that doesn't change anything.
there's no reason to add inline unless you've measured that it makes a difference.
&gt; I have no idea how to reconcile the Rust release cycle with the annual or semiannual release cycle many distros have, but it‚Äôs something the community must figure out. I spend way too much time trying to evade distro package managers, and get the Rust "manage the install yourself in the home directory" experience with other software. [I'm not alone in this either, there are serious issues with such a delayed release cycle.](https://youtu.be/uZI_Qla4pNA). I think we should _consider_ getting Rust projects like ripgrep onto package managers, but I really worry about putting the compiler up there and regressing to the third-party reliant ecosystem of C++.
Yeah, I thought it would be nice to make e.g. a rumprun unikernel that just runs a CloudABI app. But looks like the CloudABI patches for NetBSD haven't been updated in a long time‚Ä¶
&gt; I don't believe UnsafeCell has any runtime cost yes and no. like, what it does is, it removes an optimization possibility. In other words, a normal `&amp;T`, the compiler can assume it's not mutated. With `&amp;UnsafeCell&lt;T&gt;`, it can not.
&gt; Macros 1.1 encompasses three features: custom derive, procedural function-like (bang [!]) macros, and procedural custom attributes. That's "macros 2.0", "macros 1.1" is custom derive only.
Persistent collections really rely on sharing, otherwise all mutating operations have to start by memcopying everything. In rust you could do this with Rc or but this means you pay a higher performance penalty and don't gain the concurrency advantages.
Or `From&lt;custom_enum&gt; for bool`
Note: I managed to have a total brain fail here and both wrote and also therefore said ‚Äúdeadlock‚Äù instead of ‚Äúdata race‚Äù over and over and over again. ü§¶üèª‚Äç‚ôÇÔ∏è Anywhere you hear ‚Äúdeadlock‚Äù just substitute ‚Äúdata race‚Äù.
I've been using [rpds](https://github.com/orium/rpds) for some things. Alas, I'd like to [be able to switch from an `Arc` as the underlying smart pointer](https://github.com/orium/rpds/issues/7). I wouldn't use them for anything performance critical.
I've implemented Display for my struct. But I want to print Vec&lt;MyStruct&gt;, I can't implement Display for Vec so how should I deal with this?
I got the impression you were more making a philosophical point than speaking from experience. Your approach is obviously fine for "offensive" things that don't actually matter. But when a boss or coworker seem to be implying things about your role, ability, work ethic, or motivations, how can you regain power by ignoring that? So I guess my problem with "offense is taken, not given" and the point I was trying to make is that (1) it does take some effort on the part of the receiver, and (2) it only really works in cases where someone's opinion of you genuinely doesn't matter.
It's for this reason that I put the pronunciation on the readme for my current project.
It shouldn't be transparent. You want to know when you allocate.
let's make a project which relies on constant parity updates and never update. that shit is hilarious. but when you see the same joke 10000th time, all i can smile at, is how pointless life already is, yet people actively choose to make it more of a waste.
There's a bunch, actually -- take a look on crates.io! :)
You can construct the state enum before the borrowing occurs: *self = S2(mem::unititialized(), mem::uninitialized()); ptr::write(self.0, b); ptr::write(self.1, mkBorrow(b); return Yield(a) (the above won't compile but you get the idea)
The delayed release cycle is great for **users** and crap for **developers**. I never build anything from source if there's a Debian package for it I can use instead, it's not worth the headache. The only exception is, of course, the programs I'm developing. We're a community of developers here though, so it's natural to be dissatisfied with this. ;-) The primary purpose of the Debian rustc package is to compile the Rust packages that are part of Debian. If 1.14 can do that then it's Good Enough, if not quite ideal.
Typo: `Arc::make_mut` vs `Arc::get_mut`
&gt; I think a great way to judge how Rust is doing is to see there be more programs in Rust that aren‚Äôt *about* Rust. I think this is quote of the week material here.
I disagree. It's an excellent example of how Rust is different from a language like Python or Java that distinguishes by-value types (primitive) from by-reference types (object) and has different aliasing semantics for both. 
I was under the impression that procedural macros in general were a stepping-stone towards a unified Macros 2.0. Ya'll gotta stop renaming things.
With this, we're talking about ensuring `Ref` does not outlive the array it contains a pointer to. That means using lifetimes to statically guarantee the strictly-outlives relationship by having something like: fn get_ref&lt;'a&gt;(&amp;'a self) -&gt; Ref&lt;'a&gt; Even if `Ref` contains just the raw pointer.
Does `Sized` still inherit from `Move` after [RFC 1909 (unsized sized rvalues](https://github.com/rust-lang/rfcs/pull/1909)?
&gt; ConcurrentModificationException ConcurrentModificationException I didn't even know they exist until few days ago I hit one. Playing heavily moded Minecraft, which obviously crashed. Bad user experience, since it takes ages to start and ages to stop lagging brutally.
Oh, neat! Though I am not sure if that scales |a, b| -&gt; { let c = mkBorrow(b); yield a; let d = use1(c); yield a; use2(c, d) } 
I came across another way to make Tooling easier for drive by contributors, good CI! I went to build a project `cargo check` and it did not work. So I opened a PR with my change and waited for the CI to build it, it of course complained about silly things that `cargo check` should have told me, but meny commit-CI cycles later I was ABLE to contribute.
that's fair. i heard that as "macros 1.2", lol.
What's the name of the `impl Trait` in this case? Default trait implementation? I want to find documentation about it but searching for "impl trait" leads to other results. My principal question is why this doesn't work: trait Test { const A: u8 = 1; } impl Test { } But moving the associated constant to the `impl` works. Thanks!
Another option that I omitted is that you can [implement the Ord trait](https://doc.rust-lang.org/std/cmp/trait.Ord.html) for your struct, and then you can just use [sort](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.sort). In your implementation of the Ord trait, you can just switch the sign or the direction of the comparison to swap the direction. The downside to this is that it makes this the default sorting solution for your struct, and that may not be what you want. Being explicit about how the sort is occurring in the code where the sort is requested can be an important code readability/maintainability feature.
I suspect the easiest way to force the optimization not to happen would be to do something with pointers that *could* be self-referential. For example, pass a pointer to a local into a function from another TU, get another pointer back that *may* be derived from the first one, and then keep it live across a suspension point. Then move the coroutine handle between resumptions, maybe also across TUs. In general I see three obstacles: First, given C++'s relative inability to analyze pointers, the set of possible ways to "write and handle the state machine in such a way that no allocations are necessary" is simply not completely discoverable. Second, the language has ABI constraints around object layout, so even when the information is there it won't always be possible to do anything about it. Third, this all relies on inlining and/or interprocedural analysis on top of the actual allocation elision, which isn't going to be enabled in debug builds unless compiler writers go out of their way to do so, and which will slow down the build either way, much like Rust's situation with long chains of iterator adapters. In this light, it seems obvious to me that the optimization won't ever be literally 100% reliable- in practice maybe it won't matter, but an awful lot of IO happens using pointers passed across TU boundaries that will never be LTOed away (e.g. the kernel, dynamic libraries)...
In the general case you probably do need to treat the layout more like a stack frame and less like an enum. It's an opaque type anyway so it doesn't really matter outside of the implementation.
What are some examples of types like that? My current intuition is that it tends to make things cheaper and simpler to implement, like Vec reallocation and dropping.
I spend way too much time trying to evade upstream-managed packages, and get the Debian "this is not going to break or change under you and you won't have to think about it again for at least two years" experience with other software. The lack of a well-supported toolchain packaged in Debian Stable is my biggest complaint about Rust's ecosystem, and the speed of forwards-incompatible toolchain releases and tendency of projects to stick like glue to the latest version is my biggest concern.
Are you saying that you have problems with breaking changes in the stable compiler?
These issues strongly remind me of what GC implementations have to deal with. Super long term, I wonder if anything could come from integration with Rust's GC support (https://manishearth.github.io/blog/2016/08/18/gc-support-in-rust-api-design/)?
Occasionally :)
Feedback appreciated. This was my side project at Amazon and I finally got some time to clean things up a little and release it. It's still super early in development, but I think it functions pretty well already. Some Rust tidbits: * Builds on stable Rust; no `unsafe` usage * This was one of those places where I was actually able to "drop in" Rayon and get an immediate speedup. That's really cool. * The CLI is a separate crate from the library (it's in the same workspace, though). The published version on crates.io currently doesn't function as it should; it's missing the SPDX dataset this tool uses and that's too large for crates.io in source form. The dataset is preprocessed in a build script and embedded into the executable for convenience. * It built and run on Windows, first try, no code changes. Neat! There's quite a lot of work to go, and I'm not an expert in Rust at all so if this is interesting to you don't feel scared to poke around. :)
What type do you want to implement `Test` for?
It isn't my code, it's the `Any` trait: [link](https://github.com/rust-lang/rust/blob/56733bc9f8302409a2b6110f422512923c878154/src/libcore/any.rs#L88)
A use case this didn't really address is that persistent data structures (at least the HAMT based ones in Clojure and JS) is being able to efficiently determine if a vector/map hasn't changed. An identity comparison between two references coming back equal guarantees that the collection hasn't been changed. The most common use for this is in combination with React's update algorithm, which is basically `O(n_size_of_output)`. Having an `O(1)` way to cull large parts of the output tends to pick up perf wins. You could do this in Rust with copy-on-write but I haven't thought through the consequences of doing it on trees of data. In higher level languages where you're pointer chasing everywhere, the hit for going to persistent structures isn't nearly as bad. I hacked up Matt Bierner's hamt js implementation one weekend trying to get as small/fast and implementation as I could and wound up being ~5% slower than the native megamorphic objects in v8 (once it switches over from hidden classes to the hash table) but I got stuck on in-order iteration and distracted by something else. So competing with a hash table is semi-okay but a Rust vector won't happen any time soon. I've only seen a few people mentioning persistent data structures in the context of lower level languages or high performance. I remember reading about Photoshop's history implementation and believe that was implemented with CoW. I also remember a [presentation on Aeron](https://www.infoq.com/presentations/aeron-messaging) where mpt cites contention for the root pointer in a tree as a performance bottleneck and instead uses what he calls a persistent log. I have less concrete recollections about other articles but basically CoW.
Yes, maybe I should have been more specific, it's the Block that is `'static`. struct Ref { inner: *mut Block } struct Block { car: Ref, cdr: Ref, reference_count: usize, data: Data } // Data: 'static Block's are allocated in an array that is never freed (I `mem::forget` it), if I ever let the backing store be freed I'll need to tie their lifetime to it, presumably with a `PhantomData`^(0). I drop a `Block` only if there are no more `Ref`'s to it. I don't think it's possible for a `Ref` to outlive the `Block` it points to because the `Block` lives until dropped and I only drop it if there are no more `Ref`s. ^(0) Or I could check all the reference counts, but that would be difficult since the `Block`'s create a tree structure and I lazily free the child `Ref`s when I reallocate the parent `Block` instead of on drop. As such it's not as easy as checking the reference counts are zero. Trying to non-lazily free them caused a stack overflow. Anyways, I'm pretty sure that the rest of this is all fine as long as the compiler won't miscompile `(*(ref: &amp;Ref).inner).reference_count += 1;`, `(*(ref: &amp;Ref).inner).reference_count == 1;`, etc.
I wonder how Apple pull-off the swift history. ARC sound right to me, but have not find how actually implement them.
[Here's](https://developer.apple.com/library/content/documentation/Swift/Conceptual/Swift_Programming_Language/AutomaticReferenceCounting.html#//apple_ref/doc/uid/TP40014097-CH20-ID52) what the swift documentation says about solving what they call *strong reference cycles*. I don't do any swift, so you'll have to come to your own conclusions about this.
That's the slide number. As far as I can tell they technologies are in "no particular order" with the splash page labeled as #1. It's also one of only two "general purpose programming languages" awarded, the other being Kotlin. R and GraphQL are also mentioned, but I'd certainly argue those are languages of another kind.
Let me explain: The code in your original question had `trait Test` and `impl Test`, however bare `impl`s are only used on structs and enums. Traits have their (default) implementations directly in the `trait` definition.
I would restate that Rc has a higher up front cost to that of a GC. It‚Äôs not always more expensive, and it definitely can outperform GCs over the life of an application because there is far less work to do in deciding when to free the backing memory. It entirely depends on how you structure your program. Not having a GC pause is a really nice thing.
Yeah, that's what I thought but there is literally a `impl Any` in that file.
I find the description of rust being rather under-ambitious. I certainly don't see rust limited to bare-metal and systems programming.
Thanks !
Yup. Immovable object meets an unstoppable force. Impossible to reconcile to the satisfaction of everyone involved.
This is just heap-allocated continuations, a la SML/NJ, which works decently with a good allocator.
A great place to start is [A Unified Theory of Garbage Collection](https://www.researchgate.net/publication/221321424_A_unified_theory_of_garbage_collection). This paper is deep in the lambda-calculus world of sayings like "numbers are represented by how many nested functions you have in a reference," *but* it does make a startling claim: Reference Counting and Tracing GC are the same thing! They're effectively mirror images of each other: Reference Counting creates a virtual graph of every item that is still alive; Garbage Collection is just reference-counting everything to figure out what's dead. It's an interesting paper, but it might help. (Then again, it may just be terribly confusing.)
Hmm, fair. Though I think folks are far more sensitive about allocations in Rust (even when there isn't much of a perf impact). Hell, I've seen complaints about tokio boxing futures (only at the top level, and it has to).
Yeah. Jemalloc is nice but not as fast as most GC allocators. (Sure the GC cost exists too but it scales differently)
That's it, but how on earth did you find it?
Really good point, thanks.
thanks for the comments! I've updated the [docs](https://docs.rs/taken/0.1.1/taken/macro.take.html), are the use cases more clear now?
thanks, I thought a lot about what the right syntax was and I went with the copy operator. I particularly liked how `take!(&amp;mut var)` and `take!(=mut var)` look next to each other.
I googled for your name on github. I like googling. Nice project BTW. Unikernels are cool.
I was approached by a couple of recruiters about this but never got any feedback. Are you guys actually working with recruiting firms? 
There's frequent problems that developers of pretty widespread libraries adopt features only available in newer compilers to make that _one_ line of code nicer. A nice example are inferred static lifetimes for statics: error: this needs a `'static` lifetime or the `static_in_const` feature, see #35897 --&gt; /Users/skade/.cargo/registry/src/github.com-1ecc6299db9ec823/ansi_term-0.10.2/src/ansi.rs:80:19 | 80 | pub static RESET: &amp;str = "\x1B[0m"; | ^ Bam, you're 1.18.0 and above. The tiny CLI app I wrote for [this blog post](http://asquera.de/blog/2018-01-20/getting-started-with-rust-on-the-command-line/)? It requires Rust 1.20 because of things like this in dependencies. And it's only using clap and serde. I initially miscommunicated it as 1.15 first (because of serve needing custom derive). 
r is a random number representing a bracket (or entry of an array/vec)) I am filling arrays by chance with the constraint that the possible brackets of layer l+1 (aka stride) are determined by r of the previous random bracket. - L0 has only 1 bracket so every round or drawing goes into that single bracket. - L1 has two brackets, and each bracket has got a 50/50 chance to get drawn. - L2 has three brackets, but now depending on the previous r of L1 (0 or 1) The stride for the r of L2 is either 0,1, or 1,2. I posted an image here https://www.reddit.com/r/Pictures/comments/7ukbnt/brackets_and_strides/ If on L1, r = 0, then on L3 the last bracket is already ruled out, therefore r can only be 0, 1 or 2. If on L2, r was 1, the stride is shifted to the right, and r can only be 1, 2 or 3. I hope it makes things clearer :) 
Oh! Interesting, I hadn't thought of it that way. I guess that's really right then: you don't need the unsafe cell like I thought you did.
Isn't the whole point of stability as a deliverable to make upgrades in this situation a non-issue?
It's not used at Amazon at the moment, and that's entirely my own doing. I worked on this as a pet project to learn Rust, and to see what all could be improved in the license detection space (I work in Amazon's Open Source Program, so it's relevant to my day job). I didn't really have a plan or tell a lot of people about it until I was sure it was going to work out. Now it's at the point where it's usable and (IMO) pretty darn good, so yes, I'm planning on augmenting some internal systems with its functionality. Concrete example: we run [oss-attribution-builder](https://github.com/amzn/oss-attribution-builder) for prepping software releases, and integrating askalono there could save folks a lot of time in the "I don't know what this license is" area.
Did you consider using a distro with rolling releases?
Very cool, thanks!
What a *darn* shame.. *** ^^Darn ^^Counter: ^^57487
Is it though?
If one CAN upgrade, it wouldn't be a problem. The problem is that some people can't. We should also take that into account. Not to slow down the pace of development or anything, but recognize the lower bound versions of the crates people are willing to support, and make that explicit.
Rust ends up making you write things like `if !string_x.is_empty()` and does not support a Perl-like "everything is truthy" approach. 
https://serde.rs/container-attrs.html#serdedenyunknownfields &gt; Always error during deserialization when encountering unknown fields. When this attribute is not present, by default unknown fields are ignored for self-describing formats like JSON. You're deserializing a URL, right? I'm pretty sure URL encoding is self-describing, so it shouldn't error on unknown fields by default. I think this is a bug in serde_urlencoded.
Hopefully this should *mostly* sort itself out over the next couple of years as the low hanging language feature fruit is picked off, and meaningful improvements (that libraries are really keen to take up) start to come less often.
Ignoring Rust, I was pleasantly surprised that this list is mostly a nice introduction to the more interesting technologies of the previous year. (Don‚Äôt normally expect much from less specialist publications.)
Would this actually work? That'd be great, or /u/nikvzqz suggestion, either would be great for my situation if it would work.
I haven't tried to learn about jemalloc internals that much. Do you happen to know how competative a jemalloc allocation is with a SML allocation (usually one or two instructions of overhead above the initialization cost)?
I think it's very hard to get a malloc implementation to be competitive with a bump-pointer allocator (which can also be used with arenas).
**Normal distribution** In probability theory, the normal (or Gaussian) distribution is a very common continuous probability distribution. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate. The normal distribution is useful because of the central limit theorem. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
You would have to do `if custom_enum.into() { ... }`
This helps a lot, thanks! So when doing `impl Trait` you are defining functions which take some virtual object with a vtable, but if you add an associated constant to the trait, the trait cannot be made into a virtual object, that explains the error I get (E0038). If that's the case the [error index](https://doc.rust-lang.org/error-index.html) should be updated with this requirement.