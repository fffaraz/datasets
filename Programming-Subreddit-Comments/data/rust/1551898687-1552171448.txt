I just use slotmap paired with a hashbrown::HashMap for doing this. 
Can you show an example? You might be able to make your functions generic over `S: Into&lt;String&gt;`.
[removed]
It also computes a different number.
It isn't 100_000!. It's a larger number, requiring 3652592 bits.
Was it this?: https://crates.io/crates/rust-embed
Could somebody shed some light on the issue with if match etc. ? I dont see how evaluation those could be a problem at compiletime.
You got it right - thats where I ended up and just stuck with the to_strings on any references. I don't have a sample handy (at the vet) but the case is a good deal of const &amp;'static str and the (lazy static &amp; dynamic) hash maps have keys over them as well as sometimes having values of them. For the collections I went with HashMap&lt;String,Vec&lt;String&gt;&gt; vs references to static str
What advantages? It's just a different way to write code you can already write. Sounds like a small handful of functions will look (arguably) slightly better at best.
I'll just chime in to say that, as a composer and designer of tools for composers, I would really love to see all of this happen! I'm fairly new as a programmer - I've built some synthesizers in Pure Data, C++, and Processing/Java (none of which are really fit for distribution). That said, my goal is to eventually have stable software that is releasable and I want to rewrite some of my programs in Rust to that end. I'm no systems programmer, so I don't know how much help I can be, but I do know DSP. In any case, I'll be following this effort closely!
What's a good resource for learning about fuzzing and how to do it in Rust?
I'm curious how you're doing input validation. Actix will respond with 400 BadRequest if the params cant be serialized into a struct, but i'd love to capture that into more readable error messages for the client.
Nice job. I was planning on making my own type of this. Now I might not have to. 
But wouldn't Rust still not use such a massive amount of memory at startup? For example, if a machine has 64GB RAM then Elasticsearch needs 32GB. If JVM allocates it, then Linux itself knows it only has 32GB for system processes. Wouldn't Rust take a performance hit if Linux didn't know beforehand if it was going to need 32GB RAM? How would Linux handle that with Rust differently?
Are you sure? *b* starts as 1. Then for *i* in 0, 1, 2, â€¦, 99\_999, we have *b* = *b* \+ *b* Ã— *i* = *b* Ã— (*i* \+ 1). This is like saying *b* = product of (0+1), (1+1), (2+1), â€¦, (99\_999+1) = product of 1, 2, 3, â€¦, 100\_000 = 100\_000!. (Also, I diffed the output of the program and a program which just computes the factorial and no difference was found.)
Mostly it just needs time to get there. Now floating point stuff, _that_ is tricky, because `const` somewhat implicitly suggests that your compile time result and runtime result will be the same, which is unlikely to be the case with floating point if you compile on one machine and ship to another machine.
The 2nd style is a great way to trigger Undefined Behavior.
That's why eve runs so well on low speed high core count cpus. 
I believe that `b += b * i` is equivalent to `b = b * (i + 1)` which can be done with less allocations.
The Rust-Fuzz group on Github has a guide: https://fuzz.rs/book/
The title is kinda misleading, as it actually describes *two* ways, the second being a **wildly unsafe** way to do it. This article seems to be written for beginners. You really don't want to encourage method [B] for them. The most common reason for making a field private is to prevent users from tinkering with it in a way that would break your type's invariants. You really don't want to encourage clueless users to bypass that protection, just because they think they know what they're doing. Not only that, but if the layout of the original struct changes, and the user doesn't update their own version accordingly, it has the potential to cause undefined behavior. The motivation is also poorly defined. &gt; Do you want your fields to be private but got stuck in accessing them from other module. If you want your fields to be private, you probably want them to be private for a reason. Otherwise, just stick `pub` on it, if the compiler complains.
Sure?
Original was (unsugarred) `b = b + b * i` This one is `b = b * (i + 1)` Seems pretty equal to me.
Are there any good docs for things like SyncArbiter besides the auto-generated ones? The Actix book frustratingly stops right when it gets interesting at Contexts. The SyncArbiter section is also just "WIP" [https://actix.rs/book/actix/sec-6-sync-arbiter.html](https://actix.rs/book/actix/sec-6-sync-arbiter.html) 
It computes the same number as /u/nicoburns implementation: &amp;#x200B; [https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=dbe716ec4d8941ad984702108b18da45](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=dbe716ec4d8941ad984702108b18da45)
Oh, whoops. I fixed the link. It's for semantic social networking. I also ,put a toot about it at https://mastodon.nzoss.nz/@naturallymitchell/101701360428047553
I think it would look just as clean with a macro. let is_printed = pipeline!{ 1 =&gt; adder(3) =&gt; multiplier(2) =&gt; .to_string =&gt; // alternatively, ToString::to_string printer }; And if you want to roadmap this feature (and I like the idea of this) proposing/implementing it first as a macro makes a lot more sense than a new operator. It's also more powerful since you could use any identifier (instead of an existing keyword, like `self`) to denote which argument the result is piped to, e.g. fn non_commutative_fn (a : u32, b : u32) -&gt; u32 { a * (b + 1) } //... let result = pipline! { 1 =&gt; non_commutative (result, 1) =&gt; non_commutative (1, result) =&gt; }; I also like the idea of using `|&gt;` over `=&gt;` if you have to chose a pipeline delimiter (although it could be anything in a macro). I'm not a fan of adding new semantics to existing grammar, it just makes life more difficult. 
The num-bigint crate is written completely in safe Rust, and it makes use of `u32` digits, so that multiplication would involve lots of `u32 * u32` that yield `u64` values. If the minimum rustc version is bumped to support `u128` and the crate is updated to use `u64 * u64` yielding `u128` values, it would very probably be faster on 64-bit architectures. (Similarly for additions, it uses `u32 + u32` yielding `u64` in order to retain carries.) When you're looking at Python and Ruby, the algorithms are actually implemented in C, so you're really comparing Rust's num-bigint to a C implementation that probably (just guessing, I haven't looked) uses 64-bit digits instead of 32-bit digits. If you compare to the GMP library (or the Rug crate in Rust, which is a wrapper around GMP), you would then be using not just C, but also optimized assembly routines. That is why GMP is usually the fastest for large inputs.
You can use custom error type and [https://github.com/Keats/validator](https://github.com/Keats/validator) for data validation.
 fn load_async(&amp;self) -&gt; BoxResult&lt;IplayerDocument&lt;'a&gt;&gt; { .... Ok(IplayerDocument{doc: resp, url: self.url}) } I do not see the definition of `BoxResult` anywhere, so I'm guessing, but shouldn't you return `Box::new(Ok(...)) here ? Sometimes using the wrong return type messes up type inference, and results in weird unrelated-looking error messages. 
Why would Rust/Elasticsearch need 32GB of RAM in that case? Mind you I don't know anything about Elasticsearch.
C++20 modules actually still have a distinction between "interface units" and "implementation units", which are roughly equivalent to header files and source files respectively. Unlike with traditional header files, you are allowed to put all your code in the interface unit if you want (even non-inline functions), but it's slower to compile. But then, that doesn't really have much relevance to Rust, since C++(20) is still dependent on external build tools and separate compiler invocations for every source file. 
I'm porting a C++ program to Rust and I'm starting by writing end-to-end tests to ensure perfect replication of the functionality. The problem is that the program involves PRNG, namely `std::mt19937`. The good news is that the program can be passed a seed to make it deterministic, but what is the best way to make a PRNG in Rust that will match `std::mt19937` for a given seed? Obviously I can write a thin wrapper over the C++ library, but I'm wondering if there's a better way?
https://doc.rust-lang.org/rustc/lints/listing/allowed-by-default.html#bare-trait-object Add `#![deny(bare_trait_objects)]` to your crate and it will give an error, or use `warn` instead of `deny` to make it a warning.
Can you use wgpu-rs on OpenGL 2.1?
&gt;Now floating point stuff, &gt; &gt;that &gt; &gt; is tricky, because &gt; &gt;const &gt; &gt; somewhat implicitly suggests that your compile time result and runtime result will be the same, which is unlikely to be the case with floating point if you compile on one machine and ship to another machine. &amp;#x200B; I really like the idea of Posits. (not UNUMs, just Posits).
thanks! The compiler caught it right after fixing the other error ;)
Ah, thanks for the detailed information!
That's a tough call. OpenGL in general is not very friendly to layering modern APIs over, which hits gfx-rs quite hard. If wgpu-rs needs at some point to be implemented on GL directly, it would have an easier time, but still lots and lots of pain. So currently the answer is no.
Ah that looks awesome, thank you!
https://crates.io/crates/mersenne_twister seems to be a Rust version of that PRNG
Could you say a little more about how Rendy and wgpu-rs relate to each other? Would it be possible to implement the Rendy API on top of wgpu-rs?
The link to Dawn is a 404 (it goes to `/daw` which I bet is the issue)
The JVM asks the operating system for a big chunk of memory all at once, and uses that memory to store its garbage-collected heap. If it needs to grow the heap, it needs to ask for a new, even bigger chunk, and copy everything into it. C/C++/Rust programs don't need to ask for a single big chunk of memory. They ask the OS for small chunks of memory as needed, and release them back the OS when they are done. They can do this because they manage their own memory and talk directly to the operating system, rather than relying on a virtual machine to do it. (Note: The above is deliberately over-simplified. A more complete picture on the Rust side would include the "allocator," provided either by the OS or by a library, which may reserve some medium-sized chunks of memory up front and then allocate small chunks within those. And the operating system's virtual memory and paging systems are also involved. A full description of the JVM would also be more complicated, possibly involving more than just one chunk of memory for the heap.)
This is awesome, can't wait to give it at try! Could you maybe talk about how it differs from say OpenGL 3.3, in terms of complexity? Having tried at bit of Vulkan and gfx-hal, I found the API a bit daunting coming from OpenGL. Will wgpu be more approachable?
It is for big data, so it often needs to operate very quickly on gigabytes of data at a time to form aggregations and searches.
I guess that's true. However, it seems like a relatively common thing to do, and macros feel like a heavy-weight solution, enough that I usually just go with `[T]` or `Vec&lt;T&gt;`, whereas I use variadic functions fairly often in Go.
FYI: This lint is part of the rust-2018-idioms group -- and you can use cargo fix to apply it!
`rendy` is a 'build-your-own-renderer' toolkit built on top of `gfx-hal`. It provides many helpers and conveniences to assist you in writing a rendering engine. However, it does **not** hide all the complexities of gfx-hal (which is basically Vulkan) from its user, and even in the places where it does provide help, knowledge of the underlying concepts is invaluable if not required. All of the helpers that rendy provides are based around the central concept of rendy, the render graph (also sometimes called a 'frame graph' in literature). The render graph is the backbone of rendy, and its purpose is to manage the final ordering (schedule) of rendering commands for a frame, as well as manage the transitive resources needed to complete those rendering commands. The render graph should allow the user to build this schedule in a manner which is easy to reason about and composable from the simplest to the most complex of renderers. This is in many ways the hardest part of using a Vulkan-like API, as reasoning about and building this dependency tree and managing those resources manually is *really* hard and error prone, specifically when you go past a single render pass and just a few resources. In addition, `rendy` provides some other helpers which are not directly related to the graph, like a memory allocator, which is another big pain point of Vulkan-like APIs, and some abstractions for abstracting things like meshes and managing their use on the gpu. `wpgu`, on the other hand, takes a different approach, in that it *does* want to hide much of the most difficult complexities of the underlying api from the end user, and expose a much more refined, easier-to-use user-facing api. Many commands are synchronous or provide helpers to make asynchronous commands easier to synchronize, lots of things which are uber-explicit in a Vulkanesque api are simplified, etc. However, it still retains many of the same high-level concepts like render passes, command buffers, pipelines, descriptors (renamed to bind groups), etc.
How would i pass [f64] to it, for example? don't i have to box it? is it copied? It's just seppls can have templated non-virtual methods in class with virtuals. Then you just write adapter functions and you can have static uniform behaviour for completely different vectors. In rust i first searched for a lazy .map but it appears you have to collect it and allocate.
Btw., that's also implied by the [rust_2018_idioms lint](https://github.com/rust-lang/rust/issues/52047), which as the name implies also checks for other idioms new in Rust 2018.
This is true, but you have to handle the zero case a bit different since we are dealing mainly with \`u8\`. Note also that the loop starts at 1, and is inclusive on the upper bound. &amp;#x200B; `for i in 1..=100000 {` `let i = i as u8;` `match i {` `1 =&gt; {}` `0 =&gt; b *= 256_u16,` `_ =&gt; b *= i,` `}` `}` 
That should be warn by default when you specify 2018 edition if it isn't already (I'm still using 2015 because many of my crates try to support older Rust versions).
cool! well seeing as that you're the only one talking about this improvement with the skills to fix it, what do you say?
Feedback always welcome, especially if you spot any mistakes ;)
What is the best way to allow users to create plugins for a Rust binary? I want to create a binary that looks at a plugins directory, loads those plugins, and then runs.
Feedback always welcome, especially if you spot any mistakes ;)
I am wondering, is there any embedded projects where co-routines / async / await are being experimented with ? I am curious to see what is being done with these new language features in the embedded space.
Great! The methodology page doesn't appear to have been updated yet, but I look forward to it happening.
&gt; `wpgu`, on the other hand, takes a different approach, in that it *does* want to hide much of the most difficult complexities of the underlying api Is this hiding going to be a matter of "convenience wherever possible", or of mandatory encapsulation? That is, does `wgpu`'s safety rely on it being able to maintain internal invariants which could be violated by apps making direct `gfx-hal` calls mixed in with their `wgpu` ones?
&gt;Zero-extension of values is free on x86 in most circumstances. Right, I had forgotten that 32 bit operations on 64 bit registers are zero extended on x86-64 so I was wrong about always needing extra instructions. (It's 8/16 bit operations that don't affect the upper 56/48 bits).
&gt;po8 Wouldn't it be much faster to do this? ``` fn main() { let mut b: num::BigUint = 1u32.into(); for i in 1u32..=100_000 { b *= i; } println!("{}", b); } ```
Acknowledged, Thank you. (I didn't understand).
I updated it yesterday right after my comment. It's already stating the use of release builds: &gt; Version 0.35.4 of rawloader was used throughout the test, compiled in release mode
Not sure if it's the 'auto-generated one' you are talking about, but [https://docs.rs/actix/0.7.10/actix/sync/index.html](https://docs.rs/actix/0.7.10/actix/sync/index.html)
I believe the latter
I'm curious why you're asking. 2.1 is nearly 14 years old.
Sometimes I see people putting `use` statements inside a function definition instead of at the top of the file. Is there any benefit to one way of the other?
As in not needing extern?
That would make it possible to run it in current browsers as well, though.
Instead of allocating empty vector and then pushing data to it use `Vec::with_capacity` and then push, it will save time on reallocations. Alternatively use iterations to generate list of coordinates. 
So that I could actually use it. Most rust frameworks and libraries assume OpenGL 3.3 or newer, meaning I can't even run their examples.
Wrong subreddit. You're looking for [r/playrust](https://reddit.com/r/playrust).
Oof preciate it 
Got the package finally working. You need to install protobuf 3.5.1 
Out of curiosity, was the name of the not-yet-awesome repo inspired by the Not-Yet-Awesome Rust organization? :)
Actually, it seems that specifying 2018 edition doesn't enable `rust_2018_idioms` lint :( [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a65779a65a4e32e3061ad6621f349a7b)
Function-specific imports make it more clear that the import is only being used in that specific function.
Rust is giving me a lot of familiarity with the stack and the heap, but I'm still not really sure what those really are. I've seen a ton of videos that talk about them both as abstractions which work I'm different ways, but what are they actually? Is the stack some part of RAM while the heap isn't? Vice versa? Where can I learn about the real physical world side of things?
I overlooked that because I was expecting the actual command. Thanks for updating! 
If you really want to use 2.1 then you should just use raw opengl. A graphics library isn't going to get you much if you're building on top of 2.1 -- at least, not one along the same vein of wgpu or such.
Here is example https://github.com/actix/examples/blob/1.0/r2d2/src/main.rs#L13 This is from 1.0 alpha, but you can implement similar code in 0.7
In the second sentence, you probably meant to write "features" rather than "futures".
I just installed this, and it was a lifesaver. As part of our transition plan off of PHP, we'd moved all the graphql schema out of PHP and simply load the schema.graphql that it generated for us. We figured the less we were bound to a particular implementation, the better. I've set up a couple types so far and it's working well.
It's a matter of time before this happens; It will happens eventually.
Thanks for reading! Yeah I thought about it but it turns out that I can't measure any difference. Might be the sizes are too small for reallocations to really matter that much in this case. ``` // with capacity let mut pixels: Vec&lt;(f32, f32)&gt; = Vec::with_capacity(518_400); for y in (0..h as u64).rev() { for x in (0..w as u64).rev() { pixels.push((y as f32, x as f32)); } } // iterator let pixels: Vec&lt;(f32, f32)&gt; = (0..h as u64) .rev() .flat_map(|y| (0..w as u64).rev().map(move |x| (y as f32, x as f32))) .collect(); ``` These gives pretty much the same result as the original code, maybe 0.1s difference, but the std deviation from run to run seems to be higher than so not really measurable.
What would the performance and flexibility restrictions of webgpu in general be? Can you still thread out your command submissions like in Vulkan? Can you write compute shaders? Would you say it's closer as an API to something like Vulkan/dx12/metal or to OpenGL 4.x or more limited like OpenGL ES? I know the Safari blog mentioned they were modelling it after directx in terms of API useage but I was mostly interested in what you perceive as the limitations of it versus writing to a system like gfx-rs 
Yeah, thats right. Thanks, it's fixed now.
Yes, if you wanted to compute 100_000! That isn't what OP's code does, though.
This is because `str.parse::&lt;i32&gt;()` returns a `Result` since it's a failable operation. You would either need to call `unwrap` on your `k` and `n` to ignore the error (panic when there's one), or handle them correctly.
What OS/Graphics stack are you using where 2.1 is the most recent OpenGL supported?
`Stdin` implements `Read` which has `read` method, where you can give it whatever buffer you like. https://doc.rust-lang.org/std/io/trait.Read.html#tymethod.read
Thanks, this is fixed now ([https://github.com/gfx-rs/gfx-rs.github.io/pull/25](https://github.com/gfx-rs/gfx-rs.github.io/pull/25))
&gt; Can you still thread out your command submissions like in Vulkan? Can you write compute shaders? Yes and yes &gt; Would you say it's closer as an API to something like Vulkan/dx12/metal or to OpenGL 4.x or more limited like OpenGL ES? It's kind of like a mix between OpenGL 4.x, Vulkan, and Metal &gt; also does this use the new web shading language? It uses SPIR-V currently.
It is *not* defined behavior (until we specify otherwise) to transmute from `Employee` to `MyEmployee` even if they have fields declared in the same order with the same types.
You're right. My bad.
Both the stack and the heap are just arbitrary locations in RAM. A rust program compiled to ELF or exe or whatever contains some information about how much memory it wants for its stack and for static data and so on, and the OS gives the program some memory for it to live in and do its thing with.
Yeah, you're right. Apologies.
Yes, I was wrong. Apologies.
Thanks for the quick answer! Do you have recommendations on workflows for generating spir-v? I'm assuming calling out to the official tooling inside a build.rs is easiest?
Based on the limited code example, I would guess that either your queue is growing in size because your receiver can't keep up (you may want a bounded queue instead) or there is a memory leak in rlua (no disrespect meant to the author, I only mean because it's a crate that will have to deal with manual memory management due to its interop with lua)
It will be more approachable yes, but it will still be significantly *different* than OpenGL 3.3
See https://github.com/Lokathor/proc-spirv or https://github.com/google/shaderc-rs which the first one uses under the hood
Totally possible, and what makes it harder to track down is the fact that I'm programming a game so there's a lot of unsafe interop :/ I'll try changing to a bounded queue and report back, but I'm still so confused about the debug/release differences. It seems like this is an issue with 1.32 as well.
As oceanicdev mentioned, I'm using the validator crate. I just let actix return a 400 for malformed JSON, but it seems [you can configure it](https://github.com/actix/actix-web/issues/181) if you need to.
Fantastic. Thanks!
Working on [actix-web 1.0](https://github.com/actix/actix-web/tree/1.0)
I'm curious to know what would be the c# perf with the same optimizations
Is the reason for using `Mutex&lt;Cell&lt; &gt;&gt;` instead of just `Mutex`+`::std::mem::swap()`because you need multiple shared references in rlua or something?
Why does the Rust version of the raytracer have two little "backwards D" segments inside the P and the R? The encoding of the letters looks the same, so I'm guessing it must be an issue with how the signed distance to the curve is calculated in the rust version?
Well the debug version will run a lot slower, so that may be why your receiver can't keep up only in debug.
`Stdin` has a `BufReader` in it, so its `read` implementation is here. https://doc.rust-lang.org/src/std/io/buffered.rs.html#223 It seems that this is only conditionally true according to the code. It only avoids the copy when the given buffer length is longer than the internal buffer (8K) and its current cap is reached. Depending on this implementation detail seems unideal.
I need the ability to have multiple disparate parts of my program all talk to the same queue. This might be showing my naivete, but the research I did to write this basically led me to believe that using Cell would provide the ability to mutate the Hashmap inside the Arc, and since the Lua runtime isn't threadsafe I needed the mutex. std::mem::swap() exchanges the value in the mutex then, correct? I'm still learning about how this works, in other languages I'd make a singleton but since I want to avoid global state and allow for multiple message queues to be constructed, I went with this approach. Is there an advantage to using ::std::mem::swap()? How would I apply it in this case?
Alright, changed to bounded queues and still seem to have the issue. I forgot that I *do* actually drain the queues every game tick, and even when the queues are empty I still seem to have the leak :/
What happens if Rust runs out of stack memory at runtime?
Associated constants exist :)
I *think* that you probably want /r/playrust, (this is for the programming language by the same name)
[embrio-rs](https://github.com/Nemo157/embrio-rs) is the only one I'm aware of, at the moment.
Probably indirectly! We have an [awesome-embedded-rust](https://github.com/rust-embedded/awesome-embedded-rust) repo already, and I'm sure subconsciously we chose "not yet" as the opposite based on seeing your project or other similar ones.
``` use std::net::TcpStream; fn main() { let stream = TcpStream::connect("8.8.8.8:53").unwrap(); let addr = stream.local_addr().unwrap().ip(); println!("{}", addr); } ```
Mutex already provides interior mutability; the cell is redundant. Mutex is basically multithread refcell. The multithread equivalent to cell is atomics although obviously those have stricter limitations.
Sure looks like it does to me.
TIL! So I still get the leak but I'm able to get rid of the redundant get_mut() call to grab the value out of the cell. Progress!
True, let me switch off multithreading and force drain the queue and see what happens.
me too, you can get rid of the array bounds checks with unsafe C#, if they aren't elided already. I might play with it. 
Well, it turns out that I just didn't pay attention to my own code. Turns out of course it wasn't rust, I had some profiling code that was collecting statistics every frame, so *of course* it was growing the memory used. Which also explains why valgrind didn't care when I tried debugging on linux.
Iâ€™m stoked to check it out more! Great work!
You're right. Apologies.
That [sounds familiar](/r/rust/comments/aqcovd/patch_05_released_parse_and_format_patch_files/egfos64/?context=1) :) Welcome to the queue, I guess.
I tend to think of a `Mutex` as a threadsafe `Cell` but they aren't quite the same. `Mutex` lets you turn a `&amp; Mutex&lt;_&gt;` into a `&amp;mut _` by locking, where as a `Cell` lets you swap a value behind a `&amp; Cell&lt;_&gt;` but limits shared references to one thread. I thought you might want mem::swap because mutability in cells is done by swapping, but if you just want to insert into the hashmap then you should be able to get away without the `Cell`, locking a `Mutex` alone should be enough. If you aren't inserting after construction then even just an `Arc` could be enough as `Sender` and `Receiver` only need &amp;self to work. I was thinking rlua could be forcing you to use both for some reason. 
On modern systems the OS can step in and kill the program. It's pretty easy to see in action: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=8e10d2949ff6bbce4f2ef1d69809babf
Unfortunately, it comes down to me trying to remember how to do systems code after not touching it for half a decade. I appreciate the breakdown, it's really awesome that there is such a huge knowledge base out there that's willing to help me overcome these issues.
Thanks for the reply and explanation :- ) Interesting that one can pause and resume. I imagine that can be useful, if a script won't finish, and one wants to ask the user: "This script seems to run forever. I've paused it now â€” shall I terminate it, or let it run?".
This is the subreddit for the Rust programming language. You're looking for /r/playrustservers
Yes, targeting WebGL is definitely on our radar.
[removed]
Awesome read! Just one tiny typo "This is what I was our secret master plan".
You had me worried there for a second :D It *wouldn't* have surprised me if there was a memory leak in rlua.
If it ever matters, I'll probably `dup` and `from_raw_fd`. If you're doing IPC at a level of performance where a few memcopies matter, you're using shared memory, right? If you're doing io through hardware, buffer size may matter but probably not the copy.
Nope! Your library has been an absolute pleasure to work with, and I even made a separate crate to help me serialize my specs components to and from Lua via a custom derive. Maybe once I make it prettier and handle some edge cases, I'll ping you and see if you're interested in it as an affiliate, or more likely you'll teach me more about Lua and some of the pitfalls with my implementation ðŸ˜€
Not if you use `repr(c)` or equivalent :\^D There is also discussion about ensuring that structs with the same field layout/types always get represented in the exact same way in memory in feature releases of rust. This would definitely make method B defined behavior. OP: set both structs as `repr(C)` and you're golden.
Aww, thanks! Ping me if you ever get that to a workable state, I've been meaning to provide table conversion as a proc macro feature for forever now.
Sure, "if" But they didn't :/
Using rayon feels like cheating to me since the original code has no dependence.
But in case you don't (or regardless): netstat -tlpn ... that will show you all of the TCP listening sockets including the programs' names. On Linux and probably other Unix style systems, anyways. On Windows I *think* you can get similar info with the `NET` command. 
&gt; How would i pass [f64] to it, for example? don't i have to box it? is it copied? Since `fn output(&amp;self)` takes a `&amp;self` and not a `self`, you would pass a `&amp;[f64]`, not a `[f64]` to it. &gt; In rust i first searched for a lazy .map but it appears you have to collect it and allocate. Iterators are lazy and so the iterator's map is lazy as well. But I'm not sure how you want to combine this with virtual function calls. To take a step back. Are you expecting one virtual function call per array, or one per item inside the array?
Well, par_iter can be replaced with some threadpool, which will add ~70 loc. But you have a point, why someone downvoted? Still, I'm impressed with memory usage that is like 1/4 of C.
This is very nice, but there's a lot of little typos scattered around in here that started to get in the way of me understanding the article. I listed a few here if you care to fix them: - par. 2: `"pice" -&gt; piece` - par. 4 `"it's source" -&gt; its source` - par. 6 `"explaines" -&gt; explains` - par. 9 `"poastcard" -&gt; postcard` - par. 12 `"as samples count" -&gt; a samples count` - par. 18 `"se" -&gt; see` - par. 19 `"hangig" -&gt; hanging` - par. 19 `"first. The" -&gt; first: the` - par. 20 `"se" -&gt; see` - par. 20 `"Rusts" -&gt; Rust's` - par. 24 `"aresenal" -&gt; arsenal` - par. 28 `"pice" -&gt; piece` - par. 28 `"This is what I was our secret master plan" -&gt; This was our secret master plan` - par. 32 `"CPU's" -&gt; CPUs`
Or you could just use `date`
Make sure to inline your Vec3 functions and that you never allocate inside the inner loop. Use static arrays or ArrayVec. I write Monte Carlo integrators for a living, so I also have a function that is called billions of times. Not allocating will help a lot!
Is the term reduction semantics just an umbrella term for both operational and denotational semantics?
I have explained both ways to access private fields of any struct. But I have told in the blog which one is best and why. Thus, I have given this title to my blog.
Have you looked at smallvec? It keeps everything on the stack where possible.
I would have expected you to run the script using a profiler and to actually measure what's eating the cpu time to not optimize blindly. 
Happy to hear ðŸ˜ŠFeel free to open an issue if youâ€™re having any problems. 
Unfortunately, only for bare functions, there is no solution for methods
&gt; I'd have expected 2 allocations to happen, one to satisfy the `clone`, the second to write the result into. But the result of `clone()` is *moved* into the function, which can therefore reuse its storage for the result.
There is the print version that you can print into PDF. Make sure you hide the sidebar and select a light theme before printing it. [https://doc.rust-lang.org/book/print.html](https://doc.rust-lang.org/book/print.html)
&gt; this is also done differently Depends on where you come from. None of C, C++, C# or Java allow non-constants in case labels.
Really good read, thanks for sharing! A small tip: when you do the code comparisons you can remove the part of the code that is not relevant (for example the stuff inside the loop when you were doing the par_iter change). It helps to understand what has actually changed.
Great to see more people playing around with graphql! If youâ€™re interested Iâ€™ve made a similar boilerplate example using rocket, but still graphql. https://github.com/davidpdrsn/graphql-app-example I guess the main differences (besides using rocket) is that I use https://github.com/davidpdrsn/juniper-from-schema to generate the juniper boilerplate from an actual graphql schema file. Full disclosure: I maintain the juniper-from-schema crate so this is mostly meant as an example of using that crate. 
Another approach is to add a crate-private extension trait to expose access to the field, thereby making it super obvious what's going on when you do reach for this escape hatch.
Glad to see you either. Juniper-from-schema looks awesome to me. I checked that this is also you're the maintainer. Definitely, I think I give it a try. Thanks man!
Ah, right, that's true, I kinda missed that.
Nice spot. I actually didn't notice this, and thought it looked neat :) In th C++ and C# versions there is none. I'll have to investigate this when I have some time, you're probably right about where to look though.
Thanks! Corrected!
I'm not familiar with Rocket but this link appears to have some information that's useful - https://doc.rust-lang.org/reference/procedural-macros.html
Wow, thank you very much. I'll have to look for a new spellchecker. They're all fixed.
Would be cool to see. I kind of assumed the JIT might have optimized this already but let me know if you find anything interesting. 
Yeah i know, if you read it as a competition it's cheating. However, that was not my main goal of this, it was just me starting out for fun noticing they were almost equal but even I got surprized by how big the effect whas after just a couple of small changes towards using perfectly normal patterns and libraries that you'll use in Rust all the time.
[Procedural Macros for Generating Code from Attributes](https://doc.rust-lang.org/book/ch19-06-macros.html?highlight=procedural,macro#procedural-macros-for-generating-code-from-attributes) \# Attribute-like macros. This page gives an example: // the implementing function #[route(GET, "/")] fn index() { ..} // the macro definition #[proc_macro_attribute] pub fn route(attr: TokenStream, item: TokenStream) -&gt; TokenStream {..} Here the Macro "Route" takes two paratmeters, attr: TokenStream, item: TokenStream, &amp;#x200B; the implentor of the macro "route", here the function index() has two parameters defined: attr: TokenStream = GET item: TokenStram = "/" while the type can be inferred from the formal arguments of the macro definition. That's how you implement attribute-like proceduaral macros.
Yeah, point taken. I kept the functions as whole so people could mentally navigate to where we are in the code easier and keep some context but I'll consider changing this later.
feature flags are a thing in nightly builds for enabling unstable features. See: https://doc.rust-lang.org/unstable-book/language-features/proc-macro-hygiene.html for example.
I just started last week myself, and yeah it was this kind of stuff that really confused the hell out of me. What I learned was, as far as the first two lines go, that's the old way of doing things. Loading from other crates and mods used to be a fairly labour-intensive, magical thing. Macros were also considered very different from functions and traits for the purpose of importing. Lots of adding things into your scope by magic, instead of explicitly declaring the scope of everything you want to `use`. At times it felt like PHP's terrible `extract` feature that simply added variables to your local scope without you saying what they were. Fortunately, everyone else noticed these problems long before I picked up the language, and rust 2018 is much better. You simply need the crate in your Cargo.toml, and then you're good to `use` or reference directly by namespace everything in it. That includes macros too. Try getting rid of some of that stuff up top, and you may find the rest still works. It's a recent enough change that a lot of code examples still do it the old way, but fortunately it's a very easy thing to move away from. 
Little video game surely doesn't have to embed 1GB video. &amp;#x200B; Don't embed anything, it's just a source for problems. Games never inlines their data, they store them in \`data\`/\`music\`/... folders. 
It seems the issue is now resolved [https://github.com/Microsoft/azure-pipelines-yaml/issues/134](https://github.com/Microsoft/azure-pipelines-yaml/issues/134) (might be worth updating the update in the post).
The first line enables unstable attributes, they are available on the nightly version of rust. The nightly version of rust comes with features that may or may not be stabilized and gives a chance for potential new features to be played around with. But this also means that documentation around these features is lacking, at the minimum you get a link to a github tracking issue where people discuss the feature and its future but not much more. As for how to create your own procedural macros, the rust people have a [blog post](https://blog.rust-lang.org/2018/12/21/Procedural-Macros-in-Rust-2018.html) about it. Personally I looked at how existing projects worked and read [the proc_macro API docs](https://doc.rust-lang.org/proc_macro/) and just experiment to see what works!
&gt; #![feature(proc_macro_hygiene, decl_macro)] Is a feature gate. `proc_macro_hygiene` and `decl_macro` are unstable features which are only available on nightly. You must opt into using unstable features to ensure people on nightly know exactly what they are doing and don't accidentally start relying on unstable features. In short it is like telling the compiler, "Hey, I know these are unstable and might change or disappear but I am ok with that." &gt;What's proc_macro_hygiene? The documentation just redirects to a GitHub issue. It is a feature gate that protects some extra macros API that rocket makes use of. I have not been following the details of these features but it is to enable some more powerful macro features. It is an unstable feature so the main documentation about it will be in GitHub issues as people discuss it and as it changes over time. It will get more official documentation once it gets closer to stabilising. &gt;Also, if I get it correctly, #[get("/")] is a function attribute It is a [procedural macro](https://blog.rust-lang.org/2018/12/21/Procedural-Macros-in-Rust-2018.html) which will transform your code in some way during compilation. This one will add the code rocket uses to route the get request for / to your function.
O.O I don't know how to start! I should read the program file from the program itself, and this is not a problem. But how can I merge two files manually? With another program that read my program, add a "from here" flag and attach the file?
That's what I should have done however I didn't plan to do maximum optimizations and write an article about it, I started out making the simplest changes I could find and thought it was rather interesting to see the rather huge effect that had, and then I wrote an article about it. Especially since many (like myself) start out taking a known codebase and and naively porting it to Rust, and I wanted to exite them to look further. I still hope I kept it an interesting read for people that know Rust and is interested in the topic though.
Oh! Is that the reason? Wow! Thanks for the link, I'm going to read... I have created [a post here](https://www.reddit.com/r/rust/comments/ay0gdd/include_bytes_with_big_file/). I'm not developing on Windows neither, I'm on Manjaro. &amp;#x200B;
 That's what I should have done however I didn't plan to do maximum optimizations and write an article about it, I started out making the simplest changes I could find and thought it was rather interesting to see the rather huge effect that had, and then I wrote an article about it. Especially since many (like myself) start out taking a known codebase and and naively porting it to Rust, and I wanted to exite them to look further. I still hope I kept it an interesting read for people that know Rust and is interested in the topic though. 
 Yeah, point taken. I kept the functions as whole so people could mentally navigate to where we are in the code easier and keep some context but I'll consider changing this later. 
Little nitpick about &gt; the implentor of the macro "route", here the function index() has two parameters defined: &gt; &gt; attr: TokenStream = GET &gt; item: TokenStram = "/" Isn't `attr` all of `GET, "/"` and `item` the function definition? Having attributes on a function not be able to interact with the function seems like it defeats the purpose. Here's an excerpt from the book page you linked: &gt; Here, we have two parameters of type `TokenStream`; the first is for the contents of the attribute itself, that is, the `GET, "/"` part. The second is the body of the item the attribute is attached to, in this case, `fn index() {}` and the rest of the functionâ€™s body. &gt; &gt; Other than that, attribute-like macros work the same way as custom derive macros: create a crate with the `proc-macro` crate type and implement a function that generates the code you want!
 Yeah i know, if you read it as a competition it's cheating. However, that was not my main goal of this, it was just me starting out for fun noticing they were almost equal but even I got surprized by how big the effect whas after just a couple of small changes towards using perfectly normal patterns and libraries that you'll use in Rust all the time. 
Not a video games but thanks for the tip. Probably I'll add the file alongside the executable with a touch of encryption... 
I see. Though it can be used [in some way](https://github.com/rust-analyzer/rust-analyzer/blob/master/editors/README.md) in VSCode now.
Thanks, now there is a panic and the program breaks: thread 'main' panicked at 'called \`Result::unwrap()\` on an \`Err\` value: ParseIntError { kind: InvalidDigit }', src/libcore/result.rs:1009:5 note: Run with \`RUST\_BACKTRACE=1\` for a backtrace. &amp;#x200B;
Presumably, the newest in 2006 graphics technology.
you are probably right.
Many Rust functions do return [Result](https://doc.rust-lang.org/std/result/enum.Result.html) (value or error) or [Option](https://doc.rust-lang.org/std/option/enum.Option.html) (value or nothing) if the operation can fail. IOW you're forced to check results of those functions. If you don't care (playing, experimenting, ...), you can call `.unwrap()` to get the value (panics if it's error / nothing), you can call `.expect("Some message")` to get the value (panics with the message if it's error / nothing), ... But you shouldn't do it (unless you're 100% sure that it contains value) in your production code and you should handle errors properly. Here's working example: ``` use std::{io, str::FromStr}; fn read_stdin&lt;T&gt;(label: &amp;str) -&gt; Result&lt;T, String&gt; where // T can be anything that implements FromStr =&gt; we can call .parse() T: FromStr, // FromStr::Err implements Display =&gt; we can call format!("{}", e) with FromStr::Err &lt;T as FromStr&gt;::Err: std::fmt::Display, { let mut s = String::new(); println!("{}:", label); // Read from stdin and fail soon if it fails, check the ? io::stdin() .read_line(&amp;mut s) .map_err(|e| format!("Oops! Something went wrong: {}", e))?; // s.trim() is here to remove \n (new line) at the end of the string s.trim() .parse::&lt;T&gt;() .map_err(|e| format!("Unable to parse: {}", e)) } fn main() -&gt; Result&lt;(), String&gt; { let first: i32 = read_stdin("Hey mate! What's your number one")?; let second: i32 = read_stdin("What's your number two")?; if first &gt; second { println!("{} &gt; {}", first, second); } else if first &lt; second { println!("{} &lt; {}", first, second); } else { println!("{} == {}", first, second); } Ok(()) } ``` `?` is explained [here](https://doc.rust-lang.org/1.30.0/book/2018-edition/ch09-02-recoverable-errors-with-result.html#a-shortcut-for-propagating-errors-the--operator). `read_stdin&lt;T&gt;` is explained [here](https://doc.rust-lang.org/1.30.0/book/2018-edition/ch10-01-syntax.html#generic-data-types). Here's the simplified version if you do not want to use generics: ``` use std::io; fn read_i32(label: &amp;str) -&gt; Result&lt;i32, String&gt; { let mut s = String::new(); println!("{}:", label); // Read from stdin and fail soon if it fails, check the ? io::stdin() .read_line(&amp;mut s) .map_err(|e| format!("Oops! Something went wrong: {}", e))?; // s.trim() is here to remove \n (new line) at the end of the string s.trim() .parse() .map_err(|e| format!("Unable to parse as i32: {}", e)) } fn main() -&gt; Result&lt;(), String&gt; { let first = read_i32("Hey mate! What's your number one")?; let second = read_i32("What's your number two")?; if first &gt; second { println!("{} &gt; {}", first, second); } else if first &lt; second { println!("{} &lt; {}", first, second); } else { println!("{} == {}", first, second); } Ok(()) } ```
 That's what I should have done however I didn't plan to do maximum optimizations and write an article about it, I started out making the simplest changes I could find and thought it was rather interesting to see the rather huge effect that had, and then I wrote an article about it. Especially since many (like myself) start out taking a known codebase and and naively porting it to Rust, and I wanted to exite them to look further. I still hope I kept it an interesting read for people that know Rust and is interested in the topic though. 
TIL
Can this run in browsers? The wgpu repo https://github.com/gfx-rs/wgpu doesn't mention it at all.
There are lots and lots of small allocations here (most done through copies) and at least some that could be avoided (all Add, Mul, Rem operations consume `self`). I might add secondary methods that does not consume `self` to avoid allocations and see how much more code I get and the impact it has. I'll try to `#[inline(always)] the Vec3 functions though and see if that has any effect. Thats easy to do.
Ubuntu/Intel Mobile GM45
Thank you very much, this explained a lot :)
I see, that's why they're not well documented as the rest of the features
Yes, I missed that part, thanks for pointing it out!
That is a blog post that I'm gonna read very carefully, thanks!
Nice to see that I'm not the only one having trouble with such parts :)
Thank you for your answer, I missed that part in the documentation
Josh Robson Chase has written about an \[embedded futures executor he's building\]([https://josh.robsonchase.com/embedded-executor/](https://josh.robsonchase.com/embedded-executor/)).
Thanks! I'm very pleased to learn that a 1.0 is in progress, is there any expected time of release ?
get your_program_file size cat video.file &gt;&gt; your_program_file attach footer saying "FOOTER: video.file starts at your_program_file size and has XXX bytes". 
It is the Rust equivalent of surrounding the code in `main` with `try {} catch (Exception e) {}`in Java.
Certainly wouldn't hurt to support an alternative like ChaCha 
You can also use parallel linq.
iterators in c# have a lot of overhead, so you would want to avoid linq in a benchmark battle
I'm trying to call a function on a custom that that's a struct, but what struct it is depends on some data I'm serializing from a file. My first though is to look at the string and have a struct / HashMap with keys of strings and values of that struct type to convert it. 1. Is this the best way to handle this? 2. Is there a way to programmatically get a key's value from a struct? I can do `foo.bar` but `bar is hard coded. In JavaScript you can access objects like that or like `foo[bar]` where bar is a variable.
How does Rust resolve naming conflicts? For example, if I want to `impl Cron for MyStruct`, does Rust do anything to understand that my `Cron` is different from `use other_crate::Cron`? Or should I just name it `MyCrateCron`?
This sub is for the Rust programming language not the game. 
Though I do agree that the name is kinda stupid and really fonfusing at times. 
miss post Sorry
On the note of minor text fixes: under the heading "Cargo 1.33.0", I think "Compiled withâ€”release flag," was an autocorrect from "Compiled with `--release` flag."
This is so cool. Has the source code for this been released yet? I have a nRF9160 and would love to code it using Rust.
Mainly, it is the same as method chaining, just for functions. Also, it was mentioned in the RFC, the \`\_\` placeholder can be used to put intermediate value into arbitrary positions, so, it is much more functional that just method chaining. And yes, we could think of it as a some sort of sugar to avoid boilerplate code, like \`?\` does to avoid \`try!\` shit.
If you are interested in this, we will discuss it in a language working group. Tell me if you want to participate.
If you are interested in this, we will discuss it in a language working group. Tell me if you want to participate.
Why doesn't `dbg!(foo)` or `println!({:#?}, foo)` display the types for some data like `String` vs `&amp;str` or whether something is a reference? That's pretty important for debugging IMO.
One plan was indeed to turn the lint on by default for the 2018 edition eventually
No, the umbrella term for both would be â€œsemantics.â€ A reduction semantics is a particular family of operational semantics that uses what are called evaluation contexts to essentially only write the â€œinterestingâ€ rules.
Awesome, thanks for posting! Man, Rayon amazes me again and again. It's beautifully easy and straightforward to implement, and yields amazing results for workloads like this. 
Unfortunately that ship already left and such change would be a breaking one. And that discussion already happened in the RFC PR. 
Awesome!
Despite the name Firefox Send doesn't really have anything to do with the browser Firefox
I really enjoyed the format, keep it up!
Really good, thank you!
yep `cargo install-code` should build and install the plugin for you.
`rustup doc --book` should be what you need.
Yeah, Medium does that, I'll see if I can fix it somehow. Thanks.
This should be enabled by default. see this in [stackoverflow.com](https://stackoverflow.com): this could have prevent confusion from a lot of beginners.
I did the first optimization step, didn't make any difference in the C# run time. 
Pls tell me this is the new Rust 2018! Rust Keeps changing things very quickly.
That's because there is something that is not digit in the stdin. Probably newline character.
While this was written in Rust 2018 I believe it would be the same thing in Rust 2015. The changes between the editions are not that big. Hope this helps!
I know about the library but haven't really considered it for this example. Ideally I would have loved to use `const fn` to create a stack based array but that doesn't work for me yet. &amp;#x200B; I might try later to create a version where everything is optimized to the best of my abilities. But that will have to be later :)
Thank you, I have updated the post.
From what I could gather after a few quick searches, I think the WebGPU standard is still very early, and no browser has them implemented yet.
Macro is very restricted and can't be that ergonomic and straightforward as something language provides. Even contributors corroborated that fact, so I am not alone in this. You can operate with \`?\` inside a macro, for example, you can do more or less complex things in them because of restrictions. I have written this in the discussion of the RFC also. So, this can't be a simple macro to be useful as my RFC states. Also, there is already a crate there, called "pipeline" :) It is mentioned in the RFC, in the comments in github as well and why it is not a solution. &amp;#x200B; I also don't care much about tokens, we can choose any of them, we just might to bring not only personal opinion but some thing to operate with, some objective reason for it. I explained why I chose it: because it is already used in the grammar and means similar to "goto" in \`match\` context: match "abcd".len() { 1 =&gt; ..., 2 =&gt; ..., } That's what I thought. &amp;#x200B; \&gt; Another thing, how would you handle tuples as a function return? Would they be treated as an individual type, or expanded (allowing piping multiple arguments using tuple returns). Just as usual in Rust, if \`\_\` denotes a tuple in lambda, it would be the same here. &amp;#x200B; If you are interested in this, we can discuss it in a language working group. Tell me if you want to participate.
A tool to anonymise CSV files: [https://github.com/Byron-TW/anon-csv-cli](https://github.com/Byron-TW/anon-csv-cli) 
I mean, if you are interested in pipelines in Rust, we could discuss how to implemented in a good way. If you want to do that and help, just tell me.
Actually, Python uses it's own tricky mechanism for big numbers. As I managed to figure out. And Ruby relies on something like GMP.
reddit doesn't use ```, it uses indent 4, I know, total pain
Sergio (the author of Rocket) has a talk where he touches on exactly what the macros generate: https://youtu.be/t_FUZ34ahBE?t=1160
Not yet. Working on it!
Can \`mdBook\` generate pdfs and/or epub ?
It would have been nice if it was a hard error. That would have had to be have been done already last year though :(
Iâ€™d say 4-8 weeks
The stack is just a region of the programâ€™s memory space^1 that is marked as being available for storing a *stack* (duh) of function *activation records*, which is to say, the arguments, local variables, return addresses and possibly saved register state^2 of currently active function calls. Most processor architectures have hardware support for manipulating the stack in the form of push/pop instructions and registers that keep track of the top of the stack^3. The heap, or *free store*, is another region in the memory marked for use by the program's heap allocator. Unlike the stack, which operates by a strict FIFO (first in, last out) principle, the heap allows allocating (marking as used) and freeing pieces of memory in an arbitrary order. The allocator uses special algorithms to efficiently keep track of which parts of the stack are free and which are in use. Confusingly, however, the free store has nothing to do with the [heap data structure](https://en.wikipedia.org/wiki/Heap_(data_structure))! ^1 Which a virtual thing distinct from actual RAM; the operating system may map the memory the program sees into actual RAM however it likes. ^2 If a function has intermediate results stored in registers, it has to copy them to the stack before calling another function that might use the registers for its own purposes. ^3 Technically, on many architectures the stack grows *downwards* in memory (that is, towards lower memory addresses) mostly for historical reasons.
You canâ€™t import two Crons at the same time. You shouldnâ€™t repeat MyCrate, as they can use â€œasâ€ on the import to name it that if theyâ€™d like.
Subscribed
I ended up porting my server over to actix a while back. I thought the rocket api was a little more elegant, but my server would become unresponsive sometimes. Also actix doesn't require nightly. 
Citing the article: &gt; The code runs on a variety of platforms from a single source: Vulkan, Metal, D3D12/D3D11, and eventually the Web (when the browsers gain support for the API, which is also in our scope of work).
Rocket is no async.
Hm ... there is nothing else a number... But it dosenâ€˜t matter anymore. Itâ€˜s for a mvp. I will program this in python. The final product has to be in rust. But for the first step python is fine.
Definition of BoxResult is: &amp;#x200B; type BoxResult&lt;T&gt; = Result&lt;T, Box&lt;error::Error&gt;&gt;; &amp;#x200B;
Rocket: Requests per second: 25429.36 [#/sec] (mean) Time per request: 0.393 [ms] (mean) Time per request: 0.039 [ms] (mean, across all concurrent requests) Transfer rate: 4097.50 [Kbytes/sec] received Actix: Requests per second: 27264.27 [#/sec] (mean) Time per request: 0.367 [ms] (mean) Time per request: 0.037 [ms] (mean, across all concurrent requests) Transfer rate: 3434.66 [Kbytes/sec] received 
But the types are known to you anyway, why would you have to print it?
I lack the skills to implement two algorithms in a way that's acceptable. 
You'll have to do that yourself via a HashMap (which is essentially how it's implemented in python or js anyway).
Actix not requiring nightly is a very good reason to use it over rocket I think (for the moment)
&gt; FIFO (first in, last out) I think you mixed up your acronym there :p Good explanation otherwise, though.
I'm curious to know if const fn combined with const generics will enable us to write as expressive code as C++17 similar to it's template type\_traits. I guess a lot of that is covered using declarative macros, but some procedural macro type functionality could be expressed? &amp;#x200B;
If I'm debugging something then they may not be known. Maybe I don't understand something was coerced to something else. Maybe the type that was returned isn't from my own code. I don't know, but typing is IMO a big part of debugging something effectively.
This is not true, you can publish to namespaces you don't have the domain for. In modern java there isn't really a relation between namespaces and domains. The reason it's written like that is back in the days when people thought network classloaders were a good idea, and it would actually load the dependencies from that domain. Not any longer.
Unless you're working with generics or trait objects you can always get your IDE (or the compiler) to tell you what type a function returns. The docs will incude the return type as well. Rust doesn't have a lot of coercion going on, but even there the compiler will tell you just fine, no need to inspect during runtime.
`wrk` instead of `ab` gives quite different results, but still much better than yours: Rocket: 2 threads and 10 connections Thread Stats Avg Stdev Max +/- Stdev Latency 0.96ms 14.01ms 402.23ms 99.63% Req/Sec 10.37k 2.84k 14.67k 82.88% 537042 requests in 30.10s, 74.78MB read Socket errors: connect 9, read 537041, write 0, timeout 0 Requests/sec: 17842.18 Transfer/sec: 2.48MB Actix: Thread Stats Avg Stdev Max +/- Stdev Latency 64.10us 19.47us 1.30ms 93.79% Req/Sec 68.65k 2.56k 74.90k 62.79% 4111170 requests in 30.10s, 505.77MB read Requests/sec: 136585.76 Transfer/sec: 16.80MB The socket errors certainly suggest something going a bit wonky somewhere. I note wrk's triggering my ICMP RST packet rate limits on both tests where ab is not.
I'm not sure which one is more productive once you get used to both.
Nice, I didn't know that the argument binding in closures followed pattern-matching rules. That's cool, and now that I think about it, kind of obvious (I have seen `.some_func(|(x, y)| ...)` before, which is a pattern-matched closure).
I guess my use case was for learning Rust. Rust does sometimes coerce values like strings (at least in my understanding). Another example is that println and dbg will display the value a pointer points to and not the pointer itself. That's useful, but it is confusing for me coming from a language that doesn't use pointers because if I dbg or println a variable and it displays just the string value, I would not understand it's a reference and would need to dereference it to mutate the value itself. It would have saved me so much angst to have had it just say "pointer" or "reference". Or if it's an Rc or Arc it would be nice to see the actual pointer metadata like reference count.
Love it!
Oops :D
This can't be done with a single edition, it's important for editions to have a transition step where it may not be perfectly idiomatic but compiles on both. Otherwise migrating is a pain. We may do this the next edition!
What would make them differ? Do AMD CPUs produce different results from Intel CPUs?
Great explanation. This helped a lot with some issues I had and just accepted when I got the compiler to work. Now I have a bit of a better understanding of how this works, and I think Iâ€™m better equipped to figure this stuff out next time. Thanks for your effort throwing this video together. 
Sorry, can you rephrase?
for pairings, the [pairing crate](https://docs.rs/pairing/0.14.2/pairing/) should have what you need (but it only targets BLS curves)
You can always type `let foo: () = value;`, the compiler will then throw an error message showing you the type of value.
Next version of actix-web will include rocket style route registration 
You cant. The compiler needs to know it is that you assign to this variable. You could rewrite it this way: let v = s1.split_whitespace().collect::&lt;Vec&lt;&amp;str&gt;&gt;(); but i dont really like the turbofish syntax for Vecs boxes and everything thats holds a type.
If you were talking about `collect`, you can specify type like this: // specify the whole type for the result let x = some_iterator.collect::&lt;Vec&lt;&amp;str&gt;&gt;(); // or you can only specify Vec part, and // leave item type out to be inferred let x = some_iterator.collect::&lt;Vec&lt;_&gt;&gt;();
Not just by company, but it can vary by chip series https://gafferongames.com/post/floating_point_determinism/
You can absolutely store something that returns \`impl Iterator\`: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a4348473d7662809180677ab92f78b66](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a4348473d7662809180677ab92f78b66)
The only one I know of is the Zcash pairing lib [bn](https://github.com/zcash-hackworks/bn). Maybe also [gridiron](https://github.com/IronCoreLabs/gridiron) for finite-field operations (this lib is fairly new, though has apparently recieved an [audit](https://github.com/IronCoreLabs/gridiron/pull/24) recently).
If I have a function signature such as fn f&lt;T&gt;(x: &amp;T) Am I guaranteed that x will remain unchanged?
Right but in this case it lets you eliminate assigning to `v` which is I think what was being asked: `match s1.split_whitespace().collect::&lt;Vec&lt;&amp;str&gt;&gt;()[..] {`
Not testing on OSX are you. For some reason I've never figured out benchmarks that involve local networking on OSX can produce really low numbers, but then be fine in other cases. Maybe something to do with network interfaces and maybe default settings of OSX.
Check out cargo-xbuild, I am pretty sure that, even though itâ€™s sorta intended for cross compilation, it at least has the infrastructure to do this.
i do not understand how it is related to Rust
Since youâ€™re looking, a few others are Tower Web, Warp and Gotham. They are also asynchronous frameworks :)
Pijul is written in Rust at least. What I don't understand is what OP's actual problem with Git is.
Unless you invoke unsafe, yes. Mostly because without any bounds on T you can't really interact with it. If you are talking about an arbitrary type, though, f could technically mutate it if the type contains values with interior mutability (RefCell/Cell mostly).
Pijul is written in Rust and the authors have written some nice Rust crates
Only old reddit.
There are two crates that help a bit with that: &amp;#x200B; [https://github.com/johannhof/pipeline.rs](https://github.com/johannhof/pipeline.rs) (was already mentioned in this thread) &amp;#x200B; [https://github.com/andylokandy/comp-rs](https://github.com/andylokandy/comp-rs)
True this works. But I would not recommend this either. 1.) it's gets less readable 2.) would be more code changes if you decide to later reuse the result I don't know what the style guide recommends though
Yeah, just seems like a workaround
I'm not aware of it working anywhere except the new desktop site. old.reddit, i.reddit, the official mobile app, every unofficial mobile app I've used - none of them support it.
I think you're correct that neither branch lives long enough. The problem as far as I can tell is that you are calling `node.borrow_mut()` inside of the loop but storing references to it in variables that are outside of the loop. The mutable reference you have to `node` only lives until the end of the current loop iteration, so you cannot store references to it in variables that outlive a single iteration.
You could include text version in the description â€“ it can be very useful sometimes.
You are a wonderful person
Probably not the answer you're looking for, but self-extracting archives are an example of this. (They're a small EXE called a self-extractor stub, plus an archive.) On Windows, you'd use this command to combine them: COPY /B "input.exe"+"input.dat" output.exe On Linux or MacOS, you'd do it like this: cat input_executable input.dat &gt; output_executable The main caution regarding self-extractors is that, if you play around with Zip files, you need to fix up the offsets by running `zip -A output.exe` (`/usr/bin/zip` or `zip.exe` from [Info-ZIP](http://infozip.sourceforge.net/)).
If you want to play a sound, you have to call into the OS, or into a library that calls into the OS. On Windows, that means you'll need to do some FFI work to call into the Windows APIs (e.g. [XAudio2](https://docs.microsoft.com/en-us/windows/desktop/xaudio2/how-to--play-a-sound-with-xaudio2)). On linux you can (usually, sometimes) play audio using the ALSA API. A C example demonstrating using this API can be found [here](https://gist.github.com/ghedo/963382/815c98d1ba0eda1b486eb9d80d9a91a81d995283). Alternatively, you could use an abstraction library, which would allow you do what you want regardless of the OS. For example, [SDL's Audio API](https://wiki.libsdl.org/Tutorials/AudioStream). All of the above examples have one thing in common: you need to call out to a C-compatible API from Rust. I'm unclear on why you would want to do this without external crates, since at the very least, the FFI work is tedious and error-prone.
I don't know why you've been downvoted, this is absolutely true.
It was determined that the lints just weren't ready yet, so that's why they weren't turned on.
&gt;Compiler &gt; &gt; 1. **Improving "core strength"** by lowering raw compilation times ... &gt; &gt; 2. **Improved IDE integration** ... \#1 seems already like a herculean task alone, but with #2 at the same time trying to compete with state-of-the art IDEs seems impossible. Or is it not meant to compete with the best, but only to support simpler editors? &amp;#x200B; If the latter, is it really worth it? As a professional developer, I'd rather have (buy) the best IDE I can get, and have a fast batch compiler, than something that is neither. &amp;#x200B; To me, putting IDE focus in the compiler team seems like the wrong place. IMO it would be better to consult with the best IDE vendors when designing the language and library APIs. And there the answer is probably to put an emphasis on not using macros whenever possible...
oh you can, but you will end up rewriting a lot of libraries.
&gt; #1 seems already like a herculean task alone, but with #2 at the same time ... My understanding is that these two things are connected, so it's not so much two separate things as it is "the core redesign helps both things." &gt; IMO it would be better to consult with the best IDE vendors when designing the language and library APIs. Some of the compiler team members previously worked at some of the best IDE vendors. That's pretty much what's going on. Basically, this is following in the footsteps of other languages that have amazing IDE stories, like C#. An IDE needs to understand your code. The compiler understands your code better than anything else. Them working together is what's required to have world-class support.
`alsa` crate has [an example](https://docs.rs/alsa/0.2.1/alsa/pcm/index.html) for sine wave playback. Not sure about cross-platform crates though.
Trying to call a `RefCell&lt;Box&lt;FnMut()&gt;&gt;`, but `f.borrow_mut()()` gives weird error saying that "cannot borrow data in a `&amp;` reference as mutable" I have found a workaround (`(&amp;mut *f.borrow_mut())()`), but I wonder what the error actually means, and why the initial code is wrong. [link to playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=606a0b76208498c52a01eea682e255eb)
&gt; Users post millions of data on the webserver, webserver forward all those queries to a process at this specific "second or time" and need to be then inserted in a DB. Another process is then reading all of them It seems odd to store some data in a datastore and immediately read, and then delete. In most analytics workflows this pattern is called micro-batching, and doesn't utilize a datastore. I'm not sure why you're not able to use an in-memory buffer, but you should look into how logging systems achieve high throughput (e.g. [Asynchronous Loggers](https://logging.apache.org/log4j/2.0/manual/async.html)). You might even be able to get away with using a really fast Logger for your system. Basically, create a file every "second or time" and then the next process will operate on the created files.
Check out cpal 
The main difference seems to be that rocket doesn't support connection keep-alive, thus incurring connection overhead for each request. This is quite significant given how simple the actual request is. That's why /u/Freeky saw equal performance using \`ab\`, which needs the \`-k\` flag to use keep-alive. With that option, actix is a lot faster there as well. \`wrk\` uses keep-alive by default. You can try running \`wrk\` with \`-H 'Connection: Close'\` to disable keep-alive, performance numbers should be a lot closer then. &amp;#x200B; Additionally, as /u/burtgummer45 noted, if you're testing on OSX that might affect the results as well, because by default it uses a smaller range of ports for "outgoing" connections, and IIRC has different behaviour when it comes to port reuse compared to linux. So in the non-keep-alive version you might simply run out of ports, which could explain the socket errors reported by \`wrk\`. I don't recall how to increase the port range and the other settings to allow "proper" benchmarking on OSX (not a OSX user myself), google might be helpful here.
Hm. I'd expect \`SplitWhitespace\` to allow you to get the rest of the iterator, but that function is missing. If you search for the whitespace yourself and split it, you can avoid the need to collect the iterator back into an owned value. &amp;#x200B; [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5528dd408e0608f5aa87032a7321b209](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5528dd408e0608f5aa87032a7321b209)
I guess neither arm can compile because `borrow` doesn't live long enough. It just quirk of the compiler that it don't tell you about both errors, but once you fix one arm it will complain about another.
While it's true, it shouldn't be responsible for a difference of *160x*.
On a hello-world style benchmark? It's a bit out of whack, but not \*extremely\* so. Look at [https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=plaintext](https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=plaintext); actix-web gets 6,723,062 . Rocket is not on techempower, but let's look at iron: 109,815 . That's 61x. So yes, not quite that extreme, but still pretty extreme.
This is because wrk using keep-alive by default, which seems to be only supported by actix, but not by rocket. Using `-k` with `ab` enables keep-alive, while using `-H 'Connection: Close'` with `wrk` should disable it. That should give similar results with both tools.
Freeky is right. For these kinds of tools, you can't strictly enforce UTF-8. As soon as your tool gets broad use, you'll get bug reports about it. I speak from experience. Even popular repositories such as `linux` are not fully encoded in UTF-8. Some of its C files are encoded as latin-1 for example.
Thanks :)
Bingo. With -H 'Connection: Close': Rocket: Thread Stats Avg Stdev Max +/- Stdev Latency 214.13us 242.71us 34.22ms 99.97% Req/Sec 12.58k 492.62 14.13k 68.27% 753658 requests in 30.10s, 118.59MB read Requests/sec: 25038.39 Transfer/sec: 3.94MB Actix: Thread Stats Avg Stdev Max +/- Stdev Latency 276.62us 2.27ms 132.46ms 99.89% Req/Sec 12.62k 746.98 19.34k 82.20% 755012 requests in 30.10s, 106.57MB read Requests/sec: 25084.35 Transfer/sec: 3.54MB
I love it, very ambitious! Although at first it sounds like repaying technical debt isn't that exciting for users, it sounds like it will enable a lot of very cool stuff - Language Server and IDE support, especially. Also excited to see const generics, Generic Associated Types and Async / Await called out specifically to be shipped this year!
IDEA for Java proves that you don't need support from your standard batch compiler to get the best possible IDE support. And incidentally, the best current IDE support for Rust is provided by CLion, which is AFAIK not relying on rustc. Common sense would tell me, removing one design goal (IDE support) would help better tune the design (and resources) for other goals. But I'm sure the compiler team knows better than me.
&gt;And incidentally, the best current IDE support for Rust is provided by CLion, which is AFAIK not relying on rustc. JetBrains is the company I was referring to in my last comment :) It \*can't\* rely on \`rustc\` currently because \`rustc\` is not set up to be able to depend on it.
I tried https://crates.io/crates/cpal to play generated sine waves, and it works as expected at least on Windows and Mac OS.
Narwhal on iOS apparently supports fenced code blocks (somewhat strangely). As I also use old reddit and the mobile site though, fenced code blocks are unsupported more often than the opposite.
This my be an absurd question coming from ignorance, but re #1: Wasn't the Rust syntax designed to be 'unambigious' *in part* because it would speed up compile times? I only ask because ISTR reading something along those lines, and I personally find the syntax rather noisy symbol-wise. (But then, I'm used to Haskell syntax, so... :) )
Oh crap. You're right. Please excuse me ... I have some software to fix ... I'll just go with `from_raw_fd` etc.
It seems to me that there should be something like `RawStdin` etc. as quite often people building CLI tools that takes massive amount of data on stdin, would like to optimize it.
Greatly appreciate the shoutout to Amethyst! Weâ€™re working on our first showcase game, which will eventually be a good showcase of gfx-rs capabilities as well. https://community.amethyst-engine.org/t/demo-game-evolution-island-mvp/487?u=erlend_sh
Handling binary data is relatively easy =&gt; just avoid `String` and use `Vec&lt;u8&gt;`, which is encoding agnostic. You should be able to relatively easily convert the program, using `b"xx"` instead of `"xx"` for literals (in tests) to get a `&amp;'static [u8]` instead of a `&amp;'static str`.
Yes, this format is great. Looking forward to more videos!
&gt; One way to think about this is that the impl block doesn't actually create a new scope in the same way that method bodies or curly braces inside them do. So if you were to put a use statement in the impl block, which scope would the imported names be added to? That's... a slightly unfortunate phrasing. You can put associated consts/types in an `impl` block, which are scoped...
This is a really good idea. Please keep it up, it will serve to educate more people who are new to Rust and those who have been around for a while. I also suggest that you do include other less advanced topics as well. Thank you!
The term youâ€™re thinking of is LL(1), and while it was for a while, thereâ€™s one corner case (raw strings) where itâ€™s context sensitive. It is true that that would be quicker but thatâ€™s not the only reason, and itâ€™s a *very* small part of overall compile times.
I have a counter-proposal: fn find_by_name(document: &amp;select::Document) -&gt; Option&lt;&amp;str&gt; { Some(document.find(select::predicate::Class("name")) .next()? .text()) } fn trimmed_and_owned(s: &amp;str) -&gt; Option&lt;String&gt; { s.trim().to_owned() } fn parse_name(html: &amp;str) -&gt; Option&lt;String&gt; { select::document::Document::from(html) .find_by_name()? .trimmed_and_owned() } This proposal enables pipelines while being MUCH more generic. Essentially, it's about considering that anything function taking a `T` as first argument can also be invoked with method syntax on `T`. Thoughts?
Is there any kind of list of "nearly unsafe" Rust code? I'm talking about things like Panics and Overflows. I really like how I can easily see the things that can fail by looking at the `?`s, `unwrap()`s, `expects()`s, etc. But several times I had panics because integer overflows (or worse, silent overflows because of `--release`), or panics because of slicing/indexing. Do you have any tips on how to find and fix potential problems like these on my code? I think I should at least check code that uses indices on `Vec`, maybe replace with `Vec.get()`.
I see them as a virtuous cycle: Better integration means less need to invoke the compiler because the user has better feedback about what the code is doing. After all, *actually* compiling the code is always going to be slower than just being sure that the types are correct.
No, for example `AtomicUsize` can be changed through a shared reference.
The ``` syntax doesn't display correctly on old Reddit. Indent your code with spaces or tabs instead.
Done.
All in favor of banning floats in `const fn`, say Aye. I have tried working on a position library, but would like to see the `uX` types implement `num-traits` to allow for maximum generality. Perhaps I could continue working without `uX` and just focus on 8, 16, 32, 64, and 128 but values with `num-traits::PrimInt`.
Why do you think that parsing is the bottleneck? (Hint: it's not)
Love this! Def looking forward to next vid! Thanks so much for making this!
Hum... could you elaborate? I usually think of `mut` as meaning "unique-reference-at-this-time", do you see another purpose?
I don't see Unix-style CLI tools pushing themselves to the utmost of performance, generally speaking. I mean, it's not considered bad form to write them in sh or Python!
Even though there's no solution yet, glad that it helps identifying an issue :) There is a `StdinRaw` obtained by calling `stdin_raw()`, but it's not publicly exposed... https://doc.rust-lang.org/src/std/io/stdio.rs.html#66
Yeah, zero copy is more like a paranoid to myself... Buffer size matters more, trying to tune it for a while. I see some code using `File::open("/dev/stdin")` then creates BufRead on top, which is the same idea with `from_raw_fd`, definitely not portable. Example: https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/revcomp-rust-2.html It's funny that when the buffer size is too large, like as big as the file itself, the read time actually went up. Not sure if there's any magic going on in the BufReader, like unnecessary zeroing-out the buffer or something.
This is great. Bite-sized rust chunks. Could you turn up the level on your microphone just a bit for future recordings?
&gt; But several times I had panics because integer overflows (or worse, silent overflows because of --release), or panics because of slicing/indexing. Honestly, the best answer is still "unit-testing and fuzzing" because those kinds of divergence can't be eliminated by static analysis without solving the Halting Problem. Rust just provides enough protection to keep those bugs from turning into arbitrary code execution. There's a lot that can be done with formal proofs of program correctness, but again they ultimately runs into the limits of the HP and Incompleteness Theorem. If that's your need, truly "safety critical" software engineering, Ada is almost certainly a better language than Rust.
1/4 of C#, not 1/4 of C
That sounds like [clippy](https://github.com/rust-lang/rust-clippy)'s restriction and pedantic lint groups. Try out `#![warn(clippy::pedantic, clippy::restriction)]`. If that gives too many warnings (likely), then you can look through [here](https://rust-lang.github.io/rust-clippy/master/) and pick out the specific ones you want, like indexing\_slicing, and you can even make them more strict, like `#![deny(clippy::indexing_slicing)]`.
What made you think that I thought that. It's pretty obvious that parsing time would (absent pathological cases) would be completely trivial. My question is more along the lines of [assuming we know all of the previous things]... why be really particular about LL(1), for example?
What are the other reasons? That's really what I was driving at. I'm basically just curious about the trade-off that leads to the syntax being so symbol-heavy.
C FFI is the only way, by design. How you load the plugin and resolve the symbols will be platform specific.
Whatâ€™s easier for parsers is also easier for humans. Keeping things nice and regular is often easier to understand than complex special cases.
&gt; Whatâ€™s easier for parsers is also easier for humans. That's an interesting hypothesis. I think you mean "programmers" rather than "humans" :). I'm not sure I buy it. For a reductio-ad-absurdum: Binary is *really* easy for parsers... humans, not so much. &gt; Keeping things nice and regular is often easier to understand than complex This I agree with, but I don't have any real evidence.
I think you fundamentally misunderstand lifetimes. They don't influence codegen at all, they are only a tool to check if what you wrote is correct. If you get a lifetime error it doesn't mean the lifetimes are wrong, it means *your code* is wrong because of something like use after free or concurrent modifications. What you're asking for is for the compiler to silently fix your incorrect code, but there's no quick and generic fix for those problems.
I meant grammar complexity, Chomsky style, not necessarily syntax :) I come from Ruby, which has a notoriously complex grammar. Itâ€™s something you can obviously deal with, and it can make some really beautiful DSLs, but it can be really hard to tell what youâ€™re actually doing at times.
&gt; I meant grammar complexity, Chomsky style, not necessarily syntax :) Ah, but Scala has tried that tack. I don't hear people saying that it's particularly simple :).
Totally! I certainly donâ€™t think this is universal, and I donâ€™t always agree with it myself, I was just trying to answer your question. Thatâ€™s the argument thatâ€™s usually cited.
Ah, right... and thank you. :) 
When wipe Will be released!?
Looks like its a [known issue](https://github.com/rust-lang/rust/issues/51886) without much action. Basically, `borrow_mut` returns a `RefMut`, which implements `Deref` and `DerefMut`, meaning it can be converted from `RefMut&lt;T&gt;` to `&amp;T` and `&amp;mut T`. However, it looks like rust prefers the immutable version, so it tries to call on an immutable borrow instead. With `DerefMut` in scope, `(f.borrow_mut().deref_mut())()` also works, and is what's happening behind the scenes when you do your workaround.
The only guarantee you have about `x` is that you'll never be able to touch `x` - it's a function parameter and will be destroyed before `f` returns. So you can't observe any changes. But you're asking about `*x` - the *target* of `x` - and yes there is a guarantee that `f` will not change that target. This guarantee comes from two parts. First, for most data types, the fact that a location is shared - using `&amp;` instead of `&amp;mut` - means that the location *must not* be modified. There are exceptions to that rule, using `Cell` and so on. However, we can also assume that `f` doesn't make use of those those special exceptions. This is because of "parametricity." `f&lt;T&gt;` means "function `f` for any type `T` subject only to the condition that `T: Sized`." Because `f` works with almost any type imaginable, it can't use the special properties of `Cell`. The parametricity rule applies when the type parameter is mentioned *just after* the function name. --- There are a few exceptions. `f` can look at memory allocation properties of `T`: - the size of `T` - the alignment of `T` (can only be stored at addresses which are a multiple of the alignment) - if the drop operation of `T` doesn't do anything, the compiler *may* but is not obligated to share that information with `f` And if `T: Any` then `f` is allowed to guess whether `T` is equal to some type that `f` knows about.
This is pretty good, though you didn't mention the option which I probably see the most, would have used myself, and consider the most idiomatic: `if let Some(found) = collection.iter().find(|&amp;s| s == &amp;item)` You talked a lot about the the reason why those function calls have to give us references, but didn't explain what the original error was: `PartialEq&lt;String&gt;` isn't implemented for `&amp;&amp;String`, which is to say that Rust doesn't know how to compare `String` and `&amp;&amp;String`. `|&amp;s|` does indeed match against a borrowed String, and the reason we can't match against a double reference to a String is that doing so would be trying to *move* the value when we only have a reference to it, but instead of double borrowing `item`, we can match to a single borrowed `s`, and compare that with a single borrowed `item`, making it so the comparison is between `&amp;String` and `&amp;String`, which Rust can do just fine. &amp;#x200B;
When you go to the right subreddit
I'm not sure *why*, but the compiler is looking at `RefMut&lt;Box&lt;FnMut()&gt;&gt;`, which doesn't implement any of the callable traits, so then it auto-refs and eventually settles on a coercion to `&amp;FnMut()` which can't be called. But then it doesn't continue to try `&amp;mut FnMut()` I think it may be a bug. I can't find the part of the Reference that talks about this.
&gt; like unnecessary zeroing-out the buffer or something. Oh, probably yes. 
With all respect, I think all excuses like that are unhelpful BS. Rust is a system programming language, and if I want to avoid needless copying, I should be to do so. It doesn't really matter if it does give me any speedup in practice, how much or whatever. Even if it's just for fulfilling compulsory, irrational need and getting a nice warm feeling of creating an "optimal" tool, microbenchmarking. That said, piping stuff through pipes on stdio is a common pattern in Unix, and for some applications 1% of performance different (or even just CPU/power usage) can be significant, and can't be hand-waved with "nah, no one should need that".
You don't need support from the batch compiler to get the best IDE support, but that doesn't mean you can't go the other way around- C#'s Roslyn compiler was designed IDE-first much like JetBrains' tech, but is also used in the batch compiler. As I understand it that's the same thought process going into this work- /u/matklad came from JetBrains and started work on [rust-analyzer](https://github.com/rust-analyzer/rust-analyzer/), and now the compiler team would like to use some of the knowledge gained there in `rustc`.
Looks like it doesn't yet (see https://github.com/rust-lang-nursery/mdBook/issues/88 ), but you can get an epub (and everything else as well) here: https://nostarch.com/rust
* custom allocators * finish long standing RFCs * improved compile times * RLS 2.0 * MIR optimizations * separate compiler libs * easier cross compilation * custom profiles * ... What an amazing roadmap.
I'm not asking it to fix anything. I know lifetime annotations don't have any effect on code generation. Its probably one of the first things about rust I had to figure out :) Look at it this way. Say you have a function with a couple lifetime annotations, everybody is happy with it and it compiles. Now you swap those two annotations using find-n-replace on your editor. The function signature along still might be totally reasonable and probably ok with rustc. But rustc also looks at the body of the function and sees its not going to work with those lifetimes, and must know it with absolute certainty I assume. If it can be so sure that its not going to work, I'm just wondering if it can know with certainty what would work, and therefor make annotating lifetimes redundant and unnecessary. I'm just guessing but I think rustc first looks at the function signature with lifetimes annotations, decides if that makes sense in a superficial way, and then looks at the inside and decides if that works with the function signature, and then looks outside and decides if those lifetimes work when its called. The function signature is therefore a kind of source of authority for lifetimes, which makes sense because it is for type inference. I just wonder if its theoretically possible (or maybe just a huge hassle, or maybe nobody cared) for rustc to figure out for itself what lifetimes a function *needs* just by looking at the body. 
Basic version of route attributes already work in [1.0](https://github.com/actix/actix-web/blob/1.0/examples/basic.rs#L6) branch
I was trying to wrap my head around `&amp;&amp;` just last week. I wouldn't have thought to use pattern matching in a lambda like that, but it makes a ton of sense.
Someone is retarded.
So if I'm using a third party library passing my x to some mysterious f and the function signature is either f&lt;T&gt;(x:&amp;T) or f&lt;T:Sized&gt;(x:&amp;T) I can be completely sure x isn't going to be modified in any way?
This is not the subreddit you are looking for.
Ah! If it's to have a reploy from you it made my day :)
little do we know this is actually someone's custom made server for playing battlefield, written in rust
Oh, ouch
Serious? All I see is a link to some Discord server which I can imagine being focused on the game Rust and a game mode called Battlefield.
This is also offtopic for /r/playrust They need /r/playrustservers
no i was only joking, this is definitely about the game, funny to think about though
Even then, that would most likely be UB; you need interior mutability for this to work, but with no bounds on T, you can't be sure that it's a type that has it.
I fell for your sarcasm. ðŸ˜€
The syntax help, in parsing? a lot. &amp;#x200B; But what happened AFTER you have a AST is where all the heavy machinery start. Each compiler pass add time, then you have the Rust pass AND the LLVM pass. I think LLVM is where the problems are. 
I'd bet on keep-alive/pipelining being responsible for the difference there as well. &amp;#x200B; If you look at the numbers /u/Freeky has posted with keep-alive disable, actix and rocket are within 0.25% of each other WRT req/sec. &amp;#x200B; Each request and response is about 100-200 bytes, so there's hardly going to be any blocking whatsoever. So being async is not really going to provide any benefits in terms of doing useful work while waiting for IO to be performed. 
One thing I like to do with a problem like this is to try to write a recursive version of the method. It may or may not be possible to write an iterative version the way you want and the recursive version may better illuminate what's wrong with what you're doing since lifetimes, etc. are easier to understand across clear function boundaries.
Just to add on that you can see what I was saying on the different tests for the database. You can see how Rocket is doing in the "Physical" hardware compare to Actix web. Also my summary of this benchmark is that Actix web is really solid. You can check around all the types of settings for this benchmark and Actix web will still be at the top competing with the fastest ones out there like Vertx or Dropwizard. And sometimes being the fastest. Another thing to take out from this benchmark is how Actix web is consistent. It will be always at the top of each test when other frameworks are better at some and not the best at other benchmarks. So Rust is awesome and Actix web is great. Two articles I would recommand to read about real world implementation of actix web are: \- [Rust + actix-web in the on of the biggest music festival Atlas Weekend](https://www.reddit.com/r/rust/comments/8xdsx5/rust_actixweb_in_the_on_of_the_biggest_music/) from this sub Reddit and, \- [Generic Methods in Rust: How Exonum Shifted from Iron to Actix-web](https://medium.com/meetbitfury/generic-methods-in-rust-how-exonum-shifted-from-iron-to-actix-web-7a2752171388)
Yes. And thatâ€™s extra unfortunate because pipelining is... not very real-world. Sigh.
&gt;Using two different methods of dealing with concurrency that involve passing messages but in different ways seems like a recipe for confusion. I agree, but I don't know how is Warp better for this problem. Maybe you can write a minimal proof of concept to see how good is for your use case. &gt;Anything else I should be thinking about? If you only need WebSockets support (without extra features like routing) you can try the [websocket](https://docs.rs/websocket) crate. 
By async and IO I was talking about how OP is looking for "a web framework and", "very good to write an api server", and fast. If you are looking for that you will have to use somewhere a database maybe more than one with Redis and other and the truth is most of the time the framework will be waiting for these to resolve, regardless of how fast your framework is. Even it's Rails or Django. But async however will be critical.
I'm aware of that, I'm not arguing against async or actix in particular. I was specifically arguing the claim that it is realistic to expect a 61x or even 160x speedup on a hello world benchmark from using an async framework instead of a sync one.
as long as `x` isn't `&amp;AtomicUsize` or `&amp;Cell&lt;T&gt;` or similar, then yes.
I tend to prefer the syntax the other used for the reason explained. If you rely on destructuring the reference in `find` you ignore the reason `find` references the `Item` type to begin with. collection.iter().find(|&amp;s| s == &amp;item); // compiles collection.into_iter().find(|&amp;s| s == item); // cant move out of borrowed context vs // both compile collection.iter().find(|s| **s == item); collection.into_iter().find(|s| *s == item);
You may need to do some `clone`ing of the `Rc`s. So instead of keeping a `&amp;mut Rc` on the stack, you have a `Rc` so there's no problem with lifetimes and the `&amp;mut Rc` trying to outlive whatever data it resides in.
Actix is insanely fast, by default I'd assume Warp is much slower. But I don't actually know, however /u/seanmonstar might?
&gt; I agree, but I don't know how is Warp better for this problem [of having multiple message-passing methods for dealing with concurrency]. I guess I was thinking that Warp would be able to use channels and thus have less of a clash with the Redis pub/sub channel. The Warp [sse-chat example](https://github.com/seanmonstar/warp/blob/master/examples/sse_chat.rs) uses `futures::sync::mpsc` channels, which seems like a closer fit. Do you think that would still be too much dissonance? &gt; If you only need WebSockets support (without extra features like routing) you can try the websocket crate. Thanks. I do need a bit of routing and also need to support Server Sent Events (for existing clients that didn't implement websocket). But it still might be worth looking in to. &gt; Maybe you can write a minimal proof of concept to see how good is for your use case. Yeah, that's my current plan, but I wanted to see if anyone has thoughts while I work on the proof of concept. I'll report back as I get a better feel for the two frameworks.
Thanks! I'm trying that and looks interesting. Looks like that [in my code](https://github.com/martinber/noaa-apt) I have an absurdly large amount of casts, and that already gave me a few bugs so that's quite useful.
Thanks, I should learn to make better tests.
&gt; Actix is insanely fast, by default I'd assume Warp is much slower. Hmm, interesting. I'd been assuming that they're pretty similar in speedâ€”Hyper is frequently [within a percent or two](https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=plaintext) of Actix and, in my own testing, Warp is much faster than Actix for serving static files (which admittedly isn't a use-case Actix optimizes for). And I guess I was thinking that Warp was a thin enough abstraction over Hyper to be about as fastâ€”but maybe that's wrong. Why do you assume it'd be much slower than Actix?
&gt; there is a compile time error that prevents this from working that I cannot figure out how to fix. There is also an error in the call_function function that I have not got the chance to look into. Please include the error messages. Without them, we have to try to reproduce your setup on our machines while, with them, we might be able to tell you what's wrong at a glance.
I may not have expressed myself clearly. The part about interior mutability was about a function of type `fn f(input: &amp;Foo)`, i.e. an arbitrary but fixed type.
Sorry about that I meant to have it under the code. Edited and added.
``` pub use nalgebra_glm as glm; ```
I believe you can just replace `extern crate` with `use`. 
Ah well thatâ€™s delightful.
Happy to hear that they're planned to be turned on! Could you shed some light on what â€œweren't readyâ€ exactly mean?
Correct me if I'm wrong (I probably am!) but I thought: * Futures 0.3 was a testbed for new futures stuff, but not necessarily going to be the end product * Async/Await is still being fleshed out, that's what most people are waiting for * There is going to be a `std` futures, so I don't know whether Futures 0.3 is redundant or not?
The full text of the errors would be more helpful, but here's what I can tell you: error: type annotations required: cannot resolve `_: rlua::FromLuaMulti&lt;'_&gt;` This means that you have a function which can return more than one possible type. Rust would normally infer what you want from the variable you're putting the return into, but it's not getting enough information so you need some form of explicit type annotation. Beyond that, I can't say because, without the full error message, I don't see how your code relates to `rlua::FromLuaMulti&lt;'_&gt;` error: mismatched types label: expected struct `rlua::Function`, found enum `std::result::Result" You wrote `let custom: Function = globals.get(function_name);`, which forces `custom` to be of type `Function` but `globals.get` can fail, so it doesn't return a `Function`, it returns a `Result&lt;Function, SomeErrorType&gt;`. You need something like this: let custom: Function = globals.get(function_name).expect("error retrieving global");
Are there any plans to finalize the SIMD libraries?
Iâ€™m not sure what priority it is exactly but in general itâ€™s something weâ€™d like to finish.
Iâ€™m not 100% sure specifically but with lints it usually means â€œtoo many false positivesâ€ or â€œtoo few false negativesâ€.
Async/Await is considering stabilization soon, but it's waiting to be tested before doing so. The main thing preventing async/await from stabilization now is because not enough people are using it to find the issues with it, but not many people are using it because it's not stabilized. Somewhere this loop needs to get broken, and the most likely way for that to happen is to write software using it while acknowledging that it may change based on your feedback.
The \`Future\` trait from 0.3 (with tweaks) will become the std futures, but the futures crate will still add stuff (like more combinators) that aren't going into std.
Warp is built on top of hyper, which is faster when you look at [some out-of-context benchmarks](https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=plaintext). Besides the benchmarks, both hyper and warp have additional performance gains when streaming larger bodies. What gave the impression it was slow?
Than you for the info on the second one error that fixed it. Unfortunately on the first that is the entire error which is why it is confusing me so much.
Did it not draw a little arrow pointing to the portion of the line generating the error? That's what I meant when I said "full text". That said, I kept poking at it while you were responding and updated my response just before you replied. Take a look at the **EDIT** I added.
 * finish long standing RFCs Please yes. There are so many open issues and features that have been in nightly for years. Like, `Vec` has a `remove_itemâ€˜ which was proposed in february 2017. It has been in nightly for almost two years! One simple method! And there are more examples.
I'm trying to grok the new futures-preview. &amp;#x200B; They have a toy example which is shown [here](https://github.com/polachok/fahrenheit/blob/dac7430da5cbcfd810ca47dac85afc02236aa2dc/src/lib.rs). In particular I don't quite understand why they use a \`RefCell\` for the \`read\` and \`write\` and since I don't quite understand the semantics of the \`RefCell\` I'm not sure whether it's a sound decision or it's simply because it's a toy example and they'd use a \`Mutex\` in practice.
Could you give a pointer to any explanation of this please? I'm not sure whether you are saying the type system at compile time is Turing complete or some weaker notion of computation. 
Anything that can trigger undefined behavior if the context isn't set up previously should be marked unsafe. The safe version would need to ensure or check that the context is set up. You can ensure that it's set up by using something like `lazy_static` to automatically initialize it before access or by returning a token from the initializing function that is required to be passed to the other functions. Checking it means you need to either return early or panic if the context isn't set up.
It's also mentioned in the paper, but \`mut\` on a variable binding actually means something different, namely that you are allowed to reassign the particular binding later. Niko proposed eliminating this at some point (to only have the case you've referred to), and the ensuing community response (largely in favor of keeping \`mut\` for the ability to reassign a binding) is known as the "mutpocalypse."
I made that assumption because most other frameworks perform a little bit to a lot worse in that benchmark, and also because there are no benchmarks for Warp (that I know of).
The error was found and fixed in this PR: [https://github.com/cfsamson/pixar\_raytracer/pull/6](https://github.com/cfsamson/pixar_raytracer/pull/6). No significant effect on any of the measurements btw.
This is actually covered somewhat in the [nomicon](https://doc.rust-lang.org/nomicon). Section 1.2 lists all the possible causes of Undefined Behavior in Rust: * Dereferencing null, dangling, or unaligned pointers * Reading uninitialized memory * Breaking the pointer aliasing rules * Producing invalid primitive values: ** dangling/null references ** null fn pointers ** a bool that isn't 0 or 1 ** an undefined enum discriminant ** a char outside the ranges [0x0, 0xD7FF] and [0xE000, 0x10FFFF] ** A non-utf8 str * Unwinding into another language * Causing a data race If it is possible, in any way, shape, or form, for safe rust code to make one of those happen, then it is a bug.
[Here's the Mandelbrot demo](http://www.treblig.org/daveG/rust-mand.html)
Check out the [rodio](https://crates.io/crates/rodio) crate.
I use actix-web with async redis pub-sub connections.
I am _very_ excited for MIR optimizations. Out of curiosity I recently profile some of my debug builds and they spend a lot of time doing silly things like dropping `u8`s.
Ha, thanks! I hadn't expected that. 
As the RFC repo says: &gt; You may file issues on this repo for discussion, but these are not actively looked at by the teams. It's intended that people use them to get ideas, and to find like-minded people to coordinate, but the teams don't use the issues for any kind of triage, only actual RFCs. &amp;#x200B;
This is really cool! Iâ€™ve been thinking about developing something similar for helping to build a version of Clojure hosted on and easily interoperable with Rust. Iâ€™ll definitely be playing around with this :)
Very interestingâ€”thanks! I don't suppose your code is public, by any chance? I'd love to take a look at how you merge the two. Assuming it's not, I've got a couple questions (if you don't mind). First, what redis crate are you using? I was planning to use `redis-async`, which seemed to have decent pubsub support, but it hasn't been updated in a few months. I might also go with [fred](https://github.com/azuqua/fred.rs). Second, how are you passing messages from the redis channel to Actix-web? Are you making it an Actor using the primitives is actix itself (rather than Actix web)? Or using something from tokio as an intermediate channel? I appreciate any tips you can offer/the chance to learn from your experience. 
The ORM. Itâ€™s pure beauty. Also the Admin is nice.
I believe, if you are using the exact same Rust version and the exact same compiler, that you can load C-FFI Rust libraries (that return trait objects) via libloading and transfer Rust objects through those functions? (Never tried this, so I wouldn't know)
It's basically [this](https://github.com/actix/examples/tree/master/websocket-chat) example using redis to sync every machine. I use `redis`, it has async support. Although I use async only for publishing. The async publish code is: /u/fafhrd91 made this example for me when I was in your situation. So basically you have a http gateway that will handle the http connection and the upgrade to websocket. There is also an actor for the broadcast, that basically bookkeeps which user belongs to which room to be broadcasted. I create a new thread (but you could spawn as a interval in the gateway context) to keep checking redis for new messages, when it receives it sends this message to the Broadcaster (the actor that bookkeeps), and then the broadcaster sends the message to the group of users desired. When you need to broadcast you send it to redis instead of directly to the broadcaster, that means all the sync is done through redis. For the async public the code is: /// Asynchronously publishes message to redis channel #[inline] pub fn publish(msg: &amp;Broadcast, chan: &amp;'static str) -&gt; Box&lt;Future&lt;Item = (), Error = Error&gt;&gt; { trace!("Async publish to redis ({}): {:?}", chan, msg); let url = env_var("REDIS_HOST").unwrap_or_else(|_| "redis://127.0.0.1".to_owned()); match (Client::open(url.as_str()), to_string(msg)) { (Ok(url), Ok(broadcast)) =&gt; Box::new( url.get_async_connection() .map_err(Error::from) .and_then(move |conn| { cmd("PUBLISH") .arg(chan) .arg(&amp;broadcast) .query_async::&lt;_, ()&gt;(conn) .map_err(Error::from) .and_then(|(_, ())| Ok(())) }), ), (Err(e), _) =&gt; Box::new(err(Error::from(e))), (_, Err(e)) =&gt; Box::new(err(Error::from(e))), } } I either spawn it in a actor or return as a response depending on the context. It has a lot of specifics from our code-base and it's kinda ugly, but async ergonomics aren't exactly great. 
You sound so much like this fellow it makes me feel strange, are you him in disguise? https://www.youtube.com/user/Chyrosran22 Great video in any case, will be following. Thanks. :)
Well, you're still depending on the correctness of that library. But not just you, the *compiler* is depending on the same assumption and may generate code which breaks if it is broken. Say `x` is a struct and there is some field of it which you access during a loop. The compiler might just read it once and hold the value in a CPU register. If `f` mysteriously and unexpectedly changes the structure, that optimization will be wrong, and could be very badly wrong if it causes a buffer overflow or something. So Rust is quite dangerous, much like C or C++ is if you turn all the optimization flags on. On the other hand Rust will prevent you from implementing `f` in a way that's broken. It understands the signature and won't let you modify the borrowed `*x`. At least, as long as you don't use `unsafe`.
Honestly, feature parity with Django. Doesn't have to have even a little bit the same layout or API, but all of these things are super useful to me: - ORM with deep integration throughout the framework - DB Migration system (this is absolutely one of my favorite Django features) - Admin interface - Deeply customizable pre-designed views - batteries-included auth - password reset, password hashing, extensible for 2FA or 3PA - good logging by default - comprehensive best-practice-by-default security practices - class (for rust, interface) based routing - comprehensive testing framework, including testing for routes and setup data insertion Some things I'd want that Django doesn't do, or that I'd want done differently than Django: - Projects look like cargo projects. One of my biggest pet peeves with Django is that Django projects look nothing like standard setup.py packages. - async everywhere by default. This includes both the web and database layers.
So if I am understanding your answer correctly, you are saying rather than creating messages like these for 200 different queries: &gt;**struct** **CreateUser** { &gt; &gt;name: String, &gt; &gt;} &gt; &gt;**impl** Message **for** CreateUser { &gt; &gt;**type** Result = Result&lt;User, Error&gt;; &gt; &gt; } and using &gt;req.state().db.send(CreateUser{name: **name**.to\_owned()}) You are instead saying we are better off passing db: web::State&lt;Pool&lt;SqliteConnectionManager&gt;&gt; inside the function handler as an argument and doing a query like: &gt; let conn = db.get().unwrap(); &gt; &gt;conn.execute( "INSERT INTO users (id, name) VALUES ($1, $2)", &amp;\[&amp;uuid, &amp;path.into\_inner()\], ) &amp;#x200B; Is this for performance reasons for the case if we have too many different queries/models and not enough actors (since for connection pool above a certain amount of actors it doesn't help)? Or is this for code maintainability? Also I'm in the process of migrating a large web app over to actixweb, should I hold off until 1.0? Thank you if you can help me. &amp;#x200B;
I would expect it not to be popular because last time I tried Django (for a recruitment exercise) I felt like working a data entry job filling in forms (files with pseudo-OO classes that have only one useful method and no state, therefore shouldn't be classes) as in the documentation and not really coding. I didn't finish it. What I expect from a web framework: - request context must absolutely be a view argument. Putting it behind a global proxy (like in Flask) makes people wrote unnecessary impure functions and is generally is insane and idiotic. Fortunately ownership in Rust makes it super inconvenient. - no ORM. Instead either SQL templating like in Clojure's hugsql/yesql, or ORM-less query building close to SQL. Most roundtrips to the database can be replaced with at most CTEs. - templating that can statically verify HTML correctness, at least syntactic. Like Hiccup in Clojure's, JSX in JS. - minimally obtrusive syntax. Flask does it with decorators, Rust, if we want something similar, could decorate functions with attributes. IIRC it became possible to process these with macros now. - some non-obtrusive way to implement REST APIs, possibly in line with the above.
You're right, after use `-H 'Connection: Close'`, i got the same result. Rocket ``` Thread Stats Avg Stdev Max +/- Stdev Latency 3.28ms 31.74ms 500.92ms 98.90% Req/Sec 6.85k 3.34k 10.85k 73.91% 16328 requests in 30.05s, 2.30MB read Socket errors: connect 0, read 10, write 0, timeout 0 Requests/sec: 543.32 Transfer/sec: 78.53KB ``` actix_web ``` Thread Stats Avg Stdev Max +/- Stdev Latency 3.71ms 38.46ms 639.08ms 99.00% Req/Sec 7.52k 3.82k 11.16k 80.95% 16331 requests in 30.10s, 2.57MB read Socket errors: connect 0, read 10, write 0, timeout 0 Requests/sec: 542.60 Transfer/sec: 87.43KB ``` 
Thanks!
"huh, must be a playrust mod. *Checks username*. pikachu.jpg"
This is perfect for my attention span. Thank you!
r/playrust
Posting your build output (ideally as a gist) would be helpful to one of us diagnosing your problem. Usually, openssl-sys (a dependency of Rocket, and I assume also of Actix) tends to cause problems if libssl-dev isn't installed. try running sudo apt-get install libssl-dev and try compiling again. &amp;#x200B;
I donâ€™t think there should be any difference in performance. It just removes some boilerplate from sync actors. Regarding 1.0, it is hard to tell, 1.0 is a totally new codebase, it uses different concepts and it will take some time before feature parity with 0.7, especially regarding actors. On other hand if you use extractors (with() call) then it should be relatively easy to migrate to 1.0
actix error: \`\`\`error: Could not compile \`v\_escape\`. warning: build failed, waiting for other jobs to finish... error: Edition 2018 is unstable and only available for nightly builds of rustc. error: Could not compile \`v\_htmlescape\`. warning: build failed, waiting for other jobs to finish... error: build failed \`\`\` 
I already installed that dependency, and its still not working :( i tested it on my windows 10 system, and both rocket and actix are working, so i assume its something to do with linux
Rocket 0.4 should compile on nighties from (IIRC) the last several months. If you have exact error messages that might help narrow down the issue.
You must have Rust 1.31.0 or higher installed. If you're using distribution packages, you'd be better off on Pop!_OS, as we backport Rust to 18.04 and 18.10 in Pop's PPA.
Actually, Rocket intentionally does not have openssl in its dependency chain. It uses *ring* instead with default features enabled, which has been an issue in the past but not this kind of issue.
I have Rust 1.33 installed, and Rust 1.35 nightly. The same error is from both
 Compiling cookie v0.11.0 Compiling hyper v0.10.15 Compiling pear\_codegen v0.1.2 Compiling devise\_core v0.2.0 error\[E0554\]: #!\[feature\] may not be used on the stable release channel \--&gt; /home/\*\*\*\*\*/.cargo/registry/src/github.com-1ecc6299db9ec823/pear\_codegen-0.1.2/src/lib.rs:1:1 | 1 | #!\[feature(crate\_visibility\_modifier)\] | \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ error\[E0554\]: #!\[feature\] may not be used on the stable release channel \--&gt; /home/\*\*\*\*\*/.cargo/registry/src/github.com-1ecc6299db9ec823/pear\_codegen-0.1.2/src/lib.rs:2:1 | 2 | #!\[feature(proc\_macro\_diagnostic, proc\_macro\_span)\] | \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ error: aborting due to 2 previous errors For more information about this error, try \`rustc --explain E0554\`. The following warnings were emitted during compilation: &amp;#x200B; warning: Pear was unable to check rustc compatibility. warning: Build may fail due to incompatible rustc version. &amp;#x200B; error: Could not compile \`pear\_codegen\`. warning: build failed, waiting for other jobs to finish... error\[E0554\]: #!\[feature\] may not be used on the stable release channel \--&gt; /home/\*\*\*\*\*/.cargo/registry/src/github.com-1ecc6299db9ec823/devise\_core-0.2.0/src/lib.rs:1:1 | 1 | #!\[feature(proc\_macro\_diagnostic, proc\_macro\_span)\] | \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ error\[E0554\]: #!\[feature\] may not be used on the stable release channel \--&gt; /home/\*\*\*\*\*/.cargo/registry/src/github.com-1ecc6299db9ec823/devise\_core-0.2.0/src/lib.rs:2:1 | 2 | #!\[feature(crate\_visibility\_modifier)\] | \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ error\[E0554\]: #!\[feature\] may not be used on the stable release channel \--&gt; /home/\*\*\*\*\*/.cargo/registry/src/github.com-1ecc6299db9ec823/devise\_core-0.2.0/src/lib.rs:3:1 | 3 | #!\[feature(concat\_idents)\] | \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ error: aborting due to 3 previous errors For more information about this error, try \`rustc --explain E0554\`. error: Could not compile \`devise\_core\`. warning: build failed, waiting for other jobs to finish... error: build failed 
Something on your system is overriding that with an older version of Rust. You'll want to ensure that rustc reports a version that is at least 1.31.0
Thanks, its been a while since I've used Rocket (my prior experiences wrt to openssl causing issues in a Rocket based project are related to other dependencies) . My bad, I'll correct my comment above.
this is the output when i enter "rustc --verison" \`rustc 1.35.0-nightly (88f755f8a 2019-03-07)\` &amp;#x200B;
&gt; #![feature] may not be used on the stable release channel A nightly compiler should never emit this error. `cargo +nightly build` (if you use rustup) should ensure that the nightly toolchain is actually being used.
&gt;Don't always assume that you're doing a traditional, minimal JS, form-based, server rendered web application. To be fair, Django was built in an era where this was the norm. It's actually adapted pretty well to the modern era, all things considered (channels, etc).
If you see the word raw, it is more than likely unsafe by design. Rust gives you the option in many cases to drop down to unsafe code so you can write systems that may only be done in inherently unsafe ways like dealing with an FFI. If you're staying in rust , you can usually avoid raw methods
Pop!_OS is seeking contributors: https://pop-planet.info/forums/threads/organizing-pop-volunteers.96/#post-436
The issue I have with the command "+nightly" is that it wont run as super user, which is what is needed as to not get the permission denied by os 13 error that the Rust compiler states, which is an issue. when i enter the command "rustup toolchain install nightly" , i get this: "info: using existing install for 'nightly-x86\_64-unknown-linux-gnu' info: default toolchain set to 'nightly-x86\_64-unknown-linux-gnu' &amp;#x200B; nightly-x86\_64-unknown-linux-gnu unchanged - rustc 1.35.0-nightly (88f755f8a 2019-03-07) &amp;#x200B; " 
Potentially controversial opinion coming in here from a guy who's been working with Django pre-1.0, and who's built and launched a project or two in Rust (actix-web). The only thing you need to clone from Django is the admin interface. - Auth practices have changed slightly over the years, to where people tend to use more JWT/OAuth/etc. Say what you will about it... it's (sadly) the case. If you want session-based handling, it's actually really straightforward to do in actix (https://github.com/ryanmcgrath/jelly) - The ORM is... eh. I find value in ORMs, but after doing this stuff for years I really do think Diesel is the sweet spot. - Actix has basic support for Forms as a concept, it's not inherently tricky to make it Django-esque. - If you want Django/Jinja2-style templating, there's enough libraries to clone it (Askama, Tera, etc). In fact, Jelly (what I linked above) pretty much steals the project layout from Django entirely. Even does the password hashing the same as Django. It needs to be updated for newer Rust + actix + etc, so if people are interested I could throw the more modern version up (just needs extracting from my projects). tl;dr Django's surprisingly not difficult to clone in 2018, unless you want an automagic admin interface. Actix-web has a good chunk of what you want already.
Note that futures 0.1 libraries can be easily used with futures 0.3 via the `futures::compat` module.
This sounds a bit like running rustup through sudo once (pretty much never necessary or a good idea), and it messed things up down the line. Unfortunately, it's not something I can help with easily from my phone but hopefully someone else can help sort it out soon. I *think* deleting ~/.cargo and ~/.rustup and reinstalling would be an option, though.
I'm a heavy user of Django for years, and still my favorite web framework. I repeat some of the good points of [Lucretiel](https://www.reddit.com/r/rust/comments/ayj0ju/if_rust_had_djangostyle_web_framework_what_would/ei1c92x/): * batteries-included auth * good logging by default * comprehensive best-practice-by-default security practices * comprehensive testing framework This is stuff that, as user, I don't wanna to do, specially everything around security, that is so easy to make it bad. &amp;#x200B; Exist a lot of good ideas of Django, that become problematic later. Django/Rails was before we learn about some good stuff about web development and it show, BUT, are still 100% right to be included: &amp;#x200B; * **ORM-ish:** Modern ORM alike sql alchemy/dapper (that are more sql layers than old-school orm) are more than enough for most of the cases. If I can dream, I provide instead a "query DSL" that allow to plug a data layer behind, like LINQ. In where Django make the life harder, is to have it too deeply integrated around everything. Here, Rust traits can be a huge win * **Admin interface:** *THE* killer feature of Django. But I wanna it a bit more flexible , like legos. Provide "abstract" interfaces "SearchList", "EditForm", etc, give a HTML with standard and semantic tags and make it easy to customize. Allow to swap or build your own "Admin interface" without massive hacks and you can stop here!. Weirdly, the admin was like a separated project that remain hard to reuse. So, Django make easy do front end but not backends... * **Templates:** The templates in Django are not for just HTML. You can use it for any text format. This is powerful. Here, is where is required to be opinated, IMHO.Templates are the main way you can provide a varied ecosystem of skins. * **Data transforms**: Here Rust have a great concept with the From trait. Make it trivial to do son, xml, rss, etc * **Pipelined:** The more the framework is: Request -&gt; Middleware -&gt; Middleware -&gt; Middleware -&gt; View the better. * Integrated Web server. But not just for debug! * Something like the **Django** **Debug** **Toolbar!** &amp;#x200B; Some mistakes I think is good to avoid: * Not make it SPAs. Do plain-old-html-css. * Make it dependent of a specific JS library. * Make the admin a separated component. * You need a full "project" to start. A minimal sample must work in a single .rs file * Make it dependent of environment variables, configs or similar. This is for large project, not for starting &amp;#x200B; &amp;#x200B;
[`From`](https://doc.rust-lang.org/nightly/core/convert/trait.From.html) is a trait that has an auto impl for `Into`. This is not an implementation of the `From` trait but just a method named `from`.
Oh that makes sense and its super confusing why anyone would want to do this instead of implementing the trait ? 
That is a good question! Probably an oversight, but it does look like there is a potential `panic` if the chars are empty, which might be why it's not `From` as people do treat that trait as infallible.
And it seems to be you
I thought I'd share with you all my latest project. Over the last month I wrote a compiler for a LISP like language into web assembly using Rust. I really learned alot about the **nom** library. It was super fun!
I like the detailed explanation and the dive into the docs. Subscribed! Looking forward to seeing more of these.
Did you set? ROCKET_ENV=prod from here: https://github.com/SergioBenitez/Rocket/issues/315#issuecomment-308192163 I found that difference in perf with the parameter is significant in some cases.
Thatâ€™s nothing, custom allocators (not just for global allocations) have been in limbo since 2014. If it canâ€™t get done in 2019 now thatâ€™s itâ€™s on the roadmap Iâ€™ll be seriously concerned. Itâ€™s way overdue.
It's because futures are still very much in development. Cleaner surface areas will probably come in soon.
Just an FYI the help wanted GitHub links at the bottom are 404ing. 
So you'd want something where a struct variable could use pub(crate)?
Yeah itâ€™s really not hard if you want to give it a go. Itâ€™d be a pain for doing anything really big, but if youâ€™re only coordinating a few futures, itâ€™s a couple of function calls and boxes here and there which is pretty greppable later.
&gt; Fortunately ownership in Rust makes it super inconvenient. Nah, thread-local storage like Flask uses is easy in Rust. Itâ€™s actually asynchrony that prevents you from making that design error: the same thread may be handling multiple requests simultaneously, so you need some sort of handle on the request.
I was hoping the community team's section would have something about reducing the distance between the community and its representation. I'm of the opinion that communication among the different participants in Rust is generally _phenomenal_, but there have been notable gaps and surprises. I'm thinking specifically about the website redesign from last year. I know this is old news, and most would love to leave it behind them, but I don't think it's wise to do so before making some effort to actually understand what led to the friction here and what the takeaways are. Still there has been no post-mortem (to my knowledge) despite [promises](https://internals.rust-lang.org/t/followup-on-website-concerns/9018). But promoting cohesion between the different components of this community, figuring out and sharing what can be learned from these things, _has to_ be a responsibility of any Community Team. I'm just the slightest bit baffled that this isn't mentioned.
This was chosen because the API allows for a set of different implementations that are not representable in a single safe Rust API.
Agreed. But, in that case, Rustâ€™s standard library should provide a default implementation of executor. 
Is there any unsafe way for accessing private fields?
Yes, use of transmute is unsafe way. I have mentioned it my blog as well.
Right so that's not it then. Can you write a small piece of demo code that reproduces this problem on https://play.rust-lang.org/ ?
I don't know much about the subject, do I was wondering what causes the apparent variation in speed?
/r/playrust 
Yeah, I missed to mention that **MyEmployee** struct must have same fields in the same order as **Employee** struct. I'll update my blog.
An update on that postmortem has been posted a few days ago: https://internals.rust-lang.org/t/website-retrospective/9556
The AVX2 instruction I would hazard a guess. 
Is it limited by memory Bandwith? If I remember correctly that is around 40GB/s for Dual Channel DDR4.
As a side note, `collect` already knows the element type (it's the item type of the iterator); it only needs to know the container type. So you can write let v: Vec&lt;_&gt; = ....collect();
Looks pretty cool! I've had the thought of doing something like this. As a lisp programmer, my first impression looking through the readme: The closing parens shouldn't be on separate lines in those examples. Gonna play around with this some tomorrow!
Thanks! I most humbly admit, i'm not a very serious LISP person, thanks for the tip, any other feedback on things that don't look that LISPy (or could be better) would be appreciated.
It would be interesting to see how it compares to https://mollyrocket.com/meowhash since they both use avx2 instructions.
Sorry if I'm a bit of an asshole, but BF1942 had better waves and it was released 17 years ago.
[SMHasher](https://github.com/rurban/smhasher) gave a full quality testing report about the bias, collisions, distrib of those hash functions, I think **t1ha** passed [their testing](https://github.com/rurban/smhasher/blob/master/doc/t1ha). &amp;#x200B;
I haven't put exhaustive thought into it, but the show-stopping features that made me say "OK, It's not worth my time to ponder further until these are fixed" are: 1. An ecosystem that provides me with ready-made building blocks for drudgery, such as [django-filter](https://django-filter.readthedocs.io/en/master/) and [django-tables2](https://django-tables2.readthedocs.io/en/latest/). 2. SQL schema migration support which can autogenerate draft migrations for me to edit, like Django ORM or [Alembic](https://alembic.sqlalchemy.org/en/latest/autogenerate.html) for SQLAlchemy. (in concert with a query builder abstraction that allows me to support both PostgreSQL and SQLite with minimal "If PostgreSQL, do this. If SQLite, do that." However, I can also give some of my show-stopping deal-breakers: 1. No SPA or other solutions that require client-side JS to be enabled. (I'm a strong proponent of gracefully degrading websites and I'm perfectly willing to reinvent the wheel if necessary to get a page where JavaScript is only necessary for things that actually *require* JavaScript, like converting an acceptable but unimpressive `&lt;input&gt;` element into a clickable star-rating widget.) 2. No auth flow which breaks my password manager with irritating things like forcing me to log into my e-mail in order to log into a site. (I use KeePass 2.x, though I'm planning to migrate to a Linux-native option, and I generate a new hard-random password of at least 20 characters for each site.) I choose not to have a mobile phone and, if a site e-mails me a one-time password every time I login after whitelisting it in my cookie auto-delete extension, or if it's Amazon, which I don't *want* remembering me when I'm doing logged-out price-comparisons, I log in as infrequently as feasibly possible.
Unsafe is code that can trigger memory corruption due to wrong manipulation of pointers, bad data type conversions or numeric processing.
Great work. I have always thought that given the Lisp like text format of WASM doing something like this would be great. So far I have only seen interpreters being done in WASM, congratulations in making a compiler instead.
Looks pretty good overall. About what I expected. &gt; Rust has a number of in-progress features -- such as const generics, Generic Associated Types, and specialization -- that have been in development for some time. It's time to finish their designs and ship them. That one bullet point is what I'm most excited about, and honestly the only one I really care about at this point. I'd also like to see "abstract types" (for the purposes of returning impl Trait types from trait methods) on the list of things important enough to mention by name, but perhaps that's being grouped under GAT.
There's several reasons for this: * I don't agree that it must be the community teams responsibilty. We believe that proper interaction with the community on every level is every individuals teams and their members responsibility, owning it in the community team would be a mistake. * The community team does expressively want to move out of the state of being a reactive team and become a productive team again. * The community team is the only team that is a full hobbyist team, we literally don't have the bandwidth to investigate project interaction at that scope. * All teams in Rust are - independent of their roadmap - there for consulting other teams, so we're involved in relevant discussions. * We're the outwards-facing team, so we do stuff like supporting conferences, meetups, etc. * I'm tasked with the website retrospective and currently interviewing people, expect a writeup in two weeks (https://internals.rust-lang.org/t/website-retrospective/9556) * Please note community team was not the team responsible for the website, yet we're the team analyzing it. It's not our roadmap goal, though. * Talking about scope, we're currently setting up other teams around the subjects you mention. Announcements next week. Personally speaking, I think the hyperfocus on the website isn't helping (I don't want to invalidate concerns here), there's tons of broken things and it drowns out other discussions. We're got huge problems handling the volume of information and changes that the project produces even internally and I can totally see how things don't get easier on the community side. We fundamentally need different approaches here. One part of that is spending more time on roadmapping again and getting the community informed earlier, but this is a responsiblity that all teams take on by themselves, instead of shifting it to us.
I added [a simple test](https://github.com/flier/rust-t1ha/blob/master/benches/hash.rs#L228) base on [meowhash-rs](https://github.com/bodil/meowhash-rs) implementation which major use AES instructions. It seems the **meowhash** major focus on large-data digest, and can achieve 30 GiB/s with 16 KB data. &gt;To our surprise, we found a lack of published, well-optimized, large-data hash functions. Most hash work seems to focus on small input sizes (for things like dictionary lookup) or on cryptographic quality. We wanted the fastest possible hash that would be collision-free in practice (like SHA-1 was), and we didnâ€™t need any cryptograhic security. The result better than **t1ha** AES (t1ha0\_ia32aes\_noavx) with 16KB data, but terrible on small input sizes . $ RUST_BACKTRACE=full RUSTFLAGS="-C target-cpu=native" cargo +nightly bench meowhash128 hash128/meowhash128/7 time: [128.14 ns 129.36 ns 130.51 ns] thrpt: [51.149 MiB/s 51.605 MiB/s 52.095 MiB/s] hash128/meowhash128/8 time: [130.71 ns 134.67 ns 139.58 ns] thrpt: [54.661 MiB/s 56.654 MiB/s 58.368 MiB/s] hash128/meowhash128/32 time: [123.47 ns 125.38 ns 127.29 ns] thrpt: [239.74 MiB/s 243.40 MiB/s 247.17 MiB/s] hash128/meowhash128/256 time: [110.76 ns 115.18 ns 122.76 ns] thrpt: [1.9421 GiB/s 2.0699 GiB/s 2.1525 GiB/s] hash128/meowhash128/1024 time: [129.50 ns 131.08 ns 132.68 ns] thrpt: [7.1876 GiB/s 7.2757 GiB/s 7.3645 GiB/s] hash128/meowhash128/4096 time: [203.49 ns 206.03 ns 208.96 ns] thrpt: [18.255 GiB/s 18.515 GiB/s 18.746 GiB/s] hash128/meowhash128/16384 time: [485.08 ns 491.59 ns 498.21 ns] thrpt: [30.627 GiB/s 31.040 GiB/s 31.456 GiB/s] $ RUST_BACKTRACE=full RUSTFLAGS="-C target-cpu=native" cargo +nightly bench t1ha0_ia32aes_noavx hash64/t1ha0_ia32aes_noavx/7 time: [4.7743 ns 4.8195 ns 4.8660 ns] thrpt: [1.3398 GiB/s 1.3527 GiB/s 1.3655 GiB/s] hash64/t1ha0_ia32aes_noavx/32 time: [7.3209 ns 7.3589 ns 7.3976 ns] thrpt: [4.0287 GiB/s 4.0498 GiB/s 4.0708 GiB/s] hash64/t1ha0_ia32aes_noavx/4096 time: [151.42 ns 152.70 ns 154.16 ns] thrpt: [24.745 GiB/s 24.982 GiB/s 25.193 GiB/s] hash64/t1ha0_ia32aes_noavx/16384 time: [647.36 ns 652.14 ns 656.90 ns] thrpt: [23.229 GiB/s 23.398 GiB/s 23.571 GiB/s] &amp;#x200B;
Ok. Thanks. Nice blog btw
In fact, a simple [sum test](https://github.com/flier/rust-t1ha/blob/master/benches/hash.rs#L59) can achieve 50 GiB/s (4KB data) $ RUST_BACKTRACE=full RUSTFLAGS="-C target-cpu=native" cargo +nightly bench memory memory/sum/4096 time: [74.958 ns 75.253 ns 75.563 ns] thrpt: [50.484 GiB/s 50.691 GiB/s 50.891 GiB/s] &amp;#x200B;
Thanks!
Cross compilation from pristine source with system integrated stable rust/cargo and no rustup or other network traffic is still my dream. Golang does it for a few targets, but its ginormous binaries are rarely a good fit my targets.
Hi! Glad to hear that you enjoy CLion! While I was working at JetBrains, I implemented a big chunk of the [IntelliJ Rust](https://github.com/intellij-rust/intellij-rust) plugin which powers Rust support in CLion. Now I am leading this new RLS 2.0 effort. I agree that the the best theoretically possible solution is to have different code-bases for IDE and batch compiler. Specifically, **if infinite resources were available**, I would love to build two compilers for rust: * batch mode, which focuses compiler's own on raw speed, code simplicity, and implementation correctness. Specifically, this compiler, unlike rustc, wouldn't do elaborate error reporting, and would also serve as a specification of the rust language. * interactive mode, which focuses on IDE use-cases, provides nice error messages, fixes, refactorings and completions, but does not actually generate code. Besides factoring complexity nicely, this also gives an ability to compare two implementations against each other and test the specification itself in this way. However, we have only finite amount of resources, and the question arises how to best spent them... I think it is important for Rust that independent alternative implementations exist. The fact that CLion supports Rust via its own engine is awesome: if RLS 2.0 works, we'd get competition, if it doesn't work out, we'd still have a great IDE, and . However I also think that it is crucial that there's an implementation of IDE engine for Rust **in Rust**. For me personally, it is important because Rust is just the best language I know about for doing these kinds of projects. For tooling as a whole, I think there are a lot of places where you would want to use IDE-like approaches even in "batch" scenarios. To give a couple of example, `rustfmt` fails to format code with syntax errors, and clippy has an "warnings are shown only once" problem. These are artifacts of using batch-oriented compiler APIs to implement these tools. And, if we are going to have a batch compiler in Rust anyway, it's very reasonable to try to share code and approaches. This is also not a zero-sum game: making compiler more incremental, extracting libraries, etc, should make rustc better as well. Yet another benefit is the feedback for language designers. Talking to IDE vendors is much less efficient than just trying things out in the compiler and IDE simultaneously.
Looks like a fun project - I'm looking at doing something similar soon. I had just one recommendation. In the compiler you use the following pattern a lot: .map(|x| match x { TopLevelOperation::DefineGlobal(x) =&gt; Some(x), _ =&gt; None, }) .filter(|x| x.is_some()) .map(|x| x.unwrap()) This is common that the standard library supplies [filter_map](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter_map) to do the same thing.
Hi, I'm not a rust expert, and I might be wrong but to me all of your examples act as I would expect. When you call a method taking self, you allocate a new variable on the stack so the address changes, and the same happens when you make "let a = self", you make a new stack variable. I watched a youtube video of a talk about move semantics and what it means in term of stack, I'll update my comment with the link.
Clippy can suggest those things.
For #2 does it mean that you may consider [rust-analyzer](https://github.com/rust-analyzer/rust-analyzer) for instance?
Try release mode ;) Basically, as you're moving `self ` around you're not talking about the same variable/address every time, and there is no guarantee that your value stays in the same place. Not moving the value is an optimization you would expect though - that is, when you're optimizing at all. 
&gt; Any recommendations? :) Only that when implementing stuff like this, think about edge cases: Overflow for `add`, empty intersection for `intersect`. Maybe more :)
Cool project! Do you plan to write a standard pattern to create a web app, like the model/view/update of Elm?
And that's _exactly_ the reason I'm looking for a preexisting crate :D Interestingly, empty ranges work out of the box with this implementation, according to [the docs](https://doc.rust-lang.org/std/ops/struct.Range.html): &gt;It is empty unless `start &lt; end`.
That's a pretty good reason :) Note though, that the empty range has infinitely many representations, so depending on what you want to do with the result, there might be a surprise lingering there.
Ok! I did it and it works ;) [https://bin.disroot.org/?47f39c29e79e6b5f#PKelYoxkkfFz9bqAcflwD9udOvWUlPPTadLc263Re/c=](https://bin.disroot.org/?47f39c29e79e6b5f#PKelYoxkkfFz9bqAcflwD9udOvWUlPPTadLc263Re/c=)
This is awesome! Would you have an issue/PR/version tag I can track to see the improvements?
Same name. Vastly different levels of toxicity.
Same name as what what? Sorry. 
\-&gt; [https://www.reddit.com/r/playrust/](https://www.reddit.com/r/playrust/) 
Did you look at this sub before posting? If not, do it now and let me know if you notice anything.
This is the sub-reddit about the Rust programming language. You are probably looking for [/r/playrust](https://www.reddit.com/r/playrust/).
If your talking about a similar video that has been posted. I allowed that person to use my clips. Itâ€™s my boyfriend. Itâ€™s just my clips and my voice 
This subreddit is for programming language named Rust. You are posting at the wrong place.
As with any other structure in Rust if you take `self` by value it's getting moved/copied to a new location, and again on return. And of course the compiler can reuse existing locations if they know the previous one is "dead" to reduce the size of stack frames. So you create `a` at location 1, copy/move it to location 2 on `a.b()`, copy it back to location 3 on return, etcâ€¦ Now a trap here is that the compiler *knows* the previous value of `a` can't be accessed anymore after each function call and corresponding re-assignment, so it can copy the new value back into its old slot, which is why the memory location "doesn't change". Assigning each revision to a different variable is sufficient to fool the debug mode: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1dcfdf564bdc9e01f4fb43001f5c1ad9 But release will see through the trick, and will in fact replace all the copies by references to the same one location: https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=f67de4b886f7b2584f41ce5b4b6cb7c9 tl;dr: you're misunderstanding what you're doing, and you might see similar behaviour in C or C++ depending on the optimisations the compiler decides to apply
If you use TLS in the caller and still keep the reference, can callees use the same var too? I don't think it's possible with a mutable reference, not sure about immutable.
Just check fortunes benchmark on tech empower or any non pipelined bench https://www.techempower.com/benchmarks/#section=test&amp;runid=fc4c28f5-5647-4507-89ea-35428ee073b9 
Do not forget about CPU's cache. I've got over 80 GiB/s on 2700X: [https://hastebin.com/izasepebin](https://hastebin.com/izasepebin) Realized later I could have closed Spotify and Firefox but whatever.
The daily r/lostRedditors on this sub
When I read "Extracting parts of rustc into libraries, which are easier to understand and maintain but also help in developing a Rust specification" should I understand that there's a chance for the rust team to look at my [`annotate-snippets-rs`](https://crates.io/crates/annotate-snippets) as a base for the error formatting code? :)
So I think I might have got this wrong about what it's actually doing. It was my understanding that this would be compiled with wasm-unknown-unknown and talk to the WebGL apis for rendering. But what I think is happening now is that it's on a lower level than that and will interact with the browsers on a lower level for better access to the GPU on the executing device? Is that correct?
I agree the number itself is meaningless, but compare different hash functions in same scene is useful. Let's remove the number from title :) &amp;#x200B;
I don't miss this at all for Rust. Django is a bloated, opinionated monolith loaded with over-engineered solutions. It also has a cultural problem where novice programmers without credentials tout Django as the best and only solution -- zealotry. 
What other web frameworks did you try in Python?
I think admin must be a separate project, I donâ€™t use django but I use rails (not a power user nor so much experience, but did a couple of sites with backend included) and I find just easy to use the build in mvc and do stuff by hand, thereâ€™s a gem called active admin iirc but never used, I find using this kind of stuff annoying when you wanna customize something but you cannot without doing a lot of weird stuffs, so IMHO admin can and need to be a separate project. About templating I love rails because you can use whatever you want (erb, slim, mustache, etc) but still donâ€™t understand why you would use a template that is pure html with some new syntax you need to learn, thatâ€™s why I love slim, because it give me a reason to use it, easy to read, minimalist syntax and understandable (mix of css flavors), but saying this the framework just need to have a flexible syntax and thatâ€™s all, so the community can built their own template engine.
Your question should probably be "why `self` is not a real self _pointer_". And, well, `self` is not a self pointer, since it's not a pointer at all. The value of the struct gets moved to the `self` variable. When you ask for `&amp;self as *const A as u64`, you ask for the address at which the struct currently resides. When you assign it to `a`, and ask where `a` lives (`&amp;a as *const A as u64`), you get exactly that answer: where `a` lives. This can be a different location from where `self` lives, or not, depending on which stack slots the compiler mapped the variables and return-values to. A related question would be whether `&amp;self` is a real self pointer. And it is, as your code shows: pointers #1 and #4 refer to the same address, since the case of #4 gives the address of the self _pointer_ applied to the object captured by the self _value_ in method `b`.
'tls-sync' should be 'tls-async'. Also 'packet-ipc' isn't pure ipc, it uses servo-ipc and futures 0.3 to do inter process packet sharing (like dpdk, but user friendlier). I have a few other unpublished crates too, and have so far found the futures 0.3 and async/ await experience to be mostly pleasant. Glad to provide feedback or help here or on TRPL discord.
Even then, paramatricity means that `f` can't use the inner mutability - if it's implemented purely with safe code. Whether or not this creates an opportunity for undefined behavior of unsafe code is currently unspecified. It's a *really* subtle point of the memory model and Rust doesn't yet have its own one which properly and fully specifies what is special about `UnsafeCell`.
I think you are describing passing a reference to the request context around? If so, that counts as passing the context around, and why are you wasting your time with TLS?
Iâ€™m wish list is.. but before that I donâ€™t use rust for backend development so I donâ€™t know the state of the ecosystem, nor Iâ€™m a django user, I have deep knowledge of php, nodejs and moderate ruby web development.. ok, here is: - Have a powerful interface of some kind that let you add new plugins easily, donâ€™t know if is possible the same magic that rails have, but at least give a template where plugin devs has the tools to start to build a plugin with the options the init the plugin (create config files if necessary). - Nice interface for template engines, while providing support to a normal one (html with sugar, or if possible something like slim or pug would something I would love a lot). - Donâ€™t include an admin system, like other ppl suggest, IMHO just provide the minimal tools to build a custom admin quickly, forms, helpers, etc, why Iâ€™m against an admin system? because I find it hard to build one that could be adapted to every situation and it will add a lot of work on the maintainers, that could use that time to improve the core of the framework, and admin system can be easily be a 3rd party plugin, just provide an interface so this kind of stuff can be possible. - Asset pipeline, it seems a trending here to migrate this stuff to nodejs ecosystem, they have mature tools and an active community, but recently I experienced with the latest rails a lot of pain too, the integration with webpack feels clunky and buggy, while the old asset pipeline in rails works very good, but itâ€™s lot of work maintain all this dependencies for the frontend and itâ€™s easier if you use nodejs ecosystem for this, at least at the beginning, I would love to see this in pure rust, for safety and performance. - Auth will be interesting to have it built in, but again, different projects have different requirements, so itâ€™s easier to have it as a plugin. There are more things but my point is, provide a good interface/tools to allow plugins to get deeply integrated with the framework and the community will do the rest, this is the recipe of success of rails, because is the â€žofficialâ€œ web framework of rails, almost every gem has an integration with rails, this is why php failed, theres a lot of competition and thatâ€™s hurts the community because everyone wants to build their own framework, and the most popular (laravel) was inspired by rails but itâ€™s still years behind it and also you donâ€™t have a rich ecosystem built around it. So we need to build/improve one framework inspired by djando/rails and maybe other much more simple like sinatra/express and try to run great marketing campaign just to gain users and make it the â€ždefaultâ€œ web framework.
A method that takes a `self` or `mut self` parameter semantically copies or moves the value when it is called. So unless the compiler optimizes the copy/move away, the address changes. It sounds like you want a method that takes `&amp;self` (shared reference) or `&amp;mut self` (unique reference). References are implemented as pointers. Taking a reference does not copy/move the original value, so its address doesnâ€™t change.
Thanks for the clarity, much appreciated. 
Hey, this is amazing! Have you seen the embedded things we have going on in the [embedded-wg](https://github.com/rust-embedded/wg)? It would be awesome if you could add this to our [newsletter](https://github.com/rust-embedded/blog) and the [awesome embedded rust](https://github.com/rust-embedded/awesome-embedded-rust) list! I think a lot of people in the embedded rust community would be interested :)
I think that "dereference a pointer" can summarize pretty much all you can do with `unsafe`. You can do a various range of things with this: "import" a data from a FFI call, erase the lifetime of a reference, etc.
Thank you for your response! &gt; Arguably what is missing is some means of sending control messages from the user/controller. In Unix we have signals, which are pretty hard to get right. I think you still want such a control mechanism when running in process - and it could be represented by another channel. My experience is that this is almost humanly impossible to get right in the presence of cancellable futures, and it composes poorly in more complex designs. I'll explain more [here](https://github.com/crossbeam-rs/crossbeam/issues/314) either today or this weekend, with examples from a real production Rust application that has run into many of these problems.
I took a look at the script. Looks interesting. In which folder do the generated docsets get saved? I see it says the generated docsets are copied to the \`docsets\` subdir. But what's the path?
Nice library, it is indeed very fast (if not undoubtedly the fastest)! While meowhash and T1HA lay no claims using said hash function for cryptography, I can't help but wonder what the inflection point is of speed vs security. For instance, highway hash is also an AVX2 enabled hash function (64 / 128 / 256 bits), but it comes with security claims (bits are uniformly distributed and should withstand differential and rotational attacks). One does sacrifices a bit of speed though. It seems one can continue down this path of trading speed for a stronger hash. Maybe one day we'll get our cake and eat it too! Anyways, just wanted to share my thoughts. Thank you very much for contributing your library.
"Safetiness" reminds me of "truthiness", i.e. not really safe (or not really the truth).
Do I need to install anything on my machine to *run* binaries built with Cargo?
A few minor comments from a quick skim: * You're using the 2018 edition, but still using the `extern crate` syntax. With the new edition, you don't need that (as described in [the edition guide](https://doc.rust-lang.org/edition-guide/rust-2018/module-system/path-clarity.html) * You print error messages to stdout with `println!`. It's probably more idiomatic to print to stdout with `eprintln!`. * I *think* it's more idiomatic to extract the CLI/clap logic to a separate file, but with the small number of options you have right now, it's probably fine in `main`. Looks like a fun/cool project!
This looks superficial to me. If you have "inner" task bound to some "outer" take it would be limited to resources of that task, which means, it can only be executed on same "core" as the "outer" task. But if you would "flatten" the model, you might be able model more like dependency, where you can create task dependent on its "subtasks", while limiting accessible resources, without bounding them to same "core", which have potential for better "cores" utilisation. Hope this helped a bit. Cheers! 
No, if you need to install something is because of some run-time dependency just like any binary. But you don't need to install anything rust-specific.
Seems to me that this would be perfectly possible with rayon scopes: [https://docs.rs/rayon/1.0.3/rayon/fn.scope.html#task-execution](https://docs.rs/rayon/1.0.3/rayon/fn.scope.html#task-execution)
It is normal to potentially write safe code that could theoretically enable bad preconditions in unsafe code. I agree that such cases should be documented with comments warning the future programmer. The only part that should be marked unsafe is the code that must reject the bad preconditions, else it invoke some behavior that is considered "unsafe". Sometimes it isn't fully possible to do so, like when pointers are changing in the structure in safe code elsewhere, so in those cases just warn the next developer with comments explaining that they cannot break the preconditions required by the unsafe code at the safe code that could do so. You cant stop people from potentially breaking invariants using safe code wherever they want. If you wanted, you could additionally use wrapper types to make it unsafe to break the invariants, but that wouldn't be consistent with the rest of Rust that allows doing lots of things like pointer manip without any unsafe until the final deref.
&gt; To our surprise, we found a lack of published, well-optimized, large-data hash functions. Most hash work seems to focus on small input sizes (for things like dictionary lookup) or on cryptographic quality. We wanted the fastest possible hash that would be collision-free in practice (like SHA-1 was), and we didnâ€™t need any cryptograhic security. &gt; &gt; Wasn't there a gold rush some years back with fast non-crypto hash algorithms? IIRC Murmur hash kicked it off and was tweaked and was reinvented several times over since.
How does this compare to [seahash](https://crates.io/crates/seahash)?
&gt;This looks superficial to me. If you have "inner" task bound to some "outer" take it would be limited to resources of that task, which means, it can only be executed on same "core" as the "outer" task. but some physical arrangements have nesting - eg cluster of machines, each with multiple cores; dual socket motherboards; multi-chip packages. personally I think writing code with nested parallelism looking like nested loops could be an intuitive way of expressing dependancies 
&gt;**Templates:** The templates in Django are not for just HTML. You can use it for any text format. This is powerful. Here, is where is required to be opinated, IMHO.Templates are the main way you can provide a varied ecosystem of skins. There's a new template crate for Rust, [Display As](https://crates.io/crates/display-as), which also is for more than just HTML. You may find it interesting. (I'm not affiliated with the Display As crate; I just like its approach.)
thanks.. hadn't heard of that. It does seem to be the same concept I'm getting at here, but more generalised nesting..
&gt; I can't help but wonder what the inflection point is of speed vs security That is a very tough question. In cryptography, hash functions are usually (but not always) modelled as random oracles, i.e. a black box which returns a uniformly random bitstring for any input and is consistent (i.e. same input =&gt; same output). Many schemes are not provably secure without this idealization and there are proofs that no hash function can actually fulfill these constraints. We sort of just hope that the cryptographic hash functions are 'close enough'. 
Thanks for the links! I was kind of aware of the WG, but bit unsure what would be the correct venue to get feedback.
Last time I tried to use the not-yet-stabilized async/await features, the thing that ended up deterring me from it was lack of proper IDE support. IIRC I tried CLion (JetBrainz) with RLS plugin, and also VSCode. Both of them just couldn't make any sense of anything inside the async blocks.. and being relatively new to Rust, I still rely on my IDE's type inference and hints for a lot of my coding. Has that improved in the past couple of months? If not - I really think this is one major barrier to widespread adoption.
AFAIK in order to support variadic functions and methods in safe rust we will need to support variadic generics (similar to C++ parameter packs). C-variadics are SUPER unsafe (see [cwe-234](https://cwe.mitre.org/data/definitions/234.html) and [cwe-134](https://cwe.mitre.org/data/definitions/134.html)) and as mentioned elsewhere variadic macros don't work for methods.
Are you building with `--release`?
The inline JS for wasm-bindgen is really exciting!
No, it's because you posted a gaming video to a programming language subreddit. You wanted to post to /r/playrust instead. I have removed your post to spare you further embarrassment.
the avx variant of t1ha0 in comparison to the non avx-one uses avx2 to vectorize certain operations so it can use a single instruction on multiple data (SIMD-Instructions ;-) ) at the same time. ([https://en.wikipedia.org/wiki/Advanced\_Vector\_Extensions](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions)) The rust default hasher is a cryptographic hash function and therefore has more security (see [https://doc.rust-lang.org/book/ch08-03-hash-maps.html#hashing-functions](https://doc.rust-lang.org/book/ch08-03-hash-maps.html#hashing-functions) for more infos about that) but less speed. Also the t1ha0 seems to be only optimized for x86\_64. So I am not sure if that speed difference stays the same on other platforms. But great work nethertheless
Just a Django like ORM. Diesel doesn't feel good to me, I prefer writing SQL myself for now.
Take from django: * Admin. * Authentication. * Flexible upload file handling. * Asynchronous task handling (celery). But possibly make this lighter-weight, so a small app can get started by sending to threads, and then add message brokers later just by updating a configuration. Remove from django: * Templating. I don't see a whole lot of value in integrating this with the framework. * Management commands. These always seemed clunky, and poorly integrated. Improve on django: * Better native support for API views. Trying to implement, e.g. proper JSON-API, even with django-rest-framework is a nightmare. * Better support for different kinds of data sources: Make it easier to replace the database with Mongo, or with external services accessed over HTTP. * First-class support for websockets. * Unify input validation (request parameters are handled manually, forms can be used out of the box, and JSON/REST requires a 3rd party library? No thanks.) * More integrated support for app-specific settings. (Apps should be able to declare settings and provide default values out of the box.)
I'll have to try using it!
Thank you!
No plans at the moment, my biggest goal right now is to get the language self hosting.
Honestly, what I really want these days is cleaner separation between my business logic and my inputs and outputs, but that may be counter to the goals of an opinionated web framework.
Oh yeah, C variadics are definitely unsafe, but Go variadics _are_ safe. Go gets around it because it can do runtime type detection (assert `interface{}` -&gt; some concrete type). Honestly, I'd be happy with restricting it to a single type (e.g. `fn accept_lots(args...: impl Trait)`) and just use macros for the rest. Yes, you could implement that with `[T]` or `Vec&lt;T&gt;`, but it looks much more clean and is honestly what many use-cases want anyway. It could even be restricted by forcing all or none of them to be borrows, mutable borrows, or owned values. I mostly just want to be able to define a trait that can take 0 or more of a certain type of argument. The current workaround there is to implement a trait for a large set of tuples, which is _really_ annoying.
hi, I've done a lot of nested parallelism using rayon. even up to 3 nested levels. you can find an example of a complex algorithm using a scope here: [http://www-id.imag.fr/Laboratoire/Membres/Wagner\_Frederic/folding-with-help.html](http://www-id.imag.fr/Laboratoire/Membres/Wagner_Frederic/folding-with-help.html) (see prefix section)
I totally thought this was a joke, turns out wow! rust-clippy is sweet!
Next step: Emacs in the browser. Just `import emacs` and set `&lt;input type="textbox" emacs&gt;` and it will convert normal text areas to an Emacs instance.
So more generally, if you have a program where you have strange, well defined memory needs and strict performance constraints, you might want to write your own allocator. So if you have a process that knew it was going to ultimately need 32GB of memory and you want to get that from the OS up front, then manage it yourself to fine tune performance, you can do that by writing your own allocator in Rust. &amp;#x200B; [https://doc.rust-lang.org/1.15.1/book/custom-allocators.html](https://doc.rust-lang.org/1.15.1/book/custom-allocators.html) &amp;#x200B; I don't know if elastiscearch would actually benefit substantively from doing something like that, or not. &amp;#x200B; &amp;#x200B; &amp;#x200B;
What about diesel falls short of it? If anything my biggest criticism of diesel is that it's too close to old-fashioned ORMs and ties my hands when I want to include things that are clearly better written in SQL (eg complex joins and aggregate functions). 
Hah, that would be cool. Out of curiosity, how much of emacs is written in lisp?
Looks cool. One suggestion: you're reading both files into strings and then splitting into lines and walking through them. If you use a BufReader and the lines() iterator instead then you won't have to read both files completely into memory first. Unless somehow you need all of the lines loaded at once I think moving through the lines with an iterator is more natural.
* Good catch with the extern crate stuff * Ooh I didn't even know about `eprintln!`, that's awesome. * Yeah my main was hacked together just enough to be able to test my app. I agree, I will definitely want to refactor that in the future as things get more complicated. Thank you for your feedback! 
No, there is no such feature as a dynamic contexts. Storing an additional string context for every instance of your error is sub-optimal: you don't want to create many identical strings containing a human-readable error message in memory. &amp;#x200B; &gt;Suppose I have an IO{source: io::Error} in my enum, but I want to specify whether it is a Not found. And in this case I want to write the filename to the Display message. &amp;#x200B; This is how I would do it in a clean manner: &amp;#x200B; \`\`\`rust custom\_error!{ OpenFileError NotFound{filename: String} = "Tried to open '{filename}', but it doesn't exist", Other = "An unknown I/O error occured.", } /// Opens a file with a verbose error on failure fn open\_file\_verbose(filename: &amp;str) -&gt; Result&lt;File, OpenFileError&gt; { File::open(filename).map\_err(|e| match e.kind() { ErrorKind::NotFound =&gt; OpenFileError::NotFound{filename: filename.to\_string()}, \_ =&gt; OpenFileError::Other }) } \`\`\` &amp;#x200B;
Great idea, thanks for the suggestion!
I don't think Rayon has anything built in to handle [NUMA specifically](https://github.com/rayon-rs/rayon/issues/319), so I wouldn't expect it would be suitable for other kinds of non-shared memory parallelism right now.
Is it better to declare const &amp;'static str or const Strings when later doing lookups? &amp;#x200B; example 1: struct TheStringMaster { pub const SOME\_STRING : &amp;'static str = "some string"; pub const OTHER\_STRING : &amp;'static str = "other string"; } lazy\_static! { pub static ref STUFF: Vec&lt;String&gt; = { let v = vec!\[CacheWeather::SOME\_STRING,CacheWeather::OTHER\_STRING\]; v }; } // later do a check STUFF.contains() on a String variable, not sure I can coerce the types properly yet &amp;#x200B; or struct TheStringMaster { pub const SOME\_STRING : &amp;'static str = "some string"; pub const OTHER\_STRING : &amp;'static str = "other string"; } lazy\_static! { pub static ref STUFF: Vec&lt;&amp;'static str&gt; = { let v = vec!\[CacheWeather::SOME\_STRING,CacheWeather::OTHER\_STRING\]; v }; } // STUFF.contains works fine on String variables or &amp;#x200B; struct TheStringMaster { pub const SOME\_STRING : &amp;'static str = "some string"; pub const OTHER\_STRING : &amp;'static str = "other string"; pub STUFF : \[&amp;'static str,2\] = \[CacheWeather::SOME\_STRING,CacheWeather::OTHER\_STRING\]; } // use slice contains &amp;#x200B; This could be a great fit for enums in rust which I have no experience with yet (but do in other languages). Eventually there will be string data to compare to though.
Yes, that's what it's about.
Maybe, you'd have to ask them!
Yes, only for production with docker.
Ho wow ok that's cool, thank you for the reply!
Rayon is already based on work-stealing fork-join parallelism, so it recursively splits the data as much as it needs to satisfy all processors. So nested parallelism is really a core part of rayon.
Try to measure time without print statements. Rust's println! is slow. &amp;#x200B;
Not sure. I do know `remacs` is Emacs written in Rust though, but just "mostly" Rust.
I'd do #3, there's no reason for the overhead of lazy\_static and allocations.
Groovy, and I later lookup Strings for membership ( I spotted some issues converting Strings to &amp;'static str in the lookup ie leaking)
&gt; I don't quite understand the semantics of the RefCell RefCell&lt;T&gt; has pretty straightforward semantics: it enforces the borrowing rules, but at runtime, not at compile time. You call .borrow() to get a &amp; and .borrow\_mut to get a &amp;mut, and if you call them both at the same time, it panics. It's sound; the compiler won't let you do otherwise! The big difference between RefCell and Mutex is that RefCell is not threadsafe. As long as you're not using threads, there's no reason to pay for the overhead of Mutex.
WOW! Just WOW! This is going to be amazing! Can we lend you a hand? 
Is this something clippy could give a hint on? Eg. recommend your variant if the value is only referenced once in the closure body.
Look up in what? You should be able to use a &amp;str for that purpose.
like s: String -&gt; STUFF.contains(s or &amp;s), the compiler is kvetching about &amp;&amp;str. I'l 
Made more difference than I had guessed. Rust 3.6s, Python 3.7s without printing the result.
I think you're gonna wind up with something like this: const S1: &amp;str = "foo"; const S2: &amp;str = "bar"; const MY_SET: [&amp;str; 2] = [S1, S2]; fn main() { let dynamic_str = "bar".to_string(); dbg!(MY_SET.contains(&amp;dynamic_str.as_str())); } The extra `&amp;` at the front of `&amp;dynamic_str.as_str()` is there because `contains` expects a reference to the content type, which in this case means `&amp;&amp;str`. Also note that in the wild you'll see things like `&amp;&amp;*` used as a shorthand for that conversion (because `String` implements `Deref`, so `&amp;*` is effectively `.as_str()`). It's kind of noisy and gross, but on the other hand these conversions are usually sort of a "if the compiler accepts it, it's the right thing" situation, and when I'm reading code I tend to just ignore the extra symbols and assume it's doing the obvious thing :p
Flask, Bootle, Pyramid, and other less well know ones. &amp;#x200B; I like Flask/Django. Bootle for very minimal things.
That'll do it, thanks oconnor663!
You want to use cargo features and #[cfg(feature="foo")]
&gt;I think admin must be a separate project This is a mistake. &amp;#x200B; As a "normal" web framework, sure. But as "Django alike" the admin is killer feature. Instead, put the functionality of the admin as lego blocks, so you can mix them with your regular views.
Looks like you got the answer in the other sub-thread. (I personally write \`&amp;\*\`)
ok so &amp;\* vs &amp;string\_variable.as\_str() ? 
Yep. Both are fine, it's just a personal preference.
huh weird I had to write &amp;&amp;\* vs &amp;\* to quiet the compiler, &amp;string\_variable.as\_str() worked too
cool idea, and cool name!
&gt;Rayon is already based on work-stealing fork-join parallelism, so it recursively splits the data as much as it needs to satisfy all processors. So nested parallelism is already a core part of rayon. ok I guess so. When I'd implemented parallelism helpers in C++ i the past I essentially had 'a thread per core', with my data-parallel helper feeding foreground tasks into that, with some other 'background task' that they'd go back to churning away at. I was nervous about relying too much on loading up OS threads (switching overhead? stacks?). so I suppose each time you invoke something in rayon it can fire up its own worker threads.. which the OS can manage against all the others.. &amp;#x200B; I just figured you might be able to control it a bit more e.g. saying \*how far\* to fan out : if you know an 'outer task' is going to spawn a bunch of 'inner tasks', you might want to balance that overall number again the number of cores, and not even try to saturate all the cores with the 'outer level'
Rayon only has one global thread pool behind (rayon-core) it so I donâ€™t think it will cause the issues your concerned about. That said you still need balance that pools threads against, tokio, std::thread and other sources. 
Management commands helped with Django/Python environments needing to hoist a bunch of stuff for the ORM. I find them to be a thing that Django requires but I rarely use outside of Django.
Probably need to set your linker and ar in your cargo config. Here's the instructions for cross compiling to windows, should be similar. https://www.reddit.com/r/rust/comments/6rerw5/tutorial_cross_compiling_a_gtk_program_from_linux/
I cross-compile my GTK-rs application for the Pi as well. I happen to do it via Gitlab CI. The process looks like this: Add the "armhf" architecture to the system: dpkg --add-architecture armhf You'll want to run apt-get update &amp;&amp; apt-get upgrade after running that, so you can install armhf-based dependencies. Then, ensure the following environment variables are configured: CARGO_TARGET_ARMV7_UNKNOWN_LINUX_GNUEABIHF_LINKER: arm-linux-gnueabihf-gcc PKG_CONFIG_ALLOW_CROSS: 1 PKG_CONFIG_PATH: /usr/lib/arm-linux-gnueabihf/pkgconfig/ Finally, before actually calling cargo, I'll make sure the following dependencies are installed: gcc-arm-linux-gnueabihf libgtk-3-dev:armhf You may also need to reinstall pkg-config (it gets uninstalled for me when I run apt-get update &amp;&amp; apt-get upgrade after adding the armhf arch). Being that your cargo is trying to use the x86_64-linux-gnu linker, I feel like you're missing one of the bits above. I do this all from Gitlab's CI to avoid having to add the armhf architecture and all of its dependencies to my main Linux OS. 
&gt; Btw, there's no need for smug dismissals like "Hint: X". I apologize if it came across as smug; it was not my intent. I suppose I learned yet another corner case of spoken English today :/ &gt; What made you think that I thought that? You mentioned: &gt; Wasn't the Rust syntax designed to be 'unambigious' (no backtracking?) *in part* because it would speed up compile times? This looked to me like you were questioning whether the syntax actually achieved its goal (speed up compile times), and therefore I wanted to know why you thought it didn't. Apparently that's not what you meant, and now I am very confused as to what your question was supposed to mean. &gt; why be really particular about LL(1), for example? I THINK there are multiple factors which tie in together: 1. Elegance of the grammar. 2. Strength of the foundations (no ambiguity). And those two are key to supporting (a) further extensions to the language and (b) third-party tools, which may be using more rudimentary tooling.
&gt; You print error messages to stdout with println!. It's probably more idiomatic to print to stdout with eprintln!. `eprintln!` prints to `stderr`, not `stdout`.
&gt;When I'd implemented parallelism helpers in C++ i the past I essentially had 'a thread per core', with my data-parallel helper feeding foreground tasks into that, with some other 'background task' that they'd go back to churning away at. I was nervous about relying too much on loading up OS threads (switching overhead? stacks?). so I suppose each time you invoke something in rayon it can fire up its own worker threads.. which the OS can manage against all the others.. Not creating too many threads like that is a good idea when doing that kind of thread-based parallelism. One benefit of that kind of parallelism is that it can much more easily handle streams of data, which is usually not possible in Rayon. &gt;I just figured you might be able to control it a bit more e.g. saying *how far* to fan out : if you know an 'outer task' is going to spawn a bunch of 'inner tasks', you might want to balance that overall number again the number of cores, and not even try to saturate all the cores with the 'outer level' As already noted by /u/AnAge_OldProb, there is a global thread pool, so you will never use more threads than the number of cores. In addition, `rayon::join` (which is basically what parallel iterators boil down to) is really cheap and does not even allocate dynamic memory. Due to the work-stealing it only involves another thread if they have no thread-local tasks to work on. In some cases it can make sense to make the lowest level serial (eg. quicksort). In the few cases I tried making the lowest level serial, it had little to no effect.
ok so i guess when going nested.. . the 'inner and outer tasks' might pretty much fill the role of the 'foreground and background tasks' in my earlier work.. (is that the way it would work? inner nesting levels increasing in priority?)
The current thread always takes last task (i.e. inner). When a thread steals work from another thread it always takes the first task (i.e. outer). Outer tasks tend to be bigger, so this ensures as little locking and cross-thread sharing as possible.
I love the Rust community.
So, if calling `make_current` cannot _itself_ trigger UB, then it's allowed to be declared as a safe function. That's the whole ball of wax right there. Situation: If you record the len of a vec at one point and do some unsafe thing, then call the (safe) method `clear` to empty the vec. Next you go back to your unsafe thing and just pick up where you left off with the unsafety as if the safe code in between couldn't have messed things up. You're definitely gonna have a bad time. I think in this vector example it's easier to see that safe code disrupting unsafe code is unfortunate but unsafe code just needs to be paranoid against it. I don't think you'd argue that the `clear` method needs to become unsafe just because it could disrupt unsafe code elsewhere. 
You might want [byteorder](https://docs.rs/byteorder/1.3.1/byteorder/).
Thanks! All PR's are welcome, there is plenty to cover still, even with Cortex MCU's.
[Itertools](https://docs.rs/crate/itertools/0.8.0) crate works with iterators and not ranges but perhaps that will suffice?
This is gonna cause UB as soon as you realloc or drop the vector. A given allocation has a Layout value implicitly associated with it that you must also pass back to the allocator when you realloc or dealloc that allocation. Failure to pass the exact same size and alignment that the allocation was first made with is UB.
I don't see how this applies. I want both versions of the project to be built, not optionally one or the other. I could see this being useful if I have a subcargo, but I want to avoid having a cargo for the sake of another cargo.
[removed]
For the big-endian version you can do a pass over the `Vec` doing an in-place byte swap, and then do the exact same thing as the little-endian version. I don't think that your cast is valid, though. Allocating and deallocating memory involves passing a [`std::alloc::Layout` value](https://doc.rust-lang.org/std/alloc/struct.Layout.html), which includes both the size and alignment of the type. `u8` has a different size and alignment from `u32`, so if the allocator's internals rely on that `Layout`'s information to be correct, then it will break and probably corrupt memory. Currently: * The [Unix](https://github.com/rust-lang/rust/blob/190feb65290d39d7ab6d44e994bd99188d339f16/src/libstd/sys/unix/alloc.rs#L36-L38) implementation of `dealloc` doesn't use the layout information. * The [Windows](https://github.com/rust-lang/rust/blob/190feb65290d39d7ab6d44e994bd99188d339f16/src/libstd/sys/windows/alloc.rs#L46-L57) implementation does, with different behavior depending on whether or not the type's alignment is greater than [8 or 16 bytes, depending on architecture](https://github.com/rust-lang/rust/blob/190feb65290d39d7ab6d44e994bd99188d339f16/src/libstd/sys_common/alloc.rs#L9-L22). That's ok here, but it shows that this is generally not safe. * The [Webassembly](https://github.com/rust-lang/rust/blob/190feb65290d39d7ab6d44e994bd99188d339f16/src/libstd/sys/wasm/alloc.rs#L38-L41) implementation [does not use the layout information](https://github.com/alexcrichton/dlmalloc-rs/blob/165b5afef4017a0a117e652385bf32425a349172/src/lib.rs#L96-L100).
Not really sure what's going on in your code snipped, but if the title of this thread is all you want this should be enough. \`\`\`Rust fn to\_le(vv: Vec&lt;u32&gt;) -&gt; Vec&lt;u8&gt; { v.iter().map(|&amp;s| s as u8).collect() } \`\`\`
That would be amazing, but I do not think so. Seems like it would fall within other undecidable problems like the halting problem. Perhaps with a very restricted subset of Rust you could, but such a subset might be so restrictive as to not useful for anything but the most critical systems.
Got another one that I couldn't find a good example of which surprised me HashMap find, if found use it else do nothing without doing a double lookup. I ended up struggling with the Entry api and not getting my compiler to allow my unsavory tricks. &amp;#x200B; The example (working but double lookup) &amp;#x200B; ``` use std::collections::HashSet; use std::collections::HashMap; use std::collections::hash_map::Entry; use lazy_static::*; lazy_static! { pub static ref TAG_DATA: HashMap&lt;i64,HashSet&lt;String&gt;&gt; = { let mut m = HashMap::new(); let mut m2 = HashSet::new(); m2.insert("bar".to_string()); m.insert(0,m2); m }; } fn type_has_tag(place_type_id: i64, tag: &amp;String) -&gt; bool { if TAG_DATA.contains_key(&amp;place_type_id) { return TAG_DATA[&amp;place_type_id].contains(tag); } return false; // let lookup = &amp;TAG_DATA.entry(place_type_id); // match lookup { // Entry::Occupied(tags_entry) =&gt; { // let tags = tags_entry.get(); // return tags.contains(tag); // } // Entry::Vacant(_vacant_entry) =&gt; { // return false; // } // } } fn main() { println!("type has tag, type id 0 should have bar in hashset {}",type_has_tag(0,&amp;"bar".to_string())); } ``` &amp;#x200B; Here's a playground with the source [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b6bf764d70a825d6b763ffc0b622bdcd](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b6bf764d70a825d6b763ffc0b622bdcd)
Currently the crate `safe-transmute` does [the exact same thing](https://github.com/nabijaczleweli/safe-transmute-rs/blob/727ad53d29ae952da9ceb52a214af325cc49979b/src/to_bytes.rs#L241). Could you check if that's UB, and if so, file a bug on their bug tracker?
Do you think Rust would give any significant advantages over a JVM language like Scala or Kotlin for this sort of application? Perhaps maybe better security since the Java Serializable interface introduces some risks?
Believe it has to do with the Entry api being used to modify values and the HashMap here being static but googling around here, the book, rbe, and stackoverflow is coming up empty.
Found a related link, still no example with a match flow [https://www.reddit.com/r/rust/comments/2xjhli/best\_way\_to\_increment\_counter\_in\_a\_map/](https://www.reddit.com/r/rust/comments/2xjhli/best_way_to_increment_counter_in_a_map/)
You'd have to find some way to detect recursion, as that can consume arbitrary amounts of memory. This might not be doable unless you add the rule "anything that calls a function pointer might be recursive at some point", in which case you've eliminated a lot of potentially useful code.
I think not having a GC could definitely be a benefit. Flink does some weird stuff to ease GC pressure like serializing to in-memory buffers in order to ease GC pressure. Serialization could also be a lot faster. Ideally you would not even have to do deserialization and would just be able to use the memory of the buffer directly. I.e. instead of allocating space for a string on the heap, just read/write it directly on the buffer. Something like [cap'n proto](https://capnproto.org/). That would also give you great locality, since you would be able to allocate relevant values close to each other. You could more easily swap to disk if you ran out of memory. No need to do this on the application level like Flink does, just let the OS do it.
Std, core, or alloc? Different environments have different requirements and there's no one-size-fits-all solution. The futures library provides a general-purpose executor that works in hosted environments; fahrenheit is a relatively simple implementation that's easy to understand; embedded-executor is a single-threaded executor designed to work on systems without std, but have alloc; and my dumb-exec tries to provide a variety of executors for various freestanding environments including a fixed-size cache for systems without alloc, and a multithreaded executor that works without std. Make sure you know your niche in this space.
My bad I misunderstood
Yep, `transmute_to_bytes_vec` triggered heap corruption. I added a comment with the example code to the related issue.
Comments like this make me a wiser person. Thank you.
&gt; With things like tokio-fs, file serving is mostly just glue code and some sanity checks (guard against path traversal attacks, ...). There's a fair bit of HTTP-specific stuff that last I checked none of the frameworks handled. For example, I wrote [http-serve](https://crates.io/crates/http-serve) to handle conditional GETs / etag handling, byte range serving, on local filesystem or not. At first glance, there's some overlap between my work and this crate. Then there's going from a base directory and HTTP path to the correct file or the directory listing. Yes, guards against path traversal attacks are an important point (both the obvious `..` path entry and consideration of symlinks and such). Also selecting a pre-compressed file if available (depending on `Accepts-Encoding` and such, selecting a file with a `.gz` suffix if available, and adjusting the headers). Picking MIME types. Setting appropriate caching headers; for example, if you're using WebPack-based files that have a hash embedded, you might say to cache the result forever. etc. I have some half-finished code to deal with some of this. No one thing is terribly difficult but when you add it up, it's large enough that you don't want each app to do it themselves, and perhaps not even every web framework. (And the frameworks simply aren't doing all the stuff that one might expect now. I haven't seen any that do byte range serving, for example.) I look forward to looking more closely at what this crate does when I have cycles.
Thanks for answer. I had a feeling that it might trigger UB. I originally wrote only a safe version but after seeing the assembly I tried to optimize it.
AFAIK `-fsanitize=fuzzer` means "get sanitizer-coverage and also link in libfuzzer", which cargo-fuzz already does but without passing that flag. I'm not sure if it's cargo-fuzz or cluster-fuzz that should be modified here. Perhaps cluster-fuzz should become yet another fuzzing backend, in addition to cargo-fuzz/afl/honggfuzz and hopefully angora coming soon. It's high time we figured out how to unify fuzzing targets for those anyway because the fuzzing targets are exactly the same across the board, they just appear in different boilerplate. So all the crate needs to do is expose a fuzzing target function once and then we should be able to plug it into any backends we damn please.
You can make a wrapper struct that changes the `Vec&lt;u8&gt;` back into a `Vec&lt;u32&gt;` before it is dropped, or return a `&amp;[u8]` instead of a `Vec&lt;u8&gt;` and keep the original `Vec` around to be dropped.
It's the `unsafe` function that should document under what conditions you can call it. E g, if `glGenBuffers` requires somebody to first call `make_current` before it can be safely called, then `glGenBuffers` is the unsafe function and where the documentation should be. The locality or implicitness is not important (although it certainly wouldn't hurt to give a heads up in `make_current` too, it's not needed). IMO.
Usually, people from the JVM report using *far* less memory, and usage is more stable overall.
You can add two `[[bin]]` sections to your `Cargo.toml`, if that is what you mean? [Reference](https://doc.rust-lang.org/cargo/reference/manifest.html#configuring-a-target)
Writing code that has different cases for big and little endian machines is a code smell. Just use byteorder or similar and let the compiler do the rest. You'll probably end up with the compiler doing the smart thing (transmute or a bswap instruction) but don't have the risk of getting it wrong. Most of the time you'll never test the big endian code and may end up with bugs there because of it.
I found this blog post changed my perspective on byte order: https://commandcenter.blogspot.com/2012/04/byte-order-fallacy.html
 match true { true if func1() =&gt; { something }, true if func2() =&gt; { something }, true =&gt; { handle_default_case }, } 
I've found a alternative. For academics, in my CPP project they share a main() and the only difference is that a few object files are switched... though having a small boiler plate main() duplicated wouldn't be the end of the wold.
[https://exyr.org/2018/windows/](https://exyr.org/2018/windows/)
I'll re-iterate [my comment](https://github.com/rust-lang/rfcs/pull/2657#issuecomment-471024588) from the PR here to to gather some more discussion + perspective: &gt; This looks great overall! :D I am a little puzzled to see the "[Async Ecosystem](https://github.com/steveklabnik/rfcs/blob/2019-roadmap/text/0000-roadmap-2019.md#async-foundations-and-async-ecosystem)" section mention Tide but not Tokio though? Shouldn't moving Tokio onto `std` futures be a major push, since there's so much ecosystem around that already? Whereas, as far as I can tell, Tide isn't really being used yet. @steveklabnik [responded with](https://github.com/rust-lang/rfcs/pull/2657#issuecomment-471069162): &gt; tokio is not governed by the rust project, so we cannot control what they do, so itâ€™s not on the roadmap. This struck me as an odd position, [so I replied](https://github.com/rust-lang/rfcs/pull/2657#issuecomment-471075456): &gt; but isn't the point of an "ecosystem" team to work with the ecosystem? Just because a project is not directly under the control of the Rust team doesn't mean that it's not important to Rust users and to the Rust language, or that the Rust team can't be involved. Tokio in particular is an example of something that is widely used in the async ecosystem, and ignoring it seems like an odd choice; it's currently a major part of Rust's async story, and that seems like something worth taking advantage of and building on. Tide on the other hand is currently a minor project that, as far as I know, doesn't have much adoption in practice? I don't think tokio is the only example here either; hyper, actix, and h2 are also big players in this field that it seems reasonable for the ecosystem team to work with. &gt; &gt; I don't think it scales for various official Rust teams to have a mandate that only allows them to work on their own stuff, nor do I think it's a good idea. There aren't enough dedicated people working on Rust that they can reasonably own and manage all the popular components in the Rust ecosystem. Which raises the question of why the already-stretched resources of the various Rust teams are being spent on writing something like Tide when the groundwork has already been laid elsewhere? What do others think here? Given that the road map specifies an "async ecosystem" team, what would you expect such a team to work on? Is anyone currently using Tide? Are there other crates beyond `tokio`, `hyper`, `h2`, and `actix` that people would consider core parts of Rust's async story?
Basically I want to reinterpret vec of u32 as raw bytes. Code example you gave would only truncate the values.
I didnâ€™t even realise. Sorry. Thank you for letting me know 
Ironically, this code is for output of an assembler so byte-order definitely matters.
The post is about exactly this perspective and how it is the wrong way of thinking about this problem.
That certainly works! But if each of your enum variants contains a different type it might just be easier to implement From&lt;T&gt; for each type you might construct your enum from
I still consider this but I want to avoid unnecessary allocations. In the end I will probably use it.
You might be interested of [Resource Aware ML](http://www.raml.co/) which I think is one of the closest things out there that match your description and is even remotely practical.
I totally agree, I think even if its not tokio or hyper, the goal should be to help the entire ecosystem, not just the few projects under `rustasync`. I would really love to see support coming from the community and the rust wg teams. I think this will benefit the users of rust tremendously. 
slices come with an [align\_to](https://doc.rust-lang.org/std/primitive.slice.html#method.align_to) method that might suit your needs
For just cross-building locally instead of on CI using docker containers is a really convenient way to set things up and keep different build environments clean. AND have very little downtime when moving to a new computer or after re-installing your OS. The [rust-embedded/cross](https://github.com/rust-embedded/cross) building tool uses docker containers under the covers to make cross building really easy, might be worth checking out. For building gtk apps with it you may need to use a custom docker image based off the appropriate image that cross uses and just add the appropriate armhf libs.
You're looking for /r/playrust.
Just FYI, get and get_mut are usually present on most things that implement Index/IndexMut (the traits that allow square bracket indexing) and usually are the same but return an option instead of panicking on a bad index.
I donâ€™t have time for a full reply, and will on the RFC later, but the purpose of the roadmap is to provide guidance on RFCs. That is, when considering an RFC, the idea is to look to the roadmap, and if it fits, say yes, and if not, postpone. As such, while the team certainly wants to work with the ecosystem, doing so is outside of the process, and so sorta out of the roadmap too. There canâ€™t really be RFCs related to tokio. Does that make sense?
Thanks Jay, I ended using get in this situation and it worked out great. Of course later on I ran into another issue with a shared global cache and mutexes up front so I think I may need another tool to capture the index and insert if missing or just perform the get and set/get suboptimally to limit locks (single threaded ATM, but that could eventually change)
Yes, I think it's totally reasonable for the roadmap to not deal specifically with individual projects (someone pointed that out [on GitHub](https://github.com/rust-lang/rfcs/pull/2657#issuecomment-471085913) too). It was the fact that Tide was specifically called out that [I felt](https://github.com/rust-lang/rfcs/pull/2657#issuecomment-471087147) as though the other core pieces of the Rust async ecosystem should be mentioned as well.
Tide *is* a project controlled by the WG and so RFCs related to it are actually a thing.
Hmm, that seems problematic to me... I could be misunderstanding you, but it sounds like the policy is "we can't make goals with regards to the ecosystem, just our own projects"? That approach is troublesome as it encourages a degree of "Not Invented Here", where Rust teams are incentivized to either take over control over existing projects or start their own, instead of working with the ecosystem as-is. I think there is a big question of whether spending significant resources on Tide is even reasonable, given all the infrastructure that already exists out there, and "blessing" it by putting it on the official Rust Roadmap seems premature and somewhat conflict-of-interest-y. To me, it makes sense for the Roadmap to _either_ say what the overall goals are, _or_ to list all the projects that the ecosystem team will focus on; naming just _one_, even if it is controlled by a Rust team, is basically saying "this is where our focus lies".
Without `repr(C)` that's [still undefined behaviour](https://doc.rust-lang.org/nomicon/repr-rust.html).
In addition to From probably being a better answer, you can also simplify it [this way](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2731173fed692735b1789ceae018c34f).
Yes, it will interact with the browsers directly. WebGL bridge can be an intermediate step for better compatibility until the browsers mature.
Do you know of any libraries that perform that WebGl bridge?
No, Rust can not easily call into Java libraries. If you want to integrate Rust with Java code, the more typical path would be to use JNI to call into Rust code from Java. For IBM mainframe stuff, try finding C library that you can use; Rust can trivially call into those. Taking a quick look on IBM site brings up [IBM i Access for Windows C/C++ APIs overview](https://www.ibm.com/support/knowledgecenter/ssw_ibm_i_71/rzaik/rzaikcaexpapiover.htm) which seems relevant. 
It's not normal to call an `unsafe` function from a safe one if you cannot uphold its invariants. It is unsound and no such code should ever intentionally be written.
&gt;He wanted to create a single new function for his enum. That function had to have a way to specify what enum variant should be created. That's what the enums constructors are for, so this is a bit of a silly question. If you want to construct variants of enum `Enum` in a `new` fn, you could just pass the constructor in: enum Z { A(Option&lt;String&gt;), B(Option&lt;String&gt;), } impl Z { pub fn new&lt;F&gt;(constructor: F) -&gt; Self where F: Fn(Option&lt;String&gt;) -&gt; Self { (constructor)(None) } } fn main() { let a = Z::new(Z::A); let b = Z::new(Z::B); }
What a coincidence, that was my first published crate ever! https://github.com/neosmart/srtresync I implemented both fixed and linear drift correction. The code is now three years old, so it's not "modern rust" by any means, but I did use the binary a few weeks back!
You might try the [jni](https://github.com/jni-rs/jni-rs) crate, specifically its `JavaVM` interface. You'll probably need to enable the "invocation" feature. Check out the [vm test](https://github.com/jni-rs/jni-rs/blob/master/tests/vm.rs) for an example of it in action. It's certainly not pretty, but it at least works. There's also [rucaja](https://github.com/kud1ing/rucaja) which I'm less familiar with.
That's a good point, I hadn't considered recursion (how embarrassing). 
That's a good point, I hadn't considered recursion (how embarrassing). 
Are there plans to bring back the `Wake` trait such that a conversion can be provided in `alloc` for `Arc&lt;T&gt;` where `T: Wake`? I've got [this](https://gitlab.com/polymer-kb/firmware/embedded-executor/blob/master/src/wake.rs) in my executor implementation, and it seems to work pretty well.
Thanks for your reply u/zokier ! Yes, we have lived with Client Access for Windows for nearly 20 years. The beautiful thing about using the Java Toolkit was to rid ourselves of having to install Client Access on a Windows PC forever! We (at work) were very sad IBM did not take over Java when it got outbid by Oracle. {sigh} I have running code right now that uses the C++ libraries to do all this (as well as the newer Java code that is so much simpler to use). So maybe there is something there I can tap into - and not use Java at all. 
Any particular reason for not using rustfmt?
Okay thanks u/jechase ! I will check it all out, I appreciate your quick response. 
I checked into this once since I have an AS400 server I need to collect data from and I think I concluded that the ODBC route was the easiest, there are several libraries for that in Rust as long as you have the ODBC driver installed (not an easy task to get hold of one for AS400 though, but should come with the installation of Client Access for System i or something like that). However, I never followed through and actually got it working since I had an existing setup that already worked for data retrieval, but if you get it to work please let me know.
Very cool! I'm fascinated by the use of `NaiveTime` to represent subtitle time codes. That's a clever idea. But I wonder what happens if somebody has a 25 hour movie? :-) I did notice your error handling relies very heavily on `expect`, which just aborts the program immediately. This is totally fine for a small command-line tool for personal use. But it's also possible to use `?` to return errors to your caller. If I'm feeling lazy, sometimes I use the [failure](https://rust-lang-nursery.github.io/failure/) crate, setting my return types `Result&lt;T, failure::Error&gt;`, and then I would write something like: let input_file_path = args.get(1).ok_or_else(|| { format_err!("Missing input file") })?; ...and: let file = File::open(input_file_path) .context("Can't open input file")?; This passes the errors all the way up to my `main` function, where I can print them out nicely. I also have a bunch of SRT alignment code in [substudy](https://github.com/emk/subtitles-rs) that I use for making audio-visual flash cards for Anki. Feel free to take a look if you're interested!
Oh, cool. I'll be sure to check it out. I didn't even know linear drift existed but my life needs it so much. 
No, that's absolutely insane. Stick with results, never force the caller to panic when you don't have to.
No excuse. I just cargo fmt'd it and it definitely reads better now, thank you.
I saw the name came from a comic book character so I figured I'd read the comic if the author likes it so much, but it ended almost immediately because it's brand new. I didn't notice until then that the author of Meow Hash _is_ the author of the comic. Neat.
I really enjoyed the [discussion of this issue on Reddit the other day](https://www.reddit.com/r/rust/comments/awh751/proposal_new_channels_for_rusts_standard_library/ehmhk4j/), so I thought I would post a couple of tricky cases that I had run into. Thank you to everybody who has shared their experiences!
Woukd you mind giving a layman's explanation for what this does? 
That's always the rub with this - my marching orders are to not have CAX installed on the PC anymore, to either use jt400 or some other way. CAX (Client Acces/iSeries Access/etc) is a hog, when we really only need the ODBC driver and the DataQueue classes. We have messaging to and from the PC application using DataQueues, so I may not be able to get around the Java option. The alternative is to do what Microsoft has always said to do - just write a socket server between the "native" (in this case, Java jt400.jar code) and one's running code (Rust, in this new case - C# in the former case). But it would be really cool to directly talk to the 400 using Rust. I might just dive deep into the C/C++ classes of CAX and see what I can come up with. If I am successful one day, I will post here!
There's would be an additional method that returns a result, so nobody's *forced* to panic. Similar to the `RefCell` borrowing methods.
the return is redundant in your answer there, by the way.
Yep, see [https://doc.rust-lang.org/stable/std/collections/struct.HashMap.html#method.entry](https://doc.rust-lang.org/stable/std/collections/struct.HashMap.html#method.entry) &gt;pub fn [entry](https://doc.rust-lang.org/stable/std/collections/struct.HashMap.html#method.entry)(&amp;mut self, key: K) -&gt; [Entry](https://doc.rust-lang.org/stable/std/collections/hash_map/enum.Entry.html)&lt;K, V&gt; The \`&amp;mut self\` there means that it needs to be mutable to work.
Rocking, I need this in a hash table elsewhere where get won't cut it
Oh rust auto returns the last object? Haven't caught that yet 
I understand exactly what you mean. Just getting a current version of CAX to work on new OS/Hardware is somewhat of a black art in our company. If you get anything working that doesn't rely on CAX its very interesting for me and probably many others as well. Btw, I've been able to work around this earlier by setting up our AS400 as a "linked server" on our MSSQL server and just query the AS400 through a MSSQL driver (which Rust have). However, it's far from ideal, and its very difficult to know how queries end up being executed in the end, but it works surprisingly well in practice. 
It's more "everything is an expression and expressions evaluate to their last value, but yes. The \`match\` evaluates to the value of its arms, and the match is the only expression in the body, so the function evaluates to it too.
Let's say your program takes a single string as input. If it can be parsed as a usize, it attempts to allocate a Vec&lt;u8&gt; of that size, and exits. There is no practical upper limit for this program's memory footprint. We can do it again. The second program takes no input, and allocates an Vec&lt;u8&gt;, with the size being based off a random number generator, or the number of milliseconds since the epoch. You can't, via static analysis, predict the memory footprint of any of these. 
Isn't this code basically the same thing as: enum Z { A(i32), B(String) } fn main() { let a = Z::A(0); let b = Z::B("Hello"); let mut v = Vec::new(); v.push(a); v.push(b); for a in v.iter() { match a { Z::A(\_) =&gt; print!("A"), Z::B(\_) =&gt; print!("B"), } } } I kinda don't see how a single constructor for all variants help you.
Well, if you're OK with just dropping the problematic input instead of processing it and exceeding the memory limit, then I guess you could just substitute the global memory allocator for one that panics on exceeding a certain limit and then catch panics. Other than that, the only thing you can do is only ever allocate fixed-size buffers. Something like https://github.com/facebookexperimental/MIRAI could enforce that.
Exactly. This is really a question of defaults, not of forcing anything.
Isn't Zig trying to do this?
You can't just glue channels randomly, without thinking about the overall structure of it. That's akin to doing random `goto`s in your code. Channel organization has to be structured. In my code, when I structure pipelines ([even complex ones like in rdedup](https://github.com/dpc/rdedup/wiki/Rust's-fearless-concurrency-in-rdedup)), I always make the receivers quit only after senders disappeared, and never earlier. Receivers are always a loop of `recv` that terminates on lack of senders. The whole shutdown is propagating this way. I drop all the root senders, and everything else shuts down when it's done. It seems to me that channels are just no the right way to wire futures and for them maybe `send` should return `Option&lt;Result&lt;()&gt;&gt;` or something, as everything is cancelable. But for threads i can always panic on send, since it only propagates panics upstream. 
The embedded rust people (probably as part of the WG) have been working on tools to help determine stack usage. [https://crates.io/crates/cargo-call-stack](https://crates.io/crates/cargo-call-stack)
Not even as a function of the input size?
Although I suppose you could also do that with a gc language.
The default should be to Result, and then the longer named method can be the auto-unwrap one. 
rust-bio has an interval tree data structure: &amp;#x200B; [https://github.com/rust-bio/rust-bio/blob/master/src/data\_structures/interval\_tree.rs](https://github.com/rust-bio/rust-bio/blob/master/src/data_structures/interval_tree.rs) &amp;#x200B;
Awesome, I'm fairly new to Rust too and I really love having rustfmt on save in VSCode. Not having to think about formatting is such a relief compared to Swift where people are often very opinionated when it comes to formatting their code.
filter_map is I think my favorite function ever
Yeah, I agree it is a relief. I use CLion instead of VSCode, I'll see if I can configure it to run on save!
I think it's a deliberate design decision for Rust's standard library to provide minimal implementations. This is enabled by the facts that open source libraries are popular in general and the build/deployment burden of transitive dependencies is really really close to nil with rust. So everyone can just use one of the more popular public crates providing the required feature set. Though OTOH with LTO or function-sections and linkers that support gc-sections it's probably not too big a burden to have some extra code in the std lib.
Ha, funny coincidence! I noticed the 25h movie problem and was bugged by it when I saw there was no way to parse that with chrono. I decided to look around to see how other libraries parsed the files and came across your subtitle-rs, where I saw you use parser combinators. I planned to rewrite the code to use that as well (as evidenced by [this commit message](https://github.com/gallowstree/subfixer/commit/8513acf0051d7985db3ffe0f568a96238e0380da) ) But since I was working on this on my office time and was impatient to watch my series with good subtitles I decided it was acceptable to have the 24h limit for now :) Thanks for the tips, and do you by chance have anything in your code to rename subtitle files to match the video files?
Thank you for your feedback! &gt; You can't just glue channels randomly, without thinking about the overall structure of it. That's akin to doing random gotos in your code. Well, in one of the programs I link to, the structure is pretty easy. The fundamental type is `Stream&lt;Item=BytesMut, ...&gt;, a stream of binary chunks. If I need to pass between sync and async codes, then I use a channel. This gives me a connected pair implementing: - `Sink&lt;SinkItem = BytesMut, ...&gt;` - `Stream&lt;Item = BytesMut, ...&gt;`. These can just be glued together almost exactly like Unix pipelines. &gt; In my code, when I structure pipelines (even complex ones like in rdedup), I always make the receivers quit only after senders disappeared, and never earlier. Is there a nice example of how you do this? I'd love to look at the code! I'm trying to imagine how I would use this approach for an example like: cloud_data_source | to_binary | postgres_data_sink Let's say `postgres_data_sink` receives a fatal error from the database, and needs to exit early. In your approach, how would you shut everything down? We can't drain all the data from `cloud_data_source`, because it might have another 500 GB of data lined up. So I guess we must need to add control channels to `to_binary` and to `cloud_data_source`, and use those control channels to request shutdowns, then carefully wait until each output channel closes? Or do we need to wrap each `send` in some kind of check to see if we should quit early? And do we also need to be careful when we `send` to the control channels? And how do we test all this "shutdown on error" code? I can totally imagine that there is some design where this all works very naturally and makes perfect sense. But I really feel like I'm missing something really important in this discussion, because everybody keeps telling me this is much better than returning a `SendError`. I'd love to see a some examples of this approach. &gt; It seems to me that channels are just no the right way to wire futures and for them maybe send should return `Option&lt;Result&lt;()&gt;&gt;` or something, as everything is cancelable. I've been wiring channels and futures together for a year now in production, and it seems to work just fine, as long as I design with the idea that `send` may fail, and data producers will sometimes outlive consumers. Once I accepted this fact, my senders stopped panicking, I was able to delete a bunch of ugly control code, and I could snap components together like Legos. It really did feel like a win. I was like, "Oh, so _that's_ what `EPIPE` was all about!" But it seems like lots of very smart people had an opposite revelation, and I'm trying to figure it out. I think part of the difference may be futures. I really appreciate everybody who's been explaining this to me!
&gt; I decided to look around to see how other libraries parsed the files and came across your subtitle-rs, where I saw you use parser combinators. I actually use [`rust-peg`](https://github.com/kevinmehall/rust-peg), which is really more of a parser generator and not parser combinators in the Haskell sense. I feel like this is a totally under-appreciated little toolâ€”it's easier than `nom`, it handles all sorts of common things, and it's not that hard to learn. You can find [my grammar here](https://github.com/emk/subtitles-rs/blob/master/substudy/src/grammar.rustpeg), and you should feel totally free to borrow it and use it any way you want (with a 1-line credit in a comment somewhere). It's [in the public domain](https://github.com/emk/subtitles-rs/blob/master/LICENSE.txt). I also use `nom` elsewhere for parsing gnarly binary MPEG2 subtitles, and it seems like a good choice for that. But `nom` always feels like overkill for simple text formats. I also have some code tucked away in `substudy` that can automatically correct character sets and detect languages, both are which are helpful when working with subtitles. &gt; Thanks for the tips, and do you by chance have anything in your code to rename subtitle files to match the video files? Nope. Do you mean trying to automatically match them up? Or just renaming them with `std::fs`?
&gt; Next you go back to your unsafe thing If you're paranoid enough you should get a borrowck error in that situation, one of the nice new 1-2-3 borrowck errors courtesy of the NLL checker. I mean, I certainly understand that not all situations can be or should be made air-tight. That's why part two is "should" not "must." But your example certainly falls within the scope of part two and I think my recommendation is still a good idea. If there is a span of code in which clearing a Vec causes undefined behavior, that span could be protected by a borrow of the Vec. It could be small enough that the hazard is acceptable - I'm not going to worry much about a 5-line function violating its own invariants. Or it could be a hazard quietly lurking across several source files and over a thousand lines - which I wouldn't feel comfortable with even if it *is* documented and there is a good reason to allow it. 
No, it's still to early for this. I want to see us first running a rough subset of the API on top of gfx-backend-gl first, and then we can start playing with WASM target on WebGL.
If you write a \`no\_std\` crate without using \`alloc\`, there won't be any heap allocation. Of course, that probably is against the point of what you're aiming for (plus you lose most of the standard library). But that's something you can't do in some other languages. But if you don't need an actual guarantee, just rewriting your code in Rust might help to make the memory usage more predictable, at least. Since you know the memory used by a \`Vec\`, for instance, will be freed as soon as it's dropped (though what 'free' means will depend on the allocator).
&gt; AFAIK -fsanitize=fuzzer means "get sanitizer-coverage and also link in libfuzzer", which cargo-fuzz already does but without passing that flag. I'm not sure if it's cargo-fuzz or cluster-fuzz that should be modified here. Ah, hey, that's excellent news. My plan for this weekend was to give cargo-fuzz a shot, since, like you say, it's already linking in libfuzzer. I _think_ that's the crux of the issue for clusterfuzz. &gt; Perhaps cluster-fuzz should become yet another fuzzing backend, in addition to cargo-fuzz/afl/honggfuzz and hopefully angora coming soon. It's high time we figured out how to unify fuzzing targets for those anyway because the fuzzing targets are exactly the same across the board, they just appear in different boilerplate. So all the crate needs to do is expose a fuzzing target function once and then we should be able to plug it into any backends we damn please. I would be very for this. 
At least not for the moment. Even the \`RawWaker\` had not been stabilized, and \`Wake\` had been deliberately removed to reduce the amount of APIs to stabilize. An equivalent implementation as yours exists as \`ArcWake\` in futures-rs. However there were some discussions on discourse that this kind of Waker implementation comes with the drawback that it can leak the memory for the Futures/Tasks for a certain amount of time (since the Futures are still stored as Wakers inside objects that might not touch them anymore after the tasks got cancelled). So it might actually not the preferred way to implement Wakers going forward. Task Handles/ID that are stored inside Wakers that avoid that problem.
I sort of agree with the technical points, but linguistically `chan.send(foo).unwrap()` reads just fine.
Thanks for the detailed response. I agree with pretty much everything you point out, so I'll just reply to two bits of interest: &gt; Please note community team was not the team responsible for the website, yet we're the team analyzing it. It's not our roadmap goal, though. I actually didn't know that. I guess I just saw that the lead of the community team was the one responding to most of the PRs and tickets on that beta website, and from that assumed that the website was an effort of the community team. &gt; Personally speaking, I think the hyperfocus on the website isn't helping (I don't want to invalidate concerns here), there's tons of broken things and it drowns out other discussions. I can believe it. Having contributed only casually to rust, I'm in a place where most miscommunications on the technical or organizing side just don't impact me. On the other hand, the presentation of Rust in the public scene -- conferences, official website, sponsored workshops, etc -- is just a lot more visible and impactful to my day-to-day experience as a rust user. But obviously people who interact with the rust ecosystem in different ways will be frustrated by different sets of issues. Looking forward to reading the retrospective.
&gt; cloud_data_source | to_binary | postgres_data_sink If there fatal error is outside of what my program has to handle, then I would just let everything panic. If it isn't, then I would have a `Arc&lt;AtomicBool&gt;` that is set to `true` when everything should terminate, and check that before sending anything in `cloud_data_source`, set it to `true` in error handler at root of every thread that can fail, and after failure keep `recv`ing (but drooping immediately) until sender disconnected.
To kind of synthesize those ideas into something even better, you could create a wrapper for Vec&lt;u32&gt; which impls Deref&lt;[u8]&gt; (and whatever other Vec&lt;u8&gt; fns are needed) and does the byte order thing in-place on construction. This is exactly the reason functions shouldn't take Vec as an argument. At most AsRef&lt;[T]&gt; + Extend&lt;T&gt; or whatever.
Since you asked, I do have another, thought I do understand why it might be hard to implement: I assume that in the case of `(defn main "main" [] ...)` the second argument to defn is the docstring. Usually, afaik, docstrings would come after the args list/vector in a lisp. Also if you aren't that familiar with lisps, I'd suggest checking one out. I've been a dedicated emacs user for a few years now, and I love elisp. That got me in to common lisp (sbcl and picolisp) as well, which are really fantastic. Maybe even look in to getting a major mode that works with your language for emacs, since that's the #1 environment for lisp development.
It's a WIP port of the C code that emacs is based off of to elisp-compatible Rust code. Most of the functionality of emacs is the lisp on top of the C layer, but still it's a pretty ambitious project. I wish I was better with Rust so I could contribute.
In Rust, is there a way to have an enum that (1) is the size of a single pointer (or smaller) and (2) does not use any extra memory when you are on a "smaller" variant? In other words, I'd like something like: enum A { V1(Box&lt;SmallStruct&gt;), V2(Box&lt;LargeStruct&gt;), } but I'd like the tag to live *after* the pointer (in the structs) so that the `size_of::&lt;A&gt; == size_of::&lt;Box&gt;`. The equivalent C would look something like: typedef struct { uint8_t tag; } A; typedef struct { A a; uint8_t space[8] } SmallStruct; typedef struct { A a; uint8_t space[80] } LargeStruct; common * create(uint8_t tag) { if (tag == 0) return (common *) malloc(sizeof(small)); return (common *) malloc(sizeof(large)); } although this forces you to manually juggle the `tag` yourself. Thanks so much for the help!
There's actually a diagnostics WG forming, I'll forward it to them!
This is hyperbolic. Every line of channel oriented code I've ever written, across multiple languages in the past several years, including Rust, has used either deadlock or panic semantics for sending on a closed channel. This is perfectly _sane_ because in all of those cases, sending on a closed channel is a bug in the way the code is written. Panicking in such cases is perfectly reasonable. &gt; never force the caller to panic when you don't have to This is blatantly wrong advice.
Couldn't you just box a normal enum?
&gt; Macro is very restricted and can't be that ergonomic and straightforward as something language provides. People keep making this assertion when trying to convince Someone Else (tm) to implement their pet feature, and yet somehow they never provide any proof. I wonder why? What's wrong with Holy_City's approach?
I could, but the enum would be the size of the largest variant (plus the tag), which means I have to allocate more space on the heap for the smaller variants.
&gt; Usually, people from the JVM report using far less memory, and usage is more stable overall. I assume you meant "people from JVM report using far less memory *in Rust*"? As phrased it could be read as the opposite.
Hi, you could make your macro like this macro_rules! empty_my_trait_impl { ($type:ident&lt;$($gen: tt),+&gt;) =&gt; { impl&lt;$($gen),+&gt; MyTrait for $type&lt;$($gen),+&gt; { fn my_method() {} } }; ($type:ident) =&gt; { impl MyTrait for $type { fn my_method() {} } }; } but it would not be able to work with bounds on the generics, why not just make your trait trait MyTrait { fn my_method() {} } and you can implement it like that impl&lt;T&gt; MyTrait for MyStruct&lt;T&gt; {}
Sounds like a feature that belongs in an event loop, which is how a lot of application frameworks work. What are you using for your UI?
It's a tool for simulating Microcontroller based systems on a host PC. Can be useful as learning tool or facilitating testing of MCU based code without the need for real hardware. &amp;#x200B;
Ah, I see. This could be a neat thing if we ever get something like `#[repr(DST)]`, but I don't think there's any nice way to do what you want right now. You can definitely cobble together something with unsafe and give it a safe API, but it's not first class.
After a quick skim I do like what I see. Especially the separation of the CryptoProvider trait, but I feel like the signal provider should be either in its own crate or be a feature-gated extension to the crate; not just a test. Do you have any plans for other providers or supporting group chats? A suggested provider: [dalek](https://dalek.rs)'s [Ristretto group](https://ristretto.group) for DH, and [miscreant](https://miscreant.io)'s AES-PMAC-SIV (or when/if it exists AES-POLYVAL-SIV) for the AEAD/MRAE. Unfortunately the group chat problem isn't well solved. [Matrix](https://matrix.org)' [megolm](http://git.matrix.org/git/olm/about/docs/megolm.rst) uses per-room signing keys to absorb the per-user authentication. [MLS](https://mlswg.github.io) uses an [Asynchronous Ratcheting Tree](https://eprint.iacr.org/2017/666) to rotate the keys for forward secrecy after post-compromise security ([rust impl](https://docs.rs/molasses/)); but MLS/ART doesn't work in distributed contexts like Matrix due to ordering constraints on the ratchet. Few nitpicks: 1) The encrypt/decrypt methods return vectors. It would be nicer if these took mutable slices and an associated constant were used for the ciphertext expansion (or a function for variable length ciphertext expansion. I.e. to pad the message to the nearest multiple of a block). 2) The `std::` depends may be `core::`, and use [hashbrown](https://docs.rs/hashbrown) for the HashMap implementation to support no-std users. 3) Make the message counter a type parameter, 32-bits may be fine for Signal but it might not be enough or may be overkill for other users.
In that case, I don't know if there's a point in having another method, since it won't be much shorter than the code using `unwrap`.
Well that's why `gl-rs` is the unsafe raw thing and `glium` is the safe wrapper. If unsafety makes you uncomfortable, you sure shouldn't be using `gl-rs` :P
You should really use std::net::Ipv4Addr type, and use its parse method instead of implementing your own. You can then call the octets function to access each byte. Then, once you have your octets, you can do a simple loop like: let mut result: u64 = 0; for octet in ip_addr.octets().iter() { result = result * 100 + *octet as u64; } For the code posted here: definitely use u8 instead of i8 for IPv4 bytes, and return an Option&lt;u64&gt; or Result&lt;u64&gt; instead of i64.
No, it can not work. Let's say you have a function body which contains this line somewhere: `let v = vec![1u8, 1000];` Obviously, this line allocates a KB on the heap. Easy. But the hard question is: how many times is this line executed? It is undecidable in general (due to the halting problem) if that line is even executed *at all*. There's nothing we can do in a general-purpose turing-complete language like Rust to fix this. Here are some ideas what you could do, though: 1. Work in a language that is not turing complete. This language may be a restricted subset of Rust or some other language entirely. 2. Work in a language where arbitrary pieces of code can not allocate. This would require something like an effects system, and a quite advanced one at that if it should be able to bound allocations based on input size. 3. Be satisfied with an incomplete solution. There may be an algorithm that can determine how many allocations happen for a sensible amount of Rust programs and that gives up otherwise.
&gt; One notable detail of Matrix' olm/megolm is the DH-ratcheting may be tuned from just every round-trip to every N messages or after T-time (or any manually triggered event). This would be nice to reduce the ratcheting costs for high throughput users. In practice, this is also what the clients do. I just yesterday had a lengthy conversation in a Matrix room encrypted with Megolm, with many hundreds of individual devices in it. It worked very smoothly, for the most part you wouldn't even notice it was end-to-end encrypted at all.
A parallel idea: Computable max stack space. A function has a upper bounded stack if all called functions and all stack memory it allocates is bounded. Obviously recursive code wouldn't end up measurable, but a lot of other code would. The advantage of something like this would be for green threads; instead of a segmented stack or similar, you could allocate exactly enough memory space to run some specific threaded computation. And if you optionally combined it with some "no internal allocations" rule, you could probably, for some limited cases, get similar guarantees about overall memory limits you are looking for.
Excellent post! I hope to write a response later, but right now I'd love to drop a bit of info, which should be interesting to anyone participating in the discussion. Go channels have "receiving from a closed channel panics" semantics, so it's interesting to see how "fragile pipes" problem is solved there. Here's a random post about this: https://medium.com/statuscode/pipeline-patterns-in-go-a37bb3a7e61d The crucial trick they use is that each pipeline stage manages closing of it's output channel. In Rust linguo, each stage retains a `Clone` of its `Sender`. That means that if the nth stage of pipeline breaks, all the previous stages are blocked on `.send`, until explicitly canceled. 
I've worked a lot on reducing memory allocations in the sozu proxy, and I ended up putting a lot of things in memory pools. I also have [unit tests asserting structure size](https://github.com/sozu-proxy/sozu/blob/master/lib/src/http.rs#L1306-L1314). There's an idea I've been thinking about but have no time to try: writing the core processing code in a \`no\_std\` crate, without \`alloc\`, and have the calling code handle allocations, through configurable pools or arenas.
&gt; I'm trying to convert with the most efficient way possible an IP address to a decimal &gt; Example: 127.0.0.1 =&gt; 1270000001 Why? And how will you be handling IPv6?
If you are converting a dotted quad to decimal just remember the bytes need to be shifted or multiplied by their position ie q4:q3:q2:q1 = q1 + (256\*q2) ... &amp;#x200B;
Hi, i am not a pro Rust programmer. this is how i would write it `fn ipv4_to_int(ip: &amp;str) -&gt; i64{` `ip.split(".")` `.map(|x| format!("{:0&gt;3}", x))` `.collect::&lt;Vec&lt;String&gt;&gt;().join("")` `.parse::&lt;i64&gt;().unwrap()` `}`
Note that `AtomicBool` might not always work, because you can't stick it into `select!`, and so you can't cancel `send` that is in progress (blocked). Instead, I think a `Receiver&lt;!&gt;` should be used to signal cancellation. With this set up, you can select on sending to the channel or receiving cancellation. 
I'm not so sure, the API already has a history of `action` and `try_action` which either panics or returns a result, respectively. Why make the std library inconsistent?
It would be helpful for embedded systems. I have 8GB RAM on my Arduino Mega, for example. So I could easily get a stack overflow which triggers UB because I don't have an OS. 
At the risk of showing up on /programmingcirclejerk: because we're trying to drag the world kicking and screaming into an era where we handle errors instead of having the program explode. 
Is it possible to evaluate user provided string with interpolation at run time, but with limited scope? For example user types "Hello {name}" on stdin and my program writes "hello George" on stdout. But if he writes "Hello {launchnukes()}" my program outputs "Bad boy!"
That's not how you convert an IPv4 address to a decimal value. An IPv4 address is already a 32-bit integer, so if 5ou want its decimal representation, you take the first octet, shift it 24 bits to the left, then take the second octet, shift it 16 bits left and add it to the previous number, then take the third octet, shift it 8 bits left and add it to the previous number, then add the fourth octet to the number. So if o1, o2, o3, and o4 are the four octets of the IPv4 address (in your case, 127, 0, 0, and 1), you would do something like this: let ip = (o1&lt;&lt;24)+(o2&lt;&lt;16)+(o3&lt;&lt;8)+o4; If you try this with 127.0.0.1 you will get 2130706433 as the result. *This is a completely standard and valid way to represent the IPv4 address as a decimal integer.* For example, you you navigate to http://2130706433/ you will get to the same place as if you go to http://localhost/ .
Why? Existing methods use a try_ prefix instead.
&gt; Go channels have "receiving from a closed channel panics" semantics, You mean "writing to a closed channel", right? Receiving from a closed channel always immediately succeeds and you get a zero value as the result.
Detecting recursion is unfortunately insufficient. In the embedded word, there are analysis tools striving to prove an upper-bound for stack usage, and indeed recursion -- and calls through function pointers/v-table which could trigger recursion -- is the big offender. However, if you wish for a *total* memory bound, not just a stack space memory bound, then things get trickier. An iteration which pushes an item in a `Vec` at each step will consume O(N) memory. In fact, most uses of a collection expose you to O(N) memory consumption. It should be possible to annotate your function with memory consumption requirements, have an analyzer validate them from the ground up, and therefore create an overall upper-bound for the whole pipeline. This is not so different from what Prusti is doing in terms of static analysis, quite possibly simpler actually.
8KB, I think, not 8GB.
Thanks
Did someone point out yet that a difference between our channels and go channels is that you can close both ends of a rust channel separately? In go, if you split your channel into read and write ends, only the write end can be explicitly closed, so idiomatically it's the "producer" side that is in charge of closing a channel, so it's usually easy to avoid writing to a channel you just explicitly closed. The strongest thing the reader can do is abandon the channel so that writes block indefinitely (once the buffer is full). It's perfectly normal to never close a channel at all if you happen to know that everybody stopped using it anyway.
It's interesting to see the different examples, and it leads me to think that there are two clearly distinct use cases here: - Use cases where the sender is in control. - Use cases where the receiver is in control. And who is in control affects the shutting down sequence of the pipeline; or specifically whether the pipeline shuts down forward or backward. --- I think Unix pipelines are an example of receiver being in control. For example, suppose that I write: zcat file.txt | grep -Eo '[0-9]*-[0-9]*-[0-9]*T[0-9]*:[0-9]*:[0-9]*Z' | head -10 There, `head -10
I'm not sure what you're trying to say?
&gt;For example, you you navigate to http://2130706433/ you will get to the same place as if you go to http://localhost/ well fuck, this is very cool
&gt; Note that AtomicBool might not always work, , because you can't stick it into `select!` ...which means that you need to worry about race conditions. What if the receiver closes after checking the `AtomicBool` but before the `send`? Also it's a pain to pass it around everywhere. So yeah, `Arc&lt;AtomicBool&gt;` is not the first choice I'd reach for. &gt; Instead, I think a `Receiver&lt;!&gt;` should be used to signal cancellation. An interesting suggestion, and one that I hadn't considered originally. But what does this approach actually buy you, exactly? As I understand it, this means replacing a `send` which looks like this (in pseudo-code): ch.send(...).map_err(|_| MySendFailedErr)? ...with: select! { send(ch, msg) { Ok() } recv(ctrl, msg) { Err(MySendFailedErr) } } It's the same design pattern either way. `send` gets mapped to `MySendFailedErr` if the listener is gone. Except: 1. The `select!` version now requires passing a `ctrl` channel everywhere. 2. The `select!` version does not allow the `receiver` to be held by a `Future` that might be canceled or timed out, because the futures runtime doesn't know it needs to close `ctrl`. But aside from that, the basic design just uses a second channel and an external controller to recreate the basic idea of "`EPIPE` on sending to closed channel." And at that point, why not let the channel handle it? I'm definitely learning a lot from this discussion, by the way. Thank you!
Isn't SIGPIPE pretty much exactly panic-on-send for unix pipelines?
But the current std API convention and your declared goal are not mutually exclusive. The "returns a result" function can exist under the `try_x` declaration, and you can handle the error. Why you think we should eliminate all panic behaviour is beyond me, though. Panics aren't all bad, sometimes there are instances where panicking is a valid response, and you shouldn't force the user to pick error handling over panicking - that should always be at the user's discretion.
Sure, fixed, thanks for pointing this out!
&gt; https://github.com/dpc/rdedup/blob/master/lib/src/chunk_processor.rs#L123-L127 `SIGPIPE` is a really interesting data point, because I think it's main real-world purpose is that too many Unix programs just call `write` and completely ignore the error code, because C has never had anything like `#[must_use]`. So the OS can't count on `EPIPE` being able to shut down a badly-written sender. This means that the OS needs to be harsher by default, and kill the entire process. I don't think this necessarily applies to Rust, because: 1. Rust has `#[must_use]`. 2. Rust has `panic = "abort"`, which means that a panic won't just abort an isolated sending routine. It may abort _every_ single sender and receiver in a complex system. But the difference between `SIGPIPE` and `EPIPE` is an excellent point, thank you!
I use IAR Embedded Workbench at work, and it comes with stack usage analysis tools, which works as long as you: - don't allocate any memory at runtime to the heap (arrays on the stack are OK, but generally you don't want to be allocating large arrays on the stack) - don't do recursive calls (usually you don't do recursive calls on a microcontroller anyway - don't use function pointers/other things which could confuse the compiler I haven't looked up any open source tools which can do simple stack usage analysis like the above, but I'm sure there must be some out there. We are using ATMega16's in a legacy product, which has a whopping 1Kbytes of flash -_-. Generally you want to design your software so it only needs static buffers, and doesn't excessively use stack space by having deep function calls or putting too much on the stack. For typical microcontroller tasks, this usually works out OK. 
&gt; I don't think the analogy to the difficulties of manual memory management is fair. For me at least, once I started mixing "`send(...).unwrap()`" with future cancellations and timeouts, it was actually _harder_ to get things right than it was with `malloc` and `free`. :-( To be fair, this may just be because I've spent a lot of time programming defensively with `malloc` and `free`, and I'm still hopelessly bad at channels and futures. But both activities involved high levels of vigilance and stress, especially once I had to start reasoning about how futures were getting canceled after an error occurred. For example, if your `receiver`-owning futures are held inside a `Stream&lt;Item = Future&lt;Item = (), ...&gt;, ...&gt;`, and you're resolving up to `n` of those futures in parallel with: ``` futures_stream.buffered(n).collect()? ``` ...then if any of the `n` futures currently in flight fails, then the remaining `n-1` futures will be canceled (if I understand correctly) without warning. This immediately kills any `receiver` inside those futures. Now, rewriting this code to shut down all the senders in an orderly fashion when something like `.buffer(n).collect()` fails is _hard_, and it requires a lot of flawless, non-obvious code. Hence the analogy. :-)
You may use a custom allocator that forbids exceeding the limit and use fallible allocations like \`Vec::try\_reserve\` to handle the error or running out of memory; but this doesn't prevent the program from hitting it. It only forces you to handle the error yourself without needing to panic - but you might decide to panic. No, Rust, as-is cannot guarantee a function or scope does not exceed a memory bound. The bounds can be calculated with any total language, which Rust isn't due to panics. A language must be designed for this analysis, otherwise it'll likely be expensive to add later. The [POLA language](https://ir.lib.uwo.ca/etd/4740/) can only describe programs that complete in poly-time, and thus consume at most poly-space; thus known upper bound.
Are you writing this just for fun, or are there some shortcomings of qemu that you wanted to address?
Cap'n Proto only implements an arena. Mutating things in place is fine, but if you want to write a larger string you must still reallocate the string.
Holy crap. That unpacks a black box I had no idea I didn't know about.
&gt; Example of a recv loop: https://github.com/dpc/rdedup/blob/master/lib/src/chunk_processor.rs#L68 Thank you very much for the example! I find that every single example brings new clarity to this discussion, and I really appreciate people posting them. In this example, I notice that your receiver code only has two kinds of errors: 1. Errors which are OK to ignore silently. 2. Errors which are handled by `panic`. For example, [this code](https://github.com/dpc/rdedup/blob/b0dcf156b4360b9b5e1a4f0478ff631b71e3d866/lib/src/chunk_processor.rs#L123-L127): panic!( "rename failed {} -&gt; {}", chunk_path.display(), dst_path.display() ) ...and [this code](https://github.com/dpc/rdedup/blob/b0dcf156b4360b9b5e1a4f0478ff631b71e3d866/lib/src/chunk_processor.rs#L133-L138): Err(ref e) if e.kind() == io::ErrorKind::NotFound =&gt; {} Err(e) =&gt; panic!( "read_metadata failed for {}, err: {}", chunk_path.display(), e ), But imagine that your receiver routine contained a whole bunch of `?` operators, each one representing an error in an external system, and that calling `panic` wasn't an option. How would you modify your code to shut down your sender cleanly? That's the part I still don't understand as well. But thank you for the example! It looks like a really great program.
I started a personal project (apiw-rs) that does wrap a minimum selection of winapi interfaces into pure Rust version. Currently i have successfully wrote a minesweeper game with it. However i don't really have a lot of spare time to work on it. So i can't recommend you to use it directly. You may take a look of it none the less: The library: [https://github.com/crlf0710/apiw-rs](https://github.com/crlf0710/apiw-rs) The minesweeper game: [https://github.com/crlf0710/charlesmine-rs](https://github.com/crlf0710/charlesmine-rs) &amp;#x200B;
Yes, I just mean that what you described as a possible use of Megolm is also what the clients do in practice.
That second name in quotes is actually the name the function gets exported as to javascript :) functions without that string aren't exported. Yah, the more people I talk to the more I feel like I should try to dig down into one deeper.
APIW is [ECMA-234](https://www.ecma-international.org/publications/standards/Ecma-234.htm), which is meant as a standardized subset of winapi **not necessarily** implemented by windows itself. I started the project as a personal test dive, and then found that i can't even take enough time to do regular contribution to its winapi wrapping version, let alone implementing this set of APIs for other platforms. But i think theoretically there's nothing that can stop it being done. It's just that it will take a lot of time and work to do so. &amp;#x200B; &amp;#x200B;
Byte order of the output surely matters, but the byte order of the machine the assembler runs on should not matter. 
Sometimes does not a good default make. As with any side-effect (such as IO), panics are effectively lying to the type system about a program's semantics. I wish we would track effects in a more principled manner and only panicked in fringe cases. `.send()` may be one of those cases where its oftentimes rather than sometimes, but in general I think panicking for anything other than "the world is bricked, no choice but to terminate the program" is a bad default. I certainly would not think to have `arr[idx]` panic today if we'd have a do-over.
&gt; Every line of channel oriented code I've ever written, across multiple languages in the past several years, including Rust, has used either deadlock or panic semantics for sending on a closed channel. This is perfectly _sane_ because in all of those cases, sending on a closed channel is a bug in the way the code is written. I do want to make one thing very clear: I absolutely, 100% _believe_ /u/burntsushi when he says this. I've contributed PRs to his code, and every time I've done so, I've felt like just interacting with his code has made me a better programmer. Which is why I find it _weird_ that we disagree so much on this one issue, and why I continue to engage in this debate. I'm pretty sure thatâ€”in some specific, important contextâ€”he's absolutely right, and that if I were operating in the same context, the best thing I could do would be to listen carefully to his advice. But during the last year, I've had a bunch of experiences that hammered an opposite lesson into me. I feel like I can mix `tokio::sync::mpsc`and `std::future::Future` freely, and everything will be beautiful and composable and straightforwardâ€”to the point that I feel that async Rust is giving me ridiculous and extravagant superpowers. I can summon up and dismiss elaborate chains of computations with the wave of a hand, and it _works_ on the first try. But for me, the price of reaching that point was accepting that _any_ `send(...)` can fail. But I'm now absolutely convinced that /u/burntsushi has had the opposite lesson hammered home by _his_ experiences. And I know we both care about getting Rust APIs _right_. Hence this whole discussion. :-)
I did mean this is what the clients do. :)
&gt; Did someone point out yet that a difference between our channels and go channels is that you can close both ends of a rust channel separately? Thank you. I think this may be one of the key underlying points in this discussion. &gt; Rust lacks this "context" abstractions, so it is more convenient to make channels take on the additional role of communicating non-fatal cancellation "backwards". This will feel very alien to people whose intuitions have been built up by more orthogonal designs with go channels. Yes. Also, a Rust `Future` is inherently and trivially cancelable (just `drop` it when you no longer care), whereas goroutines can't be killed except by negotiating with them. Mind you, I think the Rust approach is actually amazingly good and that it fits together wonderfully well in practice.
I think having both options with a panicking `send(...)` and a non-panicking `try_send(...)` would be the best way. Anybody using `send(...).unwrap()` could get rid of a distracting function call. Anyone interested in reacting could use `match try_send(...)` or something similar.
Correct me if I'm wrong, but doesn't your library (and the linked libraries) leverage the winapi crate??
&gt;Cap'n Proto just implements an arena. Mutating things in place is fine, but if you want to write a larger string you must still reallocate the string. You would probably have to read from one buffer and write to another buffer when doing transformations that require more memory. You only want to do this for operations that require serilization obviously, and there would have to be some sort of way to chain operations together such that buffers only need to be read/written at the "endpoints". &gt; You shouldn't assume the program will run on a system with swap. If you're dealing with buffers that don't fit in RAM just mmap a file. Well sure, but which file? Many systems have a `tmpfs` mounted on `/tmp` so it ends up being constrained by RAM anyway. Swap by default is fine IMHO, but yeah allowing you to specify a file would also be useful.
I think a panic is valid if the program is incorrect - and it is not possible to prove incorretness at compile-time. Writing programs to handle errors in the same program is nonsensical - just fix the program!
For fun mostly. I needed a hobby project for learning to write Rust. Building an emulator seemed to be popular one and since I already had background in MCU's for many years I thought this should be easy enough... Couple of years later I have to say that at least I have learned a lot via concepts brought by Rust and dived into internals of MCU's.
Sure. But the FFI part(winapi crate part) is implementation detail, so you will never need to deal with it yourself.
(I've written a lot more golang code with channels and contexts than rust code that uses futures--does the "just `drop` it" approach get painful when you want to handle cancellation by doing things that don't fit neatly into `Drop` impls? I'm always positive surprised how well go context cancellation composes and it's not obvious to me if rust futures are as flexible.)
&gt; does interfacing with the Windows api inevitably dictate that we must use the C lib? Yes, because the C lib *is* the Windows API. Theres nothing else, thats it. Thats the only defined interface Windows exposes to programs. Note that Linux works differently here, exposing raw syscalls instead. IE, you directly communicate with the Linux kernel.(Well, you can, but that'd be super annoying so gnu libc exists to do it for you). And something to remember is that the windows api is *massive*, it provides a gui toolkit, it lets you debug and mess with other processes, it has several networking stacks, it has databases and encryption, accessibility features, it gives you memory and threads and processes, a filesystem, and more. Getting all that in to one library would be a challenge, and wouldnt make much sense. The Rust, or any other languages, standard library is the wrapper around the raw system api. If you want to open a socket, Rust can do that, and under the hood it's just using the system api. There are already tons of non-ffi versions of subsets of the windows api, it depends on what you use. If you read or write files, you're already using a rust version of the windows api, for example. &gt; Being intended for C however necessitates that Rust code has to jump through a bunch of hoops to make it work, using C built-ins and unsafe blocks. It shouldn't really require that many hoops? Rust interfaces with C pretty darn easily.
Panicking on incorrect programs (logic bugs) are fringe cases. Even so, with a rich enough type system most if not all logic bugs can be ruled out. Rust is not close to that richness tho.
I agree that pushing errors to compile time is preferable where possible.
&gt; This is perfectly sane because in all of those cases, sending on a closed channel is a bug in the way the code is written. Panicking in such cases is perfectly reasonable. This goes against the idea that production code should be reliable, i.e. failsafe, which means your program shouldn't crash on a bug but keep running as many systems as still possible. Kind of like how warships are designed for taking damage (with bulkheads), so reliable software should be designed with the idea that errors/bugs *will* happen and you need to keep running as many systems as still reasonably possible instead of crashing everything.
Cool :)
&gt; I've written a lot more golang code with channels and contexts than rust code that uses futures--does the "just drop it" approach get painful when you want to handle cancellation by doing things that don't fit neatly into Drop impls? I don't have any experience with this question yet. This is probably because my code tends to rely on immutability, idempotency and transactions, and so I can _almost_ always just burn everything down without consequence. But I do suspect that people will need to be careful about cleaning up resources held across an `await!`, and that you'll probably need to shoehorn things into `Drop` impls on objects owned by the coroutines underlying `async` and `await!`. But it's a really excellent question, and I don't have enough experience to give a meaningful answer yet.
Iâ€™m pretty sure instead of having a separate branch for a text tree and an ident, you can just make it a $ty and cover any type and allow for generics 
It's not really because the Windows kernel is written in C, it's mostly because they define their interface in terms of C function calls, via some DLLs. That's an arbitrary choice on their end, and code that wants to use the Windows API has to deal with it one way or another. In principle, you could look at what machine code lies behind these C functions and contrive to generate that machine code with pure Rust code. That's probably not a great plan: a) At some point, you're going to have to do some really low-level things to exchange data with the kernel, no matter the language. Taking away the C layer is not going to make this easier. I dunno how exactly Windows syscalls work but you're probably gonna need to write some amount of assembly or whatever too, so there's not meaningfully a pure-rust solution. b) Worse, since the interface is defined at the boundary of the winapi DLLs via C functions or whatever, by breaking that abstraction and attempting to usurp its responsibility, you're losing all promises regarding breaking changes. New Windows updates may break your code at any time, you're basically in a fully adversarial relationship with the Windows team at this point. Unless you have extremely peculiar needs, you probably don't want to find yourself in this position.
There is a cost to having a rich type system -&gt; complexity. Monads, applicatives, and friends are more stuff you have to learn. Just marking some code paths as incorrect/unreachable is simpler (thought not necessarily better ðŸ™„)
For what I'm doing (8-bit-esque tile-and-sprite graphics) glium is unnecessary abstraction and even gl-rs is more than I need. And it doesn't seem inappropriate to criticize published public interfaces. The lesson I've learned from this is that it is absolutely necessary to read and understand the source, and if that's too difficult it's a good sign that it may be best to avoid the dependency altogether.
That's not the only ideal for production code. Failing fast to get your system out of the "I have encountered a bug so I can't trust my own internal logic anymore" state as soon as possible can desirable too. In many situations, you would prefer your system to crash as soon as possible and be restarted by a supervisor process, rather than have all its components degrade-but-not-crash one by one until only the "yep, the process is still running" health check component still works (or worse, until it silently persists corrupt data or whatever). The bulkheads don't necessarily have to be _inside_ your rust program. :)
To answer your question, I don't think it is likely there will be a pure, safe Rust library for all of the Windows API owing to its breadth and complexity. The winapi crate is very good, but be aware that there are some gaps in its API coverage (I found this out about a third of the way into a project). The winapi crate also still requires you to call the relevant functions in the right places to free memory etc. I found that "roughing out" the functionality in C++, getting it working and then converting that into Rust worked for me when learning Windows programming, but it meant I had to re-learn enough C++ after about 20 years away in order to make sense of some of the examples from the Microsoft docs. YMMV. I would not call interfacing with the Win32 API "easy", even with C/C++; there are many inconsistencies and error handling is a pain (e.g. certain functions returning error codes which need to then be converted to different Windows error codes and so on). If your target Windows is recent (Windows 10 1803 or newer), then there is also [C++/WinRT](https://en.wikipedia.org/wiki/C%2B%2B/WinRT) which is wrapped in the [winrt-rust](https://github.com/contextfree/winrt-rust) crate as another option.
It only half works because you need the generic types listed in the impl but $gen should have been an ident and not a tt
Very interesting, I was just looking for something like this this week! You might want to mention Signal protocol in your README or description to make it easy to find. Even without having looked at the code itself, the comments from /u/james-darkfox ring true to me, especially making the Signal provider optional through a feature and providing a zero-allocation API.
Just to add to the answers you've already gotten: the language the Windows kernel is written in is irrelevant. All that matters is that it exposes its functionality via a combination of C and Stdcall interfaces. There are Rust libraries out there that expose C interfaces, not because they're written in C, but because nearly every language "speaks" C.
&lt;3 this week has been hectic for me, do i haven't had an opportunity to engage with you like I would want to. Hopefully next week I can find some time to dig into this. :-) And yeah, sorry for my strongly worded comment above. I was reacting to the GP's "insane" proclamation. 
The implication of your argument is the following: * Never use slice indexing. * Never call unwrap after compiling a regex, even when the regex is a literal string in the source code. * Never allocate using the Rust standard library. (Because am allocation failure aborts your entire process.) * Never use the boroow or borrow_mut functions in RefCell, because they panic on a borrow violation. * There are numerous methods on string slices that panic if you specify an index that isn't on a utf8 boundary. You can't use these either. * Probably a lot more. Now maybe there are certain particular circumstances where the above is necessary for reasons I don't understand. But it is clearly not necessary, in general, for building reliable production systems. Bugs exist and panicking when they occur is a good thing. Now maybe you have a high level crash recovery mechanism that doesn't allow the entire process to abort on a panic, and that's fine. Go for it. But this doesn't translate to advice "never panic." AIUI, this is not the OP's argument. They are saying, more broadly, that returning an error is useful because they write programs where panicking on channel send would _not_ be a bug, but normal expected behavior. Given that assumption, panicking is of course wrong. But that isn't the point. We're trying to decide on a default here, and this side talk about when it is appreciate to panic is muddying up the discussion.
Complexity and the word "simple" can mean so many different things. For example, the concept of a monad is quite simple, all you have is two functions and some basic laws around composition. In return, you can avoid having, and learning, the special cases of `try`, `async`, and an assortment of other effectful mechanisms. In Haskell they are even encoded as a library rather than as a language feature (aside from do-notation which could be done as a macro). Monads, and in particular the laws they come with, also provide good equational reasoning abilities which simplify thinking about your programs. Complexity is thus not just learning about concepts, its also about reasoning about existing programs and about their maintainability. Pure code and monads are important tools for reducing complexity and improving maintainability. In general, I believe that when there's a conflict between the simplicity of programs and the simplicity of languages, we should opt for the former and allow for richer type-systems. However, the good news is that expressive type systems tend to unify languages (as opposed to many ad-hoc constructs) such that they actually become conceptually simpler. As for marking paths as unreachable being simpler, I agree, but in a specific sense: it is easier to mark a function as being incomplete with `unimplemented!()`, `foo = undefined`, or "I haven't had time to make this part of the robust yet because I have a deadline to meet". This is simplicity in terms of effort and productivity. It fits a world where short term thinking is what matters. However, I want to build and encourage software that isn't throw-away and which lasts longer.
Yes, if you accept other tenets of C programming from that era: - there is no implicit sharing of memory between tasks: processes exist, what are threads? - panicking doesn't run destructors, what are destructors? In that model, robustness means tolerating the sudden failure of processes to exist and relying on os-defined cleanup actions (closing files, freeing memory). Concurrently shared mutable state only happens through io, and the file system is more-or-less transactional. (Those terms haven't been popularized yet, so it's a bit messy.) Shared memory is exotic; most in-core concurrency is expressed using the actors-and-channels model. Under that paradigm, our modern panic corresponds closely to fault signals and longjmp. The filesystem encourages a read-copy-update access pattern. Pipes are in fact used for communication between sync and async code. There's a working fork-join pattern; it just strictly enforces the rule of "no communication through implicitly shared memory". Oh, and instead of FFI, you have a stringly-typed tail-call construct which provides excellent isolation between programs: `exec` and friends. Really the only arguments against that paradigm boil down to "it's slow and inefficient to make the kernel intervene, especially when you're trying to communicate between multiple CPUs" and "but that's not how other operating systems work." The first one is convincing, don't get me wrong, but there's an element of "reinventing the wheel in userland" whenever concurrency is discussed today. Computing as a service, blurring the line between memory and io, multiprocessing - those are all ideas implemented by *Multics* back in the 70s. The wheel keeps turning.
I'm a noob. Can you ELI5? Isn't truncation basically removing some of the digits to the right? How can you go from u32 to u8 without removing those? Again, obviously a noob and dumb question.
They are not fringe cases. Every slice indexing operation and every assert are examples of such logic bugs and they are nowhere near fringe. At least not in Rust. And even so, no, we definitely should not always choose a compilation time error over a runtime error. Compilation errors are better, _all else being equal_, but it is so often the case that all else is _not_ equal because sometimes moving invariants into the type system requires a lot more ceremony. It can make APIs harder to understand. My point: this should almost always be phrased in terms of trade offs. Not absolutely.
Excellent observation. I'll noodle on this. I wonder if the next logical question is: _should_ channels be used for communicating non fatal cancellation backwards?
That text is a bit misguided in parts and wrong in others. If you have gigabytes of data, it does very well matter if you have to touch every single byte, or just pass the whole thing as-is (especially as single-byte access is extra expensive). A `uint32_t` is always 32bits long, no matter the platform. If youâ€™re using `int` in the C code, youâ€™re already doing it wrong in 99.99% of the cases, and the endian swap is not the culprit. I do agree that if you have a single 32bit value in some data transfer header, you should just use it like described in the article instead of detecting the endianess of the platform, though.
I love this reply, thanks for making all of these points explicit!
 impl&lt;'r&gt; Responder&lt;'r&gt; for Vec&lt;Comment&gt; {..} You can't. You must make a struct to wrap a `Vec`: struct PostComments(Vec&lt;Comment&gt;); 
You lost me around the `where`. So it implements a `new` for Z and you pass an argument that... is a function that takes an Optional String and returns... a Siamese tuple?
I've found it unconvincing. Endian conversions can show up in performance critical loops, and compilers appear to be quite bad in my experience at turning the bit shift logic Pike proposes into unaligned loads/stores. You definitely want the latter where possible.
So, I've tried different approaches on a toy pipeline problem here: https://github.com/matklad/glass-pipes It seems like the best solution is to return both errors and unprocessed messages from the broken pipeline section. That way, everyone has to care only about their own errors, they don't need to decide what to to with in-progress messages, and there's only one codepath for shutdown: messages stop flowing because the source of the pipeline stopped producing the messages. 
So, for example, I have a vector of four u32s. My function turns it into slice of 16 u8s.
Based on my experience using Delphi to call Windows API functions, you could pretty much have convinced me that the Winows kernel is written in Pascal. :P
The function returns a `Z`. The `(constructor)(None)` notation is just calling that function with a `None` argument.
Glad you enjoyed it. I'm patting myself on the back for making it to work on time after tapping it in. (Friends don't let ADHD friends Reddit before meds....)
This is not possible; assume you allocated the small variant, and then someone could just replace it with the large variant if it had a `&amp;mut self`, overwriting memory that is not allocated. Is trait objects a possibility? 
TIL
The WinAPI is the canonical interface to the Windows Kernel. Windows does have syscalls, but they apparently aren't documented and even change their numbers quite often. So, unfortunately, it would be an almost impossible task to do this without the FFI.
&gt; They are not fringe cases. Every slice indexing operation and every assert are examples of such logic bugs and they are nowhere near fringe. At least not in Rust. By fringe, I don't mean "rarely used by developers"; I mean that they signify a state in the program where continuing on is of little value. This includes logic bugs. I agree wrt. asserts denoted by a user in their crate (as opposed to asserts in the standard library to check invariants) because their very nature is to assert that a condition holds. However, *every* slice indexing operation is most certainly not an example of a logic bug; it could just as well be a number derived from user input that wasn't bounds checked for better user-facing error messages. I personally think it's unfortunate that indexing specifically panics by default since it's so ergonomic to use. I think we should investigate making the non-panicing version nearly as ergonomic. &gt; And even so, no, we definitely should not always choose a compilation time error over a runtime error. I'm not sure who you are arguing against here. I'm not aware of anyone saying that we should *always* favor a compile time error. Even in with full-spectrum linear-dependent typing you have to actually run programs unless all you want is a proof. When you run programs and there's user interaction, there will be incorrect inputs or the outside environment will have faults (e.g. say you have a PLC). Those inputs will need to be handled somehow, either by panicing by accident, or by handling them. I believe that either approach qualifies as "runtime error" but vary in terms of robustness. &gt; invariants into the type system requires a lot more ceremony. It can make APIs harder to understand. Certainly. Moving type system checks into run-time ones can also make APIs harder to really understand because reasoning about them becomes harder. It gets particularly hard and untestable if you mix panics and various global state. To borrow your phrasing: we need more *nuance*. &gt; My point: this should almost always be phrased in terms of trade offs. Not absolutely. Sure, and we should no doubt also be cognizant of values. I mainly want to argue for tracking effects at compile time rather than having them be *side-*effects. I don't think panics are sinful or anything (I use `undefined` or `unimplemented!()` *all the time*...), but it is helpful to know whether a function may or may not panic. Likewise I think tracking non-deterministic / impure things through monads or effect systems is, second to the idea of a type system itself, by far the most important aspect of software maintainability and correctness.
Interpreting Rust from a `str`? Not really. You'd need a reasonable Rust interpreter that supports that use (which currently doesn't exist) and you'd also need to retain a lot of information about your program and types which is either discarded or expressed in a form that's designed for debugging instead of interpretation. In theory all programming languages can be interpreted, but Rust's tools haven't been designed that way. You should consider an embeddable scripting language. Lua and Tcl are very popular for this kind of thing. I haven't looked too deeply into Rust-derived scripting languages, but there are some. I know gluon is very much like Haskell; not sure about Dyon off the top of my head. Also, be careful, very careful, about where those interpreted strings come from. It sounds like you're looking for a sandboxed interpreter and those are their own field of software engineering. (p.s. You may come across miri, the Rust MIR interpreter. It's a kind of interpreter, but it's designed for language research and testing Rust code - your code runs more slowly in miri than normal but miri can catch bugs in unsafe code. So it doesn't do what you want. It's more like a Rust-specific Valgrind.)
Interesting. I haven't seen that notation before. So where is the String you passed to it? It looks like you passed a variant.
I'm also confused about the Fn. It looks like A is a variant of Z, but you're saying that T needs to be a function and then you're calling A like a function?
I should have mentioned that this is a console application where I am reading input from a physical (USB-HID) device. 
&gt; Really the only arguments against that paradigm boil down to "it's slow and inefficient to make the kernel intervene, especially when you're trying to communicate between multiple CPUs" and "but that's not how other operating systems work." Actually, the most fascinating thing that I've observed using async Rust is that _everything old is new again_ and that I can successfully adopt many Unix interprocess architectures for my in-program architecture. This opens up all kinds of possibilities that I think Unix never fully realized. Tokio is actually pretty amazing, if you have a good enough reason to pay the price.
/u/ben0x539: I did a little experimentation, and I determined that you _can_ use `Drop` to handle cleanup-on-cancellation. [This code](https://github.com/emk/drop-async-examples/blob/master/src/main.rs) prints: START quick_task START slow_task START protected_slow_task END quick_task Result from the first task: quick_task result Dropping unneeded computations CLEANUP protected_slow_task I'll turn this into another blog post, but I wanted to follow up on your comments sooner. :-)
Fun fact dwijnand is employed by the firm that pretty much owns Scala. 
Yes.
Whatâ€™s the IDE that you use for this?
To clarify, I attributed a lot of strength to your argument because of your position on slice indexing. Effectively, I interpret "slice index shouldn't panic" as a fairly extreme position and wouldn't normally think of it as nuanced. Personally, I hope Rust never sees full blown monads or effect systems. Or at least, if we did, I'd want to hear a compelling argument for why the Rust ecosystem won't end up like the Haskell ecosystem. I hope you can discern my meaning here, even if you don't necessarily agree with it. &gt; Certainly. Moving type system checks into run-time ones can also make APIs harder to really understand because reasoning about them becomes harder. It gets particularly hard and untestable if you mix panics and various global state. To borrow your phrasing: we need more nuance. I agree w.r.t. global state and nuance. :-)
&gt;&gt; Being intended for C however necessitates that Rust code has to jump through a bunch of hoops to make it work, using C built-ins and unsafe blocks. &gt; &gt;It shouldn't really require that many hoops? Rust interfaces with C pretty darn easily. Easy might be a matter of perspective. Any use of unsafe is tricky to get right, and C FFIs require some familiarity with parts of Rust you typically wouldn't encounter.
&gt; The implication of your argument is the following It is, you are right. I had no idea Rust loves to crash to much. I can understand being out of memory, but trying to handle a utf8 string leading to a crash looks too harsh. Thanks for explanation. &gt; We're trying to decide on a default here, and this side talk about when it is appropriate to panic is muddying up the discussion. I know, but side discussions are natural. Sorry for distraction.
I looked at something like this a while ago. A good resource is probably looking at how cross-platform GUI toolkits do this. I mostly looked at Java, since I found it fairly easy to understand. The main problem is OSX, which is really "special" about some things. If you want an icon in the top bar then I think there are some restrictions, like you can't have a Dock icon as well. I don't quite remember, sorry if this isn't very helpful :/
For such a trivial use case (empty method) the best way to do this is with a provided default by `MyTrait` as already mentioned. If you really need to use macros, use a proc macro rather than a declarative macro and look into [`syn::Generics::split_for_impl`](https://docs.rs/syn/0.15.28/syn/struct.Generics.html#method.split_for_impl). This is the correct way to generalise deriving a trait over any type regardless of type generics, lifetime bounds and where clauses.
Well, that's why you wrap those interfaces with a safe Rust interface. Exactly like the rust std lib does for file access, sockets etc. It's really no different for Linux actually, since Rust's std depends on the system's C library by default. It's just the smoothest way to interoperate with system libraries, which Rust tries to make easy. The only real difference is that on Linux it's perfectly fine if you use the kernel's syscall interface directly because it's documented and guaranteed to remain stable, but on Windows that's not really something you should rely on.
Yuo are misunderstanding `impl Trait`. You cannot dynamically return different types from an `impl Trait`. You could instead use `Box&lt;dyn Trait&gt;` - which can dynamically return a instance of a trait - or return a enum containing either type.
/r/playrust 
From what I understand, `impl Trait` is opaque to the compiler, so two different values returning an `impl Trait` value cannot be guaranteed to be the same concrete type. The programmer may be able to infer two functions return the same concrete type, but rust does not infer function return types like it does expressions. The best solution here would be for `test` to return `Box&lt;dyn Tr&gt;`, which makes sense considering the concrete type of the `Tr` object could be either a `T1` or `T2`.
&gt; Personally, I hope Rust never sees full blown monads or effect systems. I think functors, applicative functors, monads, traversables, foldables, etc. in some form will come once we have GATs (but as crates in the ecosystem). Remember that aside from `do`-notation, Haskell's monads are actually just a library type-class + instances. The do-notation can be written as a macro since it's a purely syntax-directed desugaring to `&gt;&gt;`, `&gt;&gt;=`, and `return`. Also, I don't know why you hope never to see monads. They are a dirt simple (2 functions + some laws about composition) and quite useful concept that allows for good reasoning about code and they apply in surprisingly many cases. Functors even more so. We already have crates for monoids, semigroups, etc. so why should monads be any different? Moreover, we already have plenty of functors &amp; monads in the ecosystem, and indeed in the standard library. What we lack is the ability to generalize over `M` as a monad in the abstract. As for effect systems, we're already adding effects: - `const fn` (or rather this is the opposite of an effect, i.e. a *restriction*) - `async fn` - `try { .. }` The one thing that's lacking is the effect polymorphism and that is what https://github.com/rust-lang/rfcs/pull/2632 achieves. It's also important to have effect polymorphism for at least `const fn` because we want code-reuse and so that `for` loops can work in `const fn`. &gt; I hope you can discern my meaning here, even if you don't necessarily agree with it. I'll ask to you elaborate in any case re. "end up like the Haskell ecosystem". I believe Haskell packages mainly suffer from under-documentation and there are a lot of academically inclined people who actually do research with the language who make packages. That's both a strength and a weakness. Hackage is filled with amazing libraries tho. I think the Rust community is culturally different in some respects in that it's less research-y and that it values documentation highly. I don't think type system extensions have hitherto changed that and I don't think tipping towards effect systems will change it either. Monads and effects are not curiosities, they are useful for practical applications.
It is actually both: let x: () = Z::A; error[E0308]: mismatched types | 19 | let x: () = Z::A; | ^^^^ expected (), found fn item | = note: expected type `()` found type `fn(std::option::Option&lt;std::string::String&gt;) -&gt; Z {Z::A}` 
This is a fascinating argument, and one I largely agree with, but at the same time, I would like to present my preferred version of addressing all of these, from a perspective of writing mission critical software where an external DOS attack could lead to infrastructure failure. * *never use slice indexing* - yes, in certain code bases, I would consider them unacceptable, and usage would need to be converted to iterators or some checked access pattern * *never call unwrap after compiling a regex, even when the regex is a literal string in the source code* - absolutely. if it's a literal string in the source code, then it should be compile-time regex-compiled, and therefore have no risk of failing to compile at run time. Until and unless that is possible, I'm more than fine forbidding unwrap and require explicitly handling the possibility that somebody fat-fingered a commit and the regex no longer compiles * *never allocate using the rust standard library* like many other things, it depends on your code. I plan on extensively using tools like https://docs.rs/alloc_counter/0.0.1/alloc_counter/ in order to explicitly forbid allocations in critical code * *Never use the borrow or borrow_mut functions in RefCell, because they panic on a borrow violation.* - yup, I would avoid using these in certain code * *string slices* - just like slice indexing, I would disallow access patterns that could fail, and would require iteration over utf8 boundaries Ideally, I would love to see an alternative standard library (or functionally equivalent mechanism) that provided * make allocations explicit wherever used * uses non-panicking version of all calls * moves everything that could panick into a dedicated "unsafe" module. Now I don't that version should be the default for most use cases, but the fact that rust *can* be locked down very tightly in lots of ways is much of the appeal.
 fn t1() -&gt; impl Tr { T1 {} } fn t2() -&gt; impl Tr { T2 {} } if b { t1() } else { t2() } `t1` returns *some* type implementing `Tr`, likewise does `t2`. Note that two types both implementing `Tr` aren't necessarily the same. Then in the then-clause of the `if`-expression you are making the function return *some* type impl `Tr`. Therefore the type of the else-clause must be the same (the return type of functions must be known at compile-time). However, as said, the two *existensial types* aren't necessarily equal thus the compiler complains.
I want to have a decimal type that provide limited precision (but it should be %100 accurate). I'm not looking for lots of precision but efficiency/performance is welcome. Anyone knows the best library out there?
I really do not need a lesson on monads. I understand what they are. I'm not afraid of them. I leverage them all the time in my code. Where I stop short is introducing them as first class polymorphic abstractions in APIs unless they are exceptionally well motivated. They are so rarely well motivated at that level that I'd probably prefer they not be possible in exchange for avoiding their tempting abuse. Otherwise, I'm really, truly, completely not interested in debating monads with you. There is literally nothing you or I could say that would be novel on the matter because, as you know, the topic has been beaten to death. If you don't already appreciate the immense complexity they bring to building and consuming APIs already, then I am certainly not equipped to convince you otherwise. &gt; We already have crates for monoids, semigroups Yes, and they are thankfully rarely used. &gt; The one thing that's lacking is the effect polymorphism Yes, that's what I mean. I either hope such a thing never happens or hope that if it does, it's done in a way that doesn't infect the entire ecosystem like monads have for Haskell. &gt; I'll ask to you elaborate in any case re. "end up like the Haskell ecosystem". You've already guessed it. It's not an interesting position on my part. Lots of people hold it. Haskell libraries are infected with layers of abstractions. Separate from documentation quality, this makes them harder to use in my experience. &gt; I think the Rust community is culturally different in some respects in that it's less research-y and that it values documentation highly. Yes. For me personally, I very carefully consider the addition of any generic API or the addition of any type parameter. I _generally_ treat those things as costs in my API's complexity budget and try to justify them carefully. In my experience, this delicate balance is almost completely missing in the Haskell ecosystem. It's part of its charm, for sure, but that doesn't mean I have to like it. :-)
The `impl Trait` feature cannot be used to return different types depending on the situation, only `Box&lt;dyn Trait&gt;` can do that. One common solution to avoid a Box is to create something like an [`Either`](https://docs.rs/futures/0.2/futures/future/enum.Either.html) type instead, and return different variants of the same enum depending on the concrete type you want.
Thanks for the reply. Yeah, to be honest, I've never come close to programming in an environment like that. It sounds _extreme_ to me, which is totally fine, but I think definitively outside the scope of determining whether sends on closed channels should panic or not. :-) I think in extreme circumstances such as that, it's perfectly reasonable to drop the standard library and build up your own stuff. In particular, the development cost of programming under those restrictions is likely quite immense, so it makes sense to help reduce it by building up a better std specific to your use case. The other thing I didn't see you mention is whether cash recovery would be acceptable. e.g., Have a manager of sorts that catches panics that might allow one task to fail but keep the entire process up. Of course, this depends entirely on the granularity with which you want to handle the error.
I recall seeing such cryptic errors when the following occured: * my crate depends on lib version X * one of my dependent crates depends on lib version Y
Not what you've asked for, but a guide why you should not create tray-only apps. [https://blogs.gnome.org/aday/2017/08/31/status-icons-and-gnome/](https://blogs.gnome.org/aday/2017/08/31/status-icons-and-gnome/) This is related to GNOME, but there are also links to Windows and macos guidelines in the article. 
It depends on the software you are writing. I encountered a bug in a window manager where an assert would make your whole session crash (potentially freezing your whole GUI) because some opengl vertices are transformed 2 times when two specific effects run at the same time. In this case I'd rather have vertices that are incorrectly transformed (nobody would probably notice or even care) than to have everything crash because the code is not 100% correct. Same thing with OS kernels, crashing should ALWAYS be the last option.
At the lowest level this usually means using select/poll/epoll or similar, and using the timer on that call to return control to your code to handle whatever timeouts are active. \`mio\` wraps this (although Windows support apparently needs rewriting, from looking at issue comments). I've built my code straight on \`mio\`, and implemented my own timer list and so on, but you could look for something higher-level that handles all that. Doing it with a separate thread for input and a thread for each timer might also be possible, with \`Arc\` everywhere. Or perhaps having a thread for input and a separate main thread and using channels (see crossbeam, e.g. \`after\` channel and \`Select\`). I've not tried that.
https://keminglabs.com/blog/building-a-fast-electron-app-with-rust/
&gt; I really do not need a lesson on monads. Nor do I want to provide one. &gt; I understand what they are. I'm not afraid of them. I leverage them all the time in my code. I assumed so. &gt; They are so rarely well motivated at that level that I'd probably prefer they not be possible in exchange for avoiding their tempting abuse. Well we do emphatically disagree on this part; I find them to be well motivated all the time, especially functors. But... I have a low tolerance for non-code-reuse and repetition. &gt; then I am certainly not equipped to convince you otherwise. Likewise! ^,- &gt; it's done in a way that doesn't infect the entire ecosystem like monads have for Haskell. If you want e.g. `const fn`s to be widely usable in the ecosystem then I think there's no choice but to have `impl const Trait for Bar { ... }` permeate the standard library (esp. libcore) and important sectors of the ecosystem. But we try hard to find a syntax that is as non-invasive as possible, that's why it has taken a long time to even get to an RFC. &gt; Haskell libraries are infected with layers of abstractions. I think you can guess I view this as great thing.. ;) As Edward Kmett put it, Haskell is where OOP's promise of code-reuse actually comes to fruition and I think that's true of Rust as well to some extent. This is the opposite of the feeling I get from languages like Go and that's where I'd never want to be myself. &gt; I generally treat those things as costs in my API's complexity budget and try to justify them carefully. I agree that generic APIs have a cost (which is mostly up-front) but so does the lack of generic APIs since they require relearning common patterns. I think this is part of what makes language design hard; things can be both costs and the opposite at the same time and to different people. &gt; In my experience, this delicate balance is almost completely missing in the Haskell ecosystem. I know there are a non-trivial amount of people who take it as a badge of honor to only use Haskell98 and who refuse much of the GHC extensions that exist. Overall I think you are right tho. I think one of the reasons why Haskell APIs tend to be highly generic is global type inference. It's quite easy to just omit the type of a function, just ask for the type with `ghci&gt; :t myFun`, and then attach that type to the function. When I write Haskell, I tend to use that a lot and it's pretty great for exploratory rapid-prototyping-sorta programming. &gt; It's part of its charm, for sure, but that doesn't mean I have to like it. :-) To each their own! :)
Another option if you don't need a whole event loop, and you are okay with relatively heavy weight OS threads, you can always spawn a thread, and wait for a message to come back signaling completion. See [this link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=06408085991fd872d875fd196c7369b5). Additionally you could put your structure in an Arc&lt;Mutex&lt;T&gt;&gt;, so you can directly call the function you want, but this may be overkill for what you need. 
Yeah, back when I wrote Haskell more frequently, I'm pretty sure I had a lint (or equivalent) enabled that required annotating the types of top-level items. A lot of people don't do that though, which is sad. Don't get me wrong, I love sophisticated type system features as much as the next person. But their aggregate complexity is _bewildering_, and it can be very difficult to see that if you aren't constantly in contact with people who don't go out of their way to study type system features. I personally was fortunate enough to have a [great mentor](https://www.cs.tufts.edu/~nr/) that helped me with a lot of this stuff over a period of several years. But once I started collaborating with others that didn't have a similar background ("Oh yeah, I spent a semester learning some weird ML language in college"), the _immense_ difficultly of sophisticated type systems became readily apparent. Perspectives and environments change a lot. Here I am making a stand against pushing certain kinds of runtime invariants into compile time invariants, but at work, I often take up the opposite position because we don't do enough of it.
Huh, interesting... So is Rust overloading (or something like that) behind the scenes? As in "if he doesn't call this like a function then treat it like an enum"?
If you use VLC media player, you can also correct subtitle timing with the G and H keys.
The enum is `Z`. Each variant (`Z::A`, `Z::B`) is a constructor of `Z`, and for tuple structs, those are functions. There's nothing being overloaded: `Z::A` always indicates a function. `Z::A(...)` calls the indicated function, and `Z` is the return type of that function.
Everyone's answers here are good, but I think there's an opportunity here for a better compiler error message, especially since that's one of Rust's strengths in general.
But that's because interoperability with C was a major goal of Pascal implementations. Fortran is in similar position, but it's old enough that it's arguable whether C or Fortran gets dibs on "the standard 'C' ABI."
I wrote a tray-only program for Mac, and paired it with a modified version of the systray crate for Windows. No Linux support, as I never use a GUI in Linux. And no pop-up support. Have a look if you want a reference for Mac: https://github.com/mrmekon/connectr The menu interface is in `src/lib.rs`, the Mac implementation is in `src/osx/`. I also have a library for "trampolining" Rust programs into proper Mac App bundles, which you might want to use in conjunction: https://github.com/mrmekon/fruitbasket 
I can't think of anything clearer than "found a different opaque type." The types don't match, but the compiler can't tell me what types they are because the source code doesn't give them names.
"Functions that return impl types must still return a consistent implementation You are attempting to return two (or more) different impl types in your function. [E9876]"
Please note that I am by no means an expert in these things. However what I think is going on is this: Each socket counts as an "open file". Since you post to nginx asynchronously, the number of sockets that will be opened is essentially unbounded. Which means that if new message arrive fast enough (or `/the-endpoint` is slow enough), you'll probably run into this error. I'm not sure how to best implement this, but limiting the number of concurrent requests would probably work. An alternative explanation would be that some files / sockets are not closed (dropped) properly.
 &gt;I get this error even when I change `f2` That's the key to understanding. If that worked, it means you can change whether or not a use of a function typechecks without changing its signature. Existential return type means "Define a new type for every possible combination of type parameters. It implements these traits." The return types of f1 and f2 aren't equal, so Rust won't let them merge at the if statement.
Yep, you're definitely spot on! I just wanted to clarify that there is not one single maxim to achieve reliability that would determine such a decision for every possible rust program out there. It always "depends", both on big picture kinda stuff like what environment your program is going to run in and on details like what specific invariants your program structure relies on. I'd argue that even for a window manager, once you're not looking at ephemeral cosmetic glitches but larger state machine/logic bugs, you could make a case for "failing fast", terminating and letting the display/session manager start another user session, rather than risking, for example, getting the user stuck in a session with broken input devices and no way to exit short of a hard reboot. But looking at stderr of the window manager of my current desktop session, I definitely don't want it to exit every time something looks wrong. :) also your name is a bit weird
Maybe those squiggly under the source code could help. Something like â€œopaque type (impl Tr) defined here on line x.... does not match up with opaque type (impl Tr) defined here line y...â€. The current message seems really confusing since it basicslly says â€œexpecting type X but got type Xâ€.
What is the point of using something as a trait bound if the trait has no methods to actually call? The compiler itself uses a few of these for marker traits (Send, Sync, Sized, Unpin), but these are there to indicate fundamental properties of types.
&gt;â€½ Nice interrobang.
It makes sense to me that Pascal implementations for Windows would go out of their way to support the local calling conventions. At the time I was mostly surprised that despite feeling rather different as a language, Delphi ended up with a roughly compatible enough ABI surface to make the WinAPI interface feel rapther natural and not so much like going through an FFI. Of course, Delphi is a pretty late Pascal and really in love with Windows, I have no idea how earlier Pascals worked.
Itâ€™s more of a way of grouping up the data. Like if I have different types and I want to group them all into one thing but never actually use polymorphism on them 
&gt; A lot of people don't do that though, which is sad. Was it `hlint`? I don't recall if it complained about type signatures. I haven't written much Haskell for some time, but when I did, I always used global type inference as a provisional development aid (and only a few globally inferred functions at a time) rather than something that can be pushed to `git`. When I was a TA for a Haskell course I did encourage students to use type signatures on functions when they omitted them. &gt; But their aggregate complexity is bewildering, and it can be very difficult to see that if you aren't constantly in contact with people who don't go out of their way to study type system features. I am fortunate to be studying at one of the world's leading universities wrt. FP/Types/PLT but I think most of the important bits from a practitioners POV (as opposed to lang designer) I learned at the first functional programming course. I got pretty hooked and considered Haskell a salvation after being "force-fed" (hyperbole... but...) Java for 1.5 or so years. &gt; But once I started collaborating with others that didn't have a similar background ("Oh yeah, I spent a semester learning some weird ML language in college"), the immense difficultly of sophisticated type systems became readily apparent. That might be a US thing? To my knowledge "a semester of ML" is not a European phenomenon - Haskell is likely much more used here, especially in the UK. I think one aspect that makes Haskell, ML, etc. more difficult is that they swim against the tide of most commonly used and taught languages (e.g. Java) both in the sense of being more strongly typed but also because of the imperative/functional divide. However, Java's type system is by no means simple (at least from a type theory POV, and having been a TA for both Java and Haskell courses, I think also from a user POV), it's probably more complicated than most of Haskell. Personally, what I find difficult is thinking without types... JS &amp; co.-programmers must be geniuses or something... I don't know how you can hold so much context in your head at once. Even more difficult for me is things like statistics, machine learning, etc. I would also trade complicated types for `unsafe { .. }` any day... that stuff is really hard. &gt; Perspectives and environments change a lot. Oh yeah; when I talk to folks doing research on HoTT, Agda, etc. my head starts hurting too.
At a higher level, what problem are you trying to solve? The most likely answer is that if you don't need the polymorphism, then don't use it.
Is this it? Unless you are sure you are posting to http://127.0.0.1/the-endpoint maybe this is it https://github.com/hyperium/hyper/issues/1422
1) the second part is absolutely true and I agree entirely. 2) gl-rs is just a bindings crate. Every function just does whatever the C version of that function does.
If the Rust compiler sees that the two types are the same it accepts it. For instance, two slice iterators mapped over the same function are the same type and work with impl Iterator, even if they are from different slices. This isn't the case in OP's example, but just clarifying this.
The time where I did this: I had a vector but I needed the vector to be of these custom types that I made, each of them different. The structs I had were number, character, and list. I needed a vector to hold those, so I used trait objects for them. I implemented the trait onto them all and made a vector with the trait object.
I think that would be considered bad practice. Consider looking at the alga crate if you are interested in abstractions over linear algebra primitives. See nalgebra for implementations. The traits should define the behavior that you need. If you truly never needed any behavior then it is not necessary to create a trait bound. Only create trait bounds at the site they are required.
I did see that issue before when I was troubleshooting it. I was using DNS for the post but moved it to 127.0.0.1 because of that issue. It really does sound like that issue because once it shows the error, it never recovers (â€œtaintedâ€). Iâ€™ll be sure to triple check though. Looks like there is a workaround in the upstream bug report. Either that or I might try throwing the Tokio core away and starting another. Finally I could see about upgrading glibc. Lots to try! Cheers!
My thought is that the first theory is the likely one since the error tends to happen during spikes of messages. An obvious fix (I would think) is to just raise the file limits. Other possible fixes are rearchitect the binary to reduce socket use. Either by a buffet to help smooth the spikes, or fewer longer lived connections to nginx. I am not really sure. Anyway, really appreciate the response.
Okay thank you for your insight on the bounds. What about what I did with vectors? Is that acceptable to do? Just passing in trait objects of them?
I would not recommend using vectors dynamically. I would use monopromorphization via type parameters or just accept a concrete type. On the other hand, I don't know your specific use case. It is entirely possible that you need dynamic dispatch to hold different vector types, but I couldn't say without knowing the situation.
It looks cool and thanks for your hard working! I also made an attempt to wrap the C headers in [torch.rs](https://github.com/SunDoge/torch.rs) but it was far from complete. I'll try your crate these days.
Great insight. Thanks for this, time to look around at what monopromorphizafion is!
No, I mean, go up a higher level than that. What is the actual problem you're trying to solve? Taking about vectors and structs is a detail, not the problem. The reason why this is important is because of the XY problem: http://xyproblem.info/ &gt; The structs I had were number, character, and list. I needed a vector to hold those, so I used trait objects for them. I implemented the trait onto them all and made a vector with the trait object. Have you considered using an enum?
What I am doing is writing a small scripting language. I am storing all of the variables created into a vector, and I have a variable trait. The variable trait gets implemented for my types that I created, then I put the trait objects into a vector. Creating a variable struct wouldnâ€™t work because different variable require different fields, and different generic type parameters, so using empty trait objects seemed like the only option.
No expert but I'd check the process list for any excess dns processes, and then check if your /etc/hosts and /etc/resolv.conf aren't weird. Can those errors mean anything other than the client is getting too chatty with something (or just opening too many files, which I'm sure you are not). Oh and run lsof and netstat for suspicious mountains of output.
Will do! Those are great suggestions, thanks.
You could use a variable enum for each of the types of the variable.
There is an Int MySQL field for logs (from back then)... I have no choice to put it inside it with some other stuff
Thank you so much
Has to be an Int, and can't use a crate unfortunately
Sounds like a textbook use case for an enum.
Read this, [Scaling to 12 Million Concurrent Connections: How MigratoryData Did It](https://mrotaru.wordpress.com/2013/10/10/scaling-to-12-million-concurrent-connections-how-migratorydata-did-it/). Please report back if you have found a solution.
Anyone happens to have a working example in Rust? Thanks so much guys for the help 
Huh, so Enum constructors are bona fide function pointers? TIL. This is neat!
That one would be great! It explains the specific problem, and it also has its own error code, so `rustc --explain` wouldn't bring up the `let x: i32 = "hello";` type mismatch explanation.
\&gt; there are Responders for Result&lt;T&gt;, Option and Vec. &amp;#x200B; To clarify, there are Responders for \`Result&lt;T, E&gt;\` if \`T\` is a \`Responder\`, \`Option&lt;T&gt;\` if \`T\` is a \`Responder\`, and for \`Vec&lt;u8&gt;\` only and no other \`Vec\` of anything. &amp;#x200B; It's not at all reasonable for Rocket to automatically implement a \`Responder\` for all \`Vec\`s, since there is no one-size-fits all way to serialize a list of arbitrary data. &amp;#x200B; /u/WellMakeItSomehow has the right idea - use a newtype and implement \`Responder\` for the newtype.
I remember trying to do something similar in the past, and being annoyed because GTK deprecated their tray API specifically because it's so hard to create a unified cross-platform interface for it. Every DE has their own way of handling things resembling a tray icon, and they all have different features, so it's really up to the developer which of those features they want to use in each DE.
Is there a place where one could follow this? Sounds nice!
Well, not always. If you're pattern matching, you can use `Z::A(X)` as a pattern, but the same doesn't work with a function - rustc would have to compute the inverse of an arbitrary function to be able to do that!
&gt;Well, that's why you wrap those interfaces with a safe Rust interface. Exactly, but it's easy to forget how much work that is when you're new to both Rust and FFI. You need to learn a new Rust dialect to be able to translate C things like pointers, manual allocation, zero-initialization of structs and similar, to unsafe Rust. I'm not saying any of that is a huge deal (I've done it for the [subprocess](https://crates.io/crates/subprocess) crate fairly quickly), but it's not pleasant either, doesn't feel like you're doing Rust, and you won't find many tutorials explaining how to do it. For some reason unsafe [POSIX](https://crates.io/crates/libc) APIs feel more approachable when used from Rust. Also, with POSIX there are crates like [`nix`](https://docs.rs/nix/0.13.0/nix/) that do the wrapping for you, so you just go ahead and use them. With Win32, you are pretty much on your own.
I view panics as "programmer gone goofed". They signal logic errors. With a channel, frequently the sender and receiver end up in far away corners of the program. And closing the channel can happen when the receiver drops. With an array index, I could check the length beforehand, and thus it being out of bounds is a logic error and should panic. But with a sender, there's no way to check before sending that the channel is closed (the receiver could be dropped in-between the check and the send). I pretty much never unwrap on a `send`, but that may be because I'm usually dealing with networking, where the peer can disappear at any time against my will. Usually it does, and so then the receiver is the one in control. The receiver dropping is a way of propagating cancellation/disconnection backwards. I always instead want to check if `send` failed, and if so, shut down cleanly, not crash the whole thread that could have thousands of other connection tasks running.
That's why the recievers that errored, must keep `recv`ing (but dropping) messages.
You need to write migrations yourself. Having them work automatically in Django is just a huge pleasure. The other, more "magic" features of the django ORM are not as critical in my opinion. I can do without all the kwargs hacks (those won't work in Rust anyway).
Have you increased the ulimit and kernel maxfd (Linux, mac has similar but differently named functionality)? Google for info on how to do this--it's necessary to scale IO heavy workloads.
&gt;I can't think of anything clearer than "found a different opaque type." How about replacing "opaque type" with "opaque type defined at line xx"? I think that would make it: * not sound silly, because the descriptions of "different" expected types would actually *differ* * more user-friendly, because it would do a better job of describing the actual problem - opaque types defined at different places in the source not being the same type
That error could still be confusing because the same function can define multiple concrete types if it is called with different type parameters.
&gt; starts a Tokio core and for each incoming messages on the channel This is the part that sounds problematic to me. Each reactor will have its own associated resources including at a minimum an epoll fd and a unix pipe for signalling. You should just have one Tokio runtime for the receiving end of the mpsc channel.
You saved my day :)
Yeap. Data model in `rdedup` allows me to just panic and not worry. Might not look best, but it's safe and easy. Example for handling errors: https://gist.github.com/4ee6ae9618b4f0c61308bbad41629c1d (I haven't tried or anything - just the concept)
In developing this, I was hung-up on managing the lifetimes of my stdout/stderr locks. Eventually I figured I figured out I need to wrap the locks in their own block to ensure they don't outlive the underlying writers: ([source](https://github.com/mattmahn/gitall.rs/blob/43d74b2554c7d7370fafe706ae7ec300626c48c6/src/main.rs#L89-L98)) let stdout = StandardStream::stdout(output::color_choice(&amp;color_mode, &amp;atty::Stream::Stdout)); let stderr = StandardStream::stderr(output::color_choice(&amp;color_mode, &amp;atty::Stream::Stderr)); { let mut stdout_l = stdout.lock(); let mut stderr_l = stderr.lock(); for gr in rx { output::print_git_result(&amp;mut stdout_l, &amp;mut stderr_l, gr).unwrap(); } } Could someone help me understand why I need wrap my stdout/stderr locks in their own block? Shouldn't the compiler know to drop the locks before the underlying StandardStream? If I remember correctly, even adding `drop(stdout_l)` &amp; `drop(stderr_l)` didn't work.
It sounds like IEEE 764 decimal floating point arithmetic would fit the bill. I'd look at https://crates.io/crates/decimal
This is the tricky bit: loop { if quit.load(atomic::Ordering::Relaxed) { break; } channel.send("work".into()).unwrap(); } Above, you check `quit`, and then try to send to the channel. But it's possible that the receiver might shut down after the `quit` check but before the `send`. In fact, if the channel is full and the `send` blocks, then there may be quite a long wait between the `load` check and the moment `send` fails. There's another trick with `select!` that makes this check atomic. There should be a link in the OP. I'm just not sure that if we need to wrap every `send` with a check (atomic or not), does it buy us anything over just having `send` do the check for us?
I guess the aspect I'm worried about is that forcing cleanup logic into a `Drop` impl makes for a very forced cancellation, where I can't do anything that is itself async anymore. Is there an idiomatic equivalent to go context cancellation where you basically know that, say, your main objective isn't valid anymore but you might still need to do some blocking channel writes to ensure an orderly shutdown, or perform that cleanup involving async/blocking operations?
How should I react as someone who uses a different desktop environment like i3? How is a dedicated notification API supposed to work when the desktop doesn't support it?
marker trait
I use i3 as well, most tray applications work normally.
&gt; But it's possible that the receiver might shut down after the quit check but before the send. No. Receiver must never quit for reasons other than all senders being gone. That's the rule of the whole architecture. :) 
I have just googled and found this. https://github.com/qdot/systray-rs 
Yes, there is only one core. Sorry for the unclear description!
I think you have to install the Rust language server extension RLS.then add these features: #![allow(unknown_lints)] #![warn(clippy)] you your crate
The extension you're using has support for clippy, it will run it with rls and highlight things in the editor. Try searching for clippy in your vscode user preferences, there's some option to enable it.
So you store your variables in the vector, but then how do you use them? If someone gets a hold of that vector all they can see is `&amp;dyn T`s, and `T` has no methods they can use...
I think this is the very thing that [non-lexical lifetimes](https://github.com/rust-lang/rust/issues/43234) aims to address.
example doesn't work :(
It's not forced into `Drop`, you can cancel manually as well, like `rx.close()` or `tx.close_channel()`. However, `Drop` just enhances everything, such that even if you forget to explicitly cancel, most things will cancel automatically. That's not true of Go channels.
Yup this turned out to work. Just had to search "clippy" in my preferences and install clippy via \`rustup component add\` (in my case, \`clippy-preview\` because I'm on Mac/darwin).
&gt;It is entirely possible that you need dynamic dispatch to hold different vector types, but I couldn't say without knowing the situation. They aren't using dynamic dispatch because the traits have no methods.
Can't your OutputStream struct contain a Connection that you can open when you create it?
"... attempting to return _multiple_ different `impl Trait'-style types from the same function."
But he's on the 2018 edition, shouldn't he have nll then?
[Here's what I did](https://github.com/Freaky/cw/blob/ab173b962442426f52fe78f57ae927dcc98a8373/src/count.rs#L87-L129) in my [wc clone](https://crates.io/crates/cw). I have a Counter trait, and multiple implementations supporting different features. They're brought together in an enum which also implements the trait in a fashion that dispatches to a concrete implementation. All wrapped up in a macro to keep the boilerplate down. There are crates like [enum_dispatch](https://crates.io/crates/enum_dispatch) to do this sort of thing for you. Trait objects would probably have been simpler, but would have meant getting rid of the generic parameters.
Why are you trying to run it as super user? Rustup should be ran as a regular user and it should install the toolchains into your user home folder.
The `enum` approach can also be automated by some crates which provide proc-macro based solutions. An advantage of this in your case is that the impl of the enum is specific to the function itself. [One such crate is `auto_enums`](https://docs.rs/auto_enums/0.5.1/auto_enums/) (caveat, I haven't tried it myself)
That doesn't mean it isn't a function, it just means it's a special function that is designed so that the compiler can trivially compute its inverse.
I was just about to say the exact same thing. Glad to see someone else having the same thoughts. :)
&gt; Being intended for C however necessitates that Rust code has to jump through a bunch of hoops to make it work, using C built-ins and unsafe blocks. Doing it without the C libraries would be jumping through worse hoops (for the most part). On Linux, it would be totally possible to skip the C standard library and make direct system calls. But it has to be done in assembly. And it has to be implemented separately for each processor architecture. (There's an inactive project called [steed](https://github.com/japaric/steed) that aims/aimed to provide std on Linux without using libc). On Windows, it's the same but worse, since the system call ABI is undocumented. And I don't know how stable it is, relative to Linux. Now, there are totally advantages to doing this, at least on Linux where there's a stable and documented system call ABI, but it's quite complicated and doesn't help *that* much. But in the end, whichever way it's implemented, safe and native Rust APIs can be written to wrap it, so the average programmer doesn't really have to worry about whether it's going through a C API or directly using assembly.
That was my thought too, but I've been running into rather persistent ownership issues (because the Connection is a reference rather than an owned value and doesn't implement `to_owned` or `clone`. What I **want** to do is: struct OutputStream { pubsub: redis::PubSub&lt;'static&gt;, } impl OutputStream { fn new() -&gt; Self { let client = redis::Client::open("redis://127.0.0.1:6379").unwrap(); let mut con = client.get_connection().unwrap(); let mut pubsub = con.as_pubsub(); pubsub.subscribe("timeline:1").unwrap(); OutputStream { pubsub } } } impl Stream for OutputStream { type Item = String; type Error = std::io::Error; fn poll(&amp;mut self) -&gt; Poll&lt;Option&lt;String&gt;, std::io::Error&gt; { let pubsub = &amp;self.pubsub; match pubsub.get_message() { Err(_) =&gt; Ok(Async::NotReady), Ok(msg) =&gt; Ok(Async::Ready(Some(msg.get_payload().unwrap()))), } } } But, of course, that won't work because `pubsub`'s lifetime isn't long enough (it doesn't live past the `new` method). I've tried to get around this ownership issue with various combinations of `Box`, `Arc`, and `Mutex`, but so far haven't had any luckâ€”which makes me wonder if I'm approaching this from the wrong direction (in my experience, lengthy fights with the borrow checker are _usually_ a sign that I'm coming at something from the wrong angle). Did you have a different idea for how I could have `OutputStream` contain a `Connection` without running into the same barrow-checker issues?
There is a bug in glibc that $&amp;%$%^(s) up DNS resolving after you have reached "too many open files" once, and then it never recovers (at least that thread - the bug is thread-local). It has been solved in the latest glibc, but it's definitely there in Ubuntu 16.04. There was a bug report against the hyper http client code for this: [https://github.com/hyperium/hyper/issues/1422](https://github.com/hyperium/hyper/issues/1422) But it actually is a bug in glibc. The rust-lang bug is still open since there might be a work-around, but for now the only working fix is to just raise the open fd limit to a million or so. [https://github.com/rust-lang/rust/issues/47955](https://github.com/rust-lang/rust/issues/47955)
&gt; You could instead use Box&lt;dyn Trait&gt; - which can dynamically return a instance of a trait - or return a enum containing either type. Is there a performance benefit to either method?
Yeah, I certainly don't understand all the implications yet. Here's my [second blog post](http://www.randomhacks.net/2019/03/09/in-nightly-rust-await-may-never-return/) discussing these issues.
Just fyi I totally agree with you here and am kinda playing devils advocate. I'd love to see HKTs in rust if they can play nice with lifetimes. Then we can bin async, ?, et all and just use &gt;&gt;= ðŸ˜
If that's all of your main function, you're spawning two threads and then immediately go to end of main, thus quitting the program. You should call \`join\` on handles returned by \`spawn\` to exit program only after all threads exit.
libappindicator is an implementation of the [StatusNotifierItem](https://www.freedesktop.org/wiki/Specifications/StatusNotifierItem/) spec (the successor to doing tray icons by plopping a subwindow into the tray via XEmbed) with an XEmbed fallback and, unless it's changed since I last checked, it artificially limits users to Unity's non-standard HIG. Specifically, rather than Unity binding "secondary action" to left-click, they bound primary action to both left and right click, bound secondary action to middle-click, then wrote a library (libappindicator) which uses the statically-typed function signatures to force applications to bind the context menu and main window toggle backwards. Which means that: 1. If you run libappindicator applications under KDE, the applications using the old XEmbed approach or [KStatusNotifierItem](https://api.kde.org/frameworks/knotifications/html/classKStatusNotifierItem.html) will bind the traditional "left-click for activate, right-click for context menu" behaviours while libappindicator icons will get it backwards. 2. Unity may not allow XEmbed icons, but if you run KStatusNotifierItem applications or applications based on one of the alternatives to libappindicator written by people who don't like that artificial limitation, they won't match Unity conventions and you'll have to middle-click to access the context menu. (Yes, middle-click... because they bound "primary" action to both left and right click to try to force "left-click is a context menu" while still preserving the muscle memory of people who access context menus via right-click.) It's the reason I never use libappindicator in my own creations and either opt into the XEmbed fallback in applications which offer a choice (eg. Deluge) or go looking for alternatives.
Sweet! I also have this problem. I look forward to checking it out when I'm not on billable time. :)
There is indeed a risk of a sender being dropped due to an error or panic, where you would want the receiver to know it exited badly. For example in HTTP, if the sender were feeding a chunked transfer, a close would imply the transfer is done, even though it might not be the full transfer. I don't think it's common enough to be the default, but in those cases, you can wrap a sender in a guard that will send an error on drop, unless you tell the guard it can close cleanly.
I would imagine that the enum would be more performant, as it would not have to do heap allocation and would be more easily predictable by the CPU. There was [a thread some time ago](https://www.reddit.com/r/rust/comments/a7n5hb/enum_dispatch_speed_up_your_dynamic_dispatched/) discussing a macro for easily generating those kind of enums over traits objects.
Hi, Too many open files means that you are opening too many http connection. Make sure to use a connection pool with keep alive. Hyper should already give you a connection pool hence you don't need to create a new connection every time but you should keep using the same again and again. Then make also sure to set the keep alive header, hyper wont do that for you. Also it is a good idea to try to update all your dependencies, several progress have been made in the last few months. Finally, 150k requests for minute is not that high, of course it depends on the hardware but a reasonably beefy server should handle it without many issues. Make also sure to have a connection pooling on the redis side! Cheers
Locks in rust are implemented by having a method that returns a guard. While you hold that guard you hold the lock. There are no unlock methods, you just drop the guard. Going out of scope in a block drops the guard and releases the lock. You can also do it manually by dropping the lock. If you don't do that the lock stays in scope. If rust automatically released locks when the guards were no longer used you'd introduce locking bugs by running code that should be run under a lock without it.
As far as I understand, NLLs only apply to borrows because you want to drop to be called deterministically on structs implementing Drop
That seems to work nicely, thanks!
If you only need to store objects of different types in single vector - just use trait implemented for all types (almost) - `Any`. But it is extremely rare when you need it actually.
What you want is [`futures::task::ArcWake`](https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.13/futures/task/trait.ArcWake.html) (Also available as [`futures_util::task::ArcWake`](https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.13/futures_util/task/trait.ArcWake.html)). The reason why `ArcWake` isn't the default interface to `Waker`, that's because `Waker` is supposed to be usable in `no_std` environments while `Arc` is only present with `std` or `alloc`. Since `Waker` is really all that's truly needed to compile and execute futures, the more convenient wrapper got pushed to the `futures` crate as an optional (but highly recommended) utility. `core::{task, future}` exist as a minimum implementation to compile `async_await`, while anything that wants to do anything useful with them should use `futures::{task, future}`.
I use [VS Code](https://code.visualstudio.com/) with the Rust RLS (Rust Language Server) plugin. 
thank you. Did that let p = thread::spawn(.. p.join(); for the second the same. rocket doesn't seem to start, or it's been stopped. The program ends, when my profiling background action stops.
Omg I Just started a project to solve this exact problem IN RUST (I ended up using a pretty simple bash script) I will contribute if I can ;)
I made a proof of concept that I *think* has a safe interface, and supports Drop, Debug, and Clone. It's pretty messy, but it shows that its possible. [Playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a0165512292130dab3fe7abad8116410)
On the topic of using far less memory, perhaps that makes Rust a good candidate for creating a distributed computation framework for IoT? I could see certain instances where this could make sense for systems that might build on top of concepts like gossip protocols, paxos, raft, ... etc. just to throw out a few off the top of my head.
Quite possibly!
&gt; when I'm not on billable time But browsing Reddit is fine, no problem there. ðŸ˜‚ (Just messing by the way, 5 minutes breaks can help me refocus too sometimes)
I agree that's better wording, thanks!
I reinstalled the tray because some of apps didn't work without it when I most recently used Gnome 3. I don't think replicating and propagating their error is a good idea.
OP project is really cool and surely scratch a good itch, but I would have just used bash as well. gitall(){ find . -type d -name ".git" | xargs -I {} bash -c "cd {}/../ &amp;&amp; git $*"; }
Great! You're welcome :)
Do not create a new connection in your `Stream` impl. From the `futures` documentation for `Future::poll` (this also applies to `Stream::poll`): &gt; Implementers of this function should ensure that a call to this never blocks as event loops may not work properly otherwise. &gt; ... &gt; Unless it is otherwise arranged to be so, it should be ensured that **implementations of this function finish very quickly.** You first need to establish your connection to Redis, THEN poll it for data. If you need to connect asynchronously then you need to use [`redis::async::connect`](https://docs.rs/redis/0.10.0/redis/async/fn.connect.html) but this should not be done from inside your `Stream` impl, rather you would chain on the result of the returned future. --- From the `redis-rs` source code: pub struct PubSub&lt;'a&gt; { con: &amp;'a mut Connection, } In `OutputStream::new` you create a new `Connection`, but this drops when you return `OutputStream`, which invalidates the lifetime on `PubSub`. It is impossible for your `PubSub` to exist after `Connection` drops. You either need to pass in a mutable reference to a `Connection` in `OutputStream::new`, or store `Connection` in `OutputStream`. I recommend you do the latter unless you want to run into further lifetime requirements when you try to spawn your futures. Typically these entry points will have a `'static` bound (see [`tokio::spawn`](https://docs.rs/tokio/0.1.16/tokio/executor/fn.spawn.html)). This *basically* means that you need to move data into the future; you can't borrow from the outer environment because there's no way to validate that the borrowed data is valid for the entire duration it takes to drive the future to completion. You should also check the documentation/source code for `PubSub::get_message`. This is blocking and does not provide a non-blocking interface. This is bad news for your `Stream` impl. You could either try to minimize the amount of blocking by setting the read timeout to an extremely small value, wrapping `&lt;OutputStream as Stream&gt;::poll` in [`tokio_threadpool::blocking`](https://docs.rs/tokio-threadpool/0.1.12/tokio_threadpool/fn.blocking.html), or using `futures-cpupool`.
You persist it (e.g. to a database) or have the end-user supply the state with each request, if your server crashed and restarted you wouldn't expect every user of your API to have to start from the beginning so you probably shouldn't be storing the state in memory.
Do not create a new connection in your \`Stream\` impl. &amp;#x200B; From the \`futures\` documentation for \`Future::poll\` (this also applies to \`Stream::poll\`): &amp;#x200B; \&gt; Implementers of this function should ensure that a call to this never blocks as event loops may not work properly otherwise. &amp;#x200B; \&gt; ... &amp;#x200B; \&gt; Unless it is otherwise arranged to be so, it should be ensured that \*\*implementations of this function finish very quickly.\*\* &amp;#x200B; You first need to establish your connection to Redis, THEN poll it for data. If you need to connect asynchronously then you need to use \[\`redis::async::connect\`\]([https://docs.rs/redis/0.10.0/redis/async/fn.connect.html](https://docs.rs/redis/0.10.0/redis/async/fn.connect.html)) but this should not be done from inside your \`Stream\` impl, rather you would chain on the result of the returned future. &amp;#x200B; \--- &amp;#x200B; From the \`redis-rs\` source code: &amp;#x200B; pub struct PubSub&lt;'a&gt; { con: &amp;'a mut Connection, } &amp;#x200B; In \`OutputStream::new\` you create a new \`Connection\`, but this drops when you return \`OutputStream\`, which invalidates the lifetime on \`PubSub\`. It is impossible for your \`PubSub\` to exist after \`Connection\` drops. &amp;#x200B; You either need to pass in a mutable reference to a \`Connection\` in \`OutputStream::new\`, or store \`Connection\` in \`OutputStream\`. I recommend you do the latter unless you want to run into further lifetime requirements when you try to spawn your futures. Typically these entry points will have a \`'static\` bound (see \[\`tokio::spawn\`\]([https://docs.rs/tokio/0.1.16/tokio/executor/fn.spawn.html](https://docs.rs/tokio/0.1.16/tokio/executor/fn.spawn.html))). This \*basically\* means that you need to move data into the future; you can't borrow from the outer environment because there's no way to validate that the borrowed data is valid for the entire duration it takes to drive the future to completion. &amp;#x200B; You should also check the documentation/source code for \`PubSub::get\_message\`. This is blocking and does not provide a non-blocking interface. This is bad news for your \`Stream\` impl. You could either try to minimize the amount of blocking by setting the read timeout to an extremely small value, wrapping \`&lt;OutputStream as Stream&gt;::poll\` in \[\`tokio\_threadpool::blocking\`\]([https://docs.rs/tokio-threadpool/0.1.12/tokio\_threadpool/fn.blocking.html](https://docs.rs/tokio-threadpool/0.1.12/tokio_threadpool/fn.blocking.html)), or using \`futures-cpupool\`.
Not that a database is a bad idea but the reason globals must be read/written to is because they are not thread safe. Wouldn't using files just be equally unsafe? Even if you don't actually have to use unsafe blocks.
So, rather than hold a `PubSub`, hold the `Connection` instead, as that doesn't contain an inner reference (which is what is giving `PubSub` the lifetime parameter). When you need to use PubSub, then just call `as_pubsub()` as the name suggests a cheap representation change. This should be fine, as looking at the source code a `PubSub` is nothing but a struct that wraps a reference to a connection, and seems to hold no other state, and is just giving you an interface to easily send the correct commands to redis to setup a pub/sub channel, and thus subscriptions seem to be on a per connection basis managed on the redis side. So changing your example above... struct OutputStream { con: redis::Connection, } impl OutputStream { fn new() -&gt; Self { let client = redis::Client::open("redis://127.0.0.1:6379").unwrap(); let mut con = client.get_connection().unwrap(); // create a block so we can temporarily look at connection as a `PubSub` // subscribe to the channel we care about and drop the `PubSub` struct // it gives us straight away, so we can use `con` again when creating // the `OutputStream` { let mut pubsub = con.as_pubsub(); pubsub.subscribe("timeline:1").unwrap(); } OutputStream { con } } } impl Stream for OutputStream { type Item = String; type Error = std::io::Error; fn poll(&amp;mut self) -&gt; Poll&lt;Option&lt;String&gt;, std::io::Error&gt; { // And now, we can just ask to look at the connection as a `PubSub` // when we need too... let pubsub = self.con.as_pubsub(); match pubsub.get_message() { Err(_) =&gt; Ok(Async::NotReady), Ok(msg) =&gt; Ok(Async::Ready(Some(msg.get_payload().unwrap()))), } } } Give it ago, would love to hear if that's worked for you.
the idea of the cache is not to go to the DB every time, therefore, faster responses. sending state back to user using something like jwt makes sense until some point but also i have internal state for calculating accounting etc. In old version of my app which is written in go these all were tree style pointers where data had pointers to its children so i could do recursive calculations very fast without every time building the tree from a persistent data store What you are suggesting is simply not appliable in such context i cannot retrieve gigabytes of accounting information from db and index it every time theres a new request. I build this index once when the app starts.And rust is supposed to be safe but also fast with the method you are suggesting there are 3 outcomes. I store all client data on client side and ask for it everytime(clients with limited mobile data will not be very happy) this will be super inconvenient for the bandwith use. Asking data from a persistent db every time will be too slow for the data which mostly doesnt change.And in recursive accounting case i mentioned above it will be often impossible to respond without a http timeout or user nervout
exactly. this is I believe a design flow. In my case its just impossible to write the thing without dealing with raw memory addresses 
The only methods callable on a trait object are those of the trait. You can't access anything from the struct instance. Rust doesn't support the kind of dynamic typing you're expecting. In addition to enums, you could define a method on your trait that takes a function name (from your scripting language) and `Vec` of parameters and tries to execute the function on the variable, returning the result or an error if the function is not defined for the variable's type. However, enums may be better suited for this purpose.
I've never worked with grpc, but rocket has very good support for [managing app state](https://rocket.rs/v0.4/guide/state/#state). You might want to do something similar for your grpc app.
Oh actually yes i can add things to my base struct. I never thought of that :D thanks
Global variables are a design hazard because it's hard to reason about what state is touched where. Dunno if I'd call them dangerous; they're certainly not `unsafe` kinds of dangerous. I don't know grpc or rocket, but I have two broad suggestions in this kind of design situation: Create a struct that represents the whole app. This is an improvement but not a massive one over global state; all methods have access to a pseudo-global but hopefully some leaf code can take proper advantage of local reasoning. Use a global anyway, but since in Rust statics can't be mutated without `unsafe` (to prevent data races) so you need to make a bunch of static `Mutex&lt;T&gt;` or maybe one `Mutex&lt;Cache&gt;` or maybe an `RwLock&lt;Cache&gt;`? Personally I'd opt for a global cache wrapped in a mutex because it's the most general approach but be _very_ prepared to refactor it as I come to understand the app I'm writing better.
Thanks. Based on @jschievink 's answer its exactly what im going to do. It was an eureka moment for me. Again thanks to rust I had a whole different angle on the thing
Thanks for the feedback :D [ring](https://github.com/briansmith/ring) looks very promising. The reason I didn't use it was that I wanted to stay as close to the specified recommendation algorithms as possible and not all were provided by that library. 
Thanks for the feedback, I really appreciate it! Indeed group chat is a tricky problem and I'd have to read up on those papers. I'll think about writing an implementation for those as well, but right now I think that the double-ratchet crate is not the right place for those. However, generalizing *when* to ratchet (every N messages or after T time) is something I will consider for a future version. I don't really have plans for putting in other providers as I figured that is something that users of the crate can do themselves. With six types and five functions I fear a combinatorial explosion that I do not look forward to maintaining. Having said that, I know that a fourth version of [OTR](https://github.com/otrv4/otrv4) is nearing completion and I do aim to make the crate at least *compatible* with their ratchet (which means that I may have to generalize the initialization for example). About your nitpicks: 1. Yes that is something that could be improved. Is it preferable to have plaintext/ciphertext in the same mutable slice, like [ring](https://github.com/briansmith/ring) does, or in separate slices? 2. Good point, I'll do that. 3. Again, good point: I'll do that.