Yup, the system makes a lot of sense once you understand it. And if, for whatever reason, both traits are in scope, you can still easily disambiguate, e.g. [like so](https://play.rust-lang.org/?gist=371a3f1945784c7a57d3624da5fdf678). I'm not sure where the supposed confusion is coming from - in C you can't use functions such as `strlen` without doing `import &lt;string.h&gt;` either. Surely importing a trait to use it is actually very similar?
Why does this take minutes to build? const SIZE: usize = 1 &lt;&lt; 30; static SLICE: [u8; SIZE] = [0u8; SIZE]; fn main() {} The resulting executable is small (even if I add code that uses SLICE so it doesn't optimize out), so SLICE is going in the `.bss` section, not actually being emitted. But building it takes 100% CPU while `rustc`'s resident memory usage climbs to over 2 GB and back down. What's going on? Is there a way to get a big `.bss` slice without adding minutes to compile time?
In most implementations `NULL` is represented by the address zero, even though this is not *required* by the standard (as I understand it). But the ability to work around this limitation with virtual memory is not a feature of C, it is a feature of specialized hardware that is specific to a particular processor architecture. And you can't control the virtual memory mappings from pure C either, you're going to need assembly for that. In the end it doesn't really matter because accessing the first byte in memory isn't really something that you require all that often anyway. But it seems a little silly to have a language with a magical reserved 'this address is always invalid' address and claim that it features memory addressability it its core. C has so many rules about what you can and cannot do with pointers... you can't just randomly go poking at certain memory locations, you can't compare pointers to arbitrary objects, you can't create pointers that are "out of range" of an object or array, even if you don't dereference them. I find that people who claim C is portable assembly are often unaware of the incredible amount of undefined behaviour in C, and imagine the semantics to map to a naive translation to assembly instructions, rather than being defined on the C abstract machine.
I don't see how this is different, in principle, to a tuple appearing in a public API? It seems silly that we have anonymous product types (i.e: tuples) but non anonymous sum types, so this pre-RFC sounds like a very good thing to me (I definitely have a lot of use cases).
Wouldn't it make more sense to have the most common case (single candidate method only) handled behind the scenes, only requiring explicit handling if there's a conflict?
[removed]
You might be interesting in the Poly project (https://polly.llvm.org/), which aims to bring a bunch of automatic performance improvements to the LLVM through parametric polyhedra representations. Poly also aims to deliver automatic parallelization, although at the moment most of the practical benefits seems to be around data-locality and loop fusion. Since Rust uses the LLVM, any success that Poly has in automatic parallelization will apply to Rust. 
The most common case isn't using traits at all, but using inherent methods defined on the relevant type. Think `Vec::&lt;T&gt;::push`, and `&lt;[u8]&gt;::len`. And when you have an instance of some type, those methods are all implicitly in scope, even if that type was never imported.
I‚Äôm not sure that adding special cases like this to eliminate a single import is worth it.
Weirdly, the standard requires that any constant expression with the value of zero is a null pointer, but a non-constant expression with the value of zero is not guaranteed to be equal to a null pointer, so `0 == NULL` or `(1-1) == NULL` are guaranteed to be true but `atoi("0") == NULL` is not guaranteed to be true.
It could be several imports per file. I personally found it always very annoying -- at least partially due to the lack of tooling though, because I need to scroll up to add the import, then find the spot in the file again. I've always been a fan of letting tools take care of as much boilerplate as possible, but then Rust has always been more on the explicit side of things.
As /u/steveklabnik1 says, it doesn't have to be at the top of the file. If you only use a particular trait inside a single function, you can just add the `use` statement inside that function body, and it will work.
Because it was stated in the pre-RFC that there would be only one of each type in the output sum type there is no ambiguity in which arm is chosen. In the pre-RFC, it says enum(T, T) is the same as enum(T). I think this is much better than the previous pre-RFC.
I really like this pre-RFC and I think it was good that the previous pre-RFC was made as well as that helped push this along and contribute to this design. I think this is much more ergonomic, and I appreciate the additional sum type trait and associated helpers for working generically on sum types.
The ambiguity still exists in that one of the match arms must be chosen. Sure, you can just assume the first action is chosen. But this still definitely falls under the "surprising behaviour" category.
I'm using this code to get the user's config directory: extern crate dirs; pub fn testing() { let test = dirs::config_dir().unwrap(); println!("{:?}", test); } This works and prints: "/home/username/.config". Now I want to add "myapp/local.conf" so I can create that file and open it/write to it. But I just can't get it to work "dirs" returns a PathBuf and I tried all sorts of to\_string()/to\_str() methods, but it still doesn't work. What do I need to do here? And why is it so complicated? Why can't it just return a simple String?
This is why in Rust we use the newtype idiom. In this case you would wrap your usize in different newtypes, say Left and Right, to disambiguate.
 use std::path::PathBuf; fn main() { let base = PathBuf::from("/home/username/.config"); let path = base.join("myapp/local.conf"); println!("{:?}", path); } &gt; Why can't it just return a simple String? Because paths are **not** "simple strings", and any language that says otherwise is lying. You can convert strings into paths, but not necessarily the other way around.
If I wanted to use the newtype idiom, I wouldn't be trying to use anonymous enums :P
Damn that was quick. Thank you so much. Works just as I needed it to. However I'm curious: Why is it not a String? When I used my old code and I println-ed "test". It actually printed out the entire path. It printed out an """ and then a "h" and then a "o" and then a "m", and so on. Isn't a series of characters a String? 
There have been other good answers to the general question, but in your case, nearly all of your `where` clauses are unused and can just be removed. --- a/src/lib.rs +++ b/src/lib.rs @@ -21,16 +21,13 @@ type NodeId = usize; /// of a single tuple. #[derive(Debug, Clone, PartialEq)] -pub struct Range&lt;T&gt; -where - T: std::clone::Clone + std::cmp::PartialEq + std::fmt::Debug + std::fmt::Display + std::cmp::Ord + std::hash::Hash, -{ +pub struct Range&lt;T&gt; { pub ranges: Vec&lt;(T, T)&gt;, } impl&lt;T&gt; Range&lt;T&gt; where - T: std::clone::Clone + std::cmp::PartialEq + std::fmt::Debug + std::fmt::Display + std::cmp::Ord + std::hash::Hash, + T: Clone + Ord, { pub fn new(pairs: &amp;[(T, T)]) -&gt; Range&lt;T&gt; { Range { @@ -51,10 +48,7 @@ where #[derive(Debug, Clone, PartialEq)] -pub enum Language&lt;T&gt; -where - T: std::clone::Clone + std::cmp::PartialEq + std::fmt::Debug + std::fmt::Display + std::cmp::Ord + std::hash::Hash, -{ +pub enum Language&lt;T&gt; { Empty, Epsilon, Token(T), @@ -83,10 +77,7 @@ where /// it's starting point is the LAST item pushed into the arena. #[derive(Clone)] -pub struct Recognizer&lt;T&gt; -where - T: std::clone::Clone + std::cmp::PartialEq + std::fmt::Debug + std::fmt::Display + std::cmp::Ord + std::hash::Hash, -{ +pub struct Recognizer&lt;T&gt; { language: Vec&lt;Language&lt;T&gt;&gt;, memo: HashMap&lt;(NodeId, T), NodeId&gt;, start: Option&lt;usize&gt;, @@ -98,7 +89,7 @@ where impl&lt;T&gt; Recognizer&lt;T&gt; where - T: std::clone::Clone + std::cmp::PartialEq + std::fmt::Debug + std::fmt::Display + std::cmp::Ord + std::hash::Hash, + T: Clone + Ord + std::hash::Hash, { /// By default, a Recognizer recognizes only the Empty Language, /// i.e. *no* strings can be recognized. @@ -338,12 +329,12 @@ where impl&lt;T&gt; fmt::Debug for Recognizer&lt;T&gt; where - T: std::clone::Clone + std::cmp::PartialEq + std::fmt::Debug + std::fmt::Display + std::cmp::Ord + std::hash::Hash, + T: fmt::Debug + fmt::Display, { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { fn fmt_helper&lt;T&gt;(f: &amp;mut fmt::Formatter, language: &amp;[Language&lt;T&gt;], p: usize) -&gt; fmt::Result where - T: std::clone::Clone + std::cmp::PartialEq + std::fmt::Debug + std::fmt::Display + std::cmp::Ord + std::hash::Hash, + T: fmt::Debug + fmt::Display, { match language[p] { Language::Empty =&gt; write!(f, "‚äò")?,
The doc for `format!()` states that the first argument must be a string literal. I'm working on a build process that generates some rust code and I have a few big templates with placeholders that I want to reuse. Is it possible to somehow store these templates in immutable globals and format there? I would like to do something like this, however it doesn't look like it's actually possible: ``` static DEFINITION_TEMPLATE: &amp;'static str = "/// {} /// /// - **Name:** {} /// - **Type:** {} pub static {}: UID = UID {{ \tident: \"{}\", \tname: \"{}\", \ttype: \"{}\", }}; "; ... fn build_definition(defn: &amp;Definition) -&gt; Result&lt;String, Error&gt; { ... .. Ok(format!(DEFINITION_TEMPLATE, a, b, c, d...)) } ``` Is there any way around having to put giant string literals in the middle of my functions like this?
Polly can be enabled with `-Z polly` once ["Add Polly support" #51061](https://github.com/rust-lang/rust/pull/51061) merges.
What is the significance of this post, these books? 
People who have read both: which of these books do you feel is better, and why?
They are the two first big books about rust. Finding them at a local library is an indicator of the growing popularity of rust.
The thing is, if you want to distinguish between index and type size, you should spell it directly `enum(Index&lt;usize&gt;, TypeSize&lt;usize&gt;)`, instead of relying on order of variants, which are quite easy to forget and mix up.
Selecting first arm is not assumed, but explicitly defined in the proposal. I believe it's not a surprising behavior, as it's exactly how matching works today. And there is nothing new on possibility of making one of the match arm dead on monomorphization, see second example in the "Generic code" section.
The usefulness of the sum types isn't necessarily just in their ability to ferry things around when doing functional programming, similarly to how tuples are used. While I understand that what you want is perhaps something akin to tuples for functional programming, I am moreso looking to sum types to solve the problem of "what set or subset of things can this thing return" rather than current enums which would require you to have to define a new enumeration for every such subset. Sometimes I have several types that can be returned (like a Node or a Leaf), and each one has useful semantic value even if I am exposing an API. Because defining a newtype is so easy, it shouldn't be too difficult to use it ergonomically either. This is all it might take: ```rust struct A(i32); struct B(i32); let my_iter = it.map(|n| -&gt; (enum(A, B), bool) { (if n.something() { A(n.val) } else { B(n.size) }, something_else(n)) } ); // Use A and B in `it` here. ``` Typing code on a phone is silly, but I did it XD.
What about this as a motivation? let foo: enum(i32, i32) = 123.into(). Should `foo`'s tag be `i32` or `i32`?
I really like Programming Rust. It‚Äôs illustration helped me so much in understanding Rust‚Äôs ownership. 
I personally recommend the rust programming language to start off, then using programming Rust as a reference for more advanced topics
I should write a really thin book called Rust: the good parts
Rust strings are specifically UTF-8-encoded. Paths are (depending on platform) mostly arbitrary binary blobs that *can* look like valid text, but don't have to. This is partly why a new string encoding had to be invented specifically for Windows paths. Just because something *looks* like something doesn't mean it *is* something, it might be something else.
&gt; Is it possible to somehow store these templates in immutable globals and format there? No. The formatting macros need actual string literals, period. &gt; Is there any way around having to put giant string literals in the middle of my functions like this? *More* macro abuse: macro_rules! definition { ($($args:tt)*) =&gt; { format!("giant string literal", $($args)*) }; } Ok(definition!(a, b, c, d...)) 
Because when an API changes, product types having changed without the API user responding tend to result in compile time errors. In the specific formulation that OP created, because you have to explicitly affirm the variants of the sum type, we can avoid the issue with an unanticipated addition to the sum. The existing RFC doesn't do this as far as I can tell leaving an API user open to inadvertently handling a new variant with a catchall, which is the worst type of subtle error in my opinion. However, in OP's formulation, you don't really get a lot of power out of the enum because of the repetition of specifying all the variants *except* in local usage where creating an explicit type feels heavy.
Not if you have anonymous enums with named variants, dual to the anonymous records RFC.
Oh I thought OP might be indicating he's starting to learn rust
They're probably complementary, but I suppose it's possible they could also be complimentary. They're certainly complimentary at the library, provided they're returned on time! They both have nice things to say about Rust, so maybe I too hasty -- they're both complimentary. üòÇ
Ha!
&gt; It seems silly that we have anonymous product types (i.e: tuples) but non anonymous sum types Well, anonymous product types are simply more useful than anonymous sum types. Don't let a mere symmetry in language dictate your preferences to you. Good and evil are complementary concepts, but they are not equally desirable in practice do to that.
You can't flatten regular enums because the naming would be problematic. If you have `Enum1::A` containing an `Enum2::A`, how do you to distinguish the variants in the flattened result? Of course, if you don't have names and you match solely on type, then you don't have this problem. Moreover, flattening regular enums would be quite desirable if it made sense, since it would allow you to save space in nested enums. `Option&lt;Option&lt;Option&lt;T&gt;&gt;&gt;` could collapse to `Option&lt;T&gt;` and take up the same amount of memory. As is, you have to maintain some information denoting which option is `None`, and that bloats the type. However `enum(enum(enum(A, B), B), B)` will collapse to `enum(A, B)` and it is now impossible to tell which B was used. This allows you to express something that current enums do not allow -- the irrelevance of the layer.
Implementing `Add` based on the underlying type means you need to provide mirror impls for each impl the underlying type has, and that can be upwards of 4 different impls for standard numerical types. It's certainly not trivial, especially when you want to do it for multiple ops and multiple types, etc. It can be a lot of boilerplate.
That impl trait return type, is that accomplished with dynamic dispatch? Or is the object on the attack the size of the larger of the two variants.
&gt; the buffer is created inside of the function and then passed out of it which would be a bad thing to do with C. This is only bad in C if you stack allocate the buffer's contents. `String` allocates its buffer on the heap, so there is no problem here. The first case is probably less performant on the surface because you're doing a double indirection (`&amp;mut` is a pointer and `String` is a pointer.)
It would be like a normal enum, not dynamic dispatch. (That is the main point of using it with impl trait, otherwise you could just return a trait object instead.)
There is a double deref, but if it lets you re-use a String you‚Äôve already allocated, that‚Äôs still a good trade off.
To my knowledge there is no relevant difference in cost between the two implementations you've shown. There is a detail I would like to point out however; allocating the buffer externally as you've done in the first example is arguably more vestal at no cost since you control the initial state of the buffer. E.g. calling the first function with a non empty buffer would spend the files contents to the buffer. Doing the same with the second function would require appending the result of the function to the buffer which would perform the extra allocation inside the function.
It looks like Rust Enhanced has support for RLS: https://github.com/rust-lang/rust-enhanced/blob/master/README.md#rls-support Have you tried enabling this / setting it up? Rls is the successor to racer, and will call into racer itself for anything it can't complete. -- Besides that, unless you're working professionally, IntelliJ IDEA Community Edition is free, and has just as much support for the Rust plugin as any other Jetbrains IDE.
Fairly recently, someone announced a Rust interpreter with a seriously un-memorable name. Something like ESCRI. Anybody have better memory?
All of it
Crabs are just so cute!
Are you thinking of the [SUBLEQ interpreter](https://www.reddit.com/r/rust/comments/9o6vzo/constfn_compiletime_subleq_interpreter/)?
The first version lets the caller re-use a `String` between multiple calls of the function for efficiency, which is why `read_to_string` also takes a `&amp;mut String` instead of returning a `String`. There is no functional difference between let mut s = String::new(); get_file_buffer(name, &amp;mut s); and let mut s = get_file(name);
Hm, yes this is what I meant by not wanting to overload the add operator.
You shouldn't be using `wait()` in your code as it will lock. The `hub.add` method should really be returning a `Future` instead of a `String`. The example in the readme shows how to get results out of the future. Paraphrasing, you can possibly do something like: let sys = System::new("test"); let hub_addr = Hub::new().start(); let res = hub_addr.send(CreateRoom); // &lt;- send message and get future for result Arbiter::spawn(res.then(|res| { match res { Ok(result) =&gt; println!("Room: {}", result), _ =&gt; println!("Something wrong"), } System::current().stop(); future::result(Ok(())) })); sys.run();
Hmm, I have both books and I would actually recommend the reverse
Don't forget match statements
Mm yeah that makes sense if you want to treat the wrapper type and inner type similarly; I didn‚Äôt gather from op‚Äôs initial post that that was something they were after (certainly possible I glossed over that). Since op sounds new to Rust, I figured it‚Äôd make sense to mention the absolute basics first before jumping to crates/macros/etc.
As someone want to learn this language at some future time, could someone explain op's comment for me? Isn't people here using rust because it is powerful, efficient and convenient in terms of system programming (and game developing?)? why it gets this kind of widely approved (10 upvotes) negative opinions here‚Ä¶?
It's a joke, adapted from this image: https://i.redd.it/h7nt4keyd7oy.jpg
It's not quite the same, but "Dive into Rust" is very brief in a good way and for people that already know programming.
r/woooosh for me
I really like this approach. I have mainly one nit-pick about it; "sum enum" make it sound like a sum-type, which it is not. The number of possible values of a sum type is the sum of the possible values of it's parts, but since you declare that `enum(A,B,A) = enum(A,B)`, this does not hold. It's not a summation, it's a union, even though that name has been a bit tainted by the C-construct.
Again, nothing in the language forbids you to dereference a pointer that points to address 0. As far as modifying the page tables, you don't really need assembly if you know where they are in memory (I'm speaking for x86/AMD64 here). 
Can you explain that in more detail? In what regard does it not scale? 
You should stop putting trait bounds on type definitions where they are not necessarry. After that you might find other places where bounds are placed for no good reason. As the rule of thumb: * Trait bound on type only if associated type of the trait is used to define fields. * Trait bound on function only if function actually uses it * Trait bound on impl block to not repeat same bound for many functions. * Trait bound on trait impl if this trait implementation uses it. Following this simple rules you may found that all those `Clone`, `Eq` etc trait bounds required only in few places. The first rule is the most important one. Trait bounds in type definition infect all other places.
I guess that could be an implementation detail of your trait implementation. However I see that failible computations are probably still best expressed in a middleware, so that your computed Result&lt;T, E&gt; won‚Äôt get cached in the error case...
Building libcore for cross compilation *is* bootstrapping. That's the most legitimate abuse of this knob.
18:30 GMT happens when this comment is 10 hours and 2 minutes old. You can find the live countdown here: https://countle.com/Y9Hqd84Ji --- I'm a bot, if you want to send feedback, please comment below or send a PM.
I‚Äôve decided to learn Rust also and I‚Äôm gonna start with the Rust programming language :) but I made my decision on which one to buy flipping a coin.
 let handle: Arc&lt;_&gt; = server.clone(); ::std::mem::forget(handle); Ha! Now you can't drop *nothin'!* Behold my mighty hackerman skills.
yes, it does. the language says it's undefined behaviour so the compiler is allowed to optimise it away / do anything with it ‚Ä¶ 
If, instead of .clone(), you were calling .get_handle(), maybe it would feel less weird.
(That's the right answer)
And a really fat one called "Rust: The Definitive Guide".
It was actually a Rust interpreter - well, actually compiling and loading expressions as shared libraries on the fly. I remember at the time someone saying here about the name "It doesn't exactly roll off the tongue". And it isn't rusti (gone to github heaven AFAIK) or miri, which now lives in the compiler for doing const evaluations.
(A right answer. There is some information in other peoples' posts that isn't in mine.)
I can't help you; I can only sympathise having gone through the exact same thing you currently are. (This is, incidentally, why whenever someone lists "creative names" as a positive thing, I want to smack them.)
oh, nice hack
I am a little late here, but I would like to advertise [custom_error](https://crates.io/crates/custom_error) a crate I've written with the goal of simplifying error handling without having to depend on a crate with it's own error types. The crate contains a single macro, and generates a simple enum for your error types. It gives you *raw* error types, but without having to write a lot of boilerplate : ``` custom_error! {FileParseError Io{source: io::Error} = "unable to read from the file", Format{source: ParseIntError} = "the file does not contain a valid integer", TooLarge{value:u8} = "the number in the file ({value}) is too large" } ```
I guess you are thinking of [https://github.com/google/evcxr](https://github.com/google/evcxr)
Great point! When I first looked at that crate I was confused with the API and didn't realize how close it is to what I wanted. The main difference is that my crate is generating the code and so understands a lot about it. The result is it can auto-generate help message, error messages, man pages and in the future, shell completion. The `config` crate has no knowledge of anything - it just leverages `serde`. My goal when writing `configure_me` was to make a crate that helps you write very user-friendly command line programs with everything that GNU programs have - help, manual page, standard format of parameters (this is now 1/4 done as it still doesn't support abbreviations, flags in a single string like `-xzvvf` and `--foo=FOO` style args - the last is in progress right now). As you can see I didn't reach my goal yet, but I'm on the way and there's nothing preventing me from achieving that goal. :)
By invoking `wait()` you block current thread from entering the Actix's event loop and so it cannot move on to processing the second actor's mailbox, effectively deadlocking the entire thread. You can either utilize the fact that Actix allows to do fancy Future-related things using `ResponseFuture` and `ResponseActFuture` (they are hardly described, so you have to go full guess &amp; test). Something like this should work: ```rust pub fn add(&amp;self, ctx: Context&lt;Self&gt;): impl ResponseActFuture&lt;Self, String, MailboxError&gt; { self.0.send(CreateRoom).into_actor(self).wait(ctx) } ``` You can also try to spawn each actor on a different thread - Actix supports multithreading, but I'm not sure whether it allows to force-spawn each actor in a new thread or not.
You will have to specify which implementation should be used. &gt; why would you want to implement 2 different methods under the same name? There are 20k crates on crates.io. I can definitely imagine that some 2 of them came with a method named find or copy. Few years from now it can grow to 200k or more. This is guaranteed to happen. In some cases this is intentional: you can have json::serialize and toml::serialize for example and use both in the same code.
I didn't know Vulkano has a new maintainer. Has tomaka posted anywhere about why he no longer works on Vulkano? I'm curious about what his reasons are for leaving the project behind.
&gt; cute Here is a less cute one: https://cdn.vox-cdn.com/thumbor/aBeZLaCtyUTYW99Ybe07_FkeRJo=/0x0:2700x1518/1200x800/filters:focal(1215x442:1647x874)/cdn.vox-cdn.com/uploads/chorus_image/image/61401577/The_Predator_Movie_5K_q2.0.jpg (I've learned recently it was modelled after a crab)
If he sees this I'm sure he could give a better answer, but I guess hes just happy enough with his current work at parity that he doesn't feel the need to push vulkano to the point where it could become his full time employment. He has also expressed that rusts current features aren't enough to take vulkano where he wants it, I think the main issue was the lack of HKT.
Is there any reason to prefer vulkano over gfx-rs?
This could be useful in some cases but I think these cases are very rare and we should avoid adding features that are rarely used. Otherwise rust will become the new c++ nightmare. Rust is already considered hard to learn, we should focus on changes that make the language easier, not harder.
If you don't want to expose futures/async it's probably best to embrace a threaded model. Like you've found, it's unergonomic to mix the two. You can use future channels to communicate to a background thread, but at that point you're just introducing complexity that doesn't have any benefit. Might I suggest a library such as zeromq (or my async version tmq) if you want to do message passing.
I have never used gfx-rs but I'll try my best to explain its current state, from what I have heard. The current release of gfx-rs exists and is somewhat usable but that code has been thrown away in favor of gfx hal. gfx hal is an api extremely similar to raw vulkan, so similar that it can be plugged into other projects e.g. dolphin and dota2. It acts as a compatibility layer from vulkan to dx12 and metal In the same way moltenvk works as a compatibility layer from vulkan to metal. gfx hal is usable now and a release is just waiting on the gfx team to be poked enough about it. However gfx hal is too low level for some people (me included), which is where wgpu comes in. Despite the name web gpu, its also supposed to be used for desktop applications. I think wgpu is supposed to be at the same level of abstraction as vulkano. I'm pretty excited for wgpu and might swap to it if it succeeds. kvark please correct me if I got anything wrong.
You should try [IntelliJ Rust](https://intellij-rust.github.io/) to get code completion and \`Go To Definition\`. This plugin can be used IntelliJ IDEA CE. Also, you can use [CLion EAP](https://www.jetbrains.com/clion/nextversion/) for free to get debugger support.
The builder pattern is certainly nice when the constructor gets too large, but I think 3 elements is fine? Pipelines make use of this pattern, https://docs.rs/vulkano/0.11.0/vulkano/pipeline/struct.GraphicsPipelineBuilder.html The Vulkano API is certainly not perfect though.
&gt; What are the implications of this? Does Rust end up treating these two cases almost identically or is there some fundamental difference? Both cases are "correct" because `String` is essentially a pointer, so there are no allocation issues. The difference is in usability/flexibility: the second one is more convenient for the caller because you just call it and get a result back, the first one is less convenient but more flexible as it lets you reuse a string across calls (e.g. if you call `get_file` in a loop the second style will allocate a string on each iteration while the first style will allow allocating a single string outside the loop and using it every time, that can be useful if you're only using the resulting data within the loop's span).
FYI the first link is broken. Missing the `h` at the beginning.
&gt; In most implementations NULL is represented by the address zero, even though this is not required by the standard (as I understand it). And the C FAQ lists multiple systems where the NULL pointer is not zero-valued, there are even systems where NULL is multi-valued.
Why not Effective Modern Rust 2018?
`Instance::default()` would be better
I've seen `_:()` being called "Frog raises hand", love that name.
Love it or not, I'm definitely not going to be able to un-see that.
Ah yes, in the good old x86 real-mode segmented memory model there were many many possible addresses for physical address zero. I don't miss those days :-)
Yet another thing that C# does better than Java... 
Aren't they both like 600 pages?
Welp, there goes my food fund next week. ;) (Been waiting for the main book to publish.)
I'm not sure how relevant this might be for you, but "extern crate" is not required anymore for Rust 2018 and you can just use "use": https://youtu.be/RsHufu4PKPw Hope this was useful!
Wait, i tought this sub was for rust, the game...
Anyone have luck getting this new version to work on Windows? When I get to the pacman step, it can't find pacman.
As rukai mentioned, \*life\* happened. I've more or less entirely given up on vulkano/glutin/winit and all the private projects that I was building with them. &amp;#x200B; When I build a library, I usually try to reduce as much as possible the number of features that it initially has in order to push for a quick stable version (which is why I was really opposed to for example adding menus or cursor grabbing to winit). But that miserably failed for vulkano/glutin/winit. &amp;#x200B; Also, the initial idea behind vulkano was to be zero cost. However doing so builds types that are very complicated for the user to write out, to the point that it makes the library very user-unfriendly. Therefore vulkano is right now \*not\* zero-cost (at all), while waiting for impl trait to be stabilized (more than two years after I initially opened an issue that is suggesting the exact design that has finally been agreed upon). &amp;#x200B; I'm super graceful to rukai for maintaining the library (and also francesca for leading the maintenance winit and glutin), but it is quite unlikely that I find the dozens of hours required to finish the work on vulkano.
It should be `$ git clone htttps://gitlab.com/NateDogg1232/passgen-rs.git`
After installing msys2 you will need to open a special msys2 terminal in which you can run pacman, a shortcut for it should have been added to your desktop.
Happy to accept any help! To contribute, this is a good starting point https://github.com/OISF/suricata#contributing We hang out on IRC (#suricata @ freenode) too. Keep in mind that the whole team will be traveling for our annual conference, so responses may be delayed. Thanks!
Web request middleware is a pattern featured in every single web framework. You cannot write a general use web request middleware that can be shared across frameworks without those frameworks bending and contorting to fit the Tide paradigms. What incentive does any project have to adopt Tide paradigms? This isn't stdlib material at all...
Unfortunately something like `UnsafeWake` is still necessary for targets that don't support `Arc` (maybe there could be some way to have a generic `ArcLike` instead), you should never have to interact with it directly though.
It might be useful to include a link or just a short call out to builders, because builders are the idiomatic way in the Rust ecosystem to achieve this sort of thing. I very very rarely see APIs littered with `Into&lt;Option&lt;...&gt;&gt;`.
No Rust framework has won yet. Now is the perfect time to introduce new web frameworks. Personally I agree with the choices of the Tide framework more than any other framework so far.
Thanks for these answers. Honestly I am kinda disappointed that simply getting the frequency of a timer and the amount ticks passed is not easily available. But then again embedded Rust is still young :) Since I have some experience developing (embedded) software (I am just new to Rust), I will probably try add some usable timer to stm32f30x_hal and see if embedded-hal has already some kind of proposed abstraction in a PR or issue. After I want to learn Rust :)
Added a link. You're right. :) I thought I was pretty explicit that I was *only* covering optional arguments, which is a bit different than a builder (since builders are typically involving a `.build()`-style step and more than just a single function call). I'd like to write more about builders in the future as well, so I kind of saved it for then, since adding builders here would probably double the length. Also, I do not think "everyone does it"/"noone does it" is a good justification for anything. If we all followed that advice we'd not have Rust, or your lovely projects like `ripgrep`.
That's the problem with the project. It's not achieving its mission of rising all boats. It's just yet another web framework.
And so very tasty!
Sorry it took me a while to answer. Yes, it's issues with `&amp;self`. impl&lt;'a&gt; AddAssign&lt;&amp;'a Tst&gt; for Tst { fn add_assign(&amp;mut self, other: &amp;'a Tst) { if self.mem.len() &lt; other.mem.len() { //Calls this same function in reverse let mut other = (*other).clone(); other += &amp;*self; //Calls other + &amp;self *self = other; return; } } } Here, I must use `&amp;*self`, otherwise I get an error: 195 | other += self; //Calls other + &amp;self | ^^ no implementation for `Tst += &amp;mut Tst`
Interesting read, thanks! :)
Sorry it took me a while to answer. It seems to be issues with `&amp;self`. impl&lt;'a&gt; AddAssign&lt;&amp;'a Tst&gt; for Tst { fn add_assign(&amp;mut self, other: &amp;'a Tst) { if self.mem.len() &lt; other.mem.len() { //Calls this same function in reverse let mut other = (*other).clone(); other += &amp;*self; //Calls other + &amp;self *self = other; return; } } } Here, I must use `&amp;*self`, otherwise I get an error: 195 | other += self; //Calls other + &amp;self | ^^ no implementation for `Tst += &amp;mut Tst`
Sorry it took me a while to answer. It seems to be issues with `&amp;self`. impl&lt;'a&gt; AddAssign&lt;&amp;'a Tst&gt; for Tst { fn add_assign(&amp;mut self, other: &amp;'a Tst) { if self.mem.len() &lt; other.mem.len() { //Calls this same function in reverse let mut other = (*other).clone(); other += &amp;*self; //Calls other + &amp;self *self = other; return; } } } Here, I must use `&amp;*self`, otherwise I get an error: 195 | other += self; //Calls other + &amp;self | ^^ no implementation for `Tst += &amp;mut Tst`
Sorry it took me a while to answer. It seems to be issues with `&amp;self`. impl&lt;'a&gt; AddAssign&lt;&amp;'a Tst&gt; for Tst { fn add_assign(&amp;mut self, other: &amp;'a Tst) { if self.mem.len() &lt; other.mem.len() { //Calls this same function in reverse let mut other = (*other).clone(); other += &amp;*self; //Calls other + &amp;self *self = other; return; } } } Here, I must use `&amp;*self`, otherwise I get an error: 195 | other += self; //Calls other + &amp;self | ^^ no implementation for `Tst += &amp;mut Tst`
I think its a little too early to make that sort of judgement. The framework is less 3 months old and hasn't been officially released on crates.io. Let alone had any opportunity to find common abstraction points with other frameworks.
Like this? https://github.com/mit-pdos/biscuit
Constructive? I think not. I'm personally very happy that Aaron is spending time with what I think will be a very important domain for Rust in the future. Regardless of the result, I think that this is an area of the ecosystem that is a priority for thought and effort.
Hello, I'm currently trying to better understand Sync trait. From the description, it should allow me to easily share reference to Sync-enabled type to a thread, but I cannot compile this easy program use std::thread; use std::net::{IpAddr, Ipv4Addr, SocketAddr}; fn main() { let mut z: Vec&lt;SocketAddr&gt; = Vec::new(); z.push(SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 1234)); let handle = thread::spawn( || { for addr in &amp;z { println!("{}", addr); } }); println!("{:?}", z); handle.join(); } it fails with `error[E0373]: closure may outlive the current function, but it borrows \`z\`, which is owned by the current function`
Yes. I even use it on my ZFS system.
Thanks. To be clear, they are alternative solutions to the same problem, where one of them is idiomatic while the other is not. Idioms are valuable, at least in part because they are familiar and in part because they tend to reflect the "wisdom of the crowd." We don't need to deal in absolutes here. Idioms are guidelines, not hard rules. But idioms are useful signposts, e.g., if you are discussing a strategy that diverges from an idiom, then it is usually good peactice to call that out explicitly. So thanks for adding that to your article! :)
I'd say -- don't worry about it and add a proper section to your article about builder patterns and their use. Good thing about blogs is that you haven't ordered 300,000 hard copies to be printed ;-), but people will be finding your article via Google for years to come -- it is good to have the most accurate information there!
" remote within two hours of CET working hours " Which probably means that you have to work within that time window, not be physically located in a time zone +/- 2 of CET &amp;#x200B;
The error occurs because the compiler is unable to guarantee that the closure will finish executing before the current scope exits (and thus z gets dropped). I think you either want to move z into the closure (by prepending the 'move' keyword to the closure) or use scoped_threads, which is, I think, available in the crossbeam crate?
Does not seem to work on btrfs: chattr +d .rustup/toolchains/ snapper -c home create -d test ls /home/.snapshots/440/snapshot/user/.rustup/toolchains/ nightly-x86_64-unknown-linux-gnu stable-x86_64-unknown-linux-gnu
Ok, just a short update in case someone lands here with the same problem. I had a closer look again at stm32f30x-hal and it turns out although embedded-hal doesn't have the necessary abstraction yet stm32f30x-hal has very well an implementation called MonoTimer and Instant which should provide what I need. I still have to test this, but it looks promising so far.
My preferred method is just writing a new post about it and cross linking them. :)
Maybe you can write something about this and I can link to you! :)
A much better justification. &lt;3
Would this type signature fn request( &amp;self, data: &amp;mut Data, req: Request, params: &amp;RouteMatch&lt;'_&gt;, ) -&gt; FutureObj&lt;'static, Result&lt;Request, Response&gt;&gt;; have the return type `impl Future&lt;...` if traits could return impl traits? 
The d attribute is advisory, and tells your backup program not to back up anything in those directories. Of course the program is free to ignore it if it prefers. In particular, it's not likely to affect filesystem snapshots. Snapshots work not by copying all of the data to another location on the disk, but by a copy-on-write mechanism. On my ZFS system I use nested filesystems and give them snapshot policies that override the defaults. For example, my Maildir gets snapshotted daily, while the directory holding my browser cache never does.
I believe it has already been posted here, like this week.
Previously: https://www.reddit.com/r/rust/comments/9urht7/userspace_ixy_network_driver_in_rust_pdf/
I see
That makes sense. I imagine that would look something like this: (this is the best example I've seen of implementing a trait similar to what I need) struct Ascii { chr: u8 } impl AsRef&lt;str&gt; for \[Ascii\] { fn as\_ref(&amp;self) -&gt; &amp;str { unsafe { ::std::mem::transmute(self) } } } 
Sure. Usually board vendors provide GCC toolchain. You don't want go through the trouble compiling the toolchain with the correct options etc. for every possible board in existence. Maybe if LLVM will some day dominate the embedded space, this argument can be ruled out.
I already had the idea of a generic `Into&lt;Option&lt;T&gt;&gt;` to emulate the C++: extern crate rand; use std::option::Option::None as nullptr; fn gimme_a_pointer_to_an_int&lt;'a, T&gt;(foo: T) where T: Into&lt;Option&lt;&amp;'a i32&gt;&gt;, { match foo.into() { None =&gt; panic!("segmentation fault"), Some(_) if rand::random::&lt;f64&gt;() &lt; 0.5 =&gt; panic!("segmentation fault"), Some(_) if rand::random::&lt;f64&gt;() &lt; 0.01 =&gt; panic!("bus error"), // much rarer Some(_) =&gt; println!("You've got lucky this time!"), } } fn main() { gimme_a_pointer_to_an_int(&amp;42); gimme_a_pointer_to_an_int(nullptr); } 
You are mostly correct. gfx-rs team isn't most active at the moment. The only advances are made at wgpu front, which is our biggest bet on the future of rust graphics abstractions. As to vulkano vs gfx-rs, they shouldn't be alternatives. Vulkano should be [pluggable on top of gfx-rs](https://github.com/vulkano-rs/vulkano/issues/525), which gives you the best of both worlds.
The match statement is nice and... Erhmm... That's about it I guess. (joking, joking, of course. There is second thing I like, the logo looks kinda ok.)
Hahah the `nullptr` gave me a double take. :)
I can't wait to read it!
I went through a bit of the Vulkan tutorial using `gfx-hal` instead, and found it relatively easy to map things across and I know next-to-nothing about graphics. If you're looking for something closer to Vulkan to get you started, I think `ash` is meant to be basically a Rust wrapper around the Vulkan API - although I'm not sure how that'd work on OSX?
Is there something about the design of rust that makes variadic functions problematic, or does the community just not like them? &amp;#x200B; Rust often introduces a lot of syntax noise to deal with the borrow checker, its a shame to have to do it again for something simple like this. &amp;#x200B;
Whoops, sorry. Saw at the top of HN today and cross posted here. Shame me üîî
I have found it mostly useful when debugging `unsafe` code, since any seemingly unrelated change to the code can "fix" a wrong assumption. Otherwise, I'll readily admit that unit-test and println-based debugging have been working wonders.
Total noob question. How do you add an extern crate into an IntelliJ Rust project?
Hello! Is there a performance difference between Rust std mutexes and pthread-mutexes on Linux platform and what kind, if there is? How about compared to futexes? Thanks for the info!
&gt; It looks like you need to write all the shaders in another API's language No. `gfx-hal` accepts spir-v. &gt; Also I'm on mac `gfx-backend-metal` implements `gfx-hal` traits and uses `Metal` under the hood. Vulkan needs a bit more thinking than OpenGL. But once you are familiar with new concepts it all just makes sense and is not too hard to use.
Maybe Vulkano might be of interest to you? https://github.com/vulkano-rs/vulkano
\&gt; It looks like you need to write all the shaders in another API's language. &amp;#x200B; Vulkan uses Spir-V which is an intermediate representation of shader code. So you can use whatever shader language you want, as long as you compile it down to Spir-V. There is no reason to learn opengl first if you plan to use glsl for your shaders, at all. &amp;#x200B; &amp;#x200B;
Same way you would do it with any other editor/IDE, add it to Cargo.toml and add to lib.rs or main.rs (not needed for 2018 edition IIRC).
You are lucky! When I searched my local library for Rust, I only found books like this: [Rust: The Longest War](https://www.goodreads.com/book/show/22609454-rust) At first, I thought that book was all about the Borrow Checker.
I want to suggest something that is not on your list: An extensive beginner focused video tutorial series! There are quite a few videos about learning Rust out there. But the problem is that it's mostly about stuff that is pretty easy anyway. For-loops, if-clauses, structs, etc. Most people won't have problems with that. There should be more videos like this out: Strings, stack vs heap. stack vs heap with many examples (Strings, Vecs, References, etc), How are different types stored in memory, Traits, and so on. All the videos tutorials on the intermediate Rust topics I saw were too advanced and with too little in-depth explanation of what is actually going on. If Rust needs one thing to make it more beginner friendly, that is it!
TRPL is 520.
The number one bit of advice I can give you is that beginners don‚Äôt understand things so fundamental you don‚Äôt even realize that they‚Äôre concepts you have to understand. ‚ÄúHello world‚Äù is actually hard, for example. You‚Äôve got to go as simple as possible at the start, and then even simpler than that.
Hi there! I've got a question about Zero Cost Abstraction / Optimization. Are those two pieces of code equivalent in terms of compiled code? Is static code like `factors.iter()` extracted out of the closure? ```rust fn sum_of_multiplies (limit: u32, factors: &amp;[u32]) -&gt; u32 { let factors_iter = factors.iter(); (1..limit).filter(|i| factors_iter.any(|f| i % f == 0)).sum() } ``` ```rust fn sum_of_multiplies (limit: u32, factors: &amp;[u32]) -&gt; u32 { (1..limit).filter(|i| factors.iter().any(|f| i % f == 0)).sum() } ```
No worries; normally reddit attempts to catch duplicates but slightly different URLs let posts go through... and we moderators step in :)
I totally agree. &amp;#x200B; That's why I was looking into building a platform so the student does not have to install, compile or even have to run `cargo new`. A nice interactive lessons probably similar to the oni interactive tutorials but for learning programmation/rust. &amp;#x200B; [https://onivim.github.io/oni-docs/#/./oni.wiki/Features?id=interactive-tutorial](https://onivim.github.io/oni-docs/#/./oni.wiki/Features?id=interactive-tutorial)
Well yes, but with rust being build around the idea to catch hole classes of problems at compile time, I thought this would neatly fit into that concept. That of course doesn‚Äôt mean that rust did that first. 
Ah yes, that would be an option. Thank you for sharing :)
The implementation of libstd's mutex is platform-dependent, and on linux it just uses pthread-mutexes: see https://github.com/rust-lang/rust/blob/master/src/libstd/sys/unix/mutex.rs
Awesome! I had trouble building the docs for my rocket + diesel app. Hopefully this fixes that ü§û
Thank you! I want to not move z in lambda, as I'm using it in main thread as well. So, it looks like I understood Sync trait wrongly, and it doesn't allow me to magically have something shared between threads in a safe way. I'll give a shot with scoped_thread, thank you!
Yay! As a heavy MSYS2 user, the progress bar was getting on my nerves... :P
What do you mean? It is. Those are guides for the game
Ok, I understand, thanks! I think for a lot of real applications you don't need that in the enclave, you only need to do some sensitive crypto calculation, you are only using std for things like Box, Vec, and to support some serialization cofe. Then it would be simpler to just use the regular standard library, with a custom allocator implementation, than port all of the libs you want to use the tstd. I can see that having a layer that escapes the enclave transparently can be useful though, I didn't realize thats the primary motivation here. Even so I would have hoped that third party libs don't need to be "patched" and some kind of linker trick could be used. For instance in emscripten, another situation where you run native apps in a restricted environment (a web browser) all the libc and system calls must be intercepted and emulated, but it doesnt require changing system headers or downstream code. Maybe this is a limitation of rust tooling / crate system? Thanks for your answer
The OP was talking about gfx pre-ll, where we require the user to provide GLSL (multiple versions) and HLSL separately.
Sync is indeed the marker trait for safe sharing between threads, but this is a simple lifetime issue: The compiler thinks that z might be dropped before the closure runs/finishes. You can wrap it into an Arc&lt;T&gt; and then share the Arc (atomically reference-counted pointer) between threads. Although you will want to also use a Mutex, because I assume you will want to modify the Vec.
I like to split up my source code files. I always put the `extern crate` statement at the top of the source code file that contains the source code that needs it. That always worked just fine. However I just played around with `serde_derive` and I got this error message from the compiler: error[E0468]: an `extern crate` loading macros must be at the crate root --&gt; src/settings.rs:2:1 | 2 | extern crate serde_derive; | ^ This is what I have on top of my settings.rs file: #[macro_use] extern crate serde_derive; extern crate toml; So I moved it to main.rs and it works now. But I still like to know what's going on. So if I use `#[macro_use]` I can only put this into my main.rs and not into any other source code files? Is that the only exception? Should I put all my `extern crate` statements into my main.rs? How do you guys do it?
Other than /u/Quxxy's solution, for very long strings something like this is also possible since include is another "macro" (of the magical builtin sort): format!(include_str!("some_template.txt"), a, b, c)
Sorry if it was unclear. I was looking at Vulkan tutorials not because I have experience with Vulkan, but because Gfx-Hal is similar in design. So unfortunately I have no Vulkan knowledge to transfer. Also ‚Äúexpect a bumpy ride‚Äù meaning what? As in it‚Äôs very hard, or as in it‚Äôs got some Mac bugs or something?
Sounds like it deserves an issue on github...
In your opinion does it make more sense to learn OpenGL first and then gfx-Hal? Or is it just a larger learning curve that should be tackled head on? 
Actually I didn‚Äôt know that part changed after pre-II. Good to know! 
I would \_really\_ love to see optional arguments in rust. It is definitely the feature that I miss the most in my every day work and it would be an improvement similar to impl trait. &amp;#x200B; But unfortunately I'm probably in the minority :(
&gt; Should I put all my extern crate statements into my main.rs? Yes, in the 2015 edition this is most idiomatic. But `extern crate` is being phased out anyway, in the 2018 edition you won't need it anymore, and extern crates will be in the implicit prelude for every module. &gt; But I didn't have to "import" serde, meaning there's no extern crade serde; in my source code. Is that normal? Can crates just be used by other crates without needing to import them into my source code? Or is that only working because I used #[macro_use] ? That might have to do with the above, depending on which compiler version you're using.
I think it's more a technical problem than opinion problem. :)
Hmm interesting. So how do I import crates in the 2018 Edition? Will the compiler just do it automatically based on what is in cargo.toml? That would be much better. Is there a way to enable this in the current stable version? I'm using 10.30.1
I'm not sure why? I can imagine a working solution with pretty simple macro. Maybe even proc macro that would generate 'normal' macro. The only problem with macros is that it would be only useful for free functions and not for methods. On the other hand, the biggest problem currently is with number of combinations. With 3 optional arguments, there are 8 combinations. It may be not an easy task to find good names for all of them. It is also not convenient to create 8 fns. Creating builder just for this one fn args is also not convenient. BUT it is not a hard task for the compiler to generate those 8 fns for you and give them any names and then translate those names wherever the fn is called. What is the technical problem here? Most languages have optional args and fn overloads (I miss that one too!). Why is it not a problem there? &amp;#x200B;
You can use the [xargo](https://github.com/japaric/xargo) utility to swap in `sgx_tstd` for `std` in the sysroot. That should allow most std-dependent crates to compile out-of-the-box.
What happened with MioCo? Is there any good alternatives to implement something like goroutines? I've heard that there will be support of async in next version of Rust.
Alright. Thanks a lot!!
Thanks! I haven't done much compiler debugging and I totally forgot about `-Z`. I filed a bug about this here: https://github.com/rust-lang/rust/issues/55795
Has anyone seen/reported that Travis CI logs also look a bit off, presumably since the introduction of the progress bar? 
If I declare a struct as "pub" so I can use it in another source code file, why do I have to mark every single field as pub as well? Or in other words: What would be a use case where the struct is public, but some fields of the pub struct are private?
Yeah, I was trying to figure out what I was looking at because there are already a bunch of bugs about performance compiling arrays. I think this is a new one though, so I filed a bug here: https://github.com/rust-lang/rust/issues/55795
I took the challenge and put together a very easy to use solution that uses the gtk event loop. [https://github.com/Phaiax/gtk-rs-state](https://github.com/Phaiax/gtk-rs-state)
&gt;If I declare a struct as pub so I can use it in another source code file, why do I have to mark every single field as pub as well? You don't. Maybe you are mistaking it for pub enums? &gt;Or in other words: What would be a use case where the struct is public, but some fields of the pub struct are private? The idea is to hide details users of the struct does not need to know about. i.e. [Encapsulation](https://en.wikipedia.org/wiki/Encapsulation_(computer_programming)). It could be that those fields contain data that needs to be processed when they are being accessed. Or that they never need to be read directly by a user. Like a flag that modifies how a type behaves.
No the compiler told me. It said something along the lines of: struct "mystruct" is private. Then I changed it to pub and now I can use it. Meaning create a new struct even though the struct statement is in another file. So it looks like this: `let test = otherfile::TestStruct {...};`
This might be a bit tangential to the intended discussion here, but this caught my eye: &gt; heavy use of code serializing and deserializing floating-point numbers Is it an option to ser/de to a binary format instead of a string? This is guaranteed to be faster than any string parsing algorithm.
It is, when I can. Unfortunately, a lot of people decided plain-text and XML formats were the ideal choice for mass spectrometry data, and the only binary formats are HDF5 or proprietary.
Maybe use [this crate](https://github.com/peter-bertok/float_fast_print)? I have seen issues about speeding up Rust's float formatting. I wonder if it would be worth it to try to integrate a better algorithm.
Unfortunately, the issue is string-to-float, not float-to-string (which is decently fast, only ~3x slower than dtoa and my inplementation).
Minor point: you probably should've used Python's `repr` rather than `str`; `str` will truncate, but `repr` guarantees it will roundtrip exactly the same. As a result of thos, your corpus is presumably biased towards low numbers of digits rather than being uniformly distributed over all possible floats, as you seem to have intended.
Good point, let me redo that.
That's arguable. What if you want to specify only one of those arguments?
You'd think this kind of thing would be possible to optimize with SIMD.
Yes, the problem is that Travis lies about it being an xterm terminal. You have to set TERM=dumb to get the appropriate behavior.
I'm not a float parsing expert, but SIMD is all about batches of operations on many lanes at a time. I don't see how you could make that work with one string into one float. Even several strings into several floats would be hard because each string follows different code paths, and in SIMD that means following _all_ paths and then merging the branches back together with masks.
(repl.it)[https://repl.it] is a fairly barebones online IDE that supports many languages, including Rust. You write code in one panel, then click run, and your code is compiled/executed, no need for a command line or anything like that. Also random side question, do you speak French or was "programmation" just a random typo?
I would use Rc::make\_mut, but it clones if there's more than 1 strong reference, and if I have 1 local reference, and 1 reference in the tree, then I have 2 references. Is there a way to get around that?
I agree that the things you listed are important to know, but novice programmers do actually have a lot of trouble with control structures and other concepts we might consider quite simple.
If your node still has a reference to it in the tree then you need to clone it - I don't mean because of rust, I just mean in general, that's how it has to work or else you'll be modifying nodes that someone else is still using. If you don't need to modify the node, don't call `Rc::make_mut`, just clone the `Rc`?
What editor do you use? Both vim and emacs support a notion of marking your current location so you can quickly jump back. I know editor suggestions for language issues can be annoying, so sorry if you already have a good system.
Cool, that‚Äôs almost just down the street from me. Good luck!
The idea is that you use SIMD to verify and convert character bytes into numeric bytes. Then you fold them into a number.
I was in attendance at the incremental datalog talk as well. Very exciting stuff, and I'm interested in how it might play out for Rust. I was planning to be at the meetup tonight as well but unfortunately something came up. It's a really cool idea to involve the local meetup groups with the conference!
Ah the double whammy. 
This blog post by Niko may be helpful http://smallcultfollowing.com/babysteps/blog/2018/02/01/in-rust-ordinary-vectors-are-values/
Here's a playground example of how it works: https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2015&amp;gist=6dc2581076a37dcf28654de90929b47a You can see that initially I only do a *shallow* clone, so the tree itself is not cloned. Next I recurse on the tree with the general purpose "modify_tree" method. The closure passed in gets to decide whether to modify that part of the tree or not (in this case I modify it (and its children) if it's value is greater than two).
Yes, you're right! Thank you a lot!
Ahhh. It's the `&amp;mut Rc&lt;Node&lt;T&gt;&gt;` portion I was missing, I think I understand now. And yes, I am going for the top-down approach. The context is that I'm trying to build a data structure for a hex-editor supporting fast undo and redo, which is where the requirement for referencing previous trees comes in. Thanks both you and Diggsey.
One question is whether real-world data hits the slow path nearly as often as your randomly generated floats. It seems like randomly generating floats as integers that are simply reinterpreted as floats is likely to give you an awful lot of values that are quite unlikely in real life, and be much more likely to hit the slower paths. For example, taking a look at some [real world datasets](http://neo.sci.gsfc.nasa.gov/servlet/RenderData?si=1582435&amp;cs=rgb&amp;format=CSV&amp;width=720&amp;height=360), it looks like it's quite common to get values that definitely hit the fast path. It would probably be worth finding some real-world data sets that you are interested in, and try your benchmarks on those, rather than benchmarking all possible bit patterns of floating point numbers. It would probably be a good idea to have a "fast floating point parsing" function that would have slightly relaxed precision requirements. If you're interested, that could be prototyped outside of the standard library and then proposed for inclusion if you think it's a compelling enough use case. One of the perennial struggles in Rust has been correctness vs. speed, ad correctness vs. usability. An example is hash tables, where the default hashing function is cryptographically randomized to prevent DOS attacks, but that makes it slower than many other possible hashing functions; there is an opt-in faster hashing method available that is not secure against DOS, but a lot of people just notice the defaults and get frustrated.
You say it yourself: Rust tries to get everything correct. Correctness in this case is parsing the specific float represented by the string. Comparing floats to integers seems like apples to oranges. In any case, have you compared Rust's algorithm to (correct) ones in other languages/libraries like `strtod`? (My experience with scientific computing is that understanding, controlling and limiting errors (like accumulated round-off error) is crucial, and starting an application by introducing error at the input stage seems unfortunate.)
Don't worry too much about getting your design perfect on the first try. If you put the work into building it, then six months from now, you absolutely will have a better idea how to build this thing. The thing you should focus on learning is the discipline of writing clean code with decent unit tests, with as little duplicate logic as possible. Try to write small functions that only do one main thing and don't take arguments they don't need. Writing unit tests pretty much forces you to write code that's modular and reusable... Focus on these things and then it probably won't be a huge chore to make changes down the line... If you find you need to make a change and your design is in the way, refactor your code so the change is easy to make. Make sure you have tests for anything you might be about to break, then make little changes, and run your tests after each change. As for your studying rust plan, focus most of your effort on whatever you're trying to do in the moment, but maybe spend a little time each day/week/whatever learning the concepts that are new to you, and -- even just as an exercise -- try to think of how you might apply it to the things you're building. Maybe you can't. That's okay. Maybe you study 10 things and none of them really matter to you... Maybe the 11th one will completely change how you think about your program and you'll be all excited to make a big rewrite. It's okay. When it comes down to it, you basically need loops, conditionals, mutable state, and a few core hardware operations to make things happen in a computer. (Fancy functional programming languages might hide these things but they all translate into pretty much the same concepts under the hood.) So don't worry too much about learning every little feature of the language up front. Just focus on getting the computer to do what you want, and pick the rest up along the way.
Thanks so much for your input! I already started porting my tool and so far it's been way easier than I thought. It's mostly been functions, borrowing, structs, and returning Results. So basic stuff. But actually just coding is really helpful. I'm sure at some point I'm gonna be all over smart pointers! Haha ;-)
I don't dispute that.
Also a great idea (benchmarking against real data), I'll look at this tomorrow. For the subject of defaults and correctness, I feel this is different, since it's quite easy to write an Eq or Ord float wrapper that errors on NaN, or use a custom hash function (like xxHash) with a hashmap, just like in C++. But I totally get you about the how defaults should favor correctness, I just feel it should be easier to opt-in to speed in this case.
Yeah, I haven't visited the actual public libraries in my town since I was a child (some school field trip). The city where I went to uni for a while had a *really* good university library, IIRC whole floors dedicated medicine, physics, CS, etc. each (although we mostly went there to do group work in the quiet rooms one could reserve).
Thanks!
You want /r/playrust
Thx
There are always bugs ;)
Hey, I'm interested in doing the same thing as you, namely: writing a 3D renderer from scratch using nothing but SDL2-rs. Do you mind sharing which resources you used to learn the prerequisite math? Also is your code open-sourced anywhere? If not would you mind open-sourcing it? I'd be very interested in seeing it.
Just throw SIMD at it, it will solve, trust me ^^^^^^^^/s
Yea - that's where I can't find Pacman. TBH Whenever I run into a programming package that asks to compile a C lib, it's probably going to be a pain, esp on Windows.
`tokio::spawn` only works within a future given to `tokio::run`. Try doing `tokio::run(futures::lazy(|| { /* spawn in here */ }))`. If you're using async/await it would just be `tokio::run(async { ... })`.
/r/playrust You might want to *look* at a subreddit's contents before blindly posting to it.
I'm trying to understand why Box can be used in situations that Rc cannot. For instance, this compiles: struct Tree { children: Option&lt;(Box&lt;Tree&gt;, Box&lt;Tree&gt;)&gt;, } fn item_check(tree: Box&lt;Tree&gt;) -&gt; i32 { if let Some(c) = tree.children { 1 + item_check(c.0) + item_check(c.1) } else { 1 } } But this does not: use std::rc::Rc; struct Tree2 { children: Option&lt;(Rc&lt;Tree2&gt;, Rc&lt;Tree2&gt;)&gt;, } fn item_check2(tree: Rc&lt;Tree2&gt;) -&gt; i32 { if let Some(c) = tree.children { 1 + item_check2(c.0) + item_check2(c.1) } else { 1 } } Getting error: | if let Some(c) = tree.children { | - ^^^^ cannot move out of borrowed content | | | hint: to prevent move, use `ref c` or `ref mut c` Shouldn't Rc work like Box in this situation?
Can we stop with this "announcing" shit every single release? It's a patch fix. It's not a big deal.
&gt; Shouldn't Rc work like Box in this situation? No, for pretty much exactly the reason the compiler told you. `Rc` is a *shared ownership* pointer type. You can't just up and rip data out of `tree`: someone else might be using it. `Box` is a *uniquely owned* pointer type. You *can* just up and rip data out of `tree`: no one else can possibly have access to it then and there. Honestly, both functions are kinda wrong. You shouldn't be passing in `Box&lt;Tree&gt;` or `Rc&lt;Tree2&gt;`, you should be using `&amp;Tree` and `&amp;Tree2` since you only ever look at the contents of the nodes.
any more of this happening soon?
Hey good catch! I speak French natively. I totally meant programming :S
This article was mentioned in Niko's post from Splash 2018 [http://smallcultfollowing.com/babysteps/blog/2018/11/08/splash-2018-mid-week-report/](http://smallcultfollowing.com/babysteps/blog/2018/11/08/splash-2018-mid-week-report/) I found it pretty interesting. To download, click on the pdf link in the file attachments at that URL.
The default syntax also lacks a way to make some arguments mandatory and some optional :(
Enuma are tagged unions under the hood, stack allocated by default, and the size of the largest variant (+space for the tag)
Thank you! I ran into the docs building bug and having it fixed is real nice.
The meetup was great, for the record! We ordered too many burritos, I blame you for not being here to eat them!!! :)
I definitely agree, the setup process is PAINFUL. I'm dreaming of someone implementing a glsl to spirv compiler in rust so we don't need to do this anymore. Your issue sounds like msys2 is very broken. Try reinstalling it or filing an issue on their tracker https://github.com/alexpux/msys2-packages/issues Alternatively you could install cmake, python and gcc using any other method, maybe try chocolatey or manually installing them.
OK, then why does this work for &amp; but not for Rc, since they both are shared owners of the tree: struct Tree3&lt;'a&gt; { children: Option&lt;(&amp;'a Tree3&lt;'a&gt;, &amp;'a Tree3&lt;'a&gt;)&gt;, } fn item_check3(tree: &amp;Tree3) -&gt; i32 { if let Some(c) = tree.children { 1 + item_check3(c.0) + item_check3(c.1) } else { 1 } } &amp;#x200B;
Because `Option&lt;T&gt;` is only `Copy` if `T` is `Copy`. `&amp;T` is `Copy`, `Rc&lt;T&gt;` isn't. `Box&lt;T&gt;` is not `Copy`, but in this case it doesn't need to be since you can move a value out of a `Box`, but you can't move a value out from behind a `&amp;T` or `Rc&lt;T&gt;`. 
Note: this has backported one of the rustdoc fixes (it links [this](https://github.com/rust-lang/rust/pull/54199) PR), but not all of them. These ICEs during cargo doc are still present on stable 1.30.1: * https://github.com/rust-lang/rust/issues/55001 * https://github.com/rust-lang/rust/issues/54744 * https://github.com/rust-lang/rust/issues/55690 They are all fixed on nightly, by [this](https://github.com/rust-lang/rust/pull/55258) PR. It's in the process of being backported to beta, maybe there'll be a second point release.
Thanks, last question hopefully: why can `&amp;T` be `Copy` and `Rc&lt;T&gt;` cannot?
Unsubscribe if you don't care
Because if you did a blind bit copy of an `Rc&lt;T&gt;`, the reference count wouldn't be updated. Hence why it's `Clone` but not `Copy`.
I guess I'm confused about why Rc requires ref counting and &amp; does not. &amp; is on the stack, Rc is on the heap. I guess Rc does not have lifetime constraints like &amp; does? Sorry if this is obvious, its getting late :)
"Reference Count", "RC". The whole point is to have a pointer type that is *shared* and *owning*. `&amp;` is a shared *borrow*. `Box` is *unique* and owning. For any kind of shared pointer to work, there has to be some kind of coordination between the multiple owners; computers aren't magic. In the case of `&amp;`, it's the compiler doing it at compile-time, which is how lifetimes work. For `Rc`, it's the reference counting code in the type's implementation because it *can't* use lifetimes.
Unsubscribe from what? I don't subscribe to their release notes. Unsubscribe from one of the few subreddits that actually has good information because 1 group thinks that they have to "Announce" a release that comes out every week? Makes a lot of sense.
This subreddit...
/r/playrust
yo dude you don't have to click it if you don't want to jesus christ. just.. look at the posts you think are relevant, and skip the rest like a normal sane person. 
That's it, we do need to have enough of a window for remote coordination and collaboration. Where you actually live is a different matter.
We have an existing (and extensive) React + Typescript frontend, which we intend to keep. I don't see Rust + WASM as sufficiently mature for a complex web application frontend. Even if Typescript is not Rust, it is still really nice, and its structural type system allows for a different level of flexibility.
Whoops. Typo.
The microcontroller should have an RTC right? You may need to write the firmware for it but it'll give you a very accurate time you can use to use against 
It sounds like you're trying to do something similar to to the [im](https://crates.io/crates/im) crate. Maybe take a look at what they're doing?
Man, I'd probably love a joint Rust/Scala meetup. They are the two I work with most frequently these days and both are easily in my top 5 languages worth learning. I should see if there's anything like that here in the Pacific Northwest.
This is slow. You should generate random once, and it make it u8. üòâ
Yep, this is fixed at https://github.com/rust-lang/cargo/pull/6281. It might be a little while before it hits nightly, though.
&gt; I've more or less entirely given up on vulkano/glutin/winit and all the private projects that I was building with them. I think this would be useful for your patreons to read who subscribe to you for these libraries. I was under the impression you were planning on coming back to vulkano at least eventually, but it seems that is no longer the case.
What does serde use? They should be interested in improvements too
Have you considered binary formats where Rust might parse faster?
Not this month, sorry. There was a clash at the venue. We're having a demo night in December, which will be the last Meetup unless someone takes it over from me or we find a new venue, as I'm changing jobs.
Pretty cool how that doesn't use `unsafe`
I assume you built in release mode as well?
Some crates have a `prelude` module containing commonly used traits and types for this reason. Then you use it like this: use diesel::prelude::*; There's a tradeoff though, because at some point in the future they could add something to the prelude that has the same name as something in your local module, creating a conflict and causing your code to stop compiling.
Thank you, that was very helpful response! Now I see my question was simply wrong. So the only valid code is the second one since after the end of `factos_iter` iterator is reached it will no longer produce any values for any other number in a range greater than `1`.
To be fair, the previous version caused some breakage on many high-profile crates and affected a lot of users. It is only sensible for the Rust team to publicly announce that a fix is available in this patch release and that the underlying issue has been resolved.
But it is exactly that: a release announcement. To make sure everyone knows and is aware. That's literally what "announcing" is there for.
Thanks!
oh, that occurs inside an `if self.mem.len() &lt; other.mem.len()`
&gt; "{}".format(x), however, does, for some weird reason. That calls `str()`. Try `"{!r}".format(x)`, which calls `repr()`.
Can you post complete code that produces the compiler error, and simplify it as much as you can? I tried to reproduce the error on the playground, but no luck https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=789c9cd0d10d0a85f4b73f5abd016f19
Since you apparently can't ignore the posts like any sane person who don't want to look at them would do, how about you just filter out the posts using RES?
^The linked tweet was tweeted by [@repi](https://twitter.com/repi) on Nov 08, 2018 09:49:36 UTC (48 Retweets | 210 Favorites) ------------------------------------------------- [@mitsuhiko](https://twitter.com/mitsuhiko) [@EmbarkStudios ](https://twitter.com/EmbarkStudios ) No the opposite! :) We are going all in on Rust and will build our technical foundation/platform in it! May need some C++ or Go here and there, but Rust is our main language for [@EmbarkStudios](https://twitter.com/EmbarkStudios) ------------------------------------------------- ^^‚Ä¢ Beep boop I'm a bot ‚Ä¢ Find out more about me at /r/tweettranscriberbot/ ‚Ä¢
Welcome to the community! &gt;For example, for parsing a string-to-integer, my code is only ~5-20% faster than Rust's version, and converting number-to-strings was only ~3x faster (**which for most applications is insignificant**). Emphasis mine, but really, if you have code that performs better than something in the standard library while being a drop-in replacement, *we would love to have it!* So please consider making a PR, no matter how insignificant the code or performance might seem! There is a great precedent in [the PR that updated the sorting algorithms in std](https://github.com/rust-lang/rust/pull/38192) to what they are today, if you want something to look at, not to mention a bunch more documentation and people ready to help out today to make it happen. :)
This. Don't repeat the "ffast-math" naming mistake.
I'd be interested in the effect on *overall* runtime. Is their application parsing floats so frequently that this algorithm is *four hundred times slower* because of it?! 
Off-topic but I'd be interested to see a benchmark of this against Ryu for float-to-string (there's a Rust port of Ryu too).
On the Raspberry Pi 2: error: could not rename component file from '/home/foo/.rustup/tmp/muk92rmg75hujd9x_dir/bk' to '/home/foo/.rustup/toolchains/stable-armv7-unknown-linux-gnueabihf/share'
Nice! I had this issue as well. I think that it was the case in the projects that define a procedural macro. I'm happy that I can now generate my documentation ^_^
I decided to try to create my lib without actix for now. If I can't I'll come back to actix and expose the futures I think. Thanks
This is it! Is the reason for the deadlock and I don't know how I can see it before. Anyway I'll try to create the lib witouth actix and if I can't I'll come back and embrace how actix do the things and expose futures and I'll make everything actors. Thanks 
&gt;newly found game studio What, did they find it at a yard sale, or something?
Doesn't feel comprehensive enough because it omits IRC and gitter content. 
Embark, a newly found screw driving studio, will use hammers to drive screws.
Just to join in with unrelated advice to your questions. &gt; I was thinking about going through the basics while slowly building a very tiny console game. I like this idea. Because you are working with total beginners i suggest you don't even mention anything about (mut)references . You can have your game be ' fn update(state,action) -&gt; state '. Just clone everywhere. This will allow them to get something working before their encounter with the borrow checker. 
Why hyper is your choice? Use something like warp or actix, there are some easy examples with websockets: [https://github.com/seanmonstar/warp/blob/master/examples/websockets.rs](https://github.com/seanmonstar/warp/blob/master/examples/websockets.rs) or [https://github.com/actix/examples/tree/master/websocket](https://github.com/actix/examples/tree/master/websocket).
I need somebody.
Probably see Rule 2, "Constructive comments only".
I frequent this subreddit also, and are a big fan of Rust, so if you have any questions just ask away! :) \-- repi
Yes it is 'founded'. Fund is a totally different thing. 
&gt; My application for Rust is scientific computing, which unfortunately frequently requires the heavy use of code serializing and deserializing floating-point numbers, so I may be biased here. For my application, it's not worth sacrificing a small level of correctness for ~400x slower code, and I'm wondering if Rust should provide `from_str_fast` for floats (based off the naming of `from_str_radix`), which sacrifices correctness slightly for dramatic performance gains. Even if serialization of floats to/from strings were as fast as in other languages, serialization to binary is still orders of magnitude faster. I use Rust for scientific computing and only serialize/deserialize floats to binary. I used to do the same in C++, and the performance is pretty much the same. 
 impl AsRef&lt;str&gt; for [Ascii] `Self` is `[Ascii]`, so `&amp;self` is `&amp;[Ascii]`.
I assume your c++ must be godlike at this point and essentially 2nd nature to you. What feature or experience specifically made you decide to use rust?
&gt; and the only binary formats are HDF5 or proprietary. There is also NetCDF / pNetCDF, or you can just deserialize to a binary blob using MPI I/O or similar
Could you maybe help with this issue ? https://github.com/alexcrichton/jemallocator/issues/91
&gt;So, I'm using homebrew for keeping up with Rust There's homebrew for Windows??
* Did you hit any major language limitation? * On top of my head, const generics, HKT and variadic templates are missing in Rust compared to C++. Which language features do you anticipate the most? * Which version of the compiler are you using? Stable, nightly or do you have a custom fork? * Which graphic APIs do you currently focus on the most? * Have you experimented with codegen? Anything that resulted in more/less optimal code compared to the C++ version? For example [Automatic vectorization of a simple loop](https://www.reddit.com/r/rust/comments/88yu9x/automatic_vectorization_of_a_simple_loop/) where C++ would only vectorize the loop with the restrict keyword. Or currently there is no easy way to enable `fast-math` in Rust outside of nightly. * Any thoughts about Entity Component Systems? Is that something that you have considered for your technology stack? * Do you use any open source projects? Are you allowed to share the names of the crates that you are using? 
You might want to check out how [criterion](https://github.com/japaric/criterion.rs/tree/master/src/plot) uses gnuplot to produce plots.
In all cases I've been doing embedded work professionally (both with bought and inhouse developed hardware) we've used a single board per project (often reused from a previous project) and then just updated its BSP as needed if the hardware evolved. I don't see much of a scalability issue there, you plan a few hours on it for the project start and if you decide to update the hardware. The current rust HALs are already different for different boards, so you still would have to configure the BSP per board if you used rust in the project. For less common CPUs the vendors provide their own fork of gcc so you would often have to port that rust compiler for their gcc, a much larger job compared to creating a BSP. 
&gt; The use of FutureObj here reflects the current way to express boxed futures, which is expected to change to Box&lt;Future&gt; in the near future. While boxing the futures has some performance cost, it‚Äôs expected that the cost is extremely minimal, and boxing allows us to avoid much more complicated type tracking (and associated lengthy compile times). That's an unnecessary compromise that screams for [`-&gt; impl Trait` in `trait`s (RFC)](https://github.com/cramertj/impl-trait-goals/blob/impl-trait-in-traits/0000-impl-trait-in-traits.md).
r/ggplot2 ^(love from r/foundthemobileuser)
It's interesting, but it amounts to write it on my own given that they state `This is the graphing subcrate of Criterion.rs. It is considered an implementation detail and is not stable. Anything may change at any time with no warning, including the public API. For further information, see the main repository.` in their README.
I've never installed on a mac machine or used homebrew, but from the error message it looks like it's complaining about something in the rust source code. ([this file](https://github.com/rust-lang-nursery/lldb/blob/fdea743be550ed8d7b61b2c908944cdd1290a6ad/unittests/SymbolFile/DWARF/Inputs/test-dwarf.exe)). Do you think you could try to install just rustc/cargo/the compiler without std-src? If it is objecting only to this file in the source code, installing the compiler alone should work. Again I haven't used homebrew so I don't know how possible this is. Adding an exception for the file in the antivirus might also be a possibility?
Sure! However, if you search through the plot libs written in Rust, they're very young, even exploratory. The Python ecosystem has many viable, mature and feature-rich solutions. If you're trying to get work done *today*, extending Rust with Python is most likely your way forward. Port bokeh over if you're up for the challenge. :) bokeh would undoubtedly bridge to Rust if doing so boosted performance. That'll be a major undertaking but at least with porting you're not starting from scratch. 
Which appears to omit that Redox is POSIX compatible in its last point, contains a libc, etc. such that many unix apps work for it after a recompile and most of them are easy to port.
I don't see extending Rust with Python for generating beautiful plots as a negative. I have a strong feeling that an increasing number of scientist-programmers are trying out Rust and like you will look for pure Rust solutions. As you're probably batch processing plots, I don't see how that bindings guide will help your work? 
Early processing stages when you filter the raw data from sequencers or the like are mostly limited by IO. This might be a bit of poetic exaggeration, but quite possibly not by much.
Could you maybe clarify what systems you are building with rust? 
You're right, I overlooked the brackets.
Just last weekend I played around with std futures and was annoyed when this didn't work. Now things look much better. I am still a bit confused about the whole Wake topic. In order to construct a LocalWaker for calling poll, I need an Arc to struct Foo { id: usize, queue: Arc&lt;WakeQueue&gt;, } to be able to wake a task. It feels weird to require an Arc inside an Arc.
Oops, I mistyped in my previous comment. I meant "that this ~~algorithm~~ is four hundred times slower". I believe that the parsing could be that degree of performance difference.
If you haven't already read https://hoverbear.org/2016/10/12/rust-state-machine-pattern/ I recommand give doing so, it's a good read! Which is pretty much where my knowledge about the intersection of Rust and FSMs ends.
What kind of games will you make? ;)
Ah, interesting, I didn't know that!
There is also a gnuplot binding: https://github.com/SiegeLord/RustGnuplot
&gt;I was under the impression you were planning on coming back to vulkano at least eventually Well, that was true not so long ago, and I'm still a bit undecided. But over time the chances decrease more and more.
For a lot of things, I don't think Rust is harder than scripting languages. And static typing will help catching bugs beginners would otherwise struggle with!
Do you know [Ruby Warrior](https://github.com/ryanb/ruby-warrior)?
To clarify, "it" here means specifically LunarG SDK. Due to &lt;reasons&gt; they don't include gfx-portability yet, but there are simple steps to hook it up: 1. Download the library: https://github.com/gfx-rs/portability/releases 2. Setup or [get](https://github.com/gfx-rs/portability/blob/eefa248319567f6348d34dc85ca362a13114e55a/libportability-icd/portability-macos-debug.json) an ICD file 3. Add `VK_ICD_FILENAMES=&lt;path_to_icd&gt;` to your environment Note: you don't actually require the SDK, and instead can just take a "vulkan.h" from anywhere and link to `libportability.dylib` directly.
Are the videos of the talks he references available somewhere?
I didn't mean to be dismissive of your post or anything, just pointing out a bit of history. I can't even say definitively that Haskell originated the idea, just that it's the oldest version of it that I know of. Nobody really owns anything when stuff is developed in the open :)
True but there are statically typed languages that are a lot easier to use. It depends on the target audience of course, but I would also not use Rust as an introduction to programming.
Be the change you want to see in the world! :)
It's been a while actually. I think the same release that added the ability to leave off struct field names if the argument had the same name. ``` let a = 1; struct Thing { a } ```
Unfortunately, it's the latter, or else I would. The XML formats in particular are a complete nightmare.
I've been happy with ragel. I believe it has rust support but I'm not sure if it's outdated.
I just mean what's currently being used as a standard in the field. There are binary formats for all the vendor machines, and an HDF5 format called mz5. The text formats (MGF) tend to be decently fast (nowhere close to the proprietary, binary formats) as long as float parsing is fast (about 95% of the lines have 2-3 floats per line, and nothing else), and you can generate gigabytes of this data daily. I could definitely convert to a custom, faster, intermediate format, but most of the other tools in the workflow expect an MGF-like file, or one of the XML formats.
Yes.
Your benchmark in C++ might not be representative of rust performance because Rust's memory layout for trait objects uses fat pointers, while C++'s vtable is on the remote end of the pointer. So in rust the fetching of the instructions doesn't depend on the fetching of the pointed-to data. &gt; It's likely that this is a question of which is slower: memory cache misses or branch mispredictions. Deciding which function to call still depends on runtime values in the pointer case. I don't know how much magic OOO-superscalars can do but I doubt they do better than branch prediction could (because branch prediction knows it's picking from a closed set of options, while the function pointers in the vtable could point literally anywhere).
In addition to the questions already asked, * Will you be building out audio infrastructure in Rust? * Do you plan to contribute back lower level crates, or keep it proprietary as is more traditional in C++ world?
As ThePowerfulSquirrel pointes out, you should probably just derive the Serialize trait for your Record type and use the serialize() method.
Good crates tho
Hey! It's not every day I'm asked to help with something important! :) Sure, I'll take a look.
&gt; The "struct update syntax" may have been around since 1.0, I can't find it in the release notes I use this often, but I did not know that one could do `.. Default::default()` :D 
We've only existed for a week so it is a bit early to say :) We have all worked on big AAA games before, but do want to explore and see what new types of games could be made!
In the hopes of maybe getting a bit more substance in your comment, why do you think rust is inherently the wrong language for game development? 
This post was the inspiration for me to write the [sm crate](https://crates.io/crates/sm) ([GitHub](https://github.com/rusty-rockets/sm)). Definitely worth the read. There's also [several](https://crates.io/keywords/state-machine) [different](https://crates.io/keywords/state) [keywords](https://crates.io/keywords/fsm) you can use on crates.io, to compare different possibilities.
It has nothing related to game development, it is just a way of describing states and transitions between them, so that your path through the code is always determined. It is also used not only in computer science but anywhere else. I hope the Wikipedia article can explain it well; https://en.m.wikipedia.org/wiki/Finite-state_machine So basically, if your system is so big and there are a lot of states and complex transitions between them, sometimes it is good to code a state machine. There are still a lot of words to say, consider reading the article please :)
Well that's got me nerd-sniped. Where would I start if I wanted to work on this? I can dig around the compiler and take a look. Curious to see if there's documentation on prior work done to this.
Taking issue with something is not a declaration of insanity. 
* Did you hit any major language limitation? Biggest one personally have been the challenges with self-referential structs (good long thread on internals [here](https://internals.rust-lang.org/t/improving-self-referential-structs/4808)) that I'm interested in, but it hasn't been a blocker and not really hit any other major limitations. &amp;#x200B; Most current concerns are around the toolchain and some lack of maturity &amp; performance, but that will come and maybe we can contribute or help sponsor some extra attention there later on. For example want to significantly improve compilation times and really support deterministic distributed compilation (not just caching) so one can iterate efficiently on large projects on simple laptops. &amp;#x200B; Another concern is dynamic heap allocations, in our engines we typically use a lot of custom arena or slab/linear allocators to keep real system heap allocation to a minimum as it is expensive operations. Which may be good to have support for in the language and libraries. Don't know yet how much of a problem it will be. &amp;#x200B; \- On top of my head, const generics, HKT and variadic templates are missing in Rust compared to C++. Which language features do you anticipate the most? &amp;#x200B; const generics will be nice for sure, but not a must, but we barely use it in C++ (most games are quite light usage of advanced features to reduce complexity and keep it understandable and explicit). &amp;#x200B; \- Which version of the compiler are you using? Stable, nightly or do you have a custom fork? &amp;#x200B; Been going a bit back and forth between stable and nightly. &amp;#x200B; Prefer to stay on stable as it is easier for everyone + CI to have a consistent environment (including running the same rustfmt, very important), but have occasionally required nightly before and right now are using it for 2018 edition preview. &amp;#x200B; \- Which graphic APIs do you currently focus on the most? &amp;#x200B; Right now it is actually Metal (with the [metal-rs](https://crates.io/crates/metal) crate), as that is easy to bring things up on, esp. when working on a MacBook :) But do expect Vulkan being our primary API for high-end graphics and exploration. &amp;#x200B; \- Have you experimented with codegen? Anything that resulted in more/less optimal code compared to the C++ version? For example Automatic vectorization of a simple loop where C++ would only vectorize the loop with the restrict keyword. Or currently there is no easy way to enable fast-math in Rust outside of nightly. &amp;#x200B; Did recently run into this issue with SIMD functions not being inlined which was a massive performance cost compared to the C version: [https://github.com/bodil/meowhash-rs/pull/2/files](https://github.com/bodil/meowhash-rs/pull/2/files). Didn't expect AES code by default not to be inlined and require these explicit target attributes, that didn't feel explicit and generated very slow code. &amp;#x200B; Haven't done any much experiments on general code generation yet, but do expect there to be more issues and concerns here and there in it just from the back that it is less mature than the C++ compilers &amp; optimizers. Likely will need future work and deep dives to investigate specific issues. Rust does indeed have a theoretical good advantage with that everything is essentially using strict aliasing. &amp;#x200B; \- Any thoughts about Entity Component Systems? Is that something that you have considered for your technology stack? &amp;#x200B; I see them as a means to an end, and I'd rather focus on achieve a specific goal at a certain scale over using a specific design/pattern/system. Have noticed a few of the Rust ESC systems but not really used them. Will see how this develops later on and which direction we go. &amp;#x200B; \- Do you use any open source projects? Are you allowed to share the names of the crates that you are using? &amp;#x200B; Still very early days, but being pragmatic and often try to use good open source libs and crates when appropriate ones are available. Rust has such a huge and fast growing ecosystem of crates, and that is super easy to use and integrate. Find it to be one of the biggest advantages over C++. &amp;#x200B; A couple of examples of crates that use are some of the standard good ones that most use: failure, structopt/clap, serde, winit, failure. &amp;#x200B; Quite interested in tokio and how that can (or not) fit together with how we do high performance job/task systems in games. &amp;#x200B; &amp;#x200B; &amp;#x200B;
It's a sort of non-obvious confluence of a couple not-really-related language features. It just also can conveniently fake named and optional arguments in functions.
I am interested in the verbiage, not the content of the release. Is every here really that daft? Can you not look at the other comments and gather context?
Fair point - my issue, again, is with the title. I feel that Major/Minor releases are worthy of "announcements", however patch versions are not and would be better positioned as "Rust 1.30.1 Released" as opposed to "Announcing 1.30.1" - the announcement title equates every release on an equal platform, which they absolutely are not. If you look at the titles in the blog: [https://blog.rust-lang.org/](https://blog.rust-lang.org/) hopefully you can better see what I'm irritated by. Additionally, thanks for being the sole person so far who hasn't just said the same thing along the lines of "unsubscribe if you don't want to see it" and instead providing substantive discussion. I genuinely appreciate it. 
Definitely true, unfortunately, the de facto standards in my field (mass spectrometry) are plain text and XML formats. XML is always going to be slow, but the plain text, which require decimal representation of floats, have parse speeds which are extremely dependent on float parsing speed.
Really, you seem to be the only one here to read all that into "announcement". Maybe it's time to adjust your interpretation (along with your attitude).
[An issue](https://github.com/rust-lang/rust/issues/55763) was opened on rustc‚Äôs repo earlier this week. The issue‚Äôs OP stated that they have reported the false flag to Avira.
good bot
The `(2..)` is an infinite range that directly implements the `Iterator` trait. So you can `filter` it, passing on only primes. The `nth` function then goes through the filtered iterator, selecting the n-th item, than discarding the rest of the iterator. BTW, this would be a good type of question to ask in the weekly "small questions" thread that's pinned here.
&amp;#x200B; &gt;Will you be building out audio infrastructure in Rust? Probably yes, feel audio is still a fairly underdeveloped area in most games, and most of the creation &amp; engine tools are also fairly old school. * Do you plan to contribute back lower level crates, or keep it proprietary as is more traditional in C++ world? Definitely would like to open source and release crates for some components, if it makes sense and isn't too specific / too entwined with everything else.
Yeah, a vtable lookup is usually worse than a branch as the . A common optimization in profiling compilers and JIT compilers is to replace vtable lookups with branches in a process called devirtualization. Basically, you end up changing `foo.bar()` into something like `if foo.bar == Bas.bar { Bas.bar(foo) } else { foo.bar() }`.
 - Presumably you've had to onboard people new to Rust, either at SEED or at Embark (Embark may be too new?). How was the experience? Where can we improve? - Specifically from a gamedev angle, what's missing? - The rust teams sometimes hold chats with production users to better gauge needs and help out: I'm not sure what the current setup for doing this is (stuff has changed), but please let us know if you'd find something like this useful! - Do y'all use clippy? (just interested, I work on it and want to know how widespread it is for production rust users)
Sounds great! I think it's very much Rust style to make focused crates with concerns clearly separated out (ie using traits or other mechanisms favoring composability). One view of a traditional game engine is as a curated collection of libraries that work together. To some extent, the Rust ecosystem fills that role (to the extent that high quality crates are available) because curation seems to be happening organically, and things tend to work together. If there's scope to collaborate on audio, I'm open to it.
Ty for the explanation
Ah, very interesting. I had assumed the vtable machinery would be more or less the same. So does this mean that the only drawback to using sum types in this context is memory bloat if object sizes differ greatly?
That was pretty interesting to read. However in the kind of state machines I have to implement, the events come from the outside world, and we don't control that. If an event happens that isn't expected, we have to log a warning or error and do something sane, e.g. return to a safe starting state (and perhaps also reset other components). So not all state machines can be handled completely statically like this. That's worth bearing in mind for someone trying to fit their situation to this, and finding it just won't fit somehow.
It could say 'founded', but 'found' is fine too. He's just making a joke.
Thanks a lot for these detailed answers. They're really useful! 
Inside `finish` method the type of `values_builder` is `T`. So you either add trait bounds that will enable everything you try to do with it or implement `finish` only for `PrimitiveArrayBuilder&lt;i32&gt;`
You can return a \`Result&lt;T, E&gt;\` from main. 
Fair enough and a big good luck! As a Stockholm native with a couple of BF hours under my belt I'll be keeping an eye on your progress ;)
It's not *really* infinite, it's a fixed size integer and those ranges don't wrap on overflow.
The concept of lazyness is pretty good explained in this video: https://www.youtube.com/watch?v=bnRNiE_OVWA He uses Haskell, but you can kinda think of Iterators in the same way.
Also each actor in an actor system is a kind of state machine receiving events. So that may be another model to look at.
[https://github.com/rust-lang/rust/issues/24557](https://github.com/rust-lang/rust/issues/24557) might be a good starting starting point. Prior to that, you could have \`from\_str(format!("{}", x)) != x\`.
How is that not solvable by just adding extra transitions for 'Received Input: &lt;not on the list&gt;' which lead to the log-and-reset process?
It would also be interesting to have a way to *control* the precision, as one of the argument. I could see an argument indicating `n` bits of precision (or digits) for example, meaning that the number rounded to `n` bits would round-trip, but no guarantee is made for subsequent bits.
&gt; but 'found' is fine too. Not quite. You need to use the past-tense form after "newly" when forming an adjectival phrase. "founded" is the past tense of "to found" (to create). "found" is the past tense of "to find" (to discover). Therein lies the humour.
The aim of the linked article was to find a way to statically check that all transitions attempted are valid, so that it's impossible to write code that has an invalid transition in it, and it achieves that. The aim is to avoid any dynamic checking at all, i.e. no panic! because something unexpected happened. This only works when all input to the state machine can also be statically checked by the compiler, i.e. it's all embedded in the code somehow. But for my case, I have to handle the full list of possible events in all states. Also, the compiler can't check my events because they originate outside the process, e.g. I'm talking to a hardware device which might fail or misbehave, or it's a protocol to a remote client who might not keep to the standard. So it is closer to the example contributed by someone else at the bottom where they match on (state,event) pairs. I have well-defined states and events, but I have to handle all combinations, and the compiler has no way to check that the source of the events keeps to some rules. So it's a different scenario completely.
It depends on how you look at the problem. Are you modelling the entire problem or just your half of it? I still see value in following such an approach with my proposed modification because I think it would result in a more maintainable design. (After all, what is a parser but a state machine, and parsers have to deal with untrusted input all the time.) Given that [Hyper](http://hyper.rs) uses a state machine approach like that for compile-time verified request-building, it'd be interesting to see how they handle responses.
Seriously. If you want to be unnecessarily pedantic, at least do so competently.
How does one fold the arguments to a macro using `syn`?
Any examples of Rust projects that use an actor system?
Okay, I think I see what you're saying now. I'll still need a big match on all the (state,event) combinations to convert a dynamic value (incoming event) into a static type for the compiler to check. Also I'll have to add transitions from all states back to my safe state. But I see now how I could use the wrapped statically-checked version to be sure that only approved transitions ever occur. So it gives a kind of static check on the code in the huge (state,event) match statement. Whether it is worth the bother for this case, I'm not sure. In this scenario there is very little value in having statically-known outcomes because it effectively just gets fed back into the huge match for the next event that comes in. However, it's quite possible I've missed something here.
You probably want to take a look at [Actix](https://github.com/actix/examples). It's the most popular actor-framework for Rust right now.
Async/await is already available in nightly. AFAIK transformation is performed in frontend (rustc) and coroutine implementation in LLVM is not used.
The \[regex crate\]([https://crates.io/crates/regex](https://crates.io/crates/regex)) is built on finite automata. Correct me if I'm wrong, but I think those are the same thing.
&gt; I've seen some pretty amazing compiler optimization around LLVM coroutines in C++ talks at CppCon. [...] Since it's embedded into LLVM, is there a way to get the same kind of coroutines in Rust? Why would you want that? Rust coroutines do not need the optimizations that LLVM does on C++ coroutines.
The question itself is too broad to be answered meaningfully. Finite state machines have so many different representations that vary wildly around the complexity vs space vs time trade off. Based on the phrasing of the question, it sounds like the OP is trying to encode a state machine in source code itself, and I think "use an enum to represent the state" is probably the idiomatic answer there.
AFAIK there is no official support for coroutines in C++ yet, and LLVM coroutines are just *one* option that is being explored, right? It's not clear to me if the latest coroutines showcased by Gor Nishanov in [his recent talk](https://www.youtube.com/watch?v=j9tlJAqMV7U) still allocate by default; would you know?
I should probably add an explanation in the README. It rewrites the function in memory at runtime. I've been using it for mocking for a few months and my computer hasn't caught on fire but its just a bit of an inherently unsafe concept.
Yep! I created an (unrelated) issue and PR for `xsv` last week, as chance would have it :) I felt this belonged somewhere separate because it needs to sample CSV rows all in a single pass, and send them to multiple outputs, which is a different style to `xsv`.
An unnecessary allocation and indirection? The question is rather vague, perhaps you could elaborat
Macros can't see the type information so I couldn't make that work I don't think.. but I'm not an expert on macros.
Yes, is that the only drawback? Are there any pitfalls I should be aware of? Is it too bad performance wise? Haven't tested yet, but I will most likely.
If you use `impl Fn(..)` (or `&lt;F: Fn(..)&gt;`), the use will be monomorphised such that it's as if you wrote it with the function call inline. If you use `fn(..)`, you have a function reference, which is a dynamic dispatch to a static function somewhere in the binary. If you use `dyn Fn(..)`, this is a reference to some context somewhere (usually on the heap with `Box&lt;dyn Fn(..)&gt;`), meaning it can capture state (closure) but also has to (eventually) dealloc that state allocation. (I think it also has another reference in the state to the actual function to be called, but I'm not sure on this.) ----- When taking a function, you want to be as accepting as possible, so you should probably take anything that implements `FnOnce` (or whichever `Fn`-family trait) via generics unless you have some specific reason to dynamically dispatch it or not to allow state closure. When producing, you want to be as specific as possible without over constraining yourself. What that means varies per project.
The scenario I'm thinking of is a communication stack. So there are various components at different layers with their own state machines to let them manage their roles. Whether you'd regard these as half-problem or whole-problem state machines, I have no idea. I guess half-problem because what's going on at the other end is ultimately mostly unknowable -- they could have been cancelled or crashed or maybe someone pulled the plug -- and we only know something when events come in, or perhaps can guess if things time out. But what would a whole-problem state machine look like anyway when the medium is unreliable, as it almost always is?
What are your thoughts about amethyst? 
So wait, `Fn` is faster than `fn(..)`?
No. Only thing that would break it is if rust changed how it encoded plain function pointers. Which isn't possible because it's stabilized to be ffi safe from what I understand.
It depends. It might simplify inlining in some cases.
He is not the only one though. I just don't think it's worth my time to complain.
I also see *your* point. I think whether it's worth it would depend heavily on the specifics of the project's design and how it is and is likely to be maintained.
No, the states are hardcoded, the event types are hardcoded, and the response to (state,event) pairs is also hardcoded. It's just that events are not fixed in the code, i.e. I don't generate a fixed known event, do a transition and then go on to generate one of a few other fixed known events, which is where all this static checking really shines. Instead I have to deal with a stream of unknown events in a loop. So the static checking of transitions whilst partially helpful doesn't give quite so many benefits.
Not a direct answer to the question, but it seems like you could use the [entry API](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.entry) to get-or-insert rather than rigging it up yourself.
To OPs question, closures don't have a writable type so can't be pointed at by a function pointer...
+1 for using loomio 
As with all things performance: it depends. As a general rule, though, yes, taking `impl Fn(..)` will provide more information to the optimiser than taking `fn(..)`. More information for the optimiser usually leads to better generated code. Another key benefit is that `impl Fn(..)` can be a zero-sized type, whereas `fn(..)` is the size of a pointer. This can make a difference if you're moving it around a lot or storing a large number of them. More importantly, though, taking `impl Fn(..)` is more general than taking `fn(..)`, and as such you can do more with it. The performance of code that isn't written doesn't matter. Yes, you should avoid clearly worse patterns, but LLVM is good. Write the code to work and give LLVM information to work with, and then profile and optimize after it's working, if it's too slow. And using the general `impl Fn(..)` allows you to choose at the call site without changing the def site.
If you just want to use an existing thing, take a look at the [`im`](https://crates.io/crates/im) crate. Or it might be interesting to take apart and see how it works.
Let's say I'm using `rusqlite` to abstract a file, and in my abstraction of a file, I would like to return an iterator, like `let iter = myFile.getIter();` The problem comes in that to iterate over rows in `rusqlite`, I first need to make a `Transaction`, then I need to make `Statement` within that transaction, then after I execute that query, I get a `Rows` struct. And of course, using lifetimes, `rusqlite` requires that `Transaction` outlives `Statement`, which outlives `Rows`. There's no way for me to return all of these with a single method, because then I would have a self-referential struct, right? Each of these needs to exist on the stack in the right order. I necessarily have to explode my API to look like this, right? let tx = myFile.getTx(); let stmt = tx.getStmt(); let iter = stmt.query(); Am I understanding the problem right? Are there any future prospects on making something like this cleaner? I don't care about these objects `tx` and `stmt` other than they have to outlive my iterator.
Good point! Then you can still throw an error from the proc\_macro to explain that they have to specify a specific version, I guess?
You get a type error because `insert` returns old value (`Option&lt;usize&gt;`), but `+=` always returns unit, but both parameters to `map_or_else` must return the same. You can wrap insert properly like `|| { self.curses.insert(...); }`, which will make the closure return `()` and get rid of type error. But then you will get "cannot borrow mutably twice ..." - I don't think you can make this work with `map_or_else`.
Ah, that makes more sense. This sounds like it might be a bug with Clippy then since it recommends that I change it, though it probably have no way of knowing if it is compatible.
&gt;Quite interested in tokio and how that can (or not) fit together with how we do high performance job/task systems in games. Note sure if that's useful, but for what it's worth, I wrote some thoughts about a tasks system using futures a while ago: [https://gist.github.com/tomaka/61807c08693604c25fc9a585220f46cc](https://gist.github.com/tomaka/61807c08693604c25fc9a585220f46cc) The TL;DR version is that it's not possible to create a user-friendly system.
I've used the Entry API elsewhere, so I'm sure it'll work, I just completely forgot about it.
Put it in a `Mutex`?
Sure. And if you have an amount so small that you want to inspect it in a text editor then performance isn't a /huge/ concern. If you have gigabytes then you're not likely to want to inspect it manually so ETL into another format is a good idea.
Could you please share your Rust code?
Given Go's dominance in this space, what does Rust provide over Go that's substantially better (other than maybe the language is more fun to write)?
I hope the fact that every blockchain seems to be written in Rust doesn't start to harm it's reputation.
FWIW nobody has been able to point Gor to a case that LLVM optimizations cannot fix yet, so it is not that he has been wrong about this either. Its mostly that having to rely on advanced optimizations to avoid memory allocations on coroutines makes some people uncomfortable.
That position would be pretty much perfect for me, except for the time zone requirement. (I live in New Zealand, UTC+12.) Hope you find a good candidate for your team!
Why can't this approach work for std?
Broken code is better than no code, especially for demonstration purposes. Maybe just a Pastebin? I'm interested in this pattern too, this could be useful in many places.
That's fair. Cargo makes it easy to publish libraries and an approximate/fast float parsing library seems like something that could be valuable. (I still think the real comparison before drawing conclusions about speed needs to be against other float parsing code, like C or using numpy/pandas to read a file of floats.)
When your closures don't capture any environment (i.e. are self-contained) you can use `fn` just fine: https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2015&amp;gist=cbc7bf4eaaed5711fd11835615149dfa If you did need some more complicated closures, you'd need to use a form of`dyn Fn`, probably `Box&lt;dyn Fn(A, B) -&gt; C&gt;` as explained here: https://doc.rust-lang.org/book/trait-objects.html
Not on stable you can't, `T` must implement `Step` for a `Range&lt;T&gt;` to be an iterator, and the `Step` trait is unstable :)
For communications, you're almost always working in half-problem state machines. Generally, whole-problem state machines are only useful in network-distributed cases where you've got lower layers of the stack providing additional stability guarantees to reduce the variety of error cases you have to deal with. (eg. A game engine where the client needs run the state machine to render the user interface and the server needs to run the same state machine on the same inputs in a headless configuration to guard against easy avenues for cheating.) Another situation I could see a whole-problem state machine being useful would be a language like [Opa](https://en.wikipedia.org/wiki/Opa_(programming_language)) with a distributed execution engine (ie. you write one codebase and it handles abstracting away the vagarities of networking) on top of a transport like TCP. It also helps if you're working in a language like Erlang with an execution model designed around lightweight processes and "processes do what they are supposed to do or fail." (ie. A language where it's idiomatic for any invalid transition to just `panic!` the state machine for that lightweight process.)
I totally agree, naming is hard and my name was bad. I'll adjust the OP for this.
`RangeFrom` follow the same rules as any other overflow, which means it will panic/wrap depending on debug/release mode. Also note that `RangeFrom::size_hint()` returns `(usize::MAX, None)`, no upper bound.
Sounds like a great place to get started, thank you.
Well who only uses stable though? /s
Actually, I'm fairly sure closures that don't capture anything can decay into a function pointer. Them not being writable is a restriction on the programmer, *not* the compiler.
**Does Rust "Hoist" Stack Allocations?** Consider the following code: fn main(){ let a = 1; { let b = a - 1; println!("b = {}", b); // prints "b = 0" } { let c = a + 1 println!("c = {}", c); // prints "c = 2" } } Would Rust transform this into something as follows, so we could save on one stack allocation and deallocation? fn main(){ let a = 1; let mut _tmp = 0; { _tmp = a - 1; println!("b = {}", _tmp); // prints "b = 0" } { _tmp = a + 1; println!("c = {}", _tmp); // prints "c = 2" } } 
If there's an FAQ list anywhere, the Entry API belongs on it.
&gt; Are there any future prospects on making something like this cleaner? Probably not. `rusqlite` hangs its correctness on lifetimes, and lifetimes are tied to the stack. Rust learning heap lifetimes isn't on the horizon at all insofar as I know, and I don't imagine Rust introducing magic invisible stack variables would be well-received. The only thing that comes to mind is changing the design of `rusqlite` to not use lifetimes; perhaps as a second, parallel API. I'm not familiar with exactly how it's using those lifetimes, so I'm not sure if the differences can be abstracted over.
When building a crate in debug mode Rust doesn't inline your functions, so patching things works. But Rust inlines libstd stuff whether you're compiling your crate in debug mode or not, so patching libstd items is unreliable because they tend to be inlined.
Not 400x slower, IO-bound tasks still are quite slow relatively speaking, but not that far off. &amp;#x200B; Say I read 51MB from a stream, which with opening and closing the stream takes \~8ms (done with a sample file), done in \~1.4MB blobs. Now remember it takes 2.8 Œºs to parse a single float on the fast track, and \~95% of the data will be floating-point data, and will have 2 floats per line. At a rough estimate (for larger floats) of 1 float every 35 bytes (1.5 million floats), and a lower estimate of 1 float every 20 bytes (2.7 million floats), it takes me 8 ms that are IO-bound, and 3 (large floats) to 7.5 seconds (smaller floats) to actually process that data. It means this task becomes definitely CPU-bound, and decidedly. I might be a bit of an edge-case, but... that 450x-650x performance difference actually matters for me.
If the compiler inlines a particular function at a given call-site, that particular call-site won't be affected by the monkey-patch, right?
Generics, sum types, absence of null, a borrow checker, and no counting based garbage collector
You are correct. I feel a bit silly now, I quickly tried it out on the playground on mobile before making that reply, I must have misread the output
I think a lot of Go‚Äôs dominance in this space comes from the fact that so many of the cloud native tools have been based in Go (Docker, Kubernetes, Prometheus, etc.). Rust makes a very valid case to be used due to the great safety guarantees. Go is pretty fast with a small runtime which makes it great for running sidecar applications in containers, but Rust can arguably be better with no gc. If anyone from the Buoyant team can comment I would love to hear how you made the decisions.
my guess is that it would constant-fold everything so that you end up printing 0 and 2 directly.
&gt; edit: the reason I'm using `as_str()` is because I want to continue passing around references to the data being iterated over without consuming the iterator. Why not just make your function take in the input `&amp;str` instead, and create a `Chars` iterator when necessary inside your function? It will still not allocate, and the string will be given back after the function finishes.
Yeah I assume the trivial answer is that Rust * is more expressive * can be tweaked to be more performant than Go since it's closer to metal + has no GC. However, I wonder how often that's actually necessary for networking infra, and if there are other things at play that are more nuanced 
You are correct about non-capturing closures decaying into function pointers, but it hasn't always been like this; as I recall the feature was introduced somewhere after Rust 1.0.
Ahhhhhhhh that makes sense. So to make this work you would need an option to rustc not to inline anything.
I don't totally understand how it works, but I don't think it adds any extra pointer indirection. IIRC it's more like a wrapper that restricts or modifies how you use whatever smart pointers you put inside of it.
Yes, but you can always Box the big variants (e.g. if they're rare).
Thanks! I don't have any experience with it, just what I read about it, but I'm pretty sure you're right about the extra pointer indirection. Hadn't thought about it in those terms, so seems like Pin would not be a smart pointer itself then. I guess it's a "something" for pointers. :) I'm not sure what that "something" category is there, and what other things aside from Pin (present or future) might have that shape.
Correct but as long as you only use this crate in debug mode inlining shouldn't happen.
&gt;Rust learning heap lifetimes isn't on the horizon at all insofar as I know To add to this, even if you want to write unsafe code that takes responsibility for ensuring that the "outlives" relations expressed by lifetimes and normally upheld by the compiler continue to be satisfied, it isn't possible to do so generically and support all the cases that "should" be able to work. See the "Limitations" section in the documentation of the [rental](https://crates.io/crates/rental) crate for more. Also, /u/krappie, you may find the rental crate useful for your application. &amp;#x200B;
The reason **was** because essentially, I wanted to pass around this `Chars` iterator between functions so that it has advanced when it's returned to the caller. But there would be times, in these functions, when iterating character-by-character was no longer necessary, and something else would be done on the remaining characters as a whole, which was a usecase for `as_str()` since it doesn't consume the rest of the iterator. Regardless, I ended up completely changing my approach in the last couple of days so that it's much less convoluted, does not end up calling `as_str()`, and frankly I'm nt even sure what I was thinking before.
I just edited my comment with a link to a podcast describing this if you want to check it out
Thanks! I was reading through the stabilization thread as well, and it really helped clarify the concepts you're describing here. I guess I was wondering if there was some more specific category that Pin falls into. For example, you could imagine Arc not existing in Rust yet then being added, so they make an announcement about "we're adding a new smart pointer!" Does Pin represent some new category of types that wrap pointers to make additional guarantees that we might see more of in the future?
Honestly it's a shame most job offers for Rust involve yet another blockchain startup that is unlikely to make it past year one.
One option would be to use Vega-Lite (https://vega.github.io/vega-lite/) from the JavaScript world. You could generate an HTML page that loads Vega-Lite and contains the JSON description of your data and graph. Vega-Lite is pretty nice! I've been poking off and on at using the Vega-Lite JSON Schema to generate Rust structs + enums using QuickType for a while, but only recently published a first draft of this: https://crates.io/crates/vega-lite I'm currently thinking about how to go further with this. 
This is a good idea, exactly the kind of thing I'm looking forward to.
Congratulations on writing a publishing a crate! As far as the actual crate would you mind explaining in what situations I might prefer this to cargo clean?
What is loomio?
You should chat to /u/mitchmindtree about this :)
criterion-plot is probably what I would go for. I still dislike the gnuplot dependency, but it only seems to need the executable.
I just benchmarked and the results are promising for Ryu, it's significantly faster than my float-to-string implementation or dtoa, among a wide variety of inputs. \~2x as fast for 32-bit floats, and 25% faster for 64-bit floats. Not a bad deal.
You want /r/playrust 
Man, have you at least read what subredit is about before posting here? You're looking for /r/playrust 
You may want to give this Wikipedia page a read, which goes over the different methods of using typed systems to ensure correct usage of resources: https://en.m.wikipedia.org/wiki/Substructural_type_system
You may want to take a look at holyjit. It is a Rust library that specializes in allowing you to write a JIT with only basic Rust code.
rouge -&gt; rogue
[Rouge](https://en.wikipedia.org/wiki/Rouge) != [Rogue](https://en.wikipedia.org/wiki/Rogue_(video_game).
Oh, my typo in Github remains. I'll fix it. Thanks.
&gt;FWIW nobody has been able to point Gor to a case that LLVM optimizations cannot fix yet, so it is not that he has been wrong about this either. It's trivial to make cases with allocations that LLVM can't optimize out, even in theory: just make the coroutine outlive the scope it was created in e.g. by putting it in a struct. LLVM coroutines are type-erased, but Rust ones aren't. That indirection can't be optimized out *in general*.
Yeah, I looked into rental. It seems like it would be good if I was the one that wanted to return a reference into my own object or vec. I don't see how it helps me when I'm using someone's else's structs like in `rusqlite`.
Thanks for the help. I actually do think `rusqlite` is doing the right thing here. I mostly just wanted to make sure that I understand the problem and there isn't anything I'm missing. I also can't think of a better way for rust to handle it. I would love to be able to invisibly put something on the stack, but that would be an insane feature to add. I guess I wish there was some way I could easily and safely Box, and/or Pin the Connection, Transaction, and Statement, so that rust knew that the addresses were stable and the a self referential struct was OK. Then all I would need was an order to the dropping of the struct fields. But yeah, I can't imagine how this could work either.
&gt; // Variadic generics would be wondeful so we could have a [guerrilla::patch] And how would you patch variadic generics then ?
I just looked, for example, at the integer-to-string implementation, and part of the reason I was remiss to make hasty declarations that the 3x changes in benchmarks seem to have been confirmed. [https://github.com/rust-lang/rust/blob/master/src/libcore/fmt/num.rs#L214](https://github.com/rust-lang/rust/blob/master/src/libcore/fmt/num.rs#L214) This code is quite efficient, and it seems the general formatting machinery may have an impact on the performance results. This is as-efficient as itoa, or my code (and also very similar), and would be difficult to improve upon. However, it still runs \~3x slower, which I think may be due to all the \`fmt\` machinery under-the-hood. Likely not crucial for any actual workflow, and difficult to optimize beyond that. I'll take a look elsewhere, thanks for the encouragement :D.
I had one short paragraph in a [2016 blog post](https://llogiq.github.io/2016/02/11/rustic.html). Could've done with an example, though.
In [Rusoto](https://github.com/rusoto/rusoto) we've got a lot of crates, including deeply nested ones. I've had almost 100GB of artifacts left over in various target directories from compiling with different releases of rustc and different branches. Being able to run one cargo command to clean them up sounds like just the ticket to me! I've seen less of this degenerate case behavior lately though, I wonder if the cargo clean command has gotten smarter about workspace crates.
Yes, the question is exactly about coding an FSM, not about how it is used in Rust, but about how it is created in Rust.
The comment meant that with variable length generics we could have a single patch function instead of N. Patching generics of all kinda works similarly, it only patches one monomorphic instance at a time. In other words you need to specify the full type of the generic function. The patch itself cannot be generic.
Loomio.org
You might wanna cross post on /r/rust_gamedev? And /r/rougelikedev
Great question, perhaps I should add a motivation section on the readme :) To give some context, this project was created as a result of a feature request on cargo ([https://github.com/rust-lang/cargo/issues/6229](https://github.com/rust-lang/cargo/issues/6229)). I would say that for small projects there is not really a need, but for larger projects it can be really nice. You want to stop your target folder growing too big, but at the same time you do not want to do a cargo clean since that would result in you having to do a full build next time. This becomes especially important on for instance a CI server which has to build a lot of different commts each day, each with new artifacts. You want to keep this clean and fast continously, but doing a full build each time would make it very slow. The recursive flag also lets you clean multiple cargo projects at once unlike cargo clean. And if you want a full clean you can just pass -t 0 :) &amp;#x200B; Note: I have yet to try this on a CI server like Travis so it might not actually work that well depending on if the file access times are modified between runs.
Hold on, let me find by Buzzword Bingo board.
perhaps a choice like this should really be taken on whether or not you want a close or open set of types; for performance code you'd want to avoid either (i.e. sort more by actual type as far as possible, with something like this as a catch all for less common cases)
If i understand you correctly you're overvaluing the **syntax** of being able to call a function 'on' an iterator. Whats wrong with fn double( i: impl Iterator&lt;u8&gt; ) -&gt; impl iterator&lt;u8&gt; { i.map(|x| x*2) } let iterator = ....; let double_iter = double(iterator); compared to the extra trait files you are describing? 
Well, they aren't really readable in longer chains. Consider the following code (off the top of my head to illustrate a point, not sensible code in itself) `foo.iter().double().filter(|x| x &lt; 300).double().flat_map(|x| vec![x, x]).double()` versus `double(double(double(foo.iter()).filter(|x| x &lt; 300)).flat_map(|x| vec![x, x]))` The chains I'm dealing with tend to have at least 10 steps, some may end up having 30 or so. Traditional function syntax doesn't really lend itself to such complex operations, especially when mixed with regular iterator operations.
So is it just for fun to make things so explicit even if there is one outcome? 
And why this is not a problem for go or js but in rust this is a problem? 
It tells you the issue. It doesn't know if the FnOnce is Send. You need to add it as a requirement. Without that people could pass in closures that aren't Send and, well, you can see where I'm going with this.
`FnOnce` is a trait implemented by both functions and closures, and closures can have arbitrary data they keep from their definition scope. This would be unsound if you passed in a closure using a `Rc&lt;RefCell&gt;`. To require/allow your closures to be sendable between threads, use `Box&lt;FnOnce() + Send&gt;` rather than `Box&lt;FnOnce()&gt;`. If, as you say, you really only want to deal with _functions_, use `fn()`. It's a silly syntax distinction, but a lowercase `fn` means a raw function pointer rather than the closure/function trait meant by `Fn` (and `FnOnce`, `FnMut`). Hope that helps. Let me know if you have further questions about this.
This is going to be handy - thanks!
i've seen some macro crates for things like 'pipe' or 'apply' syntax. But in the end i have always gone with plain functions as the least ugly. Looking at your example again. You might be able to create a single custom iterator that takes functions like double. So you would get foo.iter().a(double).a(double).a(double).filter....... But i;m not sure this would work in your general case. 
Could you speak to what this is and who should use it? The readme provides some examples but it's still pretty unclear exactly what swc tries to do.
You can do it with a blanket trait like this: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=46549decbaf2d31164cb9df8b96018d7. IMHO the cleanest solution. I have made the trait in the example generic for all types but you can restrict it to iterators if you'd like.
I see thank you. Still things are getting more and more complicated as i go deeper i start to feel like it will never get easy with rust. After almost completing the book i have even more problems than the point i have started. 
I like the `-r` flag. Easier to remember than `fd -t d target . | xargs rm -r`.
&gt;o sh It's basically port of [babel](https://babeljs.io/) and [closure compiler](https://developers.google.com/closure/compiler/)
I' don't know Rust. Where is DSL in this repo?
Yes, being explicit is one of the Rust's core principles. That's why you cannot also, for instance, add an i32 to a float without explicitly casting one to the another.
Wow, in only 10 days!
Well, this one is from "pedantic" lint group, which are known to have false positives, and thus are allow by default. Specifically this lint in known problems section says that suggested fix flips order of evaluation, which is kind of half of the reason why changed code does not compile (well, not quite that).
But I feel like it‚Äôs so straightforward to distinguish between major and point releases. It‚Äôs right there in the version number. 
The first word in the sentence is announcing, before the version number. You can just write "Rust 1.30.1 released". Announcements are for the most interesting changes. It just looks like a bad marketing title right now IMO.
I think the part written in Go needs to talk to the Kubernetes API. That API is evolving very quickly, so it's helpful to just use an upstream library rather than maintain your own. This upstream library just happens to be written in Go. Also linkerd1 is written Java/Scala, so there's also the question of know-how transferability. Go is proven in the networking space and it's still a managed language. Having run linkerd1 in production, though, I don't have many positive experiences with it. Memory usage is still way too high for an infra component, and there were many memory and FD leaks. The biggest ones were fixed, but I would always think twice about upgrading. I hope things improve with linkerd2.
I think you misread my comment to some degree? My point is that there are some things you can do in Rust with zero allocations that will necessarily allocate in C++ **and there is no possible way for LLVM to ever elide that allocation, no matter how clever Gor is** due to the way LLVM coroutines work.
The problem is complicated in any language. In Rust you might spend more time getting it to compile. In, say, C++ you‚Äôll probably spend more time wondering why it segfaults at runtime.
You are working here with two things that have a relatively high amount of implicit things going on (a closure basically implicitly creates a struct, and an auto trait is usually implicitly implemented but this means the reason why it is or is not implemented for a type gets long quickly, as the error message shows) and then combining that with the place where Rust's rules are most restrictive (communicating with other threads). The compiler really is doing its best to explain things here ("Hey, this type bound you have here does not contain Send, so there is no guarantee the concrete type it describes is Send, which means this long laundry list of intermediate types are also not Send, but this thing at the bottom here requires things to be Send.", if I understood it correctly), and the documentation for Send is pretty explicit about Rc *not* being Send. I can understand your frustration and I'm not blaming you here or anything, but I think it's important to point out where the magic and the trouble is coming from so you know where to look next time (I think not having a good starting point for investigation is most frustrating).
I understood your comment properly. My point is that until now, every time that has mentioned what you just said, Gor has said "show me the C++ code". When LLVM or MSVC did not elide the allocation for the example provided, Gor answered "with this LLVM patch it does".
Was good to see some C/C++ lines going down, but long term the amount of lines in Firefox is not descending As an aside, remember the outrage we all had about Pest's new site having benchmarks with misleading 0s? Same goes for these graphs which start at arbitrary numbers like 75 &amp; 5.5
I just encountered this today, did you file an issue for this? I can see https://github.com/rust-lang/rust/issues/53989 which looks similar but unsure if the cause is the same.
Good job but this only work good for free functions. Without postfix macro syntax it isn't very good for methods. 
Thanks for all the answers so far. This is an interesting option for this specific case, though it still litters the code with non-domain-relevant details (as `apply` is an implementation detail for the algorithm, not actually a part of the logic). My question in a more general sense is how to deal with return values that depend on compile-time generated types (like closures) in trait method return values. This specific instance was how I ran into it, but it's not far fetched to imagine other cases exist. Thinking about it more language-wise resolving this would actually require a deduced return type for the method, which would be hard for anything else to implement and would make the function signature ambiguous: `fn double(self) -&gt; T` (where T would be set to whatever the function body resolves to, akin to auto return type in C++). Thinking of the implications I now see why this isn't a feature in Rust traits.
SDL, not DSL. SDL(2) is a collection of software libs for different things needed for gamedev, like graphics, audio, but also keyboard / controller input. Pretty widely used, afaik.
Now if only NSS would get completely rewritten with sodiumoxide.
NSS and sodiumoxide are really two different things. NSS is a TLS library with all the necessary parts for the browser. Including certificate handling, etc. sodiumoxide is just a Rust bindings for libsodium library which is written in C (libsodium is related to NaCL (https://nacl.cr.yp.to/), again in C). Libsodium doesn't provide TLS, nor certificates handling and would be completely unusable in modern web browser (except maybe few hash algorithms). However, there's rustls (https://github.com/ctz/rustls) which provides TLS implementation written in Rust. It could probably be used for replacing at least parts of NSS in a modern browser. 
&gt; Was good to see some C/C++ lines going down, but long term the amount of lines in Firefox is not descending It's unlikely to given the platform it's implementing is ever expanding, so every gain (which are probably few and far between) is more than offset by the new code implementing new features. &gt; As an aside, remember the outrage we all had about Pest's new site having benchmarks with misleading origins? Same goes for these graphs which start at arbitrary numbers like 75 &amp; 5.5 Complete agreement, the non-zero ordinate origin makes for extremely misleading graphs. 
&gt; which uses 3DES What... why? 3DES is basically completely broken at this point.
months\* &amp;#x200B; MM/DD/YY is the format they used
Great work! Looks very well done! :D
I had a discussion with the lead NSS developer on Slack about his employment at Netscape regarding NSS. The code was put into place before AES was finalized in 2001, and it does send your master password to a KDF to build 3 DES keys. The data is encrypted on disk with a random IV at each save, and uses CBC mode. However, before AES finalized, other projects were also using NSS, such as AIM, Red Hat, and others. So, rather than write wrapper code to migrate away from 3DES to AES, they decided to keep it in place. In hindsight, he says this was definitely not a wise choice, mostly due to https://sweet32.info. But it does use CBC with 3 separate DES keys, although it's not clear if it's authenticated (I haven't dug that deeply). 3DES used in this ways isn't "broken", as much as just "weak" as well as "slow".
[Rogue_(video_game](https://en.wikipedia.org/wiki/Rogue_\(video_game) != [Rogue_(video_game)](https://en.wikipedia.org/wiki/Rogue_\(video_game\)) ;)
Oh that's just *typical*.
If you make it an `Option` then you can temporarily move a value out of it. See if something like this works: pub struct Struct { stream: Option&lt;Box&lt;...&gt;&gt; } fn poll(&amp;mut self) -&gt; ... { ... let stream = func_that_returns_stream(); let old_stream = self.stream.take().unwrap(); self.stream = Some(Box::new(old_stream.select(stream)) as Box&lt;...&gt;); } In general to be safe the field needs to be nullable, because if `select` panics it needs to be in a defined state after the stream has been moved out and before a new one has been moved in.
I have to say, the idea of this project is very appealing. Will try it out! Thank you for your effort!
I found another way around this. I implemented a single special iterator type, I called it `Node` since I couldn't come up with a better name at the time. Basically it implements Iterator and can be created from an iterator (though not using FromIterator, because I couldn't get the relevant types to line up). It also has type parameter specific impls for each type of thing I want to have special iterator functions over. The code below is for using `reqwest` and `victoria_dom` (I used this as a test case to find a more general solution), but the pattern works for all my cases. The only wart is needing to do `into_node` when converting from a non-Node iterator. struct Node&lt;T, I: Iterator&lt;Item=T&gt;+Sized&gt;(I); impl&lt;T, I: Iterator&lt;Item=T&gt;+Sized&gt; Iterator for Node&lt;T, I&gt; { type Item = T; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.0.next() } } trait IntoNode&lt;T, I: Iterator&lt;Item=T&gt;+Sized&gt; { fn into_node(self) -&gt; Node&lt;T, I&gt;; } impl&lt;T, I: Iterator&lt;Item=T&gt;+Sized&gt; IntoNode&lt;T, I&gt; for I { fn into_node(self) -&gt; Node&lt;T, I&gt; { Node(self) } } impl&lt;I: Iterator&lt;Item=String&gt;+Sized&gt; Node&lt;String, I&gt; { fn fetch(self) -&gt; Node&lt;DOM, impl Iterator&lt;Item=DOM&gt;+Sized&gt; { Node(self.0.filter_map(|url| { let body = reqwest::get(&amp;url).ok()?.text().ok()?; Some(victoria_dom::DOM::new(&amp;body)) })) } } impl&lt;I: Iterator&lt;Item=DOM&gt;+Sized&gt; Node&lt;DOM, I&gt; { fn find_all(self, selector: &amp;'static str) -&gt; Node&lt;DOM, impl Iterator&lt;Item=DOM&gt;+Sized&gt; { Node(self.0.flat_map(move |dom| { dom.find(selector).into_iter() })) } fn attr(self, name: &amp;'static str) -&gt; Node&lt;String, impl Iterator&lt;Item=String&gt;+Sized&gt; { Node(self.0.flat_map(move |dom| { dom.attr(name).map(str::to_string) })) } } 
What a fun project! Nice! Looking at the code it seems like you rely on the fact that the replaced function take up at least `JMP_MAX_SIZE` (i.e., 12) bytes. Are you sure this is always the case? I think if this is not the case `patch*` will trample data directly following the patched function.
I had no idea you could just use `fn` straight like that. I thought you **HAD** to use traits, so thanks!
&gt; holyjit Thanks, I'll look into it!
Also, you seem to have linked the same example twice
see one of the other answers here for something usable!
Whoops, fixed thanks.
No problem.
Yes it was a joke.
Some reasons on why someone would use this will be great
&gt; I wish people would just use YYYY/MM/DD instead, its the ISO standard date format Correction: the ISO date format is YYYY-MM-DD.
Talking from experience from working at one of the big networking companies, Go tends to be unambiguous, and the GC is fast enough that it doesn't matter if it has a garbage collector. You know exactly what the code is doing the minute you look at it as well because Go is very limited in what it lets you do. The "free" garbage collection also means that's one less thing you have to worry about. &amp;#x200B; But it is really its opinionated nature that we're using it more. &amp;#x200B; Everything from our infrastructure to the code we ship to customers for our switches has more and more Go in it for those reasons. &amp;#x200B; With that said, I really don't like Go for a number of reasons, but I get why we use it.
This is a very cool project. It would be good to know how completely it supports the features of babel and closure, though. 
I thought they increased the character limit?
I didn't mind the graphs, they were labeled clearly and the bottom 0 to 75 / 0 to 5.5 would have been very uninteresting and just obscure the part that actually matters.
Of course this could provide great improvements to javascript development through speed and improved flow. Good job.
And how fast it is. I'd love to see some benchmarks comparing babel and swc.
would you consider adding asciinema demos of how you use it?
Yeah, that was told me before but I haven't gotten to it yet, sorry.
I'm having some problems with the config flag method. I keep getting errors and haven't been able to compile a test using it. Could you give an example?
Didn't read it because it is a thread. I never read twitter threads. 
Do you mean as opposed to babel and closure compiler or just in general why are javascript transpilers and optimizers used?
Would it be safer to use cargo_metadata to discover crates rather than doing it yourself? Looks like you use atime. Is that enabled and usable on most systems? For windows, you might want to check the registry to see if it is enabled and warn users. Also, MS changed the key value between versions. I can look up the info when not on mobile. Id recommend a dry-run mode for people to see what stays and goes. I wish we could use the lock file to detect unused artifacts more precisely.
But in Haskell you will have to worry about what Job is. A pure function vs an IO action will be different.
America is more efficient. One day is 30.
Any bugs to follow?
Both, I guess. But more on why this is better than babel
This is what I was looking for too. I assume that the C++ lines are being deleted as they are replaced with rust (I'm guessing that is what those little downward spikes are in the MLOC graph are). But C++ is either still being added at a rate which either keeps it constant or the amount of rust code to do the same thing is equivalent in size.
How does this relate to, say, [flamer](https://github.com/llogiq/flamer)?
It is similar in a sense, but it improves by requiring less of the user, regarding annotations and modifying crates for activation of the plugin.
(Sorry for the extremely late reply; I don't check reddit very often.) I think you're right, in that a modern signature for \`Iterator\` would be as you say, but we have the problem that: 1. We're stuck with the old signature. 2. The non-\`impl Trait\` version isn't \*wrong\* per se: \`Iterator::map\` is still functorial even with the signature it has, and so we want to be able to abstract over it anyway. I think the main point is that even though from some angles, it might look like traits like \`Iterator\` don't fulfill our notion of functorality, it's because we have the wrong perspective (from the point of view of the types rather than the traits). If it seems natural to you that that's how you should be looking at it, you're not missing anything. As evidenced by the original objection, that point of view isn't immediately the intuitive one for everyone, so it's worth pointing out. (One interesting observation that I didn't concentrate on in this post is that both traits and types can be functorial: for example \`Option\` is functorial as a type (it also implements \`Iterator\`, so it's functorial in multiple ways). So you want a notion of \`Functor\` (and friends) that abstracts over both of these, which is a subtlety it's helpful to be aware of.)
If 0 isn't right for you, shift your metric. The chart starting at 75 could start at 0 &amp; be "% of Rust", moving up, with the top being 10%. No need to include C/C++ there. Same goes for MLOC
Use rustup.
Sigh. Okay. The problem statement here is "make a Fibonacci sequence generator library that has a C API, internally uses coroutines, and has zero allocations." Rust code: https://godbolt.org/z/AWvbqf C++ code: https://godbolt.org/z/HyfAbu Both code samples implement a coroutine, `fibgen`, then wrap it in a C API (which is pretty messy in the Rust case). As we can see from the assembly, the `make_fibgen` function has an allocation in the C++ version, but doesn't in the Rust version. I'll let you post it on /r/cpp, because I'm sure that Gor (and everyone else who speaks authoritatively about coroutines, except you) already knows about this situation.
The meta bug is here: https://bugzilla.mozilla.org/show_bug.cgi?id=Cranelift
24.9%: https://4e6.github.io/firefox-lang-stats/ (Note that JavaScript and Assembly use very similar shades of green; don't mix them up.)
Awesome! Appreciated
Good points! Dry-run is already an issue on GitHub and will be resolved soon. Will also checkout cargo_metadata and look into eventual windows problems with atime. If you want to create issues for these points on https://github.com/holmgr/cargo-sweep you are more than welcome :) Otherwise I can do it myself. As for the lock file, that would have been great! Unfortunately cargo does not yet give us the necessary information to develop more advanced tooling :/
Yes exactly. I have not yet had the time to test it on different CI systems. The reason for why I am not certain it will work is that I am unsure on how those systems handle the caching. Since cargo does not output timestamps for artifacts I am using access time. Depending on how the caching is done this may involve accessing all files, thus always resetting the access time which means that I will not be able to differentiate age between artifacts. But I will look into it!
Is there an example I can see of setting up benchmarks using the 2018 edition? I'm‚Ä¶confused by the module system changes as they pertain to benchmarking.
Would this work with labelled breaks as well? Just curious, I don't know enough yet to figure it out myself. :)
I've previously suggested the compiler generate unique types for labels, that contain a lifetime, to have a "token" for break/continue that can't escape the scope of the label, and that the compiler "coerces away" by performing the break/continue. (break is general enough to handle returns as well. and continue could theoretically be desugared into a break)
Cool, thanks so much for explaining!
https://golang.org/pkg/runtime/#LockOSThread
When people use arbitrary-precision numbers in practice, how large do they tend to be? I would naively expect "typical" big integers to be smaller than 10\^100. Such an integer could be represented in 333 bits; 42 bytes. If big integers are only represented by such a small number of words, this would trivialise any space savings from eliminating intermediate values for the current calculation, and any speed savings from parallelising individual calculations. If your target audience is using numbers which are extremely large (kilobytes rather than tens of bytes) then the idea does seem interesting.
&gt; type Err = String; Is the biggest problem I can see - this throws away all details about the error and means you have to resort to string matching in order to tell what actually went wrong should you ever need to. Far better to have an Enum, something like (not tested): use std::num::ParseFloatError; #[derive(Debug)] pub enum ParseRectError { NotEnoughNumbers, ParseError(ParseFloatError), } impl fmt::Display for ParseRectError { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { match self { ParseRectError::NotEnoughNumbers =&gt; write!(f, "Not enough numbers"), ParseRectError::ParseError(_) =&gt; write!(f, "Could not parse: {}", self.source()), } } } impl Error for ParseRectError { fn description(&amp;self) -&gt; &amp;str { match self { ParseRectError::NotEnoughNumbers =&gt; "Not enough numbers", ParseRectError::ParseError(_) =&gt; "Could not parse", } } fn source(&amp;self) -&gt; Option&lt;&amp;(dyn Error + 'static)&gt; { match self { ParseRectError::NotEnoughNumbers =&gt; None, ParseRectError::ParseError(err) =&gt; Some(err), } } } See the [Error trait docs](https://doc.rust-lang.org/std/error/trait.Error.html) for more examples. Or look at the [failure crate](https://github.com/rust-lang-nursery/failure). --- match chr.parse::&lt;f32&gt;() { Ok(val) =&gt; c_out.push(val), Err(e) =&gt; return Err(e.to_string()) } Is more idiomatic to use the `map_err` function on result combine with `?`: c_cout.push(chr.parse::&lt;f32&gt;().map_err(|e| e.to_string())?) Or with a temporary variable is that makes it more readable. But you can also remove the loop over iter and just collect: let c_out = s .split(" ") .iter() .map(|chr| chr.parse::&lt;f32&gt;().map_err(|e| e.to_string())) // should be into an enum error type rather than string .collect::&lt;Result&lt;Vec&lt;_&gt;, String&gt;&gt;()?; --- Also the error message `Not enough numbers` can happen for both too few and too many arguments making this error message confusing for half the usecases.
Fixed. Thanks!
Indeed, even if the number is several kilobytes long, that's not enough reason to switch. Rather, it's to be able to watch an answer get built. Let's say you need to do a calculation that will be 1000 digits long, but you're only interested in the first few digits. Via this iteration format, you can stop the calculation after you've reached a certain precision. 
You may be interested in checking out the implementation of computable reals in Spire (a Scala lib): https://github.com/non/spire/blob/master/core/src/main/scala/spire/math/Real.scala There is a paper that this is based on, but it escapes me right now. The idea is similar to yours. Basically, numbers are represented as a function that can produce your number to any desired precision. It's a very cool way of supporting arbitrary precision numbers. Spire also supports Algebraic numbers, which can similarly be computed to any desired precision, but additionally support exact comparisons and sign tests (at the cost of not being able to represent non-algebraic numbers and operations, like, sin): https://github.com/non/spire/blob/master/core/src/main/scala/spire/math/Algebraic.scala Note: I am a maintainer/co-author of Spire.
Just a few comments. In the first part, regarding the language requirements, you also need \`impl Trait\` returns from trait methods, that is not the case. Regarding control flow inside \`do\` notation - I don't think it's so needed, e.g. Scala \`for\` does not allow it either AFAIK.
Stability is important too, but it's understandable that we are not there yet. I posted my work to promote the discussion toward having stable pre-processing capabilities in the compiler. It's currently a niche feature, but it's good to see whether others need this feature as well.
But that same advantage could be achieved by writing a conventional big integer library with "truncated mode" operations which stop after achieving the desired precision.
Screenshots in Github repository are certainly from the actual game.
Oh fantastic. I was going by an error handling doc that did the String conversion trick. It bothered me then and my thought was to have an enum as well but I couldn't wrap my head around the how. Thank you so much!
I've made an [improved version](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=21f9130e11b4c731896d75d7a57de74d) of your code. It's not perfectly polished of course (you'd need to `impl Display` and `Debug` for `RectErr` to print out good error messages at the very least). Here are some aspects: 1. Error handling in main is not well suited to print out nice messages. I've taken up the common pattern of calling out to a `run` function, and simply printing out any resulting errors, and then exiting with a suitable exit code. 2. I've made an error enum `RectErr`, so it can transport information better than a plain string as you chose. I had to `impl From for ParseFloatError` for `RectError` so the conversion with the `?` operator works. It makes the lines ```rust let c: Vec&lt;_&gt; = s.split(" ").map(|l| l.parse::&lt;f32&gt;()) .collect::&lt;Result&lt;Vec&lt;_&gt;,_&gt;&gt;()?; ``` possible. It's a special feature of `collect` that it can collect an iterator of `Result`s (what we get after the `map` call) into a `Result&lt;Vec...`, where we either get the first Error that occured, or the resulting `Ok(Vec)`. Note that this parses more than 4 floats if they are there, or errors if, say, the 6th one can't be parsed, which might not be what you want (sounds pretty reasonable to me, though). 3. I've made the length check more fine-grained and return proper errors for both cases.
I think you are probably looking for /r/playrust/
Thank you fellow name doppelganger =P.
I don't think single numbers are the issue but rather arrays of numbers ie vectors and matrices could benefit
From looking at both this RFC and the previous one, it looks like the changes are as follows: * `std::task::Wake::wake` now takes `self` by `&amp;Arc&lt;Self&gt;`rather than `&amp;self`. (In case anyone was as surprised as I was to see this, it appears that a subset of arbitrary self types is on track for stabilization: https://github.com/rust-lang/rust/issues/55786) * The `std::task::Wake` trait now has an unsafe `wake_local` method. * Related to the previous, a new `core::task::LocalWaker` struct to accompany `core::task::Waker` (though I think the RFC forgot to explicitly spell out `pub struct Waker { ... }` in the text). * `core::task::Executor`, `core::task::Context`, and the helper APIs for both of these all appear to have been removed. * I'm not sure if this is so much a change as it is an update to reflect the evolution of the Pin RFC, but `core::future::Future::poll` now takes `self` by `Pin&lt;&amp;mut Self&gt;` rather than `PinMut&lt;Self&gt;` (not sure if these are semantically equivalent, the new one looks like it has a reference, but maybe the old one implied a reference somehow). * Now that `core::task::Context` is no more, the second parameter to `core::future::Future::poll` is now a `&amp;LocalWaker`.
By the way, where can I read about motivation behind the current design of panic in Rust? What bothers me specifically is why do we have panics that can be caught and unwound, why not just abort the whole process always? Catchable panic makes it hard and unintuitive to write safe `unsafe` code. I'm pretty sure there are tons of panic safety bugs in the existing `unsafe` sections, especially in the third party libraries. Regular developers and users are less likely to find these bugs than security researches, because panics don't occur very often under normal conditions. So brace for some exploits, I guess. Maybe the benefits of catchable panic outweight the costs? What are they anyway? The only two use cases I came up with are test runners and web servers that isolate panics in individual requests. And in both cases there are simple workarounds with restarting the whole process.
They both have the same problem but choose to ignore it. Js would just fail silently (on two attempts to override prototype implementation one would just step on the other and you'd get some unexpected behavior - that's the closest equivalent I can think of). Go decides that matching on method signature alone satisfies any interface with that method, so you also may be surprised and call the wrong thing. Rust does the sane thing: it will point out the issue preventing hard to debug problems and allows to fix it in unambiguous way.
Yes. It's commonly done like this. 
What's interesting is that the RFC more or less got closed for not enough clear motivation of this being used in the wild: &gt; Although this looks useful, I'm reluctant to merge without more convincing motivation. If there were a multitude of obvious cases where this is useful that would help, but there don't seem to be many folks clamboring for this. And now apparently it's a reoccurring pattern.
&gt; an you talk more about what users are still on the Futures 0.1 crate that motivates the compatibility shim? I'm not /u/aturon but maybe it has to do with Actix using 0.1.
&gt; can you talk more about what users are still on the Futures 0.1 crate that motivates the compatibility shim? I'm not /u/aturon but Tokio and Actix are both still on 0.1 and I personally wouldn't bother with futures without them.
What problems, specifically, do you see with catachable panics? What exploits do they allow that wouldnt be possible otherwise? What do people using unsafe have to worry about? Monitoring, and restarting the whole process, are hardly simple or efficient workarounds. In the absence of shared mutable state catching panics should be safe because you shouldn't be able to observe anything after a panic, like broken invariants. Catching panics usually means using threads, and communication between threads should happen at well specified points and shouldnt be a problem after a panic. You have to pay attention about what happens to any mutable state you have, but you have to anyway. Or using `std::panic::catch_unwind`, which takes an `FnOnce() -&gt; R + [UnwindSafe](https://doc.rust-lang.org/std/panic/fn.catch_unwind.html)`, so is similarity difficult to observe broken invariants. `UnwindSafe` existing specifically to encode panic safety into things. Most people using unsafe shouldnt have to worry about panics at all
People writing `unsafe` sections must worry about _everything_ in the Rustonomicon, and it's unfinished, so yeah. In particular, they must worry about this: https://doc.rust-lang.org/nomicon/unwinding.html.
That's primarily a result of Haskell's purity. You can't have shared mutable state if you don't have mutable state. Pretty much the only time it would cause runtime problems to send a value between threads is when it's wrapping an external object.
Interesting, I would have expected those two to be closely tracking the bleeding edge. Tokio especially... aren't we explicitly trying to get real-world experience with these APIs? Who in the real world is kicking the tires on them if not Tokio?
&gt; And in both cases there are simple workarounds with restarting the whole process. Restarting a web server forcefully is hardly a "simple workaround". That means *every* connected user either gets a partial response, no response at all, or an error about the site being down. That would be completely unacceptable to most businesses for a web server, and launching each request into its own process would be way too costly.
I'm assuming that panics are not used for day-to-day stuff and are only caused by programming errors or really weird stuff. Yes, occasionally momentarily active connections will be cut short, so what? It already happens all the time because your router suddenly decides to update and reboot, or your ISP is acting up, or whatever. By the way, even if you insist on complete failure isolation between requests (and ignoring the fact that there is no generally applicable way to contain logic errors), you don't need to launch one process per request. Same for test runners, you don't need one process per test, only one process per panicking test.
If my server can handle 1000 connections and one bug for one edge case for one connection will cause all 1000 connections to die... I'm not going to be deploying that. Bugs that take an entire server offline are usually pretty critical and high priority - you're talking about elevating every bug to that level. &amp;#x200B; And for what? Is catching panics in unsafe a problem we should be very concerned with? Certainly there are footguns, but it's unsafe - it was already a footgun for many reasons. Are there many instances of unsafe code using panic catching? I'd be a bit surprised, but I'm sure it's probably going to exist somewhere. &amp;#x200B; Anyway, the value of reducing some potential footguns in unsafe code doesn't seem worth the cost of turning every panic into a DOS.
The point of "catchable" panics isn't to "catch and resume", it's "catch and isolate failure". For example, the very first known production user of Rust is a Ruby gem that monitors performance metrics of Rails applications; this gem is written in Rust and embedded directly into the customer's Rails process. In the event of a panic in the monitoring gem, what you don't want is for your customer's server processes to go down and unexpectedly sever client connections. Instead, you want the dying monitoring gem to log an error and exit gracefully. Abort-on-error would be unacceptable in this instance.
Sounds like _something_ in your program is unsound, and thus random circumstances are causing SIGSEGV or not. I'm not an expert in this, but I think your best chances will be running the tests in `gdb` and trying to duplicate in there, then getting a traceback and other information from there. Not sure if this is the best approach, but it's the first thing I would try. --- In general, some unsafe code has gone wrong and is doing something unsound, though. Auditing the unsafe portions of your code would be good. If you don't have any unsafe code, or if it's all correct, then some library you're using has unsound code. Maybe use https://github.com/anderejd/cargo-geiger to look at unsafe code in your dependencies, and see if there are any low-usage libraries using unsafe code which could be incorrect?
Here's the last comment made before closing: &gt; The FCP period has closed. While there remain some interesting directions to pursue here, the basic calculus remains: the RFC provides pretty thin motivation, and its detailed design suffers from drawbacks that might be addressable with further iteration. It'd be good to continue experimenting in the crates.io ecosystem, and come back with a fresh RFC (which summarizes the key points from this one) if there's a substantially new design or new motivation. It doesn't appear to have been closed on the argument that the underlying pattern never happens in the wild, it was closed (with an open invitation to file anew in the future) in favor of iterating the design on crates.io. That comment was made almost two years ago, so if people think that a solid design has been settled upon, it seems like it's prime time for someone to file a new RFC.
As someone who enjoys graphs, I found the first two graphs misleading--probably not intentionally so. The first graph starts the Y-axis at 75%, so it looks like Rust has now taken over 20% of the codebase, when it's really only 5%. The second graph, at first glance, made it appear to me that there were 6.5 million lines of Rust code and 6 million lines of C++ code--and again, this graph doesn't start at 0.
&gt;Presumably you've had to onboard people new to Rust, either at SEED or at Embark (Embark may be too new?). How was the experience? Where can we improve? Yes we started using at SEED around 8 months ago, most of our devs got a bit productive fairly fast doing simple things, but it took a couple of months to really understand the how Rust works and how one should work in it. Everyone had a bit of their own journey and was very helpful to both for devs to build their own small apps from scratch to learn, and also to jump directly in the Halcyon mini-engine / research framework that we were building. These were very experienced C++ programmers. &amp;#x200B; &gt;Specifically from a gamedev angle, what's missing? Mentioned a few things in the reply earlier here: [https://www.reddit.com/r/rust/comments/9viryw/embark\_a\_newly\_found\_game\_studio\_will\_build\_their/e9d7qtt/](https://www.reddit.com/r/rust/comments/9viryw/embark_a_newly_found_game_studio_will_build_their/e9d7qtt/) &amp;#x200B; But have been compiling up some notes and plan to write a small article in a few months with more details. &amp;#x200B; &gt;The rust teams sometimes hold chats with production users to better gauge needs and help out: I'm not sure what the current setup for doing this is (stuff has changed), but please let us know if you'd find something like this useful! &amp;#x200B; That would be awesome later on as we bring on more devs and start building more. Love how open and receptive for feedback and help that the rust teams is! And the the community in general &amp;#x200B; &amp;#x200B; &gt;Do y'all use clippy? (just interested, I work on it and want to know how widespread it is for production rust users) &amp;#x200B; Yes! Found it helpful to run every other week or so, or when building some new project and see what clippy says about it :) Have been great to find out bad / non-idiomatic patterns for both new and experienced devs. &amp;#x200B; Have been thinking about if it would be a good idea to have clippy enabled in CI, maybe not to fail builds and PRs on clippy warnings at this time, but it could be good to for example have a CI/bot pass in GitHub PRs that runs clippy and shows the warnings.
If it happens only when all tests run it might be related to tests being run in parallel by default. I had an issue with test code that mmapped in data. Since all the tests, before I fixed it, used the same test file. But as baboross pointed segfaults should only result from unsafe code, dependencies that have unsafe code that isn't isolated from the safe API properly, or a bug in the Rustc compiler or std library.
I commented in the RFC. Tokio will not be migrating away from futures 0.1 in the near term future. For users that implement `Future` by hand, futures 0.1 provides a superior experience. There will be support for async/await, but futures 0.1 will remain the primary API until async/await is able to fully replace it.
From my understanding the reason long is the most advanced example is because there in the process of rewriting the 2d and 3d render for performance and to future proof it. Right now it seems like the best place to find info and other people's projects are on its discord page. Though most of them right now are pretty simple as well. https://discord.gg/E2Dd244
So the issue is whether the cache creation or loading causes atimes to be changed. The thing that makes sense is if we run `cargo-sweep` at the end of the CI before updating the caches. So the main thing we need is a way to identify the start and end of the build to know what was accessed in that window because if it wasn't access, it probably can be cleared. - One option is to dump a timestamp at the beginning of the build and passing that as an absolute value to `cargo-sweep`. - We could have a command that runs at the beginning of the build to make all atimes be prehistoric. That'll make it easy to detect what changed. Also, something to keep in mind with any of this is that the value of using a cached target with `cargo-sweep` is greatest for projects with the `Cargo.lock` checked in. If you don't check it in, the value is contingent on how frequently your dependencies change. 
Why do we call it "catch and resume" and not "catch and release"?
Yikes. I'm a Rust newbie, and my \_first\_ program ran into this issue. I was implementing a quadtree, where each node can be empty, a leaf with one value, or a node with four child nodes. When inserting a value, I wanted to take an existing child of a node, have a new node point to it, and then make the original parent point to the new node instead. I ended up temporarily swapping to an empty node.
And some more discussion [here](https://www.reddit.com/r/rust/comments/9vxxgg/after_nll_moving_from_borrowed_data_and_the/), including a mention of the [`dangerous_option`](https://docs.rs/dangerous_option/0.2.0/dangerous_option/) crate that might be useful here so you don't need to put `unwrap` every single place you use the stream.
Cool! I made up a trait the other day so I could say `unwrap_ref` and `unwrap_mut`, but deref is much more ergonomic. Because: sometimes you really know it's ok.
The most reliable way is to use a macro. Look at [this](https://doc.rust-lang.org/src/core/ops/arith.rs.html#102) bit from the standard library for an example.
Maybe it's just me, but there doesn't seem to be a description of its purpose. There's a list of features, but is the whole purpose taking screenshots? What can/would I use it for?
The paragraph in the blog post that introduces this pattern is: &gt; There is a crate called take_mut on crates.io that offers a convenient alternative to installing a sentinel, although it does not apply in all scenarios. It also raises some interesting questions about ‚Äúunsafe composability‚Äù that worry me a bit, which I‚Äôll discuss at the end. This doesn‚Äôt sound to me like claiming this pattern is "reoccurring".
* Anything that might prevent you from trying it out? Screenshots, for one. And https://sharexin.github.io/ is currently absolutely nothing but a fork of the original ShareX website, which doesn't tell me anything about the port.
I guess this is why Rust gives you a fighting chance vs C++. In C++ the bug could be anywhere in your code, in Rust the bug will only occur in an unsafe block.
`impl&lt;T&gt; PartialEq for T where T: Measure {...} `? 
Could you put a code that compiles?
\&gt; I'll let you post it on [/r/cpp](https://www.reddit.com/r/cpp), because I'm sure that Gor (and most everyone else who speaks authoritatively about coroutines, except you) already knows about this situation and that it's not solvable, even with clever optimizations. &amp;#x200B; Is there any paper showing this situation ? or is it mentioned in some publicly-accessible resource ? If what you say is true (that Gor knows about this), then I have to revisit my thoughts on his talks, which might change from "naive" to "openly lying about it".
The issue isn't catchable panics as much as unwinding itself, since during that destructors are run that can observe invalid state. So even if panics couldn't be caught, you'd have to allow for this issue. And without destructors that run on unwind, you can kind of forget the whole RAII thing. (There are applications where it doesn't matter, and these can freely set `panic=abort`.)
You can look into the rctree crate.
Interesting that kentik is still around. Back when they renamed in 2016 I thought they didn't really have any customers. Used to work for a competitor of them that got successfully sold.
Is there a macro or a fast way for implementing fmt::Display for large enums? I've seen a lot of boilerplate for it and I'm about to write more. Basically, the ideal way would be: ``` #[derive(fmt::Display)] enum MyEnum { Val1 Val2 } ``` ... And it would generate: ``` impl fmt::Display for MyEnum { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { match *self { MyEnum::Val1 =&gt; write!(f, "MyEnum::Val1"), MyEnum::Val2 =&gt; write!(f, "MyEnum::Val2"), } } ``` 
/u/xieyuheng, any updates on the project? It seems to have disappeared from github. For some reason https://github.com/daboross/md-rs exists, but https://github.com/parsing-tech/md-rs/ doesn't?
See [P0981R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0981r0.html). Situations it alludes to: * Coroutine body in a separate translation unit from caller * Coroutine object escaping its caller (applies here, since we essentially return a coroutine from `make_fibgen`) * This includes doing anything nontrivial with coroutine objects. For example, I imagine making a `std::vector` of coroutines or putting a coroutine in a struct that doesn't have a statically known lifetime will always result in allocations. 
It's too much for me too look through all of it, but I've changed the loop at line 274 to use an iterator [here](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=9b483ae8bc583070c0965ec87174a525). Generally, using iterators should be more efficient and less error-prone than the "C-style" loops you are using. About using `Option`, they're awesome to indicate and handle data that might just not be there, but I'm not clear where that would apply to your code. Any place you're looking at specifically?
See https://www.reddit.com/r/rust/comments/9vjs95/plotting_libraries_what_to_use/?utm_content=title&amp;utm_medium=user&amp;utm_source=reddit.
I'd probably produce GraphViz DOT files and use GraphViz to render them. &amp;#x200B;
I don't want to leave this comment in the actual PR because I don't understand the context around this too much but I'm quite confused by the desire to stabilize. I'm just a user of futures by means of actix/actix-web and tokio and I have suffered quite a bit of the entire ecosystem is stuck in 0.1 land. It's a largely abandoned thing and when in the beginning I was hopeful everything would move onwards quickly it's now a really long time stuck there and it seems like this RFC will not help. In fact I know at least tokio will not use any of this until async/await stabilizes because the ergonomics of this API are not great. I wonder if it wouldn't make more sense to have this be unstable for a while longer until at least tokio can ship an experimental support for it and a compatibility shim has been created. Maybe there are lurking issues only real world use can show.
Thanks for the suggestion, seems like there are not that many options out there.
When implementing the `futures::stream::Stream` trait, you can indicate that the stream won't send anything else by giving `Ok(Async::Ready(None))` back from the poll method (iirc). If I implement the `futures::sink::Sink` trait, how do I indicate that the Sink will no longer accept values (for example because it wraps a TcpStreams writer and the stream gets closed)?
Apparently, [Fuchsia](https://github.com/rust-lang/rfcs/pull/2592#issuecomment-437599734); hopefully, its developers will chime in.
It seems that there's a catch 22 there. /u/carllerche [commented on the RFC](https://github.com/rust-lang/rfcs/pull/2592#issuecomment-437600745) that he had no plan to move Tokio to `std::future` until `async` / `await` was stabilized, on top of other current limitations being addressed. For me, it makes sense: I'd rather be able to use Tokio on stable Rust. As a result, though, it means that `async` / `await`, and all its dependencies, must be stabilized *before* they are usable in Tokio.
I did see that a few hours after I asked this question. This lead me to the `replace_with` crate, which solves the problem nicely
At line 85, I think you may want to annotate it with a type like `u32`. Or it'll infer as `u8`, and it's easy to overflow. For example, if you change `b"TTT"` to `b"TTTT"` at line 384 in the testing, it'll panic. Also it can be changed to use an iterator: let (hash_find, mut hash_patt) = find.iter().take(size_find).zip(pattern.iter()).fold( (0_u32, 0_u32), |(hash_find, hash_patt), (&amp;find, &amp;patt)| { ( (hash_find &lt;&lt; 1) + u32::from(find), (hash_patt &lt;&lt; 1) + u32::from(patt), ) }, );
As someone who only used 0.1, what makes this an inferior experience?
/r/playrust
It‚Äôs only a catch 22 because the new futures API is considered a step down from the old. If it was like 0.1 it could be used without async/await.
Thank you, I hadn't seen that paper.
Thanks! And yes would be great to have a list of all current WIP games using it
&gt; The fold feature activates the Fold trait and corresponding functions giving us a nice interface to change code on the fly. The parsing feature allows parsing TokenStreams into syn‚Äôs types. It makes me really happy to see somebody using the `Fold` trait :D
I'm glad it isn't, actually. With dangerous_option, you move the verbosity from the potential panic site (indicated by `.unwrap()` or `.expect()`) to the type declaration. It seems fine for some quick prototyping but shouldn't be used for serious code. Making it part of `std` would be a better "endorsement" than it deserves.
That's what I'm thinking every time I see posts like this. I always wonder if people actually go onto the subreddit main-page and then click on "submit a text post" or if they do it from the generic post form and then just type "rust" into the subreddit-field because they assume this must be it. 
Why is a library API that's not widely used being stabilized?
I understood /u/carllerche's comment differently. &gt; In my experimentation to port existing code to use async / await, I hit limitations pretty quickly. Specifically, as far as I could tell, it is not possible to use a transport properly without extra allocation (due to split). The problem appears to be that `async`/`await` is not ready for prime-time. Which begs the question, why stabilize the `Future` trait if it cannot really be used because `async`/`await` cannot be used ?
But do notation works with arbitrary monads, so IMO neither is strictly more powerful than the other :).
Depends on how you define ‚Äúwidely‚Äù, for example, fuchsia is using this stuff. And the compatibility later means that you can use all the stuff together.
Check what subreddit youre posting in before you post...
Many tools have been around that were never fixed and are still widely popular. C is one of them and you are on a forum for a language that tries to replace it. Facebook had to write Yarn to fix NPM because it's so broken. Then there are people like me who begrudgingly buy into NPM stack just to get compiler for JavaScript but are afraid of it for reasons that are best explained in [this article](https://hackernoon.com/im-harvesting-credit-card-numbers-and-passwords-from-your-site-here-s-how-9a8cb347c5b5). It's completely normal in NPM ecosystem that packages pull thousands of other packages and I have my doubts all dependencies that are pulled are always carefully audited. I trust stuff on NPM as much as I trust apps on Google Play store - barely. Basically I want a self-contained and easily auditable JavaScript compiler and this project might be it. I'm very excited for it. :)
Wrong subreddit. Also, it's spelled "upcoming" not "up comeing".
Why is BufReader's bytes() iterator so slow? Doing a simple fold over the bytes of a large file takes ~4 seconds using BufReader's bytes() iterator. I wrote a custom bytes iterator for Read that stops on error instead of wrapping every byte in a Result, and it executes ~9x faster. What's going on here? Is it the Result wrapping which makes the difference? Did I do something wrong? Code: https://pastebin.com/rkD7Mmhq
The whole purpose is for sending either screenshots to Twitter/Mastodon/Imgur (with an optional message for Twit/Mas) or just sending messages to Twit/Mas. Replicates the functionality I use in ShareX.
Oh, this is really smart. Thanks!
This was actually the first thing I tried. I believe the problem I ran into was that you need to define trait implementations either where the struct is or where the original trait is to avoid ambiguity. I may have been misinterpreting, though.
I think that statement you quoted is somewhat optimistic. Doing that kind of transformation at link time would be very complicated. Even so, my problem statement said that this should be a library. LTO is not performed with libraries (and can't ever be, since the contents of dynamic libraries are not known at link time), so your argument doesn't apply here. &gt;We anticipate that optimizers will reliably perform the optimization on code patterns similar to those above, but that allocations will be performed for code patterns where the object of coroutine type escapes or when compiling without optimization. In addition, "separate translation units" isn't the only restriction that applies to my example. The coroutine is still escaping its caller, and this quote implies that they won't try to deal with that. I admit that multi-level inlining could potentially deal with some, but not all of those cases. To expand a bit more about my `std::vector` counterexample: I think that coroutines that are put into a `std::vector` will always have at least one additional heap allocation per coroutine. Since the number of coroutines is not known at compile time, eliding the allocation would require some equivalent of VLAs or actually changing the layout of the type the `std::vector` stores, neither of which I think are feasible. Another thought experiment counterexample: Any code that handles coroutines generically will have to be monomorphized in order for allocations to be elided. This means that if you have N functions that can each accept M different coroutines, the inlining necessary to elide the allocations will result in O(N*M) code generation, which is unacceptable.
Does `#[derive(Debug)]` work?
https://rust-lang-nursery.github.io/api-guidelines/ https://killercup.github.io/rustfest-idiomatic-libs/index.html#/ https://killercup.github.io/presentation-rust-rhein-main/#/
Sorta. There are plans for a macro keyword, to replace macro_rules. It doesn‚Äôt need macro_use. At that point, the whole current existing macro system will be effectively deprecated in favor of macro, and so yes, so will macro_use. It can‚Äôt ever be removed for obvious reasons, of course. 
Yup! It's almost reminds me of the spread operator from javascript.
My understanding is that custom format specifiers like what you are looking for aren't supported. That might be a great thing to bring up on the internals forum (https://internals.rust-lang.org). The list of currently supported format specifiers is here: https://doc.rust-lang.org/std/fmt/#formatting-traits
Ah okay, thanks, so that's out. Do you have any suggestion of what I might do instead? Just use those traits and define my own `fn format_json` and `fn format_pretty`; then call whichever one is specified and `println!` the resulting string?
Continue could also be desugared into a jump into the same level of context, and generalized with the `become` keyword that is floated around in the context of tail calls. 
Well, `std::mem::size_of::&lt;std::io::Result&lt;u8&gt;&gt;()` is 24, which is quite a bit more compared to `Option&lt;u8&gt;`'s 2, so unless llvm manages to optimize everything out the program has to copy around a lot bigger values.
I wish this RFC was re-opened. I think many people have simply settled on the ugly solution of `Option` and `take`.
A little bit boilerplatey, but you could use functions that return newtype wrappers, like this: [Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=bf6bc18c5f709ab237d7948d0dfb5644)
You can do something like https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=bec58b0e27eaab17b40813ddbf546189 You can also have a single trait like you describe. 
AFAIK the usual way to go about implementing different formatters is to do it via the [Newtype pattern](https://github.com/rust-unofficial/patterns/blob/master/patterns/newtype.md). For every format, you create a new Newtype that wrapps your struct and you can impl `Display` on (also works well for serde `Serialize`). You then impl a `fn format_json` or similar on your usual struct to turn it into the Newtype. Depending on how sophisticated you want to make the setup, you might also want to put your `fn format_json` into a trait and make a generic Newtype that takes any struct that has that specific trait implementation.
In practice, though, `for` comprehensions are quite limiting and somewhat frustrating in Scala for precisely this reason. 
That's true in the same theoretical sense that having do notation is equivalent to not having do notation
`do` notation is simply a syntax for more conveniently manipulating monads. It's not the intermediate functions that represent the monads: it's the actual values (`monadA`, `monadB`, etc. in the example in the post) that are the monads. All the extra control-flow-related power here comes from `do`, not any extension to the monads themselves. &gt;The people pushing `do` notation in Rust aren't looking for somewhat monadic things: they're looking for monads. I'm really just interested in tackling the design problems that were raised in response to people complaining about a lack of monadic abstraction in Rust. The question for me is whether it *can* be done (in this case, whether we can have `do` notation that also works in the presence of control flow), not whether it's actually something that would be useful. 
Even if it were deprecated, that wouldn‚Äôt mean it would be removed. Java still has stuff which was deprecated like 15 years ago.
I can, but together with the logging macros I'll get other stuff - all that API for logging backends... There is [a feature request for a `log::prelude`](https://github.com/rust-lang-nursery/log/issues/272) which could be an acceptable solution. It would still be better to be able to globally import the logging macros though - logging is one of these things where you want as least friction as possible.
The default allocator is now the system allocator instead of jemalloc. I‚Äôm pretty sure this is the savings you noticed. 
Based on your intent to output to JSON; you might want to familiarize yourself with Serde. To output JSON, the easiest way is to let your data implement Serialize, and your JSON output is done. Pretty could then be the "Display" implementation, or perhaps it's even possible for to implement a custom Serde serializer for your "Pretty" format? Additional formats usually plug nicely to Serde (CSV, XML, TOML...)
Which toolchain are you on? msvc toolchain and gnu toolchain work differently. I think I had better luck with debugging in the gnu toolchain.
I mean, at the end of the day the linker will only link what you use. 
&gt; How many registers should be in a virtual machine? Now I have 4096 registers That's some crazy overkill. Here's how to consider how many registers you want your VM to have. Different trade-offs different impacts for software emulation, compared to running stuff on hardware. - Ideally, you'd have enough registers that all local variables of a function can fit in them. - Also, as you are making a VM, you don't have to have a uniform amount of registers. Functions can allocate the amount of registers they need to work properly, given some upper bound. A similar thing in hardware is implemented in Intel Itanium. Despite its flaws a very note-worthy ISA with many good ideas. - Doing that, your entire register file looks more like a big stack where you can grab the top `N` elements as a slice for random access. That slice is your software-visible register file. - You can exploit that by requiring function arguments to be put in the higher registers and return values in lower registers. This way, the I/O region can be implemented as a simple overlapping of register file slices. Just a small optimisation black magic suggestion. - Now, what is your upper bound of the number of registers in a slice going to be? To answer that question, you have to know what your instruction size requirements are. Variable length like x86, going all the way from 1 to 15 bytes? One or two fixed lengths like in RISC-V with or without the C extension? Let's say you have just 32 opcodes in your VM and want to have 32-bit fixed-with instructions. That leaves you with 27 bits for operands. Let's also say that you want to have 3-operand instructions. In that case, evenly dividing by three, you have 9 bits to address all registers, so 512 at most. - Now also note that most RISC ISAs "just" have 32 registers, many of which aren't in use most of the time or are reserved for special ABI purposes. - You want the smallest possible and still easiest to parse instruction encoding you can make, which favours a low register file size. - You also don't want too few registers, or otherwise you'd have to add scratch pads and corresponding scratch pad instructions. Not fun. - I'd guess that 128 registers maximum should be enough for 99% of all the code you'll ever run. And those that don't should use more smaller functions. &gt; - Make VM suitable for implementing OOP languages That's a question of adding magic black box instructions and has nothing to do with how your register file or computations work. &gt; - Implement an optional Just-In-Time compiler Now you especially do not want to need to add a scratch pad. Annoying enough to map your 128-or-how-many registers down to x86's poor excuse they call a register file.
Yes , you should use Options. The Err(last idx) is useless. Iterating over the length of something is 98% of the time not the cleanest way to do it. There are a lot of cases where some combination of iterators is possible. But its a matter of taste if you think its better or not. For example here is brute force in one line. pattern.windows(find.len()).position(|window| window == find) And you might have been asleep at the wheel when you wrote [this ](https://github.com/GrayJack/algos/blob/bfa31609e02cd99fba9e3cb6e88c58923a9d2b48/src/pattern.rs#L80).
Deprecation is not a breaking change.
I'm worried about the middle of the day, too. I don't like to randomly import semi-internal stuff into all my namespaces...
I did too, but that‚Äôs not happening, it seems. The reason, in my understanding, is dealing with the new Pin part of the API, it adds a little bit more details to keep track of.
&gt; Put another way, if the take_mut crate is safe, that means that an &amp;mut can never point to memory not ultimately ‚Äúowned‚Äù by the current process. Nice insight.
Cause I added support for it a couple of years ago. I even had forgotten that I did.
Thanks, very helpful. How does this look? |Type|Sharing|Ownership|Copy var|Mutate content|Move content| |:-|:-|:-|:-|:-|:-| |T|Unique|Owned|Maybe|Yes|Yes| |&amp;T|Shared|Borrowed|Yes|No|No| |&amp;mut T|Unique|Borrowed|No|Yes|No| |Box&lt;T&gt;|Unique|Owned|No|Yes|Yes| |Rc&lt;T&gt;|Shared|Owned|No|Yes iff is\_unique()|No| &amp;#x200B;
I'm familiar with serde. So you suggest just simply requiring 'command' functions return `impl Serialize&lt;'se&gt;`, and then printing the result of serialising according to whichever output mode given in args? Hm, that sounds quite elegant, not sure why it didn't occur given I'm already using it elsewhere! I'll have a play with that, cheers.
 clone_node(&amp;mut node) maybe? What line does the compiler complain about? 
&gt; I'm worried about the middle of the day, too I'm stealing this and you can't stop me.
I find a way to make things work, changing the code to: //let mut new_data = Vec::with_capacity(self.rows() * self.cols()); let mut new_data = vec![Scalar::None; self.rows() * self.cols()]; unsafe { //new_data.set_len(self.rows() * self.cols()); for i in 0..self.cols() { for j in 0..self.rows() { let data = self.get_unchecked([j, i]).clone(); *new_data.get_unchecked_mut(i * self.rows() + j) = data; } } } But now I don't see what this is happening. Maybe because the data is a enum: #[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)] pub enum Scalar { None, //null Bool(bool), I32(i32), I64(i64), UTF8(String), Tuple(Vec&lt;Scalar&gt;), } 
I'm looking for a crate with some very basic networking capabilities. Specifically, I'm looking for a crate that will allow me to ping hosts to see if they are online. Does Rust have any good libraries for this? Thanks!
Good news! Slides and video are up now: https://rust.cologne/2018/10/10/at-cisco.html (only screen recorded -- but almost two hours of people talking and live coding)
Slides and recording now available: https://rust.cologne/2018/10/10/at-cisco.html
If you just want to know whether the binary embeds jemalloc or not, you can run strings and grep for jemalloc.
You could probably see what libraries it links. If it's using the systems allocator then it's presumably a dynamic link as opposed to a static link hence the size difference. 
Or `use!`
I understand the sentiment, but it is very appropriate in many cases, i.e. where value is not None for the whole time except for short time in a Drop or something. I'm not sure if everything that can be misused must necessarily be kept out of std lib. 
&gt; It‚Äôs only a catch 22 because the new futures API is considered a step down from the old. Really? My understanding from Carl's comment is that they are a step down in terms of *ergonomics* when one wishes to implement the trait from scratch, rather than rely on `async` / `await` or combinators. They were not, however, conceived as a drop-in for futures 0.1, and notably have been designed in tandem with `Pin`, etc... for generators. Therefore, it is my hope that in the future, when `async` and `Pin` are stable, then the new design of futures will prove better than the old one. I may be disappointed, of course.
&gt; That might be a great thing to bring up on the internals forum (https://internals.rust-lang.org). I'm guessing an issue there will be that ultimately all the specifiers except for the main categorical ones (so width, alternate, alignment, ‚Ä¶) are set on [the Formatter struct](https://doc.rust-lang.org/std/fmt/struct.Formatter.html).
I was messing around with closures and can't get this to work: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=879773635ceabef1b1b0ab17df80ff0c](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=879773635ceabef1b1b0ab17df80ff0c) &amp;#x200B; The relevant bit is `impl&lt;A, B, C, Fin: Fn(A, B) -&gt; C, Fout: Fn(B) -&gt; C + 'static&gt; PartialFn&lt;A, B, C, Fout&gt; for Fin {` `fn partial(&amp;self, a: A) -&gt; Box&lt;Fout&gt; {` `Box::new(move |b: B| {` `self(a, b)` `})` `}` `}` It complains that error[E0308]: mismatched types --&gt; src/main.rs:7:18 | 7 | Box::new(move |b: B| { | __________________^ 8 | | self(a, b) 9 | | }) | |_________^ expected type parameter, found closure | = note: expected type `Fout` found type `[closure@src/main.rs:7:18: 9:10 self:_, a:_]` So how do I return that closure? I normally just use `impl` but that doesn't work since I'm using a trait. 
&gt; Ah okay, thanks, so that's out. Not necessarily, the formatting system has a bunch of sub-flags you can use however you wish e.g. `#` should show the "alternate" form which could be [whatever you want](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=944272e8d915fefdb492be6a16be15ec). So you could have e.g. alternate debug as json or something.
Why can't it be removed?
I'd expect that the system allocator would likely be part of libc, which would probably be linked either way
I honestly cannot tell. I ran into so many issues trying to use them (aside from changing constantly) that I just did not spend too much time with them any more. I just wanted to see what the current stuff looked like in 0.3 and I immediately left confused again (can't find task locals or any equivalent for them). I cannot assess it. But considering how complex 0.1 already is, 0.2 and 0.3 are definitely even more confusing *to me*.
That would break backwards compatibility.
Apparently you want r/golang instead. This subreddit is for Rust programming language.
Rust doesn't have interfaces? Could you be a bit more clear what you are talking about?
I mean traits
I asked a rust question 
Are you asking about the difference between implementing interfaces (traits) by "just writing methods that have the right signatures" and "writing `impl InterfaceName for TypeName { methods }`"? Image two libraries providing the interfaces `A` and `B` that both have a method `foo(self)` that are used for vastly differnt things. I have no idea how you can call the correct one in Go, in Rust it's just `A::foo(x)`. You can also implement the same trait multiple times with different type parameters.
Are you asking about the difference between implementing interfaces (traits) by "just writing methods that have the right signatures" and "writing `impl InterfaceName for TypeName { methods }`"? Image two libraries providing the interfaces `A` and `B` that both have a method `foo(self)` that are used for vastly differnt things. I have no idea how you can call the correct one in Go, in Rust it's just `A::foo(x)`. You can also implement the same trait multiple times with different type parameters.
Nice post! Your first link to arc is broken, by the way.
How exactly does the implicit implementation in Go "save lives"? I think most people prefer the explicitness, but generics are also a good argument. A `Range&lt;T&gt;` implements `Iterator` *only* when `T` implements `Step`. A `Box&lt;T&gt;` implements `Clone` *only* when `T: Clone`. A `[T]` is hashable *only* when `T: Hash`. If you don't have generics, you don't have these problems.
But the question is why do i need to know which one I am using? lets say i write a function that requires a fooer (something with foo method) so all i need to know is the signature of the thing and thats enough to use it. For example in go theres a io.Reader interface and you can just use that in your function signature. While calling the function you can pass anything with Read method, it really doesnt matter whether its a file reader or network reader or bufio reader or just a dummy reader for mocking. and since the signature for all those readers are identical all i need to care in my function is to use the read method and do something with the byte slice filled by the reader. So my question actually in rust why do i need to care about the implementation detail of a library i just dont get it why do i need to know what sort of reader it is ? 
More type safety. You can ensure that only the right type is passed in, and not a different, totally unrelated type that just happens to have the same name.
Yeah that works, and since jemalloc vs. system allocator are the only choices that matter right now, it's sufficient. Thanks!
Thats the idea a reader for instance is a thing that has a Read method and it always have to accept a byte slice and returns an error and bytes read so how its not type safe ? 
Why does the Read trait get to own that name and type signature? What if a different concept also happens to use the name Read?
Yup.
This is i love about rust generics are amazing the point what i dont understand is if i have a trait lets say reader and it has a method read why would i need to implement 2 read methods for my struct one to satisfy the trait other to to different things under the struct obviously im doing some reading for my struct if i have the read method already i just cannot understand the concept it looks more like overriding to me than being explicit
I'm sure we could define a silly type that accepts a byte slice, converts it to a string, and uses text to speech to "read" it out loud. It could even return the number of seconds that playing the audio took, and errors if IO fails or the byte slice is not valid utf-8 :)
Couldn't it be removed in another edition?
The compiler has to support all editions, so in some sense yes, but in some sense no. Like the code can‚Äôt be deleted from the compiler regardless of your ability to use it.
Why would i need 2 different read methods under my lets say file struct. this is simply overriding the other definition
&gt; for example, importing the logging macros from the log crate In the specific case of the log crate, I prefer not importing the macros at all. I just use them directly. For example, `log::info!(...)`, `log::error!(...)` etc. IMHO it enhances readability and since the name of the crate is so short, I think it works out nicely.
One specific case doesn‚Äôt disprove the rule. You‚Äôre saying that every concept is unambiguous, and that‚Äôs just not true in the general case.
Please consider using punctuation, your replies are very hard to read. You don't have to define a `read` method on the struct if you implement the trait, just implementing the trait is fine.
&gt; why do i need to know which one I am using? Is this not answered by "Two traits can have methods of the same name"? &gt; While calling the function you can pass anything with Read method, it really doesnt matter whether its a file reader or network reader or bufio reader or just a dummy reader for mocking. [‚Ä¶] all i need to care in my function is to use the read method That's how traits in Rust work, too, but instead of taking a chance that just because the method signature matches it also does right thing, Rust makes you name the trait. &gt; why do i need to know what sort of reader it is ? I don't follow. You don't know which precise reader it is, only that it implements the right trait. The code you describe is this, by the way: fn foo&lt;R: Read&gt;(reader: R, some_buffer: SomeBufferType) { reader.read(buffer); }
Ok, that make sense. Thanks.
I agree and would extend that statement to types like Rc, Arc and Box. Overuse of Rc/Arc/Box is turning into a pretty big Rust code smell at this point.
I am just trying to understand the concept. So can we say that if you want to pass a reader to a func you struct thats being passed must explicitly `impl struct for reader` but you can also overload the read method in `impl struct` block ?
&gt; its the ISO standard date format the world has agreed on It's the ISO standard ISO has agreed on.
Both [flamer](https://github.com/llogiq/flamer), [overflower](https://github.com/llogiq/overflower) and [mutagen](https://github.com/llogiq/mutagen) use the current unstable `syntax::fold::Fold` trait, so if the trait would've been unavailable, I'd have ported it myself. That said, what do you think about the macro quagmire and how would you suggest dealing with it?
If you want to pass a Reader (called Read in rust‚Äôs stdlib) then you must pass something that implements Reader. The name of the methods doesn‚Äôt come into play. Say I had another trait, called ReadableOutLoud, (to steal another idea from this thread) that has a read method that takes the text contents of what it implements and sends audio to the speaker of the words contained inside. ReadableOutLoud and Read have very different behaviors and expectations, even though they both have methods named read with the same type signature. In go, these two can be passed to the same functions, but in Rust, they cannot. Go is a bit more convenient, Rust is a little more safe. It‚Äôs a tradeoff.
Well, when you phrased it this way, I think you're right :-) But even if Rust isn't about being explicit _all the time_, I think that it has a strong `be strict` ideology compared to other languages (especially to C++, think: casting &amp; generics).
Rust isn‚Äôt interested in blaming the person who passed something in wrong, Rust is interested in preventing mistakes.
This would be great to have some tag line and screenshots in the README. I did not know getsharex
I see it is just different point of view and i need to get used to it as i understand. So the entire point is that something might have a Read method but it doesnt have to be a Reader and if to put it this way it actually makes a lot of sense
Yup! :)
Thanks. This was what i needed to just see untill this point it just seemed to me a different way or overloading
Glad to help! And welcome :)
I've done testing on FreeBSD, although I'm not 100% sure of its state right now due to some libraries I couldn't find back in 2017. With the current dependency listing for FreeBSD, I'd say there's a 70% chance it'll compile. For the GUI, it originally did look more like Windows ShareX's tweet message dialog. However because I use GNOME and GTK apps, implementing a HeaderBar made sense to me. Sorry if it looks odd. I've considered possibly including a compiler option for a non-gnome/non-headerbar ui. I've added a new screenshot to the website and it will be on the project's README in the next release. I have attempted in the past "dumb hosts", but I never got them working so I just gave up. But if that's something users want, I can attempt it again! Flameshot is very interesting. At one point I somewhat considered it a rival, one that is way more advanced than mine. Flameshot's actual screenshotting functionality is amazing and I wanted to somehow integrate its functionality into ShareXin. However it only uploads to Imgur which I find its major downside. If the Flameshot dev implemented Twitter API with functionality to include a message/status, I'd give up all work on ShareXin. Thank you very much for these criticisms! I appreciate you having my project bookmarked up to this point!
Also, there's a similar project to mine (with a similar name) called [sharenix](https://github.com/Francesco149/sharenix) that is basically ShareX but only CLI and only dumb hosts, if you're interested.
The internet is memory unsafe.
One option (probably not an option in your case?) is to change your api/abstraction to a function that takes a closure that gets passed the iterator. That doesn't work if you want to pass the iterator around though...
I guess that the system allocator is provided by libc, so you won't see another link anyway.
If you're going to change `macro_rules!` to something else, please let it be `macro_rulez!` or `macro_rulz!`. I would also be totally okay with increasing the number of `!` to indicate the amount of _ruleness_ for the macro.
RWIR
:p :D
You should start with [the rust book](https://doc.rust-lang.org/book/) and possibly read up on the [different editions](https://rust-lang-nursery.github.io/edition-guide/editions/index.html) that will be available shortly (2018 edition is due to be released in 1.31 on the 6th of December).
Your code snippets are really messed up, can you fix them please? And what exact error do you get from the compiler?
Implicit trait impls where a pain point for me while trying to debug an unfamiliar Go program: it made finding the relevant code more difficult as I had to search by function name which isn't effective for common names like read.
With the large caveat that you have to actually know what those assembly instructions mean
[here][stck] you can find the playground i lost hours because the book didnt just tell that there was an error at this point and i was thinking i did something wrong. what problem do you exactly see about the formatting? I will try to fix it but it looks fine for me. [stck]: https://stackoverflow.com/questions/53254039/why-does-the-multithreaded-web-server-example-from-the-rust-book-not-compile/53254119
I think in general it's hard for a community focused on one specific language to offer a good answer to questions like this. Tradeoffs aren't super clear unless you've tried both ways a reasonable amount. As someone who writes (and enjoys writing) Go at work and writes Rust in my personal projects, I'd like to provide a perspective other than "type safety". I haven't had issues with type safety of Go's interfaces. Instead, I've noticed *usability* issues, straight and simple. Rust's traits are more useable in the following ways: 1. Rust's traits enable **default methods**. Due to Go's duck typing, interfaces can't offer default methods because there'd be no way to resolve ambiguities. Example: Rust's `Iterator`. The user implements a single fn, `fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;`, and gets (if I've counted correctly) *54 fns for free!* This kind of thing is [simply impossible](https://github.com/golang/go/issues/16254) in Go. In Go, the most idiomatic approach to provide default behavior for a type that implements an interface is to have a bunch of package-level funcs. 2. Rust's traits allow a crate to **add functionality to arbitrary types**. For example, in a server framework I wrote, I wanted to add a fn to all Stream types, so I [defined a trait](https://github.com/google/tarpc/blob/master/rpc/src/server/mod.rs#L158) and implemented it [for all matching types](https://github.com/google/tarpc/blob/master/rpc/src/server/mod.rs#L178-L185). Now, users of my library [automatically get this functionality](https://github.com/google/tarpc/blob/master/example-service/src/server.rs#L48) on their types, crucially, *without even impling the trait themselves!* In Go, this is also a complete nonstarter, because funcs cannot be defined for a type outside of the type's defining package. I don't think these are the only two advantages of Rust's traits, but I hope I've offered a useful perspective on how the expressivity of Rust's traits can actually enable use cases that Go's interfaces can't. Which approach ends up being more pragmatic for *you* probably depends a lot on what you're comfortable with.
Yes, aside from: I would list `Rc` as not being mutable, with "Yes iff is_unique()" as a footnote so it doesn't make that column so wide. Reason being: if you're using `Rc` appropriately, it will usually not be unique. At that point, `make_mut` is essentially an optimisation hack to let you re-use the allocation. But that's splitting hairs at this point.
Old Reddit doesn‚Äôt support triple backticks. Looks like you already have an answer on SO.
Check out the [Iterator](https://doc.rust-lang.org/std/iter/trait.Iterator.html) trait. Note the docs helpfully separated the fns into two sections, "Required methods" and "Provided methods". When you import the Iterator trait into a module via `use std::iter::Iterator` you can now call all those "provided methods" on your types that implement `Iterator`. The Rust book [has a section](https://doc.rust-lang.org/book/second-edition/ch10-02-traits.html?highlight=default,methods#default-implementations) on this too.
oh i didnt know that. I will share the playground link instead thanks for informing
That's a good advice. And for some reason it work even without \`extern crate log;\` - which is weird, because I know it was planned to someday get rid of it but other crates still need this \`extern crate log\` line...
oh i see which basically uses the iterator and does something with the return value of it 
Perhaps the right word is "definite" or "specific". You can elide a lot of information, but what you're talking about always has to be authoritative and unambiguous.
Sure, but you usually don't need to understand them completely. For example, knowing the various jump instructions on x64 / x86 is sufficient to see whether bounds checks are being optimized out. And the highlights between lines of code and the assembly instructions can also provide a good hint about the meaning of the instructions.
Ok, good idea |Type|Sharing|Ownership|Copy var|Mutate content|Move content| |:-|:-|:-|:-|:-|:-| |T|Unique|Owned|Maybe|Yes|Yes| |&amp;T|Shared|Borrowed|Yes|No|No| |&amp;mut T|Unique|Borrowed|No|Yes|No| |Box&lt;T&gt;|Unique|Owned|No|Yes|Yes| |Rc&lt;T&gt;|Shared|Owned|No|No^(\*)|No| ^(\*) Yes iff is\_unique() &amp;#x200B; /u/steveklabnik1 maybe this table would be useful in your documentation
I have a Foo that implements the concept Bar, the Bar concept has x, y, and z constraints which must be fulfilled to work. Fail that and things break (think explosions, death, etc). I also have a Yaa which implements an *entirely different concept Bar* which has entirely different constraints q, r, and p. These also must be fulfilled to work. Fail those constraints and things break (think explosions, death, etc). With Rusts trait system, you need to explicitly explain which Bar concept you mean, they are constrained to their namespaces and you need to explicitly tell us what you are doing, those constraints which save lives? They matter and rust says you have to tell us *explicitly* that you are full filling this specific Bar trait and so you promise, super pinky swear, you are doing things right. With Go, you can mix and match and you may never realize that those two Bar concepts are different things and have different constraints. Explosions and death follow! Yes, if the two things *actually are the same* concepts, then no worries no fouls. But you are all ready entrusting the *programmer* is following the constraints correctly, why would you just hope that they magically also work if you just throw code together since *the names just happen to match!*. What if I need to use both Bar concepts (which can't be used interchangeably) in the same system? now I need to be explicit *anyway*! The type overhead requirement seems like a very minor requirement to protect against putting two pieces of code together which just happen to be named the same, but aren't actually the same concept.
&gt; For the GUI, it originally did look more like Windows ShareX's tweet message dialog. However because I use GNOME and GTK apps, implementing a HeaderBar made sense to me. Sorry if it looks odd. I've considered possibly including a compiler option for a non-gnome/non-headerbar ui. Thanks for telling me that. I was still looking for a screenshot (when it comes to GUI apps, screenshots are the first thing I look for when evaluating a program) and, if you're trying to follow GNOME's HIG, then I can say right now that your project is not for me. (I'm on Kubuntu 14.04 and, in preparation for migrating to a newer GTK+ 3.x when it falls out of support in 2019, I'm tracking down acceptably comfortable Qt-based replacements for any GTK+ apps which are going to cease following the design principles shared by Qt and GTK+ 2.x. I may also migrate to something Archlinux-based if making a PPA for [gtk3-mushrooms](https://github.com/TomaszGasior/gtk3-mushrooms/wiki/Screenshots) is more bother than switching to a new distro.)
Except that debugging into the Rust stdlib requires manually setting up a sourceMap. I wish CodeLLDB would do that automatically.
Yup, all of the provided methods are (by default) built off of the one building block `next`, which either returns the next item in the iterator, or signals the end of the iterator. Many traits in the standard library and other crates use a similar principle to great effect
When I started ShareXin, I came from wanting to port my [old Python script that used PyQt5](https://github.com/ShareXinOld/ShareXin) to Rust. But I didn't understand Qt and how to properly use it on Rust, so I gave up and used GTK like I did for my [old Ruby scripts that used it](https://github.com/ShareXinOld/ShareRin). Although that gtk3-mushrooms looks and sounds really cool. As much as I believe the HIG and HeaderBars work on GNOME, I understand that not everyone uses it, and title bars aren't that thick on other window managers. Try it out with that patched gtk3 and see how it works for you. &amp;#x200B; Also Kubuntu 14.04 wtf. I understand EOF is in 2019 but such an old version is practically unusable.
Is your test crate on the new edition?
[https://github.com/ShareXin/ShareXin/blob/cfff29f90c8442a2aeea7b750f886cffeeeecd72/README.md#ubuntu-1810-dependencies](https://github.com/ShareXin/ShareXin/blob/cfff29f90c8442a2aeea7b750f886cffeeeecd72/README.md#ubuntu-1810-dependencies) They are :)
``` pub struct Kennel&lt;T: Animal&gt; { pub animals: Vec&lt;T&gt; } ``` Here T is a *single* type that acts like an Animal, not *any* type that acts like an Animal. Your Kennel only has a single, specific type for T when you use it. You probably want to put the Animal behind a Box: ``` pub struct Kennel { pub animals: Vec&lt;Box&lt;dyn Animal&gt;&gt; } ``` Here you can put *any* boxed object that implements animal inside the Vec. I think you were half-way there given you were already trying to put Boxes in the Vec.
&gt; But I didn't understand Qt and how to properly use it on Rust, so I gave up and used GTK like I did for my old Ruby scripts that used it. I used to use PyGTK and, when I migrated to PyQt, I never looked back. (As I see it, the API's GTK+ offers are primitive compared to the conveniences offered by Qt.) For Rust, the bindings available for the QWidget API don't yet support implementing custom signals so, for simple stuff, I use Python and, for stuff complex enough to be amenable to a clear separation between frontend and backend, I write the frontend using PyQt and the backend using Rust, then glue them together using [rust-cpython](https://github.com/dgrunwald/rust-cpython) and [setuptools-rust](https://pypi.org/project/setuptools-rust/). &gt; Also Kubuntu 14.04 wtf. I understand EOF is in 2019 but such an old version is practically unusable. "Should I upgrade" is always a trade-off between "What does this enable?" and "How many hours will I spend playing whac-a-mole with new bugs?" (I've never had a major version upgrade that didn't introduce a forest of regressions and papercuts that drew out the "time dedicated to upgrading" to a full day or two.) With one exception, everything I've needed has either been fine on 14.04 or being just a PPA away. (The one exception being that I can't test whether the newest version of `dolphin-emu` has finally optimized its OpenGL code enough for Super Mario Galaxy (a copy I own, dumped myself using some homebrew on a Wii I own) to be playable on a GeForce GTX 750. Since it's just a curiosity thing and I do have an actual working Wii, I didn't really mind.)
macro fixes a number of issues with macro_rules, so I don‚Äôt think so, no.
We have a GitHub repository dedicated for this exact purpose: https://github.com/not-yet-awesome-rust/not-yet-awesome-rust See also: * https://www.reddit.com/r/rust/comments/7zpvev/notyetawesome_rust_what_use_cases_would_you_like/ * https://www.reddit.com/r/rust/comments/7p1s2l/notyetawesome_rust_tell_us_the_use_cases_you/
Is there any chance of the required rustc modifications getting upstreamed? Having this work on upstream nightly rust would be amazing.
I think I'll stick with GTK on Rust, however I am looking for ways to implement a native Twitter API (my current method of using Twitter on ShareXin is through a Ruby app called t), and implementing an API via either Ruby or Python to Rust seems easier than using existing Rust APIs.
Thanks for the resources Can I use this for Linux development 
Shit me, they are. Guess compiling is a different category.
Thanks for these great resources so rust is more like python But rust can let you do low level development 
Sorry if the README is too confusing. I should rewrite it. I could probably add a Table of Contents.
I think rewriting it is unnecessary. A TOC would be great though
Yes
Thank you . Off to the Rust book to look up the dyn keyword.
I used to be solely a script kiddie, and Rust is my first compiled programming language. So what's GC?
I guess I can do the DDS protocol, as I have some experience with that. Though the main users of this protocol are military organizations. I wonder if a lot of people would really benefit from this. 
Assembly code is surprisingly easy to learn. Everyone should know the basics of at least one machine language, and x86 is an obvious (and very useful) choice. Ain't saying it's simple, but it's easy to learn.
Isn't tokio the primary use case for the new async/await stuff? As in, we would probably expect ~70% or more of code using async/await in Rust to also be using tokio? In which case, might it not be especially prudent in this case *not* to stabilise it if the main intended consumer has issues with the design. Also, I don't see the advantage of this being available on stable rather than in made available in nightly for testing (but without frequent changes).
It depends. Fuchsia is using it, for example. Tokio will be very popular, no doubt. It has been in nightly for testing, for a while.
Garbage Collection. The mechanism for reclaiming memory originally invented for LISP and used by languages like Python, Perl, Ruby, Go, and anything running on the JVM or CLR (ie. Java, Scala, Kotlin, C#, etc.). You don't want native code requiring garbage-collected code because you'll pull in a big, heavy dependency (the runtime and garbage collector). Having two different garbage collectors in the same program because, at best, having two independent "I'm the final authority on when to free some memory" things in the same program is a major hassle to work around.
Can someone please explain to me how to read the documentation? I am making a GTK program and I want to use notifications. I know this is in the gio crate, so I looked it up: [https://gtk-rs.org/docs/gio/struct.Notification.html](https://gtk-rs.org/docs/gio/struct.Notification.html) Now I see that there's a struct method called "new" and I understand that this will return a Notification object, so my code is like this now: let test = gio::Notification::new("This is the title"); This of course works. But what now? How do I start it? In the docs it says something about: After populating notification with more details, it can be sent to the desktop shell with ApplicationExt::send_notification. Changing any properties after this call will not have any effect until resending notification. But that doesn't mean much to me. I tried test.send_motification(); But that didn't work. I am lost. Where do I need to look? Where do i see the specs? All the possible functions/methods? How do I actually trigger/call it? I don't understand why it doesn't say in the docs? Do I just need to look at the source code or do I use the docs.rs website wrong?
There‚Äôs a limit to what an edition can change; by halfway through the compiler, it‚Äôs the same for every edition. This helps both programmers and compiler maintainers.
Could you say more about why manually implementing futures is difficult with this proposed API? This is rather concerning to me as my experience with JavaScript promises has been that even with async/await support, manually working with promises is relatively commonplace. What would be the eventual expected route for this kind of code? Using 0.1 indefinitely?
This is my super-long form (sorry!) blog-post/survey on the current limitations of the borrow checker and mutability system as I perceive them. I wrote this some months ago, but got busy and forgot about it. I remembered about it just today and decided to publish after some small updates. Hopefully it's an interesting read.
Thank you!
I can't wait until IntelliJ allows me to see the assembly for a given function in the UI. Godbolt is amazing but very often the function I'm interested in had lots of dependencies on types, traits, and functions defined elsewhere in my crate, so I can't just copy it into the browser. I suppose I could ask cargo to dump the assembly, but then I'd have to track down the method I'm interested in, which would be pretty tedious to do repeatedly.
Welcome to Reddit. Before posting to a subreddit/forum/community, you should check to see what that subreddit is for. This includes reading the sidebar and the rules. You should also pay attention to warnings that you're posting to the wrong subreddit. Check /r/playrust.
Great, thank you for the explanation.
I have a set of use cases that I think could have a common crate. Lots of the things I deal with (like Cargo) have a list of async web requests to make (like downloading files) then process it (like unzipping the files) while obeying a rate limit (like failing if the connection is broken) and displaying a progress bar on the command line.
Thanks, that actually is better to understand!! &amp;#x200B; I add more testing cases: [https://up1.secretalgorithm.com/#Xj4nzstxUDvrsyMFX6E9Jw](https://up1.secretalgorithm.com/#Xj4nzstxUDvrsyMFX6E9Jw) &amp;#x200B; I'm getting a error in testing on boyer-moore and karp-rabin function, and I'm not sure what is wrong in the functions, could you help me with that?
That's all showing on 1 line with the old theme. What code syntax are you using?
have you used rust and what differences have you found between rust and c++/c
Nothing happens whenever I type `rustup docs` into my terminal. I've tried adding the docs component manually with `rustup component add rust-docs`, but get the following error message: `error: component 'rust-docs' for target 'x86_64-unknown-linux-gnu' was automatically added because it is required for toolchain 'stable-x86_64-unknown-linux-gnu'` &amp;#x200B; Help? I'm on Manjaro GNOME 18.0.0 &amp;#x200B; &amp;#x200B;
[I found this](https://joshleeb.com/posts/rust-traits-and-trait-objects/) to be a good explanation of the different ways of referring to types, traits, and trait objects.
Just put 4 spaces before every line and it will work.
WebAssembly functions currently cannot return anything other than a single value of one of four types: `i32`, `i64`, `f32`, and `f64`. `i64` is currently not supported due to limitations in the javascript specification (no standard precise u64 repr). Everything you pass to or from a wasm function must be one of these types including pointers which are just offsets within the module's memory. As far as wasm is concerned there are no structure types. Reconstructing a structure on the javascript side is the responsibility of the javascript programmer (or the wrappers provided by WebAssembly tooling). I haven't used `wasm-bindgen` so I can't comment on how it rebuilds rust types, but in raw WebAssembly, you'd have to access the modules memory and rebuild the type/structure yourself. I hope this helps!
Thanks serde_transcode is exactly what i was looking for. I may have to write a cusotm serializer. Many of the numbers in MsgPack were serialized in python and need to be handled by BigDecimal. Is there a way to do that per type?
This is actually a really good and complete list. For each item it explains why you would want to do it, why Rust doesn't let you, and what workarounds there are, if any currently exist or are in the pipeline.
That's why it's called "After NLL", not "About NLL".
I've installed and run Rust successfully on MacOS and Windows and have been playing around with Rust for a few months now. I just started booting into arch again for the first time in a while and ran the standard install script `curl https://sh.rustup.rs -sSf | sh` as is recommended because I want to do dev on here, but for some reason rustc isn't working. 
[Compiles](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=cf2e37a26201fb7fc87545ef0a4ad0f0). [Proper ("correct" version)](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=c30dc3b1a58e896defa49019583f1b95). Note that the closure must become an FnOnce closure, because it moves variables from the outer scope. Also, partial now moves self (which is expected, it moves into the closure, previous semantics were wrong), and the parameter a must live as long as self, which the lifetime of C is tied to.
Thanks! turns out a few unrelated binaries got messed up when a system upgrade failed... After reinstalling `gcc` rust works again.
I woudn't say that rust is like python except they both are imperative :)
&gt; why do i need to know which one I am using? Because when you call `foo.read()` you expect particular kind of behavior. For example you want to read bytes from source. But name+signature is not enough to know what the function would do.
Reminds me of Rx programming model. Something like ''' listOfSites.map{ |x| download(x)}.map{|y| zip(y)}.subscribe{onNext= incrProgress}. Im on my phone so hopefully this formatting works '''
On nightly I still get 0.1M reduction in binary size in an 2.5M binary with this line: static ALLOC: std::alloc::System = std::alloc::System; is this expected?
This is kind of tangential, but do you think having a test suite of broken borrow-checker code samples would make sense for learning how to deal with the borrow-checker?
I was literally just exploring these a few hours ago! Very interesting. Once generic associated types land i believe we can finally implement a 'streaming' iterator which may make the impl of crates like these (at least those that tackle futures) more optimal for event based tasks 
I'm curious, if you replace all the unsafe get_unchecked calls with their safe equivalent (the index operator), is there a significant performance penalty?
I've used the three backticks. I've switched to the new reddit, because of that exact problem (not being able to read other's code properly often), and I have no idea how to reconcile those two.
Agreed. That's one of the first things any worthwhile book on informational graphics will cover.
As much as I don't like it either, it apparently is, because they have a posting UI where you can compose a thread comprising an arbitrary number of tweets in a single dialog, then post them all in one go. (The "+" in the "Compose new Tweet" dialog.)
&gt;Why are people irrationally afraid of looking at assembly? Finding the relevant code and confirming that it actually did collapse to "one or two assembler instructions" would take minutes. Mostly because I didn't have the time to invest into learning it well enough. I sometimes try looking into it, but really, it doesn't tell me that much. Certainly not enough to go out of my way when I don't seem to have a problem right now. And honestly, I don't care much about how the compiler does what it needs, because in today's CPUs, the number of instructions isn't a good measure. So, even if it didn't eliminate let's say the range check, but implemented it in a way that run at the same speed, that would be fine. And, well, I want the code to run fast on whatever architecture, not just the one I'm currently sitting at, so optimising for a specific assembler is not a good goal either.
A data race happens when you concurrently read and write the same pointer, this would just be asking for data races
is there something like bufio for rust? 
Yes, easy example: let mut option = Some(5); let mut backup = 8; let observing: &amp;observer i32 = if let Some(ref n) = option { n } else { &amp;backup }; // Looks fine, right? o = None; // Oh no, observing is now invalid!
What does `card.draw()` do? Does it draw it from the deck or display it on screen?
&gt; Once generic associated types land i believe we can finally implement a 'streaming' iterator Really looking forward to such, been over 18 months now I think that I've been wanting a streaming iterator for a project. Doesn't seem like we'll be seeing it anytime soon though :(
From Killercup / Pascal Hertleif, there is also https://deterministic.space/elegant-apis-in-rust.html
You are correct, I was ignoring concurrency on purpose. Assuming there is only one thread, how does the situation look then?
It might be better to write functions that don't return the `Grid`, but `Cell` values. Alternatively, you could write more of your logic in Rust, since you now have bindings for everything DOM.
It looks like you miss `documentation` key in your [`Cargo.toml`](https://github.com/davidpdrsn/serializers/blob/master/Cargo.toml), as you can see, there is one for [`serde`](https://github.com/serde-rs/serde/blob/master/serde/Cargo.toml), and the link shows up on [crates.io](https://crates.io/crates/serde).
Nope
This looks really nice, reminds me a bit of how ReduxJS kind of works(a tree of composed functions that react to input state at the leaf functions, each returning new state that is input to the parent function, resulting in a state tree). I really like the ReadMe, both the example shown and purpose/context sections! How do you feel it compares to the other alternatives? Is there anything they do better or barriers they had with their implementations that yours worked around? 
&gt; However, the compiler doesn‚Äôt have any specialised knowledge about array indexing to understand this! Is there any RFC going on to fix this? This seems to be such an obvious oversight, and fixing it would actually make a lot of things nicer. &gt; So the borrow checker doesn't actually have to be super smart. I'm genuinely interested in how you got to that conclusion seeing as how the compiler lacks proper understanding of two of its three fundamental kinds of data types (scalar, array, struct). In my opinion, many pain points in Rust result from the borrow checker being daft, and this is a prime example of that. Also, the struct issues are being worked on, so why not fix this as well? The library solution only works well for simple cases, it gets very ugly when you want to cut an array into *n* slices.
Some context would be helpful, there is a lot going on in that snippet - which part of the book are you stuck on?
the multi threaded web server. The part where we are supposed to do the simplest thing in the world calling a function and failing.
That makes sense. Thank you for answering. The reason I keep those functions in generic trait is because I have a bunch of different objects. Each object can be drawn using the generic `FrameBuilder` (B) and each object has some physics properties that I would like to access without having to downcast it first. I guess those could be in their own trait, but then it would complicate my code even further, as I wouldn't be able to `Box&lt;GameObject + PhysicsObject&gt;` properly.
Dragon Ball fan?
The more formal name for this programming model is Functional Reactive Programming (FRP). There are many crates for this already, and one currently on this sub: https://www.reddit.com/r/rust/comments/9w9iig/yet_another_tiny_frp_crate_reactivers/ This is for information only, I am unable to pass judgement on whether or not they solve this problem well or if there is room for another one.
Thanks, but that's not what I want. I don't want to mutate the data through a shared reference, I just want to read it.
For 2, why does it desugar to creating the mutable borrow first? Surely with a strict evaluation order you would want to do borrows the argument expressions first then any borrows for the function call? So rather than the listed desugaring instead this: ``` let receiver_of_len = &amp;items; // A shared borrow! let result_of_len = Collection::len(receiver_of_len); let receiver_of_mutate_n = &amp;mut items; // A mutable (unique) borrow! let result_of_mutate_n = Collection::mutate_n( receiver_of_mutate_n, result_of_len ); ``` Doing it the listed way seems really strange to me, as it has a different effect to extracting the argument expression to a variable, which I would expect to have the same semantics. 
Yea, because of the lack of those, I had to force the output types for streams (input arguments for observers) to be plain references, with lifetimes bound to the trait object lifetime. If GATs ever land, this could definitely be generalised and expanded. 
Instead of having read-only shared references and a mutable shared reference, you could have simply all have them be shared references to something containing a `Cell`. If that's not good enough, could you explain why not?
Worth mentioning that on a few occasions that I thought the borrow checker was wrong, I had actually written an incorrect program. Works both ways üòâ
Hi! What crate should I use to resize images? The input format could be PNG or JPG. I'm using `image` from Piston, but it takes around 9 seconds to load a 1.74MB PNG image from memory, which seems excessive. Is there a better alternative?
Indeed, that's what many would think. However, it helps if you think about chained method calls: \`\`\` object.method\_a().method\_b().method\_c(get\_arg\_for\_c()); \`\`\` We surely agree that \`method\_a\` is going to be fully evaluated before anything related to \`method\_b\` will run, since \`methdod\_b\` is being called on the \*return value\* of \`method\_a\`. Now, \`method\_c\` takes in an extra parameter. Does it make sense to you to be evaluated before anything in the chain runs, or only when \`method\_c\` is going to be called?
Yeah, that works as well. I guess it's a matter of taste which solution to pick, as all work fine. The only thing I would change is that I would use a blanket implementation for `Trait3`, like impl&lt;T, U&gt; Trait3&lt;T&gt; for U where U: Trait1&lt;T&gt; + Trait2 {} That way you do not need to include an implementation of `Trait3` for every single concrete object.
Thanks for your help. I'll try to figure it out and ask again in the new thread. I always just make a new app so I can try something out before I put it into my actual app. Helps me understand things. :-)
Can someone please explain to me how to read the documentation? I am making a GTK program and I want to use notifications. (I use Gnome and this will trigger a simple desktop notification). I know this is in the gio crate, so I looked it up: [https://gtk-rs.org/docs/gio/struct.Notification.html](https://gtk-rs.org/docs/gio/struct.Notification.html) Now I see that there's a struct method called "new" and I understand that this will return a Notification object, so my code is like this now: let test = gio::Notification::new("This is the title"); This of course works. But what now? How do I start it? In the docs it says something about: After populating notification with more details, it can be sent to the desktop shell with ApplicationExt::send_notification. Changing any properties after this call will not have any effect until resending notification. But that doesn't mean much to me. I tried test.send_motification(); But that didn't work. I am lost. Where do I need to look? Where do i see the specs? All the possible functions/methods? How do I actually trigger/call the notification? I don't understand why it doesn't say in the docs? Do I just need to look at the source code or do I use the docs.rs website wrong?
Interesting questions! It would be nice indeed if the compiler would be a bit smarter. However, there's some limitations: indexing an array with constant indices would be easy to prove safe compile-time, but as it's possible to index with dynamically calculated values too, there would have to be some limit where the compiler gives up. At the moment, the support for `const` expressions is still at a very early phase, so there is no suitable "base" to for that kind of functionality. &amp;#x200B; Furthermore, actually think that library solutions have potential to work in far more flexible cases than anything backed up by the compiler. For one, library solutions can track borrows *dynamically* which allows them to support any cases, not only statically provable ones. And once the `const` expressions mature a bit, I think it should be possible to have library solutions that calculate a proof that some constant indices are different compile-time, and then encapsulate unsafe methods that allow getting multiple mutable references using those proofs. For an additional proof: consider that you can use `mut_split_at` *recursively* today; that allows you cut an array into n slices. There could be a helper API that takes in a slice of indices and returns slices cut at those indices. Actually there might be such an iterator already, can't remember. Anyway, it would be of course nice to have more such helpers by default.
This isn't easy. Consider a little less trivial example: ```rust let x = &amp;mut array[a]; let y = &amp;mut array[b]; ``` The compiler would have to prove that `a != b` _and_ that `array[a] != array[b]` (Indexing is free to alias on different values of the index) for that to be safe within the semantics of the language. The way around this are APIs like `split_at` for temporary subviews. This hindsight is not as obvious as it seems to be.
You still need to ensure it's not read _during a mutable change_. Mutable borrows are allowed to leave the structure in an invalid state for their duration as long as they handle panics and such and leave it in a valid state afterwards. Even in a single threaded system, it is not generally valid to read from something in the middle of some internal function modifying that thing's state. The simplest example would be using the `take_mut` crate: it lets you take ownership of an `&amp;mut` borrow within a closure, transform it, then replace the value with the new one. Within the closure, the original outside variable is left _uninitialized and invalid_! Reading from it is UB, and lifetimes protect any read-only borrow from reading it while the closure is active. In general, though, any system is allowed to do this, and having a way to bypass the mutable hold and read anyways would be unsound. Another example is with iteration. It's not valid to be iterating over a collection while you mutate that collection. This is protected by lifetimes, and the fact that no read-references can exist while there are write ones. --- My recommendation is as others: use a `Cell` for guaranteed safety with Copy types, or a `RefCell` to ensure no reads happen while a write is active at runtime. If you want some of your references to not be able to mutate the inner data, wrap the `RefCell` in another newtype wrapper which only ever allows reading, but no writing. The runtime checks _are_ necessary, though, since even in a single threaded environment mutable borrows are allowed to change internal state and leave it inoperable within internal functions.
I don't have any (expect for those I hunted around when thinking of material for the blog post), but it would be interesting as a set of excercises...
&gt; The books keep talking about owning something but im totally lost Ownership in Rust is a pretty fundamental concept. It might be worth going back over [Chapter 4: Understanding Ownership](https://doc.rust-lang.org/book/second-edition/ch04-00-understanding-ownership.html).
This snippet definecs a trait `FnBox` it is a trait very similar to other `Fn*` family. Each of them define calling operator. The only difference is how `self` is accessed. `FnOnce` takes ownership of `self`. This means that with `foo: T where T: FnOnce()` after calling `foo()` you don't have `foo` anymore. It is one-time-use. You also can't call it `foo` borrows `T`. `FnMut` relaxes it a bit. The call method borrows `self` mutably. Which means you can call `foo: T where T: FnMut` multiple times. You also can call `foo` if it borroes `T` mutably. `Fn` is most relaxed trait. It allows to call `foo` multiple time and even if it borrows immutably. While thus trait is most relaxed for caller it is strictest for implementor. Implementation cannot mutate object that is being called. All normal functions implement `Fn` as they are stateless. Closures can implement other `Fn*` traits depending on what they do with captured values. `FnBox` is very similar to `FnOnce` as it takes ownership of the object but takes `Box&lt;Self&gt;`. It allows to use it in dynamic context. E.g. `Box&lt;dyn FnBox()&gt;`. 
Well, judging from the name, `ApplicationExt` implements extension methods for [Application](https://gtk-rs.org/docs/gio/struct.Application.html), not Notification. This is confirmed by the bound on `ApplicationExt`: ``` impl&lt;O: IsA&lt;Application&gt; + IsA&lt;Object&gt;&gt; ApplicationExt for O ``` I would recommend you to also use GTK's official docs. It is not easy to make a binding for such a huge framework and also write comparable documentation. [GNotification's docs](https://developer.gnome.org/GNotification/) clearly indicate you need to call the `send_notification` "method" on GApplication: &gt; To show your notification to the user, use the GApplication function for this purpose: &gt; &gt; g_application_send_notification (application, "lunch-is-ready", notification);
Noice.
You can drop the guard manually, just like any other value: drop(guard);
It sounds like you want to explicitly drop the mutex guard: https://doc.rust-lang.org/std/mem/fn.drop.html
isnt this a bug? it just doesnt unlock itself and the book doesnt mention this at all?
isnt this a bug? it just doesnt unlock itself and the book doesnt mention this at all?
Nice, thank you very much for your thoughts. Now I just need to rewrite half of my code base...
and its impossible to actually drop it since it is moved by the unwrap method
What do you mean? [The book mentions](https://doc.rust-lang.org/book/second-edition/ch04-01-what-is-ownership.html#memory-and-allocation) that resources are freed up when a value's scope ends and that value is "dropped". It is by design. If you'd like to run whatever `Drop` does earlier, Rust provides a function `drop` which basically: 1. Takes ownership of the value you pass it 2. Does absolutely nothing with it 3. The value is then dropped at the end of `drop()`, as `drop()` is now the owner and its scope has ended.
You can just use std::mem::drop on MutexGuard you get from locking a mutex. let guard = mutex.lock().unwrap(); // Use mutex std::mem::drop(mutexguard);
I cannot drop the value because im unwrapping whatever inside of it so its being moved. And it has to wait until the thread finishes executing thus forcing my app to run 1 thread at a time which makes multi threading in rust impossible using locks see the link please
I will reply to your original thread. This seems to be an XY problem.
Are you sure its the mutex guard that is blocking your other threads and not `recv()`? From the documentation of `recv()`: &gt; This function will always block the current thread if there is no data available and it's possible for more data to be sent. 
I find it hard to believe that is true. The part where you implement the worker is taken from the book verbatim and even the book confirms that: &gt;By using loop instead and acquiring the lock and a job within the block rather than outside it, the MutexGuard returned from the lock method is dropped as soon as the let job statement ends. Once `job` is assigned, the lock should not be held anymore.
so it seems to be a bug then
I'm working in a small web scraping project. I'm using select.rs but the doc is not so helpful
&gt; i'm working in a small web sshiting project. i'm using select.rs but the doc is not so helpful Fixed the comment.
&gt; i'm working in a small web sshiting project. i'm using select.rs but the doc is not so helpful Fixed the comment.
&gt; i'm working in a small web sshiting project. i'm using select.rs but the doc is not so helpful Fixed the comment.
Thank you for making the example easily runnable. Your problem is that you bind the unwrapped guard to a variable: { let job_ch = job_receiver.lock().unwrap(); let job = job_ch.recv().unwrap(); println!("{} got the job", id); job.call_box() } This way `job_ch` survives until the end of scope (after `job.call_box()` finishes). If you do not bind it to a variable, requests are handled in parallel: { let job_ch = job_receiver.lock().unwrap().recv().unwrap(); println!("{} got the job", id); job.call_box() } This way guard is not bound to anything and it is dropped after you `recv()`. It is a subtle change, but it makes a difference. :)
not even the program runs as expected at least for me. if i send 4 request at the same time with curl it takes 2sec and i receive all 4 response at the same time. (rustc 1.31.0-nightly (f99911a4a 2018-10-23) on Linux)
The whole point of the mutex is that so long as you have access to the value inside (i.e. the value is in scope), the mutex is locked. This ensures that no two threads can ever access the value in the mutex at the same time. The mutex will automatically unlock when you drop the guard and lose access to the contained value, thus freeing up the mutex to be used by another thread.
It definitely works in parallel after I made the change: $ time (curl http://127.0.0.1:8888 &amp; curl http://127.0.0.1:8888) hello world hello world ( curl http://127.0.0.1:8888 &amp; curl http://127.0.0.1:8888; ) 0.01s user 0.01s system 1% cpu 2.021 total 
Oh yes i now noticed when i change the command to `parallel -n0 curl localhost:8888 2&gt;/dev/null ::: {1..10}` i can confirm that it works the thing that was jammed was not the threads but my sync curl command
You are right, it doesn't work for all the threads to wait on recv on a channel that is shared between all threads. You either need multi consumer channels or another solution that partitions the work. For example the sequence module is mad for this: work queues
My problem basically was to assign the chan to a variable so lock was kept until it dies. when everything is a single liner untill unwrapping the fn out of it, it just works beautifully 
thank you so much again i spent hours for this literally 
In karp-rabin, your code doesn't match the last `GCT`. Maybe add another compare before return `None`: if hash_patt == hash_find { if &amp;pattern[i..(i + size_find)] == find { return Some(i); } } Also, you can compare the equality between slice, without comparing each element by looping it.
Bad bod
Some traits in Rust have \*no methods at all\*, e.g. \`Copy\`. Without nominal typing, that's not possible to do.
I'm continuing to improve my [MegaZeux client](https://github.com/jdm/mzxplay). I figured out a solution to my mutable borrowing problem, and the answer is effectively an ECS. I feel like I am reliving the keynote from RustConf.
Yes, but I forgot to `cd` into the release directory, so I was actually using the debug build ahah thank you!
Thanks that fixed it!
Not sure thats why it failed, because it worked in the past. But I've added it and pushed a new patch version and now it works. Thanks!
/r/playrust
Yeah I use the old reddit because I don't like a bunch of stuff about the new reddit, and when someone posts code like this I just click the "source" link added by RES to see it as originally formatted.
You'll probably also want to read [The Problem With Single-threaded Shared Mutability](https://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/).
I would find that very useful. There are a few examples out there, but its not enough.
I'm not sure what you're trying to do - why you're wrapping `stuff` with `Ok` and what's the difference between `procedural style` and `combinator style` (in your words)?
What you mean by imperative 
The subtlety here is that the two pieces of code don't work the same way. `let a = my_stuff?` moves the map out of the result, while `and_then` moves the map into a nested scope (that of the function). You can get a similar lifetime issue in the procedural code by writing: let b = { let a = my_stuff?; a.get("two") .ok_or_else(|| MyError::One)?; }; Here's a possible way to solve the issue: https://play.integer32.com/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=582869d2b8649f20e730647e9e342604 Alternatively, [`remove`](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.remove) might be a good idea, if you really only care about the one value in the hashmap.
An ambitious first project! You may wish to reconsider the name, as JSC is already used as a short form of JavaScriptCore, the JavaScript engine used in Safari and iOS...
I tried to rewrite the code for my LIS crate in more idiomatic Rust using some abstractions. The "nicer" code, however, is consistently 15%-20% slower than the crude C-like code. Both functions are in [this Gist](https://gist.github.com/axelf4/1ca05d42f0df29be619b49dcc0d1482a). Slowdowns of 20% is not what I would consider zero-costly abstractions. If somebody could point out what I am missing that would be super helpful.
Owner of NYAR here -- thanks for the shoutout! :) I don't want to spoil anything, but recently I finally secured something I hope will help with a "request" flow better than maintaining a large list like NYAR proper -- I'll be making an announcement soon, once I get the details figured out (and we get a release we've been crunching on at work here out the door, heh).
Try [cargo-asm](https://github.com/gnzlbg/cargo-asm) It requires absolute type paths and ignores re-exports, so getting the correct name of a function can be tricky.
Thank you for this helpful response. You converted my\_stuff to a Result&lt;&amp;HashMap&lt;...&gt;, ...&gt; using as\_ref and then if it's an error, call Clone::clone. Why clone on error?
Hey that's actually exactly what I need for a project I'm working on, thanks!!
 Given that the condition is checked in a `Drop` impl, I suppose that means any backtrace will describe a location close to where the invariant was violated. Is this the main reason why someone would use this, rather than just placing an assert before the code that assumes the invariant?
Very much looking forward to this conference and my first visit to Russia!
If you're using the sentinel pattern described in [Niko's recent blog post](http://smallcultfollowing.com/babysteps/blog/2018/11/10/after-nll-moving-from-borrowed-data-and-the-sentinel-pattern/) where you replace an object with `Type::Impossible` (or `Option&lt;T&gt;::None`, or whatever), this may be useful to ensure that you always put a real value back into the object when you're done with it. I'd have to try it to see how mutguard would actually interact with the sentinel pattern though; it may be that `mem::replace()` always replaces the whole guard and fails to accomplish what I watn.
actix-web handles web sockets out of the box and works pretty well. You can have a look at [https://actix.rs/docs/websockets/](https://actix.rs/docs/websockets/)
Is still to soon for benchmarks for this code. I"m rebuilding the core several time while also learning rust and adapt ideas elsewhere. This is how I learn, this time about how unsafe code behave :)
Today I'm polishing my talk on low latency audio for tomorrow. I'm slightly torn between doing last-minute development on the synthesizer or just going with the code I have. Also, last week I brushed off the new_algo branch of pulldown-cmark and am pushing that forward.
Yeah, I've been playing with it for a while but I can't understand how to send a message outside the response. I thought I can only do it through the context in the trait method. Is there some other way?
Wrong sub, you're looking for /r/playrust
Chalk, which has [implemented GATs](https://github.com/rust-lang-nursery/chalk/issues/116), integration in rustc is [picking back up](https://github.com/rust-lang/rust/issues?utf8=‚úì&amp;q=+label%3Achalk-integration+)!
Chalk, which has [implemented GATs](https://github.com/rust-lang-nursery/chalk/issues/116), integration in rustc has [picked back up](https://github.com/rust-lang/rust/issues?utf8=‚úì&amp;q=+label%3Achalk-integration) ! 
Chalk has [implemented GATs](https://github.com/rust-lang-nursery/chalk/issues/116), and its integration into rustc has [resumed](https://github.com/rust-lang/rust/issues?utf8=%E2%9C%93&amp;q=+label%3Achalk-integration+)!
&gt; &gt; However, the compiler doesn‚Äôt have any specialised knowledge about array indexing to understand this! &gt; Is there any RFC going on to fix this? This seems to be such an obvious oversight, and fixing it would actually make a lot of things nicer. I don't follow the RFCs repo super closely but I'm going to guess that the answer will be no. IMO it's not worth the complexity at this time because most Rust users don't use arrays they use `Vec&lt;_&gt;`s. For vectors, indexing is library code provided via the `Index` and `IndexMut` traits but those traits don't require that `x[v] == x[v]` holds which would be a fundamental requirement for the borrow checker. Of course all of this is fixable but it seems unlikely to me that it will happen anytime soon especially since `split_mut()` is here today and works fine.
The `libc` crate [claims](https://github.com/rust-lang/libc/blob/57c4a15fd656a17801ec5f0f99970684af19b5b8/src/lib.rs#L96) to support no_std, but nono doesn't seem to recognize this. ``` libc: - Did not find a #![no_std] attribute or a conditional attribute #[cfg_attr(not(feature = "std"), no_std)] in the crate source. Crate most likely doesn't support no_std without changes. ``` Do you know if this is a bug in nono, or does this mean that libc doesn't really support no_std properly?
The warp framework has built-in support for websockets and static files. Here's the examples showing each piece: https://github.com/seanmonstar/warp/tree/master/examples
https://en.wikipedia.org/wiki/Imperative_programming
I‚Äôm starting development on a general-purpose, high-performance vote tallying library. I intend to support all known tally methods, and design it to be generic over the vote format (doesn‚Äôt lock you into strings like many other tally libs out there). 
I'm currently working on my bachelor thesis (implement numerical methods in Rust).
Spent the weekend working on a VST3 shim. There are some open questions on how it could work most effectively. It looks like the cleanest interface is going to use a lot of procedural macros and cargo-make to actually build it. There's a bit of friction between the COM/C++ design and Rust to say the least. Looking forward to /u/raphlinus 's [talk tomorrow](https://www.meetup.com/Rust-Bay-Area/?_cookie-check=DBS-8S6_hfm93rSU), if you're interested in audio in rust, check out [his synth project](https://github.com/raphlinus/synthesizer-io). 
Check out [ws-rs](https://github.com/housleyjk/ws-rs/blob/master/README.md), a light event driven websockets crate. It‚Äôs been a lot of fun to play with recently.
As it's the first version, cargo-nono is still very dumb. There is an [issue](https://github.com/hobofan/cargo-nono/issues/1) to better detect conditional no_std support independent of what the name of the feature gate is.
The entire point of a mutex is single access to the protected resource. The `crossbeam` crate might be easier for you to work with. 
When you define a struct, if all of its members implement Drop, then the struct also implements Drop. You only need to manually implement Drop if you need to do some manual cleanup.
On discord we have a "showoff" channel where people show progress and screenshots of their games. Here's the link: [https://discord.gg/TgNnZp5](https://discord.gg/TgNnZp5)
This what I was thinking, but as far as I know the book is not very explicit about when to implement drop, only how.
The first statement is not correct.
Thanks for developing and maintaining *warp*, Sean! We are currently using it both for *WebSockets* and for serving static content like a web app. I prefer libraries over frameworks. [TLS support](https://github.com/seanmonstar/warp/issues/83) will become a requirement sooner or later.
all -&gt; any
Sure, now that I've moved away from Moscow, it gets a Rust conference. Sigh.
When you drop a value, it first runs the code defined in that value's `Drop` implementation if there is any, and then it will drop each member of that struct. So if we dig into the internals a bit: [`Box&lt;T&gt;`](https://doc.rust-lang.org/src/alloc/boxed.rs.html#83) contains one [`Unique&lt;T&gt;`](https://doc.rust-lang.org/src/core/ptr.rs.html#2674) which contains one non-null raw pointer and one `PhantomData&lt;T&gt;`. None of those internal types implement `Drop`, so all that happens is `Box`'s explicit implementation [here](https://doc.rust-lang.org/src/alloc/boxed.rs.html#252-256), which...I guess isn't actually implemented in the library, it's part of the compiler. Well, if it was actually implemented in code it would explicitly call `drop` on the pointer that it contains, in order to run that value's destructor, and then deallocate the memory that the pointer points to. [`Vec&lt;T&gt;`](https://doc.rust-lang.org/src/alloc/vec.rs.html#300-303) has a similar bit of nesting as an implementation detail but ultimately contains two `usize` values (length and capacity) and one pointer to the data. The struct that contains the capacity and pointer is [`RawVec`](https://doc.rust-lang.org/src/alloc/raw_vec.rs.html#53), which deallocates the memory that its pointer points to in [its `Drop` implementation](https://doc.rust-lang.org/src/alloc/raw_vec.rs.html#714). Before that, [`Vec`'s destructor runs](https://doc.rust-lang.org/src/alloc/vec.rs.html#2182-2190), which explicitly calls `drop` on every value contained in the `Vec`. So to put it all together, when you drop your `Box&lt;A&gt;` that contains a `Vec&lt;B&gt;` which each contain a `C`, what happens is: 1. The drop code for `Box` runs. This first calls `drop` on the `A`. 2. The drop code for the `A` runs, which does nothing because it doesn't implement `Drop`. 3. Each member of the `A` is dropped. When it gets to the `Vec&lt;B&gt;`... 4. The drop code for `Vec` runs. This first calls `drop` on each `B`. 5. The drop code for the first `B` runs, which also does nothing because it doesn't implement `Drop` either. 6. Each member of the `B` is dropped. If none of them implement `Drop` and none of their members do, then this does nothing. 7. Steps 5 and 6 are repeated on every `B` contained within the `Vec`. 8. Now that the drop code for `Vec` has finished, each member of the `Vec` is dropped. This runs `RawVec`'s drop code, which deallocates the memory and doesn't do anything more. All of `RawVec`'s members are just plain pointers and memory, so none of them run anything either. 9. The drop code for `Box` finishes by deallocating the memory for the `A`. Now, since `A`, `B`, and `C` don't implement `Drop` explicitly and none of their members besides the `Vec` do, most of these steps do nothing. Looking at all of the steps that have an actual effect, all we have is: 1. The `Vec` deallocates its memory. 2. The `Box` deallocates its memory. Everything is cleaned up, the optimizer works hard to remove all those useless function calls, and everyone's happy.
I can't imagine a situation either where you can't satisfy lifetime constraints for the initial function. The meaning of `'a` is not different, however; it's just a name for the lifetime. When you associate input and output lifetimes with it, it's much more useful though :) (There is no "lifetime of the function".)
Hey all! I'm having some trouble with trait objects and general design right now. I have this trait definition: ``` pub trait IntoComponent: Into&lt;Component&gt; { } ``` where `Component` is an enum that looks like this: ``` pub enum Component { PositionComponent(PositionComponentStruct), HealthComponent(HealthComponentStruct), ... } ``` (each variant indirects to a struct to work around the lack of enum refinement types). And I'm trying to store structs that implement the `IntoComponent` trait in a struct that looks like ``` /// Maps from component type IDs to the corresponding component for a single entity. pub struct ComponentMap { data: HashMap&lt;TypeId, Box&lt;IntoComponent&gt;&gt;, } ``` . But the `Into` trait requires `Self : Sized`, so I can't make `IntoComponent` trait objects, and *this struct definition doesn't compile*. So those are the low-level details, but the *high-level effect I'm trying to achieve* is for `ComponentMap` to only be able to store the `*Struct` types pointed to by each variant of the `Component` enum. *My question is*: is there a way to fiddle with this general approach to make it compile or is there a much better way to structure these definitions to achieve the effect I described?
I just ran it locally, without trying to improve anything: running 2 tests test tests::bench_fast ... bench: 136 ns/iter (+/- 25) test tests::bench_slow ... bench: 114 ns/iter (+/- 5) So the slowdown is not universal :)
Ok, if there are no "lifetime of the function" then what does this lifetime even mean? Can it be put into words or is it just a meaningless statement? Like an identity equation, "the lifetime a is the lifetime a" fun&lt;'a&gt;(x: &amp;'a i32) {} 
Do you need to use the actual Into trait? It looks like this version of your trait compiles fine: pub trait IntoComponent { fn into_component(self) -&gt; Component; } As far as I can tell, requiring Into&lt;Component&gt; ends up indirectly requiring that anything implementing the trait is Sized, which means you can't construct trait objects for it.
Oh man, I remember MzML and MzXML. I feel your pain. They were a lot slower to deal with than the proprietary binary format, but it's nice to at least have something standardized and readable, even if it is kind of a mess.
I'm migrating [typed-builder](https://travis-ci.org/idanarye/rust-typed-builder) to the latest `syn`, so it could benefit from Rust 1.30's non-string attribute values.
Just for more context: I'm using a very recent nightly on Linux/x86-64/Haswell i7. [Here](https://gist.github.com/birkenfeld/650c7a8927149a60753f11327ef2f63b) is the generated code if you want to dig into it.
I've tried it and it seemed perfect until I had to implement some logic for static files and URL route handling. Or maybe I've missed it in the docs...
I've been trying to identify potentially reusable parts - [dhcp](https://redmine.openinfosecfoundation.org/issues/2672?next_issue_id=2671) - [dns](https://redmine.openinfosecfoundation.org/issues/2673?next_issue_id=2672) - [nfs](https://redmine.openinfosecfoundation.org/issues/2674) - [smb](https://redmine.openinfosecfoundation.org/issues/2675)
I strongly recommend codelldb. It even works with embedded cortex-m devices and openocd. For debugging stdlib (if I really have to), I have a script, which wraps rust-lldb and sets up the sourcemap.
Yeah. That's a good point. I guess I was thinking the intent of the trait would be clearer if I was using the one from `std::convert`.
This is a technique that essentially "tricks" the Go Lambda on AWS into running a Rust program. Very cool. Great write up! 
``` if hash_patt == hash_find &amp;&amp; &amp;pattern[i..(i + size_find)] == find { return Some(i); } ``` Would that better? Btw, that solves my problem in karb-rabin, comparing after the while loop once again On boyer-moore, I'm not sure yet what is causing the problem, it doesn't match the last `GCT` Comparing after the loop doesn't solve it either 
I've read somewhere that the python lambda has the least overhead and therefore better for running rust. Is this no longer the case?
Shit, I too updated rust (it was less than a week old or something) and now it's just &lt;4% slower which is well acceptable. Alright, case closed. Thanks for the help.
Yes, there is a way to do this: You want to use higher ranked trait bounds. I think something like this should work: trait GenericNumber where for &lt;'a, 'b&gt; &amp;'a Self: Add&lt;&amp;'b Self, Output = Self&gt;
&gt; Do you know if there's an allocator lib hidden there somewhere? The allocator is in `libc.so.6`.
Did you forgot about the 'taking temporary ownership of stuff'? http://smallcultfollowing.com/babysteps/blog/2018/11/10/after-nll-moving-from-borrowed-data-and-the-sentinel-pattern/ and rust insistence of having panics never see 'uninitialized' state and the workarounds that come with that (oldvalue = std::mem::replace(value, SENTINEL)) is the suggestion given. At least you didn't seem to mention that blog 'solution' of using 
i've never had to do that, a lot of it just works out the box for me. You're better off setting lldb executable to \`rust-lldb\` in settings, that may help
It would be really cool if this could target rust so that it would be possible to convert a js module into a usable crate. I've wanted something like this to convert pdf.js's jpeg2000 decoder into a safe but slow rust crate.
\+1 to \`ws-rs\`, i've been using it here: [https://github.com/xliiv/dashboard/](https://github.com/xliiv/dashboard/) And it was very easy, although it wasn't real production so i can't tell for that.
Something similar, and evolving, to this should be on the rust book. Things would get removed as the language evolved if they are implicit (such as NLL) or leave just the problem and the solution if not.
Some data is in [https://github.com/srijs/rust-aws-lambda/issues/27](https://github.com/srijs/rust-aws-lambda/issues/27). Basically it looks like rust using the go runtime is on par with python.
I've had a super-productive long weekend! Everything I worked on is (at least partially) written in Rust... and none of it is publicly available. Oops. * Fixed some problems with my Questrade investment allocation calculator * A bunch of internal refactoring and improvements to the server process of my home-brew RSS reader. It no longer keeps all of the RSS data in-memory (instead it treats the database as a single-source of truth like it always should have), it no longer blocks the entire server while downloading feeds, and I've made some improvements to the REST API so that the client can load only the data it needs (rather than dumping most of the database down the wire on every connection). * Tomorrow, I plan to set it up on some free cloud VM somewhere so I can read my news anywhere, not just on my home network. I'm not too well-informed on web-development, so if anyone has a good resource on nginx (or some other reverse proxy that supports Let's Encrypt) that'd be very helpful. * Did a bunch of work on my CUDA Driver API wrapper crate. Currently it covers (at least the basics of) initialization, memory allocation, device enumeration and context management. I just need to add module management and execution and it will actually be usable for basic CUDA programs.
It's not all roses if you're multithreaded and really heavy on allocations things can get slower. One of my core performance tests for a production system is now 22% slower in nightly. The extra couple megs is nothing next the the 100 gigs it works with. I really hope that when this gets stabilized there's a very obvious and simple instructions on how to restore the old allocator. Currently I like the idea of getting the [global_allocator_on_supported_platforms](https://github.com/alexcrichton/jemallocator/issues/88) feature for jemallocator working. Without that it looks like I'm going to have to add a lot of boiler plate code to my crates readme. Note that 22% loss is on code I've already improved to be nicer on the standard allocator. In one of our C code base I've measure a 100x difference between jemalloc and the standard linux libc in production code. That's mainly due to having way too many threads and lock contention. Also I'm not against the switch to the standard allocator, since the reasoning is well founded. Also supposedly the newer Linux allocator is much better, so it may only really affect people stuck on older systems which both have jemalloc support and a weak system allocator. I just hope try different allocators becomes a well supported option for performance tuning.
Yeah that's true. Would be interesting to see which approach results in more hits of the code in question
I was reading this article (on the front page right now) and I think your example is basically point 2: [https://medium.com/@GolDDranks/things-rust-doesnt-let-you-do-draft-f596a3c740a5](https://medium.com/@GolDDranks/things-rust-doesnt-let-you-do-draft-f596a3c740a5)
Hey, I have been working on a notation called [Frame Machine Notation](https://github.com/frame-lang/frame-machine-notation) to provide a simple machine specification and proposed default implementation mechanisms which will apply to any object-oriented language. I have some [articles on Medium](https://medium.com/@mark.truluck) and adding more that help explain the notation and give examples.
After having rewritten [flamer](https://github.com/llogiq/flamer) to use the new `proc_macro_attribute` interface, I've also converted most of [overflower](https://github.com/llogiq/overflower). There is one problem remaining: Currently I cannot look inside macro calls. I have an idea how to implement this, but I need to look deeper into [syn](https://github.com/dtolnay/syn) to pull it off. 
&gt;Afterwards, we‚Äôll need to remove the quotes from the result, and we‚Äôll use `cut `‚Äî which is probably installed in your Linux machine already. Consider using `jq -r`. `-r` or `--raw-output` are designed for bridging JSON to the non-JSON world. They will, for example, strip the quotes from selected strings. 
I hate to get political, but is Moscow a safe place for LGBT folks to attend a conference?
As long as they do not wear that on their sleeves in a very obvious manner, it is. And with everyones sleeves covered in winter coats anyway, you need to really get out of your way for it to show.
This works really well, except for the local testing which is quite slow, depending on what you are building this might be ok. I tried building a web server this way, using SAM cli which uses docker locally, each request takes a few about 7-10 seconds, which makes developing locally quite painful.
Thanks everyone 
If all your tests are talking to a real database, you're going to have problems. For one thing, `cargo test` runs all its tests in parallel, so if a test tries to drop all schemas while other tests are running, Bad Things will happen. For another, it'll be very easy for one test to accidentally leave something lying around in the DB, causing tests that run afterward to break (but only if you run the full suite, not if you run the breaking test on its own to diagnose the problem), or even worse, causing tests that run afterward to pass when they shouldn't. Also, running all your tests against a real database makes your tests run really, really slowly, which can be frustrating and encourages people not to run the tests. If you're using an ORM or some other database abstraction layer, and your SQL works with SQLite, have each test create an in-memory database (by opening the filename `:memory:`) and then everything will automatically be cleaned up when the tests end. If you can't use SQLite, maybe have your unit-tests just validate that your code generates the correct SQL for whatever queries, and then have a single integration test (a `tests/something.rs` file with a `main()` function) that loads a schema and some test data and generally does whatever end-to-end testing you require.
Tbh Rust wouldn‚Äôt disappear (after all the language is amazing) but it would certainly not gain as much traction as it does now without our amazing developer community. Throwback to pre-1.0 days when I was mentioning Rust as ¬´ a very interesting take on languages ¬ª and it would only raise a few eyebrows! 
Put all tests into separated crate and put cleanup code into crate's build script (build.rs).
Interactions with a database are the main thing I look for in a server-side format. There are no good ORMs that Rocket can interact with, so I can't see it replacing alternatives like Django.
Wow, this is almost all news to me, excellent rundown on the topic! Thank you for all the links and information :)
If you're new to Rust, the list of things you're going to need to learn is going to incur a lot of overhead. If you accept the tradeoff, then wonderful! If not, consider using [Actix](https://github.com/actix/actix) and its ecosystem. Here's the list I'd make for you: 1. Learn Rust the toolchain (`cargo`, `rustup`, etc.) 2. Learn Rust the language 3. Learn Rust the borrow-checker (yes, I consider these different steps! 4. Learn Rocket the web framework 5. **Learn Rust, the nightly-released software that may change something unstable and break your face you without asking permission.** Rust is not particularly bad with this, but you will throw a bit of Rust's stability guarantees out the window. Rocket will make you learn more because of item 5, potentially. It's a heck of a lot less likely that 5 will bite you, though, given that procedural macros have stabilized. Just be aware -- and be careful about vetting your dependencies, so you can stick as close to stable Rust as possible! Keep in mind, I LOVE Rocket.
I like that aspect too; speaks well of the overall design
But no link to the book? Or even a place we can purchase it?
Sounds like you‚Äôre trying to do more than just websockets. Maybe you should look into a web framework?
I already finished the book and I feel like I'm ready to dive in some prod things though I still have lots of questions but meanwhile I think the fastest way to learn things is to fail right before prod so I can spend the last limited time to search for it way productively. This is how I learned golang back in the day :) and it worked pretty well.
Usually "the book" refers to [The Rust Programming Language](https://doc.rust-lang.org/book/), which is online and 100% free =)
How about [Diesel](https://github.com/diesel-rs/diesel)? 
But I don't want any single test to be dropping all the schemas, just the schema that it creates at the beginning of the test. This is why I'd like something that can run either prior to all the tests are run or after. With that perhaps clarifying the original post, is there a way I can get this prior / post cleanup functionality? Part of the reason I want to use a schema-per-test is also precisely to avoid one test leaving something in the DB that will affect another test. With each test having its own schema, the idea (as I understand this strategy) is that each test has its own namespace basically and can do all the setup /testing it wants within that schema, leaving other tests unaffected. To your point about an in-memory solution, I'm using Diesel with Postgres. Would I be able to do the SQLite approach with that setup? In-memory does sound preferable to a real database. The way I have it currently is a Postgres Docker image exists as the test database.
Kept playing with peer to peer networks, buy the networking side is really too much of a headache right now... Which is my own fault for using the alpha `quinn` crate for QUIC instead of just doing TCP or such. I finally have a nice message-based framework for actually processing communication, and I'm not going to rewrite it in `actix`, honest. So I'm just going to mock out the network bits and leave them for later so I can finish this sidetrack and get back to `ggez`, like I've been promising for the last four months.
This is a pretty cool approach, I think I will play around with it! Thanks.
Rocket isn't currently asynchronous. A rocket creates a threadpool of a configurable but fixed size, and it can handle at most that many connections at once from start to finish. Contrast with hyper, which uses asynchronous I/O, so one connection waiting for data to be written to a socket doesn't prevent other connections from being handled. In this way, hyper can handle {threadpool size} connections that are actively running Rust code in addition to pragmatically-unlimited connections doing I/O (which includes recieving requests, sending responses, interacting with databases, etc) at once. The tracking issue for that is [https://github.com/SergioBenitez/Rocket/issues/17](here). Rocket also requires a lot of features only available on the nightly Rust compiler due to its heavy use of code generation. [Here](https://github.com/SergioBenitez/Rocket/issues/19)'s the tracking issue for that. A bit of a niche issue, but Rocket also doesn't currently allow running an HTTP server and a websocket server on the same host/port like warp (which is built on hyper) does. Looks like Sergio is [planning on this](https://github.com/SergioBenitez/Rocket/issues/90) too. I'm smitten with the concept of Rocket as well, but holding off for a bit while these are addressed. I eagerly `await!` the day Rocket uses async I/O and compiles on stable Rust, at which point I'll be all in.
This is the exact reason i asked this question here. Thanks I'll check the hyper now too. I wish writing macros were as easy as in C for rust so i could simply write my own .
 What about this: &gt;Besides the concerns about exposing an unstable crate in your API, people have also complained that the backtraces stored by failure slow down their apps. I don't think that's such a big concern, but I've been bothered about the increased build times; while a larger app might already be using e.g. syn (the same version, hopefully), for a small CLI app or crate, it might be too much. &gt; &gt;There's also some vague notion that failure's API is not ideal, so it might change in the future. But it looks like there can't be any breaking changes to it, because at this point failure is too big to fail (pardon the pun). &gt; &gt;Also, I've seen people (including myself) getting confused by the failure documentation wrt. the ErrorKind pattern and the Context API. If you go for an Error enum, you still need to implement the conversion functions by hand. &gt; &gt;failure gives you backtraces, the context thing, and the macro for Displaying the error messages. Not everyone wants backtraces, and IMHO that macro should be another crate, with proper maintenance (it has some limitations right now). &gt; &gt;So, while it's not unreasonable to use it, failure today is probably not how Rust error handling will end up looking in a year or two. I'm sure that the compromises will be the same (wrt. backtraces, memory allocation and so on), but I think the details will be different. And if you use it today, you're writing more code that depends on a "too big to fail (or change)" crate. https://www.reddit.com/r/rust/comments/9nu6o0/reflections_on_implementing_the_ndarraycsv_crate/e7qndd1/ 
I just tried it and your CARGO_PKG_NAME trick works if you ignore the [[bin]] section and put a &lt;program&gt;.rs in the src/bin folder instead.
Without considering anything like performance or how well it plays with different combinations of libraries, I‚Äôd say I really really like Rocket. Rocket has a beautiful, high level API that, once you get a little familiar with, is incredibly consistent and fairly straightforward. Rocket is arguably more ‚Äúmagical‚Äù than alternatives like Iron. I usually am not a fan of magical tech, but actually find it kind of appealing in Rocket‚Äôs case. That would likely be because Rocket‚Äôs magical features like route handling and state management generally only really affect function signatures. So once you have a feel for what it‚Äôs procedural macros are doing, it actually isn‚Äôt even that magical anymore. Speaking of state management, I think it‚Äôs incredible that Rocket has this feature built in. It‚Äôs really nice not to have to think about how to do this yourself every time you build something with it. Whether Rocket‚Äôs approach will work for a given use case or match your style is something you would need to evaluate yourself. In all, I think Rocket is an incredibly well designed technology that makes writing backend applications in Rust really pleasant and safe. I expect that as the project matures, it may gain a pretty strong following amongst folks doing backend web work with Rust. It will likely also influence the design of other higher level libraries and frameworks to come. 
I'm reposting here because I think is a better place. I have a interpreter on rust that wish to provide AGDT &amp; pattern matching: pub enum Scalar { None, //null Bool(bool), I32(i32), I64(i64), UTF8(String), Tuple(Vec&lt;Scalar&gt;), } Now the question is how provide the support for this. How represent it internally? Is posible to reuse rust machinery on this? &amp;#x200B;
i suspect this person is talking about _reading_ the rust book (and not writing one)
Thanks, this seems great for embedded!
good to know. thanks!
Learning Rust is probably a worthwhile investment eventually, but I wouldn't say it will make you more immediately employable at this point. There are a handful of Rust jobs here and there, but not too many that I've seen. If you don't have a lot of time to spare, you might be better off learning Python or Java instead. For what my opinion is worth, I also think Rust may not be a good first language. It's kind of concept heavy, and there aren't a lot of learning resources tailored for beginners.
I'd have a look at how `diesel` structures its tests. Depending on what tests you are running, `Connection.test_transaction` might do the trick. 
Rocker offers a wide selection of connection pools via `rocket_contrib` as of version 0.4, so depending on whether you consider `Diesel` an ORM‚Ä¶
I should clarify, Rust is not my first language. I'm pretty comfortable in Python, and I've written a few programs in Go. I've dabbled in JavaScript and Web Development, but didn't retain a lot of it. I took a Java class in college (4 or 5 years ago) but, again, didn't retain much of it, apart from some general programming/CS concepts. I like Python, but I wanted to branch out and learn a compiled, strictly typed, and concurrent language. In addition to Rust, go seems to fit that description, but in my experience, there aren't a lot of good resources for beginner/intermediate level programmers. All the stuff I've seen seems aimed at established professionals. and honestly, imo, it's not as much fun as Rust, and I like Rust's compiler better. I happened upon a book about Rust at a local bookstore and decided to try it out because it looked like fun, and so far that first impression has held up. It feels like I'm learning to code all over again (in a good way) and I think it's teaching me valuable things about programming in general, but part of the reason that I'm learning to code is to get a job as a developer and get the hell out of retail. Thoughts?
Not a ton of Rust jobs, but not a ton of Rust programmers either. If you learn Rust well, Rust jobs likely pay way better than average. Since you a beginner, for near term jobs purposes, you will probably want to pick one or two of the more popular languages to get familiar with. Javascript, C#, Python, Java etc. &amp;#x200B;
Got it. I am going for a thorough testing setup like that, so I think what I'll do is include something for Docker to run when the Postgres container is getting spun up which can clear all the schemas for a fresh test run.
Rust is an amazing language, however I would agree with /u/dead10ck. If your goal is to become employed as quickly as possible, languages like Java, C#, or Python are your best bet. The demand for Rust programmers will probably continue at it's current rate, it is growing but don't expect an explosion of new jobs.
Thanks for responding! So Rust, at this point in time, is only really valuable if you know other systems languages like C? Would you say the same about Go? Because I do have some experience with that as well, and from what I understand, it's a more popular language. If that's not the case, should I just drop everything and start learning C/C++?
Ah, I see. I definitely share your sentiment with regards to liking Rust more than Go. But if your goal is to make yourself more attractive to employers, you'll be easier to hire if you know the language they use. In sheer numbers, you'll love the most luck with Java. Even Go has a decent number of jobs in the market.
&gt; Not a ton of Rust jobs, but not a ton of Rust programmers either. &gt; If you learn Rust well, Rust jobs likely pay way better than average. So, what's your opinion on what /u/Netzapper says, that Rust is only really valuable right now if I know another low-level systems language like C, and that someone who knows *only* Rust would be unemployable? What kind of companies would be looking for a Rust developer? What kind of work would it be if it doesn't involve converting C code?
You really shouldn't rely on `Arc::strong_count` to determine whether or not to destroy your list. I would move the `Arc` outside of your `List` type and move the `FnFree` *into* it so that you can implement `Drop`: use std::{os::raw::c_void, sync::Arc}; pub type FnFree = *const unsafe extern "C" fn(ptr: *mut c_void); pub struct List(Vec&lt;*const c_void&gt;, FnFree); impl Drop for List { fn drop(&amp;mut self) { unsafe { if !self.1.is_null() { for node in self.0.iter() { (*self.1)((*node) as *mut c_void); } } } } } pub unsafe extern "C" fn vs_destroy(list: *mut List) -&gt; u8 { Arc::from_raw(list); 0 } This forces the caller to declare up-front what type the list is going to contain, or at least how it's going to be destroyed. There's not really a way to prevent them from giving you a free function that causes UB though, that just comes with the territory when dealing with C. Oh, and if you're planning to add/remove elements from the list once it's in the `Arc`, you need to add a `Mutex` as well. 