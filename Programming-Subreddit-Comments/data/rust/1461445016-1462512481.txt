Awesome to read that there won't be backwards incompatible changes for a long time!
Rust version numbers are made according to [Semantic Versioning](http://semver.org/) so 2.0 would indicate a *breaking change*. It would *not* in itself necessarily indicate an "important" change, or significant progress. In fact, I think it would be a great sign of success if we can keep improving Rust for a very long time *without* breaking backward-compatibility. Here is hoping for Rust 1.100.0 in November 2026!
Rust 2.0 shouldn't happen until people have had a good opportunity to form really good love and hate relationships with Rust's syntax and semantics. My money would be on eventual breaking changes to borrowck since it's confusing and slowly getting better. Eventually there will be consensus on what to replace. My vote is for the current concept "lifetimes are just a fancy name for possibly-phantom scopes" to go away and good riddance even if it does break things as it dies. Nobody fights with the use-after-move and use-before-initialize checks. This is because reachability analysis is much more sensible than ghost scopes. Borrow checking is a hard problem, and from a user's perspective I can see that it's not quite right yet. But it's hard to know what could be done to change it without understanding that part of the compiler. I'm still in a learning phase.
I do believe the standard library is missing something in this area. Iâ€™ve written more in my comment in https://github.com/rust-lang/rust/issues/27830#issuecomment-206823397, and I have some experiments you can look try at https://github.com/SimonSapin/rust-utf8
I guess we could use a hash of the type/the typeid as some sort of seed to a random number generator, to try to have a higher chance of uniqueness. I'm not sure it would be a great thing in practice though, as having a small tag is good (e.g. one byte if possible) so there's a surprisingly high chance of overlap (cf. birthday paradox) and some values/sets of values are more efficient, like testing against zero has special instructions and having tags contiguous from 0 to n allows the use of arrays and jump tables in `match`s.
There are to my knowledge two ways of doing this: First the in most cases more idiomatic way of using an enum: enum Object { Type1 {x: i32, y: i32}, Type2 {a: usize, b: usize, c: String}, ... } Or using boxed trait objects (https://doc.rust-lang.org/book/trait-objects.html#dynamic-dispatch)
Compile-time errors happen when you try to use the unit-thing `()`. Not even `format!("{}", ())` works. &gt; I can imagine writing something like that into network socket Don't worry. Use `"{:?}"` when you're actually debugging, otherwise `"{}"`. 
Rust is a lot like Scala in this respect. You're not forced into the most esoteric parts of FP style (and Rust culture has far less ["I (heart) applicative functors"](http://jimplush.com/talk/2015/12/19/moving-a-team-from-scala-to-golang/) than Scala), but stateless data and function purity are good for both your sanity and passing the compile-time borrow-check. Rust doesn't yet have the mature concurrency libraries that you'd find in an FP language (or again, in Scala), but it's definitely capable. Read the Rayon source. It's remarkably simple. &gt; Why would I choose to do concurrency in Rust over a FP language like Haskell? Because you can explicitly do things like struct-of-arrays and having a sane amount of mutable state in your inner loops - things that improve serial performance. You're less dependant on compiler optimization and able to push the hardware further. 
Though in practice people don't tend to bump the major version without breakage, so casual observers would likely assume that Rust had broken something.
Glad to hear that's in the pipeline. (I'm still having trouble navigating the discussions. Looks like RFC811 is [postponed](https://github.com/rust-lang/rfcs/issues/811).)
&gt; stateless data What is that?
Yeah, that sounds about right, and hints that it may not always be a win, but it is great for enums with one extra variant, like Option. That said, one could probably do tricks like not just choosing the next tag in order but instead choosing tags for the outer enum that have a bit set that the inner enum doesn't ever set, so distinguishing the layers becomes a single bit test.
&gt; stateless data A data structure with the property that no changes are allowed after it is created. (immutable, ie no mutations)
From the mioco examples: let size = try!(conn.read(&amp;mut buf)); if size == 0 {/* eof */ break; } Basically, there are situations where the mio coroutine's wrapper for read will return right away, and that is when the socket has been disconnected or has some error otherwise. You still need to handle this case in your code. So what's happening is the read_line() call is returning immediately, and therefore you're looping very quickly. On a side note, you probably shouldn't use mutexes in coroutine based code. There is likely a more "cooperative" way to do what you need. 
Rust doesn't carry runtime type information around unless you *really* ask for it. The idea is that what you're trying to do is almost probably not the best way to do what you're doing. (Sorry.) And also the type-checking machinery in `rustc` can't help so are you sure you want to turn it off, like really really sure? With that caveat said, I've played around with it and it looks like the options are - wait for specialization (specialization hype~!) and extend `Any` - write unsafe code that unpacks a trait object similar to how `Any` is implemented. Here's a use of `Any` https://gist.github.com/anonymous/27b3d64aa7cc8fbd2fe1cd64265a823b And this is a roll-my-own trait-downcasting pattern, similar to what `Any` does but probably more useful for now. (Also 100% safe code.) There's a lot of boilerplate, so it probably should be done with macros. https://gist.github.com/anonymous/49c3ea62c2cfd2300827e9bb38325197 The key elements are: - `Box&lt;Animal&gt;` - This is a trait object. Needs to be a trait object to force runtime method dispatch. - AnimalRef - a custom reference type that knows what kind of thing it's pointing to. - Trait method `downcast` defined to construct the appropriate variation. A third way would be to use an owning/wrapping enumeration. This is actually a pretty good option! The downside is you lose the ability to do anything with the enumeration without matching the specific types. The upside is that enumerations are statically sized and can go directly in a `Vec`. This can be much better for memory speed. Also you don't need fat pointers. This should probably be default if you can. But it's more like old-school C than a Java idiom. A fourth way splits the objects into two parts. One is for the general functions, the other for type-specific ones. This is probably the most difficult, but could be best for speed. --- Buuut, just because you *can* cast stuff back and forth to pack objects into the same collection doesn't mean you should. There are drawbacks, and Rust is a little bit more eager to expose the costs of abstraction. 
They use equivalent types that never modify in place. That is, you never delete a node from a tree. You return a new tree that doesn't have that node. Functional data structures are a class of data structures that support this efficiently.
Sounds nice in theory, but how does it work in real life? For a mutable binary tree, my data structure is as simple as struct Node { Node* left; Node* right; Data data; }; I'm using C here, but it should still be fair game because C is basically a thin abstraction over the physical machine, which all fancy languages will eventually be mapped into anyway. Now, let's explore struct MagicNode { ?? }; where `MagicNode` is a node representation of such "persistent data structure." What would the `??` be? How would // Returns the root of the "new" tree without node_to_be_deleted. Node* DeleteNode(Node* root, Node* node_to_be_deleted); be implemented?
How do you return a new tree without being inefficient?
Wow, that's a lot of copying. Won't the RAM usage be shooting up and down a lot?
If you put on some blinders and just look at the core mechanics of the language, Rust basically is a functional language, but it's special because * Rust isn't lazy like Haskell. Laziness imposes a significant runtime cost. * Rust has much more convenient access to mutability than Haskell * Rust readily supports unboxed data. I'd expect Rust programs to therefore have a lower overall memory footprint * Rust offers much more precise control over allocations and memory layout. This can have a dramatic performance impact * Rust programs don't have GC pauses because they have no GC I happen to think that GC, restricted effects and STM are all tremendously useful features, so my personal preference is to bias toward Haskell for concurrent systems if I think I can afford to pay the extra RAM and latency. Your mileage may vary. :)
If you want information on how Clojure does its native data structures, one dude put a cool [series of posts](http://hypirion.com/musings/understanding-persistent-vector-pt-1) up about them. Also, you can read the (admittedly funky) source [on Github](https://github.com/clojure/clojure/blob/master/src/jvm/clojure/lang/APersistentVector.java) (the other native data-structures are in the same folder)
You might want to read [Purely Functional Data Structures](https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf) or the [book for more details](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504).
The full structure becomes a DAG, you only need to copy the elements which have pointers into the modified element. Most of the structure is usually the same. Remember that the alternative way to get the same effect would be to copy the entire structure every time someone wants an immutable view of it.
Yes, for a singly linked list, you'll need to make a copy of all nodes from the head to the node you're trying to delete. And for doubly linked list, you'll need to duplicate the whole list. (And no, just incrementing the refcount will not work.) You'll need a tree data structure to do better.
&gt;As a general rule, an undocumented library is only useful if you can understand how someone would write it from scratch. (b/c you'll have to read source, and it's really hard to read source that does things you don't understand.) Having spent much of yesterday trying to do a thing with ffmpeg that would've been trivial on the command line... yup. That's at least documented as the C API it wraps, but it's still harrowing. Something like this, that is more isolated? Much worse. 
As you probably can guess, I've already thought about a few things...but first I need to make this at least somewhat usable. Currently thinking about a db scheme that let's us put type-type and fn-fn relations including generics into some tables so that a) duplicates ate removed, b) it's sufficiently simple to go from adjacency to reachability and c) lookups are sufficiently fast to use within a lint.
Thunderbird is no longer a Mozilla project. (see below, that's actually subtly wrong) That said, it's not like Rust is exclusive to Mozilla, or anything... but I haven't heard anything about the current maintainers of Thunderbird wanting to use Rust.
Removing employees happened in 2012: https://blog.lizardwrangler.com/2012/07/06/thunderbird-stability-and-community-innovation/ More recently, this: https://groups.google.com/forum/?_escaped_fragment_=topic/mozilla.governance/kAyVlhfEcXg#!topic/mozilla.governance/kAyVlhfEcXg So it's more of "Thunderbird is in the process of moving out of Mozilla", technically. But it's happening over a long timeframe.
If it makes it in to the compiler the feature should be called something like General Code Hallmark Querying, or GCHQ for short. ;)
Yes, but also conversely in SemVer there is no reason to increment the major version component without a breaking change.
[Are you sure?](http://is.gd/sJspjG)
Didn't know about `cargo-clippy`, thank you! Yeah, I used One, because I made InclusiveRange generic. I don't need it to be generic but I thought it might be useful later if I separate the code. It's like `std::ops::Range`, but the end is inclusive.
It's also useful for static stack usage analysis (assuming we can also extract stack usage per function somehow).
Regardless of how/where/by whom Thunderbird is developed, it still uses Gecko. So any addition of Rust code in Gecko also affects Thunderbird. * The "Oxidation" meta-bug: https://bugzilla.mozilla.org/show_bug.cgi?id=oxidation * The project the furthest along is an MP4 demuxer in Rust (though you rarely play videos in Thunderbird) https://bugzilla.mozilla.org/show_bug.cgi?id=1161350 * Another project is using rust-url: https://bugzilla.mozilla.org/show_bug.cgi?id=1151899
What is this benchmark supposed to do? What are the parameters? Without knowing what you're trying to test, it's impossible to say how to write it better. (as one example, you could use threads, but is that allowed?) (as a second, you allocate a million chars, but since you use 'from_digit', you can only pass in up to 10.)
Very interesting project. However I suggest changing the name. It might be cute, but the US intelligence organisations are currently killing many innocent people, mostly Muslims in the middle east, based on illegal spying programmes.
Rusts borrow system is not about concurrency itself. It prevents you from accessing potentially invalid memory. You still can easily deadlock yourself or get data races from a sequence of atomic operations. Unlike haskell it does not restrict side effects to the IO monad, so they can happen everywhere, including somewhere deep inside the call tree. In practice you avoid the raw synchronization primitives and use channels or a library for asynchronous processing (async in haskell, eventual in rust). If you need shared memory, MVar has the problem, that an exception (remember async ones) can skip the rewrite of the empy var and leave other threads deadlocked. Rusts RAII will automatically close the mutex. I implemented software transactional memory in rust and it worked quite well. Thanks to borrowing, you can't create undefined behavior. Unlike haskell I couldn't prevent side effects, mixing of different synchronization systems, the control flow needs to be explicit and you can still use threadsafe but mutable types in the TVars.
The goal was to have a simple benchmark to test the time it would take to write to write X number (passed as parameter) of char to an array capped at 1 million. It should be single threaded to keep simplicity since my goal is not to actually measure the performance yet but to analyse the generated IR code. I don't understand why the use of 'from_digit' would limit the number of chars to 10. Thank you very much.
You don't have to declare the variants' members inline if you already have the structs somewhere else, eg: struct Struct1 { ... } struct Struct2 { ... } ... enum Object { Type1(Struct1), Type2(Struct2), ... }
Do you have a suggestion (apart from stopping the project altogether and developing it within rustic)?
One of those projects I think everyone should attempt. 'Yet another' implies other projects for comparison: * https://codereview.stackexchange.com/questions/122566/parsing-rpn-calculator-algorithm * https://www.reddit.com/r/rust/comments/24x8t0/im_writing_a_calculator_in_rust_and_have_a_few/ * https://www.reddit.com/r/rust/comments/43351q/looking_for_feedback_rustyard_shunting_yard/ * https://github.com/Hoverbear/rust-rosetta/blob/master/src/24_game.rs * https://github.com/rodolf0/tox/tree/master/shunting Some notes: Others allow you to get the underlying AST and RPN representations, Pupil immediately evaluates the input. Pupil does minimal allocation (only the internal stacks are allocated). I also focussed on the public interface, documentation and a readme.
Yeah, we'll need to take care of those, too.
Things I learned in this project: * Rust is awesome but I knew that already ;) * `Peekable` sucks or I don't know how to use it. Cloning the iterator and calling next on it is way easier. * `::std::vec::Drain` should have a method `as_slice()` and `as_mut_slice()` to allow you to read the underlying items being removed at once. * Interacting with C expecting nul-terminated strings is a bit of a pain... * Macros (`macro_rules!`) is still a bit mysterious for me, I failed to use it to reduce code duplication in the builtins implementation. * How do I UFCS for builtin types? `use ::std::f64::*` doesn't work. Eg. `assert_eq!(sqrt(9.0), 3.0)`. * `::std::error::Error` is very painful to use in practice. It also makes unit testing a pain since you cannot `assert_eq!` on `Result`s containing it. * Probably more I've forgotten...
This talks covers the fundamentals of Rust ownership &amp; borrowing and why it is nice to use for concurrency http://www.infoq.com/presentations/rust-thread-safety
I'm surprised nobody mentioned MIR yet. This is exactly the sort of thing I expect to be built on top of the incremental recompilation dependency graph and MIR itself, which would make it simpler and more powerful, at the same time.
I really have to look into the MIR stuff, but it's so *new* :-)
I hadn't heard about save-analysis yet, but I'm gonna try it!
Ah, I see. &gt; I don't understand why the use of 'from_digit' would limit the number of chars to 10. Because it's 'from_digit', not 'from_digit**s**', and 9 is the highest digit in base 10. What you want is `char::from_u32`: arr[i]=char::from_u32(i as u32).unwrap(); You might want to run `rustfmt` on your code; it's got some formatting that's unusual, too. :) Oh, and I would parse as a `u32`, and then use it everywhere, except casting when you use it to index. I think that's a bit cleaner, personally, but I guess I could see it either way. And, also, `char` is meant to be a unicode scalar value, and so only a certain range is actually going to work here... you'll error out at 55296. As for printing out the array, that's because without it, LLVM will see that you do nothing with the array, and completely optimize it out, I'd think. I wouldn't look to the LLVM IR, I'd use `perf`. It's not that useful here, though: even the maximum argument runs basically instantaneously on my machine. Hope that helps!
Project names are rarely a good place for satire and most people I know that picked one later say that it was a bad choice. You can understand the concept of satire and still think that it is overused as a way to thwart disagreement.
You want to go here: /r/playrust Please ensure before posting that the subreddit is actually the one you are looking for...
How can rustfmt help me? from what I read it is just for formatting rust code. What am I missing here? So I could just i%55296 to avoid the error? I see, isn't there a better way to do this like the #[inline(never)] that would prevent this optimisation? I'm looking into the LLVM IR because I want to write a new pass so no way around that ;) 
Nameless name, data without mutation... It means that if a pointer doesn't change value, the data behind it cannot change either. Following this rule perfectly means you wouldn't need `&amp;mut`, synchronization primitives, or interior mutability (`Cell` and friends). It's still possible to update this kind of data structure. For example: adding an item to a red-black tree is O(log n) operations. It also creates O(log n) new nodes, starting from the insertion point and moving up to the root, adding a few new nodes where rotation was required. The result is you have an old tree and a new tree. Different roots, so as expected a new pointer for the new version. Both are valid. The unchanged nodes are shared between them. Reference counting is a pretty good fit most of the time. (Simulations might use a kind of ping-pong buffer, or [even something more sophisticated](https://www.youtube.com/watch?v=-cWJRZhALDg).) Data shared between threads needs `Arc`. You cannot create cycles without interior mutability, so there's no need for tracing GC. Bottom line is that stateless data absolutely rocks when you're sharing data, need versioning, and don't want to think about consistency bugs like iterator invalidation or what order to update your data.
The answer to more or less all of your questions is: because Rust doesn't have generic value parameters. Basically, Rust can't be generic over the `N` in `[T; N]`; you *always* have to use an actual, specific value for `N`. As a result, you can't implement things for "arrays"; instead, you have to implement traits for every specific size of array you care about, and you have to stop at some point. You can't write a `flatten` function because even if you erase the outer array's length, you *can't* erase the inner array's length; the signature would have to look something like: pub trait FlattenSlice { type Element; fn flatten(&amp;self) -&gt; &amp;[Self::Element]; } impl&lt;T&gt; FlattenSlice for [[T; N]] { type Element = T; fn flatten(&amp;self) -&gt; &amp;[T] { ... } } You can't express `[[T; N]]` because of the `N`, and `[[T]]` isn't a valid type. You *can* default them (go look at the list of implementors in [the `Default` docs](http://doc.rust-lang.org/std/default/trait.Default.html)), but only up to 32 elements. You have to know the length because *it's part of the type.* It's like how you have to know how big you want an integer or floating point type to be. There's no function to convert from `[T]` to `[T; size]`, again, because you can't have generic value parameters, plus constructing arrays is a bit of a pain because you have to do it in a single step (you can't do it one element at a time, at least, not without tricks). All of that aside, use std::mem::transmute; let arr: [[[u8; 4]; 160]; 144] = panic!("..."); let arr_ref: &amp;[[[u8; 4]; 160]; 144] = &amp;arr; let arr_ref: &amp;[u8; 4*160*144] = unsafe { transmute(arr_ref) }; let slice: &amp;[u8] = arr_ref; That *should* be safe, because the nested and flat array types should have exactly the same layout.
[removed]
Would this also be useful for seeing how often publicly exposed functions get used by external programs? I could see this as being helpful when refactoring and wanting to change an api or broadly looking at what functions need to be optimized.
Prism? The logo makes itself!
Yes, inlining would have a huge effect on stack usage, so you couldn't get accurate information until after optimization.
An LLVM Pass like its described here: [WritingAnLLVMPass](http://llvm.org/docs/WritingAnLLVMPass.html). Am I doing it wrong?
Nope! If you were writing it in rustc itself, I was going to suggest looking at MIR, but if it's in LLVM, that's the doc you want.
I could imagine a crater-like service that let's you look up dependencies by type or function.
Fwiw I do agree, and will change the name when I get around to it.
You could just read to the wiki article, has nice pictures and everything: https://en.wikipedia.org/wiki/Persistent_data_structure Keep in mind, these are just examples in the wiki article. Clojure and Scala use persistent versions of "hash array mapped tries" -- a really awesome data structure.
Newbie here (just started going through the book) This must be simple but, let mut x =90; let ptr_x: &amp;mut i32 = &amp;mut x; *ptr_x +=1; println!("{:?}", *ptr_x); -------------------------------------------- let mut x =90; let ptr_x: *mut i32 = &amp;mut x; unsafe { *ptr_x +=1; println!("{:?}", *ptr_x); } Any idea on why does the second piece of code needs an unsafe block actually ??
&gt; I believe it's meant to be satirical Oh I know that. But just because it's satirical doesn't mean it's not going send a message, about what's acceptable or not. &gt; Have you ever read A Modest Proposal? it's common in American (and probably British) schools Yes. It was written in Britain after all. :)
That was one of the big A-HA moments for me learning Haskell, and there's lots of places where it gets used. One of my favorites is "fromInteger"; when you write `4` in your code, that's syntactic sugar for *fromInteger 4*, and `fromInteger` abstracts the constructor for all kinds of types, allowing you to do [cool stuff like this](https://hackage.haskell.org/package/simple-reflect): -- this is a short module from hackage ghci&gt; import Debug.SimpleReflect ghci&gt; 1 + 1 :: Expr 1 + 1 ghci&gt; map f [1,2,3,x] [f 1, f 2, f 3, f x] ghci&gt; sum [1..4] :: Expr 0 + 1 + 2 + 3 + 4 ghci&gt; foldr f x [1..4] f 1 (f 2 (f 3 (f 4 x))) or this -- a small module I wrote ghci&gt; import AutomaticDifferentiation ghci&gt; let f x = x * x ghci&gt; :t f f :: Num a =&gt; a -&gt; a ghci&gt; f 5 25 ghci&gt; derivative f 5 10 -- and even more awesome ghci&gt; import Debug.SimpleReflect ghci&gt; import AutomaticDifferentiation ghci&gt; derivative (\n -&gt; n*n) x 1 * x + x * 1 Basically, you can take any expression that is generic arithmetic that works on any number type, and apply it to more "interesting" number types than just the usual suspects (`Integer` / `Int` / `Rational` / `Float` / `Double`). [And speaking of BASIC](http://augustss.blogspot.com/2009/02/regression-they-say-that-as-you-get.html), you can abuse this feature pretty hard too.
thanks, that's way better
Because `*mut` denotes a raw mutable pointer, and those things have no guarantees as to what they point to. Using them is inherently unsafe.
Thanks, I assume that means I can also `pub type Value = f64` followed by `Value::sqrt(..)`. Don't worry, I've already had several floating point-life crisises in my life. I'm pretty confident it'll be fine using on such well-rounded numbers for unit testing purposes.
`thread::spawn` spawns a thread that may not capture borrowed data in its closure. This is painful, because rust can express this pattern safely and so well if you use scoped threads, but you need a third party crate that exposes scoped threads. You can use crossbeam, rayon or scoped-threadpool for example.
The pain for me (to underline) is that this is something that rust really excels at, but that it's not available out of the box. It will be in the future, I think. Rayon is really cool, if their high level approach fits your use case.
It means you are trying to move some owned content while there is an outstanding borrow of the owner. I feel like that is worded a little opaquely, so maybe code will illustrate the problem and its closely related errors E0506 and E0507. Uncomment the lines with the error codes to see them in action. struct A; struct B { a: A, } fn main() { let a = A; let mut b = B{ a: a }; // borrow b let bb = &amp;b; // b is borrowed - cannot move a out of b else you might invalidate the borrow // let e0505 = b.a; // b.a = A; // e0506 - cannot mutate while b is borrowed else you might invalidate the borrow // try to move out of borrowed content - can only move what you own // let e0507 = bb.a; } 
&gt; `Peekable` sucks or I don't know how to use it. Cloning the iterator and calling next on it is way easier. Something like a network stream can't necessarily be cloned like that, or if it can, the clone consumes values that the original can't access afterwards.
Diclaimer: I'm not a compiler guy and not familiar with `rustc`'s source code. Someone more knowlegable might weigh in on this. &gt; how low level is it? I think it's safe to say that it is (or can be) as low-level as C for all intents and purposes. You can still look at the assembly code and check it out. The only differences at this low level that come to my mind are * there is no `alloca` in Rust (yet) * a function's prelude might be a little more complicated if it needs to check that there is enough stack space available (I've seen this via `objdump -d`). But as far as I know, it should be possible to make modern CPUs do this kind of checking for free (via its MMU/MPU) with a bit of assistence from the OS. * You might want to look into how stack unwinding is implemented. I'm not familiar with this but since panics are somewhat simpler than C++ exceptions and C++ exceptions are free of a runtime overhead on the success path, I would expect Rust's panics not to hurt on the success path either. But the way I see it, the last two points are features. Point 2 turns undefined behaviour into defined behaviour basically getting rid of exploitable stack overflows. Point 3 allows a more "graceful shutdown" (stack unwinding) compared to just terminating the process.
Assuming, of course, that the compiler can't figure out that the old list is not going to be used. GHC is very good at figuring that out.
Obviously it depends exactly what you mean by "close to the metal", but here's a few possibilities: 1) Runtime overhead. Rust is just as low-level as C in this regard: it doesn't require a runtime, and as with C, the language itself can be used without the standard library. 2) Correspondance to machine code. If we're ignoring optimizations, then the translation from Rust to machine code is still very simple. Trait objects and DSTs are slightly more complicated, so I'd say Rust lies somewhere between C and C++ in this regard. 3) Language features. This is where Rust is about as far from C as it's possible to be. There's a *lot* of stuff that the rust compiler does that isn't directly relevent to how it gets translated to machine code: lifetime checking, type checking, type inference, generics, macro hygiene, traits as type constraints, modules, crates, privacy, safety, etc. Or in other words, all of the most important parts of Rust. I think Rust excels because it's one of the few languages to recognise that (3) is separate from (1) and (2). 
One can get these benefits with a bit of instrumentation on their C++ code. We refer to the instrumented form as 'Rust'. Hopefully most C++ will be instrumented as 'Rust' in the future
You get the data structures you ask for, more precisely than C++. The Rust representation of struc`s isn't as rock-stable as C, but you have the option of C-compatible structures if you want them. No inheritance rewriting your structs for you. Code generation is LLVM with inline assembly if you're feeling particularly low-level. There's a [toy OS](http://os.phil-opp.com/) project, and another that's [quite a bit more ambitious](http://www.redox-os.org/). &gt; Some of that uniqueness looks like it can leave the programmer less aware of what's happening under the hood. Are you thinking of something specific? I'm sure folks here would have something to say. I've found the standard library to be a pretty easy read whenever I'm curious about how things actually work. I do find the compiler's inference of closure types and the borrow checker to be more opaque than I'd like.
Both C and Rust are compiled to LLVM IR if you use clang as your C compiler, and that IR is then translated to machine code. Therefore, both Rust and C have the potential to have identical performance with equivalent code because compiler improvements to LLVM also improves performance in both Rust and C applications. In practice, Rust easily provides the same performance as equivalent C code, but because it's much easier to write and manage large Rust applications and libraries, it's easier to write faster code with more features and less boilerplate. Being able to reduce a few dozen lines of C into a single line of Rust while also having better error handling is always a good thing.
&gt; a function's prelude might be a little more complicated if it needs to check that there is enough stack space available Rust does not have preludes that check for stack space. However on Windows LLVM does add stack probes for any function that uses more than a page of stack space, but that applies to C/C++ as well, so it isn't Rust specific. (Other platforms need stack probes too, but nobody has gotten around to making them work yet) &gt;You might want to look into how stack unwinding is implemented. On most platforms Rust uses dwarf unwinding, which is typically what C++ also uses for unwinding. On Windows (except maybe the mingw targets which still use dwarf) Rust uses the native SEH exception mechanism, again just like C/C++.
As a dutch person I'd suggest: Accessible Introspection Via Dependencies
I wrote up [a comment answering a similar question](https://www.reddit.com/r/rust/comments/4eu0vq/performance_characteristics/d240n7c) a few days ago; you might want to read the original question and parent comments to get some context, but I discuss a few aspects of memory layout and the like that might be relevant to performance or getting an understanding of how Rust code works.
Oh i see, thanks &lt;3
&gt;Generally, by low-level one means ability to access (1) OS functions or (2) hardware functions (by manipulating specific pointer addresses directly). &gt;It appears that this is not what you are talking about here No. That's definitely part of what I'm talking about. &gt;you are focusing on transparency, that is a direct/intuitive mapping of programming language constructs to assembly. I'm sorry if I wasn't clear. I tried to keep things short, and I assumed "close to the metal" would encapsulate the perspective I'm coming from. From my perspective, this includes low level access (interrupts, direct allocation of RAM resources, hardware bitstreams, etc). Obviously, transparency and low level access access should be touched on separately, but a language with C level transparency yet no way to access the system left unobsfucated by that transparency isn't as close to the metal as C. After all, such a language forces the programmer to depend on some higher abstraction.
"Doesn't have runtime" mean that language per se has no runtime. `libstd` is just Rust library and can have own dependencies, just like any other library. In Your context even C has ~~library~~ runtime called libc. 
This seems very similar to mio, can anyone comment on how they're different? Additionally, does this async I/O lend itself to non-network-protocol applications? I'd like to get serial-rs support into one of these async libraries, but mio currently doesn't support that application.
&gt;As an aside, why are arrays so janky to use? Because arrays *are* fundamentally janky. Everything you're wanting them to do is implemented as `Vec`, as those are just arrays with an associated length. You can't convert a `[T]` to a `[T; size]` because you need to know the size at compile time, when type-checking occurs, and a `[T]` explicitly means you don't know the size at compile time.
&gt; but there's no language support for C-style unions. [Note that this](https://github.com/rust-lang/rfcs/pull/1444) [won't be the case for long](https://github.com/rust-lang/rust/issues/32836).
miow is to Windows as [nix](https://github.com/nix-rust/nix) is to Unix --- safer, more Rustic bindings to native system calls.
Rust is as low level as C (and can be used to write kernels and device drivers, just like C). C can call a Rust function and Rust can call a C function without overhead. Rust it has a repr(C) attribute to lay out structs in "C" layout, for FFI purposes. See [this](https://doc.rust-lang.org/book/ffi.html). The low-level parts of Rust are typically marked as "unsafe", because getting them wrong may cause undefined behavior, just like in C. [Rustonomicon](https://doc.rust-lang.org/nomicon/) is a book about unsafe Rust.
So, does gjio depend on miow unconditionally, even when building on Linux? (shouldn't it conditionally depend on it, based on a Cargo feature?)
Good point -- it ought to depend on it conditionally. I only learned a few hours ago how to do that: `[target.'cfg(windows)'.dependencies]`.
[rush](https://github.com/Xion/rush) also started as an arithemetic evaluator, though it's a little further towards a full scripting language now :)
You need to generate a link to *your* code on the Rust Playground! At the moment, your link is just to play.rust-lang.org. Try clicking the Gist button on Rust Playground to get a proper link. FWIW, I fell into exactly the same trap myself.
Fixed, thanks for the hint :).
/r/playrust
I've built it on ARM Linux but unlike passivedns written in C, which stays up sniffing, yours just quits. Strace shows this problem: `setsockopt(3, SOL_PACKET, PACKET_RX_RING, {block_size=0, block_nr=0, frame_size=0, frame_nr=0}, 16) = -1 EINVAL (Invalid argument)`
[All set.](https://github.com/rust-lang-nursery/regex/blob/master/CHANGELOG.md) I've been doing less frequent releases since I started maintaining a `CHANGELOG`.
&gt; Note: Standard compliant C is rather transparent, however non-Standard compliant C, especially once optimized Even standards compliant C doesn't have any guarantees of corresponding well to what is run on the machine once optimised... and on modern CPUs, even the machine code isn't a great model of what is actually executed (well, more accurately, when it is executed).
&gt; The difference is that the vptr is stored next to the object pointer rather than in the object itself, leaving the struct layout unchanged. I did not know this, very interesting!
&gt; From my perspective, this includes low level access - interrupts In kernel code, sure. - direct allocation of RAM resources Absolutely. Standard library uses `jemalloc` but there's interest in making the application-level allocator an injectable dependency. Of course if you ditch the standard library you're entirely free. Even with the standard library, it's pretty easy to grab a `RawVec` or `mem::uninitialized` - hardware bitstreams Again, in kernel mode you can DMA and PIO to your heart's content. Most of the activity is low-level application programming and utility libraries. This isn't because Rust is unable to express kernels, device drivers, or bootloaders. It's just where the interest is. The vast majority of Rust code that exists today has limited access to hardware because it's running in a user-mode process. 
Wow, strange. If it's crashing on setsockopt it might have to do with the pcap API because I'm not making any manual networking calls outside of a few lines using [this](https://github.com/ebfull/pcap). I'll have to check what call is crashing here. Did it quit immediately after running it? Did it manage to successfully parse and print any good output first? Did you run it as root so it could sniff in promiscuous mode? Literally the only network code is in main.rs where it uses the pcap crate, three lines: Where it gets the device: pcap::Device::lookup().unwrap() Where it gets a capturing interface: pcap::Capture::from_device(dev).unwrap().promisc(true).open().unwrap() the loop where it gets each packet: while let Ok(packet) = cap.next() The first two parts are just the initial set up before it enters the infinite loop. The rest of the code is the business logic which parses packet.data, nothing that would change networking settings or handle sockets. Just reading the byte array, nothing more. The dependency on the pcap crate is set to "*" so it might even be they upgraded pcap and introduced something. At a glance I see they updated it 3 days ago and it mentions fixing ARM compilation errors: https://github.com/ebfull/pcap/commit/9005f315bb87c8554534032c154d3fcfd92593c5 You might experiment with setting the dependency in Cargo.toml for libpcap to 0.5.4 then 0.5.5 and see if you still get that. I can test later on a raspberry pi and see if I can replicate any ARM issues. If anything it'd be helpful to report this behavior to [ebfull/pcap](https://github.com/ebfull/pcap) if we find any issues relating to it.
&gt;However, unlike C/C++, Rust provides much better high level iterator constructs, so indexing is rarely needed. I would say this is very debatable until HKTs are around.
Thanks!
No. But you can usually make them reappear by calling `cargo` again.
Of course. What I meant is that there is implication, not iff, between "bump major" and "braking change". 
&gt; C can call a Rust function and Rust can call a C function without overhead. That is not fully true. It'll insert a wrapper function that'll switch between the C-ABI and the Rust-ABI. I'm not sure if this is the case for all architectures, but it's definitely the case on PowerPC for example (at least when calling Rust code from C, not sure about the other way around).
I think you can literally just add `#[no_std]` or `#[start]` and it won't have a runtime. Might be different in the most recent Rust. Even with C if you're going to write Linux ring-0 driver code you have to ditch the C standard library. No stdio.h or stdlib.h, just plain C and the linux kernel API.
If you want a clippy challenge, try it on diesel. I did so for fun a few nights back and clippy found about a gazillion things (half of them seem to be inside macros, but there was one documentation error that I should probably write a PR for). Edit: https://github.com/diesel-rs/diesel/pull/305
I'd argue array indexing in Rust is still "close to the metal", because the machine semantics are obvious. This is the same sense in which the `-&gt;` operator in C is "close to the metal".
Yeah, looks like you are right. I just compiled my code with a recent nightly and the additional symbols are gone. Nice :)
&gt; the language itself can be used without the standard library. I think it's a bit more complicated than that, due to * lang_items (on the intersection between language and library) * stable vs nightly compiler (you need a nightly compiler to change lang_items, and it won't stabilize any time soon AFAIK) * libstd vs libcore (most lang items are defined in libcore, but at least "box" is outside it) I wish there was some over-all long term design plan to try to minimize the amount of lang items, and put the remaining ones into just one or two submodules of libcore (such as e g `core::ops` and `core::marker`), but other things seem to be the core developer's priorities right now.
Do you need to implement lang items if you don't use them? For example in C you typically would need libc, but if you don't use it (e.g. never `malloc`/`free`) you don't need it.
No, you don't need to implement most of them. I think the only ones absolutely required are `panic_fmt` and `eh_personality`, though I don't even know what the second is used for...
Finally managed to get back working on my wayland-server abstraction. :) Hopefully the hardest is done, and once I've fleshed out all the other small things I'll be able to release the wayland bundle version 0.6, with server support! I'm still blocked on [`static_recursion`](https://github.com/rust-lang/rust/issues/29719) for handling `xdg_shell` though :(
&gt; Rust does not have preludes that check for stack space. Do you know whether this was always the case? Because I remember seeing calls to some `__morestack` function assuming that this was some kind of handler for the not-enough-stack-space condition.
Why is this line necessary? https://github.com/Kixunil/image_concat/blob/master/src/main.rs#L280
Working on the Metal backend for gfx! I finished up my bindings a while ago and have started doing some actual work on the backend now. I'm pretty close to getting a empty window showing, but there's a few functions left to implement. The branch is currently at +1571 -4, and if it continues like that, there won't even be a breaking change in the API! :=)
I think the usage string (or help message) might need some improvement, it's not clear how this program should be used.
The syntax `i: mut i32` is not legal, so that's the difference! Using `mut variable: Type` just means you can mutate the variable within the function. This code: fn do_stuff(mut i: i32) { is equivalent to this: fn do_stuff(i: i32) { let mut local_i = i; AFAIK, this applies to ref parameters well. Some might prefer one form over another (I prefer the first). Neither form allows you to make any changes that will persist after the function returns - for that, you need &amp;mut parameters. 
Yes, it is a bug (hence the "currently"), but it's a long-standing bug: it has been open since __Oct 31, 2013__, so it may stay there for a while. What I meant was that it is possible to invoke UB in safe Rust, not that Rust condones UB.
As an ignorant American, I had to google this to get the joke.
So the only reason there's no method like: fn to_fixed_length_slice&lt;T, N&gt;(slice: &amp;[T]) -&gt; Option&lt;&amp;[T; N]&gt; { if (slice.len() == N) { Some(unsafe{mem::transmute(slice)}) } else { None } } Is that the support for generics over non-types isn't there yet? All the types involved would be known at compile time (and would panic/return None if slice.len() != N). NB: The above transmute wouldn't actually work as is, since sizeof(&amp;[u8]) != sizeof(&amp;[u8; N]).
What's an easy (but useful) project so I can get started on rust?
Rust used to check for stack overflow through LLVM's segmented stack feature. The stack limit was stored in thread local storage and every function compared the current stack pointer with it in its prelude. The checks were [removed](https://github.com/rust-lang/rust/pull/27338) at some point since most stack overflows are catched by the guard page. However, it is possible that the guard page is missed so that a segfault occurs. (For example, try `let x = [0u8; 999999999]`.) Stack probes would fix this issue, but they are only implemented on Windows at the moment (due to LLVM problems). For details and the current state, see https://github.com/rust-lang/rust/issues/16012#issuecomment-165589146
&gt; One thing I'm not completely happy with is my use of Rc::would_unwrap to automatically remove elements that are no longer referenced, whilst it works and makes forgetting to remove an element hard it does feel a bit hacky. That almost sounds like it might be appropriate to be a [`Weak&lt;T&gt;`](http://doc.rust-lang.org/std/rc/struct.Weak.html) pointer (to not keep the internal data around for longer than necessary), and then failures to `upgrade` indicate the data is unused.
&gt; The first of these two functions, `eh_personality`, is used by the failure mechanisms of the compiler. This is often mapped to GCC's personality function (see the [libstd implementation](https://github.com/rust-lang/rust/blob/master/src/libstd/sys/common/unwind/gcc.rs) for more information), but crates which do not trigger a panic can be assured that this function is never called. &gt; &gt; -- https://doc.rust-lang.org/book/no-stdlib.html According to a quick glance at the linked `gcc.rs` file, the "eh" in `eh_personality` stands for "exception handling" and it's "used by all cleanup landing pads". A quick double-check on the GCC function it wraps confirms that it implements stack unwinding.
I'm getting ready for finals so I can binge on Rust and build my recipe recommendation engine/API over the break
Thank you for your work, and thank you for that article. 
I'm working mostly on additions to Crossbeam - flat combining, bounded queues, and left-right locking. I'm also starting a dataflow project similar to the lmax disruptor. The general idea is that one defines a graph of actors which recieve data from a message queue, apply some stateful transformations, and potentially send the data to multiple other actors. This is different from timely-dataflow in that it's built purely for low latency streaming (~30-50 ns max per hop) and high throughput, but isn't very good for say data parallel processing or "big data" applications. This is pushing the limits of my internal C++ -&gt; Rust translator, so I'm spending more time now figuring out how to represent this in Rust.
You seem to have missed [the implementation of naked functions](https://github.com/rust-lang/rust/pull/32410).
Awesome!
Actually, there is no easy way to say whether a function is pure or not, except for trivial cases. For example, almost all functions use loggers, should they be considered as impure? Ok, loggers calls can be just skipped. Then there comes more complex question: Is the function mapM pure? Since rust doesnt have HKT, let me write this signature using haskell: mapM :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] And the answer is, that the function is pure, if the monad m is List or Maybe monad, it is impure (i.e. can have side effects), if m is IO. So, we cannot say anything about purity of mapM at all until we have it bound with some context (i.e monad). Just a small notice, awesome project idea anyways. 
I think you are confusing specifications with safety. If you specify that cap always truly reflects the capacity of vec, but then add a method that violates that, that's a violation of your software spec. Until compilers can read design docs there is no way to catch it. :) Ie it's possible to still write bugs in perfectly safe code...
Oh, I must've missed that.
Have you had a look at [Turbine](https://github.com/polyfractal/turbine)? It doesn't currently work though.
If you want to go even smaller, take a look at this: https://github.com/Hoverbear/rust-rosetta/issues/170 That won't involve any trait-fu in it, but you'll get to know the syntax better. Try to (over)use iterators and closures!
Of course purity is undecidable in the general case. However, we can err on the side of caution and mark functions calling referenced closures (whose type need not be known at compile time) as impure.
I'm not a big fan of putting things into category A or category B. Regardless of what you actually call it, the end result doesn't change and that's what I'm interested in. I guess what I'm saying is I don't like the idea of arguing that the compiler should or shouldn't do something based upon which category someone decides a thing should be in. Specifically, safe code can mutate state that unsafe code relies on, thereby inadvertently causing undefined behavior. In addition, the cause may not be obvious by inspecting unsafe code. Regardless of which category you think that falls in, I think it's useful to consider ways in which we can be helpful in these circumstances by a) minimizing the opportunities for these sorts of errors, and/or b) documenting the dependent state to assist a developer in identifying potential problems and to assist new developers in the future in understanding which state can be potentially dangerous to mutate in safe code. And maybe the current state is the best that can be done, I was just curious enough to get others perspective on it.
I'll read up on them, thanks.
Split it into server and client (like MPD) and you have +1 user. 
I hadn't! It doesn't seem to have a whole lot of code or be active, but it will certainly be helpful for figuring out how to best represent graphs and the desired API in Rust. I plan on there being a wider variety of communication primitives (spsc optimized, broadcasts, standard spmc, mpsc, etc) then are in Turbine - a solid amount of my crossbeam work is actually with this in mind.
I just remembered it because I managed to get it to compile some months ago, but I never got it to pass all the tests, so it's probably somewhat buggy.
Not a bad idea. I'll try it out, thanks
I would love to see headless as a base with some standarized protocol. Just like MPD. 
Lots of compilers have bugs. There's almost certainly undefined behavior in Rust that's due to long-standing LLVM miscompilations nobody has found yet. That doesn't mean Rust is unsafe, just that it, like all languages, has bugs.
Hey so I set up `CARGO_HOME` to point to the correct location. But I wanted to confirm that the variable being set is an environment variable right? Because external crate completion is still not working :(
I remember: - Some suggestions for more code tags in doc comments (incl. some false positives for "PostgresSQL" since the lint thinks camel case text `clippy.toml`) - `match *thingy { Variant =&gt; "yay" }` instead of `match thingy { &amp;Variant =&gt; "yay" }` (about 10k occurrences thanks to the macros in `tuples.rs`) - clippy likes `if let` more than matches with one arm + `_` - `map_or` instead of `.map.unwrap_or` - there is an enum with variants that have an `Error` as suffix, but I think it was public? (clippy _hates_ suffixes of enum variances and even calls them 'postfix' to show how much it despises them! ðŸ˜œ)
You're in the wrong Subreddit; you want /r/playrust
I'm happy to see that your implementation is helping to uncover ambiguities in the protocol specification.
Fixed now.
[removed]
OT: that article on procrastination, and its successor, were a really great read.
&gt; You can combine two vectors efficiently just by using the append() function, which will move the contents of one vector to another, leaving the moved vector empty. Not if they contain objects of different types, you can't. The example had a `Vec&lt;X&gt;` and a `Vec&lt;Y&gt;`.
&gt;The problem there is shared mutability. Awesome, thanks for the tip! My situation is a bit more complex, as I can't use IterMut, but you put me on the right track.
I want a macro to call a function of a type. Why does the compiler complain ? http://is.gd/ALExcW
[Found it.](https://github.com/rust-lang/rust/pull/33090) Definitely looks promising!
is this behavior desired ? calling T.method() looks weird. http://is.gd/3zDC9c
Typestate doesn't really do anything for this problem, as far as I know. More or less trivially follows from patrick's post declaring type-state dead, but still encodeable (use PhantomData).
yes I have, but it actually compile and run the same way on my desktop. What is the problem ?
I look at it this way. These are encapsulation keywords: `mod` `pub` `crate` These are data-structure keywords: `struct` `enum` `type` These are dispatch keywords: `trait` `impl` `fn` --- Safety can't generally protect against data corruption because data corruption is too difficult to define. Best we can do is to funnel interaction through an interface of functions. This is done by making part or all of a type's value opaque. "All these fields are yours except `.europa` Attempt no access there." *Any* chunk of code that is complex enough for its own module is likely to need protection for its types. Thus `unsafe` is kept separate from data. `unsafe` adds an intermediate level of exposure to the dispatch-control keywords `trait` `impl` and `fn.` It means "I cannot check if you're using this correctly." `pub unsafe trait` -- You must write these functions carefully. `unsafe impl` -- Yes I have written these functions carefully. Trust me. `pub unsafe fn` -- You must call this function carefully. `unsafe { ... call() ... }` -- Yes I am calling this function carefully. Trust me. I call these dispatch keywords because they have the unique ability to control which calls are possible (especially calls to a type-generic function) and where they land (even across threads!). The one partial exception to this principle is an `unsafe` marker trait (like `Sync`). It doesn't contain any methods, but does controls how the type system will monomorphise unsafe code.
First of hopefully many blog posts on Rust! Appreciate any feedback on content/approach/next steps. Stay tuned for posts on lower-level explorations and scalability experiments on other architectures.
I enjoyed reading them, up to the point that the author starting proposing techniques to fight procrastination. Then my monkey seems to have gotten scared, and decided it was time procrastinate finishing the article by cleaning up my .emacs file, which lead to mucking with racer and switching from multirust to rustup.rs. Garbleblerg!
There's a lot I want to say, but for brevity I'll try to nail it down to the essentials, this will probably read in a disjointed manner as a result. I think it's unfair calling out the vec.cap example as people reach for it because it's in the rustonomicron, and I think the bikeshedding characterization is also unfair. Wanting to communicate intent in software that's maintained for years across developers who don't necessarily have the ear of the guy who came before them is a perfectly valid reason to discuss potential additions to the language. I don't think it's important enough by itself (and I tried to convey that in my OP), but calling it bikeshedding is awfully dismissive. I don't have a horse in this race, I thought it was interesting and gave me a chance to learn more about the language. My response to crusoe was more because I dislike the mindset represented in his post. To me it's like watching TDD proponents argue unit test vs integration test when really it's the end result (robustness of the software) that should be guiding your decisions and not the category that test technically falls into. I wasn't necessarily arguing for the feature, but that probably wasn't clear. You mention a false sense of security, but I think it's arguable that 'safe code' gives a false sense of security when it can actually cause bad things to happen. My experience has been a lot of people talking up the safety of rust due to the safe/unsafe dichotomy, which is why I was surprised at the observation that safe code can be very unsafe. It makes complete sense and I'm not lambasting the design, but that issue isn't necessarily always communicated by the community and that may very well be because many people *do* have a false sense of security surrounding safe code. Ultimately I think it's perfectly valid for developers to want to be able to communicate which mutations in code are more dangerous than others (from a safe/unsafe perspective, not a correctness perspective). **Maybe it should be in the tooling instead**. Static analysis could probably get us most of the way to that information, why not build it into the compiler and have racer hook into it. The tooling around rust could then enable developers to do things like query which fields in a particular struct get accessed in unsafe blocks. In my mind that's more maintainable than using the unsafe field tag since, like old comments, they run the risk of falling out of date. It doesn't change the language semantics at all, and it starts rust down the road of supporting tooling specifically around unsafe code usage (you may already be doing that, I don't know enough about rust yet).
How does combining the allocation of two vectors improve space efficiency? Seems like it might reduce it, if malloc is now required to find a larger block instead of two smaller blocks which might fit in holes, or be on free lists...
very well written and very interesting!
If your safe code is unsafe it's more likely a bug in your spec than the language, whereas in c the language practically guarantees a bug. Safe code never intended to protect you from bas implementation. It's only there to try and reduce memory errors which are a huge source of trivial exploits under c or c plus plus.
Thing is, is `cargo add` a nightly feature? Because cargo throws an error saying it isn't a valid sub command. 
Thanks man. Please post yours as well - didn't mean to step on your work! Would be happy to start collaborating on this if you're interested :)
&gt; You can also parallelize when the k-dimension is large and see improvements. Yeah so I had another chart in the 'rayon performance' section that I didn't include which has k going from 0 to 512. the interesting thing is that any blocksize &gt; 64 resulted in identical rayon performance across all k. I wasn't really sure what to make of it, and so I wanted to discuss that in the next post of the series after doing some valgrind stuff &gt; You get much better performance on the dimension you're reducing than I can! (Despite seemingly equal algorithms). Yeah so rayon's kind of a black box so it's hard to control what's going on. Even the variance of individual benchmarks are pretty high. I think once I do some more benchmarking on EC2, we'll have some solid baselines we can compare. In the repo I link to, I have a parallel implementation of matrix multiplication with simple_parallel, which may provide more consistent performance since concurrency is guaranteed. &gt; I think we need to look at parallelizing within the matrixmultiply algorithm if we want to compete with BLAS in more areas. This is probably a lot of work though! Totally agreed. In the paper describing BLIS etc, they mention that the process is definitely parallelizable. we should look into it, discuss with bluss.
Thank you!
Cool writeup, I'm interested in where this takes you. Also: &gt; achieve unparalleled performance Huehuehue
You might know this already, but there's some other black magic that BLAS takes advantage of that you (unless I missed it) don't. It turns out that the computational complexity of matrix multiplication is actually better than O(n^3 ). In fact, it's not known what the complexity is, and it's an active topic of research in algebraic complexity theory. A guy named Strassen figured out how to get it down to approximately O(n^2.8 ). It's a big enough time save in big enough matrices (bigger than 100 x 100 or so) that BLAS uses Strassen multiplication rather than the naive textbook approach. There's been advances since then, but they're really only advantageous for staggeringly huge matrices that a desktop computer would struggle to even keep in memory.
Well, I was using "space efficiency" in a very loose way, to refer to a lot of memory-related properties, like just the cache advantages of being next to each other in memory or the saving of an allocation. First I should clarify that this only really makes sense if you're not reallocating the vectors frequently (eg. either they're really `Box&lt;[T]&gt;`s or you just only very rarely add to them). The space advantage then is that you can pack the elements together, so they only get padded out once by the allocator.
Just a heads-up: Name changed to metacollect.
Not in any modern implementation I know of. The benefits to good pipelining and cache coherency wins up to matrices that can fit into memory, even for large systems.
Since `T` is a unit struct, `T` as an expression means create a new unit struct of type `T`. Therefore `T.method()` is the same as `U(9).method()` if `U` was defined as `struct U(u8);`
So, that would mean that once we have panic-as-abort, it'll be possible to compiler rust programs without any lang item ? That's neat, I guess... Not my usecase at all to be honest `^^"` _(how can I even to this smiley without it being misinterpreted by reddit ? &gt;&lt;)_
To expand a little, you should probably box the decoder and then turn it into a mutable pointer using [`into_raw`](https://doc.rust-lang.org/std/boxed/struct.Box.html#method.into_raw), this will also prevent issues with potentially multiple mutable references. Once decoding is done, you can turn the decoder back into a box using the `from_raw` api, which will then ensure that it will be dropped and resources will be closed.
&gt; We achieve about a 2x speedup with this change [unsafe indexing]. Damn LLVM, why aren't you hoisiting the bounds checks out of the loop in this case? 
He can use as_mut() instead, if he doesn't want to give up Ownership of the box like so: http://is.gd/D9kQkP
Huh. Older posts remember older flairs. I wonder whether that's intentional or just a side-effect of performance-related schema optimizations. **EDIT:** Just some kind of cache coherency issue. Reloading the page pushed the older two flairs to the new name.
I see nsa on one and metacollect on the other, I hadn't opened this thread before. I have no desire to understand how that's happening.
Well turns out racer works perfectly in vim, but something is up with my spacemacs's config. Fun! 
Perfect, thank you both for the help!
Your series is great! Keep up the good work!
If you are talking about padding put in place to align the allocations, then allocating them together saves nothing. If you are talking about some padding used for bookkeeping by the allocator (like storing the size) then one can use an allocator that doesn't store the size and get the same overhead from two allocations or one. So I still don't see the advantage.
non-sequitur
&gt; panic instead of segfaulting This may be possible on some systems, but it's very tightly tied to the OS. For example, a process running under Linux *can* handle SIGSEGV, even if it has no stack space. It's not trivial. You have to set up a signal handler (POSIX portable) with its own stack space (not portable). Then the SIGSEGV will land execution in that handler, on the signal stack, with architecture-dependant execution context (basically a dump of the CPU) pushed onto that stack. So decode that, start unwinding the application stack. If you find a recovery point: modify the execution context appropriately, then return from the signal handler or otherwise make the appropriate system call to restore. It would be slightly more practical for the handler to check for (or unreserve) sufficient stack space to run the usual panic code on the application stack. The bottom line is that Rust, like C, is not suitable for deep recursion (not a huge penalty, the real limit is on the order of 10MB) or arbitrary recursion when parsing untrusted data (Denial-of-Service vulnerability). 
https://github.com/rust-lang/rust/issues/16012
Yup it is possible to do that. I made your example work, with some extra testing code - [here](http://is.gd/RXZp19). Generally, all you needed was to change your impl a little. There's a bit of a special syntax going on when it comes to calling your end method. That's because your trait is generic over a type `T`, but since the compiler can't deduce `T`, you have to specify it explicitly.
I am back from my trip! It went really well. People really like Rust. :) My big tasks this week are: 1. some administrative stuff that i've been ignoring. emails :( 2. catch up on my review queue 3. get at least one chapter of the new book done, if not two 4. integrate the reviews on he existing chapters
You actually do see a better behavior if you [recursively use up all of the stack with smaller allocations instead of one huge allocation](https://play.rust-lang.org/?gist=179ac3a85dd66383b623b143262d210b&amp;version=stable&amp;backtrace=0): fn main() { main(); } Gives you: thread '&lt;main&gt;' has overflowed its stack fatal runtime error: stack overflow playpen: application terminated abnormally with signal 4 (Illegal instruction) Before running `main`, Rust sets up a signal handler that will catch an access to the guard page at the end of the stack, and in that signal handler will print this error message and then abort with a different signal. However, since Rust doesn't currently insert [stack probes for allocations larger than a page](https://github.com/rust-lang/rust/issues/16012), it's possible for allocations larger than a page when overflowing the stack to run beyond that guard page, thus causing your program to simply terminate with the original SIGSEGV. So yes, it's possible to catch these, by statically detecting allocations larger than a page, adding stack probes which are just no-op accesses to memory one page at a time within that allocation to insure that you trigger a segfault in the guard page if you are going to overflow the stack, and then catching that particular segfault and printing a more informative message. Right now we only catch it for smaller allocations, but we really ought to be doing the stack probes.
I'll definitely take another look now we have JSON output. Also I think we can have both â€“ save_analysis for types and call graph and metacollect for some nitty-gritty stuff (like `std::ops::...` invocations, collecting inherent panics, allocations, etc.)
I knew there was some handler to generate that message and that it (probably) didn't start a panic. But I wasn't expecting it to be written in Rust itself. Very cool.
Do you know about `perf script`? (In any case, I imagine parsing the binary `perf.data` is faster than shelling out to `perf script` and capturing/parsing the text it prints.)
&gt; but it will certainly be helpful for figuring out how to best represent graphs and the desired API in Rust. Two great posts on graphs: http://smallcultfollowing.com/babysteps/blog/2015/04/06/modeling-graphs-in-rust-using-vector-indices/ and http://featherweightmusings.blogspot.com/2015/04/graphs-in-rust.html
I apologize for blowing up a bit. For context, I'm the author of the nomicon. The example was meant to make people aware that unsafe code is all-tainting, but a lot of people have taken it to mean "oh this is a flaw in Rust; we need to fix the Vec.cap issue". So I consider this whole situation basically a failing in my writing. I'm also contractually barred from improving these docs for the forseeable future, so it's just gonna be there until someone else decides to change it.
:( File a bug? https://github.com/rust-lang/rust-playpen
As you've found, the idea has been proposed in different variants (e.g., for public fields also) by different people. Many people see a "theoretical" value to it. I am one of the participants of issue 381 you linked to, and tried to formalize the argument you are making here: by properly using the proposed feature, you can inspect less code than the whole module for the invariants encoded in unsafe fields. /u/Gankro has raised there and here the same two arguments, both valid on the face of it: 1) Maybe the set of cases where this is useful is empty or very small? 2) The usefulness of the feature needs to outweigh the cost to the language (complexity to users, implementation, etc). nikomatsakis gives an existence proof for the useful case of (1): in working on Rayon, he would have found this feature useful. Overcoming the quantitative parts (are there many cases? do they justify the complexity costs?) is hard, and I would love it if people (/u/gnzlbg?) point out more examples in issue 381, rather than here. I believe the usefulness of this feature will become clearer as work on formally proving properties of safe+unsafe code, which has already begun (Ralf Jung's, or applications of SAW [1] to Rust), progresses further. I am not an expert in either, but my guess that any method will benefit from needing to apply to less code. [1] http://saw.galois.com/ I hope people participating in this discussion here will read the issue and be clear on what they are responding to.
(edit: meant to repy to /u/thiolliere) I don't know why the parser can't cope with this. It's arguably a bug. As /u/DroidLogician says you can use an ident, but then you only get one path element. You can use a repetition of idents separated by ::, like [this](http://is.gd/xU5zMO), but that _still_ doesn't cover paths with generics in them, like `Ok::&lt;i32,_&gt;::is_ok`. For that you'd need repeated tt, and to get _that_ working, you need an [AST coercion](https://danielkeep.github.io/tlborm/book/blk-ast-coercion.html) to work around _another_ parser bug: http://is.gd/xZyyVd.
[removed]
Reddit has several caching reverse proxy servers. You are seeing variations because the caches on each server are of different age. As time passes and the pages are evicted from cache, old posts will be rerendered and will then have the new flair.
Nothing to apologize for, I can understand the frustration :) FWIW I found the nomicon to be super helpful and I don't think there was anything wrong with the example. There's always tension between explaining an idea and capturing nuance. I can't tell if your 'contractually barred' comment was tongue in cheek or not, but one bit of feedback is that as someone not familiar with rust already I found the [how safe and unsafe interact](https://doc.rust-lang.org/nomicon/safe-unsafe-meaning.html) page a little confusing until I read the [working with unsafe](https://doc.rust-lang.org/nomicon/working-with-unsafe.html) page. Confusing isn't the right word for it, due to my inexperience with rust I found myself understand the theory but not immediately realizing a concrete example of unsafe code having to protect itself from safe code. I mentally put the question aside and of course it became crystal clear on the next page. I've noticed the rust guides in general will link to other pages with ideas that tend to be missing in the current explanation and I thought that might be a good place to add in a similar link. I'm sure you get this a lot, but thank you for writing the Rustonomicron, it's definitely a huge help in picking up rust.
Treating unsafe code as "all-tainting" is a pretty practical approximation, and it has served rust well so far. You seem to imply it is the only or the best solution, for example that the approximation is actually exact. Are you making that claim? because this would be a strong claim requiring strong evidence. Maybe we can keep an open mind about potential improvements in this area. I think efforts to understand, explain and control unsafe rust are worthwhile, even if they are not The Ultimate.
To make it absolutely clear, the behavior exhibited in the OP is indeed a bug: see https://github.com/rust-lang/rust/issues/16012#issuecomment-165589146 for the latest discussion. TL;DR: this currently isn't exploitable on Windows, and patches to LLVM will ideally solve this for non-Windows platforms as well.
Seems to me that unsafe fields fit this pattern exactly. The programmer declares modifying this field is tricky, because it participates in non-trivial invariants that the type system does not enforce and which affect safety. Hence Rust cannot check whether code modifying such fields is maintaining the invariant. Hence `unsafe {... a = 3 ...}` -- Yes I am modifying this field carefully. Compiler: trust me. Maintainer: don't trust me, check me!
That is not the proposal at all. Please read issue 381.
I don't think your reply was very kind. It was a little constructive (feedback mostly on how he wrote and how he interacts with your community in your opinion) but IMO, pretty much off topic: There is a concrete, serious proposal at hand, which is not ignoring the existance of bugs, hardware, etc. It helps deal with some categories of bugs, while treating hardware as an abstraction (yes the hardware abstraction leaks, but this proposal is not about that). This proposal is about allowing programmers to document invariants. If we add it, clippy lints might be written to e.g. detect unused unsafe annotations, but if you think Clippy can by itself detect all such safety relevant invariants I might depend on, its up to you to prove it.
We have a sophisticated quadruple-stochastic bankwise neural net that scans all comments posted to the subreddit and automatically assigns flairs based on a Bayesian tort subanalysis. Check back in a few to see if it's picked up on you!
Isn't `Box&lt;Iterator&gt;` more or less what C++ does?
ok thanks
I'm working on a clone of [agar.io](http://agar.io) with a webGL frontend and a Rust backend. Haven't done anything serious with Javascript or webGL before so this is fun and frustrating.
&gt; If you are talking about padding put in place to align the allocations, then allocating them together saves nothing. Why? It seems to me like, even with a no-bookkeeping allocator (which itself sounds challenging, since at minimum it needs to track which allocations are used), the result is *no more* space being used. Consider allocation sizes 160 and 80 bytes long. Padding each out would take 256 + 128 bytes whereas allocating both together takes only 256 bytes. Consider also the case with 3 or 4 variables, rather than just one.
&gt; [a no-bookkeeping allocator] sounds challenging, since at minimum it needs to track which allocations are used No, it only has to track which allocations are free (using the free memory itself to create the data structures). In-use allocations don't need any bookkeeping if the allocator allows a size to be passed to free(). &gt; Consider allocation sizes 160 and 80 bytes long. Padding each out would take 256 + 128 bytes whereas allocating both together takes only 256 bytes. Why is your allocator padding all the way out to multiples of 128 bytes? The padding should be no worse than the max alignment. Consider a max alignment of 8 bytes. In your example, they both get 0 padding. Consider 156 bytes and 76 bytes. Together you need (156 + 4 padding to align the second vector + 76 bytes + 4 padding to align the next free block) == 240 bytes, and separate you need (156 + 4 padding to align the next free block + 76 + 4 padding to align the next free block == 240 bytes). Same space, but if you do it together, you lose the ability to shove that 80 byte allocation into some free space which may exist in a hole on a free list somewhere.
The ndarray library's arrays that are used here are natively strided (a stride per axis), so it is actually part of the code all the way, and enables the efficient splitting used (`split_at` method).
What are your plans for the integration with Spotify?
Can someone explain this: http://is.gd/Q6wDEh I wanted to see how infinite recursion works in release mode, but instead of an infinite loop it gives constants.
&gt; The programmer declares modifying this field is tricky The problem is that tricky fields don't commonly fall into a single read-or-checked-modify pattern that the compiler would make available. `Vec` is the example that's been floating through this thread. It's relatively simple - exactly three fields - but all of them are interconnected. Something simple as `push` can and does modify all of them. It's always unsafe (risks data corruption) to go behind a public interface.
What's wrong with the old name? The new name is clearly more self descriptive, but what were the actual objections with the old name? I sincerely hope it's not the same sentiment that would complain about parts of a compiler where you have files named anal.c and similar.
I was thinking of something along the lines of DeepDiver or just diver (sounds cool in combination with racer), or divR ;)
It will let you link your account and integrate your Spotify collection (favorites?) into your combined cloudfm collection. Since libspotify is discontinued, I'm waiting for the new library that they promised for this year. Long-term, all backends will also support download for offline listening. This is of course not something Spotify gives third-party apps access to, so if you mark a Spotify album for download, we could instead try to look up the tracks on YouTube or Jamendo, and download from there. I hope that answered your question.
The old name apparently triggered negative emotions in some people (and no, I'm not going to discuss possible reasons). I don't want that, so upon hearing about it, I changed the name. No need to make a big deal out of it.
You probably want `Vec&lt;PathBuf&gt;`, not `Vec&lt;Path&gt;`. A `Path` corresponds roughly to a `str`, so basically it's slice of bytes (owned elsewhere!) that happens to describe a path.
I already renamed it. It's now metacollect.
Totally agree with this. 
You keep talking about specific allocators though. If you really care about the finer details of your application's allocation patterns, you can replace the allocator in Rust just as you can in C or C++. So I guess I lost your point.
The intended usage is through the gonum matrix package. Here is the high-level access point https://godoc.org/github.com/gonum/matrix/mat64#Dense.Mul The actual code is in our BLAS implementation, which was built for consistency with existing BLAS packages. Here is the signature https://godoc.org/github.com/gonum/blas/native#Implementation.Dgemm And the code is https://github.com/gonum/blas/blob/master/native/dgemm.go#L20 . You can see the serial implementation at the current line 283. Instead of using the recursive split, it currently distributes blocks via channels in dgemmParallel on line 101. At the time the channels were faster. I suspect there'll be a difference in performance with Rust in that Go uses a FIFO-like queue while it seems yours is more like a stack. Stacks should help in this case I imagine. Our code could certainly use another cleaning pass. The Go compiler has gotten significantly better since I last tried to optimize the code, and so lines that are there now for performance reasons can likely be removed or replaced. I'm also a lot better at programing low-level matrix operations than I was then. We've just been busy adding to the gonum functionality in other ways.
`peekable()` is perfectly fine if you know how to use it. I use it in places like this in my [media-rename](https://github.com/mmstick/media-rename/blob/master/src/main.rs#L144:L186) application.
It would appear so, yes, for slices at least. Array indexing is done by the compiler, I guess that's what I was thinking of.
&gt; The problem is that tricky fields don't commonly fall into a single read-or-checked-modify pattern that the compiler would make available. I don't understand this sentence. The proposal does not assume that tricky fields fall into a particular pattern, only that breaking safety relevant invariants requires writing to the tricky fields. &gt; Vec is the example that's been floating through this thread. It's relatively simple - exactly three fields - but all of them are interconnected. Something simple as push can and does modify all of them. I'm assuming we are talking about Vec as defined in [1]? I think its an excellent example. Under current rust, taking `self.len += 1` out of the unsafe compiles. But assuming that `get` tests boundaries only w.r.t. `len` (not `cap`), if we now also modify that line to `self.len += 2` (as below), we have broken an invariant. impl&lt;T&gt; Vec&lt;T&gt; { pub fn push(&amp;mut self, elem: T) { if self.len == self.cap { // not important for this example self.reallocate(); } unsafe { ptr::write(self.ptr.offset(self.len as isize), elem); } self.len += 2; } } Under current rust: "of course you can break an invariant, you are in the module!". Under the proposal, all of `Vec`'s fields are unsafe (because as you say they interconnect) so you cannot modify them outside of unsafe sections, so it is enough to inspect: - functions with unsafe sections need to ensure the pre-conditions assumed by the unsafe sections (this would catch the `&lt;` vs `&lt;=` bug described in [1]). - unsafe sections need to be safe and also correct. - functions called from the unsafe sections need to be correct (but their safety is already given if they are not unsafe). Under the proposal, you would write a few functions (a trusted kernel) that modify the fields carefully to preserve the invariants, and then write the rest of the module with no unsafe sections, just in terms calling those trusted kernel functions. So the privacy boundary is the module, but the (safety) trusted code base is (possibly) much smaller. [1] https://doc.rust-lang.org/nightly/nomicon/working-with-unsafe.html
That's why I had a question mark next to your name, wasn't sure just how relevant you thought `crossbeam` is to this particular discussion.
Rusts ability to effectively abstract low level concerns continues to surprise me :) thanks for that. There is a syntactic cost, `self.a = 3` is nicer than `*self.a.get_mut() = 3` etc., but it might still be good enough to try the idea out. Added this to issue 381. Edit: Fixed a typo or two, put a demo on playpen at http://is.gd/PmL2RI Edit 2: now with the obviously necessary privacy http://is.gd/9kGSvF
&gt; You seem to be strongly assuming that no other approximation is needed beyond privacy, and I do not understand why. I think if the module is small enough he's right. I don't know what sort of work Gankro does in rust, but I bet he would tell you if the module has gotten large enough that this is an actual issue, you have a design problem. Which may be true, or it may indicate a different area of development (or a little of both, I don't know).
Nice! I've been wondering if something like this would be a good way to develop native apps, by just using Servo plus a widget toolkit using a DOM and CSS for layout. I wonder if the best way to deal with the first point would be to integrate upstream; just have it be configurable when building Servo whether you have the script component of servo, or the IPC layer.
Why have you decided to run Servo in separate process? Is it not possible to run it in the same process as application itself, i.e. use it as library?
The protocol for the Steam controller isn't documented? That's a bit weird, seeing as how the hardware is pretty open.
I read [recently](https://www.reddit.com/r/rust/comments/4g9kis/slug/d2fsfat) that the vtable pointer is stored next to the pointer instead of on the object. This means that two pointers to the same object can have different vtable pointers. Is this used in Rust to do things that C++ virtual methods aren't capable of or would that be an erroneous state?
I looked around and there are other projects attempting to reverse the controller. I couldn't find any data sheets, source code or otherwise from Valve. Keep in mind the controller is useless without Steam. Without an user land driver it behaves like a mouse and keyboard and not like a normal controller. But maybe none of us looked hard enough, regardless my implementation is complete, the only missing stuff that Steam can do and I can't is updating the firmware and knowing the battery status, fine enough by me. 
Posted twice btw. But great project, a native sublime-style editor would be amazing! 
[#18785](https://github.com/rust-lang/rust/issues/18785)
The work by LorenVS (mentioned in my other comment in this thread) is now published: https://www.reddit.com/r/rust/comments/4gl8kx/servovdom/
Sorry for the confusion. I will clear this up in a future video.
This was posted by two users within a short span, and ordinarily I'd keep this one due to it being posted first, but the other one was posted by the author of the project, so I'm going to close this one in favor of that one in order to make it easier for them to answer questions: https://www.reddit.com/r/rust/comments/4glski/xi_editor_a_modern_editor_with_a_backend_written/
See also the author's own post: https://www.reddit.com/r/rust/comments/4glski/xi_editor_a_modern_editor_with_a_backend_written/ (was posted ~10 minutes after this post, oops :p)
From your statements above, I am not at all sure whether you understand the argument made there, about shrinking the amount of code that needs to be correct merely to ensure safety. In this sense, it is _only_ supposed to help, not "solve" anything. I am not at all sure how the subtlety of the invariants is related. Can you give a simple example of when there is a safety bug despite verification as I mentioned in [1] [1] https://www.reddit.com/r/rust/comments/4gdi4a/curious_about_state_in_safeunsafe_code/d2ig978 As far as solutions to safety despite unsafe code, I think SAW (linked in this thread) is the most short term promising tool for making guarantees about safe+unsafe code, but even that is not ready and arguably still research. Would you call retrofitting dependent types onto rust anything but research? Unsafe to write fields are so mundane a technology that as /u/diwic has shown below, they can be expressed as a library in current Rust (with a little syntactic cost). I think it is clear that there is no point in changing the language until and unless this becomes so popular that the syntactic cost is considered worth shaving off.
The next protocol I would add is http+js.
It's not personally a priority for me, but I encourage experimentation. I have given some thought to running the core as a web worker through emscripten - I believe it could lead to good results.
/r/playrust
You are hinting at a possibility of minimizing the safety TCB by splitting a module into a "safe" outer module and an inner "unsafe" part, so that privacy safeguards all the invariant carrying fields. I don't see a way to allow all of the following together: - the unsafe code in the inner module has access to both subtle and regular fields. - the safe code has access only to all regular fields - code outside the module can be denied access to private fields (whether subtle or regular). But maybe there is a design pattern that solves this without unsafe fields. Given unsafe fields (whether as a language feature or as a library), I at least, would always use them for any appropriate field, simply because: - smaller TCB is better, and - letting the compiler catch me in unthinking error is good - it is documenting something important Whether the TCB is *much* smaller or just a little... why do I care again? But even if unsafe fields carry some per-use cost (say, syntactic in the library) some modules probably will have large portions that can be safety irrelevant, and we can use unsafe fields only then. Anyway, now agreement is not necessary. unsafe fields are now a library, people can vote with their dependencies.
thanks for calling me out on that i feel really dumb now xP
I've just gone and randomly replugged some of the neural ethernet cables and blown vigorously on the cartridge. Lo and behold, your flair has appeared! The future is an amazing place to live!!
&gt; Those would be pointers to different traits of the same object. I'm not too familiar with Rust, would you be able to give example code that exhibits this scenario?
It's not pure because `println!` is a side-effect. If that is removed, the `for` loop boils down to finding and returning the first error. fn read_log() -&gt; Result&lt;(), io::Error&gt; { File::open(LOG_FILE) .map(|file| io::BufReader::new(file)) .map(|file_reader| file_reader.lines()) .and_then(|lines| { match lines.find(|line| line.is_err()) { None =&gt; Ok(()), Some(e) =&gt; Err(e), } } } If you want the side effect of printing, I'd prefer the `for` loop. 
Maybe write your thoughts in an open issue? Could be a great project for someone.
&gt; The end function means that a calculation is completed and no more value will be produced. The emit function should not be called anymore. An iterator with reversed control? Interesting pattern. I can see explicit `end` being good for readability, even if it does go against the grain of Rust. You can't destroy something that you don't own (makes sense) which is why you can't destroy something through a plain trait pointer (like `&amp;Consumer`) but can destroy a box. On the other hand, if you own something you destroy it simply by letting go. Bottom line: I think that not implementing Drop for every kind of `Consumer` is a mistake. Otherwise it's possible to leave a dangling `Consumer`, one that has seen its last `.emit` but not `.end`. You could default to an automatic end, or panic, depending on how you want your code to be used. Otherwise a dangling `Consumer` is an undetected bug.
I only wanted specific details when you made specific claims about rust not being able to allocate things as efficiently as C. But since you can swap out allocators in either, your point was moot. How you got from there to assuming specific allocator properties is unknown, but since your drifted away from the topic, it isn't worth chasing down. Allocating multiple things together is not always a blanket win. I already pointed out one reason why. Here is a second. By combining sizes, you may end up forcing an allocator to use a larger allocation policy, like allocating a multiple of the page size. If you kept the blocks separate, they may have been satisfied by blocks on exact sizes free lists, or split from larger blocks. Memory allocation is a trade off. If you need different properties, use a different allocator. But generalities aren't useful.
Do you intend to emulate the keychord paradigm used by Emacs and just about everyone else, or the command-grammar model of Vim? The name sort of implies the latter, but being you're at Google which I read as more likely to try and pull a Github, anything's possible. â˜º
One of the limitations I've found with some editors is opening large files (over 250 MB). Do you think it's a worthwhile goal to have good performance with large files?
Yes. One of the test files I use in daily development is &gt; 300M. It takes about a second to open and 2s to word-wrap, as opposed to 30s for Sublime Text to open it. Further, RAM use is a small epsilon over the file size, which I think is one advantage of a non-GC'ed runtime.
My long time wish is for having a markdown(+katex) editor which shows the full page of my text in rendered (html) view, but with the ability to move cursor around. And at the bottom of the window it has one line (editbox) which shows the source markdown code of the actual line where the cursor is in the rendered text. If I move up or down with the cursor, it moves in the rendered text, but the source line changes respectively in the editbox. If I move the cursor sideways, then the cursor moves sideways in the source with each step, but sometimes jumps a step also in the rendered view reflecting the source position. So, this way finally we can have nice full screen rendered text, with only one line occupied with ugly looking markdown source editing. I think it would look much nicer than the usual half split screen editors.
Small epsilon? I had [to search a bit](http://mathworld.wolfram.com/Epsilon.html) for that one. I'm not quite sure what you mean by that, though.
As it says there; an infinitesimal quantity. If you're opening a 300 MB text file, then the whole editor (file included) would only take a tiny bit more than 300 MB.
Ah, thanks!
&gt; Asynchronous operations. The editor should never, ever block and prevent the user from getting their work done. For example, autosave will spawn a thread with a snapshot of the current editor buffer (the peristent rope data structure is copy-on-write so this operation is nearly free), which can then proceed to write out to disk at its leisure, while the buffer is still fully editable. That's my pet peeve with Emacs. It runs everything in the same thread as the UI, and being Emacs, it often has a lot of stuff to do.
Shouldn't `as_mut` return `&amp;mut T`? You're using it like [`into_raw`](https://doc.rust-lang.org/std/boxed/struct.Box.html#method.into_raw).. (and it works!)
I can't agree more with you! Now I will sacrifice some bugs in an old ceremonial fashion to express my eternal gratefulness to the rust deities (and to appease the borrow checker).
this is kind of weird, a couple of days ago I began a nearly identical project with basically identical goals. server/client, rust back end, async jobs, high performance above all, etc. race you there :) thoughts about the other offerings right now, namely iota and rim?
I've been told by at least a couple googlers that 20% time just means you work a 50+hour week.
This is great - I will certainly find this useful. Thanks!
Oh, I see. Thanks!
&gt; I'm also contractually barred from improving these docs for the forseeable future Why is that? (can you disclose something?)
&gt; We will begin the evening with a short recap of what has happened on the way from 1.0 to 1.8. Exciting stuff! Hm. So far I haven't done anything in rust, but I worked through the rust book and have a background in C, C++, Haskell and several dynamically typed languages. Do you think I can follow the talk(s), or should I skip this one?
&gt; Native UI. Cross-platform UI toolkits never look and feel quite right. The best technology for building a UI is the native framework of the platform. On Mac, thatâ€™s Cocoa. Actually I think you can get away with not needing to write platform specific UI if you do something like SublimeText does, as in write the UI in OpenGL or maybe give Vulkan a try (not sure how "production ready" is yet but just an idea) Also I would love to see support for huge files (even if syntax highlight is disabled - at least let me scroll/search trough the file)
With some noteworthy differences with iron, nickel, and pencil.
&gt; Unsafe to write fields are so mundane a technology that as /u/diwic This is neat, if it gets popular it will give a lot of examples of how unsafe fields are used in practice without having to change the language.
nice work, are you in China? Welcome to participate our community. Rust-China http://wiki.rust-china.org
Historically, mixing mmap and normal IO has been a problem when updating a file. Bugs ahoy. Git hit a few problems with it on non-Linux unixes, I think. To be as "cross-platform" as possible, you would need to support the few extant OS(s) which lack unified page and buffer cache. I'm not completely sure how you establish that if you're modifiying existing pages :(. SQLite apparently doesn't support using mmap() on such OS(s), and doesn't enable it by default for this reason. "bup" hit problems with this on Linux (which has a unified buffer cache), on certain ARM hardware. It only appeared after running "bup" on a fair amount of data, so their tests didn't pick it up. They worked around it somehow, because "my backup program claims to work, but real-world restores fail" is a bad look. As would be having to prominently document "tested on the following OS+CPUs only; some other combinations are known to cause the above." I reported the problem, but I was not able to establish the root cause. I dislike having code which is so vague about why it is necessary. I believe avoiding mixed mmap() and normal file IO would have been a more natural fix. `msync()` comes up; I think it was what was used to "fix" bup. But that's the wrong way round for what you want. And yet `msync() without MS_SYNC` (flush to disk) on Linux should be a no-op; that's what the manpage says and the code doesn't appear to contradict it. My suspicion is some issue relating to cpu caches, which was not uncovered due to the less used platform. So IMO, if you don't enjoy uncovering such issues I would suggest avoiding it.
Do you plan to make a full SublimeText clone? With mouse navigation, multiple cursors, search and all that? How ambitious are you?
I'm not the author, I believe you should ask /u/qinwf.
Yes indeed, don't know how this happenend, was answering to a different post. Thanks :D
really would like to see it go somewhere
Thanks a lot for sharing.
Thanks. Some people (Dunno why. I suppose maybe because SSD storage is expensive, and bandwidth is cheaper.) would store their entire library collection on a bucket on S3 (e.g. Media Bucket/Music/), so having this work in an existing bucket (Rather than a dedicated CloudFM music bucket.), would be fantastic. 
I believe in your case specifically, since your views will be to immutable regions, and the only writes will be appends that the views cannot observe, it should be safe to map views of the file as you append for your case. Coherency only really matters when one side could observe changes made by the other side, or when both sides are writing to the same file.
I want to cater to both "average joe" and power users (that actually know what an S3 bucket is). The manager will have a simple option that will create a bucket behind the scenes and upload files to it, but you will also be able to add a bucket manually if you prefer managing it yourself.
The Github page mentions the editor uses a rope internally. I presume there would be a lot of complexity paging in and out of that data structure?
Mmaping a file that is potentially being modified by other processes is a bad idea. There is no way for the operating system to signal safely that the data you are looking for no longer exists and your program crashes. It's bad.
HS256 *is* HMAC, with SHA256 as the hashing function!
Eventually, yes, but unless someone else contributes it, it's unlikely to happen pre-1.0. I'd rather work on implementing discovery inside cloudfm.
*shrug* standard tech job shiz
I see, thanks. I wasn't really hinting at any sort of specific architecture, just pointing out that may simply have a tendency to either design things or work on things that don't exhibit the problems being solved by unsafe fields, and therefore he doesn't see the need for them. Or perhaps we're off our rockers, it's always possible we're the ones who are wrong :)
&gt; frequently it is Citation needed. I've worked on a large number of low level code bases and rarely is it a benefit to go out of the way to allocate two things like vectors using a single call to malloc. In addition to all the reasons I already listed, it just complicates the code for gains that haven't been justified yet. &gt; I mentioned specific allocator properties because you asked how it might reduce space overhead. I asked how, in general, allocating two vectors using malloc (your example) would improve things. You gave me examples using specific allocators (jemalloc). I'm not sure why. &gt; And no, using another allocator isn't remotely equivalent Your argument was that you can save space by allocating two vectors using malloc (which I don't really follow anyway - do you mean C++ vectors using malloc? Or C arrays using malloc? Or C++ vectors using new? or std::allocator?), which is hard to do in Rust (it isn't, you can use unsafe blocks to allocate the memory, or replace the allocator). For your assertion to be correct, first, you have to either select a specific allocator (which means your argument is tailored to a specific allocator, which means replacing the Rust allocator with something just-as-tailored is the same exact argument, and completely relevant), or you have to demonstrate that your claims of saving space apply in general to allocators (which you haven't been able to do). Please, I'm curious, what your point is.
Yeah but if you get paid for working on your project it's still true to the original idea, isn't it?
The README of the project talk about a persistent rope structure..
I'm thinking about this in the context of writing deltas to a log file. This could be done at the rope level, but I have the `Delta` abstraction (which I use for incremental computation and soon-to-be undo) so I'm thinking of that instead. The only loss is copy-pasting large blocks. I'm not too concerned about how long it takes to write a file though, as I'm planning for the save operation to be fully async - it writes a snapshot (easy due to the immutable ropes) and you can keep editing.
In this context, "persistent" is just functional programming lingo for "immutable." I'm going to try to stick to the latter as it's less confusing, outside the FP community "persistent" means "on-disk."
I also had the same problem. Everyone suggested to use hyper instead of frameworks because of it's fast. Also what about the status of mio on production?
That's how I initially interpreted it actually. I always assumed these guys worked crazy amounts of time.
Is the name a play on vi and/or Haskell's yi? EDIT: Part of me now thinks it would be cool to have a subreddit for text editor authors to talk about their work.
http://www.arewewebyet.org/
As a mac user, Cocoa is fine by me :) I wonder if structuring the project directory so that it's easier to add more front-ends might give a good impression?
As a sublime user, multiple cursors + âŒ˜D are super handy for bulk, interactive editing. Kind of hard to describe in text though.
This is the second part of a short series on implementing multithreaded matrix multiplication in Rust. The first post is [here](https://www.reddit.com/r/rust/comments/4f5gg6/multithreaded_matrix_multiplication/). There was a great [post](https://www.reddit.com/r/rust/comments/4ggcca/optimizing_rust_matrix_multiplication_pt_1/) from /u/pegasos1 on the same topic. Their post has a lot more information and is really a fantastic write up - I highly recommend you check it out. That said, this post briefly details how I removed some overhead from my implementation.
`Path` is to [`PathBuf`](https://doc.rust-lang.org/nightly/std/path/struct.PathBuf.html) what `&amp;str` is to `String`. Thus what you want is pathbuf = PathBuf::from(new_path.trim()); _EDIT:_ Better to use `from`.
Next bug report: Editor crashes when watching a youtube video or compiling in the background. 
[video](https://air.mozilla.org/bay-area-rust-meetup-april-2016/)
&gt; Since libspotify is discontinued, I'm waiting for the new library that they promised for this year [librespot](https://github.com/plietar/librespot) is an open source client library for Spotify. I don't know if it is ready to be used in something like cloudfm, but at least you don't depend on a deprecated library.
The [vis editor](https://github.com/martanne/vis) does this. The readme also has some interesting thoughts on editor data structures.
Oh, was this published on the 19th already? (And now I'm off to actually reading the post.)
man, I didn't ask what framework to choose.
No, that's when the draft started, it was published just now.
Yeah, that was the reason I asked this in the first place! I think Xi also uses the same idea, and made that linked-list a tree. (TBH, I have been thinking about doing this exact same thing -- CoW, thread-safe, tree version of "piece chain" -- but they beat me to it!)
This is some top-notch technical writing. Makes a complex subject seem simple and obvious without dumbing it down to the point of uselessness. Kudos.
Huge relief that this is possible. I had done a ton of work that was all structured to need this. I've looked at the code and it seems to do exactly what I need. Thank you very much. 
This is an excellent post, I look forward to the launch and super fast compiles, and more.
All of this sounds absolutely fantastic. The name is a little confusing at first, but it's well chosen as far as its scope to Rust is concerned. I could make a namespace joke. 
Crashing the whole program does seem extreme, but on the other hand, I'd love to write plugins for an editor using a debug build which enforced speed and crashed those plugins if they took too long, especially if a sane standard practice were outlined for me. Here's two examples (Skip to second half of my post if this looks like a wall of text): * Replace text plugin: (i.e. Typing -&gt; is replaced by â†’) When considering the idea of writing this plugin myself, I'd think first: "How can I make it easy for others to export and import their replacement rules?" and second, "Can I benchmark Xi with and without my plugin, and with a table larger than anyone will ever have?". The first is difficult with most plugin-editors, the second is impossible AFAIK. If my replacement plugin causes *any* measurable slowdown, I probably don't want to publish it, because it has to do a look up for every keystroke, not just those that are replacements. I don't think it can be done quickly enough with regular expressions, unless Xi provides it's own regular expression engine designed for this exact task, in which case the developers (you) likely made the plugin already. It's really insane that implementing this plugin for, say, Sublime, looks pretty much the same as a spell-check-auto-replacement plugin (Right-click mispelled words and click "Always replace *this* with *that*), when the performant version of each would be entirely different. * External tag table plugin: (i.e. a separate window which can track down a specific tag, like "TODO{...}" to display what is in the brackets, and is clickable to navigate to the location in the main editor. This one is very useful and extensible for a lot of purposes, but uniquely does not need performance in the secondary window. On the other hand, it shouldn't cause any measurable performance hit in the primary window, but if it takes three seconds to reach the secondary window after the closing bracket is typed, no one cares (to use an extreme example). * * * * * * * * * * * * * * * * * * * * * * * I don't really have a specific suggestion about how to make this all possible, but maybe bundling the debug source with a combination of property testing examples and benchmarks using the same would make it possible. That and maybe some convenience Lorem Ipsem generators, and local rules. Using my TODO example above, such a Generator could show me that a quadruple-escaped closing bracket such as: &gt; TODO{Find out how to use closing brackets in JSON literals within JSON, if writing in a language which uses double slash for comments, something like \\\\\;;};;?} suddenly causes my plugin to hang, because I've rolled my own escape sequence, rather than simply realizing that it already would work out of the box. What a fool I am! Bear in mind, however, that I've been learing Rust for two weeks now, and haven't even dived in to Rust's version of Quickcheck ( I jsut know it exists). But it always seemed strange to me that you can't just use a label on properties to say "Log the duration of this function call if it passes and be done". I think a good environment and plugin developer community could involve such a process, however: * Make something that works, Unit-Test it. * Make some property checks * Release an Alpha version on github * Figure out which property checks should be benchmarked and how, probably with help from the community. Likely add more property+unit tests as well. * Release final version when those pass. 
Nice article! I didn't understand every part of it, but the high level aspect of it makes complete sense.
Thanks, that is pretty interesting! But it doesn't seem to support access to the raw audio files, which we would need for web playback (that or a JS library that does all the DRM magic behind the scenes, like the one that Rdio used to provide).
I am looking forward to constant evaluation, as it is the key enabler for non-type generic parameters.
I see, rust-openssl doesn't seem to be complete in this area. But it looks like ring (rust bindings to boringssl) does have something that may work. Here is an [example for RSA signature verification w/ SHA256](https://briansmith.org/rustdoc/ring/signature/fn.verify.html) 
How do you get vertical editing without that craziness? ctrl+alt+up/down doesn't do what I would like for it to do. I'd like something more akin to shift+alt+up/down in most other editors (visual studio, for example). 
I was surprisingly riveted by this presentation. I don't know whether I'm more excited about this or rumpkernels, but at some point we're going to have a solid core Rust kernel that runs nothing but std, on which users can easily build their own, secure, single-purpose operating systems and unikernels.
I don't think you can do this without higher kinded types.
Can we take a moment to realize and appreciate what a blessing Niko is to the Rust community? He's been consistently producing easy-to-grasp material about not-so-straightforward concepts (e.g. his talk on borrowing is what really got me to understand that important Rust concept) and this post was **great**: very clear on why MIR exists and what it looks like. As a compiler person, I found everything that I wanted to know and more!
I'll join the chorus on praising the writing. Really good article indeed. Out of curiosity, was there a particular reason for not having a MIR since the beginning?
Crashing for performance in a debug build would be silly, IMO; debug builds are always going to be much slower. A perf test suite for release builds makes complete sense to me. Something in debug builds that warns "this might be slow in release" could be handy. I don't think I'd object if there were some light profiling at runtime in release builds that could non-intrusively warn about slow plugins and prompt to disable them, or even prompt to report a bug if the main editor is too slow.
And I could make a space name joke. :P
I couldn't have imagined receiving much better news than this today.
This is closely related to one of my biggest issues with Rust at the moment. When writing library code (or using other's libraries) it should be easy to make code generic on either a reference, an `Rc`, a `Box`, an `Arc`, or whatever I want based on my particular use case. In particular, libraries that require internal references cause all sorts of hell for users who want to have access to data from multiple threads. Instead, doing this requires (to the best of my knowledge) excluding references entirely and having painful generic type parameters such as `&lt;Thing1Ref: Deref&lt;Target = Thing1&gt; + Clone, Thing2Ref: Deref&lt;Target = Thing2&gt; + Clone, Thing3Ref: Deref&lt;Target = Thing3&gt; + Clone&gt;...` Hopefully now that [MIR has been making serious progress](http://blog.rust-lang.org/2016/04/19/MIR.html) we can get higher-kinded types soon. 
Note in particular that speakers' travel expenses will be covered!
Could you provide link to his talk? Thanks!
Watch closely, these jokes pass quickly overhead...
This will likely prevent it from being able to work on Wayland, where two distinct processes are totally isolated from each other and youâ€™d need the conceptually embedded one to send its buffers to the main one for it to be composited. Making it a library exposing a C ABI would be the easiest way to allow clients written in any language to use it, while keeping performances and fast turnaround.
I don't understand how mailing lists work. How do I see the contents of the patch?
I'm/many people are completely uninterested in the discovery element of Last.FM. Most useless feature that they've made so far. We just want the scrobbling aspectâ€“saving our play history to Last.FMâ€“to be there.
This link is to "[PATCH 0/8]", which just serves as an introduction. Look at the follow-ups at the bottom for the 8 individual patches.
This was considered, but dropping objects "early" (before their lexical scope ends) breaks RAII patterns: https://github.com/rust-lang/rfcs/pull/239
You may want to change the content type to `application/pdf` instead of `application/octet-stream`.
[this branch](https://github.com/tromey/gdb/compare/ab4896bce48b1c8d1bd22f750862dc4197ba02fc...rust) contains the changes in a finer-grained set of commits, if you want.
&gt;I'm really interested in the LOC measures here. Any ideas why there's 1/10th of the Rust? C++ code includes a parser while in rust version parser is in libpnet. Also first prototype was a quick and dirty proof-of-concept type of thing, so I had the benefit of hindsight. &gt;I'm also interested in the speed, any idea why it's slow? I covered it in the end, my current theory is that it's slow because I'm not using netmap-aware driver (there's a lot of sys load).
&gt; curl -s -D - -o /dev/null http://plhk.ru/trash/netmap-20160427.pdf | grep Content-Type I got `Content-Type: application/pdf`, it probably has something to do with cloudflare caching static assets. 
What I sent is mildly different from that branch. It needed a few last changes to be upstreamable. However it's not radically different.
Thanks, I was wondering the same thing while reading the article.
I actually don't think it's *that* bad. It is a bit long, but it's also pretty clear what's happening, which is good. Here are a few thoughts: * I see you're using `regex!`. Is that a macro you defined around `Regex::new`? If so, OK. If you're using `regex_macros`, then I'd advise against it and go with `Regex::new`. * If you're calling `parse_input` a lot, then you should wrap your `Regex::new` calls in a `lazy_static!` so that they only compile once. * It looks like all instances of `buf.as_str()` can be replaced by `&amp;buf`, which will deref a `String` to a `&amp;str`. * Some of your regexes use `\d`, which is Unicode aware. This means that a match of `(\d+)` may not necessarily correspond to a successful call to `the_match.parse::&lt;usize&gt;()`, which means your code can panic as written. You'll have better luck with `([0-9]+)`, but even then, this can match an arbitrarily large number, which can also cause calls to `parse` to fail. Alternatively, if know how big the number can be in terms of its digits, then something like `([0-9]{1,5})` should work and (I think) always result in a successful call to `parse::&lt;usize&gt;()`. * This feels like an ideal use case for a `RegexSet`, but I have to figure out how to [fix this bug](https://github.com/rust-lang-nursery/regex/issues/186) first. Gah. Finally, any call to `x.captures(...)` that is guarded by a call to `x.is_match` should elide the guard. I think I see 7 redundant calls to `is_match` in your code. So for example, this code: if cluster.is_match(buf.as_str()) { match cluster.captures(buf.as_str()) { Some(cap) =&gt; { return Ok(Input::Cluster( cap.at(1).unwrap().parse::&lt;usize&gt;().unwrap())) } None =&gt; { return Ok(Input::None) } } } Could be rewritten as: if let Some(cap) = cluster.captures(&amp;buf) { return Ok(Input::Cluster(cap.at(1).unwrap().parse::&lt;usize&gt;().unwrap())) } In particular, if `is_match(...)` returns true, then `captures(...)` should never return `None`, so all those early `Ok(Input::None)` returns should never actually happen in your current code. I note that the first `unwrap` is OK (since all of your captures are a required part of the overall match), but the second unwrap can fail, as mentioned above. It would be ideal to handle the second case with `try!`, which probably means adjusting your error type in the signature of `parse_input`. (If you're feeling lazy, try `Box&lt;Error + Send + Sync&gt;`. If you're feeling ambitious, [define your own error type](https://doc.rust-lang.org/book/error-handling.html#composing-custom-error-types).)
Augh I really need to think up a rust project!
Works for me! That's crazy good. I'll update the README to point to your project.
I looked into the concept of multi-threaded emacs once and came away surprised that it works in *single* threaded mode. Basically everything seems to be a global variable in emacs, From the buffer you're writing in to every variable set with a let binding its all global, mutable state. Don't get me wrong I love emacs and use it every (work) day but the internals are an unholy, organic mess which has growing since (it seems) the dawn of time (maybe the dawn of unix time ;)).
Is that because Mir helps set the stage for incremental compilation? Or is there more to it?
Yeah, it makes everything else easier to implement, generally.
Does the netmap driver use packet_mmap or are millions of packets per second getting copied around? ( https://www.kernel.org/doc/Documentation/networking/packet_mmap.txt ) Are Rust channels responsible for a lot of the overhead? (syscall/copy per packet?) Maybe a mmap'd ringbuffer would help? 
I'm sorry about that! I hadn't really considered people may switch their screen colors - it must have been pretty obnoxious when you came across it. Personally I find it difficult reading black-on-white with back-lit screens so I went with the darker theme. &gt; Otherwise: Nice Post :) Thanks!
I won't be around myself in that time-frame, but someone might be... Have a good time, anyway!
It's technically undefined, but I've heard skepticism that we could ever change it, due to people relying on the current ~~right -&gt; left~~ behavior. In my understanding, it's only undefined in C due to some old architectures (and either a recent C++ standard or C++17 defines it ~~right -&gt; left~~ now) EDIT: I do not know my right from my left, ugh.
&gt; It's solely a personal project, so I don't need to worry about non-nightly compilation. Is there another reason to avoid it? Yes. It's ***MUCH*** slower. Order**s** of magnitude slower. There's almost no reason to use it. I guess the docs don't make that especially clear. There's a sentence about it, but it's buried. &gt; Thank you. Any time! If you can think of ways that I can improve the regex docs, I would be very happy to listen.
It's not undefined in Rust, it *can't* be, borrowck &amp; friends require a strict ordering of expressions.
&gt; sophisticated rope data structure Color me skeptical. Did you consider gap buffering? 
Wouldn't it be better to use gfx-rs over glium?
So my question has to do with default type parameters. Given this code: http://is.gd/fwke4P Why is the compiler not able to infer the right type, even though I specified the default type parameter? What can I do to fix this, besides let wrapper: Wrapper&lt;Nested&gt; = Wrapper::new(); ?
Thanks for the explanation, that helps clear things up.
The video and audio are offset by 1.5 seconds.
Ahhh, I see. It's the last sentence! Yeah, I apparently interpreted "ideally faster" as "DEFINITELY FASTER." Thanks again.
Doesn't that just make it unspecified? If someone built their own rust compiler, would they be breaking a spec somewhere if they did left -&gt; right?
I'm making a website using iron with handlebars and it's fairly easy. I'm not doing to much server side stuff at the moment but in the end it will be using server side features such as an information retrieval system
Why are things like this not turned on by default?
I used to have copy of Emac's git repo. I went to remove it because it was so big. Turns out it's version control history goes back to the 18th April 1985.
No worries! Glad to have it cleared up.
Is there actually a Rust spec somewhere? I thought the Rust spec was basically "whatever rustc does".
Oh I agree for sure. It's the sentiment. In reality it should be a part of the test suite.
even though Iron isn't that mature, isn't it by default considerably fast compared to Java, Clojure, Scala, Ruby frameworks? I think it should be faster anyway already.
how about performance? specifically, compared to the web frameworks in Ruby, Python, Java.
There is a spec, but I don't know if it is up to date with the latest changes. I hope the language stabilizes enough for the existence of a specification to make sense. I would love to see gcc's rust compiler come back to life.
Oh man, multiple cursors are *the* feature that made me switch from vim to Sublime. I use them *all* the time. Multiple cursors coupled with vim's powerful text manipulation commands are extremely powerful. Importantly, each cursor has its *own* copy/paste buffer. I'll typically start a multi-editing action with Cmd+F (which accepts regexes), and then instead of hitting Return to find the first match, hitting Opt+Return to find _all_ matches and get a cursor for each. From there I'll use Cmd+Left Arrow or Opt+Right Arrow, to move to the beginning of the line, move right by word, etc. Often times I'll couple the movement commands with Shift in order to highlight as I go (e.g.: shift+opt+right arrow to highlight a word), then cut/copy each multi-highlighted word into its own buffer. Maybe Cmd+Left Arrow to move back to the beginning of the line and then paste, so that each individually copied word is pasted at the beginning. For particularly tricky movements I'll drop into vintage (vim compatibility) mode, and can then do `ci"` to replace all the strings I'm within, or maybe `f:` to jump each cursor to the next colon on their respective lines. You can also apply Sublime Cmd+Shift+P commands like upcase or plugins like Text Pastry to drop in ascending numbers into each cursor. I started using multi cursors as kind of a replacement for vim macros, but they're so much more powerful since they give you instant feedback and let you interactively edit a bunch of related but not identical stuff at once. In fact, I'll now actually copy/paste stuff into Sublime and fiddle with it with multi-cursors when previously I might have written a script to process it. For instance, if I'm logging `user_id=XYZ` on certain requests, and want to get out all the user_ids from a logfile into an array literal to explore in the terminal, I'll just multi cursor it: * Cmd+F (find) * "user_id=" (string that I'm finding) * Opt+Enter (select all matches as multi cursor) * Right arrow (position cursor to the right of the `=`) * Shift+Opt+Right arrow (highlight all the user_ids) * Cmd+X (cut all the user ids) * Cmd+A (highlight the buffer) * Delete (empty buffer) * Cmd+V (paste all the user_ids, one on each line) * Cmd+A (highlight the buffer - which is just user IDs now) * Cmd+Shift+L (get multi cursors at the end of each line) * "," (just type a comma, which will appear at the end of each line) * Cmd+J (join all the lines into one) * Cmd+Left arrow (all the multi cursors condense to one at the beginning) * "[" (Just insert an open bracket at the beginning) * Cmd+Right arrow (go to end) * "]" (Close bracket) And voila, my log file now looks like `[243, 3463, 123, ...]`. I can copy paste that into my console and load my relevant users `User.where(id: [...])`.
What do you mean? Is Cmd+click &amp; drag up/down what you're looking for?
Recent feature addition I would guess? I don't think it was *too* long ago that travis even got good lang:rust support. If it's new, or even if it isn't, I think they'd want it off by default to not unnecessarily store caches for projects which don't run very often.
I think [this SO post](https://stackoverflow.com/a/28552082) covers it pretty well. The first component to understanding this is the basic algorithm. For a type `T`, the post suggests thinking like this: 1. Look for an exact match for `T` 2. Look for a match for `&amp;T` or `&amp;mut T` 3. let `T = *T` (a deref coercion), and go back to (1) The next component is "universal function call syntax", UFCS. Basically, given trait `Foo` with method `foo`, then `a.foo()` is sugar for `Foo::foo(a)`. In this situation `a` is called the receiver of the method. The last component is that the _receiver_ of the method is matched against, _not_ the type for which the trait is implemented. That is, if the method is `Foo::foo(&amp;self)`, and there is an `impl Foo for T`, then it is `&amp;T` that will match the method `Foo::foo`and not `T`. Let's compare that to your examples. In your first example, `myint: i32`, so `T = i32`. When trying to find `myint.foo()`, according to the algorithm 1. Look for `MyTrait::foo(i32)`. No such method match, so 2. Look for `MyTrait::foo(&amp;i32)`. The method matches since there is an impl for `i32` with `&amp;self` as the receiver, making it accept `&amp;i32`, therefore `1` is returned. Now, `myintref: &amp;i32`, so `T = &amp;i32`. The algorithm now goes 1. Look for `MyTrait::foo(&amp;i32)`, which we just saw matches, so `1` is returned. Finally, `myintrefref: &amp;&amp;i32` and `T = &amp;&amp;i32` and the algorithm is 1. Look for `MyTrait::foo(&amp;&amp;i32)`, which matches because there there is an impl for `&amp;i32` with `&amp;self` as the receiver, which makes the method actually accept an `&amp;&amp;i32`, and thus `2` is returned. For `myint: i32` in your second example, `T = i32`, leading to 1. Look for `MyTrait::foo(i32)`. This matches because there is an impl for `i32` with `self` as the receiver, so `1` is returned. For `myintref: &amp;i32`, `T = &amp;i32`, leading to, 1. Look for `MyTrait::foo(&amp;i32)`, which matches the imple for `&amp;i32` with `self` as the parameter, and `2` is returned. For `myintrefref: &amp;&amp;i32`, `T = &amp;&amp;i32`, leading to, 1. Look for `MyTrait::foo(&amp;&amp;i32)`, which does not match anything. 1. Look for `MyTrait::foo(&amp;&amp;&amp;i32)`, which does not match anything. 1. Let `T = *T`, so `T = &amp;i32` 1. Look for `MyTrait::foo(&amp;i32)`, which matches the same way `myintref` matches, and `2` is returned.
I admit, I was imagining they would do some kind of deduplication of such things; I hadnâ€™t thought about what was actually happening as explained at https://docs.travis-ci.com/user/caching/#How-does-the-caching-work%3F. Caching the Cargo package downloads would be much better handled by â€œsomething like our APT caching proxyâ€.
I'll keep that in mind. He really makes hard to grasp topics very approachable. Wish he made a video series on rust/programming, like railscasts. 
Deref coercions don't work? You can treat `&amp;Box&lt;T&gt;` (or any type that implements `Deref&lt;Target=T&gt;) as `&amp;T`. You have to work *entirely* with references, but that is much more ergonomic. [Example](http://is.gd/nisR7Q)
Awesome, thanks! Getting spoiled can be a good thing. :-)
This works fine as long as you never have to move between threads or work in other types of situations where `Rc` or `Arc` are needed. Where references work, they're great! Edit: [Example](https://play.rust-lang.org/?gist=bf7ee375074df70d111aac899ec0ca82&amp;version=nightly&amp;backtrace=0)
What are your ideas for the general shape of the protocol? The idea was thrown around in the iota project once that a declarative query language could be a good protocol for manipulating text.
I've been playing with the "vis" recently, a vi-style editor which uses both `sam`-style structural regexes and multiple cursors - a combination which works surprisingly well. It's a bit like ".", or more like recording a macro, except that instead of thinking for a bit, trying something, undoing it and trying something else, you can experiment interactively. Reducing the cycle-time of bulk editing like that seems like a powerful advantage.
Have you looked at the [vis editor](https://github.com/martanne/vis). It's modelled after Vim, but supports multiple cursors instead of Vim's "visual" mode, and structural regexes instead of Vim's "ex" mode. It's still pretty new, but if you want to play with multiple cursors in a familiar environment, it's a great place to start.
[This](http://z-petal.com/subfixer) is a small web app I made. It is a tool for fixing/syncing movie subtitle files. It uses Hyper, and Multipart library for handling file uploads. Building It was a very nice experience, even though I had to ask around a bit. I haven't tried other web frameworks, but I am very happy with Hyper. I also used Horrorshow library for html templating. 
In regards to load testing, you might be interested in a little utility I did for hermes. It's a python script that queries domains extracted from the DMOZ catalog, on an arbitrary number of parallell threads. Was very helpful in helping me find bugs: http://c0la.s3.amazonaws.com/dns_stress_test.tar.gz
Yeah, actually. I've been eyeing multiple-cursors since I saw that one recently and [Kakoune](http://kakoune.org/) a few months ago, but I haven't tried them out yet. I would rather see multiple-cursors added to Neovim, though, since I use so many Vim plugins and it's hard to give anything up.
Is it already included, or is this just a prerequisite?
I can't really tell as there is no heavy processing You can check the WIP here: http://matchuper.kloumpt.net :) 
That's true. I need to improve this part.
I don't see the issue, and I'm pretty familiar with wayland, you just have the servo binary open the window and send stuff to it, i.e. communication only happens like this: Your Program &lt;=&gt; Servo VDom &lt;=&gt; Wayland Server
You put a few unnecessary `&amp;`s. Here's the working `impl`: impl MagicEntry { fn f_p(&amp;self) -&gt; &amp;NP { match self { &amp;P(ref x) =&gt; x, &amp;E(ref x) =&gt; x.f_p(), } } fn f_p_mut(&amp;mut self) -&gt; &amp;mut NP { match self { &amp;mut P(ref mut x) =&gt; x, &amp;mut E(ref mut x) =&gt; x.f_p_mut(), } } } That is, `ref mut x` means `x` already has type `&amp;mut T`.
Prerequisite. AFAIK, there hasn't even been an RFC to flesh out the details
That makes sense. Yet `Box&lt;[u8]&gt;` really _is_ smaller than `Vec&lt;u8&gt;`: http://is.gd/8gyggl
mioco and coio will work. My problem with them is that they don't make it easy to integrate with external loop events or there are minor lack of capabilities that make them not-ready for a foundation that could be considered into the core library because they can't cover all the cases.
As I said, &gt; Maybe you'd like to demonstrate your allocation scheme so I can see how you're dealing with the issues I've raised. since you're not actually answering my comments directly. &gt; Why bring it up? Because it's a good example of a low level thing that is both done with some frequency and not nice to do in Rust. Given the question is "how low level is [Rust]?", that seems like a totally valid point to make. &gt; If you don't like discussing the flaws in your weak criticisms, make stronger criticisms. No, what I don't like is how many times you've put words in my mouth and how you're just ignoring my actual arguments. No doubt it's unintentional, but it's quite tiring regardless and I feel we're getting nowhere. 
More experimenting: http://is.gd/KnXFWr It seems you're right and the extra capacity is just forgotten about, means you're effectively (temporarily at least) leaking memory. I'm still wondering how the underlying allocation works, here's [`deallocate`](https://doc.rust-lang.org/alloc/heap/fn.deallocate.html) take note of the `old_size` parameter and its requirement...
Yes. A single executable for now... I use [horrorshow](https://stebalien.github.io/horrorshow-rs/horrorshow/) library for html generation. 
Oh ok thanks.
Tyvm!
Cool, thank you.
The biggest problem I've run into with standard webapps in rust is the DB layer. Diesel may solve some of that but I find the documentation to be pretty lacking for doing anything other than "select *" queries. The diesel API documentation can be hard to navigate since most of the actual types are hidden behind macros or deriving Queryable, making finding what you're looking for difficult.
Glad to hear you are enjoying Rust! Inheritance is one of the biggest challenges coming from a standard OOP language to Rust, because Rust doesn't have inheritance in the normal OOP sense. Rust prefers composition, using Traits, which are similar to Interfaces in C#. If you think of it in this way, for example, you can have a Trait `ServerBase` that defines the method signatures that are common between UDP and TCP, then define the actual implementations on `UdpServer` and `TcpServer`, just like you would with two classes sharing an interface in C#. Then on any function that expects a server as a parameter, you can typehint it to expect an object with trait `ServerBase`, allowing you to pass in UdpServer and TcpServer interchangeably. You can read more of the technical details here: https://doc.rust-lang.org/book/traits.html
I have been using multiple cursors in Vim until I discovered `gn`. Now I have mapped `c*` to `*Ncgn` and after that just `.` everything else. It is IMHO as nice as multiple cursors. 
As Steve said itâ€™s Graphviz / DOT. https://github.com/jrfonseca/xdot.py is a GTK-based DOT viewer that supports panning and zooming.
it sounds like it should be possible, and it would actually give you a borrow checker on your code for free, if you want it for your language (or maybe its actually necessary?). but I'd think the compiler won't really expose an interface for "compile-your-own-MIR" intentionally, so setting it all up would probably require quite some hacking. also, MIR won't really be stabilized I think, so that could be annoying as well. but yeah, in principle, that sounds totally possible! edit: I just noticed that MIR is type-driven, so your type system should be similar to MIR's I guess.
Yeah, that would be much better, but I guess they would have to themselves integrate trustworthy build independent dependency compilation, which would be hard. Another thing to think of is that a lot of projects use tho nightly versions too, or multiple versions - and caching dependencies for those builds would be fairly pointless AFAIK.
Good point. And the syntactic cost of getting the mutable reference... is just fine :) I did try to create an UnsafeDerefMut, but apparently lang_item traits can be defined only once, which pretty much makes sense.
&gt; AFAIK, a boxed slice takes up one less word of memory, I hadn't thought of that. One situation where it might be useful is an array of boxed slices: tighter packing for better cache use. There's spooky compiler magic around `Box` too. It's a single pointer-sized word when pointing to a sized type. http://is.gd/zEVuMl
What is the correct place to ask questions about a crate? Github issues don't seem appropriate. &gt;I'm looking to generate de/serialization for json tagged union messages, eg. &gt;{ status: "error", message: "wrong"} &gt;or &gt;{ status:"ok", response: { a: 1, b: 2}}. &gt;Right now with serde, it looks like this requires a hand implementation, but I wanted to ask before I did so since it looks somewhat complex for me. &gt;I found http://stackoverflow.com/questions/33323891/conditionally-decoding-json-based-on-a-field-in-the-json yesterday but that involves a lot of computation, would the hand-written visitor going to do any less work?
You say things like "it's a pretty frequent idiom" and "it is done with some frequency" in the same discussion that you say "I was talking about special cases". You say things like "allocations that are combined are only padded once by the allocator" when referring to general allocation overhead, and then you back it up with numbers from a specific allocator (which was irrelevant to your initial claim), then you claim you were really talking about specific programming idioms in game programming... You keep claiming Rust is not low level enough to do the things you can do in C... and then you are afraid to use "unsafe", which does give you C-like power. Instead of asking me for code, how about you come up with some first. Show me how hard you think it is to do something in Rust which you find easy in C and let's discuss that specific case. I don't doubt there are some; Rust is young and has more safety belts than C. But having to work a little harder to bend the rules for special cases doesn't seem like anything meaningful to complain about. But I don't think "combining allocations to save allocation padding overhead" (your initial claim) is one of them. It's not on me to prove your claims wrong if you can't substantiate them (or even keep to one claim). Start by solidifying your claims and let's go from there. Because if I come up with some code (just like I came up with valid arguments), you'll just shuffle to the side and claim you were really talking about something else (vectors? or structs? space efficiency by avoiding padding? or "space efficiency" via data locality? frequent idioms? or rare special cases?)... No one is putting words in your mouth. We just can't understand what is coming out of it.
There's nothing wrong with a "negative tone" when that negative tone is about products not people (i.e. is objective). I looked at Mioco myself not too long ago. It wasn't production-ready then: if you use you'll end up developing it. Not necessarily a bad thing but something that users should be aware of. Furthermore, the Rust calling convention is not stabilized yet. This means you can't expect stack manipulation to work unless you use Rust to do it. The calling convention may change and cause all kinds of strange behavior. Thus it's very difficult to do context switching at all. You either hope that the rust-to-rust calls will continue to work the same way they currently do (yikes!) or you do some magic with `extern` declarations. I'm suspicious of Mioco's dependence on Boost.Context. It's a good library, but it's a C++ library. `rustc` doesn't generate C++ compatible functions. Sooo something tricky is going on and I wouldn't rely on it without taking the time to figure out exactly why it works.
note .org rather than .com. It's a fork, basically.
JW Player did not want to work for me today. Here's the same talk on YT https://www.youtube.com/watch?v=WQbg6ZMQJvQ
Ah, I am sorry I wasn't clear. Loading by chunks is a great idea, but for the NAS case here the issue less one of throughput and more one of latency: getting the initial bit is very slow (or even times out), afterwards there's no issue. Of course, the editor cannot magically make up the data; however while it's okay to display something in the tab "content pane" indicating that it's loading, it's really annoying when the whole UI freezes. Or even the tab freezes. Also I want to be able to close the tab even if it's not finished loading (cancellation of order). --- On a related note, sometimes if the NAS is unstable, the editor might find that the file it is editing seemingly does not exist any longer (at least, that's what Eclipse reports). In this case, Eclipse proposes to close the file and throw all your changes out the window. If you can manage to do better, it would be great too.
The front-end/back-end protocol will grow a bit from where it is now, but philosophically stay similar. It's all about drawing the visible text onto the screen and plumbing UI events (keyboard, mouse, scrolling). It's not about accessing the text. It will grow in the direction of supporting multiple tabs, more rich text annotations (I just pushed colors last night), and more precise invalidation. I'm still rapidly absorbing ideas on the plugin architecture but I imagine it will be quite different. I think the plugin will get a cached window into the buffer. For small files it will be the whole thing, but for big ones it will have to scan through. This will be async, so it should be possible to tolerate, say, syntax highlighting of the entire buffer taking a while, if the first screen pops up instantly. It will obviously also need to be able to hook keybindings and add menu items, but there are of course lots of details to work out.
Thanks knipil. I'll check it out. What sort of throughput did you get? One of the goals of using Mio was get high throughput if, you know, it happened to be an Internet facing DNS server one day. Funny how a few us chose a DNS server. I guess it's meaty enough, yet constrained enough to do something useful, yet self contained. Any thoughts of doing a 'serious' open source DNS server in Rust?
You should be able to avoid the `let s = self` if you pass `self` as an `:expr`, not an `:ident`.
Ah, right! Didn't know that there were two sites. Thanks.
Awesome work, when can we expect to see it in the packages? 
It works on windows too
Throughput wasn't really a concern of mine since I mostly wanted something to use locally for some simple use cases, although I've pushed it as far as about 100 QPS without any problems (took some tweaks before it worked smoothly though :)). I think that your asynchronous strategy is really useful for a server acting as a zone authority, since servicing the requests only requires some fast memory lookups. I'm less convinced that async io will make a significant difference for a forwarding or recursive resolver since network latency will dominate and the time wasted by context switching and lock contention is mostly negligable in comparison. Obviously that will depend a bit on the fraction of cache misses though -- if you're hitting the cache 99% of the time it will probably perform impressively. :) I've had a ton of fun with it, so I'm excited to see others doing the same! Not sure where I'm going with it exactly. I've been reading about DNSSEC lately, considering whether to go ahead with implementing that. After stress testing, I saw some evidence of DNS cache poisoning in the wild, so I've realized that I'll have to figure out some strategies to manage that if I want to get serious with it... :/
I am not aware of any, but this could actually be a good idea for a cargo sub-command if anyone is looking for a pet project! ;) However, it is more complex than it sounds. For example tests and examples can rely on the name in `extern crate &lt;name&gt;` and `use &lt;name&gt;::foo`, sub-modules could have the same name as the crate etc. I'm sure there are a couple of other edge-cases that you would have to consider.
Even if you write it like this: fn get(&amp;self) -&gt; &amp;'a [u8] { &amp;self.buf[..self.n] } it doesn't compile. I think it has to do with the fact that the `WritableBuf` has a mutable reference, so you can't decouple the lifetime of the buffer slice from that of the WritableBuf, otherwise you could have mutable and non-mutable references at the same time. However, for this particular use case you could just consume self: fn get(self) -&gt; &amp;'a [u8] { &amp;self.buf[..self.n] } 
I don't want to see your rust version. I want to see your "easy" C version. You were the one claiming it was easy in C and super hard in Rust. Show me your "easy" C version, and I'll show you the version in Rust that isn't super hard. Why is this so difficult? Stop trying to fix the mistakes you already made in this discussion, and focus on the task at hand. You have one more chance before I lose patience.
First, I mostly care about instances where we know the size at compile-time at the moment, so no need for smallvec. For this use case, men::uninitialized does the job (modulo panic safety).
Ah. I was imagining the front-end/back-end boundary being at a lower level, so that front-end would implement the behavior of command keys in terms of a generic text manipulation protocol implemented by the back-end.
Good catch.
Also perhaps look for a README and see if the old name is mentioned? (I figure this should be a manual process)
Yeah :) it probably should have different levels of aggressiveness. With the least aggressive one replacing just enough for it to compile and the most aggressive one replacing every occurrence, even in doc comments etc.
I agree. I gave you two chances. If the easiest example C code you can come up with is someone else's code that doesn't even meet the topic being discussed, then I am out of patience. I hope you don't waste the time of the next person who tries to help.
Here are the shortcuts that I have set up in Firefox, which should provide some inspiration for the kinds of queries I find useful: * `rust %s` â†’ `http://doc.rust-lang.org/std/?search=%s` * I use this one all the time, it is by far the most useful * `crate %s` â†’ `https://crates.io/crates/%s` * This is useful if I want to jump to a specific crate, and be able to browse from there to its docs or repo * `crates %s` â†’ `https://crates.io/search?q=%s` * This I use when looking for a crate to do something, but I don't know the name. In particular, "crates time" shows me that I can pick between the [time](https://crates.io/crates/time) crate and [chrono](https://crates.io/crates/chrono), which arguably has a better API In particular, I notice that in the Rust Cargo Packages IA, it only links to the one with an exact matching name, which isn't very helpful if you're just looking for crates that deal with a topic without knowing the name. Not sure if IA's can provide a list of the top few matches, but a lot of times that would be more helpful. Also, it can be invoked with "cargo package" or "rust package", but not "cargo crate" or "rust crate", which is generally the terminology used in Rust.
Thanks for the MIR link! I definitely agree re: generic references, though fortunately in this use-case I can probably ignore it for now (this is a early prototype).
You called it catch_panic, though -- it's catch_unwind now (and forever).
I see, that makes sense, thank you. I mixed up the test_maps in my head when I wrote my question. I guess I'll start by pre-allocating my vectors from now on if possible.
Nothing wrong with it. It's just spooky because it's impossible to write that behavior in pure Rust. 
do you mean a problem with ORM? 
Got my feet wet with Tony Aldridge's SDL2 bindings because I wanted a little utility for myself: a Brownian noise machine. It was fun! Freed from thinking about data structures and memory safety, programming Rust is as light and straightforward as Ruby. Except at the end of the day I get audio synthesis with negligible CPU use. Very cool. I learned a fair bit about "digital harshness" along the way (audio is less forgiving than video when it comes to aliasing) and implemented an rng from scratch. Tiny project, but a lot of fun.
To be exact offset is about 1850 ms. The good thing is that video can be easily [downloaded](https://d3fenhwk93s16g.cloudfront.net/f9j2h7/hd_mp4.mp4?t=14618999255722d2956530f) and viewed for example in VLC with adjusted audio delay. (using "k" hotkey)
There's some new [documentation](https://github.com/google/xi-editor/blob/master/doc/frontend.md) of the protocol, which should definitely be of interest to people who want to experiment with the code.
Cool! As a side note, I've been wanting benchmarks comparing GenericArray to primitive arrays for some time, but have been too lazy/busy to do any myself. Glad to see it does alright, although I'm curious how much faster it would be if you ignored panic safety.
&gt; For example, coio allows coroutines to migrate between worker threads, but mioco doesn't. If I don't misunderstand you, it can now. :) http://dpc.pw/mioco/mioco/sched/struct.Coroutine.html#method.migrate Last time I looked at coio the work stealing scheduler was a thing that mioco didn't have, and I think it can now be implemented as http://dpc.pw/mioco/mioco/sched/trait.Scheduler.html 
Mioco does not depend on `Boost.Context`. Mioco uses `context-rs` is a pure Rust (+ assembler) library that provides API similiar to `Boost.Context`. Also, I don't think Rust calling convention needs to stabilize. Context switching is done via `extern "C"` functions implemented in asm, so Rust calling convention is not even involved. https://github.com/zonyitoo/context-rs/blob/master/src/context.rs Mioco is not production ready, yet but it's getting there. There is some production level involvement is on it's way, and first evaluations were very positive, and with time all issues will be resolved. As encouragement - it has some nice unit tests and I'm trying to be responsive about fixing any issues reported. :)
Thanks for sharing! :D Been spreading the word about intermezzOS around my peers too.
Just a thought, how practical/feasible is it to convert dynamic OO languages like Python or Ruby into something equivalent with Rust's type system? If it were to be efficient then I could imagine safety guarantees, lack of the GIL (I suppose the compiler could identify which parts are accessed in parallel) speedups going for Python should it compile into MIR.
Are you aware of [custom_derive](https://crates.io/crates/custom_derive), and [newtype_derive](https://crates.io/crates/newtype_derive)?
Yes I have seen that project. This works for more kinds of structs and enums and it uses the regular derive syntax instead of custom macros. 
Sometimes I miss the nice methods that c#'s linq offers, or you'd be able to do something like first.zip_with(second, Eq::eq).filter_count(|id|id) The `zip_with` function would accept a function that combines the items from both iterators, e.g. passing `first.zip_with(second, |a, b|(a, b))` would be the same as `first.zip(second)`. The `filter_count` method would take a function that accepts the iterator items by value (and not by reference, like `filter`), and returns `true` iff they should be counted. Sadly neither Rust nor C# have an identity function in the standard library.
I think the other two work on stable though?
Glad you liked the change! :)
Instead of what? `*` and `cgn` (and `.`) work for basic variable renaming, and I use them, but I'm talking about *general macros* which may do a lot more than replacing a bit of text with `cgn`. Likewise, there's a lot more to multiple cursor usage than renaming variables.
Awesome. Lets see how the build time decreases by this. At the time of writing, we have build times near 30 minutes for [imag](https://github.com/matthiasbeyer/imag)
Hey there, newbie learning Rust here. I have the following snippet: let error_msg = "ERROR!"; let mut s1 = String::new(); let mut s2 = String::new(); io::stdin().read_line(&amp;mut s1).expect(error_msg); io::stdin().read_line(&amp;mut s2).expect(error_msg); s1 = s1.trim(); s2 = s2.trim(); But the compiler keeps yelling at me: rc/main.rs:12:10: 12:19 error: mismatched types: expected `collections::string::String`, found `&amp;str` (expected struct `collections::string::String`, found &amp;-ptr) [E0308] src/main.rs:12 s1 = s1.trim(); I tried everything but I can't see. Does the `trim()` method return a reference? If yes, how can I save the string so?
&gt; Does the trim() method return a reference? Yes. This is what the error is telling you, but you could also look at the docs for `trim()`: http://doc.rust-lang.org/std/primitive.str.html#method.trim &gt; If yes, how can I save the string so? Well, do you need it to be a `String`? If so, call `.to_string()` on it. If not, you could make a new variable rather than trying to mutate `s1`.
&gt; It's solely a personal project, so I don't need to worry about non-nightly compilation. If the project is just for learning, I still recommend trying to stick to stable if there's a workable solution for you problem. It gives a lot more appreciation for what Rust can do and makes sure stable doesn't feel like the less capable brother of nightly. Nightly is always a language that _could be_, not that _will be_.
I wish there was a lint that detects basic math operations (ie. using `+`, `-`, `*`, `/`, `%`). This lint would be allowed by default but could for example be switched to warn or deny these operations in your server code. This way you would have to explicitly specify the overflowing behavior of every single maths operation in critical code paths. 
Really nice overview, cool :) However, since you linked to production bugs from Diablo 3 and Hearthstone, I feel compelled to ask: Rust's overflow checks wouldn't have helped for those cases either, because they are (usually) disabled in release mode, right? Not that this would be different for C/C++, and it still serves as pretty good motivation, but since you explicitly mentioned them, just wanted to confirm. Thanks! :)
Hey cool, you used my webrender fix as an example. The bug there would probably have never been encountered if not for overflow checks. The root problem was subtracting two `u32`s, getting a negative result, and then casting to a `GLInt`. On all common platforms `GLInt` is an `i32`, so with wrapping arithmetic it would actually work the same before and after the change. Until someone compiles on an exotic platform where GLInts aren't `i32`s, since they are only specified to be signed integers of width at least 32.
&gt; Rust's overflow checks wouldn't have helped for those cases either, because they are (usually) disabled in release mode, right? Yeah, unless they enabled it manually, it wouldn't catch it in production. However, it may catch it in development if they had some sort of automated test-suite that pushed those sort of limits (but... I'd guess they didn't). Rust's checks probably wouldn't help with the Boeing example either (an unfortunately-timed aircraft DoS from a `panic!` could be as bad as a DoS from the integer overflow itself). They were more trying to serve as motivating examples for why integer overflow is something one might care about.
Ah, whoops, I guess I didn't read closely enough: I was trying to only include non-false positive bugs; things that were truly incorrect, rather than things that just required adjustment to express the intention correctly.
I think it's close enough to truly incorrect, it's good example of subtely wrong behaviour that this catches, since some day (with 64 bit `GLInt`) you might start getting mysterious incorrect behaviour.
You mentioned existential quantifier in Haskell's type system in the article. I am curious since I've worked on graphics libraries and also learned a bit logic recently. Could you explain when this is useful in library design?
Cool. I'll have to ask though: why octal?
There is at least one group that calls this "optimization unstable code": https://css.csail.mit.edu/stack/ With undefined behavior you want to allow the compiler to generate better binaries, but in general you don't want the results of your binaries to be affected by the optimization level/options that you enable in the compiler.
This subreddit is for a programming language called Rust. You're looking for /r/playrust.
Oops
Oh woops, you're right. 5 * 2 / 2 would be 5, indeed in that case the only issue would be if an overflow was allowed before the division. For clarity: 5 * 2 / 2 = 5 5 / 2 * 2 = 4
The idea around small vec was specifically that given that the parts are created incrementally, the number of initialized parts has to be tracked to cater to panic safety.
&gt; Rust's overflow checks wouldn't have helped for those cases either, because they are (usually) disabled in release mode, right? Rust's overflow checks are disabled *by default* in release mode, but may be enabled if you so wish. They are enabled because people expect release binaries to provide *fast* code, and since overflows did not affect safety (only logic) it was decided to honor the "Do Not Pay For What You Do Not Use" philosophy here. Note, though, that such checks being disabled by default is not set in stone. It is hoped that as the performance of checks improves (notably with delayed checks, better value propagation, etc...), at some point in the future they could be switched on by default. In the meantime, people are encouraged to actually *measure* the slow-down in their code caused by overflow checks, and activate them if this slow-down is acceptable. Unless your code does some heavy number crunching, for example, the checks should not provoke much of a slow-down or cause an explosion of the size of the binary.
I see. I missed the build.rs part. Thanks for explaining this. Ping /u/zonyitoo . Were you aware of the unwinding issues?
There's currently no middle ground between "shove strings into a driver and receive a ResultSet" and "Fully typed querybuilder (diesel, rustorm, deuterium)". Writing wrappers around the driver to return an Iterator/Vec of a struct that maps to the rows is fairly easy though.
&gt; There can be no data races (although Rust already prevents that) with coroutines. Mioco picked multithreading route, so concurency is still possible (and taking care of Sync and/or Send is required). &gt; You need to convince me that alternative approaches cannot be easy to understand and etc. I don't really have time to try to convince anyone, neither do I care much. If people find other solutions more fiting their needs that's perfectly fine with me. My personal argument is: take a look at tcp echo example of mioco, and other systems like rotor, etc. I like how mioco version looks like. &gt; However, for my case, I really want a single thread and I don't think it's beneficial to have such extra thread. It is possible to do this with mioco (you can just run different event_loop in mioco coroutine and do some signaling scheme etc.) but from overal notion of this and your previous requirements, I'd say you really want just `context-rs` and your own glue. &gt; Coroutines and executors (a.k.a. executors, services...) are different concepts and they should be usable independently. You merged them both and added mio to the mix. It's MioCo for a reason. Also scheduling is completely optional. With async code everyone have different opinions and requirements, so one library can't satisfy all needs. &gt; TBH I hope this doesn't enter on the core Rust as is because it is too immature at the moment and needs to evolve more. No worries about it. There's no need for that and noone is advocating for it. BTW. You really do sound very negative, as other people pointed out already. I personally don't mind, but some other people might. You need to know that you're writing judgments on other peoples work, which they might have personal attachment to etc.. They have put it for free in public, without forcing you to use it or guaranteeing it's suitable for everyone, so being to harsh is not well perceived.
In Haskell you can't put different data structures all implementing an instance of the same class (trait in Rust) into one list. With an existential quantification "wrapper" for your data structures you can do it.
Yeah, like **dan00** said, Haskell has no support for *â€œtypeclass objectsâ€*. However, it has support for *existential quantification*, which enables you to state that a type variable can be anything and its type cannot be inspected from the outside. newtype Foo a = Foo { runFoo :: (a -&gt; String, a) } data SomeFoo = forall a. SomeFoo (Foo a) -- you can now imagine the following: let fooInt = Foo (show, 3) :: Foo Int let fooBool = Foo (\x -&gt; if x then "yes" else "no", False) let foos = [SomeFoo fooInt, SomeFoo fooBool] traverse_ (\(SomeFoo foo) -&gt; let (f, x) = runFoo foo in print $ f x) foos Itâ€™s a bit more powerful than trait objects. The Rust â€œtype objectsâ€ would map to this in Haskell (imagine we want the *Debug object*, which is the *Show* typeclass in Haskell): data Show' = forall a. (Show a) =&gt; Show' a -- use like this: map (\(Show' x) =&gt; show x) [Show' False, Show' (3 :: Int), Show' $ Just "foo"]
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust_networking] [A forwarding\/caching DNS resolver](https://np.reddit.com/r/rust_networking/comments/4h1036/a_forwardingcaching_dns_resolver/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Yes, we have wanted this but haven't had the chance to build it. It's not simple, there are a lot of edge cases.
Typically Cargo lets the root crate being compiled determine how its dependencies are compiled, but other than that caveat that's what "globally" means.
I agree whole heatedly with this. But I'd also add that one of the most helpful things one can do to help (without actually writing documentation) is simply filing documentation issues where docs are unclear or can be improved. Speaking from personal experience only; there are times when I may be unaware docs are lacking or unintuitive since I'm already familiar with the internal workings of my lib(s), and it's hard to recognize what knowledge I'm taking for granted. 
FYI, the link to Diesel's website is an HTTPS URL. The site is hosted on Github pages so no HTTPS. 
A very nice article, thank you. &gt; Haskell will avoid overflow entirely by default, via arbitrary precision integers. I think values of type Int are sufficiently common in Haskell code and they overflow: Prelude&gt; let f = (\x -&gt; x ^ 100) :: Int -&gt; Int Prelude&gt; f 2000 -3190895993503612928 Rust overflow handling is awesome, it's one of the best I've seen in a system language. But I can see three ways to further improve it: Is something like the RedundantOverflowCheckRemoval of Swift (linked in the article) going in Rust? :-) A system language needs an unchecked integral cast, but I think it's a bit unfortunate that a nice short name as "as" is used for that. I think the Rust prelude needs a nice short checked integral cast function/feature (unless it's already there and I've missed it, I am using Rust only since few months). Another thing that I think could be sufficiently important for Rust "integer overflow story" is to offer library/crates bigints that are very flexible (ideally as flexible as built-in integral numbers, including literals), very efficient on small numbers (with a small int optimization, plus some way to tell the compiler the big integer type follows the usual axioms, allowing the compiler to optimize expressions among such big integers as well as built-in integral numbers). This allows programmers to replace the native fixnums with bigints in some kinds of Rust programs that aren't performance-critical (this is the fifth option after wrapping_add, saturating_add, overflowing_add, checked_add). Such bigints don't overflow for lack of bits, and when the small int optimization kicks in (avoiding heap allocations), they are sufficiently efficient for many purposes.
Sure. My idea with mrusty is not to pull any Ruby dependencies in. Of course, you're free to do it if you want, but the basic idea is that it's a nice, portable way to add scripting to a mature Rust project. Like a game engine. :)
And I don't mean to sound like I don't think the thing has no merit. I really think it is a valid and cool project. I just hope that people will exercise restraint in the usage of it. Cross language dependencies is a place where reinventing the wheel has real value.
I have a question (or two): &gt; to build arrays from `IntoIterator&lt;T&gt;`s and `Fn(usize) -&gt; T`s, respectively. Isn't creating an array from `Fn(usize) -&gt; T` redundant with the former one? If there was a method along the lines of: fn from_iter&lt;I, F&gt;(iter: I, f: F) -&gt; [Y; $n] where I: Iterator&lt;Item=X&gt;, F: Fn(X) -&gt; Y then couldn't I do the following to get the same effect? let array: [_; 10] = ArrayMap::from_iter(0..10, my_mapper); Or is this not what you're intending? Unrelated: Wouldn't this crate encourage people to move stuff to stack, since it becomes too easy? :)
&gt; And it's not clear to me that such a revision would be a terribly welcome contribution I am quite sure that if you were interested in refactoring Cargo's code, it would be welcome. Cargo is a very old project, and while it doesn't suffer as much as `rustc` does, it also doesn't see nearly as much help.
Fixed that. Thanks for letting me know!
&gt; Basically, it's a project that needs a champion. Rust has done a good job tracking easy issues for people new to the language so they may contribute early on. I wonder if a similar thing could be done for larger features like this where a community member could have some ownership.
Proof of concept bindings to Go: https://github.com/BurntSushi/rure-go Example in C: https://github.com/rust-lang-nursery/regex/blob/master/regex-capi/examples/iter.c I'm not much of a C programmer, so I'd very much appreciate feedback!
One of the largest problems I had, is that methods are not always defined and listet in the datatypes definition but are inherited from traits. Maybe a rust equivalent of [hoogle](https://www.haskell.org/hoogle/) that can automatically search for method names, may help. I think, that additionally to "crate of the week" we may have "documentation of the week" for encouraging good documentation.
There will be a `target/doc/&lt;your-crate's-name&gt;/index.html` and possible also folders for your dependencies. Tools like `travis-cargo` automatically add an `index.html` at the top level which redirects to the `index.html` in you crate's folder.
This is really cool! I love the idea of ffi-ing Rust into go code. How does the go api you present compare to other regex apis in go? What would it take for the masses to adopt this regex implementation? I imagine like performance must be better than whatever is built into golang if it's also written in golang. The `regex` crate is already the de-facto standard regex engine for Rust, it would be lovely to see it take over golang as well :).
Again, visual block and `:s` work in *some* cases where macros could be used, but not nearly *all*.
There is limited type-based search in rustdoc. It's not as good as hoogle though.
**cargo:** I don't think I'm up to the challenge of refactoring in general, but I can start by commenting the `util` modules and try to make it easier for someone else. And I do need a project, so I'm jumping in. Thanks for the encouragement. **generally:** It really depends on the project. A lot will be happy for the help, others won't be, and that uncertainty is intimidating for a potential contributor. Even a gentle nudge in the README can help, like /u/peschkaj said, something like: Contributions Welcome: If &lt;this library&gt; does something that's surprising, confusing, or just plain weird you're probably not the only person affected. Please file an issue, even if you're not sure it's a bug. This goes for documentation too. If you think "I wish it had told me that," that counts as a documentation issue. New users are especially helpful because your feedback is the only way we can make the API easy-to-learn. Clean-up, commenting, and missing test cases are always welcome. See CODING for style guidelines and drop by the mailing list to develop ideas, especially big ones like refactoring, new features, or your experimental fork. As always the issue tracker benefits from your troubleshooting, fixes, documentation, and peer review. 
Oh damn im so sorry! thanks for the headsup 
(not stupid question, I could have clarified before)
The only folders in *target/doc* are *src* and *implementors*, both empty. I have a --bin project, does that make a difference?
Interesting, thanks!
Simple solution - I've [got a script](https://github.com/deadalusai/iron-pipeline/blob/37e203533dbd0f0c11d914f5d3dadfd8735ea50c/update_and_publish_docs.ps1) that does the work for me (this is PowerShell, but could be trivially ported). It relies on a clone of the `gh-pages` branch in a separate directory, and performs the following steps: 1. Run `cargo doc --no-deps` 2. Generate a simple `index.html` 3. Move Cargo doc output into the `gh-pages` directory, erasing/overwriting whatever is there 4. Prompt the user to continue 5. `git add -A`, `git commit -m "Updating docs"` 6. `git push origin gh-pages` (the work starts at line 58 in the linked script) I just need to run the script whenever I want to update the docs. This is pretty basic - ideally you could have the work done by your CI build (e.g. Travis as mentioned), but it's a simple script to drag around in the meantime.
If you are going to cross compile take a look at [rustup.rs](https://www.rustup.rs/) it makes cross compiling simpler because you can easily install the `std` libraries for other targets with a simple command like `rustup target add i686-pc-windows-gnu`.
&gt; $ rustc -V &gt; rustc 1.8.0 Yeah, it seems the commit hash is the problem. &gt; and not from pacman Actually I think we can fix this problem on the Rust side. I'll open an issue on the rust-lang/repo.
This is awesome! As a college student , this is something I'm pretty curious about, because I really want to keep using rust, and seeing more companies use it motivates me. I'm just learning it but mostly practicing my coding in that, and I may even end up using it for coding / internship interview questions. Hopefully that serves me well. Although I still use python and java a lot. Thanks for putting up that site! I hope to see more companies using it!!!
You want to borrow from `self`, but *also* allow dropping `self`. A major problem with this is that you can't drop a borrowed value (`Drop::drop` takes `&amp;mut self`). If such a method was possible, you could implement a `Drop` that mutated `self.buf`. The simplest way to avoid this is to split the logic "around" the drop; have a method that borrows the value *while alive* and another that "steals" it. Since you don't use it while alive, only the latter is actually needed. --- One problem with this argument is that [dropping borrowed values is *already* broken](https://github.com/rust-lang/rust/issues/31567#issuecomment-182799488). As far as I can tell, the proposed fix involves tracking which values have `drop` implemented. So when that fix arrives, shouldn't your original code work, as long as `drop` is not implemented? Well, I'm not totally sure. My *guess* is that it's a byproduct of the way Rust derives lifetimes - the return value of something like fn f&lt;'a: 'b, 'b&gt;(x: &amp;'a mut bool, y: &amp;'b mut bool) -&gt; &amp;'b mut bool will borrow both `x` and `y` even if the body of the implementation just returns `y`. This is because Rust conflates *borrows-from* with *necessarily-lexically-smaller-than*. Not doing so would mean that the body of a function affects its type, which Rust has been avoiding quite strongly. Right now we know that a function fn get&lt;'b&gt;(&amp;'b self) -&gt; &amp;'a [u8] *cannot* borrow from `self` since there is no restriction `'b: 'a`. Extending lifetimes to split the two would require something more like // return borrows-from 'b, but // 'b isn't necessarily-lexically-smaller-than 'a fn get&lt;'b&gt;(&amp;'b self) -&gt; &amp;'a: 'b [u8] and nothing of the sort exists.
Take a look at `ghp-import` tool [1]. It greatly simplifies process. Just run `cargo doc &amp;&amp; ghp-import -n target/doc`. [1] [ghp-import github page](https://github.com/davisp/ghp-import)
Yeah, when I and Leonard were testing the new context-rs with boost's ASM, we worried about whether it could be unwound successfully. You may see this issue in coio: https://github.com/zonyitoo/coio-rs/issues/51 which causes unwind fail. So ... hmm, this issue may be related this.
I would recommend: `let s1 = s1.trim;` However, don't do this blindly. It is important to understand how a `let` statement differs from mutating assignment. `let x = ` - creates a new location named `x`, which may have a different type - delayed drop of the old value - borrows of old location are still valid - can use destructuring patterns i.e. `let (a, b) =` - cannot move a value to an outside scope `x =` (assignment) - re-uses old location, must have been created as `mut x` - exactly the same type - instantly drops old value - conflicts with borrowing - no destructuring pattern - can move value outside scope The only other ways to move a value outside a scope are: result of a block, return from function, or call a function that does assignment.
From reading the header, I see that the convenience function `rure_compile_must` is documented to abort on failure. How do you handle panics otherwise; are there other C functions that could panic with invalid input (except for OOM, which leads to an abort as well, I assume)?
Visual block only works in rectangular blocks, unlike arbitrary multiple-cursor implementations. `:s` won't always work because the multiple insertion points chosen by users aren't necessary regex-matchable. What do you want an example of? Is there even a point to this conversation? It's obvious that the stuff you are mentioning doesn't match the *full generality* of macros/multiple-cursors. They're just more convenient than macros for a lot of cases, and I'm not arguing against that point. So I guess I just don't understand what point you are trying to make.
Learning a bit about constraint satisfaction problem solving using back tracking: https://github.com/bvssvni/quickbacktrack
In recent rust versions, integers have gotten `From` implementations where the conversion is lossless, for example `From&lt;u16&gt; for u32` and so on.
&gt; Is something like the RedundantOverflowCheckRemoval of Swift (linked in the article) going in Rust? :-) Rust uses LLVM, which has some features like this and is gaining more soon, [apparently](https://twitter.com/johnregehr/status/726084145080864768). Additionally, the compiler is actually gaining the SIL-like [MIR](http://blog.rust-lang.org/2016/04/19/MIR.html), which will allow more Rust-level optimisations like that. &gt; A system language needs an unchecked integral cast, but I think it's a bit unfortunate that a nice short name as "as" is used for that. I think the Rust prelude needs a nice short checked integral cast function/feature (unless it's already there and I've missed it, I am using Rust only since few months). Yeah, the state of casting is a definitely a weak point. [RFC 1218](https://github.com/rust-lang/rfcs/pull/1218) is a proposal to help address it (but it won't use the nice `as`, as you mention). &gt; offer library/crates bigints Yeah, that's another good point. It will take some time to be maximally flexible, and resource management means it won't ever quite get there (i.e. primitives can be `Copy`, but bigints with their need to manage backing allocations etc. cannot).
I don't understand the panic-safety problem. &gt; the panicky path can be simply mem::forgetting the array (which is compiled away to nothing) and resuming the unwinding This sounds like a no-op. Why does it provide safety? It sounds like you're saying the following isn't safe, but I can't see why fn f () -&gt; T { T t; t = unsafe { mem::uninitialized() }; panic!(); return t; }
I guess the language could specify that the compiler can assume certain algebraic properties. However, it seems to me that it is just asking for pain and confusion, e.g. if `x * a / a` doesn't match `x * a / b` where `a == b` in a way the compiler can't see statically (such as, user input).
Panics cause unwinding, and unwinding drops all variables that were in scope. If `T` has a `Drop` impl, it's likely to cause undefined behavior when `drop()` is called on uninitialized memory.
May I suggest the [`buffer`](https://crates.io/crates/buffer) crate? It does exactly this, implementing writable buffers of possibly uninitialized data, for various data structures (slices, `Vec`, `ArrayVec`).
TL;DR: It's impossible right now to write a context switching API ontop of the Rust ABI, because that ABI is undefined behaviour. Thankfully the C ABI is well *defined behaviour*. This means that it does the same thing in every situation every time according to a certain spec, which is great because we can exploit that fact and make use of it. Alright... So I'm the person who wrote most of the parts of the current context-rs crate. And yes... You got some parts mixed up there. First of all: Context switching works by swapping certain instruction registers with those stored away somewhere. Thus you switch execution from a stored state with an active state and vice versa. For this to work it's *impossible* to use the Rust ABI, even if we'd wanted to. The reason for this is that the Rust ABI is currently still *undefined* (see [here](https://github.com/rust-lang/rfcs/issues/600)) and as long as it is undefined we can't rely on it to write the necessary assembler code to switch the instruction registers (since we don't know which registers Rust uses for what). The `extern "C"` ABI in Rust is instead *well defined*, and will never go away (for obvious reasons). The performance overhead in Release builds is exactly 0 because LLVM will optimize the ABI wrapper away (yay~). We can thus generally use it without worrying about undefined details. Even such things like unwinding through `extern` methods is well defined (they abort - or at least they should). Furthermore: `context-rs` is in fact a "native" (!) port of Boost.Context. Reason being that if you use Boost.Context you don't use the private assembler code (which will be fully unaccessible in the upcoming version) but a public API. That public API is actually the "face" of Boost.Context - not the assembler code we use. Unfortunately context-rs is still missing a safer, public API, but only exposes 3 primitive methods. Furthermore I decided to fully and as overt as possible make it clear that context-rs is relying on Boost.Context, because the assembler code is quite complicated and was developed and "battle tested" by them for free over many many years. I'm glad that I can use their code for this. So... why is this "unsafe" code not an issue? * Context creation: The "context function" which you pass to `Context::new()` must be `extern "C"`, because we must acquire a function pointer of it. In the C ABI this pointer is basically the same as the instruction pointer and get's stored in the context struct on the stack as the initial state. The first `resume()` loads that pointer and sets it as the next instruction address in the matching CPU register. It is *your* responsibility as a user of context-rs to fulfill the requirements of the C ABI and make sure to use [`std::panic::catch_unwind`](https://doc.rust-lang.org/nightly/std/panic/fn.catch_unwind.html). * Context resumption: Since the C ABI is well defined the register states of the CPU *inside* the FFI call are well defined too. We can thus swap the current state of execution with any other thing that is in the same FFI call (since it's environment will have the same "rules" under which it performs). We can thus simply swap each relevant CPU register one by one with the context struct on the stack (or heap or wherever) to form a snapshot of the current state of execution. Since we not just load but actually swap the registers we are simultaneously restoring the saved state and save the current one. A call to `A.resume(B)` will thus pause `A` *inside* the C ABI call to `resume()` and restore `B` from the C ABI call inside `resume()` - both well defined environments. Thankfully a CPU is not a magic box which means that suprises are inherently impossible here. * Context resumption with a callback (ontop): Same as `resume()` but here the user of the API can do things wrong by not catching panics for instance which can crash the program. Just use [`std::panic::catch_unwind`](https://doc.rust-lang.org/nightly/std/panic/fn.catch_unwind.html). Some people/projects (including coio-rs) do rely on undefined behviour at the moment though. For instance We unwind the stack from a ontop function. This works quite well at the moment, especially since Rust will probably adopt the C ABI in the future, but I think your argument is valid and a good reason for us to finally remove that last unsafe bit from our code and use e.g. flags or something instead. cc: /u/glasswings /u/dpc_pw /u/zonyitoo
I am not sure what exactly you are asking, but you may have mixed experience, because XP is [Tier 3 platform](https://doc.rust-lang.org/book/getting-started.html). Does it survive resets? Doubtfull, I suggest you upgrade to a more recent Windows version. Or try luck in /r/playrust ;)
Thanks! If you need some help or tips, shoot me a pm! I idle on #rust* channels on IRC, also. :)
How do you manage to create raw storage suitably sized and aligned for an arbitrary type `T`? On github I got a hint that I could use `[T; 0]` to force alignment but I cannot manage to make `[u8; mem::size_of::&lt;T&gt;()]` work because the size is not const...
No. The right solution is in bluss' nodrop crate (an Option-like enum with `#[repr(u8)]` to defeat pointer optimization + some drop-preventing method).
Unfortunately, the `NoDrop` type has an overhead of one `u8` so it does not satisfy the constraint that `size_of::&lt;RawStorage&lt;T&gt;&gt;() == size_of::&lt;T&gt;()` in general (it does when there is tail padding in `T` but that is not something one should count on). The ultimate point of `RawStorage&lt;T&gt;` is to be able to separate the information of whether it's alive or not from the storage itself, for example, you could implement a: struct Bucket&lt;T&gt; { bit_mask: u32, array: [RawStorage&lt;T&gt;; 32], } where the `bit_mask` contains `1` in bit `i` iif `array[i]` is alive and do so compactly (only 4 bytes of overhead compared to `[T; 32]`) whereas with bluss' crate you would have at least 32 bytes of overhead, and worse if padding get involved.
&gt; but the rust complier says It's actually your linker (`ld`) that says that. It seems like it can't find the `cuda`, and `cudnn` libraries. [This SO answer](http://stackoverflow.com/a/10526145/1254484) explains where ld will look for libraries.
&gt; Did you click the synchronize button on the global queue i.e. /queue/all? Yes &gt; The synchronize button on the per repository queue does work but the repository must be configured (+collaborator,+webhook) before you click it. Great! Now it works! &gt; My bot reacted to it, but doesn't seem to be testing anything... I deleted the previous `try` comment, and suddenly [homu reacted](https://github.com/critiqjo/cmark-hamlet/pull/1#issuecomment-215963388) and did a branch and merge, _but_ the name of the new branch is "try", not "auto". So Travis is not testing it. (Are there any other branch names such as this?) Thanks a lot for your reply! _Update:_ Home commented "Test successful" [twice](https://github.com/critiqjo/cmark-hamlet/pull/1#issuecomment-215964100). Any idea why? From the past pending state?
&gt; I deleted the previous try comment, and suddenly homu reacted and did a branch and merge, but the name of the new branch is "try", not "auto". So Travis is not testing it. Oh, that was the issue. Thanks for pointing it out I didn't noticed. +1 &gt; Are there any other branch names such as this? No, that's all of them AFAIK. I'll update the guide with all the info I learned from this exchange. Thanks!
&gt; Home commented "Test successful" twice. Any idea why? &gt; From the past pending state? It could be that, yeah. It could also be that Homu needs to do a temporary commit (when you are not using the local_git feature) so Travis does two tests per "try" command and Homu is reporting the success of both tests. That would be a bug because Homu shouldn't report the result of the temporary commit. EDIT: Or! You could have registered the webhook twice in your repository. This seems less likely though.
What would be the best crate to create a simple instant messenger with? Hyper? or would Iron work, or is Iron just for web based stuff? I plan on making it very simple like all I want to do is have the server running and 2 different clients from the command line. When you write 1 message from client1 the server takes the request and sends it to client2 and vice versa. 
No, from the timing of comments, I don't think that's the reason. See [my other comment](https://www.reddit.com/r/rust/comments/4h26o5/homuonheroku_how_to_deploy_a_homu_instance_to/d2nay6t).
That sounds like a bug. Probably Travis sends back two similarly looking webhook payloads (one for Build jobs and one for Allowed failures) but Homu doesn't know how to tell them apart. Note that the original barosl/homu repo is currently unmaintained, and the Heroku app is using the servo/homu fork. So you may want to file a bug report against the later.
Thanks. That's really clear, especially about the user's obligations.
I recently tried with Iron. I wouldn't recommend it unless you are comfortable with Rust and figuring out things yourself. I think most of the features are there but finding how to use them was a mess. 
:) This is the wrong subreddit! :D Head over to /r/playrust
Can I use this one ? The text is very nice, and way better than what I can do not being a native speaker !
You don't need the type parameter `T` in this case. If you remove `T` and add `where I::Item: Eq`, that works too.
I'd agree, in debug mode. But in release mode, I think enabling such optimizations could be a part of the solution to reducing the overhead of overflow checks.
Feel free. I give all permissions to you and everyone else.
Question, why to write your own OpenGL backend depending on the low-level [gl](https://crates.io/crates/gl) instead of building luminance on top of [Glium](https://github.com/tomaka/glium)? Glium is stateless (emulating it on drivers that don't implement [direct state access](https://www.opengl.org/wiki/Direct_State_Access)), so I think it would make your life easier. Also: if you used [gfx](https://github.com/gfx-rs/gfx) instead, you could target DirectX as well (and in the future, Vulkan and other APIs). Gfx is stateless too. Only trouble is that the documentation is a still sparse, and the type-level machinery is a bit intimidating. (Glium at least has [this excellent tutorial](https://tomaka.github.io/glium/book/index.html))
So it turns out you can specify a default target in `.cargo/config`, according to https://github.com/rust-lang/cargo/issues/2332. But I still don't think that lets me change the target according to a feature. 
XP = Experience
This is not quite true. It only generates documentation for publicly visible items. Things in a binary are generally not marked `pub` at the top level, because there's nothing that will be importing the code for which public visibility would be necessary.
Awesome! Great to see this sort of foundational UI work. Your use of operator overloading is quite creative here.
Yes, this looks very good. If I end up needing something like this, for example to manage panes in xi-editor, I'll definitely consider it carefully.
It definitely does, and I think that's an important thing.
That's fine, that's what the C compiler's optimizer is for! :) "Poorly optimized" could also tend to mean "looks more like the input", which might make the translation easier to reason about (a la TypeScript).
It does matter because the conversion to C is lossy: things that may be obvious to an optimizer running off the MIR, such as pointer aliasing, etc. can be impossible for even a very good optimizer to figure out from the C code.
It's definitely possible. One big caveat is that you have to work around C undefined behaviors that don't have Rust equivalents, such as signed overflow and strict aliasing, but this can be done either by using verbose workarounds in the C code (temporarily cast to unsigned before doing signed arithmetic, memcpy rather than direct pointer dereferencing, or compiler-specific attributes), which should still be optimized into efficient code in most cases, or by requiring the code to be compiled with special options (-fwrapv -fno-strict-aliasing). Another is that by the time you get to MIR you've already dropped the structured control flow (if/while/match/...) in favor of a raw control flow graph (goto) - and MIR optimizations may have modified that graph - so you have to either try to algorithmically reconstruct it (similar job to a decompiler), or just emit ugly C code that uses a ton of gotos. This is only a problem if you want to read the generated code, of course; the C compiler doesn't mind gotos.
[The author posted it here](https://www.reddit.com/r/rust/comments/4gz7ir/porting_a_haskell_graphics_framework_to_rust/)
Hah, I didn't even notice that until you mentioned it.
&gt; My point is that Vim already has features people want but they do not understand them. Sure, this happens. But you haven't brought up anything new to me and yet I'm still interested in multiple-cursors, because I think they are worth checking out. I never stated they *are better* because I've never used them yet. &gt; Also I do not understand in what use cases multiple-cursor is superior This has been pretty clear... As I said, I don't have the experience to come up with great examples, I'm just interested in the possibilities. Try watching a couple GIFs of people using multiple cursors in vim-like editors and using a bit of imagination. &gt; Multiple cursors are handy in Emacs-like editing style, not mode-like. You're simply wrong here. Multiple cursor implementations in editors like Kakoune and Vis don't have multiple cursors as a mode, but let you switch between modes freely while still editing in multiple places, so you still have the full power of your editor at hand. It's as useful for a modal editor as it is for emacs. The only point *I'm* really trying to make is that, yes, Vim can do a lot of amazing things, but it's obviously not perfect and I'm always on the lookout for potential improvements.
&gt; In my dreams, this would even allow a single C output that can be both 32/64-bit compatible, since it would hook in prior to any LLVM optimizations that have made assumptions about sizeof(void*) This will not be possible. I think rustc from early on uses information about `usize` size.
&gt; why you find it superior lol, please, I just finished saying "I never stated they are better because I've never used them yet." I'm just open to the possibilities. As for using multiple cursors in Vim, from what I've researched so far, the Vim implementations are fairly ham-stringed in what they can cleanly implement. Perhaps the Kakoune and Vis implementations have more to offer. I've been using `*`, `gn`, `.`, macros, etc for years. I'm just open to new ideas. I'm not asserting that new ideas are automatically better.
I wonder if there's some other event I could use instead of hover, of if needs to be rethought for mobile.
Hilarious! Top notch humor, here.
/u/pcwalton has strong feels about compiling to C...
Given that you'd still have to port the standard library, which is basically what you need today, I'm not sure it would be easier for platforms LLVM supports. For ones it doesn't, though...
You probably want /r/playrust.
&gt; Sorry, I missed that. Fair enough. I can definitely imagine that `vim-multiple-cursors` is more trouble than it's worth. I've taken a couple looks at it but it seems too bolted-on to Vim to really integrate the way I would want. And as for Sublime Text, not having a powerful vim-like command language makes it a pretty dull proposition for me. And I agree that Vim has plentiful features that make multiple-cursors and macros unnecessary 95% of the time. But when I do have to reach for macros, I think multiple-cursor live preview could actually make a big difference. Anyway, Vim might not be the editor where we'll see a great multiple-cursors implementation any time soon, but I love to see people experiment with editors like Kakoune and Vis which try to integrate multiple-cursors as powerfully as possible. Experiments are great. Sometimes the result turns out not to be useful, but with enough experimentation we end up finding better ways to do things.
But OP is welcome here. We don't bite...... much. 
This isn't the most substantial post. In fact I'm mostly posting it as a way to force myself to commit to this project! It's fairly big but something I have been wanting to explore for a while. This post is about convolutions in general and touches on how they tie into deep learning. Hopefully in the coming weeks I'll explore implementing convolutional neural networks natively in rust (a frankly stupid, but very exciting idea). A big thank you to the Piston Developers who's [image](https://github.com/PistonDevelopers/image) library made this work a lot easier!
The comparison operators return `false` when there's no defined ordering (ie. when partial_cmp returns `None`). This is because when comparing, there are four possible cases: 1) A &lt; B 2) A &gt; B 3) A == B 4) A != B, but there is no defined ordering between them For equality, there are only two possibilities: 1) A == B 2) A != B 
Check out: http://playground.tensorflow.org
&gt; go nuts So... compile to Go?
Would it be possible to do much in the way of concurrent/parallelised output? Haven't really looked at the Cassowary algorithm all that much. Would be interesting to see a UI toolkit that also hooked into [webrender](https://github.com/servo/webrender/).
Thanks to you I discovered it was far more stupid. I just don't have an nvidia GPU... Is there a way to tell to cargo "I don't have this GPU just ignore the calls to CUDA" ?
Wouldn't it be easier to port miri to C and run that?
Hey, they compiled the compiler from C into Go to make the language self hosting. Why not?
The idea is to build a MIR interpreter (rather than a compiler), because while slower, it's easier and would still be sufficient to bootstrap (as long as there's a LLVM target).
There are a lot of different things that would have to happen to make that feasible. Firstly, there is currently no MIR output from rustc that could be used as input to an external interpreter - you have to literally link to librustc and use the internal MIR data structures like Miri does. Defining a serialized format that could be re-parsed by non-rustc programs is a big job in itself. It seems like a lot less work to bootstrap to a new platform by defining a new LLVM backend (if one doesn't already exist for the target) and doing the necessary standard library work. Then you could use rustc on an existing platform to cross-compile rustc for the new target platform.
What determines what type a variable will be if I don't specify the type explicitly? In this piece of code a becomes i8 because of the comparison with b. let a = 1; let b: i8 = 1; let c: i32 = 1; assert!(a == b); assert!(a == c); // error
This is cool to see. I am also interested in what sort of UI library you're making. 
Good point about perf script, there is also perf report -D which prints an ascii representation of every sample in the perf file. When I did my initial investigations both approaches had their limitations, but I guess in the end curiosity about the binary file format was also a big part. Speedwise I think shelling out would have been viable. The most costly part for me currently is resolving instruction points to function names.
I think /u/killercup had some PDFs, too.
Really really nice. I remember doing some "mechanics" in high school were we would be using SolidWorks or AutoCad to model the pieces we were working on and the constraint solving approach for specifying the relationships between the various pieces was really powerful. When I discovered HTML and CSS afterward I was like "Wut? That's horrible!" :x
I have taken some level of concurrency into account when designing this library. In fact the main differences between this implementation and kiwi, the implementation this is based on, are due to this. cassowary-rs allows concurrent construction of constraints, which is not safe in kiwi. In the future it should also be possible to parallelise some of the internals of the implementation, though I'm not anticipating them being a bottleneck in typical use cases. Regarding parallelised output, I would need a bit more clarification on what exactly you have in mind. Once `update_variables` is called, immutable reading of the variables can be done from multiple threads (using some form of scoped access). I expect however that you mean something more than that. webrender is interesting, though the documentation is lacking and I'm not sure of its platform compatibility or capabilities. At the moment I'm in the process of writing a UI backend targeting Glium, with plans to expand to gfx-rs and vulkano. Another one on my list is a native Win32 backend.
super awesome!
Thanks! I'm glad that I've managed to come up with some syntax that isn't too clunky, but I wish that the comparison operators in Rust weren't tied to the `PartialOrd` trait. I'm sure other DSL use cases will come along that would greatly benefit from being able to use those operators.
Hope it can get resolve so I can read it on kindle . Thanks for working on it :-)
I really like the proposal! Finally something concrete
Yeah, I didn't update them in quite a while. It's really nice that you did this, though! I would love to have them up-to-date more often.
No. It assumes that you only have a single member that's a borrow of another one, and is the opposite of convenient to use. 
Is there a collection of links to accepted RFCs (with implementation status) somewhere available? Reading the discussions in those was always worth it.
Is MIR pre-monomorphization, post-, or both?
&gt; No way to store an object and its borrowing in the same struct. "No way" is also not true. It is possible (http://is.gd/5P8iis) it's just not convenient.
Flexbox is nicer at least. (Thankfully we have been given license to use it on our web-app's font-end).
&gt; https://github.com/rust-lang/rfcs Similar but not really. I always found clang's C++ status page useful to keep up with the language features that a particular version supports: http://clang.llvm.org/cxx_status.html It is just a small table with: - the names of the RFC and a link to it - the status of the implementation (not implemented, work in progress, first appeared in release x.y, not going to be implemented). For some features that make it into a release partially implemented it says which part of the feature is finished. For rust something like this should also consider stabilization and distinguish between nightly, beta, and stable.
The question is, why? What are you hoping to get out of such a backend? Are you hoping to get readable C code that replicates what Rust is doing? That's not really going to happen, MIR doesn't really represent any kind of idiomatic code that you would write. Are you hoping to target more platforms that LLVM doesn't yet support? How many platforms are really of interest, and don't already have work in progress to support them? The two that I can think of that I've heard any interest about recently are AVR and asm.js/WebAssembly, but there's already some work in progress for those, so it seems like it would be easier to just finish up those efforts rather than entirely replacing the LLVM backend with a C backend. In the abstract, sure, it's possible to compile from MIR to C; it would also be possible to compile from HIR to C, or directly from Rust to C, if you put enough effort into it. C is a Turing complete language, you can always do it somehow. But just because you can do it doesn't mean the result would actually be useful, and describing how feasible it is to produce a useful result will depend on why you are interested in doing it. So, why is it that you dream of a Rust to C compiler?
You can implement pointer-to-member types with a macro, which works quite nicely: http://is.gd/oHQrzG
I prefer CSS to constraint solving systems, actually: I've never actually been limited by what CSS supports, if you count flexbox, and documents really want something with flows and floats. But it's a matter of opinion, and I don't want to derail your great work here. :)
Definitely. At the moment the signal graph propagation relies upon there being only one output signal per graph (though of course interior signals can have side effects), so there would need to be work done to support multiple output signals efficiently (basically so that they have as few dependencies between their subgraphs as possible). Then it would be possible to use one "runtime" managing the signal graph(s) to orchestrate everything. At the moment my focus is just on UIs, but use for general game events and control flow, including stuff like timed and delayed events for scripting purposes, is on my list. Note that Elm's FRP semantics rely upon a signal graph not being modified/extended once started (otherwise it raises some hairy questions), so this may rule out some use cases. In terms of concurrency the "runtime" is given at least one dedicated thread for processing signals. For this reason it won't work as is in scenarios where you can't use multithreading (e.g. on the web via emscripten, without HTML5 web workers), but the dedicated threads are just a design choice - an event loop on the main thread could be used instead.
Would love to, but it won't install stable channel builds for me. I've opened https://github.com/rust-lang-nursery/rustup.rs/issues/381 about it, but so far no attention.
How are you planing to do the graphics? I'm partway through writing a chip-8 emulator in C (with SDL) and plan on doing the same in Rust afterwards, but I'm not sure of the best way to tackle the graphics.
I can see where you're coming from, and I am considering a hybrid approach, as I do appreciate that constraint solving has its limitations (without going full mixed-integer - I wish I knew how to code a fast incremental mixed integer solver - that really would solve everything, even flow). I'd like to support UI elements that at least partially control the layout within them to perform stuff like flowing elements and wrapping text/inline elements around others. This gets tricky when supporting elements like this that have a dynamic size, but I've got an idea in mind for resolving the interactions between this and Cassowary. Still yet to test it. Current web app I'm working on is practically screaming for constrained layout - it really is nothing like a document - nothing needs to flow or float. Lots of use for vw and vh. And it's got to work on IE9. Oh, and thanks for the compliment!
Any chance you could add comments to this, to explain how it works?
&gt; What determines what type a variable will be if I don't specify the type explicitly? Hindley-Milner type inference plus the constraint that types must be declared in `fn` signatures. This is similar to most functional languages... http://akgupta.ca/blog/2013/05/14/so-you-still-dont-understand-hindley-milner/ ...except that the compiler isn't allowed to make as many implicit casts. Without that freedom it can't make both comparisons valid, so it satisfies the first and errors on the second.
Hmm, that's a bit hacky but why not. I'm probably going to provide a macro for `BufferSlice` itself without caring about being generic. Thanks for the idea.
https://github.com/nickel-org/rust-mustache I use that and it's quite nice.
Diesel supports sqlite? Awesome, so I can use it for integrating newsbeuter into [imag](https://github.com/matthiasbeyer/imag)
(Note: the unsafety can be avoided with a `Default` bound on self, and using `Default::default()` in place of `mem::zeroed()`)
Thanks!
&gt; No way to store an object and its borrowing in the same struct. I ran into this as well when trying to use capnproto as an in-memory representation.
It is possible, but after you've done that, a lot of other things become impossible, because the struct is locked "for life". You can't, e g, have a `Test::new() -&gt; Test` (that sets the reference up).
This page has fallen behind a little. If you are aware of new papers about Rust, please submit a PR.
Flexbox completely solves the vertical centering problem (and Absolute Centering basically does too).
I don't really understand why an object and its borrowing need to be in the same struct. Could someone give me a concrete example ?
Yeah, I admit that that problem is basically solved by now. I don't have any good examples anymore, but what I'm trying to say is that stepping outside the document model still feels like fighting the system, and it seems like /u/ZRM2 has had about the same experiences.
Well, I do often try writing things like let mut nums = Vec::new(); nums.push(nums.len()); and making the borrow checker yell at me. It's not a fundamental limitation of the language, but it is kind of surprising that it doesn't work. I can't think of many other situations where let value = func(); other_func(value); is substantially different from other_func(func());
this code: fn main() { {40}+{40}; println!("hello"); } 
Alright, I am definitely surprised. Now why does this happen?
I've wrapped this functionality up into a crate: https://crates.io/crates/field-offset I also fixed the unsafety due to Deref by using destructuring instead of field access. Unfortunately this means that for the moment it won't work on tuple structs. The macro now supports nested fields to arbitrary depth: ``` let foo_bar_x = offset_of!(Foo =&gt; bar: Bar =&gt; x); ```
What's going on there? `{40} + {40}` even works as an expression, but not a statement. Judging by the compiler error it must be an oddity of the grammar but I have no idea what specifically.
It's because every block expression, when appearing in statement position, is parsed as a statement. If the compiler didn't do that it wouldn't be able to distinguish the (impossible to write) expression-statement ``` if x { ... } else { ... } + 0 ``` from the statement and subsequent tail expression if x { ... } else { ... } +0 (edit: this is definitely one of the most unfortunate corners of the parser).
There is a bug in your code: the result of `mem::zeroed` should not be dropped, you need to call `mem::forget` on it to avoid running the destructor with an invalid value. I also wrote [something similar](https://github.com/Amanieu/intrusive-rs/blob/master/src/lib.rs#L229) for my intrusive collections library, but I went with a slightly different approach (I need reverse pointer-to-members to get a pointer to an object from a pointer to a member).
I'a working on TiKV project. 
Doing more hacking on telemetry in rustup. This week I'm trying to get terminal highlighting behavior consistent and also collecting rustc version so we can add that as an additional metric.
Working on servo while learning the basics.
I'm writing a blog post about creating a 64-bit higher-half kernel. I was inspired by Phil opperman's blog on the subject of a 64-bit Rust kernel. Higher-half 64-bit kernels are something I'd never seen written about so I thought it would be useful to have some resource out there for others.
Thank you very much! I think I will need a bit of time to consume your point. The idea behind the double indirection is that the writer doesn't maintain a tail pointer. I agree, the reader doesn't need its own buffers. It was just easier for me to reason about this problem (the first time in my life). Is there a pointer you would recommend about this?
judge from what i see with this code, it work with nightly? since its is an inner attribute(s) that one enables a feature for it? Just to make sure...
I'm not sure what you mean, but it works on stable.
Unfortunately I don't know of anything that documents all this stuff, or any good, readable implementations. Linux kernel kfifos do claim to do lockless single producer/consumer, but I wouldn't call the code particularly readable nor is the design documented. Might be worth looking at, but it's using every trick in the C book... https://evilpiepirate.org/git/linux-bcache.git/tree/include/linux/kfifo.h https://evilpiepirate.org/git/linux-bcache.git/tree/lib/kfifo.c The kernel does have an implementation of lockless singly linked lists that's pretty readable and has useful documentation, though: https://evilpiepirate.org/git/linux-bcache.git/tree/include/linux/llist.h https://evilpiepirate.org/git/linux-bcache.git/tree/lib/llist.c Also, does Rust even have memory barriers yet? Because you do need those...
 struct MyStruct {} Thankfully it's fixed! :)
This is something that's got me in the past, and I worked around it using `let value = func()`, but I still don't really understand why. `len` just returns a usize. What's the major difference between it being an expression in the function param and it being kept in the outer scope with `let value`? Shouldn't it just copy a usize by value into `push`, and `nums` own a usize? I never understood what's going on here. Does it have to do with the lifetime of `nums` existing in the outer scope and the lifetime of the usize returned by `nums.len()` ending after the function is called or something like that?
Happened after changing "&gt;" to "&lt;": https://play.rust-lang.org/?gist=c46ab36af474afbb1973bb59f1174563&amp;version=stable&amp;backtrace=0
This compiles #[derive(Clone, Copy)] struct Coord { row: i32, col: i32 } fn call_it&lt;T, R, F: Fn(T) -&gt; R&gt;(f: F, value: T) -&gt; R { f(value) } fn foo1(a: &amp;Coord) { let cap = call_it(|x| x.col, a); } fn main(){} This doesn't compile fn foo2(a: &amp;Coord) { // error: the type of this value must be known in this context let f = |x| x.col; let cap = call_it(f, a); }
There is a chapter in the [book](https://doc.rust-lang.org/book/enums.html) and also in [rust by example](http://rustbyexample.com/custom_types/enum.html)
Could this complexity be fixed in a Rust 2.0?
Why doesn't the type require a `::&lt;` like it does for function calls?
Well, `gfx` is kinda targeting the same niche. Have you said â€œUse `gfx` instead of writing your own graphics framework`, itâ€™d have broadcast the same idea. Also, I *really* donâ€™t give a shit about DirectX. Sorry. Proprietary technologies are out of my scope ;).
Oh god this. let cpu = self.cpu; self.renderer.with_lock_mut(|r| r.draw(cpu.get_screen())); If I don't bind self.cpu to cpu, then the whole of self is borrowed by the closure for the rest of the method. It's so stupid.
It's pretty hard to find, to be honest. How does one actually get to the forge from the homepage?
That code has an additional layer: creating a closure that mentions an external variable usually means storing a pointer to that variable in the closure. The closure in that case is a lot like a struct `struct Closure&lt;'a&gt; { cpu: &amp;'a CpuType }`, and the call is like `self.renderer.with_lock_mut(Closure { cpu: &amp;cpu })`. However, if you use `self.cpu` in the closure instead, the variable that is being captured is `self`, so the struct is more like `struct Closure&lt;'a&gt; { self_: &amp;'a Self }`, and the call looks like `self.rendered.with_lock_mut(Closure { self_: &amp;self })`. This falls afoul of the borrowing analysis, because the compiler can't tell that `self.rendered` won't be touched by just looking at that call site (which is what the borrow checker sees, AIUI). Theoretically, it seems like this would be most easily solved by making the compiler smarter about borrowing when only some fields of a struct are used, allowing the code as written to compile to the first case (just borrowing `self.cpu`). However, this is different to the solution of the parent comment.
I think the problem would be your compiler would not magically pick up the host machines architecture, so sure you'd have rustc available on your pdp11, but it would still make x86-64 code....
Despite grammar difficulties, I personally think the current behaviour is the right choice. Yes, it does make the grammar more annoying, but going the other way would make writing code more annoying, moving busy-work from computers (and a small-ish bunch of compiler/tool writers) onto all Rust programmers. In particular, consider: fn foo(x: &amp;mut i32) -&gt; i32 { if foo { *x += 1 } *x } If there wasn't special-casing for statements followed by binary operators, the compiler would parse that as a multiplication `(if foo { *x += 1 }) * x`, generating a type error (and, there are situations of this type where code compiles incorrectly). (I could be convinced otherwise, if the grammar difficulties were particularly egregious, but I think they'd have to be pretty bad.)
You mean `expression as::&lt;type&gt;`? I've got no idea if it has ever been considered. It also looks quite out-of-place to me.
Hmm, how come the gist has comments from 2015 while the Vulcan spec was released in 2016? 
I cannot answer (yet?) since I donâ€™t really know `glium` nor `gfx`â€™s full scope. `luminance` still has the concepts of buffers, framebuffers, but is heavily type-driven.
Thank you for the pointer. I will have a look into it. I have few thoughts about loosing messages. The first is, whenever the receiver is slower than the sender, we need to make a decision. Either increase the message queue, which in the Erlang world can lead to crash an otherwise very reliable VM, or block the sender, which depending on the system may lead to the case when the slowest part determines the entire throughput. So for me this is a question of compromises. My feeling is that the land of the two options above is very well explored, so this other way makes me interested in loosing messages. The other interesting point came from reading "Designing Data Intensive Applications" from Marin Kleppmann (http://shop.oreilly.com/product/0636920032175.do). He argues very well that in a networked system it is impossible to tell if a message gets delivered or not. Moving this thought further, I think it is interesting to see that at the application level most libraries make lots of effort to guarantee delivery within the application. Plus when the application talks to another component over the network, libraries aims at the same guarantees. I am interested to look at this problem the other way around. Let's accept that networks are unreliable and make the applications unreliable too :) . This sound like a joke, but I am serious. I'm interested in what algorithms and solutions would work in this setup. Sorry for the long blurbing...
Finally working on writing an RFC for regex 1.0!
This one bit me before: let v = vec![1, 2]; for x in &amp;v { println!("{}", x == 1); } Seriously? I can't compare integers? The fix: for &amp;x in &amp;v {
Closures are the only place where Rust does cross-function inference and the only way it actually works, sometimes, is that we don't have *just* HM-like type inference, but also downward "expected type" propagation, so a closure within a call will most of the time have enough type hints to work. The type-checker can be potentially rewritten to make it order-independent, but it may have an impact on performance. It's a lot of work either way, so it won't happen any time soon, not AFAIK.
Worth reading: https://caremad.io/2013/07/packaging-signing-not-holy-grail/ 
Still working on [Serkr](https://github.com/mAarnos/Serkr), although I haven't done much the last two weeks due to other things. Around two weeks ago I implemented a new heuristic function for control of the proof search which works really nicely (except with some specific problems). Unfortunately, it exposes quite nasty issues with the efficiency of some of my algorithms (does O(n! * 2^(n)) with a very small n, usually, sound good to you?). Luckily there is a nice solution called indexing which deals with this, mostly. Now, I was going to write a nice and long post explaining it, but before I could post it I accidentally closed the tab which caused me to lose everything. I can't be bothered to write it again, at least today, but let's just say that after implementing that the prover should be several orders of magnitude faster.
&gt; Despite grammar difficulties, I personally think the current behaviour is the right choice Agree 1000%. I really don't think it's worth "fixing".
Vulcan being a lot inspired from mantle, I guess u/tomaka17 was extrapolating how it would look like
I'm still working on [clippy](https://github.com/Manishearth/rust-clippy), [metacollect](https://github.com/llogiq/metacollect) and a few other projects. Also I'm trying to find out why my benchmark lets me iter + map + collect an `ArrayVec` (incidentially crate of the week) in 0ns. I'll probably just [publish](https://github.com/llogiq/arraymapbench) it and ask around. Furthermore, I'll give yet another talk at our next [Rhein Main Rust Meetup](http://www.meetup.com/de-DE/Rust-Rhein-Main/events/230396961/), which I have to prepare. Finally, I'll probably continue to lint various projects and send PRs if time permits.
It seems you've used the wrong syntax for linking a word with reddit. Try: \[Word\]\(http://link.com) instead. :) \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\- *^^I'm ^^a ^^bot* ^^^[Contact](https://www.reddit.com/message/compose/?to=bsmith0) ^^^| ^^^[Code](https://github.com/braeden123/MarkdownFixer)
Sure, if the answer is just for fun, then yeah, MIR might be a good level to attack it at, or even re-doing the LLVM IR to C compiler could be feasible. If it's just for fun, I wouldn't necessarily hold my breath that someone else will do it, but instead probably try to dive in an start working on it; just like someone's working on [miri](https://github.com/tsion/miri), the MIR interpreter.
As always the clarity of Niko's writing is impeccable. Can't wait for the next part(s)!
Can [Rayon](https://github.com/nikomatsakis/rayon) threads be mixed with non-Rayon threads? I would like to structure my app as one producer and one consumer (P and C), but each could be parallelized. Rayon would be a great way to add parallism inside P and C, but `rayon::join(P, C)` would not guarantee that P and C will run concurrently, so P could deadlock. Similarly, can a crate use Rayon internally when other crates are using other threading or even also using Rayon?
This has the same `Deref` bug as my original code had: if $T implements `Deref` and `$field` does not exist on $T, then you'll run the `deref()` implementation on uninitialized memory, and then take the address of some random location on the heap. Also since it just returns a number it can't be used safely: the point of the `FieldOffset` type is to provide a safe API for users. It also doesn't provide a way to get the offset of nested fields without multiple macro invocations.
Yeah, I find diagnostics bugs to be a very fun way of getting involved in the codebase since they're usually pretty straightforward but are very useful once solved -- I often use them as a source of E-easy bugs. Also, I love having a helpful compiler. Thanks for doing all this!
[Related XKCD](https://xkcd.com/1656/)
[removed]
Wow! Awesome, thank you!
&gt; we talked about hosting cratesâ€™ documentation directly on an official doc website I'm very excited about this. Great to see it starting to move forward again!
https://github.com/japaric/criterion.rs Iirc it has the ability to save results and warn you if something slows down too much. It provides KDE charts too. http://www.serpentine.com/criterion/tutorial.html 
You Ã§ant! It's still pretty half baked. I should clean it up and make sure it's linked in places.
Thank you for doing this. Shows what I've told people for some time â€“ clippy is the ideal entry gateway drug to compiler hacking :-)
Yes, this is the rustic way to re-export items from submodules. You could shorten it to pub use cat::{Cat, SCRATCHY}; pub use mouse::{Mouse, ITCHY};
Curious what /r/rust thinks about the proposal.
Actually yeah, that made a lot of sense. I'm not sure what I would use them for, but I think I get how it works. I think the syntax kind of threw me off, because it can look like a function call when assigning to a variable. Can you store functions in enums?
There was some discussion in a vulkano issue about this, [where I give another example](https://github.com/tomaka/vulkano/issues/20#issuecomment-211735905).
I'm not sure I like the idea here. Part of the reason I'm not sure I like it is because I think the cost might not be unoptimizable (so it may not be so much of a hidden cost). For example, in Java and Javascript, everything is dynamic dispatch. Yet they can get at or near C/C++ speed because they can collect info on the current types being used and jit in a better version of the function that assumes the type for a method is something more concrete. I've even heard talk of C++ doing a similar sort of optimization. The compiler may have enough information (or it may be explicitly stated) to duplicate a method removing the vtable lookup and then optimizing from there with all the necessary inlining needed. Finally, while dynamic dispatch can be expensive, often it is something that will end up in L1 cache. It isn't too often the case (granted, it can happen) that you are dealing with, say, a collection of dynamic dispatch things that are all different things. It is more likely that you are dealing with a mostly homogeneous collection of dynamic dispatch things. In that case, you will likely have a good cache usage. It may be slightly slower than direct dispatch, but not extremely so (I say this without benchmarking anything :) ) The costs are low, probably much lower than memory allocation. The change may be a bit confusing as there is now yet another keyword to remember. Deprecation tends to muddy the language, I like to avoid that. And it may not be such a cost to matter all that much. Just my $0.02
Yes, though if we do this, it won't be on crates.io itself, it will be on another domain, for XSS reasons.
That makes sense! Two related things: first, types use `CamelCase` and functions use `snake_case`, so you can usually tell them visually if you're following conventions. Second, if you use the variant name without the `()`s, it actually _will_ give you a function that you can call. So your instinct was kinda right. You can store functions in enums: enum Foo { Function(fn(i32) -&gt; i32), Closure(Box&lt;Fn(i32) -&gt; i32&gt;), } fn lol(x: i32) -&gt; i32 { x + 1 } fn main() { let function = Foo::Function(lol); let closure = Foo::Closure(Box::new(|x| x + 1)); call(function); call(closure); } fn call(f: Foo) { match f { Foo::Function(func) =&gt; { println!("found a function!"); let answer = func(5); println!("the result was: {}", answer); }, Foo::Closure(closure) =&gt; { println!("found a closure!"); let answer = closure(7); println!("the result was: {}", answer); }, }; }
If the RFC passes, it will go to `rust-lang`. Technically, there could be a `0.2` release before `1.0`, we'll see.
What is non-lexical may occasionally borrow. 
I'm curious too. Been working with MSYS2 and it's mostly effective at making me homesick for a top-to-bottom Unix-like. (At least it has Emacs...)
Have you been able to get local values working?
strongly agree with this - although unlike brson I find rust doc to be sufficiently heavy as to be a little painful to use (and would therefore probably find the ddg feature more valuable)
Rayon uses `std::thread` under the covers, so it's the same kinds of threads as everything else.
It worked out of the box for me, at least on the latest nightly of rust.
And it's a dup of one I filed and forgot I filed. :/
In OO, one would use another pattern instead: the enum becomes a base class, and each variant is a class that inherits from the base class. So we could have a class `MaybeInt` that has two classes `No` and `Yes` that inherits from it -- and the `Yes` constructor takes an `i32` as parameter. Then, you can pass either to a function that accepts `MaybeInt`. The difference is that Rust enums use static dispatch and OO inheritance would use dynamic dispatch. Rust has another feature to provide dynamic dispatch too, but by using [trait objects](https://doc.rust-lang.org/book/trait-objects.html) - with it, `MaybeInt` would be a trait, and `Yes` and `No` would be types that implement this trait. Also note that Rust enums are kind of similar to C unions - except they are type safe. You could call enums as *tagged unions*, because they store a tag alongside the other data (to distinguish between variants).
`expression as(type)` looks only a little funky but its a lot easier to parse. can also enforce `expression as type` as a warning/error, it should always be surrounded by parens.
Another option: fn main() { let v = vec![1, 2]; for x in &amp;v { println!("{}", *x == 1); } } 
I'm not seeing the real advantage here. Building up an awareness of how function calls and traits work is just a very fundamental part of learning rust, and it falls out pretty naturally once you have a mental model of what a trait is and what information is available to the code at compile time. (Things you'll want to know anyway.) If someone said to me "you've got to stick a `dyn` in there for that to work", I'd be thinking "Well, why? Clearly there's not enough information embedded in this reference to do static dispatch here, so what else could I be trying to do?" And then I'd be confused about what parsing ambiguities required a dedicated keyword to solve... Which is to say: the *real* problem is finding a good way to teach people how dynamic dispatch works, not needlessly inhibiting them because they accidentally wrote non-performant code. A warning should suffice. Some good advice should suffice. But preventing compilation? Because the code is mildly inefficient? No thanks. --- Besides, what if someone finds a compiler optimization that allows for static dispatch where dynamic is currently being used? Does the `dyn` keyword *prevent* this optimization from occurring? Does it force dynamic dispatch where it might not be needed?
I mean without context, let f=|x|x.col compiler can't infer the correct type of 'x', just know x have a field named col; but let cap=call_itï¼ˆ|x|x.col,a) is a fun call, compiler can infer the type of 'x' from function parameters. As for whole thread inference ,I think rust compiler can't do that yet. 
It's disappointing how old that is, but somewhat encouraging that it has been discussed through Feb of this year.
Does rustc produce reproducible binary builds? Meaning, if you and I both build the same source for the same architecture, do we get the same binary with the same shasums?
Thanks for the comments! Makes sense, I'll tweak it and see if any benchmarks change (I should also make sure it still compiles :) ) &gt; Your try_push() definition seems... odd to me (returning the value if it wasn't pushed) It's been quite a while, but iirc the rationale for this was ownership driven. The collection has to take ownership of the value to insert it into the queue. But if the insert isn't possible, the caller will likely want to try again. Which means ownership of the value needs to be returned from the non-blocking `try_push()`. Also, if `T` is a large, non-trivial value (e.g. a chunk of memory, not just an integer or something cheap) the caller may wish to re-use that memory rather than letting it get de-allocated. At least... I think that's what was going through my head at the time :)
What parts of rendering does Stylo perform? There's a lot of activities between loading CSS/HTML and actually painting the page.
Yeah, I mean, to be clear, I would like to see more stuff going on in this space. I only mean that it's not simple.
We're close, but not perfect. There are a number of factors at play here; one is the unstable `const fn` which allows compile-time evaluation, which may cause differences between runs. In general, we go to lengths to ensure that you can compile with the exact same source; the lockfile, for example, that Cargo uses, is geared towards this. But that doesn't always guarantee 100%, same shasum builds. Such a thing is generally desireable though, and we've taken patches to fix determinism issues (like, there was a bug where symbol name generation was accidentally nondeterministic...)
The post says that scopes are always a suffix of a block, but what happens if the value is moved halfway through the block?
&gt; If the code is instead renderer.with_lock_mut(|r| r.draw(cpu.get_screen())); then later in the function I use self.foo, it still fails as the closure is holding onto self. I think this depends a lot on the exact signature of `with_lock_mut`, and how it uses/connects lifetimes. I personally wouldn't expect it to hold on to the borrow, but maybe it does. &gt; That's what OP is reffering to right? (The closure's borrow should have died after with_lock_mut returns). Not quite, the OP is basically the order of when borrows are taken in a single function call, while borrows sticking around for longer than one might expect is a different problem. This one (and solutions to it) are actually more intimately tied to non-lexical lifetimes than the plain old function calls of the OP.
Given that path-guided optimization is like JIT for AOT languages, has there been any success in improving devirtualization in C++ when paired with PGO?
Can someone elaborate on Stylo, and its intended role in Firefox?
That's a major part of PGO, yes. I once read a lovely mailing list post about an implementation for it with all sorts of neat information, but I don't seem to be able to find that again. Instead, [here's a discussion about it in D](http://johanengelen.github.io/ldc/2016/04/13/PGO-in-LDC-virtual-calls.html). It's not quite as good as a good JIT, but it gets you a lot of the advantages for not much of the cost.
Thanks for being so responsive. I'm comparing what Rust offers with what I'm used to with Java and Maven.
I've removed your post, as it doesn't belong in /r/rust (as in Rust the programming language). You may however want to repost in /r/playrust (as in Rust the game).
Iron is nothing to do with a database. Any caching you will need to do.
The integers are Z, not quotient rings/groups of it. ;P
Deep rework of my wayland libs. I discovered some soundness issues and inefficiencies, that I'll need to fix. In a general way, the fact that resources are shared between the client and the server causes a lot of problems to arise, regarding a proper handling of lifetimes. Maybe I'll write something about it, once I'm finished with all this rework.
Yeah, my next step was to switch from the GNU to the VS version and give that a try - do you know if it's possible to run the GNU and VS versions side by side?
Do you have a sample launch.json you could share for your debug configuration?
[removed]
:-)
This looks pretty interesting. Given that this is a port from Haskell, can I safely assume that it contains an immutable description of what is to be rendered? Asking, because I'd love to try using this in conjunction with purely functional reactive streamsâ€¦
Guessing it is based on the `unix` and `windows` directories?
https://github.com/danburkert/fs2-rs/blob/master/src/lib.rs#L4-L12 #[cfg(unix)] mod unix; #[cfg(unix)] use unix as sys; #[cfg(windows)] mod windows; #[cfg(windows)] use windows as sys;
Kind of. It has the concept of pure data to represent GPU renders but I had to cut things off. For instance, as I state it in my artilcle, Rust lacks some features Haskell has. For instance, **type operators**, which makes pure uniform updates cool. In the Rust version of `luminance`, uniform updates are performed via closures (`Fn()`), which I dislike because it was a *semimonoidal* computation in the Haskell version, which is way safer. But well, I decided that it wasnâ€™t that bad, and that I could implement something similar with macros later on. So yeah, in `gfx` for instance you have the concept of *pipelines*. I guess itâ€™s a bit similar to my `*Command` types. Iâ€™ll describe that in a next article pretty soon, but the idea is pretty simple: batched rendering for efficient resource sharing. :)
Z_n is a common notation for this.
I use [Sublime Text 3](https://www.sublimetext.com/3) with the following packages: * [Rust](https://packagecontrol.io/packages/Rust) For snippets and highlighting * [RustAutoComplete](https://packagecontrol.io/packages/RustAutoComplete) For autocompletion (Only on the standard libraries seems to work for me) * [BeautifyRust](https://packagecontrol.io/packages/BeautifyRust) For help with formatting When compiling I use [cmder](http://cmder.net/) and `cargo build`, debugging with print statements. I'm sure there's a way to set up a build system for Rust in ST3 but I've never bothered.
I use Rust stable MSVC ABI (64-bit), 1.8 at the time of writing. I used to use Sublime Text 3 with the RACER plugin, but it just wasn't good enough. When I upgraded to a newer Rust compiler, I forgot to redownload the source for its std autocompletion making me wonder why RACER sucked so hard... I've since switched to VS Code with RustyCode plugin and this works 'ok'. ~~I'd love to be able to mouse over variables and have it spit out the type (like how Visual Studio works in general). It's not quite 'there' yet.~~ EDIT: Looks like you need to manually update the extension. I've seen someone show how to set VS Code up for proper debugging but it requires mingw and gdb (both of which I have zero experience with) so I'm stuck with printf debugging... Would love a better (native) debugging experience :)
Have you tried using it with YcmdCompletion with a YCMD setup? github.com/valloric/ycmd https://packagecontrol.io/packages/YcmdCompletion Someone once mentioned that racer has issues on windows, but this was a long time ago. I'd like to know if YCM works and if not fix it. 
I tend to use Rust in a very functional way and have to say that this has never actually been an issue in practice. It is unusual to re-use a label like that and should hint at an issue with naming. Furthermore, this accidental mutability is limited to the local scope of some block, which should not be that large anyway. So essentially it is not a major issue in my point of view and I don't see much value in the rules you propose, especially since they break backwards compatibility.
The problem is that *you* tend to use Rust in a very functional way, your *colleague* however is an awful programmer who likes to subvert any styles and language guarantees and you have to read and understand his code.
This is easy enough if you use rustup. I use a similar setup locally. You can download from https://rustup.rs
This comes up every once in a while, and every time, I can really only say one thing: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;*I've never found it to be an issue in practice.* &amp;nbsp; Let me liken it to something only *vaguely* similar... You know what's great on paper? Explicitness. You know what actually *massively sucks* in practice? Having to manually remove layers of indirection with explicit `*`s. Life is *so much better* with implicit `Deref` chaining, like you *would not believe*, holy *cow* was that a pain in the backside.
Rust already chose to make mutability explicit, it's just that local variable shadowing removes much of the power. I feel it is a greater burden to the coder that they cannot express that a label will not change value as well as to the reader that they cannot trust a label will not change value. To me, losing that power is like loosing `Deref`. (Actually worse because `Deref` is just some nice syntactic sugar)
You can create an immutable wrapper type, with Deref but not DerefMut implemented on it, and use it whenever you want proper immutability. I don't think cases where a variable needs to stay immutable crop up much in practice. `mut` is mostly a lint, and it catches your eye, making things explicit.
Don't actually see this as a problem. Even in functional languages it's not uncommon to allow this. Read `let` as `new variable/value` and forget everything you know about similar named variables/values above that line.
I was looking for a way to do true immutability (ie `const` in C++). Does this have runtime overhead associated with it? Clearly that trick would never commonly be used in practice due to the LOC to express it. I tend to find that `const` is a good way to reduce mental burden as you can think of a variable as having a single value whereas in Rust you can only think of a variable as *probably* having a single value.
The scope may be a suffix of a block, but that doesn't mean the lifetime of the value is the complete scope. Or that's the way I think of it at least.
Variable shadowing does not "cast away constness", and it's just as easy to shadow in Haskell: main = do let a = 1 let a = 2 print a 
&gt; I believe it is common in functional languages derived from ML but not functional languages in general. Erlang is the only functional language I know where shadowing is impossible (because it has a somewhat odd take on bindings). What are the others?
Why would he bother shadowing variables when he can just make every binding `mut`?
I'm using the *GNU ABI* version of Rust, [*Atom*](https://atom.io/) with several plugins like the minimap and obviously Rust-related stuff, and my debugger is *GDB* from [*MSYS2*](https://msys2.github.io/). As a shell I usually use *CMD* to just run simple commands or *MSYS2-Bash* for some shell scripts, both wrapped using [*ConEmu*](https://conemu.github.io/). Why the GNU ABI? Just because I preferably used Clang back in my C++ days on Windows, instead of MSVC. Although neither ABI is superior by design, cross platform stuff is pretty much sexier indeed. And... well... freakin' LLVM.
I agree that it should at least be a compiler warning, but not a breaking change. This is one of the most headache-inducing anti-patterns in debugging open source code, especially given the number of partly abandoned but very useful "first crate" rust projects out there. Note that the "compiler warning" in this case is not so much for the original author (who is absent and may be forgiven) but for the person using the crate who might be trying to debug something. In this case, compiler warnings are a good record of possible errors in the code, places where bugs might have been introduced, to make maintenance of OSS easier. That said, I am a fan of going `mut`-to-non-`mut` via a scope: let x = { let mut x = 4; x += i32::consts::LOL; x };
Which extension and compiler are you using?
In the past few days [VisualRust](https://github.com/PistonDevelopers/VisualRust) has been working on adding toml parsing so fully working cargo integration is coming pretty soon. Debugging in VS with the `-msvc` targets of Rust already works fairly well and doesn't need VisualRust, although LLVM currently doesn't know how to emit local variable information for codeview debuginfo so you can't really debug local variables just yet.
oh that is AMAZING
That kind of variable shadowing is used *all the time*. Unlike mutations, it can only appear in places upscope - not within loops - which in well-structured code makes it harder to miss.
Exactly. This is a lot clearer in a language which isn't trying to appeal to the C(++) crowd, and uses `let x = 5 in ...` (one of Rust's parents/relatives: OCaml) making it clear that this is just an expression to be continued, and not a standalone statement. I don't fault Rust for it's choice here, as I once thought it was rather verbose/redundant to have these explicit `let` and `in` everywhere... except that they are actually decent syntax for the semantics being expressed. Semicolon is shorter and makes things C-like, but isn't as clear (and whatever the syntax, once familiar it disappears, like the minutia of how to walk).
I like variable shadowing and I'm responsible for it. I see it as a way to allow using immutable variables *more often*, which is why it's a common idiom in ML. You can do things like: let x = ...; ... let x = x + 1; ... let x = x + 1; In another language, you'd have to use mutability here, but in Rust, because of variable shadowing, you can use immutable variables and know that the value won't change in between `let` statements.
RustyCode and nightly-x86_64-pc-windows-gnu
The Rust compiler wont let you make a variable with the same name as a `const` in the same namespace. If you try to compile this: const FOO: u64 = 42; let FOO = 0; you get something like this: &lt;anon&gt;:5:9: 5:13 error: only irrefutable patterns allowed here [E0414] &lt;anon&gt;:5 let FOO = 0; ^~~~ &lt;anon&gt;:5:9: 5:13 note: there already is a constant in scope sharing the same name as this pattern &lt;anon&gt;:5 let FOO = 0; ^~~~ &lt;anon&gt;:4:5: 4:26 note: constant defined here &lt;anon&gt;:4 const FOO: u64 = 42; ^~~~~~~~~~~~~~~~~~~~~ which is a horrible error message, but at least it doesn't compile. 
No, no runtime overhead. &amp;/&amp;mut also gets you this but it's tied to a scope. Try it: https://play.rust-lang.org/?gist=b7e055ef45ebaac7cd11a6f780fc2646&amp;version=stable&amp;backtrace=0 , that line can't be uncommented without introducing an error. Note that this still allows for interior mutability. It is possible to forbid interior mutability using OIBIT, but I don't think you're looking for that strong a guarantee.
What exactly is a block suffix? Is it just a fancy way to say a closing curly brace?
You will be interested in Honza HubiÄka's blog series on improving devirtualization in gcc: it starts [here](http://hubicka.blogspot.fr/2014/01/devirtualization-in-c-part-1.html). The big idea exposed here is speculative devirtualization. The problem of virtual calls is two-fold: - the obvious: branch-prediction. On top of multi-target branch prediction being more costly in general that binary branch prediction, it also requires the CPU making the right prediction. - the subtle: no inlining. Inlining is the cornerstone of many optimizations, as it peels away the layers of abstractions, and losing inlining can really hurt performance wise. Speculative devirtualization consists in analyzing the implementers of an interface: - are they all known? In C++, this is the case if (a) the interface is declared in an anonymous namespace or (b) all potentials "paths" are closed off by the `final` keyword - even if not all known, do we have an idea of which ones are more likely? (PGO helps a lot here) Then, it will change the call from: object-&gt;call(); to: if (typeof(object) == A) { object-&gt;A::call(); } else if (typeof(object) == B) { object-&gt;B::call(); } else { object-&gt;call(); } // only if not all known And the benefit is that: - the first few branches are binary, more swiftly predicted - the calls `A::call()` and `B::call()` are devirtualized and thus inlinable
I also think that even with the necessary FFI, moving the handling of *user input* to Rust is beneficial: user inputs are attack vectors, so it makes sense to harden the parsers. I would expect that the parsed input is somewhat less error prone (less potential states to deal with), although of course it only takes one mistake...
Aha! Suddenly the `.clone()` method makes sense.
In error messages and such, it usually means "the rest of the block".
I only dimly recall the discussion. IIRC Graydon was lukewarm on shadowing (was slightly inclined to remove it but didn't really care either way), but I stuck up for it and it stayed. My memory could be wrong though.
Here's a block of code: { foo(); bar(); foo(); foo(); } If we wanted to talk about "everything after the call to `bar()` until the end of the block", that would be a block suffix, starting at `bar()`. I would like to see the term abolished from errors, frankly.
A few things that came to mind since writing this post (those not previous mentioned): * We must be explicit in calling `sort_by` for a collection `[T]` of type `T: PartialOrd`: the language does not allow you to use `PartialOrd` and `Ord` interchangeably in places where they are not in fact interchangeable. We must be explicit about what `sort()` is doing in cases where gaps must be filled. * We explicitly `use` traits to bring them into scope and access the methods they provide.
I consider Rust's error handling to be very explicit. I've been a python/javascript/clojure developer most of my career and wasn't even aware that a lot of the syscalls involved in working with files even could fail. Not that they don't make sense after thinking about it but it's not something I've ever needed to think about.
Actually, you're probably right with the current restrictions on `const fn`, it's not immediately obvious to me at the moment. Plugins are a better example.
By "building a web server" do you mean building nginx, or building a web application?
&gt; because it's semantically different Oh, just keep rubbing that in, dontcha? \^_\^
The [ndarray](http://bluss.github.io/rust-ndarray/master/ndarray/index.html) crate is just made for storing this kind of map state. Too bad the game of life is already implemented as an [example](https://github.com/bluss/rust-ndarray/blob/master/examples/life.rs) there, so it could feel a bit unsatisfactory and copy-ish. Definitely worth a look though, also faster than your hashmap implementation.
Ah cool! There have been one or two static file serving things done before, I believe /u/brson built one. And I know /u/pcwalton has wanted to try to prototype a more 'real' server. Maybe they have some pointers for you.
Primitive type casts aren't really that explicit IMO: You can't tell from the code whether * it's a simple promotion, * it should never be outside the target type's range or * whether it should really use the usually unwanted behavior of wrapping when it's outside the new type's range.
v0.0.1 was only uploaded so that I could reserve the crate. All it contains is a function that panics immediately upon being called, and I don't want anybody to accidentally download it instead of v0.1.0.
That's pretty clever, I like it.
/r/playrust
&gt; Promoting to a larger type (like float -&gt; double, short -&gt; long, ...) is usually implicit. To be, er, explicit, it's other languages that this is usually implicit in, not Rust.
Crates should be yanked when they accidentally break semver, or contain some soundness bug that was fixed and you don't want Cargo grabbing the wrong version.
Rust newb here. I'm trying to get the top stories from Hacker News using hyper. Per the HN API, I issue an initial request to get the list of IDs and then for each ID, I issue subsequent requests to get the metadata for that story. Obviously sequential synchronous requests will be slow, so I'm spinning up 8 threads (this seemed to be the sweet spot) which each take the next ID from the vector of IDs and dispatch a request. However, the Rust implementation underperforms the reference Go implementation by a factor of 10 (2.5s vs 30s). Could someone please point out what is wrong with my Rust implementation? * [Rust version](http://pastebin.com/6iess1FU) * [Go version](http://pastebin.com/RxTdTCKz)
Haskell, you can't define two values with identical identifiers in same scope.
 int a = 1; const int b = a; You can't do this in Rust, can you?
No, consts have to be known at compile time, so it's not really the same as the C example. Still, I figure the number of times that you need to be sure that a binding doesn't ever change and need it to be attached to a value that is computed at runtime is very small.
 let a = 0; const b: u32 = a; // error
Honestly, I probably didn't need to and in most circumstances I wouldn't reserve a name - projects get dropped all the time, and mine are no exception. If someone made a better generator before mine were ready/nearly ready I would have happily given the name to them. However, this is the sort of project that multiple people might start working on independently with basically identical results and it would suck for pretty much all involved to have most of that work wasted by one person publishing first. Because I could reserve the name in a place where someone potentially interested in using/developing a generator would almost certainly check I could make sure that sort of thing didn't happen by making it clear that such a crate was in active development.
Awesome! I've recently switched to VSCode from Atom and loving it so far :)
You want /r/playrust
This isn't a problem if you use the following style: void foo(void) { } because you can just grep for `^foo`.
Not everyone using that style, and it doesn't help for variables, only functions. But I wasn't really saying one was better than the other, just that the rust way is explicit (it literally says `fn`), and the C way is implicit (nothing says "this is a function", you have to recognize it by the pattern and the context.).
This has more rightwards drift though.
You can't do that in Haskell, that's local variable shadowing. I'll just copy paste here: Your example changes local variable shadowing to scoped shadowing. let x = 5; { let x = 7; println!("{}", x); } println!("{}", x); is not equivalent to let x = 5; let x = 7; println!("{}", x); println!("{}", x);
Oh, my message was not really directed at you, I hope you didn't take it personally. I can understand people who want to reserve crate names for projects they are actively working on. But the fact that you **can** reserve crate names worries me a little. I am worried about people who would frenetically reserve crate names for all project ideas that they may or may not bring to completion. I am also worried about spammers and bots. The fact that you can reserve crate names opens the door to abuse. That's why I think it's a design flaw. :)
`const` in Rust is like `constexpr` in C++, it must be known at compile time.
Thanks! I'll get into this tonight!
Sorry, I should have expressed that a label assigned at runtime cannot be made constant.
Really upping the game on release notes! I think all major rust announcements should be music videos. 
As I understand it, there's a key difference between runtime dependencies and runtime overhead. Rust has admittedly a lot of dependencies to provide the stdlib functions, but is very limited in the inherently running overhead, especially compared to any garbage collected language - iirc, and someone should correct me if I'm wrong on this, rust doesn't have any abstracting runtime structures like virtual threads controlling flow at all - rust threads are purely OS threads.
Other rust rewrites I'd love to see: OpenSSL, ImageMagick, Apache, Nginx
But let itself in Haskell introduces a new scope, doesn't it?
Why should it be equivalent? When parent post said "It is fairly safe to think of each let statement as implying a new block.", I believe it was meant a block that reaches to the end of the current block (letting braces stack at the end), your example clearly doesn't do that.
&gt; I am worried about people who would frenetically reserve crate names for all project ideas that they may or may not bring to completion (this already [happened](https://github.com/rust-lang/crates.io/issues/58#issuecomment-64254889) at least once)
You can use an AtomicBool from std::sync::atomic. Even better would be to get rid of global state, but it's not always practical.
Often times, it's easier and more practical to start from scratch than to improve an existing monolithic project.
Yeah, it wasn't your code - the pcap crate itself fails in some tests.
https://github.com/jkraemer/ferret but no commit for the last 10 months. Edit: I'm not 100% sure if this is what you're looking for.
I think his point is lack of consideration around other aspects of the project. Is the userbase willing to sit around and wait while a large project is rebuilt? Even as the new product is built it will be buggy and probably function differently than the original. Is the userbase willing to stand by the product and support it through this phase? It takes a lot of time, and more often than not when confronted with these tradeoffs most users would rather time spent adding features and fixing bugs rather than completely rewriting everything from scratch. edit: I'll add that working in the private sector I've only ever once seen a full rewrite of a product, and the product itself was more along the lines of "a prototype gone too far". All other attempts only succeeded through a slow rollout across versions.
you're wrong.
Redis for caching? why redis? how is it done, for example, in Rails or Django? They don't use Redis for that. I'm asking about middleware, probably.
Redis is an in-memory key-value store. Lookups are very quick since they will never touch the disk. Redis has additional features beyond being a simple in-memory key-value store. The [Redis introduction](http://redis.io/topics/introduction) has a nice overview. It sounds like you are looking for an ORM with an LRU cache. Iron is simply a web framework; it makes is easy to built a Rest API, but it doesn't know anything about what goes on in your handlers. Rails built-in caching is somewhat limited. If you run an identical query twice in an action, it will return the result from memory the second time. The result is not stored after completing the action. There's also `Rails.cache` which can be used to store arbitrary data between requests, but it's entirely manual. The most mature ORM for Rust is diesel. This makes it easy to run your SQL queries. If you want to cache the Result, you could maybe just key on the query in Redis.
To begin with absolutely, although it's often very easy to do 70% of the functionality, and the remaining 30% is really difficult and has a lot of edge cases. I've been involved in several re-write projects where people thought "this legacy code is so over-complicated and hacky - we could re-write it and make it so much nicer." This then happened, but once you start throwing real live production data instead of just simple units tests at it and seeing the edge cases you have to cope with, you start to realise why that code was there in the first place, and the new code, while refactored and more modern, still ends up almost as complicated. And sometimes worse.
https://github.com/brson/basic-http-server is the one I made. It's well-commented. I still use it for simple file serving. If you just want to get started though I wouldn't copy this architecture or you'll immediately get bogged down in async matters. Instead just use std + hyper. 
Wrt OpenSSL, I think you'll like this [presentation](https://speakerdeck.com/tarcieri/thoughts-on-rust-cryptography) if you haven't seen it already. It talks about the strengths and (present) weaknesses of Rust while implementing crypto.
There is no way to cast an integer to a smaller type that does not potentially result in a loss of information, and loss of information is generally not what the programmer wants in most cases, so truncation is just as good (which is to say, bad) as any other choice. The important point is that, in C and C++, this potential loss of information is implicit, and in Rust it isn't.
The reddit way of summoning people is /u/username not @username, it ensures that the reply goes to the persons 'inbox' ;)
&gt; There's also Rails.cache which can be used to store arbitrary data between requests, but it's entirely manual. how is it manual? it's not. I can use it without Redis and anything like that. I can just it out of the box, that's it. And so I want to be able to do in Iron. I don't need an ORM.
When searching "vulkan" in crates.io, I noticed that both `vulkan` and `vulkan_rs` were reserved (or maybe they were real projects that were abandoned after two hours? who knows). I don't want these names, but I wonder how many people will lose time and get angry when they look for bindings for Vulkan and the first two results don't work at all. 
This is a very clear explanation of what is going on. Over the last few months, I've gained an intuition so that I can usually anticipate when some of my code will or won't pass the borrow checker, but I often wouldn't be able to explain it to someone else. This post has cemented for me the concepts of lifetimes and scopes, and I think I understand this better now. Hopefully that means I'm cargo-culting less and will be able to quickly shift my intuition when borrow rules are relaxed later.
Fair point. Seems more of a social than a technical issue though. Nothing that can't be fixed by pairing on the code regularly, which is something that should be done anyway, if the team has a few people new to Rust. I guess, I would focus breaking backwards compatibility on things that allow the language to be more expressive rather than fixing little inconveniences like this. And there is still no way this issue can leave the scope of the function and cause actual harm.
Note that once the [Pattern trait](https://doc.rust-lang.org/std/str/pattern/trait.Pattern.html) is stable, regexes will implement it and make the whole thing let word_re = Regex::new(r"[a-z']+").unwrap(); let words: Vec&lt;_&gt; = string.matches(&amp;word_re).collect(); (add a `.cloned()` if you prefer new allocated strings.)
Take this approach if you want to start from zero and DIY everything. If you want to focus on features you should look into an already available HTTP server library (like http://hyper.rs/) and just build on top of that.
I tried both and didn't see any advantages using VSCode over Atom. Can you tell what you like in it ? I'm just curious about this .
It's also true about lisps. I think the only language I know of that does not allow you to shadow variables is erlang and elixir.
It's faster. Like: a lot.
&gt; it ensures that the reply goes to the persons 'inbox' ;) Only if they have gold though, no?
Haskell also disallows local shadowing. Scoped shadowing is allowed but that is a different issue. (I would also disallow scoped shadowing, personally)
The issue is expressivity though. You cannot express this in Rust: const int x = runtime_func(); The same argument you're making could be made against `unsafe`. Any issues caused by not making unsafe operations explicit can be fixed by pair programming regularly.
Well horrible was probably harsh, but the "irrefutable pattern" line thew me a little. My brain isn't in "pattern matching mode" every time I make a variable, so the fact that `FOO` could be anything other than a new name doesn't really occur to me. Or another way of saying it is my brain is usually in `let _ =` mode, which should always be irrefutable. I spent a while writing out a long response, but the more I thought about it the less sense it made, so lets just say I was too harsh and the confusion is probably just me.
Problem: no one says you have to roll out a rewritten project while it is unfinished. People can keep using the old project until the new one is complete. Problem Solved.
All [commits](https://github.com/jkraemer/ferret/commits/master) in the last years seem to be only for compilation fixes. The project is quite dead :(
Please prove him wrong. BTW he is right.
For me the name shadowing is perhaps the harder to accept design decision of "basic Rust design". I have had experience of shadowing causing me bugs and problems in past in other languages, so I assumed a language focused on safety as much as Rust would forbid all shadowing. So far the shadowing in Rust has not caused me significant problems in practice (and it's sometime handy), so I am still looking for more data :-) It was surely a bold design decision.
I think Apache makes it too easy to do the wrong thing by using it as intended to be worth worrying about trying to secure it at the lowest levels. But yeah, things like ImageMagick and Nginx could definitely be better with Rust.
`vim` with the `rust.vim` plugin.
On Windows, comment taken from [another thread](https://www.reddit.com/r/rust/comments/4hjqr2/windows_developers_whats_your_development/) &gt; I use [Sublime Text 3](https://www.sublimetext.com/3) with the following packages: &gt; &gt; * [Rust](https://packagecontrol.io/packages/Rust) For snippets and highlighting &gt; * [RustAutoComplete](https://packagecontrol.io/packages/RustAutoComplete) For autocompletion (Only on the standard libraries seems to work for me) &gt; * [BeautifyRust](https://packagecontrol.io/packages/BeautifyRust) For help with formatting &gt; &gt; When compiling I use [cmder](http://cmder.net/) and `cargo build`, debugging with print statements. I'm sure there's a way to set up a build system for Rust in ST3 but I've never bothered.
To [quote myself](https://www.reddit.com/r/rust/comments/4hjqr2/windows_developers_whats_your_development/d2r1wgj): &gt; I'm using the *GNU ABI* version of Rust, [*Atom*](https://atom.io/) with several plugins like the minimap and obviously Rust-related stuff, and my debugger is *GDB* from [*MSYS2*](https://msys2.github.io/). As a shell I usually use *CMD* to just run simple commands or *MSYS2-Bash* for some shell scripts, both wrapped using [*ConEmu*](https://conemu.github.io/). &gt; &gt; Why the GNU ABI? Just because I preferably used Clang back in my C++ days on Windows, instead of MSVC. Although neither ABI is superior by design, cross platform stuff is pretty much sexier indeed. And... well... freakin' LLVM. What I didn't mention: I work on both, Windows 10 and Windows 7. I sometimes work on an Arch Linux system, but that's very unfrequent.
If your code is single-threaded, you can use [Cell](http://doc.rust-lang.org/std/cell/struct.Cell.html) with [`thread_local!()`](http://doc.rust-lang.org/std/macro.thread_local!.html), though all that really does is replace your `unsafe {...}` brackets with `.with(|cell| {... })` calls: use std::cell::Cell; thread_local!(pub static MY_FLAG: Cell&lt;bool&gt; = Cell:new(false)); fn main() { let flag = MY_FLAG.with(|cell| cell.get()); // Or more concisely let flag = MY_FLAG.with(Cell::get); } With `thread_local!` you could use one thread-local with all your state in one struct and `RefCell`. Note that it inherently means having separate state for each thread, so it's not appropriate if you want your flags to be the same across all threads. If your code is multi-threaded, you're basically just asking for race conditions and should be using atomics or more serious synchronization. If you're on nightly and don't mind locking your codebase to it for a while, you can just use them with `pub static` and `#![feature(const_fn)]`: pub static MY_FLAG: AtomicBool = AtomicBool::new(false); Otherwise, you should use [`lazy_static!`](https://crates.io/crates/lazy_static). It seems stupid to pull in a whole crate for one macro but it should be in pretty much every Rust programmer's toolbox because it's so damn useful. I think it's actually on-track for inclusion in the stdlib, given that it's in rust-lang-nursery organization. And if you have multiple flags that you want synchronized, you should either read up on how atomics work and why fences are important, or wrap all of your flags in one struct and use `lazy_static!()` together with [`RwLock`](http://doc.rust-lang.org/std/sync/struct.RwLock.html) for concurrent reads without blocking. Edit: I may not know as much about atomics as I thought. Some of the language around them was confusing, so I thought fences synchronized operations between separate atomic values but apparently that's not the case. 
Not sure about neocomplete, but deoplete is asynchronous meaning it doesn't freeze up the editor every time you want to make a completion. 
This is my setup too.
I use emacs with: * [Flycheck](https://github.com/flycheck/flycheck) for Error highlighting * [Company](http://company-mode.github.io/) for auto complete and jump to definition * [Rust-mode](https://github.com/rust-lang/rust-mode) For syntax highlighting etc. An easy option for set up is [Spacemacs](http://spacemacs.org/) as it has a prepackaged layer with all of these installed. 
Yes, there is. You can generate an error, or panic. The problem I have here is that Rust does not distinguish between wanted information loss and assertions that there is no information lost. If we had `.cast()` which panics on truncation and `.truncate()` which just truncates on overflow, bugs like the one where we wouldn't fill random buffers completely on Windows because of an integer truncation would not happen.
Beside coreutils, all mentioned projects are gradually adding rust without burning everything down. However for every project there will be a point where starting over will bring enough benefits to be worth doing.
Then the users might be waiting forever, and you have to either maintain both versions or make users on the old version live with existing bugs and no new features :-/
I would add python to this list.
No idea, but it would be cool if anybody could check it out via https://wiki.scn.sap.com/wiki/display/ABAPConn/Download+and+Installation+of+NW+RFC+SDK and see whether it's possible to port.
&gt; I'm sure there's a way to set up a build system for Rust in ST3 I have a "Cargo" build system in ST3 (dev channel), which I think is actually the built-in one added in build 3109. Just select a .rs file and press ctrl+shift+B as usual.
I'm using emacs on os x with racer as an auto completion backend for company mode. I don't have the most ergonomic setup, so I usually split the frame and keep an eshell in the lower window where i run different cargo commands. **EDIT** [I see now that OP wanted pictures!](http://imgur.com/2N23iEr)
The `thread::spawn` API is designed to prevent such errors. It requires that the data passed to the other thread have a 'static lifetime which would prevent (at compile time) sending a borrowed reference with a lifetime bound to the stack, like in this case. No error should arise in the current or in the proposed lifetime checking scheme.
&gt; what happens if the capitalize function in this example from the blog post spawns a new thread using that mutable reference to data? `std::thread::spawn` requires the passed closure to be `'static`, which means that all it accesses should be either of value types or in static scope. So using a mutable reference in a new thread is already not possible.
[Spacemacs](http://spacemacs.org/) using the [Rust](https://github.com/syl20bnr/spacemacs/tree/master/layers/%2Blang/rust) layer. I'll make some screenshots after work if I remember to.
YCM with ycmd is too, IIRC
Windows with * Powershell as my terminal (it's okay... can be a bit slow at times though, but I love my wierd mix of unix comands and windows commands... * gVim with rust.vim, and various other plugins. (I also installed the toml plugin as well, which provides syntax highlighting for toml files.) If you set up racer, you can type "gd" over a function, and jump to it's definition, which is pretty neat. * Cargo-check is a third party cargo command I find useful.
&gt; Also, I'm sure it's just left out to keep the blog post manageable but what happens if the capitalize function in this example from the blog post spawns a new thread using that mutable reference to data? thread::spawn has a 'static bound, so it won't work. scoped thread solutions tie the lifetime to a destructor, which extends liveness to the end of the scope. This would work. 
Just in the language itself (not discussing LLVM). * Rust doesn't support native AES-NI operations which make implementing AES slow, and painful. If they were added tomorrow they'd still fall under SIMD support which isn't stable. * Rust assembly support isn't stabilized, which could address the above, and is needed for constant time RSA. * While you can use *fun* extensions in assembly then you have to pass args to the LLVM to enable those extensions which requires passing args to LLVM which idk how to do. Or the LLVM will vomit in your face about assembly commands that don't exist. * Technically how overflow is handled in Rust breaks the [specification](https://github.com/libOctavo/octavo/pull/79) of some crypto algorithms. The output is still correct but ya know its not *right*. Most these issues are solved by just statically linking against LibreSSL crypto module.
I tried it yesterday and it was pretty great.
&gt; But the fact that you can reserve crate names worries me a little. I am worried about people who would frenetically reserve crate names for all project ideas that they may or may not bring to completion. Yeah don't tell kik about this.
This is very outdated, you should use https://github.com/rust-lang/rust.vim instead. (It was created because the plugin used to be in the main repo, so it was hard to use with tools like Vundle. We then split the configs into their own repos, so no need for an external mirror anymore.)
Did you compile with `--release`? EDIT: I see your thread on users now. I agree with what the others have said there; my first instinct was to reach for crossbeam. EDIT: re-posting from that thread: the first version took 2mins on my machine, this one takes 20seconds: extern crate hyper; extern crate rustc_serialize; extern crate scoped_threadpool; use std::io::Read; use hyper::Client; use hyper::header::Connection; use rustc_serialize::json; use scoped_threadpool::Pool; #[derive(RustcDecodable, RustcEncodable)] struct Story { by: String, id: i32, score: i32, time: i32, title: String, } fn main() { let client = Client::new(); let url = "https://hacker-news.firebaseio.com/v0/topstories.json"; let mut res = client.get(url).header(Connection::close()).send().unwrap(); let mut body = String::new(); res.read_to_string(&amp;mut body).unwrap(); let vec: Vec&lt;i32&gt; = json::decode(body.as_str()).unwrap(); let mut pool = Pool::new(8); pool.scoped(|scope| { for id in vec { let client = Client::new(); scope.execute(move || { let url = format!( "https://hacker-news.firebaseio.com/v0/item/{}.json", id, ); let mut res = client.get(url.as_str()) .header(Connection::close()) .send() .unwrap(); let mut body = String::new(); res.read_to_string(&amp;mut body).unwrap(); let story: Story = json::decode(body.as_str()).unwrap(); println!("{}", story.title); }); } }); } We use scoped threads through `scoped_threadpool` to eliminate the need for synchronization generally, and create a client per-thread rather than sharing a single client across threads.
you said [you could only stand it for 48 minutes though](https://twitter.com/steveklabnik/status/727528094647619584) ;)
VSCode, not the Rust part. The vim bindings were the issue there.
Emacs rust-mode and Flycheck here too.
Yes, it was RustCamp.
I think this is fine except that there should be a way to turn it off because sometimes lifetimes are used to represent explicit regions. The simplest example I can think of is a `Mutex`. fn smaller_critical_section(only_run_one: Mutex&lt;()&gt;) { let guard = only_run_one.lock(); do_someting(); do_something_else(); } Since we don't use `guard` it could easily be dropped earlier. This isn't, because it has a destructor, but because developers depend on knowing how long the variable is going to exist. There could be variables with destructor that can be freed as soon as they aren't explicitly used. The post did say that dropck would probably extend the lifetime of the object far more. I think that this is dangerous: implementing the `Drop` trait for an object would have a huge effect on code that suddenly wouldn't compile for obvious reasons. The problem I see, with allowing this for all things, or making an exception for types that implement `Drop`. Maybe the solution is to have another default Trait that allows you to choose to turn it off or on? I hope the next posts will cover this cases and explain how they plan to go about it in more detail.
Does this mean that we make references a bigger special case than they already are? Currently mutable references implemented in library just don't support implicit reborrowing (i.e. `fn reborrow&lt;'a&gt;(&amp;'a mut &amp;mut T) -&gt; &amp;'a mut T`). With these non-lexical lifetimes, it seems that references can go out of scope *inside a scope*, which structs cannot. Do I understand this correctly?
I'm pretty sure it's run a lot more than 2 in total. I haven't checked in a while.
I will investigate, thanks / sorry :(
No worries! Just thought I'd mention before this gets more publicity
Does autocompletion work for external crates? (not std) I just tested this setup and when I type something like use image:: no autocompletion shows up, even though it's in my Cargo.toml
Yeah, I compiled with `--release`. I'll check out crossbeam when I get some time.
My understanding is that these improvements are purely done *without any observable effects*. It is to allow more (previously invalid) code to compile, but it doesn't change the behavior of the program at all. [In the first post](http://smallcultfollowing.com/babysteps/blog/2016/04/27/non-lexical-lifetimes-introduction/), he distinguished between the lifetime of a *reference* and the lifetime of a *value*. The non-lexical lifetime proposal deals with the former, whereas destructors are related to the latter. So, in your example code, `guard` will be dropped at the end of the function just as before.
This change doesn't affect when destructors run: destructors will still run at the end of the scope, in the reverse order of construction, exactly as you would expect. Types with both lifetimes and destructors will extend those lifetimes to the end of the scope because the destructor counts as a "use".
The code of conduct link still mention RustCamp and not RustConf btw
gah, thanks!
&gt; Since we don't use guard it could easily be dropped earlier. `guard` has a destructor, which, to MIR, counts as a use, and is always inserted into a scope as late as possible.
I don't know about Neocomplete, but with YouCompleteMe, autocompletion works for external crates most of the time (modulo glob imports, and probably a few other things). I've actually found it to be a great way to explore a crate's API without leaving my editor :p
&gt; "jump to the start of the last command's output" And here I was, clearing the current buffer with cmd + K every time I wanted to see the most recent errors :( Would you mind explaining _how_ you jump to the start of the last command's output? I've tried to use "Select Output of Last Command", but it's inexplicably grayed out.
I am actually pretty bad at vim, so I don't use a ton of features, so it usually takes me a bit longer to get fed up.
It's faster than Atom, but not as good as Atom's Tokamak plugin. 
I started working on an [ASN.1 parser](https://github.com/haxney/rust-asn1) in Rust, which would be needed to parse the SSL/TLS certificates. I quickly ran into the problem that the ASN.1 standard is kind of horrible, plus I haven't devoted a ton of time to working on it. I thought ASN.1 would be a useful contribution that wouldn't require doing any of the fancy sensitive stuff that makes crypto programming so hard.
I think one of the advantages of non-lexical lifetimes (as a very new Rust programmer) is that it makes them *easier* to understand because many bits of code that previously "should" have worked, but didn't, now work. It gets more complicated behind the scenes, but because it matches my intuition it seems easier. But you might well be right. What would be useful is a number of examples showing both examples that didn't work before and do now, and ones that either stop working now or have strangely different semantics and see it's an overall win.
Ah, I see. Does this sound clearer? https://github.com/rust-lang/rust/pull/33406
This + YCM
[Screenshot](http://imgur.com/W0110dG)
I tried it, I wasn't a huge fan. Though I think it has a lot of promise.
VS Code is working towards being an IDE, although it's still very new. It has projects, you can debug, set breakpoints, compile... etc. I have no experience with Atom though, so I'm not sure where it sits.
The last point should not be an issue, you can pick your overflow behavior by using the appropriate types/functions. For wrapping arithmetic, use `Wrapping&lt;T&gt;` and you're done.
Yes and no. Your variable is borrowing until it's dropped, and an implicit `drop` is inserted if necessary at the end of the scope. If you want borrowing to end earlier: - introducing a scope is possible - otherwise just inserting a `drop` call manually should suffice This is the main difference of the liveness analysis with the current situation: today only the first solution is available because lifetimes are based on scopes, but the algorithm Nico is proposing would detect the last use of the variable (`drop`) and therefore release borrowings after that.
Cool. I'm a Portland native so I just might have to drop in on this.
I am 99% sure that is the plan. We did last year.
Tha's a very interesting project,c an you give more details?
Hashmaps (or hashsets) are nice for Game of Life because the universe is generally extremely sparse (OP's starting density is 14%) and theoretically infinite. With a vec, you get a limited universe size and computing the next generation is a function of the matrix size, with hashmaps it's a function of the number of live cells. The step beyond is usually quadtrees and pattern memoisation (e.g. Bill Gosper's HashLife). I guess you could also use a compressed bitmap like roaring.
Oh you're right (I hadn't checked the code). In that case the hashmap is just a waste indeed.
Me too! High five!
Yes, I think so.
I can't stress YCM enough. Its freaking awesome, since it bleeds trough all vim uses, even python or latex editing.
As matthieum mentioned, this is only true for *automatically inserted* drops. You can call drop explicitly to give even types with destructors a lifetime that isn't tied to their scope. For example, `RefCell` gives you a `Ref` guard when you borrow it. You can manually call `drop` on this guard as soon as you are done with it, and you have just as much power as you do with a normal reference.
If you only want to temporarily shadow, you can: let x = 5; { let x = 6; } // x is still 5 here Or in situations where you only plan on using a reference afterwards: let s = &lt;some String value&gt; let s = &amp;s[..]; You can't do that with mutability.
[Screenshot](https://imgur.com/5YaRynu). Minimal vim config, tmux, fish shell. No autocompletion or autoformatting - I use `cargo fmt` manually as-needed.
Interesting. I don't generally like to blame libraries for bugs that show up in my own code since it feels like a cop out, but really the system networking logic isn't present in rust-passivedns. Do you have any error string or anything we could submit to them as a bug report? If not I could try to reproduce on a raspberry pi.
So inserting `mem::drop` calls will actually be useful as a way to fix borrowing problems now?
Full disclosure: I don't like auto completion. I think that manual completion work just fine, and doesn't slow me down (auto completion can sometimes slow down UI or whole editor). I find built-in `^n` and `^x&lt;something&gt;` completion just fine for 99% of my use-cases. What I find overengineered in YCM is: - Installation. Compare it to Deoplete and it seems that it is ride through hell. - `.ycm_extra_conf.py` is another Pandora's Box I prefer my plugins to be "free" in term that they need no configuration OOTB and everything Just Worksâ„¢. Using `racer.vim` with `^n` and `^x^o` cover 99% of my use cases without all gibberish.
Installation and configuration. Compare it to the Deoplete or VimCompletesMe and it will be like heaven and hell.
Every event is actually organized by someone so.. please hold an event where you are?!
rustc under this proposal will start out by rewriting this: struct X; impl Drop for X { fn drop(&amp;mut self) { ... } } fn do_stuff() { let x = X; // foo bar baz let x = X; // bla bla bla } into something a lot more like this: fn do_stuff() { let x_1 = X; // foo bar baz let x_2 = X; // bla bla bla &lt;X as Drop&gt;::drop(&amp;mut x_2); &lt;X as Drop&gt;::drop(&amp;mut x_1); } It runs borrowck, and all the lifetime inference therein, on MIR that looks like the second example; the rules this blog post described do not run until after the drop glue has been inserted. This means borrowck itself follows the same rules whether or not destructors exist, but to the user it's going to look like borrowck is being stricter in regards to types that implement Drop than it is in regards to types that don't. When destructors end up running does not change.
/r/playrust
rustw: https://github.com/nrc/rustw/blob/master/screenshot.png
/r/playrust
Was it ever considered to use a symbol for cloning to avoid the inconsistency introduced by `Copy` ? let c = a.clone(); let c = @a; // @ means clone // vs. let c = a; // Only if "a" is Copy
I jump back and forth between Sublime Text 3 and Atom both intermixed with Vim. All under Arch Linux, Gnome, and full-screen Guake (drop down terminal) + zsh and [oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh), using [rustup](https://www.rustup.rs/) to manage Rust versions Plugins I use are: SublimeText 3 * [RustAutoComplete](https://packagecontrol.io/packages/RustAutoComplete) * [Vintageous](https://packagecontrol.io/packages/Vintageous) (way better vim support than built in vintage mode) * [VintageousOrigami](https://packagecontrol.io/packages/VintageousOrigami) (vim like split screens) * [GitGutter](https://packagecontrol.io/packages/GitGutter) (shows git +/- in gutter) * [AdvancedNewFile](https://packagecontrol.io/packages/AdvancedNewFile) (just easier way of adding files) * [SidebarEnhancements](https://packagecontrol.io/packages/SideBarEnhancements) * [SublimeLinter](https://packagecontrol.io/packages/SublimeLinter) (with [sublimelinter-contrib-rustc](https://packagecontrol.io/packages/SublimeLinter-contrib-rustc)) Atom * [ex-mode](https://atom.io/packages/ex-mode) * [minimap](https://atom.io/packages/minimap) * [minimap-git-diff](https://atom.io/packages/minimap-git-diff) * [tree-view-git-status](https://atom.io/packages/tree-view-git-status) * [vim-mode](https://atom.io/packages/vim-mode) * [tokamak](https://atom.io/packages/tokamak) Vim * [rust.vim](https://github.com/rust-lang/rust.vim) * [YouCompleteMe](https://github.com/Valloric/YouCompleteMe) * [tabular](https://github.com/godlygeek/tabular) (smart tab alignment) * [bufexplorer](https://github.com/jlanzarotta/bufexplorer) * [vim-git-gutter](https://github.com/airblade/vim-gitgutter) (shows git +/- in gutter) * [NERDTree](https://github.com/scrooloose/nerdtree) (tree file explorer) * [ctrl-p](https://github.com/kien/ctrlp.vim) (fuzzy matching ctrl+p) * [Gundo](https://github.com/sjl/gundo.vim) (tree view of undos) * [Syntastic](https://github.com/scrooloose/syntastic) * [tcomment](https://github.com/vim-scripts/tComment) (quickly comment/uncomment) * [clever-f](https://github.com/rhysd/clever-f.vim) (better vim `f` functionality) * [vim-surround](https://github.com/tpope/vim-surround) (better pair patching) * [vim-repeat](https://github.com/tpope/vim-repeat) (repeat everything, not just native commands) * [vim-abolish](https://github.com/tpope/vim-abolish) (better substitution) * [racer](https://github.com/phildawes/racer) I've briefly tried VSCode with RustyCode, and it was nice but the vim bindings left me wanting more. 
You missed my `1` :) &gt; To my understanding, RustConf is a production by Tilde, at least partially funded from the Rust budget. RustConf is a production by the Rust core team. We are employing Tilde to help with logistics. &gt; That's a very different outset when asking someone to run a thing that usually costs north of 50k. I mean, it depends on what kind of conference you're running. Rust Belt Rust is going on, for example, and I don't think /u/carols10cents has 50k just lying around to shove into it :) But it is true that conferences can take a certain amount of money up front, and depending on where it's being held and what your ambitions are, they can be expensive.
Which inconsistency are you thinking of? This specific syntax wasn't ever proposed, no. Rust did used to have a lot more symbols, and a lot of people _really_ hated them, so we steadily removed the amount over time.
I'm speaking full budget and 50k is not unusual, even for small-scale events. That's 200 tickets at 250 Dollar. (smaller sizes might obviously be smaller)
Yup, that is what I fear might turn out to be confusing. I fear it might be confusing to have some variables get their lifetimes implicitly extended where most do not. That could make it quite surprising what code compiles and what code does not, and make lifetimes seem magical.
Sorry, but that point has been proven wrong at least a thousand times so far. Atom and VS Code are just as much IDEs as any other IDE. IDEs are all editors with development features.
only if there are destructors involved
The 2 are not the same but they are similar. It's more like "local mutability" I suppose. Consider: let len = 5; ... unsafe { //use len } Someone modifies the function and adds: let len = 5; ... let len = 1000; //use len ... unsafe { //use len } Alternately, someone could add the unsafe use of `len` without realising that the variable had been redeclared. Because `len` is (locally) mutable these mistakes are possible. You need to read the entire function before declaring a variable because you might change an assumed invariant. If `len` was declared `const` then this mistake cannot happen.
SEME regions are not closed under union. The motivation for our rule is that when exiting a region all types containing that region become indeterminate and must not be used again. 
This looks really cool. Is it a novel algorithm?
I installed [hardtime](https://github.com/takac/vim-hardtime) to train myself to use the rest of vim. Within a day I was my old speed, within three I was much faster. It strikes a nice balance over disabling arrow keys and `hjkl`, because they are occasionally useful.
VSCode + RustyCode + Racer
I agree that setting YCM up is not as easy it is to set up other Vim plugins, but I disagree that the root cause is overengineering. For C-family languages one _must_ provide compilation flags in some way, otherwise YCM can't compile the source files (and thus can't provide semantic completion). The way to provide these flags to the plugin is with the `ycm_extra_conf.py` file which allows the user to write arbitrary code to compute these flags for a given filename (because the flags often differ across files in the same project). For most people, the process of setting up that file is to copy the example file linked from the docs and change one list of strings. That's it. But lots of people need integration with their build systems and a custom extra conf file makes that possible. Examples of such integration: using CMake as a build system, or Bazel, or Facebook's internal build tool, or Gradle, or waf, or scons etc. All of these can be supported with YCM's approach. For people editing Rust code, the `ycm_extra_conf.py` file is irrelevant. For such users, the setup is almost always just`./install.py --racer-completer` in the YCM folder. That's it.
Assuming that you can end a variable's lifetime by giving it to a function that takes it by value (such as `std::mem::drop`), this will actually fix a lot of really annoying edge cases I've had to wrestle with.
Neither of those plugins provide semantic code completion (which is the reason for the extra configuration pain in YCM), so it's not a fair comparison. If one didn't use the semantic completion in YCM, one could easily skip that extra configuration and have a similar setup experience as for Deoplete or VimCompletesMe. You'd literally have to only run `./install.py` in the YCM folder and you'd be done.
My Rust editing is done in Vim with `rust.vim`, `syntastic` (for error/warning messages in the Vim gutter) and `YouCompleteMe` (for semantic Rust code-completion and GoTo).
Always happy to see satisfied YCM users! :D
Someone could do this, too: let len = 5; ... some_move_function(len); let len = 1000; ... unsafe { //use len } And this would be perfectly legal yet also break the code (assuming `len` weren't `Copy`.) So you couldn't reason that code is unchanged between arbitrary statements even if local shadowing weren't allowed. Aside: You keep saying things like "`len` is mutable", but it's the binding that's mutable, not the variable. It's an important distinction to make when you're assuming they are not related in 1-1 fashion.
Of course, yes. But it wouldn't be automatic, unless we added some kind of opt-in `#[drop_whenever] ` mechanism for structs. So a future newbie could conceivably learn the non-lexical lifetime system, not realize it wasn't always that way, and wonder why `Ref` doesn't work the same way. I'm still massively in favor of non-lexical lifetimes though.
&gt; Which inconsistency are you thinking of? **Implicit** copying instead of taking ownership with `Copy` types. I'm not sure, but I think I read somewhere that `Copy` was implemented for ergonomic reasons (to avoid typing `.clone()` over and over with some types). Using a symbol is both explicit, and ergonomic IMHO, hence my question if it was ever considered. 
&gt; Yes, this can lead to unexpected behaviour (cause bugs) but it is an issue of mutability (the label's mutability, not the variable's). Yeah, definitely, it is fundamentally the label that is mutating - it is referencing some new value. However, I see this as being very strongly and fundamentally different from mutable data, and it leads to a whole other, independent area of bugs. I just wanted to be clear on that in my post since it doesn't really feel like this subverts 'const correctness' in any way. So, yeah, I think it's valid to not like variable shadowing. Personally, I *really* like it. But I can see how it may lead to bugs, though I think the type system protects you here to some extent - if x1 and x2 are different types, you will get a compile time error when you try to treat x2 like it's still x1. I like it because it lets me guarantee that I'm "done" with a variable, or I can freeze a mutable variable by reassigning its name to itself ie: let mut x = 5; x += 1; // necessary mutation for whatever reason let x = x; Now I know that x can't be mutated again - that original x can't be referenced anymore. This is one of my favorite patterns in rust, because I find it to actually limit mutability, though I do like the scoped version of this: let x = { let mut x = 5; x += 1; x }; But to each their own, really. Thankfully, projects like clippy provide lints for these things.
The only difference between copy and move is that you can't use a previous value after a move. They're identical otherwise. So really, this relationship is the other way around. We did use the "move" keyword for moves, which is kind of the reverse of what you're asking about, but it didn't feel great. It was essentially busy work to annotate what the compiler could unambiguously infer. Clone is a very different beast than the other two: they are "memcpy some bits", clone is "run arbitrary code to produce a copy." So clone has the explicit behavior, because it has very different characteristics, but copy and move are almost identical, and so aren't distinguished from each other. Does that make sense?
Never heard of such a thing, this looks super cool.
I've never submitted a talk before, but I'd like to - what makes a good abstract for a conference submission?
&gt; I want to learn what makes such things work. (disclaimer: I haven't written either of these yet) Real web servers should be able to handle multiple connections per thread. That requires plate spinning (technical term "asynchronous I/O") and you'll learn an *awful* lot about plate spinning. The most popular Rust crate for async I/O appears to be `mio`. A simpler protocol, like a subset of IRC, might be a wise choice. But for messing around with HTTP and client-side/server-side scripting you really just need something that can handle a few dozen local connections, serving files and CGI. Spawn one thread per connection, read the HTTP RFC, make it work with a real browser. From what I understand, node.js uses both extensively under the hood. Pick your challenge.
You're correct, I shouldn't be flippant and call `len` itself mutable. Could you elaborate on `some_move_function` (perhaps give an example)? I'm not aware of how `len` could be mutated in this way.
I'm not an expert in information theory, but I'm pretty sure the comparisons to Levenshtein distance are suspect. It measures the actual number of edits needed to turn one sequence into another, while OP's distance operates on hashes and appears to be able to overflow (though it's late and I'm on mobile, so big grain of salt). https://en.wikipedia.org/wiki/Levenshtein_distance Also, was the creator of "Eudex" (presumably OP) a linguist? How does "Eudex" actually compare to Soundex (or other established phonetic algorithms) in terms of, say, correctly grouping all common English words?
While y'all are in portland can someone check up on Carl Lerche and make sure that guys ok? Git has been quiet. The guy who made mio. Nothing broken there just haven't been any commits in a while.
Thank you for your detailed reply. And for clarifying the difference between `Clone` and `Copy`. I didn't know `Copy` behavior was the default. &gt; The only difference between copy and move is that &gt; you can't use a previous value after a move. &gt; They're identical otherwise. So, with `move`, the old value is copied then dropped? It would be interesting to know at what level that's optimized out (HIR, MIR, LLVM).
If you visit South Korea, ping me so we can meet in real life. :)
Technically, move is the default; you opt into copy by implementing Copy. The old value is copied, but not dropped: dropping happens when the owner goes out of scope.
Carl is very much around and doing Rust; what things are you referring to? I'll ping him about it.
Spacemacs deserves another screenshot. Neotree showing a git project, split windows, &amp; auto-completion. I'm using the Dracula theme and the excellent Hack typeface. http://imgur.com/gMlCjXm
https://doc.rust-lang.org/book/syntax-index.html
It turns out real work takes a lot of time :)
Which color scheme are your using?
&gt; You need to read the entire function before declaring a variable because you might change an assumed invariant. In general you need to read the entire function anyway because of type inference.
You don't even need nightly. `std::sync::atomic` contains hacks such as `ATOMIC_BOOL_INIT` expressly so you can put them in statics.
One of the first plugins I ever installed. Still using it to this day after 5 years. Thank you for your awesome work!
Eudex is an excellent name. I'm interested in where the algorithm comes from as well. And, agreed, Levenshtein is much more general (having applications to DNA, any strings, etc.) but as they're both used in the domain of spellcheck suggestions I'm less suspect.
Ok, I see what you're saying. Thanks for that. It seems like this is just a pattern I'll have to get used to as idiomatic Rust. I still think we're missing an equivalent to `const` though but I doubt adding it will be contentious.
You can totally shadow variables in Elixir. The following is valid Elixir. x = 0 x = 1
That is true. Coincidentally I'm also not a fan of type inference but that's the way modern languages are going. At least with type inference a good IDE can tell you the type via mouse over or something.
I am on Mac OS X. I dicided last night to do some profiling and noticed high cpu usage and a lot of context switches
I've since updated the hello world example to now spawn a thread per core, sharing the listener among them all. You may see better performance. 
I'm running rust on a machine without a DE at the office (so no browsers), having man pages would be great
I think the idea itself is quite clever. The algorithm exactly works like Soundex but uses a Hamming distance to *improve* the similarity measure. Each consonant (modulo a weak `h`) receives the following 6 bits: DISTINGUISHES FROM +------ bcdfgjklmnprstvxz qw |+----- bflmnpqrw cdgjkstvxz ||+---- dlmnqstvw bcfgjkprxz |||+--- cdgklqrtvw bfjmnpsxz ||||+-- bcjlqvw dfgkmnprstxz |||||+- cfjkmrstvwx bdglnpqz b 001101 c 011000 d 010011 f 001110 g 011011 j 011100 k 011010 l 000001 m 000110 n 000111 p 001111 q 100001 r 001010 s 010110 t 010010 v 010000 w 100000 x 011110 z 011111 (Actually they receive 8 bits, but most significant two bits are padded presumbly for the later extension.) Vowels and duplicate letters are ignored as like Soundex. The resulting hash is `u64`, and the most significant 8 bits are reserved for the first letter. The algorithm considers two words to be similar when there are at most 4 bits of mismatches. While the algorithm is clever, if I have analyzed correctly there are some deficiencies and problems that can be significant: * How did you derive the mapping table? I first thought that it is initially derived from Soundex mapping then refined (especially when there are 6 bits used per each letter, as it's same to the number of Soundex-mapped letters), but it is... quite random. Some pairs are too much apart: d(b,v)=4, d(q,k)=5 etc., and some pairs are too close: d(q,w)=1, d(p,z)=1 etc. It is hard to make all pairs reasonable, but maybe reserved two bits can have been used for that. * It overflows when there are too many non-duplicate consonants, as the comments say. What the comments do not say is that i) it will panic in the debug builds and ii) it will bleed into the first letter slot. You'd better stop at 7 consonants. * It makes much more sense to me that the first byte slot is also mapped. ASCII itself is not very good fit for Hamming distance. It will also have an effect of vowels being mapped to the same value, as Double Metaphone and later did. 
The reason for this is that in Haskell, all bindings are recursive by default. So let x = 42 in let x = x + 1 in ... would loop forever, since the `x` in `x + 1` refers to the *inner* `x`, not the outer one. ML does not have this problem, since it has different keywords for non-recursive and recursive bindings (`let` vs `letrec`). It's one of the things I prefer in OCaml over Haskell.
Or [`wrapping_macros`](https://github.com/lfairy/wrapping_macros)!
Can you give some examples where it would fail and what one of those "many factors" are? I am just curious and never worked in that kind of problem domain.
nikomatsakis's proposal is that regions are unions of live ranges, not live ranges by themselves.
If niko's plan comes to life, yes. And as a bonus, it will be automatic if no destructor is involved.
The original table is derived such that similar phones are adjacent (their XOR value is low), in a similar manner you describe. I then adjusted it to various dictionaries (see the readme), to get better results, so it is far from random. I'll look into improving it more, though. &gt; It overflows when there are too many non-duplicate consonants, as the comments say. What the comments do not say is that i) it will panic in the debug builds and ii) it will bleed into the first letter slot. You'd better stop at 7 consonants. [I explained it here](https://www.reddit.com/r/rust/comments/4hwygb/introducing_eudex_a_blazingly_fast_phonetic/d2tj32p). TL;DR is that it is a very little problem in pratice, and it is not worth trading for the performance. &gt; It makes much more sense to me that the first byte slot is also mapped. ASCII itself is not very good fit for Hamming distance. It will also have an effect of vowels being mapped to the same value, as Double Metaphone and later did. It is based on the fact that the first slot is rarely misspelled and often very crucial to the pronounciation. --- Thanks for the feedback, and your explanation of the algorithm is spot-on. Edit: I have adjusted the table to reflect your criticism, and the first slot is now mapped, although a slightly different table.
&gt; So quick and dirty, ok, but only on a smallish data set, or with some extra step to filter the better. The result should be refined obviously, which is the whole point. The special cases are not worth treating due to performance concerns.
But its only faster when searching through a linear list. If you build a trie, you can do Levenshtein pretty quickly. &gt; The result should be refined obviously, which is the whole point. Then you'd still need to search a fairly large chunk of the items in order to guarantee the best candidate is in the initial list. I can try to see how much time it takes to retrieve the best candidate in a trie, if you're interested. For now I can tell you that loading takes 0.08 seconds for a 90k unique word list, and 0.25s for the 250k /usr/share/dict/words, using my own trie implementation.
Is it possible to have a way to upgrade a Rc pointer to an Arc pointer when we need to pass it on multiple thread?
&gt; You don't need that many Well, it's quite difficult to find the similarity between the various homophones in English using your method. What's the distance between write and right? I think it is 360E15, and there are over 60000 words within that distance between write and right. E.g. the distance between write and abdominohysterotomy is 289E15. So I'd say you need quite a few for general spelling correction.
I copied your source code, but println!("distance = {}", distance("write", "abdominohysterotomy")); distance = 289927017402079509 Odd. Version 1.8.0, OSX 10.11. Edit : copied the wrong line.
Would be cool if we have one in Europe too, maybe in Germany as there is a lot of programming going on in Rust here too. We have RustHamburg, RustCologne and RustBerlin.
In this case I would just use `Arc` in general. On paper, it's significantly slower than `Rc` because of the explicit atomic operations in updating the refcount, but unless you're cloning/dropping them at an absurd rate it shouldn't be a bottleneck. `Rc` is for when you *know* that you won't need to share the value across threads.
Either add #[allow(dead_code)] or call the functions in the private modules from the pub mod
For reading integers, I recommend the [byteorder](https://crates.io/crates/byteorder) crate; the [`ReadBytesExt`](http://burntsushi.net/rustdoc/byteorder/trait.ReadBytesExt.html) trait is what you should be looking at. Note that you have to explicitly designate an endianness, while that is left implicit in your C++ example, which is a potential footgun: if it uses the local CPU's endianness then you're just asking for bugs when sending data between machines of different architectures. Edit: I later realized that, if they knew what they were doing, the people who wrote the C++ TCP stream class you're using should have hardwired it to read and write integers in network byte order (big-endian). It would still be an implementation detail which, if it isn't documented, you would have to either assume is correct, or dig through the source to make sure it is. As for reading strings, you can use [`BufRead::read_until()`](http://doc.rust-lang.org/std/io/trait.BufRead.html) from the stdlib to read until a nul byte, then convert the bytes to a string with [`String::from_utf8()`](http://doc.rust-lang.org/collections/string/struct.String.html#method.from_utf8). ~~You'll want to do `.consume(1)` on the `BufRead` to eat the nul byte so it doesn't get added to the next value you read.~~ Edit: the nul byte will be at the end of the vector you read into, where you'll want to remove it so it doesn't screw something up later. If nul-terminated strings aren't a hard requirement, I highly recommend [bincode](https://crates.io/crates/bincode) for a batteries-included solution, where you just define your datastructures and it does the encoding/decoding work for you.
... to add to the edit above: The table now looks like | Position | Modifier | Property | Phones | |----------|----------|-------------|----------| | 1 | 1 | Nasal | mn | | 2 | 2 | Plosive | pbtdcgqk | | 3 | 4 | Fricative | fvsjxzhc | | 4 | 8 | Approximant | vrhjwc | | 5 | 16 | Trill | r | | 6 | 32 | Lateral | l | | 7 | 64 | TypeÂ¹ | mpbfv | | 8 | 128 | ConfidentÂ² | lrxzq | Â¹1 means labial or dorsal, 0 means apical. Â²hard to misspell. This better results than the previously used table (oh, how I wish there were more than 8 bits in a byte).
&gt; but as they're both used in the domain of spellcheck suggestions I'm less suspect. Yeah, as I said elsewhere, it only replaces Levenshtein in certain use cases. The algorithm is based upon the pulmonic consonants' classification, which are used to permute the byte values such that the Hamming distance is more accurate as a word metric, and then a (slightly more complex) Soundex-like reduction round.
Its been a while so I could be wrong, but don't those warnings go away once tests have been written for the code? It would probably be a good idea to write those tests.
Don't really want to allow dead code; that's a bit too extreme for me.
I like to use `cargo doc` to browse dependency documentation locally. Is that different from what you're suggesting?
This is probably it. If on the off chance that you're including an external library as a module, you can achieve the same setup by moving it to it's own subcrate. This will stop the dead_code warnings and still allow you to have local dependencies.
How do you include `b`, `c`, and `d` in `a`? Do you use `use b::{whatever}`, or do you declare these modules again by including with `mod b;`?
Thanks
Really pretty website! One nitpick though: I was going to recommend that you have pages demonstrating short code examples for the list of features at the top of the page, but then I noticed that you could click on those items and it leads to a really detailed page! I'd highly suggest that you add buttons or something else to show that those items can be clicked on. Edit: after reading through all the docs, I'm convinced that this is now the gold-standard for library documentation in the Rust ecosystem.
Oh yeah, but man pages would be so much nicer. I have a little bit of envy for `perldoc` I guess.
Well, I think the base of the objection is that it would mean implementing `Drop` could mean you have to rewrite lots of code. I would assume this is also the case if one of the types your types contain does that. I'm not sure there are current widespread usages that fail to compile once `Drop` is implemented. It is certainly the case for other parts of the language, like removing a `Copy` implementation. But it could be argued that in that case, you need to choose to opt-in to `Copy` semantics in the first place.
Tests don't count as using code. If you write a function that isn't public facing, and isn't called by anything that is, then it's considered dead code whether you test it out not.
Awesome thanks, I'll give it a try.
If reading the entire function while changing it is too much to ask, it should clearly be broken up into smaller functions. I doubt the language is the actual problem here.
While not a short-term solution, I'm hoping that Zeal will eventually catch up with Dash's support for downloading and autogenerating docsets from sources like godoc.org. Then, it should be trivial to add a `cargo doc` backend.
There are plans to allow custom mirrors, or running a completely private crates.io service -- certainly, this has been the desire all along. AIUI, you can do it today, but it's a bit rougher than we'd like.
Cargo currently downloads from crates.io by default, which is in turn hosted on S3. That is, if github goes down, repos are deleted on github, or github just disappears overnight, you can still download crates. Note, however, that the *index* is [stored on github](https://github.com/rust-lang/crates.io-index), but there's no intrinsic tie to github, it just needs to be a git repository hosted somewhere. Custom mirrors and custom registries are planned for crates.io (see https://github.com/rust-lang/crates.io/issues/67), but unfortunately aren't implemented right now. That being said, crates.io and Cargo are certainly being designed around this use case, so it should always be possible to do so in the future!
That looks really nice!
But these warnings are telling you you have dead code. :-\
This. I've had this exact scenario on a plane recently. What's worse is all the downloaded crates were there, but cargo simply wouldn't continue due to not being able to update the registry.
Please file bugs if this happens, because it shouldn't.
This is a *tremendous* amount of work! Really well done. With all the work being done, I see nphysics becoming the go-to open source physics engine. I was thinking about writing some Bullet bindings for anima-engine just because it has better joints, but I'm seriously considering integrating nphysics just in order to promote it.
&gt; Npm still doesn't emit a lock file so you have to manually lock versions by editing package json. OT, but I want to prevent you from going further down into keeping-track-of-library-versions-hell: You have to manually call `npm shrinkwrap` to write lock files. Which is also far better than just editing the `package.json` because you want to lock the versions of indirect dependencies as well.
I haven't had a need for using external debugging tools with Rust throughout the last year of programming with it, so those are not features I use or need.
It still seems weird to me to also use it as an appstore. I'm working on a GUI program that doesn't have any library component, but I would think it should be distributed separately from crates.io, as tarballs, rpm/deb, or as part of the OS. There doesn't seem to be any clear advice on what should be on crates.io, but if it isn't going to be a dependency of something else, I don't see what crates.io provides that gitlab/github don't. And you can duplicate less by having it just in a repo instead of in a repo and on crates.io.
&gt; Today, Cargo will automatically share dependencies between crates if they depend on the same major version (or minor version before 1.0), since Rust uses semantic versioning. This means that if `nix` and `datetime` both depend on some version of `libc 0.2.x`, they will get the same version. In this case, they do, and the program compiles. &gt; &gt; While this policy works well (and in fact is the same policy that system package managers use), it doesnâ€™t always do exactly what people expect, especially when it comes to coordinating a major version bump across the ecosystem. [...] &gt; &gt; We have some thoughts on ways to improve Cargo to handle these cases better, including the ability for a package to more explicitly express when a dependency is used purely internally and is not shared through its public interface. Those packages could be more readily duplicated if needed, while dependencies that are used in a packageâ€™s public interface must not be. You should expect to see more on this topic in the months ahead. Yeah, this one is critical to get right. The Haskell community has struggled for many years with "cabal hell" problems where libraries specified complex version bounds for dependencies and it often took quite a lot of effort to identify a set of dependency versions that actually could build together. This was made even worse by the fact that the Haskell community, like just like proposed in this Cargo article, is a big fan of very small libraries. The solution that Haskell has been using lately is [Stackage](https://www.stackage.org/), a repository of **snapshots**, that you can think of as public curated `Cargo.lock` files for a large set of crates. When you initialize a new source tree then the build tool examines your declared dependencies against the available snapshots and the regular uncurated Haskell repository (Hackage), picks a snapshot for you to use and records it in a `.stack.yml` file. This has been in place for less than two years now and it has done a lot to make Haskell easier to use. Though I worry how scalable the whole thing is.
Great stuff! I'm getting ready to do some benchmarks against Bullet and FCL. I'd love to use this in my robotics work.
It sounded like /u/Danylaporte wanted to avoid unwrapping if possible, as that'd put extra pressure on the allocator if it was being done frequently (although a caching allocator likely wouldn't have a very hard time of it for the same reason as below; it'd probably just directly reuse the allocation). It'd be a horrible, horrible hack, but hypothetically, if it was unique, it would be safe to transmute an `Rc` to an `Arc` of the same contained type, or vice-versa, because both are just wrappers around a single on-stack pointer and their backing stores have the same memory layout: two pointer-sized integers and the data. AFAIK, atomic vs nonatomic just comes down to an alignment requirement, which isn't a problem because the allocation will be aligned anyway.
Ah, I must have glossed over that in the documentation. You'll want to drop the nul byte, still, if you're converting it to a Rust string which doesn't need the nul terminator.
&gt; I'm very new to systems programming so I'm not familiar with all of the terminology just yet so please excuse me if I misspeak. No problem! Welcome :) "cc" is the most generic name for "a c compiler". So gcc should work. But a few things, and I am not a Windows user, so I might mis-speak myself :) Did you download the msvc or gnu versions of Rust for Windows? I wonder if you downloaded msvc, but you're trying to use cygwin, I bet that'd have issues. Furthermore, the docs seem to imply that you want msys or mingw, not cygwin. I wonder if that's part of it.
Why not host the index on S3 too?
The conventional wisdom is that "cargo install" is supposed to be for tools that slot into Cargo (e.g. cargo-check), or otherwise for tools that directly relate to developing Rust. Though of course there's no hard rule that that's what it *must* be used for, and I would encourage people to distribute more typical applications through the usual channels rather than relying on Cargo.
I wasn't aware that we had integrated something from xi into Servo, I'm impressed that not only did someone take a look at xi and realize that it contained a component that would be useful to Servo, but also managed to integrate it so quickly.
So, suppose I have a `pub struct MyStruct(i32)` in my current API. I now realize I need to do either * `impl Drop for MyStruct` * Change `MyStruct` to `pub struct MyStruct(i32, Vec&lt;u8&gt;)` (i e a type with an internal destructor I understand the first one will increase `MyStruct`s liveness, which is a API breaking change, I'd assume. But what about the second case, where the destructor is hidden somewhere inside the field? Will that also increase the liveness of `MyStruct`?
Hah, I actually did download the MSVC version of Rust. I'll give msys or mingw a try. Thanks for the help, I really appreciate it!
&gt; It'd be a horrible, horrible hack, but hypothetically... All the best threads start this way :)
Maybe you can just install the MSVC c compiler then? Should work that way too.
Please share your results. :)
How can I write a generic function that wants a T that once borrowed implements IntoIterator? This is what I currently have: pub fn voxelize&lt;'b, T, B&gt;(mesh: T) -&gt; VoxelBuffer where T: 'b + Borrow&lt;B&gt;, &amp;'b B: IntoIterator&lt;Item = &amp;'b Triangle&gt;, B: 'b { unimplemented!(); } While this compiles, it complains when I want to call it, as it can't infer the type of B. I feel like this can be solved with Higher Ranked Trait Bounds maybe? But I couldn't figure it out so far. Update: I figured it out pub fn voxelize&lt;T&gt;(mesh: T) -&gt; VoxelBuffer where T: Deref, for&lt;'c&gt; &amp;'c &lt;T as Deref&gt;::Target: IntoIterator&lt;Item = &amp;'c Triangle&gt; { unimplemented!(); }
 &gt;In general you can gain the same benefit with caching in Iron just as far as I know you have todo more manual work because rails is more high level than iron. Bingo. But my question initially was how.
Does cargo distribute binaries or only source?
This looks useful. What is the workflow like for repos that contain multiple crates? Do you publish them individually? How does that affect the tags and docs?
Only source at this time.
Crates are just files. The index is a git repo so it's not really appropriate for simple S3 hosting.
You could allow dead code on just specific functions (the ones that are never called from public code)
&gt; Are there other cases where implementing a trait will cause semantics in remote code to change like that? There are: struct Foo; trait Bar { fn blah(&amp;self) {} } trait Qux { fn blah(&amp;self) {} } impl Bar for Foo { fn blah(&amp;self) {} } // Try uncommenting the following line... //impl Qux for Foo { fn blah(&amp;self) {} } fn main () { let foo = Foo; foo.blah(); // ...and you'll get an error here! } In particular, note that it's perfectly legal for a type to implement two traits that happen to have identically-named methods, but it means that if you ever attempt to invoke that method while both those traits are in scope then you'll get a compilation error at the invocation site (this is what UFCS is designed to circumvent). It's quite rare to encounter this in real code, and I bet that most Rust developers have never even heard of UFCS, let alone used it. &gt; do you disagree that having to rewrite it is bad? Yes. In practice, it is rare to implement `Drop` on something once your design is in-place (generally when you need a destructor, it's obvious from the outset). Less rare is changing the definition of a type to now include something that just so happens to have a destructor (like sticking a `Mutex` on some field), but people are already accustomed to needing to perform widespread refactoring when changing the structure of their fundamental data types. In the meantime, the patterns that NLL would allow us to express that cannot be expressed today would go a long way towards easing Rust's learning curve ("that damn borrow checker"). People will attempt to use references in ways that they want to use them, and in today's Rust there's plenty of "obvious" uses of references that don't pass borrowck's muster, which can lure people into thinking that even *actually* incorrect usage of references are just false positives. Right now they may be forced to contort their code for wholly spurious reasons. But in the NLL future where implementing `Drop` can cause a lifetime to become extended, that means that they'll just need to contort their code for a wholly *legitimate* reason.
for multiple-crate repo, you need to run `cargo release` for each crate just like `cargo publish`. The tag is prefixed with sub-directory name, for example, `serde-macros-0.2.1`. You can also specify custom tag prefix using `--tag-prefix` option