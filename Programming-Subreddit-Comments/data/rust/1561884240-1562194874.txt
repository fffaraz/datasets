Writing 1,000 lines to the terminal with `print!("{}\n")` will probably be fast enough. :-) (Note: Manually appended newline, not `println!()`, since it flushes the output.) If you do start doing it many more times, maybe the speed of the logger becomes a question.
Generally as fast as possible. If asynchronous writes push the performance, why not? :) The performance will of course be capped by the consumer, but there isn't any way to avoid that, is there?
ok, so there is two ways to use generics. One is to have a struct/function template that the compiler specializes for each real type that its used for, and one is to have one function that takes any type that implements that trait. the first is called static dispatch, the second dynamic dispatch. When using dynamic dispatch, the compiler can't know how big the passed type actually is, because it might be one of many, in fact, the compiler might not even know all the possible types yet!. So to make that work you need to give a reference to the trait object. i have written a [detailed example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b48340cb344eb4f888be479015d64d47) for you. also notice how it says dyn in the dynamic dispatch. you don't have to write that, but it helps clarify things for human readers.
You have to acquire the stdout lock once. And you can‚Äôt avoid being dependent on the speed of consumption, unless you want to lose data. I.e., you buffer output in memory but memory is finite and when you run out of it, you‚Äôll have to either wait for the consumer anyway or start dropping data from memory.
Ad 1: Using Redis is pointless here IMO - Rust's channels or async programming would impose a ton less overhead. Ad 2: Java threads are not OS threads? Are you sure?
Post your entire program, more specifically the declaration of `fn main()`.
Perhaps you can hire someone like me who speaks some Chinese and can work with you in the same city so the two of you as a small team can cooperate better with the big Chinese teams in China üòÅ
It is important that the printing scales well - while printin 1000 lines with print!() might be fast enough, printing 10.000 lines must not take 10 times longer. How does a simple print!() call scale?
I feel your pain. r/playrust was in my recommendations, /r/rust wasn't. :D
The `main()` is shown, and is wrong. Use fn main() -&gt; std::io::Result&lt;()&gt; { as in the linked example and the code should compile.
So a simple stdout lock and several print!() calls will do the trick while being fast? It is ok to dependent on the speed of consumption, since correctness is even more important than pure speed.
https://rust-lang-nursery.github.io/cli-wg/tutorial/output.html#a-note-on-printing-performance might be of interest to you
If you're printing to the console, the dominating factor will scale linearly no matter what you do. Going below linear, you would need something very clever, like multiple threads each writing to their own buffer, and then piecing them together later... And most importantly you need to not be writing to the console. ;-) `print!()` just formats the message and writes it to a buffered `stdout`. Eventually, the data will be written with the `write()` system call. At that point, your terminal will spend some time rendering the lines, which is likely to be by far the slowest operation here. Using `println!()` flushes the buffer on every write (similar to `std::cout &lt;&lt; std::endl` in C++), so it is slower, but you need some volume to be able to measure the difference.
Nice.
You're looking for /r/playrust.
‚Ä¶at its finest
Why the main function should have return like that but the TCP main function doesn‚Äôt ( I saw that in the rust book ) ?
Fair enough! A lot of real projects out there, use High level logic / orchestration logic in one technology, while hard core functionality implementation in another. I can see this to work very well with Javascript and Rust, and I'm considering to do my next project with similar structure
You need to [read up](https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html) on the `Result` type and error handling in Rust. In recent versions of the language `main()` may choose to return a `Result()` for convenience.
 let stdout = std::io::stdout().lock(); stdout.write(...); Not sure how `print!` works. You can also wrap `StdoutLock` in `std::io::BufWriter` for less write calls.
Honestly, that was a bit my impression. Its mentioned nowhere.
I'm running rust on an embedded platform, and to get the timing of something right I want it to idle the processor for a fixed number of cycles. The idea is that I want as precise control as possible over when a pin goes up or down. Ideally I would like to be able to adjust it to one cycle earlier or later. In assembler this would be easy, but rust optimizes, which makes it harder. I tried to do a loop like this: for _ in 0..10000 {} but (as expected) it just gets optimized away. Is there a good way to do this, or is inline assembler the only way to go?
I may be misunderstanding why you use Zeal, but if it's because the core/std documentation is online, you can also browse it offline with \`rustup doc\`, if you use \`rustup\`.
I personally like a mix of asynchronous and synchronous communication. Asynchronous is good at giving you time to think, whereas synchronous is good to hash things out (aka brainstorming). As long as you use each to its strengths, which is easier said than done, both are quite empowering. *Source: 12 years of working in "follow-the-sun" companies.*
If speed is of the essence, and you *really* wish to print to the console, use a better console. The difference between writing to the console and redirecting `stdout` to a file (or even `/dev/null`) is generally pretty significant for any program, which means the problem is not in the program, but in the console, most of the times. There are mistakes a program can make, especially no buffer, too small buffers, and flushing too eagerly (don't use `println`), of course, however once those are solved the console itself becomes a major bottleneck. It may be something that alacritty helps with, personally I just redirect to file. Plus the file is great to play with later (`grep`, ...).
The point is that the program is a general-purpose text search which should work in any terminal. I guess the only thing i can to to reduce the console as a bottleneck is to advice my users to use a fast console.
At the same time, I wonder if you are not worrying too much for nothing. Normally, output to the console is meant for *human consumption*. *I* don't want to manually read through 10,000 lines. It'll take me ages. And I may miss the one important line as my eyes start glazing over. In general, I don't think outputting more than a page worth of results is any useful for a human reader. Especially if the terminal has limited scrollback.
/r/lispmemes would have a field day. Orange crab bad!
Absolutely, I have this problem in mind. However, I don't have a solution yet - since limiting the search results to a certain number isn't desirable regarding correctness.
[This thread](https://www.reddit.com/r/rust/comments/c4x9ww/i_havent_programmed_since_school_looking_to_pick/?utm_source=share&amp;utm_medium=ios_app) from a few days ago has some suggestions.
One solution could be using paging when you detect that the output device is a `tty` (C's `isatty`?). When you `git log`, for example, the results are displayed in the program of your choice: `less`, `vim`, etc... so that only the first page is displayed and the user can "scroll" at leisure. This allows the user to realize that they vastly overestimated the specifity of their search results and to refine their search, while at the same time allowing a user who wants the full set to just pipe everything into a file or into another program.
your comment made my smile merci
Why not build a Linux machine?
Or even better, provide tools for volunteers to help translating crate documents. I think the team is indeed considering this, and the translation of website is just the first step.
AFAIK, it is recommended to use \`dyn\` where it applies.
I use xclip as well but recently I accidentally discovered fcitx( I need it for Chinese) has a gtk pop-up of the clipboard contents when you press ctrl ; combined with Linux mouse middle button is extremely useful.
I was using `env-logger` and noticed it took over 90% of my servers cpu time. I couldn't find a lightweight, fast alternative so I wrote my own. I spawned a dedicated thread that does the writing, and logs are added to a non blocking queue.
I've applied Gamepad&lt;'_&gt; everywhere I can think of. Here's the actual problem at hand: method gilrs_gamepads(&amp;self) -&gt; Vec&lt;Rc&lt;RefCell&lt;Gamepad&lt;'_&gt;&gt;&gt;&gt;; alias getGamepads; And here's the error: error: 'Can not find such rust type &amp; Rc &lt; RefCell &lt; Gamepad &lt; &gt; &gt; &gt;Can not find conversation from &amp; RefCell &lt; Gamepad &lt; &gt; &gt; to &amp; Rc &lt; RefCell &lt; Gamepad &lt; &gt; &gt; &gt;'', /home/mitchell/.cargo/registry/src/github.com-1ecc6299db9ec823/rust_swig-0.3.0/src/error.rs:86:5 Removing the smart pointers gives this: self_type Gamepad&lt;'_&gt;; And this error: error: 'Do not know conversation from such rust type 'Vec &lt; Rc &lt; RefCell &lt; Gamepad &lt; &gt; &gt; &gt; &gt;' to foreign'', /home/mitchell/.cargo/registry/src/github.com-1ecc6299db9ec823/rust_swig-0.3.0/src/error.rs:86:5 And here's where I define Gamepads foreignly error: 'Can not find such rust type &amp; Rc &lt; RefCell &lt; Gamepad &lt; &gt; &gt; &gt;Can not find conversation from &amp; RefCell &lt; Gamepad &lt; &gt; &gt; to &amp; Rc &lt; RefCell &lt; Gamepad &lt; &gt; &gt; &gt;'', /home/mitchell/.cargo/registry/src/github.com-1ecc6299db9ec823/rust_swig-0.3.0/src/error.rs:86:5 Here's where it gets gamepads from: pub fn gilrs_gamepads(gilrs: &amp;Gilrs) -&gt; Vec&lt;Rc&lt;RefCell&lt;Gamepad&lt;'_&gt;&gt;&gt;&gt; {
Thanks for sharing your experience with that crate.
Nice post, will definitely try the JetBrains tools + Rust plugin
Holy shit, I never heard of Zeal but I've always wanted something like it! I will definitely try it asap
I don't expect that you may get much sympathy here. Cross-culture teams seem to be very common in English-speaking countries and companies. There are usually significant portion of team members come from different culture background in the western world especially in tech industry. I have worked in this kind of teams and companies. And that works fine as far as everyone considers English to be the work language. So I think the problem you and your team hit is probably more from the fact that your dominated language is Chinese, but to communicate with remote people you have to use English. This inconsistency may add confusion and friction. I don't have a solution to this, though...
RDBMS refers to any relational database, including research databases that follow the relation model. A research database is a database (not necessarilyaa relational one) that is being developed as a research prototype, usually in academia.
Hmm.. That really depends on the food. Balsamic Vinegar maybe?
I don't think any compilers are real time? The Rust compiler has to do a lot of work that other compilers do not (like borrow checking), though I don't think that's where most of the compile time comes from. Instead, it probably arises from Rust compiling all of the code from a single crate as one unit. This enables more optimizations, but also slows compilation. There's some work on improving that situation, like multiple codegen units paired with ThinLTO, but we still have some way to go.
I must admit, I never thought of `rustup doc`. I got so used to using zeal for all sorts of technologies it just intuitively followed to use it for rust as well. Thanks for the hint!
Oh that sounds very nice! I'd love to hear more on this once you have something you can share.
That... Seems wrong. I'd be curious to know what exactly env-logger is doing so badly as to take 90% of your CPU. Just moving it to another thread shouldn't lower the CPU time it takes, that just moved it out of the "hot path." At any rate, the slow thing about logging usually isn't CPU related at all - it's the IO, which is several orders of magnitude slower than CPU-only operations.
Unfortunately, to my knowledge, there's not a way to tell LLVM not to optimize a function call out. [Looking at this SO question](https://stackoverflow.com/questions/42891039/how-to-prevent-function-calls-from-being-optimized-away#answer-42893060), it looks like you can use nops with volatile (or dummy a value) to get LLVM to not optimize it away: ``` pub fn foo() { for i in 1..10 { unsafe { asm!("nop":::"volatile") } } } ``` [This Godbolt link](https://godbolt.org/z/fLL9N_) has it compiling to the correct number of nops, even with optimizations turned on.
Well you get the error because `LoadBalancingStrategy` is a trait, and a trait is not `Sized`, because the compiler doesn't know which implementor of `LoadBalancingStrategy` you're actually going to be using, and different implementors may have different sizes. If you're confused about what `Sized` means in rust, you could perhaps read [this][1] and [this][2], which are also related to `Sized`. That said, I can understand why you're confused about that documentation page. If we are to figure out what the stuff about the `paged` method means, well first we must understand the where clause `Session&lt;LB&gt;: CDRSSession&lt;'static, T, M&gt;`. Now what is `CDRSSession`? Well if you open it, you'll see it is implemented for `Session&lt;LB&gt;` where `LB` is a `LoadBalancingStrategy&lt;Pool&lt;M&gt;&gt; + Sized`, which wouldn't match up with your `Compression` thing, because that's different from `Pool`. Anyway, to make this work, either pick a specific `LoadBalancingStrategy` such as `Random` or `RoundRobin`, or make it generic like this: ``` fn do_something&lt;LB&gt;(sess: Session&lt;LB&gt;) where LB: LoadBalancingStrategy&lt;Compression&gt; { ``` In this case it would compile a separate version of `do_something` for every possible type of `LB` that you actually use, which means it knows the size of `LB` inside each particular version. [1]: https://users.rust-lang.org/t/beginner-using-rev-with-a-range/29337/3 [2]: https://users.rust-lang.org/t/str-string-literals/29635/6
nalgebra supports `no_std` and `alloc` https://nalgebra.org/wasm_and_embedded_programming/ Note, default-features must be disabled to compile without `std`.
I also agree that the lack of unification here isn‚Äôt great, but those are the rules. Zulip not being mentioned sounds like a bug to me.
\`println!\` doesn't flush the library, the newline character itself does. Currently \`print\` is implemented using \`Stdout\`, which in turn is \`LineWriter\`. And \`LineWriter\` flushes whenever the the newline character is written. [https://doc.rust-lang.org/std/io/struct.LineWriter.html](https://doc.rust-lang.org/std/io/struct.LineWriter.html) &amp;#x200B; There's a note in the source code suggesting that when standard output is not a tty, the standard library should switch to \`BufWriter\`. I'm not sure that's a good idea because it means interactive programs which \`println!\` and try to read a response will behave differently if they're run as a child process with pipes for i/o.
Yes, iirc env-logger was locking and flushing stdout for every single log message, so that would explain the trouble. Or anyway, since I was logging from multiple threads, it blocked all of them waiting to be able to log. Now, this is mostly solved by the lock free queue.
I think this is what you are looking for: [https://github.com/japaric/trust](https://github.com/japaric/trust) Thanks to the great /u/japaric!
 struct Foo { bar: &amp;i32 // lifetime parameter needed here, why? } struct Foo { bar : Box&lt;i32&gt; // lifetime parameter not needed here, why? }
Currently \`print!\` and \`println!\` and even \`StdoutLock\` are implemented with a \`LineWriter\`, which means the standard library searches for a newline character whenever you write, and if it finds one it always flushes its buffers and makes at least one system call\*. (\*strace on linux shows that this is usually only one \`write\` syscall) You can and probably should further wrap \`StdoutLock\` in a \`BufWriter\`. Although that sounds like more overhead, it will keep your program from making a system call for every single line of result. &amp;#x200B; If you want the absolute least overhead, you can use \`File::from\_raw\_fd(1)\` to access the standard output file descriptor without locking or buffering, then wrap that within \`BufWriter\`. Ideally you'd hold a normal \`Stdout\` lock while writing through the faster \`BufWriter\`. If you don't the worst that happens is that conflicting output becomes interleaved, which can garble terminal command codes. Under the C standard, the raw file descriptor for \`stdout\` could be defined however the C library wants to define it. But in practice everyone except possibly Windows uses the POSIX standard: 0, 1, 2 are stdin, stdout, stderr.
A reference can only live as long as its referent does, so `Foo` needs to keep track of that. That's why it needs to be ``` struct Foo&lt;'a&gt; { bar: &amp;'a i32, } ``` - the `'a` is externally determined by the thing `bar` is referencing. A `Box&lt;T&gt;` is owned, so it doesn't need a lifetime - it will live as long as its containing scope (the `Foo` here) lives.
At home I have an 1800x (previous gen ryzen, on chilled water OCed to 4.5GHz, but I've never noticed a difference in my compile times when setting it to stock frequencies so... unless you can go LN2 or.... your project is bigger than anything I've ever compiled such that you'd see a difference you're wise to stay away from overclocking).
This might be a stupid question, but anyway: If I use the crate [ws-rs](https://github.com/housleyjk/ws-rs) to communicate over websockets, do I have to use another crate (like rocket or actix) to serve the actual html page, in which I intend to use the websocket? I could not find any information on that on the ws-rs page (probably because it is too obvious, but I have zero experience with websockets).
It is a little bit unclear if it is still using regex after the optimizations. I tend to use regex in early stages of development as a way to get faster to a usable state of the code. But i often revisit those places in code especially if those things are used in hot loops etc. I've seen remarkable increases in performance if you switch out your regex with a custom parser ‚Äì of course it depends on the complexity of your regex if it is really worth it. I've never tried nom or anything yet in that case, just handwritten custom code. I hope to get my feets wet with the new functions based approach of nom and see if i can reduce the complexity of my custom parsers and how it affects performance. This was just a friendly reminder that i have seen this behavior.
This needs to be upvoted.
You can also suffice with building a short /(make)?/ script that just calls a few commands in order
looks delicious
Results on my Ryzen 3 2200G with DDR4 3000 RAM: 955.12user 52.22system 4:33.28elapsed 368%CPU (0avgtext+0avgdata 812904maxresident)k 24inputs+15294792outputs (0major+16845277minor)pagefaults 0swaps Which is about proportionally worse compared to your 6-core setup. So yes, looks like throwing more cores at the problem is a good idea.
I've compiled Rust to a TI nspire (https://github.com/coolreader18/rsspire), I'd recommend looking for a c compiler for your target or for TI 85 and try copying what it does to Rust's toolchain.
Yes. ws-rs handles only WS connections, so you need another service/crate alongside it to handle HTTP connections (if you use actix, you might not need ws-rs, as it has native WS handling.)
I tried your code against current rust_swig master, and it doesn't report any errors: struct Gamepad&lt;'a&gt; { x: &amp;'a i32, } foreigner_class!( class Gamepad { self_type Rc&lt;RefCell&lt;Gamepad&lt;'a&gt;&gt;&gt;; private constructor = empty; }); struct Gilrs; fn new_gilrs() -&gt; Result&lt;Gilrs, String&gt; { unimplemented!(); } impl Gilrs { fn gilrs_gamepads(&amp;self) -&gt; Vec&lt;Rc&lt;RefCell&lt;Gamepad&lt;'_&gt;&gt;&gt;&gt; { unimplemented!(); } } foreigner_class!( class Gilrs { self_type Gilrs; constructor new_gilrs() -&gt; Result&lt;Gilrs, String&gt;; method Gilrs::gilrs_gamepads(&amp;self) -&gt; Vec&lt;Rc&lt;RefCell&lt;Gamepad&lt;'_&gt;&gt;&gt;&gt;; alias getGamepads; });
I‚Äôve read the ‚Äûbook‚Äú and struggled a little, what really made it click for me was reading ‚ÄûProgramming Rust‚Äú (ISBN 978-1491927281) afterwards.
My very fist comment.
&gt;Thanks! After reading [A note on printing performance](https://rust-lang-nursery.github.io/cli-wg/tutorial/output.html#a-note-on-printing-performance) I've decided to try the BufWriter solution. Is there any best practice (or a very efficient way) to use that BufReader with a StdoutLock in multiple threads?
If you want something comparably limited, [SDCC](http://sdcc.sourceforge.net/) (Small Device C Compiler) does both Z80 and STM8, and you can get a minimal STM8S board on AliExpress for under $1 US including shipping. (A fan even wrote an "as close as you can get without C++" port of the Arduino API under the name [Sduino](https://tenbaht.github.io/sduino/).) They're an unbeatable option for making permanently assembled projects so cost-effective that you don't think twice about it.
``` trait ParseFn&lt;I, R&gt; = Fn(InputRef&lt;I&gt;) -&gt; Output&lt;R&gt;; ``` desugars to ``` trait ParseFn&lt;I, R&gt; = for&lt;'a&gt; Fn(&amp;'a mut Input&lt;I&gt;) -&gt; Output&lt;R&gt;; ``` The output type `R` isn't allowed to be bound by the lifetime `'a` of the input, since it has to be valid for any `'a`. However, in the signature of `any_regular_char`, the output is bound with the same lifetime as the input, which means that it cannot implement `ParseFn`. However, you can get around this by making the type of `discard` more generic: ``` pub fn discard&lt;I, P, T&gt;(parser: P) -&gt; impl Fn(I) -&gt; Output&lt;()&gt; where P: Fn(I) -&gt; Output&lt;T&gt;, { move |input: I| parser(input).discard() } ```
Could this be used for a multiplayer browser game, sorta like a .io game?
Thanks for the quick answer. I think I'll go with actix then.
Is there any crate or article that concerns leveraging another program like `vim` for my search results?
Not a single unsafe block in the `jieba-rs` crate, too! Nice!
Genuinely adorable.
I believe git log just pipes its output into `less` or `more`. I don't think it's something your rust code should handle. The unix philosophy is to build a program which does one thing well and pipes it's output into another if you want to process it. So you could just build a bash script around your rust program which just has this: ``` search | less ```
I mean how other compilers are able to compile changes from only one file.
The regex crate is magically fast, though; it's actually difficult to beat its parsing performance in a lot of cases. Performance gains on parsing are much more just doing less work rather than writing a custom parser.
Huh, it's still pointing me to this same error: error: 'Can not find such rust type &amp; Rc &lt; RefCell &lt; Gamepad &lt; &gt; &gt; &gt;Can not find conversation from &amp; RefCell &lt; Gamepad &lt; &gt; &gt; to &amp; Rc &lt; RefCell &lt; Gamepad &lt; &gt; &gt; &gt;'', /home/mitchell/.cargo/registry/src/github.com-1ecc6299db9ec823/rust_swig-0.3.0/src/error.rs:86:5 I just pulled master, and I'm at the commit that was posted today. I commented out everywhere gamepad is mentioned, just in case it's an unrelated error? Here's my full lib.rs.in https://pastebin.com/2xEVTVVm
Okay, this is strange, I put Gamepad&lt;'_&gt; everywhere, and removed the Rc&lt;RefCell&lt; ... &gt;&gt; and build the java tests..... They worked flawlessly!! It gets the gamepads names from rust now! Thanks for all the help again lol
&gt; self_type Rc&lt;RefCell&lt;Gamepad&lt;'a&gt;&gt;&gt;; May be you miss idea behind self_type. It is rather straightforward. Often for language with not Rust semantic (like Java) it is impossible to use Rust structs and trait "as is". You need wrap them into Rc/RefCell or Arc/Mutex. But it is boring to write duplicate of all methods that accept Rc/RefCell instead of Self. And rust_swig automates this. You write self_type Boo; constructor xyz() -&gt; Rc&lt;RefCell&lt;Boo&gt;&gt;; method my_method(&amp;self); and rust_swig generate code that work with `Rc&lt;RefCell&lt;Boo&gt;&gt;`, but to call every method it generates code that call borrow or borrow_mut. So in self_type you should use type to call methods, and as return type of constructor the real type that will be used to cooperate with Java.
Yeah, so this ties in to what I said above: Rust compiles all of the files in a crate as a single unit, and for good reason! It can open up some pretty significant optimization opportunities. But, as you observe, the downside is that compilation takes longer. Incremental compilation and ThinLTO help a decent amount with this problem, and they will only get better going forward, but I don't _think_ you'll see a reversal of that fundamental decision any time soon.
Specifically, you can configure the editor that vim uses through various ways. See https://stackoverflow.com/questions/2596805/how-do-i-make-git-use-the-editor-of-my-choice-for-commits
https://github.com/pingcap/talent-plan/tree/master/rust
Holy crap thank you!
Note that an [impl Mul&lt;i32&gt; for Duration](https://docs.rs/time/0.1.42/time/struct.Duration.html#impl-Mul%3Ci32%3E) exists, so you can already write let SEC = Duration::seconds(1); sleep(SEC * 2); The only problem I see is that the type of `SEC` is not obvious from the name, so someone might assume it's an integer. ‚ÄúSEC‚Äù is not the best name, since it could also mean ‚Äùsecurity‚Äù or the trigonometric function.
It does some compile time optimisation tight?
I like it. That texture is perfect for a ü¶Ä this way.
Thanks, that's very helpful!
Recommendations?
Thanks for your response! Good point on polishing existing features. I agree this would be good for the Rust language, to make it an even more rock-solid choice to work with for new projects. Yes, documentation on how to work on large projects would be awesome as well. I guess information on how to modulaire (and separate in crates) large projects, and on how to use a self-hosted registry in a useful manner would be great for companies looking to start their new big project in Rust. I believe `#![feature(nnl)]` can be removed quite soon, can't it?
No, the regex crate does all its construction at runtime. If you want to build the automata before runtime, you can use [regex-automata](https://crates.io/crates/regex-automata). Of course, then you miss out with further optimization on top of the automata that the regex crate provides, such as literal optimizations.
Yes, after registering you get some recommendations for subreddits to join.
"You pay for what you use" wouldn't apply to GC languages that always have overhead when you don't use the GC (for example, with the JVM when all of your data is stored on the stack).
Log to /dev/null. It's web-scale and shardable.
Interesting. There's a lot of cloning in that Rust implementation. Can any of it be removed. Does any of it matter?
Looks like you want to use Cast::downcast or Cast::downcast_ref (https://docs.rs/gtk/0.7.0/gtk/prelude/trait.Cast.html). You'll want to import Cast into scope to use its methods
Well, all the cloning is largely an attempt at being more general. In `lib.rs`, clone() is applied only to C and S, which are char and bool in the example code. I don't think the explicit cloning really hurts performance in that case. Now that I think about it, it should be possible to pass Cs everywhere by reference. Shouldn't matter much for chars, but it may help when matching on heavier structures.
It doesn't seem like it should matter since The only Semiring implementation is bool and everything else is trivially cloneable amusing its type parameters are.
Java gc is only paid when you allocate from the heap and the thread local slab is full. You only hit a collection when when you hit the watermark. It is very efficient. Ive written programs when after startup they never gc, like ever. I should say it is common, but the better high performance system don't pay for gc because they don't use it.
That worked! Thank you so much. I've only tried upcast, but never did I tried downcast.
Yeah, I think I would say: * If you're printing to the console, speed is not of the essence. Users can't even fast-scan your output at more than a few kilobytes per second. * If you're printing to any output *other* than the console, speed is likely not a concern. File I/O and Unix pipes are *extremely* fast from the perspective of text data. (One note and possible exception: Rust's stdin/out/err implementations are synchronized, so you will spend a lot of resources locking and unlocking the output stream. This is a major reason people find e.g. `env_logger` slow. If only your logging implementation will be writing to the given stream‚Äîwhich should generally be the case‚Äî[grab the lock on the given stream](https://doc.rust-lang.org/std/io/struct.Stdout.html#method.lock) to give it exclusive unsynchronized access, which will be much, much faster.) Note that even if your user does have a super-fast terminal that can scroll through megabytes of text in the blink of an eye, this is functionally indistinguishable from just using `tail`, and `tail` would be more efficient and almost certainly faster anyway.
I didn't say overhead specifically from reclaiming memory. I said GC overhead. It allocates memory and constructs data structures related to GC regardless of whether you actually use the heap or not. This is not "you pay for what you use".
Multiple threads? ü§î BufWriter isn't designed for concurrent mutation. It's possible to make the filesystem do it; there's an atomic-append mode which provides the required guarantees. The problem is you get whatever performance characteristics the OS gives you. What's the purpose of the multiple threads? To put it another way, which step is slowing down the operation? If searching requires a lot of CPU cycles per match found, then it makes sense to just use an `Arc&lt;Mutex&gt;` around a `BufWriter`. Threads will have to wait if they conflict with each other, but each thread spends most of its time searching, not writing. I suspect that's what the kernel will be optimized for as well. Or if the lock is the bottleneck, it's likely better to give each thread a `BufWriter` wrapping its own instance of `Stdout`. In this case, each thread will buffer a couple pages of output and only hold the Stdout lock long enough to write that chunk. If the bottleneck is outside your program, it pretty much doesn't matter.
I agree that all projects are different and ecosystems are different. My point was that with rust there is no reason to stay at older compiler version. It's not like javascript or ruby where you won't get ideas what is going on until application actually running.
Amazing, thank you for sharing!
I like how they're using Criterion and Criterion.rs to do the benchmarks. I'm going to have to steal that "variance introduced by outliers" display, it seems much more readable than how Criterion.rs displays outliers.
You can use `Cow` if you are unsure.
Bt that standard many abstractions are not zero cost., including rusts: eg unions when the tag isn't needed or you uselessly have multiple tags when only one is needed. Or tokio which is filled with little corners of cost.
&gt; each thread spends most of its time searching, not writing. Thanks for your advice - yes, each thread will spend most of the time searching, so a `Arc&lt;Mutex&lt;BufWriter&lt;Stdout&gt;&gt;&gt;` (is that correct?) would make sense.
The phoronix post shows much faster LLVM compilation for a 32GB system over a 16GB system. The huge performance difference could be because it avoids swapping. ryzen 3 sits in AM4 which is dual channel memory, while threadripper sits in TR4 which is quad channel. So the biggest boost from the phoronix benchmarks cannot be achieved by ryzen 3 because it does not have 4 memory channels.
Is there a way to pass in a specific instance of a generic type to a macro invocation? doing ``` macro_name!(Vec&lt;u32&gt;) ``` gives me an unexpected token at the first `&lt;`. My current workaround is doing the following: ``` type Vecu32 = Vec&lt;u32&gt; macro_name!(Vecu32) ``` but this is clunky. Is there a better way to do it?
Huh, that's not really what I got from your "anti-vaxxers" comparison.
That's true, but op should still get dual channel memory at a decent clock speed.
it's literally in documentation. and i actually don't mean to imply "you're an idiot, RTFM", i mean to imply that rust's documentation is fucking amazing, it's a pleasant surprise each time i can't figure something out. compare that to fishing template hijinks off stackoverflow. basically there's the :pat macro variable type. which means pattern. literally that. declare $p: pat and then put $p =&gt; println!(x) in match, and you write SpecificType(x) when you call macro. that's it, god i love rust. tldr: just google rust macros doc page.
1. You are right, but if I have a queue with rwlock, isn't there a chance that a bunch of readers will lock the writer? I'll test and see if mutex does a better job here. 2. I'm not sure. I think I read it somewhere years ago, but I can't verify it.
If the macro is in your code you can change the `ident` to a `ty`.
Ad 1: You don't have to implement a queue on your own nor you have to fiddle with rwlocks / mutexes - https://doc.rust-lang.org/rust-by-example/std_misc/channels.htm
Thank you!
Maybe it isn't clear enough but upcast is to cast into one of the parent types whereas downcast does the opposite.
Maybe it sounded a little bit too harsh.
Have you tried nom? It's a parsec (haskell) inspired parser combinator library. It just went through a major refresh, which was posted on here last week. It's really worth giving a shot!
In my country almost all programmers know English because all the documentation is in English anyway. There are tons of translated tutorials, sure, but if you want to write real programs outside tutorials and/or participate in open source, English is a must. This seems to hold for most of Europe as well. The fact that English is taught in schools and is a comparatively primitive language also helps.
I'm not sure of the disconnect here, since I was explaining "You pay for what you use" rather than "zero cost" in some other sense. Nothing you've mentioned is paying for something you don't use.
I thought I had to use mpsc with a queue on the receiver. That's even better, I will try that instead. Thanks :)
I thought I had to use mpsc with a queue on the receiver. That's even better, I will try that instead. Thanks :)
Can you post the errors you get when you build? In general, I agree with tombardier's suggestion to use `nom`. However, if you already have the tokenization code done, it'd probably be faster to just use your current method. The problem with your macro is that the `t` in the pattern and the macro aren't the same thing. You could maybe do: ``` macro_rules! pop_next { ( $p:pat, $tks:expr ) =&gt; { match $tks.next() { } } } ```
Unions are tagged. Often you don't the tag because it is implied elsewhere in code or if you have a union of unions, it's redundant (there was talk of optimizing this away one time). You definitely pay for the type check on the union even though you do not need or use it.
This might belong into a FAQ, but for a gamedev newb, why not use quic?
I'm building an application and have a couple modules I'm writing for it, but have no idea how to use one module in the other. The file tree is like this: ``` . |--- tichu | |--- hand.rs | |--- player.rs | `--- mod.rs `--- main.rs ``` And I hope to use structs from hand.rs in player.rs. I can't figure out how to tell the compiler this. I've included `mod hand;` in player.rs and it demands that hand.rs be under some "player" directory. If I change it to `mod tichu;` I get the same complaint and it doesn't like `mod tichu::hand;` for apparently parsing reasons.
The problem with macro is that after expansion you have `tokens.next().next()`, and also it doesn't return `Return` so `?` makes no sense. I would replace it with `fn expect_... -&gt; Result&lt;..., ParseError&gt;` function for each variant in the enum.
It could be the same mistake? Can you link to it?
If you want a more functional approach: ``` use std::collections::HashSet; fn main() { let elms = vec![1,2,3,4,5,6,7,8,9,10]; let mut indices = HashSet::new(); indices.insert(0); indices.insert(5); indices.insert(9); let result = elms .iter() .enumerate() .filter(|(i, _)| indices.contains(i)) .map(|(_, e)| e) .fold(0, |acc, x| x + acc); assert_eq!(result, 17); } ```
I'm aware of at least two things that are slow with `env-logger`: *by defautl matching is very slow (even if most stuff is filtered out) because it used `regex` matching; disabling `regex` feature on `env-logger` helps a lot with this * io is slow; in practice this is not such a big deal, unless you enabled logging to the point where it's massive `slog-envlogger` + `slog-async` + `slog` give similar functionality but with much better perf.
 let result = indices.iter().fold(0, |a,x| a + elms[*x]);
Cow is absolute magic, I love it.
You need to put `mod tichu` in `main.rs`, and `mod hand` in `tichu/mod.rs`. You'll also need an appropriate `use` statement where you actually want to use the modules (it will look something like `use crate::tichu::hand::*`, but can be of many different forms.)
It was the \`use crate::tichu::hand;\` &amp;#x200B; Thank you!
Will do! I‚Äôm tagging all the PRs and discussion in `log` with a label so you can find them and have [a working list of libs to ensure we have good support for](https://github.com/rust-lang-nursery/log/issues/328) so will add `async-log` to that.
My version would take advantage of the existing `sum()` function and just do `indices.iter().map(|&amp;i| elms[i]).sum()` or `elms.iter().filter(|&amp;x| indices.contains(x)).sum()`. You might want to benchmark the 2 since I'm not sure which would be faster, HashSet iteration, or lookup.
Thanks a lot for all the suggestions. I have cleaned up most of the bindgen output and added a high level wrapper struct with drop implementation to have all the memory managed automatically. Definitely less typing needed for common use-cases :)
I haven't seen that before! I've actually [started work](https://lights0123.com/ndless-rust/) on a very similar project, where I already have idiomatic Rust bindings to SDL, file I/O, message boxes, and more.
Yeah, I've seen that! Yours looks a lot more advanced than mine, if I actually wanted to write a rust program for the TI-nspire I'd probably use yours üòÅ
Your allocator looks a lot more comprehensive, though, so I might switch to that.
Re game development streams: Maybe you're thinking of https://handmadehero.org/
Match arms only need to terminate in a comma when their expression is not a block - i.e., not wrapped in curly braces. Blocks don't need commas, and it's best practice to leave them off.
I have no idea why but I'd really like to know, so I'm gonna check back later.
Feel free to, I translated the code from C code I found in a stackoverflow answer in order to emulate `malloc_aligned`'s functionality (I think) as I put in a comment there.
This is amazing
`()` is not a block.
Yes, but it is the last expression of the match, so it doesn't need a comma either.
Folding over elems is much less efficient than folding over the indices. Almost all of the filter iterations will be fruitless.
I wouldn't say nobody uses it‚Äîit's actually a part of the default style that rustfmt uses. It will be removed in cases where the comma is present.
In the code I've copied above, the match has three arms. The first two arms have their expression in a block wrapped by curly braces. The first arm ends with a comma. The second arm does not. Further, Listing 18-15 ends the first arm (a curly brace delimited block) with a comma, but not the second arm (also a curly brace delimited block). If I understand this correctly, this means that The Book is not following best practice? If I'm right, that's rather confusing and a bit unfair for beginners.
I'm not sure why the first has a comma; maybe to demonstrate that both are valid syntaxes? All I know is that rustfmt removes them, and that's generally considered the standard as far as styling goes.
If it were the Book's intention to demonstrate both being valid syntaxes, I'd expect the Book to say so explicitly, especially as being inconsistent is not a good idea in general. That said, we're second guessing. More importantly, should I raise an issue? It definitely confuses learners (well, me), and if rustfmt removes them, it seems weird to have non-rustfmt code in the Book without an explicit reason.
Raising an issue seems like a good idea.
Will do :-) I have to say, as a newbie, being able to raise issues like this is quite empowering. Nice to know that, perhaps, I might help other learners avoid a tiny bit of confusion.
 let start = std::time::SystemTime::now(); ... time_computing_diff_ms += start.elapsed().unwrap().as_millis(); Use [`std::time::Instant`](https://doc.rust-lang.org/std/time/struct.Instant.html): let start = std::time::Instant::now(); ... time_computing_diff_ms += start.elapsed().as_millis(); Note the lack of unwrap(), because monotonic time is guaranteed to never go backwards. `SystemTime` *will*. For interest, I wrote a streaming [diff parser](https://github.com/Freaky/rust-diffparser) a while back. Not done anything useful with it, but you can at least laugh at it.
Hot take: Rust is faster than C and C++. Its runtime performance is similar, but Rust's type system and testing facilities make optimization easier, more fun, and less likely to introduce bugs. So Rust programs, all else being equal, will receive more optimization than C and C++ programs, and thus be faster.
Apparently when I add the output Result to the main function it worked. I need to read about the error handling as someone suggested. I‚Äôm very beginner By the way
I set up a systemd service to run my Rust program but all my `println`s aren't showing up live when I tail journalctl with `journalctl -fu my_program`. They're just stuck. If I run my program directly the output shows up fine. Should I not be using `println` for journalctl output? Why is it stuck?
 let result: i32 = indices.iter().map(|&amp;idx| elms[idx]).sum();
What's the different with that and `--color-words`
I'm going to have to check some of my code later, because it is totally possible that it all looks like this and I just never noticed that the comma gets removed.
It will only if making it a hackintosh* fail! * ie: Is a bit harder with AMD and wish to not upset my workflow much. In the other hand I move between OSX, Linux and Windows semi-often...
Thanks. I was not aware of a coming wawe on motherboards. I targeting now the upcoming rizen chips and probably wait a month after to see how it goes.
Thanks for the test! I still not have a definite answer but still look is a good idea overall...
Good to see memory speed matter!. I have never pay attention for it before...
I checked all my code and all of it has commas. I wrote a short test program and made sure it was being run through rustfmt and the comma survives there as well. It actually adds a trailing comment as well. The same is true for the most recent Rust docs as well.
I thought about building a server that hosted pre-compiled binary crates for the latest versions myself. It's a pretty straightforward task but getting people to trust the downloads would be a tough sell. Plus it would be hard to support various architectures and what not.
Codebase says Kotlin.
I understand this isn‚Äôt what you are talking about but it seems to help build times for me https://github.com/mozilla/sccache
I think GeckoView has two parts. The backend is basically the normal Firefox rendering engine. Same as on Desktop. Only difference in places where they access platform API's, and maybe a bit in the rendering engine, how drawing/tiling is handled. JS has to spit out ARM code. etc. etc. But anything that's written in Rust on Desktop is also probably used on Mobile. The big exception might be webrender. They've been so busy tuning/fixing it for desktop, I doubt mobile has gotten much love. GeckoView the git repo is just a frontend to make embedding it easy.
Have you looked at [diff-so-fancy](https://github.com/so-fancy/diff-so-fancy)? Your tool looks quite pretty similar, but I prefer diff-so-fancy for its headings.
This is the best way for us to improve! Thanks for helping.
Good idea thanks I work it üòä
This is not possible in today's rust. There's potential for this to work once specialization is fully implemented and stabilized, but unfortunately that's not very close. The best thing I can recommend for this would be to create a new method on `Foo` to enact this transformation.
I wish such tools would expose their core as a library. I could really use this for my insta snapshot testing tool.
With the number of different platforms rust supports, I suspect setting something like this up would require many resources in and of itself. But, it would be possible. The easiest way would probably be to leverage [sccache](https://github.com/mozilla/sccache)'s ability to use a distributed build system, and just set up a global instance which only ever builds things on its own servers, and schedules builds on demand. Another thing to think about, though, is the number of variables which can affect the crate output. Including but not limited to: - build flags passed to cargo - configuration in `.cargo/config` - what native libraries are installed, and what features they support - version of C compilers (some crates call into a C compiler to build C libraries on the fly) I wouldn't personally think of it as worth it to set this up. We'd still be paying the cost of hosting all of these binaries, and the network cost, and then there's the whole issue and cost around making sure this hosting and building is secure. Per-computer local caches (with sccache) are pretty good already, and I'm not convinced that building each crate for at most a minute or two per crate version per computer is a huge ecological cost.
I kinda wish `rustfmt` would add/permit trailing commas.
See also this discussion, from a week ago: https://www.reddit.com/r/rust/comments/c20aed/facebook_just_picked_rust_to_implement_their_new/
There is a `rustfmt` config option for that, `match_block_trailing_comma`. A full list of configuration options is available [here](https://github.com/rust-lang/rustfmt/blob/master/Configurations.md). (Unfortunately, the heading links don't work on github's markdown)
There's also the case of different feature flags etc, plus crates that link against system libraries. Of course I'd expect any artifact repo to just host the most common options (ie defaults) since users can just recompile the others locally.
I also want this as a library. I need it for `cargo crev diff` :) Someone mentioned https://github.com/so-fancy/diff-so-fancy, which seems to have `lib` directory...
You've probably already done almost exactly this without thinking about it with statements. The comma isn't needed here for the same reason that you don't need to write the semicolon in if foo { return 0; };
Some googling revealed that only GeckoView supports multi processes (e10s) on Android, and [WebRender is not ready for non-e10s](https://hg.mozilla.org/integration/autoland/rev/dc03893f95d7#l1.39) i.e. the existing Firefox for Android. Well, I wish this blog post talked such context.
Actually, WebRender works: https://paste.xinu.at/6E2WtN/ I enabled it and its profiler back when about:config wasn't yet broken.
&gt;Woah. Woah. Woah. Did you just say something that wasn't super negative about Chrome? One million downvotes for you. I was hoping people downvoted me because it was off topic. On the other hand the whole "chrome forces upon users" was just as off topic so I guess you're right.
&gt; back when about:config wasn't yet broken. This is news to me, what's up with it?
Why?
Why are you still converting to `String` to query a the hashmap? A hashmap of String accepts `&amp;str` for `get`, `contains` (just not for `entry`). Also, `(&amp;s[a..b]).parse` can be just `s[a..b].parse`. Instead of `String::chars().enumerate()` you want `String::char_indices()` when you use the indices for slicing. Finally, I'm not sure all the explicit lifetimes are required, but that's not something I can decide without the compiler :)
Opening it crashes the browser. This might in fact be a fault of WR, since I did not try without. There was also a period where it just displayed a blank page.
Last I heard we're not really sure why FPreview is faster. Although my hunch is that Fennec's (aka Firefox for Android) code is really messy and not easily understood, while GeckoView was built from scratch and has a clean API that makes it (more) easy to reason about avoiding performance foot guns.
Works on my Phone
&gt; extraneous yields between pure computations? Why would there be such a thing in the first place?
I'm not sure what the implication here is. This change seems to infect a lot of the program with lifetime annotations. But the commit message seems to suggest that it nets a good performance increase.
Rustc also doesn't have a fixed ABI so each rustc version might need their own version of the binaries as well so it would take quite a lot of resources. Would be nice to have a local binary cache though. Many crates are used by many projects and when you do cargo clean it would be nice if not all the dependencies were being rebuilt when you build again.
Color words looks like this : https://images.app.goo.gl/H56fRtQ7YT6vJ3vb9 It displays a diff which is not line oriented at all. I find it a bit confusing. While the diffs emitted by diffr contain strictly their source diff (by removing all formatting info).
If your goal is to write to stdout as fast as possible I'd suggest looking at this article on implementing gnu yes in Rust https://matthias-endler.de/2017/yes/
I think that is the complaint. A small change cascaded into a lot of changes. I can't see a way around it without unsafe code tho.
Thank you a lot!
See also, discussions: - https://www.reddit.com/r/rust/comments/c5w36i/brave_browser_from_the_inventor_of_javascript/ - https://www.reddit.com/r/rust/comments/c6smnj/in_rust_we_trust_brave_smashes_speed_limit_after/
What do you mean heading links don't work? &amp;#x200B; [https://github.com/rust-lang/rustfmt/blob/master/Configurations.md#match\_block\_trailing\_comma](https://github.com/rust-lang/rustfmt/blob/master/Configurations.md#match_block_trailing_comma)
I see the following implication. This kind of change in Rust is annoying, but not a big deal and very mechanical (basically you just follow compiler suggestions). In C++, this would be bad for multiple reasons. Change would be hard to do, hard to review (you may even warrant a rewrite), and harder to maintain. And if you wanted to do it more safely (less raw pointers), you would consider using `shared_ptr` and therefore negating some performance wins. In Rust, there is no compromise.
I gave it a try but hit some issues with it. I can't remember precisely what, but I think the output was not always correct. Because I intend to use that tool for most of my review needs, I tried to write it myself so that I could fix any bugs myself.
I will work on it :) I should polish the interface to nail it first try tho.
Personally I tend to avoid using references in errortypes because preventing an extra allocation when occasionally returning an error does IMO not outweigh the benefit of having a completely owned error. Having to deal with lifetimes when returning errors is quite limiting, especially if you want to send the error between threads. I am not sure, but I think you could use a `Cow&lt;str, 'static&gt;` as the key to the map to enable you to use both static keys (defined directly in the binary) and dynamic keys. That way you do not need to allocate a static string on the heap to use it as a key in the `HashMap`. (I doubt this will affect performance in a big way, but it should not bring any downsides and might be beneficial if the allocations are in the hot-path. Remember to always measure when trying to improve performance).
[Quantum](https://wiki.mozilla.org/Quantum) was originally an umbrella term for several projects that aimed to land at around the same time, and the associated marketing push for the release of Firefox 57 in 2017. Nowadays it‚Äôs mostly synonymous to either desktop Firefox, or Gecko. Firefox for Android (aka Fennec) is also based on Gecko, and so includes most of what was originally called Quantum. As another comment said it does not however have [Electrolysis](https://wiki.mozilla.org/Electrolysis) (aka e10s), the use of multiple processes. We can speculate that e10s is a lot of what makes Firefox Preview (aka Fenix) feel faster than Fennec, since for example the main process can keep the UI responsive while a content process runs JavaScript. e10s technically arrived (on desktop) before the term ‚ÄúQuantum‚Äù was used for anything at Mozilla, but I suspect the blog post didn‚Äôt want to split hairs over this detail. Unlike ‚ÄúQuantum‚Äù, ‚ÄúElectrolysis‚Äù was always a code name and never part of the marketing or branding. [GeckoView](https://mozilla.github.io/geckoview/) is a library that wraps Gecko with a Java API similar to Android‚Äôs WebView (but more powerful). See blog posts from [last year](https://hacks.mozilla.org/2018/09/focus-with-geckoview/) and [this year](https://hacks.mozilla.org/2019/06/geckoview-in-2019/). As to Rust, there‚Äôs [a lot of it in Gecko](https://wiki.mozilla.org/Oxidation#Rust_Components). Tokei counts 1.8 million lines of Rust code in the `mozilla-central` repository. (This includes crates.io and git dependencies, since Gecko uses `cargo vendor` to keep them in-tree.) Compare with 11.4 millions lines of C and C++ together. Parts of this are two Rust components that come from Servo: Stylo (aka Quantum CSS) and WebRender (aka Quantum Render). WebRender is not yet enabled by default on Android, but might be soon.
Hi, I have sent you a direct message. Thanks.
This blog post talks about Rust std's `HashTable` being the same as `hashbrown`s, but that's not the case in stable quite yet. Might be that they compiled with beta or nightly, or might be that there is another performance boost coming for free in three days :)
This, x100. I see people confused about some aspects of Rust and suggest that solutions would be easier in other languages quite often when in actual fact, Rust is permitting them to do something that would just be utterly unmanageable or exceedingly slow in another language.
Depends on the context. Parsers tend to create (and swallow) a lot of errors even when no actual error has occurred. A pattern I use is to have an internal `ParseError` type that uses references and then a publicly-visible `Error` type that does not use references and only gets emitted when there's an actual error. I then `impl&lt;'a&gt; From&lt;ParseError&lt;'a&gt;&gt; for Error` ofc.
Rust only supports a handful of platforms at tier 1(technically the only tier guaranteed to actually work), 32-bit and 64-bit Windows, Mac, and Linux, and windows again but with mingw. Binaries are useful but not essential, worst case you have to compile, just like today. With that in mind, if they were going to provide crate binaries at all, it'd probably make the most sense for the rust team itself to only support 64-bit tier 1 targets and the latest version of popular frequently used crates, with default features/flags/etc, with cargo caching them locally on first use(not in `/target` though, somewhere user-global like it does for sources). Speed up common cases, minimize infrastructure costs, and establishes an API for providing binaries in the first place, and with alternate registries stable now(and this hypothetical keeping that in mind), you could setup your own thing. &gt; Per-computer local caches (with sccache) are pretty good already, and I'm not convinced that building each crate for at most a minute or two per crate version per computer is a huge ecological cost. Yeah, that said, it probably doesn't matter much. You only have to compile once(unless you run `cargo clean`), and should be able to setup a local cache anyway for stuff like CI
That is a very good point. In this implementation all errors seem to be propegate to the top, hence I think it's an appropriate context.
&gt;A small change cascaded into a lot of changes That is the hole point and you don't want a way around! A way around it is to make mistakes. Rust prevents you from making mistakes. If you went from copying every string to having just one string and pass references/pointers around, you want to be absolutely sure you make no mistake when you refactor your code and have every place that uses the new scheme being up to date and you absolutely want to prevent the original string to get wiped out of memory before you access it down into the core of your codebase. This is a textbook example of how awesome Rust is ‚Äì no need to have a way around it and follow the darkest depth of hell with dangling pointers and undefined behavior!
Traits don't exist at runtime (ignoring trait objects, of course). They're a form of static dispatch, and so each one is simply equivalent to a function call. I believe that the compiler is permitted to elide unused fields in a struct, although in practice I imagine it probably doesn't yet. An extra unused field likely has almost no performance overhead. I wouldn't care about it if I were you.
The Python code uses a regex to get the dependencies instead of a toml parser. This will miss a lot of dependencies. ( All that are not a simple 'name = "1.0"'
Problem is when the object is loaded from outside, let's say you have an database mapper, and want to load eagerly only those columns, which are accessed directly during the scope, rest can be loaded lazily, if necessary.
If you're doing database access like that, you should think about using some sort of dynamic data storage rather than using struct fields. Struct fields don't exist at runtime (unlike, say, Python): they're just locations relative to a larger block of memory.
exactly, but you can determine, which columns you would like to access in the specific code block, and it would be nice if it would be possible, to determine in compile time, which columns to limit the select statement
I'm sure you could write a macro to generate a struct with the appropriate fields to do this.
This, plus the \`target\_cpu\` setting. If you know on which computer your binary will be run, you should compile it with the right \`target\_cpu\` so you can get maximum benefits from auto-vectorization. If you have a a global binary repository, you either need to provide a version per cpu type (there's 75 possibilities for x86) or just a default one, working everywhere but without auto-vectorization‚Ä¶
Using [nom](https://docs.rs/nom/5.0.0/nom/), how can I take until a `newline` (either `\n` or `\r\n`) is found? There is `take_until`, which takes a character string. I can't seem to combine it with `alt` like this: `take_until!(alt!(tag!("\n") | tag!("\r\n")))`. Then there is `take_till`, which takes a function that tests a single character, not the rest of the input. Thanks in advance!
There are some cases where Rust can fall behind due to having to Arc/Rc certain types just to have it make sense to the compiler. I think vulkano kind of ran into this problem. But rust's aliasing rules do allow for tons of cool optimizations that C compilers often can't safely do.
I think an equivalent version would be: fn main() { let elms = vec![1,2,3,4,5,6,7,8,9,10]; let indices = vec![0usize,5,9]; assert_eq!( elms .iter() .enumerate() .filter( |(ind, _)| indices.contains(ind) ) .fold(0, |f, (_,val)| f + *val), 17 ); }
Tasks like this would be great to have as a "refactoring" action (well, technically not really refactoring more like code transformation) in IntelliJ for example. It would add the lifetime parameter to the struct and all its use sites (and possibly optimize usage of the field, i.e. remove `.clone()` etc.).
* environment variables accessed with `env!` * files embedded with `include_bytes` * The *exact* version of dependencies (Mismatches will be common, since dependencies are usually specified with an approximate version and resolved to precise versions at cargo.lock creation time)
Newbie Question: What do I google to understand what's happening with the `String` -&gt; `&amp;'a str` change? I've seen `&amp;str` but not this `&amp;'a` thing.
fixed, thanks!
fixed, thanks!
What do I do after building a base made of the metal frags, a hazmat suit, and tons of wood, sulfur, and stones?
Very cool! FWIW, there's a nice convenience function, `rustChannelOfTargets`, in the Rust overlay that lets you replace: rust = (native-pkgs.rustChannels.stable.rust.override { targets = [ "armv7-linux-androideabi" ]; }); with: rust = native-pkgs.rustChannelOfTargets "stable" null [ "armv7-linux-androideabi" ]; The first two parameters it takes are a channel specifier (e.g., "stable" or "1.35.0") and a date, so you can say things equivalent to "Nightly as of 2019-06-30"
I am implementing a port of the Sieve of Eratosthenes. I get all the candidate numbers (which are 2 and then all the odd numbers), then I iterate over the candidates with this code let candidates = init(max); let collected = RefCell::new(vec![]); let upper_limit = ((max as f64).sqrt() + 1f64) as i32; let primes = candidates .iter() .filter_map(|x| { let mut col_ref = collected.borrow_mut(); if *x == 2 || !col_ref.iter().any(|y| *x % *y == 0) { if *x &lt; upper_limit &amp;&amp; *x != 2 { col_ref.push(*x); } return Some(*x); } else { return None; } }) .collect(); This code works fine and the borrow checker does not yell at me. However I don't like the fact that I am using a `RefCell` to keep a vector that then I borrow mutably. I already imagined that I could follow up this code by splitting the results from my `init` method to get two vectors, one with the results up to the square root of the max number (that I need to put in `collected`) using this algorithm in a single thread and then use rayon with an immutable vector copied from `collected` for the numbers that exceed `upper_limit`. I have an half idea to create a new method which gets the current `number`, and borrows an immutable vector of `collected` primes to check the visibility, and then feed the same method both to the non parallel part and to the parallel part, but I would like to know if there is a way to make this computation better and more rustic. [Playground](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=bac89d6afb49f34d1aebeb80f75b07cb)
Is there a split in the crates ecosystem as well? Eg, is the Chinese community using crates that are not on crates.io, or are there popular crates that are rarely used by the Western community? I've rarely come across crates that seem to be written by Chinese devs/teams.
I wonder: would halting compilation at emission of LLVM IR cut down on resources required?
&gt; You can also, use the Sum iterator, but you asked for fold :P Ah, the classic example of asking what I was thinking of, not what I wanted :D You're correct though :)
Both `&amp;'a str` and `&amp;str` represent a reference. One has an explicit lifetime provided by the programmer, while the other has an implicit lifetime determined by the compiler. The general term for this is lifetime elision. https://doc.rust-lang.org/book/ch10-03-lifetime-syntax.html#lifetime-elision
Unsure if that will solve your use case but someone created a lib with diffing algorithms (I was kind of surprised this project didnt use it) : &amp;#x200B; [https://crates.io/crates/diffs](https://crates.io/crates/diffs)
Still displays a blank page for me on Moto G4, :( Sucks because I'm only really interested in WebRender, and I can't enable it.
Hi only really interested in webrender, and i can't enable it., I'm dad.
Nom includes a parser for this - `nom::character::complete::not_line_ending`.
/r/playrust
But that's exactly what I said. There is no way around it without unsafe code. Well, for this one, maybe CoW would be appropriate.
Those are lifetimes. [The book has a good primer on them](https://doc.rust-lang.org/book/ch10-03-lifetime-syntax.html), or try [this older guide](https://doc.rust-lang.org/1.9.0/book/lifetimes.html). Tip: Read as much about them as you can and experiment a lot, because when you get into advanced Rust they will become very important and powerful.
You don't need the `RefCell`, as you're only using the vector in one place: ``` let mut collected = Vec::new(); ... .filter_map(|x| { if *x == 2 || !collected.iter().any(|y| *x % *y == 0) { if *x &lt; upper_limit &amp;&amp; *x != 2 { collected.push(*x); } ... ```
I don't really see unsafe code as a way around it. This is all important book-keeping. It's like a police officer saying that forging evidence records is a way around the hassle of this "chain of custody" stuff.
That's essentially what `cargo check` already does
Thanks! My question has more to do with how to use this parser until the condition is met? `take_till` and `take_until` both seem unfit for this task.
`many_till` is the general way to repeat a parser until another parser condition succeeds - `many_till(anychar, alt(tag("\n"), tag("\r\n")))("abc\r\ndef")` would return `Ok("def", (vec!['a', 'b', 'c'], "\r\n"))`. For your case, you could use: ``` recognize(many_till(anychar, peek(alt(tag("\n"), tag("\r\n"))))) ```
Worked fine, thanks.
Thanks, helped abate my curiosity until I get up to that part in the book.
Thanks. I definitely have to keep powering through the book.
Actually, now that I think of it, I don't think I've seen any examples where Arc/Rc was identified as a bottleneck. Possibly because people writing performance-sensitive code start off by figuring out how to remove the Rc before anything else, perhaps, but still it would be interesting to see an example of how big a difference it actually makes.
Why another app for preview? Why not test preview in the Nightly build?
Thanks a lot, I got it to work!
Wait what?
I assume that all those lifetime annotations lead to greater performance due to avoided allocations of String, am I right?
Source? Spanish guy here. Hi.
&gt;But that's exactly what I said I don't think so. You think it's a complaint, i think it's compliment and i laid out the reasons why.
This put a smile in my face. Better troll than usual.
What's going on?
Okay
wtf, what about Spain? Which kind of warning is this?
!Remindme 1 day
I will be messaging you on [**2019-07-02 14:26:12 UTC**](http://www.wolframalpha.com/input/?i=2019-07-02%2014:26:12%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/rust/comments/c7sj2x/yarte_continues_to_be_maintained_after_the/eshewi4/) [**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Frust%2Fcomments%2Fc7sj2x%2Fyarte_continues_to_be_maintained_after_the%2Feshewi4%2F%5D%0A%0ARemindMe%21%202019-07-02%2014%3A26%3A12) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20c7sj2x) ***** |[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/c5l9ie/remindmebot_info_v20/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=Feedback)| |-|-|-|-|
I don‚Äôt see the troll. It‚Äôs factual that the repository at https://github.com/mozilla-mobile/fenix/ contains mostly Kotlin code. It‚Äôs only missing the fact that a lot of the code that ends up in the app is in other repositories, including `mozilla-central`.
How does ServoView relate to this? Can you replace the GeckoView in the Firefox preview app with ServoView?
So, I've spoken about building Rust with Bazel previously, but it does support remote caching. https://docs.bazel.build/versions/master/remote-caching.html
That's.....quite an interesting post history you have.....
I thought that [parentis\_shotgun](https://www.reddit.com/user/parentis_shotgun/) was aware that Firefox is not developed in Kotlin and was kinda joking.
Some of those crates may be small, self-contained and effectively complete.
I have a curiosity restructured my code moving the test with the "collected" primes under inside test_if_prime and using rayon and now the algorithm runs blazingly fast! However I don't understand why the Borrow checker isn't yelling at me. Basically I am expecting that the second time collected, even if immutable, would clash with the capture of the closure, but mind you that the last time I toyed with Rust was some little time after Rust 2015 was released. pub fn sieve(max: i32) -&gt; Vec&lt;i32&gt; { let candidates = init(max); let mut collected: Vec&lt;i32&gt; = Vec::new(); let mut primes: Vec&lt;i32&gt; = candidates.0.iter() .filter_map(|x| match test_if_prime(x, &amp;collected) { Some(n) =&gt; { collected.push(n); Some(n) } None =&gt; None, }) .collect(); // let cstar = collected.to_vec(); let high_primes: Vec&lt;i32&gt; = candidates.1.par_iter() // here begins the parallel section .filter_map(|x| test_if_prime(x, &amp;collected)) // why isn't the borrow checker yelling at me here? // ^^^^^^^^ .collect(); // println!("cstar: {:?}", cstar); primes.extend(high_primes); return primes; }
Nix package management doing that maybe it would be possible to replicate what they are doing
Context: I tried to learn Rust ~2yrs ago and "failed". Earlier this year I decided to rewrite a project at work and I now love rust _(and use it for everything at home/work/etc)_. On the 2nd try I picked up [Programming Rust](https://www.amazon.com/Programming-Rust-Fast-Systems-Development-ebook/dp/B077NSY211) and _mostly_ everything "just clicked". That's one of the least dry programming books I've read. It's quite enjoyable, though I'm not sure why. Some advanced generics stuff can still make me go "uhh, what's the best way to write this?" but largely I am more productive than with my previous language _(~5years of Go)_. My advice to people in my company is typically to simply write Rust as if it were Go. This means using `Arc`/`Rc` to solve problems rather than lifetimes, and in general avoiding generics. This seems counter intuitive, and obviously not blanket advice for performant Rust, but it helps you avoid getting bogged down in too many details at once. I have this perspective because in my first attempt I believe I "failed" because I tried to take on way too much. I spent forever fighting the compiler and if an error message sent me down the wrong path I'd struggle for hours/days with that wrong path. Simplifying lets you tackle new concepts at your own pace, while still largely being productive. Anyway, your mileage may vary. Hope this helps a bit :)
!Remindme 1 day
That‚Äôs the goal of ServoView, yes. But I‚Äôm not sure how up to date or complete it is.
It would help with compile time, as /u/jschievink says, but to be clear, it wouldn't be a way to help produce pre-compiled binaries: it's not platform independent.
Fair enough
&gt; it's not platform independent good to know :-) thinking about it (specifically a lot of the preprocessor blocks you see in C code), this makes sense.
I just wanted to tell you watched your video on youtube on Noria and found it really interesting. How did you come up with the one writer many readers scheme?
Thank you
Thank you
If we treat this as a cache that we populate lazily and evict, it'll make it adaptive to the platforms and feature flag configurations people actually use. The other big issue is trust. It would be hard to scale a centrally built cache and distributing the building for the cache means you are trusting the nodes.
If you want to easily multithread this code, [Rayon](https://crates.io/crates/rayon) is a perfect candidate.
The only real way that works for me is writing code. You can read 10 books and watch 20 video series but in the end, the only thing that matters is how many lines of code you have written. Reading [The Book](https://doc.rust-lang.org/book/) is a good start ‚Äì if you have a good understanding of any other programming language that is remotely similar, skip through the chapters that interest you the most. Try and understand some of the [Rustlings](https://github.com/rust-lang/rustlings/) and/or [Rust By Example](https://doc.rust-lang.org/stable/rust-by-example/) and walk through the [cli book](https://rust-lang-nursery.github.io/cli-wg/) and [learn section](https://www.rust-lang.org/learn) of the website is always a good place to visit. That's for the fundamentals ‚Äì you don't have to go through all, but it does not hurt either. You will find your way back anyway even at a later stage of your learning. But what you should do midterm is to find a "project" alongside. This is ‚Äì in my opinion ‚Äì the crucial part. If you don't have something unique choose one of your older projects you have build in another language. This has the big advantage that you know the "field" and don't have to also struggle with the problem domain of your project at the same time you're learning Rust. But in the end, you just have to make something with relevant complexity. I started making a Software Renderer, because there is not much surprise for me ‚Äì i can basically write one in every language you throw at me. Find a mildly complex topic you're comfortable with. And now you find yourself struggle through many problems along the way and have the opportunity to ask people here or in chats/stackoverflow etc. this is how i learn, solving problems and learn solutions this way.
Absolutely. My son is also implementing the ‚Äúraytracer in a weekend‚Äù in Rust (like many others) and all you need to distribute the work across all cpus is this single line: https://github.com/Iquiji/raytracer/blob/master/src/main.rs#L170
Nice! The cli and rustling pages were completely unknown to me and looks like a lot of what I need. Thank you!
From the blog post: \&gt;. Googling yielded a few stack overflow posts with similar questions, and were answered by people basically saying [use my crate](https://stackoverflow.com/questions/31644152/processing-vec-in-parallel-how-to-do-safely-or-without-using-unstable-features)! &amp;#x200B; &amp;#x200B; Couldn't chunks\_mut be used here though?
Sorry, but apart from this coming from a Rustacean, it is pretty much off topic here. While I am sympathetic to your wish to raise awareness to your personal plight, I'm afraid this is not the right venue for that. Btw. the link just goes to the crate, so there's no information indicating what actually happened. You may want to put up a blog post or something. For now I have removed your post. If you disagree with this decision, please send a modmail so other mods can look into this.
I think keeping with simple structures and interact somewhat close to one's known style is valuable advice. Grateful to see a relatively shared experience and what light at the end looked like üòÖ
So its not a documeny oriented database for storing and retrieval of research, its any database being studied?
You *could* do that safely, but then you'd need to manage the distribution of the slices to the threads. Rayon do that for you.
Subterfuge and misinformation. I wasn't around for the original drama and the author deleted the original repo and issues where apparently people pointed issues with licensing which I'm afraid poisons the project for anyone intending to use it in any halfway serious setting. :/
&gt; I'm just completely avoiding lifetimes right now They say the best musicians are the ones who spend almost all of their practice time working on only the most difficult parts of the piece they're learning. If you're avoiding lifetimes in your practice, that's going to contribute to the language feeling clunky. Effective practice is going to require hitting lifetimes head-on, and generally spending as much time as you can with the parts of the language that are the _least_ comfortable to you. As far as a specific learning strategy, maybe go back to a piece of code you wrote before, and try to eliminate a `.clone()` somewhere and replace it with a borrow instead. If you have trouble, you can try to reduce the problem to a toy example, and see if you can get that to work. Then if you still have trouble, you can post your toy example here and ask for help. (But if you post a real program here, that's often too much code for people to read and understand to help you.)
Weird. I've used it for years and not encountered any bugs.
Thanks!, that's a good callout, I've switched it over. I think I started with a stream, and then thought it would be nice to have the try option, but just implemented that instead of just using the method. Extension traits are awesome.
&gt;if one message is received exactly at the same time I call my check_length function It won't. As your code stands it can't be executed concurrently, since `Mailbox` is not `Send` and you're using `Rc&lt;RefCell&gt;` instead of `Arc&lt;Mutex&gt;`. If you wanted your code to execute concurrently, you'd need to use `Arc&lt;Mutex&lt;Vec&lt;i32&gt;&gt;&gt;` for `received_messages`, which will require locks to get interior mutability.
Your link to [libs.rs](https://libs.rs) should be pointing to [lib.rs](https://lib.rs), unless you're speaking about another project? There is a redirection, but the misspelling could make people remember the wrong name... This was mostly an excuse to point out that lib.rs supports the [prefers-color-scheme](https://developer.mozilla.org/en-US/docs/Web/CSS/@media/prefers-color-scheme) media feature, and has a dark color scheme out-of-the box! Anyway, I really liked the post, I thought it was interesting and easy to read, I've never got around implementing a ray tracer, and your post sparkled my interest again!
I probably have. But I'm a learner, not a reference work!
It's safe because the browser run script on single thread unless using web worker and it's also thread safe
Thank you for the explanation, I had read about `Send` being necessary for concurrent execution, but I had not dived deep in that just yet!
Thanks! This really cleared my doubts.
This is mentioned in another comment, but [sccache](https://github.com/mozilla/sccache) implements a local cache as a rustc wrapper.
Ray tracing is always a fun project ‚Äî try path tracing next!
Hey guys! I wrote a library for graphics research and we published a paper at EGSR. It was a pleasure to use and in the long run I think it saved me a good bit of time. I‚Äôm also hoping the graphics community at large does some more stuff with Rust, since it seems like a good candidate to replace a lot of C++ codebases.
Oops. The link is fixed now! :)
I'm aware of Rayon, and it's something I will have to take a look at down the road. It looks incredible though! I thought it might be better to do things a bit more "manually" at first to see how threading works under the hood in Rust. The way I ended up doing it isn't great though, as others have pointed out! I figured I would take a look at `chunks_mut` next to see if I can get rid of the unnecessary sync over the pixel buffer.
It's because the `?` after `UdpSocket::bind` and `socket.recv_from` makes the function return early if it fails, so the return type needs to match that.
I posted about this a while ago, but I'm in the final stages now of building a data structure. Basically, it's a list which can be arbitrarily cut into sections. Depending on where you want your section data stored sections can overlap, cycle, and be arbitrarily re-ordered or follow any sort order. No matter what though, if you're using \`\`\`.filter()\`\`\` multiple times on a set of data this basically lets you do that work up-front (and once) while you're making your list rather than multiple times later. I have the basic design down, I just need to make a few variations on the theme and write tests or at least a solid example program.
You can turn: ```rust fn add_one&lt;T&gt;(x: T) where T: NumCast + FromPrimitive + Copy { x + T::from(1).unwrap() } ``` into: ```rust fn add_one&lt;T: NumCast + FromPrimitive + Copy&gt;(x: T) { x + T::from(1).unwrap() } ``` and then further simplify (requires nightly): ```rust trait Bikeshed = NumCast + FromPrimitive + Copy; fn add_one&lt;T: Bikeshed&gt;(x: T) { x + T::from(1).unwrap() } ```
Not sure if this classifies as an "easy" question but here goes. I'm trying to implement a `tensor_product` function that takes as input two ndarrays (whose dimensions may not match) and returns their tensor product (another ndarray whose dimensions will be m1 \* m2 by n1 \* n2 where m1, n1 and m2, n2 are the dimensions of the two input matrices). The tensor product in this case is illustrated in this image: [https://media.discordapp.net/attachments/533004930349531136/595053092454662144/tensor\_product.jpeg?width=400&amp;height=285](https://media.discordapp.net/attachments/533004930349531136/595053092454662144/tensor_product.jpeg?width=400&amp;height=285) I've gotten as far as ``` let result = x.map(|xval| { y.map(|yval| *xval * *yval) }); ``` I'm not actually sure what this nested map iterator produces, since the two matrices `x` and `y` don't have matching dimensions. Does it just produce a 1D array? How do I take the result of this iterator and turn it into the desired 2D ndarray?
Ah yeah I do this in the library, I was lazy in the code examples. I was actually referring to all of the unwrap calls as being unergonomic, not the trait
I continued working on the trait to abstract over many kinds of trees trees, trying to be the Iterator of trees. Ran into quite some limitations of the type system, or my understanding of the type system at least.. But overall it is almost done and i will probably still publish a version with an introduction post this week.
Python's wheels is a source repository, just like crates.io, right?
I just posted a [Rayon-based answer](https://stackoverflow.com/a/56840441/823869) to that SO question.
If you also want to explicitly and portably vectorize your code, you can use Rayon + packed_simd. There is an example of implementing ambient_occlusion with the different approaches here: https://github.com/rust-lang-nursery/packed_simd/tree/master/examples/aobench
Yes and no. Rust makes it easier to stack allocate and avoid defensive copies, certainly. C++ has constexpr and more powerful metaprogramming features that make it easier to move more work to compile time. Rust has more optimizing *potential* than C++, I'd think, though.
I'd phrase it as a database that is being developed as a part of ongoing research, but yes, it is not just a database that _contains_ research :)
I'm glad! I'd recommend the [CoRecursive podcast interview](https://corecursive.com/030-rethinking-databases-with-jon-gjengset/) too then. The scheme sort of evolved over time, but the starting thought was that the simplest way I could think of allowing reads and writes to co-exist was to make them use different maps. Once that idea was established, the rest was just figuring out the scheme for how you correctly make them swap between the two.
A way around it could be to create a trait for your commonly used type upfront and proxy everything through it, using impl trait everywhere. Make your trait `AsRef&lt;str&gt;` and you can avoid lifetime annotations in a lot of cases I've noticed. Not sure if that's applicable here though, I've just recently started using that technique.
When is it recommended to use unsafe?
Parser Combinators as brilliantly explained in [this recent article](https://www.reddit.com/r/rust/comments/bepi63/learning_parser_combinators_with_rust/) are another good way to write parsers with little to no duplication.
I'm working on a crate for controlling the iRobot Create 1 &amp; 2. It's my first near-serious Rust project. I'm starting simple, but I'm aware that async and no-std support would be good to have eventually.
When you understand UB.
That's only with safe Rust. With unsafe Rust, it would be just as easy as C to avoid reference counting but you would still have things like aliasing restrictions that give better opportunities for optimizations.
That is not at all equivalent. You turned an `O(m)` algorithm into `O(n*m)` (for appropriate n, m).
[blag](https://github.com/agausmann/blag), my own static site generator. I plan on releasing an MVP early this week, optimizing the API, and putting together a list of future goals.
Nightly builds based on the original Android (fennic) codebase are still developed in parallel to Firefox Preview. It stands to reason that Preview won't be in the nightly builds until it's ready to go through the nightly-beta-stable track to replace fennic.
From the description of the results on https://github.com/FiroSolutions/cifiro it seems like uses the same Rustsec database. &amp;#x200B; So this just looks like a proprietary, hosted version of the same thing. Advertising it as "open source" seems deceptive at best. &amp;#x200B; The client appears to have a janky TOML parser that maybe works, but doesn't seem very robust. The code itself looks fairly amateurish overall. &amp;#x200B; I don't see any reason to use this instead of cargo-audit.
I'm pretty sure the first is faster. It doesn't involve a HashSet, just array lookups.
This is only a problem if is made in a naive way. The smart way is to look at [crates.io](https://crates.io) stats, and work for the most downloaded crates. Purge with time the oldest versions and promote the news. &amp;#x200B; By [pareto](https://en.wikipedia.org/wiki/Pareto_principle), surely a small subset of cached crates will yield significant savings for most cases. Also, inspecting crates.toml you could get a sample of with variants to cover. or look form WHERE the crates are dowloading. You bet most are linux, osx and windows ( and I guess: Mostly 64 bits). You not need to cover all platforms &amp;#x200B; For security: Hash and compiled straight from repos in blessed build farm and add validations to sscache? &amp;#x200B; \---- Other possibility is to spread it alike torrents?
Wow. Sometimes you seem other projects mentioned and it's not clear how to make a choice, but that certainly says something.
FYI, Rust will verify that your threading code is data-race free at compile-time, and proving that is hard. Rayon proves a lot of things for you, so that you don't have to, and provides an API that makes proving data-race freedom easier.
I think I could read an entire book about memory allocations in Rust. This is great.
Note however that it's not race condition free. Solving that would be solving the halting problem IIRC. But data race freedom is a very nice property to have in a language.
i am building a rest api with actix-web to learn and understand rust in action !! so far the journey was hard but rewarding. I learn allot about how memory is working under the hood and how rust is protecting and helping you writing fast and secure code !! here is the project [https://github.com/knuckerr/rust-webapp](https://github.com/knuckerr/rust-webapp)
The only place you truly need unsafe is when doing FFI - anything not checked by Rust can't be verified to not violate its invariants, so you have to maintain them manually. Besides that, if you can do it in safe, you should do it - at least at first. If you benchmark your program and find out that you absolutely need to speed up something, and can verify an invariant that Rust can't, then unsafe may be warranted, especially if it's in a hot loop or a function that will be called repeatedly.
Off topic, but: Man, I feel tempted to buy The Rust Programming Language simply to get that Rust art. Great looking book cover!
I think this is mostly Windows problems. I tried it again just now: the colors are wrong (https://ibb.co/zX1w15C) on cmd.exe, while on git-bash colors disappear when you pipe into a pager.
Did I read it right that RISC-V is designed in such a way that it would not need special CPU extensions to implement efficient virtualization? And that this hypervisor implements said efficient virtualization? Is the performance comparable to the RISC-V planned H-extension approach? What are the limitations?
Nice write up! I think the suggested Rust version of the code snippet is very imperative, however, and I would feel like something similar to: pub fn calculate_diff( image1: DynamicImage, image2: DynamicImage ) -&gt; f64 { let max_val = 2_usize.pow(8) - 1; let total_possible = (max_val * image1.len()) as f64; image1.raw_pixels().iter().zip(image2.raw_pixels().iter()) .map(|(&amp;a, &amp;b)| abs_diff(a, b)).sum::&lt;u64&gt;() as f64 / total_possible } would be nicer.
This happens pretty common with future combinators such as map. What happens is that you have the first future awaited, then a pure computation that runs on the result of the first future. The way that futures are polled means that you can't just run the pure computation, it needs to be bundled up in a future and polled itself. You can see this in the implementation of `Future` for map: [https://github.com/rust-lang-nursery/futures-rs/blob/1eebcbe883b10c573a192d2e3fd90214a88fd75c/futures-util/src/future/map.rs#L36-L46](https://github.com/rust-lang-nursery/futures-rs/blob/1eebcbe883b10c573a192d2e3fd90214a88fd75c/futures-util/src/future/map.rs#L36-L46) Each of these futures introduces a yield point at the polling place. What I'd like is if I have something like `iofut.map(...).map(...)` so that there is only a yield point for the custom io future and the pure computations get folded into the same future such that `Poll::ready` returns the io future with the computations done. It looks like I partially answered my own question as there's a `FusedFuture` but that doesn't quiet do what I want as it just prevents further polling, and `async`/`await` syntax has no way to specify that your future implements it.
Better yet: fn add_one&lt;T: NumOps + One&gt;(x: T) -&gt; T { x + T::one() }
Re-writing some numpy code to use ndarray, maybe end up writing a computation graph library that generates rust code. Hopefully spend more time on my personal project to generate fast serde-based parsers for Python. https://github.com/ethanhs/abserde . I need to make some improvements to parsing the type definitions so it supports nested items and also start benchmarking.
Wait, I thought LLVM and by extension rust does not work on RISC-V?
There isn't really anything like this available, no. I think the usual way to go about this would be to have the required traits listed up front, then you can have not specifying enough be a compiler error?
tryna work on building a compiler :) [https://github.com/cnguy/mini-java](https://github.com/cnguy/mini-java) i like rust's test system and \`cargo\` a lot :) really loving enum's and variant pattern matching as well
Guessing checking stdin for a tab character?
It's riscv64 that doesn't fully work yet, IIRC. 32-bit works fine.
In addition to what others have mentioned (that you don't need a comma after a curly brace), also note that by convention, a multi-line function call should have a comma after each argument. rustfmt won't make that change for macros (because technically macros \*might\* care about trailing commas), but you should have a comma after the \`b\` and \`v\` parameters to \`println!\`.
Yes, RISC-V is designed so that it can be virtualized efficiently without special CPU extensions but they're using "efficient" to mean something like "less than an order of magnitude slowdown". That alone isn't trivial because a naive simulator could easily have a 1000x overhead, but it does translate into a lot of wasted cycles. I don't have precise performance numbers for RVirt, but my rough measurement translate to a \~2x overhead relative to running a guest without virtualization. So not unusablely slow, but much worse than dedicated hardware support which may have only a couple *percent* overhead.
Well I need to check it before enter is pressed, how would I do that?
That clearly isn't quite true. :) My understanding is that LLVM can't run on a RISC-V processor but will happily generate code for it if you run on a x86 host, modulo a couple outstanding bugs/missing features. Rust itself has had tier 2 RISC-V bare metal targets for months now.
Pat wrote a really great book on Ruby interpreter internals, if you‚Äôre interested in language implementations.
Rustc currently does not have a `riscv64gc-unknown-linux-gnu` target, but it does have support for the following bare risc-v targets on stable toolchain: - `riscv32imac-unknown-none-elf` - `riscv32imc-unknown-none-elf` - `riscv64gc-unknown-none-elf` - `riscv64imac-unknown-none-elf` A huge thanks to [asb](https://github.com/asb) for implementing much of the llvm backend, [fintelia](https://github.com/fintelia) &amp; [disasm](https://github.com/Disasm) for pushing the risc-v rust targets and and tooling, and everyone else who has made it possible so far. risc-v support has been tracked a bit on https://github.com/rust-embedded/wg/issues/218 the last half year. The rustc issue for risc64gc: https://github.com/rust-lang/rust/issues/62117 An hour ago, disasm opened a PR to [add support for pc-relative addressing on rv64](https://github.com/rust-lang/rust/pull/62281) :)
I mean, showing arbitrary code is safe is equivalent to the halting problem too. the point is that rust only allows a subset of programs that it can prove are safe.
Now I'm wondering how many languages could be ranked based on their runtime performance, then on their logo, and have the same place in both rankings. Obviously one of those two is hard to judge objectively, but with a lot of efforts we can get usable benchmarks I think.
Sorry that I didn't read you reply earlier. Yes, that might be exactly what I needed. I have since moved to try golang, but I will keep this in mind in case I get back to writing in Rust. But I really appreciate the code, it's the best technical answer I got to the main technical issue here, thanks!
I don't have any FFI code for now (I'm doing this as a hobby, not for survival), but I have two cases of unsafe code: In [bytecount](https://github.com/llogiq/bytecount), I need unsafe code for SIMD intrinsics (because misalignment could cause UB). In [compact-arena](https://github.com/llogiq/compact_arena), I use it to ensure the validity of indices with the API, so I can implement safe unchecked indexing.
&gt; Its source code is also only about 3kb, but including the Pillow dependency, it weighs in at 24mb(!). Again, not a fair comparison because I‚Äôm using a third party imaging library, but it should be mentioned. I'd argue it's a completely fair comparison, :D because both Go and Rust statically link their native dependencies, so Go's `image` package and Rust's `image` crate get bundled into the executable. Rust's is even third party itself. That said, while I'm not certain about Go, *most* of `image` probably doesn't exist in the end binary due to the compiler and linker just not bothering including functions that are never called. &gt; match image::open(path) { Ok(image) =&gt; Ok(image), Err(msg) =&gt; Err(format!("{:?}", msg)), } Because I'm personally a fan of functional programming, this could be rewritten in a single line as `image::open(path).map_err(|msg| format!("{:?}", msg))`. There are also a few other places where even more functional programming can be used, like using `.map(|(&amp;p1, &amp;p2)| u64::from(abs_diff(p1, p2))).sum()` to replace the entire `for`-loop. I'm pretty sure there are also ways to use more functional programming in `create_diff_image()` too. Something to note is that using iterators rather than indexing can actually be more performant in some cases since the compiler can more easily tell that the accesses are always in-bounds and can elide the bounds checks. In case half a second is too slow, the operations being done here appear to be trivially parallelizable. With such, it would be possible to use [`rayon`](https://docs.rs/rayon/1.1.0/rayon/) to automagically calculate the pixel diffs in parallel. After swapping the `for`-loop for the functional `.map().sum()` and importing the `rayon` crate, you would be able to replace `image1.raw_pixels().iter()` with `image1.raw_pixels().par_iter()` and the same with `image2`. This would effectively cut the time for larger images by however many CPU cores you have. Thanks to Rust's type system, you wouldn't even have to worry about the usual dangers of parallelism.
Seems like you could make the Rust code fragment even more similar to the Python version by omitting the for loop entirely: let mut diffsum: u64 = 0; for (&amp;p1, &amp;p2) in image1 .raw_pixels() .iter() .zip(image2.raw_pixels().iter()) { diffsum += u64::from(abs_diff(p1, p2)); } to let diffsum: u64 = image1.raw_pixels().iter() .zip(image2.raw_pixels().iter()) .map(|(&amp;p1, &amp;p2)| abs_diff(p1, p2) as u64) .sum(); Other than that, it's cool to see how Rust's focus on learning resources and tooling help make its complexity more approachable!
Some of what exactly oars is goes over my head, but it's still nice to read about! One nit: when assigning the vec to an integer, I think you probably meant to do `let v: Vec&lt;i32&gt; = vec![0.2, 0.2, 0.2];` rather than `let v: i32 = vec![0.2, 0.2, 0.2];`? Will still result in an error, but will be one about integers/floats rather than mismatching `i32` with `Vec`. Thanks for writing this up!
Nice to see news from the mercurial world :) Even though I use it via an extension to talk to github (hg-git) I still find it the most usable RCS I've encountered.
&gt; Python and Go pick up your trash for you. C lets you litter everywhere, but throws a fit when it steps on your banana peel. Rust slaps you and demands that you clean up after yourself. This may be the best comparison I've ever heard for memory management. I'm probably going to use this in the future. I'm only familiar with Python on a basic level (and even then it's been a while), and I'm not a professional developer, but I agree very much with your takeaways. From a devops perspective, I don't see Python going away any time soon, but if any language has a chance of replacing Python in that space, it is Go. Most stuff in devops doesn't need to be high-performance, so I don't think Rust stands a chance over Go given the extra complexity.
Building on actix. A microservice boilerplate here: https://github.com/mattlockyer/actix-json Basically trying to take out the setup pain when you need the usual suspects: API endpoints and calls, Postgres, JSON. Help appreciated.
Write code and let the compiler teach you.
If you're using a crate like [termion][0] or [crossterm][1], they should both have some kind of raw mode and event API for reading key presses without waiting for a newline. Here's an [example using termion to read keys][2]. If you want to get your hands dirty, assuming you're on a UNIX system, you can also read up on the underlying details of the termios API, which you can access via [libc][3]. I've found [this article][4] to be pretty helpful in the past, and [this is the documentation for the actual C API][5]. [0]: https://github.com/redox-os/termion [1]: https://github.com/TimonPost/crossterm [2]: https://github.com/redox-os/termion/blob/master/examples/keys.rs [3]: https://github.com/rust-lang/libc [4]: https://blog.nelhage.com/2009/12/a-brief-introduction-to-termios-termios3-and-stty/ [5]: http://man7.org/linux/man-pages/man3/termios.3.html
Thanks! I'm glad it was interesting. Also, good catch. Will fix soon :)
Yeah I wasn't quite sure how much to explain about orthogonal arrays and all that in this post. I also wasn't in the mood to rewrite my thesis haha. It's a little tough sharing this because the underlying math can be a bit dense and OAs themselves are a very niche topic as far as I can tell.
&gt; Rust slaps you and demands that you clean up after yourself. &gt; I definitely wouldn‚Äôt recommend attempting to write Rust without at least going through the first few chapters of the book, even if you‚Äôre already familiar with C and memory management. &gt; but managing memory will always take more time than having the language do it, These statements make me think we're really missing something fundamental in teaching people Rust. The impression they give is the complete opposite of my experiences with the language. This isn't the first time I've seen someone's takeaways from the ownership system being that Rust makes you care about memory management, and I'm curious what leads people to that conclusion. In my personal experience (and this is certainly an example of survivorship bias), I've had to think about memory management in Rust exactly once, and that was when writing a Ruby extension where its interactions with Ruby's GC were critical for performance. The language *does* handle memory for you, it just does it without a runtime, which is rather the point. You *never* have to clean up after yourself or free memory explicitly, unless you're doing something very abnormal for the language. Ownership absolutely is an interesting concept, but I'm not sure what's drawing people to comparing it to having to call `free` at the right time in C. In my personal experience, once I really got a handle on the ownership system, I've felt that it was making invariants which have always been present in my code in any language explicit, and giving me a way to express them (there's that survivorship bias again). I wonder if there's a way we can put that more front and center. I'm curious how the author would feel about this subject if you replace memory with file descriptors, sockets, or any other limited resource that requires cleanup which isn't heap memory. You have destructors in Python, but they have gotchas with circular references, and run non-deterministically (the GC doesn't know to run when you're out of file descriptors, only memory). You've got `defer` in Go, but ultimately that's way closer to C than Rust is. This comment is way longer than I intended... But to summarize, this isn't the first time I've seen folks make comparisons I don't understand between ownership and manual memory management. I'm worried that we're missing something in the teaching process that is leading to this trend.
I'm not a huge fan of the way the Rust is written, but good write-up other than that. Got super tripped up as well because you talked about Python, Go, then Rust, but the benchmark was Rust, Go, then Python. So I got hung up on why the Go was twice as fast as the Rust for several seconds before I realized what I was looking at.
I am following along with Raytracing In One Weekend. The book is presented in C++, so I am learning Rust by translating its concepts. It's a good time, seeing some recurring patterns that translate nicely into Rust. Most implementations I've seen on Github to help myself when I'm really stuck rely deriving Copy, which leads me to believe that instead of borrowing references they rely on wasteful unnecessary copying, which I'm also trying to avoid.
Oh cool, thanks for the tip!
You can use GNU make or [cargo-make](https://github.com/sagiegurari/cargo-make) to run cargo build --release for every platform, copy binaries to the output directory adding platform description suffix to the name, use strip on them etc.
The first time I read about rust‚Äôs ownership concept, it was immediately clear that it obviated the need for thinking about memory management. The book (especially version 2) is pretty clear, as long as you read that portion in its entirety and don‚Äôt skip to the examples. I have the feeling that people that lack a C background (people coming from java/python/JavaScript) struggle with rust because they‚Äôve never had to really think about memory management, and when they smack full-speed into the borrow checker, they falsely conclude that they‚Äôre being forced to think about memory management, when in reality the compiler is just enforcing a paradigm that allows for automatic memory management, which is a subtle but distinct difference.
I take "demands that you clean up after yourself" to mean that it requires that you clean up your code so that it does things right, not that you deallocate allocated memory.
&gt;I'm curious how the author would feel about this subject if you replace memory with file descriptors, sockets, or any other limited resource That's the point, in those languages you don't have to deal with memory like it's a limited resource. Note that it's not comparing Rust to C, it's comparing Rust to Python and Go. In those languages you can reference memory from anywhere and everywhere, and things will just work‚Ñ¢. In Rust the compiler forces you to make the lifetime of your data explicit, and no references can outlive it. I think that "clean after yourself" is just a way of saying "mind what you're doing", rather than literally cleaning after yourself.
Certainly not these three, in my opinion. I love Rust and it performs beautifully but I still think Python's current logo is the most aesthetically pleasing.
I hope there is a Rust specific book in the pipeline too.
In the job ad for the new crates.io position, there is the mention that the hire would build precisely such a system.
Rust aside, that was a really well-written article. OP, please consider a career as a science writer!
&gt; [Images at the top] One of these things is not like the other... You put up Python's logo, Go's mascot, and Rust's mascot. &gt; (and more sophisticated options like `ipdb` are available) [WinPDB](https://sourceforge.net/projects/winpdb/) is nice if you don't drop into a debugger often enough to have gotten really familiar with it. The original project is unmaintained, but there *is* an [effort](https://github.com/bluebird75/winpdb/tree/winpdb-reborn-python2) to revive it. (The `master` branch is an in-progress port to Python 3 which is currently buggy, so that links to a different branch.) &gt; `.iter()` creates an iterator for that vector. Vectors by default are not iterable. There *is* actually [a mechanism](https://doc.rust-lang.org/std/iter/index.html#for-loops-and-intoiterator) for using a `for` loop directly on a sequence type without having to manually call `.iter()`... it's just that letting that mechanism also resolve method accesses like `.zip()` would be too magical. Plus, some types (eg. `&amp;str` and `String`) have more than one kind of iterator that you might want to request (byte-wise, codepoint-wise, ...grapheme-cluster-wise with a supplemental crate) and making one the default would be a footgun. &gt; but you can use `rust-gdb` and `rust-lldb`, wrappers around the `gdb` and `lldb` debuggers [gdbgui](https://gdbgui.com/) explicitly lists Rust as one of the supported languages, if you want something a little nicer. &gt; Performance Given that there exists an [SSE2 intrinsic for Compute Sum of Absolute Differences](https://www.felixcloutier.com/x86/psadbw), which looks like what you're doing, and it's [exposed as an unsafe function](https://doc.rust-lang.org/1.32.0/core/arch/x86_64/fn._mm_sad_epu8.html) in Rust's standard library, you might want to see how much faster you can get your Rust version by [adding a conditional compilation switch](https://doc.rust-lang.org/1.32.0/std/arch/index.html) to use it when available. I don't think it supports that intrinsic yet (if not, the author asks you to open an issue about it), but [SIMDeez](https://docs.rs/simdeez/0.6.3/simdeez/index.html) would also be an option which would allow you to abstract across the platform differences, and it does build on stable-channel Rust. &gt; I should also mention the binary sizes: Rust‚Äôs is 2.1mb with the `--release` build Is that with or without having run `strip` on the resulting binary? By default Rust bundles debugging information into the release-mode output to power the `RUST_BACKTRACE=1` option for death by `panic!`. Also, if you're trying to see what size it can attain, you might want to try adding these to your Cargo.toml if you haven't already: [profile.release] lto = true codegen-units = 1 opt-level = "z" (LTO is needed for the best dead-code elimination, disabling parallel compilation improves optimization effectiveness, and `opt-level = "z"` asks it to optimize for size strongly. The FreeBSD manual describes the corresponding llvm-clang options as " -Os is like -O2 with extra optimizations to reduce code size. -Oz is like -Os (and thus -O2), but reduces code size further.") &gt; Go blows Python away, but many of Python‚Äôs libraries that require speed are wrappers around fast C implementations - in practice, it‚Äôs more complicated than a naive comparison. Writing a C extension for Python doesn‚Äôt really count as Python anymore (and then you‚Äôll need to know C), but the option is open to you. Rust is also an option for writing Python extensions, thanks to its lack of a heavy runtime and there *do* exist crates like rust-cpython and PyO3 to help abstract away the C-ness of `libpython`'s API.
Honestly I don't see anything displacing Python in that community. You want easy to read code with a high degree of expressiveness. The ability to hack on a file and re-run it without extra build/package/deploy steps is also super handy. The one thing that I don't like about Python is, despite its maturity, the chaos of Python deployment and environment setup. pypi, virtualenv, pip, Python 3 vs Python 2, module version differences between boxes, and more. Since functions run ad-hoc/periodically (usually in human time scales), the difference between 4s and 0.5s (to make up some numbers) is pretty trivial. Plus, if you do happen to need something performance critical, Go, Rust, nim, zig -- all these contenders -- allow you to export C shared objects, which can then get linked into Python. And personally, when it comes time to optimize something to speed up that devops pipeline, I would gladly take Rust, nim, etc. over dropping down to C.
Thanks! I really appreciate the feedback since I wrote this post in a bit of a rush. I am planning to apply to grad school so hopefully I end up writing a lot of papers
https://stackoverflow.com/jobs is a good place to post things, and then you can link it here.
As a Ruby/JS/PHP dev who recently got in Rust, I'd say this is it exactly. You all are right, you _don't_ have to think about memory management in Rust but as a concept, borrow checking is a solution to a problem us GC guys have never even been exposed to before. If you're going to use a tool because of its innovative approach to an age old problem, you're kind of obligated to learn about both the problem and the solution which can seem like a lot to someone without any prior experience. In practice it turned out to not be that bad of course. And I have to say that IME, Rust's docs and community are second to none.
State channels being the things to help blockchains scale by taking some actions off chain
Two of the three quotes I gave are literally saying rust is closer to c than the other two
For me that just links to the top of the page. Not sure what's messed up with my setup then.
Personally, I prefer explicit loops to avoid the complexity of closure capture rules sometimes. Also they end up a bit more readable somtimes. As an aside, I'm not sure you can call those examples "functional programming," rather than just preferring iterators.
You're looking for r/playrust
The main difference is that sending a UDP packet is a one-off operation, not guaranteed to succeed, and not explicitly attached to other udp packets beyond being from the same origin. So senders just "send to an IP", and receivers just receive data a packet at a time. It shouldn't be too much different from a TCP server, just no "connection" would exist. The details depend on what you're trying to do with it, I guess? --- Beyond that, I can point you to a [tiny example in the UdpSocket docs](https://doc.rust-lang.org/std/net/struct.UdpSocket.html#examples)? If you have specific questions about how to do something with this, or what specifically your goal is in using a UdpSocket, I can probably help more.
I should mention, every other use of the library works flawlessly, all is well except for this function and another function relating to GamepadId, which causes a similar crash. In case you want to see where this is coming from, take a look at this: for (Gamepad gamepad : gilrs.getGamepads()) System.out.println(gamepad.getName() + " is " + gamepad.getPowerInfo().getStatus() + " " + gamepad.getID());
Yes. Any management at all (Rust) is closer to full management (C) than to no management (Go, Python). That's my point.
[beej's guide to network programming](https://beej.us/guide/bgnet/) is a good starting place, but it's not written in Rust so you'll need to do a bit of translation from C to Rust. Fortunately, std::net mostly used the same names for things as libc.
Thanks for sharing!! I'm not familiar with FFI at all but I love both Mercurial and Rust!
You can test the type that it returns by annotating `result` with an obviously wrong type (`()` is traditional) and letting the compiler let you know what it is. I'm not very familiar with `ndarray`, but looking at the docs, I think you'll end up with a 2d `Array` of 2d `Array`s, with the same dimensions as the original. To glue these sub-arrays together, you'll likely want the `stack` [macro](https://docs.rs/ndarray/0.12.1/ndarray/macro.stack.html) or [function](https://docs.rs/ndarray/0.12.1/ndarray/fn.stack.html). It might be more efficient (fewer allocations) to just compute the desired size of the tensor product, allocate an array of that size filled with some default value, and then just iterate over the two arrays to populate it. [`indexed_iter`](https://docs.rs/ndarray/0.12.1/ndarray/struct.ArrayBase.html#method.indexed_iter) will probably be handy there, so that you can compute where each product should end up.
Fwiw for me it isn't just the ownership system. It's stack vs heap allocation. My 'vec' isn't boxed but it's on the heap implicity. Does any other construct pulls that trick? Its having to box things altough they are stored in a vec... aren't they already on the heap? Oh, i have to Rc them as well? It's also hard for me to tell what is a move and what is just taking a referance. I never had to care in gc languages. For me, this means 'i have to think about memory'
I'm really referring to the usage of combinators by saying "functional programming." The readability is fairly subjective so I can't argue against it, but I can't see why you'd have trouble with capturing when you're not capturing. While readability is subjective, performance isn't and the usage of combinators over an explicit for-loop, maybe with indexing, has very real performance benefits, including the bounds-check elision that I mentioned, and the use of `rayon` to get multithreaded performance for nearly completely free. image1.raw_pixels().into_iter().zip(image2.raw_pixels()) .map(|(a, b)| abs_diff(a, b) as u64).sum::&lt;u64&gt;() as f64 / total_possible Is the pull request Axel Forsman made, and image1.raw_pixels().into_par_iter().zip(image2.raw_pixels()) .map(|(a, b)| abs_diff(a, b) as u64).sum::&lt;u64&gt;() as f64 / total_possible Is 4 characters longer, just as readable, and 4 or 8 times faster depending on your machine.
I feel the same as you. I recently finished reading the Rust book, so I've been trying to do some code challenges like Advent of Code, Rosalind, Cryptopals. I'm feeling kinda stuck or just like my learning process is too slow. I'm trying to think in some nice beginner-intermedium projects ideas so I can practice!
&gt; Honestly I don't see anything displacing Python in that community. You want easy to read code with a high degree of expressiveness. Honestly honestly, speaking as someone who _loves_ python, packaging a Python app (cli, daemon, etc) is a _nightmare_. I don't mean nightmare as in "it's complicated and impossible to figure out" (just pip install it!) I mean a nightmare as in fixing other people's broken python install code (or installations) probably has cost me a day or two a month, every month, for the better part of the last decade. Go's single static binary and trivial cross compilation looks amazing for developer tooling and infrastructure stuff, from the outside.
They *do* make you treat file descriptors and sockets like they're limited resources. Rust and C++ are the only languages that I know of that handle them automatically. They just *happen* to handle memory the same way. As far as Rust is concerned, there's no difference between writing to a closed file and writing to deallocated memory. They're the same problem to Rust; one just happens to have fewer protections than the other. Python and Go will give you absolutely no trouble when writing to a closed file beyond a runtime error. Since their GC is only meant to handle 1 type of resource, if any of the others fill up, you have a problem and the GC doesn't understand it, because it doesn't see file leaks to be as important as memory leaks.
[WASM support for Coffee!](https://github.com/hecrj/coffee/pull/63) Still a lot to do, but this week I will focus on reviewing and try contributing my changes to [`winit`], [`gfx-backend-gl`], and [`wgpu`]. [`winit`]: https://github.com/hecrj/winit/tree/web/src/platform_impl/web [`gfx-backend-gl`]: https://github.com/hecrj/gfx/tree/web/src/backend/gl [`wgpu`]: https://github.com/hecrj/wgpu/tree/web/wgpu-native
https://this-week-in-rust.org/ lists Rust job openings.
[WASM support for Coffee!](https://github.com/hecrj/coffee/pull/63) Still a lot to do, but this week I will focus on reviewing my changes and contributing to [`winit`], [`gfx-backend-gl`], and [`wgpu`]. [`winit`]: https://github.com/hecrj/winit/tree/web/src/platform_impl/web [`gfx-backend-gl`]: https://github.com/hecrj/gfx/tree/web/src/backend/gl [`wgpu`]: https://github.com/hecrj/wgpu/tree/web/wgpu-native
Indeed. I've been finding that the rustling exercises are really helpful and then do something kinda cool like writing a cli wrapper around some web api
We're all talking about https://doc.rust-lang.org/book/, right?
I want to add ".mp3" to all the strings in a vector. I have a mut Vec&lt;String&gt; . I got this to work. items-vec = items-vec .into_iter() .map(|mut x|ÔΩõ x.push_str(.mp3); return x; ÔΩù) .collect(); This works. Seems like there should be a better way. Thanks.
Oh for sure. At least in the projects I've worked with, the DevOps environments are under tight control so wide distribution isn't an issue. But those sorts of issues would give me pause. And I've been on the receiving end, too. When a Python developer prefers a completely different package and environment stack than the one you like... so now you need both. For instance.
Actually this seems fine to me!
There is no need to dissolve and re-form the vector. Use for x in &amp;mut items_vec { x.push_str(".mp3"); } or equivalently, in more functional notation, items_vec.iter_mut().for_each(|x| x.push_str(".mp3"));
I don't believe that there is any performance difference between combinators and a for loop, and any bounds checking difference doesn't apply since both use iterators. `.zip` is one example of a combinator that can't be easily replicated, and so it is better. My point was simply that sometimes, I like to use a bare for loop rather than for_each or map. That's all I have to say on that.
There *is* a performance difference between multithreaded and singlethreaded, and using combinators lets you switch to multithreaded for the cost of 4 characters.
Yes, I am familiar with Rayon.
I'm pretty sure I tried that but couldn't get it to work. Let me try again. Maybe the I was using iter instead of iter_mut. Thanks
There is actually a machine learning framework called leaf for rust.
Yes, and much more. When we talk about boxing, pointers, references, cloning, and so on we are talking about memory. You have to go way more functional/declarative to get anywhere near a hermetic programming environment that decently abstracts away memory. Some other examples off the top of my head: * Recursive types require boxing. * The first line for the RefCell docs is "A mutable memory location with dynamically checked borrow rules". * Usage of Rc and Arc needs to take care to not create a memory leak. * Polymorphism/dynamic dispatch requires pointers due to the fixed-size restriction. Are we really pretending that pointers are not related to memory? It's some kind of fanboy syndrome of not seeing the trees for the forest. It is one of Rust's *strengths* that you are encouraged and in various ways *forced* to balance memory consumption vs speed vs binary size. These things comes with a *cost of learning*. Why do we have to pretend that it's free?
What's the right RLS invocation? I use `neovim` as my editor, and I've set up [`vim-lsp`](https://github.com/prabirshrestha/vim-lsp) to communicate with it. All is well, except that I feel like I'm not clear on the particular meaning of various invocations. Most examples have something like `rustup run stable rls` or `rustup run nightly rls`. Both of these invocations work on my machine, but I don't want to tie myself to a particular toolchain -- I try to use mostly stable, but certain projects I work with require nightly. In those cases, I've configured it as a per-directory override. Ideally, my invocation would detect that override and use the appropriate toolchain automatically. Does running `rls` alone do this automatically? Does `rustup run &lt;toolchain&gt; rls` actually tie the invocation to `&lt;toolchain&gt;` for compilation, or is the language server smart enough to figure it out regardless? In any case, is there any disadvantage to not running it through `rustup`? Thanks in advance for whatever advice you might have!
Eh... They will "just work" at small scale. Which sure, that appears to be what most people do, and they're absolutely great at that scale As soon as you start pushing the boundaries of memory and threads etc, (either through mistakes or just user adoption) you will definitely care about memory in a GC language, and care about it a lot more. A garbage collection halt is a violent thing that can cause brownouts or take your service down at regular intervals, and it can be difficult to understand how much breathing room you have before that happens, even with great logging, metrics, and alarms. If memory is handled safely and constantly, you can reason about your performance characteristics with a *lot* more certainty
In a 40 Core machine the combinator version is 36x faster than the raw loop for me. But I agree that the raw loop reads nicer.
As a professional python dev for over a decade, I can‚Äôt stand go, and I can‚Äôt imagine many python secs taking to it. The error handling, lack of generics, it‚Äôs all very *un*python. The logical progression from python is nim
It isn't memory management *per se* that I find to be the problem for new students ‚Äî it's ownership itself. No other language I am aware of has the property that a value must always have a unique owning location at any point in the program; the programmer often must keep track of what that owner is pretty carefully to get their program to work. (I guess the new fancy C++ smart pointers are like this, but I don't know as I'm not really a C++ person.) The exception to single ownership ‚Äî `Copy` types ‚Äî only makes things more confusing because sometimes you can "get away with" not worrying about ownership. I think that `Clone` proliferation and gratuitous user-defined `Copy` types are a symptom of trying not to think through the ownership restriction and its implications. I find that exercise worth doing, but it's easy for me to understand how somebody who already can program fluently in other languages finds Rust's ownership restriction difficult to understand and annoying to work with, especially at first. The borrow rules aren't so bad once you get the hang of thinking about ownership, but ownership is an initial cognitive burden that takes some time and effort to master. Maybe there's some way to teach Rust that would make this key concept easier: suggestions are welcome.
Rust heavily optimizes iterators, most of the times the iterators will be much faster than hand rolled loops.
Upvote for Sun logo love. My University notes are littered with it
I don't know the details well enough so can be wrong by QUIC tries to be the new TCP - reliable streams. It aims to support lower latency for new connections, optimized for HTTP traffic. HTTP over TCP multiplexes many channels over the same stream, if one packet is lost all channels will be blocked. QUIC solves that problem by letting other channels continue if one channel's packet is lost so you still get progress. Games usually want different things. You often have long lasting connections, so the initial connection latency is not as important to optimize, but once connected latency is very important. Different messages have different requirements - some messages would be too old if resent so they are not resent in case of packet loss, some messages' order doesn't matter as long as they are delivered, some messages need to be delivered in the right order and resent if the packet was lost. Laminar aims to provides support for all of this, QUIC does not.
\&gt; Rust makes you care about memory management It cares about memory management in the sense that you have to understand how stack allocation / heap allocation works, how 'enlarging' data works, how 'moving' data works, to THEN understand ownership and the need for the borrow checker C++ ALSO handles memory management for you - nobody's suggesting you don't need to know about memory management to program in c++ though For a simple pseudocode ish example: struct Foo&lt;'a&gt; { a: i32, b: Option&lt;&amp;'a i32&gt; } fn create\_foo() -&gt; Foo { let mut foo = Foo { a: 10, b: None }; foo.b = Some(&amp;foo.a); return foo; // I'm 99% sure this doesn't compile } The reason why this doesn't work is totally non-obvious if you're looking at everything from a super high level, you NEED to understand that 'moving' foo is actually just copying, and pointers WON'T be update, leading to a dangling pointer.
You should have a look at the PyOxidizer project which builds standalone Python applications leveraging Rust. Read this blog post by the author for some background: https://gregoryszorc.com/blog/2019/06/24/building-standalone-python-applications-with-pyoxidizer/
This is my favorite comment. I was working on a tokenizer for another project but noticed that I did not need to pass a \`String\` around everywhere in that project, but I did here in RSC. So I did what any ordinary Rust programmer would do and changed the referential semantics of my entire project in under an hour, with absolutely zero major trouble. Besides whittling down 20+ errors into a runnable program, this was, simply put, easy. Only thanks to Rust. And indeed a "textbook example of how awesome Rust is."
Without having yet tested the benchmark performance of the code, I could only assume that not having the several owned String-s should lead to a greater runtime performance when lexing, which was the biggest problem before.
&gt; Each of these futures introduces a yield point at the polling place. I think either I'm misunderstanding you, or one of us has an incorrect understanding of how futures work. If you have `iofut.map(lambdaI).map(...)`, you get a `Map&lt;Map&lt;IoFut, LambdaI&gt;, LambdaO&gt;`. Now, you put the complete future into an executor. The executor calls `poll`. The outer `Map`'s `poll` forwards to the inner one's, which forwards to the I/O poll. That, presumably, registers a callback and returns `Pending`, which the outer two `poll`s dutifully forward outwards. The executor returns to its event loop. At some point, the I/O event happens, the callback is called, and calls `wake` on the `Waker`. The executor sees that the future is ready and calls `poll` again. The outer `Map` calls the inner `Map`, which calls the `iofut`. The I/O has completed and now can return a `Ready(whatever)`. The inner `Map` sees that `Ready`, takes the `whatever` out, calls `lambdaI(whatever)` and wraps the `mappedI` it returns inside another `Ready`, which it passes to the outer `Map`. That one sees `Ready`, calls its own lambda, etc. The complete `Future` therefore returns `Ready`, and whatever further processing should happen happens. (Presumably some async function picks up where it left off. Or a `executor.await_block` returns the final result.) I don't see any extraneous yields here. Maybe you can point out where your understanding differs from mine?
If I want a globally unique id that is threadsafe what should I use? I want to use it in the default trait so I want to access from a function. ``` pub struct Trigger { id: u64, } impl Default for Trigger { fn default() -&gt; Self { Trigger { id: // ??? How should I call it? } } } ```
The way I see it is that managing ownership is effectively the same thing as managing memory in Rust, at least in some (I would argue many) contexts. That isn't to say that managing ownership is as difficult as managing memory manually, but it's still something you have to think about that very closely relates to memory. For example, if you're doing something with heavy String manipulation, you could write a naive approach and clone everything. Then when you start to wonder why you're program doesn't perform as well as you'd hoped, you'll find out (maybe not in these exact terms) that it's because you are allocating and deallocating (and copying data) way more than necessary. So the questions "how to better manage memory" and "how to better manage ownership" are the same questions. But when the question is framed in the context of "ownership" (and with the rules enforced by the compiler), the question usually becomes much easier to answer. However in Rust, the concept of ownership can be extended beyond *just* allocating and freeing memory. You might pass around a MutexGuard, in which case you are using ownership rules to manage when a Mutex gets unlocked. So it is obviously fair to say that ownership is *not* memory management, but it is a tool used to solve memory management. Specifically in terms of the analogy OP used (in C you can throw things around wherever, but Rust forces you to "clean up" after yourself), I thought of dangling pointers immediately: in C, you might leave a pointer in memory somewhere and slip on that banana peel later, however in Rust if you tried to do the same thing (excluding unsafe), whether it was knowingly or unknowingly, the compiler makes you fix it. In GC languages, you just don't have to worry about that.
You can use an [atomic](https://doc.rust-lang.org/std/sync/atomic/index.html), [AtomicU64](https://doc.rust-lang.org/std/sync/atomic/struct.AtomicU64.html) for example. To get a new id you just have to call [`fetch_add`](https://doc.rust-lang.org/std/sync/atomic/struct.AtomicU64.html#method.fetch_add)`(1, ordering)`.
As someone who spent a fair number of years writing advanced allocators for C++, I see Rust as the logical conclusion of RAII and destructive moves, and it is good. But. Rust forces you to think, not about memory management, but about *ownership*. Move-by-default means you can't have internal references between members of a struct. This can be... troubling... at first. You learn to use offsets for graphs, something that highly performant and/or movable solutions in C++ also forced you to do. You end up using ref-counted and locked types in places where, in C++, you would spend a few hours running through a proof of correct access semantics... and there is both a gain and a loss in this. But you don't really think about memory management at all. Hell, I rarely use drop, and I tend to have custom destructors everywhere in my C++.
C++ doesn't strictly require it, but many C++ codebases - including every one I've owned - do mandate that all resources have explicitly known ownership. Abusing shared\_ptr is a terrible habit...
As I understand it, the `rls` binary is linked to rustup's wrapper anyways, so calling `rls` in your project directory will use that directory's override. You can verify this with `rls --version`. As for the difference between rls on stable vs. nightly, I think the main thing is just the version of rls itself. On stable, it's a relatively stable version released with that rust version, on nightly, it will be a more bleeding-edge `rls` nightly build. I believe this will also tie to the version of rustc libraries it uses, but that shouldn't matter too much since rls uses a separate build directory anyways, and that won't conflict with your main build directory used by `cargo build`, etc.
Not necessarily. Can be, but it is not guaranteed.
Thanks for briefing and the link. i knew both of them but right now i wanna create my own game server and i need to keep the connection's IP in order to send appropriate data back to them. i read that the TCP has big overhead compare to UDP and makes it heavy (in size) and also, TCP will check the received package and make sure to collect in order otherwise stop receiving etc. another hand i need to send my data from the client to the server and i wanna authenticate, manipulate, etc. the data and send back to the client while other clients are connected to the same server too. i've done TCP server-client app in golang and i was able to store the connected IP addresses and send specific data back to specific client. now is different. im on rust and wanna send data only to each other( without checking of lost packages) and for sure i need the IP addresses . The Rust doc, the one you shared, doesn't talk much about the UDP and how to design it
C# also has good support for deterministic finalization, and some lints when you forget to do that.
Yep! Great!
yea i think i need guides to know what to make in Rust. but is that only source can refer to ? any better like linux docs etc. ?
You can't even create that circular reference this way in the first place, I think. Since you borrow (a field of) `foo` immutably, blocking the mutable access from the assignment. Plus, if I recall correctly, the lifetimes in a type have to outlive how long the variable with that type is in use (I don't know how to explain it better), so even if you'd use a `Cell` the borrow checker would probably complain.
The only crash that I see is when object passed by value and then there is attempt to use it from Java. So I have no idea what is going on. You can try to run java with: `-Xcheck:jni -verbose:jni` to get more verbose and correct output about errors.
That's why I said *most* of the times.
For me, you definitely have to think about memory management more than say Java. I need to decide whether to put it on the stack or box it, use Arc or Rc etc. It's extra choices and you need to use when to use each one.
Is there a way to move something's ownership without moving it by value? I understand that `move` does both, and maybe the compiler is able to optimize this away sometimes. What if I want to guarantee it always? Example: I want to expose a library function `fn foo(x: Bar)` which _must_ consume x. How can I prevent this resulting in copying an enormous value on the stack? I can't expect my users to do: `foo_ref(&amp;x); drop(x)`.
The compiler error messages are still absolutely hideous and don't help newcomers at all. Not saying they aren't good for debugging if you already know everything there is to know, but it is not at all a good idea to say that the compiler can teach you.
A small git-backed web server, [gite](https://github.com/bzar/gite), for personal needs.
these look like more actively maintained alternatives: &amp;#x200B; [https://github.com/danclive/bsonrs](https://github.com/danclive/bsonrs) [https://github.com/danclive/mongo-rust-driver](https://github.com/danclive/mongo-rust-driver)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/node] [Remote Job\/Contract - Rust &amp; NodeJS Development](https://www.reddit.com/r/node/comments/c875bx/remote_jobcontract_rust_nodejs_development/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
[This StackOverflow answer](https://stackoverflow.com/a/35046435) claims that a move into a parameter does not necessarily mean pass-by-value but could alao be pass-by-reference. Haven't found an official source asserting this information yet. You might be able to look at the assembly output via https://godbolt.org/. The idea of an `&amp;own`/`&amp;move` [has been floating around](https://github.com/rust-lang/rfcs/issues/998) for some time.
Wait, python doesn't have generics either, does it? Instead you just have objects which don't have a static type, and if you pass the wrong type somewhere, it explodes. That sounds exactly like how Go works with its use of interface{} everywhere, doesn't it?
Maybe that‚Äôs true when it comes to topics like the borrow checker, but coming from ‚Äòundefined is not an object‚Äô, I promise you the Rust compiler is a joy to work with. Much of the time the error messages give you the exact solution. I‚Äôm obviously not suggesting to use no outside resources, but OP will have to learn to work with the compiler to get literally anything done.
No it doesn't. If you don't call dispose then your resource will leak until gc calls finalizer
You don't close files on go and python?
/r/playrust
Thanks for your reply! It being an open problem at least explains why everything I found seemed so uncertain. I'll just rely on the compiler for now.
You can also make a small macro `foo_big!` calling `fn foo_big(x: &amp;mut Bar)` and `drop` right after, I've seen it in a crate but I don't remember which one.
Is there a simple way to just get the request body for a POST request as data in an actix server? Maybe I am blind, but all I can find are ways to handle JSON.
Hi, Do you need a freelancer or is it a permanent position? What is your language for communication? English? Maybe French, or German?
Thanks. This works when I trust the user I suppose :) I guess I can expose an `unsafe` function which allows passing by reference as you say, and let the user handle it.
 I appreciate your ability to see both the big picture and the small details i like you :)
What are you hoping to do with Rust? That might gives some clues as to what to work on and how much to worry about things like lifetimes
Hi! Thanks for asking. English is preferred, but I am fluent in German as well. We recently received our first batch of VC funding through the Crypto Valley Labs, but our runway is about 6-7 months. We can't offer anything truly permanent at this point, so we would prefer freelancers. However, if you have a large skillset, are interested, and think you may be useful beyond a number of constrained development tasks, please let us know. We have an options pool for new team members and are looking to bring another person on. Thanks again!
Just like you can use `Json&lt;T&gt;` extractor to get request body as json, you can use `Bytes` or `String` to extract it as raw bytes or as a string (the latter one will probably bail on non-utf8 data).
I'm working on wrapping gil-rs to Java, and writing a Minecraft mod for controller support. Gil-rs has support for Rumblers, SDL mappings, and even multi controllers Eventually, one should be able to play it with two windows open and two controllers, like a couch game It's working alright, but I'm fighting the borrow checker all over again, just over the JNI üò≠
A great thank you!
Hello there. I'm in same boat, looking for like-minded individuals that'd be interested in examining some interesting problems in medical or IOT. &amp;#x200B; Are you still looking for like-minded individuals? I've dropped you a PM here on Reddit, see you there.
But with the macro, no need for trust (who needs that =), [little playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6fcc81b2c526c0e63ac0484cca927597). The only downside is that `x` has to become mutable.
I'm sure that's cool... for six months, until something else replaces it.
&gt; making one the default would be a footgun. `iter` is already "the default".
:D
Yeah for us the only environments that every really break are dev envs (laptops and our Wild West zone), and of those the only thing I ever need to spend time investigating are laptops, and the only reason I do _that_ is to try and make sure I understand the problem in case it is trying to make it's way into production. Still, setting up a robust pipeline for deployment is non-trivial, although several of the problems I care about would be the same in go, pre-modules, IIUC.
I was hoping `time::SEC` would be able to work around that for the most part, but if someone just imports `SEC` by itself lol yeah that definitely seems confusing.
`Box` it up. This will move the value to the heap, but passing `Box&lt;Bar&gt;` is just passing along the reference. If you need to stay on the stack, how about passing an array `[Bar; 1]`?
Oh fantastic!
writing docs for a 0.1 release of [rustika](https://github.com/MattsSe/rustika) on crates.io, which brings rust bindings for the apache tika rest api. Also the release of nom 5.0 encouraged me to put it to use, therefor I started working on [nom-sparql](https://github.com/MattsSe/nom-sparql), a parser for the sematic query language SPARQL
&gt; Given that there exists an SSE2 intrinsic for Compute Sum of Absolute Differences, which looks like what you're doing, and SSE2 is guaranteed to be in all x86_64 chips, and it's exposed as an unsafe function in Rust's standard library, you might want to see how much faster you can get your Rust version by conditionally using it when making 64-bit x86 builds. I [tried](https://github.com/polachok/diffimg-rs/commit/e2df651369e4f0b062ddf814acc41b45a74cb503) using mm256_sad_epu8, and got about 5% speedup. I expected more, am I doing something wrong (this is the first time I'm using simd)?
You are right about the heap, but in my case this will usually just move the problem. I wanted to avoid the object being moved by-value in memory. In this case, the move would occur when _creating_ the box. I am not sure that `[Bar;1]` will end behaving any differently. It's my understanding that arrays like that are passed by value also. `&amp;mut [Bar]` or `&amp;mut [Bar]` will have the same problems as `&amp;mut Bar`.
What about hand rolled iterators? I've got a project where two large iterators need to be "aligned". Think of it like SQL rows from two different databases, where ordered records exist for user transactions. These are ordered by `(user_id, transaction_id)`. I wrote an `Align` iterator that allows you to compare a LHS and RHS for equality on the mentioned key. Usage would look like: lhs_rows .iter() .zip(rhs_rows.iter()) .align() .for_each( ... ) The `.align()` skips LHS or RHS items based on a `Ord`, ensuring that the resulting dataset is the composition of the LHS and RHS. Now, obviously you can't comment on my code that you can't even see lol. Yet, I'm curious if you think these compiler benefits fall apart the moment you use non-stdlib combinators such as my `.align()` call. I don't expect any amazing answer, but figured I'd ask :)
Ah, yeah, array isn't correct, I meant a slice `&amp;[Bar]`. Could also use a `Vec` in the same spirit.
I think the problem stems from when you have a Result with both types being T, so you have a Result&lt;T, T&gt;, in this case it may not be clear whether doing Result::from(val_t) will give you an Ok or an Error. Option does not suffer from this problem though, since there's only one generic.
When using vim mode, disable `Enhanced Typing` in the extension's settings.
Well as I said it would always just take the Ok. It would never go to Err so `From&lt;E&gt;` would just not be implemented for `Result&lt;T,E&gt;`, only `From&lt;T&gt;`. One can say this is "asymmetric" but the Ok branch is already "blessed" in many methods as the "sensible default" with the Err branch just tagging along getting its own special treatment. But reading more on it I think I figured it out; it's actually pretty obvious: they just want you to use `TryFrom` instead which makes some amount of sense except that `Infallible` doesn't properly work yet so say this doesn't work: fn foo () -&gt; Result&lt;i32, ()&gt; { 0i32.try_into() } Because `Infallible` and `()` are a different type. There are apparently plans to eventually update the type checker to make an empty type be the subtype of any type in which case it would pass but right now you need to add an ugly `.map_err(|_|unreachable!())` after it it seems.
One cool thing that rustc implements is niche-filling optimizations. If the compiler can prove that some bit patterns of every variant in an enum are never used, it can use those values to represent the enum tag (discriminator) instead of reserving extra space for the tag. For instance, `size_of::&lt;Option&lt;T&amp;&gt;&gt;() == size_of::&lt;T&amp;&gt;()` for any T, because the compiler knows a `T&amp;` can never be null (all zeroes) in safe Rust, so it can instead use that value to represent `None`. Any nonzero value can then be assumed to be a `Some(_)`. This is not a trivial improvement either; even though `Option`'s discriminant is in principle a single bit, alignment concerns may balloon that to four or eight bytes in practice! Indeed, on x64, the size of `Option&lt;&amp;T&gt;` is 8 bytes, while `Option&lt;*mut T&gt;` is [16 bytes!]([https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=548a85770d2b79a0f8c731dbc7073e6f)
As always, the devil is in the details.. &amp;#x200B; Thanks for the write-up!
I don't quite understand the bit about it not working with a cell and why, but i'm happy to accept regardless, this language is already incredibly frustrating sometimes, i'm pretty sure not knowing WHY I can't do stuff would probably drive me over the edge, I think going through a couple segfaults in C++ probably helps a great deal
Just TIL about `std::hint::unreachable_unchecked` which is useful when you are certain it will never happen, so you don't even need the panic backstop. Being able to convert from never-type seamlessly would be still better, I agree.
how do i do this? struct st { size: i32, arr: [f64; 1000], } impl st { fn s(&amp;self) -&gt; i32 { self.size } fn f(&amp;mut self) -&gt; &amp;mut f64 { let array = &amp;mut self.arr; let size = self.s(); &amp;mut array[2] } } shouldn't borrow checker understand that i only want to borrow one field, and another one cannot possibly change?
It's not that the compiler handles the stdlib iterators any differently, but the developers for stdlib made sure the iterators were as optimal as possible to allow for optimisations to take place. Generally though the iterators will be pretty self contained and so will be ripe for heavy optimisation.
I can't test it right now, but I think types with lifetimes shorter then their usage (like references that you keep around after the original object was moved or dropped) are completely disallowed. So while `Cell` helps because you don't need mutable access to write, it doesn't solve the problem that the object cannot outlive itself. It really comes down to more or less simple rules at the end though. Including only static types (-&gt; types that don't have lifetime parameters or where all of them are `'static`) and references to those types, the rules are *really* simple. Don't move an object away while you have a reference, mutable -&gt; no immutable ones and vice versa, reborrowing, you can shorten the inferred lifetime. But as soon as you introduce `Cell`, or heck, even only references to references (`&amp;'a mut &amp;'b T` for example, `'a` can be shortened, but not prolonged (so far so good), `'b` can do neither (if it were made longer, the reference you read might not work, if it were made shorter, the reference you write could die too early)), shit hits the fan. Hard. For example: in `fn(&amp;'c T)`, `'c` can be made longer. So yeah, I agree that understanding where those rules actually come from might help. A lot. Although you can of course learn them without Segfaults, but it might be a little harder for more complex stuff.
Some libraries have been rewriting stuff to reverse that change, because it's turning out in the long run that the explicitness lost by the From&lt;T&gt; for Option&lt;T&gt; impl isn't worth the slight convenience it gives. I'd advocate for getting rid of it entirely for the next edition of Rust.
Also already a language [https://leaflang.org/](https://leaflang.org/)
Do you have any examples at hand? I am intrigued where this is problematic. To be fair, I myself wouldn‚Äôt use this to return an `Option` (like in `fn foo() -&gt; Option(i32) { 42.into() }`, I‚Äôd rather write `Ok(42)` expliclitly). But in builders and constructors I find it very handy to able to accept `Into&lt;Option&gt;`, as in: // self.foo is an optional field fn set_foo&lt;T: Into&lt;Option&lt;Foo&gt;&gt;(&amp;mut self, optional_foo: T) -&gt; &amp;mut Self { self.foo = optional_foo.into(); self } So that the user can call both `bar.set_foo(foo)` and `bar.set_foo(Some(foo))` (as well as `bar.set_foo(None)` if they wish to unset it). I don‚Äôt consider it just *a slight convenience*.
I don't really think that it's all that explicit if the `Some` is so far removed from it and needs to encapsulate an entire computation. I'll say though that I dislike that `into()` can't accept type paramatres due to how it works an it has to go by inference. I would often like to write `into::&lt;Option&lt;i32&gt;&gt;()` because I do feel that indeed makes it more explicit but you can't do do that but I don't think a `Some(` 5 lines higher is that obvious either.
I'm late to the party, but what exactly does `Option&lt;Foo&gt;::into()` do when it's a `None`? Panic? That code seems crazy implicit to me, and more importantly I like to be very explicit when a line would result in a panic.
Well, no, but the effect is similar in practice. You can just pass as arbitrary list of foos around. You‚Äôll only get into trouble if you start assuming the food have certain methods. - but it‚Äôs certainly possible to write generic code the operates on, say, any object with show() and hide() methods. It‚Äôs sort of ad-hoc interfaces
How can someone come up with such a bad name ??
`into::&lt;Option&lt;Foo&gt;&gt;()` called directly on `Option&lt;Foo&gt;` (that is both `Ok(Foo).into::&lt;Option&lt;Foo&gt;&gt;()` and `None.into::&lt;Option&lt;Foo&gt;&gt;`) is just a no-op, it returns itself. `let opt: Option&lt;Foo&gt; = None.into();` is the same as `let opt: Option&lt;Foo&gt; = None;`. That‚Äôs because every type automatically [implements `From&lt;Self&gt;` this way](https://doc.rust-lang.org/src/core/convert.rs.html#546-548), as `fn from&lt;T&gt;(t: T) -&gt; T { t }`. But for `Foo` it is `Ok(Foo)`, so that if user passes an instance not wrapper inside an option, it is wrapped into the `Ok` variant.
Seriously. This is almost as bad as Atlassian calling its Git solution "Git Stash".
Ohhh, I was thinking you `self.foo` was a `Foo` type. In other words, I thought you were pulling the `Foo` out of `Option&lt;Foo&gt;` as you might normally with `.unwrap()` or w/e. Okay this seems much more sane now haha. I missed the comment of `self.foo` I guess, apologies, thanks for the explanation :)
You can only go one way. You can go from `T` to `Option&lt;T&gt;` with `Into` which obviously just returns the `Some` but not in reverse.
While looking up the name I did find another language named Leaf, although it had been abandoned 4 years ago and was not in a usable state. Not sure if this is the same Leaf though. &amp;#x200B; I'll change the name if it becomes an issue.
I want only a better autocompletion and try from time to time rust-analyzer. Just tied to build it again with the latest nightly+VSC on my macbook, but I simply can't get it working: use nalgebra::Vector3; fn main() { let v = Vector3::n // Nothing happens } Am I doing something wrong?
The borrow checker can't tell that the fields are disjoint because you're calling a method `s` which requires a reference to the entire structure and not just `size`. It works if you just get a reference to the field: ``` let array = &amp;mut self.arr; let size = self.size; ```
The borrow checker does let you borrow separate fields simultaneously, but it doesn't look into the body of the `s` method when it's checking the `f` method, it only considers `s`'s signature. So for all it knows, `s` could be accessing `self.arr`.
I'm not sure which sub this was intended for but it doesn't look like anything related to the Rust language.
Heh, `nalgebra` is definitely one of those crates with above average trait complexity. We indeed can't complete the associated functions in this case. However, that's a bug specific to completion: we can *resolve* associated functions, and so completions works for methods: https://user-images.githubusercontent.com/1711539/60521147-60672500-9cef-11e9-9c0b-4b757d2e4ef7.png For the curious, here's the bit of code that should handle completion this case: https://github.com/rust-analyzer/rust-analyzer/blob/dd698fc3f7090b7781504566a8b82cf2c3d62420/crates/ra_ide_api/src/completion/complete_path.rs#L59
Any suggestions? Naming is hard...
no shit guys. is there a way to bypass this without boxing/unsafe?
No.
RLS Still seems to crash failure regularly for me. Unwraps somewhere deep in the code. And the tooling isn't robust in restarting it.
A little new with Rust so bear with me. I'm trying to split a string on whitespace and then trimming each word of punctuation, but I'm running into compiler errors that are leaving me stumped. Here's the crux of what I'm trying to do: fn main() { let mystr: &amp;str = "Hello there, Rustacean!"; let splitstr = mystr .split(char::is_whitespace) .map(|x| x.trim_matches(char::is_ascii_punctuation)); for x in splitstr { println!("{}", x); } } Here's the compiler error I'm getting: error[E0631]: type mismatch in function arguments --&gt; src\main.rs:6:20 | 6 | .map(|x| x.trim_matches(char::is_ascii_punctuation)); | ^^^^^^^^^^^^ | | | expected signature of `fn(char) -&gt; _` | found signature of `for&lt;'r&gt; fn(&amp;'r char) -&gt; _` | = note: required because of the requirements on the impl of `std::str::pattern::Pattern&lt;'_&gt;` for `for&lt;'r&gt; fn(&amp;'r char) -&gt; bool {std::char::methods::&lt;impl char&gt;::is_ascii_punctuation}` error: aborting due to previous error If I'm understanding this correctly, it seems like the compiler doesn't like the signature of char::is\_ascii\_punctuation? But I see several examples like this on the documentation for `trim_matches` of `str` which is confusing me.
`trim_matches` expects a function that takes a `char`, but `char::is_ascii_punctuation` takes a `&amp;char`. The example in the documentation is `char::is_numeric`, which does take a `char`, so it works. You can make it work by replacing it with `trim_matches(|c: char| c.is_ascii_punctuation())`. I'm not quite sure why one char function takes a ref and the other doesn't, given that char is `Copy`.
Maybe something that hints at its functionality or doesn‚Äôt mislead the reader. Everyone that comes across this in the wild will initially think this is some kind of alternative to Vec. In fact that is exactly what I was thinking as I glanced at the title and ended up doing a double take.
These are very interesting observations! The stack vs the heap is definitely related to what's going on here, but I think the real explanation is a little different in each case. &gt; Its having to box things altough they are stored in a vec... aren't they already on the heap? It sounds like you're talking about something like a `Vec&lt;Box&lt;dyn MyTrait&gt;&gt;`, for example maybe something like a `Vec&lt;Box&lt;dyn Iterator&lt;Item = u64&gt;&gt;&gt;`, which holds a heterogeneous collection of `u64` iterators. And yes, `Box` is required there. The reason for that isn't that the contents need to be on the heap, though. Rather it's because the contents need to be of a _known size_. When you index into a `Vec&lt;T&gt;` with an expression like `v[i]`, it does a really simple calculation like `mem::size_of::&lt;T&gt;() * i` to figure out where the object you want is sitting in memory. That won't work if the size of `T` isn't a known constant. (Rust expresses this requirement through the `Sized` trait, which is implicit, though sometimes you'll see `?Sized` indicating the places where it's not required. One very notable example being that `Box` is declared as `struct Box&lt;T: ?Sized&gt;`.) Boxing takes a trait object which might be of any size, and sticks it behind a fat pointer of a known constant size, which satisfies `Vec`. &gt; Oh, i have to Rc them as well? This one is probably clearer than the one above, but I think it's useful to think of it in the same terms. If `Sized` is about the size of an object being statically known, then lifetimes are about the point where an object gets destroyed (or moved) being statically known. If I see a type with a lifetime like `&amp;'a str` or `Ref&lt;'a&gt;`, I can imagine that the compiler knows exactly when each instance of that type is going to go away. (That's kind of half true: The compiler does need to insert `drop` calls in all the right places, so it definitely does know when objects go away, but all of the reasoning about lifetimes is done locally for each function by itself.) So putting something in an `Rc` is less about heap allocating it, and more about saying that the time when this object goes away is "unknown"...which does end up requiring heap allocation, because the lifetime of any given spot on the stack is "known" and might not be long enough. &gt; It's also hard for me to tell what is a move and what is just taking a reference. I never had to care in gc languages. This is probably more of a side point of my own than anything you were really driving at, but I want to make a similar observation here: Putting things on the stack vs the heap is definitely one reason we care about moving vs borrowing in C/C++/Rust, but it's definitely not the only reason. Another big reason is that C/C++/Rust let you get your hands on _interior pointers_ into other objects, while most GC's languages don't. (Go is actually an exception here in some cases, though not all, for example interior pointers to a `map` aren't allowed.) Here's a really simple code example of what I'm talking about: fn add_two(x: &amp;mut i32) { *x += 2; } fn main() { let mut x = 2; add_two(&amp;mut x); assert_eq!(x, 4); } The `add_two` function doesn't care where `x` lives. In this case it's another function's local variable, but it could also be the field of a struct somewhere, or an element of some `Vec&lt;i32&gt;`. As simple as it looks, there's no way to write this function in Python! Python doesn't want to hand out pointers to eg. the interior memory of a list, because it has no way to guarantee that such a pointer wouldn't get invalidated as the list grows and reallocates. So yes, one reason for all the ownership and borrowing and moving rules in Rust is that some things live on the stack, and we have to be careful about when they go away. But another equally important reason is that one object can point into the guts of another, and doing that safely requires more than just making sure each object lives long enough. --- At the end of the day, is all of this "memory management"? Certainly some of it is -- Rust does use ownership to free memory. And some of it is "stack vs heap" too -- Rust does use borrowing to make stack allocation safe. But I guess what I'm trying to argue here is that borrowing and ownership go deeper than both of those things. Rust also uses ownership to keep a `File` open, and to prevent multiple calls to `JoinHandle::join` for threads, and to unlock a `Mutex`. And Rust uses borrowing to make sure that no pointer into the `Mutex` lasts past unlocking, and that a `Vec` doesn't reallocate while you're iterating over it, and that ordinary functions can't mutate objects they're not supposed to. Ownership and borrowing are multi-purpose tools that the language and its libraries use in a ton of different ways.
`NotVector`
I'd look for inspiration from logging industry terms like lumberjack, cutter, axe... since your kind of in the middle of the sources and sinks maybe something to do with the river, sawmill, log rolling... or maybe woodsman terms, Woody (which is short for Woodrow), or something like that. I kind of like "sawmill" or something "saw"-related... a google for "sawmill logging" only turns up sawmill.net which is related but just one company's product.
Python supports generics better than does go if you're including using [mypy](http://mypy-lang.org/) with type hints [like Generic](https://docs.python.org/3/library/typing.html#typing.Generic). Your runtime performance still wont hold a candle to Go, but that isn't always a big factor.
I think one of the reasons for "pretending" is that thinking about things in terms of memory layout depends a lot on what previous language experience you have. Of course folks who come from C/C++ immediately see the connection between ownership and stack allocation, and it's very reasonable to introduce ownership to them in those terms. But for folks coming from Python or JavaScript, without any experience with non-GC languages or explicit pointer types, talking about the stack vs the heap can be unnecessarily confusing. It's totally possible to learn the ownership and borrowing rules without knowing or caring what the heap is, and I think a lot of people teaching the language prefer to take that approach. There are also many cases in the language where memory layout really _isn't_ what ownership is about, and I wrote some more of my thoughts about that [in this comment](https://www.reddit.com/r/rust/comments/c7z4m1/one_program_written_in_python_go_and_rust/esl9kwl/).
Yes, naming is hard, but not using totally generic names is easy. Just looking at the github page, that's what comes to my mind. HODR: High-Performance Observability Data Router Frilogs: the (fast and) friendly log service T-RAS: Timber-Rust Agent or Service ...
This reminds me of the old Yahoo Pipes feature. Multi-source inputs mapped to multiple outputs. I didn‚Äôt dig deep enough into the docs to see if mapping more then one sink per source is doable or not. By adding the TCP source, this feel like it could have alternate use cases beyond logging and metrics. Naming is hard. If you‚Äôre thinking about changing it and want some suggestions, feel free to message me. I love spitballing project names. So far, I really like the docs ‚Äî esp the template. I‚Äôll dig into the code later.
Damn I love HODOR.
&gt;you're very unlikely to enjoy working with it. A lot of people mess around with 8-bit era hardware *because* they find it more fun. It's a perfectly good thing to do, as long as you realize how dated it is. &gt;go hit up Adafruit or Sparkfun or something and find a nice modern microcontroller dev board for $20. You can even get a Chinese STM32 board for a couple dollars, which should be much faster than the z80. But in addition to just fun (whether or not it's practical), a TI 85 has a screen, battery compartment, buttons, etc. The cost will go up if you try to put all that together, and it will be more difficult, especially if you want a case to fit it all neatly. So using a pre-built device one already has is cheaper and even more practical, depending on what you want to do with it.
rust-analyser is rapidly improving the IDE experience with Rust. I love it already! I'd reconsider using Open Collective for handling donations. Their policy of only supporting email-based logins with no 2FA doesn't convey a lot of trust from me. Last time I tested it, you could use the same login link for a long period of time over multiple devices which is account hijacking waiting to happen.
I might be a bit late to help you out, but you're literally the only not-spam result in Google for `required_unless_once` so I figued I'd pay it forward for anyone else who might wander here. Buried in structopt's documentation at the very end of [How to `derive(StructOpt)`](https://docs.rs/structopt/0.2.18/structopt/#how-to-derivestructopt) is this little remark: &gt; For functions that do not take a `&amp;str` as argument, the attribute can be wrapped in `raw()`, e. g. `raw(aliases = r#"&amp;["alias"]"#, next_line_help = "true")`. So you have to combine `raw()` with a sorta gross raw string, but it'll work. For example, if I wanted to specify connection parameters or have a `--local` set of defaults, I could do this: ``` #[structopt(long, raw(required_unless_all = r#"&amp;["local", "username", "password", "host", "port"]"#))] local: bool, ``` The good news is that this syntax won't be around forever. Now that [Rust 1.34 supports token streams in custom attributes](https://blog.rust-lang.org/2019/04/11/Rust-1.34.0.html#custom-attributes-accept-arbitrary-token-streams), `structopt` recently [merged some changes](https://github.com/TeXitoi/structopt/pull/198) that does away with `raw()` and lets you just stick the attributes normally into the attribute. That should be available with the 0.3 release of `structopt`.
* micro controllers * lots of linear math across large matrices * cli clients * api servers * distributed message processing
\+1 for HODR!
Interesting article. Unless I've missed something, the Markdown content here seems to be tiny. Any benchmarks on a big Markdown file?
I can only speak to API Servers, but will say that this is a topic that's eminently approachable as a beginner. [rocket](https://rocket.rs/) is a great starting point that will give you the chance to apply much of what you've learned in the book, as well as build your way up to some of the more challenging Rust concepts over time.
Super cool! Render time for the video?
&gt; I'd advocate for getting rid of it entirely for the next edition of Rust. This is not the kind of changes editions can make, to be clear.
woah man, seems like everyone's following that book series in rust now.. Awesome! I too, need to work on completing mine. it was supposed to be 1 weekend and it has been months. üòÖ
Don't write useless accessors, for one thing. Rust is not C#.
That wouldn't be any better than passing `&amp;Bar`, and will have erased the fact that there's exactly one element.
Why do you need to access the size after? It's more difficult to help you with your problem if you leave out information.
Yeah, it is tiny for now. I will progressively enhance with a bigger content.
Very interesting. Have been using FluentD heavily lately, and this looks like it aims to fix a number of the issues I've come across with it. Is there any plan to support a plugin architecture so that new sources / sinks can be added without replacing the binary?
My projects have been more in the vein of text processing and PIM, so I'm not a SIMD expert, but the general rule in a situation like this is to turn to a profiler like `perf` to investigate where the program is spending its time before making any further changes.
Please don't send me spam mails
I was referring to how `&amp;str` and `String` don't implement `IntoIterator` because it's the developer's job to choose between `bytes()`, `chars()`, `char_indices()`, `lines()`, etc. and defaulting to one of them would invite programmer error.
Seems like this has been answered but I haven't seen what I think is a pretty basic and easy reason to not do this. What if you are working with strings in such a way that a fn returns `Result&lt;String, String&gt;`, should `String:: into ()` give you Ok or Err? There is really know way to know other than looking at a case-by-case basis, which means it would be a bad idea to implement either way for every situation.
Designing transport protocols (especially for the WAN) on top of UDP is a very deep subject and an area of ongoing research. It is well out of scope for the language and standard library documentation. You should search for detailed writeups from people who have tried to solve similar problems in the past, and accept that you'll have to deal with them being in other languages.
It's gonna be required with rust 1.37 I think. It is with the current nightly
Sorry you don't like it! Totally get where you're coming from, especially when we post it in /r/rust, but hopefully you find it less offensively bad in the context of infrastructure and observability tools (where no one is thinking about `std::vec`). Either way, we'll be sure to keep `vector` out of any crates we end up extracting :)
A rustc import for firefox caused a build regression in simd crates since they've enabled unstable features with RUSTC\_BOOTSTRAP=1 in their [build.rs](https://build.rs). Failure to build from source is a release critical bug, which would recursively remove all packages depending on it. This also applies to optional features since the source package itself is removed if an optional feature has broken dependencies. &amp;#x200B; There's a patch and an unblock request for encoding-rs which would mitigate the issue, fingers-crossed it's getting accepted in time. ü§û
Might be better to just make /r/rustjobs ?
I love this because it's a case where rust can actually be \*faster\* than even the best C++ template, and because always been touted as one of the advantages of algebraic data types but has historically been so rarely implemented.
The name is fine. I took it as "vectoring" your events around. Of course, with \`Vec\` being what it is, the name is somewhat dubious; but then again, there is no risk of confusion there.
At last! I was looking at the fluentd and fluentbit mess and thinking "about time someone rewrote this in Rust" It's weird that the only benchmark where your product loses is regex, because Rust has an _excellent_ regex library. I didn't look at the code, but the following is very relevant for performance: &gt; Advice: Prefer in this order: is_match, find, captures. Make sure you don't use `captures` where all you need is `is_match`, that can give you a 2x speedup.
It should _always_ give you Ok. Basically `.into()` should not work on errors. It purely exists to wrap it into the Ok variant.
Those are all horrible names, though. ;)
Thanks, this works perfectly!
When you're dealing with logs, there are so many woodworking puns just waiting to happen! Given that you're splitting logs and sending them in various directions, you should be calling it... "splinters" _snickers_
What about writing a test where you "send" it to another thread?
The nominicron has a good chapter about implementing sync and send.
You can just add a test that will fail to compile if a particular type does not implement `Send` and/or `Sync`. Example: https://github.com/BurntSushi/aho-corasick/blob/fa956e606269f61cda244fe15f6e88967516a8ec/src/ahocorasick.rs#L2017-L2031
How about `flume` based on [log flumes](https://en.wikipedia.org/wiki/Log_flume_(ride))?
Yep, we've been digging into the benchmark results and trying to get that one up where it should be. Unfortunately, we do need `captures` since we're extracting those captures out to fields on the event, but we do work pretty hard to avoid things like extra allocations, etc. And for the record, it seems much more likely that the problem is ours and we're simply failing to feed `regex` data fast enough. In general we're finding it very hard to beat its overall parsing speed in more isolated tests :)
If you don't need the flexibility of regexes, a special-purpose parser built using a library like [`nom`](https://crates.io/crates/nom) would probably be faster. Maybe you could make parsers for some common formats and then keep the regex engine in as an option for any other format.
You definitely should be able to. We use Vero to send emails and an unsubscribe link is added to all emails sent from there (to my knowledge).
What's the difference between `extern "C"` and just plain `extern`? Is there one? I've seen it both ways, both in `extern "C" fn my_func() {}` and `extern { fn external_func(); }`. If they mean the same thing, which is better style?
`extern` defaults to the `"C"` ABI if none is provided. Per [the style guide](https://github.com/rust-dev-tools/fmt-rfcs/blob/master/guide/items.md#extern-items), all extern blocks should be marked with their ABI (so `extern "C"` and not `extern`.)
Done a little experiment that took me 1h, a tool that converts brainfuck into Rust and then compiles the Rust into binary. https://github.com/Kerollmops/brainfrust/
My cross post to the c++ Group: [https://www.reddit.com/r/cpp/comments/c8c4qc/crossplattform\_application\_logic\_development\_as\_a/](https://www.reddit.com/r/cpp/comments/c8c4qc/crossplattform_application_logic_development_as_a/)
Also make a library that makes sled support TTL (time to live) keys on a specific Tree. It is not that clean but it works great! I would have wanted to make it work like the Redis TTL by accessing 20 keys randomly and removing the expired ones. https://github.com/LegendreM/sled-transient/blob/master/src/main.rs
I think thats more just thinking about memory layout. Afaik "management" Is usually just for dynamic memory (managed memory) and refers to allocating and deallocating. You dont manage memory in safe rust, you just work with lifetimes (Which as others mention are a very different thing which can be used and explained without memory (i.e a file).
I think combinators read nicer. They express intent, sometimes have added sem√°ntics, use human readable language, and are more declarative. But thats subjective
The problem is more that vector is not a good keyword to search for when you have a problem. Someone that types "Vector" into a search engine is never going to find you library. Even if you type "vector log", you are gonna see a lot of vector images of logs.... A vector is already so many things at the same time: [https://en.wikipedia.org/wiki/Vector](https://en.wikipedia.org/wiki/Vector) But more importantly, I think the name "Vector" is totally misleading because I totally don't see how your application relates to any sort of vector. \+1 for the documentation though
Must be really long...I'm running an i5 @ 3.5 Ghz, and it's been rendering a 480x320 for 10 minutes already. To achieve that level of sharpness it must be &gt; 1600x1200
What alternatives would you prefer? We chose Open Collective as it seemed to match our needs, and allowed us to share our budget and planning around the project.
I'm not the one you replied to, but I have seen people using Donorbox (https://donorbox.org). Amethyst, the rust game engine, uses it for example.
\&gt;r/rustjobs doesn't exist Well, they're not wrong...
I think being googleable is a bigger concern
Thank you for being on top of this. It‚Äôs a shame that encoding_rs does the wrong thing here, even though they were informed about it quite a long time ago.
That's fair, it's certainly not the most google-able. But I would argue we're not far off the [original Latin](https://en.wiktionary.org/wiki/vector#Etymology) or even the modern aviation and aerospace usage of the word, for whatever that's worth :)
I think we should not divided this type of things into separate subreddits. It will make things, difficult to find, especially for newcomers. How about introducing a flair system, like on Firefox subreddit? Ex, help, solved, discussion, job and etc.
The link is about the `rust-simd` package, but I can‚Äôt find a dependency between it and ripgrep, when starting at https://packages.debian.org/source/sid/rust-ripgrep.
[Already occupied](https://flume.apache.org/)
I don't know how I would feel about that. Personally I think it rather be more explicit, especially when it is unclear if T could be an error type. I'm not even sure how useful it would really be. If you wanted to use it to return a result, it's fewer characters to type it out as Ok(...). It could be useful for passing a T into a fn that wants a result as the input parameter, but that didn't seem to happen often. Is there some scenario other than these that you think this feature would be useful?
The dependency tree in sid has been patched, but there is no automatic import into testing due to the freeze. encoding-rs has a dependency to simd, if simd is removed that would also remove encoding-rs, which would break ripgrep. https://packages.debian.org/buster/librust-encoding-rs+simd-accel-dev
Donorbox is geared towards individual donations more. Open Collective is the only product we found that is built for having public, larger sponsors upfront. They have facilities for bookkeeping and people getting expenses out. While this is not a feature we will use heavily for rust-analyzer, the potential to have people expense e.g. small work on the projects they were doing easily on their credit card is helpful I'll relay your feedback to Open Collective though, they are quite responsive.
Very timely post, and quite informative. However, it needs improvement by adding examples, such as source code snippets - or better yet, a complete toy project.
This is something we've thought a lot about would love to support. Some ideas we've had (in roughly increasing order of complexity): * Making things like the http sink configurable to cover most http-based systems * Providing a lua (or eventually wasm) API for building sources and sinks * Good ol' dynamic loading of sources/sinks built in rust * Designating some kind of RPC protocol for use with external plugin processes, probably paired with libraries to make writing and running them more convenient
We do actually use nom for some other parsers! Having such great options for parsing was a big advantage to using Rust for this project. In the benchmarking we've done, regex will actually beat a lot of parsers written with nom. Obviously it depends enormously on the complexity of each parser, but for our purposes we've found regex performance to be pretty outstanding (which is why we'd love the end-to-end test to better reflect that).
Well as I said I primarily use `into()` if there's a chain of adapters like for example: Database.list_of_users()?.iter() .filter(|user|user.id &gt;= 1000) .filter(|is_user|alphanumeric(user.name)) .map(|user|user.id) .take(1000) .collect::&lt;HashSet&gt;() .into() To wrap Ok around such a large expression would make it less clear to me. But it's fairly easy to just define a `IntoOk` trait as a blanket implementation onto `Try` so you can just use `.into_ok()` at the end that works with both Option and Result.
1. i don't. i'm interested in general situation where you'd want to do something with part of structure while another part is affected. strange that borrow checker can't figure out field access. 2. because i want to declare next to usage
The C interop in Rust is nice. The blog posts that cover oxydizing libraries are good reads.
Several prominent JavaScript projects use Open Collective to great success, as well.
Something doesn't smell right here. `encoding_rs` ditched the `simd` dependency quite a while ago. Today, `encoding_rs` **optionally** depends on `packed_simd`. Neither ripgrep nor `encoding_rs` needs `packed_simd`, which is a nightly-only crate (the intent is to merge it into std at some point).
Understood. In that case I don't have much to add that wasn't already said. There are some weird antipatterns but generally you'd be better off doing the mutable borrow last.
So I would probably write out the expression assigning it to variable (I'll call it result), and then just `Ok(result)` on the last line. That said, I had thought about having traits for `into_ok()` and `into_err()` as well, and I think that would be my preferred approach.
Thanks for sharing this! Your paper‚Äôs already on my ‚Äúto implement‚Äù list and this will make it a lot easier (my renderer is written in rust). Looking forward to future extensions to allow progressive sample sequences!
I am a begginer programmer. I find it quite hard to transfer variables across rust files/mods. The classic way is to set a const or a static, but what if i need a variable that's not constant. A mutable static, which provokes the use of unsafe rust, even just accessing the variable. If this isn't the only way to share variables, it would be cool if someone could point to me some of the other methods. If not, how can i make unsafe rust safe? I heard there are these crates that kind of do lazy borrowing, and if that's the only good option, i would like to know how it works. I thought about creating a mutable variable that tells whenever the variable is being borrowed, but it would also need to be static mut. Hope i find a good soultion, Thanks in advance !
The one I received didn't have such a link, so I thought it was spam. I like your project on a second look :) Would still appreciate an unsubscribe link, most newsletters have one.
Oh awesome! I think we're slated to release the PBRT code soon so you can have a reference in C++ for a full sampling implementation. Also, if you need help with the different variants we introduce (jittered/multijittered/cmj) feel free to hit me up! There are some clever tricks to ensure that we uniquely map substrata for each large strata in each dimension to ensure the different jittering guarantees. They may seem a little dense in the code samples but the basic gist is that we use the base N representation of the index to select different strata (where N is the N x N grid that we break the domain down into).
That's only an issue so long as you can't specify that T and E must be different types. I get the impression that Rust can't do the following: impl&lt;T: !E, E&gt; From&lt;T&gt; for Result&lt;T, E&gt; { ... }
Still very new to Rust but I‚Äôve done a few searches and couldn‚Äôt come up with an answer. Is there any way to get Rust Racer running on stable and not nightly? If not, is there any official ETA on when it will be possible?
Absolutely, here you go: https://pastebin.com/raw/SLFQwDL9 Should I be passing around an &amp;GamepadId? I've tried cloning GamepadId instead of passing it along as a hack, to see if it would work and it gives me a similar error.
So you really feel that `into()` vs `into_ok()` makes such a huge difference when the function return type is `Result&lt;_,_&gt;`?
Instead of making a global variable, would it be reasonable to have functions that need this variable to take it as a parameter? Or, if you are absolutely sure that you need this variable to be global, and cannot be a function parameter, you may use one of the solutions described [here](https://stackoverflow.com/questions/27791532/how-do-i-create-a-global-mutable-singleton).
It's unlikely. That said, you can use the rls, which builds on top of racer. By installing it through rustup, you're able to use it on rust stable.
Rust is making this hard for you because there's a ton of pitfalls. That said, if you \*really\* want to do this, you can use the lazy\_static crate, plus a mutex. If you're only using one thread, you could also use [https://doc.rust-lang.org/std/macro.thread\_local.html](https://doc.rust-lang.org/std/macro.thread_local.html) with refcell, like in those docs. &amp;#x200B; The alternative is to pass the data you need to the functions you need, instead of making them global. This is probably better in most cases.
&gt;strange that borrow checker can't figure out field access. I think framing this of "can't figure out" isn't the right way to look at it. Rust only analyzes function signatures when determining safety features, because the signature is the contract. If your signature says that you borrow everything, then that's what you are doing. If it \*did\* analyze bodies of functions, then a change in the code in the body could break code that's calling the function, which seems bad: the interface didn't change, but it no longer works!
I wouldn't call any of those examples 'functional programming' personally. I love FP, but I hate how it's become a buzz-word for anything even remotely related.
Looks cool.
Thanks for the fast reply Steve! I‚Äôm currently using Visual Studio Code with the Rust (rls) extension. In all honesty, I haven‚Äôt read too far into the configuration yet. I was under the impression you needed Racer for the code completion but rls seems to do that too. Is rls effectively Racer plus more? Will read into the docs some more and get it set up, thanks again!
Yes, the rls is racer plus more. Let me know if you run into anything else!
How do I have multiple projects in a same repository? I am planning to keep all my rust programs in a single repo for consistency and brevity. I am still a beginner and would prefer them all at one place. The problem I am facing is that when I load the project directory in VS Code, I cannot build projects inside a directory. For example I have a project in `/book/guessing_game` with Cargo.toml in the guessing_game directory. I want that the rust-extension should build and run the project as is. However, It doesn't. And it instead complains of the missing cargo.toml file in the root directory. How can I achieve this?
The "object oriented" or "data oriented" way to do this would be to have a struct that has whatever shared variables you need inside of it, and methods for that struct which use those variables by taking a `self` parameter. By using Rust you may have to structure your program differently and think about how exactly you're coupling different parts of your code.
[It won't necessarily be copied on the stack.](https://users.rust-lang.org/t/can-i-trust-rust-to-optimize-move-semantics/1137) The Rust compiler will optimize it for you.
It makes it much more clear exactly what is being converted into what. My immediate thought if I were to see `into()` resulting in a `Result&lt;T, E&gt;` would be that the source is some `Result&lt;U, E&gt;` and `U: Into&lt;T&gt;`, not that you're converting `T` into `Result&lt;T, E&gt;`. `into()` usually does at least some transformation to a different structure at a type level. Turning it into a result is just kind of... wrapping it?
[`assert_impl!`](https://docs.rs/static_assertions/0.3/static_assertions/macro.assert_impl.html) from the [static_assertions](https://github.com/nvzqz/static-assertions-rs) crate can do this: assert_impl!(str; String, Send, Sync, From&lt;&amp;'static str&gt;);
By 'combinator version' are you talking about the version using rayon? That's hardly a helpful comparison.
There are some more context and links provided in https://github.com/rust-lang/cargo/issues/6627#issuecomment-507809208
One way would be to make a [cargo workspace](https://doc.rust-lang.org/book/ch14-03-cargo-workspaces.html) at the root, and add all projects as members? The root can just have a workspace-`Cargo.toml`, and building that will build all projects in the workspace. I'm not sure if this will solve the issues with rust-extension, as I haven't used that - but this is a common strategy used by multi-crate repositories.
/r/playrust
&gt; I never had to care in gc languages. In some you do, and I'd argue if you want to develop a mastery of any language you have to think about it eventually anyway. For example, in Go the use of references is pretty abundant, and it's a GC language. Even in a language like javascript, it's helpful to know that if you pass an object or a map to a function, it's passed by reference. I do agree that's not really the same 'thinking' that you have to do in Rust, but I am arguing that you can't simply ignore it.
 [https://www.reddit.com/r/playrust/](https://www.reddit.com/r/playrust/)
Wtf is going on? Does that mean no ff in debian?
I linked https://github.com/RustAudio/lewton/pull/50 that is more or less that. I guess a toy project can be used also for CI purposes.
How about "Victor", oh yeah! That Victor from despicable me. This is not very far from the original name, but it doesn't confuse with std::Vec anymore.
keep in mind that that set of games that need UDP protocols is smaller now than it was say, 20 years ago. If you are making a twitch shooter, then yes, you still need UDP. But many strategy, role playing, puzzle, or slower paced games can now do just fine with TCP, and it saves you a bunch of complexity.
There are many `From`s that just trivially wrap or are even zero-cost type conversions; the language is full of of that like for Box, Cell, RefCell, Rc andsoforth which can all be created by `into()`, non-ops are things like a String can be converted to a Vec&lt;u8&gt; this way. That `Option&lt;T&gt; : From&lt;t&gt;` is consistent with that the Rust stdlib provides this for pretty much every trivial one-element container; rather it seems like `Result&lt;T,E&gt;` is the one exception which is probably with the rationale that there is also an Err branch but that the Ok branch is the default variant having special status is well established by many other things that are implemented for `Result&lt;T,E&gt;` so I find it a design inconsistency that `Result&lt;T,E&gt; : From&lt;T&gt;` is not implemented.
I have tried that, but since cargo build allows only one single entry point, I cannot have two main functions in the different files. It results in error.
The version of `encoding_rs` packaged in Debian (0.8.15, released 2019-01-29, according to git history) still has the (optional) dependency on `simd` via the `simd-accel` feature. The 0.8.15 `encoding_rs` release happened after the [Buster freeze started](https://release.debian.org/buster/freeze_policy.html) (2019-01-12). According to [Debian Rust Packaging Policy](https://wiki.debian.org/Teams/RustPackaging/Policy), AFAIU, every feature is mapped onto a (binary) package, and thus if _any_ Rust feature of a package has a release-critical bug, such as failing to build, the whole (source) package is affected, and in turn all packages depending on _any_ of the binary packages produced by the source packages are affected. So, in summary, if package A (ripgrep) depends on package B (`encoding-rs`) and package B has a bug in _any_ of its optional features, this is a RC bug for package B, which must either be fixed, or package B will be removed, leading to the removal of A as well, as it is now missing one of its dependencies. I hope I got this right; I'm an experienced Debian user and also a Rust developer, but I've no first-hand knowledge of the Debian Rust packaging intricacies.
&gt;You have destructors in Python, but... Idiomatic python doesn't make you care about destructors and reference cycles and all that. Context managers (a.k.a. with-blocks) take care of clean up for you.
axed - axe daemon
Thanks for explaining. That sounds ridiculous to me though. If an optional feature is broken, especially one that depends on a nightly compiler, then I don't see why Debian can't just drop support for that optional feature. Moreover, presumably Debian builds everything with a stable compiler, and the simd crate never compiled on Rust stable.
If true this does not bode well for the future of Rust in Debian packages.
Sorry, I might be misunderstanding or miscommunicating? The main feature of cargo workspaces is being able to have two completely separate cargo projects joined together under a top-level umbrella project. All separate projects in a workspace can have main files, since they all have their own `src` directories and own `Cargo.toml` files? For instance, see the structure of the https://github.com/serde-rs/serde/ repository. The `Cargo.toml` in the top directory is just: [workspace] members = [ "serde", "serde_derive", "serde_derive_internals", "serde_test", "test_suite", ] Then each of the projects in the directories "serde", "serde_derive", "serde_derive_internals", etc. have their own `Cargo.toml` and `src/lib.rs` and/or `src/main.rs`. ---- With that said, if your end goal is to make multiple executables, it's also possible to do with only one project. If instead of having `src/main.rs`, you make multiple files `src/bin/a.rs`, `src/bin/b.rs`, then `cargo build` will produce multiple executables `a` and `b`, each with their own entry point (`cargo run --bin a` and `cargo run --bin b` will run `a` and `b` respectively). Both `a` and `b` can depend on shared functionality exported from the `lib.rs` file for the crate. This can be done in addition to or instead of a workspace. Hopefully that's useful?
I agree with everything you said. Just one small point I want to make: &amp;#x200B; \&gt; But for folks coming from Python or JavaScript, without any experience with non-GC languages or explicit pointer types, talking about the stack vs the heap can be unnecessarily confusing I could see that, but that's also the advantage of Rust! I love telling people how Rust "objects" can allocate on the stack (vs. say Java where all objects must be heap-allocated), thus Rust is faster for it! No heap allocation, no pointer to deference! So I think it's important to mention these details, since ultimately we can view them as ergonomic trade-offs that Rust made in pursuit of speed.
Hello! I'm a newbie for coding in general and, with a sincere and quite desperate interest in coding so that I could get a job in the coding industry, I've been looking for a language that I could learn. Rust seems to be the language that could indeed suit my interests. (Whether I could indeed master the language to the professional level that the potential employers would require of me within a year is a mystery at this point but I can't afford to worry about that now, since the whole Rust language has really got me captivated atm!). What I wanted to ask, was that during my search for learning materials for Rust, I'd discovered the CS140e course, that ran last year, for learning to code an entire OS in Rust! I managed to get the resources re-uploaded by u/sbenitez/ as mentioned on this thread, but the lecture videos and/or audio recordings were not there. I was wondering, has anyone had any better luck in finding those lecture recordings? or were they not made available to the public to begin with? Best regards.
Fixed by https://github.com/sinkuu in [#1472](https://github.com/rust-analyzer/rust-analyzer/pull/1472): https://user-images.githubusercontent.com/1711539/60546857-e5b9fc00-9d26-11e9-8709-3d2d77915e50.png
&gt; This also applies to optional features since the source package itself is removed if an optional feature has broken dependencies. What's the rationale behind this guideline ? Many packages have an optional feature named `unstable` (or similar) that enables unstable Rust features, or dependencies that require them. These features enable unstable Rust features or unstable dependencies, but as long as one does not explicitly enable them, breakage cannot happen. So how come that an optional disabled feature in a dependency graph is removed?
Also, increased jobs means decreased SNR. Jobs will continue to be posted here if they don't know about the new subreddit, meaning this problem is not addressed.
Indeed, it seems there's something amiss in the Debian build process, since _actually building_ `encoding-rs` with the `simd-accel` feature enabled would have failed, thus exposing the bogus dependency. So there's some things I see that would need addressing: - On the Rust side, there's no way to mark a crate as nightly-only (or, more generally, the minimum Rust version required, which may be "nightly") - On the Debian side, excluding features that rely on too-new rustc versions, which would obviously _always_ exclude dependencies that rely on nightly. - On the Debian side, building libraries with all features (or combinations thereof, I guess).
Whoops, I had meant to mention that and completely forgot. Sorry! That falls into the same bucket as `defer` in Go from my point of view.
A weekly stickied "who's hiring" thread would be quite useful here
You seem to be optimising `[profile.dev]` instead of `[profile.release]`.
Say I'm importing a struct called "Hash" from a blake2b dependency and want to rename it to "Blake2bHash"... What do I do? &amp;#x200B; \`\`\` *use* blake2b\_simd*::*Hash; \`\`\` &amp;#x200B; Tried searching and coming up dry.
Sounds good, thanks for the update. Better auto-completion in VSCode will be a great help. And it's nice to know there is a team working on this, rather than it being the effort of just one person.
&gt; This also applies to optional features since the source package itself is removed if an optional feature has broken dependencies. What's the rationale behind this guideline ? Does it apply to optional features that are used, or also to the ones that are not used?
My god, those performance metrics look **impressive**.
 use blake2b_simd::Hash as Blake2bHash;
OpenCollective is great except for this. There's no other alternative for organizations that I know of which doesn't lead to more management.
Doing some more tinkering on [`pdf.js-wasm`](https://github.com/ThomasdenH/pdf.js-wasm). I also [opened a PR on `wasm-bindgen`](https://github.com/rustwasm/wasm-bindgen/pull/1638) to add the convenience methods `is_truthy` and `is_falsy` to `JsValue`.
I have followed the same structure. The problem happens because the rust extension for VSCode runs cargo using the root file for config. I however want them to run independently. The error given states that I can't have two main() functions and if I have to build it, I would have to use --bin as suggested by you. What I wanted simply was to run a nested project in VSCode with single build command. Currently inhave to resort to a side terminal window.
This is neat! I just started dabbling with tool for structured-logging -&gt; feature-extraction -&gt; transport, with the intended destination of columnar storage. The idea is to immediately compress a structured log into a vector of integers for efficient transport and storage. I have some hard-realtime requirements, but I still need logging. I see that columnar storage is on your roadmap. Does Vector support a packed binary message format by chance?
Wow! Arigato gozaimasu sinkuu!
This is the best problem to have. I'm encouraged that there's enough rust work to need to look for engineers.
Nicely done @ctz This comment made me giggle: &gt; OpenSSL functions seem to have quite significantly deep call trees (25 frames in places) and significant allocator use. webpki, in contrast, features zero allocator use and does not copy the certificate data during parsing. It is very nice how lifetimes let you ensure that single buffer remains alive to be passed around.
These are some extremely impressive numbers, but when it comes to security-critical code like this it's definitely not my main concern. How many side-channel attacks is rustls vulnerable to that OpenSSL has had forever to harden against? How much of this performance difference is due to this hardening?
`Debug` is pretty useful.
Pretty much everything in [`std::prelude`](https://doc.rust-lang.org/std/prelude/index.html)
&gt; The version of `encoding_rs` packaged in Debian (0.8.15, released 2019-01-29, according to git history) still has the (optional) dependency on `simd` via the `simd-accel` feature. The 0.8.16 `encoding_rs` release (which switched to `packed_simd`) happened on 2019-02-07, slightly before the [Buster soft-freeze started](https://release.debian.org/buster/freeze_policy.html) (2019-02-12). This is why I use rolling release.
That book is widely regarded as a very good resource for network programming.
I started using RA a few weeks ago and am really impressed; definitely worth $5 a month to see how far you can take it. Is mousing over identifiers for type information held up on something bigger or is that kind of type information harder to get internally than it seems?
There hasn't been a security audit yet so I guess for public web servers, this is an security risk. However, this is a great candidate to use for scrapers/spiders/crawlers which may only use SSL to encrypt traffic and not necessarily any authentication or message passing.
There hasn't been a security audit yet so I guess for public web servers, this is an security risk. However, this is a great candidate to use for scrapers/spiders/crawlers which may only use SSL to encrypt traffic and not necessarily any authentication or message passing.
Runtime as in "while the procedural macro is executing" or runtime as in "while the generated binary is executing"? ...because, from what I know about how Rust compilation works, I don't think you can do the latter. That sort of stuff is stripped away as part of generating the machine code, similar to how your code comments don't wind up in the final binary.
Another option, that is not quite as convenient but will avoid a lot of the pitfalls, is supporting a `Plugin` trait in the code and offering a library. That would mean custom plugins require you to compile yourself, but a custom `main.rs` file could look as simple as : ```rust fn main() { vector::build() .with_plugin::&lt;plugin_crate::SomePlugin&gt;() .start() } ```
&gt; OpenSSL was built from source with default options, using gcc 8.3.0. rustls was built from source using rustc 1.35.0. This is making the comparison unfair. gcc and llvm optimizers are different from each other so it is always also a benchmark of gcc vs llvm. You should have used a clang corresponding to the LLVM version your particular rust version used. But otherwise of course it's great. Looking forward for cargo and [rustup](https://github.com/rust-lang/rustup.rs/issues/568) to adopt rustls!
Ah, alright, that makes sense. I guess I can't really help much more then personally - I've always just used a terminal to run projects. Hopefully someone else here has more experience with that plugin. If not, maybe it'd be worth the effort to submit a feature request to the plugin for this?
\&gt; I'll relay your feedback to Open Collective though, they are quite responsive. Thanks, I've opened an issue describing the link reuse I mentioned before: [https://github.com/opencollective/opencollective/issues/2192](https://github.com/opencollective/opencollective/issues/2192)
Thanks! It takes a lot of time to render. In AWS c5.18xlarge, the video rendering took about 2.2 hours. Here is the reason. The number of total pixels to render is 12902400000 pixels. \- the frame is 1280 x 720 = 921600 \- for anti-aliasing, each pixel uses an average of 70 times. so 921600 x 70 = 64512000 \- the number of frames in the video is 200. so 64512000x200 x 200 = 12902400000
Rustls uses *ring* for all the stuff that is sensitive to side channels, and there are many fewer side-channel issues in the *ring* code than in OpenSSL, since we fixed many of then in BoringSSL and in *ring* itself. (*ring* is forked from BoringSSL which is forked from OpenSL.)
1. [`std::borrow::{ToOwned,Borrow}`](https://doc.rust-lang.org/std/borrow/trait.Borrow.html) are pretty important as if you can grok them you basically understand the ownership model. 2. [`std::convert::From`](https://doc.rust-lang.org/std/convert/trait.From.html) is just extremely nice as you can avoid declaring `new()` all the time. 3. [`std::std::FromStr`](https://doc.rust-lang.org/std/str/trait.FromStr.html) just lovely since it makes a high level "_just parse that data from a string_" interface. 4. [`std::ops::Deref`](https://doc.rust-lang.org/std/ops/trait.Deref.html) &amp; [`std::convert::AsRef`](https://doc.rust-lang.org/std/convert/trait.AsRef.html) lets you do magic pretending some types are other types. Allows for trait inheritance, flexiable generic parameters. 5. [`std::iterator::Iterator`](https://doc.rust-lang.org/std/iter/trait.Iterator.html) honorable mention because it lets you do SO MUCH!!
You're assuming that OpenSSL is high quality software that's been throughly validated to weed out any issues. I don't think is necessarily a valid assumption.
In real world usage, OpenSSL is almost always compiled with GCC, whilst rustls is always compiled with an LLVM backend, so it seems like a pretty fair and realistic comparison.
Finally gave rust-analyzer a try today, but can't seem to get it to work (besides some degree of auto-completion) at all (go to definition, etc all absent) on vim-lsp. Back to rls for the time being.
[std::ops::DispatchFromDyn](https://doc.rust-lang.org/std/ops/trait.DispatchFromDyn.html) ... just kidding :) It's pretty useful to know how[FromIterator](https://doc.rust-lang.org/std/iter/trait.FromIterator.html) relates to the `collect()` method and more importantly what types it is implemented for (hint: not just your standard collection types) In a similar vein is the [FromStr](https://doc.rust-lang.org/std/str/trait.FromStr.html) trait and corresponding `parse()` method.
All evidence is to the contrary.
I don‚Äôt like to shit on it cause I feel like the authors don‚Äôt deserve any flak, but yeah.
We generate &gt;= 1 installable "binary" packages from each source package, if one of them isn't installable (eg. due to missing dependencies), that's considered a grave bug in the *source* package and could eventually cause removal of the source package and all binary packages. Used/unused doesn't matter much, but we can strip features that are unused and problematic, this is what we currently do in the patched upload we're trying to get unblocked.
It is fair to be glad that there are people working on it, and they are doing good work, and also it's a really old, crufty codebase with a lot of problems.
But it was never started with the intent to be such a critical piece of software
My point is that it isn't shitting on anyone at all.
Right everyone is being polite while acknowledging that OpenSSL is not very good security critical software.
This is very useful project. This could save a lot of time porting java projects that used swagger. The installation instruction failed with `error: no binaries are available for install using the selected features` Looking at the code I did manage to install it using. ``` cargo install paperclip --features "default cli" ``` As I understand it, this will generate a stub/boilerplate code for a new rust project based on the `openapi yml` configuration file. I'm working on something that has a reverse of the workflow. Parse an existing rust project and create an openapi yml configuration.
Hey There is [https://github.com/jasonwilliams/boa](https://github.com/jasonwilliams/boa) There's also a talk you can watch here: [https://2019.jsconf.eu/jason-williams/lets-build-a-javascript-engine.html](https://2019.jsconf.eu/jason-williams/lets-build-a-javascript-engine.html)
As someone looking for a Rust freelancer at the moment, this would be immensely useful. For sure!
Surprised it's that fast. Cool!
I bumped into this, and wondered if anyone is using it with rust: https://rr-project.org/
So I'm ignorant of OpenAPI, but isn't the latest on v3? I see so many projects on v2, makes it feel like we have a Py2 vs Py3 scenario. Cool project though!
I'm not assuming that OpenSSL is high quality, only that it's old and widely used. Both of these tend to attract the sort of attention that weeds out bugs and potential attack vectors, but that by no means implies that the current state of OpenSSL is 100% bug free. All I'm saying is that rustls is not yet old or widely used, and may not have had similar levels of attention paid to it yet. And when we're talking about security critical code, I'm personally going to pick the option that's been battle hardened.
That's awesome to know, that kind of information definitely makes me more comfortable with using/trusting rustls.
Surprising that the Wasm is 20% larger than the JS. I wonder why.
That looks awesome! Makes me want to build a log analysis engine based on tantivy :)
Nice! Now I really want to read the security analysis on this! Would love to use it for the personal web app I‚Äôm writing for myself (and plan to host from home).
Yes I‚Äôm aiming for battle Royale game. Which is fast paced
This is exactly what Rust should be used for. Very, very nice.
Thanks. I‚Äôve already start to read that and I‚Äôm enjoying. Thanks for sharing
I find myself using `std::fmt::Display` for displaying my custom types in CLI apps.
CharlesIngalls.io
How about maul, as in a maul for splitting logs?
We remove features that depend on crates we can't compile (clippy being a common one), the problem is that the simd crates did in fact compile on our stable compiler. It's possible to use nightly features on a stable compiler by setting RUSTC_BOOTSTRAP=1. Obviously our build system doesn't set that, but what we didn't know is that crates are able to set this variable inside build.rs. This is how crates with unstable features slipped through. The situation is unfortunate but hopefully we're getting our patched encoding-rs approved the next few days, release is due on Saturday.
Oof. That is nuts. I didn't think the simd crate always had that set though, but my timeline might be off. Seeing that environment variable is a bad idea. I argued against it for packed_simd, but apparently failed to be convincing enough for other crates.
Also refers to a type of foliage: https://www.merriam-webster.com/dictionary/leaf
I used it heavily on a Rust project a couple of years ago. I highly recommend rr + Rust.
We do this and it has worked out great. For android it is very nice that you can write the JNI interface in rust completely with the jni crate.
I wrote a blog post, [A FizzBuzzy Tour of Traits in Rust](https://www.joshmcguigan.com/blog/fizzbuzz-tour-of-traits-rust/), which I think provides a pretty good (if limited) introduction to traits in the standard library, once you've got a grasp of the derivable ones (copy, clone, debug, partialeq, etc).
How is the current yanking behavior?
At this moment I'd still like to leave the job postings to individual top-level threads. If we could have more than two stickied threads at the same time then I'd be more open to considering a job thread, probably refreshed monthly, but at the moment our two stickied threads (which are refreshed weekly) are both consistently more active than I suspect a job thread would be, and we already cycle one of those two out for a few days per week to sticky TWiR. If you think that job posts would be more common if we explicitly encouraged them, then I'm open to suggestions of how we could encourage them in ways other than a stickied post. For example, we could have a monthly non-stickied thread summarizing the Rust positions listed in Hacker News' monthly "Who's Hiring" thread, with explicit encouragement for companies to post further job opportunities in that thread. If someone would like to take the initiative on doing such a thing, or on some other idea, then post here and I'll deputize you. :)
Doesn't the compiler already enables specialization tho?
&gt; But it was never started with the intent to be such a critical piece of software. That goes right out the window when [people are paying you money for support](https://www.openssl.org/support/contracts.html).
Thanks! I‚Äôll definitely take you up on that.
Sweet! What a fun project. I took a graphics course from Pete Shirley!
Thanks! &gt; The installation instruction failed ... Yeah, I'd feature-gated CLI but I didn't realize that this would become a problem until after publishing the crate. I made a quick fix by updating the docs, but for some reason, Travis took an awful lot of time to pick up the job. I've triggered the build again, the docs should be updated in a bit. &gt; ... this will generate a stub/boilerplate code for a new rust project based on the openapi yml configuration file. I'm working on something that has a reverse of the workflow. That is correct. The CLI will generate a Rust project (lib/bin) whereas the crate itself generates Rust modules. Aside that, I'm envisioning paperclip to have whatever [go-swagger](https://github.com/go-swagger/go-swagger) currently has. And, go-swagger supports checking an existing project and generating the corresponding spec. Would you be willing to add that support to paperclip?
I think it's exactly the Py2 vs Py3 scenario. A great deal of projects are using v2, that's why I started paperclip with v2 support. v3 is relatively new (it was born ~2 years back, IIRC), but I do have plans for it too! :)
I think a periodic thread would be useful for people like me who are looking for Rust coworkers but who are not part of a known company or team, not part of a hype ring or anything. I don't see myself spamming the main /r/rust page and annoying a lot of people with an ordinary Rust job opening but I'd like to advertise it in a dedicated thread where it would be welcomed.
There is an 'while let' construct for this. Sorry I am currently on mobile so I can't really format it properly.
You can use `loop` and `match` like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5cb1c117a5c79bce23194cf99d53173f), but Rust has `while let` specially design for this case, [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=91ec40029510b92fd309211b974fb2ad).
Yes. Instead of .iter I changed it to par_iter, and then I had to add rayon to the prelude, and chunk the slices. if I then change back to iter, that version was faster than the proposed, but the rayon version was much faster.
I've definitely done it before using Visual Studio Professional and Community editions, done it with Code too but I don't have that in front of me right now. First things: Make sure you are using the MSVC Rust toolchain and make sure you have debug symbols enabled (you should see .pdb files in target/release or whatever target you are trying to debug). Assuming that is all good. You'll need native debugging tools installed for VS (you should have this if you have C++ stuff installed). Open VS, do File -&gt; Open -&gt; Project/Solution and open the executable you want to debug. This will create a solution with just an exe in it. Then you can start debugging. If you break during debugging you should see your Rust source in the VS source window. You can also browse to your Rust source add breakpoints and the debugger will stop on them. VS Code is probably nicer in that it can open a Rust project so it's easier to find your source, but VS Pro has a nicer debugger. VS also doesn't understand Rust types and won't display everything correctly in the locals/watch windows.
Thanks!
i don‚Äôt recall (am on mobile and lazy) but i believe the doc‚Äôs are autogenerated. starts with a D i believe but if you visit the docs (idk about the Book) it should say what‚Äôs powering the autogen stuff.
kek
I wouldn't be so sure about that. Heartbleed for example, while it probably wouldn't have happened in a memory safe language, [did affect clients as well](https://security.stackexchange.com/questions/55119/does-the-heartbleed-vulnerability-affect-clients-as-severely).
I would say the same for just about every project lol
Great post! I also appreciated the ‚Äútoo much detail‚Äù article that you linked at the beginning. I find that `From` is something I get some of the most use out of personally. Another one that could maybe have been added is an example of using the `map` function with `nums_iter.map(FizzBuzz::from)`
My understanding is that the Cargo bug is fixed so it doesn't matter.
Which editors / IDEs are the developers of rust analyzer using?
Here is a full working version,which uses a `while let` loop to get to the last node:[https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=45aac56b13f419cff42d796712bd299e](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=45aac56b13f419cff42d796712bd299e)
The isn't the sub you are looking for. This one is for the programming language rust. You need r/playrust
Yes
Can my code below be improved upon? // Given a list of integers, use a vector and return the mean (the average // value), median (when sorted, the value in the middle position), and mode // (the value that occurs most often; a hash map will be helpful here) of the // list. // From https://doc.rust-lang.org/book/ch08-03-hash-maps.html#summary pub fn mean_median_mode(vector: &amp;Vec&lt;i32&gt;) -&gt; (f64, f64, i32) { let mut v = vector.clone(); v.sort(); let median = if v.len() % 2 == 0 { let index = v.len() / 2; (v[index] as f64 + v[index+1] as f64)/2.0 } else { let index = (v.len() + 1)/2; v[index] as f64 }; use std::collections::HashMap; let mut map: HashMap&lt;i32, u32&gt; = HashMap::new(); let mut max_count = 0; let mut mode = 0; let mut sum = 0; for e in &amp;v { sum += e; let count = map.entry(*e).or_insert(1); *count += 1; if count &gt; &amp;mut max_count { max_count = *count; mode = *e; } } (sum as f64 / v.len() as f64, median, mode) }
Would still be interesting to do a clang-based comparison and see how the difference factors out, though!
It has been done by Chris Krycho for [New Rustacean](https://newrustacean.com/show_notes/). You can probably look at he did it.
Ah I keep getting things mixed up, sorry for that, I blame the lack of coffee. What I meant was [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=74f3a2aedb4669845252971364868e33), you stick your thing in a `Vec`, and pass that. The elements of a `Vec` need not be on the heap, but passing a `Vec`, amounts to passing a somewhat small tuple `(Pointer, Length, Capacity)`, but does move ownership appropriately. Indeed it's not possible with a slice I guess, my thought would be to take a mutable one and drop the elements manually, but that would mean you'd have to put in a default element, which gets ugly quickly (could work through `&amp;[Option&lt;Bar&gt;]` and you'd put in `None`).
I shall ask the extension maintainers. For now the terminal it is.
Yes, I d√∂.
See [https://ferrous-systems.com/blog/sealed-rust-the-pitch/](https://ferrous-systems.com/blog/sealed-rust-the-pitch/) for some work in that direction
Oh sorry, I just Seen Rust and thought this was the right one, lol. Sorry
What about something like snowdrift.coop? Woukd that fit your bill? Disclaimer: I'm not affiliated, I just like the principle behind it. Also, it's not fully finished yet.
I fail to see "paid software" = "high quality code". If I understand correctly the issue there is the architecture of the code, some difficult to get right even for experienced programmer, and openssl folks are primarily mathematician.
 fn main() -&gt; std::io::Result&lt;()&gt; { let queue = Arc::new(Mutex::new(Vec::new())); let mut queue_copy = queue.clone(); //this works thread::spawn(move || myModule::myLongRunningFunction(&amp;mut queue_copy) ); HttpServer::new(move || { App::new() .service(web::resource("/ws/").route(web::get().to( move |r, stream| // ? ws_index(r, stream, ????)))) }) .bind("127.0.0.1:8080")? .run() } fn ws_index(r: HttpRequest, stream: web::Payload, in_queue : Arc&lt;Mutex&lt;Vec&lt;Message&gt;&gt;&gt;) -&gt; Result&lt;HttpResponse, Error&gt; { //create new websocket with the given arc clone } I am trying to give each websocket (which to my understanding may live in another thread or not) a copy of the arc. If I replace `????` with `queue.clone()`, I get `cannot move out of captured variable in an Fn closure`, which I don't really understand. I suspect it has something to do with the nested closures?
I'd also add the recently stabilised [`TryFrom`](https://doc.rust-lang.org/std/convert/trait.TryFrom.html) trait, for when you want `From` style conversions could fail.
this will be awesome for our deployed Rust services :)
Wrong sub. &amp;#x200B; [https://www.reddit.com/r/playrust/](https://www.reddit.com/r/playrust/)
I apologize. Thank you.
No problem.
 &gt;I'll say though that I dislike that `into()` can't accept type paramatres due to how it works an it has to go by inference. Usually this isn't a problem because you can just invert it and use `From` instead: MyType::from(foo) Instead of: foo.into() // where MyType is expected
I've just rebuilt everything using clang (7.0.1-8) to see if there's much difference compared to GCC. Positive numbers here indicate clang is faster: - bulk sending: -0.02% - bulk receiving: +1.2% - full handshake: -0.4% - resumption: -1.2% So there's not much in it. (Note, though, that rustc 1.35 uses llvm 8, not 7.)
Your median is not right, you'll always get the one after the real median. You have to add a `-1` for even length and remove the `+1` for odd length. The rest is just nitpicks, I'd go with [`sort_unstable`](https://doc.rust-lang.org/std/primitive.slice.html#method.sort_unstable) instead of `sort`, `HashMap::with_capacity(v.len())` to avoid reallocation and I'd do `*count &gt; max_count`.
&gt; I fail to see "paid software" = "high quality code". I do as well, because no one said that. Perhaps you should read the context once more =b
I'm not 100% sure since I can't test it but try to clone outside the closure. Since you have a `move`, `queue` will have to be moved inside the closure to make the clone, defeating the purpose.
Thanks for your review! I'm just confused as to why my median calculation would be wrong since the calculation is based on what's given in [Wikipedia](https://en.wikipedia.org/wiki/Median#Easy_explanation_of_the_sample_median).
Excellent
Thanks for the answer. I tried to create the clone outside the inner closure, and outside the outer closure. In both cases, I get expected a closure that implements the `Fn` trait, but this closure only implements `FnOnce` Which, to my understanding, means that `to()` wants to have a function it can call repeatedly, which makes sense given the context, but if I move the arc copy inside the closure, it can be called just once? You are right about the `&amp;mut queue_copy`, I changed it to `queue_copy`.
Th√£≈Üƒ∑ √Ω√∂√º
Yes this was my thought too. Artificially freezing software to old, possibly broken versions doesn‚Äôt seem like the best approach these days.
No amount of C monkeys can grasp the size of the openssl codebase, so the details escape every last one of them. Rust version maintains a coherent view through the compiler.
OpenSSL being battle-hardened did not prevent Heartbleed.
Has Rustls been audited for attacks such as https://mitls.org/pages/attacks/SMACK yet?
People do use rust-analyzer with vim, but using some other lsp plugin: https://github.com/rust-analyzer/rust-analyzer/tree/master/docs/user#vim-and-neovim I haven't tried rust-analyzer with vim myself though.
Hover already shows type information: https://user-images.githubusercontent.com/1711539/60578228-86410800-9d89-11e9-83b5-aed7a8c9bce5.png Does it work for simple cases for you, like this one? ``` fn foo() { let x = 92usize; } ```
Can you try this: HttpServer::new(move || { let copy = queue_copy.clone(); App::new() .service(web::resource("/ws/").route(web::get().to( move |r, stream| ws_index(r, stream, copy.clone()) ))) })
I personally mainly use VS Code, but I am not married to it. I really like the editor experience of IntelliJ, but it's impossible to use *just* the editor. I've recently did a bit of Kotlin in IntelliJ again, and that reminded me how many more things we need to do until Rust has adequate IDE tooling :-) I also open Emacs for quick things from time to time, because `emacs --daemon` is the fastest editor to start :)
Shouldn't this env variable be blacklisted from build.rs scripts? The env variable only exists to help bootstrap rustc, and from what I understand, their usage in the `simd` crates is a violation of the stability guarantees rust is supposed to provide.
YÃµÕåÃîÕùÃûÃ†Ã¢ÃôÃ¶oÃµÃçÃäÃÇÃòÕâÃ∞Ã°Ã®Ã∞Ã†ÃôÃπÕáÕúÕúÃπÃòÃπuÃ∂ÕõÕÅÕÑÃìÕêÕõÃ£ÃßÃ´ÃóÃüÃªÃ¶Ã±Ã†Ã§Ã≤'Ã∑ÕÜÕÅÃÑÃçÃìÃïÃÑÃãÕÜÃäÃÅÕêÕùÃéÃüÃ∫Ã≤ÃóÃ©Ã™ÕáÕîÃ•Ã®ÕúÕôÃúrÃ∂ÃÑÕÇÃÇÃØeÃ∑ÕùÃàÃãÃçÕäÃàÃøÃïÕãÕõÃ±ÕöÕçÕñÃ´Ã´ Ã∑ÃêÃíÃãÃÉÕíÕÄÃìÃÑÕóÃçÕÜÕäÃøÕôÃûÕàÃ§ÕïÕçÃØÃóÃ¢ÕôÃ¨wÃ¥ÃîÃõÃæÃπÃôÕñÕïÃ®Ã§Ã£Ã≥ÕçÃòÃùÃªÃ≥Ã¨Ã¶eÃ∂ÕùÕùÃàÕäÃæÕäÕùÃîÕãÕõÃäÕåÕÇÕëÃÉÃ¶Ã†ÃºÕúÃ∞ÃûÃ∞ÕâlÃ∏ÕÑÃõÃæÕÑÃãÃãÕÅÃäÃéÃãÃïÕòÃ®ÃôÃ∫Ã±ÕñÃ°Ã¶ÃûÕôÃÆcÃ¥ÃçÃïÃçÃèÕäÃçÕÑÃ•ÕàÃ¢Ã¨ÃóÃØÕúÃØÃñÕïÃùÃªoÃ∑ÕùÕçÃ±ÕöÃüÃ•mÃ∂ÕåÕåÃéÃÉÕÉÃΩÃàÃéÃøÕëÕÑÃåÃôÃôÃüÕîÕïÃ∫ÕâÕïÃ¶eÃ∂ÃãÕùÃ§Ã∞ÕïÃ®ÃôÃÆÃ§ÃùÃ∞Ã≥ÃπÃ±ÕéÕà
Check mate
The most CPU using paths in OpenSSL (crypto), use quite a bit of assembly, so not that much room for optimizers to make a difference.
It's not just rust. Whatever the programming language , I think people expect something about linear algebra... And then as many people stated, this will be very very very difficult to search. Even "log vector" will not lead to your project's page.
Wow, this is the first version that actually works! Thanks a lot! But I would really like to understand why this extra copy is needed. Why can't I use `queue.clone()` in the inner closure? After all, just like the `copy` variable you create, `queue` should be available inside the outer closure, since it is moved there...
Then you have to wrap it around the entire thing again. The problem is that Rust lacks universal function call syntax but often `expression.function()` is far more readable than `function(expression)` when expression is very long. Clearly the developers agree with this with the async/await syntax chosen. So using "methods" over "functions" often just becomes a hack to get the postfix rather than prefix syntax which is often more readable. There's a thing to be said about reverse polish nation which often enhances readability.
I recommend implementing `Iterator` for this and then simply using `for`. As others here have said, it‚Äôs possible using `while let`, but that doesn‚Äôt mean that it‚Äôs a good idea.
Im struggling (i think) with nesting closures. I something like this ([playgound link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=eb9c2a16d4585ae78c1eeffa4b352f81)): use std::time::Duration; use tokio; use futures::sync::mpsc; use futures::{Stream, Future}; fn main() { let (_, rx) = mpsc::channel::&lt;bool&gt;(100); // rate limiter let timer = tokio::timer::Interval::new_interval(Duration::from_millis(100)); // every time we recieve a value... rx.and_then(move |data| { // make sure we are not exceeding rate limit timer.for_each(|_| { // use `data` in here Ok(()) }) .map_err(|_| ()) }); } I get the error error[E0507]: cannot move out of captured variable in an `FnMut` closure --&gt; src/main.rs:17:3 | 10 | let timer = tokio::timer::Interval::new_interval(Duration::from_millis(100)); | ----- captured outer variable ... 17 | timer.for_each(|_| { | ^^^^^ cannot move out of captured variable in an `FnMut` closure The code is futures / tokio but im pretty sure that is not what the problem is. Any ideas how to fix this ?
I think it's because `HttpServer::new` and `Route::to` both ask for a `Fn (+ Clone)`, so they both have to be repeatable or at least clonable. If you only put `queue.clone()` in `to`, one of them can't repeat. Making two clones solve the issue. I suggest you ask on the forum when it's up again, you'll have a way better answer than mine.
I definitely would make a PR when I got it working. Right now, I'm fiddling with `syn` to extract the url from app that uses actix_web.
I think I got it. Thanks a lot and have a good day!
Oh cool!
Iterator would definitely be the winner for me. It shortens the code, instead of writing for loops, iterator does it one line. Learning elm before rust force me to understand and use iterators since there is no loops in elm.
You too!
I'd add `Send`, `Sync` and `Sized`: understanding those is required for feeling the language at the deeper level.
So this guideline is problematic for Rust. Some crates do not only have cargo features for unstable toolchains, but also have cargo features that are only intended for use in CI. That is, these features are actually only intended to only work in the particular os / available packages / compilation profile / ... combination used by the library's own CI. IMO, if an unused feature causes any kind of failure, that's a Debian process bug.
I'm not familiar with the futures/tokio API, but I believe the problem here is that the closure passed to `and_then` is potentially called multiple times, but the `for_each` method takes `self` and consumes the timer once run. Perhaps the right thing to do here is to call `poll`, as it takes `&amp;mut self` instead.
`Read` and `Write` and of course the `Iterator`
See https://github.com/rust-lang/cargo/pull/6608
So are there plans to do this in breakpoints? What about removing `From&lt;T&gt;` for all the other trivial one-member containers that implement this? Is it considered objectionable that Box, RefCel, Rc, Arc, Cell and what-not all implement From::from as an alias for new?
Feel the SQL flow through you..
Have you looked at Diesel?
Hopefully not. iso charges for copies of their standards. I'd prefer a different standardization body that doesn't charge money. Drafts are cool but not the real thing. I always hated this about c++.
Thank you, i found that the only option ,besides making every function request its own parameters, is lazy\_static. Although i don't know how to use u/0332353584 's solution. I don't know how to access the struct from outside the scope.
This is great! I've been using python for this mostly, but I may port a bot I wrote to rust in my free time and check it out.
Thanks for doing this!
What's a Greenfield project?
Create timer inside and_then?
&gt;Borrow checker have you in a headlock? Why yes it does. Here goes: 8.3 Exercise 3 want an interactive tool that stores people by department. Example command: "Add Amir to Sales". Simplified setup: // HashMap to hold the Deptpartment and Empoloyees let mut employee_database: Hashmap&lt;&amp;str, Vec&lt;&amp;str&gt;&gt; = HashMap::new(); let mut buffer_input = String::new(); let mut input_words: Vec&lt;&amp;str&gt; = [...] loop { buffer_input = "".to_string(); // PROBLEM: When the loop comes back around, this is still borrowed by the hashmap. // get input, place in buffer_input stdin().read_line(&amp;mut buffer_input).expect("Failed to read input."); // split input into indexable words input_words = buffer_input.split_whitespace().collect(); // [...] // ensure the first word is add, etc // if department does not exist, create entry, and insert a new vector employee_database.entry(input_words[3]).or_insert(Vec::new()); // PROBLEM Borrowing from input_words, borrowed from buffer_input, which will be cleared. // employee pushed to a hashmap vec using the department as a key. employee_database.get_mut(input_words[3]).unwrap().push(input_words[1]); // PROBLEM Same here. //[...] // rest of the program including breaks, etc, irrelevant to the problem. } I've considered making a vector that just holds all the input so nothing ever goes out of scope/or cleared, and a dozen other ugly solutions that I can't remember right now. **Question is this, Is there a way to stick the second and fourth word directly from a single stdin().read\_line(\_) into the hashmap, or maybe some other magic bullet?** All other problem have been dealt with, and this is the only major obstacle in my path, if I stick a "break;" just before the end of the loop, everything works. Don't know if this compiles, just wrote the simple because my actual env is a horrid mess that I was going to clean up after I got the basic ideas down. After reading documentation, in depth lifetimes, and fighting this for the past twelve hours, I am mentally drained and exhausted. I'm just sticking this out there before sleep, so I can come back later and see if someone left a helpful hint/solution for me. Thanks for reading.
While this post will surely get removed, a greenfield project is one that's brand new from the system design through implementation, as opposed to going to work on some infrastructure that's been around for years just adding functionality and fixing bugs.
Everything's new, no legacy cruft to support!
I thought of that too. The problem is that the timer yields the first value immediately regardless of the duration. I believe redeclaring the timer inside would reset this timer to zero iteration of `and_then`, which in turn would not rate limit correctly.
Thanks. I learned something today.
Thank you.
Specialization currently only works when one implementation is more specific than the other. It doesn't allow implementing both `From&lt;T&gt;` and `From&lt;E&gt;` for `Result&lt;T, E&gt;` because either one isn't more specific than the other.
I‚Äôm not aware of any plans in any direction, I‚Äôm purely commenting on the ability to do so directly.
() isn't a type that can never exist. You can reasonably use it, and it's not UB for it to exist. Don't you mean `!`, to indicate that the result can never fail?
That pull request is locked (as closed), so I'll share my thoughts here. To me, it looks like what the rust ecosystem needs is another environment variable. Current: \* global variable RUSTC\_BOOTSTRAP=1 -- allows nightly features on stable compiler \* [build.rs](https://build.rs) has variable RUSTC\_BOOTSTRAP=1 set -- allows nightly features in a crate, when otherwise built on the stable compiler My idea: \* leave the global variable intact \* create a new global variable RUSTC\_BOOTSTRAP\_PER\_CRATE = 1-- allows [build.rs](https://build.rs) to use RUSTC\_BOOTSTRAP=1 Use of [build.rs](https://build.rs) to set RUSTC\_BOOTSTRAP=1 should be an opt-in. My opinion is that having a build error of something like 'error: crate simd uses nightly features, set environment variable RUSTC\_BOOTSTRAP\_PER\_CRATE = 1 to allow'. &amp;#x200B; Thoughts?
The idea of charging money for (a digital copy of) a standards document seems very strange to me. The entire point of the standard is that people are able to refer to it as a source of truth when there are differences in understanding. If you put that behind a paywall, how can people benefit from it?
I _think_ you are looking for vec.remove(). [Documentation is here](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.remove). You could perhaps have let last_word = input_words.remove(3); // takes ownership from buffer let second_word = input_words.remove(1); // same here let last_word_ref = &amp;last_word; employee_database.entry(last_word).or_insert(Vec::new()); employee_database.get_mut(last_word_ref).unwrap().push(second_word);
The meta of this system is that normal people can always read the draft, which is exactly like the standard but free, and mega corporations that implement their own compiler can afford to buy the full spec document for legal reasons of being "full spec compliant".
Oh I found that [tokio already has `StreamExt::throttle`](https://docs.rs/tokio/0.1.21/tokio/prelude/trait.StreamExt.html#method.throttle)
I did develop one during my time with Storiqa. https://github.com/StoriqaTeam/libstqbackend/blob/master/db/src/statement.rs#L453 Bear in mind that it was never formally open sourced before the company went under, so use at your own (unlikely) legal risk.
No problem! Never stop learning.
Thank you so much, the error message changed, and now I will put it off until tomorrow. Cheers!
What does that mean? Are you still yanking all versions except the last one?
FWIW, I tried rust-analyzer on three different vim plugins a few months ago and couldn't get goto-definition working, but an identical setup worked for RLS. I wish I could provide a good bug report for you, but I just had no good ideas on how to start debugging it without learning how the LSP protocol itself worked. My lack of experience developing sophisticated Vim plugins doesn't help either. I'm using ALE right now with RLS in Vim. It's a bit slow for my taste, but goto-definition works better than my previous hacky ctags setup.
It is just a demo to showcase the benefits. I added a note, thanks for highlighting.
That should do the trick. Thanks!
I was looking for a pure Rust alternative to SDL, went with ggez even though it's not for a game. Might try this as an alternative.
Interesting concept, I'm gonna try to get it to work with my powershell scripts
this also depends on SDL btw
Dammit. Guess I'll never escape it.
IMO this PR should me merged right away. It doesn't mean any disruption to Firefox: a) because there is now a way with [-Z allow-features](https://github.com/rust-lang/rust/pull/59169) ([second pr](https://github.com/rust-lang/rust/pull/60289)) to specify which features to allow at the top level how hsvionen wants and b) if you want to be even more fine grained you could create a rustc wrapper that sets `RUSTC_BOOTSTRAP` like how rustc does itself, though .cargo/config. In fact, cargo could clear any `RUSTC_BOOTSTRAP` env vars before invoking rustc (not passing them on when you e.g. did RUSTC_BOOTSTRAP=1 cargo build), forcing people to use that method.
This is a very important milestone for bringing rustc up to feature parity with Clang. IIRC, Firefox leverages PGO heavily for squeezing out every last bit of performance when producing release binaries.
It's possible for a piece of software to have some advantages without having every advantage.
Finally! I was thinking to write one when I found out there were none. Do you take contributions? I might want to contribute sometime soon when I actually have some time to do so.
Is there a Rust PGO issue in Mozilla's bugzilla that I can track?
I feel similar üò¨
&gt; reverse polish nation I smell autocorrect
https://bugzilla.mozilla.org/show_bug.cgi?id=1437452
It doesn't *need* to depend on SDL. It would be possible to implement this using Glutin (which uses Winit instead of SDL) and OpenGL calls. That's what I'm doing for a 3D game I'm working on.
I'm not sure about most other industries, but the purpose of these standards in automotive is to ensure companies like mine have a more difficult time competing with companies like Audi.
rustls uses ring which bases itself on BoringSSL which itself is an OpenSSL fork. I doubt that there is a large difference in the assembly part of ring to the OpenSSL assembly. It should rather be in the non-assembly parts.
I recommend you check out diesel. Queries can be boxed using .into_boxed(). That allows you to conditionally change them at runtime.
yes, i understand the reasoning, but this specific thing where methods should see which fields of class they are mutating seems reasonable. it wouldn't influence the interface, since it would only be present within class implementation. calling mutating class method from outside the class would still borrow the whole class.
We don‚Äôt want to encourage this even more; ideally we would ban it entirely.
Will don't corporations just use the draft then?
I ***highly*** recommend you use a library like [laminar](https://github.com/amethyst/laminar)
A second problem is probably the `&amp;str` in the HashMap. You borrow data from somewhere, from your input buffer. However you later overwrite that with new data. You will need to either clone the parsed data so no borrows exist anymore or permanently store your input (before building the map, the borrow checker is not strong enough to know that reallocations of a container don't affect borrows of strings). The first approach gets you `HashMap&lt;String,Vec&lt;String&gt;&gt;`. The second gets you `buffer_input` with either type `String` or `Vec&lt;String&gt;`. Caveat: as long as you have borrows you cannot return the HashMap. EDIT: A third way is using `Rc/Arc` for the original line or some global string interner.
Good. I think it's time to RIIR my bot to Rust now, just to check if I get any benefits from it.
I've used it because rustc was unable to hoist the bounds checking out of a hotspot loop. No amount of asserts was able to convince rust that a decrementing loop stayed within bounds.
Still extremely stupid.
Starts at 40:25
Talk starts around 40:23 FYI
I'm working through Pingcap's key-value store onboarding project, \[Practical Networked Applications in Rust\]([https://github.com/pingcap/talent-plan/tree/master/rust](https://github.com/pingcap/talent-plan/tree/master/rust)). Incidentally, this was almost exactly the same project I was working on before they released the materials, but I kept trying to tackle too many pieces of it at once, and getting myself lost. The step-by-step process, while loosely specified, is very helpful, as is the selection of supplemental readings.
Interesting idea. Wouldn't SVG be a better choice in many situations? It looks like `librsvg` doesn't currently support SMIL SVG animations, but that could conceivably be fixed. In any case, a bit surprised by the backend being SDL instead of `librsvg` or Cairo. The graphics language here looks a lot like Cairo calls, I think.
Any performance hit when using diesel? I've got an async server using futures. What's the best way to have diesel execute normal queries?
Is there any Rust binding for the GNOME Online Account API ([https://developer.gnome.org/goa/stable/](https://developer.gnome.org/goa/stable/))?
Similar, indeed, but context managers are a little more powerful, in that the programmer has more control over when destruction happens, and a little less powerful, in that custom deferred code is a little harder to set up.
&gt;The problem is that Rust lacks universal function call syntax &amp;#x200B; It does. There are couple of very minor edge cases, but you can mostly be very precise. In this case, you could use \`&lt;Foo as Into&lt;MyType&gt;&gt;::into(foo)\` instead of \`foo.into()\`. I'd argue that \`MyType::from(foo)\` is clearer though...
good point. He could simply use `std::env` to make that clear.
consider MiniFB?
Speaking from experience, this approach will work great until one needs complex rendering or high performance, and then that will be impossible to achieve. SDL2's drawing support isn't really useful for more than prototyping and debugging. That said, this looks great for those tasks, and the idea of reading text input is a powerful one, as displayed by PostScript. Good luck!
its really great, so not so bad.
That's not what UFCS is; the term was coined in D; it means that "functions-calls" and "method calls" are one and the same and that any function can be called as method and in reverse. So if `From::from(foo)` exists then `foo.From::from()` would also exist or something like that. In Rust it only works in one way; methods can always be called as functions but not in reverse so you don't automatically get postfix notation when it's more readable.
What about coverage information? Is any flag to get it?
Szukasz innego subreddita: /r/playrust Chyba ≈ºe chcecie programowaƒá w parach‚Ä¶
It's a liability thing; they may look at the draft at some point, but they have to be able to demonstrate that they have access to the specification in order to demonstrate that they follow the specification.
Last I used it it seemed to have issues dealing with subqueries and certain joins.
I hope not. C++'s progress has been stifled by the endless language lawyering, proposals and committees and board members who's sole responsibility seemingly is to prevent new features from breaking their broken by design compiler resulting in a language so complex and riddled with gotchas no one understands it.
That's why they are a different type. What I mean is that in theory the Rust type checker could detect that a type is empty which infallible is and make it a subtype of every type including `()`; this is completely sound and there is no reason why it can't be done but it simply isn't done right now. There are numerous other things that could be done like branches in matches that contain empty types can be omitted and still be irrefutable. A lot of `Result` APIs are in some special infallible in their error like in this case. In theory the type checker could just allow this: let Ok(x) = 2i32.try_into(); This pattern despite not covering `Err` is irrefutable because the type is `Result&lt;i32, Infallible&gt;` and `Infallible` is an empty type; the type has no members so the `Err` branch can never be reached. Currently one would just use `unwrap` of course because one _knows_ that it never fails but with the compiler properlty detecting and reasoning about empty type you could just use that and have the type system prove for you that it always work.
Does it still have issues with subqueries? That was a problem the last time I tried to use it.
Ok. This is the terminology used in at least some versions of [the book](https://doc.rust-lang.org/1.35.0/book/ufcs.html).
Thanks. It might give me some ideas even if I don't use it as is.
Yes as it says that's an old version; if you go to the new one it dropped that nomenclature. Rust indeed used to call what it does "universal function call syntax" but after considerable complaints that it was not "universal function call syntax" they agreed and dropped the term. https://en.wikipedia.org/wiki/Uniform_Function_Call_Syntax#Rust_usage_of_the_term The currently used term is "fully quantified syntax".
Why did you chose rust over e.g. c++?
Because I like rust more
I wonder if there's an easier way of handling those variants in the match arms.
Doing this with the Linux kernel is definitely not viable - it moves _way_ too fast even for a single big team to keep up, and when rewriting in Rust, you don't just need to keep up - you need to go _faster_.
I'd say it depends; if it's just a one-off loop, I'd probably do it using `while let`, but if it's used more often, or I anticipate the need, implementing `Iterator` is the way to go.
Eesh. Thirty five minutes of dead air before it starts. Talking starts at [about 36 minutes](https://youtu.be/F1AquroPfcI?t=2156) but is mostly housekeeping for the gathering. About [40 minutes in](https://youtu.be/F1AquroPfcI?t=2408) is when the real content starts.
Wait - SDL2 isn't good for drawing things fast? That honestly surprises me, I've used it a lot (and never ran into too much trouble). What are good alternatives, if I might ask?
I'd say one example with the ENV variable and another one with just passing the actual token string should make it really clear.
Implementing `Iterator` isn‚Äôt that hard. All of the basic pieces are already there in OP's code, the only real things missing are the actual `impl Iterator for` and the type declaration.
Wops, deleted. You said the opposite xD
?? Im looking for clan to play with.
I think the answer there is to use a futures CPUPool. You move your query to another thread, giving the callee a future for the query result. The async reactor thread to keep doing work on sockets, etc, and when the diesel query is done it wakes up the task in the reactor.
This subreddit is about the Rust Programming Language, not the game. Next time please read the sidebar before posting.
Jeste≈õ na z≈Çym subreddicie. Ten dotyczy jƒôzyka programowania Rust. Nie znajdziesz na nim klanu do grania.
One that people get confused about because it doesn't really show up in the docs is [`ToString`](https://doc.rust-lang.org/std/string/trait.ToString.html). Because of the blanket `impl ToString for T where T: Display`, you actually never really need to implement `ToString` yourself. Implement [`Display`](https://doc.rust-lang.org/std/fmt/trait.Display.html) for a type and you'll automatically be able to call `to_string` on it. Similarly, if you're wondering if you can convert something to a string, just look for an `impl Display` in the docs.
Create a generic function that takes T: Send + Sync and then pass an instance of your trait to it
Not sure honestly.
The simplest solution with the current architecture would be to let `Cpu::step` take the other components as a tuple (see example below). This is essentially the same as making the struct `AddressBus`, but you don't have to define it etc. ``` impl Cpu { fn step(&amp;mut self, (video, serial): (&amp;mut Video, &amp;mut serial)) -&gt; { /* ... */ } } impl Console { fn step(&amp;mut self) { self.cpu.step((self.video, self.serial)); } } ``` Another solution would be to rather just implement the functionality for stepping the CPU within `Console::step`. I would assume that you are only going to call `Cpu::step` from `Console::step`, so it might not be a necessary separation. Especially since you already have to find some way to interconnect alle the fields of `Console`, it sounds like the functionality should rather exist directly inside the `Console::step` function.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/u_suhyamugsasmut] [tbot, a crate for writing Telegram bots](https://www.reddit.com/r/u_suhyamugsasmut/comments/c8pwix/tbot_a_crate_for_writing_telegram_bots/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Glad I'm not the only one who recognizes that C++ is an inconsistent mess. Sometimes it feels like anyone and everyone will defend it to their death.
Sometimes, I miss ASCII.
[Tarpaulin](https://github.com/xd009642/tarpaulin) is Rust-specific and getting better quite quickly.
I get the impression icefoxen is talking about the Render API, which isn't meant to be the be-all and end-all of graphics but, rather, a modernization/porting shim for games that used APIs like DirectDraw and an easy way to develop a certain class of retro games. (To paraphrase Ryan Gordon at Steam Dev Days, it's the most hyped up Super Nintendo you can imagine.) For needs beyond that, you do the same thing you did back in SDL 1.2... use SDL as a portable way to initialize an OpenGL window and then speak OpenGL.
A lot of man-hours have been put into making SDL an abstraction over a *lot* of hardware and platform quirks.
All `Arc` gives you is cheap clones. It doesn't make your data able to be sent across threads. So the intent of a `Clone` implementation is to always clone your object. For `Arc` or `Rc` types, creating a new smart pointer that reuses the same data makes sense and doesn't have a different intention. From a performance perspective, it can indeed be confusing just calling `something.clone()`: "am I doing something expensive here or just calling some atomics?". Usually the recommendation I've seen is to always call `Arc::clone()` like this: `Arc::clone(x)` and to then call "normal" clones like `x.clone()`.
I'm pretty sure Stdin implements Read which should allow you to read a single byte/char
yoo mb :/ Can u link me good one?
I linked the game‚Äôs subreddit in my first comment: /r/playrust
&gt; and one can be heavy (cloning data), and the other is often very cheap They are all cloning data, but it means something different for each implementation. That's true of any trait though. Each trait that is implemented for a type will do something slightly different. `std::ops::Add` do exactly the same thing for `f32` as it does for `usize`. The purpose of `Arc::clone` is to increase the inner ref count and create a new pointer. To me, that sounds like what a `clone` should be for a reference counted type.
I sometime like to write my \`Rc\` clones \`Rc::clone(&amp;my\_rc)\` instead of \`my\_rc.clone()\` to make explicit that the cloning is cheap and, more important, to make sure we still cloning an \`Rc\` after a refactor.
Obviously the rayon version is faster-- it's not even a fair comparison. If you want to compare iterators vs for-loop you can't use par_iter or else you're just measuring single vs multithreaded performance.
But it does mean they Heartbleed has already been fixed.
Quicksilver doesn‚Äôt depend on sdl! It‚Äôs not the greatest api because of its ability to build to wasm but it does the trick if you don‚Äôt want that dep.
/dev/null like on your machine!? Should at least use a https://devnull-as-a-service.com/ to ensure your program is really web scale
Is there a reason that the CPU doesn't own everything? I mean it's not the way it works in the hardware, sure, but the CPU is the driving unit of everything else.
I mean I do respect the second wonder of the world of graphics APIs. SDL is great in a field that takes much effort to be good. But I'd still like a memory safe alternative.
&gt; The purpose of Arc::clone is to increase the inner ref count and create a new pointer. To me, that sounds like what a clone should be for a reference counted type. Agreed entirely. However when you have my example _(`Foo(Arc&lt;Bar&gt;)`)_, and when you run into a line of code that shows `Foo::clone`, you don't have any information at hand to understand what the goal here is. Is it copying big objects like maps or vecs? Or merely a small reference count? This feels akin to Copy vs Clone. I _could_ put Copy on big objects. Heck, I could make `Foo(BigValue)` `Copy`, but I wouldn't. There is an implication that `Copy` is cheap. The trait is informative in a way that `Clone` does not seem to be. `Clone` is often expensive, like cloning a `Vec`, but it is also often crazy cheap, like cloning a `Rc`. To me, those two concepts feel meaningfully different. Perhaps even enough to be distinct.
There was a mention some time ago that LibreSSL was not encrypting at-rest keys in memory to mitigate memory leakage attacks (such as the Spectre and related variants). It has some costs, as it requires decrypting the key for every new session established, however I can definitely see interest in this kind of hardening.
The sound when people are talking is just a bunch of chirping on my Android phone in the YouTube app. Does anyone else experience this problem? All other videos work just fine.
&gt; From a performance perspective, it can indeed be confusing just calling something.clone(): "am I doing something expensive here or just calling some atomics?". Usually the recommendation I've seen is to always call Arc::clone() like this: Arc::clone(x) and to then call "normal" clones like x.clone(). Which is problematic if you have a type owning the `Arcs`, as in my example of `Foo(Arc&lt;Bar&gt;)`, since I can't call `Arc::clone(foo)`. It's a good tip though, appreciated!
Some sites are never done loading and may load further content as you scroll
This is actually the behavior that `clippy` encourages.
I personally think that the C++ committee is setup in a vicious circle: 1. Proposals should be of high quality. 2. This demand for high quality requires that the writers of a proposal spend an inordinate amount of time and effort; it requires ongoing motivation and dedication for years, as well as the means to travel physically to the location where meetings are held for each "presentation" of a revision. 3. As a consequence, many proposals are trimmed down to the bare essentials, in an attempt to lessen the pain by reducing the number necessary revisions. 4. Which leads to a plethora of tiny features with ill-defined interactions. The problem has been recognized, and a Governance WG has been setup to try and provide a "vision" for each standard version which defines the "priorities" of the next version and asks the other WG to concentrate on proposals aligned with the vision and priorities, and work on the other proposals as time allows... but I am not sure if it is sufficient to reverse the trend. Each version of C++ is even more "frankensteiny" than the previous one, with more bolted on parts, more oil leaks, ...
ISO also charges for membership; so that participation as a committee member requires paying a subscription fee. So it's paywalls in and paywalls out.
Interesting package. I'm curious if other people feel it's odd to have packages with the std prefix that aren't really a "standard" or part of the standard lib?
I was originally confused by this too, but came to the conclusion that the author of whatever I‚Äôm cloning was in a better situation to determine what the appropriate behaviour should be. As far as I‚Äôm concerned when using clone (which I try to avoid in general), the implementation will likely always be the cheapest option available. I don‚Äôt have to personally care about it unless it becomes an evident bottleneck, for example.
I guess there isn‚Äôt a reason, although at that point the `Console` might as well be the CPU as suggested in the other comment.
&gt; Tarpaulin is Rust-specific and getting better quite quickly I know there is a lot of third party tools that try to do this job. But they are use "strange" enough methods: debugger emulation, CPU emulation and similar things. As I know tarpaulin is debugger emulator, it uses ptrace to get line coverage information. I call it strange, because of it is too slow. So I am waiting the ordinary implementation, when compiler instrument code, and execution of each line cause counter incrementation. llvm + clang has this mechanism, and PGO implementation that I suppose depend on llvm support should have gathering information mechanism similar to what needed for line coverage one.
Are there any glaring memory safety bugs in SDL or is it just the fact it wasn't written in an explicitly safe language that is the issue? Because to draw, you have to use unsafe code.
It probably isn‚Äôt clear from my example but the `AddressBus` has methods which abstract away the individual components, e.g. `read_byte(&amp;self, addr: u16)`, that‚Äôs part of why I didn‚Äôt go for a tulle, although use a newtype would solve that. I could move everything up to the console level but then I‚Äôd have to move the CPU‚Äôs internal state up with it, e.g registers. Not necessarily a problem, just means there is going to be a lot in there.
This is kind of out of scope, but once you've gone through the trouble of `unstable_sort`ing your `Vec` (or `sort` if you like kicking puppies), you really don't need the `HashMap` which has high overhead costs. Because equal elements occur in runs, you only need to keep track of the identity and count of (a) the mode-so-far and (b) the the current element.
Looks interesting! I cloned it and tried running it. First, there were lots of cargo warnings, mostly unnecessary mutability. Then, running the commands in the readme caused a panic "expected 2 arguments for window command" - the `window` command was missing. Fixing that, the rectangle didn't show up because it's now `fill rect`, not `rect` - though no error this time, just silent failure. The Go and JavaScript examples seem to be similarly out of date. Once I got past those hurdles it worked alright, but it would be great if the readme and examples were kept up to date, because figuring out the bugs in the example isn't a great first impression. As for the program itself, I can see myself using it sometimes, especially if there are more advanced examples to work from.
I think that Rust's `clone`, in general, is a lot easier to analyze than, say, C++‚Äôs copy constructors and their default behavior, which is a shallow clone. Clones are almost always deep copies in the Rust world. The obvious exception is with `Arc`/`Rc`, but...that's the entire reason that those data structures exist in the first place. I think of them semantically as being references with a runtime-defined lifetime.
Awesome!
I've dealt with many leaks and double frees in code that uses SDL, and granted most of the time it's the calling code that's bugged which is solved by a safe wrapper but there's always the ever so elusive library bug or sneaky undefined behavior. It's rare for a lib so mature, but it does happen.
Same here
I agree this is unfortunate. We don't upload it - it's done by the IT, and I need to figure out how to control this, so that in the future we can make better videos.
Yeah, basically this. The Render API is rather flaky in my experience, I've had a game run 60 fps on Linux and 6 fps on Windows. What do you do then? Sit down and cry, 'cause that's the only option you have. Using OpenGL or such is far better, for certain values of "better".
If the purpose of the type is to own Arcs, I would still use the \`Type::clone(obj)\` syntax, as it still communicates the intent. If the type just incidentally holds arcs among other things, then it's more like a normal clone operation, and I'd stick with \`obj.clone()\`. &amp;#x200B; \`\`\` struct ArcHandler(Arc&lt;String&gt;, Arc&lt;String&gt;); let handler = ArcHandler(arc1, arc2); let new\_handler = ArcHandler::clone(&amp;handler); \`\`\`
&gt; Because I like rust more I'll second this sentiment. Other than a few very niche cases, I will always pick Rust over C++ when given the chance. Having an actual module system is a game changer, and Rust's generic types are far more useful than C++ templates when it comes to organizing and modeling business logic. C++ is ahead of Rust in a few areas, due to age, but it doesn't sound like the OP will need anything C++ specific, so the safety and productivity bonuses from Rust are a definite win.
üëã, happy to answer questions here.
I believe that `(Foo as Arc)::clone(x)` is valid if you want to keep the semantics clear, but I concede it isn't pretty.
When I said "a toy project" I really meant something very small, very simple/obvious, and self-contained. I realize there are several full-size projects (and parts of them) that can serve as illustrations to this subject - but I'd rather not dive into Rust Audio in general and Vorbis decoder in particular, merely to learn how to make a crate exposing C ABI. Also, how necessary is learning `cargo-c` to the ability to build a crate exposing C ABI? Also, I was rather surprised to not see the `Cargo.toml` attribute `crate-type = [ "cdylib" ]` or something like that mentioned.
You should read the presentation linked from the blog for all the details, TL;DR: dynamic linking is complex and the platform-specific logic currently is not present in cargo for a number of good reasons. Pull requests are welcome btw :)
librsvg, lodepng-rust and miniz_oxide spring to mind
An address bus seems like a awkard abstraction in this context because it needs to have a mutable pointer to the different components. This means that you either need to construct it and deconstruct it everytime you want to use it, or use some form of interior mutability (`Rc&lt;RefCell&lt;_&gt;&gt;`) for each internal component so that you don't need it to have exclusive mutable access. Another option could be that its the `AddressBus` which is the top level structure driving the machine (cause it is responsible for transferring information to/from the different components), however that means that the address bus is rather the actual _machine_ itself. In a emulator I would say that its the internal state of `Console` which implicitly acts as the address bus (in the sense that it is the top level common state of the system). Further as suggested by u/addmoreice, it would make sense that its rather called `Cpu` instead of `Console`. And also remember when creating systems; encapsulation is not always a benefit. If you find yourself trying to work around your own constraints because you want to encapsulate highly connected processes, it is probably a hint that there is a better way to make the abstraction in the first place.
The reason why I named it \`stdg\` was because the ideas is that instead of piping your program's output to \`stdout\` you pipe to \`stdg\`. &amp;#x200B; With that said, I'm open to a name change if people feel it is necessary.
I hope you reported that. Who knows how many games are using the Render API.
Sorry, I didn't mean to pick you out specifically. Your reasoning makes sense, I just wonder if it makes sense in the future to limit crates that use std as a prefix.
I just fixed the examples on the README as well as the examples in the `/playground` directory. They should all work ok now. I also resolved the warnings. &amp;#x200B; Sorry about that though. I will work on some more advanced examples when I get time.
The default mdbook theme is basically the same as what rustdoc generates. [https://github.com/rust-lang-nursery/mdBook](https://github.com/rust-lang-nursery/mdBook)
Stdg does handle user interaction, though. &amp;#x200B; It's not documented yet, but basically `stdg` prints out lines containing details about events that you can pipe back to your own program and handle by reading a line (just like you would read a line from the user) in a while loop. Since each piece of information is on a new line, there is no complicated parsing that needs to be done. &amp;#x200B; Unfortunately, "circular" piping does not seem easy to do so using this event handling feature may be a sub-par experience for a user of `stdg`.
Thank you! I found the links you mentioned very helpful. &amp;#x200B; Also, I reconsidered - installing `cargo-c` turned out simple/trivial enough - and the potential benefits of using that tool over doing things by hand are too good to ignore.
Yes. I will likely switch to a different dependency for doing the actual drawing of stuff to the screen as well as handling window events. &amp;#x200B; SDL just works though and I'm really only targeting situations where you want to draw very simple graphics or animations to the screen. It's not something you would use to re-write your GTK app with. It's more of something you would use to get a quick scientific visualization together or a small game prototype.
Nice work! Are the definitions for all the types and methods autogenerated or manually implemented? I started a similar project a year ago but I found it too tedious to implement all the API, and there is no nice machine readable description available (like there is for the regular non-bot API).
Happens on my machine too.
To be honest I'm not sure what my thoughts are around the subject but the fact that \`clippy\` and the community is working around calling the method directly is at least interesting.
This happens on my phone as well. Not just in the YouTube app, but also through the embedded YouTube player in both Chrome and Firefox Mobile.
https://palez.github.io/papers/paper2.pdf
I don't think the "many eyes" or "battle hardened codebase" concepts have proven to be meaningful at all. The concept that with more eyes bugs become shallow is pretty clearly incorrect at this point, I would argue. Linux kernel is a great example. Dedicated security research into a library is probably a somewhat better metric, but is it so much better than being able to say that you're using a language where certain problems simply don't exist? I don't really thinks o.
Writing a web-application using Rocket and Diesel. Rocket is fantastic - I‚Äôm super impressed. A little disappointed with diesel honestly. I recognize that this is a super hard space, they‚Äôre not funded and my expectations are sky-high (the rust ecosystem has spoiled me), but it doesn‚Äôt feel like 1.0 quality yet. I expected that working with diesel would feel close to working with serde, but there‚Äôs actually a lot of trait fenangling and custom impl that‚Äôs needed even for common-ish use-cases. I‚Äôm taking notes and hope to put together a gentle review of diesel (along with a call to funding diesel) that I hope the maintainers will find helpful.
Thanks! I totally understand how these things happen, I just thought it was worth mentioning because that was my experience. I'll be looking forward to the advanced examples.
I think it's a lot cleaner to use type annotation instead. `let thing_two: Arc&lt;_&gt; = thing_one.clone();`
Then again the existence of a standard doesn't mandate that. C# is an ECMA standard, but the language still evolves at a much more regular pace than C++.
Nice One
I don't expect something like that to be attempted for the current kernel, an LTS one or something older which isn't undergoing updates will be fine. It is more of an experiment to see what rust can bring to the kernel as it is. Its got to start somewhere. I don't think remacs are focused on the latest emacs. They are working from the version 26.1.9 thereabouts.
From memory if I recall correctly, in the early days of Starbound development watching one of the developers writing calls to SDL_Render prefixed functions. They might just use it for context creation/event handling now however. I agree the Render API itself is rather lack luster.
I'm looking at Diesel as an ORM but confused about what databases it works with. I can see MySQL and Postgres references in a couple places but I don't see any comprehensive list. Is it possible to use it with DynamoDB?
r/playrust
The experiment to see what Rust can bring to a kernel already exists - lots of kernels are written in Rust. An experiment to rewrite parts of Linux in Rust won't get you anywhere, because no one is going to use it, ever.
I don't have that behavior by default, should I enable an additional set of warnings?
I believe wheels are precompiled.
Have you gone through ensuring hardware accelleration is on, and that you're not using CPU, or the wrong driver (in SDL2) to render? Or mixing pixel format surfaces? Maybe you've gone past the ability of SDL2 -- but there were a lot of ways to unintentionally cause slowdown in SDL1.x
/r/playrust
Is there an easy way to look at the fields in a `serde_json::Value` and, even if they are all `Value::String`s, try to convert them to `Value::Number`s?
Yup! More things are being inlined.
You can't typecast like that.
Yes, that's normal.
that'a a strange way to spell m$-OS
In my emulator which I'm currently working on, I have similar situation. The CPU accesses the registers of the other components through memory read/write. Therefore I have modelled a memory component that holds/owns RAM, ROM and registers of the other components. The memory is passed as a mutable reference in the step function, like so: struct Memory { ram: [u8; RAM_SIZE], rom: [u8; ROM_SIZE], video_registers: VideoRegisters, serial_regsiters: SerialRegisters, } impl Memory { read_byte(&amp;self, address: u16) -&gt; u8 { // match over address range and read from the underlying memory or registers } write_byte(&amp;mut self, address: u16, value: u8) { // match over address range and write to the underlying memory or registers } } struct Console { memory: Memory, cpu: Cpu, video: Video, serial: Serial, ... } impl Console { fn step(&amp;mut self) { self.cpu.step(&amp;mut self.memory); self.video.step(&amp;mut self.memory); self.serial.step(&amp;mut self.memroy); ... } } impl Cpu { fn step(&amp;mut self, memory: &amp;mut Memory {} } impl Video { fn step(&amp;mut self, memory: &amp;mut Mewmory {} } The point is that the registers of the components are not owned by the components itself, but by the Memory struct. Not sure if this fits exactly to your needs, but hope that this could help finding what's right for you.
`clone` method clones. It doesn't say how expensive cloning is, it may be very cheap, like with `1.clone()` (although here you are better of making a copy) or `(1..2).clone()`, or very expensive. As long there is no inner mutability involved, `Rc`/`Arc` can be used to make cloning faster (especially useful with functional data structures), essentially ending up with clone on write types.
In my understanding, `.pyc` files are not version-compatible, so plain source `.py` files are packaged instead. From [pep 0472](https://www.python.org/dev/peps/pep-0427/#file-contents): &gt; 13. Wheel, being an installation format that is intended to work across multiple versions of Python, does not generally include .pyc files.
Looks like it's allow-by-default, part of the `restriction` lint group: https://rust-lang.github.io/rust-clippy/master/index.html#clone_on_ref_ptr
It's called decoupling technologies. By using html templates, you can technically switch from one template engine to an other without much headache, likewise you can change the backend without modifying the templates.
See openssl... Still, SDL's use cases seem a bit safer, overall, than a stream encryption library.
Did a test with a Windows web-view app I'm building: |opt-level|lto=false|lto=true| |:-:|-:|-:| |0|4.94 MB|4.28 MB| |1|2.34 MB|2.14 MB| |2|2.20 MB|2.05 MB| |3|2.32 MB|2.17 MB| |s|1.93 MB|1.68 MB| |z|1.85 MB|1.60 MB|
This is a really cute idea, and a great way to add simple rendering to any console app. You could also pipe text files into it so basically do 2d editing in any text editor. Nice one! \--- For those people now dying to get started making games in Rust, A great alternative for those just starting their games journey is [Godot](https://godotengine.org/) (the opensource engine) paired with the Rust bindings [https://github.com/GodotNativeTools/godot-rust](https://github.com/GodotNativeTools/godot-rust) Also see [Are we game yet?](http://arewegameyet.com/)
I believe Zig has a very flexible approach to allocators, it's probably worth a look as well.
https://github.com/michaelwoerister/rust/blob/stabilize-pgo/src/doc/rustc/src/profile-guided-optimization.md
You are probably looking for /r/playrust
https://github.com/jrmuizel/raqote could be used as the drawing backend. I wonder if it's possible to combine it with minifb? Then you'd have a pure-Rust pipeline.
Have you thought about adding other bot API support? It would be great, if the same code could work as a Telegram/slack/Messenger/etc bot with only a config change.
In Rust a default (derived) clone is also always a "shallow copy" because of the behaviour for Arc/Rc/raw pointers. A user defined clone might do anything - just like a C++ copy constructor.
If I understand correctly, you have a Value which is an object with field values that are strings, and you want to turn those string values into number values? ``` fn transform(value: Value) -&gt; Option&lt;()&gt; { for v in value.as_object_mut()?.values_mut() { if let Value::String(s) = v { let num = Number::from_f64(s.parse().ok()?)?; std::mem::replace(v, Value::Number(num)); } } Some(()) } ``` Is there something specific you're trying to do with this?
You can build safe code on top of unsafe code.
There's winit as well, but it doesn't have a renderer and support for something is lacking, ex: gampad
Raquote and Piet were two of the many Rust libraries I looked at for using as a drawing backend. I will also need some way of rendering to a window and handling events, though.
Hey! I've never thought in something like this. Good work!
its a pretty thin layer on top of some inherently unsafe stuff, so while I can understand the sort of OCD urge to have a memory safe alternative there may not be much pragmatic reason to worry about it.
That‚Äôs interesting, I never would have thought of separating the memory addresses/registers from the components. I might give this a go and see if it feels more ergonomic.
Rolling [C2Rust](https://github.com/immunant/c2rust) to a new nightly version (more work than one might think) such that we can leverage the [recently merged PR](https://github.com/rust-lang/rust/pull/59625) improving C variadics to transpile more (nearly all?) C programs into Rust :) (Transpiling everything is only step one and doesn't add any safety over the C version, step two is refactoring for safety and is very much a WIP.)
Does IntelliJ support LSP that rust-analyzer uses?
Yep! That looks right. I'll test it out in my program. Yes, I'm using the `csv` to deserialize CSV rows into JSON Values. The CSV values themselves are numbers with quotes around them, plus I am not able to derive `Deserialize` on a struct because I am not sure of the headers of the CSV beforehand. The `csv` crate says if you don't know the structure ahead of time you can deserialize them into a `HashMap&lt;String, String&gt;`, but then all the values will be Strings. I can't make them u64s since there are usually a few columns that are not numbers and are strings, so I've been deserializing them into a HashMap, turning that into a `Value::Object` then looping through the keys to parse anything I can as a number. My current implementation of that made my program way slower than it was without parsing them as u64s so I figured I was doing something wrong.
That seems to match my experience, except I'm using vim-lsp. goto-definition seems to start executing but then nothing happens, but it works with RLS and other language servers. I'm trying to see if I can get it to dump some debug logs.
Type and trait are compile time concepts. Your `Vertex` type implements both `VertexTemplate&lt;VertexBase&gt;` and `VertexTemplate&lt;VertexUV&gt;`. It doesn't care what instance value is. How would you expect this code to work if you will mix vertex with tex coord and without in one array? If may convert into either variant, specifying manually which one, but expect panic to happen if you will choose wrong vertex variant. And `VertexTemplate` trait doesn't need associated type in this case.
/r/playrust
There was an article by Niko a while back. It was about copy-on-write data structures versus normal ones. So for example you could have a normal Vec and a copy-on-write Vec. For normal Vec, clone is expensive, and write is cheap. For copy-on-write Vec, clone is cheap, and write is expensive. There's a certain symmetry to that. (Probably the article was about more complex data structure, I don't remember.) However I can see the point of not knowing when clone is expensive.
Interesting... So if I `dbg!` the `value` before the loop, it shows it is an Object with lots of String values inside. However, if I `dbg!` `v` right after the for loop, it only shows the first String field for each time the function is run.
I'm new to Rust! I am trying to use the std HashMap for a dependency graph. Each value has a list of keys for the values it depends on. I want to iterate through the Hashmap and then sum up the values of the direct dependencies of each value. Here's the code: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9395aa3d10685d347f1f6fa6efc05473](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9395aa3d10685d347f1f6fa6efc05473) use std::collections::HashMap; struct Commodity { direct_labor: f64, indirect_labor: f64, dependencies: Vec&lt;(u64,f64)&gt; } fn labor_calc_iteration(commodities: HashMap&lt;u64, Commodity&gt;) -&gt; HashMap&lt;u64, Commodity&gt; { let mut commodities_calced = HashMap::new(); for (curr_ID, c) in commodities { let mut indirect_labor: f64 = 0.0; for (ID, quantity) in c.dependencies.clone() { let dependency = commodities.get(&amp;ID); indirect_labor += match dependency { Some(d) =&gt; { quantity * (d.direct_labor + d.indirect_labor) }, None =&gt; 0.0, }; let calced_c = Commodity{ direct_labor: c.direct_labor, indirect_labor: indirect_labor, dependencies: c.dependencies.clone()}; commodities_calced.insert(curr_ID, calced_c); } } commodities_calced } I get this kind of error: error[E0382]: use of moved value: `commodities` --&gt; src/main.rs:21:30 | 18 | for (curr_ID, c) in commodities { | ----------- value moved here ... 21 | let dependency = commodities.get(&amp;ID); | ^^^^^^^^^^^ value used here after move | = note: move occurs because `commodities` has type `std::collections::HashMap&lt;u64, Commodity&gt;`, which does not implement the `Copy` trait As you can see, I've already tried making it work by not mutating the original hashmap at all, and instead creating a new hashmap with the updated values. Other than putting a write lock on each value, I guess this is the only safe way to do it. But apparently not safe enough for the BC! As well, it is more memory intensive. Please help me out! Also welcome to code critique.
r/playrust
The transform fails if there are any strings that aren't parseable to `f64`. The inner loop is better as: ``` if let Some(num) = v .as_str() .and_then(|s| s.parse().ok()) .and_then(Number::from_f64) { std::mem::replace(v, Value::Number(num)); } ```
No worries \^_^
Ah, that does make sense! Thank you so much!
To get the code as is to work, you just need to borrow the commodities in the loop: ``` for (curr_ID, c) in &amp;commodities ``` as well as deref the curr_ID when inserting entries: ``` commodities_calced.insert(*curr_ID, calced_c); ```
Does it make a difference with `codegen-units = 1` alongside lto?
r/playrust?
Thanks so much! That was surprisingly simple.
You should be able to follow the [Embedded Rust Book](https://rust-embedded.github.io/book/intro/index.html) but using the [sam3x8e](https://github.com/ammoniak/sam3x8e) peripheral access crate.
SDL isn't a graphics engine, nor it is meant to be. They do have a 2D hardware accelerated renderer, which is usually enough for games that just copy-paste a bunch of sprites, but that is about it.
You can make this a function that updates the hash map instead of cloning it, though it does require the creation of a temporary holding structure: ``` impl Commodity { fn indirect_labor(&amp;self, commodities: &amp;HashMap&lt;u64, Commodity&gt;) -&gt; f64 { self.dependencies .iter() .filter_map(|(id, quantity)| { let d = commodities.get(&amp;id)?; Some(quantity * (d.direct_labor + d.indirect_labor)) }) .sum() } } fn labor_calc_iteration(commodities: &amp;mut HashMap&lt;u64, Commodity&gt;) { let indirect_labor_values = commodities .iter() .map(|(id, c)| (*id, c.indirect_labor(commodities))) .collect::&lt;HashMap&lt;u64, f64&gt;&gt;(); for (id, indirect_labor) in indirect_labor_values { if let Some(c) = commodities.get_mut(&amp;id) { c.indirect_labor = indirect_labor; } } } ```