You're not going to play that game? If you're throwing around the hyperbole that I quoted above, you're already playing it. And yes, blindly trusting the Rust org (or anyone) is irresponsible engineering. If you were a responsible engineer, you would have evaluated the actual state of project, realized they rushed 1.0 out the door at the end, and adjusted expectations accordingly. 
As you mentioned, I've also been working on a way to do COM abstractions, mostly to support DirectX which exposes everything through COM interfaces and uses COM reference counting, but does not use most anything else from COM as it has its own factory methods. It's interesting to see all the different approaches everyone is taking towards making COM work with rust. It would certainly be useful to have a common set of COM functionality, so I can focus more on writing safe interfaces into DirectX and worry less about the details of COM itself.
It's ok, I understand. I can't say I'm fond of hyper's API, it can be pretty confusing.
&gt; I can't say I'm fond of hyper's API, it can be pretty confusing. I welcome feedback! Please tell me which parts are confusing. It may be the docs are weak, or poor design on my part. But I need to know!
I haven't seen *any* benchmarks that could be said to satisfy your requirements comparing Rust and Go, and other than regex and thread spawning benchmarks I'm similarly unaware of any where well-optimized Rust isn't faster than well-optimized Go. Can you point me to some?
Ah ok, I see what you meant. My mistake, I think we are agree on that point :)
Yes, you do. Also, don't believe what Reddit the site says; the gold actually expires tomorrow night, believe me :P
If you''re passing an `&amp;'a Box&lt;Trait+'a&gt;` that doesn't meet `&amp;'a Box&lt;Trait+'static&gt;` in June 2015, you definitely are following Rust development and also know enough about the type system to understand whatever bug this will generate. You'd be doing something so against the grain that its unbelievable you've done it without knowing what you're doing.
&gt; It would be nice if set_raw was made more ergonomic. Yea, I've had an internal battle over that. Perhaps if I added docs showing how to simply use the `header!` macro (and the benefits), so people only use `set_raw` when necessary? &gt; why does the `RequestBuilder` support chaining calls while `Headers` does not? Headers is pretty much a HashMap, which also doesn't have method chaining... *shrug* As for `RequestBuilder`, it may make sense for it to use `&amp;mut self` instead of `self`. I'll have to try that out (or I'd welcome a PR showing it's better!). 
&gt; docs are hard :( Yep :( will doc harder
I have a toy implementation at https://github.com/Kimundi/scoped_thread_pool_cache that combines the scoped API with a reusable thread pool. Like the RFC proposal, it suffers from a compiler bug that makes it memory unsafe though, which is why I haven't turned it into a polished cargo package yet.
It produces less optimizes code specifically because it works in parallel. Parallel generated pieces of code can not be optimized against each other, and there might be some slight duplication like with regular separately compiled libraries. etc.
Small program that tests if a string has umlauts: https://gist.github.com/hjr3/c7995e6a3cbef9e62547 edit: I updated the gist. The `char` type implements the Copy trait so you don't need to pass it by reference. That also means the `has_umlauts` method does not have to dereference `x`. That also means we can just pass `has_umlauts` to the `any` function instead of writing out the full closure syntax.
This ..... does work and seems to solve every problem I had (after deleting the "\n" thanks to ayosec). Thank you very much :).
My teacher said only "aeiou" back in school :p.
(Alternatively, `!line.chars().any(has_vowels)`.)
That isn't the same, though, because of the reference.
Note that those benchmarks measure full memory, and Rust links statically by default with no LTO (both can be toggled). Edit: LTO won't affect much
Oh, I missed that, nevermind
Go uses much smaller stacks by default--Rust needs a fair amount of memory for jemalloc.
And then there's "Yeti" where it's not a vowel. Natural language and its special casing everywhere...
What's a character in UTF-8? You can pick: * byte * codepoint * grapheme Any of these could be correct based on what you're doing. But most of the solutions here have picked codepoint, and that's generally the wrong answer.
The thing with minor breakages means that it might break 1% (one in every hundred projects breaks) of the projects out there (either in their code, or code they depend on). The idea is that in a specific release you'd get a break, but the amount of releases that would happen before you had a second break would be much larger. Assuming the above case, the average amount of releases before you got breakage would be something like ever 99 releases(or every 594 weeks or about every 11.5 years), with a mean deviation of about 73 releases. Basically minor breakage might hit you, it should rarely hit you, and if it does chances are Rust is over 10 years old. Now the idea of releasing every 6 weeks is that it's hard for mayor breakage to go through, because issues that appear are quickly fixed. If Rust were updated every 6 months more projects would become dependent on the situation. Older breaking changes therefore have a higher chance of affecting your code, or the code of libraries your code depends on, therefore increasing the chances of your code getting broken greatly. This also means that this will only work for features that were added recently, we won't get to see rust 1.65 doing a breaking change on how associated types work (as by the a huge amount of code would depend on that feature). The big, and important question, is if this change truly causes minor breakage or not, but the evidence seems to point to it being the case. 
Those are really bad benchmarks, since they usually end up devolving to who can bother linking to the most super-optimized native code library available, making them all but pointless as a comparison of performance between idiomatic programs.
Can you provide a link?
I was surprised by Julia's performance on the Matmul benchmark. I tried to reproduce the results locally. On my machine Rust and C are a bit faster than reported but Julia is 5 times slower than reported. Juila-native was 0.75s, instead of 0.15s reported. Rust was 3.02s instead of 3.70s. C was 3.00s instead of 3.64s. Anyone have any idea why I'm getting such a different result here? I'm using a 2.5 GHz core i7 on OSX whereas the benchmarker wrote that they were using a 3.1 GHz core i5 on Linux. Is Julia known to have much worse performance on OSX? Another unlikely, but possible difference is the Julia version: I'm using 0.3.8, the benchmarker is using 0.3.7.
Yeah, it may take a bit longer than I thought. The channels are built as a multi-producer single-consumer object, and really, I want the opposite, or really a generic multi-producer multi-consumer model. Or really just build the synchronization objects between the stages in the pipeline. So really, I need to write my own safe FIFO shared queue, and then my own shared hash map. So I'll be working on the queue part first.
That first link said they did one of the javascript tests in IE6 on XP. WTF! 
On line 94, match diff{ 2 =&gt;return rotate_left_successor::&lt;T&gt;(root), -2 =&gt; return rotate_right_successor::&lt;T&gt;(root), _ =&gt; unreachable!() }; can be replaced with (more idiomatically): match diff { 2 =&gt; rotate_left_successor::&lt;T&gt;(root), -2 =&gt; rotate_right_successor::&lt;T&gt;(root), _ =&gt; unreachable!() } It is generally considered to not explicitly return if the thing you are returning from is the last expression in a function anyway. I would replace all of the `return` statements on the last lines of rotate_right, rotate_left, rotate_left_successor, rotate_right_successor, diff_of_successors_height, insert and search with just plain expressions (no return statement because the last expression in a function is returned as the result).
I'd argue it would just add more opaqueness to signatures which really aren't that bad in the first place.
I'm not sure if this would be helpful for you, but [oyashio](https://github.com/viperscape/oyashio) is a single-producer multi-consumer channel implementation (opposite of mpsc). I don't know if a multi-producer multi-consumer library exists yet for rust.
In general (not just concurrency), the Java standard library is very large compared to mostly all other languages. Java packs everything in std, from xml parsers to a full UI library. They even have a [script engine interface](http://docs.oracle.com/javase/8/docs/api/javax/script/ScriptEngine.html) built in. Part of this is also that Java doesn't really have a package manager built in at all. They have maven, gradle, ant and the like, but no one central package repository that everyone uses, so they pack as much as possible in to the stdlib. Rust is the complete opposite of that (even more than other languages like python/javascript), with cargo being a first-class application packaged with rust and a huge community of packages to provide common functionality. Rust's stdlib is minimal because depending on a crate is just one more line in your Cargo.toml.
\r\n will move to the new line on pretty much every system (even though its non-standard on Unix), but \r \n (note the space) will not. println! already prints a newline char after your input, so "{}" produces {}\n, meaning in your case that it ends with \r\n.
Can't people write benchmarks that don't link anything external?
I really want to know where the idea is coming from that GPUs are some sort of magical solution (and none nobody thinks about for that matter!). GPUs are not magic. They are just heavily concurrent floating point processors.
I still think this is a horrible API if for no other reason than that you need to join all the threads at the end of the artificial scope. Also the extra indentation is really just ugly and makes this API look completely out of place. //EDIT: just for the record: I don't think that collecting into a vector was a particularly ergonomic API either.
Agreed, Java's standard libraries have collected a lot of random stuff through the years (though I would argue it's comparable in size to Python's). Regarding dependency management, it appears that mvn has mostly won, with gradle being a comfortable second. Many projects will be content to manually put a set of jars on their classpath. One trick Rust could learn from java is creating a common standard API and letting third-party library implement it. This decouples the code from the libraries and affords users more choice, for example java.xml.* or java.sql.*. However, I'm not sure how to implement this in Rust, as it would require a feature-based code lookup or some sort of plugin architecture. Also the design of the common APIs has to be done *very* carefully.
Yes, or Arc if I need concurrency (no big deal, the compiler will tell me anyway).
I remember the spec being quite dense, so well done on that.
&gt; that you need to join all the threads at the end of the artificial scope. I'm not sure what the issue is here - do you mean that you need to re-spawn the threads all the time? Because my prototype there doesn't do that.
I mean that generally you have very little control over the individual thread lifetime. You can basically collect all the outstanding threads at the end of the artificial scope, but you can't join them individually. A good example is N worker threads but one separate producer thread you want to join first before picking up other work.
Ah, okay.
The bug is that the inner closure can capture state from the outer closure, even though its only supposed to be able to capture state from the outside of the outer closure. This leads to dangling or invalid references. Apparently there isn't actually an issue for it, but its definitely being worked on by one of the core devs.
Why? 
Je l'ai passé à 400. Tu peux me dire si c'est mieux pour toi ?
That's an understatement; I remember comparing yaml to json because I needed a bit more... and settling for json because I did not need *that* much more!
In general, Rust has lived up to its promise, and plays in the same ballpark as C (sometimes a bit faster, sometimes a bit slower). Most benchmarks I have seen to date however tested less the language itself and more the maturity of one or two libraries; for example regular-expression benchmarks or bignum benchmarks... which just does not mean much (for any of the language in the comparison).
Is there a reason you are not publishing the crate on crates.io?
a̬͇̣̖̤̰̭͆́ͪ̓ͭ͌ͮ̒ͪ̽̉̕͠ͅ The first line of this comment "fits into 1 whitespace". It is 43 bytes long.
It seems to me a number of memory allocations could be removed with minimal work. 0 allocation document parsing is always ideal when possible :) Cool stuff.
It does! Check out https://github.com/mahkoh/comm. Very many performant channels there.
Actually, this one is even more interesting: 1. Rust has a very capable macro/plugin system, and indeed the `regex!` macro actually invokes a plugin underneath which compiles the regular expression into Rust instructions at compile-time, and therefore the regular expression is optimized ahead-of-time (like the rest of the code) 2. Nothing actually prevents C or Rust from using a JIT to compile run-time regular expressions into code (see [PCRE-sljit](http://sljit.sourceforge.net/pcre.html) for example); however embedding a "full" optimizer pipeline + backend (such as LLVM) seems a bit much so I suppose it would not be as "optimized" by default.
Please see Rule 2.
Fun fact: YAML is a superset of JSON
&gt; That restricts you to statically-known regular expressions though? Yes, which is in my experience the dominant use case. Note that the Rust regex-dna benchmark isn't using compiled expressions.
The second, unconditionally. It's easier for me to scan.
required space after the colon :(
I'm not building a shared library. Cargo is creating the static libtest_cci.a library fine and has combined symbols and implementations from Rust code and the static libcci.a library. The problem is with Cargo building and running the tests/simple.rs file. Verbose mode gives me this: (shows Rust linking against the wrong library: libcci.a. The test code should link against the crate code libtest_cci.a) Process didn't exit successfully: `rustc tests/simple.rs --crate-name simple --crate-type bin -g --test -C metadata=f94ccc523e43d1cf -C extra-filename=-f94ccc523e43d1cf --out-dir /home/deadbead/src/test /rust/test_cci/target/debug --emit=dep-info,link -L dependency=/home/deadbead/src/test/rust/test_cci/target /debug -L dependency=/home/deadbead/src/test/rust/test_cci/target/debug/deps --extern libc=/home /deadbead/src/test/rust/test_cci/target/debug/deps/liblibc-2eda841eb12a3090.rlib -L /home/deadbead/src/test /rust/test_cci/lib` (exit code: 101) [lib] name = "test_cci" crate-type = ["staticlib"] 
Thanks - I'm back at this question again. I'm going to research Any and its implications.
There are a bunch of suspect things about that benchmark. Ancient versions of Java, Javascript, Fortran, C; old versions of ruby, haskell, python, Common Lisp, and rust of course. I also find it hard to believe that well-written C performs worse than VB.NET. I'm sure the author found the benchmark useful for their purposes, but I wouldn't take too broad of a conclusion from them.
&gt; what about codepoint and grapheme? They are a variable number of bytes. &gt; How do I access codepoints and grapheme? `chars()` iterates over codepoints: http://doc.rust-lang.org/std/primitive.str.html#method.chars `graphemes()` iterates over graphemes: http://doc.rust-lang.org/std/primitive.str.html#method.graphemes (this is probably better used from crates.io now) If you want bytes, `bytes()` does that http://doc.rust-lang.org/std/primitive.str.html#method.bytes All three options :) &gt; And which of the three options is most correct in this case? Usually, it seems to me 'graphemes' is what most people want, or comes closest to what people think of with 'characters'.
Thank you!
any time :)
You can join them individually (at least, last time I looked the proposed API returns `JoinHandle`s): the API just ensures that all threads are joined at or before the time the scoping function exits, it doesn't require that the joining *always* happen then.
They can, but that's not against the rules, so they'll just be obsoleted by another benchmark that does in order to compete in the arms race.
jdh changed the link in his comment to point to somewhere else. That's probably why you're getting downvotes, because it doesn't say what you claim it does. I, however, looked yesterday (ie. before "this gist was created 5 hours ago") and saw the thing you saw. This jdh guys is rubbing me the wrong way. (Sorry jdh, but changing a link doesn't make a great case for yourself)
I got two times when I ran the benchmark. Julia Native -143.50016666665678 0.12793803215026855 0.79s, 87.8Mb Looks like the reported time is [the one measured in the program](https://github.com/kostya/benchmarks/blob/df7b1c75ff9b588e41ecdf076c6ead1856d51db8/matmul/matmul-native.jl#L18) (0.13s) rather than the execution time (0.79s).
Nice catch. On my machine it prints that too. I hadn't realized that it was timing itself.
I just don't know if some part of the compiler would be done by a heavily concurrent processor. Perhaps chasing data structures with a lot of pointers isn't the best fit for GPUs though. But the whole issue isn't that the GPU would be a magical solution, but that they sit underutilized in most workstations; in that situation, any substantial GPU offloading would be a win.
&gt; Experience with [somehwat hip/insightful languages we'll use as a proxy for interest in programming (languages)] a plus Yeah...
EDITED *Reader beware!* &gt; To give you some idea, I optimised knucleotide in F# and made it 3.6x faster. J wants you to believe that k-nucleotide program is an example of - *"I submitted lots of much faster implementations to him and he rejected or ignored them."* - but never actually makes that claim. J wrote a blog article about a k-nucleotide program. J **did not contribute that program** to the benchmarks game for measurement. If you have nothing better to do you can search through the tracker items from [2012-12-19 01:12](https://alioth.debian.org/tracker/index.php?group_id=100815&amp;atid=413122) onwards. "Simple Filtering and Sorting" tab =&gt; "State" Any :: "Order by" ID Ascending :: click "Quick Browse" *Reader beware!* &gt; Here is an example of a program "de-optimized by Isaac Gouy". A program which existed temporarily until Don Stewart had time to provide a program with strict allocation - so the required allocations would actually happen.
About the original challenges: I don't know any Swift, but it looks like the solution the author provides to challenge #2 doesn't actually work; I expect in their implementation that flexString("") would return "none" rather than "".
I actually found it last night, and was going to remark on it today. It seems to be interesting - so I am playing around with it to see if I can get it to work!
I'm beginning to wonder if, when 2.0 rolls around, Rust should just *remove* the `char` type entirely. It's obvious that no amount of arm-waving is *ever* going to get the message across: "`char`" is a great, big, fat *lie*.
My rust code succeeds at finding libsnappy.so.1 with the .cargo/config shown in my other comment. No need for env vars at runtime.
*waves hand* This isn't the subreddit you're looking for. /r/playrust Go about your business.
Yeah! Go Cargo! Congrats to the whole community.
Good job. Another example showing that stable rust can be used – well, soon, at least. :-)
Definitely not in the same way, in most cases. For example in the #5, reversing a String: while the swift solution uses a recursive function, the Rust approach would probably be something like: fn reverse_string(s: &amp;str) -&gt; String { s.chars().rev().collect() } Which is much more straightforward You could forbid the use of iterators, sure... But it'd be throwing away one of the main features of Rust.... On the other hand, #6 is not doable in Rust at all. We cannot impl Mul&lt;u32&gt; for str { ... } because both types and the trait are from the std.
You know, as a side in the conflict (as opposed to me, being a mere spectator) you could do much better that snark comments. You could, for example, write a post explaining how you accept/edit/choose/whatever your benchmarks, and your motivation for it.
This is not about dynamic linking, but link-time optimizations. Which contains inlining (which likely increases code size), loop unrolling (worse code size increase) and other items trading memory for runtime.
Yeah, but not stable... Which was why I didn't use it Though indeed, using graphemes is a better approach, but the general idea (using iterators) is still the same, which was my point.
Beside the technical points mentioned by others I really think we (as in the rust community) should stay away from the ninja/rockstar/wizard narrative as far as we possibly can. Imo it's a trope that is toxic on many levels. We don't need persona cult, we don't need the belief that one has to be extraordinary talented to become a programmer and we certainly don't need to load up programming with masculine power fantasies.
Oh, nice, didn't know about this crate. It's better to use this indeed. But the general idea is still the same : *iterators*. ;-)
&gt; For example, the first question is "create a function that takes references to two variables and swaps their values in a single line of code". But in Rust I wouldn't use a function You probably wouldn't in Swift either. Ignoring the restriction to swift syntactic sugar (single-line implementation), I'd guess it's really a way to show off swift having pass-by-reference semantics.
The Rust project uses Homu (namely `@bors`) to test its code. The zen of Homu is, tests should be done before merging, not after merging. However, most popular continuous integration services only do the latter. When I discovered this for the first time, which was called "bors" at that time, I found the concept extremely useful yet very simple. It came as a surprise to me that there aren't many open source tools that provide this concept! That's why I contributed to Homu, which is a complete rewrite of bors, mainly to fix the performance problems of it. Today I'm pleased to announce Homu-as-a-service, namely [homu.io](http://homu.io/). This way, you can use Homu for your projects without manually installing it. It works on top of GitHub and Travis, so you don't have to set up yet another continuous integration service. Just register your repository on homu.io, and that's all! One more thing is that homu.io is written in Rust. It communicates with the "real" Homu (which is written in Python, btw) internally, but the web part is entirely written in Rust. I'm even considering a complete rewrite of Homu (again!) in Rust, just to feel more rusty. Anyways, the overall experience of the web development with Rust was fun and pleasant (of course you should understand that I'm a Rust fanboy!), so I may grab Rust again to write a web site in the near future.
That's why the sidebar says "11.422 human beings" :)
Many of us use their favourite editor, be it vi, emacs, SublimeText, atom, geany, kate, Notepad++ or whatever. Then there's a number of Rust IDEs in the works, of which RustDT seems to be the one with most progress (subjectively, I may be wrong).
I'm not sure, but it may have to do with some build script.
GPUs are massively parallel processors designed to do the exact the same thing on a million pieces of data all at the same time. This is true to the point that conditional control flow is bad in GPU programming because all the execution units can't run in lockstep anymore. The whole point of a GPU is that each piece of data can be handled completely independent of the others, when you have a stupid number of these pieces, GPUs are brilliant. But most of what a compiler does is so far removed the ideal GPU workload it almost seems deliberate. Pretty much every stage has complex interconnections and decision making dependent on a wide range of data. To put it simply, trying to use the GPU to speed up compilation is like trying to use a sports car to move house. Sure the sports care can move fast and get your forks to your new kitchen in record time, but it's not gonna be faster than a moving truck when you have to make 30 runs instead of the trucks 1.
"Experience with OCaml, Haskell, Rust, Nimrod, or Julia a plus"
I've been using vim for Rust on Linux for a while now, but now I am writing Rust with Atom on Windows now. How bothersome is it to set up racer, and is it worth the effort?
Does it autocomplete only on std code, or your own code too? How about deps?
I have been using RustDT for awhile now, I say it's polished, I can go to declaration by hitting F3, which I think is the most useful feature when editing code.
We use gitflow and build the feature branches before merging them into develop. It involves more workflow then Homu seems to do (only took a glance) but it works for us.
you mean the setters and getter methods? It doesn't, besides setters and getters is not a common pattern in rust. Just mark the member of a struct pub, and you get the same functionality.
[This isn't just directed at you, rather at everyone here] Let's try to avoid digging up such conflicts and discussing them here from this point on. Discussing benchmarks and how to improve them is okay, discussing people and behaviors is not. It's pretty obvious that the situation is more complex than it looks, but I don't want to have that discussion over here about it because it's bound to get ugly eventually. Suffice to say that the benchmarks game, regardless of any "deoptimizations" is indicative of some things Rust could work on; though their importance/priority (and relevance to performance in larger applications) is a different matter. Thank you.
Would you mind posting a gist of your existing code? I believe the `clear_timeout` function should be what you're after.
Also on your own code to some extend. It doesn't on deps unfortunately (at least for me, maybe there's something I need to configure).
Actually it's probably not that hard: github seems to have an [enterprise version](https://enterprise.github.com/home) which can be hosted locally. Travis does [also](https://enterprise.travis-ci.com/). Does homu integrate with those or just the global one?
Known issue: https://github.com/rust-lang/cargo/issues/1162
If you're interested, I have my [first working implementation here](http://www.reddit.com/r/rust/comments/39toz3/first_steps_for_a_pipeline_model_in_rust/)!
Could you kindly specify the redundant memory allocations so that I can remove them in the next release? Thanks!
There is a *very* big difference between 6 weeks and 38 weeks.
Rust noob here - What are the advantages of this compared to a [RwLock](https://doc.rust-lang.org/std/sync/struct.RwLock.html)?
I use Sublime Text 3 with [Rust](https://packagecontrol.io/packages/Rust) and [RustAutoComplete](https://packagecontrol.io/packages/RustAutoComplete) plugins. RustAutoComplete is basically just a binding for [racer](https://github.com/phildawes/racer), though it integrates with Sublime Text very well. Most people use Sublime Text, Atom, Emacs, vim, etc. with racer &amp; rust plugins installed. Because all the editors use racer for auto completition, there isn't that much advantage to using one text editor over another besides just what you are familiar with.
homu = homura akemi, from the anime mahou shoujo madoka magica
It doesn't seem to have any configuration values to use different servers, though it looks like just changing the urls to specify enterprise versions would certainly work.
This is certainly another massively useful feature.
Well, stable rust means rust 1.1 in this announcement.
So you start with struct Thingy; impl Thingy { fn new() -&gt; Thingy { Thingy } } fn main() { let thing = Thingy::new(); thing.die&lt;cursor&gt; and hit ctrl-1 and get struct Thingy; impl Thingy { fn new() -&gt; Thingy { Thingy } fn die(&amp;self) { &lt;cursor&gt; } } fn main() { let thing = Thingy::new(); thing.die ?
check out [oyashio](https://github.com/viperscape/oyashio) for a look at spmc. rust currently lacks streaming iterators, which could make this even better. There are a few mpmc out there to check out as well, sorry no links
Talking on Free Software or GPL tends to become a holywar in some cases, I'd like to avoid that for now. What I want to point out is the overall policy behind the project. Currently, maidsafe writes that "no person, company or organisation should own the technology" and at the same time it owns all the code and all contributions. This is totally asymmetric to everyone else.
&gt; It even shows many compiler warnings and errors What mode are you using? Also, what are you using for tags?
&gt;error: linking with `cc` failed: exit code: 1 For the record, I have openssl and libssl installed.
Not taking away anything from it, but it's also because cargo has guided what needs to be stabilized. I guess all those distros that want a cargo that builds with stable rust play a part in that.
You mean by manually building homu? That would probably work.
yeah. I would assume that if you are going to host github and travis locally, you'd want to host homu locally as well. Also, side note: I don't think it would involve any "building" of homu, since it's written in python.
The piston developers are also working on a Visual Studio integration (https://github.com/PistonDevelopers/VisualRust). This requires a little bit of extra acrobatics, since VS requires a project file and Rust doesn't normally have a project file per se, but this is mostly handled transparently by the addon. On my macbook I use Sublime. It works great, but I'm not an OSX expert, and racer was a pain in the butt to set up because of the crazy way that OSX handles $PATH differently between Terminal vs Dock vs Spotlight. I don't know if these are the best options, but both of these are good enough that I haven't felt motivated to go find other options, when I already know these editors quite well.
Our current plan is to release Cargo master as of release 1.(n-1) for Rust 1.n, so yeah it'll be ~8weeks for this to be release on the stable channel of rust (1.2), but for now nightly Cargo should do the trick!
PR submitted! https://github.com/rust-lang/regex/pull/91
Amazing work! This will certainly reduce the gap on *ahem* certain benchmarks. Edit: Looking at the benchmarks, the pessimisation in `no_exponential` does stand out. What does this actually benchmark?
Some of those people were people who are involved with the Rust project itself (no reason to name names, since that could get into a he-said-he-said thing). Since they were talking about this as a *breaking change* in the Rust definition of it, I then presumed that they thought that this change was beyond that definition! But clearly as shown in this thread, there seems to be disagreement among people who are actively involved with the project. I've tried to make these assumptions, and the assumptions of the knowledgeable people that I've seen in this thread, expressively clear in my comments!!
What a terrible non-argument. This can excuse any behaviour that people don't like because they were too trusting, thought that the person would act within certain agreed upon confines, and so on. A free pass to do anything, and blame other people for being stupid when they have to suffer the outcomes. Note that you are debating this in the context of *the Rust community* itself, among them people that make decisions like this. It is one thing to say to people that should not be too trusting of backcompat in other venues than /r/rust. In that venue, the advice is "don't get burned". But in this venue, the advice should be "don't burn people needlessly". Not "if people get burned that's their own fault".
Ruby is by the way removing $SAFE this as of version 2.3: https://bugs.ruby-lang.org/issues/5455#change-52856 Perl uses tainting as some kind of lint, nothing more. Charles has a good rundown of the flaws and the scope of the model here: https://bugs.ruby-lang.org/issues/8468#note-3 Basically, the issue is as follows: tainting of externally loaded data means that the system needs to know about all ways to create data in the system. Is something created by a C library "tained" or not? It might have involved I/O or not. $SAFE uses a "let's label the world"-like approach that usually leaks. If the implementing person fails misses a label, you have a problem. You could implement a marker type though that can only be consumed if a certain predicate passes. That might be for local use only, but seems like a worthwhile endeavor.
A breaking change can still be included into minor version, if it *fixes compiler bug or type issue*. This one obviously does, ergo it doesn't change minor version.
Ooo. Yes. Clever! Thanks!
I use vim w/racer. I remember seeing some rust thing that actually highlights error lines for that, but have not installed it (I think)
I've never used perl, or a language with this feature, so maybe I'm missing something. Why not just a wrapper type `Tainted(String)`, have IO functions return `Tainted`, then have validating functions like `f(x: Tainted) -&gt; String` (or `g(x: Tainted) -&gt; i32`)?
For the heck of it I decided to encode an esolang in rust macros. I picked [Ook](http://esolangs.org/wiki/Ook), (a textual variant of brainfuck) and substituted `Ook` with `HODOR` because the code is much more readable that way :P A small thing to note is that the recursion structures (`[` and `]` in brainfuck) are not to be written in their text form, continue to use symbols. This is because I can't tell the parser to balance `HODOR! HODOR?` with `HODOR! HODOR?` whilst parsing. /u/Quxxy is working on a version which uses a compile time stack to achieve this, which might work.
*My name is Quxxy, macro master of macro masters: Look on my works, ye Mighty, and despair!* [No-Compromises Hodor! Macro](https://play.rust-lang.org/?gist=7efa9b1f539b8b3fcd34&amp;version=nightly).
I guess the problem with this is that it's either too easy or too complicated to create a `SafeSqlQuery` object.
Using it, I wrote a brainfuck interpreter running on the hodor compiler written in Rust macros. [yo dawg](https://gist.github.com/Manishearth/fabbc041f007fc21499c) It takes longer to compile than Servo's libscript. I suspect it will take longer than librustc too (edit: confirmed. 2 hours and counting, which is way longer than a full Rust stage2 build). Congratulations, we're no longer the slowest compiler!
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 4. [**Dependent types**](https://en.wikipedia.org/wiki/Idris_%28programming_language%29#Dependent_types) of article [**Idris %28programming language%29**](https://en.wikipedia.org/wiki/Idris%20%28programming%20language%29): [](#sfw) --- &gt; &gt;With [dependent types](https://en.wikipedia.org/wiki/Dependent_type), it is possible for values to appear in the types; in effect, any value-level computation can be performed during typechecking. The following defines a type of lists of statically known length, traditionally called 'vectors': &gt;This type can be used as follows: &gt;The functions appends a vector of m elements of type a to a vector of n elements of type a. Since the precise types of the input vectors depend on a value, it is possible to be certain at compile-time that the resulting vector will be have exactly (n + m) elements of type a. The word "total" invokes the [totality checker](https://en.wikipedia.org/wiki/Partial_function) which will report an error if the marked function doesn't cover all possible cases. &gt; --- ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cs76xb8) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cs76xb8)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
On mobile. Haven't investigated. My guess is that the new bounded backtracking engine is slower on that particular regex than the nfa simulation. The benchmark is supposed to demonstrate that the matching engines have worst case linear performance. I think if you try that same regex in, say, python then it will take a very long time to terminate.
On a more serious note, assuming the [RFC to allow macros in type positions](https://github.com/rust-lang/rfcs/pull/873) gets approved, you can use similar methodologies as shown here, to compute types. One example would be the ability to specify parameters of FFIs to be `in`, `out`, `opt`, etc and have a macro that generates the appropriate function signature and return type. This is already possible, but it's incredibly nasty to write right now. Allowing macros in type positions would really simplify computing types.
That is not possible. Each closure is its own anonymous type, and as such you can't store types that use them. A workaround would be to use a free function in place of the closure (which would have the type `fn(char) -&gt; bool`)
I'm not an expert but here's what I know: It's not possible to define the concrete type of a closure. Fn, FnOnce, and FnMut are traits. FnMut(char) -&gt; bool would be a trait object, whose size is not known at compile time. That's why you're getting the error you're getting. You don't actually *need* a closure for this use case because the closure isn't referring to anything on the outside. A function will do fine. pub struct Tokenizer&lt;'a&gt; { words : std::str::Split&lt;'a, fn(char) -&gt; bool&gt; } impl&lt;'a&gt; Tokenizer&lt;'a&gt; { pub fn new(line: &amp;'a str) -&gt; Tokenizer { Tokenizer{ words: line.split(char::is_whitespace) } } } 
Awesome! I'd tried using a library [1] for matrices, but got terrible results. Using the code from the benchmark, I could compute the product in ~3s, using the library took ~25s. I don't think I did anything too naive while using the library, but I haven't had any time to look into what's going on behind the scenes. If you really want to pursue this, you could make a small crate that provides dot_product and maybe some other math utilities and promote it's use in the community and with library authors. It seems to me the path to std goes through crates.io I'm also curious if you have any thoughts about the performance of the library I chose or any other recommendations. [1] https://crates.io/crates/nalgebra
This is a thing that the mods change all the time, so I'm sure it'll be back to Rustaceans at some point :) The current one is a reference to http://scattered-thoughts.net/blog/2015/06/04/three-months-of-rust/ &gt; The Rust community seems to be populated entirely by human beings. I have no idea how this was done.
There is an issue in the project discussing their plans for Cargo integration. Issue is [here](https://github.com/PistonDevelopers/VisualRust/issues/3) and the comment outlining their decision and rationale is [here](https://github.com/PistonDevelopers/VisualRust/issues/3#issuecomment-100860137). Basically, there are *some* occasions where you would have to add files manually.
Does the Vec option above copy the string parts in new memory? If not then the slight overhead of Vec itself may be acceptable.
Last time I used it, linter-rust didn't support cargo. Has this changed?
Up for me.
&gt; Does the Vec option above copy the string parts in new memory? A `&amp;str` is a pointer and a length, it won't copy the underyling string.
`Split` is a lazy iterator, while `Vec` will allocate the `&amp;'a str`s on the heap.
For example? column A -- pi-digits (gmp), regex-dna (pcre, re), … column B -- spectral-norm (don't include numpy), n-body, fannkuch-redux, … 
Anyone working on an IntelliJ plugin for Rust?
Sidebar updated.
LOL. &lt;3
Phantom types can be used for this: https://play.rust-lang.org/?gist=3b343ead30e237362cc5&amp;version=stable The way that you'd set this up in your application would be that all functions that accept tainted input would return a `MyStr&lt;Dirty&gt;`, and all functions that do things with input would accept a `MyStr&lt;Clean&gt;`, and you'd have one validation function used to turn a dirty string into a clean string. What's also cool about this approach is that the type system guarantees that you can't accidentally call the validation function on the same data twice, which means that you don't have to worry about things like doubly-escaped strings.
As someone who has done this, I smiled. Good look! I almost didn't catch the reference!
Awesome that it got put up!
Live coding always goes wrong. A little bit ago i did a short talk in house about js tooling and such. Covered ES6/Babel, TypeScript, using those with grunt, with rails, etc. Naturally when i first show typescript, Atom decided to not show any of the compile errors. When i later showed a cordova example they worked again lol.
The unsafe line is calling `libc::string::memchr` as far as I can see.
interesting.. never knew it was! :) I also didn't expect it to be a hidden reference to some person's obscure blog post i'm still playing around with Rust. I came from Go, and i'm just trying it out ;) liking it so far!
&gt; I also didn't expect it to be a hidden reference to some person's obscure blog post Yeah, I mean, the post was made two weeks ago, so it's probably a little old for a reference. That said, the CSS gets a lot of tweaks generally speaking, as you can see this thread has already caused a change... ha! Glad you're having a good time.
This is great! You may want to add a little more context for people on what some of these things do. If this is for a beginner, they may not know what pathogen is, what number toggle does and what racer is for. Of course, they can find those github pages and see, but I think walking them through it is best.
As a bunny that likes to program in rust, I feel very left out by the category `human beings + bytemr`.
We are not ready to commit to a specific price, but I am willing to say that it will certainly not be a four-figure number.
Hope somebody enjoys it :-) It's a bit rough on the edges (just extracted it from another project), nevertheless code reviews always welcome! Edit: And I would *really* like to get rid of that 'static, even if it doesn't really matter with enums.
YOU GET NOTHING
Why do you want to focus on the license? All my comments are about ownership (heh:). They say no one should own the technology and they fail at that. That is enough to scare away, isn't it? For me it is.
... and now that you say that I realized that I said it wouldn't be four, not that it would be less than four. Bwahaha, $10,000 a ticket! :P (but seriously it'll be three digits)
The stdin line reading or the file line reading?
You don't need to write the FFI bindings yourself, there's the https://crates.io/crates/user32-sys crate, which _should_ have this particular API
Use [winapi-rs](https://github.com/retep998/winapi-rs).
I would just make sure that any separate input/output operations are in separate threads. You could do it either way, as long as they aren't both trying to be done in the same thread.
Hi, a bit of Rust-specific and general context. Chaos Communication Camp is happening all four years and is the summer equivalent to the well-known Chaos Communication Congress. We had great experiences being present at the last one and hope to get more people involved during the summer. In contrast to Congress, where assemblies are free, it is recommended to rent a tent at the Camp. The tent is already funded for a reasonable size. We try to raise a more cash for a larger tent - we can definitely fill it. All raised funds will be used for the assembly tent. All Rust Berlin organizers are involved in other groups like NodeSchool and Ruby Berlin, so we decided for a joint effort. Instead of running separate actions, we just get one tent for all, providing training for all three groups. We all have experience coaching and running community events. Finally, you may notice that fundraising is done by Ruby Berlin e.V. RB is a registered non-profit similar to US 501(c)3, so you will get a tax-deducible statement by the end of the year. We can provide IRS papers to get those accepted in the US. Best, Florian
This came 1 day too late for me :p I tried setting up vim yesterday and it took me a few hours to figure out why syntax wasn't working. I was using vundle and turns out it has an issue with detecting the correct filetype. Anyway thanks for the writeup, gonna try install racer and the other plugins your suggested. btw I think you made a minor typo in the "install vim racer plugin" section `cd .vim/bundle/` should be `cd ~/.vim/bundle/`
The problem of this link was the reference is difficult to find but if i insert in the share i do a big spoiler XD
I've been learning syntax extensions to accomplish something that I could do in just a couple lines if I could use macros in type positions. Maybe I'll hold off until I see if that RFC gets approved. Thanks for the link.
Right, but in pcapng files the endianness is determined by a magic value in the header. How would you do this in nom without creating two very similar parsers?
But this only works for methods of the base type that take the Board by reference, not by value. Am I correct in assuming this? 
I just tried it and I was able to pass by value pub fn test_print(board: Board) { println!("{:?}", board[0][0]); }
Honest question: is there any advantage to using libraries like these (e.g. channels, mpsc) rather than using something like ZeroMQ?
Great job!
Yes, though you could also define the new methods as a trait and then implement them for Board; you just can't implement the type alias itself.
Is UTF-8 not named after "8 bytes for every character"? And shouldn't Rust reserve more memory when there is such a big character? And how did you wrote that nice made up letter?
[Nope](https://play.rust-lang.org/?gist=f888f460b3dcd583033d&amp;version=stable).
This is great, thank you!
The name for Ruby Berlin actually comes from the fact that it was founded just to run the EuRuKo in Berlin. We didn't want to choose "Ruby Germany" without asking first and were in a rush. It was was later evolved and registered as a non-profit. Changing the name would be quite an effort nowadays. Even organizations have legacy :D In posters and advertisement materials, we generally refer to it as ".rb". CCC actually runs Congress in Hamburg nowadays, the Camp is somewhere on the fields north of Berlin (because there's a lot of space and nothing else ;) )
Done! Looks like it's going through regardless, though.
Good question! This is for internal usage, where you have a lot of threads running. For instance, I'm using this in a non blocking chat client I'm working on. Queue managers are a seperate process entirely. In a distributed system (think many processes/servers), a distributed queue manager can remove dependencies in time and in space. Meaning that servers don't have to be up to guarantee something gets processed and servers don't have to know about each others address. That can make a distributed system much more robust and easier to scale.
But now the counter has an off-by-one error :-P As a workaround, we could petition bytemr to unsubscribe...
Seems to load&amp;play fine for me.
does `validate_string(x: MyStr&lt;Dirty&gt;) -&gt; MyStr&lt;Clean&gt;` have any advantages over `validate_string(x: DirtyString) -&gt; String` (where `struct DirtyString { s: String }`)? Or is it just another way of expressing the same thing?
See also http://stackoverflow.com/questions/30012995/how-can-i-read-non-blocking-from-stdin/30013230 (My answer there has a *bit* of information about using mio).
I could see it being useful for something like `MyStr&lt;Dirty&gt;`, `MyStr&lt;HTML&gt;`, `MyStr&lt;URL&gt;`, `MyStr&lt;SQL&gt;`, etc. After all, using something of the wrong type of `Clean` is just as dangerous as using something `Dirty`.
Passing a function to a macro is actually passing an ident, so it would be possible to have the integer parsing function passed from a variable.
Yes, you can, using a generic type param, see nwin_'s solution.
Look for "RUST" in the comic strip.
More like Homu-as-a-demon, no? Madokami help us all. :P
Did you also use float32? See this commit: https://github.com/kostya/benchmarks/commit/6e77966f73580ffc2801c88b9fd29e193a00bed4
Oh, I thought that the vanilla Vim can show line numbers and relative numbers too (`:set [inv]number`, `set [inv]relativenumber`). What's the point of `numbertoggle`?
With a `type` alias, `Board` and `[[Square; 8]; 8]` are the same thing. `impl Board { ... }` is forbidden to avoid misleading readers into thinking that methods defined there apply to `Board` but not `[[Square; 8]; 8]`. Which can’t happen, since they’re the same thing. You *can* however use `impl [[Square; 8]; 8] { ... }` and methods defined there will also be available on `Board`. Types like `struct Board( [[Square; 8]; 8] );` are also called "tuple-like structs". This one just happens to only have one member. Maybe the `board.0` syntax will seem less strange if you think about such tuple-like structs with more than one member: with `struct Foo(u8, char, String)`, `foo.0` would refer to the `u8` member, `foo.1` to the `char` member, etc.
[**@nick\_r\_cameron**](https://twitter.com/nick_r_cameron/) &gt; [2015-03-23 20:40 UTC](https://twitter.com/nick_r_cameron/status/580106932297465856) &gt; Using Deref for polymorphism is a hideous anti-pattern in Rust. ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
[It has pretty decent documentation if anyone's interested.](http://tombebbington.github.io/llvm-rs/)
Some time ago I wrote this as a macro ([playpen](http://is.gd/VzPVvn)). Newtypes and Deref are pretty amazing.
&gt; Sorry jdh, but changing a link doesn't make a great case for yourself Isaac objected that the content was behind a paywall so I took the commercial content and posted it on Github so everyone can see it for free. I meant no offense. 
/r/playrust
We generally stay away from that term because it has awkward connotations.
One thing I've wondered, incidentally, is whether our C FFI bindings to Rust are making codegen slower than it could be. Building all that IR takes a lot of time, and clang is at an advantage being written in C++ and thereby having the ability to inline across LLVM and clang. The best solution, I suppose, would be to have the ability to do cross-language inlining and take advantage of it in the Rust compiler. It'd be a bit tricky, though, because you wouldn't want to, say, LTO all of LLVM and rustc together, or bootstrapping times would get much worse! Possibly the optimal approach would be to LTO just the LLVM C binding layer and rustc together.
Ah the closure trait breakdown isn't really correct. Well, it doesn't have anything to do with threads at all, only with sharing in general.
I used a cbox library I made, I'm a bit busy now but I'll explain later
Are we able to profile and see if FFI has a significant cost?
Yes, absolutely.
Perhaps tweak the generator to just stub out all C FFI functions with an unreachable landing pad and measure the diff in codegen time?
You don't need to collect it, unless you really need the ownership: let url = "http://example.com/path/file.jpg"; let parts = url.split("/"); println!("File name: {}", parts.last().unwrap()); EDIT: Well, /u/steveklabnik1 explained properly how to handle URL's, while my point was that it's usually better for you to work with iterators until absolutely necessary to collect the result into a `Vec`.
I would assume this way about as simple as it gets: let path = &amp;url[url.rfind('/').unwrap()+1..]; ... but I don't really know how the fairly generic pattern searching machinery underlying `rfind` works out. Edit: In a tiny benchmark, &amp;url[url.bytes().enumerate().rev().find(|&amp;(_, c)| c == b'/').unwrap().0+1..] seems to run twice as fast. :|
Where does it say this? Indexing strings as anything other than bytes is O(N)
This looks really nice to use and far more complete than any other wrapper out there, but I'm a bit worried about safety - I think (though I may be wrong) that LLVM has a bunch of thorny, not incredibly well documented preconditions that can result in unsafety. For example, it might be (I can't actually tell one way or another) undefined behaviour to... - mix types/modules/functions/values from different contexts. For example, creating two contexts, using one to make a module, and using the other to make a function in that module. - use types that don't actually work. For example, what happens if you create a function with type 'i32' instead of a function type? - use a module/type/function/value after the context that created it is destroyed. You actually have this problem solved very nicely with your `CBox` - this is a really clever and nice mechanism. - use a value created in one function in a different function. These aren't insurmountable obstacles - I came up with an (admittedly fairly ugly) [solution](https://github.com/gereeter/compiler/blob/5d8a7c6cdd449e2673929ff0c1a76bac3b6fbdd0/src/id.rs) for the LLVM wrapper I was writing, and there might be nicer solutions possible. On a smaller note, the empty structs you currently use as placeholders for the different LLVM constructs (as in `pub struct Type;`) have a slight safety issue in that consumers of your library can create instances of these types, which should be impossible. This can be easily fixed by adding a dummy field like `_priv: ()`. I don't mean any of this to demean your library - it really is quite well done, has some clever ideas (I definitely will want to use `CBox` in the future), has great coverage, and is easy to use.
Ah yes. That's a byte index, and so it will be.
UTF-8 is _trivial_ for such cases, no? You split your format-that-uses-only ascii-characters exactly the same way as if it was ascii. '/' is in ascii, it's encoded as the same byte, and in UTF-8 that byte always represents the character '/'. Even for non-ascii delimiters, you can treat them as byte strings. (UTF-8 awareness might allow optimisations though). A non-ascii character is encoded as a sequence of bytes none of which could be mis-interpreted as ascii. Also such byte sequences also never contain the encoding of other UTF-8 characters, or prefixes thereof. A.k.a. "self-synchronizing".
https://en.wikipedia.org/wiki/UTF-8#Compared_to_other_multi-byte_encodings Note this property for ascii was specifically designed to protect '/' (and NUL) characters in existing filesystem apis. http://www.cl.cam.ac.uk/~mgk25/ucs/utf-8-history.txt
&gt; such cases, That's just it, isn't it? Yes, in this particular case, due to it being entirely ASCII, no big deal. However, not all URLs are. https://url.spec.whatwg.org/ defines &gt; The URL code points are ASCII alphanumeric, "!", "$", "&amp;", "'", "(", ")", "*", "+", ",", "-", ".", "/", ":", ";", "=", "?", "@", "_", "~", and code points in the ranges U+00A0 to U+D7FF, U+E000 to U+FDCF, U+FDF0 to U+FFFD, U+10000 to U+1FFFD, U+20000 to U+2FFFD, U+30000 to U+3FFFD, U+40000 to U+4FFFD, U+50000 to U+5FFFD, U+60000 to U+6FFFD, U+70000 to U+7FFFD, U+80000 to U+8FFFD, U+90000 to U+9FFFD, U+A0000 to U+AFFFD, U+B0000 to U+BFFFD, U+C0000 to U+CFFFD, U+D0000 to U+DFFFD, U+E0000 to U+EFFFD, U+F0000 to U+FFFFD, U+100000 to U+10FFFD. For example, you can put "www.✈️🎰💸.ws" into your browser, and it will work, just click that link. Your browser may expand this to http://www.xn--3bi0638mwwa.ws/, and mine does, but if you're writing a general URL parsing routine, you need to handle these kinds of cases. Only if you _know_ that this particular `&amp;str` does not ever have any non-ASCII characters does this work in the general case. (also, my 'UTF-8 is hard' is more generally speaking to the fact that dealing with ASCII-only `&amp;str`s are going to be a bit less ergonomic, because we can't assume that they're ASCII-only, not the details of URLs specifically.)
I sent one tiny PR. I also have a small soundness concern in the Rust implementation, `transmute` causes aliasing on an `i32`, which I'm not sure _exactly_ how dangerous it is... from IRC: 19:02 &lt; Gankro&gt; steveklabnik: I would suggest they transmute the u32 to a [u8; 4] directly 19:02 &lt; Gankro&gt; It's a no-op, and any copies should be trivially elided 
Both ``rfind()`` and the index operator ``[]``work with byte index. Is this a coincidence? I think not :) These tools will let me do lightning fast slicing.
Clicking the link doesn't work for me, but copying and pasting it works. Wut.
UTF-8 form of such an URL won't contain `/` byte and the expanded form won't neither. So even treating URL string as `&amp;[u8]` and searching for `b'/'` should be correct here (and Rust `&amp;str` guarantees that `/` will be stored always in one-byte form!). Nice property of UTF-8 is that when you care only about ASCII characters, you can just ignore all non-ASCII parts of string, as long as you'll treat them as undividable binary blobs. (ok, maybe you may break some compound characters like `é` into parts (but still keeping it valid UTF-8), but that rule perfectly applies to all separators such as space, newline, `/`, etc.)
As someone starting to learn rust I found the book burning example (from the rails conf talk) helpful in shaping how I think about the data. The visualization with the code is nice. It also helped me understand a little more of the rust vernacular. I would love to more talks like this around lifetimes, traits (composition vs inheritance) or testing. 
Please keep the good work. It's something i'd really like to see, good rust libraries for desktop linux, dbus, wayland, xdg stuff, window managers, desktop environments :D 
Great write up. I've been developing rust in VS Code for a while now (since they added highlighting support) and I'm relatively happy with the workflow and overall experience. I've tried writing multi-line regex to match out full errors (so you can get all the diagnostics for a given error), but VS Code seems to not support that, even though the RegEx works just fine in JavaScript.
Another thing I noticed that's not likely to be particularly safe is functions like [this](https://tombebbington.github.io/llvm-rs/llvm/struct.Context.html#method.new): &gt; fn new&lt;'a&gt;() -&gt; CBox&lt;'a, Self&gt; &gt; Create a new context, owned by the block that calls it. The trouble is that while this may be what happens in practice with type inference in typical coding patterns, what it *actually* means is that the caller gets to choose any lifetime they want - including `'static` - so it's very easy to break. If there were a way to go from `&amp;CBox&lt;'a, T&gt;` to `&amp;'a T` or something of that nature it would be obvious how to violate safety with it, but it's not clear to me whether there is (`Deref` is different - that goes from `&amp;'b CBox&lt;'a, T&gt;` to `&amp;'b T`). (So in other words, I don't completely understand how `CBox` works and how it's intended to be used. But an unconstrained output lifetime is very unlikely to be the right approach to anything in safe code, being equivalent to `'static`.)
Sure, you can get 'everything after the `/`' easily, but what about a URL which has query parameters? Or fragments? As I said above, it's _really_ about knowing your exact set of inputs and how it's structured.
Thank you. I have started using Rust at work. And VS Code is being useful.
Ha! But not when I looked: https://github.com/jonhoo/volley/commit/4feaa9d57f02afe319896f02ed7d85ac80ad576d
Oh
So reddit doesn't parse URLs correctly. That's funny.
No, the issue was that the Julia runtime takes a long time to start up, which is being considered a confounding variable by the author of the benchmark. Therefore, the Julia code samples have timing code embedded that report the time it takes to actually perform the operation once it's gotten going. My mistake was that I didn't realize the second number printed out was the time in seconds that the benchmark was reporting, which was ~0.15 seconds on my machine.
Since it happens every 6 weeks, it would probably be trivial to add a recurring event starting at the release date for 1.0.
Awesome. I have a passion project that I never get around to that needs this.
I've started a [thread](https://users.rust-lang.org/t/this-week-in-rust-editors-thread/1806/8) on users.rlo for coordinating TWiR.
Niko wrote some things about the pros and cons of Rust’s `enum` and classes in object-oriented languages: * http://smallcultfollowing.com/babysteps/blog/2015/05/05/where-rusts-enum-shines/ * http://smallcultfollowing.com/babysteps/blog/2015/05/29/classes-strike-back/ … and promises to write about a proposal for a new language feature that, as far as I understand, would enable a form of polymorphism: &gt; The next post will describe a scheme in which we could wed together enums and structs, gaining the advantages of both.
Does anyone know of any alternatives to deuterium? I really want a good SQL query builder I can use with all SQL variants, but I've been having issues with deuterium. The author is also really unresponsive.
A quick note - the nicer fork is just a fork with a build script that has error messages and support for Ubuntu where the `llvm-config` binary usually has the version appended to the end of its name.
This font ... my eyes ... it burns. Sorry, but I can't read that with this "design" :/. Any possibility to change to another font in your blog?
I think `CSemiBox` would be a better rename as it gets the point across.
The font looks fine to me, but assuming you are using firefox `alt -&gt; view -&gt; page style -&gt; no style` works to clean up pretty much any page (at the cost of some formatting, but this page works pretty well). You can then set the font in `alt -&gt; Edit -&gt; Preferences -&gt; Content -&gt; Default Font` (I think that's the right place at least).
Is the book really deeply immutable? Isn't it really "the reference is not unique"?
I haven't seen other such projects.. Have you considered creating a fork?
Just a heads up - the escape characters in your regular expressions appear to have been stripped out.
I've seen it talked about a lot, but I just do not get VSCs appeal. It just seems like another light text editor that mirrors Sublime Text. It builds off atom(?), so it is not like its anything special. Sublime text has had Rust support for a while, through the super easy to use package manager. Is there any reason to try out VCS? Any good features?
Rust can still do unsafe things. And no software can help you if the hardware is compromised... which it certainly can (and indeed might) be. High security is entirely possible but will drastically change how people use computers, which means it has a significant social element as well as technical. It's a much larger problem than "just a new OS". Though it might be an interesting step.
For those who haven't heard of Diceware before, I should point out that part of the point of Diceware is that you roll a physical die yourself and consult the word list manually, so that you don't have to worry about compromised software or weak PRNGs. :) Still, passphrases are great and far preferable to passwords (though I have at times found difficulty in getting services to accept spaces in passphrases (or worse, services that impose some absurdly low *maximum length limit* (which is basically acceptable only for the sake of mitigating a DoS vector, and doesn't need to be less than 100))).
Are you planning on publishing any Rust-related papers yourself with this? :)
This is great, I'm happy to see all the work people have put into Rust's debuginfo is paying off.
This was created because I'm writing [this](https://github.com/Hoverbear/rust-education-paper). :)
Isn't Intel vTune really expensive?
Aha! Have you considered showing this to the person who taught the OS class in Rust a few years ago?
I appreciate this is testing the languages standard libs but would be interesting to see how rust fairs using mio.
Awesome stuff! I like VS Code but find it hard to get used to any editor without a vim mode. Hopefully they'll finalize the plugin system soon.
It’s all about resources (people’s time, money). We (the Servo team) have decided a few months ago that we don’t have enough of those to build a competitive JavaScript engine. (Servo uses SpiderMonkey, Firefox’s JS engine, which is written in C++.) https://github.com/servo/servo/wiki/Workweek-alt-js Disclaimer: opinions are my own, etc. IMO the same reasoning can be applied to a new kernel: we could probably manage making a toy one, but doing it “for real” and maintaining it it such a huge amount of work. We can only do so many “impossible” projects at the same time.
Phil, will this functionality be generically available? Because it would be also the starting point for implementing a correct 'jump to definition'.
**This is so cool**! Just yesterday I was asking about something type-inference related regarding RustDT, which uses racer for auto-complete, and here we are. This invites a few questions: 1. Is it generally possible to get the improved code back into rustc proper? Granted, typeck is not the bottleneck, but any improvement on that front is worth it, especially for people with large code bases. 2. How would the type be reported back? Especially generic or trait-bounded incomplete types could get quite complex, I imagine. 3. Would the collected information also allow to ask racer for compatible types? I could imagine RustDT having a quick fix to "widen" a type in function signatures, e.g. from `Vec&lt;T&gt;` to `&amp;[T]`. 4. Can I help? I'm a newbie, but already a bit into syntax parsing stuff, having written a few lints for rust-clippy.
It's hard to answer that without really understanding things (on the `llvm-alt`, rather than Rust side of things, that is)... the [new approach](https://github.com/aturon/rfcs/blob/scoped-take-2/text/0000-scoped-take-2.md) planned for scoped threads might also be applicable here? 
Just like with many of Rust's features, there is plenty of kernel research that is being ignored in the mainstream. I'd love to see an L4 version of Linux or something like that, regardless of the language.
By HM you mean Hindley-Milner? Rust hasn't been using actual HM since [a while now](http://smallcultfollowing.com/babysteps/blog/2014/07/09/an-experimental-new-type-inference-scheme-for-rust/)... I don't know what's the right way to characterize what it *does* have. 
I iterated a bit from this and made it a crate: https://github.com/SimonSapin/rust-movecell/blob/master/lib.rs https://crates.io/crates/movecell I’m thinking of moving Kuchiki back to reference counting with something like: struct Node { parent: MoveCell&lt;Option&lt;Weak&lt;Node&gt;&gt;&gt;, first_child: MoveCell&lt;Option&lt;Rc&lt;Node&gt;&gt;&gt;, last_child: MoveCell&lt;Option&lt;Weak&lt;Node&gt;&gt;&gt;, previous_sibling: MoveCell&lt;Option&lt;Weak&lt;Node&gt;&gt;&gt;, next_sibling: MoveCell&lt;Option&lt;Rc&lt;Node&gt;&gt;&gt;, data: NodeData, } ... but I’d rather the memory overhead of `RefCell` (which would be 5 extra words per node). Thanks for your original idea!
Much better than before, I too was reading at at least 150%. Maybe add `line-height: 1.4;`, too?
Thanks for the guide. I appreciate people taking their time to help make expeirencing rust an easier experience.
That seems *really* useful.
Cool. Feel free to ping me if you have specific questions :)
There's an open ticket for it.
&gt; Do you think that impl could be auto-generated with a macro? I don't think so; that would involve destructuring the type itself, and macros *do not* have any access to type information whatsoever. Now, that said, I had a similar problem with `error_type!`: I ran into a library error type that didn't implement `Display`, which just ruined *everything*. The solution was: * The `Display` impl for the unifying type forwarded to a generated `ErrorDescription` trait. * Each variant's payload type gets an automatic `ErrorDescription` impl that just forwards to `Display`... * ...*unless* that variant is given a `disp` clause, which allows you to specify a custom impl. Thus, in most cases, you don't need to write a `Display` impl or `disp` clause; it's only when the payload doesn't already impl `Display` that you have to. Now, you could do something similar, though I'm not sure if that *should* be integrated into `error_type!`, or if it'd be possible to work out some sort of forwarding system. It'd be rather cool if you could just say "extend this expansion with the `derive_partial_eq!` macro" and the rest happened automatically. Personally, I just hacked around that *specific* problem by converting `Result`s into `Option`s and not looking too hard at the exact cause of failure. :P
This just calls out to be used in my window manager. Thank you soooo much :D
FWIW, there was some work by mitsuhiko on a crate to introduce stack traces for errors, though it hasn't been worked on for some time: https://github.com/mitsuhiko/rust-incidents.
Thank you very much, now it is much more readable :).
Thank you, too. It really is :)
&gt; The details of such an error are not important to a caller, and and a stack trace would expose implementation details that a caller should not know about at all. That might be a use case for the `Debug` trait.
There are actually massive problems with the Java memory model for not just the the optimiser but also the processor! There are a number of subtleties of the Java memory model which make aspects of it more strong than that of most weakly ordered CPUs (most notably ARM), resulting in the compiler spewing barriers everywhere
**Update**: Having tried this, I *don't* believe it's possible to add an extension mechanism. The problem boils down to *not* being able to compare idents for equality. Everything else is doable, if *abominably ugly*. (Att: /u/Ruud-v-A)
Maybe one could make a pragmatic solution using macros? E g, I could imagine a macro `make_err!(e)` that would call `error!` and return `Err(e)`, and a `try_trace!` that, given an error would call `trace!` and then early return `e.into()`, just like `try!` does. (where `error!` and `trace!` refers to the [log crate](http://doc.rust-lang.org/log/log/index.html))
Interesting, can you provide some examples? My impression was that unordered was explicitly designed to try to make it easy on processors. Is this related to stuff like dynamic class loading?
The biggest obvious one I could come across is the requirement of serialization on final class fields and the consequent necessity for barriers, but they're also required during pointer chasing to avoid the visibility of pre initialisation object contents
It's pretty convenient to have optional logging in your application that can print out a stack trace for you as desired, without killing the app/task - particularly if (for whatever reason) it's impractical to attach a debugger.
I agree. Stack traces are crazy useful in the java world for tracking down where and why something went wrong. You can sort of get by with descriptive error messages, however, a stack trace is preferable imo.
Yea, I benchmarked the whole thing with the test-crate, which seems to automatically compile it with optimizations.
The thing you're missing is that the value type is no pointer, but a bare u32. In this case, Option reserves another byte per value, which LLVM or rustc (I don't know who) pads to 32 bits to get a nice 64bit `Option&lt;u32&gt;` (at least the assembly my system generates makes this appear to be the case).
Of course, that makes perfect sense. For some reason I was reading that as an Option&lt;Box&lt;T&gt;&gt; or something like that.
&gt; 2) I'm not confident about what standard traits my types shoudld implement. I implemented Debug, Eq, PartialEq, Copy and Clone, but I'm not sure if some are useless or if I miss important ones. `Hash` and `Ord`, maybe? There's also [a bunch of `fmt` traits](http://doc.rust-lang.org/std/fmt/index.html#formatting-traits) you could implement for better Display/Debug behavior, e.g. `LowerHex` for `{:x}`. &gt; 3) […] However, I'm a bit reluctant to having a strong dependency for each possibly useful traits that I could support. I though about using the 'features' section in Cargo.toml for that, is it the way to go? I'd try to use Cargo features, too. At least so that my users could disable `rand` support if they don't need it, even if it is available as a default feature.
Ah, okay. I thought it might be something like that--I always wondered how the JVM was actually supposed to enforce those properties :) I figured it might be JIT magic (so the barriers are there initially but get elided later, or something).
I'm pretty sure it's literally an environment variable.
That might work but: i dont know much about how much memory usage would actually be saved. enums are the size of their biggest variant so i think in both cases (`Option&lt;MyEnum&gt;` and OptionMyEnum) would be the same size. and because everything is allocated inline (no pointers) its already at optimum cache friendliness (i think) in fact, the extra generated code would cause the executable size to inflate
Yes I'm hoping too once it's more fleshed out!
Ha, that's an interesting question. I guess it comes from Ruby rarely being a persons first language and Ruby never having reached a level of importance where they can fill a whole space. Also, the Ruby community values curiosity (which also leads to the pesky habit that they _do_ run after every new fancy thing every couple of days).
What is the size of `Some(MyEnum)` (which is arguably the biggest variant) anyway?
Probably the same as the size of `MyEnum` itself, unless there is padding or something to take into account
Racer does already provide 'jump to definition' functionality, although it fails a bunch of the time due to racer's type inference limitations, hence the exploritory work with rustc. When you say generically available, do you mean as a library? 
Why not just `spawn` + `join` (I understand that it would require additional `move ||` sometimes)? Are there usecases where funky does work, but the former not? Can you show them?
Too bad. One day earlier and I might have made it (Hannover lies on my way on the 22.). Maybe next time.
Ah, that’s true, sorry! So in this case you’re stuck with a new type, unfortunately.
It amazes me that more people don't whine about the lack of stack traces in Err() code. The hack I use is to create my own Error type and attach a stack trace on creation using std::rt::backtrace::write(). Then I use the std::convert::From machinery to insure this is generated when try!() converts any other errors into my Error type. Hmm.. maybe I should write a post about this
I bought the book (pdf and paper). I should also state that I have reviewed books for Packt. I read rustbyexample.com, rustforrubyists, everything else I could find and some of the official documentation before the book. I did not type up the exercises or do most of them. I think it might be a good source of exercises to help get started if you are just looking for someone to guide you in doing something. I found chapters 5 and 6 the most helpful beyond what I have already read. I often wish they went deeper on the different subjects. Chapter 5: Generalizing Code with Higher-order Functions and Parametrization Chapter 6: Pointers and Memory Safety "Programming at the Boundaries" chapter 9 seemed to me beyond the essentials to me. I did enjoy the book. I read the book once and then re-read chapters 5 and 6. I haven't picked it up since. I think it would be good for a total beginner. If you are an early beginner I dont know if it will help much. 
Just curious: Can rustc/LLVM detect the dummy switcheroo as a dead store if the `MoveCell` is mutably borrowed or no other code can observe the default value due to other reasons?
&gt; 2) I'm not confident about what standard traits my types shoudld implement. I implemented Debug, Eq, PartialEq, Copy and Clone, but I'm not sure if some are useless or if I miss important ones. `Hash` and `Ord` just like killercup said, probably whatever other num primitives implement would be a good idea to implement &gt; 4) I chose to make the Bigwise trait implement Copy, because all the types that implement Bigwise are actually just structs full of u64. However, because these types can be very (very) large, I wonder if there are any drawbacks to marking them Copy? if im not wrong moving actually is a memcpy (it says so [here](https://doc.rust-lang.org/complement-design-faq.html#no-copy-constructors), the compiler just doesnt allow you to use the old copy, so i think performance-wise there similar (unless LLVM is able to optimise it in some way) the "When should my type implement `Copy`" section [here](http://doc.rust-lang.org/std/marker/trait.Copy.html#when-can-my-type-&lt;em&gt;not&lt;/em&gt;-be-copy?) might be helpful &gt; 5) On a related note, the standard operator traits deal with values and not references. So, my understanding is that calling my own reference-based methods (fn or(&amp;self, rhs: &amp;Bw64) -&gt; Bw64) will be a lot cheaper than calling the corresponding operator (fn bitor(self, rhs: Bw64) -&gt; Bw64). It's a shame because calling a | b is a lot nicer than calling a.or(&amp;b). Did I miss something here? if you implement the trait for a reference to that type (`impl&lt;'a&gt; Add for &amp;'a Bw128`) then it will take by reference, but youll have to use `&amp;a | &amp;b` &gt; 6) I am not very confident about my Iterator implementation. Is it right? Should I implement IntoIter too? IntoIterator seems like a good thing to impl (no real reason not to, that i know of) i see that your `iter()` method takes self by reference, so here also you'd have to implement IntoIterator for a reference (to preserve semantics), you could also probably do a blanket impl for everything that implements `BigWise` (`impl&lt;T: BigWise&gt; IntoIterator for T { ... }`) i dont know much about 7 or 8, but i hoped this helped :) PS. i think this is the longest comment I've ever typed :P
The get is horribly inefficient: It goes throught the whole BST. It should be doing binary search...
I never needed one. It was always pretty obvious where it came from.
AFAICT the dummy value _can_ be observed: if you call peek() again in the closure you pass to peek. Edit: Maybe you meant if both the call to peek() and closure get inlined. In that case you're probably right and it should be dropped.
Excellent! This belongs on crates.io.
Paging /u/fgilcher
I actually figured it out - I was being super stupid with my threading. I wasn't waiting for the threads to finish before the program ended, and so I was getting a panic since they were then held dangling. I got it to work in basically the same way as my comm model. I do find it interesting, and against what I'm doing, that oyashio seems to be more of a broadcast mechanism than a channel. Do you know a way to configure it so that it acts more like a thread-safe queue/channel, where once the data is read by one thread none of the other threads can read it?
We talked about [some of the compiler's internals](http://www.reddit.com/r/rust/comments/396rg3/a_graphbased_higherorder_intermediate/) before, but this paper has a lot more detail on the goals of the language and the cool things it can do.
Added to the [calendar](https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com). Have fun.
&gt; No existing OS/kernel is safe. There's always a trade off between security, complexity and convenience. Creating a new operating system as complex as Linux or Windows and making it "safe" is pretty much impossible, regardless of the language. Think of the mantra repeated by Linus over and over again: "We don't break user space" or "if people rely on it, it's not a bug, it's a feature!". Complex software has bugs, some of them get fixed eventually and some of those fixes cause other bugs to occur. As an example, check out this [video](https://youtu.be/5PmHRSeA2c8?t=2653) where Linus Torvalds argues *against* fixing kernel bugs immediately when they break another application (i.e. Adobe Flash) which relies on these bugs. How does he justify this? &gt; when I can't watch kittens on YouTube, I get upset. Well, he has a bit more to say about this, but the point is: You can't expect something as complex as an operating system to be 100% secure and still usable for the average person for all his everyday tasks.
Bought the paperback and have read it. This book is for beginners. The value of it is in things being introduced gradually throughout it, while the official book breaks into independent sections quite soon. Mind that all the information (and more), is freely available in official book, the official book is actually bigger.
After trying to use Options methods to simplify insert, hitting to a problem and asking in #rust /u/eddyb worked with the code and simplified it: https://play.rust-lang.org/?gist=40a6b3fa5858878dab64&amp;version=stable
Two subreddit moderators are already on the team (me, kibwen=bstrie). Even if we weren't, I don't see why that would need to be done.
It opens communication channels, and gives people a badge. It's useful, and there's no motive why it shouldn't be done.
It works for me. Could you retry please?
play.rust-lang.org doesn't work for me with Iceweasel on Debian. Would you mind pasting it somewhere else please :).
Specifically https://github.com/jonhoo/volley/issues/5 If anyone wants to implement it, I'd be happy to merge.
I almost want to reinstall Linux just to run and hack on wtftw :D Almost :'(
&gt; Types are first-class values Hm.
Cool, I like cell experiments. It looks like `peek` could be expensive in case the memory pointed to is large (several memory copies). Also, recursive peeking could give some surprising results - although it wouldn't be unsafe, you'll see the default value instead of the expected one. Maybe `borrow` would be a better name instead of `peek`, and if you like, you could also consider making this an RAII-guard instead of taking a closure (or maybe that would the likelihood of dual borrow even more common)?
That and `set` could work regardless of whether `T` in `Cell&lt;T&gt;` implements `Copy` or not.
Depends a lot on the size of the codebase, I think. Rust likely doesn't have any mloc+ codebases. If you have an error originating deep within a function that could get called from hundreds of different places, backtraces are invaluable imo.
I'd not be too sure. Last time I compiled something with `rustc --test`, it did *not* get optimized unless I also added `-O` to the rustc command line. Perhaps cargo does this? To be sure, add an empty benchmark: #[bench] fn empty(b: &amp;mut Bencher) { b.iter(|| ()) } If empty comes back with more than 0 ns runtime, your code doesn't get optimized.
I would have made my node class an enum Node&lt;V&gt; { Branch(Box&lt;Node&lt;V&gt;&gt;, Box&lt;Node&lt;V&gt;&gt;), Leaf(V), } or something like that.
I agree that the default assumption should be non-nullable. Unfortunately, structs are also rarely used in C#. Even in the cases where you do want value-semantics, you'll often see people use classes instead. The framework itself doesn't tend to fall prey to this problem, but it's very common in projects.
It does indeed, plus a whole lot of other stuff. 
Check out the [source code and examples](https://github.com/anydsl), too.
`Cell` is it is in `std` right now requires `T: Copy` but yeah, it could probably be generalized.
What if you have only 1 branch?
https://github.com/rust-lang/rfcs/issues/1164
Heads up, your link to Rust after clicking "Home" is broken.
Hey, you're my cup! (Thanks for the list of traits, I never realized that Debug had to be in every public type)
Couldn't you get all the benefits of `T: Default` without actually requiring `Default` by storing an `Option&lt;T&gt;` and replacing with `None`? Edit: wouldn't interact well with `take` and the other methods that rely on a sane default, but if you don't need those ...
I was curious so I ran a trivial benchmark on every method proposed in this thread. https://gist.github.com/mattico/ec2933668b88625a581a I think the results were as expected: test tests::slice1 ... bench: 107 ns/iter (+/- 3) test tests::slice2 ... bench: 9 ns/iter (+/- 0) test tests::slice3 ... bench: 4 ns/iter (+/- 1) test tests::url1 ... bench: 928 ns/iter (+/- 30) test tests::vec1 ... bench: 213 ns/iter (+/- 5) Moral of the story: Unicode ain't cheap.
The tradeoff then is that the type will (often) double in size, since it needs to store the variant flag in most cases.
I found the "Kelvin versioning" link to be more interesting than the article...
Well, old `for` loops are a cool heritage, I think, and I'm a bit sad to see them so neglected by the more modern languages (`Swift` still has a bit of the old `for`, I think). Relevantly, I'm still looking (just out of slightly morbid curiosity) for `Algol`-like loops in a modern language (`while &lt;pred&gt; for &lt;id&gt; in &lt;r&gt; by &lt;s&gt; do &lt;stmt&gt; od`; glorious).
I made little graph implementation based on this and arena: https://github.com/bbatha/movecell_graph/blob/master/src/lib.rs.
I wonder if Halide could be made with rust as a host language... 
&gt;Rust likely doesn't have any mloc+ codebases. …yet.
But it is not based on Rust right? I see no rust code on [their repo](https://github.com/AnyDSL/impala), and nothing like the Rust compiler, and seemingly very small code (which is positively impressive). I wonder if I'm looking at the correct repo.
Omg that missing parentheses help message was my first PR for rust about a year ago now! Glad you like it! I remember when I thought it would have been good to have. 
I bought it, read it, enjoyed it. As good as the official Rust book is, I still had a hard time think like a Rust programmer instead of a Rubyist. Having a fresh presentation of the topics in the official docs helped move me in the right direction and I'm finally feeling comfortable with the language. That wasn't entirely the book's doing but it was a big help. If you're already feeling confident with Rust, I'm not sure it's worth the price. For everyone else, it's worth it for reference, especially since modern, thorough writing is a bit hard to come by. 
fwiw in my experience internal iterators in the world of unboxed closures optimize as well as external iterators, if not better in some cases.
There is also a really relevant tool: [zxcvbn](https://dl.dropboxusercontent.com/u/209/zxcvbn/test/index.html), developed by the folks from dropbox, they mention the same xkcd post :)
I wanted to add question mark in the title, but somehow I'm not able to edit it. Anyway, considering what crazy stuff is able to be ported from C/C++ through emscripten to current browser javascript runtime, looking through WebAssembly MVP and https://github.com/WebAssembly/design/blob/master/CAndC%2B%2B.md I don't see any reason why someone would not be able to produce working servo browser compiled to webasm, only by just simple (yet pretty long) analogy.
It's not like that. It was more of "the subreddit is working great as is". Note that the sub has one core team member, and some other really active people on the mod team. We're all friends here; so it's not some totally-separate universe. If it spins off into uncontrollable politics that's indicative of a much larger problem. 
I think cargo already does the right thing by default – however you may still run a debug/test-executable by hand.
I'll use another language to describe that with two examples: Ruby. &gt; [1,2,3].each { |e| puts e } This is internal iteration. The iteration is a property of the object itself (in this case the Array), the caller has almost no control over the way it works and it cannot be combined with other strategies. The Array itself holds the state of the iteration. It has a simple model, though: you just pass in a callback and it will call you back for every item. &gt; e = [1,2,3].each =&gt; #&lt;Enumerator: [1, 2, 3]:each&gt; &gt; e.next =&gt; 1 &gt; e.each { |e| puts e } ... &gt; e.map =&gt; #&lt;Enumerator: #&lt;Enumerator: [1, 2, 3]:each&gt;:map&gt; (This is an interface oddity in Ruby: each without a closure yields an object for iteration) This is an external iterator: it holds the state of the iteration itself and a reference to the data iterated over. It gives you more control, you can call "next", you can "rewind" (if possible), you can pass it around and other things. This has the advantage that you can combine this with other iterators (see the last line). Rust used to have internal enumeration and switched to external.
You mean a `Branch` with only one child node? That wouldn't be much use for searching, yes?
Nice! [Here is mine](https://github.com/SimonSapin/kuchiki/blob/0387dde49a28595b5be91e6f570de92453a2b087/src/tree.rs#L54-L62). (Though it’s a specialized kind of graph.)
&gt; Arc&lt;RwLock&gt; Or `Arc&lt;AtomicBool&gt;`
As nwin_ says, threads that are blocking on something are not easy to wake up in other ways. If you can set a timeout on the blocking operation and then check an atomic kill flag, that's probably simplest. If you need a more full-fledged solution, you can have a look at [mio's eventloop](https://carllerche.github.io/mio/mio/struct.EventLoop.html).
Right, but as far as I can see, WebAssembly won't enable anything that couldn't have fundamentally been done with asm.js previously. So if Servo in Servo is possible with WebAsm, it was *already* possible with asm.js, and thus isn't news. :P
As it should, because `let` defines bindings, it doesn't assign to lvalues. Imagine if you had: let mut collision = thing(); let (colision, _) = another_thing(); // Whoops; assigned to wrong variable! Any typo would introduce a new variable rather than a compilation error.
Aha yeah I guess. I wanted to do something with macros and this looked like a fun idea. If there's multiple return values, how would you ever know a thread is finished?
I expanded your example code (without the let definition), but now there's another error, which isn't documented :/ struct Test { a: u32, b: u32, } impl Test { pub fn move_vals(&amp;mut self) { (self.a, self.b) = (1, 1); } } fn main() { let mut t = Test { a: 5, b: 5 }; println!("{} {}", t.a, t.b); t.move_vals(); println!("{} {}", t.a, t.b); } Compilation on Rust Playground returns: &lt;anon&gt;:8:9: 8:34 error: illegal left-hand side expression [E0070] &lt;anon&gt;:8 (self.a, self.b) = (1, 1); ^~~~~~~~~~~~~~~~~~~~~~~~~ &lt;anon&gt;:8:9: 8:34 help: see the detailed explanation for E0070 error: aborting due to previous error playpen: application terminated with error code 101 I'm not quite sure why this results in an error ...
Ahh, I accidentally had the let binding. I still have no idea why the left hand side is illegal though
Great project! It would useful to add a `cargo remove` command.
This code results in ~0.06s using cargo bench. #[bench] fn benchdat( b: &amp;mut Bencher ) { let mut contents = Vec::&lt;u8&gt;::with_capacity( 21621888 ); b.iter( || { let mut file = File::open( "image.raw" ).unwrap(); test::black_box( file.read_to_end( &amp;mut contents )) }); } &amp;nbsp; For comparision, this python code does the same in 0.016 seconds, which is a fivefold faster. t = time.clock() data = [] with open( "image.raw", "rb+" ) as f: data = f.read() print( len( data ), time.clock() - t )
Yea, setting the opt-level unfortunately did nothing.
Yeah, that's what I meant. I haven't really used binary search trees, but it seems like unless you have a power of two minus one nodes, you're going to have to have some branches that only have one child.
It seems like maybe the *self.* portion is messing up the semantics that parse the statement. From https://doc.rust-lang.org/reference.html#lvalues,-rvalues-and-temporaries, it seems like it should work
Because `(a, b)` is the syntax for constructing a new value. Rust doesn't *have* unpacking, it has destructuring via pattern-matching.
Rust does not support tuple assignment. Your code creates a tuple containing 2 values, the value of `self.a` and the value of `self.b`, and tries to assign to that (fresh) tuple. That makes as much sense as `1 + 1 = 3;`
Note you committed ` .Cargo.toml.swp` to the repository. I have an excludesfile defined on my ~/.gitconfig that contains entries like *~ .#* .*.swp (I find it better than inserting those generic matches on a per-repository ~/.gitignore)
Not that I think it would matter in this case, but I do not understand why you're using `test::black_box` for a test involving I/O.
Another thing: As `peek` gets an &amp;T, would it make sense to also create a `poke` (sorry for the old basic pun) method that would `take` the value, compute a replacement via given `FnOnce` and `set`s the computed value? E.g. pub fn poke&lt;F&gt;(&amp;self, f: F) where F: FnOnce(&amp;T) -&gt; T { self.replace(f(self.take())); }
Looks like a joke ... &gt; `foo` and `bar` are reserved keywords that don't do anything! Install, and Other download do not leads anywhere.
This should also allow a switch like `--dev` to apply the change to `[dev-dependencies]`.
We need a rust based cms :)
That's really nice, interesting at least. Is the graph immovable? I think the new_node and set_root methods would freeze it in place.
 Nice post! One point about "readiness": &gt; However, several libraries required me to use a bleeding-edge, nightly build of Rust rather than v1.0. .. &gt; In both cases ([clippy], [html5ever]) these libraries depend on compiler plugins. Clippy isn't a library, it's a plugin. You can use clippy with nightlies at development time and make it optional otherwise (use a cargo feature); so that your CI and releases work on stable. Or use `-Z extra-plugin=clippy` while compiling with the correct link flags. html5ever is a library which uses a plugin, but could be made to run on stable with [syntex](https://github.com/erickt/rust-syntex), just that this hasn't been done yet as far as I can tell. I could try to do it today. (The difference between these two libraries is that the first is a plugin intended to help developers of other libraries, whereas the latter is a library which uses a plugin intended to help the devs of the same library) Plugins are going to take a _really_ long time to stabilize (and are an exotic feature most languages don't have -- most use Mako templates or something to achieve the same) because they depend on internals, so I think it's a bit unfair to make this an issue of Rust's readiness. With syntex it's possible to make such libraries stable, too. 
You might want to take a look at the [io reform rfc](https://github.com/rust-lang/rfcs/blob/master/text/0517-io-os-reform.md#revising-reader-and-writer) for some investigation over why the current API was chosen over the old one.
Absolutely. I like using CLIs with subcommands that have similar parameters. Ideally, you could define a whole bunch of new cargo commands as `cargo {deps,dev-deps,build-deps,collaborators,…} {add,remove,list,show,…}` (plus a bunch of options each).
To do that, we need to get the self arg of the peek MethodCall, then look up its Def for later comparison. Then we'd need to walk the ast of the closure arg and see if any Expr touches the same Def. While this is quite a blunt sword, I believe it may already catch a few bad cases. The alternative would require symbolic execution (following possible control flows and seeing if any of them results in a path to our MoveCell), but this is probably breaking a fly on a wheel.
You need to do: impl NodeBuilder { fn build(self) -&gt; Box&lt;Node&gt; { self.root } } instead of ```&amp;mut self```. This way you 'consume' the builder and transfer unique ownership of the root to the caller. Makes sense? PS: If you want to be able to reuse a builder, add a ```#[derive(Clone)]``` on ```struct NodeBuilder```. Then you can build multiple trees from the same builder using ```builder.clone().build()```. (edit: formatting and fixed code thanks to /u/Quxxy)
Ah yes, that is a good idea. It would take some reworking to be in the library-- perhaps as both a broadcast or unique stream. I feel like the library needs a bit of a rewrite anyways, but I'm not sure when I'll get to that point-- I was sort of waiting for streaming iterators. If you want to try this, using the [latch from promise](https://github.com/viperscape/rust-promise/blob/master/src/latch.rs#L45) would be a good idea. It's a primitive to signal across threads.
[rust-clippy](https://github.com/Manishearth/rust-clippy) already contains a few lints that perform similar 'compiler magic', and it's a library on crates.io.
Ok, fair enough. Still, I’d rather keeping this library on the stable channel if possible.
There is no need to do: let inst = ... return inst; Simply create the struct and leave it for return: Edge { ... } Also, I would think that if the edge will have references to each vertex, then each vertex might also have separate lifetimes, so 'a and 'b: [rust play](http://is.gd/9BYCPa)
Yes, you're right, just mindlessly changed OP's code. Edited to fix, thanks!
As Manish wrote in another [comment](https://www.reddit.com/r/rust/comments/3a7zxb/exploring_rust/csalc2x), you can use an external nightly-requiring crate as a dev-dependency and still build on stable. Granted, people would need to add the lint to their project to benefit from it.
You can `Vec::new` instead of `vec![]` as well...
Someone also made an external crate that gives a very simple i/o interface that panics on error, allocates unneccesarily, and all that stuff. I can never remember its name though :(
Alright, I tried to reproduce again, this time locally with hyper and the latest rust nightly but failed. The following code compiles without problem: use hyper::client::response::Response; use hyper::header::ContentType; use hyper::mime::Mime; use hyper::mime::TopLevel::Application; use hyper::mime::SubLevel::Json; extern crate hyper; pub fn test(mut res: Response) { match res.headers.get() { Some(&amp;ContentType(Mime(Application, Json, _))) =&gt; (), _ =&gt; (), } } Do you have `extern crate mime` somewhere in your code? If so, ensure it's the same version hyper uses or use `hyper::mime` instead. This is the only other thing I can think of which might be the problem.
Because I currently don't have a Linux installation. I used to.
What class of parser is it? i.e. LR(1), PEG, GLR, something else? Or does that depend on the meta-rules? Seems very meta :)
It's called readext. But IMO this is the kind of thing which does *not* belong in a crate. It's a very simple addition with no maintenance overhead and universal, practical use. It should be in std.
Thanks!
&gt; Do you have extern crate mime somewhere Ah ha! That was the problem. When I use ``hyper::mime`` all is well. Thank you sir! I somewhat suspected this. But Hyper's own documentation seems to be referring to ``mime::Mime``.
VSC is free, sublime is not (nag screen). VSC has put a lot of effort in language integration (currently for TypeScript and C#), which is probably a good fit for rust.
Thank you! I had read the phrase "consume" in reference to the Builder pattern but I didn't really understand the implications. Yes, it does make sense.
No problem!
I thought I saw people saying you couldn't compile Rust to Javascript yet?
They're pretty awesome. Outside of just being necessary for unboxed closures, they give you some of the neat parametricity properties found in languages like Haskell, which let you do (limited) proofs about safe code. I am currently using them to make unchecked indexing safe using a similar technique to this: https://doc.rust-lang.org/src/collections/btree/map.rs.html#615. How I do it: I use an invariant lifetime on a wrapper type (`Array&lt;'a&gt;`) for a (nonresizable, at least in the closure) array. I make sure the only way to receive an instance of the wrapper type is through a closure with a higher ranked lifetime (using the same trick I linked above). This ensures that there is at most one instance of `Array&lt;'a&gt;` for any `'a`. Then, I create a wrapper type representing checked indices, `Index&lt;'a&gt;`, which also includes an invariant lifetime marker type. The only way to construct one is to pass it a reference to `Array&lt;'a&gt;` and an index, which it bounds checks (in this case, by wrapping around too-long values, which is appropriate behavior in this instance since it's part of a hash table). Finally, I create a safe method, `get&lt;'a&gt;`, which takes an `Index&lt;'a&gt;` and an `&amp;mut Array&lt;'a&gt;` and performs an unchecked index into the array, returning a mutable reference into it. Because of the lifetime invariance, I don't need to store any additional information (e.g. an extra reference to the array) with each index, or compute bounds checks more than once (so I can pass the indices around and reuse them as many times as I want within the closure), but I still achieve complete memory safety. This is mostly useful when you have an array that is not resizable (at least, not in the section you're in), but still allows you to mutate the entries (for example, if you have a `&amp;mut [T]`). If your array is totally immutable then this technique is a bit less useful, since you can just pass around references directly. That being said, this technique isn't just useful for bounds check elision. It lets you deal with all sorts of situations where a safety property of some value is checked at runtime (so you want to represent "the value is checked" by a type), and doesn't need to be rechecked for that value, but you also don't want to have to carry around the value everywhere with the "value is checked" type. For example, you could use this to avoid `unwrap()` on an `Option&lt;T&gt;` that you know is never null after a certain point. Obviously, in all cases, this is less safe than just doing the check every time, so if you use this technique it's important to keep the properties you're trying to enforce simple and obvious enough that the unsafe code responsible for enforcing them can be verified. **Edit**: [I decided to quickly whip up a variant of this API in the playpen](http://is.gd/is7RSY).
I regard all of /u/burntsushi's [Rust](https://github.com/search?q=user%3ABurntSushi+rust) as must-read. ed: networking-related, [libpnet](https://github.com/libpnet/libpnet) is interesting.
Hey, thanks for this. Writing docs for this is something that's been on my list, but I wanted to talk to someone who actually uses such things...
This is a nice way of looking for some Rust code! It seems that `user:XXX language:rust` also works.
I've found it to be very useful in practice. Reading the whole file at once from the getgo lets you do things like index directly into the buffer from a parser, rather than copy. I would add that I wholeheartedly approve of the current API.
Sure, that will work, but it still won't get you an even number of nodes, as you'll run out of single child branches. For example, how would you have a tree with four nodes?
The examples would be easier to read if you used raw string litterals.
Since rust will be able to compile to web, rust will may be a langage of choice for the web !
`read_to_string` is not necessarily for reading a whole file. I usually take a subset of the buffer with the `Take` adaptor, and call `read_to_string` on it.
Destructuring and unpacking shall always be the same thing to me. The point remains, though - why can't we destructure into arbitrary locations? `(a, b)` is only tuple creation syntax in an rvalue context, after all.
I don't pay THAT close of attention, but my understanding is that patch is still in the queue, this just landed first.
Is this code actually being run in nightly, or just in tests?
The project page for rsedis states that: &gt; "Unlike Redis, rsedis does not rely on UNIX-specific features. Windows users can run it as a replacement of Redis. but Redis works fine on Windows: https://github.com/MSOpenTech/redis/releases
Do you think, we will be able to trash all the css/html/javascript and replace all of it by rust code ? It would be awesome !
That's a fork of redis.
Right. There are two keys here that enforce this property. One is the higher-ranked lifetime in the closure, and the other is invariance. Both `Array` and `Index` are invariant with respect to their `'a` lifetime. This means that `Array&lt;'a&gt;` cannot unify with `Array&lt;'b&gt;` if `'a` outlives `'b` or if `'b` outlives `'a`. https://github.com/rust-lang/rfcs/blob/master/text/0738-variance.md has more details on how invariance is established in Rust; in my case, I used `PhantomData&lt;*mut &amp;'a ()&gt;`. The reason you still need the higher-ranked lifetime, not just the closure and invariance, is that otherwise there is nothing stopping you from creating two `Array`s and having their lifetimes unify (e.g. one in an outer and one in an inner closure). But by requiring the lifetime to work for *all* `'a` (i.e. not depend on any specific `'a`), you prevent it from ever unifying with any lifetime *outside* the closure. As long as the only way to create an `Array` involves creating a new closure and receiving it with a parametric lifetime like this, one of the closures will always be "outside" the other. (The `BTreeMap` link I gave above has a probably better explanation of this).
It already exists; it works like `tree` rather than `ls`, but it is called `cargo ls` https://github.com/jakub-/cargo-ls
https://github.com/WebAssembly/design/blob/9974cded38e762aa6e00098a7d1e6b6f0c270ba0/FutureFeatures.md If post-MVP features will be addressed, you will be able to ditch javascript and use any weapon of choice, Ruby, Rust, Python, C, C++, C# or Java. But there will be lots of things that will need to happen in the mean time, first WebAassembly needs to be ready, then you need to see specific language implementation that is based on WebAssembly runtime.
I think I got it. I wrote the following playground: https://play.rust-lang.org/?gist=f6e482ac27e4b1a433bb&amp;version=stable that implements a really basic "unchecked indexing" into `Vec&lt;T&gt;`.
Yep, you've definitely got it :)
I also survey some of the details here: http://cglab.ca/~abeinges/blah/rust-btree-case/#the-cool-impl
I'm pretty sure it does completely avoid index-out-of-bounds, unless the logic inside `checked` is incorrect. In the implementation I'm actually using, which wraps around, it can't ever fail to return a valid index, either. Unless you are making a point that is too subtle for me.
It will work for any binary file format where the length (in bytes) of a field is stored alongside the content, which is either ascii or utf8. This happens to be the most common way to store text in binary file formats, in my experience. `.chars().take(n)` would work if we stored the number of chars instead of bytes, but I never saw that convention used anywhere (and it has obvious dowsides). Edit: now that I think about it, `take(n).chars().collect()` would be a reasonable replacement for my use case. Thanks!
No signup needed. Just show up and enjoy :)
I'm **really** excited about the work on MSVC. I can't wait to move my project over to MSVC and try it out.
There is no demo since it's unfinished and not integrated into firefox in any way. It's ~400 lines of code described as "This isn't functional yet", but it was deemed easier to develop further when it's checked in to the official source tree.
`cargo web`. Please make it happen.
I have no real connection to Firefox, so don't take this as some sort of official proclamation. As you note, XP users don't get operating system fixes. Is not giving them browser fixes either somehow going to help them more? 
Well, to keep it close to an actual sentence, I'd want to write `cargo add dep git2`. But to me, commands as sentences is not as important as code and logical structure; so, I'd rather create a hierarchy like cargo -&gt; deps -&gt; add (cargo -&gt; part -&gt; action).
Really importantly, I think, IE on XP is especially insecure since it relies on the built in windows crypto libraries which aren't getting patched either. Firefox uses its own crypto so it has the opportunity to be secure on XP.
In my opinion by supporting software on XP you are essentially encouraging people to keep using it when they should really be upgrading.
So with Rust now part of the pipeline in Firefox builds, is it possible for new features/components/js-interfaces to be implemented in Rust now?
How long until the rust compiler is compiled to the web so people can compile rust on the web to the web?
Not giving them any updates at all might force them to move on. There comes a time when it's just not doing anyone any favors.
Meta rules are just Rust objects that you can combine. You can use them with function/closures to generate rules in the language you want. However, using the meta language is easy because if you can imagine a language solving a problem then you just write it down and figure out the rules afterward. The rules tell you what data structure to use, so you create those objects in Rust and convert from meta data to Rust. If you have dynamic objects then you can add features without restarting, just like scripting.
If their concern is security they could always alert users to the fact that their OS is no longer supported. It's a tough situation, I sympathize and their decision is not necessarily the wrong one. But I think there are better alternatives.
That's interesting. Any idea what could cause the slowdown? Your multithreaded approach should, theoretically, have the advantage with many concurrent clients. Excessive locking?
at least in this case, I feel like you don't gain anything because there's a logical grouping either way. but in this case you gain sentence structure which (imo) makes it easier to compose and discover the command you're looking for.
Excessive locking is definitely possible: I have one global lock for the database. But I believe it is not, I don't have benchmarks to support it at hand (I'm pretty sure I ran the same benchmarks just doing pings without locks and got similar results), but having 4 cores to do 100 tasks (50 clients and 50 thread serving them) comes with a price. Check out this project https://github.com/jonhoo/volley/ and maybe this thread https://github.com/jonhoo/rucache/issues/2 that goes into further details.
From my very limited 10 minute look, my guess would be the fact that rsedis doesn't use non-blocking I/O. At least from my cursory look, rsedis seems to rely on single thread per connection [1]. Meanwhile, Redis is based on non-blocking I/O [0]. Basically, the same difference between Apache and Nginx If you aren't aware of the difference Google will do a better job of explaining it than I would, but the simplest explanation is that: - More threads means more context switching / harder to schedule - More threads means more RAM; More paging; Fewer cache hits; However, all of that is just theory. In practice, single thread per connection should easily be able to scale to your 4 core MacBook. [0] http://redis.io/topics/internals-eventlib [1] https://github.com/seppo0010/rsedis/blob/15eb1425ed4bd8acc93a30622e0d62a2030596ef/networking/src/lib.rs#L190
Correction: Ctrl+C generates `SIGINT`, not `SIGKILL` (`SIGKILL` is what `kill -9` generates and cannot be intercepted.) Judging by the unstable declaration on [this function](https://doc.rust-lang.org/nightly/libc/funcs/posix01/signal/fn.signal.html), you'll find a binding for the POSIX `signal(signum, handler)` function in the [`libc`](https://crates.io/crates/libc) crate on crates.io. * http://doc.rust-lang.org/libc/libc/funcs/posix01/signal/fn.signal.html * http://doc.rust-lang.org/libc/libc/consts/os/posix88/constant.SIGINT.html
Yes, you are correct, and I'm aware of that. Rust does not support cross-platform asynchronous I/O and that's why I'm using one thread per connection.
A is for area.
Here's another presentation of basically the same technique: http://okmij.org/ftp/Haskell/eliminating-array-bound-check.lhs
I asked. The answer was: based on usage numbers, we’d drop desktop Linux before we drop Windows XP.
Okay, I'll take a look at that when I get a chance. I'm super busy right now, and just playing around with a lot of the Rust stuff. I do want to build a couple of threaded data structures, though, that are very very fast and thread safe. But that's another story. At some point, I'd love to get into doing development on Rust libraries, I just don't have the time right now. Oh well.
A is for area, T is for the team they relate to. B is about where they belong.
[See this StackOverflow answer](https://stackoverflow.com/questions/30555477/try-does-not-compile/30556164#30556164).
Does anyone know if WebAssembly will remain single-threaded, like js?
This is common misconception. XP is still in support, just not for non-commercial or small business users. Custom support plans are still available for organizations that can and want to pay. Those often use Firefox instead of IE to get a modern browser. http://www.pcworld.com/article/2139929/windows-xp-support-will-be-available-after-april-8-just-not-for-you.html
Is [this](https://hg.mozilla.org/integration/mozilla-inbound/rev/8d802bb49f33#l1.176) recommended way of dealing with panics in interop code: // Parse in a subthread. let task = thread::spawn(move || { read_box(&amp;mut c).unwrap(); }); // Catch any panics. task.join().is_ok()
It is, unfortunately, the only *stable* way to do it.
Using the `Rust` and `RustAutoComplete` plugins, code highlighting works correctly for me. Could you take a screenshot of what you mean?
Well, yes if you’ve the length this will work. I was just thinking you wanted to read "a few" bytes from the utf8 stream which will fail unless it only contains chars from the ascii subset.
This is a massive overgeneralization of the issue. Not everyone can afford to upgrade. Not everyone has infrastructure they can easily upgrade to a new OS. (Hell, I work at _Microsoft_ and &lt;some project&gt; only builds on &lt;a very specific non-new OS version&gt;). Besides, upgrading an OS is a major process. If I have an XP machine in front of me and want to use a modern browser, having to upgrade is not a solution in most cases. If there were major XP issues dragging Firefox down, sure, it would be justified to drop XP support. But cutting support to force users to upgrade (something which is the job of Windows/MS) isn't right. 
There are more XP Firefox users than Linux, IIRC. Encouraging people to upgrade is Microsoft's job, and they probably have done their fair share of nagging. See also: http://www.reddit.com/r/rust/comments/3ab5bc/land_initial_rust_mp4_parser_and_unit_tests_in/csbn7pp
You may be interested in the [`byteorder`](https://crates.io/crates/byteorder) crate.
Oh, ok. You're creating your own ALSA binding. In that case I would avoid `io::{Read, Write}` because it forces your users to write unsafe code to read/write i16 samples. This encoding issue is something the users of such a library should not have to worry about.
You do not need html and css to create DOM and set style properties. Or am I wrong? So getting rid of html/css does not mean getting rid of DOM/styles.
Yep, probably. I figured using the standard read/write traits would be ergonomic for the library user, but it seems now that they are not, and that Read/Write works better for the former case (platform-independent and portable) rather than mine (platform dependent, performance sensitive). Back to the drawing board. EDIT: I wonder if there's a smart way to make the read/write API generic over the sample format. I could certainly do fn read&lt;T&gt;(&amp;mut [T]) -&gt; Result&lt;usize&gt; But the question is what bounds one must have on `T` to make this a safe function. I was thinking `T: Copy` first, but that won't do, because immutable references are `Copy` (and reading into that would turn into random memory pointers).
Imagine the resounding ***YES!*** on arewewebyet...
&gt; { "key": "shift+cmd+r", "command": "workbench.action.tasks.runTask" } &gt; Any more direct findings ?
So when MIO supports windows, you'd consider switching to that?
https://github.com/WebAssembly/design/blob/master/PostMVP.md#threads
I'm just using /// Returns on error, converting the `Err` value to `String` and prepending the current location. /// /// cf. http://www.reddit.com/r/rust/comments/29wwzw/error_handling_and_result_types/cipcm9a #[macro_export] macro_rules! try_s {($e: expr) =&gt; {match $e { Ok (ok) =&gt; ok, Err (err) =&gt; {return Err (format! ("{}:{}] {}", file!(), line!(), err));}}}} /// Returns a `Err(String)`, prepending the current location (file name and line number) to the string. /// /// Examples: `ERR! ("too bad")`; `ERR! ("{}", foo)`; #[macro_export] macro_rules! ERR { ($format: expr, $($args: tt)+) =&gt; {Err (format! (concat! ("{}:{}] ", $format), file!(), line!(), $($args)+))}; ($format: expr) =&gt; {Err (format! (concat! ("{}:{}] ", $format), file!(), line!()))}} until something better comes up.
Loving this new format. I've been feeling very out-of-the-loop without the weekly meeting notes to keep me in touch with what everyone is working on.
&gt; I wonder if there's a smart way to make the read/write API generic over the sample format. I [tried something like this](https://github.com/sellibitze/rustaudio) but it stopped working during Rust updates.
I do that, but I have error in err... fn main() { let port = 12345; let mut socket = UdpSocket::bind(("127.0.0.1" ,port)); if socket.is_err() { let err = socket.is_err(); println!("failed to read from stdin : {}", err); let y = in_i32(); return; } let mut buf = [0,3]; for i in 0..buf.len(){ buf[i]=i; println!("{}",buf[i]); } //socket.send_ socket.unwrap().send_to(&amp;[7], ("127.0.0.1", 23451)).unwrap(); let y = in_i32(); } 
Interesting, but I don't think there is any correct bound you could use :(. Copy + 'static is not enough since you could still have static references. AFAICT there is no default bound which would cover usize, but not &amp;'static ... Maybe we need an OIBIT: unsafe trait Pod: Copy {} unsafe impl Pod for .. {} impl&lt;'a, T&gt; !Pod for &amp;'a T {} impl&lt;'a, T&gt; !Pod for &amp;'a mut T {} // These are optional as they do not actually cause memory // unsafety since a deref requires `unsafe` impl&lt;T&gt; !Pod for *const T {} impl&lt;T&gt; !Pod for *mut T {} 
In this method: fn as_bytes(&amp;[i16]) -&gt; &amp;[u8] ...then Rust does not allow the resulting array to be used when the original goes out of scope. But do I need to make it a method for Rust to understand that the lifetimes of both references are the same? 
And 0./0. is NaN.
More common methods that panic on error: vec::remove, refcell::borrow and _mut.
Oh, thanks! I was using a simpler version, that's a good one :) To be honest, it feels like the Rust error handling really needs some more love. 
nice!
I'm no rust expert, but I had a lot of fun (and some frustration) figuring out the type system for this particular problem, so I thought I'd share what I ended up with. I'd like to write more in the future, so let me know what you think!
This is implicitly the same as: fn as_bytes&lt;'a&gt;(&amp;'a [i16]) -&gt; &amp;'a [u8] which means that the lifetime of the returned slice is the same as that of the parameter slice. Is that not what you want? Edit: Using the resulting slice after the original goes out of scope would be a potential use-after-free memory safety issue.
Is there not enough information from the Result? To get the reasons for the error it could be any listed [here](http://doc.rust-lang.org/std/io/enum.ErrorKind.html) and here is how you would get it: match std::fs::create_dir(&amp;Path::new("/bin/rm/boop")) { Ok(_) =&gt; { /* meh it worked */ } Err(e) =&gt; { match e.kind() { std::io::ErrorKind::PermissionDenied =&gt; {}// awh snap!, _ =&gt; {} //fill it in yourself :p } } I dont know much about mkdir(2) but im pretty sure that ErrorKind has what you need.
Just note that you can’t call Box&lt;FnOnce()&gt; yet. You need to use a Box&lt;FnBox()&gt; for that.
Always remind myself: I am an old dog and Rust is a new trick.
&gt; First I try `std::fs::create_dir`. I do not find any way to distinguish the reasons for error. create_dir returns an [io::Result](http://doc.rust-lang.org/std/io/type.Result.html) which typedefs to *Result&lt;(), [io::Error](http://doc.rust-lang.org/std/io/struct.Error.html)&gt;* which provides an [io::ErrorKind](http://doc.rust-lang.org/std/io/enum.ErrorKind.html). I would expect you can just match on `io::ErrorKind::AlreadyExists` to know if the directory creation failed because the directory already exists, something along the lines of http://is.gd/Pz1Gvy fs::create_dir(path).or_else(|e| match e.kind() { // ignore AlreadyExists ErrorKind::AlreadyExists =&gt; Ok(()), // otherwise passthrough _ =&gt; e }) 
I thought B was "blocked by ..." 
Here's a noob friendly version of @Sinistersnare's answer that I finally adopted. fn awesome_mkdir(path: &amp;str) -&gt; std::io::Result&lt;()&gt; { let err = match std::fs::create_dir(path) { Ok(_) =&gt; return Ok(()), Err(e) =&gt; e }; match err.kind() { ErrorKind::AlreadyExists =&gt; return Ok(()), _ =&gt; return Err(err) }; }
Divison by zero is also prevented in safe Rust (technically). From the description in the blog post, at least one of the errors was due to improper UTF-8 validation. So that brings it to 20.
Or negative infinity, for negative number/positive zero, and vice versa.
In addition to that: rust-afl is really easy to use. I think infinite loop are the most evil category you can trigger in Rust. Leads to a stalled program and if you’re unlucky you’re running out of memory pretty quickly. It was the only bug AFL catched in one of my libs I really didn’t anticipate.
Slightly less n00b-centric, but cleaner to my eyes are either: fn awesome_mkdir(path: &amp;str) -&gt; std::io::Result&lt;()&gt; { match std::fs::create_dir(path) { Ok(_) =&gt; Ok(()), Err(e) =&gt; match e.kind() { ErrorKind::AlreadyExists =&gt; Ok(()), _ =&gt; Err(e) } } } or fn awesome_mkdir(path: &amp;str) -&gt; std::io::Result&lt;()&gt; { match std::fs::create_dir(path) { Ok(_) =&gt; Ok(()), Err(ref e) if e.kind() == ErrorKind::AlreadyExists =&gt; Ok(()), e =&gt; e } }
Oh, yeah, my bad with the signal name, thanks! Dumb question: How would I actually use the `signal` function? It looks like it takes two ints?
That's why I said "technically." The division doesn't ever happen, because every division is checked and a panic is thrown if the denominator is zero. It's obviously not what people *really* want, but it's fundamentally the same as "unwrap() on a `None`", which prevents a null dereferences. Either way, the really important part is that Rust avoids UB here.
Ok, I have now given this a thought and implemented something similar. Fixing the sample format at pcm object creation time is just too far away from how ALSA works, but for the primitives `i16`, `i32` and `f32` there are now [special IO functions](https://github.com/diwic/alsa-rs/blob/master/src/pcm.rs#L110) that verifies the sample format and only gives access to the IO object if it matches. The IO object now has functions for [reading/writing from slices of those primitives](https://github.com/diwic/alsa-rs/blob/master/src/pcm.rs#L149). The `u8` buffer fallback needs to stay though, for the cases where you have a format that does not directly match a primitive (e g, big-endian formats on little-endian archs).
What's unsafe about casting pointers to bytes? The data would be junk, but there's nothing *unsafe* about that.
I guess it lets you set SIG_IGN / SIG_DFL, but you can't pass a function without using `mem::transmute`. It could have used an abstract data type, but it doesn't look like much time was spent defining rust signal handling yet. man signal: "The behavior of signal() varies across UNIX versions, and has also varied historically across different versions of Linux. Avoid its use: use sigaction(2) instead." Maybe use sigaction() from the nix crate. At least, it has a more sensible signature. Read [this thread](https://users.rust-lang.org/t/unix-signals-in-rust/733/6) for an example &amp; a reminder that signal handlers aren't allowed to do very much. Fortunately `kill()` is on the list of safe functions (says `man 7 signal`; conformance posix 2004).
`std::mem::copy_lifetime` does this, though I see it has recently been deprecated without an obvious replacement. But depending on the context you might not need it. If there is already a named lifetime `'a` you can assign to `&amp;'a [u8]`: fn foo&lt;'a&gt;(input: &amp;'a [u16], ...) -&gt; ... { let bytes: &amp;'a [u8] = unsafe { ::std::slice::from_raw_parts(buf.as_ptr() as *const u8, buf.len() * 2) }; 
Casting pointers to bytes is not unsafe. Casting bytes to references is unsafe, i e, the other way around, and to references rather than raw pointers (which are unsafe to dereference instead).
It is not entirely clear to me what you want, exactly. Your code example does not appear to use `old_node`, and where does `new_node` come from? Could you summarize your goal, focussing on the *what*, not the *how*? :) Edit: your code could really use some comments, btw :p
What I'd like is to be able to convert one graph into another e.g. fn times2_1(i: u8) -&gt; f32 { i as f32 * 2.1 } === 1u8 =&gt; apply times2_1 2.1f32 / \ / \ 2 3 4.2 6.3 As a related thing (I'm pretty sure if I have map working like I'd like I can implement this), I'd like to be able to aggregate parts of the graph like so: 0 =&gt; aggregate over average 0.5 / \ / \ 0 0 0.5 0.5 / \ / \ / \ / \ 1 0 0 1 1 0 0 1 &gt; Edit: your code could really use some comments, btw :p This is the minified example from a larger type I didn't backport some of the comments :). I've updated the repo here with some.
We (Dropbox) are quite possibly the ones who horrified Alex, since we just told him recently about the usage. :-) We have a custom `Error` type that snapshots backtraces for pushing into a common clustered aggregation system. We just switched over to his crate over the last couple of days.
Thanks for doing this! We did switch over our internal stuff to use it.
Mapping works like this: * You have created an iterator, which will present each object in the collection (in this case, each node in the graph) one at time. * A `.map()` takes a function which takes 1 node as an argument and can return anything. So you can map `times2_1` over the nodes in your graph, but then you have an Iterator of nodes, rather than a graph of nodes. You'd need to be able to call `collect::&lt;Graph&gt;()` on them (which would mean `Graph` would need to implement `FromIterator` in a manner which will maintain the structure of the graph; this is problematic when you have multiple ways to iterate over the graph). You'll probably want to fold() rather than map(), so that your function is responsible for inserting the new nodes in a manner that maintains the graph's structure. Alternatively you can iterate over mutable references to the nodes, and mutate the graph in place, but this will make it very tricky to do something like your second goal, which will need references to child nodes.
I know rust has issues directly interacting with more abstract and unsafe structures like linked lists, trees, etc. So if you have them working you likely have an underlying vector or slice to store the real values, while an abstract interface 'pretends' to be the abstract structure while offering similar performance albeit with a higher memory cost. Your generic map operator (provided no positions are changed) would just be a function that wraps calling " map " on the underlying array/slice. I don't know what to say about the average operation, that's structure aware so you'll likely have to implement it yourself. 
&gt; You'll probably want to fold() rather than map(), so that your function is responsible for inserting the new nodes in a manner that maintains the graph's structure. I've been toying with a fold example, but its not clear to me how to make the fold aware of moving back up the tree, e.g. fn change_datum(i: u8) -&gt; f32 { i as f32 * 2.1 } let graph2 = Graph::new(); // Could make the accumulator a stack but when do I know to pop it? All of that info is // in the DfsIterator adapter and I can't access it in the fold. let graph2_root = graph.fold(None, |last_parent, old_node| { let new_node = graph.own_node(Node::new(change_datum(old_node.datum)) if last_parent.is_none() { Some(new_node) } else { Some(last_parent.add_edge(new_node)) } }).collect(); graph2.set_root(graph2_root);
I would be wary about tying this into iterators much. I wouldn't write `FromIterator` or `IntoIterator` for this, for example. Linear traversal doesn't really work on its own, and traversal over nodes requires traversing over some already-instantiated graph! The most obvious method of implementing `map` would be to implement it directly on graphs. This might not be ideal, though, since graph.map(f).map(g) will create an unwanted intermediary. You might want to, then, create an abstract `GraphView` trait that abstracts graphs *without* creating copies. This could allow something like Graph::from(graph.view().map(f).map(g)); Not all operations are amiable to this design, but if you only want lazy maps, filters, zips and a few others this should be sufficient. It will be more work than a strict map, though.
Well you can do parallel codegen and LTO in the sense that C does it, no? Compile lots of modules in parallel to IR (inc. some optimisations), then do LTO at link time? Or am I missing something obvious here?
Pretty much, yeah. To put it another way, `GraphView` can just be a specialized iterator (eg. DFS) that guarantees that any transformations result in another valid graph. (Note that you probably don't want it to actually *be* an iterator, since it makes seeking `O(n)`.)
So what you want is something like the following: fn map&lt;'a, 'b, T: 'a, U: 'b, F: FnMut(&amp;T) -&gt; U&gt;(graph: &amp;'a Graph&lt;'a,T&gt;, into: &amp;'b Graph&lt;'b,U&gt;, mut f: F) { let mut map = std::collections::HashMap::new(); into.set_root(Node::new(f(&amp;graph.root().datum))); map.insert(graph.root() as *const _ as usize, into.root()); for old in graph.dfs() { let new = map[&amp;(old as *const _ as usize)]; old.edges.borrow().as_ref().map(|edges| { for &amp;child in edges { new.add_edge(map.entry(child as *const _ as usize) .or_insert_with(||into.new_node(Node::new(f(&amp;child.datum))))); } }); } } I couldn't get it to work by returning a `Graph&lt;'b, U&gt;` because it seems that using the `new_node` and `set_root` methods (needlessly?) root the graph in place, forever making moving impossible.
It depends a lot on how the Graph is implemented, but probably you want to fold(Graph::new(), |graph, node| { ... }), inserting each node you create from the node in the old graph into the new graph correctly. If this closure can fail, then you'll want fold(Some(Graph::new), ||{}) or fold(Ok(Graph::new) ||{}).
Why do you multiply the first byte of the port by 256?
Iterators aren't structure-preserving, and for efficiency a graph view (probably) also needs to have stateful iterations where the state is public, which means it needs to give a known iterator type. Further, the majority of iterator functions will do the wrong thing on a graph view. `zip`, for instance, will just make a mess of things. `chain` on a DAG is an extremely strange thing to want. Etc.
I would not even remotely consider doing something like this without a good green threading library. Speaking of which, is there one yet? What about STM?
I probably can't give a sufficient explanation, but I highly recommend reading [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/) to see the best kind of concurrency model. Basically, green threads allow you to write your server (BT clients are basically servers) as though every client gets its own OS thread. You don't have to worry about what the other threads might be doing. They're just little robots doing their little task that live and die with the client connection. You can spawn 100,000+ of them with very little overhead, 1 or 2 for every connection. An awesome thing about Haskell is that if a green thread makes a blocking system call, Haskell's runtime will automatically spawn an OS thread to wait for it to finish before resuming the thread. Then with abstract data types and channels you can create domain specific protocols for the threads to talk to one another, with no overhead. STM allows you to fire-and-forget safely in a way that doesn't block.
You can't. That would involve there being *both* a mutable *and* an immutable pointer to the same storage at the same time, which Rust is specifically designed to prevent. For this, you will need some kind of interior mutability. Here's the code re-worked to use `Cell`: use std::cell::Cell; fn factory&lt;'a&gt;(val: &amp;'a Cell&lt;u32&gt;) -&gt; Box&lt;Fn() -&gt; u32 + 'a&gt; { Box::new(move || { val.get() }) } fn main() { let val = Cell::new(0u32); let f = factory(&amp;val); val.set(val.get() + 1); assert_eq!(val.get(), f()); } **Edit**: You may have deleted your comment, Mr Anonymous, but *I* still saw it. ;) For those who might be wondering the same thing: you can't just capture `val` by value, since the point of this exercise (as implied by the assertion) is for the closure to return the *current* value of `val`, not the value it had when it was captured.
*Client Side™*
Thanks for this. The characterization of lazy vs incremental really crystallized some things for me.
For Error types, I love if the [Error trait](http://doc.rust-lang.org/std/error/trait.Error.html) is actually implemented. That way, chains of errors can nicely be kept intact using the `cause()` method.
&gt; `_ =&gt; panic!("Bad message id: {}", id)` That sounds like a pretty bad idea. Isn't this remote-controlled input?
As someone who's written a bittorrent client using mio, and another one using green threads (in Haskell): while I agree that threads probably give you the simpler programming model for this, green threads aren't really necessary here. You don't want "100,000+" threads, because you really don't want your bittorrent client to have that many connections open. Most consumer-grade NAT routers will be very unhappy if you have more than a few hundred (at most) TCP connections open, and at that level the overhead of native threads isn't a big deal. So it's fine to just have a native thread per connection, no green threads required.
Yes, that's parsing peer messages. But since they're using a thread per connection, that will just terminate that peer's thread/connection, which is what you would want to do anyway.
Ah, I'd obviously misunderstood which part was slow; I thought it was the individual crates that were slow to optimise initially!
As I said elsewhere in this thread, I don't think it's a real problem. You probably don't want too many open connections anyway (consumer-grade NAT routers really don't like that), so the total overhead should be fairly small. Yes, that argument depends on certain assumption about the cost of an OS thread and your network hardware, but I think those a legitimate assumptions to make for a hobby project.
Aren't you likely going to be at the mercy of LLVM abandoning XP support here?
Whenever I have questions like these, I usually read the generated assembly first. Unfortunately I'm on mobile right now...
&gt; Did you not read this part? I did, but as /u/fhartwig alludes to, that seems more applicable to servers than clients to me. I was reading that paragraph as 'general background on green threads', and the two others are concerned with blocking. Anyway, like I said, just trying to understand. Thanks! :)
This is a good example of how you could prevent confusion by simply writing the code in a different way. The following code is exactly equivalent but instantly communicates what's going on: let port = ((v[4] as u16) &lt;&lt; 8) | (v[5] as u16); 
Trying to benchmark atomic instructions like this is a fruitless pursuit regardless of the language; too much depends on the particular CPU, its cache size and cache consistency mechanisms and guarantees, the placement of threads on various cores in your system (and thus their ability to share caches), the operating system's thread scheduling etc etc. The padding may prevent false sharing, but you will not get any meaningful numbers regardless.
So, if I understand your solution correctly, this works because the references to the Cell are all immutable ( thus allowing us to have several ), and modifying and reading its internal value is done using the access methods, whose internal references to the current value go out of scope before the next method is called, thus ensuring only one mutable reference exists to it at a time? 
The method is implemented for `io::Take`, but you try to call it on `iter::Take`! And the reason you have wrong `Take` is the `.bytes()` method call which produces an iterator. There's also `take()` method on `R: Read`, which you should call directly.
This makes completely sense! Thanks for clearing that up.
&gt; BT clients are basically servers. Your reading comprehension is terrible.
Not having read the article yet, I just want to comment that this "difficulty -&gt; aha! -&gt; understanding" experience is the common pattern for me and most of my classmates when we were studying any new concept in computer science. EDIT: I thinkt the first of these for most of us were: - Understanding evaluation order - Understanding recursion
I'd argue that you're being a bit too agressive with modularization. Just dumping everything in `lib.rs` is totally acceptable and IMHO makes it easier to keep the whole system in your head. It's a sorting algorithm, not a framework! We also generally don't split off unit tests to separate files (rust-lang/rust does this a bit because of technical issues around being-the-standard-library). Having a `mod tests { .. }` and `mod bench { .. }` in the relevant file is usually favoured (keeps tests with the relevant code).
It would be very valuable to have this kind of information captured somewhere that is easy for a newcomer to find I'm new to Rust; I learned a lot about how to structure my projects from reading your paragraph.
 ptr::copy_nonoverlapping(&amp;tmp, list_j, 1); mem::forget(tmp); Is just ptr::write(list_j, tmp) ---- let len = list.len(); for i in 0..len { It's cool if you just like to do that, but it's worth noting that this is equivalent to for i in 0..list.len() { (neither of the bounds are re-evaluated `..` syntax is just sugar for making a struct) ---- `return foo;` at the end of a function is unidiomatic. You can just write `foo` and it will be returned. Similarly matches and if-statements evaluate to the last expression in each of their branches, so you can get rid of the `returns` like https://github.com/notriddle/rust-timsort/blob/master/src/gallop/mod.rs#L108 `return` is only necessary for "early returns" like `if guard { return false; }` or in loops. ----- lim &gt;&gt;= 1; I prefer lim /= 2; simply because it has better semantic clarity and will be trivially optimized to the shift (if it's not, that's a horrible compiler bug). ------ If you're using an enum a lot, you can pull its variants into scope: use std::cmp::Ordering::* match c(a, b) { Less | Equal =&gt; foo(), Greater =&gt; bar(), } ---- if list_len == 0 { return (false, 0); } if list_len == 1 { return (false, 1); } is the same as if list_len &lt; 2 { return (false, list_len); } ;) ------- I haven't looked to closely at the unsafe code (auditing it would take all day), but a word of warning: User-provided comparision functions can * be inconsistent (not a total order) * panic and trigger unwinding The former is unlikely to be a problem, but the latter can be tricky. If you're putting stuff in an inconsistent state and interspersing calls to compare elements, you need to put a guard in place to "clean up" in the case of a panic. In unstable rust you can use thread::catch_panic. In stable Rust you can set up a struct with a destructor. 
We're trying to get better at it, but a lot of this stuff is still emerging, so it's hard.
I was mostly concerned about some places I saw where some slices were temporarily having indices (logically) de-initialized. If you unwind the owner of that slice will observe it and do Bad Stuff.
Nice, yeah, I found an example online of someone doing it with Iron. Learning a lot, thanks.
&gt; Ultimately, I don't know if I will like dealing with memory in the Rust way. It may be that I don't. But I do know that I can't judge it properly until I've come through to the other side and gained the understanding of how to use it well. That's the spirit! :-)
For somebody who is a completely self-taught programmer Rust is a godsend. Yes, it is difficult to understand the borrowing system at first, but because the compiler will actually tell me what I am doing wrong at compile-time, I learn to write better code. In C/C++ I never quite know if my code is ok. Java takes care of it for me, but that also means I don't learn to understand WHY things are done the way they are, and that makes it harder to fix problems that occurred because I did not understand what is actually happening (This is a general issue with high-level languages I feel.) In contrast, Rust will **tell me** why the things I try to do is a bad idea, and thus I end up learning how to do it properly. It does not work for everything, and I still have to try to understand what is going on obviously, but the value of having the compiler complain when you try to do something stupid is much more than just making the code more reliable. It also helps with understanding memory management and why things are done the way they are.
Java doesn't tell you either. The difference is, having all data accessed through references prevents a certain class of use-after-free bugs.
`option.map_or(Ok(None), |x| f(x).map(Some))`
Depends on the situation I guess. Java is generally quite good at handling the memory and does things like bounds checks on arrays ( unlike C which just trusts you to get the pointer arithmetic right). I guess you can still do a lot of silly things in Java which Rust will throw a fit about ( like writing with one reference and reading from another ), but at least you are unlikely to overwrite your application's memory because of a pointer error, or completely screw up the stack by accident. Programming in C feels more like tap-dancing on a wet log in a river full of crocodiles.
Of course! Thanks edit: Before now I've passed over `map_or()` without examining its signature too closely; it's unfortunate that it is named so similarly to `map()`, which returns an option.
That's the idea, `map_or()` = `map()` the option if it's Some(T) or return a default if it's None.
You may need to register to access IRC, what error message were you getting?
I can't establish a connection with irc.mozilla.org or belew.mozilla.org (can't ping them either). I think it's likely something about the network I'm on right now (a university network with guest access).
Right but map returns an Option&lt;U&gt; and map_or returns a U; it makes sense but it made me ignore the map_or() too quickly.
The problems you're describing are fairly unique to C. The vast majority of programming languages are not like this. Rust is a step forward for avoiding data races and achieving memory safety without GC, but most functional languages handle the data races and memory safety well, and are easier to work with. For example, you never have to think about lifetimes when you're working with immutable data structures.
Okay, I tried this out, and now I am heavily confused. The error is now: `error: no method named 'fill_buf' found for type 'std::io::Take&lt;&amp;mut R&gt;' in the current scope` It is the same code as earlier except I changed the match to: match reader.take(buffer_size as u64).fill_buf() { **EDIT:** Alright, I found the error. It is because `R` in this context also has to implement the Trait! Confusing error message! 
tnx :)
Off-topic true story: I was once supervising an exam of first-semester CS students. During the exam, one student waves at me. I come over and he asks me if I could tell them about recursion. I said, "I'm sorry, but to tell you about recursion, I'd have to tell you about recursion". Unfortunately the student did not get this. Back on-topic: I believe that the detailed, thought- and helpful compiler advice is a crucial part of what makes Rust so awesome, and there obviously has been a lot of work poured into it. Given the slope of Rust's learning curve, this is a very good investment.
The virtual machine is your friend ;)
Still usable for tinkering around
I actually did think about both of those while writing it (I'll comment on the merge algorithm, because that's the huge part): No matter what the comparison function returns, if merge is in one-at-a-time mode, it will still progress one of the pointers and the destination by 1. It can't make it go out of bounds, and it can't cause it to finish early. If the merge algorithm is in galloping mode, the comparison is being done in the gallop function, so safety relies on gallop always returning a result within range. The narrowing stage obviously returns something within range, because it explicitly compares, but I haven't got a proof handy that the binary search stage will always return a result within range (if you wan't to make it scramble memory, galloping mode is the way to do it). As for panicking, I have the same solution the original Python implementation used for exceptions. In the Drop implementation, I copy any remaining items in the temporary storage into the result array. This is the same action taken if the comparison function had, for MergeLo, indicated that the first run's item was smaller, and for MergeHi, indicated that the second run's item was greater. 
**Edit**: Changed it to simplify the macro; it went from O(X\*Y) expansions to O(Y), which is a *teensy* bit better. Sorry; I only just woke up, and I'm used to parsing way harder stuff. :P You can solve this using [a recursive pushdown macro solution](http://is.gd/ryRhzl) (link to playpen version). This lets you process the two sequences one element at a time. Here's a cut-down, comment-stripped version of the above link: macro_rules! as_expr { ($e:expr) =&gt; {$e}; } macro_rules! trial_impl { ($xs:tt; ($($ex:tt)*);) =&gt; { as_expr!(&amp;[$($ex)*]) }; (($($x:expr,)+); ($($ex:tt)*); $y:expr, $($tail:tt)*) =&gt; { trial_impl!(($($x,)+); ($($ex)* $($x + $y,)+); $($tail)*) }; } macro_rules! trial { ($($x:expr),+; [$($y:expr),+]) =&gt; { trial_impl!(($($x,)+); (); $($y,)+) }; } 
Using the [byteorder](https://crates.io/crates/byteorder) crate might make it even clearer what's going on.
The stack size won't always be small for every problem, just like spinning up 10,000 threads won't work for every problem (like anything CPU bound). If what you are doing is IO bound and does not have a high memory requirement for each thread, why not just spin up a bunch of OS threads?
The closure is marked with `move` which means it captures the vec *by-value*. If we desugared the closure, we would see a struct being made and the vec being copied into a field of it *on every iteration of the loop*. Since (non-copy) data can only be moved once, this is plainly illegal. If we *don't* move the vec into the closure, then this code is unsound for a *different* reason: the parent thread can terminate before the child threads, deallocating the vec. 
You aren't getting responses, so I figured I would give you a clue. Unfortunately, I don't know rust, so this might not help. Also this will be Unix specific, so you will need to google if you are on windows. Using the posix Api, you would keep track of the pid of your sub process, then when you are looking to check if it is running, you would call waitpid with the WNOHANG option. Waitpid usually would hang until the child process terminates, but the WNOHANG option causes it to return immediately even if the sub process is still running. Edit: if your subprocess is your code, you could share a semaphore, which it could decrement when it is done. You could even reuse the same subprocess if that makes sense for your application by having the parent process increment the semaphore when there is work to do. 
It is actually not safe and that's not an error. If you take how the programmer intended you're accessing each member of the array in a separate thing. That's safe. But what rust sees there is that you're accessing the whole array 'data'.So it moves it. and that's causes the error.
D'you have a particular example?
Hmm, what I would normally do is const value = function_to_init_value(); fn do_something() { result = { ... use value ... } do_something_else(result) } fn do_something_else(result) { { ... use value again with result ... } } but I currently have to do this fn do_something(value) { result = { ... use value ... } do_something_else(result, value) } fn do_something_else(result, value) { { ... use value again with result ... } } { // some scope let value = function_to_init_value(); do_something(value) }
That looks really interesting, thanks! edit: That was really easy, Until CTFE is finished in rust, this is a great alternative!
That looks complicated (I certainly don't understand at least half of it). As a starting point though: can you elaborate on the interaction between sequences of `whatever` and the `tt` token type?
You can use `tt`s as a kind of catch-all: they'll match *anything*. For example, because I represent the set of `x`s as a *parenthesised* list of expressions *and* I don't need to match against the contents, I can match the whole thing as a single `tt`: in that case, it's me being lazy. In the case of `$($tail:tt)*`, it's effectively just saying "and the rest". In the case of `$($ex:tt)*`, it's the same thing, but keep in mind that `tt` *will not* match `)`, `]` or `}` (which is partly why that match is in parentheses). Basically, all this macro is doing is progressively building up the final expansion (which is stored in the second set of parens), appending to it for every `$y` encountered; once there are no `$y`s left, it dumps the contents of `$($ex)*` into the array literal. The only catch being that because it ends up being an undifferentiated soup of tokens, the parser isn't quite sure how it's supposed to interpret it. That's what `as_expr!` solves. Sorry if that was confusing; if you have more specific questions, I'm happy to answer them. Also, [this article may be of use](https://danielkeep.github.io/practical-intro-to-macros.html).
The subprocess is not my code. I suppose I could wrap up some unsafe code with libc bindings to get this done, but I was hoping there would be a native rust way of doing it. Thanks, at least I know, if I have to, I can do it that way.
No, see, `(...)` is *always* a single "token tree", *no matter what the `...` is*. The same is true of `[...]` and `{...}`. They are all *single* token trees. All other tokens are matched individually. So `1, 2, 3` is five `tt`s, but `(1, 2, 3)` is *one* `tt`, which happens to contain 5 more `tt`s.
Yes, this is what I ended up doing as well: fn next&lt;'b&gt;(&amp;'b mut self) -&gt; Option&lt;&amp;'a mut T&gt; { let coords = self.neighbor.next(); coords.map(|coords| { let cell = &amp;mut self.grid[coords.0 * self.size + coords.1]; unsafe { &amp;mut *(cell as *mut _) } }) }
Is it at all possible to invoke a macro inside a macro invocation inside a macro? (Just stay with me for a sec \^\^' ): macro_rules! some { ($id:ident, $id2:ident) =&gt; ( another_macro``!`[ concat_idents!($id, $id2) ]; ); The `!` after `concat_idents` doesn't 'match' with anything in `another_macro`; but I want to pass its *result* to `another_macro`, so how??
lazy_static does require a synchronization instruction every time it's accessed, which is unfortunate.
Alright, did a little more research. It looks like rust has a Process::signal function/method/whatever that can send a process a signal. If you send the signal "0", no signal is actually delivered to the process, but it will error out if the process is no longer running. Using the standard C API on linux, you would still need to call waitpid first to reap the process, but it looks like "signal" in rust calls waitpid(WNOHANG) for you. Here is a PR that should help you out. [https://github.com/rust-lang/rust/issues/13124](https://github.com/rust-lang/rust/issues/13124) I feel that the way rust handles this is a bug. Consider this: when you launch a subprocess, you get a unique identifier that is guaranteed not to change until the process is reaped. Because Process::signal now automatically reaps subprocesses, it will silently invalidate your process ID without warning, leading to race conditions in your code where you may try to wait for some unrelated PID.
That it's not a compile time known constant. If it were a real constant the compiler would be able to do many interesting optimizations. Making it a static removes this opportunity. Another disadvantage is where/when your memory is allocated. If it were a const the backing memory would be the binary. With a static the memory is allocated on startup so it's on the heap. When the lookup table becomes maybe this actually becomes an advantage because it keeps down the size of the binary.
Hmm. I can keep the issue on the backburner for now, but if you file a bug report I'd be grateful for a link.
Well, it is only a bug if the library exposes the pid to you when it may be invalid. I will do some poking. 
I think [this](http://doc.crates.io/build-script.html#case-study:-code-generation) is the only option that will keep the table `const`. Rust doesn't have anything like C++'s `constexpr` (yet).
It looks like Process::signal was removed from the standard library, which while it makes the call to waitpid not cause a bug, it also makes it so you cant use it :). I looked over the process interface, and it appears that there is no capability to poll the status of a running process from the system agnostic API. However, if you use the underlaying sys/unix/process, there is a Process::try_wait, which calls ```c::waitpid(self.pid, &amp;mut status, c::WNOHANG)``` which is exactly what you want.
Thanks, I'll give that a try when I get around to that bit again.
Wrong sub, try /r/playrust. But if you're a programmer stick around! ;)
As an alternative you can compute a lookup table at build-time. This is what I did for the [crc24 crate](https://github.com/sellibitze/crc24-rs).
 How about making a macro that expands to whatever you need?
Totally going to use this once stable supports it. (1.1? Fingers crossed...)
&gt; ... didn't MS already offer free upgrades? A free upgrade to Windows 7 or whatever is often not the issue; the OS is usually cheapest portion of their total cost of ownership. Usually folks using XP are stuck with it because of expensive proprietary or customised software that only works on XP.
Methods with type parameters (generics) are not "object-safe," meaning that traits with those methods cannot be made into trait objects. This is because type parameters are monomorphized at compile-time &amp; this cannot be done on the trait object.
This article doesn't mention glium/glutin and I'm having a hard time picturing how those projects fit (or don't) into Piston. (Other than that, great job!)
Sound like you want something like [the bytecode stuff from game programming patterns](http://gameprogrammingpatterns.com/bytecode.html), where you're writing a little task specific VM. The book has some example C++ code that can be translated fairly directly to Rust. You may also want to look at a JIT library like [libJIT](https://github.com/TomBebbington/jit.rs). LLVM might have something too, but I can't find it. Related, but not helpful, having [computed gotos](https://gcc.gnu.org/onlinedocs/gcc-3.1/gcc/Labels-as-Values.html) in the language would be useful for this. Rust doesn't have them, but there was some talk of adding something like them in the past? 
There is a [window backend for glutin](https://github.com/PistonDevelopers/glutin_window) and a [graphics backend for glium](https://github.com/PistonDevelopers/glium_graphics), but they're otherwise completely independent.
Most likely the ports are blocked.
So there are 3 window backends for Piston! Can you tell about the differences between them, which one is recommended etc.?
That is likely the case (a very unfortunate thing for a university network). I was confused when ping also failed, but now that I am on a different network (from which I can make an IRC connection, but still not ping), it appears that Mozilla's IRC servers have ICMP ECHO disabled.
Is there planned support for using glium for 3D graphics in piston?
No. - Rust hash maps are homogenous collections. All keys need to have the same type and thus their individual lifetime is bounded from below. - If the lifetime of a key would expire while the hash map is still alive the compiler would prevent it from being used in the hash map. - Lifetimes are not “active” i.e. they are just a compile-time annotation. Thus they cannot “expire”. This is why `Rc` and `Arc` exist.
Lifetimes won't really help you here because you're talking about dynamic (runtime) tracking of the liveness of values, which is exactly what a garbage collector does. Lifetimes facilitate the static analysis of pointer usage; they help Rust determine, *at compile time*, whether or not a given pointer dereference in a given context could cause undefined behavior or a segfault. Rust references compile to raw pointers at runtime. Since Rust doesn't have a garbage collector, the next best thing is the `Rc` reference-counted box and `Weak`, its non-owning counterpart. `Weak` can be converted to `Rc` (also called a "strong reference") if other `Rc` instances still exist; if not, the value in the backing storage is dropped and the backing storage itself is deallocated when all `Rc` and `Weak` instances are dropped. With this you can build a wrapper around `HashMap&lt;K, Weak&lt;V&gt;&gt;` that returns `Rc&lt;V&gt;` on-insert. You can clean up all the unused keys (ones with only `Weak` instances) on the next insert to the map, or just leave them. If you want to deallocate strings as soon as you're done with them, this is probably your best bet. Unfortunately, and for a not really satisfying reason, `Weak` is marked as Unstable, so you can only use it with Rust Nightly.
Thanks for the reply, I'll look into that. I'm starting to think that caching should not be part of the core library anyway, since I don't have to mimic the Lua API exactly. I can return matcher objects instead, and let the consumers of the library deal with caching if however they want to.
Hi, What I'm curious about here is how the binding is done. Do the backends register somehow with the core (and if so, how)? Can more than one backend be compiled in and if so, how would the end user select which one? Etc.
The back-end simply depends on the core. You add it to the Cargo.toml and type in "main.rs": extern crate sdl2_window; extern crate piston; use sdl2_window::Sdl2Window; use piston::window::WindowSettings; fn main() { let window: Sdl2Window = WindowSettings("hello", [128; 2]).into(); ... } More than one back-end can be used at the same time.
&gt;More than one back-end can be used at the same time. 1. What's the purpose of using more than one? 2. Can you determine at compile-time which back-end library is installed?
I believe that specific case (`$a:tt $($bs:tt)*`) is already possible on nightly. There aren't any real macro *collections* I can think of. Because of how macros are scoped, you tend to want to keep macros in very small, self-contained crates. I'm also not aware of any particular way of searching for them. Two personal examples I can think of off the top of my head: [`error-type`](https://crates.io/crates/error_type) and [Hodor!](https://www.reddit.com/r/rust/comments/39wvrm/hodor_esolang_as_a_rust_macro/cs769ip).
A feature of Glium is that it minimizes state changes, and (I think) the Gfx page used to say that such optimizations were out of scope for it. What would be the Gfx approach? If Gfx is supposed to have multiple backends I think it doesn't make sense (or even be possible) for the user to do this kind of optimization. It also seems a bit odd for Gfx to use Glium as a backend.
Gfx does a similar optimization and was a bit faster than Glium in the last benchmark https://github.com/bvssvni/rust-snake/issues/153. I am told on IRC that Glium does more state caching, though.
I call this phenomenon "Rust Stockholm Syndrome" (mostly tongue-in-cheek)
The results actually vary a lot between the gfx backend and the glium backend depending on the machine, but gfx always seems to bit faster. I tried to understand why glium was slower, but didn't really succeed. For example here is a profile of a frame with glium on my Windows machine with AMD drivers: http://i.imgur.com/uVPYM0x.png And here is the same frame with gfx: http://i.imgur.com/KTdH1UK.png By optimizing state changes, glium seems to be initially a lot faster and it would take around 6ms to draw that frame (compared to gfx's 14ms) if it wasn't for that call to `glNamedBufferSubData`. In my opinion the driver's queue reaches its limit, causing the driver to flush and wait for the GPU to finish drawing before continuing. I don't really know why it has to wait though. Experts usually recommend to not do what piston does if you want maximum performances (lots of draw call, reusing the same buffer, uploading everything), so I don't really see this problem as critical. But I would still be curious if someone had an idea. 
I've started reading the implementation of the regex crate, and I think there are some significant similarities between what it does and what you are describing. It might be worth a look. `program.rs` defines the [opcodes](https://github.com/rust-lang/regex/blob/c9e6781a6845478aa2d8ebc86972755f854fdbe0/src/program.rs#L29) (it calls them`insts` -- for "insrtructions"). `compile.rs` generates a `program` from a parsed regex; a major part of the `program` is the vector of `insts`. After that I think it starts to diverge from your design. The `program` can be executed differently by either a NFA or a backtracking algorithm. Instead of a collection mapping opcodes to closures, the executors use pattern matching on the enum of instructions. Some of the behavior is defined right there in the executor, and some of it is defined as methods on operands to the opcodes in `program.rs`. 
Not missing anything, that's unfortunately exactly right. I think you will have to impl Clone manually, but since you can use Copy it's as simple as it can be: `impl Clone for Board { fn clone(&amp;self) -&gt; Self { *self } }`.
Hopefully this will be added soon! It seems limiting to have a base type for which you can't implement new methods if its implementation is bounded to some size.
Yes, this is indeed limiting. :(
Doing it in `macro_rules` is going to run into recursion depth issues, I'm pretty sure. At least it does using this approach: https://play.rust-lang.org/?gist=5481887a9e0f5f0e1a87&amp;version=stable If you replace "one | incr | ... | incr" with "thirty_one" in `main` you'll run out of recursion depth. --- Edit: Actually, it works if you use a cruder implementation of `thirty_one!`: https://gist.github.com/64bf4e4733c1cc7efae6 You can't run it in the playground, though, since compilation takes too long. --- Edit 2: Found a more convenient number representation: https://gist.github.com/anonymous/93ea06542d323f37ed70
Ah, I didn't catch that, but to be fair that would have been pretty hard to find if I didn't guess that the size of the array mattered
The implementation is 'hardcoded' in the sense that it is generated by a macro, for arrays of up to 32 elements: https://github.com/rust-lang/rust/blob/9cc0b2247509d61d6a246a5c5ad67f84b9a2d8b6/src/libcore/array.rs#L42
I could see iterators/lazy evaluation being a benefit in some cases.
Rust iterators can sometimes be a performance win for the same reason that C++ iterators are, but they can also leave extra instructions behind and are often used to construct the same loops that would have been written in C in the first place. Rust's codegen is definitely not perfect around iterators right now in any case (a good example: range iterators for 32-bit integers [the default] are totally incapable of being inlined at the moment). The recent LLVM upgrade on master also caused some regressions for me--it's going to take a while (and constant vigilance, and possibly language changes like specialization) to really squeeze the most out of iterators.
thread pool in Node? What do you mean?
http://nikhilm.github.io/uvbook/threads.html
Not really about this example in particular, but there is one case where Rust might be better than C: aliasing. Typical Rust code often provides much stronger guarantees about aliasing than typical C code does. C does have the `restrict` keyword, but it's not used that much compared to Rust where all mutable references are unique.
Check that you actually have a library in the folder specified in `ffi.Library` call. This example is somewhat misleading, they are building without `--release` flag, this will put library in /target/debug instead of /target/release. Use `cargo build --release` to build library in release mode.
Neat! Once `wasm` is a viable target platform for `rustc`, would it be theoretically possible to build out a `canvas` backend?
Makes sense :P
Fixed. Thank you!
I'm on my phone or I'd write out some code, but you need to use Arc's clone() method to bump up the refcount and get another reference, and then move _that_ into the closure. The book's intro and concurrency chapter should both have examples.
Link missing its target: `We [know][previous blog post]` 
Yes, learning ownership takes a while but it's worth it. Also if the writer wants more advice on the rust way, if let and while let are really great for destructuring generics: use std::fs::File; use std::path::Path; use std::error::Error; use std::io::{stdin, BufRead, BufReader}; use std::collections::HashMap; fn sort_str(s: &amp;String) -&gt; String { let mut v: Vec&lt;char&gt; = s.chars().collect(); v.sort(); v.into_iter().collect() } fn main() { let path = Path::new("../word.lst"); let file = match File::open(&amp;path) { Err(why) =&gt; panic!("failed to open {}: {}", path.display(), Error::description(&amp;why)), Ok(f) =&gt; f, }; let b = BufReader::new(file); let mut dict: HashMap&lt;String, Vec&lt;String&gt;&gt; = HashMap::new(); let mut lines = b.lines(); while let Some(Ok(line)) = lines.next() { dict.entry(sort_str(&amp;line)).or_insert(Vec::new()).push(line); } let sin = stdin(); let mut lines = sin.lock().lines(); while let Some(Ok(line)) = lines.next() { if let Some(v) = dict.get(&amp;sort_str(&amp;line)) { print!("anagrams for {}: ", line); for a in v { print!("{} ", a); } println!(""); } else { println!("no dice"); } } } 
I guess the rationale behind `read_line` which appends to the buffer without overwriting the existing data, is to "reconstruct" the original file content just by calling it multiple times without further copying if the user desires. This is a valid reason I think, but might be confusing to newcomers who are used to other languages. (In fact, I was confused too.) Maybe we could add some "destructive" read methods in the future?
Some simple feedback: - Missing comma in `Iter&lt;'a T&gt;`. - “Changing the type to &amp;(player, _score) will satisfy the compiler”: s/type/pattern/ - “In each call to .into_iter(), we are simply moving the vector into a new IntoIter&lt;T&gt; struct.” Well, the *first* `.into_iter()` moves the vector into a new `IntoIter&lt;T&gt;` instance, but the subsequent ones do nothing (the implementation of `IntoIterator` for `I` where `I: Iterator` is, as stated, the identity function.) - “Why does .iter() not work in this example?” s/does/would/
"Exercise for the reader: Why does .iter() not work in this example?" Hate that stuff. If I had the time required to do these "exercises", I'd know the language well enough not to need to read an effectively using iterators tutorial in the first place. As it is, I end up reading it on my cell phone during my commute, so no rust available to test it out.
`$rest:tt` catches a *single* `tt`, which is either a single basic token (like `for`, `+`, or `484.65902`) *or* a matcher (`(...)`, `[...]`, or `{...}`). `$($rest:tt)*` catches *all* `tt`s from that point on. Note that `)`, `}`, and `]` are *not* `tt`s by themselves, so they *won't* be matched. I'm not sure what else to say about it if it still doesn't make sense, other than to suggest just playing with the macro system more (which is what I did).
This may be of interest to you: [Iterator Cheat Sheet](https://danielkeep.github.io/itercheat_baked.html).
&gt; to count the number of bytes (not characters! Rust is UTF-8 by default) for all strings in the names vector. nit: this is not just a default. The `str` and `String` types are UTF-8, and there is no way to change this.
Thank you for the feedback. I have correct all those issues.
Good point. I fixed the article. Thank you.
Hey All, thanks all for the useful feedbacks! I updated the blogpost - also added a note that the goal of the post is not to do performance comparisons but to help on how to use Rust from Node.
Ah, you might want to clarify that you explcitly meant "replacing the `.into_iter()` with `.iter()` in that expression". As it is, I was wondering just what is asked since it will also change the type of the resulting vector, and fixing that gives you countless opprotunities to make the code compile.
Not this specific method. It'd be backwards incompatible to change it. But another method under a similar name possibly.
I'm going to bet that half of the time is spent building up the JITted code. You're calling the same function with the same type. After the third or fourth time doing that, it JITs and then should be close to the same speed as a C function. Especially since a lot of shape checking can be elided since it's JIT code calling JIT code.
Personally I try to avoid them when possible. Seeing an explicit return at the end of a function makes me cringe.
It is low level stuff, a bit of ugliness is inevitable here, I think. However, it's not hard to brush all complexities under the carpet. use std::io::{stdin,BufRead}; fn read_all_lines&lt;F&gt;(br: &amp;mut BufRead, mut f: F ) -&gt; Result&lt;(), ()&gt; where F:FnMut(String) { loop { let mut line=String::new(); match br.read_line(&amp;mut line) { Ok(0) =&gt; return Ok(()), Err(_) =&gt; return Err(()), Ok(_) =&gt; f(line), } } } fn main() { let sin=stdin(); let mut sinl=sin.lock(); let mut vec:Vec&lt;String&gt;=vec![]; if let Ok(_)=read_all_lines(&amp;mut sinl, |s:String| {vec.push(s);}){ } else { } } 
Yes. It has already been demonstrated by running Hematite in the browser through emscripten. http://www.reddit.com/r/rust/comments/367wt6/hematite_running_in_the_browser_through_emscripten/
return Surely you're overreacting.
I know it's just semantics (that are backwards-incompatible to change in this case), but I too would've liked something like Lang | EOF | Error | Normal ------ | ------- | ------ | ----------- Rust 2 | Ok(Eof) | Err(…) | Ok(Data(n)) (with n ≠ 0)
```io::error::Result``` is private, but it's publicly reexported as ```io::Result```. I generally do ```use io::Result as IoResult```.
&gt; Experts usually recommend to not do what piston does if you want maximum performances (lots of draw call, reusing the same buffer, uploading everything) The evidence so far is not what you say experts recommend, but that a single buffer, with position and uvs separated is faster. How can not you not upload everything when the render logic happens on the CPU?
I think it's more likely to be the overflow and recursion checks that can't be elided. Considering the body is basically just three additions, these checks could take up a disproportionate amount of time.
FWIW, I think we could consider having a "superfluous return" lint.
Oh, now I get it. I'd understood that `tt`, being a 'token *tree*', matched `&lt;delimiteds&gt;` *only*; so no 'single token' stuff like you're saying. Yeah, playing with it now. This clears up a lot of stuff, though. +1
Strings in Rust and strings in C are *completely different things*. The type on the Rust side should probably be `*const libc::c_char`, using the `libc` crate on `crates.io`.
What's the python signature used for this function? Indeed String is different that what python sends. On the python side you want ctypes.c_char_p('abc'.encode('utf8')), on the rust side it will be Cstr.
It's generally true across all expression-based languages, Rust included. 
Could you post the modified code?
There's no need for intermediate state, you should be able to deserialize directly into Vec&lt;u32&gt;. let vec: Vec&lt;u32&gt; = json::from_str ("[1, 2, 3]") .unwrap(); But if for some reason you really need the intermediate representation, let vec: Vec&lt;json::Value&gt; = json::from_str ("[1, 2, 3]") .unwrap(); let vec: Vec&lt;u32&gt; = vec.into_iter().map (|value| json::from_value (value) .unwrap()) .collect(); 
Relevant: https://github.com/rust-lang/rust/issues/20098
&gt; I'm thinking that the String type is not compatible with Python's, and I have to use c_str or something That's correct. ctypes will convert the strings to C nul-terminated strings when passing them in, but Rust is assuming its own String which is a completely different data type (it's essentially a Vec, so `{length, capacity, *data}` in an undefined memory layout). Also it's generally recommended to use [cffi](http://cffi.readthedocs.org/en/latest/) these days, it's pypy-compatible and often less broken.
I really do not understand why people are so eager to throw away the error information that Rust provides and other languages do not. You can hardly argue that EOF is some sort of unexpected error that will never occur in practice!
&gt; but the manual is fairly terse on that, certainly not as detailed as Huon's blog The book chapter on trait objects is basically huon's blog... &gt; There is no trivial way to get an object to implement both traits. Well, I just woke up, and I'm pretty jetlagged, but I _think_ in the case of your gist, this has more to do with an improper formulation of the question in Rust, but I'm just experimenting with it now, let's see what others come up with :)
I'm experimenting with it right now, too. I've noticed a few things. * The other parameters are `&amp;other: T` which AFAIK is a pattern destructure and not the same as `other: &amp;T` * Attempting to return out `MyObject` as `Value` doesn't work because there's no way to return a type as a trait type. Using a generic doesn't help here, either * It seems you can't compare `MyObject` to an `&amp;Value` because they don't implement similar `Eq` semantics.
Yes, that's a good summary of my first few minutes as well.
&gt; Huon wrote a follow-up article Ahhh, given that it was mentioned as 'trait objects', I didn't click the link. You're right about object safety, and there's a bug open to basically move huon's blog post into the reference. :) Thanks for the nice comments about the book :) &gt; apologies for the dodgy code No worries! This is using a number of fairly advanced Rust features together. I didn't mean "this code is bad so you should feel bad," I meant "I think you're having trouble because the code isn't quite right yet, not because it's impossible to do." If it was easy to just fix, I would have already posted the fix, you know? &gt; I hadn't realised either place was syntactically sound. One place where patterns on the left is useful is with `mut`: fn main() { let v = vec![1, 2, 3]; let v = foo(v); println!("{:?}", v); } fn foo(mut x: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { x.push(4); x } This _particuar_ code might be better served by taking a `&amp;mut [i32]`, but you get the idea: when you need to mutate an argument binding.
Okay, so I'm not 100% sure this is what you want, but here's something that works: https://play.rust-lang.org/?gist=2b5a7604a03a263cb643&amp;version=stable Notes on this: * You cannot make a trait derive `Eq`/`PartialEq` and have it be object-safe because `PartialEq` has a generic parameter that references `Self` by default * You cannot compare `&amp;MyObject` and `&amp;Value` directly as a consequence of this * Attempting to `impl PartialEq&lt;Value&gt;` for `MyObject` yields some weird errors about lifetimes. Which change whether the `other` or `self` parameters are on the LHS of the comparison I don't know if you were aiming for reference equality semantics or structural equality. I landed on structural equality by simply comparing the results of `get_val` against each other. This unfortunately is a *very* limited form of structural equality as it only compares the result of `get_val` and not potentially the entire structure of the two types. Which may not even be possible, since the `&amp;Value` could be any other type that implements the `Value` trait. **EDIT:** I've seen the idea of a syntax along the lines of `fn foo() -&gt; impl Foo` thrown around that would let you return any type that implements a given trait. I believe it is being called "anonymous return types" (someone please correct me if I'm wrong). I thought there was an RFC, but I can't find any. So it has either been merged, closed, or didn't exist in the first place. Anyway, I believe this syntax combined with generics would potentially make what you want to do possible, without object-safe traits and using generics, instead.
Sorry, I was imprecise. They can be inlined, but many optimizations you'd expect from inlining don't work properly with them (you should know, you filed the issue! I just ran into it).
The decision of whether to use them or not should really depend on what is most clear in the situation. Consider: ... some code... return foo; ... more code ... return bar; ... last bit of code... baz //wait, what? In this case it would make more sense to have return in the last line as well, because to somebody skimming the code it is easy to see all the return statements and where they are. For similar reason you may decide to specify u32 for some variable even where the compiler can infer it, because in some cases it helps explain what the code does. 
&gt; The aliasing guarantees Rust provides are nothing compared to the guarantees a correct C program supposedly provides which compilers use as a baseline for optimizations Links/More infos? Pleeeeease
Oh, [that issue](https://github.com/rust-lang/rust/issues/24660). It is now fixed. (And yes I do know, but your description was vague and the issue is closed.)
&gt;I don't know if you were aiming for reference equality semantics or structural equality. I landed on structural equality by simply comparing the results of get_val against each other. Just to check, but if you want to check for reference equality you would compare the raw pointers ( which is safe as long a you don't dereference them) , right? 
This gist of object safe traits is that they cannot contain generic functions or contain types of `Self`. The `PartialEq` trait is defined as `trait PartialEq&lt;Rhs=Self&gt;` because sometimes it makes sense to compare two different types, but in most cases it does not, so `Rhs` defaults to `Self`. So making a trait derive either `Eq` or `PartialEq&lt;Rhs=Self&gt;` is not object safe. If you were to make the trait derive `PartialEq&lt;WellKnownType&gt;` then you're fine because you've defined what the RHS must be. Whereas RHS being `Self` in the default case leads to an issue with dynamic dispatch, thus making the trait no longer object safe. Self is a special sort of type annotation for traits that acts essentially as a place holder for the concrete type that is implementing the trait. For instance you could define a trait like: `trait Foo { fn foo(&amp;self, rhs: &amp;Self) -&gt; Self; }` and that then means `Self` is the type that implements the trait. Like: `impl Foo for MyObject { fn foo(&amp;self, rhs: &amp;MyObject) -&gt; MyObject { ... }`. This is not object safe, because you've essentially said, "These types are the type implementing this trait." and if you were to then try and access this object as the trait itself... what's the type of `Self`? It's going to vary between every single implementation of that trait. As /u/Veedrac pointed out, there's a few different ways to handle the equality here. Without getting overly complex, the simplest form is comparing the values of `get_val` for equality. If you want to get more complex on the equality semantics, you'll have to get much more creative in implementing that, especially across types that may differ in structure, but implement the same trait.
This glitch happens when I take out any object in the game, also my graphics are glitched in general(Walls, Stairs, and such). I have reinstalled, updated Cache and Updated my drivers.
Note that if you want to accept arrays which could have some non-int elements (and just filter out those elements), you could write something like this (instead of unwrapping and potentially panicking) let vec: Vec&lt;json::Value&gt; = json::from_str(str).unwrap(); let vec: Vec&lt;u32&gt; = vec.into_iter().filter_map(|v| if let json::Value::U64(n) = v { Some(n as u32) } else { None }).collect();
ahh thank you haha. Sorry about that.
Yep!
What is your high level goal? I suspect that what you actually want is to use type parameters instead of trait objects. Trait objects are fairly limited in what kind of operations they allow and they are usually not the right answer (unfortunately, they also have less verbose syntax, and I've definitely found myself stuck up a trait object tree several times). If `Value` types are supposed to be `Eq` with any other `Value` type, that's going to be really tricky to do. If they're just supposed to be `Eq` with other values of the _same type_, then that's not a problem. But being `Eq` does make it impossible to instantiate a trait object of that trait (for the object safety rules others have mentioned). I think what you really want `Smashable` to be like is more like, e.g. `Add` and other arithmetic operators. Parameterized like this: pub trait Smashable&lt;T=Self&gt;: Value where T: Value { type Output: Value; fn smash(&amp;self, other: &amp;T) -&gt; Self::Output; } This says that all `Smashable` types must be `Value` types, that it is parameterized by another `Value` type (which defaults to `Self`), and that it carries another `Value` type, which is returned by the function `smash`.
Ok this doesn't look so bad
I think the problem is that you're trying to compare apples to oranges. `Eq` is not for comparing *any* two objects that implement `Eq`, its only for comparing two objects that **are of the same type** (and implement `Eq`). The `self` that gets passed to `value_by_smashing_other_value` may be a *different type* than the `other` that gets passed. They superficially look like they are the same type — because they both implement `Value` — but they might not be. You could implement `Value` for both `String` and `f32` (which both implement `Eq`), but you still can't check if `s : String == n : f32`, but you could try doing something bad like this: let s : String = "abc".to_string(); let n : f32 = 2.0; s.value_by_smashing_other_value(n) And then, clearly, your impl of `Smashable` would try to do `s == n` which the compiler knows is meaningless. What's also going on is that you're using `==` to mean "Are they the same object?", whereas `==` is usually meant to determine *equivalence*: If I have two apples and you have two apples, then my two apples `==` your two apples, *even though they're different apples*. You, however, are asking if my two apples are *the same* apples as your apples. If you want to know if they are *the same object*, you're looking for something that was talked about in [this thread](https://www.reddit.com/r/rust/comments/2dmzf6/why_do_equality_tests_of_references_seem_to/).
I'd say the rule itself is hardly important enough to warrant such dogmatism that people feel "sneered at" for not adhering to it.
Yes, I would say 'sneered' is a bit strong, but I just took it as a joke.
It would add a stack frame per pattern element, a pattern element being one of cartesian_product( [character, any, charset], [once, maybe_once, greedy_repeat, non_greedy_repeat] ) + captures and other special cases. As an optimization, consecutive literal characters can also be treated as a single pattern element, reducing the number of stack frames. Switch in a loop has the drawback of creating a diamond-shaped control flow which is [suboptimal for modern CPUs](http://lua-users.org/lists/lua-l/2011-02/msg00742.html). Actually, using closures, I can eschew the dispatch step altogether. I must compile the pattern from the end, and, for each pattern element, pass the closure that matches the rest of the pattern as a continuation. I may try and benchmark several implementations.
Yeah I already pinged them via e-mail. I think ideally this project would be end of lifed once they get to a point where the pure-Rust driver is production ready.
I think having multiple different ways of doing a thing is super healthy, so I'd like to see both survive :)
The final price: $169 http://rustcamp.com/ :)
Can you talk more about what your shop is doing with Rust? We're currently reaching out to companies using Rust and we'd love to get your feedback.
How would an `impl Foo` be any different from a `T: Foo` trait bound? Or would that just be a different way to write it?
No, that's not the case. If the compiler can assume something can't happen, it can optimize around it, and if the thing *actually* can't happen code correctness will not be altered. For example, because panics affect external state, LLVM can't generally optimize assuming that array accesses will succeed in Rust, even in situations where they always will. This can lead to much poorer codegen than is possible otherwise. Of course, LLVM will try hard to prove that an array access will always succeed wherever possible, but it is far from perfect at this.
Ah, I didn't realize it was fixed now. Nice!
Hello, I am the author of this project. As you have noted, I have abandoned this project due to other commitments. A lot has changed, and I have not been following rust as closely as I should be. When I started this I barely new any rust. As of now, I would not use rustpy. A library that was being developed at a similar time, and still being developed that I would recommend is: https://github.com/dgrunwald/rust-cpython. It employs a number of the same strategies but seems to be further along in the stability and functionality. Hope that helps! 
In `impl Foo`, the callee chooses the return type. In `T: Foo`, the caller chooses the return type. Either by `let t: MyT = that_fn();` or `that_fn::&lt;MyT&gt;()`. With `impl Foo`, the concrete types are erased to the callers.
The RFC was accepted.
what's the time complexity of doing this?
It's all good, it was good of you to get the ball rolling and open source your work. Thanks for the link, looking into it now.
My final try [serde_example](https://github.com/linuxaged/learnRust/blob/master/serde_example/src/main.rs), there are many places can be improved, any suggestion?
This is a great article. The *100ms ballpark* gives a good idea at the target to look for. Unfortunately you do not provide the *ballpark* for the entire search (ie including the code *pruning*/cleaning part). I decided to go and try alternatives as well ([here](https://github.com/tafia/naive_complete/tree/master)). My objective is to keep things as simple as I can imagine. In particular I will **not** rely on the compiler at all. Eventually I hope it can provide benchmarks you can compare with. I'd like to leverage on your experience on this area to know if I'm totally wrong.
Let's make that the quote
I'd like to add that RFC PR #1147 was almost completely rewritten. It's no longer restricted to API changes, and is easier to implement. It even has a feature name now: Target Version Unfortunately I'm on mobile, else I'd submit a PR.
Its harder because namespacing/overloading works differently in rust, the C++ library would have to have been written with other-language bindings in mind.. Rust isn't the only other language out there without adhoc overloads etc , so I guess it isn't out of the question that some C++ libraries are doable.. but I know they had problems trying to do Qt bindings, for example. If a library uses every C++ feature, binding will be hard. It would be nice if Rust, Go,D, C++, people got together and made some sort of intermediate ABI above C, below C++ allowing simplified first parameter mangling or something eg. imagine 'extern "C"' on a C++ member-function creating a C compatible function `Typename_methodname(Typename* , args..)` - which would be easily representable in go, rust, etc with less pain - and imagine a similar assist in Rust, Go,... `impl SomeType{..extern fn foo(&amp;self,..),..}` === `extern "C" SomeType_foo(SomeType*,...)`. 
I also want to know what happened to the the Friend of the Tree thing.
Clearly, the tree has become anti-social and no longer has any friends. Its existence is an endless well of suffering that only those who have plumbed the blackest depths of the abyss *MOOOOM close the curtains I'm brooding!* No, it's *not* a phase, this is who I am now!
The same as unwrapping. O(n) unless serde parsing is superlinear (I doubt it is).
That'll do; ship it.
 let obj = data.as_object().unwrap(); let meshes = obj.get("meshes").unwrap(); This can be shortened into let meshes = data.find("meshes").unwrap(); --- let vertices: Vec&lt;json::Value&gt; = mesh.get("vertices").unwrap().as_array().unwrap().to_vec(); let vertex_array: Vec&lt;f64&gt; = vertices.into_iter().map (|value| json::from_value (value) .unwrap()) .collect(); This seems unnecessary. Can be shortened to let vertices: Vec&lt;f64&gt; = json::from_value (mesh.get("vertices").unwrap()); P.S. I prefer macro-based struct deserialization myself (https://github.com/serde-rs/serde#making-a-type-serializable), it feels more typed, much nicer to work with the program that way rather than picking at JSON fields one by one. And it should be faster. But macro-based deserialization needs Rust Nightly or jumping through some hoops. And skipping missing fields (https://github.com/serde-rs/serde/issues/44) isn't yet implemented. Also don't forget that in the real program you should give error handling some love.
Tell me if I'm interpreting this right OP: As far I can tell, you're not asking 'how can I solve this problem for my use case' so much as making a point that in a cost-benefit analysis, adding hashes is something that would unequivocally lean to the benefit side for everyone involved (except attackers). I agree. It needs to be coded once and then it provides security for everyone. Hashing a downloaded crate is going to be negligible timewise considering the amount of time that will be spent compiling a crate will typically be at least three orders of magnitude larger. Answers like "the security is pretty good" or "if security is a much higher priority for you, there are ways to use your own tools to do it" are arguable in the context of integrity solutions which 1. (are prohibitively complex to solve 2. || place noticeable burdens on the user 3. || have major tradeoffs in confidentiality) 3. &amp;&amp; (negligible security benefit 3. || a benefit unrelated to the project.) But in a context like this, managing sprawl in the number of 3rd party entities a typical rust user needs to trust (we trust cargo, but do we trust all the private mirrors?) is an obvious feature for providing basic data integrity automation (where non-automation looks like having to write your own scripts or re-reviewing the source code every time you download it)
Yes, this is what I was thinking about.
That analogy doesn't fit. It's not about signalling the end of a function, it's about showing that a certain value gets returned. The point is not the `goto`-like behaviour, the point is that a value will be returned by the function. The return value needs to be specified somehow, the question is just if `return` or a missing semicolon is the better way to do so.
I'll be there, and I also just purchased a ticket that I would like to transfer to a fellow woman in tech who would like to attend. If that's you, please get in touch with me at carol dot nichols at gmail. If that's not you, please spread the word to the women you know who might be interested! Depending on how much travel costs end up being, I might be able to chip in for that as well :)
*Twitches at the use of the word 'app'.* I don't think there's anything you can really do about this right now. Cargo has no way of installing binaries, so I don't know that anyone bothers to put binary crates up on `crates.io`. There's also no (native) packaging system, so it's not like you can "bundle" anything anyway. If you're manually doing builds and making archives... uh, just build the CLI tool at the same time and add to the archive? Otherwise... add a note to the readme file?
The only problem is this tends to fixate versions of things, and *encourages* people to fixate versions of things. This can also be very bad: let's say, a popular underlying package releases 2.1.x which fixes a dramatic security bug, some input malhandling error. However, everyone who uses this package is likely using 2.1.a or 2.1.c or 2.1.w. Some of them update fast, some of them update slow, some don't realize there was a security fix. And your package is using one of these in your end application... what can you do? Your build logic is sworn *not* to install 2.1.x, because by golly that has as wrong a hash as the string "hello" does. This is *not* a good security story. A package version specifier like "&gt;=2.1.0, &lt;2.2" is useful precisely because it allows security updates through subsequent package updates. A dependency hash does not. What you want for this, like most situations in "secure packaging" end up needing, is **package signing**. You want this so you can tell *who's* a package is. And then say *who* you trust. Completely avoids version fixation, and is a *lot* more flexible. Curiously enough, you already gave the correct solution to your problem in step 2., but somehow managed to have it apply *only* to your friend. Use that solution to solve the thing for everyone, instead.
Mmh...pity! But thanks...
https://github.com/rust-lang/crates.io/issues/75. crates.io doesn't have a security model right now.
&gt; _Twitches at the use of the word 'app'_. Resistance is futile.
"Rust, the most exciting language since Scala." The [presentation on Rust is available on YouTube](https://www.youtube.com/watch?v=TZmJC6uRTCQ) [fr]. Devoxx appears to be a [fairly large developer conference in France](http://www.devoxx.fr/). It's really great to see Rust being adopted across the world! Et merci d'avoir partagé ce lien. Il y a une communauté de « [Rustacés](https://fr.wikipedia.org/wiki/Crustac%C3%A9) » qui parlent français. :-) Et n'oubliez pas le [Tutoriel Rust](http://blog.guillaume-gomez.fr/Rust) de Guillaume Gomez.
We do. The implementation is [here](https://github.com/rust-lang/rust/blob/master/src/libsyntax/util/interner.rs).
We do, with a simple pair of `HashMap&lt;Rc&lt;String&gt;, u32&gt;` and `Vec&lt;Rc&lt;String&gt;&gt;`, IIRC. Too bad I can't just cc rust-lang teams on reddit, but maybe /u/dbaupp and /u/kmc_v3 want to take a look at this. Although, TBQH, I don't think such a trie could work nicely for long strings, like all the identifiers found in the rustc crates. The space used for each byte of an unique tail would be something like `[[u32; 1 &lt;&lt; N]; 8 / N]`. That's 128 bytes *per byte* for N=4 (16 entries per node, common choice) or 64 bytes for N=1 or N=2. EDIT: s/a trie/such a trie
OP's article uses a trie, while the libsyntax interner uses the "nuclear option" (HashMap). Nevertheless, I believe the bottleneck of rustc isn't in interning new strings, and I don't think changing this to a trie will help much.
The word 'app' is probably not appropriate to use here. I would try 'binary' instead.
There are various tries that do path compression which avoids the problem you mention by collapsing unique tails (and even unique substrings like the `st` in `{rustc, rustdoc, ruby}`).
There was only one quote submitted this week and I declined to use it. At the moment we don't have a great venue for FOTT since the weekly meeting was canceled. I do have a list of future FOTTs though.
Ah, thanks. I missed the ``mut`` bit in the example in the official docs: https://doc.rust-lang.org/std/iter/index.html. 
I feel like adding that if you actually read the error message, it is very understandable. [playpen](http://is.gd/5VnLOM) cannot borrow immutable local variable `it` as mutable That is a pretty clear error message.
The question was not about why that line does not compile. The question was how the for loop manages to call next() for an immutable iterator. @thiez has answered it above.
We need GPG signing support in Cargo. :)
Well, isnt "its syntactic sugar" a pretty good answer? If you took the time to look at the type of the iterator, then you would realize that it is not an immutable iterator. I know thiez said that, but you could have taken a few minutes to look at it in the first place.
I am sorry. I didn't mean to offend you. I spent close to two hours researching before posting. As I said I missed the ``mut`` part in the example in the official doc. That would have explained everything.
Sounds like how it's done for Linux distributions and Chrome extensions. Much more convenient than just locking it to a single hash, too, for reasons outlined throughout this submission's comments.
I looked at the source as well. Exactly what I needed is in a private struct which is quite annoying....
Mutability is rooted with the owner. Two things happen with the for loop: - You move the value `it` to the loop. The for loop now owns it - The for loop calls `IntoIterator::into_iter()` on that value, potentially producing something else At the end, since the for loop now owns the value, its binding determines the mutability. let x = "abc".to_string(); let mut y = x; // New owner can do what they want.
Yes, the optimum is the same. But compilers aren't (and can't be) fully optimal. There will always be plenty of true information they can't prove automatically, especially in a language with an unsound type system like C's.
This was a wicked juicy article! I've poked around in `vec.rs` a few times when a profile leads me there, but it was nice to see some of the stuff explained a bit more. Thanks!
["Abstract Return Types"](https://github.com/rust-lang/rfcs/issues/518) (though closed and currently not under discussion AFAICT.)
Shove the file in a `Mutex` to synchronize access to it. You may have to shove that in turn into a newtype of some sort and implement `Write` on that.
Ah, thanks. I knew it had to be something obvious I wasn't thinking of. I was not aware I could lock an immutable Mutex and get a mutable type out of it. But now that I think about it that does seem like the best idea.
Am I missing something, or is the size_hint implementation not right? It looks like it counts the number of bytes instead of the number of elements. 
I'm fully support CyruzDraxs comment. If you cannot link to C++, ahem, library using another C++ compiler, it is a bit overoptimistic to wait for Rust crate to do this anytime soon.
The fact that `FileLogger` is `Send + Sync` means in practice that your log function can be called from several threads in parallel. That's why a `Mutex` would work (well, you might get lock contention), and the idea of always opening the file would not work (the second thread would get an error that the file is busy rather than wait for it to become available). As a side note, I'm a bit surprised to see that `File` implements `Sync`. It seems more logical to me that it didn't, but I guess file operations are synchronised by the OS anyhow so it would be okay.
Having `Option&lt;Unique&lt;T&gt;&gt;` Means that `Option&lt;Vec&lt;T&gt;&gt;` won't be null-pointer optimized to be the same size as `Vec&lt;T&gt;`. Using intrinsics is usually bad because they're by-definition implementation-defined and will *never* be stable. I imagine the standard library should expose an `abort`/`oom`, but we haven't really done much design there.
For those unaware, this is Gankro's project to write a companion to the [The Rust Programming Language Book](http://doc.rust-lang.org/stable/book/) (TRPL) that focuses entirely on how to properly and safely write unsafe code, hence The Unsafe Rust Programming Lanugage (TURPL). If you don't write unsafe blocks, then you pretty much don't have to worry about it. If you do write unsafe blocks, ever, then you absolutely have to worry about it and should help proofread his book and give feedback. :)
Here's a recent, relevant article on how `for` works: [Effectively Using Iterators](http://hermanradtke.com/2015/06/22/effectively-using-iterators-in-rust.html) by @hermanradtke.
Good point - the initial, minimal test would be an hour or so I think - it could be extended as much as is convenient but it is not necessary to convert the whole project! If that little test went OK then there are some future projects that would be worth talking about that could be larger. 
That's how I would start out, too. If you get too much contention (does Mutex have a way to measure it?), you can swap in a MPSC queue and spawn off a logger thread (this is what Log4j2 does, btw. except they get problems because you can log mutable objects which are mutated meanwhile. Rust can guarantee this won't happen, which is pretty cool).
A closure can only borrow entire objects, not parts of them, so in your first case it is borrowing `self` mutably, while in the second it’s only borrowing `self.field_b`.
Yes this is how you need to do it. Actually you only need to “pre-borrow” the value you’re using inside of your closure. One should also note that is is just a current limitation of the compiler but nothing unfixable.
 &gt; If you get too much contention (does Mutex have a way to measure it?) I guess that would require some minor overhead: first call `try_lock` and use the result of that to increase your "contented" vs "not-contented" count. And if it fails, just go ahead and call `lock` to wait for your turn. My biggest fear with logging threads is that when the program crashes, some vital logging items are still in-flight and not pushed to disk.
/u/Gankro, do you plan on adding this as a section to TRPL (after Nightly Rust, for example)? Or will this be its own book?
In this particular case, [libsword](http://www.crosswire.org/sword/index.jsp). As well as good SWIG support, it has a flat API already (src/flatapi.h and bindings/flatapi.cpp), but said flat API is pitifully incomplete and I very rapidly found a critical bug (hint: `i++` three times per loop iteration) that strongly suggests that no one actually uses it anyway.
Just thought i'd mention it can be nice to destructure the struct when making bindings to multiple members, i.e. impl Foo { fn do_things(&amp;mut self) { let Foo { ref field_a, ref mut field_b } = *self; ... } } 
Servo must be one of the most exciting projects at this moment. Hopefully, in contrast to what is stated in this article, it _will_ one day find its way into desktop Firefox ;-)
I've made pretty much the same thing, except I called them `TryFrom`/`TryInto`; I tend to use "maybe" for `Option`-related things.
One difficulty there is Firefox add-ons, which has huge API surface.
 impl From&lt;Foo&gt; for Result&lt;Bar, Baz&gt;
This dropck business sounds really tricky!
Unfortunately, that runs afoul of the coherence rules when implementing: impl From&lt;NotMyType&gt; for Result&lt;MyType, MyError&gt; {...} while the following works: impl From&lt;NotMyType&gt; for MyType {...}
Good article. I was wondering why Vec used Unique&lt;T&gt; rather than a raw pointer. Also zero sized types are pretty annoying! So much code to handle a pointless use case (or is there a point in having a collection of a zero sized type?).
Semantic versioning as employed by Rust is pretty much subjective IMO.
&gt;Also zero sized types are pretty annoying! So much code to handle a pointless use case (or is there a point in having a collection of a zero sized type?). Not really, no. But generic code can mean it might arise under perfectly reasonable circumstances and therefore should be handled. 
This is more of what I wanted as it is a bit safer than the unsafe code I had implemented. I'll try factoring something like this into my code later today and let you know the results. I tried your code out in the playpen and it seems to be working no problem.
There's no magic that C can do to avoid converting an integer to a float. It just does it without telling you. I would say that a better optimisation would be to not use floating numbers at all. Or delay conversion to them for as long as possible. 
I use the `onchange.sh` script: https://gist.github.com/evgenius/6019316
Then unfortunately the value of semantic versioning is greatly diminished.
By no small margin either! Good work!
Interesting. Should we mark `Regex::new()` as a constfn so that we don't need to pay the parsing overhead?
It isn't possible, it does far too much to be a `const fn` at the moment.
I'd say `Regex::new` touches about two-thirds or maybe even three-fourths of all the code across the `regex`, `regex-syntax` and `aho-corasick` crates, so I think /u/dbaupp is right about that one. :-) (The remaining ~third are the matching engines.)
Why doesn't `regex!` just rewrite itself as `Regex::new` for now?
One of the main features of `regex!` is that it can be put into *static* data. e.g., `static re: Regex = regex!("abc")`. `Regex::new`, among other things, heap allocates, so that seems tricky to pull off.
This is actually quite similar to my initial pass at `regex!` over a year ago. (It just stored the parsed regex, it didn't do any matching. So it guaranteed that the regex compiled---but that was it.) The problem is that the [`regex::Program`](https://github.com/rust-lang/regex/blob/master/src/program.rs#L194-L218) data structure is more complex (in particular, `Prefix`). It will just take some real work to provide an equivalent---but static---representation.
Oh, and I'll leave these here. :-) http://benchmarksgame.alioth.debian.org/u64/performance.php?test=regexdna http://benchmarksgame.alioth.debian.org/u64q/performance.php?test=regexdna
I strongly disagree, especially from a business point of view. If you start throwing medium-sized two digit version numbers at your customers it’ll better be the current year or they’ll stop being impressed by it. Look at Chrome &amp; Firefox, their version numbers totally get neglected in ads. And Apple &amp; Microsoft apparently both stopped counting at “10” aka. “X”.
&gt; It's true that you could just do (release version).0.0 in Cargo but then that still would look to some people like implying that every release breaks compatibility. Release versioning just ignores compatibility breaks So each releases does, essentially, break compatibility. It also clearly states that dependent packages can not assume they're compatible with a new release. 
&gt; There are tools that checks that you follow the semantic versioning rules (for example both Elm and Scala has such tools) so you don't accidentally break something. The OP is right that there is a measure of subjectivity and impossibility to quite know, just about any change aside from adding brand new structures and functions may break user code (the only way to guarantee this will not happen is enumeration test where every single possible input is tested, and that's pretty much impossible for non-trivial anything)
I don't get it. Isn't the intended trait chosen based on the `&lt;T as Tr&gt;::` syntax? And if I'm implementing a conversion from some type into *my* type (or `Result&lt;Mine, Mine::Err&gt;`), then it's a safe bet that there's no impl From&lt;NotMyType&gt; for MyType {...} impl to be conflicting, no?
It will be a separate parallel text using the same infrastructure.
As /u/aatch said, theres no magic in what C does, it just does it implicitly. There looks like there might be a lot of ways to speed this up. How efficient is count_ones? It might be worth making sure thats as fast as possible. Maybe create a 16 bit lookup table, and do 4 lookups. Though, in todays world, you might want to use something like whats at [bithacks](http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel). Hitting a lookup table can be deadly. Is scale consistent between queries? Then do what /u/aatch said and defer the conversion. Store the counts in a u64 then do the multiplication once at the end. Make sure your numbers coming in are cache coherent. If they arent, it probably doesnt matter _what_ your algorithm does, youll just die from the CPU waiting for memory. Check out [this talk](https://www.youtube.com/watch?v=WDIkqP4JbkE) from Scott Meyers about memory cache speeds. And thats why I love rust (though technically I havent done anything in it yet). Its a modern language that respects what the CPU is actually doing, and isnt just monad this monad that. 
The problem is orphan rules (not overlapping rules, which you refer to). The orphan rules restrict in which crate you can implement which traits for which types. They currently prevent you from implementing a non-local trait (e.g. `From`) for a non-local type (e.g. `Result`), even if that type is parameterized by local types. The goal of orphan rules is to prevent overlapping implementations in two separate crates. The reason for this is that a design goal for Rust is for any two crates to be able to be linked together.
&gt; [edit]: ok, so if I understand correctly, the new engine doesn't support unicode properly. Let's hope unicode support doesn't affect performance too much. Wherever did you get that idea? :P It supports Unicode better than most regex engines! (Regex engines like Python require you to pass a specific flag to make `\w` be Unicode aware. It is Unicode aware---and *only* Unicode aware---in the `regex` crate. Similarly for `\s`, `\d` and `\b`. It also does simple case folding automatically for case insensitive matching. There are areas for improvement, of course, but overall it's pretty good.) That said, the `regex-dna` benchmark is not impacted much (if at all) by Unicode support. Take a look at the regexes used: http://benchmarksgame.alioth.debian.org/u64q/program.php?test=regexdna&amp;lang=rust&amp;id=2 To be clear: nothing about Unicode support in `regex` has changed recently.
Worth noting that this is basically *only* a problem data structures have. The allocator stuff in particular could be avoided by just having allocation be typed (which it will be when we create the Real Allocation API).
Semver doesn't actually say "will break no user code". For example, a patch release can break user code, as it is "A bug fix is defined as an internal change that fixes incorrect behavior." If you were relying on that incorrect behavior, your code is now broken. SemVer is a social mechanism for signaling API change. For example, in the FAQ, while discussing inadvertant breaking changes: &gt; Remember, Semantic Versioning is all about conveying meaning by how the version number changes. If these changes are important to your users, use the version number to inform them. This is why https://github.com/rust-lang/rfcs/pull/1122 and https://github.com/rust-lang/rfcs/pull/1105 were so important: SemVer is all about "public API" but does not actually define what that means. The project itself needs to declare exactly what is considered public and what isn't.
&gt; "A bug fix is defined as an internal change that fixes incorrect behavior." If you were relying on that incorrect behavior, your code is now broken. That's where the subjectivity lies though, [one's bug is another's feature](https://xkcd.com/1172/), and then there's the fun case where the fix for a bug breaks a workaround for that same bug. &gt; SemVer is all about "public API" but does not actually define what that means. Sure but again that makes OP right about the subjectivity of SemVer, if every project must define what semver means for it, and every project can give a different meaning to it, it's not objective at all.
I didn't have an associated type for errors because I just used my library's unified error type. I haven't open sourced it because it's a few lines and I'm not a fan of "libraries" whose entire source could fit on a sticky note :P Now, that said, if it came with a full set of implementations for the standard library... well, that's different. :) (As an aside, there are times when I find Rust's forbidding of adaptor libraries a huge PITA. I'm tempted to dig up that proposal for "extern impl"s I sketched up a while ago...)
&gt; It amazes me that more people don't whine about the lack of stack traces in Err() code. Then again, you could very well have people whine the other way around if it occurred by default. Capturing a backtrace has a cost. It's a matter of balance, I suppose.
&gt; I was following the conversation on github and it seemed like \b would not work with unicode for a while. I'm probably missing some subtleties and not-very-subtle-ties anyway. Oh. Yeah, you're *way* in the weeds. :-) We were talking about how to support `\b` in a DFA engine, which hasn't been implemented yet. If we can't figure it out, then that just means that the DFA engine can't be used when `\b` is used. Other engines (like the already implemented backtracking or NFA simulation) can handle it just fine today. The Unicodeyness of `\b` isn't going anywhere. :-) &gt; By only Unicode aware, do you mean that unicode awareness can't be switched off? Correct. For example, if you don't want to match every possible Unicode digit with `\d`, then you could just use `[0-9]`. There are also ASCII character classes that you can use, e.g., `[:space:]`. &gt; I mean, it's impressive work for sure! But matching \w{10} surely has different performance characteristics between unicode aware and US ascii implementations? It does, yes. But it's not necessarily slower. Naively, matching would be a binary search over the ranges in `\w`, but it will actually do a quick linear scan of the first few ranges first. If you're matching ASCII text, then you really haven't lost much. (Hopefully soon, something like `\w` will itself be an automaton instead of a sorted `Vec`, which should also only degrade if you're matching non-ASCII text.) The key to making a fast regex engine is to avoid the regex engine entirely for certain classes of regexes. :-) So I'm not too worried about the impact of performance that Unicode has. It's measurable, sure, but very small. (And completely irrelevant when an optimization kicks in to skip most of the search text.)
Pretty sure `count_ones` is implemented as an intrinsic (e.g. https://doc.rust-lang.org/std/intrinsics/fn.ctpop64.html) and uses the relevant assembly instruction if it is available. It's rather unlikely that you can do it faster (though it's not unheard of).
&gt; Oh. Yeah, you're way in the weeds. :-) Well, can't say that was entirely unexpected :D &gt; We were talking about how to support \b in a DFA engine, which hasn't been implemented yet. If we can't figure it out, then that just means that the DFA engine can't be used when \b is used. Other engines (like the already implemented backtracking or NFA simulation) can handle it just fine today. Oh, so what makes the current version so much faster than other engines then? I thought using DFA rather than having to backtrack would be a major speed increase, which is why I thought that's what was implemented. Btw, thanks for taking the time to explain stuff like this
Have you thought about *not* immediately converting to a `String` format? I wonder if performance wise early transformation is better or not, in case the `Result` is never printed this is wasted effort.
Have you profiled the overhead? I am wondering if the addition of `file!()` and `line!()` does not risk to inflate the code size (compared to backtraces) as well prevent function merges (which also increases the code size). Also, have you thought about *not* formatting right away, but instead just capturing the information and formatting lazily if need be.
Oh, i didn't look closely enough.
C just makes the cast implicit, it still happens on the hardware. In any case, it's a red herring: the default `count_ones` will be taking much more than the cast (3&amp;times; in my measurement): #![feature(test)] extern crate test; #[bench] pub fn loop_(b: &amp;mut test::Bencher) { b.iter(|| { let mut x = 0; for i in 0..1000_u32 { x += test::black_box(i) } x }); } #[bench] pub fn cast(b: &amp;mut test::Bencher) { b.iter(|| { let mut x = 0.0; for i in 0..1000_u32 { x += test::black_box(i) as f32 } x }); } #[bench] pub fn count(b: &amp;mut test::Bencher) { b.iter(|| { let mut x = 0; for i in 0..1000_u32 { x += test::black_box(i).count_ones(); } x }); } The `loop_` benchmark is to give a baseline about the overhead of the loop+addition+`black_box`. With `-C opt-level=3` on an x86-64 CPU: running 3 tests test cast ... bench: 1,069 ns/iter (+/- 57) test count ... bench: 2,118 ns/iter (+/- 12) test loop_ ... bench: 537 ns/iter (+/- 8) With `-C target-feature=+popcnt` to get the hardware instruction: running 3 tests test cast ... bench: 1,067 ns/iter (+/- 34) test count ... bench: 1,074 ns/iter (+/- 0) test loop_ ... bench: 534 ns/iter (+/- 14) Hence, optimising the `count_ones` interaction is what you want to focus on. If the `scale` is constant across a long sequence of numbers (contiguous in memory, i.e. a `&amp;[u64]`), using something like [`hamming::weight`](http://huonw.github.io/hamming/hamming/fn.weight.html) (disclaimer, [`hamming`](https://crates.io/crates/hamming) is mine) will be much faster.
I think it'd be cleaner to use `&amp;mut` rather than `let ref`, but besides that, that's the way to do it! impl Foo { fn do_things(&amp;mut self) { let field_b_borrow = &amp;mut self.field_b; self.field_a.iter().map(|&amp;index| { field_b_borrow[index] = true }); } }
In which version are scoped threads expected to work again?
&gt; at the moment. Do you know hat the future of `const fn` is? I remember a discussion a few weeks ago about `const fn` with another redditor and the idea of labelling the functions themselves (much like C++) did not seem that worth it in Rust. I thought initially it would ensure reproducible builds, but with `build.rs` and the plugins being able to execute arbitrary code the "reproducible" part of builds seem to have shipped a long time ago. And therefore, it seems that today functions are marked `const` only to guarantee that constant evaluation can occur; however if `rustc` was able to evaluate anything (a VM for LLVM IR?) then it would no longer be necessary.
My first attempt at a rust podcast. We have 5 hours in the can to release still: )
Rustacean /u/posix4e noticed that there was a dearth of Rust podcasts and has taken it upon himself to rectify this, and managed to drag along yours truly as his trusty co-host. :) Note that neither of us have *any* experience making podcasts before, so we gladly welcome and and all feedback.
The community subteam is not up at [http://www.rust-lang.org/team.html](http://www.rust-lang.org/team.html); also, the headings are broken, and there doesn't appear to be a link to that page from the front page. Busy at work now (and yeah, shouldn't be checking Reddit) or else I'd send a PR to fix it.
Typed allocation? Something like `fn alloc&lt;T&gt;(num_elements: usize) -&gt; *mut T`?
Yep! I mocked out potential simple designs here: * https://github.com/Gankro/raw-rs/blob/master/src/alloc/plain.rs * https://github.com/Gankro/raw-rs/blob/master/src/alloc/lazy.rs But pnkfelix has grander plans, and we need to wait for those.
The biggest change of this version in my opinion is the fact that `Rc`, `RefCell`, `Arc`, `Mutex` and maybe others can now take unsized types. This isn't even mentioned in the release notes.
Given that it's not stable still, the answer is "1.3 at the soonest." It's a relatively high priority.
It looks like great improvements! However, the rust benchmark is making use of threading. How would it compare if it did it non-threaded, such as the C++ solution?
Isn't it in 1.2? EDIT: seems so: use std::rc::Rc; trait Foo {} impl Foo for i32 {} fn main() { let rc = Rc::new(5) as Rc&lt;Foo&gt;; } using: steve@warmachine:~/tmp$ rustc hello.rs hello.rs:8:9: 8:11 warning: unused variable: `rc`, #[warn(unused_variables)] on by default hello.rs:8 let rc = Rc::new(5) as Rc&lt;Foo&gt;; ^~ error: internal compiler error: translating unsupported cast: alloc::rc::Rc&lt;Foo&gt; (cast_other) -&gt; alloc::rc::Rc&lt;Foo&gt; (cast_other) note: the compiler unexpectedly panicked. this is a bug. note: we would appreciate a bug report: https://github.com/rust-lang/rust/blob/master/CONTRIBUTING.md#bug-reports note: run with `RUST_BACKTRACE=1` for a backtrace thread 'rustc' panicked at 'Box&lt;Any&gt;', /home/rustbuild/src/rust-buildbot/slave/stable-dist-rustc-linux/build/src/libsyntax/diagnostic.rs:230 steve@warmachine:~/tmp$ multirust override beta multirust: using existing install for 'beta' multirust: override toolchain for '/home/steve/tmp' set to 'beta' steve@warmachine:~/tmp$ rustc hello.rs hello.rs:8:9: 8:11 warning: unused variable: `rc`, #[warn(unused_variables)] on by default hello.rs:8 let rc = Rc::new(5) as Rc&lt;Foo&gt;; ^~ 
Which benchmark? The benchmark game? See for yourself. ;-) http://benchmarksgame.alioth.debian.org/u64/performance.php?test=regexdna (That still has threading in it, but it's run on a machine with a single core. The timing between what's on the benchmark game and [the same benchmark with no threading](https://github.com/rust-lang/regex/blob/master/examples/shootout-regex-dna-single.rs) is negligible on my machine.)
Here's a use case I often run into, is this solved with Cargo's shared target directories? Say I work on library A which depends on library B. Library B depends on library C. If I make a change to library C and build it, when I build library B I will end up using the updated library C (no rebuild required). Now, when I build library A, I don't have to rebuild library B or C, because they share the same target directory.
I don't know of concrete plans for its future. The reason we have explicit annotations is that it is part of the interface: the programmer opts-in to `const fn` and it is explicitly a breaking signature change to make it non-`const`. Of course, as you say, if *everything* could be evaluated at compile time... it wouldn't be necessary, but it seems that that is an extremely hard problem to solve. For example, there's questions around cross-compiling and things like FFI calls and inline assembly (and especially how that interacts with cross-compiling).
Probably more like 1.5 at the earliest. It's not low priority, but there's higher priority things we need to tackle, and once we do tackle it, it's a HUGE change.
&gt;Rust 1.1 stable provides a 32% improvement in compilation time Is this meant to be relative improvement (132% as fast) or in percentage points (compile times down to 68% of previous ones, 147% as fast)? Edit: Reading further it's probably the former, but the use of the word 'time' instead of 'speed' is still ambiguos. Edit2: Apparently not! Happy to hear that.
&gt; It's true that you could just do (release version).0.0 in Cargo but then that still would look to some people like implying that every release breaks compatibility. Release versioning just ignores compatibility breaks, with the reasoning being that such breaks cannot actually be predicted in advance so tracking them is counterproductive. This paragraph of yours sounds like 1.0.0, 2.0.0, 3.0.0 is exactly what you want. Since your versioning scheme is based on the assumption that such breaks cannot actually be predicted in advance, everyone using your library should assume that when there's a new release version, there's the *possibility* of breakage that they should be prepared for when upgrading, and that's what people assume with these version numbers in SemVer. Are there any other open source projects that follow the RN or N scheme you're suggesting?
If performance of parallel codegen builds is still inferior to sequential codegen, please don't turn it on by default for release builds in future versions. Release builds should not be optimized for compile time by default; they should be optimized for the best performance for the end users. Otherwise you end up having to take even more command line switches into account, which is terrible. You end up having to read monstrous man pages that are tremendous time wasters. Please pick a sensible default setting, even if it makes compilation slower by default for release builds.
Are there any installers for the MSVC target with the 1.2 beta? Looking at the other downloads page doesn't show any.
&gt; There are two hard things in computer science: Cache invalidation, naming things and off-by-one errors. That said, whoever feels like helping to find the best names for this feature is hereby invited over at internals.
&gt; If the automaton has exactly one transition out of the root state, then memchr is used to skip along the search text very quickly. I'm sure its handled ... but what about Unicode normal forms?
[**@rustlang**](https://twitter.com/rustlang/) &gt; [2015-06-25 20:06 UTC](https://twitter.com/rustlang/status/614162712180953088) &gt; Big intro to Rust by @pnkfelix at \#mozwww. Slides here: http://pnkfelix.github.io/cyot/tutorial/slides/whistler_rust_intro.html#/ &gt;[[Attached pic]](http://pbs.twimg.com/media/CIXxufPWEAAoT66.jpg) [[Imgur rehost]](http://i.imgur.com/cXKG1rp.jpg) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I mentioned this in the RFC comments, but note that the term "target" already has a well-defined meaning in Rust's context.
Currently I have tooling built on top of PyBT[0] that I would be incredibly interested in porting to rust (And rust's thread model would be a lot more comprehensible). If nothing else, a literal port of PyBT to rust, assuming you only want to support linux, would be a reasonable place to start. I can't commit to doing anything, but if you ping me (@richo on github) I'd be happy to submit patches to support the stuff I need specifically if the groundwork is there. github.com/mikeryan/PyBT
I think many might be interested to take a swing at your problem, especially if it could be done by playing around for an hour or so, but without first seeing the C++ code the barrier for volunteering becomes a bit high. Is there a reason you cannot post the code online? Perhaps only the parts that the minimal test should cover (if it is part of some larger application)? I think making the C++ code available would quickly get you some volunteers.
Would be great to have the video along the slides.
You're looking for /r/playrust. 
That's a big deal! I knew compilation times were a priority, but didn't expect these results this quickly. Good stuff!
Yeah, if scale changes all the time, moving to fixed point might be a way to speed things up. Ide do multiples of either 16 or 256, but the same basic idea. 
 &gt; I was happy to see that Rust supports the abilitiy to create a const array of struct Note that this currently only works if your struct doesn't contain private fields. In the future the `const fn` work will make it possible for a struct to expose a constant constructor so that it will be possible to change those `Point { x: 0, y: 0 }` calls to `Point::new(0, 0)`if needed.
Didn't know that, I guess I was lucky to get it to work.
At the moment there are still ways to make that specific use case work: https://crates.io/crates/lazy_static
`json::from_value` need a `Value`, but `mesh.get("vertices").unwrap()` return a `&amp;Value`. when i dereference it, there is an error: cannot move out of borrowed content.
Seems like it'd be a fine fit. There are workable [BLAS](https://github.com/stainless-steel/blas) and [LAPACK](https://github.com/stainless-steel/lapack) bindings etc if you need them (maybe your work doesn't).
Well, &amp;Ok works. Thank you! Really not even sure why &amp;Ok is a thing that makes sense, but there you have it.
Would be sweet to have the ability to match against multiple regexes in a single pass. :)
It's more flexible than that. The position of the field doesn't matter, as far as I know. In theory you also do something clever with `Option&lt;Option&lt;Option&lt;(&amp;u8, &amp;u8, &amp;u8)&gt;&gt;&gt;` but I don't believe we do today.
Are there docs for the details of musl support? (Not including the stuff Alex describes in the PR where it landed.) I'm unclear if the official rustc distribution supports musl or if you need to build your own rustc compiler. My best understanding is that it's the latter, which is unfortunate because I'm trying to package the compiler in the smallest possible Docker container by using Alpine Linux, which uses musl instead of glibc, and I want to use an official distribution so people will trust my public Docker image. Another related question: What is the reason that the included rustc is not a static binary? When I was playing around trying to compile Rust programs on Alpine Linux, I had trouble getting the compiler to work because it needs the libs installed to e.g. lib/libarena-xxxxxxxx.so. I also noticed there seem to be multiple copies of these libs, one in .so form in the root of the lib directory and the other in both .so and .rlib forms in lib/librust. Do I need all of these in order for rustc to work, or can I remove some of them to save space? Edit: If compiling on Alpine with musl is not achievable yet with Rust 1.1.0, I'd love anyone's advice on how to make the existing Docker image smaller. It's currently over 1 GB which is absolutely insane. I will definitely remove the rust-docs component, but that only saves a few hundred MB. Link: https://github.com/jimmycuadra/docker-rust
That optimization has nothing to do with encoding. It operates on the bytes only. More generally, the regex engine doesn't care about Unicode normal forms. (I don't know of any regex engine that does.)
What, you mean like, last week? :P Python's regex engine is no slouch. It's [written in C](https://github.com/python/cpython/blob/2.7/Modules/_sre.c). (Most of the regex engines benchmarked are actually written in C/C++.)
Thanks for the great benchmarks! Is there any way I can pass that flag to cargo when building? Edit: Nevermind, that was just added with cargo rustc! http://blog.rust-lang.org/2015/06/25/Rust-1.1.html
If you're doing a single-search, then this seems like a non-issue. Unless that single-search is inside a loop, in which case, you'll want to benchmark it and optimize based on that. Most of the interesting benchmarks of regexes are on their throughput. Benchmarking construction can be useful too, but it's typically easy to work around it if it's really a problem so it's a lot less interesting.
I've been thinking about this. It should be doable since we use automata. What do you think the API should be?
You might like to look at [criterion.rs](https://github.com/japaric/criterion.rs). I haven't used the crate myself, but I've had a lot of fun with the Haskell version.
Which makes up 99% of the time I'm compiling, so it's great. How do we enable this for dev mode if we're on nightly?
No, it takes 32% less time, which is equivalent to 47% faster! (duration of a unit of work is the reciprocal of the speed of that work)
Was http://rust-conf.com supposed to be RustCamp? Edit: made link a link
For anyone creeping this thread in the future, I found this article that has some to say on the subject, albeit without the template arguments on the function. http://camjackson.net/post/rust-lang-how-to-pass-a-closure-into-a-trait-object
Every Rust compiler is a cross compiler, but you _do_ need a stdlib for the target for things to work. We're working on making this process better and easier in the future.
Hmm, but isn't the problem that the bytes for a unicode character can be different between NFC and NFD? So if you regex has, say, an accented e in NFC, but the string its matching has an accented e in NFD, the bytes are different but the character should be treated the same.
 [profile.dev] codegen-units = 4 in Cargo.toml will work afaik. 4 being the number of parallel units.
If you didn't already know it, bors is a bot.
I think the community *really* wants a solid green threading crate, but we're waiting for more comprehensive non-blocking I/O support to show up in the ecosystem first. Green threading doesn't really work without it.
Would `mio` qualify then?
Great podcast. It was very interesting. It would be nice to have it in a free format, I couldn't play it in Firefox on Linux. OPUS is really good for this kind of stuff.
That "&amp;mut is about exclusive access, not mutation" helped me better understand the concept. Now I think I can explain the interior mutability types better. (Why are they mutable even if the types themselves seem immutable.) Also, this looks like a great presentation material for a first-time or second-time Rust audience. It covers many deeper areas of Rust, but is still very friendly. Great work and thanks, /u/pnkfelix and others!
This would be great! I'm about to start a bluetooth LE project, first time working with it though so I'm pretty unfamiliar. But, as I'm learning Rust, I want to do it in Rust!
`cargo rustc`’s usage information is wrong; it says: cargo rustc [options] [--] [&lt;opts&gt;...] It should be: cargo rustc [options] [-- &lt;opts&gt;...] That is, any options to be passed to rustc need to come after a `--`. So your line would be: cargo rustc --release -- -C target-feature=+popcnt
Basically what /u/retep998 said. `mio` is a good start, but a comprehensive solution would be crossplatform and support all I/O, not just networking.
Good to know! @/u/Manishearth: We could extend clippy's README.rs to show adding clippy to the build without requiring source annotations.
What makes multi-threading safer in Rust than in most other languages is it's intelligent static type system. An elisp compatible interpreter wouldn't have that, since elisp is dynamically typed. An interpreter for a lisp like language that included a typesystem similar to rusts ( i don't think something similar exists ) could have those guarantees but wouldn't be compatible to elisp.
You're looking for /r/playrust, this is for the Rust programming language. :P
I wouldn't personally recommend expending the effort, unless you're planning on implementing another language's runtime or an OS or something. Making them a first class language feature would require making stuff like thread locals work with green threads (which would likely require linker support), and if they're not first class the value proposition drops substantially IMO. The only reason it ever worked before was because the entire standard library and language runtime opted in.
This is exactly what I think about the subject. I fought with the python evented library gevent for months because I was blocking the reactor with FFI calls. A green thread library should not freeze up because of an FFI call and that's a very tricky thing to do right -- it needs support at a very low level.
Not just that... think about stuff like segmented stacks (the vestiges of which are hopefully soon being removed from Rust proper in favor of stack probes). I think you'd have to have a *really* compelling specific reason to do all this stuff.
What I mean is that the rules on how you can break backwards compatibility are very clear in semver. And you can have a tool that checks that your API doesn't break it. That part is totally objective. In addition to this you can have a subjective debate of the contracts of functions which are not clearly specified. This should of course be avoided as much as possible by either specifying the contracts in the type system when possible (in a dependently typed language you can encode very strong contracts in the type system), or otherwise documenting them clearly.
Regex::new(...).chain(...).chain(...)?
Those are different unicode strings, though. Matching both of them would be easiest done as `(é|é)` - which then goes back to matching on bytes.
There are other things I think `const fn`s can help with. In particular, on debug mode I would like it if `const fn`s always got proper placement new treatment; right now, they do not, which can easily lead to stack overflows.
No, I'm not even sure if it is on any agenda. There are many other issues and the workaround for this one is trivial.
I have a question about the performance thingy: 32% (or 47%) improvement sounds really great, especially because it was done in just 6 weeks. But I'm asking myself what parts of the compilation process are mostly affected. For example: I recently wanted to make a really small program that generates some random images. So I used the `image` crate from the piston devs. But the link-times were just incredible high... Up to 30 seconds, just for including this one crate. I know that LLVM is responsible for linking, but isn't there a way to improve link times too? Or was it improved in 1.1?
The way I'd like things to work: - release branch: everything is in git, no download needed - master branch - on build machine: optional check if there's new versions of pinned/unpinned dependencies, sets build to yellow. - master branch - on development machine: warn on new versions (probably by default), optional update 
&gt; For instance: I have hear that Emacs lacks of parallelism, if a elisp interpreter were built using Rust, it would be possible to endow elisp with safe multi-threading? Nope, these are two separate concerns. The interpreter itself would be easier to cheaply make thread-safe (without having to rely on e.g. a global interpreter lock) but the language running in it would be no more thread-safe than usual.
Sure. What does the matching API look like? Same as what we have? Or does it need to tell you which regex matched?
In languages like Go, green threads *are* a first class language feature. There are no non-green threads, system calls are optimized at the assembly level for that purpose, default stacks are small and moving (which requires GC), etc. On Linux spawning a new thread is quite fast, particularly if you pin to a core, and its scheduler is very customizable. On Windows, as already alluded to, you can use user mode scheduling. I don't think scheduling is a compelling argument for green threads on modern operating systems.
Subscribed!
The `test` function doesn't return anything, so the borrow ends when the function ends. On the other hand, `into_iter` returns an iterator, which borrows the vector mutably. So the borrow lasts as long as `iter1` lasts. It works like that, because when you look a little below [here](http://static.rust-lang.org/doc/master/collections/vec/struct.Vec.html#method.into_iter), you see: impl&lt;'a, T&gt; IntoIterator for &amp;'a mut Vec&lt;T&gt; type Item = &amp;'a mut T type IntoIter = IterMut&lt;'a, T&gt; fn into_iter(self) -&gt; IterMut&lt;'a, T&gt; which means that if `into_iter` was a standalone function, it would have this signature: fn into_iter&lt;'a, T&gt;(&amp;'a mut Vec&lt;T&gt;) -&gt; IterMut&lt;'a, T&gt; This `'a` (little bit of simplification here) basically says that returned struct will have the same borrow scope as the reference passed to a function, and that means that if that reference was mutable borrow of a vector, the returned iterator would hold the same borrow too.
Right, I forgot you'd need to `remove` the Value instead of `get`ting it. (And if you can't afford to `remove` the Value for some reason then you can `clone`).
Hmm... I find myself partial to the `&amp;uniq var` proposal, after reading the slides and niko's blogpost about it, but that's history now...
Please read the side bar before posting...
I've been considering it. My main concern is translating what the original source does compared to what Piston supplies. It's something I intend to investigate in the next month or so.
Really, the killer feature of `regex!` is that it can report invalid regexes at compile time. I’d be happy with a `regex!` that does just that: compile-time check of the regex syntax, and translates to `Regex::new`.
Please see /r/playrust
Thank you so much! This becomes easy to understand now.
I would go with the mutable approach so you don't force the user to clone the board. If someone wants to keep the old board, he is free to clone it before solving it. This way, the decision about (im)mutability is in the hands of the user of the function. As for having different board types, it really depends on what you are aiming at. Do you want to guarantee that a board is solved at a certain moment? Then maybe it would be a good idea to have a special type for it. Otherwise I think it will just overcomplicate things. Btw, take a look at the [sudoku](https://github.com/aochagavia/sudoku) library if you need some inspiration ;)
I don't see a need for any OOP or traits for this design. A simple functional design with a `solve(board: &amp;Board) -&gt; Board` function will do just fine. I also don't see the benefit of discriminating between unsolved and solved Boards at the type level. However, representing the board as a 9x9 vector is really inefficient if you intend to use it as immutable data and create lots of copies of it. In general Rust has more support for FP than OOP, which is a good thing.
What's the current status of stack probes? The last thing I heard was that it's missing llvm support... 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust_gamedev] [\[piston-adventures\] 3. Falling Sand game prototype](https://np.reddit.com/r/rust_gamedev/comments/3b7ef0/pistonadventures_3_falling_sand_game_prototype/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot)
Right, I guess what I mean is, the initial support may have landed, but it's buggy enough that we're not really ready to call it a finished feature until 1.2.
I wasn't aware of the `image` crate taking that long, maybe we can use it to figure out more improvements. I just let nrc know that crate is maybe degenerate, and maybe he can figure it out. :) (I'm not personally familliar with the details of how it was improved exactly.)
Yes. That someone was me. :-)
&gt;individual pixels look dimmer when they move This is probably caused by your screen. Pixels don't light up instantly. The update logic looks really neat.
Yes, the fact that cross-compiling precludes JIT-ting is an especially tough pill to swallow, but with the Systems Programming focus of Rust, it seems unreasonable to ditch it. Afterwards, I see 2 main possibilities: - interpreting Rust itself, as done in C++ with `constexpr`, however it seems a lot of work and only works for pure Rust - pushing interpretation/execution lower down to integrate more smoothly with at least C, and maybe as down as assembly I believe the second alternative, if executed at LLVM IR level, could be rather successful: - it is the last "common" model, after LLVM IR the representation is specialized and requires as many "virtual machines" as there are assembly dialects which is impractical - it could potentially be a shared effort with other users of LLVM, as they too would benefit from it Of course, the inline assembly issue remains. I would contend that it is rare and that by including "mocking" of the functions that contain inline assembly the LLVM IR interpreter would impose not too much of a burden on the end user.
Slight correction: Chaos Communication *Camp* is not a yearly event. It's every 4 years during summer (so, it's extra special/rare). The Chaos Communication *Congress* is a yearly event during winter. I'll probably be at the camp. I'm told it's an awesome experience and I still have some Rust stickers I could distribute… :)
I see what you're saying, and thinking about how Rust's modules and namespacing works that would probably be a much better solution. I can just pass the board in as a `&amp;mut` and the whole thing ends up being a lot simpler. Thanks. I'm very used to JavaScript's (ES5) modules where everything is encapsulated in an object, which to me roughly translates to a struct/trait in Rust.
Oh, neat
Watching development of the programming language is different from games which have such aspect like a story. Consider for example reading a final book, and a half cooked draft. Surely you can help shaping the story as an early reader, you can point out things to improve and so on. Authors need that kind of help, same as game developers need alpha and beta testers (aka "early access" which I just find to be a weird reinvention of the alpha/beta testing concept, coined in order to obfuscate the fact that in the past people were paid for such testing, or at least they could do it for free, and now people pay to test unfinished games, how weird is it?). If you value the story experience, any kind of early variant will be a spoiler. In case of games (or books or any kind of other art) it's a major downside. It can be needed to help authors, but otherwise I'd prefer to avoid such things to experience the final result properly.
I'd imagine it could prevent some exploits with carelessly coded unsafe blocks. But if only safe Rust is used, that already does a good job of preventing (completely) things that this "only" makes harder to exploit.
Someone is already preparing an LLVM upgrade to include this.
Gotta agree with you on that. Our safe codebase is basically standing on shoulders of unsafe (=manually safety-checked) code.
The safe stack can include local variables if they're verified to be accessed correctly. Presumably most rust code would land there. So turning it on is probably not going to incur much cost at all.
brson is working on seeing what can be done to improve linking.
I don't know what stack probes are, but what's mentioned in the post is control flow integrity, not really stack cookies. Stack cookies are secret values placed on the stack to detect overflows (which would overwrite the cookies, allowing detection when compared to the secret value).
When running, I get this error: Running `target\debug\tetris-piston.exe` thread '&lt;main&gt;' panicked at 'gl function was not loaded', C:\Temp\tetris- piston\target\debug\build\gl-9653698dd50d604e\out/bindings.rs:22460 An unknown error occurred To learn more, run the command again with --verbose. I'm running win7 x64. I'm connected to it through RDP. I also get this problem when trying to do the default piston spinning square example. I don't get this problem when using glutin alone to create a window. Thanks! Edit: I've been using Rust 1.3 nightly.
I want `box foo()` where `foo` is a `const fn` to always be placement new, even in debug. Otherwise some of my code *will* stack overflow when `UnsafeCell`'s `value` field becomes private (even on release mode, ATM, but I believe this is an important optimization in debug mode too since it affects program correctness; since it is a `const fn` it should be possible to desugar to the equivalent struct initializer form, at least at the moment).
An error might be appropriate, but I know this may not be feasible if it couldn't be checked at compile time
You might be interested in [this](https://github.com/rustcc/coroutine-rs)
There is technically nothing stopping from making something like slice[1..2] = &amp;[8, 9]; work, although it'd be limited by the need to panic! when sizes aren't matching, which would overall prove to be too crashy.
No more than an index out of bounds. I see no reason why this shouldn't work, using a optimised memcpy-like operation. D does something like this. 
I'm sorry I don't know what could be causing that error. Maybe the --verbose command might help? For what it's worth I'm testing on Windows 8 64-bit and Ubuntu
That sounds like what I know of stack probes. This sounds like it will be very useful. EDIT: Though I don't think it necessarily subsumes stack probes entirely.
I had the misfortune of installing this from Cargo in between when the public docs were updated and when the actual crate was updated. The error messages were really confusing me.
There's a report [for exactly this issue.](https://github.com/rust-lang/rust/issues/25069)
Until OpenSSL 1.0.2, it has no faculties for doing hostname validation.
It's interesting to see the `&amp;uniq` proposal! I searched the RFC repository and found some relevant URLs: - https://github.com/rust-lang/rfcs/pull/58 - http://www.reddit.com/r/rust/comments/2581s5/informal_survey_which_is_clearer_mutability_or/ - http://smallcultfollowing.com/babysteps/blog/2014/05/13/focusing-on-ownership/ (Probably what you referred to) After reading these, I'm now sure that the `&amp;my` or `&amp;only` or `&amp;uniq` references definitely have their values. I think it's something like what happened to `enum`. Some say that Rust's `enum` should have been named `union`, because it *is* a tagged union and enumeration is just a special case of it. However when I first approached Rust the naming of `enum` definitely helped me grok the concept. I feel similar concerns about `&amp;mut` vs. `&amp;my`, `&amp;only`, and `&amp;uniq`. They better explain what they are, but are a bit alien from the outsiders' view. I think this could hinder the "friendliness". I admit I was also confused when having met the interior mutability types for the first time due to the `&amp;mut` naming, but as a newbie that naming could have helped me to start learning. (Frankly speaking I now don't remember what happened to me at that time!)
sry no idea just was writein a srt parser in rust and was using this. edit: was starting rust first with 1.0
AFAIK from_str is an old function which used to be the way to do it, but after `parse` was added as the recommended way of doing it, `from_str` wasn't stabilized, and will probably be removed.
no real idea over good programming just was making somthing usefull for me 
https://github.com/rust-lang/rust/issues/26612
I haven't done any benchmarks, but the code is *extremely simple*, and the file, module, line and column are all determined at compile time. The code which is actually run for throwing is https://github.com/daboross/rust-throw/blob/master/src/lib.rs#L286 for throw!() and https://github.com/daboross/rust-throw/blob/master/src/lib.rs#L261 for up!(). It amounts, in addition to anything `try!()` would do, to a single `Vec::push` operation on the error at runtime in both cases. If the result is Ok, the operation is identical to `try!()`.
Yes, exactly! Fixed! I'm not entirely sure what I was thinking writing that.
Cool. I'd benchmark both OK and throwing case for both just to be sure. That way you can quickly see any regressions that might arise in future rust versions.
It seems technically nonobvious, at the least, since it needs to work through assignment to a mutable (which currently means a statically sized `memcpy`). Something like `slice[1..2].set_all(&amp;[8, 9])` would be simpler with today's semantics.
After writing some benchmarks for the `Ok` case, it seems that I'm either terrible at writing benchmarks or by some optimization `up!()` and `throw!()` are faster than `try!()`: https://travis-ci.org/daboross/rust-throw/jobs/68586319#L149 (https://github.com/daboross/rust-throw/blob/master/benches/lib.rs)
That regression happened in the type checking phase. That shouldn't be the issue here.
Oh so you'd need an unwrap() in that example then?
Throw, the verb, usually means that an error *happened* and should be propagated somewhere. let mut file = throw!(File::open("some_file.log")); We don't know there whether the error happens opening some_file.log or not, yet the `throw!` verb might suggest that it does. Please rename the macro to something less ambiguous, like `throws!`, while it's not too late?!
You don't necessarily know at the point of throwing the error whether it's fatal or not (nor at any of the nearby stack frames).
I agree. `throw!()`, from the name, seems to work on Error objects. Maybe rename the macro `try_or_throw!()`?
I wonder how intuitive it would be that if you have `slice[0..10] = (1..3);` that elements `3..10` aren't affected in any way. This differs from what one would expect in Python, where elements `3..10` would be gone from the list.
You can't resize a slice in Rust, so it makes more sense.
Great slides, I think this is a great introduction. 
Capturing stack traces has a cost too, a lightweight solution such as this might end up costing less in a number of situations. There is also the issue that `RUST_BACKTRACE` is global, while here you can have piecemeal use. Might be useful to skip them in hot spots.
I thought we had it down to very little overhead (once the `box` representation was fixed)? Did you try using it in a larger program and find that not to be the case?
Forgive my naivety, but isn't this just delaying the inevitable? (i.e. learning a decent debugger). I guess stack traces are convenient, but I'd much prefer that kind of metadata shunted to a debug segment where it can lie forgotten on disk until required
It's being run on [Rust 1.1.0 stable](http://benchmarksgame.alioth.debian.org/u64q/program.php?test=regexdna&amp;lang=rust&amp;id=2#about).
Thank you very much. I merged your improvements. Runs amazingly fast now. Starts to slow down on approximately 600k particles. Unfortunately, on my mac it still flickers. Maybe it's a display problem? I'll try to compile it at work on Ubuntu tomorrow...
If an error may or may not be fatal, I'll use a normal Result and bubble it up to a site where a decision can be made, and panic is needed.
which means you lose the information of where the original error occured if it is fatal.
That is a perfectly natural pattern in Rust, imo. If you don't like the unit type, you could always create a type alias.
[Here](https://github.com/juggle-tux/srttool/blob/master/src/lib.rs#L81) you're repeating the exact same block of code twice with slightly different inputs and outputs. You can declare a function/closure inline and call it twice to make it more obvious what you're doing.
Cool, it just felt a little strange, wanted to make sure. Thank you.
Seems like the alternative would be an Option&lt;Error&gt;, but that's just ugly since functions that take an Option are usually designed to treat the value as the success case, and None as the error case. Your idea sounds better.
Does ```vec1.extend(vec2)``` work?
&gt; keep banging my head against the compiler This! But hey, it's so satisfying to get a working code in the end of the day. I haven't felt such accomplishment for a very long time :)
It's even [in the standard library](https://doc.rust-lang.org/stable/std/io/trait.Write.html#method.write_all), so it has the closest thing to an official blessing.
As mentioned in the blog post, this is my first ever programming related blog post. **ANY** feedback that can help me improve is greatly appreciated!
I've seen a lot of interest recently in using Rust in an FFI context. I started the *Omnibus* to collect a set of examples that people can just use to get going. I'd love to get feedback (here or in the [github issues](https://github.com/shepmaster/rust-ffi-omnibus/issues)) about missing parts, better techniques, or what more stuff you'd like to see. Thanks!
There's a mode for `PCMPxSTRx` that allows searching for substrings (up to 16 bytes) as well. It's only the "find any one of these bytes" that is ASCII-limited. I'd love to implement this mode as well!
I had actually meant to include that. I'll update it when I get home. :)
This was [cross posted to Stack Overflow](http://stackoverflow.com/q/31084967/155423).
This is one of those insane things that Rust does. I know I wouldn't have come up with even the possibility to implement traits on generic types, and it's a very nice feature to have under certain circumstances, like the one in the article. Sure you could have done something like this in anther way, but it just works out so well like this.
You're looking for /r/playrust; this is about the programming language Rust.
oh yeah maked that out of it https://github.com/juggle-tux/srttool/commit/253bdaa9088a5f761eb004f9267a8fbf5aba41d5 thx
Looks good! FFI is sort of unavoidably a two-way street, so if you want to make a Rust library with a complete C API, you probably also need to cover stuff like std::ffi::CString and shoving bytes into a caller-allocated buffer. The official docs do already have some of that, though.
Thanks for putting this together. I'd love to see some meaty examples which include stateful operations on the Rust side. That is, I imagine an application where Python calls in to initialize the Rust code, then makes subsequent calls to it where internal Rust state is used to perform various operations, then a final call into Rust to tear down all that state properly. It's something I'm working on understanding myself at the moment.
Oops. I always get that backwards. I'll fix it when I get home, thanks. :)
Taking a Rust string, encoding it as a C string and writing it to a provided address is the kind of common task I was thinking of.
`Frobulate` or `serialize`? Great piece!
Haha, whoops. I originally named the trait `Frobulate` in my rough draft because I was having a hard time thinking up names. Pushing a fix now. :)
haha it's all good :) My first-ever PR to Rust got rejected because I sent it to the wrong branch. 
try to add stat() call before dlopen() http://nullprogram.com/blog/2014/12/23/
Thanks. I pushed yet another fix. I really need to proof read these things better and not push them out in a rush because I had to run to the store.
Currently parsing f64's is not very precise. If translated the libc strtod function to Rust. This function does have a very high precision. I've created a crate for this which you can find on https://crates.io/crates/strtod.
The problem is that `fs::metadata` follows symlinks. If you don't want to follow symlinks, you have to use `fs::symlink_metadata` instead.
Oops; fixed now, thanks! Guess I self-corrected at some point. I still remember when I learned that `foo()` and `foo(void)` weren't the same thing for C. The look on my face then was probably amusing.
Yay, legacy code 😑
I think we are on the same wavelength. I'd have it return an object though: handle = lib.load_all_zip_codes_from_the_internet_for('California') myDistance = lib.calculate_distance_between(handle, 92376, 92110) lib.all_done_now_clean_up_those_zips_on_the_heap(handle) Which is a much more common practice, as it avoids global variables, especially global mutable variables! I think I'l ~~steal~~ borrow your example code though!
Solution is to make a pull request? Btw, it's possible to implement Copy for your trait, isn't it?
Thanks for the post and the code. I've made additions to your code some time ago, but I wasn't sure that it will be of any use. Now I have low-level D3D12 Rust bindings, thus I decided to make [a fork](https://github.com/red75prime/dxgen). All new functionality is in OnlyParse.fs. I'm sorry for the code. It's very quick and dirty. Crude low level bindings are [here](http://www.pastebin.ca/3038836). For the high-level bindings I decided to keep COM functionality to the barest minimum. I think it's low-level detail and it doesn't need high-level counterpart. The bindings are not ready yet. I plan to implement them as unrelated Rust structures. The structure will contain bare COM-pointer and it'll implement [HasIID trait](http://www.pastebin.ca/3038843), thus it'll have Drop, Clone and QueryInterface traits. Also, there's no need for generic traits, generic function works fine. I plan to implement high-level functions directly on that Rust structure. Something like this: struct D3D12QueryHeap(*mut *mut ID3D12QueryHeapVtbl); impl HasIID for D3D12QueryHeap { fn iid() {IID_ID3D12QueryHeap} fn new(ppVtbl : *mut *mut IUnknownVtbl) { D3D12QueryHeap(ppVtbl as *mut *mut _ as *mut *mut ID3D12QueryHeapVtbl) } fn expose_iptr(&amp;self) { *self as *mut *mut _ as *mut *mut IUnknownVtbl } } impl D3D12QueryHeap { fn set_name(&amp;self, name: &amp;str) -&gt; HResult&lt;()&gt; { let wname = to_cwstr(name); let hr = unsafe{ (**(self as *mut *mut ID3D12QueryHeapVtbl).SetName).(*(self as *mut ID3D12QueryHeapVtbl), wname as LPCWSTR) }; hr_to_hresult(hr,()) } // ... } Abundance of type conversions is non-issue, as code will be autogenerated. I'm new to Rust and maybe this plan has critical flaws. Any comments are appreciated. 
Well, I don't know, maybe I'm strange, but throw!(something) reads "throw this error just about now" to me, while throws!(something) reads "this something might throw". The same as "do" is different from "does".
I've experimented a little with rules and discovered that it's a really nice effect if you add this as a last rule (before a wildcard, of course): [_,o,_, X,X, X,X,X] =&gt; ( x, y-1 ), It basically says "if it's too crowded, I try to move up" and makes sand looks like it's boiling, adding some nice randomness. And little bit off topic, I'm really happy that we've reached the state in which I just can `cargo run --release` and it just works, even on ARM and with month-old rustc!
I think this would best go into a crate like clippy. The compiler should ideally know nothing about the standard library. It can therefore only report (abstract) violated rules and make generic suggestions.
I don't *want* a stack trace for every error I get. Many errors are entirely normal, like EOF, and having a stack trace for that would be virtually useless. Most of the time when I see a stack trace I have to manually filter out all the third party code that I had nothing to do with anyway.
Sure, I'm not saying this wouldn't be useful--only that I wouldn't want to use it everywhere :)
Thanks everyone. I don't know why I'd put is_ok() and hidden the problem from myself. Adding .write(true) does work. But that brings up the question of why doesn't append(true) imply write(true)? The doco for append() says "This option, when true, means that writes will append to a file instead of overwriting previous contents. ", which makes perfect sense, and the example in the doco is... use std::fs::OpenOptions; let file = OpenOptions::new().append(true).open("foo.txt"); ...which also makes sense. But the doco is wrong right? To my mind that should do the same as... let file = OpenOptions::new().append(true).write(true).open("foo.txt"); which does work, and let file = OpenOptions::new().append(false).open("foo.txt"); could (should?) helpfully fail. Is there a reason I can't see why .append(true).write(true) should need the write(true)? Al
What about having them in a debug build, but not having them in a release build?
There's seemingly a bit of confusion about that: https://github.com/rust-lang/rust/pull/26103 (I have it on my list to actually get this PR shipped this week)
printf is my debugger.
I can say that I haven't really kept up with Python. I picked 2.7 because it was the version that is pre-installed on OS X. \^_\^ Any idea what the current adoption rate of 3.x vs 2.x is?
Using + for concatenation (or even extension like in this case) is a bad idea and can only lead to confusion. Remember someone will need to *read and understand* the code later. Edit: Since it appears someone does not like it, let me outline my reasoning: Addition is usually defined as a commutative and associative binary operation. This fits things like numbers, same-sized vectors or matrices of numbers, or even sets (where it sometimes is used to denote union, in lieu of a special operator). However, concatenation is order-specific, and thus not commutative. Also defining addition on slices by blanket impl would preclude use to denote vector- or matrix-addition later on. In general deciding whether to use an operator for a specific operation should be based on strong tradition, common usage or both, otherwise its use may bring more confusion than the resulting reduction in code size is worth. A named fn at least has the name to denote the nature of the contained operation. Even if the operator is an uncommon one, its use may lead to hard to read code, as some Scala testing code I recently read exemplifies.
This is great! I was just thinking of doing something like this myself. I'd love to expand this to cover more than just C, Python, and Ruby... I want examples with *every* language. :)
Sounds kinda dangerous having your code change behaviors between debug and release builds. What are you trying to do? 
Is it correct to consider burritos a meme in the context of PL...
I don't know if you understand what I tried to say, so let me try summarizing. Sometimes Rust errors could give more specific error messages when there's specific types in specific places. Should a mechanism for this exist in rustc itself, or should it be bolted on via a compiler plugin crate? And if it is via a compiler plugin crate, is it currently possible? I was using the specific error message that I showed as an example literally pulled from a blog post talking about this specific instance. But in this case, no, there's no trait to implement Copy for. HashMap is a type and as it's part of the standard library, I can't implement Copy on it.
See, monads are easy! All you need to know is that monads are like memes in that... i'll stop
Output a log file to the directory the program was called from and enable logging of log levels such as "debug". I currently have the log file being put in a directory in the user's home directory. I suppose I could just implement something like a `--debug` flag in my program though.
The traits are named `Decodable` and `Encodable`. They are defined in [`rustc-serialize`](https://crates.io/crates/rustc-serialize).
Yeah, right. Thanks. I'll better make use of `CString::from_ptr` and `CString::into_ptr` then. It's still a bit sad that I can't check whether the memory has leaked out or not. And, both of those methods are only in the nightly version :(
Sorry, I am afraid I was arguing over something slightly different. There have been suggestions that the backtrace be captured in `Error` as well, and not only panics, which would be much more costly. The alternative presented here by /u/daboross appears more flexible and lightweight, and might therefore be much more compelling than full-blown backtraces.
Is this more efficient than assigning it to a variable? And is there another, more efficient to initialize all of the vectors without a for loop?
I usually don't like video because it's probably not the right learning speed for most, needs massive amounts of data where plain text can transmit more precise information at your personal learning speed with much less effort. That said, to each their own. Perhaps others not trained so much in the art of abstraction will find text too dull and learn better from video.
For what is worth, I know this game by the name of Mastermind. 
`game.board = vec![vec![0u8; height]; width];`
I am currently one of those newcomers and I would watch it. I find that watching video tutorials can make things click for me in a way that a text can't.
&gt; Perhaps others not trained so much in the art of abstraction will find text too dull and learn better from video. That's a nice jab.
Thank you. I was not aware of `Rust-PHF`. it looks very useful for many projects I have planed including the extension list as you suggested. Thank you very much for mentioning it. Since we're talking about compile time look up generation, do you know if anyone has written a compile-time tree-search lookup generator yet?
&gt; `pub extern fn mimty_blob(b: *const u8, length: u32) -&gt; *const u8` (in [mimty/src/ffi.rs](https://bitbucket.org/joshmorin/mimty/src/d19683a37bd199f16212db877f94f534374f1bbe/src/ffi.rs#cl-52)) Should `length` be `libc::types::os::arch::c95::size_t` instead? &gt; `pub extern fn mimty_file(path: *const u8) -&gt; *const u8` &gt; At present, paths that contain invalid utf8 will cause this function to return `null`. &gt; This will be fixed as soon as `from_osstr` or `as_osstr` functionality is implemented for `[u8]`. &gt; If this is an issue, consider passing a dummy filename with a matching extension and using `mimty_blob` to identify the contents. (in [mimty/src/ffi.rs](https://bitbucket.org/joshmorin/mimty/src/d19683a37bd199f16212db877f94f534374f1bbe/src/ffi.rs?at=master#cl-22)) It looks like one would need a separate return value or type to signal such internal/usage errors. There seems to be [`OsString::from_bytes()`](http://doc.rust-lang.org/std/ffi/struct.OsString.html#method.from_bytes) (but I don't follow the FFI RFCs, so I don't know about its stability/future). I don't quite understand the dummy filename workaround; Do I need to rename/move a file with a new sanitized path that is valid utf-8, but having the original file extension (which itself then needs to be valid utf-8)? If I pass a filename that does not exist, the `blob()` magic inside `file()` can not read the file. If I just use `mimty_blob()` and read the file contents myself, why would I need a dummy filename? P.S.: The `mimty` (MIME type?) name is somewhat outdated, as the new formal [RFC2046](https://tools.ietf.org/html/rfc2046) name is `Media Type` :) P.P.S: Using `libmagic (3)` with `MAGIC_MIME_TYPE` should yield similar results. 
&gt; The struct also has a Deref impl that returns a reference to the base type, so that you can call its methods and any methods of its bases. You reflect inheritance structure of underlying C++ interfaces. Am I right? Is there any advantages to it? I decided to go with flat (no inheritance) interfaces. Plus side: no Deref needed. Minus side: code duplication. Code duplication is not a big issue as wrappers are thin. About thread safety. Most D3D11's methods aren't thread safe, so the utility of adding Send to all Rust wrappers is questionable. D3D12 doesn't have threading guidelines yet. Thus I decided to postpone decision, as interface-level and method-level sync are easy to implement, by wrapping COM-pointer in mutex and by adding mutexes to Rust COM-interface wrapper. Thread safety of queried interface is tougher, but nothing unmanageable. I think about "Sync on QueryInterface": enum SOQ { Plain(*mut *mut IUnknown), Synced(Arc&lt;Mutex&lt;*mut *mut IUnknown&gt;&gt;), } struct D3DDevice { vtbl : SOQ, } QueryInterface will promote vtbl to synced version. However, there's problem with Clone, as clone takes immutable borrow. Yes, I need to think more. Thanks for pointing out. Edit: I misunderstood you. I thought you aim to provide Send for all wrappers. 
Hi! &gt; Should length be libc::types::os::arch::c95::size_t instead? I tend to shy away from types with architecture-dependent sizes. I deliberately chose `u32` (hence `uint32_t`) because I, as an API user, don't want to deal with the inconsistencies on my side. I've adopted this view relatively recently, so feel free to correct me if I'm misguided. &gt; It looks like one would need a separate return value or type to signal such internal/usage errors. Both functions, and the library in general, report the MIME type on best-effort bases. Neither extensions or magic numbers can guarantee that a file is of a certain type. The only way to be sure is to parse the file in its entirety. That is the reason why the functions return `Option&lt;&amp;str&gt;` instead of `Result&lt;_, _&gt;` in Rust, and pointer or NULL in C. The results should be taken as "here's what I could find out" rather than solid facts. If the function fails, returning `None` would be within semantics. &gt; OsString::from_bytes() I tried to use it. It seems that OsStringExt is private. &gt; P.S.: The mimty (MIME type?) name is somewhat outdated, as the new formal RFC2046 name is Media Type :) That's why I chose it; no chance of conflict. (The was originally named mimety, but I opted for the shorter name to make life easer for C programmers. One character can make all the difference)
It's probably not more efficient - it just allows you to explicitly say to the compiler 'I know I'm not using this, and it's not a mistake.'
I'd like to add to this since I didn't know it was available for a long time: you can keep the variable name for readability as long as you prefix it with an underscore. So either `for _ in 0..height` or `for _row in 0..height` would work, though the latter provides a bit more context.
Do you know if there's any plan for the *BSDs to get official support? Edit: This is a great step too, so thanks :)
Does it work on older, non-SSE2 processors?
Get over yourself
Not in this case, but it probably doesn't matter because LLVM is pretty good about figuring out when you're not using a variable at all and optimizing accordingly.
I think [CStr](http://doc.rust-lang.org/nightly/std/ffi/struct.CStr.html) or [CString](http://doc.rust-lang.org/nightly/std/ffi/struct.CString.html) is more suitable than OsString. OsString is meant for interaction with the OS and may have odd character representations, while CStr and CString simply conforms to the C standard, as far as I know.