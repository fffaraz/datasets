Takes a genius to draw a flag.
Hey cool thanks for the tips! &amp;#x200B;
It's easy to get into a debate about terminology here, but I think the essential answer to your question is yes. Rust and C and C++ all combine down to the same stuff. If you have a Rust program that does something reasonable for all possible inputs, there's no reason you couldn't write C code that compiles to exactly the same program. Or for that matter, you could just write the assembly yourself, even though assembly is about the unsafe-est thing imaginable.
rustc may be emitting IR from a newer version of LLVM than ikos understands. One could try using an older compiler (e.g. jump back to 1.0.0 or before using rustup) to see if the versions line up better.
/r/lostredditors (you want /r/playrust) 
&gt; must register with the government when you get the software This fails both [the Desert Island Test **and** the Dissident Test](https://en.wikipedia.org/wiki/Debian_Free_Software_Guidelines#debian-legal_tests_for_DFSG_compliance).
A very nice picture, but I'm very sure you wanted /r/playrust
1. Yep! It'll always be around in the foundations of the language. The only way to avoid unsafe code in the implementation of fundamental data structures like `Vec` or `Mutex`, would be to move those implementations into the compiler itself, such that `Vec` and `Mutex` were magical built-in types. But that wouldn't actually make anything safer -- it would just mean that we need to audit more code in the compiler instead of auditing code in the standard library. 2. The rules for defined vs undefined behavior in Rust are sorta-kinda-well-specified. In most cases, everyone can look at a block of unsafe code and agree that it does or doesn't uphold the guarantees it's required to uphold. (No aliasing mutable references are created, no dangling pointers are dereferenced, etc.) The project to create a completely rigorous memory model for the language is ongoing, and I don't know when we should expect it to land. Maybe a year or two? Apparently it's a pretty big project. But even without a rigorous standard for the memory model, there's been some initial work to prove the soundness of parts of the standard library: http://plv.mpi-sws.org/rustbelt/ 3. At some point, the problem of checking the safety of a program converges with the problem of verifying proofs in mathematics. How do we know that the mathematicians who checked a proof didn't make a mistake somewhere? It's [happened before](https://math.stackexchange.com/questions/139503/in-the-history-of-mathematics-has-there-ever-been-a-mistake)! Programs like Coq can automatically verify proofs for you, but I suppose the obvious next question is how do we know Coq doesn't have any bugs? I don't know much about this topic in mathematics, and I'd love to grab a pint sometime with someone who does. 4. Totally. One of the big ones (this was before 1.0) was that the original API for spawning "scoped" threads was unsound. Take a look here for all the gory details: https://github.com/rust-lang/rust/issues/24292
/r/playrust
this doesn't seem to be working for me in my post below. It's just four spaces like stackoverflow, no?
awesome thanks for the feedback! Everything's starting to click. I'm going to try to get the project structure set up over the weekend so I can focus on actual coding
I want mrustc to become a fully-capable Rust compiler, capable of building `rustc` HEAD as well as real-world projects, for targets that `rustc` can’t build for.
You are probably looking for /r/playrust
This is the subreddit for the Rust Programming Language. /r/playrust
SORRY lol xD !
This type of thoughtfulness really makes me hopeful about the future of Rust! Thanks!
preface: I apologize for any code formatting issues, I don't really post often and following reddit formatting, 4 spaces wasn't working for me when editing my other post. another high-level question I have is, lets say I have some type (or object) that has 4 possible variants determined by the field `kind`. The variants look similar but not entirely. For example, one of the 4 variants requires the `quantity` field and the value of that field must be greater than 0. Another variant has the `quantity` field as optional, but if set, it too must also be greater than 0. The other 2 don't have a quantity field (or rather it will just be ignored). In my head, I start reaching for enums to represent this type using serde's internal tagging: #[derive(Serialize, Deserialize)] #[serde(tag = "kind")] enum MyType { #[serde(rename = "variant1")] VariantOne { id: String, quantity: u32, ... }, #[serde(rename = "variant2")] VariantTwo { id: String, quantity: Option&lt;u32&gt;, ... }, #[serde(rename = "variant3")] VariantThree { id: String, ... }, #[serde(rename = "variant4")] VariantFour { id: String, ... }, } However I think you can see where I'm going with this, the anonymous structs can be rather large and each with a number of similarities which makes it feel very unwieldy. Also I don't think I can use named structs e.g. create a `VariantOneStruct` and use a tuple variant `VariantOne(VariantOneStruct)` since it seems like serde doesn't support internal tagging for tuple variants. Another option is to have the variants done on the field itself: #[derive(Serialize, Deserialize)] struct MyType { id: String, kind: MyVariants, quantity: Option&lt;u32&gt;, // ... } But I feel like I'm missing a good opportunity to have the type system help me out in this case since it means I'm going to have to do a lot of checking myself. Really stretching my rust knowledge here, I have another idea where the variants are actually struct tuples e.g. `struct VariantOneStruct(String);` and then implement validations that are strictly for those variants, passing a `MyType` struct to those validation functions. and then the `kind` field would be an enum using tuple variants with each variant taking a struct tuple which serde should be fine with in this case since it isn't internal tagging. 
Const generics is _more powerful_ than type-level integers, though. It allows types to be generic over const value in general, not just integers. I think part of what this post is saying is that the general case is not as important as type-level integers in particular, in which case the distinction matters.
&gt; I want to build Rust binaries on my Arch desktop and run them on a Raspberry Pi and Chromebook (GalliumOS), but openssl is a constant pain I'd like to see https://github.com/rust-lang/cargo/issues/1197 for such cases (another one: `rusqlite` on Windows).
If we're talking long term, the communication has to be through a C ABI (extern fn in rust). This is because Rust's ABI is very unstable. Not as in C++ is unstable, since they have de facto stable ABIs (for the moment). Alternatively you can serialize the data to an intermediary format to communicate, like JSON. 
I personally think that using a different compiler backend for debug builds is a terrible idea that will break debuggers and harm the ability of developers to use the language, especially with something so young as Cranelift. It should be an option in .cargo/config and nothing more. 
Yes, but if you use the reddit editor you can highlight all the text and click the button and it will automatically do it so you don't need to type it
If you read the blog post, the network services group is pushing for more abstraction between the web server and the web framework. Decoupling these two things allow greater flexibility and makes it easy to swap out web servers without rewriting your application code. “These interfaces specify what it means to be a web server, which generally looks like some kind of function that is given a request and produces a response (usually for HTTP). Having such an interface means you can decouple a web framework (which helps you produce such a function) from an underlying web server (which actually handles the work of implementing HTTP). It also makes it possible to provide generic low-level middleware that’s usable with any choice of server and framework. Today in Rust most, but not all web frameworks use hyper directly to provide basic HTTP functionality, which prevents this decoupling and easy middleware application. However, the tower-service crate is poised to provide a more generic interface through its Service trait, and there is already a substantial amount of middleware as part of the Tower project, though it has not yet been published to crates.io.”
There's no way any corporation would stand for that kind of thing. The only places I could see it used would be medical device embedded code or aerospace code.
Yeah I figured it would be through a C ABI. I tried prototyping it earlier today and couldn’t get it working, but didn’t spend much time on it. I’ll give it another shot soon
Not only that, if we followed up on that wish we would all be using Algol 68, K&amp;R C, Fortran 66, Lisp 1.5, and so on. 
It is an option available at least on GHC, OCaml, Eiffel and .NET, and I hardly seen complaints about it.
Yep, you can check my dummy Gtk-rs port of an old Gnomemm article. While the old C++ code takes about 5 minutes for a first compile, the Rust version takes about 18 minutes. First compile on a dual core netbook with 8GB. Naturally it isn't properly fair, because with C++ I get to enjoy binary dependencies, while cargo has to compile everything from source.
&gt; Note: this trait must not fail. If the conversion can fail, use TryFrom or a dedicated method which returns an Option&lt;T&gt; or a Result&lt;T, E&gt;.
&gt; C++ instantiates templates over and over in each TU and then combines them in the linker; Since C++11 it is possible to use extern templates if we know in advance the most common use cases for the type parameters, thus reducing significantly the compilation speed. Also some of the GCC talks related to the ongoing modules support seems to hint that it will also be a very good help.
Great talk, I feel like I have a much better understanding of simd and how to effectively use it now. My only question though is if you are loading and storing these values 4 at a time via a pointer, what happens if your array is not a multiple of 4? Wouldn't this just clobber unrelated data that is stored past the end of your array?
Currently i struggle to create a r2d2 ConnectionManager for a trait `Store`: `impl ManageConnection for StoreConnectionManager where Store: Send + Sync + 'static` `{` `type Connection = Box&lt;dyn Store&gt;;` `type Error = StoreError;` `...` `}` The compiler gives me `error[E0277]: \`(dyn store::Store + 'static)\` cannot be shared between threads safely` `--&gt; lib.rs:18:1` `|` `18 | / impl ManageConnection for StoreConnectionManager` `19 | | where` `20 | | Store: Send + Sync + 'static,` `21 | | {` `... |` `39 | | }` `40 | | }` `| |_^ \`(dyn store::Store + 'static)\` cannot be shared between threads safely` `|` `= help: the trait \`std::marker::Sync\` is not implemented for \`(dyn store::Store + 'static)\`` `= help: see issue #48214` How can i convince rustc? The strange thing is, before i had a type parameter where i managed to make it work. I had hoped that things would be more easy, but they got more difficult.
New warnings can definitely be added within an edition. (\`-D warnings\` is explicitly *not* stable, and *will* break your build unless you pin the compiler version.) It's specifically error-by-default lints that can't be added within an edition. &amp;#x200B; (The one escape hatch is still future-compat lints which upgrade to a full error, since they all are warning against broken code, but IIRC we still prefer to upgrade those on an edition if possible.)
You're looking for r/playrust
[You can write it that way](https://doc.rust-lang.org/std/primitive.f64.html#impl-From%3Ci32%3E), and [clippy recommends it](https://rust-lang.github.io/rust-clippy/master/#cast_lossless).
yeh thay doo
My biggest concern with is idea is that it decreases readability. It is no longer visible on the first glance that a closure is involved. And the Rust language is already not easy and names are often abreviated. Adding further complexity just to safe on a few characters to type is simply not worth the effort that everyone has to learn to read that syntax. Especially beginners would be overwelmed. I know that Scala has a lot of syntax abreviations, so you are probably good at understanding them. But Rust is probably as simple as it can be for what it does (and that is still complicated).
Any development area as young as Rust web frontends will always attract multiple people trying things out, seeing what works, what doesn't, and generally trying to follow their vision. Consolidation comes later, as some projects attract users and others don't. This isn't reinventing the wheel. It's engineers tinkering with this newfangled wheel thing, trying to find which design works best.
Both crates can just pretend that the other side is C code, and then go from there. Examples (using winapi DLL loading): * https://github.com/Lokathor/hotload-win32-rs * https://github.com/Lokathor/vibha-rs 
A missed #Rust2019 entry: https://internals.rust-lang.org/t/rust-2019-correctness-and-stabilizations/8991
To be fully compatible with the C++ version is one goal. Two others I have [thought of](https://users.rust-lang.org/t/getting-started-with-rs-pbrt-rust-based-pbrt/22332/7): 1. [Render Arnold's scene description](https://github.com/wahn/rs_pbrt/issues/55): That would give me access to production scenes (@ The Mill, where I work). But in general that goal could be extended to a **more general idea** to have several parsers reading scene descriptions in a kind of renderer agnostic way, so people can use those libraries/crates to get scenes for their own rendering experiments. 2. **Denoising**: There is an [issue](https://github.com/wahn/rs_pbrt/issues/62) open about a particular algorithm, but obviously there are other algorithms with a similar goal, e.g. [this one](https://benedikt-bitterli.me/nfor) or just implement what Blender's Cycles renderer does ... 3. In general I would like to spawn **rendering research** using **Rust** instead of C++, but of course all the credits so far should go to the authors of the [book](http://www.pbr-book.org). It would be nice though if newer research could use `rs-pbrt` as a foundation to build upon.
ikos requires llvm 7 though
And then there is research on the C++ side as well about making things more **production ready**: https://pharr.org/matt/blog/2018/07/16/moana-island-pbrt-all.html Thanks to **Disney Animation** for providing such scenes: https://www.technology.disneyanimation.com/collaboration-through-sharing 
Yet the code struct Foo; impl From&lt;usize&gt; for Foo { fn from(_value: usize) -&gt; Self { panic!() } } fn main() { Foo::from(0); } compiles just fine in saferust. Definitely don't rely on `From` not panicking for safity guarantees in unsafe code.
It took me five minutes to realise that every \_ stands for something different! This is really confusing and decreases readability just to safe a few characters. Please keep in mind that not everyone is as good in Rust as you :)
The point of that paragraph was to say that type level integers are _more_ important than const generics. 
I agree Edition was necessary, but I don't agree with the way they were handled. The edition system tried to mix two orthogonal goals : introduce breaking changes in the syntax and produce milestones to advertise the Rust evolution. IMO The problem with changes is that the Rust team has been so traumatized with the pre-1.0 era, they refused to admit what they were doing a necessary breaking changes. Because Edition actually are a breaking change, even if there is a great compatibility system with the previous edition. The Edition system is just confusing. Rust compilers supporting the new syntax should have been labeled 2.x. As for the marketing, the fixed date completely defeats the goal. The message is completely confuse when you want to advertise all the improvement since 2015 but ends up presenting the actual breaking changes witch are for unfinished functionalities. If you want to present the evolution of the language, your edition must be feature complete.
Is there any non-framework front-end? I really find it useful to use libraries instead that I can call into. Iirc there was/is a C++ library that you call and it returns opengl invocations that you can run yourself so you have a GUI, that's really cool and I think it'd be perfect for Rust.
&gt; They serve as a standard point for books to be written around Rust. They serve as an opportunity to "catch up" on all the Rust happenings for those who don't follow everything going on in the 6 week release cycle. Not very well.. Theres still things in the 2018 edition guide that *aren't* in 2018 edition. One i really wanted was the `?` kleene operator for macros.
&gt;"must register with the government when you get the software" Seems like the very definition of overbearing :) &amp;#x200B;
How will it break debuggers? Cranelift will certainly need to have a good debuginfo implementation, and I don't see that happening for quite a while (although work on it has started), but I think it is a given that Cranelift needs that before it can become a default option, and it needs to mature. Long term though, are there any fundamental problems?
What does the Rust native UI landscape look like for someone looking for a way to contribute? What is likely to survive in a long run and what is likely to die?
The automotive industry has killed a lot of people with software bugs. They are very interested to prevent that in the future with standards like ISO-26262
&gt; The 2021 edition Could we call it the 202x edition? Let's not repeat the 2018 experience of prematurely branding the edition and then rushing it.
Lighthouse is open source. If you just want source, find it at https://github.com/sigp/lighthouse.
Totally agree on compile times. Not so sure about 80% solutions. I think the fact that crates are so well thought out js one of my favourite things about the Rust ecosystem. It means I can use the de facto standard with confidence and not have to worry about it not working for my later use cases.
&gt; I think it's important for Rust to find an answer to NPM incident. This is a common thought, as it should be. NuGet in the .NET ecosystem is also moving this way: https://blog.nuget.org/20181205/Lock-down-your-dependencies-using-configurable-trust-policies.html
I completely agree on the edition bit - the final stretch of releasing the 2018 edition definitely felt like slightly rushed and against the “avoid ‘moving fast and breaking things’”. I probably felt the breakage more than most on nightly, but I still don’t think this is how we should be preparing for indefinitely-stable releases.
Hey! I'm working with BLAKE2 for a cryptocurrency application and stumbled on this. There's surprisingly little code out there for parallel BLAKE2 using AVX. My particular need is for a pure C implementation of BLAKE2 for AVX512. Does that sound like something that might be up your alley? We could work out compensation.
Think about what you're doing. The point of trait objects is so you can call methods on structs without knowing what exact struct it is, only that it implements that trait. So what methods does Store implement? Both Send and Sync are marker traits, and 'static is a lifetime. There are no methods. I'd recommend opening the Rust book and finding the page on Trait Objects. It shows the definition and requirements for using one. Also think about if this is this X Y problem, ie. do you really need trait objects? Or can you do this in another, better way?
I found reading through the rust book (I didn't bother with the exercises), super helpful, even though I'm normally more of a jump into projects kind of person. Rust has a lot of concepts that weren't familiar to me (although they may be more familiar to you if you know both C++ and haskell), and the way programs fit together isn't quite like any other language. So it was important for me to know what was out there and available for my use before I started experimenting.
Once again: Thanks for your input. All this is so interesting. But before I get into the questions: How do you know all this? Are you part of the Rust team? Are you a professional programmer? Long time Rust user? I get that at some point I'll be able to handle Rust coding, but how do you know all the behind the scenes stuff? And on to your points: 1. Understood everything but this: &gt;it would just mean that we need to audit more code in the compiler instead of auditing code in the standard library. Why is auditing `std` better/easier than auditing the compiler? 2. I checked out the link and holy shit they're looking for postdoc and PHD level contributors. So this must be quite non-trivial. Makes me feel better about myself. Haha ;-) 3. I'm nowhere close to actually being able to understand it, but the fact that there are ambitions/efforts to proof this is awesome. Way to go Rust team. I didn't even know such a thing as Rust belt existed. Is this something other languages have/try/want as well? I guess not since none of them is claiming to give safety guarantees. (with the exception of Ada maybe?) 4. and your closing note: You are right. I don't know the correct word (English isn't my primary language) but when I learned that Rust uses unsafe() on the "inside" I was a little bit "disappointed". I thought that Rust totally completely solved this memory/safety problem. But when I think about it: It actually solved this problem, right? If I write thousands of lines of code and I don't use any unsafe() blocks, Rust still guarantees memory safety. That is correct, no?
Good question. Yes it is common that you have to deal with the remainder data. Often you may already have a scalar version of your function around which you can use to compute the few remaining values, or you can just use the SIMD function and then extract the lanes you need. 
Thank you for your input. With `where Store: Send + Sync + 'static` i've tried to declare, that a thread-safe store is wrapped behind a connection pool. I don't understand how object safeness is related to thread safeness. Can you elaborate? 
I think you have part of the documentation switched up between [these two methods](https://gitlab.com/chrysn/sealingslice/blob/master/src/lib.rs#L72-81).
Yeah, you can’t rely on From::from not panicking for safety, but you can complain if someone adds a From implementation that does panic. In the future we could try to extend From::from with a ‘#[no_unwind]’ attribute that, eg, aborts if a panic happens, and does nothing if the compiler can prove that the method cannot panic. While the docs do state that you shall not write a From::from that can fail, I don’t know if such a change would be considered “breaking” - a From::from impl that panics is broken anyways and breaking broken code is fine.
Note that if you use the SIMD function, and it reads memory out of bounds, then that’s undefined behavior.
The important thing about the edition is not that everything is ready, but that all breaking changes are known. Const generics and async can be done within rust 2018 because the syntax is already known and can be introduced without breaking existing rust 2018 code.
You probably wanted to declare `Connection` like this (and then remove the `where` bound): type Connection = Box&lt;dyn Store + Send + Sync + 'static&gt;; Currently you have a `where` bound for a concrete type - the type of `Store` trait object. That bound is like saying `where i32: Iterator` - it does not really make any sense - `i32` is either an iterator, or it isn't. Currently when the compiler sees such bounds it tries to check that they hold. So if you added a `where i32: Iterator` bound to a function, you would get a compiler error that the bound does not hold. Currently the bound on your trait is like that - it asks that the trait object type is `Send`, but the compiler knows that such bound is simply not true (because `Store` does not have `Send` as supertrait, and so anyone could implement `Store` for a non-Send type and then make it into a trait object), so that's why you get the error.
Right, rather than reading from the start of your remaining data which would venture into memory you don't own, you should read such that the last lane is the last of the remaining data.
&gt;Currently you have a where bound for a concrete type - the type of Store trait object. I thought that `Store` is the trait and `Box&lt;dyn Store&gt;` is the type of the trait object? I will try your suggestion tonight, thank you. I am honestly a bit lost regariding trait restrictions right now.
Whoa, that looks great. Sometimes it seems to me that the Rust Book doesn't do enough work in making people not use macros :)
I've been reading about this rust edition thing in a few articles now. I still don't really get what it is and I'm unable to explain it to anyone in a coherent way. It seems to generally require lengthy explanations that are mainly about what it is not and why it is not that. I get that the frequent Rust releases using good old semantic versioning tend to be mostly backwards compatible (which is great). But this whole edition thing seems to be completely orthogonal to that. People seem to insist that it is not like bumping the major version at all. So great, it's not that either. Compare this to other recent languages like Kotlin and Go. Go is currently preparing for an eventual 2.0. Mostly it's supposedly going to be backwards compatible but they are taking the opportunity to do some limited amount of change. OK, I get that. Should be fine mostly but expect some minor pains migrating/upgrading. Kotlin has had a 1.0 a few years back and just pushed out 1.3. 1.3 stabilized a few experimental features introduced with 1.2 , adds a few new experimental features, and deprecated some features. Easy to explain what is what. &amp;#x200B;
I am grateful, for I love the style
I am trying to create something like this for Virtual DOM:s with [vfin](https://github.com/axelf4/vfin).
All code within a rust crate must be the same edition, Rust 2018 and Rust 2015 code can't be mixed in the same crate, however a crate written in Rust 2018 can use crates written in Rust 2015 and vice versa. The same compiler is able to compile Rust 2015 code and Rust 2018 code. If your Cargo.toml doesn't specify an edition then even the newest Rust compiler from the year 2038 will assume that the crate is Rust 2015 and compile it that way.
The core team explicitly asked for these posts to get community feedback, it's part of how they decide what to focus on for the year. I'm not sure why you would want this to stop. You're probably getting down voted because your comment adds nothing too the conversation, which the sidebar says is what you're supposed to get downvoted for.
Thanks, indeed I had. Fixed for the next release.
&gt; You're probably getting down voted because your comment adds nothing too the conversation, which the sidebar says is what you're supposed to get downvoted for. Yes, and that actually means nothing in Reddit's context. Votes don't matter, but my point was that there could have been a single thread for all these points. Right now, all it does is clutter up the front page. 
When you declare a trait called `Foo`, you get a trait called `Foo`, and also a type that is also called `Foo`. To reduce the confusion, `dyn Trait` syntax was added, but because of backwards compatibility simply `Foo` is still allowed in type position. You might want to add `#![warn(bare_trait_objects)]` to your crate root to warn about the cases when you forget the `dyn`. Now the important part is that the trait object type is simply `dyn Foo` - not `Box&lt;dyn Foo&gt;`, not `&amp;dyn Foo`, not `Rc&lt;dyn Foo`, and not `MyCustomSmartPointer&lt;dyn Foo&gt;`. This allows you to use trait objects without forcing dynamic memory allocation, dependency on `std`, or other horrible things you might want to avoid. The thing that makes seemingly ambiguous syntax (having both a trait and a type called `Foo` in scope) work, is that you can actually always determine whether a name refers to a trait or a type by simply looking at the syntactic structure of the code - you don't need to know where specific names come from. The rules are rather simple, because traits can only appear in three positions (if I didn't forget anything): 1. In supertrait list of trait declaration: trait Foo: Bar + Baz&lt;Quux&gt; {} // ^^^ ^^^ ~~~~ a type, because it appears // | | in generic parameter list // | trait // trait 2. In bounds list of some type. Note that the bounded thing is always a type. where Foo&lt;T&gt;: Bar + Quux&lt;i32&gt;, // | | | | ~~~ type // | | | quux the trait // | | trait // | type // type Quux&lt;U&gt;: Send, // | | ~~~~ trait // | type // quux the type (should have been "dyn Quux&lt;U&gt;") 3. After `impl` in type context, as part of `impl Trait` syntax: fn foo(x: &amp;i32) -&gt; Foo&lt;Bar, impl Baz + Quux&lt;Baz&gt;&gt; { ... } // | | | | ~~~ baz the type (should have been "dyn Baz") // | | | trait // | type baz the trait // type 4. After `dyn` in type context, as part of `dyn Trait` syntax. In case you have multiple traits (like `dyn Foo + Bar`), only one of them can be non-marker trait: fn foo(x: &amp;i32) -&gt; &amp;dyn Foo + Send { ... } // | | ~~~ trait // | trait // the whole thing is a type, or a trait object type to be specific // in this case either Foo or Send (or both) must be a marker trait So in your case bounded thing was `Store`, so it must have been a type - and `Store` was declared to be a trait, so that means that you tried to put a bound on a trait object type.
Here's a good article with suggestions on how to introduce NPM package permissions [https://hackernoon.com/npm-package-permissions-an-idea-441a02902d9b](https://hackernoon.com/npm-package-permissions-an-idea-441a02902d9b) 
Great talk. Also the way that you presented this worked really well on Youtube. &amp;#x200B; Checking out your library now.
Thank you so much for the clarification.
(I'm the original responder, not /u/oconnor663. /u/oconnor663's answers are 100% spot-on though!) &gt; But before I get into the questions: How do you even know all this? Are you part of the Rust team? Are you a professional programmer? Long time Rust user? I get that at some point I'll be able to handle Rust coding, but how do you know all the behind the scenes stuff? Personally: I just really like programming languages and I've been following Rust's development for years! I also contribute to [a compiler](https://www.pyret.org/) as part of my work. As for the points: 1. **Why is auditing `std` better/easier than auditing the compiler?** It's not necessarily better; it's just different. As a rule of thumb, however: compilers are _really_ complicated. If a language has enough expressive power to express a type like `Mutex` in the standard library without resorting to built-in magical types, it's almost certainly to define `Mutex` as part of standard library. Another benefit of this: _anyone_ can use `unsafe` and build their own version of `Mutex` if they really need to! 2. **I didn't even know such a thing as rustbelt existed. Is this something other languages have/try/want as well?** Absolutely! [CompCert](https://en.wikipedia.org/wiki/CompCert) is C compiler formally verified to be correct with Coq. [LiquidHaskell](https://ucsd-progsys.github.io/liquidhaskell-blog/) extends Haskell's type system with a SMT solver to allow for proving invariants at compile-time. 3. **But when I think about it: It actually solved this problem, right? If I write thousands of lines of code and I don't use any unsafe() blocks, Rust still guarantees memory safety.** Yeah, basically! An important caveat: there are still occasional compiler bugs. I cannot stress enough just how complicated compilers are. Sometimes the bugs are in `rustc`, sometimes they're in LLVM (which is responsible for generating the actual machine code, but isn't part of the Rust project). For practical reasons, a fully-verified Rust compiler is unlikely, but with each additional part of the compiler or standard library that's formally verified, you can gain confidence that your safe programs are absolutely free of memory unsafety. 
I think an HKP mechanism is something that's really interesting. Not that it would be used right away, but people will definitely cook interesting things up.
* Editions are for breaking changes. Any language change that isn't breaking is shared across all editions. * You can mix dependencies that have different editions. * A compiler supports running code from all editions released when it was released. What does that mean? Imagine that python 3 was instead a python edition - you could call a python2 library from your python3 code, and if you had python2 code lying around, you could run it with the same interpreter as your python3 code. I'm not sure how Go's v2 migration will work. Kotlin's experimental features are most like Rust's nightly channel/nightly features.
My biggest issue with From is that it doesn't work in const contexts. `const fn` doesn't support trait methods, and restricting `From::from` to const would probably be too limiting. I would be interested in some kind of system where there would be a way to implement a trait method as const, even if the function is not declared as const in the trait definition. In contexts where the compiler would know about the constness, it could then be used just like a normal const fn. Example: `const BAR: i32 = i32::from(FOO)`. Otherwise, `as` will remain widespread in const contexts. ... Or we would need to have a `ConstFrom` trait.
An Edition is a set of breaking changes that you opt into on a per-project basis.
My attempt at a brief summary: Rust 2015 has Some Features. Rust 2018 has Some Features Plus Some Backwards Incompatible Stuff. When More Features get added to the language, it gets added to both editions. Yes, it's more complicated than it should be.
I’ll take a look at those, thanks!
I'm absolutely down for collaboration. I chose to add to the pile since I couldn't get the others to work on account of minimal documentation.
In my experience a tool-assisted audit is, for whatever reason, easier to get signoff on than refactoring an existing, entrenched codebase for meaningful unit testing.
It would be nice to extend this to allow signatures from non-authors. That way you could feel safe depending on crates that you don't know the author directly, but the author/crate has been vouched for by a reputable source.
Start with writing something simple... Like REALLY simple. For me it was Conway's Life. Made me concentrate on the core of the language instead of all the cool features. Also whenever you see a `&amp;` in code don't think "reference", think "borrow".
It may be open source, but definitely not free software according to https://www.gnu.org/licenses/license-list.html#NASA for the reasons mentioned by /u/Chandon.
Sure, I'd be interested in working on a project like that. Could you reach out to me over email? I'm [same username] @gmail.com.
https://tldrlegal.com/license/nasa-open-source-agreement-1.3-(nasa-1.3) states the following: &gt; This is a license used by NASA for some of its software. Note that you must register with the government when you get the software and are asked to tell a specified agency how to use any modifications you make. But this may be a wrong interpretation of the license.
I started working on something very similar what seems like ages ago: https://github.com/sagebind/ingots In that repo I have a minimally working example of traits used as a contract between server and app, compiling the app to a shared library, and loading the binary dynamically by a web server. I shelved the project after a while due to lack of interest and lack of stability of needed crates. Now that the http crate exists and standard futures looming, I agree now is a good time to look into this stuff again.
/r/playrust
[crev](https://github.com/dpc/crev) has been mentioned in recent related discussions, and allows for that.
Maybe we could look at Rust SO answers and questions that haven't been edited since a given date, in order to validate their compatibility... I wonder how much work that would be, and whether other people could help out.
In particular the edition flag is only running on the topmost parser level. Its meant to change the set of restricted keywords / semantic grammar of the language in backwards incompatible ways, like reserving "try" in 2018. Language features in the compiler can be gated on the edition flag, but for the compiler itself all it really cares about is its parse grammar thats changing. That is how 2018 has the new module system - the grammar of imports changed.
As I'm understanding it, this would stop a third party library from using a certain package however it doesn't actually restrict the capabilities right? Would there be anything stopping that library from just copying/pasting code from the "http" package and using it directly?
&gt; Editions are for breaking changes. Unfortunately not only for them. It was explicitly stated that editions are also marketing tool and synchronization point of "good practices", to which many (including me) objected and requested to keep only the technical side (i.e. do editions only for "breaking" changes), but to no avail.
IKOS uses LLVM 7.0.0, the latest version. I don't know about rustc.
&gt; I also contribute to a compiler as part of my work. OK that sounds pretty advanced! ;-) Thanks for clarifying my questions. I understood all of it, but one exception: &gt;Absolutely! CompCert is C compiler formally verified to be correct with Coq C isn't even claiming to offer guaranteed memory safety. How can the compiler be varified then? I must have misunderstood what Coq actually does. What exactly does it verify? That the executable is free of bugs? 
Do you know where it's spending most of its time, in import or in report or elsewhere? And the specific nature of the inputs? Maybe both of those are obvious for someone with more experience in Rust and this particular domain, but I am neither. One thing I did notice is this line: `self.entries.push(LogEntry::from_raw(&amp;raw));` Even when the vector has all the space it needs reserved, push can have some funny overhead that doesn't always get optimized away by the compiler compared to explicitly replacing/mutating a default entry. &amp;#x200B;
There is no rules. Depending on your requirements, access patterns, etc. you might decide one way or the other. In Rust, it's actually fairly easy to abstract it away and/or change it after you change your mind. Compiler will guide you with fixing all relevant places. Also, unless you're writing a high-performance code, or a library that might used in high-performance code, you probably don't care much anyway.
I agree about new sugar, although I think it's fine to add stuff to nightly. I do think however though that new sugar should spend a lot of time on nightly before being stabilized.
Most time is spent importing the log. I'll try making a vec with default values and mutating. Thanks 
You can get rid of all the `to_string` calls in `report`. Instead of: let mut ebuf: Vec&lt;u8&gt; = Vec::new(); ... ebuf.extend_from_slice(b"LiveObjects: "); ebuf.extend_from_slice(entry.liveobjects.to_string().as_bytes()); you can do use std::fmt::Write; let mut ebuf: String::new(); ... write!(ebuf, "LiveObjects {}", entry.liveobjects).unwrap();
&gt; What exactly does it verify? That the compiler is free of bugs? Exactly this. CompCert doesn't guarantee that the C programs _you_ write will be correct, but it guarantees that they'll behave exactly how the C specification says they're supposed to. Compiler soundness is a hard problem (rust certainly has its [fair share of compiler bugs](https://github.com/rust-lang/rust/issues?q=is%3Aopen+is%3Aissue+label%3A%22I-unsound+%F0%9F%92%A5%22)) so this a major achievement! 
&gt; I think it's important for Rust to find an answer to NPM incident Come /u/newpavlov , join the reviewed side! https://users.rust-lang.org/t/alpha-testers-needed-for-cargo-crev-crate-review-system-for-rust/23210 
Will do, anything is good right now. I should be able to use the 'itoa' crate here right?
No it does not restrict the capabilities as such, however if you are copying code from "http" then you will probably need other modules e.g. "net", which still raises suspicion. Perhaps, you can write/copy everything from scratch, but then it is no longer a sub-dependency security problem.
&gt; However, those also aren't the kinds of things to excitedly look forward to or plan for or ask for people to write blog posts about 3 years prior. So my impression is that people are thinking about editions as being for language evolution via breaking changes. I could totally be wrong about that, but that's my impression, and that's what I was responding to. I agree but I think it is helpful to be clear in messaging so Editions don't get an unnecessarily bad reputation.
I followed this guys playlist: https://www.youtube.com/watch?v=vOMJlQ5B-M0&amp;list=PLVvjrrRCBy2JSHf9tGxGKJ-bYAN_uDCUL Hugely helpful, make sure you type along with him and compile and run the stuff he walks through
Drat, I forgot it was only elevating to errors. [We really need a stable-in-edition warning group for the sake of CIs.](https://epage.github.io/blog/2018/01/crate-management/).
Not to be a stalker, but I'd bet Denis Kolodin is /u/denis-kolodin, he just hasn't been active for quite some time and is probably not an active Reddit user.
Not sure why though? Also, another guess – perhaps you'd be better off printing the logs one by one instead of collecting all then printing all? You can then get rid of cloning `String`s in log and `get_meta`.
Not implemented but I'll be doing operations that require them to be loaded later
Itoa is faster than write 
https://github.com/koute/cargo-web exists which might be what you'd want. I don't know if/how well it supports WebGL but it's at least what yew is built on.
I am biased, but the most promising projects currently are Azul and OrbTk (from the Redox project). Conrod from the Piston project is also here to stay but it has its own way of thinking about GUI that is a bit "uncommon". Keep your eyes open for upcoming GUI #Rust2019 Blogposts. I have insider information that some are on its way :P
/u/jswrenn replied too and linked to some stuff that was new to me. Fun thread :) &gt; Are you a professional programmer? Long time Rust user? Yes, programming is my job. But I've only been playing with Rust since 1.0. There are a lot of people on the subreddit (and the core teams of course) who've been around a lot longer than that. The Rust community is super duper transparent, in a way I've never seen before, and that makes it easy to follow RFCs and get a good sense of how things work if you stick around for a while. Honestly I think the way they've managed the development of the language is _at least_ as interesting as any of the fancy features in the language itself. &gt; Why is auditing `std` better/easier than auditing the compiler? If you're auditing `std`, you're basically looking at "regular Rust code". There are some unstable features that get turned on, but more or less if you're a Rust programmer, you can read that code. The implementation of `Vec`, for example, isn't very big. It's subtle and unsafe, but it's not like, that much code. The compiler, on the other hand, is a big project. There are a lot of concepts and data structures you need to understand when you're reading any particular piece of code in there. For example, there are several "intermediate representations" that code goes through in between the Rust the human wrote and the machine instructions the CPU will run. I've heard that the `Box` type (which remains a magical language built-in) has its implementation details scattered far and wide across different parts of the compiler, at least for now. I'm not a compiler guy, but that sounds scary to me :) &gt; So this must be quite non-trivial. Yeah those formal semantics folks are no joke. And they've already found real issues! There was some case where one of the Mutex helper types was Sync (I think) when it wasn't supposed to be. And some other cases where the type restrictions were unnecessarily strict. &gt; Is this something other languages have/try/want as well? I guess not since none of them is claiming to give safety guarantees. /u/jswrenn's answer includes more detail than I know, but I remember from my time back in school that computer scientists have been proving the correctness of programs since the very earliest days of programming. Proving something like "this loop will only terminate if `x` is a prime number" isn't too hard when you're working on small examples, depending on your level of rigor :) Rust's major advancement in this area is in giving library authors the ability to enforce their invariants on their callers. For example, a regex library might allocate new strings to hold its matches, or it might return pointers into the input string. Those two different versions impose different requirements on the caller: in the first case, the caller will need to free the matches, but in the second case the caller has to make sure the input string stays alive as long as the matches do. In Rust, those requirements are *visible in the API*, which (as we all know and love) leads to the compiler being able to enforce them in safe code. That explicitness is the key -- that's what makes it possible to propagate things like lifetimes and thread-safety through the interfaces where different parts of a program interact. The end result isn't so much that the programs are 100% automatically correct, but that they're clear about which parts of them need to be manually verified, and those parts are small. &gt; English isn't my primary language You people make me ashamed of my own sorry foreign language skills :p &gt; I thought that Rust totally completely solved this memory/safety problem. But when I think about it: It actually solved this problem, right? If I write thousands of lines of code and I don't use any unsafe() blocks, Rust still guarantees memory safety. Others have mentioned the numerous caveats here, like unsafe code in your dependencies or bugs in the compiler. It's turtles all the way down :-D But I think what you can say with confidence is this: If you write thousands of lines of safe code, and your program triggers undefined behavior, it's _not your code's fault_.
I think it's clearer to say you opt in on a per _crate_ basis. A binary can depend on crates that each rely on different editions.
Changed to take a Model. Examples updated. Can use an immutable-design/FP approach by leaving the model as-is, or use a mutable approach by re-defining model as mutable at the top of the update func.
This is probably the wrong place to ask this, but do you know if you can `await!` multiple futures without using join? It's always bugged me in JS (though I understand why it's the case) that you can't just do const [res1, res2, res3] = await [promise1, promise2, promise3]; and I wonder if it wouldn't be possible to just do let (res1, res2, res3) = await!(f1, f2, f3)?; // using tuple destructuring, but any other form of destructuring would be fine as well in Rust. Seeing as the whole duck-typing thing isn't a problem in Rust, I see no reason why this wouldn't be possible as a "shorthand" for `join_all!`. The only issue I could see would be that it's technically ambiguous whether `await!(f1)?` should result in a single-element tuple/vector/collection being returned or the item itself. Maybe an `await_all!` could be established as well?
What are the implications on build time for build scripts and/or macros? Are build scripts run on every build, or only when they've changed? Are macros re-expanded on every build, or only when the source file they exist in has changed? Are macros expanded in parallel under the new parallel compilation features? Basically, if I want to do potentially complex code gen, should it go in macros or in build scripts from a user friction perspective?
if the objects are in a HashMap, aren't they already on the heap? also - for performance, it's preferable to keep many small objects contiguous in memory, so perhaps you could push the "raw data" to a Vec and store references or indices to those objects in your complicated representation of it.
Personally, I find that putting style gates (clippy, rustfmt, -D warnings) on CI works well in their own job anyway, so you can tell at a glance whether "tests failed" or "style failed" [just by looking at the Travis page](https://travis-ci.org/pest-parser/pest/builds/465696308). If you're pinning a MSRV (which good CI should be doing), you can even do MSRV, minimal-versions, and style tests in the same build (assuming you can find a nightly that represents your MSRV, or your MSRV is post minimal-versions stabilization, otherwise /shrug), so you don't even need a job you wouldn't have otherwise. So I guess my end goal is two jobs: - name: stable tests rust: stable script: cargo test - name: pinned tests rust: lts-prior install: - rustup component add rustfmt - rustup component add clippy before_script: cargo generate-minimal-lockfile script: - cargo clippy -- -D warnings - cargo fmt --check - cargo test Then add os jobs and/or other specialized jobs if your project specifically would benefit from them.
\&gt; I think it's important for Rust to find an answer to [NPM incident](https://users.rust-lang.org/t/how-does-crates-io-differ-from-npm/22658). &amp;#x200B; [crates.io](https://crates.io) should at least develop a roadmap for ... what would you call this? stewardship? trust?
so looking into this more, I think I can go with the 2nd approach with some changes to it: #[derive(Serialize, Deserialize)] struct MyStruct { id: String, #[serde(flatten)] variantDetails: MyVariants, } #[derive(Serialize, Deserialize)] #[serde(tag = "kind", content = "variantDetails")] #[serde(rename_all = "lowercase")] enum MyVariants { VariantOne { quantity: u32, // etc }, VariantTwo { quantity: Option&lt;u32&gt;, // etc } // etc } this should let me keep stuff unique about each variant contained in that variant while keeping the similar stuff contained in `MyStruct` (renamed from `MyType` since that wasn't proper nomenclature)
I've been having them be their own job for a while. It is still nice to have it be something that has less management to it. Combining with something else locked down helps to a degree but then you lose the isolation. Plus, I've been running into a lot of problems correctly maintaining MSRV checks. It is too easy to accidentally get a new dependency, breaking your MSRV builds even if you are still compatible. It is then a pain to unwind this so you can still validate your MSRV.
&gt; I think it's important for Rust to find an answer to NPM incident. It's bound to happen with crates.io sooner or later. I must say, this thing just frightens me and not just for my own projects. Rust compiler [depends on hundreds of crates](https://github.com/rust-lang/rust/blob/7489ee9c6f92050a12a3a3097df0a7d3737d82ec/Cargo.lock#L3376) from crates.io. Any of them can introduce malicious code into rustc. Does anyone actually review code published on crates.io for these packages when they are added or updated?
Thanks for releasing the tool! From what I've heard, the lack of formal verification tools for Rust is one of the issues holding back mission-critical applications development; without any such tool, there's not much point certifying a Rust toolchain, for example.
It is a blatantly wrong interpretation. Also why restate what I already said...
Why without using `join`? `join` and `try_join` (if your futures return `Result`s) do exactly what you want. let (res1, res2, res3) = join!(f1, f2, f3); let (res1, res2, res3) = try_join!(f1, f2, f3)?; 
A `HashMap` internally allocates heap storage for its contents. Assuming you're putting these big structs into a HashMap (not just references to them) then they're already "boxed" in a way. I'd guess you're running into problems because of _recursion_. You might especially run into a problem if you have these structs as locals (temporarily for construction before putting them into the HashMap) in a function which then recurses deeper into a parse tree.
You could probably use rayon to multithread it.
Could work as importing is the time consuming task
Yeah, give it a try.
Can anyone point me to why `format!` is slow? Here I am thinking it expands to some pretty standard Display calls and push_str, like what else could it possibly be doing?
It expands to a lot of code relatively to just appending to a buffer. No big deal with a dozen calls but when doing millions like me it gets slow
&gt; I just finished refactoring it which brings a ~100x speedup (yay) by removing the use of format and switching out BufReader for the filebuffer crate. Are you saying that your current strategy of treating the file as bytes and using `from_utf8` is faster than treating the file as text and doing something like `let entries = file.lines().map(|line| serde_json::from_str(line).unwrap()).collect();`? I'm not sure of the technical reason but I've found that collecting out of an iterator doesn't incur a realloc cost like pushing without a reserve does. So the code would be simpler and likely the bottleneck in the whole thing is the json deserialize anyway.
Yup, mine was faster. I benched it
The problem of 80% solutions, is that different people have different 80% in mind. I remember an anecdote about a tech-writer reviewing a text editor which covered 80% of the functionality of Word. Deciding to dog-food it, the tech-writer wrote the review using it, and it went pretty smoothly... up until the very end. Before submitting the review, its length needed to be checked: how many characters/words are there? In Word, it is readily displayed in the status bar, but here it could not be found. Crawling through menus and preferences our tech writer despaired. Finally caving in, an e-mail was sent to the authors asking how to display; their reply: "Oh, nobody uses that, so we didn't put it in." Promptly, the tech-writer opened Word, and wrote a scathing review of this useless text editor which didn't even have such a basic functionality as counting characters.
&gt; I think that 3-year editions is a bad thing and I would have prefer "when it's ready" approach instead (yes, something similar to C++2x). Actually, after the C++0x (initially planned for 2008) which got delayed 3 years, the C++ committee decided to ship every 3 years (2014, 2017, 2020) and simply only ship what's ready and defer the rest for a subsequent version. So... The current plan for Rust editions is on-par with C++ standards.
&gt; Why without using join? Because I didn't know `join!` and `try_join!` existed. =)
It seems the \`SealingSlice\` seals from beginning to end, always extending the prefix that is sealed and shrinking the suffix that is mutable. &amp;#x200B; The documentation, especially the high level description of the crate, should make this clear. Currently, the documentation makes it sound like arbitrary subsections of the slice can be sealed, because of the phrase "parts of itself", rather than "a prefix of itself".
If you goal is a highly optimized logger this is what `slog` attempts (and succeeds) at doing. While there is a some spin up it it handles a lot of the weirdness involved with doing next to zero allocation logging and serialization 
Nice work! Wondering if this could share any code with the Servo/WebRender SVG work? After taking a look, it looks like they each have their own DOMs, and there's no real way you could avoid that since Servo's DOM needs to be tied into the HTML DOM and JavaScript engine, while resvg works with its own custom DOM and SVG DOM simplifier for efficiency. The [work pcwalton is doing on using Pathfinder for SVG rendering in webrender](http://pcwalton.github.io/2018/12/07/plans-for-2019.html) looks like it could be an alternative backend for resvg, which now supports Cairo or Qt backends. Looks like that's mostly still in the planning stages though.
I interpreted OP as saying Cranelift should be the default for Debug builds, which is why I said it should be an option in the Cargo configuration. 
I know. The app that logs is written in go and uses a go library. It logs as json mad this is for crafting that into readable reports
It's not possible to write an OS without any `unsafe` blocks, because you have to interact with the hardware. For example on ARM architectures, configuring hardware means writing to memory at specific absolute addresses in memory. This can never be a safe operation.
&gt; but this is effectively blocked by existential types, another feature on track to stabilization relatively soon This surprises me a lot. I was expecting existential types to be stabilized far into the future. Especially considering they are still extremely unsound and ICE the compiler all the time: https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=659807e9774daee517dd76915ce30983
No problem. I also remembered: There are a bunch of projects on GitHub to do with Rust that have mentored issues. If you're looking for those, I'd go to the Rust topic: [https://github.com/topics/rust](https://github.com/topics/rust) And click on a large project that catches your interest. (the really big main Rust projects do seem to all have "good first issue" or "mentor available" issues, but really small projects presumably don't?) You can go to the Issues tab there and just browse, or better: search for things like "First" (as in good first issue) "Easy" (some of the main Rust projects use "effort" labeling such as: E-Easy E-Hard etc.) or "Mentor" &amp;#x200B;
This library cannot render using OpenGL, is that correct?
That part was very vague; I've clarified the description and added a note in the roadmap indicating that while non-prefix SealingSlices could make sense (and would, given use cases, be welcome additions) in the crate but are not implemented. Thanks for pointing this out.
I'm trying to implement a tree structure where I can call a function that returns me mutable references to nodes which contain a certain value. Since I have a Tree I want to use recursion. This is a abstraction of my code with the same issue: #[derive(Debug)] enum Tree { Node(Box&lt;Tree&gt;, i32, Box&lt;Tree&gt;), Leaf, } fn find_elements&lt;'a: 'b, 'b&gt;(t: &amp;'a mut Box&lt;Tree&gt;, value: i32, list: &amp;'b mut Vec&lt;&amp;'a mut Tree&gt;) { if let Node(l, _, r) = t.as_mut() { find_elements(l, value, list); find_elements(r, value, list); } if let Node(_, v, _) = t.as_ref() { if *v == value { list.push(t); } } } [I also put it in the playground so you can see the error message in detail](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=35ccdeba8e2a929534cfd8a35251a33e) The borrow checker complains that I borrow t in the first if-let-condition and also in the second if-let or when I push it to the list. Shouldn't the first borrow be out of scope when the if-let finishes? How do I need to change my code to make it work the way I want? 
also, if folks are interested in how this all fits together under the hood, I wrote about this last year https://manishearth.github.io/blog/2018/01/10/whats-tokio-and-async-io-all-about/ (some things are out of date but I've left notes)
wow, excellent article
Thanks for helping out! I tried the code but I ran into some errors so I tried this instead: document .find(Class("container")) .next() .unwrap() .children() .filter(|child| child.is(Text)); when I use map, it gave this error: .map(Node::text); | ^^^ | | | expected signature of `fn(select::node::Node&lt;'_&gt;) -&gt; _` | found signature of `for&lt;'r&gt; fn(&amp;'r select::node::Node&lt;'_&gt;) -&gt; _` which I don't understand Thanks a lot!
Are there plans to move the `Stream` trait into core/std after all this is done? Async for loops (previously available via [futures-await](https://github.com/alexcrichton/futures-await)) were much nicer than having to await individual futures in a while loop. The new crates kinda just further muddy the waters for me, but I guess I'll wait and see if and how they're used. I tried out Tide and it seems okay, but it's still kinda nebulous how it fits into the bigger picture. I'm hopeful but currently wary of Warp, Tower, and Tide. Rocket and Actix seem to have a much stronger vision and forward movement. I understand that the crates just announced here are not web frameworks, it's just not clear that they address any existing issues I've had.
So basically it should be an absolute *not-goal* for wasm to become a JS framework and an even bigger *not-goal* to attempt to replace JS. It would end up as just one amoung many frameworks and the resulting feature creep would eat our laundry. Rust+wasm should only be one thing: The best extention to JS if performance is needed.
Might help if you posted some sample data you’ve benched it on
No, but can be. There is a partial implementation in [azul](https://github.com/maps4print/azul/wiki/SVG-drawing).
First step I'd take is setting up criterion so you can easily reproduce performance testing, and share it with others so we don't need actual logs to feed in. self.entries.reserve({ let size_hint = iter.size_hint(); let mut size = 0; if let Some(x) = size_hint.1 { size = x } size }); I feel like you could just do let size_hint = iter.size_hint(); let size_hint = size_hint.1.unwrap_or(size_hint.0); self.entries.reserve(size_hint); Why would you default to 0 anyway? Is that a common case, where you'd have an empty entry? Seems like you could probably use your own knowledge of the data format to preallocate - after all, in the event that you're wrong and you *under* allocate the rest of the program is basically a nop anyways. You could use `for b_line in iter.par_iter() {` and then collect into self.sentries. Like I said, I'd start with benchmarks and find the real bottlenecks. I get that it's the import function, but that function is pretty large, any component in there could be optimized separately. 
Yes, it cannot be used in browsers, because they have to keep an original DOM for scripting and stuff. 
I think “relatively” does a lot of work here. My understanding is that this relies on chalk, which was come a long way. That would trade all of those problems for new problems, hopefully fewer. :)
Ah yes, try `stuff.map(|node| node.text())`. This line just maps the iterator of Nodes to an iterator of Strings. 
Ah yeah, chalk will definitely help reason about them a lot. So I guess that explains it :D
Has anyone ever told you that the name "pom" is visually very similar to "porn"? I had to do a double-take when I saw the title of this post.
Glad to hear it :)
Is there a way to hold a weak reference without reference counting (`std::rc`)? I only have exactly one owner of the object that defines its lifetime, but multiple weak references to it (like parent references from an array of child objects).
What does existential types do in Rust? I know that they are in Haskell.
It's the same as impl Trait in return type position, but named. Essentially you construct a possibly unnamed type (like a closure or async generator) within your function and return that. With impl Trait you can describe the trait by what traits it is implementing, but it still doesn't have a proper name that you could for example then use as a field of a struct, an argument for another function or an associated type in a trait implementation. So with proper existential types you can give those anonymous types proper names and then use them just like any other normal type from that point on.
Ah. Yeah. That makes sense. Thank you!
That's really cool. I had a use case for these the other day, but couldn't figure out how to get it to work without wrapping it in a box. This explains why it didn't work :).
If you want backtraces from your program, they'll be pretty useless. &amp;#x200B; Debuggers will also be a lot less useful, but you can at least strip the debug symbols out into a separate file - debuggers know how to use a separate debug symbols file from your binary.
I believe it doesn't work that way. First, if it was that easy to just make the compiler faster, the devs would do it. Big optimizations are rare, so it is probably hard to do. Second, there is this bottleneck called LLVM, which is the compiler backend of Rust. It does most of code optimisations and makes all the zero-cost abstractions possible (afaik). And optimizing takes a lot of CPU time.
Any reason I might want backtraces from a release binary? My normal error handling will still produce the output I made it right?
&gt;This can never be a safe operation. You mean in current Rust? Or ever? I don't really see why a future language couldn't have a compiler that is able to ensure that everything you may ever want to do is done safely, or at least enough to be able to interact with hardware. &amp;#x200B;
You'd need a compiler that is able to handle hardware definition files and validate their proper use. This means marrying the compiler to the architecture. Right now, these things are handled in a crate instead, which is a better solution, since it encapsulates the platform dependencies that can be called from safe code. Since these crates are implemented in Rust, they need unsafe blocks. So, to answer your question, it's possible to do this, but not a good idea and not the direction Rust is heading (towards platform independence).
How is this different from `split_at_mut` and then re-borrowing one part as immutable?
There is [human-panic](https://github.com/rust-clique/human-panic) crate that will save backtraces automatically to file, so user can send bug report to developer. So i think it needs debug symbols for that.
Sure, that all makes sense. Was mostly wondering if I was missing something fundamental :)
Have you tried mmap(2)? I’m guessing you’re dealing with very large files because num_entries is u128. Also, as a small note in the name of speed unless you really need 128 bits it will be slower than u64. 
That calculation happens only once. Im using file buffer which is a wrapper around mmap. Yes, I'm dealing with multiple services logging multiple times per minute
+1 for Criterion and sample data. I don't know what filebuffer does, but you definitely want to mmap your file. 
I think TideLift already does this. 
&gt; Fun thread :) Haha it is indeed! :-) &gt; I'm not a compiler guy, but that sounds scary to me :) Don't worry, I understand you better than you think!! ;-) &gt;In Rust, those requirements are visible in the API, which (as we all know and love) leads to the compiler being able to enforce them in safe code. That explicitness is the key -- that's what makes it possible to propagate things like lifetimes and thread-safety through the interfaces where different parts of a program interact. The end result isn't so much that the programs are 100% automatically correct, but that they're clear about which parts of them need to be manually verified, and those parts are small. Ahhhhh ok. I've never heard it explained like that. That actually makes sense. I don't have a CS background/education so sometimes things aren't as obvious to me as they should be. That's why I'm a "frequent asker" in the "Hey Rustaceans! Got an easy question? Ask here!" threads. :-) It's the little things that go on in the background that REALLY help me understand Rust as a whole. This is one of the things that really stick out in Rust: How nice and helpful the people here are. I'm also active in /r/django (Python web framework) and /r/golang and the difference in helpfulness and overall tone (especially compared to /r/golang) is MASSIVE! I'm not even exaggerating. So thanks for all your input! This was really interesting. However I'm pretty sure you'll read more questions from me soon anyway! ;-) 
 I am trying to understand the following syntax in actix-web https://docs.rs/actix-web/0.4.0/src/actix_web/resource.rs.html#34-39 `pub struct Resource&lt;S=()&gt; {` Can someone tell me what is the struct generic on? 
Sugar is fine unless/until it hides information from the developer. I’m still routinely frustrated with match ergonomics because of that. 
Assuming a theoretical future in which Rust has an ABI, do existential types make it impossible to tie ABI-compatibility to semvers? Or did that ship already sail with impl trait?
No one's surprised that an unsupported mode doesn't work out of the box. The more interesting question is finding out what's necessary to make it work.
Perhaps you wouldn't be able to target only unsafe blocks, but ideally safe code will avoid triggering most of the checks in the first place.
This is great! I just read on filter and map. Cool but somewhat difficult. You just helped me figure it out. Thanks!
I think we are approaching the NPM package hell territory in quick strides. I recently started a new, relatively simple project with a web server using warp and tokio. In no-time I was up to 240 dependencies. It'll probably be another 50 extra before the project is somewhat finished. And I'm not even talking about the security aspect here, but also compile time, complexity and upgrading burden, including the potential gridlock when dependencies don't update, leaving you on older versions. To a certain extent, this is unavoidable with the small std library of Rust. At least we don't have `even` and `odd` packages like the JS crowd. I do think that e few things should be in the standard library though in some form when the language and design patterns settle down. Unfortunately, there is no sign of this happening soon. With GAT/ATC, const generics, existential types, potentially anonymous structs, keyword function arguments, et.... Which can all have quite the impact on API design. Also, not needing separate crates for proc/derive macros would be very valuable. Just some things that would be very nice in std, because so many projects need them: * regex! * chrono::Date/DateTime! * common hashes (md5, sha*) and crypto primitives * rand * serde / serde_derive / (json) * url::Url * uuid::Uuid * log * some crossbeam / parking lot stuff 
I'm not really familiar with the implementation of Rust's HashMap, but doesn't reallocation causes every data in the map to be moved to a new memory location? I would think putting large values in a HashMap should be boxed, to avoid moving large chunks of data and only move pointers. Yeah, that's my main problem. Because it's a pretty deep hierarchy, i have to create these structs on the stack, before putting them into a HashMap. I decided not to care about allocation optimization right now and just Box-ed every struct.
The type `S` defaults to `()` unless another type is provided.
https://www.reddit.com/r/rust/comments/a4tr69/hey_rustaceans_got_an_easy_question_ask_here/ebnov5h/
So we now have: * [yew](https://github.com/DenisKolodin/yew/) * [draco](https://github.com/utkarshkukreti/draco) * [seed](https://github.com/David-OConnor/seed) * some other one whose name I forgot They all look very similar and follow React+Redux or Elm inspired designs. I understand it's nice to experiment in this space, but I whish we could get together and work on one stable go-to library. 
Thanks for this succinct yet insightful explanation!
&gt; Currently the difference is like 5m C++ vs 18m Rust, on average, for my dummy projects, on a dual core with 8GB. Wow. What kind of dummy projects are you doing? 
&gt; Rust's standard library and culture of both "generic programming by default" and also heavy use of macros (primarily for #[derive]) are one thing I'm afraid of here. I understand that, but I've yet to, you know, see anyone conclusively demonstrate that it's impossible to get better build times with such algorithms in place. I *know* Rust is more complicated than C and thus `rustc` will take longer than `gcc` on idiomatic code that does the same thing. But what I don't think has been proven is that `rustc` has no way to improve its performance.
wasm-bindgen or stdweb
Holy shit! (excuse my language) Love it ❤
Tupp, file buffer handles mmaping 
Is there a specific reason why you don't want a reference-counting mechanism to be involved (e.g. by using one `Rc` and several `Weak` references to it)? If you want safe weak references, you need some way for the weak-reference implementation to check whether the referred-to value still exists. Off the top of my head, I can't think of a safe way to implement this in Rust without heap allocation(s)\*, at which point there's no practical difference I can think of between using a refcount implementation or something else. \* for example, trickery like intrusive linked lists won't work on types that can be moved, and as it stands, immovable types via `Pin` must be on the heap, afaik.
Why not an `await` operator instead of a macro?
A very basic Gtk-rs hello world, originally written in Gnomemm for an "The C/C++ Users Journal" article, almost 20 years ago. The large majority of those 18m are spent compiling Gtk-rs and its dependencies, hence why C++ compilation wins, because cargo does not support binary dependencies.
I have the following code, which works as you would expect: struct Point { x: f32, y: f32, } fn distance_to&lt;'a&gt;(point: &amp;'a Point) -&gt; impl Fn(&amp;Point) -&gt; f32 + 'a { move |p| ((p.x - point.x).powi(2) + (p.y - point.y).powi(2)).sqrt() } The function returned by `distance_to` can't outlive the given point, which makes total sense. However, if I move the returned function's lifetime to its parameter (i.e. `impl Fn(&amp;'a Point) -&gt; f32`) the code still compiles, and I don't really understand why. Some testing seems to indicate that the borrow checker *still* doesn't allow the returned function to outlive the given point, even though it doesn't have an explicit lifetime anymore. Does it somehow inherit this lifetime from its parameter?
You still have to count the weak references so you don't deallocate while any of them are alive. The contained value will be dropped (its destructor run) when all strong references are gone but the container on the heap can't be deallocated while any weak references are alive. They still have be to check the weak count to know if it's safe to upgrade or not.
Or could be made that way.
Yeah, that would be even better.
"Hmmm what's this?" "Looks like another /r/playrust post" "Ha ... Rusta Rhymes ... that's actually kinda funny" "Hmmm Ferris is mentioned. That can't be a coincidence. Might be a /r/rust post after all" *click* *click play* *head-nodding* -&gt; Thumbs up my friend!! :-) 
TLDR of editions: `2.0` but we hate `2`, and theres a backwards compatibility path for the different code versions, they can be freely mixed.
It's just for safety. I don't want to allow several strong references to the same object, so I can make sure they're really dropped when I'm done with them. Otherwise, locally in the owner of that strong object I have no idea when an object really is destroyed, I only know its minimum lifetime.
It's unlikely that op will have more than u64 entries for the next several decades, u128 also seems wildy optimistic to me.
TLDR of editions: `2.0` but we hate `2`, and theres a backwards compatibility path for the different code versions, they can be freely mixed. Slightly longer: Backwards compatible `2.0`, but we don't call it `2.0`. You can write code in `2015` version or `2018` version. The `2018` compiler can work with `2015` code and `2018` code simultaneously, so backwards compatibility. All the breaking changes go in the opt-in `2018` mode, which may not be valid `2015` code.
Changing the global allocator is stable now, so you can do that to your program if you like: https://doc.rust-lang.org/std/alloc/struct.System.html the system allocator will become the default in a release or two from now as well, so soon enough you won't even have to do that step.
Perhaps you could share a stack trace for one of those stack overflows, and / or possibly the code?
In your code as written, you cannot use `t` in any way after the `if let Node(l, _, r) ...` block. That block recursively calls `find_elements` on references `l` and `r` that were obtained by mutably borrowing `t`, and `find_elements`, by design, can potentially keep that borrow active after the recursive calls by having added `l` and/or `r` to `list`. For what you're trying to do, do you actually need to return a list of mutable references to the full tree node structs, or only to their contained values (in the example code, `&amp;mut i32`)? In the second case, what you're trying to do would be possible to implement safely, e.g., by implementing a method on `Tree` similar to the `iter_mut()` available on various collections, and then filtering and collecting that iterator. Implementing `iter_mut()` and the associated iterator type would probably require unsafe code, though. Alternatively, you could replace the node values `i32` with `RefCell&lt;i32&gt;`, return a list of immutable references to those refcells, and risk panics at runtime.
LLVM 8 probably: [https://github.com/rust-lang/llvm/tree/rust-llvm-release-8-0-0-v2](https://github.com/rust-lang/llvm/tree/rust-llvm-release-8-0-0-v2) 
Could you get what you need by hiding the single strong `Rc` as a private field in a newtype and having a method on the newtype to produce weak references?
~Ooh! That's why I was having trouble finding out about how to switch. Recent change. Jemalloc while more performant than malloc, shouldn't realistically be noticeable with a "script" that will feasibly max out at 1k-2kLoC and allocates most memory at initial start, yeah?
Yeah, jemalloc is prolly overkill for a program like that.
&gt; In the case of enums, should I Box the enum, or it's variants? enum Foo { Bar(u8), Baz([u8; 1024]) } println!("{}", std::mem::size_of&lt;Foo&gt;()); // 1025 enum FooFoo { BarBar(u8), BazBaz(Box&lt;[u8; 1024]&gt;), } println!("{}", std::mem::size_of&lt;FooFoo&gt;()); // 16 There is actually a clippy lint for this which I believe triggers on enums that are greater than 256 bytes but don't quote me on that. Essentially it can be a good idea to box variant fields if they are comparatively large to avoid inflating the size of the variants which carry less data.
 This the first and only Rust Rap Well, darn. If your plan was to "leave them wanting more," it worked. Very nice. 
Yes, that should work. Thank you!
My usual approach would be to have a library crate rather than the subs module itself and then use that library from the mains. If you have a `Cargo.toml` which does not list the bins, and instead you have: your-project/ | + Cargo.toml \ src | + lib.rs + sub.rs \ bin | + main1.rs \ main2.rs Then `main1.rs` and `main2.rs` can simply `use your_project::whatever;` as wanted.
Ah okay. I suspect Rust could do a lot better if `cargo` cached builds globally but... unfortunately there seems to be little interest in that. 
https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md#final-syntax-for-the-await-expression This is the discussion of other possible ways to do await. await!(...) seemed to be the least bad IMO.
HashMaps can move stuff around on updates, though -- I think at least some types of HashMaps suffer performance degradation when the values are too large.
Thanks for your reply. Your explanation does make sense to me. I'm still trying to wrap my head around the more complex borrowing and lifetime situations. In my actual code I'm looking for certain patterns (I have different node types where I want a certain node followed by certain other node) in my tree on which I want to do mutating operations which change the tree structure. The original idea was to find all those patterns, stored as &amp;mut, and then let the caller decide on which node it wants to do the operation. Do you think I can achieve this behaviour with RefCells, I haven't yet had the chance to use them properly, or should I go with a different design altogether (Maybe you can give me some pointers where to look for useful patterns in this regard)? 
You should really profile this first*. We're kind of optimizing in the dark without a profile of some sort. But at the very least take a good measurement of the current performance. I saw criterion suggested, that may be a good fit for this, though it's usually for microbenchmarking. Since I'm without a profile, I'll just point out things that look odd to me along what I think is the hot path. But this is the wrong way to optimize code, since I don't have any way to measure. Start here: let line = str::from_utf8(&amp;b_line[..byte_len]).unwrap(); let raw: LogEntryRaw = serde_json::from_str(line).unwrap(); This looks pretty funky to me. The slide should be a no-op and `serde` can take in a slice of bytes. I'd write let raw: LogEntryRaw = serde_json::from_slice(&amp;b_line).unwrap(); You also _never_ touch the `.Name` attribute of a `LogEntryRaw`. Remove it from the definition of the struct and `serde` will skip right over it, saving you at least the work on the small-object allocation. (I think rustc should have issued you a warning about an unused attribute) You walk over `self.entries` a second time after populating it. You can sum `total_lo` and `total_ha` inside the loop over `iter`; this is marginally faster because the data is cached and most likely in registers. --- *`perf record -g ./target/release/cratename arguments here` then `perf report`. You can try `perf record -g --call-graph=dwarf` if it doesn't complain too mightily.
Thanks that make sense.
Another beginner question. Why is it necessary to define trait bound once for the `struct` and once for the `impl` when the trait bound is apply to the `struct` definition struct MyStruct&lt;T: MyTrait&gt; { myField: T } impl &lt;T: MyTrait&gt;MyStruct&lt;T&gt; { pub fn myTraitMethod(&amp;self) -&gt; bool { false } } why can't `impl &lt;T: MyTrait&gt;MyStruct&lt;T&gt; {` be replace with `impl &lt;T&gt;MyStruct&lt;T&gt; {` am I doing something wrong? 
Awesome &amp;#x200B; One point, you missed writing the line before: "No dusting: quite easy to maintain"
I'm trying to run an external program \`zsync\`, which works with no or minimal arguments passed to \`zsync\`. &amp;#x200B; However, when I pass an argument where \`zsync\` must read a file on my disk, zsync gives me this error: &gt;`fopen: No such file or directory` &gt; &gt;`could not read control file from URL` [`http://cdimage.ubuntu.com/daily-live/pending/disco-desktop-amd64.iso.zsync`](http://cdimage.ubuntu.com/daily-live/pending/disco-desktop-amd64.iso.zsync) (I think only the "fopen" message is relevant, since I can get it to download the remote file at that URL just fine.) Basically \`zsync\` runs fine on its own, but inside my Rust "script", it errors out. Do I have to do something special so my called process can read a file from the disk?
This is probably the LLVM svn trunk. LLVM 8.0.0 is not out yet.
Honestly... not performance critical at all. Got a large number of text files with RGB codes I want to read in and change in different ways based on a switch. As long as its not more than 5s on an SSD to read in 100~ files and modify about 1000 specific strings, I can't see performance being an issue. At all. Thanks man! Testing using malloc, it shaved off like 50% of the size. Really impressive reduction in size.
sorry, may I ask what is this syntax `impl Fn(&amp;Point) -&gt; f32 + 'a` ? is it similar to `fn distance_to&lt;'a, F&gt;(point: &amp;'a Point) -&gt; F where F: Fn(&amp;Point) -&gt; f32 + 'a` 
Why on Earth would you want to target Win98?
It's in the works: https://github.com/rust-lang/rfcs/blob/master/text/2089-implied-bounds.md
I'd guess that the args need to be split like this: .arg("-k") .arg("~/.zsyncs/disco-desktop-amd64.iso.zsync") //.arg(etc...) The reason is that your shell will split the command line on spaces and provide the arguments like this, but `Command::arg` is not a shell, and does not automatically do splitting to allow arguments that actually want to have spaces in them.
Want to point out: This works with only the first ".arg()" passed in. But as soon as I pass in the "-k \[local file on my disk\]" option, where zsync needs to read from my disk, it gives me the error message.
Ah, good catch! The missing line was "gushing super fast code from your brain" Updated the post!
https://rust-lang-nursery.github.io/edition-guide/rust-2018/trait-system/impl-trait-for-returning-complex-types-with-ease.html &gt; [For arguments, `impl Trait`] is a slightly shorter syntax for a generic type parameter. It means, "arg is an argument that takes any type that implements the Trait trait." However, there's also an important technical difference between T: Trait and impl Trait here. When you write the former, you can specify the type of T at the call site with turbo-fish syntax as with foo::&lt;usize&gt;(1). In the case of impl Trait, if it is used anywhere in the function definition, then you can't use turbo-fish at all. &gt; &gt; In return position, this feature is more interesting. It means "I am returning some type that implements the Trait trait, but I'm not going to tell you exactly what the type is." Before impl Trait, you could do this with trait objects [...] however, this has some overhead: the Box&lt;T&gt; means that there's a heap allocation here, and this will use dynamic dispatch. &gt; &gt; In Rust, closures have a unique, un-writable type. They do implement the Fn family of traits, however. This means that previously, the only way to return a closure from a function was to use a trait object [...] With impl Trait we can now return closures by value, just like any other type!
Thank you. I have tried that, and it works the same. It either gives the same error message if I pass a -k \[path-to-local-file\] or doesn't error. But when you say `Command::arg` isn't a shell... Maybe I need to remove shell-isms (`~/ == your_home_dir`) and give an absolute path. If you or anyone happens to know, can I get the value of something like $HOME into Rust somehow? The current user's home folder path as a string?
I _think_ I've had trouble running Rust binaries on Windows 7, but I haven't looked into it.
The reason Rust is limiting the lifetime of your function is that you're \*telling\* Rust that your function dies at the same time as the point. You'll need to remove the lifetime limitation from the return type. Here's the way I found to do that: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=095fa0d85ef5e75105953a6cfebc7456](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=095fa0d85ef5e75105953a6cfebc7456) &amp;#x200B;
Thanks a ton! I made some changes and added rayon and the importing got a 4x performance boost!
That syntax is used to specify that the return type is "any single unnamed type which returns a value implementing this trait." For instance, you might with to have a function like this: fn open\_file(file\_path: &amp;str) -&gt; impl Read ...which means that \`open\_file\` returns anything one thing implementing \`Read\`. (It has to be the same thing every time, but it could be whatever. The reason it looks so weird in the example above is that the syntax for function traits looks weird in and of itself. Like, a function accepting an int and returning an int would implement the following trait: Fn(i32) -&gt; i32
The big problem (from what I can tell) is that a future returning a result is somewhat ambiguous if you have an await keyword: `await some_future()?` With regular precedence, you'd "try" the value first and THEN await on it, when usually you want the other way around: `await!(some_future())?`
First time hearing about it. Can't wait to see what will come out of it!
The bit of sugar we need most is directly taking a raw pointer to a value.
thanks, @aturon (and all who helped), for the comprehensive summary! 
GUI framework development is complicated. None of the existing one is perfect, far from that. However, they all require heavy and continuous developments, which means a lot of money generally (even if this a free open-source project, maybe even more). It'll take at least years before such a project appears and is mature enough. Let's just hope that this day will come as soon as possible. :)
If you have at least some kind of companies list, that'd be very nice actually!
Because I can. Anything I write in VB6 would work from Windows 95 through to the latest Windows 10 -- that's 23 years of binary compatibility. Old computers should be able to benefit from better engineering; there are recent ports of RetroArch and ZDoom to Win 98 / XP systems.
For build scripts, see the `rerun-if-changed` args near the bottom of https://doc.rust-lang.org/cargo/reference/build-scripts.html &gt; Normally build scripts are re-run if any file inside the crate root changes, but this can be used to scope changes to just a small set of files. As for macros, I believe that only codegen is parallelized at this point. This is likely to change [soon](https://github.com/rust-lang/rust/pull/56765), but macro expansion hasn't been [queryified](https://github.com/rust-lang/rust-roadmap-2017/issues/4) so it doesn't benefit from incremental compilation or parallelization. As for performance based on the above build.rs should be faster if you use `rerun-if-changed`. However, if you're providing a code generation facility for your users to use (and not just as an implementation detail of your library/program), I'd recommend providing a macro since they're more ergonomic to use.
Congrats on your project, but I have a major question before jumping in. Can I attach something like a context to the error? Suppose I have an `IO{source: io::Error}` in my enum, but I want to specify whether it is a `Not found`. And in this case I want to write the filename to the `Display` message. In `failure` and `error-chain` there is a notion of context that can be added in a very nice way: let x = do_something().context("something went wrong opening config file")?; Is something like this possible? Or some alternative. This is an important point, otherwise we get pretty meaningless messages sometimes.
&gt; Jemalloc while more performant than malloc ... for some workloads, and less so with recent glibc. YMMV
If you're using the `i686-pc-windows-msvc` target, the fact that it uses a modern MSVC runtime might limit the minimum Windows version (Win7+ I think). On the other hand, `i686-pc-windows-gnu` should, in theory, run on earlier versions of Windows. The binaries for this target are also truly standalone; the target computer does not need any need anything extra installed beyond what any dependencies might require.
Yes, I believe that `~` is usually expanded by the shell. You can use [std::env::var](https://doc.rust-lang.org/std/env/fn.var.html) to access environment variables. A complete example: use std::process::Command; use std::env; use std::path::Path; let home_var = env::var("HOME").unwrap(); let home_dir = Path::new(&amp;home_var); Command::new("zsync") .arg("http://cdimage.ubuntu.com/daily-live/pending/disco-desktop-amd64.iso.zsync") .arg("-k") .arg(home_dir.join(".zsyncs/disco-desktop-amd64.iso.zsync")) .arg("-i") .arg(home_dir.join("Downloads/disco-desktop-amd64.iso")) .arg("-o") .arg(home_dir.join("Downloads/disco-desktop-amd64.iso")) .status() .expect("zsync failed to run");
Lol that's great. Reminds me of Monzy's stuff
Same. I'm totally of a mixed mind regarding the match ergonomics, even still.
&gt; I can't see performance being an issue. At all. Is binary size actually an issue then?
Yeah, I get that. I know how binary compat works. I just don't *get it*. 
&gt; Are there any other optimizations I can try for size that don't involve stripping or feature removal? Run `cargo install cargo-bloat` then you can `cargo bloat --release --crates` and get _some_ idea for what's taking up space. LTO should dead-strip pretty effectively, but you can set `default-features=false` on each of your dependencies then opt back in to any features you do actually use. If you're down to 400 KB already, the heavy hitters are probably code size tradeoffs due to optimization and the panic runtime. You can ask the compiler to try to make the binary small by setting `opt-level = "s"` or `"z"` in your `[profile.release]`, both ask for a smaller binary but `"z"` is more ready to make performance tradeoffs for size. Going `#[no_std]` and losing the panic runtime will get you a very small binary but you may find it difficult to do any of the things you want these scripts to do.
I get it as a hobbyist thing. Maybe. Otherwise, it seems like a weird principle to take to an extreme.
&gt; I think one way to fix this would be to provide a library that can call and interpret the results of cargo metadata. So that instead of using "cargo as a library", sub-commands would just use these other library that calls the cargo binary. I just came across [cargo_metadata](https://crates.io/crates/cargo_metadata) and [escargot](https://crates.io/crates/escargot), both libraries doing what you suggested. I will be reaching out to the owners to improve the communication between the projects. Thank you for the suggestion. &gt; because there are very few docs, it is hard to know how things are actually supposed to work, which is a pre-requisite to know whether they are working correctly or not, submit / fix bugs, fill issues, etc. This is an excellent point. I will endeavor to reiterate this message to the team. For example, in this past Cargo meeting, we were discussing what will be needed to stabilize custom registries. It was brought up that it will need better documentation, and we wish the community had built more tooling for it already. Thinking of you I pointed out that if there is't good documentation, it is hard for outsiders to build things that use it. Similarly I have been trying to ask for more documentation instead of asking for an explanation when I don't know something, hopefully this will incrementally help. &gt; I'd like a detailed-enough specification of the dependency-resolution algorithm such that others can re-implement it from the specification. And get a result that Cargo will be able to use... Yes! I should definitely do that! After my fight with `pub-piv deps`, I will attempt to make a list of the guarantee properties of a successful resolution and get it into the repo. And get the exact same result... may be more commitment then I want for this complex code. I can try and make a blog post explaining some of the how and why. I also have some other ideas for making an interactive version for educational purposes, but that is vaporware so far. &gt; Dependency-resolution is a Constraint Satisfaction problem I would love to have and alternative implementation built on and existing stack, mostly for differential fuzzing, but also as a form of checked documentation. I have been following the development of Chalk, Polonius, and the SAT solvers in rust but have not yet succeeded at making it happen. 
Yes, accurate. `human-panic` relies on having debug symbols available in production. Although admittedly it would be cool to add support for a separate symbol file too. But not sure what form that would take.
This worked! Thank you. Now to look up std::env and std::path so I know how this works.
Do we still need to \`extern crate test\` for benchmarks? I can't seem to find the modern way to do benchmarks in rust.
I don't know if this is what you're looking for, but I'm also a fairly new to Rust, coming to the language with experience generally in higher-level languages (C#, Python, Java, JS, etc.). I have passing knowledge of C and C++, but I wouldn't consider myself familiar with either of those languages. I've been trying to pick up Rust for a while, and I think a lot of it is finally sticking this (third or fourth) time around reading the first half of \[TRPL\]([https://doc.rust-lang.org/book/index.html](https://doc.rust-lang.org/book/index.html)). I'm late to the party, but I'm also catching up on this years Advent of Code problems, all done in Rust. My approach has been to get a working solution, and then look at other Rust solutions for the same problem to see how I can improve my code/how I "think in Rust". I referred to /u/burntsushi's repo (\[link\]([https://github.com/BurntSushi/advent-of-code](https://github.com/BurntSushi/advent-of-code))) for the first three solutions or so, but I find that I am beginning to get more confident as I progress. If you're interested in seeing a beginner's solutions in Rust, you can take a look at my repo ([link](https://github.com/tverghis/aoc_2018)).
I wonder if the benches could be sped up by a lot by adding the `#[inline]` attribute to all functions in the JSON example. Might be worth a try?
I recommend debugging your program with gdb or whatever your favorite debugger. Follow the stack. I don’t know how big these structs are but even if they are a couple kB you need a ton of function calls before you reach the stack limit. I would be stunned if you were reaching the limit without recursion. Make sure you don’t have a loop which calls a function which got inlined. That’s more likely in my opinion than huge structs on the stack causing this. Good luck
Though `write!` uses the format API which is a bit slower than it needs to be.
It uses dynamic dispatch and trait objects. This isn't necessary; IIRC it is done to reduce code size.
More like, if I end up sharing it (which is the plan) I'd rather not be sharing a many MB file that arguably could be done in a few KB of a shell script. Not worried about the rust binaries load time or anything, just don't exactly like the idea of sharing something truly simple that takes any more than 1-2 seconds to download.
Great, thanks! &gt; I'd recommend providing a macro since they're more ergonomic to use. Can you elaborate on this? I think maybe I know what you mean but I want to confirm.
Not the same user, but, if I could use Rust to target my DOS, Win 3.11, and Win98SE retro-computing environments, I would. As-is, I'm stuck using some mixture of Open Watcom C/C++, Free Pascal, early versions of AutoHotKey and Python, Windows Scripting Host, and whatever other stuff I can think of which wouldn't encourage people to pirate old IDEs if I upload it to GitHub. (I'm a collector and do have various old versions of Visual C++, Borland C++, Delphi, and Visual Basic... but I want to encourage contributions once my hobby projects are ready to share.)
+1, until more happens, consolidation is premature (and certainly can't be mandated, that happens organically). Even then, not every framework exists to serve a purpose -- sometimes it's to scratch an itch.
So I was reading through the book installing, and am a little concerned. Also sizes appear different rustup-init.exe (from https://rustup.rs/ is 7049KB, from rust-lang.org 5,490KB). I know this is just an installation tool for the language, but having a warning on one and not the other is concerning. 
I've had `sstrip` from the [ELFkickers](http://www.muppetlabs.com/~breadbox/software/elfkickers.html) pack of utilities drop the binary size by over a kilobyte and a half (I was using `ls -lh` and it rounded up to 2K). It strips the ELF section header from the binary, which isn't needed for the Linux loader to execute it. (In practice, the section header is [used like a lower-level counterpart to debugging symbols](https://em386.blogspot.com/2006/10/elf-no-section-header-no-problem.html). Binary analysis tools use it, but the loader doesn't.) Another option to consider if you're not targeting a platform where a lazy virus scanner might use it as an indicator of malware if packing your executable with [UPX](https://upx.github.io/). That said, if you care about size because you want to share it around, depend on only pure Rust libraries and it's trivial to build against the `i686-unknown-linux-musl` or `x86_64-unknown-linux-musl` targets. That'll statically link libc and you'll have a binary which depends only on the Linux kernel itself. (You'll want to use `lto = true`, of course, and it'll still increase the size slightly.)
Sorry, I'm having trouble seeing what your specific problem is. what code would you like to be able to write that you can't, or can't figure out how?
One thing that I'd like to see is a really easy way to vendor all the things using cargo. This way the review would be part of the normal process. You review always when you update your dependencies. Perhaps even a separate to tell cargo "I have reviewed changes"... before the build can start.
That IntelliJ Rust gif is nice. The problem is that this is the plugin doing what it's supposed to do and actually knowing the type (which just happens to be stupidly long); I typically run into it not knowing types (due to a large usage of proc macros in what I'm working on/with). Obviously it needs some safeguard against pathologically long types to avoid behavior like this. And for `Iterator`/`Future` implementing types, the solution is probably to just display `impl Trait&lt;Item=_&gt;`, since that's mostly what you care about..... _but_: you do care about the actual concrete type since that does change what methods you have available, and it's still not a general solution (though it is an 80%, as the majority of these kind of combinatorial type sizes come from combinations of these two traits).
It can be extremely helpful for end user bug reports. And the primary drawback behind *not* including them in release builds is storage space/download size. Although with storage being relatively cheap, and bandwidth speeds increasing it's becoming increasingly popular to include debug symbols in release builds.
Another question from me. I've spent a few hours trying to write a `Counter` trait that works with both slices and arrays. I was able to get it working with slices as you can see here. ``` use std::collections::HashMap; use std::hash::Hash; pub trait Counter { type Item; fn counts(&amp;self) -&gt; HashMap&lt;&amp;Self::Item, usize&gt;; } impl&lt;'a, T&gt; Counter for &amp;'a [T] where T: Eq + Hash, { type Item = T; fn counts(&amp;self) -&gt; HashMap&lt;&amp;Self::Item, usize&gt; { let mut counts = HashMap::new(); for k in *self { *counts.entry(k).or_insert(0) += 1; } counts } } ``` I tried getting it to work with arrays too by doing this. ``` impl&lt;T, U&gt; Counter for T where T: AsRef&lt;[U]&gt;, U: Eq + Hash, {} ``` Unfortunately, cargo complains. ``` error[E0207]: the type parameter `U` is not constrained by the impl trait, self type, or predicates --&gt; src\main.rs:25:9 | 25 | impl&lt;T, U&gt; Counter for T | ^ unconstrained type parameter ``` I stumbled across const generics RFC 2000 (https://github.com/rust-lang/rust/issues/44580). Is this something we need to wait for to solve problems like the above?
It would be interesting to see how much security could be gained (at least as far as malicious build.rs on developer machines) by wrapping cargo and rustc with chrome-sandbox called as a component called via rustup. It's not perfect but it's probably the most battle hardened sandbox short of containers or hardware virtualization.
I need to fill data from different containers, similar to: INSERT INTO table FROM SELECT..... I implement the relational trait in things like vectors, tree, hash maps, ndarrays, etc. When executing a relational operator, I need to pass the info: Vector.new(BTree.where(...)) So I could pass data with iterators. However, I can't figure how. Currently I copy everything into a vector, and copy again into the relation: Vector.new(BTree.where(...).to_vector()) but wish to cut that step, because I will later fill data from larger sources,like CVS, Rest services, etc 
As others noted, what I meant is that integers specifically are something I would like to see pushed in 2019. Maybe the problem is too interwoven to separate it out from the more expansive and general const generics. But also, maybe it isn't. And if it isn't, and we can focus on integers as a simpler problem, and push that across the finish line faster, I would like to see that. Then the more expansive feature of general const generics can come later. Essentially, my point is that type-level integers are currently a _hole_ in the language, due to the existence of fixed-size arrays. The more expansive const generics, while certainly a welcome addition, isn't currently causing _problems_ due to its absence like type-level integers are. So I don't want to see integers blocked on solving the larger feature if we don't have to.
I suspect you could go a lot farther if you were willing to go `#[no_std]` as well.
I'd say you should worry more about windows making shit up than Rust. It gave me nothing but trouble when I tried to use the language with it. As usual I had issues with some stuff like rls, and I'm sure that other stuff probably doesn't work very well with windows, at least not for me. If I were you I'd try using another OS. I use Rust with Ubuntu. 
That makes sense. ... ...however, if concept of a sync point is actually so tightly coupled to the concept of technical Editions that it is difficult to talk about them as separate things, I feel like that's also a problem. Or, in other words, if you have to make a real effort to talk about these two concepts separately (without, for example, making up new terms like "technical edition" like I have in this thread), then I think the fault isn't entirely on my side for the poor messaging. I don't mean that as an excuse (I agree with you, and will make an effort in the future if the topic comes up again), but rather to suggest that maybe re-thinking that tight coupling may be wise if it causes communication issues even in good-faith discussions.
That may be worthwhile, assuming its not a totally herculean effort. :-) However, I'm sceptical if its a sustainable solution, even if it can be done.
It seems likely here that the no-warning copy is codesigned, while the warned-against copy is not codesigned.
For Firefox Rust supports (or at some point recently supported) Windows XP but there are pieces of the standard library missing like locks. Other than that I believe the target is Windows 7.
That would be [criterion](https://crates.io/crates/criterion).
Is there a reason you can't do this? fn new&lt;R: Relation&gt;(names: Schema, of: R) -&gt; Self;
I'm regretting beginning that section of my post with such a sensationalistic paragraph. It doesn't actually represent my view accurately. The point I was trying to make is that Editions have a particular kind of cost associated with them, and I therefore think we shouldn't treat them as a given thing with a regular cadence. Rather, we should only do them _when the benefits outweigh the associated costs_. I absolutely expect Rust to continue to evolve over time, and some of that will involve breaking changes. But I think it's... well, frankly, silly to put Editions on a cadence. For example, if we end up sticking with the three-year cadence that's been talked about, then by the time Rust is 12 years old, we'll have _five incompatible, concurrently-existing_ versions of Rust. That will make Rust as a language and ecosystem really hard to grok, even for experienced users, and will also (I suspect) be a significant maintenance burden on rustc devs.
Small nit: my username is cessen2, not cessen. (I believe I also registered cessen, but it was a long time ago and I didn't associate an email with it... and forgot the password. But actually, it was so long ago, I'm not even 100% sure it _was_ me. Heh.)
This also explains the size difference: The size of the 32-bit installer from [rustup.rs](https://rustup.rs) is 5489.5kB.
If you're messing with `lto = true`, you should also try out `codegen-units = 1`. If you set it on the release profile, it'll be much slower at building because it won't be parallel, but it should produce better binaries.
It honestly reminds me of Deltron 3030 https://youtu.be/FrEdbKwivCI
Yeah, it does make working with futures a bit painful! It does look like it's been resolved in newer versions of the plugin:https://imgur.com/Sk7w55o 
Yeah, that would definitely be the reason.
Don't be That Guy.
&gt; But actually programming directly in that way is incredibly difficult. An I the only one who finds using futures and threads and tokio not that difficult at all? I mean, starting a future and `then` is actually quite neat! What an I missing? What makes `async/await` so awesome? 
As a reference point, C++ already has 5 incompatible versions: * `-std=c++98` * `-std=c++11` [(list of breakage)](https://stackoverflow.com/questions/6399615/what-breaking-changes-are-introduced-in-c11) * `-std=c++14` [(list of breakage)](https://stackoverflow.com/questions/23980929/what-changes-introduced-in-c14-can-potentially-break-a-program-written-in-c1) * `-std=c++17` [(example breakage)](https://stackoverflow.com/questions/47656093/changed-rules-for-protected-constructors-in-c17) * `-std=c++2a` [(example breakage)](https://twitter.com/timur_audio/status/1055771081993269248) This isn't necessarily a good thing, but it also doesn't seem to cause much trouble in practice – at least I've never seen anyone complain about it – because the breaking changes are generally minimal and relatively obscure (especially for the revisions after C++11). In theory, Rust could do the same. Rust 2018 seems to break a relatively large amount of code due to the module import changes, and it would be good to avoid similar changes in the future. But for more minor changes, such as adding keywords, it's not the end of the world even if there are technically N different incompatible versions.
Actually, you should not put a bound on the struct unless necessary.
The issue for me started when I started composing huge chains of futures with complex logic inside. For instance, if you are in an `and_then` closure and you match on something and have a lot of patterns and want to return a different future for each match, you can't do that directly, because the futures will be different types (in my case they were), even though they have the same `Item` and `Error`, which would likely require you to box them and cast them, which doesn't seem entirely ideal to me. The future combinators are a good idea, but didn't provide the best experience for me, because there is usually a lot of nitty-gritty mapping of errors and being super careful about what you return in each closure, casting futures to get the compiler to just compile it. Otherwise, when things start getting complex you can implement your own futures wrappers, which works quite well, but it's rather ugly in my opinion, just to compose futures. There was a blog post not too long ago demonstrating the new async await, with an experimental tokio crate, with ways of using 0.1 futures and 0.3 futures (std futures) interchangeably. I could write an async program, with futures, without using a single future combinator. It feels and looks like you are writing regular rust. I think in essence, it's just a much nicer / more ergonomic way of composing futures. Also, the `?` operator works beautifully with async, making the error handling story much, much easier and nicer. That's my 2 cents of why async await is awesome
Setting up diesel cli on windows is a nightmare. I tried everything possible to set it up, but to no avail. That's why I made a drastic switch to Ubuntu. No regrets switching.
Have you tried cargo crev? The experience should be quite like that. 
This is amazing!
`panic = abort` instead of `panic = unwind` seems to give a significant size reduction, also.
You can't "just ship cpp code" and magically work in the browser... This article contains many falsehoods and misconceptions.
TBH, in my experience the vast majority of Windows issues with Rust are the direct result of crate authors who either know very little or nothing about Windows making incorrect assumptions about various things in their `cargo.toml` files and `build.rs` files. All the infrastructure is there and does work when used properly, though.
This is really awesome. The license MPL2 could be limiting the adoption of a library, though.
I use [https://github.com/benfred/py-spy](https://github.com/benfred/py-spy) regularily on Win 7.
This has nothing to do with const generics. `U` is unconstrained: It does not appear where it should be. If you switch from the associated type `Item` to a type parameter, you get the means to contrain `U`: trait Counter&lt;T&gt; { fn count(&amp;self) -&gt; HashMap&lt;&amp;T, usize&gt; } impl&lt;T, U&gt; Counter&lt;U&gt; for T where U: Eq + Hash, T: AsRef&lt;[U]&gt; { … } Concerning your slice impl, it's idiomatic to `impl Counter for [T]`, then you don't need to deref `self` in the head of for-in.
https://doc.rust-lang.org/book/
Why wouldn't they distribute the same package on both website?? This looks like a big issue to me
So with `async` you don't pattern match on different futures? Do you have a different `await` fit every case it how is this problem solved? Also, thanks for your answer 😊
I thought `codegen-units = 1` was the default for release binaries already?
Is the async book out of date? I found some places where I thought there was old information. For example: For this one, we're going to have to include the futures crate in order to get the `FutureObj` type, which is a dynamically-dispatched Future, similar to `Box&lt;dyn Future&lt;Output = T&gt;&gt;`. The author even enables `arbitrary_self_types`, which allows dynamically dispatching on a `dyn Future`. Using `FutureObj` adds confusion, as its necessity is not properly motivated.
It differs from split_at_mut as it allows all the split-off parts to be read as a contiguous slice.
I find it a pain that syntax highlighting and type hints just kinda stop working inside macros (like `lazy_static` for example). It makes using them a little less convenient.
This might sound like a weird suggestion but you could try going through the tutorial for learning Django (a web development framework in Python). Without focusing too much on the idiosyncrasies of the language, it gives a good tour of the various pieces that make a web application. Link: [https://docs.djangoproject.com/en/2.1/intro/tutorial01/](https://docs.djangoproject.com/en/2.1/intro/tutorial01/)
Whether the structs are large enough for the cost of HashMap operations to cause issues can only really be answered by benchmarking. Reasoning about performance in such specific cases is never as certain as just measuring it.
In what way? AFAIK, you can do anything with it as long as you are not modifying it.
Is going through the whole chain really necessary, can't 1.28 be built with 1.19 directly?
On aspect is familarity/understanding. Until today i assumed that it is comparable to the GPL - my bad. And i am not sure how to comply with the obligation to provide the source code if e.g. used in an app. 
&gt; What an I missing? Loops. That's where the code transformation really pays off.
The GNU toolchain uses the MinGW build tools (mostly the linker) and produces binaries that use the GNU ABI. The MSVC toolchain uses the Visual Studio build tools and produces binaries that use the Microsoft ABI, making them more compatible with most other Windows binaries/libraries.
It could be that the particular copy from rust-lang hasn't been downloaded a lot yet. Windows tends not to trust new things that are new unless it's either signed by a publisher that hasn't had problems, or it's signed with an EV signing certificate. If there's no signature, then every binary starts with a clean (untrusted) slate, afaik
\&gt; mrustc is now good enough to compile Rust 1.19.0. 
Allow me to demonstrate with perhaps a bit of a long example, but it is a problem I have encountered before. If there is a better way to do it with old futures, then I will be glad to hear it, but I have as of yet not found a nice way to do what is done here async fn case1() -&gt; i32 { 1 } async fn case2() -&gt; i32 { 2 } async fn case3() -&gt; i32 { 3 } async fn case_else() -&gt; i32 { 0 } async fn deal_with_num_new(x: i32) -&gt; i32 { /* all of these case_x funtions, we consider as different futures to make it comparable to the old way of doing futures. */ let num = match x { 1 =&gt; await!(case1()), 2 =&gt; await!(case2()), 3 =&gt; await!(case3()), _ =&gt; await!(case_else()), }; num * 2 } //old futures way to follow from here struct Case1; struct Case2; struct Case3; struct CaseElse; struct OldExampleFuture&lt;T&gt; { _tp: PhantomData&lt;T&gt;, ret_val: i32, } impl&lt;T&gt; OldExampleFuture&lt;T&gt; { pub fn new(x: i32) -&gt; Self { OldExampleFuture { _tp: PhantomData, ret_val: x, } } } impl&lt;T&gt; OldFuture for OldExampleFuture&lt;T&gt; { type Item = i32; type Error = (); fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { Ok(Async::Ready(self.ret_val)) } } fn deal_with_num_old(x: i32) -&gt; impl OldFuture&lt;Item = i32, Error = ()&gt; { //simulate different old futures with generics, but they all return the same item and error, but have different types let case1_fut = OldExampleFuture::&lt;Case1&gt;::new(1); let case2_fut = OldExampleFuture::&lt;Case2&gt;::new(2); let case3_fut = OldExampleFuture::&lt;Case3&gt;::new(3); let case_else_fut = OldExampleFuture::&lt;CaseElse&gt;::new(0); /* Here is the crux of the old futures, this doesn't compile, and nor should it. all of the futures are different types, but they return the same item and error, but the match arms are different types, which isn't allowed. So you need to coerce all of the branches to the same type. You can box and cast all of them or make a new future wrapper or probably use some other methods, none of which are very nice. Maybe there are nice ways to do this that I haven't discovered yet, but regardless, async await still seems more natural to me. let final_fut = match x { 1 =&gt; case1_fut, 2 =&gt; case2_fut, 3 =&gt; case3_fut, x =&gt; case_else_fut, }; */ //This version works, because after the cast, the compiler doesn't need to know a specific type when the future is behind a box // but this method doesn't look very nice. The async await version, reads more like regular rust code let final_fut = match x { 1 =&gt; Box::new(case1_fut) as Box&lt;OldFuture&lt;Item = i32, Error = ()&gt;&gt;, 2 =&gt; Box::new(case2_fut) as Box&lt;OldFuture&lt;Item = i32, Error = ()&gt;&gt;, 3 =&gt; Box::new(case3_fut) as Box&lt;OldFuture&lt;Item = i32, Error = ()&gt;&gt;, _ =&gt; Box::new(case_else_fut) as Box&lt;OldFuture&lt;Item = i32, Error = ()&gt;&gt;, }; final_fut.map(|x| x * 2) } If you ignore the small implementation of an old future and my comments, then both the new and the old way appear to be roughly equally long, but to me, the async-await version looks cleaner to read and thus understand. And let's say you are using these functions elsewhere, for old futures you'd be using them in combinators, again, async await will still look like regular rust if you `await!` something. Where async await also really shines is that because you can just make an async function that returns a result, you can use `?` everywhere you normally would. Error handling in closures are less nice, to me at least. 
Trend Micro also flagged rustup 1.16 for me as doing "Unauthorized file encryption". Whatever (broken) heuristic they're using for detecting malicious code is getting triggered by rustup downloading, verifying and unpacking packages.
A sorry I somehow didn't see that /u/texelot already posted the tldrlegal link, my fault.
Maybe [https://github.com/thepowersgang/mrustc](https://github.com/thepowersgang/mrustc) helps to lower the requirements?
Just keep in mind that might require unaligned loads instead of aligned ones, so one might not be able to reuse the exact same code for that.
I don't know anything about licenses, but I would guess you could link to the git.
So I guess we are on the same page. About extracting the dependency resolution algorithm, from my little experience with cargo, the problem is going to be that it is a bit "entangled" into the code base. This is both technical debt, and a necessity, cargo has a lot of features: optional/non-optional dependencies, target dependencies, crate features, different types of libraries (e.g. proc macro), ... So a lot of parts of the code base need to communicate and affect dependency resolution. Also, `cargo` is so big that I don't think a rewrite is feasible. Extracting things into different crates, where every crate solves one task, would improve enforcing the privacy boundary, and make the code base more approachable (no need to understand all of it to make changes).
I agree. Semver is treated in a more professional way in the rust ecosystem and the reasons why critical crates are at 0.x are correct. The counter-argument is also true though: if you want to catch managers' attention, you need to be 1.x, at least for the main components. The professionalism in rust in regards to semver further widens the gap between reality and expectations.
Not really. rustc itself uses a large number of unstable Rust features.
It’s very much a work in progress.
Okay, so if I understand correctly it's about interoperability between particular binaries / libraries in windows? Is there a particular preference / set of drawbacks for choosing one over the other?
Something clearly needs to be done about this. Either the compiler stops using unstable features, or we do it like Nim and provide the compiler source code compiled to a more common language. Nim uses C as an intermediate representation, so they obviously use it for the compiler bootstrap code as well. Since Rust doesn't do that and LLVM IR is platform dependent (or so I heard), perhaps webassembly would be a good target. I would argue that almost everyone who wants to bootstrap Rust also has access to a wasm-capable browser, and building the compiler once with a slower version of rustc is probably still better than going through this huge bootstrap chain.
General update: I've taken the input from here and [a GitLab issue](https://gitlab.com/chrysn/sealingslice/issues/1) and updated the crate accordingly. The unsafe part has changed from "declare I won't use it any more" to "safely get a slice, and then concatenate two (assertedly) adjacent slices into one". The discussion has also shown me that for the primary application I had for this crate, `split_at_mut` is indeed sufficient if the internal API is designed tight enough, so I'm not in active need of this crate any more (but still try to keep it in shape). For others to not repeat my mistake, the README now contains a section that outlines when it is actually needed, and when `split_at_mut` can suffice.
There is a great [documentation](https://rocket.rs/v0.4/guide/) in Rocket. Just think up a task and develop a web application via Rocket. I am not familiar with web applications too (I'm not a programmer), but I have developed an [application](http://130.193.58.61) in a few weeks (caution! Russian language!). If you know the Russian language, I wrote an article about how to develop the application(please pm). If you want to learn rocket - all you need is interesting (for you) idea and desire to develop
How is the rustc compiled to webassembly better than the real one? It doesn't give you more targets. Maybe you can run it on special hardware but I doubt that running rustc on a target that rust does not support is a real usecase.
With gnu tool chain you can cross compile Windows binaries on Linux host. With msvc toolchain you are stuck in hostile Windows environment :) 
I'm coming from a Django background and I second this. If OP needs something even more basic, then I can also recommend the Django girl tutorial: https://tutorial.djangogirls.org/en/
It's not supposed to be better, it only eases the process of compiling rustc without downloading a rustc binary beforehand.
What I meant is that if you do not want to bootstrap the compiler then simply use the original one. If you cannot use the original one due to trust issues then you cannot use the webassembly one either.
To offer a counter argument: there are 2 extremes - 1. Move at lightning speed and make a shoddy product 2. Make sure you implement everything perfectly (i.e. do nothing because it is impossible for humans to attain perfection). What happens in practice has to be somewhere in the middle. I think one of rust's strengths is that it moves slower than other languages, carefully evaluating everything before inclusion, and this has led to a better language. However, deadlines help to focus minds and drive productivity. They also force you to make decisions when there is no "right" answer and there may be a temptation to put off the decision indefinitely. So I can see the value of setting yourself a deadline and then making sure you hit it.
Yes but if I *want* to bootstrap, then that's a process in which I have to compile like ten different versions of the compiler to finally obtain one that doesn't choke on the latest release's source code. Do you not see the problem with that?
I thought the whole point of getting a gcc front end going was that cross compiling for gcc supported platforms was a non-issue?
That makes a lot of sense - thanks!
I guess we just have different understanding of what bootstrapping means. I wanted to point out that there is no real difference between using the real rustc and the webassembly version. So if you download the previous version of rustc in webassembly form just to save time then you could also download the real rustc. There is nothing gained by the webassembly version.
Except if your target supports webassembly but has no binaries built for it. I'm not sure there is such a target though.
This is already amazing! Nice work!
That's what this part in my first comment in the chain was meant to cover: &gt;Maybe you can run it on special hardware but I doubt that running rustc on a target that rust does not support is a real usecase. &amp;#x200B;
There’s definitely value in a deadline, but it’s important to remember that this was a self-imposed one, and could have been moved without negative effect, which is what I think should have been done. Moving it by a month to allow a slightly slower pace would’ve helped a lot id say. That being said, I do appreciate that Rust moves slower and more carefully than many languages and we definitely haven’t ended up with the python 2 vs 3 mess, for example. The edition must’ve been difficult to manage and I think the teams have done a great job, given the conditions
&gt; Each target we want to support has to have support in LLVM, AND mrustc needs to have a specification of the alignment and sizes of the base types. I don't agree with this. With rustc 1.28 and LLVM on a given platform, one can probably cross-compile rustc 1.28 (and LLVM) for another platform without having to go through the whole chain again. Cross-compiling at any point does not break the bootstrap process.
Why of course there is a difference! You compile the final compiler yourself, only you use the wasm version instead of starting with mrustc and doing it 10 times. How does that qualify as "nothing gained" in your book?
Bootstrapping is a way to avoid the [trusting trust](https://www.ece.cmu.edu/~ganger/712.fall02/papers/p761-thompson.pdf) issues. Using WASM as an IR doesn't help here.
There are tradeoffs between each three. That being said, actix is probably the best choice overall. It's the most mature web framework on stable that's maintained. Rocket requires nightly at the moment (this may change soon-ish, and if you're comfortable with nightly it's not a problem. Tower web is rather new, and is pursing a different structure for web frameworks that separate the http handling and the web app framework into separate pieces it's still pretty new, last I checked.
As others have said already, if you use the 2018 edition, you opt-in to improvements or new features that could not have been introduced into Rust 2015 without a breaking change. However, the compiler can still compile Rust 2015 crates and produce compatible artifacts - and those compiled crates can be mixed freely. This is not a 2.0 in the sense that you can keep using your old crates with the new compiler without needing to change them. So the Rust developers were able to improve the Rust language in incompatible ways without needing a 2.0, and thus without splitting the ecosystem. The short version is this: If you start a new project and you only care about Rust 1.31 and later, use Rust 2018 - it's much clearer in many ways, and it's also Cargo's default for new projects. Similarly, if you just want to learn Rust and play with it, use Rust 2018. (There are two main reasons why Rust 2018 is more fun to write: First, the module system changes make project structure much more intuitive. Second, Rust 2018 uses NLL by default, which means the borrow checker will be much friendlier, especially to a beginner.)
because you're still using a webassembly binary, missing the point of this bootstrap nonsense. you could do the exact same thing by downloading rustc `version-1` and building the final rustc `version` yourself
&gt;Either the compiler stops using unstable features It's not just the compiler. The standard library uses tons of unstable features under the hood as well, some of which are perma-unstable because they link std to compiler primitives. For lack of a stable API, it is impossible to guarantee that Rust 1.X std can be built with anything but 1.(X-1). In fact, std uses flags to specifically allow building it with the previous compiler version, by disabling features that are not needed in the bootstrap and would otherwise be incompatible with 1.(X-1).
Any idea of what makes it trivial to build against `i686-unknown-linux-musl` or `x86_64-unknown-linux-musl`? Is it just using certain (or no) Rust libs other than std?
Looks like it is the default. Either that or with my small project it has no effect.
Looking around online it seems that `unwind` is only really useful if I plan on some extensive error handling and recovery, such that for example, I could have a thread panic and then catch it in another thread and preventing a full crash. Yeah... Definitely don't need that in what is effectively a small script.
What does a GCC Rust frontend has to do with this post?
The rocket guide. It's pretty clear and basic.
Thanks all for replying to this. I think the length of this thread is starting to prove my point a bit: this is not at all that clear and I accidentally triggered a lot of people to essentially state a lot of different things about this that are not that consistent with each other. My take after all this is that edition 2018 means for all practical purposes means it's Rust 2.0 with a small amount of new stuff that isn't backwards compatible but mostly a clean upgrade path; plenty of features to prevent that from becoming a problem (ability to target older versions, ability to use crates from older versions, etc.); and a commitment to not do this again for a while until about 2020. In general, when things require a lot of explanation, that means they are not self explanatory. Education is one solution (though often futile), reconsidering available options another strategy. IMHO Rust 2.x might have been a lot easier to explain but I'm fine with this either way. I'd recommend to actually bump the version number to 2.0 as well to avoid confusion about which versions of rust correspond to which edition. 
Thank you for sharing this status, Aaron
this may be a scenario where you will adopt websockets, so I recommend trying actix
I suggest posting it over on /r/playrust too.
If all of your dependencies are pure Rust, then you can just `rustup target add x86_64-unknown-linux-musl` and you're ready to go. If any of your dependencies depend on C code, then you also need to set up a build environment for compiling C code against musl-libc... and that's significantly more complicated. For example, with `error-chain`, you want to set `default-features = false` because the `backtrace` feature pulls in a C dependency.
Nice effort sadly targetted towards a wrong audience. see /r/playrust for, hopefully, a more enthausiastic response. 
I went ahead and changed the code to use `impl Counter for [T]`. I'm going to look further into unconstrained issue. Thanks for putting me on the right track.
When is it necessary? Can you show an example? Thanks in advance.
Some platforms (debian comes to mind) forbids cross compiling in the bootstrap process.
Most likely it's specifically an oversight in the implementation of the new [rust-lang.org](https://rust-lang.org) website. I imagine (or at least hope!) that it is *supposed* to use the same kind of platform detection [rustup.rs](https://rustup.rs) does in order to ensure everyone always gets linked to the right copy, but just currently doesn't. Might even be worth reporting as a Github issue?
So relatively simple to determine. Thanks!
&gt; Nim uses C as an intermediate representation Did the LLVM C backend get resurrected for this?
Hi, Pietro from the Rust infrastructure team here! Thanks for reporting this! I [opened an issue](https://github.com/rust-lang/rust/issues/56815) on the rust-lang/rust repository, and you will be able to follow the progress there. The next time please report those issues in our issue tracker instead of Reddit though! We don't actively monitor this subreddit for issues `:)`
It's great that the language can evolve rapidly and that the compiler can capitalize on language changes. Bootstrapping being this complicated is an unfortunate consequence, though. Perhaps with the "Edition" strategy, language changes can be introduced similar to how it's done in C++. Maybe with a more explicit indication, the `rustc` team would be more aware of how/whether they're increasing the requirements of the host compiler.
If `mrustc` continues its development, the chain can be shortened.
Thank you! Sorry about that!
Thank you! Sorry about that!
The Rust team isn't accidentally using new features which require a new compiler version. The policy AFAIK is to always use the previous stable compiler. You can see the full history here https://github.com/rust-lang/rust/commits/master/src/stage0.txt
I thought the whole point of LLVM IR is that it’s not platform dependent tho
I don't think so. https://doc.rust-lang.org/cargo/reference/manifest.html This says that the default for release is 16.
https://doc.rust-lang.org/cargo/reference/manifest.html This says that the default is 16. If you saw any drop in file size at all, then it proves that changing it had an effect, right?
&gt;perhaps webassembly would be a good target. I would argue that almost everyone who wants to bootstrap Rust also has access to a wasm-capable browser I think this is very much not at all technically feasible/possible, for many reasons.
Upvoted for suggesting the use of a profiler! Measure first, then optimize. Rinse and repeat.
Idea debugger doesn't work with msvc yet
Nim just literally uses C the language as an IR. There's currently no LLVM anything anywhere AFAIK.
Oh wow, as a nonbinary rust user, I think it's really cool that ferris uses they/them. I'm sure I'm not the only one. Thanks, everyone responsible for that &lt;3
I used to feel very guilty about how entangled the resolver is with the rest of the code. Then I started to read the code for the newly redesigned [resolver for Dart](https://medium.com/@nex3/pubgrub-2fb6470504f), while we have a lot to learn from them, there resolver is just as entangled. The strategy of finding small parts that can be crateefide, is generally accepted as on the todo list. It is just perpetually low on the list.
MSVC - uses Microsoft linker, PDB for debugging information (compatible with Visual Studio debugger), is compatible with C and C++ static libraries compiled with MSVC. GNU - uses GNU linker (ld), DWARF for debugging information (compatible with GDB), is compatible with C and C++ static libraries compiled with GCC. By the way, you can also debug Rust executables compiled with msvc toolchain in Visual Studio. Just use "open project" and open target/debug/your_program.exe. 
It was late and I thought that g++ at the end of that chain and the fact that this was a post from gnu.org meant something. Sorry!
Oh, I was so stuck moving things around that I assume I need to become Relation&lt;R&gt; to do something like that. I will check and see how things going. 
I wonder, is it possible for bugs in an early version of a compiler to bubble up to succeeding versions? Even supposing that what you're compiling has no bugs as we understand it, if the thing you're using to compile it produces erroneous output, then isn't it possible that what you've compiled also has errors?
I was gonna ask where C and C++ libraries compiled with clang fit in there. I looked it up and found https://clang.llvm.org/docs/MSVCCompatibility.html
Somewhat nitpicky, [but you can do that in javascript](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/all): ``` let [res1, res2, res3] = await Promise.all([promise1, promise2, promise3]); ```
My biggest difference is that -gnu actually works for more than a week, since my visual studio can't keep track of itself and keep itself from being lost. But my situation is kind of special. The real difference is going to be with libraries linking to system resources. Some crates won't work on one or the other because they make assumptions about what they can link to, which differs between them.
I‘m pretty sure with lto, there‘s only one codegen unit anyway.
If you want an ergonomic syntax, I'd recommend Rocket, but beware that it has some downsides: it does not work with stable Rust, and the framework does not handle async code. On the other hand, it has tons of cool functionalities easy to use and useful. I guess that your choice will depend of your priorities.
Hey, I didn't mean nothing by it. I didn't know this would be received so negatively. It's just that it is my impression that windows usually has a harder time adjusting to what many lanaguages and open source tools do. 
&gt;because the breaking changes are generally minimal and relatively obscure (especially for the revisions after C++11). &gt; &gt;\[...\] &gt; &gt;But for more minor changes, such as adding keywords, it's not the end of the world even if there are technically N different incompatible versions. I think this is the main thing, yeah. As I said in [my reply to epage](https://www.reddit.com/r/rust/comments/a5nd4i/rust_2019_its_the_little_things/ebo8x3y/): &gt;So in the case of new warnings and keywords like you mentioned, those are likely to have less impact in the way I'm talking about, and I suspect I wouldn't be too concerned about an edition consisting only of those. &gt; &gt;However, those also aren't the kinds of things to excitedly look forward to or plan for or ask for people to write blog posts about 3 years prior. So my impression *is* that people are thinking about editions as being for language evolution via breaking changes. I could totally be wrong about that, but that's my impression, and that's what I was responding to. I guess, maybe to sum up what I mean, is that what I'm concerned about is *practical* breakage--significant shifts in the language, as we saw in the 2018 Edition, that cause educational material to become incompatible with the current Edition, and not just in obscure corner cases. So I _think_ we actually agree.
&gt; It's just that it is my impression that windows usually has a harder time adjusting to what many lanaguages and open source tools do. What exactly do you mean by this? There's nothing of significance you can do on Unixes that you can't do on Windows. In all cases cross-platform compatibility is just a matter of the implementation, I'd say. For example literally the *only* reason that the (very cool!) [Rapidus JavaScript JIT](https://github.com/maekawatoshiki/rapidus) doesn't work on Windows is that the author makes direct use of `fork`/`waitpid` and such in this [one specific function](https://github.com/maekawatoshiki/rapidus/blob/master/src/main.rs#L212) instead of a cross-platform abstraction over process-handling functionality.
Wrong sub big fella r/playrust
Yeah I'm using Jetbrains IDEs currently
Just out of curiosity, does that mean that rust binaries compiled by the two different toolchains are incompatible?
That was never a goal of LLVM IR. Classic: "LLVM IR is a compiler IR" http://lists.llvm.org/pipermail/llvm-dev/2011-October/043724.html
Oh super interesting. I’ve only ever used LLVM with the API haven’t touched the IR much
Its more of a personal thing. I have always had issues using windows with technologies that aren't in the Microsoft stack. Sometimes you have to deal with things such as compiling the source for some tools, creating configurations that are otherwise available on the unix versions. Sometimes I also get the feeling that Windows doesn't always play well with some tools. Anyway that is what I meant, I'm not saying that you can't do things in any OS, just that with Rust I didn't have a nice time with it so I switched. 
Note that "Clang on Windows" does **not** necessarily always mean "`clang-cl` as a replacement for `cl` in the context of an MSVC toolchain". Clang can be (and often is) built against the MinGW-W64 ecosystem in such a way that it's a drop-in replacement for MinGW `gcc` and `g++` (i.e. `clang` and `clang++` are then the compiler executables used.) The same applies to the rest of the associated LLVM tooling also, i.e. `LLDB` can also be built against MinGW-W64. The only exception to being a literal drop-in replacement is (unfortunately) `LLD`, as although when built that way on Windows it can technically work with the MinGW ABI just fine given the proper command-line flags, the `LD.LLD` frontend for some reason by default always assumes it is working with ELF files and the GNU-style linker script parser is disabled entirely. I do hope the LLD thing something they fix in the future, as it really is the "missing piece of the puzzle" as far as LLVM tooling goes in that context. That said, until then, normal MinGW GNU LD does of course work with MinGW Clang.
Just when I think that the fanboi-ism around this language can't get any more eye-rolling and stupid, you do something like this... AND TOTALLY REDEEM YOURSELF! yo, that's fire.
No problem. Also, just to be clear, `i686-unknown-linux-musl` and `x86_64-unknown-linux-musl` are equally simple. Cross-building from 64-bit x86 to 32-bit x86 is supported Rust and GCC by default.
Yes, I know, but I was specifically trying to avoid doing let (res1, res2, res3) = await!(join(vec![f1, f2, f3])) which is what that would look like in Rust. `Promise.all` is necessary in JS because `await 5` =&gt; `5` so `await [p1, p2]` must =&gt; `[p1, p2]` rather than `[res1, res2]`. Because `await!` is a macro in Rust rather than an operator, it can be given multiple futures rather than just one collection, whereas `await` in JS just awaits the right side. For whatever reason, they're preferring `join!` which seems fine to me.
Actually that is very interesting question that goes far beyond that. Consider https://www.win.tue.nl/~aeb/linux/hh/thompson/trust.html
You might have an easier time compiling stuff that was originally developed on Linux if you use the GNU toolchain, especially if you use it in combination with MSYS2. For example if you were wanting to use something like `relm` with the MSVC toolchain you'd have to get GTK built against the MSVC ABI yourself, which is possible but often annoying to do. With MSYS2 you could just install the GTK package from its version of pacman and start developing with `relm` right away.
Wasn't LLVM's ThinLTO compatible with multiple codegen units at the expense of weaker optimization... or is my sleep-fogged mind playing tricks on me?
What kind of binaries, exactly? And compatible in what way?
Yeah thin-lto is probably different in that regard.
Clang supports both GNU and MSVC targets.
Even binaries compiled by different versions of the same toolchain are incompatible, because Rust does not have a stable ABI. If you stick to communicating via the C ABI then all binaries will be compatible, but you still have to be careful to avoid mixing allocators.
I'm just working on the topic of concurrency, threads, mpsc channels, etc. Knowing that this exists and what it does, why is async/await such an often requested feature? If I want to do something asynchronously I can start a new thread and send the data back via a channel. That's already perfectly asynchronous, no? What did I misunderstand here? How is one better/worse than the other? (Please note that I don't have a CS background, so an ELI5 answer is appreciated)
For me bootstrapping rust breaks at when it tries to download anything during compilation phase.
Since your mentioning tower web I'm guessing you're ok with new, less mature frameworks? In that case I wanna throw [warp](https://github.com/seanmonstar/warp) in to the mix. I've tested actix and some other frameworks and I really love the simplicity and lack of magic in warp. It's also a lot more lightweight than actix and compiles faster. It uses a nice filter construct to chain stuff together in a really understandable way. It's young and a lot of things will probably change. But it's looking really great! Compiles on stable.
Threads are pretty heavyweight - more than a couple hundred and you run into issues (you have to have stacks for each thread - meaning the amount of memory you're using expands massively, and context-switching between threads is time consuming because it pops out to the OS scheduler.) Async avoids these issues because 1. there's only the single stack, and 2. context switching is entirely within your program (controlled by tokio or whatever you want to use.) So the basic benefits: * Less memory usage * More efficient switching between "threads" * Also you don't have to bother as much with mutexes or synchronizing threads
Wow. I was waiting for someone to work on something like this. Pretty impressive stuff.
Please read a subreddit's sidebar before posting to it. You are looking for /r/playrust. Also, mods, should there be a minimum account age before posting? This account is 100% fresh with no posts beforehand. Should it be allowed to post?
I concur - as a user of both actix and warp I much prefer warp. I love how tiny it is. But on the other hand my needs are very simple, don't have things like user sessions.
Thanks for sharing that link. Fascinating stuff.
I'm not a compiler developer for Rust, but from what I can tell from my own experiments, writing something that takes Rust's AST structures and emits valid C, for example, shouldn't be all that difficult. You can skip parsing, the borrow checker, and most of the compiler's internals by hooking this up to the right stage in the compilation process. Once you've done that, Rust is suddenly portable (provided you have a working C compiler - but that's relatively trivial). What's more, hooking the C generation code into a later stage in the compiler - after syntax sugar has been unraveled - allows you to partly piggy-back on future syntax updates without needing to do much to update your C generating module.
We've played around with these settings quite a lot - it's difficult to draw a balance. There are plenty of legitimate Rust users with young accounts too. cc /u/kibwen
Thanks, this makes a lot of sense now. But now I'm wondering, if this is essentially a "named impl trait," why is it existential type Foo: Debug; and not just type Foo = impl Debug; I know it seems like bikeshedding, but it's a huge improvement from a learning standpoint just by relating it to something familiar. 
I'm surprised nobody mentioned [Rouille](https://github.com/tomaka/rouille) yet. It's extremely straightforward and has the lowest friction of any framework I've seen. Hello World looks like this: fn main() { rouille::start_server("localhost:8000", |req| { rouille::Response::text("Hello, World!") }); }
I think `log(js_sys::Reflect::get(each, "a".into()).unwrap())` should work. I don't know if anyone with more experience can help more. &amp;#x200B; However, you should look into using [`JsValue::into_serde`](https://rustwasm.github.io/wasm-bindgen/api/wasm_bindgen/struct.JsValue.html#method.into_serde), and creating a `struct` for your data.
but why?
This is what my head says ... * Actix, this is the most mature web framework that builds on stable. It has the biggest ecosystem. For these reasons this should be your number 1 choice IMO. * Rocket, this is probably the main alternative to Actix. However it currently only builds on nightly. The core developers are working on moving it to stable. * Tower-web, lots of potential. Downside is it's very new. So it has the smallest ecosystem, the least examples, and so on.
I think with a little work you can get MSVC toolchain running under Wine. And it may be possible, now or in the future, to target MSVC from Linux without Wine using `lld`, though it will probably take either Wine or a Windows host to get the SDK unpacked. But anyhow, yeah, the GNU toolchain lets you cross-compile without any proprietary software from any host. The advantage of the MSVC toolchain is that you can link with other libraries (like proprietary, binary only libraries) that have been built with the MSVC toolchain on Windows, and you can get better tooling integration with Windows development tools and debuggers. Basically, you want to use the GNU toolchain if simpler cross compilation and fully free software toolchain are important to you, and linking with binary Windows libraries and integration with Windows dev tools is not. Vice versa, you want to use MSVC if linking with binary Windows libraries and integration with Windows dev tools is important, and cross compilation and free software toolchain are not important to you. Most people are probably best off just using the MSVC toolchain.
The thing is that impl Trait is just a normal „universal“ / generic in argument position, so it doesn‘t always correlate with existentials. People are definitely suggesting this as an alternative. You‘d have to read the RFCs and the issues for it for all the specific bikeshedding.
/r/playrust
Functional programming is more a paradigm than a style. 
Ahhhh OK that makes sense. Thanks for explaining. Just to make sure I understood this: Part of the reason is because Rust uses OS threads instead of green threads like Go for example, correct? Meaning if Rust had green threads, async/await would be not/less be needed? But of course there are also advantages to using OS threads and Rust having both OS threads AND async/await is a "perfect" mix?
I have used Rocket and Actix and I personally prefer Rocket. Found Actix to require more boilerplate, and actor stuff is weird imo. Rocket just kinda got out of my way. Don’t mind being on nightly. 
Whats the benefit of bootstrapping over cross compiling and then using that if you want to compile locally on the platform?
Agree with the first two but &gt; Compile Times Don't care. I mean, yes, better compile times are good, but not at the cost of features. Especially in release mode, compile times when compiling final builds are a non-issue. If you use a source based distro then that's your fault. &gt; Rust 2021 Edition Again, don't care. Any issues that you have would be your fault, assuming editions are crate-level and they interoperate fine between editions. It's not like Python 3 where you have to only use Python 3 dependencies.
The instructions for using a macro are generally: 1. Add library to Cargo.toml 2. `use` or `extern crate` library in crate root 3. use macro with codegen you have all of that plus 1. Add crate_codegen dependency (sometimes) 2. Create or add to build.rs file 3. `include!()` generated code
Sorry to hear that; I got it going pretty smoothly! Maybe I should write a blog post.
`build.rs` is not a script for installation. This script is run *before* a crate is built to allow building native dependencies and generating code.
 &gt;You can skip parsing, the borrow checker, and most of the compiler's internals by hooking this up to the right stage in the compilation process. Why would we want to skip all that to target new platforms? 
Oh I misunderstood. Is there anything I can use to install these files on build? I really like cargo install - - git for installing utilities and was hoping still use this. 
[LDC](https://github.com/ldc-developers/ldc) is doing it right: `master` can always be built with `ltsmaster` (which is written in C++). I guess the maturity of D helped them a lot :) Evolving Rust while keeping an OCaml version of the compiler would've slowed Rust evolution down significantly. But now that Rust has stabilized and mrustc exists, the easiest way might be to "adopt" mrustc officially and make sure rustc always builds with mrustc.
What’s worse is that if you define structs/impls with macros, IntelliJ can no longer resolve any of those types which renders macros basically useless.
&gt; DWARF for debugging information huh, wow. Does it embed DWARF in PE?
and what about iron ?! as is seen in this [link](https://www.arewewebyet.org/topics/frameworks/#pkg-actix-web) iron have most download buy community ?! which framework and ORM are stable for my service ?! this service will grow up and I must develop it fast and secure with high responsibility. 
https://www.reddit.com/r/rust/comments/a5fi0k/seed_new_wasm_framework_for_webapps/?utm_source=reddit-android Brand new but looks promising. I thought yew looked good but found it difficult to actually use. The API for seed looks much simpler and closer to elm.
Really short and incomplete work imho.
This is great news for Redox! One of the difficulties in self hosting is cross compiling rustc, which is very time consuming and error prone. On the other hand, it is easy to cross compile GCC. If it is possible to set up a build as described in the article, it would help a lot.
mrustc is not the only consideration. imho it would be nice to be able to compile current rustc with distro-packaged rustc without needing to walk through long bootstrap chain.
There's another way to shorten it. Which is just cross-compiling the most recent compiler version from another platform, which should be supported via rust and LLVM. However there might be some reasons (e.g. trust) why someone doesn't want to go that route. 
Both async and support for rust stable coming with Rocket 0.5: https://rocket.rs/v0.4/news/2018-12-08-version-0.4/
I agree that better tooling is one of the most valuable things Rust could gain in 2019. But I think you understate the difficulty to getting there. What we have now is "decent", particularly given the complexity of the language. It is just that to be competitive with languages that have had countless millions of dollars of investment requires us to get far beyond that. What Rust needs is world class tooling.
&gt; It is currently an ongoing version and all contributions are welcome. It says it's ongoing. I think it's very cool that this is being pursued. I think many places using MISRA C would be completely unwilling to take on Rust without some sort of guidelines.
Iron is one of the first web frameworks and has a much weaker API then the newer alternatives. Also, it's not really developed anymore.
That sucks. It has such a good name.
By "skip", I mean "you wouldn't need to rewrite them", not that you couldn't make use of them.
Rust used to have green threads - they were removed because to make them work fluidly (the way they work in Go) requires a lot of runtime support, which Rust has evolved away from. We might still have async even with green threads, but it probably wouldn't be as necessary. The advantages to using OS threads is that you basically get them for free; they're part of the OS, so you might as well use them. Having both OS threads and async isn't so much a calculated "perfect" mix as it is the two technologies that are feasible (async is nice because it really doesn't require any special machinery to run besides maybe closures and something like epoll/select, but you don't technically even need that.)
As mrmonday notes, in the past we've had policies where AutoModerator would automatically filter posts from brand-new accounts, and though this did catch submissions like the one here, it also caught a huge number of legitimate posts, which we endured for quite a while until the recent surge in the growth of the subreddit caused it to simply be too much burden for the mods to bear. For now it's easier for us to manually remove these submissions rather than manually approve all the false positives.
Linux is how old computers benefit from good engineering.
Well, that's surprisingly mundane. I expected some compiler bootstrapping instructions with reproducible builds, some tooling based on abstract interpretation (which is mostly done in France), or guides to formal verification with SMACK. And all it actually says is "use rustup, avoid unsafe and fuzz your programs".
[Trust in rust](https://youtu.be/0MXMvYQ4j3A?t=154)
Excuse me sir, have you ever been to /r/rustjerk ?
The slides seem to be unavailable. Can you give us more context?
My guess is that they were bitten by issues with C/C++ at some point, possibly around glibc exposing different symbols based on version/architecture, and decided on this policy to avoid problems in the future.
Debian has an explicit policy against cross-compiling; they may not be the only one. I must admit I am curious as to why, I suppose they got bitten in the past.
They welcome your contribution. Looks like a starting point. 
I'm not an expert on this but from what I know mingw supported this for C/C++ for a very long time and rustc does the same when you are using mingw (i.e. gnu) target.
I am very glad to see that resvg is still being developed! As someone who has contributed to the elementary icon theme, I find it satisfying that it's used as a benchmark :) It would be interesting to have some kind of correctness information for it. I know elementary theme is designed in Inkscape and rendered with rsvg, so that would put resvg on par with those for at least one production use case. Also, I would like to see benchmarks for something other than icon themes. For one, e.g. the elementary theme uses only the most basic SVG features, so there is a lot of code that is not covered by the benchmark. Also, icon themes by definition have lots and lots of small files, which penalizes implementations with long startup times but very fast rendering.
Depends on your goal. I would stay clear of tower-web until it's stabilize. Rocket.rs is pretty neat. It's missing some features, but some of them are coming in 0.5. Pure web application with REST/CRUD is perfect for this. Actix-web is a bit more complex. Rather than web application is more suitable for making applications that talk http. If it makes sense. There are bunch of other sintra/exress style frameworks that are suitable for quick and easy *small* apis.
I believe this is already the case. If a distro packaged rustc already exists at a given stable version, it can be used to build the next stable version. Anything more involved is only needed to, well, bootstrap a new distro or new architecture.
No, post build scripts are a requested feature but there's no plan to implement them currently.
That's a bummer
Thank you both for your help! 
That’s obviously a default that can be configured differently.
I chose the elementary icon theme because it's not completely flat and simple, like let say Breeze. As for other icon themes - there is the Oxygen icon theme, which is by far the most heavy one, in terms of used SVG features. As for the icons size, it doesn't really matter, because it will boil down to the 2D graphics library performance. Yes, there are filters and stuff, but the only slow part there is Gaussian blur. Benchmarking is hard. For example, librsvg doesn't support proper clipPaths, which makes it a bit faster. Also, it uses multithreaded blur filter, which is faster, but uses more CPU. And since I'm converting icons using a single thread - librsvg has a benefit here.
Completely unrelated, but I love how nicely formatted your post is.
Benchmarking gets even harder when two different implementations are bottlenecked by different resources, e.g. one uses more CPU and the other uses more memory bandwidth, and you get one or the other being faster by a margin on different hardware configurations :P Also, I'd be interested in some real-world correctness information. Feature tests are all good and fun, but do not tell me much as an app developer who's not deeply familiar with SVG whether I can use it instead of rsvg or not. Since elementary icon theme already renders correctly to the human eye on both Inkscape and rsvg, you could measure visual difference between these two implementations and then check how much resvg differs from them without looking through thousands of images manually.
Thank you! I like to keep things organized. 
Since resvg has a no-panics guarantee, I wanted to fuzz it for quite a while, but I keep getting sidetracked by other projects that are even more promising. According to [Choronzon presentation](https://census-labs.com/media/choronzon-zeronights-2015.pdf) the binary mutation strategies used in current feedback-driven fuzzers are not particularly effective for discovering XML parser bugs, let alone SVG parser bugs. Choronzon itself was eventually open-sourced, but its XML mutator was not. It is described in the presentation, though. The more mature honggfuzz, libfuzzer, and Mozilla's fork of AFL all support custom mutators, so I believe whoever actually writes one will be able to discover a bunch of CVEs in parsers for all sorts of XML-based formats, including SVG parsers.
DWARF is just specifically a debugging format that is entirely unrelated to any particular executable format.
A lot of governmental work, from what I understand, is "mundane" like this. :) They want something boring and workable, and I'm glad that somebody is making efforts towards making a "boring" procedure out of using Rust for this context.
This crate relies on the internal representation of a trait object "fat pointer", and would break if the representation changes in the future. I think the crate should have a warning about this.
I learned about yew today. From what I understood from a short introductory article, much of the things aimed for here are already demoed by yew? Would someone like to elaborate?
The release tarballs have vendored crates, and you need to provide a PATH for the stage0 toolchain. ./configure --enable-vendor --local-rust-root PATH
Use `pub mod` in your main files to avoid the warning. &amp;#x200B; [Simplified example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=436eb7a986d32e5ad6f83432b26e8ca2)
Yew is built on stdweb, rather than wasm-bindgen (at least for now, it seems [stdweb might port its js ffi to wasm-bindgen at some point in the future](https://github.com/rustwasm/team/issues/226). This means it is serializing and deserializing structures back and forth over the wasm &lt;--&gt; js abi boundary. On the other hand, wasm-bindgen was built from the ground up to be a zero-cost bridge between wasm and js, where only cheap handles (pointers at the ABI level) are passed back and forth. This is future compatible with `anyref` and the [host bindings proposal](https://github.com/WebAssembly/host-bindings/blob/master/proposals/host-bindings/Overview.md) which will eventually unlock even-faster-than-JavaScript dom method access because calls can be statically validated once at instantiation time, rather than dynamically on every call. We want the whole ecosystem to be able to leverage these benefits, so we would like to invest in libraries built on wasm-bindgen. That said, I wanted to point out this text from the article as well, just in case you missed it: &gt; Some of the aforementioned Web APIs are already wrapped up into high-level APIs in crates that already exist. However, few of the extant crates fulfill all of our requirements. Most commonly they are lacking modularity: we’ve seen more “frameworks” than single-purpose libraries collected into “toolkits”. Nonetheless, we should collaborate to improve existing crates and tease them apart into small, single-purpose libraries where it makes sense and everyone is on board.
Well that seems to be how it's done, thank you! I'll forget about \[\[bin\]\].
Ah, thanks for finding that out.
Very true. I’ll note it in the post and in the crate description when I get a chance.
On the off chance someone sees this and had the same thoughts and aspiration I did check out [cargo make](https://github.com/sagiegurari/cargo-make). It is a pretty neat build tool for rust. I sincerely hope that cargo gets support for post build scripts but until then this gets the job done.
&gt; `type Foo = impl Debug;` That would be defining a normal associated type with an existential default, which probably wouldn't work very well.
Debian has to keep in mind that some software they support will lose its maintainers, and at some point will drop support for some architecture. If at that point the compilation is dependent on some old architecture, that would suck. Of course, I don't think that's very realistic for Rust. But I think that's the sort of problem Debian is thinking of.
Technically, the way you should be doing this is transmuting to [`std::raw::TraitObject`](https://github.com/rust-lang/rust/issues/27751), but that's not stable precisely because the representation may change in the future. Actually, it looks like there's an [RFC proposing an API that could be stabilized](https://github.com/SimonSapin/rfcs/blob/ptr-meta/text/0000-ptr-meta.md), which is motivated by exactly your use case; it even contains a link to the playground with a similar data structure.
I'd like to see a high-level fetch-api crate based on wasm-bindgen/web-sys. Working on one, but having some trouble.
Is this something standard/blessed? I really like #\[bench\] and using a simple \`cargo bench\` to benchmark. I don't want src/bin entries just to benchmark.
&gt; I'd like to see a high-level fetch-api crate based on wasm-bindgen/web-sys. Yes, this is exactly the kind of thing I'm talking about. &gt; having trouble due to not understanding the distinction between JS-sys promises, Rust futures, and wasm-bindgen futures... Have you seen https://rustwasm.github.io/wasm-bindgen/api/wasm_bindgen_futures/#example-usage ? I'm not sure how on top of futures in rust and promises in JS you are, so I'll try to just give a quick overview from the top an then you can ask more specific questions if anything isn't clear. The `Future` trait in Rust is not a specific type, rather it is a trait that describes anything that is a placeholder for a value that could become available at some point in time in the future. Many different concrete Rust types implement the `Future` trait, eg there are concrete types that trivially implement `Future` by [always being ready](https://docs.rs/futures/0.1.25/futures/future/struct.FutureResult.html) and there are more complicated concrete types that represent [some sort of I/O or side effect having happened](https://docs.rs/futures-timer/0.1.1/futures_timer/struct.Timer.html). JavaScript promises exist outside of the `Future` ecosystem since they come from JS. Therefore, if we want to bridge them into the `Future` ecosystem, we need some sort of shim. This is what `wasm-bindgen-futures` provides: a concrete type that implements `Future` and is backed by a JavaScript promise. Does that makes sense?
I guess they haven’t got speakers slides yet. I will update you if I get them
There's no way your idea would ever work in real life though. The fact that you're hugely overestimating the current capabilities of Wasm is just the tip of the iceberg.
Isn't `x.py` and `config.toml` a better way to go when building the compiler? I've never had any issues building that way as the whole Python/Rust bootstrap system seems to have all the logic properly hardcoded for downloading things and whatnot.
What's the use of having GC in Rust? In what way would programming become easier by having GC on top of what rust already has in terms of memory management.
While a mix of RAII and reference-counters solves most memory-management problems, a few cases are still better handled with a GC, like a long-lived cyclic graph manipulation system. In this case, neither simple-owner like `Box`, reference-counters like `Arc`, or slab/arena-based allocation systems are appropriate.
I hacked on `stdweb` and used `yew` to build a full featured webapp a few months ago, but have been on haitus for a while. This is really good context to see how things are moving! I'm super excited to rewrite my frontend (again) in a toolkit-style rust frontend :D
[Answered in the thread](https://twitter.com/SenojEkul/status/1073698807886430208)
In addition to the applications in data structures mentioned by /u/Gyscos, there's also a lot of GC-ed languages that Rust integrates with- Javascript in Firefox and Servo is a big one, Lua is the one linked in the Twitter thread, Ruby has some production use, etc. Being able to talk about objects from those languages safely is useful.
The thing is though: things like stdlib and very essential crates are likely to be used by people from *every* domain, including medicine, aerospace etc.
I see this as the most sensible option going forward as well. The bootstraping compiler doesn't need to be something fancy. It doesn't even need to borrow check or emit code that is byte-identical to rustc (an mrustc goal if I remember correctly), all it needs to do is get a full rustc on a target platform so that it can compile itself.
`configure` just generates a `config.toml`, and if you use the `Makefile` it just invokes `x.py`. There's no difference in how downloads and such work, and if you need to be able to build offline, you can set the options I mentioned either way.
Graphs don't need to be represented as graphs in memory. You can store nodes and edges separately.
The point still remains: how do you know when a node can be freed to make room for a new one?
This is an anti-pattern. People need to stop trying to insert their poor programming practices from other languages into rust.
Maybe I'm missing something but what is the point? mrustc is no more guaranteed to be "correct" than rustc itself is to be "correct". Having additional other language compilers for rust is fine but its more of a scientific experiment than anything useful.
&amp;#x200B; &gt;but also compile time Is the compile time that much worse if the code is part of `std` vs a [crates.io](https://crates.io) dependency? The biggest takeaway for me with having 240 dependencies is that building a web service has an *iceberg* like complexity. It seems simple but there is a lot more to it. I would hate to lose all of the innovation we have seen in the community because we want to jam a "blessed" implementation into `std`. &amp;#x200B;
I don't think GC will ever be intended to replace Rust's memory management. It could be useful for inter-op with GC'd languages, or to implement your own GC'd language entirely using Rust.
GC is not a poor programming practice.
Will there be a `std::Executor` coming too? Or would the only way to use async/await be to also use Tokio or another runtime?
Basically, MPL2 has similarities to the LGPL, but on a per-source-file level. You have to make available any MPLed source code files on request, but everything else, you may license however you wish. Unlike the LGPL, there's no requirement that people be able to build a replacement for the covered code and swap it in, because that's not a feasible thing to require with per-source-file granularity. (eg. By the time an MPLed C compiler sees a `.c` file, the `#include`s have already merged any non-MPLed `.h` files into it, so it lacks the requisite information to draw such boundary.)
I've always wanted async await in rust stable without needing crates .. :/
You probably need str, not &amp;str, but this is just a wild guess.
Sadly: ``` error[E0277]: the size for values of type `str` cannot be known at compilation time --&gt; examples\mgba_panic_handler.rs:80:45 | 80 | if let Some(payload) = info.payload().downcast_ref::&lt;str&gt;() { | ^^^^^^^^^^^^ doesn't have a size known at compile-time | = help: the trait `core::marker::Sized` is not implemented for `str` = note: to learn more, visit &lt;https://doc.rust-lang.org/book/second-edition/ch19-04-advanced-types.html#dynamically-sized-types-and-the-sized-trait&gt; ```
&gt; Maybe I'm missing something but what is the point? It's not about correctness. It's about proving yourself free of the attack detailed in [Reflections on Trusting Trust](https://www.archive.ece.cmu.edu/~ganger/712.fall02/papers/p761-thompson.pdf). If you don't re-bootstrap Rust, your bootstrap compiler could have a self-propagating backdoor in it. Truly proper bootstrapping Rust actually involves this process: 1. Bootstrap off a C++ compiler by using `mrustc` to compile a `rustc` bootstrap compiler. 2. Use the bootstrap compiler to rebuild `rustc`, so it can be compiled according to how *rustc* thinks the code should map to machine instructions. 3. Verify that the build is stable and reproducible by using the rebuilt `rustc` to compile `rustc` one more time and then checking to see if the re-built and re-re-built binaries match. The reason mrustc helps with this process is: 1. We know how to read the rustc source to make sure it's free from backdoors 2. We know how to read the mrustc source to make sure it's free from backdoors 3. We have other techniques for ruling out the presence of Trusting Trust backdoors in compiled C and C++ code. Things C++ has which Rust doesn't: 1. Many different compiler implementations, which makes it exponentially more difficult for a Trusting Trust backdoor to survive undetected when bootstrapping compiler X from compiler Y. 2. Not many hops required to bootstrap modern GCC from the C-only codebase of 2010... further expanding the circle of compilers which a backdoor must successfully pass through without getting caught. 3. A significantly longer history of people actively checking for such backdoors. It ties in with [reproducible builds](https://en.wikipedia.org/wiki/Reproducible_builds) by providing confidence that you can generate a trustworthy hash for others to compare against.
really thanks, then I think I must use actix, Which frameworks have a better prospect for the future? our service it will be long-term :) actix is good choice ?! 
yes, thank you 
yeah, `&amp;str` is correct, but it's strange that it doesn't work for you: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=69f20c4e5bbe3cd66e4d703111829160 As for your question of how "reliable" it is... I mean, it's either 100% reliable or 0% reliable. There's no way that it works 25% of the time or anything.
Depends on how broken the hardware is.
One of the conclusions of the trusting trust paper is that you can never be free of it. Doing so helps mitigate the issue, but it’s never solvable.
"Reliable" was just a catchy but perhaps a hyperbolic way to put it :P But yeah, that's very strange.
I don't want GC in rust, especially because I don't want to depend on crates that were made without an understanding of memory allocation and its cost. There are well paid java developers writing "beautiful" idiomatic code that is actually garbage when you look at how much unnecessary work is being done at the hardware level.
Runtime GC makes no sense in rust. so in this case it would be poor practice. There are already reference counted objects and other abstractions that can be applied as needed - I don't see a need for runtime GC'd allocations.
Yes it is, because as part of std it is already compiled, while as cargo.io dependency, the more you have the longer it takes, given that they are compiled from scratch.
I created actix, I use it at work. It part of my day to day job. It should be fine long term
so, there are cases where a certain type of GC ends up being an optimal algorithm. By all means, make those things available, explicit choices - just not the default in this language.
There’s nothing built into rust that’s stable.
This is something people are split about. Many people do put them on both.
Thanks should have thought of it, appreciate the teams quick response. Really liking the language! 
What are some examples where GC really shines?
In `graph.remove_node(nodeid)`
"Rule 4: Pawn attacks diagonally" How exactly this is relevant to GC?
Seeing a bunch of comments asking why you would need a gc in a language like rust, and it’s a fair question. Other people have mentioned graph structures and gc’d language interop, but I wanted to provide some more detail. I need a gc in rust because I’m writing a language runtime with a gc. I’ve spent a lot of time designing it so that it is a small amount of unsafe code to manage the gc and everything else can remain safe, without any or a bare minimum loss of performance. For example, one of my design limitations is that gc pointer types are Copy and machine word size, aka implemented as a genuine plain pointer. I’m doing this as an experiment to see whether implementing a “real” gc’d language runtime is feasible in safe code with competitive performance. Experiment is not complete yet but initial results are promising. I’m definitely not focusing on trying to add a gc to rust wholesale, or even add a globally available shared gc, my use cases are isolated gc containers that contain a single graph structure, in the case of a Lua runtime this graph structure is complex :) Mostly I’m just doing this because I must have one to implement Lua, but I do have an eye towards that use case of a single isolated gc sandbox. Another reason for spending so much time on a safe gc api, and really this was the other half of the reason for this project, was to provide a scripting api with “zero overhead”. I want to make a heavily mod driven user scripting game... thing, and I have a lot of experience with the pain points of trying to use something like PUC Rio Lua, and there are many things that the API makes very difficult or impossible. (I don’t strictly NEED I make an entire scripting engine to work on a game idea, this is kind of just an excuse?) Anyway, afaict the major reason that scripting API boundaries are so gnarly and slow is actually so the user does not have to touch garbage collected pointers... it’s just too hard to be safe, you have to worry about write barriers and rooting constantly. If there were a theoretical magic API that was impossible to misuse AND safe AND fast, then your external API and your internal one could be the same. Rewriting a Lua function in Rust would always be faster, rather than having an awful constant time overhead on the Lua FFI. The project is super early though, I’ll go through it in more detail when it gets to a more super minimal MVP state. I’m happy to answer any questions though if you want to talk about why gc api design in rust is so hard :D
https://twitter.com/ManishEarth/status/1073701247029399552
Everything you said sounds so cool.
https://www.reddit.com/r/rust/comments/a69zh3/usefulness_of_asyncawait_for_gc_in_rust/ebtbjsc/
A sane module system would be nice.
&gt; When discussing other programming languages, maintain civility and respectfulness. 
but if nodes make a cycle you never free their memory even though they're no longer reachable from the root, thus not able to be found &amp;#x200B; A GC actually makes sure you don't actually have a reference to this cycle and makes that space available.
I've got no problem with lifetimes, that was simple to grasp and I'm running smooth. I'm dumbfounded by how to fight the module system. How does the directory hierarchy in a rust project relate to what we have to type to get access to items in submodules? The documentation is woefully inadequate showing multiple modules and submodules in a single file, which let's face it, pretty much never actually occurs in reality. I only just got the hang of it in 2015, but now 2018 has changed and I tearing my hair out again.
So it is not a technical thing, if I want, I can always skip the struct?
...and now all the nodes that referred to `nodeid` make no sense.
It’s coming! Gonna be a few months, but the end is in sight.
That's awesome!
Take a look what [std does](https://github.com/rust-lang/rust/blob/1897657ef09eea3d0b0afbbbd154a12fbb8fbf3f/src/libstd/panicking.rs#L189-L195): It checks for `&amp;'static str` and `String`: let msg = match info.payload().downcast_ref::&lt;&amp;'static str&gt;() { Some(s) =&gt; *s, None =&gt; match info.payload().downcast_ref::&lt;String&gt;() { Some(s) =&gt; &amp;s[..], None =&gt; "Box&lt;Any&gt;", } };
I made a blog using Rocket, and am currently using it on my website to share my notes and code snippets on Rust and Rocket. I had a little difficulty getting into Rocket but once I got it I completely loved it. I looked at other people's Rocket projects, it helped me a lot. Rust and Rocket stuff https://github.com/vishusandy/blogr Code for my blog: https://github.com/vishusandy/blogr Hope that helps
If you skip the struct, then you can create instances of the struct where those methods don’t exist. If that’s right or wrong to you just depends.
The only way to truly bootstrap rust is to first invent the computer. As the linked paper points out, such pathological trusting problems can go down all the way to the hardware. The C++ compilers could all be affected. Their assemblers? Your operating system itself? Their common ancestors? &gt; further expanding the circle of compilers which a backdoor must successfully pass through without getting caught. ---- &gt; A significantly longer history of people actively checking for such backdoors. Isnt the point of the backdoors described that they *can't* be caught? No source evidence, unless you want to analyse every bit of assembly i guess. If your assembler doesn't have a backdoor, that is. Or their common ancestor compilers?
&gt; Does that makes sense? It's one of the clearest descriptions of the `Future` trait I've seen.
The idea is to expand the circle of trustworthiness, hopefully eventually making a backdoor prohibitively difficult. The Intel ME and AMD PSP are a significant setback on that front, unfortunately. As for countering Trusting Trust, there's a technique known as "diverse double-compiling" Bruce Schneier explained it in a [blog post](https://www.schneier.com/blog/archives/2006/01/countering_trus.html) back in 2006 but the gist is that a Trusting Trust backdoor isn't an A.I., so there's a limit to what it can recognize as a compiler and propagate itself into. By leveraging the wide variety of C and C++ compilers out there, you can trick the exploit into revealing by its presence or absence in two builds that are supposed to be identical.
This subreddit is for Rust the programming language, not Rust the game.
&gt; As for your question of how "reliable" it is... I mean, it's either 100% reliable or 0% reliable. There's no way that it works 25% of the time or anything. It works 1/2^64 of the time. If there is a hash collision, `downcast_ref` will have a false positive. 
Sure but as you can see on the playground link this is a no_std build so there's no confusion that it might be a String instead.
True, but that's science for you. You can never prove a negative. (Used to humorous effect in Freeman's Mind when he says "Are they gone? How can I know? Science is no help here. You can never prove the absence of ninjas... only their direct presence.") The whole point of diverse double-compiling is to establish the probable absence of a Trusting Trust backdoor to a high degree of confidence.
I work on a browser engine. Browsers run javascript. Javascript has a GC. I have to deal with that from Rust. The other person I talk of in the thread is implementing a Lua runtime. Again, you kinda need a GC for that. GC in Rust absolutely has valid use cases.
Nobody's going to use GC in Rust that way. You need GC in rust to implement or work with managed runtimes. I work on a browser engine that needs to interface with JS. The other person I was having this discussion with is implementing a Lua runtime. These need a GC because the languages they work with have a GC.
Adding to /u/kyrenn's excellent description of her use case for a GC in Rust, I'll try to explain my motivation for a GC in Rust. In Servo, we need to implement DOM APIs, which operate on Javascript objects. These are all GCd, and to be able to do this well we need Rust to work safely with this. Currently it cannot, the options available are either doing a whole bunch of global locking of the GC (unacceptable for the use case), or using a bunch of imperfect compiler lints to enforce safety. We do the latter. Using pin and async/await is one better option for this. Another use case is graph-like datastructures. Sprinkling a _little bit_ of GC in there can be beneficial. In some cases you can use crates like `petgraph`, but you still need to implement collection on your own there if you want to deallocate, and you may have a performance hit from all the checked indexing. Concurrent GCs are a whole new world of complexity but if you can design one like that you can have cool things like persistent datastructures, which can be more efficient than the alternative! GC doesn't imply "slow", especially if it's being used for a small set of things. Nobody's asking for _pervasive_ GC in rust. But having a safe `Gc&lt;T&gt;` type that you can use in a couple places, and having safe ways to interface with other GC'd runtimes are both desirable things. I've written about all this before, see https://manishearth.github.io/blog/2015/09/01/designing-a-gc-in-rust/#motivation (also https://manishearth.github.io/blog/2016/08/18/gc-support-in-rust-api-design/ ) 
If you can say more about what's confusing, that'd be very helpful. The largest change in 2018 is that you don't need `mod.rs`, that is, `foo/mod.rs` can just be `foo.rs`.
&gt; why is async/await such an often requested feature? https://aturon.github.io/2018/04/24/async-borrowing/ is one of the best descriptions I've ever seen.
Why? Im asking for things ingame.. 
kk ty im gonna delete this then &amp;#x200B;
I wholeheartedly agree and the same principle applies to non-libraries as well... they just tend to get it right more often. * For a GUI, every project should have at least one prominent screenshot... ideally with links to a gallery of them. (The most common mistake I've seen is people thinking that waiting through a demo video or wandering aimlessly through a demo instance of a web app is an acceptable substitute.) * For a command-line application, I'm surprised how often people think "there's nothing to screenshot" rather than showing the `--help` output and/or what it looks like when you run the tool. (Whatever mix does the best job of quickly giving an overview of what it's like to use it and what it's capable of.)
It's actually been at that stage for a few months now. Still working on different architectures, and fiddling with re-targeting to 1.29 instead of 1.19
&gt; cheap (criminals) I would think it's easier to protect the distribution source in that case, hopefully not just anyone can distribute a binary thats then used to compile all future versions that get distributed. &gt; wide-ranging (three-letter agencies) back-dooring. And in this case I would assume they have the resources to get a backdoor in anyway, if they really wanted. Probably through the CPU management software. Or just a $5 wrench[.](https://www.xkcd.com/538/)
It means that g++ is used to build mrustc, which is then used to build rustc (... mrustc i kinda like a gcc frontend, except that it emits C code and not GIMPLE)
They grow up so fast. Makes me want to try cleaning up `dumb-exec`to hopefully get it into a position where it can be used by `no_std`. The fact that async/await itself isn't `no_std`-compatible is a bit unfortunate.
I saw that comment, but I guess I just didn't understand it. Would a good example be something like a GPS backend keeping track of nodes in a road net?
You can build your own vtables, I wish I had some code on hand, but it's more or less that `&amp;MyVtable { fn1, fn2, ... }` (a struct full of function pointers) will be `&amp;'static MyVtable`. Not compatible with Rust's vtables but totally usable (manually) for many purposes.
Traits in Rust are super similar to Typeclasses in Haskell, and typeclass calls are basically implemented identically with the exception that Haskell doesn't support unboxed types (or at least not unboxed polymorphic types) -- so a function `Show a =&gt; a -&gt; String` takes a vtable pointer and an object pointer argument, just as a Rust function that takes `&amp;Printable` actually gets 2 pointers as arguments. However, existential types allow embedding this data directly into data structures if it would be convenient: data TraitObjectShow = forall a. Show a =&gt; MkShowable a -- MkShowable :: Show a =&gt; a -&gt; TraitObjectShow -- basically identical to Box&lt;dyn Printable&gt; I wonder if supporting existentials would be useful for Rust. You might be able to simplify some of the craziness around `impl trait`. It certainly could allow implementation of this feature. Who knows what else?
I'm not sure what the elegant fix is but downcast_ref is a fancy wrapper around transmuting equal TypeId's . https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2015&amp;gist=b8f76c46d7fafebd6f72c78bdf3732bf 
std has an explicit static lifetime. Maybe this is the critical difference: the hash of static and inferred is different?
Hmm, using the playground that another person posted to check type id values, and adding a line for static string, it seems to be the same https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2015&amp;gist=2fb948ff5d809358e7ff804c8a37adb4
All of this is correct, but i disagree with the conclusion. The last time i wanted to use the MSVC toolchain it was a pain to setup a minimum version without all of visual studio. So unless you have all of that already installed. Most people are probably best off just using the GNU toolchain , until you need the MSVC toolchain.
The confusion is the difference between "can" and "need". Do we need a "foo.rs" to import all the "foo/a.rs" and "foo/b.rs" or do only need it if we want to define at foo-level? So, let's say I have a directory like: main.rs a.rs b.rs foo/c.rs foo/d.rs bar/e.rs bar/g.rs and I want to say, use g from within d, or g within a, or a withing e, what do I need to type where?
You still need something to define the foo module. Given that it was in a mod.rs before, it’d be in a foo.rs now. 
I understood some of those words. Hope this develops further into another talk, Catherine! The closing keynote about ECS in Rust is a personal Rust favourite :)
How would I best get involved in these initiatives, to help make this happen? I am new to Rust, and I am here for the exact reasons that wasm-bindgen exists. 
Ok, thanks, that helps. I still had to do a major refactoring to add "crate::" in all my "use"s. I can't understand what the Rust devs were smoking when they decided this was the best way to do things. Python and Java got it right. If you have such a directory structure as I showed above, then you shouldn't need any "pub mod" module definitions/re-exports, since the directory itself is the module definition/re-export.
&gt; you could measure visual difference between these two implementations and then check how much resvg differs from them without looking through thousands of images manually. That would probably be a good starting point. Not sure I understand you. Why I should do this?
I've tried AFL, but there are no results. I plan to write a simple SVG-specific "fuzzer".
Re-using known and optimized algorithms of GC for graph partitioning detection is a good way to solve this problem, but actually, you don't really need a GC, you need graph partition checks and apply a policy on it (dealloc entire disconnected graphs from root handle). The real question left is to know if a GC implementation working at the allocation graph level in rust would be faster (using the ton of knowledge on this problem) than a pure graph management API working at an higher level.
It will be useful for tree structures. 
So when do you reclaim that memory? You still need the same strategy of async running your algorithm.
I don't really get what is the goal of this project. Can someone explain?
This is a very good point, first impressions are a huge deal, and simple, accessible examples are one of the best ways of making a good one.
What makes you think that Rocket will adopt stable soon? It took a year for a single subversion release. Release notes targeting stable in next release doesn't tell you when that will happen.
One thing I hope for 2019 is that these rust+wasm blog posts stop ignoring that stdweb and cargo-web exist. It's a much nicer ecosystem in my opinion than wasm-bindgen, and yet these posts I've seen recently never seem to mention them. 
!RemindMe 365 days 
I will be messaging you on [**2019-12-15 11:52:44 UTC**](http://www.wolframalpha.com/input/?i=2019-12-15 11:52:44 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/a63ulw/which_web_framework_in_rust/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/a63ulw/which_web_framework_in_rust/]%0A%0ARemindMe! 365 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I distinctly recall some OpenGL implementations that passed conformance tests but utterly failed to render some real-world workloads. And that's a trend with many open standards. If you could advertise on resvg website that it not only passes X number of test but also handles such and such real-world workloads, it might help adoption.
Yeah, that's kind of expected. AFL doesn't work well for XML-based formats as-is. You might get some quick wins with [Radamsa](https://gitlab.com/akihe/radamsa), but I also wouldn't count on it: that thing is kind of format-aware, but unlike AFL it's not feedback-driven. AFAIK the most promising strategy is plugging a custom mutator into one of the feedback-driven fuzzers, such as honggfuzz, libfuzzer or Mozilla's fork of AFL.
This makes sense to me. At work we use Lua VMs as isolation containers for Lua components and drivers which may fail or misbehave and need to be restarted (safely, with no memory loss, etc). I've built up a whole monitoring system around them, as well as support for actor-like cross-VM call APIs. Lua's per-VM GC seems to work well for this. For one thing the GC overheads are spread about (since each VM can GC independently), and on the other, each VM can clean up nicely if restarted. So having separate GC pools, one per VM, is ideal for this. (Extreme performance isn't a goal, though -- if it was, then Pony's approach of lending memory between actors would help avoid copying.) I'd be interested in following your work on this. Is there some blog or repository I could watch?
Wouldn't using unsafe be faster for b+ trees for example (not saying that using unsafe is a better solution)
Any thoughts on whether this would be a global GC, or with many independent GC pools? Pony does ref-counted lending of data between pools to aid sharing (and avoid copying).
I am new to programming. I decided to try my hand at Rust because it is the foundation for a crypto project I am really into. Of all the Hello World's I've done so far (python, ruby, and Rust, and maybe one more) Rust by far was my favorite. The online handbook/guide is awesome AND entertaining. The code is clean, cool, straightforward and solid, imo. Now if I could only stop talking myself out of practicing because "it's too late" anyway (I'm 35 :/ )
Didn't the rust home page used to have an example? A match statement IIRC
Do note that the Rust front page used to be a lot better until ~ a week ago; the community is in a bit of uproar about this and many people hope to improve the main page again, with, amongst other things, some very simple code examples. 
One of the possible alternatives to the memory representation of a trait which requires that `std::raw::TraitObject` be unstable is for "multi-traits": `&amp;(Add + Sub)` could be represented as `(v-ptr to Add, v-ptr to Sub, ptr to data)` in which case there are two v-ptrs.
The reason is already explained on this very page, [right here](https://www.reddit.com/r/rust/comments/a69hwf/rust_and_webassembly_in_2019/ebt3kdl): &gt; Yew is built on `stdweb`, rather than `wasm-bindgen` (at least for now, it seems `stdweb` might port its js ffi to `wasm-bindgen` at some point in the future). This means it is serializing and deserializing structures back and forth over the wasm &lt;--&gt; js abi boundary. On the other hand, wasm-bindgen was built from the ground up to be a zero-cost bridge between wasm and js, where only cheap handles (pointers at the ABI level) are passed back and forth. [...] As well as in the article itself: &gt; Some of the aforementioned Web APIs are already wrapped up into high-level APIs in crates that already exist. However, few of the extant crates fulfill all of our requirements. Most commonly they are lacking modularity: we’ve seen more “frameworks” than single-purpose libraries collected into “toolkits”. Nonetheless, we should collaborate to improve existing crates and tease them apart into small, single-purpose libraries where it makes sense and everyone is on board. In short, there are two issues with `stdweb`: 1. It's a framework, whereas this is a call for a *toolkit*. The main difference being that with a toolkit you can cherry-pick a handful of pieces that suit your needs to get as minimal a footprint as possible, whereas with a framework you ask for a banana and you also pull in a gorilla holding onto that banana and the whole jungle. As mentioned in the article, the goal would be to pull out pieces of existing frameworks into independent libraries and let the frameworks sit on top. 2. It currently is inefficient compared to `wasm-bindgen`, serializing back and forth. None of them are unsolvable, of course, but whether starting from `stdweb` and adapting it to the vision laid out or starting from the ground up is best/easiest is unclear. Also, being such a young field, there's no reason not to allow room for experiment.
&gt; tooling based on abstract interpretation (which is mostly done in France) That's usually done by a research institute like INRIA, not the &gt; Government's Cybersecurity department
Yes; so I am a bit perplexed by /u/razrfalcon's answer :) He may have meant graphs; while you can use `unsafe` to implement a doubly-linked list with a reasonable chance of having a sound implementation, for arbitrary graphs things get really complicated. And there's of course the whole issue of lock-free/wait-free concurrent data-structures. Epochs and Hazard Pointers are cool and all, but still error-prone and it's really easy to accidentally have a single thread being able to completely block reclamation... in GC'ed languages, this is much easier.
No worries, nobody wants a global GC for all of Rust memory! What we are talking about here is surgical use of a GC in various circumstances: binding to GC'ed languages, complex graph structures, lock-free concurrent data-structures, ... each with their own GC.
I feel the proper solution to that problem would be some kind of cache for compiled artifacts instead of pulling more code into the stdlib
You can watch the repo for the Lua interpreter that I’m using to test all this, it’s [here](github.com/kyren/luster). It’s still early days though as far as actually interpreting Lua. It should move faster though now that I *think* the most important api questions have been answered.
&gt; This looks pretty funky to me. The slice ought to become a no-op and serde can take in a slice of bytes. Also serde should be able to take the file directly and generate an iterator of JSON entries, though that may or may not be faster (should be checked). Either way, /u/dtolnay's [PR on /u/bcantrill's statemap](https://github.com/joyent/statemap/pull/40) can probably be used as a guideline.
Awesome! Here is a bit of info: https://github.com/rustwasm/team/blob/master/README.md#get-involved The goals section needs to be updated for 2019 (after we adopt a roadmap) and also we've paused WG meetings (which are usually every Thursday at 17:00 UTC) so things will be fairly quiet until 2019. I recommend joining our WG meetings if you can slot it into your schedule. Also, assuming we end up pursuing the loosely coupled toolkit and libraries further, there will be lots and lots of room for folks to step up and be leads on particular libraries and components!
Interesting stuff :-) An implementation question: pub struct DynStack&lt;T: ?Sized&gt; { offs_table: Vec&lt;(usize, usize)&gt;, dyn_data: *mut u8, dyn_size: usize, dyn_cap: usize, _spooky: PhantomData&lt;T&gt;, } Any reason you're not just having a `Vec&lt;u8&gt;` instead of reimplementing parts of Vec's functionality for the data storage part?
Unsafe is hard and dangerous. PS: I'm talking about DOM-like tree structures, that has self-recursive nodes.
It sure did: [https://prev.rust-lang.org/en-US/](https://prev.rust-lang.org/en-US/)
&gt; but utterly failed to render some real-world workloads What do you mean by that? If you are talking about anti-aliasing and stuff than it's out of scope for `resvg`, since it doesn't render anything by itself.
Sadly, I'm not familiar with the fuzzing theory. It's a black box for me.
&gt;Now if I could only stop talking myself out of practicing because "it's too late" anyway (I'm 35 :/ ) It's only "too late" when you're dead. Seems like you're alive and kicking to me!
No there isn't. But there are threads and channels: https://doc.rust-lang.org/book/ch16-01-threads.html
[Yes!](https://deterministic.space/readme-driven-development.html)
While the concept of "goroutines" is unique to Go, they represent a feature of many languages known as [green threads](https://en.m.wikipedia.org/wiki/Green_threads). Once upon a time in pre 1.0, Rust natively implemented green threads. It was decided however that green threads was somewhat antithetical to Rust's philosophy, so they were dropped in favour of native threads.
&gt; I would think it's easier to protect the distribution source in that case, hopefully not just anyone can distribute a binary thats then used to compile all future versions that get distributed. That's what all this trusting trust prevention is about. Verifying that nobody got access of that sort in the past and setting up procedures to prevent it from going uncaught in the future. &gt; And in this case I would assume they have the resources to get a backdoor in anyway, if they really wanted. Probably through the CPU management software. Or just a $5 wrench. The point is that Intel and AMD have enough money to make things difficult for TLAs if they go the $5 wrench route to gain access to the Management Engine signing keys, and it costs time and money to do it with their cooperation on a scale beyond what you'd normally get a warrant for. It's not about preventing them from doing it. It's safe to assume that's impossible. It's about making it harder, so they're less likely to do it on what passes for a whim with them.
There is [osaka](https://github.com/aep/osaka) by /u/arvidep, albeit still somewhat experimental. Related tweets: https://twitter.com/arvidep/status/1067169156229984256 https://twitter.com/arvidep/status/1067383652206690307
Goroutines are in a sense very lightweight threads ([green threads](https://en.m.wikipedia.org/wiki/Green_threads)) while a a thread in the standard libraries of Rust is a real OS [thread](https://en.m.wikipedia.org/wiki/Thread_(computing)). Correct me if I'm wrong, but you could use [`rayon`](https://crates.io/crates/rayon) to spawn closures onto a thread pool which will be more like a goroutine.
Alright, given that my original suggestion didn't work, I took a look at the `libcore`sources. TL;DR: Use the `Display` implementation on `PanicInfo`. `panic!()` in `libcore` calls `panic_fmt(...)` in [`libcore/panicking.rs`](https://github.com/rust-lang/rust/blob/master/src/libcore/panicking.rs). This, in turn, calls `PanicInfo::internal_constructor(...)` in [`libcore/panick.rs`](https://github.com/rust-lang/rust/blob/master/src/libcore/panic.rs), which uses a private, empty `struct` as the payload. The actual message used in the panic is available as `PanicInfo::message()`, however that is currently unstable, so unless you are on nightly, you'll need to go through the `Display` implementation.
Is there a roadmap for stabilizing benchmarks? They're so incredibly cool and useful
Nope. There hasn’t been a ton of demand. If you want to do that work, you should! They really need a champion.
I managed to write a little test CLI program that ran flawless under Windows XP, but that's the lowest I have ever tried.
As I understand it, it is a bit like println!("{}", x) vs println!("{x}", x=x). Both work today, but you wouldn't want to use the explicit version all the time right?
You give linux users a bad name (including me). Has the year of linux on desktop arrived yet? 
What would it look like as code? Just `yield`/`await` something while the GC runs?
(Stop me if this is already clear.) One of the important features of goroutines is that, because of their prominent place in the language, all the Golang IO libraries know about them at a low level. So e.g. making a regular network call on a goroutine will free the underlying thread to do other work while the goroutine sleeps. That's not the case with Rayon, since IO facilities in the Rust stdlib do tend to block the thread, and do doing a lot of IO will exhaust the thread pool.
In case of OpenGL games would render incorrectly to the point of being unplayable, or just crash, even though the OpenGL implementation was officially conformant. So results of tests did not mean that you could actually use the OpenGL implementation in practice. It's a recurring theme with open standards designed by committee in general. So it may be a concern for people looking to adopt resvg. So having some information on the website to say that resvg not only passes tests but also renders there and these real-world SVGs the way they were designed to look (barring differences in anti-aliasing, which are pretty much expected) may help adoption.
I was considering mentioning this, hence the "more like a goroutine". Thank you for pointing it out non-the-less! It is an important difference. 
README Drive Development is blast!
If you are able to go through the Rust book and find it straightforward then you already have a strong foundation in programming. Practicing doesn't have a stopping point -- all of the best programmers I know practice constantly, many in their fourties and fifties. If programming is something you're interested in then I say keep at it and see what happens.
If you weren't already familiar with the upcoming "async" keyword, that's the closest answer to your question. It's expected to stabilize early in 2019. There are some big differences between how goroutine work and how async is going to work, but they solve similar problems.
AFL and the fuzzers it inspired (libfuzzer, recent versions of honggfuzz) are actually pretty simple. https://lcamtuf.blogspot.com/2014/08/a-bit-more-about-american-fuzzy-lop.html provides a nice overview of how and why it works, and https://lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html describes the mutation strategies it employs, which are also very simple.
&gt;Now if I could only stop talking myself out of practicing because "it's too late" anyway (I'm 35 :/ ) I don't think there is an age limit to getting started with programming. If you're into cryptography you likely have a firm grasp of at least basic mathematics, which is a good foundation. The people who pioneered programming had to learn it well into their careers, and it was way harder to get started back then. Imagine your "Hello world tutorial" began with an introduction to your company's punch card operator and a 500-page assembly reference.
Return a `Box&lt;dyn MyTrait&gt;`
Wow, I feel like a moron. I solved it already.. Haha. [storing\_unboxed\_trait\_objects\_in\_stable\_rust](https://www.reddit.com/r/rust/comments/a68ydk/storing_unboxed_trait_objects_in_stable_rust/) I tried using `Box&lt;&gt;` before but didn't work, probably because it was `Box&lt;T&gt;` and not `Box&lt;dyn Test&gt;` So this is what I got after changing it up. .. pub fn get_the_other() -&gt; Box&lt;dyn Test&gt;{ Box::new(Other { }) } fn main() { let other = get_the_other(); println!("{:?}",other.is_test()); } And it builds and runs ... Not to familiar with `Box&lt;&gt;` yet, is this the best way to accomplish this? The intended tool for the job?
It sounds to me like any GC in Rust would effectively be implemented as a library/dependency, and that sounds fantastic. 
I believe 'may' implements green threads a library in rust. https://github.com/Xudong-Huang/may
I spent a while trying to recall my reasoning, and now I remember. I need to have very precise control over alignment here, since every type you add to the dynstack has its own separate alignment requirements. With a a `Vec`, there’s nothing preventing the resize operation from changing the alignment I specified (since `u8` can be aligned to anything). Now that I’m thinking about this a little bit more, I think my current implementation is a little bit flawed as well. Something to take a closer look at!
I'm not a Rust expert, but I think this is the expected solution. That or using `impl Test` as the return type (I'm not clear on the difference, honestly). The issue (as I understand it) is that Rust wants to put the return value on the stack normally, but if it doesn't know the concrete type, it can't do that, since the returned values may be of any size. So, it needs to place the return value in heap allocated memory - hence the `Box`.
Yes, it's called tokio, here is an example of green threads that run in parallel across all cpu cores. [https://hastebin.com/aquxezewod.php](https://hastebin.com/aquxezewod.php)
One alternative (it's not really the same, but it can be used to solve similar problems) is the Futures.rs / Tokio libraries. A future can either be chained (same thread) or spawned (when spawned the behaviour depends on the runtime, but in Tokio's case it'll be executed by a thread pool.
I don't see any differences between test files and real world SVG. &gt;the way they were designed to look This is basically impossible for SVG, since there is no reference implementation and no one knows how it should actually work.
And that is the same reason it worked with enum, a enum is the size of its largest variant.
Yes, I am aware. So the next best thing is saying "on these real-world samples where rendering is consistent between inkscape and rsvg, resvg produces the same results, so at least it's not worse than those two". The elementary icon theme is already known to render correctly to the human eye in inkscape and rsvg, so it would make for a decent real-world test.
nice to meet you fafhrd91. then if i use actix for my services it will be ok ?! these services will be long term and performance it's really important for these
I don't thinks that this is a necessary clarification.
Actix is core component of the large and important project I am working at the moment. And performance is one the main priorities for us. 
Something like that. Every operation that can GC allocate will await.
Could be either. rust-gc has a global GC. kyrenn's design allows for multiple GC "arenas". The async stuff also works with multiple independent executors (each executor is a GC) The rust GC design space is large, and you can make this choice based on what you need.
The difference between `Box&lt;dyn T&gt;` and `impl T` is that `Box&lt;dyn T&gt;` heap allocates and `impl T` does not. The general reason why the OP's code doesn't work is that, for generic functions, the caller specifies the type (`T` in this case), not the function body. In OP's case, what if there was another type which has `impl Test` (say, `Another`) and somebody called it like `let x: Another = get_the_other();`. This would make the function body (returning `Other`) invalid. One of the solutions to this is to place another bound on the trait T, so that it's `where T: Test + Default`. Then you could `impl Default for Other` and `impl Default for Another` (returning the right things in the corresponding `default()` calls), and in the function body of `get_the_other`, now just return `T::default()`. See (here)[https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=584e9f39c6a91d6685915b16d387af1a].
Should you want something simpler than cargo-make, I can also recommend [just](https://github.com/casey/just).
I'm in with all of what's said here. I'd like to see work to unify the wasm/asmjs community. Stdweb was made before the non-emscripten target existed, but as the article says something similar is needed for the wasm-bindgen setup and there is no reason to have two projects duplicate functionality. I think this is more an issue of reaching out to people who write things like yew and working with them to make their projects have a pluggable backend than actually using their libraries. I also want to draw attention to the typed-html library. I found it slightly fiddly to use, but it shows the extent to which you can type-check your html, even which tags are allowed in different places.
If you just want to create new threads, you can use `std::thread::spawn` If you want lightweight threads that are IO aware, you can do this using Tokio and async/await (async/await is actively being worked on, so it's still unstable) I talk about this space and compare it with what Go does in https://manishearth.github.io/blog/2018/01/10/whats-tokio-and-async-io-all-about/
Not „without needing crates“. While the basic building blocks for async/await will be in rustc and std, no task executor has even been proposed for std. Most high level utilities around async also live outside std. While I think this is a good thing for now, over time, it has to (and will) change.
/r/playrust
Great, i'll start developing. thank you for reply
I’d say 1000 hours is way too much already. Learn some programming instead! 
&gt; I'd also like to help. Is coordination happening in the wasm working group or elsewhere? Great! See this comment: https://www.reddit.com/r/rust/comments/a69hwf/rust_and_webassembly_in_2019/ebu9h99/?context=1
You have no idea how much disappointment you just elicited in me. I thought a lone lamb who had once found Rust too difficult had decided to give it a try again with the coming of the Rust 2018 edition, and now I'm all disappointed :(
Lol opssss I’m sorry, maybe I should start learning Rust
It's somewhat reasonably possible for a Rust implementation to implement green threads using techniques similar to how Go implements them. That is, garbage collection isn't strictly required for implementing green threads, but you *do* need cheap resizable stacks. That in turn means tweaking the way function calls are implemented in machine language (the ABI). The current mainstream Rust implementation uses LLVM's ABI for C/C++ (in a way that can be but might not always be compatible with C++ as compiled by clang - there be dragons), and LLVM doesn't pay much mind to stack resizing. The result is that a Rust thread is just an OS thread (with a large-ish fixed-size stack). It *is* possible to do a user-mode context switching and to implement green threads to some extent (by using assembly code, deep knowledge of how the C ABI works, and `extern "C"` declarations to force the compiler to follow those conventions) but the compiler certainly doesn't go out of its way to help you. 
Rayon is perfect for CPU-intensive work, especially when you can figure out the *what* pretty easily (map-reduce) but don't want to get bogged down in the details. It's interesting to know that Linux strongly encourages device drivers to be written using closures. Since hardware interrupt handlers don't have their own thread - they just borrow whichever thread is running on the same CPU - they can't sleep or take too much time. So a well-written device driver does the bare minimum to take care of the hardware, creates a closure to do everything else, and returns. E.g. a network driver would receive incoming packets from the card's buffers (or generate a DMA request to do it) but routing and filtering and TCP state-machine stuff are all handled in a closure. That "return" operation is designed (partially in assembly) to intercept a return to user mode and ensure that the kernel mode stays in control and executes the closures and has the opportunity to do important housekeeping things like switching between threads when multitasking.
What makes “goroutines” unique to Go (except that particular name?). 
You're welcome to :) To dip your toes, you can [check the Rust Book](https://doc.rust-lang.org/book/); it's both explanations and tutorial combined in one. Learn a little, practice with an exercise, rinse and repeat. I won't promise it's easy all the time, but you're welcome to ask when there's something you don't get, and on top of programming being fun, it's easy to monetize, killing two birds with one stone.
It's precisely the name that makes it unique, so just that really. I was just saying that, especially to a new programmer, it may not be immediately obvious that the nomenclature is specific to golang.
&gt;It's interesting to know that Linux strongly encourages device drivers to be written using (a C equivalent to) closures. Where can I find more about this?
Lol yes, I guess that's.. for the most part true ;) 
They're more heavily used by the standard library than other languages, but otherwise nothing about them is unique to go. 
&gt; It's precisely the name that makes it unique Clojure's [core.async has the go macro](http://clojure.github.io/core.async/#clojure.core.async/go) specifically because it implements the same basic channel idea. Not sure if you were aware of that, but I guess the name "go" was too convenient.
Oh, I'm sure! I believe the farthest I got was... I know I made a number guessing game, which was pretty awesome; then I believe there might have been something else.. (shows I need to go back to it lol). I actually stopped Rust in order to try my hand at Ruby because that was the language they wanted you to learn in order to be accepted to App Academy "Coding Bootcamp" program. But I didn't really like Ruby too much, though. Although in a way it was somewhat "simpler," it didn't feel near as clean and efficient. And something about the mutable variables threw me. I think I much preferred Rust. 
Right haha, I bet it was much different back then. Now, of course, we have Google, etc. But of course that's all relative too, right? As in, I'm sure the ratio of effort to progress was probably pretty similar. 
`Box&lt;dyn MyTrait&gt;` is mostly for when you could return multiple types, or in other words, the type will not be known until runtime. This cannot be monomorphised since it has multiple possible return types. Instead, dynamic dispatch is used, where Trait Object stored in the box has a vtable pointing to its methods. fn foo_or_bar(wants_foo: bool) -&gt; Box&lt;dyn MyTrait&gt; { if wants_foo { Box::new(Foo::new()) } else { Box::new(Bar::new()) } } On the other hand, `impl MyTrait` is when there is only a single possible return type, but you don't want to or cannot use its actual type in the return. This is common with iterators (which are difficult to name) and functions (which are impossible to name). fn some_chars() -&gt; impl Iterator&lt;Item = char&gt; { "Howdy".chars() } fn add_one() -&gt; impl Fn(u32) -&gt; u32 { |x| x + 1 } Although that by single return type that still includes this, since there's only one type getting filled in for the `impl` part. fn maybe_add_one(b: bool) -&gt; Option&lt;impl Fn(u32) -&gt; u32&gt; { if b { Some(|x| x + 1) } else { None } } Another use for `dyn` is when you want to store multiple different types sharing a trait. let showable: Vec&lt;Box&lt;dyn Display&gt;&gt; = vec![Box::new(1), Box::new("test")]; for item in showable { println!("{}", showable); } In general, `impl` should be used over `dyn` whenever possible, but `dyn` will be needed whenever the type cannot be known until runtime.
I don't think you can have graphs in rust without ref counting, and or delegate allocation to another structure that will manage allocations for you within a common lifetime.
Yes. I can't cite specifics, but I THINK I recall that there's been a codegen bug in rustc at some point that had been overlooked and required fixing the bug and rebuilding the compiler with a rather earlier binary of it, since it could no longer compile itself successfully. I think the typical defense against this is having the compiler compile itself, and ensure that it has the same functionality (unit tests, etc) as the original. It is impossible to prove a negative, though.
[https://github.com/arnetheduck/nlvm](https://github.com/arnetheduck/nlvm) There's an attempt but it's far from perfect
typed-html looks amazing, I should try to play with it a bit!
If you want to understand the rationale behind the current module system, here it is in all its glory: https://github.com/rust-lang/rfcs/pull/2126
This is amazing. I knew async would get used for things other than I/O. :D
Rouille is very good. We use it extensively in production. Rouille doesn't use tokio and/or async i/o, instead it relies on good ol' threads. One request in = one server thread. That isn't hipster cool and will never win performance wise against async i/o. It does however have its advantages: * Small dependency tree compared to tokio based frameworks. * Easy to understand (no "a future must polled to run") * Less magic. The rouille code is very easy to understand.
There's various thread pool libraries that offer some similar abilities, sharing a number of OS threads among lightweight threads (green tea, tasks, whatever you call them). `threadpool` or `crossbeam` are where I'd start.
I'd be interested to see the performance differences between the various webassembly frameworks like Yew, Ruukh, etc.
What about `num_bigint` vs `uint`? Which one's faster? I'm in need of a bigint crate atm.
The various parsing crates like nom, pest, lalrpop, etc. 
Those are already part of my library. `num_bigint` gets signficantly faster as soon as you need to math something larger than about 128 bits.
`nalgebra`, `cgmath`, and `vek` is something I've been curious about for a while. That type of benchmark is somewhat tricky though because there are so many different use cases which might have different performance characteristics.
That makes sense, thanks! Can `dyn` be used on its own, or does it have to be boxed? (I assume the latter.)
&gt; Just to make sure I understood this: Part of the reason is because Rust uses OS threads instead of green threads like Go for example, correct? Meaning if Rust had green threads, async/await would not/less be needed? That sounds right to me. &gt; But of course there are also advantages to using OS threads and Rust having both OS threads AND async/await is a "perfect" mix? Well, I think it's the only thing that works within the constraints that Rust has chosen for itself. (What use is async IO if it can't run on the microcontroller in my toaster?!) But I want to give credit to Go where credit is due here: One of the huge attractions of the goroutine approach is that your regular every-day synchronous code and your magical high-performance async IO code both look the same. When I call a function in Go, I don't need to know whether it's going to spawn goroutines or wait on channels internally. I just call it and let the runtime handle the details. That's an _enormous_ simplification, and the amount of engineering effort that went into supporting it is truly impressive. Also whoever decided the keyword for this would be `go` deserves an honorary doctorate in programming language marketing.
It can be used as `&amp;dyn Trait` as well. And in any place that accepts `?Sized` types
Whew! I more often hear complaints that it's slow, especially compared to gmp (with its asm kernels and huge development history...), so a positive result is nice. 😀 That said, I also list alternatives at the bottom of num-bigint's readme. It's fine if people choose another crate for their needs.
Preaching to the choir, this is all stuff that was brought up [before](https://www.reddit.com/r/rust/comments/a1jjuf/a_new_look_for_rustlangorg/), and [after](https://www.reddit.com/r/rust/comments/a42gti/concerning_the_new_website_design/), release. Unfortunately discussion on this is now [banned](https://www.reddit.com/r/rust/comments/a5fw5u/two_things_about_new_website/ebmuin0/), only making duplicate issues detailing the same concerns already brought up multiple times is allowed now i guess. The colors and the much [improved mockups in the issue](https://github.com/rust-lang/www.rust-lang.org/issues/421#issuecomment-443098003) is just straight up not for discussion, too. So if issues aren't up for discussion as issues on github, where then?
&gt; Note that it isn't useful to benchmark a crate by itself, as we'd judge speed relative to other crates. So any suggestions require at least two crates that can do the same function. I'd argue that it would be useful to track the performance of a single crate over time. With your application's ability to notice new versions this would allow the performance of a crate to be tracked over time. This would help see general trends in its performance and perhaps any significant regressions. Kind of like [perf.rust-lang.org(https://perf.rust-lang.org/index.html) and [arewefastyet.com](https://arewefastyet.com/win10/overview?numDays=60).
Have you seen the [official follow up on the website concerns(https://internals.rust-lang.org/t/followup-on-website-concerns/9018)?
Nope, i'll look at it. But i did just find an issue for a front page example, see edit. Yippee!
&gt; more heavily used by the standard library Lol. You should check Erlang standard library, Go is nowhere near to that. 
I'd like to see the big integer libraries especially. 
Read through it, seems like the typical PR thats become standard from the team, trying to seem like they're listening but not actually. This is the *second* big issue recently with listening to the community, though i can't remember what the first one was over.., same kind of response. It's hard to take stuff like &gt; But we do hear the concerns, and we want to work through them! Seriously when you see all the issues on the github closed *and locked*(discussion is evilll, hm?) and "not being considered at this time". Not a good look. Or from the very end of the post, "In the meantime, the new web site will stay in place as-is, aside from ongoing tweaks and small improvements." &gt; Our plan is to put together a retrospective for the web site work (and to highlight the various concerns) early in 2019. How convenient that their plan is.. do nothing until early 2019 and then, again, talk amongst themselves, and only *then*, allegedly(but i doubt it), "work together as a community to determine how to move forward.". Which was said last time, too.. &gt; That gives everyone time to enjoy the holidays — and play with Rust 2018 — before fully engaging in discussion about the process. Brilliant! Rush a new design through, to widespread criticism, after asking for feedback only a week before release, then ignoring it, then locking all future issues, and all before the holidays so you can use it as an excuse to keep ignoring feedback and "play with rust 2018"(as if thats what people are having problems with..) I wonder what the reason will be when 2019 comes around. &gt; Along those lines, we ask that people with process concerns or proposals for major overhauls to the website hold off on opening issues on these topics for the time being. discussion is evilllll, no issues, the obviously implied "hopefully they'll forget and go away by then" ---- I know i'm not exactly assuming good faith from the team, but really, at this point i see no reason to assume as much anymore. And that "followup" sure doesn't help. "No issues or discussion until 2019, wait until we talk amongst ourselves and then we say we'll work with you! Not before! No!". Yeah, that *reaalllly* feels like they're listening to the community and care.
Nice write-up. I believe this quick-n-dirty solution would also work. (Note that an empty HashMap does not allocate). fn neighbors_with_send_capacity_iter_empty(&amp;self, a: N, capacity: u128) -&gt; impl Iterator&lt;Item=&amp;N&gt; { let a_map = match self.nodes.get(&amp;a) { Some(a_map) =&gt; a_map, None =&gt; HashMap::new() }; a_map.keys().filter(move |b| self.get_send_capacity(&amp;a,b) &gt;= capacity) } 
I've also been giving serious consideration for both a pull-request to implement this in libcore, and also an RFC to implement a routine called `parse_lossy` or `from_str_lossy` for floats, which would avoid the use of arbitrary-precision arithmetic during float parsing. Any feedback, or suggestions, would be wonderful.
Unfortunately for us, there is already [an issue](https://github.com/rust-lang/www.rust-lang.org/issues/396) on having code examples on the front page, which is locked and closed, not being considered at this time. 
(mongo-driver)[https://crates.io/crates/mongo_driver] and (mongodb)[https://crates.io/crates/mongodb]
I've thought about this for some time when deciding how to build this benching application. If you want to judge how fast rust is in general or see how fast it has been progressing, those are good websites to get an overall perspective. I wouldn't be able to build significantly more value on top of what they have provided. Instead the target audience is a rust developer asking: &gt; Which crate is faster? A or B? Can I see an example use of each? By limiting myself to look at crates relative to eachother, I significantly simplify what needs to be done. With such a narrow focus, I have more time adding more crates, rather than doing more analysis on each crate.
I'm trying to use `syn` for a pet project, and I feel ya. Just show me a basic AST visitor, goddammit! I don't want to wade through a web of trait impls in an example that may or may not be absolutely necessary for this thing to work!
&gt; I know i'm not exactly assuming good faith from the team, but really, at this point i see no reason to assume as much anymore. Are you sure? I think the core team are just people like you and me doing the best they can at time. There are missteps maybe but the linked post is an acknowledgement that they want to look at what happened and then make the situation better in the future. [Aaron talks more about the pressures of developing in the open in this Twitter thread](https://twitter.com/aaron_turon/status/1073757752600784896).
not that again :D
That they are built into the syntax instead of being functions in the library.
Fuzzing might be very useful to discover both memory safety issues and discrepancies with e.g. glibc implementation. [Rust fuzz book](https://fuzz.rs/book/introduction.html) guides you through the process, it will take no more than 15 minutes to set up.
Thanks for sharing! I'm wondering about the slice *Architecture: Aarch64*, why does it immediately go out of bounds with the first argument?
&gt; linked post is an acknowledgement that they want to look at what happened and then make the situation better in the future. It just doesn't read like that to me. Sure, on the surface it is, but it doesn't seem *genuine* to me, especially considering other recent missteps in this area about this same kind of thing. It just seems like the same old, same old "excuses" for recent problems and empty promise to work with the community more. Like it's been done before and nothing happened last time. As i mentioned there was some other recent issue over listening to the community, still can't remember what about(might have been over unicode idents?), and the same acknowledgements and promises to work with the community were made. But we still got that closed-door redesign, rushed to match 2018, to widespread criticism, [targeting managers and CTOs instead of developers](https://github.com/rust-lang/www.rust-lang.org/issues/431#issuecomment-442985053). I guess Rust is the one enlightened programming language website and everyone else is doing it wrong? Someone should tell Google that the [go](https://golang.org/) website is terrible and no company would use it, it should target managers instead of programmers. Same for [Python](https://www.python.org/) It's hard to take them at face value when they straight up ask not to make issues about any of this until after the holidays. They might not want to deal with them over the holidays, and they don't have to, but to say not to have issues or proposals period? It just seems like an attempt to get people to forget, wait for it all to blow over. Or closing and locking most of the existing issues, especially ones that reflect peoples primary concerns with the site. Or that, even after all this, their solution is yet another closed door meeting and *then* work with the community, even though that kind of stuff is specifically what the problem is! I'm seeing a lot of retrospectives on listening to the community but not a whole lot of "doing" and, well, "listening"
This looks pretty awesome, great job! Worth opening an RFC? Not sure how much it would have to be reworked though if it were to be accepted since it’s a fairly massive piece of work with lots of unsafe code. Btw: I’ve noticed you also have atoi implementations that are claimed to be slightly faster than libcore? Another note: might be worth running a fuzzer over it, given all the unsafe stuff.
Definitely, I'm working on it. For the integer parsers, they're faster, but not that much. I know for integer-to-string conversions, most of the overhead comes from fmt (the implementation is very efficient, using power-reduction, lookup tables, etc.), and not the actual serializer, so differences with 1-3 fold are pretty normal. If I can improve the actual code though for atoi in libcore, I'd gladly do so. And I would definitely open an RFC. I think with the amount of unsafe code I use (the entire core part is unsafe) is great for a proof of concept, but it should likely be "reigned in" for inclusion in Rust. I doubt using safe code will drop performance by more than a small factor, if I use slices internally, so I think it's worth a careful re-write.
I'd love to see my fast `smash` hashmap crate benchmarked against other hashmap crates like `smash` and `fxhash` under proper benchmarking conditions: https://github.com/zesterer/smash
Awesome to see `vek` being noticed. I've been using it for some time, and it's superb.
If you mean Rust the game, you're in the wrong subreddit. You'll want /r/playrust for that.
A great suggestion. Can I ask a somewhat related question: I've been using quickcheck to simplify the generation of random floats to test parsing, but ideally, I'd like to a fuzzer that generates random bytes with certain constraints on some of the input, since I stop parsing when invalid digits are found (anything other than '[0-9.eE+-]' will immediately cause me to stop parsing and return the result currently found. I would love a fuzzer, but also additional tests with random data that matches a regular expression to test conditions that are more likely to be problematic. Do you have any ideas of a good tool (doesn't have to be Rust either)?
Nope, I'm taking about the language. 
I'm working on a data reader library, so I'll try out your library to see what kinda performance I get from it when compared to the std library calls I'm using it now. I'm curious if you have any plans to support the i128 and u128 types as well. If not I'll just add another macro for the various other Rust primitive types that I support in the library.
That's interesting - I didn't think about benchmarking unpublished crates. Do you intend to publish your crate any time soon? 
The Aarch64 architecture stores integer values and floating point values in separate save areas. Since the va\_list contained no floating point values, we're immediately out of bounds when we call arg with a floating point type.
If you need another term for your comparison, I guess the strtod implementation of the Google's double-conversion library could be interesting a well: [https://github.com/google/double-conversion](https://github.com/google/double-conversion) . &amp;#x200B; A few other string to double implementations are available in [https://github.com/shaovoon/floatbench](https://github.com/shaovoon/floatbench) , but I am not sure how much robust/complete these other implementations are. 
Awesome, this is a much more complete collection. Thank you.
Note that with this style of slide deck, if you just hit right on your keyboard you're going to miss half the slides. Watch the arrows in the corner to see when you can press down to view extra slides in a set. And good topic for a presentation! One of those very obscure dark corners of Rust that exists only for C interop and that are easy even for veterans to overlook.
I actually support u128 and i128 intrinsically, I just didn't write benchmarks for them because NumPy doesn't support 128-bit types, so generating random data was slightly more work (a bitshift and bitwise or, but still, slightly more work).
I'm personally interested in benchmarking FFT's. I'm actually more interested in benchmarking pure Rust FFT's like [rustfft](https://crates.io/crates/rustfft) against mature C implementations like fftw, but perhaps this is ok because you're able to deal with Rust wrappers (in this case, [fftw3](https://crates.io/crates/fftw3))?
I expected this to be a useless explaination of a trivial problem... But it turned out to be less trivial than I thought, and well explained. Thank you!
There's a bunch of ye olde fuzzers that accept a formal grammar and start blindly generating stuff that fits it. There's an entire ecosystem of those, that's the only fuzzers we had before the current feedback-driven ones (AFL, libfuzzer and honggfuzz). But feedback-driven fuzzers should be both roughly 10x faster than them due to being in-process and also should able to isolate such cases automatically, so I would rather focus on a really good AFL or cargo-fuzz harness than use something driven by a formal grammar. AFL in in-process mode should be capable of over a billion executions a day per core on your machine; cargo-fuzz is boggled down by address sanitizer but still should be plenty fast. I know libfuzzer has a flag that makes it generate ASCII-only input. You could also reshape the input bytes into the desired distribution (digits and select symbols only) inside the fuzzing harness, or just skip bytes that don't fit. But that shouldn't be necessary to get good coverage thanks to the sheer amount of executions per second that Rust fuzzers put out. For example, I've [successfully found a zero-day vulnerability](https://medium.com/@shnatsel/how-ive-found-vulnerability-in-a-popular-rust-crate-and-you-can-too-3db081a67fb) in a FLAC parser overnight using AFL *despite forgetting to disable CRC16 verification.* And fuzzing with CRC16 disabled didn't turn up anything else. So don't worry about guiding guided fuzzers too much :)
Wonderful, thank you.
Very nice, but the slides alone seem to miss most of the "why you never should" content. Are there presenter notes to go along with it? (Always a hard problem with slides, since ideally they hit the high points and leave the details to the presenter.) That said, `va_list` always struck me as part of the most beautiful and awful parts of C. It's a reasonably nice way to walk a stack with an abstract API doing the nasty work, which isn't something you can do well in C without OS support... But at the same time is a terrible way of basically doing the task of "implement printf without language special cases", making assumptions that usually don't apply to modern register-rich CPU's.
I wasn't aware! But I think my point holds - it's non-standard to use "go" as a keyword for a stackful coroutine/green thread.
While not a fuzzer, you may consider looking at [proptest](https://github.com/AltSysrq/proptest), an alternative to quickcheck. It has strong support for [composing input data generators](https://altsysrq.github.io/rustdoc/proptest/latest/proptest/#compound-strategies) and in particular has neat helpers for generating randomized input *from* regexes.
Huh? You don't need Visual Studio installed at all, just the build tools.
One can press space to just go to the next slide, whichever direction it is.
Yes. Perhaps in the next few days when I actually have time to write some proper unit tests 😁
I was not attempting to imply that it had the heaviest use of green threads of any language in existence
This isn't *quite* right, though; the thing that makes vtables (in C++) a bit magic is that the function pointers operate on objects of the type of the structure containing the vtable, and the language enforces this type-matching. Consider this C++: class IExample { public: virtual int f(int x) = 0; }; class AddOne : public IExample { public: virtual int f(int x) override { return x + 1; } }; class AddStoredValue : public IExample { public: AddStoredValue( int v ) : mV(v) {} virtual int f(int x) { return x + mV; } int mV; }; In C, you could write something like struct VT { int (*f)(void* owner, int x); }; typedef const VT* IExample; struct AddOne { IExample vt; }; int AddOneF(void* arg, int x) { AddOne* this = (AddOne*)arg; return x+1; } const VT AddOneVT = {&amp;AddOneF}; struct AddValue { IExample vt; int mV; }; int AddValueF(void* arg, int x) { AddValue* this = (AddValue*)arg; return x + this-&gt;mV; } const VT AddValueVT = {&amp;AddValueF}; void example() { AddOne ex_inst = { &amp;AddOneVt }; IExample* ex_base = (IExample*)&amp;exInst; int ok = (*ex_base)-&gt;f(ex_base, 3); assert(ok == 4); // everything is fine up to here, but... int undefined_behavior = AddValueF(ex_base, 5); AddOne also_bad = {&amp;AddValueVT}; IExample* also_base = (IExample*)&amp;also_bad; int also_undefined_behavior = (*also_base)-&gt;f(also_base, 6); } The problem is that nothing guarantees that the "downcasts" in AddOneF and AddValueF are safe. Similarly just building your own vtables in rust, nothing guarantees that those vtables are only connected to objects of the matching type. Trait objects are Rust's built-in method for connecting vtables with objects of the matching type, just as virtual functions do that job in C++. Without some sort of language support for this kind of 'linked types', this isn't possible to implement safely.
Oh my god YES. The only problem is that a few people on the team need to understand Rust now. I wish everyone would adopt this, validation is such a glaringly missing thing in k8s :(
Sure, but as I noted, my benchmarking library is not public yet. I copy and paste the results I get, just to confirm the speed for you once the crates are added in.
Maybe ndarray too?
Benchmarks alone don't answer that query, unless you're certain both crates are solving the same problem. For example, you want to compare jetscii and memchr, but those crates are solving different problems. There is some overlap between them, and possibly in that overlap is where benchmarks might make sense, but it wouldn't be good to just publish numbera without accompanying context.
I've actually found a few integer overflows and read-one-past-the-array memory bug that my unittests with "-Zsanitizer=address" did not catch, so this has been hugely useful. Right now, everything is running quietly so it seems pretty good.
Live: https://flypage.chinamcloud.com/h5/tpl/index.html?id=6470&amp;tid=809
I've been thinking about this problem for a while, and I'm not sure you really need arbitrary precision integers. But I haven't implemented a proof of concept, so maybe I'm missing something? Here's a summary of my thoughts: * Start by reading some prefix of the decimal string, let's say ".31". * At that point, reading more digits can only increase the value (since it's assumed all the existing digits are 0) * So you have a boundary for the value; it's in the range [.31,32). * If we are out of digits, we have an exact value and choose the FP value closest to the bottom value of the range. * If both the top and bottom of the range are closest to a single FP value, we can stop. * Otherwise, we may need to read more digits. Nothing I've said so far is particularly interesting or revolutionary, I believe; but this algorithm pretty clearly requires arbitrary precision math. Now, consider the example for distinguishing between the IEEE floats with integer representation 0x0 and 0x1 (the smallest denormalized float). What we really want to know is "is the number bigger than `((double)((const float&amp;)(int)1)) * 0.5`. This seems like a tractable problem that can be solved entirely during string parsing. Here is a dumb version of my idea for a no-high-precision-arithmetic-involved algorithm: * Manually implement a floating point representation with slightly more precision than double. * If you are close to a halfway point, *print* the representation of that halfway point. * The above step can be implemented as a coroutine that only generates digits as desired using a fixed amount of space and no high-precision arithmetic required (maybe not for all possible doubles? grisu3 had to fallback to dragon for some edge cases) * Compare the input string to the printed string, to find if it represents a larger or smaller value * Round the correct direction based on that printed value.
We actually do this, using bigcomp. The issue is, to correctly represent the halfway point, we need big integers, but the actual size of these big integers is relatively small (no more than 10**324, as compared to up to 10**1092 for Algorithm M). I try a quick approximation, using only 128-bits for the numerator and denominator to print the digits from the halfway point, however, this is only accurate for ~36 digits. Unfortunately, anything more requires arbitrary-precision math: luckily, just not that much.
Would `parse_lossy` have the property that `parse_lossy(x.to_string())` is always exactly `x` and correctness is only sacrificed when extra precision is provided in the input string (like 2.471e-324, which `f64::to_string` cannot produce)?
That would be a great feature, I know for a fact using the "Fallback Moderate Path" in the original post would make this true (it likely is true right now, but formally, nothing guarantees it, and I'm not testing every input in a 2^64 combinations to verify it). Thanks for the suggestion, I hadn't even thought of it.
Perfect, above and beyond what I was even hoping for.
Asking from a place of ignorance: are other languages that implement green threads as aggressive as Go is about inserting yield points onto your code? My (very shaky) understanding was that this is one of the main selling points of Goroutines: the compiler does a lot of magic to make sure you can't stuff up by accidentally blocking for a long time.
Looool that's exactly why I mentioned it :sob:
Completely understood. The way I have built the framework here to be "function based", not "crate based". A crate that has some extra special features that no other crate has, will not be benchmarked on those special features, because I am only comparing same functionality. For example with jetscii and memchr: * memchr specializes in searching for a character inside of a string. * jetscii has the ability to search for characters in a string, but it does other things too (search for strings in string, search for varying chars, etc). So the function comparing them is simply to search for a character in a string. I plan to have a different function comparing searching strings inside of other strings, but memchr would not be included in that function benchmark. The target audience is someone who thinks "I need a certain functionality - What is the fastest crate that does this?" ..rather than "I heard this crate is good. I wonder what value it has compared to other crates."
This looks really useful. Can I request an example of how this could be isolated to tests? I'm not too familiar with how changing allocators works and I wouldn't want to permanently change allocator, or be commenting and uncommenting lines for testing.
I see, you need to use 10^324 to print the digits for 2^-1074. I still wonder if there is a way to do this incrementally, but perhaps you're right that just doing the 'big' math is the right way. I will note that 10^324 has a factor of 2^324 so you could get away with only ~750 bits of precision if you were clever.
Pretty much any guide / book to writing Linux device drivers. (If I wanted more experience I'd fire up a virtual machine and try writing something simple like a reimplementation of `timerfd` as a character device. Any device that supports non-blocking I/O will require async programming techniques to implement. Traditionally something like a serial or parallel port driver was a popular project - maybe a ps/2 mouse driver these days.) https://www.oreilly.com/library/view/understanding-the-linux/0596005652/ch04s07.html Tasklets are the kind of closure that are executed before returning to user mode. (Maybe even before returning to "kernel mode process context" - a syscall or fault handler. I'm not 100% clear on the control flow.) They stay on the original core and don't need to be `Send`-safe. Workqueues are the ones that are executed by a worker thread. In both cases the C idiom of callback+data pointers is used to implement the closures. 
Yeah, that should work. Example: Prelude Data.List&gt; let f 0 = Nothing; f x = let d = 2^751 in let r = div x d in Just (r, 10*(x - (r*d))) Prelude Data.List&gt; take 10 $ unfoldr f (5^324) [2,4,7,0,3,2,8,2,2,9] 
&gt; So the function comparing them is simply to search for a character in a string. Well, that's kind of the problem though. jetscii is (probably) not going to do as well as memchr on this benchmark. Without accompanying context explaining what's going on, it might give off the appearance that memchr is "better" than jetscii, but it's only better on certain cases. Basically, my request here is that you don't just publish numbers. It leads to confused users. A blurb for each benchmark giving context would be very helpful.
You are right about the powers of 2 (and I actually exploit this fact). If you can solve this solution incrementally, I'd ~~love~~ pay to hear it. The exponentiation part isn't too costly, but it still adds a lot (and is likely the significant factor in the performance of my Algorithm M implementation). For example, to calculate x\*5^(324,) we can do (in Python): def mul_small_power(x, n): '''Calculate x*5**n using native integers''' # precomputed small powers of 5**n fit in u32 powers = [1, 5, 25, 125, 625, 3125, 15625, 78125, 390625, 1953125, 9765625, 48828125, 244140625, 1220703125] step = powers.len() - 1 while n &gt;= step: x *= powers[-1] n -= step return x * powers[n] def mul_large_power(x, n): '''Calculate x*5**n using big integers''' # precomputed large powers of 5**(2**n) # obviously this table would be a lot larger, but you get the general idea. powers = [5, 25, 625, 390625, 152587890625, 23283064365386962890625] idx = 0 bit = 1 while n != 0: if n &amp; bit != 0 x *= powers[idx] n ^= bit idx += 1 bit &lt;&lt;= 1 return x In practice, we use a hybrid approach of these two, but there's likely a lot of fine-tuning parameters for this I could do. Powers less than 2^(1075) tend to be more efficient using iterative small powers, while anything large tends to be more efficient using larger powers (this is effectively a modification of exponentiation by squaring when x is 1). The reason why multiplying two large powers can be decently efficient is due the asymptotically better performance of Karatsuba multiplication, but this typically requires at least 640 bits for both operands to be efficient. Anything less and the performance of using iterative small powers tends to win out. Fine-tuning both the division and exponentiation steps would likely improve both.
What is the proper user of a machine like this? I am under the impression that this ought not be used to play a Nintendo emulator on.
Emulators, messing around with rayon. Running long tasks etc. If you are wondering. The machine has already been claimed. Happy holidays :) 
I do have an explanation listed for each benchmark, but you are right - I will try to think on how to stop people from generalizing about crates by just looking at one function.
Yeah, having big integers built into the language (like Haskell) is always a major win. &amp;#x200B; To be fair, the bigcomp approach also uses a few other tricks to make it extremely fast. We don't need to do a full division step, since we know the resulting value will be in the range \`\[0, 9\]\`, which allows to do some very nice tricks. We set the bits in the denominator for bigcomp to be 4 bits below normalized (having a 1 in the most-significant bit of the most-significant limb), since 10 has 4 bits. We can then divide by only the most-significant limb in each, and if we underestimate the value during the calculation of the remainder, we simply increment the quotient by 1 and re-calculate the remainder without any multiplications required. We can't do this for Algorithm M, but we can optimize the division algorithm for the use-case. I still wish I knew how strtod does its black magic, but I haven't looked at the implementation. I'd rather not touch GPL-licensed code for a project I wish to contribute to Rust within a 39.5 inch pole.
Warning: the default test harness for Rust runs tests in parallel so I'm not sure how well this synchronous toggle will work for them. But you should be able to annotate the tests, or call the enable fn in tests in order to do so. (And `#[cfg(test)]` the allocator assignment.)
Also, just an FYI: If you care about performance at all costs without sacrificing correctness, I'd recommend using the "algorithm_m" feature (it's not enabled by default). Both algorithms are quite efficient, however, since bigcomp uses significantly less memory (neither require allocations, but algorithm_m requires a stack-allocated array of ~4Kb, while bigcomp only requires ~1kb), I've set bigcomp to be the default parser. If correctness is not a concern (my field is mass spectrometry, so for me, an error of 1 ULP is significantly less than instrument error), using the lossy parser may speed up your code further, if you have unusually complicated input data.
The [QADAPT tests](https://github.com/bspeice/qadapt/blob/master/tests/allocations.rs) themselves are a good example, though I'd need to double-check on how inline tests fare. Currently there's code for triggering a panic if you use QADAPT code without the allocator set up (in testing there were too many successes because I forgot to enable the allocator) but it may be worth it to strip code in `#[cfg(test)]` mode as well. I'd be interested in feedback on that. 
Multi-threading is fine, and there was a lot of work put in to make sure that it behaves without deadlocking. The issue with async is that code within a function might not be run until after the function is finished (closures are an issue here as well).
Perhaps a comment linking to this should be added to the end of each of the locked issues. A graveyard of locked issues, each ending with some variant of "stop talking about this" is not a great look.
Yeah I am using division / multiply by power of 2 here to represent just bitwise masking. The only expensive operations are the initial multiply of the floating point value to by the proper power of 5 and a 750 bit multiply by 10.
they will upload all the slides after the conference
&gt; I don't think commenting on that blog will move anything in the right direction. I took the time to do so and it seems to have helped. A mixture of ignorance and frustration doesn't always lead to dogmatic refusal to consider new information.
Except where the framework is tweaked so that the Hello-world is super-elegant but application to real problems is now contrived. I think a clear set of examples showing implementations of common use-cases is better than a Hello-world. No one wants to do a Hello-world in reality but those example help not only with getting started but with idiomatic adoption. In making the examples you'll discover your own frameworks rough edges and gaps too.
Does the go compiler arbitrarily insert yields all over your code? I didn't know that. 
Goroutins handles stack very differently compared to any other green-thread implementation. Also, goroutines are very deeply integrated in Go, so entire stack uses it while using any 3rd party green thread library you have to make sure everything in your stack (as in call stack) plays well with it. That's about it. `May` mentioned here doesn't handle it at all - you just hardcode stack size and _pray_ it doesn't go over it. 
Lol
Now instead of being able to use normal k8s apis, new devs will have to onboard to your frankenstein one for ... what benefit? Also, validation like this is insane to do on dev machines. Those policies absolutely must be implementation as Admission Webhook Controllers. Otherwise, one way or another, I'm going to send whatever I want to the k8s API and not use your custom objects and bypass everything you've done. I hate to be a pooper. I love k8s and rust and want to build something with them soon.
Yep. https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html offers a pretty good overview. Currently inserted at (quoting the above article): &gt; - The use of the keyword `go` &gt; - Garbage collection &gt; - System calls &gt; - Synchronization and Orchestration This doesn't mean that your Goroutine _will_ be interrupted at every one of these points, but only that it will offer itself up for suspension then. In future, the compiler may insert yield points in hot loops, or even introduce a kind of preemption (https://github.com/golang/go/issues/24543) to address the same issues. 
 &gt; I need to have very precise control over alignment here, since every type you add to the dynstack has its own separate alignment requirements. Ah, right. So if you had something which was guaranteed to have "maximum alignment", e g if `u128` was guaranteed to require an alignment of 16 - I don't know if it is - then you could potentially have a `Vec&lt;u128&gt;` and then typecast a pointer to the first element to `*mut u8`. &gt; Now that I’m thinking about this a little bit more, I think my current implementation is a little bit flawed as well. Something to take a closer look at! Right, it does not seem to handle OOM errors well?
I use `mongo-driver`, and now I looked at `mongodb` at it seems to have a nicer API, newer release, more depdendencies (more functionality), more owners, etc. They are both using `bson` crate under the hood, and since the DB access is going to dominate whatever they are going to do, I doubt there is going to be significant difference. I should switch to `mongodb`...
&gt; They're more heavily used by the standard library than other languages Unfortunately this is exactly what you said. 
Erlang also has preemptive scheduler built in into core, and by extension all languages that are built on top of BEAM (like Elixir) as well. 
Which isn’t new at all. Erlang had it waaaay before Go.
I would be vary of words like “any other”, because anything that is “innovative” in Go existed much earlier, and in case of goroutines the answer will be almost always the same - Erlang. 
“Innovation”…
Also, I wouldn't suggest this casually, but I think you might actually appreciate this. There's a great book called the "Handbook of Floating Point Arithmetic" that gives an introduction to converting radices for floating-point arithmetic (you can get what I believe is a legal copy for free [here](https://www.researchgate.net/publication/47735229_Handbook_of_Floating-Point_Arithmetic)). Specifically, sections 2.7 (on radix conversions) and 3.2.3 
I've been using mongo-driver in production for a couple years now. Every now and then I look at mongodb but some of the issues they have listed in their tracker scare me away. I posted this request thinking of this one: https://github.com/mongodb-labs/mongo-rust-driver-prototype/issues/277 I like the fact that the core of mongo-driver is the official C library. It's pretty heavily used, well trusted code.
Replace any with many if it makes you feel better. Erlang is the polar opposite of Go. Actors &gt; CSP in my opinion. Lightweight user-space scheduled processes != lightweight user-space scheduled threads. There way to many differences between what Erlang/OTP offers and what Go has. I'm not saying go is innovative in fact I would jump on every opportunity to bash it, but you have to admin that Go's implementation of channels and green-threads is dope.
Are there any other crates similar to Tantivy?
Even casual posts here on this sub are locked and closed. The folks in charge give off a strong impression of not being interested in what rust users think nor are they willing to let them discuss it among themselves. Makes me question whether this project is truly community driven or merely just in name so that they can get development done for free. Very frustrating.
Sadly there isn’t any such “maximum alignment” value. Consider an AVX512 vector, or even just a struct with `#[repr(align(128))]` or whatever. That actually ties in with the bug I was thinking of. The implementation currently assumes you don’t want to store anything with alignment &gt; 16, though I’m working on fixing that right now. As for OOM... well... you’re right that I don’t handle it. Then again, Rust kinda just assumes OOM is impossible :shrug:
I'm specially talking about the first piece of code you ever show to a prospective user. Everything after that should be more interesting. If you don't show an idiomatic example, you're showing an example that might have parts I need to take out for my use case. The hello world example shows parts you can't take out other than the hello world part. You can take an intersection of two examples like this to figure out the parts you need and the parts you don't.
Wow. Thanks for letting me know. :)
When is garbage collection checked? At every allocation? 
My experience has been that the MSVC tool chain is easier to set up and more reliable. You have to drink the MS development kool-aid, but if you do the tools (debugger, profiler, etc) are excellent. It's mostly a matter of whether you want to embrace the Windows dev tools MS gives you, or insist on being able to pretend that it's a Unix system. If you're developing for Linux first or porting Linux software, the latter is easier. If you can abstract the details away some, it's easiest to use whatever tools are native to your platform whenever you can still have the basics still be cross-platform. If that makes any sense at all.
Yep. I have a project demonstrating that: https://github.com/idubrov/crazy-traits/blob/master/src/dynvtable.rs I'm currently working on a more sophisticated implementation that allows to "attach" given trait "Foo" to any data type "Bar" plus provide any kind of metadata to it. Basically, your implementation of "Foo" trait will get both pointer to the data itself, all the arguments to the function and also the provided metadata as an extra parameter (so, the same data type "Bar" could get different behavior -- because of different "metadata"). This might (or might not :) ) be useful for what we are doing at my company. Kind of a crazy trick for desperate times :) I wish trait objects ABI was stable. Both vtable layout and maybe even some "metadata API" for them (could be something like `#[derive(..)`] macro producing constant with vtable layout). Would be incredibly useful for "enterprise" software where late binding is a common thing (plugins, etc).
I love the effort you have put into this. I'd like to mirror what everyone else has said in that I'd love to see this work brought into the core library. You could always start the process now, knowing that you are intending to refactor things along the way. :)
You can use Beta or Nightly for LLVM 8. Also I believe there's ways to use the system LLVM (although you may need to compile rustc yourself then).
that’s really well written. thanks.
nom and combine and few others already have an existing benchmark repo? Would this benching be any different/useful?
To run this people have to manually clone your "Test-Game-Derive" repository and rename it to "test\_game\_derive" to match Cargo.toml. In my opinion, people should be able to clone and run "cargo build" and have the entire project build without having to do anything extra.
You could probably combine `iter::empty` with [`either::Either`](https://docs.rs/either/1.5.0/either/enum.Either.html), then you would be able to use `impl Iterator`.
Box works but you may be able to avoid it. Why do you want the return type to impl a trait if you don't need any of the trait def in your fn. Have your fn just return `Other`, have `Other` impl your trait and you're good.
I didn't mean to imply a particular schedule. For what it's worth, here's the tracking issue [https://github.com/SergioBenitez/Rocket/issues/19](https://github.com/SergioBenitez/Rocket/issues/19) In looking over that issue, along with considering the focus of the discussions around 2019, makes me think that the unstable compiler features that Rocket is using right now won't all be finished in 2019. (Here's a case where I'd be glad to be wrong) &amp;#x200B; In stead, I'm expecting rocket to implement some stable workarounds for those features. Again, I don't want to place expectations on when Rocket may become usable on stable rust, just that I'd be very surprised if it was soon. &amp;#x200B; &amp;#x200B;
Sure, thanks. I'll add both of those to the main repository tomorrow morning or so.
There is no LLVM 8. LLVM 8 is not released (for that matter, not even branched) yet.
[removed]
For CLIs you can use [asciicinema](https://asciinema.org/)!
&gt; Who the bright fuck decided to hardcode LLVM-7 into the rustc 1.31? That's like asking "Who the bright fuck decided to hardcode Python 2.x into ...?" when a project doesn't work under Python 3.x. The whole reason LLVM (or any project) increments the major version number is so they can make breaking changes to free themselves from past bad decisions. rustc needs to be updated whenever LLVM changes an API it interacts with. The only reason things like LLVM Clang are always usable with any LLVM version is that they're maintained as part of the same project, so it all gets updated together.
Agreed. Strong point of K8s is exactly it’s stable API and yaml. Why would you want a general abstraction that covers only half of the API (or less)? Make it for a specific purpose and it’s a fine crd, though. 
I was able to actually read my code when I used pom. It was very easy to get started.
I don't think this needs an RFC. This does not change the interface or behaviour of libcore, it only improves performance and fixes parsing correctness. If I were OP, I'd open a PR against libcore. This will of course be a time consuming process, but it will be worth it eventually.
Wow, I applaud your effort. Doesn't look to me though like new information was accepted there, otherwise the most glaring errors in the original post would have been corrected.
"other languages" != "every other language", though. There are many, many languages which don't do this, right?
Yes, you need unsafe code, but without rvalue promotion to 'static, you couldn't do anything at all. There are ways to link the types up, using generative typing, btw, but it's tricky and still needs unsafe code.
&gt; (The most common mistake I've seen is people thinking that waiting through a demo video or wandering aimlessly through a demo instance of a web app is an acceptable substitute.) Note my comment about GUI apps: &gt; (The most common mistake I've seen is people thinking that waiting through a demo video or wandering aimlessly through a demo instance of a web app is an acceptable substitute.) The same thing applies to CLI apps. I don't want to wait through a video. I want some screenshots I can skim through at my own speed.
I like to see such a high quality post submitted as a self post to Reddit. I browse Reddit from my phone in the morning and this is easy to digest. 
Thanks. A lot of the validation in there would benefit from being moved/copied into an Admission Webhook Controller - we do generate a CRD from this. Just to point out though, that validation isn't just done at the user level but as a gating access point to the configuration management repo. There's many things that needs to be checked (that aren't integrated with kube) before we can blindly apply kube yaml to production. And there are other systems that need to be config managed from these manifests that benefit from this validation outside kube. I'm not saying this isn't a bit awkward, but moving to a world where every little resource has its own self-contained validation controller and crds within kube takes time. Particularly when the rest of your stack has other well-defined ways of doing things.
oooh i've wanted to mess with this forever. Note that Rust has support for _calling_ declared C variadic functions, so I've always wondered if you can hack in support for defining them by defining non-variadic functions in a macro and using link_name hacks to link them to a declared variadic functions
Makes perfect sense! Thank you so much for your explanation. I guess it's a shame that, as yet, the jetbrain IDEs don't support debugging MSVC binaries, otherwise it would just be a whole lot easier for me!
Is "lexical-algom" related to "lexical"? If so - in the following graph https://i.redd.it/3xx9aciugi421.png why does it have a spike on 20-30 digits? The spike is even bigger than 100 digits. Seems interesting 
Not necessarily. I've seen people leaving original posts un-corrected for a variety of reasons.
[mpeg2ts-reader](https://crates.io/crates/mpeg2ts-reader) includes a simplistic comparison of `mpeg2ts-reader`, `mpeg2ts` and `ffmpeg-sys` Transport Stream parsing throughput: [https://github.com/dholroyd/mpeg2ts-reader/tree/master/shootout](https://github.com/dholroyd/mpeg2ts-reader/tree/master/shootout) (I'm the author.)
Sorry, the placement new RFC has been [cancelled](https://github.com/rust-lang/rfcs/pull/2387). You might have better luck on /r/playrust
Wasn't Erlang's \`spawn\` function a function?
Hi. I'm trying to use Criterion to benchmark some code. My code currently takes about 2-3 seconds to run. I've got Criterion configured. When I run 'cargo bench' this is what it says after warmup \&gt; Collecting 100 samples in estimated 7334.2 s (5050 iterations) I can't for the life of me figure out how to configure Criterion. The docs don't provide any help. I think Criterion supports cmd line arguments... but I have no idea what they are? There is a "measurement\_time" function. But I can't call it because it moves the Criterion object which I only have borrowed access to. Thanks!
This is a thing I've been wondering quite a lot since writing `smash`, my hashmap implementation. `std` collections and non-trivial code tend to be exceedingly slow. Is this a general pattern across the entire standard library?
The results in that repo are outdated though as they require manual updating (so I haven't had the time/will to go through them :( ). Having something that can easily collate, compare and present results easily and automatically should be good to avoid this!
It would be better to use `System.alloc`, etc. rather than `libc::malloc`, so that your crate works on Windows too. Even better if you'd allow arbitrary `GlobalAlloc` implementations so that one could use e.g. jemallocator.
thanks for that, i guess whenever I see arrows my brain instinctively presses the arrow keys :S 
As far as I know, what you are attempting is not currently possible on stable rust. On nightly, it may be possible to implement using the specialization feature(s) (although I don't know the current state of those). &gt; Or even if I'm going down the right path to begin with. I think that, in general, the recommendation is that if you need a `BufRead` you should just take a `BufRead` and leave it to the caller to create one (as you already noticed, it doesn't take much code to create one). Alternatively you could provide a separate function, e.g. `MyThink::from_read(...)`, for callers who want to provide just `Read`.
Why wouldn't this apply to the Rust's main page?
Niiiice! Could you add them to https://github.com/rust-fuzz/trophy-case? Bragging rights aside, that list is very useful for evaluating what kind of issues actually occur in Rust code. Since you mention overflows, does that mean you're fuzzing in debug mode? Or those were resulting in out-of-bounds access as well?
`+` is also function (`erlang:'+'/2`). However this is NIF so this is built-in into language itself, not offered by OTP. So this can be viewed in the same category as `go` keyword in Go. 
I'm afraid you're in the wrong subreddit. This subreddit is for the [rust programming language](https://www.rust-lang.org/). For the game, see r/playrust.
No it's a very actively developed programming language.
No, it's not dead. - It's [very quickly growing](https://octoverse.github.com/projects#languages). - According to a recent [Stack Overflow survey](https://insights.stackoverflow.com/survey/2018/#most-loved-dreaded-and-wanted), it's most loved. - The recent 2018 release of Rust improved many of issues with Rust.
In most jurisdictions reverse engineering is explicitly allowed, so you can get someone else to look at the code and write documentation for you, and if you write code based on that documentation you're not bound by the original license. Well, IANAL, but that's what I've been told.
AFAIK shipping a correct implementation with a stable API soon was higher priority than producing the fastest implementation on the planet. So I would expect that to be the general trend. For example, the b-tree in Rust stdlib is already faster than binary trees you'd normally use in C, although it's likely that somebody somewhere has written a faster b-tree.
Given the sheer number of threads we see on it here, I would say probably not.
Happy holidays to you!
Quick question: If I compile the same Rust code 10 times (on the same PC, with the same version of Rust, etc.) will I get the exact same bit-for-bit binary every single time? Meaning the binaries will be 100% identical? Is that the case?
Writing your own allocators is fun :) I'll be upfront, if your goal is to produce an allocator that's faster in average than dlmalloc or jemalloc, you're in for a rude awakening: those are beasts. jemalloc clocks at in at ~20 CPU cycles per allocation in average. That's not a lot of CPU instructions/branches. On the other hand, there are valid usecases. Since you mentioned finance, for example, I had to write a specialized multi-threaded wait-free allocator for work because unpredictable pauses are a bane in latency-sensitive applications. In average, it's unsurprisingly slower than jemalloc for allocations, losing out in terms of throughput. Latency-wise, however, it's beautifully stable, both for allocations and deallocations. It was a fun exercise; although debugging the correct use of memory barriers/orders was "interesting".
I don't really understand why returning an empty Vec is not sufficient? An empty Vec does no allocation, so already have no overhead.
Empty vector case is fine, it's the non empty case that does an allocation to produce a vector which is unnecessary. 
This is pretty cool! One other solution I've used in the past is `Option::into_iter()` combined with `Iterator::flat_map` - you'd need to store your None or Some value in a temp variable before doing `res.into_iter().flat_map(|iter| iter)` but it's an otherwise simple solution.
Benchmarking is exceptionally challenging when the goal is to create a meaningful comparison. The analyst running a benchmark has to standardize conventions used on both sides. Variability driven by the operating system has to be managed. The conditions where a benchmark is run have to be carefully controlled. You have to describe the steps taken to compare apples to apples. How results are reproduced matter. 
I didn't think about \`either::Either\`, thanks for that! I have seen it being used in some futures code but never fully understood how to use it. Does \`either::Either\` automatically implements \`Iterator\` if one of his sides is an iterator, or is it the user's responsibility to implement it?
For float to string conversation. serde-json uses https://crates.io/crates/ryu for fast conversation.
If you read carefully, `lexical` is a crate coming with two algorithms for parsing: bigcomp (the default) and Algorithm M. This is why you have two entries in the graph: `lexical` is the default you get, using bigcomp, and `lexical-algom` is `lexical` with the `algorithm_m` feature on, using Algorithm M.
I will not sit here idle while you make practical design decisions to solve problems. You have to make do without a garbage collector, as the rest of us do. /s
Your comment doesn't deserve to be down voted into oblivion. Reddit is a terrible place for independent, and perfectly valid arguments that a group disagrees with.
Thanks. I think my stubborn C++ background keeps on making me try to mimic overloads in Rust all the time. :-) I think having a `from_read` and possibly a `from_bufread` would be the way to go. Or perhaps implementing `From&lt;T: Read&gt;` and `From&lt;T: BufRead&gt;`. I'd need to play a bit with that. The reason I was actually looking into this is because I was diving into an existing library that has a struct in a `read` module and a `readbuf` module, where the one in `read` is based on the one in `readbuf`. But in my mind that would cause many consumers of this library to naively use the one in `read` always, which would mean their perfectly fine `ReadBuf` based struct will be wrapped in a `ReadBuffer` for no good reason. So I wanted to make it 'better' somehow.
Exceedingly slow? Really? The performance of `std` is not "best-of-breed" for a variety of factors; most notably because performance is not the first criterion. It's probably not even the second. If I were to lay out which factors I wish `std` to focus on it would go: - **correctness**. - **clarity**: Clear algorithms are easier to maintain, and it's easier to ensure their correctness. - **portability**: Rust is a systems programming language, it should run on x86, ARM, MIPS, ... - **suitability**: The `std` library is used in for a wide variety of purposes, and should therefore suit a wide variety of workloads. Performance is nice, but not at the expense of those. For example, since you mention Rust `HashMap`: how does `smash` fair against Denial Of Service attacks? Taking a hint from DOS against web servers experienced by Python and Ruby, the default `HashMap` in Rust uses (1) a random seed and (2) a secure hash (SipHash). This is slower in average, but does not suffer a *performance cliff*, and thus remains suitable for the widest variety of workloads. And of course, beyond all those, there's also simply a matter of time. With correctness first, a naive but obviously correct implementation is always a good starting point; as long as the API is correct, the internals can always be improved at a later point.
Why plume in the first place? Looks like all you need is a mastodon instance focused in microblogging 
If you want faster `write!`, maybe [`fast_fmt`](https://crates.io/crates/fast_fmt) could help? It wasn't maintained for a while, but if you need something, I will merge PRs.
A NIF is still a function, which fits into the typesystem as a function and you can do with it anything you can do with a function (which is, in FP friendly languages, quite a lot). This is not a bad thing - this is a good thing, and most languages do it like this. &amp;#x200B; Go \*\*can't\*\* do it with a function, because they want the thread object to eventually return a result and since Go doesn't have generics that result will have to be \`interface{}\` which is too cumbersome for something as widely used as concurrency. &amp;#x200B; So Go had to make it syntax, and market green threads with a fancy new name to make this shortcoming of the decision to not have generics look exciting and innovative.
If you're unsure, you could write all your code to be generic over `T: Borrow&lt;U&gt;`, so that you can use it with `T` and `Box&lt;T&gt;`. Benchmark both to find out which is faster.
I cannot promise that the following information are accurate. I just searched the web with terms like "rustc deterministic" and "rustc reproducible" and haven't got a satisfactory answer. From what I've read, neither rustc nor cargo are deterministic. Here is a link to an (old Reddit thread)[https://www.reddit.com/r/rust/comments/5g5hib/reflections_on_rusting_trust/] (Ctrl+F "deterministic") about rustc. The cause might be hash tables used inside the compiler which use random hashing. Theoretically, compilers can be deterministic as they "just" turn an input string to tokens to an AST (abstract syntax tree) and/or several IRs (intermediate representations) to finally ASM (performing name resolution, type checking, ...). It's a composition of mathematical, hence pure, functions. Still, there are factors like name mangling, generated identifiers (from macro invocations) that *might* show up as debug symbols in the finally binary. But honestly, I am not qualified to state that as a fact. [Here another old thread](https://internals.rust-lang.org/t/verifying-rustc-releases-with-reproducible-builds/4502) about the non-determinism of rustc. Also, look at [this GitHub issue](https://github.com/rust-lang/rust/issues/50556) which suggests that cargo compiles dependencies in a random order (because of hash maps) which might be relevant to our question. [Apparently](https://github.com/rust-lang/rust/issues/50556#issuecomment-408600587), rustc uses a deterministic hasher in most cases. You can play around with this as well by alternating between the commands `cargo clean` and `cargo build` and compare the resulting binaries (`target/debug/&lt;binary&gt;`) with `diff` or `cmp`. So far, testing this on personal projects, output identical binaries. This reply should give you a short overview and could support your quest for a definitive answer because I don't know, too.
Naive question: are a few kb of stack size important? They are usually 1000x more important, so it should be less important than speed for most use cases. 
I guess I missed that. even though - I would really like to know why it was slower on those two iterations. Thanks 
Don't forget to include hashbrown. 
&gt;Nobody's asking for &gt; &gt;pervasive &gt; &gt; GC in rust. It's really good that you said this, as I and many others are very performance sensitive and having a pervasive GC in Rust the language would kill it (to be honest I was getting a bit anxious from these talks until I read that sentence). Maybe it should be called a "GC Library for Rust", since "GC in Rust" implies it's bolted onto the language itself.
Aren't these technically coroutines? I'm not sure I like this crate to be honest, it's very macro-ey for something that could be done by returning functions \`-&gt; fn(X)\`. How would you express \`deferred\` without macros in a concise manner?
You may also want to look at https://blog.anp.lol/lolbench-data/ 
I think `from_str_lossy` or `parse_lossy` might, since it's effectively a feature request that might have useful discussion (in fact, just from this Reddit post I have better ideas about some useful properties such a method might have). A small one, but one nonetheless. For improving the implementation of float parsing in libcore, you're absolutely right: this is a patch, nothing else.
It's likely because I use a "fast" algorithm for short input strings, and the fast algorithm is actually slower than algorithm M under 36 digits. You can see the logic that decides this [here](https://github.com/Alexhuszagh/rust-lexical/blob/cc8778384e6d77031b45aef2cb9cb831670573d6/lexical-core/src/atof/algorithm/correct.rs#L767), where `use_fast` returns if the number of significant digits is &lt;= 36 for decimal strings. This could be fine-tuned (it's a huge performance win for bigcomp, not so much for algorithm M), however, my overall goal was to avoid using arbitrary-precision arithmetic when it made sense to avoid it, so I perhaps naively didn't consider this important. Good catch, and maybe something I should fix.
Yeah, some of that does not translate well outside of the presentation. I talked about this in the section dissecting what happens in the save area(s) as you call \`va\_arg\`. &amp;#x200B; IMO the gist of "why you never should" is due to just how unsafe \`va\_list\` is. In particular: 1) CWE-234 - [https://cwe.mitre.org/data/definitions/234.html](https://cwe.mitre.org/data/definitions/234.html) 2) There are no type guarantees and mixing types can be more consequential than a cast (CWE-234 again). &amp;#x200B; I definitely see where you're coming from though. \`va\_list\`s are sort of like a necessary evil in my mind. I don't like how unsafe they are, but IMO they're absolutely essential to rust's FFI story and I don't know of an alternative.
I use it internally, by default. I originally had a Grisu implementation I ported, but I switched to using his Ryu code after making a [feature-request](https://github.com/dtolnay/ryu/issues/5) to ensure I could do so. David Tolnay is a magician.
Just for completeness, I'm linking the [report](https://fahrplan.events.ccc.de/congress/2011/Fahrplan/attachments/2007_28C3_Effective_DoS_on_web_application_platforms.pdf) that identified the denial of service attack. Also, changing a default hash function is quite easy (I come from a C++ background, and with MSVC's glacial hash function prior to MSVC2017, it was practically a requirement), and doesn't require re-writing the actual container at all. And, if you don't care about potential denial of service attacks, there are already crates for fast hash functions like [xxHash](https://github.com/shepmaster/twox-hash). I appreciate the commitment the Rust ecosystem has placed on correctness.
My program needs a dynamic circular double linked list with a cursor that doesn't need to be sharable between threads. It could of course simulate the circular requirement by simply moving to the back/front when hitting the start/end. Has anyone already written a crate for that? 
I fuzzed in debug mode for ~5 hours last night, and I'm fuzzing in release mode now (it's great to catch low-hanging errors with debug checks on, and then ramp up later to see if you can induce a crash or anything). I definitely will.
That is a really interesting idea. Definitely would be a good experiment. How would \`va\_start\`/\`va\_end\` be called? I started work on defining C variadic functions in rust a few weeks ago, so any tips/thoughts would definitely be appreciated. I'm currently trying to figure out the best way to generate the correct MIR and IR along with injecting \`va\_start\`/\`va\_end\`.
Generally not, unless you start declaring them recursively, especially since you're generally not recommended to make the stack size less than 64k (muslibc comes close, but glibc uses 2 or 8M by default I believe). But it's almost a principle not to declare large variables on the stack, and it might have real consequences at some point, so I'd rather feature-gate the worst offender if possible.
Broken link, correct URL: [https://ferrous-systems.com/blog/rust-analyzer-2019/](https://ferrous-systems.com/blog/rust-analyzer-2019/)
It might be a good idea to drop back down into debug mode afterwards, in case the fuzzer has discovered new and interesting inputs. They might not do anything interesting in release mode but actually trigger something in debug mode. Personally I tend to start with release mode and then drop down to debug mode once I have a corpus of interesting inputs generated. Also, if you need computational resources for fuzzing, I could give you $500 in Google Cloud credit on top of their $300 free trial.
Dammit! Sorry about that :(
You can't swap out glibc without rebuilding pretty much every other third party library that links against glibc, that means e.g. pretty much everything in /usr/lib
While the crate [is tested on Windows](https://ci.appveyor.com/project/bspeice/qadapt), I certainly agree that at least using `System.alloc` instead makes a ton of sense. I'll get that updated soon, thanks!
I wasn't aware of that. Is it possible to create a single binary that uses 2 separate libraries to communicate with kernel or there will be conflicts?
The problem is having incompatible global symbols coming from both libraries intermixed with e.g. structure and macro definitions from both libraries A glibc binary using a glibc macro for accessing the contents of a FILE* will explode horribly given a a FILE* constructed by a musl function, etc.
IIRC, Go uses a variable `GOGC` to determine the ratio of live to newly allocated, and scans the stack when allocations approach the ratio. It's mostly concurrent, so stack scanning is the only part of GC algorithm that stops program execution. https://golang.org/pkg/runtime/
I've been making some additions to the [indexlist](https://github.com/steveklabnik/indexlist) crate by u/steveklabnik1 for this exact use case. My [latest PR](https://github.com/steveklabnik/indexlist/pull/17) adds a cursor which can move to the previous/next element, and you could implement the circular requirement in your own crate.
Great post. I personally think nailing the IDE experience would have far more impact on attracting users than the ergonomics efforts; if users can have a great development experience with fast feedback, they will have infinitely more tolerance for learning the language. This should be one of the highest resourced efforts for Rust 2019. 
As you can see, I have basic familiarity with how to use fuzzers, but no real in-depth knowledge of how they work. This has been so informative, thank you.
It might ramble a bit but I figured I'd quickly jot down some ideas about Rust 2019. I might go back and add to it if anything else occurs to me.
Moved parsing for my command line todo app https://github.com/abhijat/ya2d2 to nom. Makes the parsers much easier to manage for me. Also added tab completion for existing entries for the pop command as well as a little asciinema demo. Will try to add support for tagging entries next so commands like ls can use them. 
I wonder, do you have any wishlist items for k8s itself in light of this? I might be able to help make those happen.
AFL and the fuzzers it inspired (libfuzzer, recent versions of honggfuzz) are actually pretty simple. https://lcamtuf.blogspot.com/2014/08/a-bit-more-about-american-fuzzy-lop.html provides a nice overview of how and why it works, and https://lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html describes the mutation strategies it employs, which are also very simple.
Oh, also I forgot to answer a part of your question last night: using Grisu or Ryu would not be advantageous, because it tries to create the shortest possible decimal representation of a binary float possible, rather than merely reproduce the exact digits after converting radices. This feature is great for humans, since it properly formats strings how we'd expect to read them (0.1 to "0.1", for example), but means that the resulting digits are rounded compared to theoretical result. We need to use a completely naive algorithm to generate digits for this.
Plume is like Medium, that was created by CEO of Twitter and they are complementary. What's odd in having the same decentralized using ActivityPub for federation? In general how can Twitter/Mastodon/microblogging replace blogging platforms like WordPress, Blogger, Medium, Tumblr etc etc?
I agree, but something like Conan, pkg-config, Maven, NuGet,... doesn't seem to be on cargo's roadmap.
Just from an amateur-bystander perspective, I think your README should be more up front about the security properties of `fxhash`. The biggest security hazard of code reuse is that someone will take an algorithm that's perfectly fine for many applications and drop it somewhere it doesn't belong. I mean, it's not like you're forcing anyone to be lazy and foolish or merely ignorant. But because your code is even more ready-to-use than `fxhash`, the chances that someone will put HTTP headers or IRC nicknames into it and get themselves lololtrolled are correspondingly greater. Maybe something like this: ---- ###Security `smash` is not faster if keys are intentionally chosen to cause hash collisions. This creates an opportunity for denial-of-service attacks, which can sometimes be leveraged for even greater effect. For example if you write an IRC server which keeps channel names as the keys of a `smash` table, it will be easy enough for an attacker to create many channels that are hashed to the same bucket. Then sending messages to those channels causes excessive CPU usage. If this creates enough latency or CPU to cause other servers to disconnect, that will cause a netsplit. IRC doesn't have the best security design, so a netsplit might be used to impersonate another user or to take over a channel. As a rule of thumb, data from network traffic - such as HTTP headers - should not be used as keys. Data from file types that could be downloaded (HTML, GIF, PDF, ...) might need a similar level of suspicion. Even file names could be a vector of attack. On the other hand, a program that only works for one user at a time or is known to handle clean data probably doesn't need the collision protection features of the standard Rust HashMap. `smash` is intended for that kind of non-secure use.
Why not have the None branch of the match block yield an empty hashmap of the same type as a_map?
Thanks for explaining and clarifying! I'm just a Rust noob and Rust is my first compiled language. That's why I have 100s of small little questions like this one. I'm really glad you included all the and especially the "buzzwords" (like deterministic, I didn't know that's what it is called) that I can just google if I really want to go in-depth on this topic. I just have two more follow-up questions: 1. Is this a goal of the Rust team? I mean is this something you'd want to have or does it not really matter? 2. What about other languages? Does C or C++ produce the same binary every single time?
James from Ferrous Systems here, I've added a redirect, so the link in this post now works :)
Hey, author of the post here, let me know if you have any questions, we're working to improve the state of IDE tooling in Rust. We'd love to get funding to accelerate this work!
The way you're doing argument processing could be massively simplified by using [structopt](https://crates.io/crates/structopt), just a tip.
Thanks a ton for the explanation :)
What is your strategy for debugging type errors? I have been looking at this one for several hours, and I really don't know how to make progress. error[E0271]: type mismatch resolving `&lt;std::sync::Arc&lt;vulkano::buffer::CpuAccessibleBuffer&lt;[[u8; 4]; 4096]&gt;&gt; as vulkano::buffer::TypedBufferAccess&gt;::Content == [_]` --&gt; src/main.rs:448:14 | 448 | .copy_buffer_to_image(img_buffer.clone(), img_img.clone()) | ^^^^^^^^^^^^^^^^^^^^ expected array of 4096 elements, found slice | = note: expected type `[[u8; 4]; 4096]` found type `[_]` (The whole program is [here](https://gist.github.com/kbob/85aa12865631029a059544d6d369ac7d). The error is on line 488. I think I declared `img_buffer` incorrectly, but I don't know what `vulkano::buffer::CpuAccessibleBuffer&lt;T&gt;.copy_buffer_to_image` is expecting. It looks like `TypedBufferAccess` is [implemented](https://docs.rs/vulkano/0.11.1/vulkano/buffer/cpu_access/struct.CpuAccessibleBuffer.html#impl-TypedBufferAccess) for `CpuAccessibleBuffer`. Now what? Rather than fix this one bug (though I would like that too), I'd rather ask: when you're looking at a type error like this involving half a dozen different traits from one or more unfamiliar libraries, how do you unravel what type you need to provide?
Hey, this is awesome! Great project and I’m looking forward to keeping up with the progress on it. In particular, I think the architectural approach you’re taking makes a lot of sense.
The reason Rust exists in the first place is to write browsers. Which have to talk with javascript engines. Sending messages doesn't help, you still need to handle ownership implications of your handles (that's also much slower). kyren isn't _integrating_ Rust with a garbage collected language, she's writing an interpreter for a GCd language in Rust. This is also something that should be doable. Even ignoring integration with other languages, there are use case for a `Gc&lt;T&gt;`-like type with Rust for dynamic graph-y datastructures. None of this is "reducing Rust to Javascript's level", there's no runtime being added or pervasive costs. This is no different from people opting in to using `Rc&lt;T&gt;` for a couple things.
Excellent explanation - thank you! I'll post back here if I run into roadblocks implementing the fetch API. I'm suspicious that someone more skilled in bindgen/web\_sys than I could make one easily; it's surprising one doesn't exist.
If the burden is low and the motivation is clear you can reasonably land new lib functions without an RFC since they're just unstable. Getting stabilized is the tricky part :)
Should someone open an issue about converting unsafe to safe code? What unsafe operations do you use? From a cursory glance I saw raw pointer access (which should probably use a slice).
I’d be more curious to see this parser compared to the parsers in fast json parsers like RapidJson. Those benchmarks are very competitive.
Dang, I never knew that existed, thanks for the tip
What are the advantages to static link?
Really nice work! Your approach seems to be the best long-term solution for Rust's IDE needs. Really hoping Rust can make some strides in the debugging/profiling area as well.
IIRC, static linkings advantage is that you can run the binary on any system that has the appropriate system libraries/architecture. Meaning you're not dependent on some dynamically linked library being on the machine of the end user. &amp;#x200B; Disadvantage: larger binaries, since everything is baked into the binary. This can be bad when you have many individual binaries on one system, all of which use some statically linked library, which in theory could also be installed once and then used by the binaries, resulting in filesize reductions when using it in a dynamically linked fashion.
Why does `from_iter` take an `IntoIterator` rather than simply an `Iterator`? The `collect` method only seems to exist on the `Iterator` trait, so I don't really see why `from_iter` would need to work with anything besides an `Iterator`. Presumably `from_iter` is always going to call `iter.into_iter()` anyway, or is that not always the case?
Planning on getting into Rust very soon, saving this for inspiration. :) Small, not related to code idea for improvement: do explain what your application _does_ in the README.
Other disadvantages: Cannot update a linked library except by recompiling the executable, which makes it harder to fix bugs and security problems with the library. This is also transitively true of the libraries these libraries depend on. Some libraries just aren't shipped by some distros as static libraries at all, making it impossible to use them in a statically-linked binary. Heck, some library codebases have no reasonable way to build a static library. The UNIX/Linux world has pretty much left static binaries behind, with a few small odd exceptions. It's really pretty much only an embedded thing now. (I am not a Windows person, but I have heard it's all still a mess there, some kind of DLL Hell if I understand correctly.)
Ahh right. Probably should do that. I’ll add a description under the build status. Thanks for the feedback.
Iterators also always implement `IntoIterator`, so this is strictly more powerful.
Thank you for the good starting point!
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [Making Rust Float Parsing Fast and Correct](https://www.reddit.com/r/programming/comments/a6r14p/making_rust_float_parsing_fast_and_correct/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Interaction of debugging and IDEs is actually an extremely fascinating topic! They have complimentary information: smart IDE knows all about static structure of the project, while debugger knows dynamic values of variables and such. And one can combine both sources of information to get awesome features. For example, to evaluate expression like `x.foo()` in debugger, one needs to know what trait does the `foo` method refers to. This is what an IDE can do easily (it is a compiler, after all), and what would be difficult to debugger. But an IDE can lower this expression to basically C-syntax and use debugger's API to evaluate that. Another cool example of a similar IDE/debugger interaction is the CLion feature where it shows the current value of the variable next to the place in the source code where the var is declared. The way it works is that IDE gets the name of the variable from the debugger, and then uses it's internal knowledge of scopes to learn where this name was defined. Here are [two lines](https://github.com/intellij-rust/intellij-rust/blob/4f315ea08e9a8366279f68a10d5e4b40e4dc9b3b/debugger/src/main/kotlin/org/rust/debugger/lang/RsDebuggerTypesHelper.kt#L32) from IntelliJ Rust which implement this bridging: `position` and `var` is info from debugger, which lacks Rust semantics. In the first line, we bind current line/column position to the actual Rust source code element. In the second line, we find the declaration of `var` at this position.
Random suggestions as I look through. When you have something like this: if arguments.file_of_configs().is_some() &amp;&amp; arguments.list_of_configs().is_some() { let mut config_list = arguments.file_of_configs().clone().unwrap(); config_list.append(&amp;mut arguments.list_of_configs().clone().unwrap()); ... You can often shorten it with an `if let` binding. Maybe something like: if let (Some(file_ref), Some(list_ref)) = (arguments.file_of_configs().as_ref(), arguments.list_of_configs().as_ref()) { ... Note the use of `Option::as_ref` to pull out a reference to the contents of an `Option` without needing to clone it and unwrap it. Another way to reduce the number of clones you need to make is to take something arguments like `list: &amp;[PathBuf]` (which lets the caller keep their Vec) rather than `list: Vec&lt;PathBuf&gt;` (which forces the caller to either give up their Vec or clone it). Clone are no big deal when you're talking about small lists of command line arguments, but it might start to matter when your lists get huge or when you use lists in tight loops. Cloning your lists is perfectly fine when you're trying to get some code working for the first time, since it sidesteps quite a lot of lifetime questions, but trying to reduce the number of clones you make later is also a good way to learn more about borrows and lifetimes and more idiomatic Rust code in general. In your `backup` function you're returning an `io::Result`, but you also `unwrap` some of the results you get from `Command::status`. (It might not've been clear that those are `io::Result`s, but in fact they are.) It would be more idiomatic to return those errors with `?`. Also, a subtle issue with subprocesses: Only a complete failure to spawn the process will result in an `Err` return from `Command::status`. If the child spawns successfully but returns an error status, that's still an `Ok` return as far as `Command::status` is concerned. If you want to return an error when your `cp` or `tar` commands fail, you need to explicitly check `ExitStatus::success`. A convenience library like [`duct`](https://github.com/oconnor663/duct.rs) (full disclosure, my library) will return `Err` by default when your children report failure, but the standard library routines will not. When you shell out to `tar`, you have this line: format!("{}.tar.xz", out.display()) That will work in most cases, but only when the path you're dealing with is valid UTF-8, which Unix in general doesn't require paths to be. When you have a path that isn't valid UTF-8, the `display` method is going to stick replacement characters in there (�). That might appear to work for a while, but it'll definitely cause different non-UTF-8 filenames to map to the same �-ful filename, which would be a confusing bug. Rust's `Path`/`OsStr` machinery can generally handle all these details, as long as you avoid coercing path data into a `str`/`String`. In this case, the `Path::with_extension` method might work well, otherwise you could dip into direct `OsString` concatenation.
Would controling more of the compiler backend make this easier? It seems like a lot of the type/namespace information is lost even in debug builds. More of this information could potentially be cached in debug builds with a Rust-specific debugger being able to read this. This could be an interesting thing to discuss with the inclusion of the Cranelift backend.
Probably? I haven't studied the debugging problem in depth, because I debug mostly with `printf`s, so for me personally static analysis capabilities of IDEs are more important :)
Static analysis is great! :) I've also been working on a general framework for static optimization, but it's still very early and I haven't had much time to invest in it lately.
Why hashmap of hashmaps instead of vec of vecs? 
Your post seems relatively pessimistic on the possibility of using rust-analyzer / libsyntax2 as the main rustc frontend ("Adding lossless syntax trees to rustc seems much more complicated though. Sharing macro expansion and name resolution hence seems quite challenging."). I was wondering if you could expand on why this is difficult. From my perspective it seems like the end-to-end on-demand architecture is something that rustc would also gain considerable benefit from. Especially around incremental compile times... In any case, I'm definitely behind this approach to IDE support.
Woah, those are a lot of awesome suggestions. You’re absolutely right about unwrapping all the `io::Result`s, didn’t know what I was thinking. You’re also right about making the list argument for `backup` an array instead of Vec&lt;PathBuf&gt;. But if I might ask, what is the difference from using `&amp;[PathBuf]` and &amp;Vec&lt;PathBuf&gt;`. Is is comparable to using a `&amp;str` and `&amp;String`? Definitely gonna try to fix up my code with your suggestions. Thanks again!
How does rust-analyzer handle chained/second-order completions? One part where RLS really falls down is when I'm typing at a rate greater than it's updating its model, or the update is blocked (I think because of an error). So if I do \`[foo.b](https://foo.bar)a\` I get a code completion for \`bar()\`, but if type that quickly, type another period, I have \`[foo.bar](https://foo.bar)().\` and no completions at all. &amp;#x200B; To me that's the real test for an IDE plugin, and it makes me very sad to say RLS fell short of my expectations by miles compared to TypeScript's language server protocol, or the Scala language service in IntelliJ.
I have no particular thoughts on this post yet, other than that boats is consistently wise and eloquent.
I understand, though that isn't of much use if `from_iter` is only ever called with an iterator. Would you ever want to call `from_iter` with an `IntoIterator` that isn't an iterator, instead of calling `into_iter().collect()`? Or am I reading too much into this and did the implementers not have a particular use case in mind for this generalization? I guess it was just a little surprising to me that `FromIterator` works with `IntoIterator` and not `Iterator`, which the name suggests. But I understand that this only concerns the type system and doesn't incur a runtime cost.
/much/ faster process startup
You're exactly right that `&amp;[T]` vs `&amp;Vec&lt;T&gt;` is analogous to `&amp;str` vs `&amp;String`. The reason to prefer the former over the latter in method signatures is twofold: 1. The owned types (`Vec` and `String`) will automatically dereference into the borrowed types (`&amp;[T]` and `&amp;str`), so callers who have the owned type can usually pass in a reference to it and expect that to Just Work. 2. Sometimes callers have static `&amp;str` or `&amp;[T]` constants, and it's nice when those work too. A function that takes a `&amp;[i32]` doesn't care whether it's caller has a `&amp;[i32]` pointing to static data baked into the executable (e.g. a literal like `&amp;[1, 2, 3]), or whether they have an `[i32; N]` on the stack, or a `Vec&lt;i32&gt;` on the heap, or perhaps some more exotic collection that's willing to hand out slices.
You're exactly right that `&amp;[T]` vs `&amp;Vec&lt;T&gt;` is analogous to `&amp;str` vs `&amp;String`. The reason to prefer the former over the latter in method signatures is twofold: 1. The owned types (`Vec` and `String`) will automatically dereference into the borrowed types (`&amp;[T]` and `&amp;str`), so callers who have the owned type can usually pass in a reference to it and expect that to Just Work. 2. Sometimes callers have static `&amp;str` or `&amp;[T]` constants, and it's nice when those work too. A function that takes a `&amp;[i32]` doesn't care whether it's caller has a `&amp;[i32]` pointing to static data baked into the executable (e.g. a literal like `&amp;[1, 2, 3]), or whether they have an `[i32; N]` on the stack, or a `Vec&lt;i32&gt;` on the heap, or perhaps some more exotic collection that's willing to hand out slices.
Yes it does. $ cat main.sh fn main() { println!("Hello, world!"); } $cat test_compile_hash.sh #!/bin/bash source $HOME/.cargo/env for ((n=0;n&lt;10;n++)) do rustc ./main.rs -o main$n sha256sum main$n done $ ./test_compile_hash.sh 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main0 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main1 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main2 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main3 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main4 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main5 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main6 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main7 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main8 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main9 
Yes it does. $ cat main.sh fn main() { println!("Hello, world!"); } $cat test_compile_hash.sh #!/bin/bash source $HOME/.cargo/env for ((n=0;n&lt;10;n++)) do rustc ./main.rs -o main$n sha256sum main$n done $ ./test_compile_hash.sh 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main0 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main1 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main2 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main3 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main4 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main5 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main6 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main7 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main8 56262bf31a7f682c0ef25544ced6602766248088ec181afc28a350f848fe3f69 main9 
How about `syn` vs. `rustc-syntax`? Not really an apples-to-apples comparison but both are concerned with parsing Rust.
Rapidjson does not have a correct float parser, by default, and falls back to strtod for a correct implementation. The analysis would be interesting, but not complete. My lossy parser also has a max error of 1 ULP, compared to 3 for Rapidjson, so a comparison would be somewhat limited. https://github.com/Tencent/rapidjson/issues/120
The entire back-end is unsafe, and use raw pointer ranges which should be changed to slices. I also use unchecked gets a lot, but these can be remediated. Definitely open an issue, I'm likely going to create a new branch that uses only unsafe code when required, and merge it back to master.
I definitely agree with the general thrust of this post. Something I want to point out specifically, and which withoutboats perhaps touches on in one of their anecdotes: it can be very difficult to be both a team leader and a primary implementer/designer. I personally think that the level of emotional (and other) energy required to be a good leader is often underestimated. And open source community engagement is probably even 10x of that. &amp;#x200B; (or perhaps, the toll is not underestimated; it's just that it's hard to change course once roles begin to solidify).
That makes perfect sense. Thanks for the explanations, it’s really helping me learn how to write _quality_ rust code.
That issue is 4 years old and out of date. RapidJson offers its own implementation of full precision double parsing. See https://github.com/Tencent/rapidjson/blob/master/include/rapidjson/internal/strtod.h But anyways, the larger point is that json parsers are where the fastest parsers probably reside. Look for the best performers for Canada.json, which is full of doubles. https://rawgit.com/miloyip/nativejson-benchmark/master/sample/performance_Corei7-4980HQ@2.80GHz_mac64_clang7.0.html#1.%20Parse (look at the _fullPrec variant for RapidJson in there) You can see conformance results at https://github.com/miloyip/nativejson-benchmark where RapidJson_fullPrec scores 100%
A common hack if you want to know the exact type of some expression `x` -- but you don't have a magical IDE that just tells you -- is to write a bogus assignment statement like: () = x; Then the compiler will spit out an error that tells you exactly what x is. For example: error[E0308]: mismatched types --&gt; src/main.rs:2:10 | 2 | () = "foo bar".split_whitespace(); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ expected (), found struct `std::str::SplitWhitespace` | = note: expected type `()` found type `std::str::SplitWhitespace&lt;'_&gt;` I don't know enough about vulkano to say more about this specific case though.
Wow. Best 2019 post yet IMO. 
Are there any crates for parsing and building HTTP requests in a no_std (embedded) environment?
And Erlang exists to write telecommunications systems. And C exists to write Unix... A Gc&lt;T&gt; sounds reasonable..
Thanks! Nice to know. I didn't realize that CWE was even a thing, I'm going to have to dig around in there for fun things. There are plenty of alternatives in principle, ranging from Rust's "macros sort things out at compile time" to Modula-2's "print() is overloaded for various numbers of arguments, all strings" (IIRC). In C alone though? Unless you're willing to basically make object vtable's, I don't know that a better option exists.
I agree completely with the tracking issues. I've subscribed to several, wanting to know the general progress, like `!` or `TryFrom`, but it exposes my inbox to a deluge of comments that frequently have nothing or very little to do with tracking the progress. On other side, many tracking issues don't seem to "track" the progress that well. It's usually hard to know what the next steps are for some feature. I wish the tracking issues were **only** status updates as to the progress of stabilizing a feature.
\*Chuckle\* That's one way of solving the problem ;) 
Your binary has no load/link-time dependencies. I don't know how often it is for people to run into these issues in professional software development but it's a constant pain for me. I have a development machine that runs a rolling-release distro, and machines I want to run those programs on that... aren't. For reasons (some of them decent ones). If I naively build then copy over a `--target=x86_64-unknown-linux-gnu` binary, one will discover that the machine I want to run on has an incompatible version of openssl, and that the GLIBC load/run-time check is gonna fire and tell me that I have the wrong version. --- I often seen this response to people (like me) who really want static linking &gt; But you dynamic linking lets you get/do security updates without recompiling! While this may be _technically_ true I'm not convinced it's helpful. For this system to work both of these must be true: * The security update is ABI-compatible; not only do the developers release an update that fixes security problems without breaking their ABI _as you use it_, the responsible fix isn't "Remove this symbol from our ABI, it was never a good idea." * You actually get a new dynamic library on the system in a meaningful time frame. I'm no security expert but the latest Ubuntu-provided version of `openssl` (1.1.0g) isn't available on the openssl website, and instead openssl offers a newer version (1.1.0j and 1.1.1a) and mentions "bug and security fixes". Is your app going to be exploitable for a long time until Ubuntu ships you a fix? Could you do better?
When trying to use linux as a desktop machine, the fixation on not using static libraries, even for client applications (word processor, game etc) is probably the biggest source of user frustration. 
Many of these wheels never see the highway but it doesn't mean that nothing was learned by creating them. Good ideas spread across projects.
Oh I didn't see that it had finally implemented a correct parser, that's good to know. I haven't used RapidJson in ~3 years.
\&gt; So my proposal for Rust 2019 is not that big of a deal, I guess: we just need to redesign our decision making process, reorganize our governance structures, establish new norms of communication, and find a way to redirect a significant amount of capital toward Rust contributors. :P I do agree with most if not all points though. Growth may be a first world kind of problem, but it still needs to be solved.
&gt; The UNIX/Linux world has pretty much left static binaries behind, with a few small odd exceptions. I mean, except for the whole container craze, which is conceptually static binaries on steroids.
\&gt; it can be very difficult to be both a team leader and a primary implementer/designer Beyond emotional involvement, I am afraid there's also a simple matter of \*\*time\*\*. Leading a 2/3 persons team can possibly be done "on the side", leading a 10 persons team is getting close to be a full time job, leading a whole area like "libs" or "compiler" is likely to be more than a single individual can accomplish without delegating. The current team leads have been doing a great job, however I think at some point they will need to choose between "leading" and "doing". There's no shame in not leading, many organizations have "Distinguished" or "Principle" Engineers whose role is strictly \*technical\*. And there's no shame in leading; Linus himself has often noted he rarely if ever coded in recent years, spending more time discussing high-level design instead. 
That said, libc symbols could be renamed, especially in Rust, where source symbol name doesn't have to match linkage name. But you'd have mindful of the possibility of having multiple libc's in your process: be careful when calling out to libc via ffi, be prepared to debug strange linking errors, and so on. Given that libc.so will always be present on a Linux system, the potential problems aren't worth the benefits for most people.
We should probably limit RFC to line-comments only in order to have comments sorted by threads. Also, inactive comments or those with little relevance should be closed lest the person who left it is willing to provide more detail or maybe offer proactive insight into how the issue could be resolved. Also, all RFCs should probably have some way to signal how close to consensus they are so that people have actionable threads that they can try to resolve. Generally, it should be easy for any maintainer or Rust enthusiast to constructively move the RFC forward. Anyway, great post! Excited so hear where this conversation will lead.
I don't know about you, but if I ever inherited a project that attempted this I'd be ripping it out with extreme expedience, and possibly aiming to deprecate the whole thing, because choices like that aren't made in isolation!
I’m hoping to review your PRs soon by the way! Thanks for sending them in.
It’s a goal, though not a super high priority one. We gladly take PRs to fix this kind of thing. Other languages can do this too, but it’s not really about the language. That’s necessary but not sufficient. Your program needs to support these kinds of things too. Time stamps are a common example. See https://reproducible-builds.org/ for more.
I feel like the github issue format isn't ideal for a tracking (unless you are actually involved in the implementation / reviewing). Something like http://chromestatus.com might be easier to follow...
It automatically implements `Iterator` if both sides implement `Iterator` and yield the same type of items.
Attempts at building a proper platform for a structured discussion of pros and cons for a particular solution have been made. See https://www.kialo.com/ and a few others like it.
I don't want to sound harsh, but Rust's development is absolutely driven by whoever is paying money to keep developing it, and whoever it is, they are not looking for "unconstructive" ideas that go against what they've decided to do. It is the routine way to tell somebody "I made the decision to do X, and I'm not turning back".
Maybe you can create some donate page on your site and track list of donated people? I'm sure a lot of rustaceans will support this project.
This is definitely an ambitious post! I agree with some chunks of it. I think the thing I would most like to see happen from this post is a stronger focus on professional-level conduct in Rust spaces that are intended to serve as a way to get work done (e.g., the issue tracker). "drinking from a fire hose" is really the perfect analogy, and couldn't be more true. I don't really know how to achieve it though. Even if we found a way get folks to buy into a higher standard of interaction in work spaces, you'll always have the problem of new users joining that don't yet know or appreciate that higher standard. I don't know how to fix that without tooling support in the spaces in which we communicate, or otherwise continually remind new folks what the standards of interaction are. I do the latter on some of my more highly trafficked issue trackers, but the incidence is very very low. I'm not sure how well it scales. But it's worth trying something I think!
[there is a workaround](https://github.com/rust-fuzz/afl.rs/pull/144/files) for the bug you've mentioned
/r/playrustservers
I read "we want to look at what happened and then make the situation better in the future" as essentially "let this go, we're not going to change anything now". It does nothing to resolve the current situation, and it's really the standard "community management" thing that people do to make it look like they care. I've seen this happen many many times in corporate environments. Maybe there's a textbook somewhere teaching this, that's what it feels like.
Thank you for taking care of this. IMO this is currently the most important issue, but unfortunately it doesn't get the attention it deserves.
musl doesnt have certain features (for example no dlopen support) and so can't be a full replacement to dynamic linking.
It's kinda weak-pointer if you won't reuse ids :)
What's root? We are talking about arbitrary graph structures here, not trees.
I'll investigate `message`, since I'm already on nightly for like 3 other reasons as well
That's why open hardware and independent foundries are important. Ideally it's possible to build a fast and very simple CPU that doesn't have any complicated branch prediction logic, and then use that to bootstrap your process, publishing it and using as many eyes as possible to make that $5 solution useless.
Good to hear.
Let me unpack this a bit! Long term, I think having a single frontend which works well for both IDE and rustc is a very desirable and achievable goal. However, I'd rather approach this long-term goal by birding implementation which works good for IDE with current rustc, rather than starting with the current rustc and morphing it towards IDE. I feel that the second approach has a high risk of building something which is easy to add to rustc, but does not necessary works best for IDEs. To give some concrete examples, currently `rustc` works in a "crate at a time mode". Changing that to "a dag of crates, some of which are often modified, some of which are rarely modified" is going to be a lot of work, which won't make `rustc` better by itself, but which would be a per-requisite for IDE compiler. Another example is syntax trees: currently rustc, rightly, uses abstract syntax trees: they are faster &amp; more memory efficient. For IDE, we need concrete (or lossless, or full-fidelity) syntax trees, and refactoring rustc to use them is a significant amount of work, without immediate benefit. What's common between two examples is that, due to differences on architectural level between a IDE and a batch compiler, there isn't really a series of small incremental steps to morph one into another. And to dive-in into a huge "atomic" refactoring, we'd better make at least a proof of concept that the end result of the refactoring will indeed fit the IDEs. I also hope that we can dodge "morph X into Y" steps altogether by extracting bits of rustc/IDE-compiler into reusable libraries. Another angle to look at this is in terms of incrementality of development. With a separate implementation, you can develop incrementally in terms of language coverage: first you get parsing right, and do completions based on parsing, then, you add `use` resolution and can complete paths, then you add macro expansion, type inference, etc, slowly growing the set of features you can do in IDE-fashion. With "morphing existing compiler" it's really all or nothing game: either you support all features of the language or none of them. This actually was a pretext for my switching from contributing to RLS to hacking on rust-analyzer: I needed a syntax tree to implement a particular feature, and RLS still lacks access to syntax trees, while in the from-scratch implementation it would be the first bit you implement.
This is an example from the TRPL about generics: struct Point&lt;T&gt; { x: T, y: T, } impl&lt;T&gt; Point&lt;T&gt; { fn x(&amp;self) -&gt; &amp;T { &amp;self.x } } My questions: 1. What is the `T` called? generic parameter? generic placeholder? generic type ...? 2. Why do I need to specify the `T` for `impl`? Is there ever a impl with a different type? Like so for example: &amp;#8203; impl&lt;X&gt; Point&lt;T&gt; { ... } &amp;#x200B;
RLS uses racer (a mostly heuristics based too) for code completion because its impossible to fit code completion properly into the current rustc architecture. Rust analyzer does not do such completions at all, but for another reason: completion after dot needs type inference, which is not implemented yet, at all. However, I am pretty confident that, based on rust-analyzer approach, you can add type inference which will work both robustly enough and fast enough to power code completion. 