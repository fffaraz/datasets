&gt; Also, since I'm out of the loop apparently, which forums and chats are people using for Rust? Well, theres this subreddit. The discord is also helpful, along with the forums on the website, and all the glitters. and all the ircs. subreddit sidebar.
&gt; C++ &gt; &gt; And copy constructors happen invisibly at many places ‒ like when passing something as a result out of a function. C++ has some conditions where copy elision/RVO is used, meaning that an object is neither moved nor copied out of a function, but instead constructed directly in the caller. When copy elision does not kick in, C++ tries the move constructor first and only then falls back to copying.
I'm a bit confused - why does the author mention implementing a usermode? Unless that itself is going to run in Ring0 as well?
&gt; After all, many programmers of modern C++ don’t touch raw pointers either ‒ they know these are dangerous even without the unsafe warning signs. *Owning* raw pointers are avoided. Non-owning raw pointers are just fine. See for example slide 5 of [Essentials of Modern C++ Style](https://github.com/CppCon/CppCon2014/blob/master/Presentations/Back%20to%20the%20Basics!%20Essentials%20of%20Modern%20C%2B%2B%20Style/Back%20to%20the%20Basics!%20Essentials%20of%20Modern%20C%2B%2B%20Style%20-%20Herb%20Sutter%20-%20CppCon%202014.pdf).
Really excited to see where this goes! Glad somebody sponsored your work.
It's certainly isn't; The future for Nebulet is quite bright!
&gt; Yeah they suck. I have to look them up pretty much every time I touch Rust code. AFAIK this will be fixed in the 2018 edition. &gt; Having done both, I can confirm this. All of it. My experience is that people who struggle to understand how to write Rust code that works with the borrow checker efficiently instead of code full of workaround and hacks to avoid the borrow checker will have problems with reading and properly understanding Rust code in general, and especially C++ developers get frustrated because in C++ becoming somewhat productive with only a portion of the language works, in Rust, not so much. But Rust's approach has a huge benefit, although everyone is "forced" to understand more idioms, because these idioms interact with each other more ergonomically, it's much easier to just use other peoples crates, the library ecosystem's code is much more uniform, the feeling of "I have to learn yet another obscure C++ language feature to use this library" is much less common. 
Isn't object identity in c++ technically defined by the path? That is, if you access a bit of memory via an oob array access then the optimizer doesn't have to care about aliasing? Though that only matters for undefined behavior, I think. 
With `cargo install -f crate-name` (with crate-name being the name of the crate you want to update). You can also use [`cargo install-update`](https://github.com/nabijaczleweli/cargo-update) 
Thanks, I'm excited to be able to continue working on it!
I see multiple problems in your approach. You assume that Rust works similarly to C++, and you expect that in Rust putting a function into another source file is the simplest thing in the world, so you also expect an example right away showing this simplicity. Instead the book explains that Rust doesn't work like it seems to, putting a function into another source file requires the usage of modules without using the `mod` keyword in the module's file. All of these are explained (from the simplest use cases to yours) in chapter 7.1 "mod and the Filesystem", the chapter you incorrectly assumed is about crates, and not the module system.
A usermode doesn't necessarily have to run in ring 3. I've taken to calling it a "sipspace", but that's confusing if you don't know the terms, so usermode is more useful.
I guess the important word there is *technically* ‒ which might be true. When writing the formal definition of a language, the authors/committee must choose some wording that works. But I was trying to write about what the human-intuition principles the technical writing describes. I must admit, I don't know exactly how these things are defined, not in C++, nor in Rust. But identity defined by path seems a bit odd in some sense ‒ if I allocate an object in C++ with `new` and then pass the pointer to it around a bit, the path changes, but it is still considered the same object.
Nice work putting that together! I must admit I rarely, if ever, rebuild from scratch, if you have tips for incremental builds it would be pretty useful too.
For avoiding LTO except for release builds, we use the following Cargo.toml: ``` # Give default profiles higher optimization levels to avoid extreme CSV load times. [profile.dev] opt-level = 2 [profile.test] opt-level = 2 [profile.release] opt-level = 3 lto = true ```
Great article! The car analogies really help internalize a practical understanding of the concepts. :D 
Fwiw I don't use any of this at the moment and I am doing fine. Of course things get weird if you aim for performance. But I wouldn't consider caching a particularly bad "trick"
What's the benefit of using sccache over relying on Rust's built-in incremental compilation?
Nice article. Of particular interest is the piece on not pulling in optional features of dependent crates if you don't need them.
Understanding computer systems is kind of a prerequisite to understanding operating systems. Both of the books are very hands-on (check out their companion sites too). Tutorials are more useful if you already understand the fundamentals, but they can also motivate learning the fundamentals. So tutorials and books can be used at the same time.
Incremental compilation reuses an already compiled part of your project when you make changes to a different part _of the same project_. That is, if your project depends on clap, and you do not change clap, it won't be recompiled. `sccache` caches compilation in the whole system, that is, if you have N independent projects, all of which depend on the same version of clap, then without `sscache` clap would be compiled `N` times, but with `sccache` it is compiled just once.
One word of caution: [On the dangers of Intel's frequency scaling](https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/), a 2017 article from Cloudflare, illustrates that while AVX2 instructions can speed up a benchmark, they might actually worsen a whole program performance. The gist of the article is that AVX2 instructions consume so much power that the cores they are used on heat up more than usual. To protect them against a meltdown, a mechanism is in place to downclock part or all of the cores on the die, so that improving the performance of one core by 33% (here) may actually worsen the performance of all other cores. Also, it's to be noted that the downclocking itself neither starts instantaneously (it takes time for the cores to downclock, so a pause is noticeable) nor ends instantaneously (whether downclocking is warranted is re-evaluated every so often). As a result, AVX2 instructions are best used in batch.
&gt; BoringSSL code does not use AVX2 multiplication instructions for Poly1305, and only uses simple xor, shift and add operations for ChaCha20, which allows it to run at the base frequency. Super interesting. Eyeballing Samuel Neves' code and my clone of it, it looks like there are no multiply instructions. Maybe that's deliberate? It would be easy enough to add a function to control which implementation you get, if there are callers who want to use it. (There's already an undocumented function `benchmarks::force_portable`.) It might also make sense to port over the SSE4.1 implementation from libsodium, if there are callers who actually get better results from that one despite supporting AVX2?
Great work! Currently I will not be able to do a review, but will try to look into it later. BTW would you mind if I'll use your code as a reference for improving [`blake2`](https://github.com/RustCrypto/hashes) crate? (with the relicensing under dual MIT/Apache license) It of course would be great if you'll find time to contribute as well, but current version needs a bit of rework for migration to `target_feature`, so it's not the easiest code to get into.
I'm browsing Reddit while waiting for my code to compile...
That sounds interesting! In a good way.
On the other hand, I'm more likely to contribute to it or at least spread the word about it because its license.
As far as I can tell sscache is pointless for iterative development. We tried to for a few weeks and it made iterations slower rather than faster.
Do you mean you're in high school? Whoa :o
Absolutely use it! The AVX2 code might be useful, but I'd also be interested in seeing more implementations expose the associated data features. (There's one obscure feature in particular, the "last node flag", that you can set even after you've read some input bytes.) I thought about splitting the `Params` object out into its own crate, so that other implementations could use it, but then I didn't want to complicate things.
I love this so much I wish it was part of the docs. Great work!
Definite pain point is using C++ lambdas as callbacks and passing them to Rust through FFI. I had to do that last week, it's certainly a headache, especially when mixing C++ GUI toolkits with rust crates. 
You would need reproducible builds as a prerequisite. Otherwise how do you know if someone poisoned your cache?
There's talk of adding an LSP plugin. It's on their radar for sure.
If `pNumberOfBytes` is being passed by pointer instead of by value, that means that it probably is writing something to it, not just reading it. /u/the-kenny's suggestion is how you pass that value in, but you should also try to figure out whether you need to do something with the value of the local variable `len` after the call.
I would definitely try it, and not just package it blindly. I already had this use case several times, and used simple-http-server until now, but the next time I will try out thumbcloud. From what I have seen taking a quick look at the files I assume the licenses are OK. It just would help a lot if that this was documented somewhere in the LICENSE or COPYING file, because right now one would have to know what project they are and look them up. I also think attribution is a requirement for MIT code, so we would have to include the attribution of the upstream projects.
Without showing us the code, or naming the crates and protocols that you used to implement your client-server it's unlikely you will receive the help you wanted. --- And if you have no idea what I'm talking about; it may have been a good idea to read the sidebar and head over to r/playrust instead.
So, just read the Book straight through?
I don't think there was such aggressive frequency scaling for SSE4.1 or AVX (256 bits), so it may indeed be interesting to offer them as alternatives even for AVX2-capable CPUs.
Fun fact, the reason that C++ doesn't have 0-bytes data-members is specifically because like in C each object should have a distinct address^1 . This is also the reason why distinct functions have distinct addresses even if their implementation is byte-for-byte identical, which has led compilers to develop trick to nonetheless merge functions (using NOP padding at the beginning), etc... All those work-arounds for a property of the language which is rarely if ever used in practice :/ At least, for data-members, C++20 introduces the [`[no_unique_address]`](https://en.cppreference.com/w/cpp/language/attributes/no_unique_address) attribute to avoid having to leverage EBO and meta-template programming tricks to achieve the effect. ^1 *A specific optimization, the Empty Base Optimization, is permitted by the language.*
If I implement the `Drop` trait for a struct, does this mean that its destruction will be automatically handled (i.e. something will call `drop` upon program exit/panic), or does `drop` have to be explicitly called?
Why not supporting licenses like LPGL then? I personally don't understand why people label the GPL 3 "free as in freedom", considering it forces anyone using that llibrary even by linking to completely change your entire project license too. I am more than happy to contribute to something that helps me being more productive or solves a problem more efficiently than other's implementation, but if I can't use the library myself, don't expect me to support or suggest it to anyone else. 
I started reading through it a few days ago. So far, I find the examples used to illustrate concepts are well done and easy to wrap your head around.
This is amazing! It'd be so cool to get this integrated with rustdoc.
When the variable goes out of scope, `drop` will be called. You can also explicitly call `std::mem::drop` on the struct. By default, when a struct is dropped, all of its fields will also be dropped, even if you use a custom `Drop` implementation. Only use a custom implementation if you want to do something else (e.g. close a database connection, write redis db to disk, etc.) As an example: ``` struct CustomDrop; impl Drop for CustomDrop { fn drop(&amp;mut self) { // stuff println!("Being dropped!"); } } fn main() { let cd = CustomDrop; println!("This will be printed before CustomDrop is dropped."); // cd goes out of scope (end of function call) } // =&gt; This will be printed before CustomDrop is dropped. // =&gt; Being dropped! ```
I thought for sure this was a post for r/playrust (based on the title) and was going to be referring to some serious heavy-weapons. Pleasantly surprised that that wasn't the topic!
Yep. Rust Book -&gt; Rust by Example -&gt; Rustnomicon if you're up for it.
While this was valuable information, it's still not clear when to use build rather than check. Thoughts?
lol, same here. Was all set with the standard r/playrust redirection message. Neat article! I'm not going to pretend that I understand this super well on a first pass, but that's exactly why I'm happy to see it. It's always good to be exposed to things you aren't comfortable with. :D
This is fantastic!
That being said, It actually would be cool if Rust had BFG's in addition to BFGS! But, I guess you can kind of think of the borrow checker as the BFG of Rust!
Use `cargo check` during development to make sure you are not forgetting to import crates and modules; to verify if you got the syntax of a complex `where` clause correct; to ensure that your new method is well-typed; etc. When you are done hacking and coding and want to generate an actual program that you can run, use `cargo build`.
&gt;How to alleviate the pain of Rust compile times I use weed
However note that sometimes, the turbofish doesn't work and you have to explicitly annotate the type (I don't have an example on hand right now.)
Curious, how do you work with grapheme clusters? Or is there a way to combine the `a` and the combining diaeresis together into an accented `a`?
In my testing, wasm doesn't require a release build.
I wouldn't do that, just read the first few chapters so you have a basic understanding of the language. Then try writing stuff and when you need more specific knowledge, read those parts of the book. I myself couldn't read the full book and keep attention/stay motivated. 
You might find this easier to understand than the book: https://learning-rust.github.io
Yep! I'm headed into grade 11 come fall
Making each line of code do one thing is not 'excessive refactoring'. And in any language where you don't already do this, you end up having to do it as soon as you want to debug it or trace it, because you want to log the results of individual steps and/or insert breakpoints between individual steps.
Some of this is stuff that you'd need to be aware of one way or another. Like some optimisations having a big impact on compilation time. That's fine. I agree with your sentiment though that we shouldn't be needing tricks to cut down on compilation time. Use `cargo check` over `cargo build` is one that is especially silly for various reasons, and `cargo check` is still pretty slow anyway in the grand scheme of things.
That's impressive!
I skimmed the whole thing, trying a few bits as I went. Then I went back through properly. It's a good read and covers enough that you can do a lot of stuff with just the info it contains. After that I found [this tutorial](http://nercury.github.io/rust/opengl/tutorial/2018/02/08/opengl-in-rust-from-scratch-00-setup.html) quite useful. On the surface it's about using Open GL, but it covers a lot of stuff like project structure, loading resources, build scripts and using unsafe code. I just finished reading [this](http://cglab.ca/~abeinges/blah/too-many-lists/book/README.html) tutorial on linked lists. The lists themselves aren't that interesting, but it does a great job of explaining lifetimes and overcoming the various pitfalls you encounter with them.
That neural network to detect if a /r/rust post should be redirected to /r/playrust would surely be confused :3
Quite the opposite. That makes it worse, and can make certain optimizations less effective due to the non-locality. In theory, the most compiler-friendly codebase possible is one that's just a single enormous file that contains absolutely everything.
&gt;Oh really? Than why the pipeline operator became so popular in modern functional languages? It isn't. &gt;And most people are still ignorant of it after so many years. For the same reason most people are ignorant of large cardinal theory, constructive topology, diophantine geometry and geometric algebra: category theory is a *particularly* abstract and irrelevant part of very academic mathematics. There's no reason at all for any programmer to have any interest in category theory whatsoever. There's a common theme of Haskell programmers thinking that calling something a 'monoid' and something else a 'monad' means they are programming 'in category theory' or are 'doing category theory'. They aren't. And they don't know category theory. I doubt they even know what a limit is. The reality is that people don't care about category theory because it's a fucking ugly and completely irrelevant area of mathematics that nobody really likes. It abstracts away all the interesting parts of what it studies and looks only at the boring bit everything has in common. &gt;In reverse order. Because to get the result of f(g(x)) you have to compute x first, then g(x) and then f(g(x)). Again, no more so than your suggested `+ 5 * 3 8` is 'in reverse order', given that 3 * 8 happens before 24 + 5. And no more than 5 + 3 * 8 is 'in reverse order'. It's not in reverse order. It's **outside to in**. This is perfectly logical and understandable. Nobody has ever had any trouble understanding what is going on if they had any experience at all. 
Wouldn't it work if T = &amp;S for some S? That's what I meant by "whenever possible".
I'm not talking about speed of the generated binary, I'm talking about compilation times. Also, wouldn't a single large file be terrible for incremental compilation?
One common source of larger than necessary compilation times are the integration tests. If you have tests/ one.rs two.rs three.rs Than each one of those tests gets compiled as a separate binary, which links in a separate copy of you library, and that can add up to a significant space and time overheard. Most of the time, it is actually possible to have a single integration test with many modules: tests/ all/ main.rs // mod one; mod two; mod three; one.rs two.rs three.rs
You may want to check out Dyon:[https://github.com/PistonDevelopers/dyon](https://github.com/PistonDevelopers/dyon)
Well. The second one is not entierly true. It tries move constructor for rvalues.
Strongly-typed is not the opposite of Dynamically-Typed. In fact, it is an independent axis, like this: Weakly-Typed ^ | Statically-Typed &lt;------------&gt; Dynamically-Typed | v Strongly-Typed So, Dyon is a Strongly-Typed, Dynamically-Typed language (like Type-script compiled into JS)
This is a good question! "Useful" is an interesting word here. In many settings, a local minimum instead of the global minimum might be good enough. E.g., maybe I don't care whether my classifier is 68.4% vs. 68.6% accurate. In some situations (e.g. early stopping for neural networks), it is even desirable to stop before reaching any minimum at all. So, an algorithm can still be pretty useful even if it doesn't converge to a global minimum. BFGS will often converge to one of the local minima. A lot algorithms falls into this category; unfortunately finding the global minimum is incredibly hard! To maximize the chances of finding the global minimum, we often run algorithms multiple times with different randomized starting points. Hopefully that's kind of a coherent answer.
I am interested, thanks!
&gt; No, as mentioned, incremental compilation does not cache clap (or any other &gt; dependency) across multiple independent projects; sccache does. His point was that incremental build will only cost one single extra build per independent project. Since after it is compiled the first time in each project, incremental build will have it cached. So all that sccache buys you, is avoiding that first one-time build in each additional project. after that, you're doing no better than what you'd get with the build in incremental build.
&gt; I'm not talking about speed of the generated binary, I'm talking about compilation times. I was talking about both. &gt;Also, wouldn't a single large file be terrible for incremental compilation? Possibly, but you wouldn't need incremental compilation in the first place then. Notice I said "in theory", too. My point was just that in general, the extreme modularity of the Rust ecosystem is unfortunately exactly the opposite of the sort of input you'd want to give a compiler (for any language, not just Rust) if you were trying to reduce build times as much as possible. So yeah, you had asked if splitting things up helps and like I said, no, it will almost always do the reverse. The more individual dependencies you have, the slower things get overall.
This is quite awesome. Maybe you could include the code generated by each rule in some way (although I can't think of a good way to display it). I tried the live demo briefly and it failed to parse [this huge macro](https://gitlab.com/jD91mZM2/jrust/blob/master/src/lib.rs#L1-269), so you might want to look into this. But still, great job!
While harvesting examples I realized there are a lot of macros that try to implement (or "simulate") some syntax, yet do it in slightly different ways, usually with subtle errors or quirks. One simple example is the `verify`-exercise in the [WASM-demo](https://lukaslueg.github.io/macro_railroad_wasm_demo/), which is just bad documentation. There are quirks in the `convert_args`-macro, where it has this weird way of trying to simulate a macro-call (with an infinite number of commas). If you [look at](https://htmlpreview.github.io/?https://github.com/lukaslueg/macro_railroad/blob/master/examples/various_examples.html#cfg_if) the `cfg_if`-macro, you notice that `cfg_if!(else {})` is [valid syntax](https://play.rust-lang.org/?gist=6f359dc5a6cb64950b9b2c7ae631e00f&amp;version=stable&amp;mode=debug&amp;edition=2015). 
Holy spaghetti. I've just dumped in this clusterfuck macro_rules! join_streams { ($($stream:ident($stream_item:pat) =&gt; $block:block,)*) =&gt; { join_streams!{$($stream($stream_item) =&gt; $block),*} }; ($($stream:ident($stream_item:pat) =&gt; $block:block),*) =&gt; { #[allow(non_camel_case_types)] struct JoinedStream&lt;$($stream : Stream),*&gt; { $($stream: $stream),* } #[allow(non_camel_case_types)] enum JoinStreamEvent&lt;$($stream),*&gt; { $($stream($stream)),* } #[allow(non_camel_case_types)] impl&lt;$($stream : Stream),*&gt; Stream for JoinedStream&lt;$($stream),*&gt; where $(io::Error: From&lt;&lt;$stream as Stream&gt;::Error&gt;),* { type Item = JoinStreamEvent&lt;$($stream::Item),*&gt;; type Error = io::Error; fn poll(&amp;mut self) -&gt; Poll&lt;Option&lt;Self::Item&gt;, Self::Error&gt; { $({ match self.$stream.poll() { Ok(Async::Ready(Some(item))) =&gt; { return Ok(Async::Ready(Some(JoinStreamEvent::$stream(item)))) }, Ok(Async::Ready(None)) =&gt; return Ok(Async::Ready(None)), Ok(Async::NotReady) =&gt; (), Err(e) =&gt; Err(e)?, } })* Ok(Async::NotReady) } } $(let mut $stream = $stream;)* loop { let mut s = JoinedStream { $($stream),* }; let (event, rest) = await!(s.into_future()).map_err(|e| e.0)?; $($stream = rest.$stream;)* match event { None =&gt; break, $( Some(JoinStreamEvent::$stream($stream_item)) =&gt; $block ),* } } }; } that I had laying around. It's beautiful now... Anyhow: if you put this one above in, I notice that optional trailing comma's aren't somehow represented/optimised. Would be a nice addition, if at all possible.
C++ tries the move constructor for lvalues as well: &gt; If expression is an lvalue expression […], then overload resolution to select the constructor […] is performed twice: first as if expression were an rvalue expression (thus it may select the move constructor), and if no suitable conversion is available […], overload resolution is performed a second time, with expression considered as an lvalue (so it may select the copy constructor taking a reference to non-const). [cppreference.com](https://en.cppreference.com/w/cpp/language/return)
If you look carefully, the commas can't be folded any further. In one arm the comma has to be there, even if no repeated elements follow. In the other arm the comma can be there, yet if it is, you have to repeat the group.
Local minima is thought to usually be good enough: https://stats.stackexchange.com/questions/203288/understanding-almost-all-local-minimum-have-very-similar-function-value-to-the/203300#203300
I've opened [an issue](https://github.com/lukaslueg/macro_railroad/issues/5) to keep track of this.
Hmm, poly2tri is purely mathematical. lyon seems to have rendering capabilities as well. This is what I'm after in the end, so I might consider just using lyon instead, thanks for the tip.
In machine learning, you don't even want the global minimum on your training set: that would be trivially achieved by memorizing the examples, and most deep neural networks have the capacity to do that (for the datasets that are typically used to benchmark those NNs).
[Original Post](http://www.becomethememe.ga/)
if you check out slide 25 and 25 of the presentation here you can see its use as a library. [https://sites.google.com/secured.org/malwareunicorn/xori?authuser=0](https://sites.google.com/secured.org/malwareunicorn/xori?authuser=0) Not sure if those are in an examples folder, but you can sort of treat the src/bin folder as such. Sorry for the 'picture' of source code, but I'll try to get them into the repo as examples soon if they aren't there already.
Nice work. One suggestion I would make is regarding test organisation. I don't think it's particularly idiomatic to have separate test files in Rust. For unit style tests they are usually put in the same file as the module they are testing. For integration style tests they are put into a \`tests\` directory. For further details refer to the \[Test Organization\]([https://doc.rust-lang.org/book/2018-edition/ch11-03-test-organization.html](https://doc.rust-lang.org/book/2018-edition/ch11-03-test-organization.html)) section of the book.
Uh, I thought it was *very* clear that I was not suggesting people should *actually* put everything in one file. I was just making a purely hypothetical point about what is technically most ideal from the point of view of compilers.
Not sure what you're referring to, but splitting a project up into multiple libraries, when possible, usually has the effect of improving compile times because separate crates are easier to compile in parallel than single crates, and also easier to cache (if you split crate A into B and C, and C depends on B, then editing C will never require B to be recompiled).
this is awesome, I love it! I'll see if I can add some of those diagrams to nom's docs :)
Well, it cannot be folded, but perhaps it can be represented differently? Some kind of annotation for optional trailing symbols? Going on a limb here. It's already beautiful as is :)
&gt; I was just making a purely hypothetical point about what is technically most ideal from the point of view of compilers. According to what source? Even C toolchains lean heavily on splitting code into many files; that's how ccache works, for instance.
The thing is that *actually* having everything in one large file and compiling it normally, without any LTO feature enabled, is significantly different from what LTO *attempts* to do. LTO is slow because it's artificially reassembling everything. It doesn't really correspond as far as build times with what I was talking about at all.
&gt; I was amazed at how the community failed to grasp what he was talking If you can explain it to me, I'm all ears!
It also helps when you're building a project for the first time with dependencies you've in other another project before. That's the main use case I think.
This seems to be a popular crate for doing it: https://github.com/unicode-rs/unicode-segmentation Sometimes there is a way to combine the two characters in Unicode, and sometimes there isn't. In this case there is also `LATIN SMALL LETTER A WITH DIAERESIS (U+00E4)` which represents the "composition" of those two characters. I believe it's for legacy reasons that both forms are possible, and I believe it's the exception more than the rule. (There's no special character that represents 中 with a diaeresis, for example.)
Since the compilation unit in Rust is independent of the file system, shouldn't a "one big lib.rs file" project compile exactly the same as a project with only one library, but split into many modules? My understanding was that compilation units in Rust were not decided based off of physical files, but using other criteria.
I think sccache predates incremental compilation, so it would've been more valuable to more people before 1.24.
I'm all for the way Rust's package system currently works due to the high level of convenience, but I'm very surprised to see people claiming it is somehow beneficial to the performance of the compiler. It's absolutely not. Compile times *would* be faster if the dependencies for an average Rust project looked more like that of an average C++ project, and if Rust libraries were sized more similarly to average C++ libraries. Please don't allow some kind of ideal of the "Unix philosophy" or whatever it may be to take precedence over basic technical facts.
That doesn't (and could not ever, obviously) help the linker, though.
Sorry, I'm not sure if this was meant as a reply to my comment. Could you clarify what you mean?
Also true!
&gt; basic technical facts I'm confused, what is the source of this assertion? The only basic technical fact here that seems relevant is "to make something faster, make it do less work", which seems contrary to the idea that Rust compilation would be faster if only Rust libraries contained more code for the compiler to chew on.
I just meant that linkers know nothing about any programming language: their job is just to glue everything together. The more object files they have to deal with, the longer they'll take, in all cases. They're simply not designed or optimized for that level of multiple input.
Thanks for the link/reference. Interesting!
Your post is quite abstract, can you argue against points which I've provided? Yes, having many small crates makes linker's live harder, but I think that in the most cases parallelization will result in a net-win (remember we are speaking about total time, not total amount of computational resources used). Same goes for caching (usually in practice you are not rebuilding the world each time). And as for total amount code *maybe* dead code elimination is fast enough, so you will lose more time on linking, but it will largely depend on a project and kinda hard to measure. In theory you could parallelize and cache single file compilation, but currently Rust does not work in such way.
&gt; In theory you could parallelize and cache single file compilation, but currently Rust does not work in such way. Parallel codegen of a single crate does exist (I think the compiler refers to it as "codegen units"), though I don't know if it's enabled by default.
I recognize the stress that having more object files puts on the linker, and linking time is indeed usually a noticeable component of the compilation times of large Rust projects, but I think it's uncommon for that to be a majority of the total compilation time, and it seems as though making the linker's job easier at the expense of giving the compiler a harder time of parallelization and caching is the sort of trade-off that would require investigation.
The compilation unit in Rust is the "Crate". The compilation unit in C/C++ is the File (and all it's includes, but, that is just textual inclusion for the most part). I think you are just wrong when it comes to Rust.
That's how it starts tho, so next year whoever starts won't be the first.
I'm sorry, but this comment makes no sense. What it boils down to is, regardless of how things are supposed to work conceptually speaking, every time you divide something into another part (or file), you're introducing all kinds of additional complexity at countless different levels that the tool binaries have to deal with. Compilers are not magic, and as such aren't magically capable in reality of doing just as good a job in the same amount of time with three files as they are with one file. Do I really need to explain the reasons why data locality is universally technically beneficial?
&gt; The thing is that actually having everything in one large file and compiling it normally, without any LTO feature enabled, is significantly different technically speaking from what LTO attempts to do. Well that is exactly how LTO works in Rust today though. It just puts all the code into a single compilation unit. 
Exactly, so even if we compile each crate as it’s own compilation unit before we get to linking, we’re no worse off than the average C++ project with one object file per source file, in terms of the number of translation units. I agree we should push against the direction npm is taking with a whole crate that defines six symbols, but I’m not convinced this is really the linker’s bottleneck. Without the data to back it up, my gut feeling is that compiling 8 crates in parallel on an 8-core machine easily wins against the increased linking time, against using a single process
&gt; Compile times would be faster if the dependencies for an average Rust project looked more like that of an average C++ project, The dependencies of an average C++ project are either "None" or *maybe* "Boost", though.
Yes. One \*.o for each Crate. The "Crate" is the compilation unit. There is talk of having an option of making the compilation unit the module, but, that is unlikely to happen. So, today, if you have a Rust create consisting of 1 file or 10,000 files it produces 1 \*.o file. So, your whole argument regarding putting everything in one file is simply wrong.
In the section "Stateful Callbacks", I was confused for a few minutes. The chapter switches, without warning, from calling a C function from Rust, to calling a Rust function from C. I think it should be made clear that this is happening. Not a big deal, and otherwise, very clear and well-written.
You're completely ignoring (and twisting) the very, very simple point this guy was making. And you don't seem to realize that parallel compilation is something made necessary to begin with by having lots of different files to compile, as opposed to being an always-present part of the process. You don't *need* parallelism to use one compiler binary to compile one file. It's way more than fast enough already. That's not where the bottleneck is.
No. Also, if I were you I'd ignore the whole comment chain of people responding to you, also. A whole lot of people not knowing what they are talking about at all going on in there. I don't mean to be rude, but I really really hope the people who actually work on the compiler are a **lot** less clueless...
Do you have anything to contribute to the conversation besides eyeroll-worthy hyperbole, though?
That's in no way a hard rule and actually just depends entirely on the `codegen-units` flag as far as I'm aware, and furthermore does not apply to any of the crates used by the compiler. All of the `rlibs` found in the `rustlib` folder will always have a varying amount of numbered `*.o` files in them.
Yes- but I don't think that contradicts what I said? Rust doesn't treat multiple files as multiple compilation units at all. Before https://github.com/rust-lang/rust/pull/44853, all different files in a crate were compiled as one compilation unit, just the same as one file would be. After that pull request, Rust uses 16 compilation units regardless of the number of files. One file or many doesn't make any difference.
I didn't know about the discord. Thank you for summarizing everything in one place. It's very helpful.
C has great build times, the whole linux kernel builds in under 6 minutes on my laptop. Glibc builds in like 4 minutes. Rust and template heavy C++ code (firefox takes 3-4 hours to compile) have shitty build times because abstraction is indeed inherently computationally expensive. You probably have no clue about what you are talking, you clearly seem to be out of touch with reality. I bet you didn't even build a large C or C++ program once in your life.
I'm pretty sure you're trolling at this point.
I've joined, thank you.
I have a string of the form "len1:data1:len2:data2 .." and I want to split this up into pairs of (len1, data1) and (len2, data2) etc.. I know that I can do ``` str_data.split(':') ``` which gives me len1, data1, len2, data2 but how do I combine into the appropriate pairs?
Thanks. I'll give it a read.
I'm pretty sure parallel compilation is made necessary by a. modern processors having a bunch of cores so that you need to do things in parallel to make the most effective use of the processor and b. compilers generally needing to do a bunch of IO so it helps if the compiler has multiple things to work on while waiting on IO.
I think he covered it pretty well [here](https://github.com/rust-lang/book/issues/328#issuecomment-260378152) and /r/oneisthelonliest did so [here](https://www.reddit.com/r/rust/comments/96ghod/i_tried_rust_and_heres_my_opinion_of_it/e40gtal/). He wanted a way to split herpderp.rs into herp.rs and derp.rs. Not make extra crates or be both a binary crate and a library crate or make multiple libraries from a single crate or anything like that. Just take code that's in one file and split it up into two. The rest of the discussion consists of people saying "X isn't idiomatic" and him saying "I don't want to do X, I want to do Y" and other people saying "Sorry the compiler doesn't work the way you want" and him saying "It does exactly what I want, it's just not explained as clearly as it could be". And as it happens, this *is* explained in the modules chapter, but in section 7.2 (at least, that's the first place I found it) and it's not made 100% explicitly clear that everything in this chapter works for executables as well as libraries. FWIW, I don't think that rust modules are particularly confusing. The whole "module name corresponds to path name" is weird, but that's more or less the same as in Java so at least it's definitively weird. The bit about a module foo needing to be in `foo.rs` or a directory called `foo` depending on whether or not it has sub-modules is more weird (I'm not sure why you can't have foo.rs and foo/sub_module_of_foo.rs) and I recall being confused by that, but once you get used to it, it's fine.
Even low-end CPUs these days have multiple cores. A compiler that takes advantage of multiple cores will be faster than one that doesn't, even if it needs to do a bit of extra work to pull it off. You might see some slowdowns from splitting 1000 100kb files into 10000 10kb files, but if you split one 100MB file into 100 1MB files, that would probably make things easier for the compiler, especially considering that even the highest end CPUs wouldn't be able to hold the entire AST for a 1MB file in its cache, so you wouldn't benefit from locality at a larger scale than that.
Assume that this is my project structure : project name : test \- - some \- - - - dir \- - - - - - lib.rs \- - main.rs Is it possible to include lib.rs in main.rs? I know how to do it if both of them in the same directory. However, this would be a mess as I add more files later. Thanks before :)
&gt;Can you elaborate? What's there to fit together properly is this a joke? &gt;you'd pretty much just need to wrap foo.rs in a mod foo {} before concatenating, no? Not even close.
Look at SQLite, for example. It's well known that splitting things across translation units can disable some optimizations such as inlining. These problems are also the reason why we have LTO (link time optimization, essentially allows the optimizer to inspect all translation units at once).
`sccache` is most effective on a CI server such as Travis where only release builds are performed and no state is preserved between runs. Point it to an S3 bucket and let it rip. I agree it's not as useful on a dev machine now that we have incremental builds.
Given that split gives you an iterator, you could call `.next()` twice to get the values. So you can have something like let mut s = str_data.split(); let mut acc = Vec::new(); while let (Some(l), Some(d)) = (s.next(), s.next()) { acc.push(( l, d)); }
&gt; I thought that cascade expressions in particular was a really cool and unique feature, and the only other language that used something like this was Smalltalk. Kotlin: fun main(args: Array&lt;String&gt;) { val strings1 = java.util.LinkedList&lt;String&gt;().apply { add("Hello") add("world") } println(strings1) /* or */ val strings2 = java.util.LinkedList&lt;String&gt;().also { it.add("Hello") it.add("world") } println(strings2) } 
Tutorial Page 7 says "404 page not found."
My first thought when I saw this post was Kotlin. While the global functions of `apply`, `also`, `let`, and `run` more or less cover things, they are often weird from an ergonomics standpoint. I'd love an operator (to replace `.` or `?.`) which gives me "ignore the return type of this function, and instead allow me to chain off of this again"
Give us a webassembly front-ent and let's wrap it with electron.
There is work on an electron frontend. It kind of goes against some of the philosophy of xi being low power, low latency, with a native look and feel, but it might be interesting.
I was a joke tho, but I would like the webassembly frontend for the browser.
This PR adds support for it, but it won't be merged in a while: (https://github.com/rust-lang/rust/pull/50882) 
Normal is sufficient
Last week, I was very busy with $work, $family and our Rust meetup, which was totally awesome! Now I'm back to my usuall stuff, a few improvements to rust docs and code, still unquoting mutagen and perhaps find the time to get bytecount up and running with std::arch.
Very cool!
&gt; AFAIK this will be fixed in the 2018 edition. I believe I've read that as well, and look forward to it. &gt; will have problems with reading and properly understanding Rust code in general Nah, a huge part of what makes Rust difficult to learn is the gap between understanding an existing piece of code and knowing how to apply the same techniques in new situations. There is only one way to learn it and that's banging your head against the wall until the wall breaks. &gt; especially C++ developers get frustrated because in C++ becoming somewhat productive with only a portion of the language works, in Rust, not so much That's another big part of why Rust is difficult to learn: understanding one uncommon new mechanic is not enough, you need to master several at once. &gt; because these idioms interact with each other more ergonomically What does ergonomic mean here? Are you referring to the fact that the different parts of the system are mostly orthogonal, yet designed to work together? (In that case I got nothing on you) Or are we talking about actual usage idioms here, like the builder pattern? &gt; it's much easier to just use other peoples crates, the library ecosystem's code is much more uniform, the feeling of "I have to learn yet another obscure C++ language feature to use this library" is much less common. There usually is more than one C++ library for a given task, so if you dislike a feature they use, you can almost always choose another C++ library or bail out to C. By the way, Rust libraries may be more uniform in terms of features, but at least for me, they have an issue as well: I can't easily tell beforehand whether any two of them are going to work together nicely or I'm set up for a `borrowck`-flavoured nightmare.
As with most things, "it depends". Is your question "will splitting my library into multiple crates help compilation times for downstream users?"? In that case, probably not. The compiler's doing the same amount of work, really, and on the parallelism front there are usually enough crates to saturate all jobs, so it's not a bottleneck. You may need to do this if your crate is particularly giant and forming a bottleneck, but codegen-units is also a thing. Such splitting _can_ benefit downstream users if your library does multiple things and most downstream users pick one, and there is not that much code shared between the things it does. ----- Is your question "will splitting my application or library into multiple crates help compile times for _me_?"? The answer is in some cases "yes". Ideally, sever as many dependencies as you can so that modifying code requires a recompile of the least amount of code. Usually for application projects you'll have a couple of bottleneck crates near the end and splitting those helps parallelize things. This is also made less necessary by codegen-units and incremental compilation, though IIRC the state of the art for both is still lacking in various ways which can make crate-splitting useful (IIRC the non-codegen part of the compiler isn't yet fully parallelized? And I've seen bugs where codegen-units doesn't utilize many cores for large crates).
IIUC all the operations that the crate is using are provided by `packed_simd` (rotates, permutes, xor, add, aligned/unaligned loads, broadcast, gather/scatters,...), which is pretty much completely cross platform already (sse2,sse3, ssse3, sse4.1, sse4.2, avx, avx2, avx-512, avx-512f, avx-512vl,... neon, altivec, vsx, msa, ...). 
&gt; IMO the general concept being discussed isn't really up for contention and hasn't been for a very very long time. I keep seeing people assert this and saying it's so basic that no evidence should be necessary, but the existence of this thread should disabuse you of that notion. Surely there's *something* you could link to reinforce the claim that having all source code in a single file is "technically most ideal from the point of view of compilers" with respect to reducing compilation time?
Awesome to see Embedded Working Group growing so fast!
&gt; You don't _need_ &gt; parallel compilation to use one compiler binary to compile one file. It's way, way more than fast enough already. That's not where the bottleneck is. That ... definitely is one of the bottlenecks for rust. Bear in mind, from a compiler standpoint "files" are irrelevant, it's only crates (all crates are a giant lib.rs with inlined modules from the compiler's POV). And the code generation for a single crate definitely causes bottlenecks. Less so down the dependency tree when you have plenty of other crates to fill up cores, but more so as your tree narrows down to the top crate. Typecheck/borrowcheck/etc are less of a problem (still a problem, but not the same order of magnitude IIRC)
&gt;&gt; Can you elaborate? What's there to fit together properly &gt; is this a joke? Nope, please teach me. :)
It does, they have nothing to do with files.
You wanted /r/playrust, this is the subreddit for the Rust programming language. Please double check the subreddit before posting.
Oh shit Thanks
My bad
I think Shepmaster has closed 2 or 3 of my own questions as duplicate. Turns out they were. If I recall correctly, only one wasn't fully answered to me by the previous question. I think he does a great job at curating questions, and of course it can't be 100% perfect.
Read the book first. Implement the examples yourself and try that out. Once you are done with the whole book, make a list of small projects (1,000-10,000k lines of of code) that you have already implemented in at least some other language, and post here asking which one of these would be the _easiest_ to re-implement in Rust. 
Run `where rustc` in a command prompt and delete what you must. You should definitely be able to see Rust under the list of installed programs though, that's weird.
IIRC this is also what Ruby has with its `tap` method. I couldn't find a crate that adds this, but maybe I just searched for the wrong words. It's quite easy to write, though: [playground](https://play.rust-lang.org/?gist=787f775b9c10553ef9066289fd1c0720&amp;version=stable&amp;mode=debug&amp;edition=2015). (Feel free to bikeshed the names, I just wrote this in 2min)
The playground code is exactly what I'm using in Rust. Very handy.
Sorta like the Delphi 'with'... var foo :TFoo; begin foo := TFoo.Create(); with foo do begin AddToY(2.0); x := 2; DoubleX(); z := 'bar'; end; ... ...or something like that. It's been rather a few years - not long enough...
As always, and this time even with [a link to the project](https://blog.subnetzero.io/project/iridium-vm/index.html): &gt; Iridium refers to a language VM written in Rust. It attempts to merge a high-level, dynamic programming language with a resilient, infrastructure-focused VM. Examples of similar projects include Erlang and the BEAM VM, Dis and Limbo, or even Java and the JVM to a lesser extent. 
codegen units is enabled by default and the current default value is 16. (Source: release notes of 1.22.0 and 1.24.0.)
It's sad that this comment is downvoted so havily, since the child-comments are quite helpful and now are hidden by default. Here, have my upvote!
Just my 5 cents &gt; I believe beginners would find C++ much easier to learn than rust. Yes C++ can be more complicated than Rust but you need not use all the features in C++ because they're not mandatory. You can write in C++ to be as simple and clear as possible but in Rust you seem to be forced to do things their way. I'd say that beginners may find C++ a bit easier to start writing some simple code. But mastering it to the level where you start writing some sensible production code is much, much harder. Because compiler is usually of little help, especially in case of template programming.
Java has this too - they're called initialization expressions: ``` new ArrayList&lt;Integer&gt;() {{ add(1); add(2); }}; ```
&gt;but files are an irrelevant unit in Rust compilation, it's only crates. That's untrue in several ways though, and also a huge oversimplification. Not only are you not considering the use of Rust to build applications as opposed to libraries there at all, but you're making it as though `codegen-units=1` is the only possible setting anyone could ever be using, when it's not even the default for rustc. The number of files absolutely matters, because of course it does. It's not something that a compiler can just somehow work around.
Wurstscript (a domain specific language) also has Cascade https://wurstlang.org/manual.html
I’m discovering nphysics, and plan to make contributions to nphysics testbed to help developers quickly prototyping with it, as in https://github.com/sebcrozet/nphysics/pull/129 ; next up, figuring out testbed callbacks to easily add specs.rs to a binary with testbed and specs.rs as dependencies.
For anyone willing to help, I’ll update https://github.com/Vrixyz/testbed-callback when I have time (for now I only did it via my phone, so no chance it runs)
It also strongly resembles with blocks in Pascal (Delphi only?) and some Basic variants (VBScript certainly).
&gt; Basically I want to write this code without any unwrap calls (I feel like using unwrap defeats the purpose of using Rust), but since this is a void function I cannot use the ? operator. I'm confused, you made it a void function. Why don't you make it return a `Result`? Note that `?` will automatically convert the different kinds of errors if the required `From` implementations are available. You can also use a crate like `failure` to make this easier.
It would be nice if github had a "code review" feature. 
I think you should take a look at [pattern matching](https://doc.rust-lang.org/stable/book/2018-edition/ch06-03-if-let.html).
Create a function that does return an error (so you can use ? there), put your logic in there, handle error outside of it by returning appropriate http response.
The "makes too many new classes whose instances hold onto references" was relevant pre java 8 - now not so much. Lambas do exactly the same thing nowadays, and it's fine. 
Thank you for your insight! Unfortunately, according to this page: As a consequence, source replacement is not appropriate for situations such as patching a dependency or a private registry.
I think you will need `some/mod.rs` and `some/dir/mod.rs` to be able to do this, but I don't see why not.
If you don't want to replace crates.io and just want to host your own internal stuff separately, you'll have to use unstable features in Cargo for now :( I don't think it's documented properly, but you should be able to figure it out [from the PR introducing the functionality](https://github.com/rust-lang/cargo/pull/4506/files).
[You could also use `Itertools::tuples`:](https://docs.rs/itertools/0.7.8/itertools/trait.Itertools.html#method.tuples) extern crate itertools; use itertools::Itertools; let pairs = str_data.split(':').tuples();
Preparing to release what I've currently got for my game into early access on Steam tomorrow and hoping it goes smoothly.
Nice, but I'd rather see that as an embedded language feature
The [Itertools crate](https://crates.io/crates/itertools) has the [tuples function](https://docs.rs/itertools/0.7.8/itertools/trait.Itertools.html#method.tuples) that can do this. This code: extern crate itertools; // 0.7.8 use itertools::Itertools; fn main() { let data = "len1:data1:len2:data2"; for (a, b) in data.split(':').tuples() { println!("A: {}, B: {}", a, b); } } Will print this: A: len1, B: data1 A: len2, B: data2
The fallout from the [macro_railroad](https://lukaslueg.github.io/macro_railroad_wasm_demo/) public demo.
This is very cool! Can't wait to see more! Thanks for this!
Oh, I was talking exclusively about return statements. What you are describing is copy initialization and will try the copy constructor or an implicit conversion.
That video is hilarious! It's going to be a fine addition to my collection. I totally agree with you on the tradeoffs of Rust and the borrow checker, but I will stand by my claim that C++ is easier to get into (at least in my experience). Most of the time, I stumble across one of its features by wondering whether something that's ugly or a bit inefficient could be done differently, like "can't I just construct this object inside the `std::optional` if it's going to allocate anyway", looking it up and lo and behold, C++ got `std::in_place_t`. In Rust, you don't go use the borrow checker if you feel like removing some segfaults, it gets shoved in your face together with the rest and you better be using it RIGHT NOW. Which is great from the perspective of a proficient user, but it means you can't really control the pace at which you learn the language as a beginner. &gt; You have to learn that most of times borrowchecker just tells you that you don't understand your own program's relations between types. That's really refreshing. But yeah, in some cases it just not smart enough to understand your code is safe and correct. Personally, after the initial stage of getting accustomed to the language, I've mostly hit the latter kind, judging by the corresponding google results. Then again, I got into programming by writing a kernel, so my experience may not be representative ;)
Scala too `new util.ArrayList[Integer]() { add(1); add(2); }`
Why is using Git URL's not a useful solution? If you have your own internal Git/GitLab/GitHub/Bitbucket or whatever will all your internal-only stuff, why won't using Git URL's work for you?
Don't get bit by the other case: compiler proving your code can't execute and eliminating the message, because it's unused!
 [https://learning-rust.github.io/](https://learning-rust.github.io/) might be helpful for you to catch the basics quickly. 
I know it's not what you asked for (and sorry if you already evaluated using git for that purpose), but I would strongly recommend sticking with git. I maintained private gemservers in two big Rails projects and it was very annoying — git gives much more flexibility, you don't waste time releasing library packages, you can just specify branch for (temporary) forks of your application that need something specific in your libraries etc. etc.
If I read the algorithm correctly, you shouldn't be doing any allocation inside the loop. I'm not sure this is currently the case. Is Rust able to optimize code such as ``` let y: Array2&lt;f64&gt; = (&amp;g_x - &amp;g_x_old).into_shape((p, 1)).unwrap(); ``` by moving the allocation outside the loop and reusing the memory?
I have been working on this but ran out of time. Anyway lots of work is already done. If you have time and will to continue my work please msg me. 
I have a program I've been working on. I've made lots of little crates, sometimes with just a single simple data structure. When I want to put my main program up on [crates.io](https://crates.io), what do I do with all my "private" crates it depends on? I could document them but I also don't think they'd be super useful or of general value, and don't want to pollute the index.
Thanks for the comments! The fact that it was a void function definitely made things complicated. This is my updated code: fn read_content_length(url: &amp;Uri, client: &amp;Client&lt;HttpConnector&gt;) -&gt; Result&lt;u64, Box&lt;Error&gt;&gt; { let req = Request::head(url).body(Body::empty()); let fut = client.request(req?); let content_length = Core::new()? .run(fut)? .headers() .get(header::CONTENT_LENGTH) .ok_or("No content length present")? .to_str()? .parse::&lt;u64&gt;()?; Ok(content_length) } pub fn update_content_length(&amp;mut self) { if self.content_length.is_none() { self.content_length = HttpReader::read_content_length(&amp;self.url, &amp;self.client).ok() } } Feel free to indicate possible improvements. I will look into the failure crate to see what it is about.
&gt; I totally agree with you on the tradeoffs of Rust and the borrow checker, but I will stand by my claim that C++ is easier to get into (at least in my experience). I agree you could easier get C++ rather than Rust but there is no premise for it, I've seen lots of posts with the opposite. You could just write some examples of your troubles in Rust? You are probably diving too deep for the first program. For example, you could just use owned types everywhere until you master in lifetimes. It's not that efficient, but it's easier to get it. And don't forget about learning curve - Rust really *could* be harder to learn at beginning, but it should become easier when you understand basics. &gt; In Rust, you don't go use the borrow checker if you feel like removing some segfaults, it gets shoved in your face together with the rest and you better be using it RIGHT NOW. Which is great from the perspective of a proficient user, but it means you can't really control the pace at which you learn the language as a beginner. You have to just learn two things: 1. You should have one mutable reference at once 2. You cannot reference a thing that is going to die in some context outside that context. They are obvious enough to get them. Then you can start mastering at returning references, working with generic lifetimes and so on. But you can easily avoid it at beginning.
From my experience, it does not seem that you can automatically keep your dependencies from git "up to date" without being on master. E.g. if I have a tag \`v0.6.0\` that my Cargo.toml is point at, Cargo can't recognize that there's a \`v0.6.1\` tag and update to it. It's not a deal breaker, but it's annoying.
Would you support unicode? I will use railroad A LOT if I can write in chinese and spanish too :)
Not OP but similar boat. Not used git urls but I'm assuming you lose all the nice semver `cargo update` features with it.
You having C experience is the first step: it will be easier for you to understand what those weirdly looking rules of Rust protect you from. I don't know if this will work for you, but I did it in three steps: read the book to understand general concepts, try to write code of my project, then read those parts of the book again that explained something if I hit a compiler error. Repeat few times to become good at Rust. :)
If you have multiple cores and only a single thread, then part of your CPU is idle when it could be doing something. Anything single-threaded is not using all of the processing power that it could be using.
Quick glance: `TrustedLen` is the only `unsafe` code you have, thus the question boils down to "would it somehow affect correctness of `TrustedLen`? In other words, would the length of the iterator change if it panicked there?
YMMV: While it parses and displays just fine (try "大家好" as an element), the underlying library simply assumes a fixed - monospace - width for all characters. It does not compute the actual width of text-elements and neither does SVG. Characters and symbols which are are outside the dimensions of a monospace grid will not be displayed correctly.
It’s often not a situation where you can make a choice about how to handle it. Often the build server is cut off from the outside at any rate. To that extent, it would be great if Cargo had a way to bundle all dependency sources into your project’s repo like a “vendor” thing so you can upload it to your own source control server. That would satisfy the “must have full control over all sources for audit” issue.
[https://imgur.com/txuke1k](https://imgur.com/txuke1k) I don't know if it's something wrong on my side... the element is: term!("大家好")
You can use a custom branch from which Cargo pulls, something that for example Hyper uses. Still not a perfect solution, but might be a little better then manually updating each time.
&gt; You could just write some examples of your troubles in Rust? You are probably diving too deep for the first program. Just so there is no misunderstanding, I am writing from past experience and things I've heard from other people. I have been at the exact place you describe, diving too deep and trying to write a tree structure, but I got it sorted out, and get along pretty well nowadays. Thank you for your offered help though :) &gt; For example, you could just use owned types everywhere until you master in lifetimes. It's not that efficient, but it's easier to get it. And don't forget about learning curve - Rust really could be harder to learn at beginning, but it should become easier when you understand basics. Yes, that would work. Now I don't know how many good tutorials for Rust exist, because I didn't go out of my way to find them and just started writing programs. But this is something I believe most people (including me) won't recognise themselves when starting out, at least not before hitting a wall for the first time. &gt; They are obvious enough to get them. Then you can start mastering at returning references, working with generic lifetimes and so on. But you can easily avoid it at beginning. Pretty much the same situation here, this would be great as a tutorial with exercises, no idea whether it already exists. Ultimately it still comes down to training until one understands how to apply the newly-learned tools in new situations, though. It's a property of all the languages with strong PLT roots (Haskell, the dependently typed languages, ...). With good guidance the process can be eased a bit, but they likely won't ever be as easy to learn as a more "get stuff done quickly"-oriented language.
Because git is not used in my company as far as I know… (please don't ask what's in use T_T)
If git was already part of the toolbox, yeah maybe. But the ppl I work with don't even know what a distributed version control is…
you could host your own crates somewhere and make use of the `crate: {path: "path/to/crate"}` thing
Don't worry about sharing. We're using Git, Hg, SVN, CVS, TFS and I'm almost positive we have at least one SourceSafe repo somewhere. Moving everything to Git is a sloooow process.
Oh, you are using the DSL ? The problem most likely stems from the way your input is read from the input file. The text-file is read assuming UTF-8 ? Are you using that encoding?
Visual SourceSafe? :-D
If we want Rust to be used in more companies this is an important one to tackle. This is why Java is still so popular. So may good enterprise grade tools.
&gt; To that extent, it would be great if Cargo had a way to bundle all dependency sources into your project’s repo like a “vendor” thing so you can upload it to your own source control server. https://github.com/alexcrichton/cargo-vendor https://github.com/alexcrichton/cargo-local-registry this is what rustc itself, as well as firefox uses, as I understand it.
Ah, Ok. Yeah, I just modified your example and then run: cargo run --example visuals in railroad Thanks!
Hi, I'm one of the maintainers of `wasmi`. `wasmi` isn't suitable for this, you should use cranelift.
&gt; the ppl I work with don't even know what a distributed version control is… 😮😯😶
Then I would start by advocating git and worry about Rust in a year.
Then I would start by advocating git and worry about Rust in a year.
You could also make your method use a scoped pub too. If the method is truly pub then it wont work that way.
Probably nothing, currently playing a bit with the 1.0 release of Julia.
Even if you could, a GPU would be way to slow to have it even be remotely faster.
This is probably a crazy dumb question but on the [OO design pattern examples](https://doc.rust-lang.org/book/second-edition/ch17-03-oo-design-patterns.html), in the third section where it's implemented the posts using the type pattern instead of the State. pub struct PendingReviewPost { content: String, } impl PendingReviewPost { pub fn approve(self) -&gt; Post { Post { content: self.content, } } } There's an additional task to "Require two calls to approve before the state can be changed to Published" but I can't think of a way of doing this that follows where the refactoring seemed to be going – Other than maybe returning a `Result`?
This depends on the objective function and class of learner. Many objectives have guarantees about optimality of solutions because they are [_convex_ functions](https://en.wikipedia.org/wiki/Convex_function#Functions_of_n_variables). &gt; Any local minimum of a convex function is also a global minimum. A strictly convex function will have at most one global minimum. Typically the local-minima is "good enough" comes up in the context of deep learning.
&gt;this What's this?
One possibility is using a monorepo, which houses the crate code and the business logic. I am currently doing this in a in-development project, where I extracted a big chunk of code into it's own crate. You can use a folder for the crate: ``` [lib] name = "mylib" path = "src/mylib/lib.rs" ``` which then you can use like you'd use any other crate `extern crate mylib;`, `use mylib::MyLib;`
Private crates registry
Cargo has support for alternative registries. I've never used it, but withoutboats wrote a blog post about it: https://boats.gitlab.io/blog/post/2017-10-28-alternative-registries/
Still working through the book. Figuring out how to split work in modules, made an example to reference reading in json data https://github.com/camccar/rustreadingdataexample
Oh great!
This should be the top-voted answer!
Totally unrelated to this discussion about crates.io... mind sharing a link to that `rocket-jwt-auth` crate?
[https://git.onewebdev.info/rocket-microservice-starter/rocket-jwt-auth](https://git.onewebdev.info/rocket-microservice-starter/rocket-jwt-auth) Not documented, but the code is fairly small and shouldn't be too hard to follow.
I don't mean to put words in your mouth, but your answer reads to me like you think Rust's compilation model works the same was as C/C++'s. In C/C++, the compilation unit is each individual file which are then linked together. In Rust, the compilation unit is a crate (regardless of how many files are in that crate). &gt; Possibly (depending on how sophisticated the compiler was), but you wouldn't need incremental compilation in the first place then. Notice I said "in theory", too. Rust's incremental compilation isn't tied to files at all and having one large file or many small files doesn't make a different in that regard. &gt; My point was just that in general, the extreme modularity of the Rust ecosystem is unfortunately exactly the opposite of the sort of input you'd want to give a compiler (for any language, not just Rust) if you were trying to reduce build times as much as possible. The situation is more complicated than you describe. If your crate dependency graph is very wide with many independent dependencies that are tied together in a single "root" crate instead of very tall with many dependencies which depend on other dependencies which depend on other dependencies, then having more crates **will** speed up your build. That's what [this GSoC project for Servo was all about](https://blog.servo.org/2018/08/09/gsoc-generic-servo/). 
You're right, I am definitely doing allocation inside the loop, and I think it should be possible to eliminate that. I'm guessing that the compiler is not able to reuse memory because ndarray arrays are sized at runtime instead of at compile time like in nalgebra. In the Fortran implementation of L-BFGS-B, they preallocate all the arrays at the start and then just reuse them as necessary. It does make the code kind of ugly though. Could you explain the connection between the panic and reusing memory? I don't know that much about how such optimizations work.
&gt;(with an infinite number of commas) IIRC this is the only way to get an optional trailing comma and is a fairly common idiom
VB also has the `with...end` statement: ``` With theCustomer.Comments .Add("First comment.") .Add("Second comment.") End With ```
&gt; I have been working on this but ran out of time. Anyway lots of work is already done. If you have time and will to continue my work please msg me. I'm not sure I can commit to this work, but I'd love to see this, even in an unfinished state!
Serena PVCS? :P
Interesting tutorial idea! I don't use `usize` in my Rust code in any context except where it is required: I suggest using `u64` instead here for portability and clarity. You might want to think about using `T: Into&lt;u64&gt;` as the parameter type here to allow more general use.
Perforce? We just moved away from that, to git (thank God) at my job a few months ago.
Not sure if I should update this post with the link, but I did: r/https://github.com/jacksoncoder/cascade.
To be honest, I'm not really sure now you ask out loud :) The guide I originally starting using (some website, I've now misplaced) seemed to be putting everything in a separate crate, so I was just following along. I'll merge some of those crates back into my main project (at least when I release, I'll wait and see if any of them become independently useful first.)
Yeah, that is completely not what I understood. But that's kinda the nature of it, isn't it? :) It probably doesn't help that this ticket is only one of many; this person was opening all kinds of issues and making internals posts, calling me incompetent, and so I was pretty much out of charitable interpretations at that point. &gt; (I'm not sure why you can't have foo.rs and foo/sub_module_of_foo.rs) You'll be able to in Rust 2018. Looking forward to it.
So this week I'm releasing [tarpaulin](https://github.com/xd009642/tarpaulin) 0.6.6 as we speak which lets people exclude modules of the form `mod x;` from results. Gonna try and continue with tarpaulin and close some more issues. Also going to be progressing the timer traits on my embedded-hal crate for the stm32f469. And as a non-rust thing rewriting my VHDL synthesiser project into Verilog and getting up to speed with Verilator.
As others have said, I'd recommend getting advocating for a gitlab or any git server instance first. You don't need to use it for your work projects, but it'll open up a lot more possibilities in the future. Consider it a building block to advocate for more stuff in the future.
Whoops! I've even done some programming in Kotlin, so that makes it all the more embarrassing. When I checked what other languages had this feature before writing this post, I only looked at the Wikipedia entry, and Kotlin wasn't on there. Someone should probably add it.
The look of the game reminds me of Theme Hospital, which I played a lot back I was actually in university
That looks more like what an import statement would do 
As far as I know, the only request for method cascading to be a language feature was [4 years ago](https://github.com/rust-lang/rust/issues/6679), and I don't think an official RFC was even filed. Since cascades are just syntactic sugar, and rather trivial to write a macro for, it makes more sense to have a macro for this than to bother making cascades a feature of the language.
\`&amp;.\` maybe, but not sure about ignoring return type for that one. Ruby uses that as a shortcut for \`.try\`, which is basically \`map\` on an \`Option\`.
From the example above, it seems the only thing the computation requires is a `&amp;str`. That is actually very cheap to pass around. I'd go with `struct MyError(&amp;'str);` and convert it with `.to_string()` if you just need to display it, or even better, do all the formatting etc by implementing `fmt::Display`.
Yeah, might be better that we are worse on StackOverflow then!
&gt; Possibly (depending on how sophisticated the compiler was), but you wouldn't need incremental compilation in the first place then. Notice I said "in theory", too. So here it feels like the two of you are using "incremental compilation" differently, you're referring to the Rust feature of incremental compilation, whereas the person you're replying to is referring to the general concept of being able to make a change and have the least amount of files affected. A single file is _terrible_ for the latter, _unless_ the compiler is sophisticated enough to do dependency tracking within a file (crate, in Rust's case). C++ compilers don't do this typically, but Rust's incremental compilation feature does. However, Rust's incremental compilation (and relatedly, codegen-units) support aren't perfect, so splitting crates _does_ sometimes have an advantage when it's a bottleneck crate that codegen-units and incremental are somehow not very good at handling. &gt; My point was just that in general, the extreme modularity of the Rust ecosystem is unfortunately exactly the opposite of the sort of input you'd want to give a compiler This is true, but this isn't exactly what's being asked, and the reason this is true is more due to duplication and doing extra work compiling things you don't use. Your responses elsewhere in this thread seem to come from this focus on "the extreme modularity of the Rust ecosystem" and that's missing a whole facet of this situation -- applications. What's being asked isn't just about ecosystem crates, it's about Rust projects, which can include the toplevel applications. Cratesplitting isn't that useful in libraries. However, for the final crates leading up to your application, it becomes useful again. This is often where you have bottlenecks in parallel compilation due to there being just one crate which is probably monomorphizing a lot of things and thus doing a _lot_ of codegen. codegen-units helps but like I said not always. Here duplication and extra work aren't issues -- these aren't libraries which may end up duplicated in your dependencies, or libraries with code that should have been shared had they been more monoolithinc, but instead these are your application crates which contain everything you need to build your application. Typically there's no extra fluff in these -- you're the only consumer, so _everything_ in this crate is necessary for you. Because everything in the crate is necessary for you, it's all a matter of how it's presented to the compiler. Cratesplitting ends up with fluff being compiled and going unused making build times worse, but here there's nothing like that. It's the same amount of work. Cratesplitting may aid in better parallelism. And yes, codegen-units, incremental compilation, and general parallelism within the compiler (not done yet) should solve this problem, but they don't, yet.
https://git.onewebdev.info/rocket-microservice-starter/rocket-jwt-auth/blob/v0.4.0/src/lib.rs#L84 It may be wise not to provide error details, as a change to underlying JWT lib can result in [spectacular error like this](https://github.com/jwt-dotnet/jwt/issues/61).
Did you use any of the open source engines? What graphics libraries is it running on?
After a certain point I just stopped using it for Rust stuff. Usually thats because my issues these days are problems with libraries that have tracked issues instead of questions about the language.
Did you mean to reply to /u/vks_? 
No engine, just sdl2 and opengl. [Heres my Cargo.toml so you can see everything.](https://gist.github.com/Thinkofname/1670cbddc0e512a9310c61e01ca23df7). I started this a long time ago (Almost 2 years I think) so a lot of the engines/libraries were really unstable at the time.
You could make a third type `PendingReviewPostWithOneApproval` that also has `approve` on it. You could add an `approvals` field to `PendingReviewPost`, and increment it in `approve`. Have it return `Result&lt;Post, PendingReviewPost&gt;` (or some other two-type enum) depending on whether or not it has enough approvals. Then using it would be something like `post = match post.approve() { Ok(post) =&gt; { send_to_forum(post); return; }, Err(post) =&gt; post };`. No promises that this passes the borrow checker.
That sent a cold chill down my spine. 
Good job :)
It will be even harder than pushing for Rust. I think we have some ppl using git in the US (I'm in a UK division) through bitbucket but no one knows how to use git here. 
Thanks for explanation.
like rust and compiler errors
This project right now is a great *mirror* of crates.io, but can be extended with writeability. It has a nice crates.io like interface but is much lighter on dependencies (doesn't need a ton of tools from the js ecosystem): https://gitlab.com/est/cargo-local-serve
You should add link to your article to your repository as your article is incredibly helpful in explaining what is it about.
From what I understood: 1. The "red" car is `11`. 2. The goal is for the "red" car to reach the right-hand side of the board. It might be worth indicating in the README.txt.
Going to get my 0.5.0 release done for [orion](https://github.com/brycx/orion), only supporting SHA512 for HMAC, HKDF and PBKDF2 but now with ```#![no_std]``` support.
I believe this is usual semantics for functional languages, Haskell or Erlang behave the same. `_` not binding is pretty important not just because it avoids borrowing or taking ownership of values you don't care about, but because that means you can use `_` multiple times in the same pattern, or across "shared patterns" e.g. in Python, where `_` binds you can't define a function with two arguments `_` and `_` because you need a function of arity 2 but don't care about either argument. It works just fine in Rust.
Nice post! Great How you dig into the reason for the unexpected behavior of _. 
not my post, but I found it really interesting
Does anyone have a good example of how to use traits to do Mocking for testing and/or dependency injection? I feel like I'm missing something. I'm trying to create a function that calls out to an api and returns a result&lt;custom\_model&gt;. I'd like to have a generic ISearch so that I can more easily mock the api for testing and so I can easily swap out a search from the api to another api or file or w/e in the future. I can try to post some code later if it's helpful when I get time but I just feel like I'm missing something between creating the trait and implementing it. To actually injecting it into my specific implementations. I'm working on a podcatcher app mostly to learn rust a bit better and I'd like to have a specific implementation for Itunes but be able to swap it for another podcast repository if I need to for w/e reason for the more concrete example of what I'm doing.
Mozilla keeps all of their rust dependencies in their repo for FF. You could do what they do and keep all of the dependencies you need across the company in a single, or include them in an existing repo. I imagine if FF does it, the rug won't get pulled out from under that functionality.
Due to the "one exe output" design of Rust many individual crates are linked into the final result. Unfortunately this also produces a wild mixture of crate licenses and it is a challenge to know what the combined result is. For pure open source projects this is no problem, but for commercial binaries this may make the choice of Rust impossible. For example, for a minimal application that uses rust-postgres to access a postgres database, the following crates are pulled in (according to "cargo license", and it seems to be incomplete in comparison to "cargo tree"): &gt;Apache-2.0/MIT (27): base64, block-buffer, byte-tools, cfg-if, crypto-mac, digest, fake-simd, fallible-iterator, hmac, iovec, libc, log, md5, postgres-protocol, rand, rand, sha2, siphasher, socket2, stringprep, typenum, unicode-bidi, unicode-normalization &gt;BSD-2-Clause (1): arrayref &gt;CC0-1.0 (1): constant_time_eq &gt;MIT (9): bytes, generic-array, matches, postgres, postgres-shared &gt;MIT OR Apache-2.0 (2): hex, safemem &gt;MIT/Unlicense (2): byteorder, memchr What does a combination of these licenses of rust-postgres dependencies mean? Of course the individual crates used must be credited properly in the "about" section of a program, but can one distribute/sell the resulting.exe, keeping the proprietary main program code a secret? If Rust wants further adoption in commercial environments where all-open-source is not an option (for various reasons), how does one assess whether adding a crate with all its dependencies still permits a closed for-pay shippingproduct.exe?
Bonus points: Is there a tool/command that combines "cargo tree" with "cargo license"? :-)
This seems really cool! I'd definitely add some rustdoc documentation, in addition to your README. Having the rustdoc example checker is a real life saver, in my experience!
Sure, it's not unusual. It's still an annoying issue to hit. Maybe this should be a warning when used outside of a pattern (where I imagine this would most often bite people)?
That's awesome. I did not know this existed.
Again you don't have to push for usage of git, just the resource allocation of a server to host it. In terms of using git, most people only need to know a handful of git commands and there are several UI tools to negate even that. You'll be fighting an uphill battle in terms of setup for a private crate repo if you can't get the necessary infrastructure in place first.
&gt; The modules management are not to my liking. I don't like the way it is implemented. They need to fix this. &gt; &gt; [...] &gt; &gt; I believe beginners would find C++ much easier to learn than rust. C++, categorically, has the worst module system I've ever used. I'd rather use C or JavaScript ES5. C header files are basically ABI definitions, whereas C++ has to put code into the header files to do templates, and doesn't have a stable ABI anyway. C and C++ both end up parsing the same header files over and over again, but C headers are small, while C++ headers are sometimes the entire library. While 2009 vanilla JavaScript doesn't really have a module system at all, at least it's easy to understand (either the function got loaded or it didn't). 
Programming Rust by Jim Blandy is highly recommended ([http://shop.oreilly.com/product/0636920040385.do](http://shop.oreilly.com/product/0636920040385.do))
I'd like to write a proc macro attribute to put on a mod declaration (on unstable). Is there a way I can add attributes to structs within the mod that the macro can pickup on? I'm not finding a way to do that. I keep getting the "The attribute x is currently unknown to the compiler and may have meaning added to it in the future (see issue #29642)" error
I meant unexpected in the sense of unexpected by the author (and I’m not sure I’d have anticipated the behavior either, even though it makes sense). 
I agree, but the article makes a good explanation of why it's not the same as with Haskell or Erlang or Elm. Rust is the only language were lifetime of objects has important lifetime semantics. Also many programmers come to Rust from other languages, and when they see `_` they might assume that it's convention vs. actual special symbol. This is, at the very least, a good argument for a lint, "let expression not binding anything, all values generated get immediately dropped". I can't think of a case, of the top of my head, where you couldn't change `let _ = foo` for just `foo`, and generally any statement starting with `let` wants to bind something. I think it's fair that at least clippy have it. Also maybe one solution is to allow a `with $expr {...}` which guarantees that whatever `$expr` created lives until the end of the block. It may be doable with a block but I'm not 100% sure it would work. Maybe it can be made more interesting by allowing the `with` to also let you do expressions that start with `.foo` and it's the equivalent to calling `$expr.foo` without ever giving `$expr` a name. At this point though, it might be easier to just give a name to the damn thing.
Some interesting stuff in there, but I haven't really tried to use it yet. It seems the core disassembly stuff is autogenerated from something LLVM-based, which is probably a good thing because writing a new disassembler for x86 from scratch is a daunting task. Also, it's great that it's pure Rust since that opens up a lot of possibilities for speeding things up using inlining and related tricks. I may just be overlooking something here, but where is the build code for these generated files? I'm particularly curious whether this is based on LLVM's vanilla disassembly code or the Capstone project's variant.
What happened in 1.28.0 ? Are you using `hyperfine` ?
What happened in 1.28.0 ? Are you using `hyperfine` ?
Alright, I just mean that in `let _ = ...` it is unlikely that the the underscore is doing what's expected and there should be a warning.
&gt; What happened in 1.28.0 ? I think some aliasing hints were re-added that helped LLVM generate better code. &gt; Are you using hyperfine ? No, it's all my code. 
There are some functions in the standard library that use the `fn do_a_thing(self) -&gt; Result&lt;SomethingElse, Self&gt;` pattern where they consume the value, but give it back if something went wrong. [`Arc::try_unwrap`](https://doc.rust-lang.org/alloc/arc/struct.Arc.html#method.try_unwrap) is one example.
Hi, sccache author here. :) Incremental compilation is only enabled for crates that are specified as path dependencies, FYI, and the top-level crate. This makes sense, because those are the crates you're likely to be editing the source of, and "edit and rebuild" is the workflow that incremental compilation was designed to speed up. FWIW, sccache was written primarily to support CI--we use it in Firefox CI for caching C/C++ and Rust compilation and it's incredibly successful there. We run thousands of builds daily, with many of them running in parallel as developers land new changes, and we get a very high cache hit rate (it hovers around 99%) because most code changes don't touch the majority of the source that gets built. sccache works for local development, it just hasn't had a lot of work done to support that use case. We'll likely spend some time focusing on that soon as we're landing support for doing icecream-style distributed compilation in sccache, and we expect Firefox developers to use that functionality heavily.
I tried something similar to this but I think I had issues with a struct holding a trait but that could be because trait objects are a little different than what I'm used to working with. This idea makes sense to me. When I'm out of work I'll take a look at my code again and post back if I run into any problems. Thank you.
You misunderstand `let _` _is_ a pattern. `let` takes patterns. (also, `let _ = foo;` is a pretty well-established idiom)
Man, I used to see that style of syntax railroad diagram all over the place in old books, but you almost never see it anymore these days. Did they just go out of fashion?
We've thought about this exact same thing for Firefox development, but since that's mostly C++ compilation we got hung up on C++ specific issues like path translation. I think the story for Rust compilation would be simpler but haven't had time to really get there. I don't think it'd be all that expensive, honestly, S3 storage is dirt cheap. I could probably convince people at Mozilla to pay for that. The security aspect is definitely something we'd have to think about, though. Currently sccache unconditionally trusts cache entries, which could be a bad thing for that scenario.
Cargo sets [several environment variables](https://doc.rust-lang.org/cargo/reference/environment-variables.html#environment-variables-cargo-sets-for-crates) during compilation, which you can access through the `env!` macro: let version = env!("CARGO_PKG_VERSION");
Thank you! I will check that out
AFAIK it's the standard way to drop a `Result` on the ground out of some IO methods.
Note that `let _ = some_op()` is a useful idiom for the case where `some_op()` returns `Result&lt;(), Error&gt;` and you want to attempt to run `some_op()` and don't care if it fails - e.g. if `some_op()` succeeding will populate a cache.
From the article: &gt;One downside of sccache: it won’t make your continuous integration builds faster. (You could configure sccache to use an S3 bucket, but I prefer to have completely clean builds from my CI.) Humorously, sccache was originally developed *for* CI use at Mozilla! CI compiles the same source over and over zillions of times, so it benefits a lot from caching. ccache was proving not suitable for us because we use ephemeral build machines (EC2 spot instances) and we always have multiple builds happening in parallel, so we built sccache so our CI builders could share an (S3-backed) cache. We get an extremely high cache hit rate for Firefox builds, and sccache saves us several minutes per build (which really adds up multiplied over the thousands of builds we do every day).
When is this a good thing to do? I imagine if you're doing that, you're trying to get out of handling errors, where you should be using `.unwrap()` or at least somehow more explicitly/verbosely ignoring it. But maybe I'm mistaken?
It really depends, people are of two minds. I prefer to always `unwrap`, but sometimes, you can just ignore an error, you don't need things to blow up.
At first it seems like a success story. But what of those high outliers? And those single low-outliers: are those representative of the initial or ultimate samples? Maybe a run-chart/time series of these would help uncover that. Oh: and maybe take a look at the non-normalized samples: what was the original distribution like?
If you're talking about an inline mod declaration, #[your_macro] mod foo { #[custom_attr] struct Thing; } then you can simply strip out the custom attributes in your macro's output. 
1.22 is the fastest, there was performance regression and it's slowly improving.
It's an [oldie but a goodie](http://primepuzzle.com/tp2/syntax-diagrams.html).
I must be misunderstanding something. How does that differ from just not matching at all?
&gt; At first it seems like a success story. But what of those high outliers? They seem to occur in a few specific uncompressed formats that are extremely fast and so small changes have big proportional impacts. I haven't looked at them in detail yet but I bet it's to do with array bounds checks not being elided since LLVM 6.0. I'm not following the second part of your comment. There are no positive outliers just a few formats that got quite a boost, like one of the Samsung ones, from what is probably better code generation in LLVM 6.0. 
We just use private GitHub, then point the cargo.toml dependency at the git repo directly. 
You didn't show the actual error you got from `built` (just the line number)... the example project in its repo works fine for me. 
Please no...
For the most part it looks good and readable! Here's some tips and opinions: - Instead of `String` you could pass `&amp;str` or even iterator in `new` - Instead of ad hoc iteration with `nth`/`done`, you could just use an actual iterator stored in the struct. I believe that this also provides better performance. - Uses of `unwrap`, `assert_eq` and `panic` are unnecessary and should be replaced with `Result` - I don't like `discard_whitespace` method. It's easy to forget calling it in every situation. In think that a "read next non-whitespace character" method would be nicer.
May I strongly recommend im-rs a wrapper around using RC that provides a drop in replacement for Vec
Zipballs and email?
I set the project aside months ago, but I realized I never did talk about it, so hopefully it counts! I wrote a [compile-time-sized feed-forward neural network implementation](https://github.com/pshendry/neurotic); it makes thorough use of compile-time `nalgebra` matrices so that the training can be done on data that's contiguous in memory. It's not multi-threaded, but it reaches 95% accuracy on MNIST handwritten digits in 60 seconds or so on my i5, which seemed pretty zippy.
sorry, yes, build.rs fails, when I call it manually with backtrace: 8: core::result::unwrap_failed at /checkout/src/libcore/macros.rs:26 9: &lt;core::result::Result&lt;T, E&gt;&gt;::unwrap at /checkout/src/libcore/result.rs:782 10: built::write_built_file at .cargo/registry/src/github.com-1ecc6299db9ec823/built-0.3.0/src/lib.rs:803 11: build_script_build::main at ./build.rs:5 
&gt; It works just fine in Rust. A lot of people do not understand than the rust grammar actually takes PATTERN in multiple cases. I find this incredibly useful: struct A { x: i32, y: f32 } fn foo(A{ x, y}: A) { // PATTERN: TYPE println!("{} {}", x, y); } let a = A{x: 2, y: 3.}; foo(a);
Hi all. I'm writing a wrapper over the unofficial SpaceX API, and am running into issues trying to get things to work. When [running this code,](https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/main.rs#L7) I get the follow error: error[E0599]: no method named `send` found for type `space_rx::v2_api::CompanyRequest` in the current scope --&gt; src/main.rs:7:64 | 7 | let cr = CompanyRequestBuilder::default().build().unwrap().send(); | ^^^^ However, I have this send method defined as a default trait method. Is it because I'm not passing a \`&amp;self\`? If so, how do I pass a \`&amp;self\` when I'm building my my request like this? Am I going about this the wrong way? r/https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/v2_api/models/company.rs#L7 r/https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/lib.rs#L29
Hi all. I'm writing a wrapper over the unofficial SpaceX API, and am running into issues trying to get things to work. When [running this code,](https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/main.rs#L7) I get the follow error: error[E0599]: no method named `send` found for type `space_rx::v2_api::CompanyRequest` in the current scope --&gt; src/main.rs:7:64 | 7 | let cr = CompanyRequestBuilder::default().build().unwrap().send(); | ^^^^ However, I have this send method defined as a default trait method. Is it because I'm not passing a \`&amp;self\`? If so, how do I pass a \`&amp;self\` when I'm building my my request like this? Am I going about this the wrong way? [https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/v2\_api/models/company.rs#L7](https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/v2_api/models/company.rs#L7) [https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/lib.rs#L29](https://github.com/twilco/space-rx/blob/a4613db46b4bdd32acca8e4458ce0396042d9378/src/lib.rs#L29)
Hmm... I tried that already and failed somewhere. Now tried again and it builds and runs fine. Have to try tomorrow with a larger project :-) Of course "This is not legal advice and is not guaranteed to be correct." 
I participated in the Ludum Dare 48-hour game jam! My game was written, naturally, in Rust using ggez. Check it out here: https://ldjam.com/events/ludum-dare/42/running-in-to-space On the whole, the experience was good, and I managed to get something actually mostly done even after life and ennui ate an evening. Other major tools I used were: ggez-goodies, ggez's game template, specs, warmy, nalgebra, and ncollide2d. I'm hoping to write up a more detailed postmortem in the next day or two.
Git submodules would probably work in such a scenario quite well. I’ve used such setups before with c/c++ projects.
&gt; Languages are not only (in Ken Iverson's words) "tools for thought", they are also expressions of software systems: many aspects of deployed software systems are a consequence of the language that they are written in. Like many systems software engineers, I have been on a career-long odyssey through programming languages -- always returning to C for its explicit simplicity and performance. But like many, I have become increasingly Rust-curious, and this summer I chose to finally sate that curiosity and cut some code. Would the infamous learning curve crush me underfoot? Would I find Rust's compiler-heavy approach to be pedantic or empowering? Would a summertime crush end in a screaming argument with the borrow checker? And (perhaps most importantly) could I overcome the bigotries of whitespace? &gt; In this talk, I will talk about my summer to date -- including (yes) some of my fights with the borrow checker, but also highlighting many of the delightful features of Rust that seem to be less commonly discussed. I will also discuss my own approach to learning Rust (very much a work in progress!), and why I am so optimistic that my summer of Rust will blossom into a longer term relationship. 
Why not just use git URLs with closed/private repos, out of curiosity?
They could even use `Cow` to abstract over using a `String` (in the case a iterator was passed) or using `&amp;str` (in the case `&amp;str` was passed). For iteration, the iterator could have `.filter(|x| !char::is_whitespace(x))` and the `&amp;str` would have to be the same as `discard_whitespace` is right now.
Almost done with my alarm clock client, going to push soon. It's a client/daemon interface, but it's not really a daemon, more a background process. I use JSON to communicate requests, decoding/encoding with Serde, of course. Even at this early stage, it would be nice to see feedback from the community!
You want to use usize when pointer size matters, which is what is architecture dependent. The kinds of situations in which you use an usize is stuff like index values. A usize is exactly the right size to represent all the possible memory addresses on a particular architecture. A usize on a 64-bit architecture will be a u64, u32 on a 32-bit architecture, i16 on a 16-bit architecture, It shouldn't be your default int type to reach out for when you are thinking about numbers are representing anything other than memory. u32 is a good int type default and is usually what the rust compiler will assume if nothing in annotated to suggest otherwise.
What a slick idea. Thanks :)
The limits of this program change by using usize. Running it on a 32 but machine, you'll only be able to fizzbuzz out to 4 billion and change, but on a 64 bit machine, you can go on much, much longer. This limit is arbitrary, but by using usize, it's inconsistent for no good reason. By using a fixed type, your program will provide the same functionality everywhere.
I think this hints at something deeper. You have to be careful how you use `Drop`. I'm assuming a lot of things here, but based on the examples it seems as if `Service` implemented `Drop`. Otherwise the examples wouldn't make sense. And dropping a `Service` was significant to the way the `Service` operated.. That's perhaps spooky design. `Drop` should be something that just cleans up; it shouldn't generally perform "significant" actions. To be clear, this is unrelated to the post's point about `_` not being binding. That's really interesting!
Thank you, that was quite informative!
Even then it might not be obvious. Say that I start with a simple code that does the following: let in_chan, out_chan = some_channel&lt;f64&gt;::new(); // This function creates async tasks, but they don't return anything // so we don't need Result&lt;TaskHandle, TaskErr&gt; returned. // The do_cancelable_task_and_pass_on_to will create a cancelable // function that can passes it on to in_chan, closing the channel if // the calling task is canceled. let _ = task_manager.start_task( do_cancelable_task_and_pass_on_to(expensive_foo, in_chan)).unwrap(); let second_result = bar()?; // If bar fails we can just return, expensive_foo should get auto-cancelled let first_result = out_chan.with_timeout(5000).pull_or_panic("Timeout"); ... Can you spot the problem? This API exposes the ability to cancel tasks, this is done by simply dropping them. The task that runs `expensive_foo` gets automatically canceled and the channel gets closed up. The api, btw, isn't wrong, because the ability to cancel tasks is fine. The only reason the program crashes is because the user decided that if the `out_chan` was closed it should panic. We could make `TaskHandle` a `[must_use]` specifically to avoid this kind of mistake, but again the user has followed the idiomatic way around it without understanding the subtle difference (the special treatment `_` as a variable identifier receives vs anything else. As far as that user knows, when you write something with `let` it will last until it moves elsewhere or the end of the current block. The thing is that the user doesn't realize that `let _ = ...` means, unlike every other `let` expression "create, and then immediately afterwards drop it ignoring the results but acting as if though you used them". It's not that hard but it's an exception. Imagine that the `let _ =` lint was in place, this means that `[must_use]` types would have to expose an api to explicitly choose to opt out and not use the value, the api explaining the consequences fully. For `Result` I'd use `drop_result()`, but for `TaskHandler` I'd use something like `cancel()`. task::start( do_cancelable_task_and_pass_on_to(expensive_foo, in_chan)).unwrap().cancel(); Now it becomes very clear why this is a misuse of the character. I could use a named `_underscore_variable` which would guarantee things are kept around but also say the variable shouldn't be used (enforced by lints), or I could give it a name, or whatever makes more sense, but the lints guide me to better use. And threaded code is where RAII becomes interesting, as you may be assuming that some evens must be able to happen in a specific ordered, when they might be guaranteed to always happen in an unexpected fashion. Of course the rare exceptions, but at this point we are getting to the point were it would be rare enough make sense to simply turn off the lint in that line with an explanation. OTOH we'll have to wait. With async Rust will have a lot more parallel code, and we'll see if the above becomes an issue or not. Better workarounds may exist.
AFAIK it doesn't warn about unused result.
You have to specifically import the trait into scope i.e use space_rx::ApiRequest; In the main file.
You have to use ``` use space_rx:: ApiRequest``` to bring the trait into scope and the you can use the methods that defined by the trait.
TIL. ... if we have those, then why do we not have the ability to define functions by multiple declarations composed of nonexhaustive/refutable patterns which, taken together as match-arms, form an exhaustive/irrefutable match? e.g. fn foo(Some(x): Option&lt;T&gt;) { unimplemented!() } fn foo(None: Option&lt;T&gt;) { unimplemented!() }
It's an interesting spectrum... I'd like not the "dependencies are a huge pain to add" of C++ and not the "compile 16 different versions of bitflags" of Rust but somewhere in between.
You're right about `Drop` playing a pivotal role, but it looks like it's reversed from what you're saying. I think it's more that `Service` is running in a background thread and performing actions. When you run this code: Service::new(); wait_for_exit(); The `Service` won't be around until the end of the `wait_for_exit` statement, rather it will be created and dropped immediately, so `Service` can't serve requests. If you do this instead: let _service = Service::new(); wait_for_exit(); Then the `Service` will live until the end of `wait_for_exit()` and can serve requests until that time. There are obviously changes that you could do to architect this differently, for instance you could use a memory arena to hold onto service instances, etc... But without seeing the guts of their code and understanding their decisions, I'm just making sweeping generalisations here. 
[removed]
Ahh, well that was it. I guess I thought when I wrote `impl ApiRequest for CompanyRequest`, I was "applying" those methods onto the `CompanyRequest` type itself. Ergonomically it feels weird for users to now have to `use space_rx::v2_api::*;` and `use space_rx::ApiRequest;`, so I guess I'll have to work out something there...maybe a v2\_api prelude.
Hi, uanirudhx! Thank you for your feedback! * I assume you mean Cow made generic over type T? * I can't completely discard whitespace because I need it to know when to stop parsing certain things. Did you mean something else that I'm not understanding? * I've never seen this syntax in enums before and a google didn't yield anything in regards to this. Could you post a small snippet of code? Again, thank you for taking the time!
Your is\_possibility() can be more concise: `pub fn is_possibility(&amp;self, other: &amp;CountedWord) -&gt; bool {` `for (character, number_in_other) in other.letter_counter.iter() {` `let number_in_self = self.letter_counter.get(&amp;character);` `match number_in_self {` `Some(number_in_self) =&gt; {` `if number_in_self &gt;= number_in_other {` `// this letter doesn't rule it out` `continue;` `} else {` `// we don't have enough of this letter, not a possibility` `return false;` `}` `}` `// we don't have this letter, not a possiblity` `None =&gt; return false` `}` `}` `true` `}`
Thanks for the example. Do you know of any other resources on this topic? I'd be particularly interested in examples of correcting nested match statements. 
In Servo, some types become uninhabited opaque wrappers depending on the build config, mainly to make other types definitions not have to depend on the build config. Wouldn't the auto-never transformation make the uninhabitedness leak through match expressions?
No, I mean `Cow&lt;str&gt;`. For your second point, that is something I didn't realize. For the third point, whoops! it doesn't actually work, you have to replace `Node::List` and `Node::Id` with `Node`. Your `Node::List` definition doesn't make sense; apparently you want to store the first element separately from the rest? Why not just use one single `Vec`? (see below) ``` #[derive(Debug)] pub enum Node { Id(String), List(Vec&lt;Node&gt;), } ``` The recursive datatype would be defined as: ``` #[derive(Debug)] pub enum Node { Id(String), List(Node::Id, Option&lt;Box&lt;Node::List&gt;&gt;&gt;), } ```
While that's common sugar in functional languages, I feel like due to Rust's higher level syntactic noise, it significantly reduces code readability compared to just using a match.
I see your point. I originally wanted to keep the arguments separated from the function for ergonomic reasons, but this breaks down when you have lists like (0 1 2 3) which doesn't have an operator. I have changed it back to Vec&lt;Node&gt;. As for your bottom code listing.. this definition doesn't compile? Is this a nightly feature or something?
"Do not fall into the trap of anthropomorphizing the borrow checker"
Not OP, but I'll reply: * Yes, the user could keep their string, and even pass in a string they borrowed from someone else. * &lt;good&gt; * In `discard_whitespace`, you should use an `if let Some(x) = chars.peek()` block. For the `assert_eq!` in `parse_list`, you should return a `Result::Err(e)` if you didn't get the right character, or `Result::Ok(list)` if you successfully parsed a list. Same with `parse_node`. * The debug build has no optimizations afaik and contains debuginfo. The release build has optimizations and no debuginfo, which is probably why it's faster.
Rust has a history of features like this starting out as macros, then advancing into language features. The most obvious example is `try!` becoming `?`, but I'm sure there are others. A macro is the best place for this functionality to be tried and explored.
&gt; One of the main use cases for uninhabited types like `!` is to be able to write generic code that works with `Result` but have that `Result` be optimized away when errors are impossible. So the generic code might have a `Result&lt;String, E&gt;`, but when `E` happens to be `!`, that is represented in memory the same as `String` – and the compiler can see that anything working with `Err` variants must be dead-code. I don't understand this part. When writing generic code, you have to assume that `E` might be inhabited and handle that case, right? Why would `!` be useful there?
So what should you do with a Mutex&lt;()&gt;? It would be good to just ```let _ = mutex.lock().unwrap()```, but it drops immediately, so if I use ```let _lock = mutex.lock().unwrap()```, am I assured by the specs that the _lock will only be dropped when it leaves its scope and not before, because it's not used? I know NLL automatically drops references, but will some optimization ever automatically drop unused variables?
You can build the method you suggested outside of std, via an extension trait: [https://play.rust-lang.org/?gist=0d47d1c80f292df9628b15aadcb34493&amp;version=nightly&amp;mode=debug&amp;edition=2018](https://play.rust-lang.org/?gist=0d47d1c80f292df9628b15aadcb34493&amp;version=nightly&amp;mode=debug&amp;edition=2018)
If I recall correctly, the commonly used algorithm for this just creates the key for a HashMap by sorting the characters of each dictionary word alphabetically (e.g., 'small' becomes 'allms'). Initialize each HashMap value with an empty Vec and push the dictionary word to the Vec. Then to find anagrams of some word, just sort it by character, look-up that key in the HashMap, and return the resulting value (Vec of dictionary words with the same key). 
I don't get it, how were empty enums different than ```std::mem::unreachable_unchecked```? I mean, the compiler proving is impossible is awesome, but the example is kinda bad, it made a function impossible to call and then it didn't call it, but that's not useful at all, all of them are incomplete. I didn't even manage to understand how to implement the let x = Some(22); match x { Some(x) =&gt; println!("{}", x), }
When writing generic code, you have to handle the case where `E` is inhabited, but that means you're writing a whole bunch of code that won't be used if `E` is not inhabited. However, if the compiler can prove to itself that the extra code won't be used (for example, because `E` is not inhabited), the compiler won't compile that extra code, and the resulting program can be as efficient as one that never wrote any error-handling at all. Zero (runtime) cost abstractions, yeah!
You might have a generic *interface* that demands an error type, but specific implementations can't fail, so they use a void type. If you're using one of those specific types, you can leverage that fact to avoid handling the (impossible) error case. It also means that when the compiler monomorphises those generics, you don't pay for the `Err` variant if it can't ever be used.
&gt; am I assured by the specs that the _lock will only be dropped when it leaves its scope and not before, because it's not used? You are, because it's a valid name. Basically in my view you'd immediately get a lint error explaining that you are accidentally dropping the `Mutex` without using it (which is honestly a great justification for the lint!). Basically `_foo` is a perfectly valid name you can use anywhere, but you are prevented by lints. &gt; I know NLL automatically drops references when they are not used anymore, but may some optimization ever automatically drop unused variables? That's not what NLL should really be about. NLL realizes that even though multiple references exist, the way they are used is legal. When we actually drop/delete objects remains at the same place and it will as it's one of the fundamental needs of RAII and one of it's benefits: you always know exactly when memory is allocated and freed without being specific (vs. GC which you never know when you'll pay the cost of deleting). So that's also a guarantee: things are only deleted at the end of the block they were created on.
I see, thanks!
Doesn't bindgen use enums with no variants as opaque pointers? How does this interact with that? Is "pointer to never" special-cased?
Saw this on /r/ProgrammingLanguages; apologies if it's already been x-posted and I missed it.
Thanks! This is exactly the sort of thing I was looking for. It seems like I should spend some time getting more familiar with the "functional" aspects of Rust. I never reach for those in Python, but without comprehensions available I just forget about them and fall back to a for loop apparently.
Both of those options are great! I'm impressed that you got it down to basically a one-liner. Thanks :)
Working on implementing a geneic react algorithm clone in rust. Had to switch to nightly to get proc macros for attributes because macro rules are to painful.
53% larger (1752x1054) version of linked image: https://image.prntscr.com/image/7o-9ONCZSUS3uZ3iAsFUZg.png ***** ^[source&amp;nbsp;code](https://github.com/qsniyg/maxurl)&amp;nbsp;|&amp;nbsp;[website](https://qsniyg.github.io/maxurl/)&amp;nbsp;/&amp;nbsp;[userscript](https://greasyfork.org/en/scripts/36662-image-max-url)&amp;nbsp;(finds&amp;nbsp;larger&amp;nbsp;images)&amp;nbsp;|&amp;nbsp;[remove](https://np.reddit.com/message/compose/?to=MaxImageBot&amp;subject=delete:+e45ozw0&amp;message=If%20you%20are%20the%20one%20who%20submitted%20the%20post%2C%20it%20should%20be%20deleted%20within%2020%20seconds.%20If%20it%20isn%27t%2C%20please%20check%20the%20FAQ%3A%20https%3A%2F%2Fwww.reddit.com%2Fr%2FMaxImage%2Fcomments%2F8znfgw%2Ffaq%2F)
Wrong sub?
Result is tagged `must_use`, if you don't use it somehow you get a warning. 
&gt; let pile = CountedWord::new(&amp;env::args() .skip(1) .fold(String::new(), |a, x| { a.push_str(x); a })); A bit easier: let pile = CountedWord::new(&amp;env::args().skip(1).fold(String::new(), |a, x| a + &amp;x)); 
I believe lint is used to refer to such things. Specifically default deny lints that cause compiler failure. They are considered lints because the compiler isn't causing the failure but a best practice is so allowing opting in to allowing them to fire during a build is fine. For instance generated code could allow the lint to avoid special casing the empty case.
Not really. At the end of the day, the Rust grammar tells you what's allowed. Everywhere where you see `PATTERN` you can use a pattern. I'd just skim through it with the playground open, and try all the places out.
^The linked tweet was tweeted by [@rustwasm](https://twitter.com/rustwasm) on Aug 13, 2018 21:56:11 UTC (46 Retweets | 113 Favorites) ------------------------------------------------- Just published `wasm-bindgen` 0.2.16 🎉🎈🎉 Changelog: [https://github.com/rustwasm/wasm-bindgen/blob/master/CHANGELOG.md#0216](https://github.com/rustwasm/wasm-bindgen/blob/master/CHANGELOG.md#0216) At the same time, we also published the first release of `wasm-bindgen-futures`, a crate for seamlessly converting between JavaScript Promises and Rust Futures! 😻 [https://crates.io/crates/wasm-bindgen-futures](https://crates.io/crates/wasm-bindgen-futures) ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
Here's my Rust anagram generator [sugarmantra](http://github.com/BartMassey/sugarmantra) for comparison. I also have some [word lists](https://github.com/BartMassey/wordlists) that are a bit nicer than `/usr/share/dict/words`.
Still learning Rust, so there are few things I'm trying to wrap my head around, especially with borrowing. I have a mutable struct with two properties, a `Vec&lt;u8&gt;` and a `u8`. I want to add the `u8` into the vector at a specific index, but by doing `thing.write(address, byte)` (where thing is the struct, address is the 'index' and byte is the u8) I'm hitting the issue that I've borrow `thing` mutably and can't borrow `byte` at that point. A workaround is using a temp value that copies the `u8` (if I understand correctly?) and therefore doesn't have the borrow issue. My question is mainly, what would be the correct way to do this? Would it to simply use a temporary value? The example below demonstrates what I'm talking about. I have the following example ([playground](https://play.rust-lang.org/?gist=9fc2e152c220a748b0b0f5b3c41c11f7&amp;version=stable&amp;mode=debug&amp;edition=2015)) struct Thing { pub list: Vec&lt;u8&gt;, pub a: u8, } impl Thing { pub fn write(&amp;mut self, addr: usize, byte: u8) { self.list[addr] = byte; } } fn main() { let mut thing = Thing{list: vec![0, 0, 0], a: 0xAB}; // Works, but feels like a workaround? let temp = thing.a; thing.write(0x00, temp); assert_eq!(thing.list[0], 0xAB); // Not allowed thing.write(0x01, thing.a); assert_eq!(thing.list[1], 0xAB); }
1. Love this effort. Thank you! 2. Since you’re apparently happy to improve the benches :), I would love to see a variant of the benches after force merging to only one segment. Or even better multiple runs with different number of normalized segments (I.e. both engines with 5 segments, both with 10, etc). Without normalizing the segments, the difference in performance could easily be attributed to a more aggressive merge policy in Tantivy. P.S. I tried looking at the Readme and code but couldn’t easily tell if the segments were getting normalized down to the same number.
thanks God no ^^ SVN
SVN, not this bad but come on, it's 2018…
SVN, so not this bad, but come on, it's 2018…
[removed]
Link to the documentation here: https://rustwasm.github.io/wasm-bindgen/api/wasm_bindgen_futures/
&gt; Observation #1: stick to sequential consistency unless you really need the performance edge of weaker orderings. &gt; &gt; Observation #2: stick to sequential consistency unless you are really, really smart and have really really smart people checking your work. No, not really, sequential ordering is usually unnecessary. It's often sufficient to use `Release`/`Acquire` orderings. What is dangerous however is `Relaxed` ordering. The code using it is usually subtly wrong.
Allocation might fail, aborting the program. Moving the allocation to a different place (i.e. outside the loop) might result in different behavior, making it an invalid optimization. This applies to any side effects. You would get the same problem if you make every allocation print something: Moving the allocation out of the loop would result in less output being printed. A similar case where this matters is bound checks: `a[0] + a[1] + a[2]` cannot be optimized to a single bound check due to possible out of bound panics, but `assert!(a.len() &gt;= 3); a[0] + a[1] + a[2]` can.
In the published run, \- lucene had 4 segments \- tantivy 0.7 had 4 segments. \- tantivy-0.6 had 23 segments. I can certainly optimize it to 1 and see what happens.
I just watched the whole talk. It's very interesting. I thought it was cool how much he gushed about Rust in the end. Rust has this reputation of being a language of hipsters and programming newbies, but this is a veteran of the industry getting excited about Rust.
Is [this](https://github.com/lise-henry/books) the most up to date copy of the rust book available that I can just *download and read offline with minimal additional effort*?
Probably not. That is over a year old, and TRPL is currently at Rust 1.21 IIRC, and receiving changes quite often, so the pdf/epub is probably gonna be outdated.
(Take this with a grain of salt; I'm not an expert.) A lot of research in machine learning goes to studying the behaviour of different classes of models (= the state that represents the learned knowledge of the system) in use. Some naturally tend to converge into a single global minimum – they are called convex (linear regression is an simple example of this). However, even if models are not convex, there can be mathematical or experimental work that show that "on average 30%" of local minima are within the "10% of performance of the global minimum" or something like that. If one is able to do that, that gives some reassurance that at least we are able to train a "reasonably well performing" model, especially if we do that with many random seeds.
I don't get how would that be coded. Why doesn't anybody provide an example? Sorry, but it's kinda hard to get how the syntax works, I get what it is. Use my example, None is impossible, how to do that match without None being in there without the unreachable_unchecked?
For what its worth. Somewhere around 1:20 he talks about a lifetime issue where Bryan describes wanting to tell the compiler ' no i'm done with that ' and casually mentions using 'unsafe'. If i understand the problem correctly [this](https://play.rust-lang.org/?gist=53934d58005b47731c61ea32a0a64d72&amp;version=stable&amp;mode=debug&amp;edition=2015) solution is way simpler. 
This is just a cultural mismatch. Void in C and its progeny does in fact (in as far as they manage to treat it as a first-class type at all) mean a unit type with a single inhabitant. Meanwhile in FP and type theoretical circles `Void` *is* actually used as the name for the bottom type with zero inhabitants. I'm not sure where this mismatch actually originates (but it sure is annoying).
Maybe... What relaxed guarantees is that you will never read a value that wasn't ever stored, but that's about it. What may happen however is this. - Value A is stored into atomic - Value B is stored into atomic - Atomic is loaded, the retrieved value is B - Atomic is loaded, the retrieved value is A If that's fine for your application, it's probably fine?
Yes, or NLL.
I guess I should have mentioned that I want to keep it more general, I actually have 3 u8 properties that I want to be able to write at an address. If it was only ever self.a, then this would be perfect 
Hm, I'm under impression that after a thread has read a value B from the atomic, it's never reverted to an earlier value A within the same thread? That is, a single thread may load A and then load B, or load A and never load B (* side note) but it's guaranteed that it never loads B and then A...? Side note: although I'd be quite interested to know what kind of situation (that is: is it theoretical, practical but rare, practical but common) it is that B is never observed, even if the threads continue to access it in the timescale of seconds; there being multiple context switches, cache invalidations etc.
Within a single thread, there is an implicit ordering. However, if your application is a web server and requests are handled by different threads, it's possible for your application to retrieve a request, send value B, then get another request, handle it in another thread and send value A.
A wrapper wrapping an uninhabitated type is itself uninhabitated. Instantiating `struct Wrapper&lt;T&gt;(_: T)` with T=! yields the uninhabited type `Wrapper&lt;!&gt;`.
&gt; Hm, I'm under impression that after a thread has read a value B from the atomic, it's never reverted to an earlier value A within the same thread? Not guaranteed, due to possibility of optimizations reordering loads.
After reading the documentation\*, I end up in a different conclusion. The updates to the \_same\_ memory addresses are always ordered. The reordering has to do with loads or stores with regards to \_other\_ memory addresses. \*: Rust docs: [https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html#variant.Relaxed](https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html#variant.Relaxed) LLVM docs: [http://llvm.org/docs/Atomics.html#monotonic](http://llvm.org/docs/Atomics.html#monotonic)
Yeah, seems correct, according https://stackoverflow.com/questions/27333311/will-two-relaxed-writes-to-the-same-location-in-different-threads-always-be-seen. My mistake.
My intend is to automatically overload the + operator for every Vector&lt;T&gt;, where T implements the Add trait
Please provide code as a text next time. It makes easier for others to do quick modifications. Your issue lies in the fact that `T + T` is not guaranteed to return `T`. It might return some other type `U`. So after adding the components of your Vector, the new Vector has elements of type `&lt;T as Add&gt;::Output` and not `T`. [Here is a fixed version](https://play.rust-lang.org/?gist=25e1f7abc06215383c789e1382a91c9d&amp;version=stable&amp;mode=debug&amp;edition=2015). I only needed to change the `Output` type.
I like to use both `include_str!` and `include_bytes!` for loading assets, at least in the beginning of a project. However, this can slow me down significantly when I iterate on the assets, since each iteration requires a recompile. On the other hand, loading the assets at run-time has typically been too much of a disctraction for me to implement. Not any longer: I have made `load_str!` and `load_bytes!` specifically to be mostly drop-in replacements for their compile time equivalents. Maybe some of you can also have use for this :)
You can't use `!` with `None`, because None doesn't have any type parameters. You can use it with `Some` though: #[feature(never_type)] struct EmptyIterator; impl Iterator for EmptyIterator { type Item = !; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { None } } fn main() { match EmptyIterator { Some(!) =&gt; println!("This code will be eliminated"), None =&gt; println!("The compiler can realize the match is unnecessary") } for value in EmptyIterator { // This will also get removed } } Part of what this is discussing is that if you don't provide a `Some()` match pattern, the compiler will automatically insert a `Some(!)` for you, then check to see if it makes sense. The key difference between never types and `unreachable_unchecked` is that the latter requires reasoning on the part of the caller (this value *will* never be possible), while the former is a type-level assertion (this value *can* never be possible). The latter works with generic code too, e.g. anything that takes an Iterator.
That makes sense, thanks, but it's a huge bummer, the compiler could help in other situations too. It's too restrictive to have to be a enum variant with a value. My example uses pairs of variants of two enums without values, some are impossible, but can be checked at compile time.
Fantastic work. I was unaware that webrender, and servo in turn, depended o the gfx crates. 
I'm not exactly sure what your data looks like, but this is what I came up with: extern crate itertools; // 0.7.8 use itertools::Itertools; fn fix_text&lt;'a, I&gt;(iter: I) -&gt; String where I: Iterator&lt;Item=&amp;'a str&gt; { let no_newlines = iter.flat_map(|s| s.split('\n')).collect::&lt;String&gt;(); no_newlines.split_whitespace().intersperse(" ").collect() } 
`void` is probably not a good idea. I could easily see newcomers from C/C++/C#/Java trying to write code like this: fn foo(x: u32) -&gt; void { println!("x is {}", x); } Because of the way `void` is used in those languages. I like `never` personally and that's what TypeScript uses (I think) but it would probably require making `never` a keyword and so it will have to wait for an epoch. 
the information for my example is all there; i'm not going to hold your hand through reading 2 more sentences outside of the code block. the option example is probably: match x { Some(x) =&gt; // ... ! } 
Yeah you are wrong... Other people just came here to say it's impossible. Because the article doesn't make things clear. Not even you, all cocky, understood it...
Didn't know `Iterator::collect` is a thing for String. That alone makes things look way better. intersperse seems good too but I don't want an additional dependency to use just a function. Thanks for help.
The gfx webrender implementation is being worked on [by the university of szeged](https://github.com/szeged/webrender), and isn't part of the master branch of servo as of now
Tell that to my old employer :)
I use the following simple snippet for this, handling errors in the caller: std::fs::read(path) .map(Box::from) .map(Box::leak) or `read_to_string` respectively where I want `&amp;'static str`. What's my advantage with the macro provided by the crate?
Why isn't OpenGL backend used for Win7 ?
Ah, that makes sense. Thanks for the explanation.
Well, look at that! That's convenient! I might not have created this crate if I knew those, but now that it's here, let me point out the advantages I can think of: * The `load_file` crate produces better error messages because it makes sure to include the file name * You can switch between `load_` and `include_` without changing the path * The returned types are more similar, including the `'static` lifetime 
SQS has push and pull functionality. With push, you are tied to the request or lambda time limit. With pull, you can pull the messages and take as much time as you want to process them. You can set up workers quite easily with ElasticBeanstalk with one of the supported runtimes, or with docker. ECS/EKS are also options.
The advantage of `load_file` here I guess is primarily that it makes sure to include the file name in the error message. If you start out with `include_*` and no error propagation, switching to `load_file` will be slightly more convenient. As I noted in another comment, I might not have made this crate if I knew `read` and `read_to_string` :)
If you run `rustup doc`, that opens a local copy of the entire set of Rust documentation in your browser, including the book.
Thanks :) I hope it's understandable and helps you with learning more about Rust.
OpenGL support on Windows is not great. Implementations are generally buggier than DirectX and performance can be subpar, especially on AMD GPUs. It still works *fine*, lots of OpenGL software works great on Windows; ID Software has been releasing AAA games with OpenGL for decades and they run great after the drivers get updated. However, if you want the best performance and compatibility you need to use DirectX. Which is why all (?) browsers on Windows use [ANGLE](https://github.com/Microsoft/angle) to translate WebGL into DirectX.
You could do something like this: enum Property { A, B, C } impl Thing { fn write(&amp;mut self, addr: usize, prop: Property) { match prop { A =&gt; self.list[addr].push(self.a), B =&gt; self.list[addr].push(self.b), C =&gt; self.list[addr].push(self.c) } } } To make the original code work, you can use my crate [unborrow](https://crates.io/crates/unborrow): unborrow!(thing.write(0x01, thing.a)); 
Beautiful work gfx team!
Does this mean implementing D3D12 interfaces with D3D11?
And then `Void` in Java is fun... You can't instantiate `Void`, so it's an uninhabited type, thus the only valid value of type `Void` is `null`, since that doesn't claim to be a valid `Void`. A situation like this is probably where the mismatch comes from. `Void` the object is uninstantiable but a variable of type `Void` can only be `null`, so `Void` is simultaneously the unit type and the never type.
Unlike references, raw pointers have no requirement to point to a valid instance of a type. So I don't think anything about `*const !` would change.
More generally, the number of inhabitants of a struct is the product of the number of inhabitants of the field types (thus, "product type"). The number of inhabitants of an enum is the *sum* of the number of inhabitants of the enum alternatives (thus "sum type").
How to pipe stdout of one process into another *and* wait for both? let mut first = std::process::Command::new("cmd1"); first.stdout(std::process::Stdio::piped()); let mut first = first.spawn().unwrap(); let first_out = first.stdout.unwrap(); let mut second = std::process::Command::new("cmd2"); second.stdin(first_out); let mut second = second.spawn().unwrap(); println!("{:?}", first.wait().unwrap()); // &lt;-- error here println!("{:?}", second.wait().unwrap()); This doesn't compile since `stdout` is being moved from first `Child`, but it isn't cloneable.
Doesn't rocket do the same thing? What is different about this? 
If you mean implementing the COM interfaces then no, but gfx's api is very similar to Vulkan, which shares a lot of new concepts with D3D12
Great example of the community kicking in and helping figure out a better answer. Thanks for sharing!
Return Vector2&lt;T&gt;(...) rather than just Vector2(...) since the type is unknown in the add situation
Rocket needs a compiler plugin to do its magic, which means it only works on nightly (plugins rely on unstable compiler internals).
Tower Web and Rocket both use macros, but the similarities end there. * Tower Web support stable Rust, Rocket requires nightly. * Tower Web is fully async and built on Tokio. Rocket uses synchronous, blocking I/O. * Tower Web focuses on decoupling HTTP and application logic. Rocket requires HTTP concerns in application code. There are more differences, but those are the first 3 I thought of.
A keyword is not needed (for types), `enum Never {}` in the prelude and off you go :)
Okay, I'm probably missing something. How would I use futures in WebAssembly? Does the user have to drive the event loop from JS? Who calls poll? How does this work with non-blocking code?
Can you compare to actix web also?
I am not sure what to say specifically, actix web is quite different.
Looks great! I'm not a macros expert (and I'm a bit out of the loop with what's stabilizing with Macros 1.2): what's the reason for the `impl_web!` macro? Or roughly, why isn't the following expressible: ```rust #[service] impl JsonResource { #[get("/")] fn hello_world(&amp;self) -&gt; Result&lt;serde_json::Value, ()&gt; { Ok(json!({ "message": "hello world" })) } } ``` (Again, this looks _super_ cool! I can't wait to give this a spin.)
&gt;Tower Web focuses on decoupling HTTP and application logic. Rocket requires HTTP concerns in application code. What do you mean by this?
Wooh, go dtolnay - another rust dev that's had a ton of positive impact.
Sorry, I'm taking that comparison back :P It is subjective and I try (and failed in this case) to not talk subjectively about other libs. One reason being that I am really not familiar enough with Rocket to talk about it.
I don't want my environment to have anything to do with the build environment. The fact that build.rs scripts usr this all the time I a source of a lot of my frustration. But it's almost always due to working with cmake or whatever.
&gt; Custom attributes are unstable, so you wouldn't be able to write the #[service] attribute in stable Rust. This is the primary reason why, for instance, Rocket requires nightly. The impl_web! macro takes its input, scans for tokens that resemble attributes, and transforms those into what are effectively invocations to a custom derive, which are, of course, stable. That's bananas—thanks for explaining! Since you might have a closer view on custom attributes that most—am I correct thinking they're meant to stabilize 1.29, or am I misinformed? (iirc, there was some weird interaction with the new module system...)
Well... What I am looking for is setting an environment only for the build. My problem is with crate llvm-sys. I need to override system-wide llvm configuration with a local one.
The trouble is that I want the bool to be set to false \*only\* in the event of an early return. The logical flow looks like this: \`\`\` // next\_sep is true loop { next\_sep = false; accum = func(accum, sep)?; next\_sep = true; accum = func(accum, element)?; } \`\`\` The optimization is that I don't want to keep setting next\_sep to true/false/true/false. So, instead, I only mutate it in the event that \`?\` causes an early return.
I think the OP is asking for something like `cargo-audit`, but extended to include serious, but not necessarily security-related, bugs.
I do agree with his perspective writing a linked list first. It's a trap that a lot of people fall into. I would say my experience is outlier in that it exposed to me what the borrow checker was doing and eventually appreciate it once I jumped through all the hoops but I can definitely see how for many it can just kill their enthusiasm for the language.
The tables were/are hand tweaked / hand crafted. The emulation for instance is pure rust and not copied from anywhere since pulling math from C -&gt; Rust can have odd consequences. So yeah Amanda did all that, call it NIH syndrome or just lack of fear of daunting tasks. We met a guy at Black Hat who has written a suite of unit tests for disassembly that we may try integrating at some point.
[Moved into "Easy Questions" from top level post since it's probably an easy question ...] Ok, so I have been using Rust for over a year now and feel really stupid asking this, but why does the generated Rust documentation not produce links to **all** the available methods / traits that apply to a type in a given crate? Very specifically: [the rand crate](https://docs.rs/rand/0.5.5/rand/). I just wanted to produce a random number. No big deal. Remember there was [`rand::thread_rng`](https://docs.rs/rand/0.5.5/rand/fn.thread_rng.html). Open the documentation for that method, which returns a [`ThreadRng`](https://docs.rs/rand/0.5.5/rand/rngs/struct.ThreadRng.html). Cool, open that one. See that `ThreadRng` only implements [`RngCore`](https://docs.rs/rand/0.5.5/rand/trait.RngCore.html) and [`CryptoRng`](https://docs.rs/rand/0.5.5/rand/trait.CryptoRng.html). Neither one has the method I'm looking for. Now I start to read for real, and see that [`Rng`](https://docs.rs/rand/0.5.5/rand/trait.Rng.html) is "an automatically-implemented extension trait on RngCore ". Still, just glossing over things, there does not seem to be any auto-generated (structural) documentation connecting that with `ThreadRng`. I never really noticed that before, but now that I see it, I find it mildly confusing. 
Related (but different): [pipeline](https://crates.io/crates/pipeline).
Thanks for the explanation!
If you want to talk about it, I can help mentor adding websocket support. Ping me here: https://gitter.im/tower-rs/tower
Yep. llvm-sys is the crate I was struggling with. As a fun bonus, if you build with debug symbols, your target dir will be 3G. Enjoy! I say bring on Cretonne!
Not easily. You can probably set up a .NET environment somehow, but it's going to be _really_ painful.
Defining attribute macros is set to stabilize in 1.29 while importing them is set to stabilize in 1.30. That means 1.29 will let you implement custom attributes but not use them. Long story!
This is neat and I have had use cases for something like this. But this is a confusing name. By name I thought this might be something related to fluent-rs, or project fluent.
I understand. But the name is not a random choice. When I asked about what terminology to use, I was told that *chaining methods* is what Rust users usually use, but [*fluent interface*](https://en.wikipedia.org/wiki/Fluent_interface) is the common terminology used in other languages. So I opted to use the common terminology in the name, and the one Rust users use in the description.
TIL Cretonne Btw, apparently they have changed the name to Cranelift.
That's gonna be really tricky. The problem is that C# doesn't compile down to native code as far as I'm aware, but rather an intermediate form. You'd need a runtime that is capable of understanding that intermediate form for it to run, and getting into that state from Rust would be really weird. If you can find some way to compile the code down to native code and can make it expose a C abi compatible interface then calling it would be just like calling C, but I'm not sure if something like that exists.
Here is another way to write the same Deserialize impl, without a Visitor. [**playground**](https://play.rust-lang.org/?gist=def7c3698647cbc0a5c985470364cbfb&amp;version=stable&amp;mode=debug&amp;edition=2015) // Private helper. #[derive(Deserialize)] enum StringPublishState { Published, Unpublished, Pending, } // Private helper. #[derive(Deserialize)] #[serde(untagged)] enum UntaggedPublishState { String(StringPublishState), Bool(bool), } impl&lt;'de&gt; Deserialize&lt;'de&gt; for PublishState { fn deserialize&lt;D&gt;(deserializer: D) -&gt; Result&lt;Self, D::Error&gt; where D: Deserializer&lt;'de&gt;, { use self::StringPublishState::*; use self::UntaggedPublishState::*; match UntaggedPublishState::deserialize(deserializer)? { String(Published) | Bool(true) =&gt; Ok(PublishState::Published), String(Unpublished) | Bool(false) =&gt; Ok(PublishState::Unpublished), String(Pending) =&gt; Ok(PublishState::Pending), } } } 
Congrats and thank you for all your work!
&lt;3
Making progress on my S3D (also does EQG) file reader. This is an old format used for everquest data (mostly dds, bmp, and world files). I've got the thing correctly loading, parsing, and unzipping files, but I don't have an external API yet to allow a library user read data. If you want to take a look (and please tell me if I'm doing something wrong!) it's here: [s3d](https://github.com/addtheice/s3d). Also, I have a quick question. I'm trying to create a Read/Write implementation over the compressed/uncompressed data contained within the zipped files. My idea was to create a enum with Compressed(Vec&lt;Vec&lt;u8&gt;&gt;) and Uncompressed(Vec&lt;u8&gt;). The idea is that you can read the compressed data or the uncompressed data as well as *write* compressed data or uncompressed data and the type will just 'handle it' under the hood since there is issues with breaking up data into certain sized chunks for the compressed data then concating these chunks after they have been uncompressed. My issue is that...how do I create a standard read implimentation of fn read(&amp;mut self, buf: &amp;mut [u8]) -&gt; Result&lt;usize&gt; { } When I need to write my buffer into their buffer? I can't seem to figure it out. It seems like it has to be something really simple...but I'm not getting it. My goal is to implement the Read &amp; Write traits so I get all the lovely goodness which comes from the standard library. 
The reason the variable isn't used is because it isn't directly referenced anywhere. I think it's very rare in practical cases that we encounter a situation where the difference between _ and _my_var is important. Usually, if you need a value to live longer than the statement that produces it, you would bind it so you can pass it somewhere else. Hidden behavior like this, where constructing a value somehow updates global state which is then acted on by a function behind the scenes, makes it harder to tell what's going on in code. In Sunday, I think this behavior of rust is a good thing. Being punished by it requires that you be doing something kind of dirty, and if you really need that thing, just like everything else in rust there's a way around it.
Thanks for that. Yes, I was aware of that but kept it out of the discussion to keep it simple.
Do you think it's a good idea to link to the reddit thread (when available) next to each line in "News and Blog Posts" section?
There's a level between proving acquire/release correct and just using SC and hoping that it is correct: proving SC is correct. The interleaving model of SC is much more intuitive to reason about, and so writing a convincing proof is easier.
Hopefully https://rustwasm.github.io/wasm-bindgen/api/wasm_bindgen_futures/ answers these questions, and if not, maybe you should open some issues :)
try using this to export function from C# dll https://github.com/3F/DllExport and than import it to Rust like you would any other C library
Awesome. rls is making great progress as well. I just saw the quick fixes start to appear in vscode!
Sweet!
Yes please! Many times I wanted to read reddit comments on some of the blog posts.
Typo on the blogpost's link to [RFC 2436](https://github.com/rust-lang/rfcs/pull/2436) it is written "RFC 2346". ;)
Thanks!
I think `Executor` may have been clearer than `Spawn`. Traits don't always need to have the methods named exactly after them. For instance, `Future::poll`, `Iterator::next`, etc.
The one pain point I've had with Rustfmt is that it doesn't work within macro's even when the content of those macros is regular Rust syntax. [quickcheck!](https://github.com/BurntSushi/quickcheck) being one where I had the friction. I would comment out the macro, rustfmt, then delete the comment. It would be nice, if a macro could say its contents can be run as is through rustfmt.
There is a [very recent change](https://github.com/rust-lang/rust/pull/52585) to add "Blanket Implementations" to rustdoc which should list traits like `Rng`. However, [a small bug](https://github.com/rust-lang/rust/issues/53374) means it doesn't seem to work in this case. Hopefully we'll see this soon!
Would running strip on a binary destroy backtrace information?
Try Boxed Fn traits, with arguments and return type.
Closures are effectively structures containing their captured variables, and since different closures may have different captures, there's not a single type representing all closures with a given signature. However, the closure types do implement the Fn / FnMut / FnOnce traits so you can Box them and store the trait object in the HashMap. For example: keys_to_commands: HashMap&lt;Keycode, Box&lt;Fn() -&gt; u32&gt;&gt;
Closures are each a unique, un-nameable type, so you need to dynamicitize (totally a word): https://play.rust-lang.org/?gist=ba0e72df96c4fd680ede0853c36236d4&amp;version=stable&amp;mode=debug&amp;edition=2015
I'm not sure if you were there when I proposed the idea in wg-net, but the reasoning I had was that I had written a number of Executor trait implementations for things that didn't actually execute futures at all, but were capable of sending them to something that did. The executor name doesn't make sense if the functionality provided is task spawning, not running.
A pain point for me is that rustfmt doesn't work in doctests. I have a ton of then and have to format them manually because they are ignored. It kinda sucks, everything ignores it. Any code coverage tool, rustfmt, clippy. It seems doctests are second hand citizens in rust...
I'm having a hell of a time trying to get rustfmt to install. Following the directions in the post, this happens: $ rustup install rustfmt-preview error: invalid toolchain name: 'rustfmt-preview' Someone else here suggested `rustup component add rustfmt-preview`, which does this: $ rustup component add rustfmt-preview info: component 'rustfmt-preview' for target 'x86_64-unknown-linux-gnu' is up to date And doesn't appear to update it. Even after removing and re-adding the component, the version is still: $ cargo fmt --version rustfmt 0.8.2-stable (08da30d 2018-06-06) Any help would be much appreciated!
From ``` #![feature(async_await, await_macro, futures_api)] use futures::spawn; let future = async { /* ... */ }; spawn!(future).unwrap();``` That seems like it executes. Am I missing a default executor that just exists, or does that sample need something like ``` ThreadPool::new().expect("Failed to create threadpool").run(my_app); ``` Been thinking about trying to throw together an example using some of these different features similar to the toykio post.
&gt; The unsized rvalues feature will make it possible to store !Sized types on the stack. I was unaware this was coming. Anyone have a link to more info on this? A specific question I have, can the dynamic trait be unresolved at compile time, or must it resolve to one or more concrete types?
Might be an old install before it was bundled with rustup? Use `which rustfmt` to see where it's installed. And then maybe run `cargo uninstall rustfmt`
Rustfmt does work in doctests, just not by default. You need to define `wrap_comments = true` in rustfmt.toml. Admittedly this is not intuitive and [rust-lang-nursery/rustfmt#2844](https://github.com/rust-lang-nursery/rustfmt/issues/2844) tracks picking a better name for this setting.
This is not obvious from the blog post nor from the API docs, but the name of this crate on crate.io is [futures-preview](https://crates.io/crates/futures-preview)
Oh whoops, I guess I had read it as "`!` is needed when writing the generic code". But yeah, uninhabited types are useful when instantiating `E`.
rustfmt-preview component is a thing since several releases (including stable). I believe the blog means that the version 1.0 rc is the one currently on beta/nightly.
You’re probably looking for /r/playrust this subreddit is about Mozilla’s amazing Rust programming language (which you could probably used to make an archery simulator). If you’re interested in programming, keep this sub bookmarked for a rainy day :)
Which is what I also ment? The full command would be rustup component add rustfmt-preview --toolchain=nightly 
I am about to finish Rust book (currently on the chapter "Fearless Concurrency"). What's next after this? Are there any beginner friendly open source projects to contribute to? My background is in web development but I don't mind and probably want to try out other areas described above as well.
I am primarily a JavaScript developer. I tried many languages including Elixir and Haskell, also dabble in minimal C and C++. Syntax wise, yeah Rust is quite ugly especially after seeing Haskell, it is even uglier than JavaScript. But after a while I don't mind it at all as long as it is readable and straightforward. The language and the environment/tooling just works with no fuss, documentations are great and the tutorials are great. It is very usable. For an average developer like me those long winded manual/documentation helps me a lot. 
https://github.com/rust-lang/rust/issues/48055
Could we also implement something like this for non impl functions? like D's UFCS?
hopelessly so 
Hey thanks man! :)
The subtle difference here is between *edition of the language* and *version of the compiler*. `rustc 1.31` the program adds support for "Rust 2018" the programming language, while retaining support for "Rust 2015" the programming language. Rust 2018 and Rust 2015 are still very similar languages, with a few small differences to keywords and feature stabilization, but will diverge over time. For example, Rust 2018 will gain proper async/await syntax, but Rust 2015 won't. It isn't `rustc 2.0` because the rust compiler has stayed almost exactly the same. A lot has changed behind the scenes, which is why it is a big deal to the compiler devs, but on the surface, the only change is the new `--edition` argument which switches the compiler between the two different programming languages. Rust 2018 isn't important *yet*. People are excited because it represents, in the abstract, a huge milestone. But, honestly, Rust 2018 will really only become important over time, starting with `rustc 1.31`.
WRITTEN IN RUST
Nice to see overlap between rust and idris. Also, choosing rust (in part) for compilation speed is a first. ;)
May be better to use \`Box&lt;dyn Fn() -&gt; u32&gt;\` to make things explicit. Using trait objects without \`dyn\` or \`impl\` can be deprecated in future editions. It is more common to store enum values and execute actions depending on stored value instead of storing closures. Because this way you can locate code responsible for actions and avoid performance penalty of dynamic trait objects.
`spawn!` only works (and is a macro because of that, similar to `await!`, but performing only a single call, without a loop) within an async block/closure/fn, by using the spawner from the context given to `poll`. That is, `spawn!(f)` will spawn `f` in whatever spawner you manually spawned the async code it's in, which is now executing. `f` may not execute for a while, and in a single-threaded system, will *not* start executing until the async code `spawn!` was used in, suspends.
Is `tantivy` running the search in `crates.io`? (If not, what is?) Seems like a great place to demo it if that's possible.
Both `await!(future)` and `spawn!(future)` will cause the future to execute, the difference is the former executes "serially" while the latter "parallelly", with the original future. I believe that `await!(future)` and `await!(spawn_with_handle!(future).unwrap())` are functionally the same (blocking until `future` completes), but the latter is less efficient. You shouldn't spawn a future unless you want to *also* do something in the meanwhile, and only *later* `await!` the handle (if ever), because that gets you some parallelism.
I'm normally all for Rust, but it seems a little silly to write a package manager for Idris not in Idris.
Thanks! Don't know how I missed this
It's the more conservative approach. Wrapping comments might destroy some carefully designed alignment.
* What are businesses using today? Elasticsearch? Lucene? * What are their pain points? Memory usage? Complexity? Installation/deployment? Security? * What value is tantivity bringing to those businesses? * How would they integrate tantivity with their current stack? I am a beleiver in "If you want to build a ship, don't drum up the men to gather wood, divide the work and give orders. Instead, teach them to yearn for the vast and endless sea."
Building bindings for Python and JavaScript is definitely a must if you want to "attack" ElasticSearch market share. But you should only go for it once you are confident that the library is sufficiently stable, to avoid over constraining your future development efforts. In particular, RAM and CPU requirements are one of the major pain points of ElasticSearch, as well as the self-deployment complexity.
This is adressed in the the blog post: &gt; Yes, elba is written in Rust, even though it’s supposed to be a package manager for Idris. This isn’t an ideal state of affairs for a few reasons (it makes extending and using elba in Idris harder, and it just feels wrong), but I would consider this the least of elba’s drawbacks; using another, more stable language which a much-better-established package ecosystem has its own perks: &gt; &gt; - There are actively-maintained libraries for things like interacting with git repositories (git2), parsing structured configuration/metadata (toml, serde_json) and other miscellaneous stuff (url, nom), making http requests (reqwest), etc. &gt; - Rust itself lends itself to okay build times and fairly fast runtime performance (this is most important during dependency resolution); both of these are relative unknowns for Idris (though from personal experience I can say that code generation with Idris is definitely not that fast) &gt; - Plus, I didn’t (and still don’t) know that much Idris when I started working on elba, and attempting to learn the nuances of a dependently-typed language along with solving the typical package manager design concerns would make development a lot slower. &gt; &gt; Maybe in the far-off future the Idris package ecosystem will have matured enough so that elba can be rewritten using itself. :)
tantivy\_cli seems outdated and won't compile, maybe....
`rustfmt` will be updated with the rest of your toolchain AFAIK. So just run `rustup update`.
&gt; as well as the self-deployment complexity. That’s funny, as one of the selling points of elasticsearch in the early days was the ease of setting up a search cluster compared to lucene and solr. 
I haven't tried to deploy Lucene or Solr, but I know for certain that we are paying an expensive bill to get a fully managed ElasticSearch cluster due to the deployment + management complexity.
Incidentally, there is a dependently-typed variant of the whitespace programming language called Idris Elba. I love the fact that there are people willing to put this much effort into a joke.
FYI: [ERROR][its_not_cool] GameError GamepadError("Gamepad error: Gilrs does not support current platform." my system is macOs. anyway thanks
I guess maybe alignment is more important in comments. But this is also the problem I have with auto-formatting my code!
Maybe something like wasm-bindgen could be done for Idris&lt;-&gt;Rust bindings, IIRC Idris has a few features (uniqueness types?) that should allow slightly nicer interop.
Replicating something like this would’ve awesome: https://medium.com/palantir/indexing-and-querying-telemetry-logs-with-lucene
Well, I'm just starting to build a log storage so I'm all in for the log search engine :) even in a couple of months I may contribute if you'd be kind enough to mentor me
Integration tests don't set the *test* cfg. Only unit-tests.
&gt; spawn! only works [...] within an async block/closure/fn Ah, this is a critical detail missing from the documentation. Other macros seem to explicitly state this in the docs, but `spawn!` does not. Moreover, the example for `spawn!` even makes it look like it's normal to use it _outside_ an async block. Looking at the commented out portions of the example in the docs actually clarifies this point. I also have no idea, from reading the documentation, why I would be using `spawn!` instead of `await!`.
Now I understand, thanks!
This does not answer your question and I am not an expert in security; but I'd advise against statically linking against any security libraries that might need to be updated urgently. When you statically link to a library, it becomes your responsibility to ship a new version of your application in case of vulnerabilities. Users might update their system via security patches, but they might never update third party software.
oh cool! &gt; socket_address = ["127.0.0.1:4000", "127.0.0.1:4001"] Are unix sockets supported?
See https://www.reddit.com/r/rust/comments/97dtn3/futures_030alpha3/e47yh5q?context=1 In less words: await blocks, spawns runs independently in the background (sequential vs parallel).
I'm not sure what you mean. Can you give an example? 
`first.stdout.take().unwrap();` [Option::take()](https://doc.rust-lang.org/std/option/enum.Option.html#method.take) transfers ownership of an Option, so you're effectively stealing stdout from Child, leaving None in its place.
We might want to have a Clippy lint against that then. I am tempted to make this my first contribution to Clippy :). Or is it too early now? Should I wait for futures 1.0?
You mean turning the function into a chaining method? Or just returning its first input?
You should bring this up on the repo, to make sure the arguments are addressed :)
Yes, my question here is inspired from your issue!
&gt; As part of it's stability guarantees, it's formatting will be frozen (at least until 2.0). Is there any plan for how frequent major releases (that can break the formatting) will be? It would be quite inconvenient to have to wait, say, a year, for formatting improvements.
Don't know about that book in particular, but the [official book](https://doc.rust-lang.org/book/) is available for free online (and the second edition as a hard copy as well). But I doubt there is anything wrong with the one you have, might also be worth reading both for some of the more advanced sections or comparing their contents to see if they cover slightly different areas. It can also help having more examples from both the books. There is also [rust by example](https://doc.rust-lang.org/rust-by-example/) and the [rust cookbook](https://rust-lang-nursery.github.io/rust-cookbook/) which should supplement the materials from either book to give you more examples to work from. As well as the [std library documentation](https://doc.rust-lang.org/std/index.html) which you can reference as required after you have been introduced to the different parts in the books.
This feature ("formatting anything related to macros") is not part of rustfmt 1.0. 
It also can actually break doc tests depending on what they do. Basically, rustfmt 1.0 won't format comments IIUC.
for example, ``` fn add(a: i32, b: i32) -&gt; i32 { a+b } ``` I can use it like `10.add(2).add(3)` etc
I don't think you can do this with types you didn't define without traits.
Hi! Just pinging to let you know that I've since submitted a [PR](https://github.com/danburkert/bytekey/pull/4) for this in case you missed it :)
How does rustfmt’s macro rules work? A while back macros were hardcoded and that included *some* community crates. Is that still the case?
Having written proofs for concurrent SC algorithms, I disagree. The interleaving model is hell to reason about, because of rampant non-determinism. Once things get subtle, you want to reason with some kind of separation logic and use ownership transfer. However, ownership transfer is exactly what release-acquire gives you -- so you might as well use that.
Yes, thank you. :) Could I invite you to partake in the discussion at https://github.com/rust-lang/rfcs/pull/2503 ? I am pretty alone there arguing against the common meme that "anything but SeqCst is black wizardry"...
It's a good book. And as mdaffin said there are several online resources for learning as well.
Oh maybe I should clarify. I don't think rustfmt should necessarily mess with text comments. I wish it would format blocks of Rust code between triple ticks though (the blocks that get automatically executed as doctests).
It's a pretty good book -- try to do the exercises too, even just to get a feel for typing Rust code :) Bonus tip: If you install Rust via rustup, it comes with a truckload of documentation incl. the official books and std library docs -- just run `rustup doc`. This is available offline, so it works great when you're on a plane, too.
Or perhaps add in a Rust backend for Idris? I mean it has a C, JS and PHP backend, and the last time I looked at the backends, it seemed quite small and self contained. I tried to write a Rust backend but my Haskell/Idris wasn't good enough.
I really enjoyed that book. For me it's chapter on ownership is what made it click. I still usually recommend the official rust book for beginners and that one as a follow-up. There is nothing wrong with starting there though
The example Hyper instance they deploy in the blog post is publicly available at [https://rust-http-microservice-v2.now.sh/](https://rust-http-microservice-v2.now.sh/). 
You need to put a placeholder in your format string: `println!("The result is {}", result_temp)`
Thank you for noting that, but i also got this error. ------------------------------------------------------------- Compiling FtoC v0.1.0 (file:///C:/Users/X/Desktop/FtoC2) error[E0432]: unresolved import `text_io` --&gt; src\main.rs:1:5 | 1 | use text_io; | ^^^^^^^ no `text_io` in the root error: cannot find macro `read!` in this scope --&gt; src\main.rs:8:21 | 8 | let mut menu_num = read!(); | ^^^^ error: aborting due to 2 previous errors For more information about this error, try `rustc --explain E0432`. error: Could not compile `FtoC`. To learn more, run the command again with --verbose.
For more information about this and what else is possible with formatted strings read the [std::fmt documentation](https://doc.rust-lang.org/std/fmt/index.html).
Yes, of course I support it. Set as follows. socket_address = ["/tmp/foo.sock"]
Arguments to formatting macros like `println!` interpolate the arguments you give into the string - and it expects you to show it where to do that by putting "placeholders" in the string. So in your example: ``` println!("The result is {}", result_temp); ``` See more info in the `std::fmt` docs: https://doc.rust-lang.org/std/fmt/.
`println!` does not concatenate its arguments; you need to include a placeholder (e.g. `{}`) in the first argument for each successive argument.
Got it, but sorry whats the difference between extern crate text_io; and #[macro_use] extern crate text_io;, and should i only put it on the main code or also in toml file? ------------------------------------------------------------- I Used extern crate text_io; and i got Compiling FtoC v0.1.0 (file:///C:/Users/X/Desktop/FtoC2) error[E0463]: can't find crate for `text_io` --&gt; src\main.rs:1:1 | 1 | extern crate text_io; | ^^^^^^^^^^^^^^^^^^^^^ can't find crate error: aborting due to previous error For more information about this error, try `rustc --explain E0463`. error: Could not compile `FtoC`. To learn more, run the command again with --verbose. 
Cool! Similarly is it just a small performance hit to respawn handles? use futures::spawn_with_handle; // Lets day the first spawn is hidden deep in the call stack let join_handle = spawn_with_handle (future).unwrap(); let join_handle = spawn_with_handle(join_handle).unwrap(); let output = await!(future); An unintuitive spawn could have happened within some deep call stacks in heavy client libs like the AWS SDK or similar. Because it’s entirely unclear, we respawn the future. What happens? Is it worth giving handles their own type signature so these issues all become moot?
I did that and it download it the packages but, when i run the program i got this. Compiling FtoC v0.1.0 (file:///C:/Users/X'/Desktop/FtoC2) error[E0277]: the trait bound `(): std::cmp::PartialEq&lt;i32&gt;` is not satisfied --&gt; src\main.rs:19:15 | 19 | if menu_num == 1 { result_temp = _temp -32.0*0.5556 | ^^ can't compare `()` with `i32` | = help: the trait `std::cmp::PartialEq&lt;i32&gt;` is not implemented for `()` error[E0277]: `()` doesn't implement `std::fmt::Display` --&gt; src\main.rs:8:21 | 8 | let mut menu_num = read!(); | ^^^^^^^ `()` cannot be formatted with the default formatter | = help: the trait `std::fmt::Display` is not implemented for `()` = note: in format strings you may be able to use `{:?}` (or {:#?} for pretty-print) instead = note: required by `std::fmt::Display::fmt` = note: this error originates in a macro outside of the current crate (in Nightly builds, run with -Z external-macro-backtrace for more info) error[E0277]: `()` doesn't implement `std::fmt::Display` --&gt; src\main.rs:8:21 | 8 | let mut menu_num = read!(); | ^^^^^^^ `()` cannot be formatted with the default formatter | = help: the trait `std::fmt::Display` is not implemented for `()` = note: in format strings you may be able to use `{:?}` (or {:#?} for pretty-print) instead = note: required by `std::fmt::Display::fmt` = note: this error originates in a macro outside of the current crate (in Nightly builds, run with -Z external-macro-backtrace for more info) error[E0277]: the trait bound `(): std::str::FromStr` is not satisfied --&gt; src\main.rs:8:21 | 8 | let mut menu_num = read!(); | ^^^^^^^ the trait `std::str::FromStr` is not implemented for `()` | = note: required by `text_io::parse_capture` = note: this error originates in a macro outside of the current crate (in Nightly builds, run with -Z external-macro-backtrace for more info) error: aborting due to 4 previous errors For more information about this error, try `rustc --explain E0277`. error: Could not compile `FtoC`. and the code is //extern crate text_io; #[macro_use] extern crate text_io; fn main() { let mut _temp:f32 = 0.0; let mut result_temp:f32 = 0.0; let mut menu_num = read!(); println!("Welcome to the world greatest temperature converter."); println!("Please choose one of the two formulas to convert/"); println!("1/From Fahrenheit to Celsius."); println!("2/From Celsius to Fahrenheit."); if menu_num == 1 { result_temp = _temp -32.0*0.5556 } else {result_temp = _temp +32.0*1.8 } println!("The result is/{}",result_temp); } and thank you for engaging and helping me.
First of all, thank you for developing tantivy. It's a great project that captures a lot of expertise and experience. I've been watching tantivy for a while and have been considered using it for several websites I maintain that have a lot of content. One thing that became an issue for me recently, however, is concern about the future of tantivy. I know that you've put a lot of effort into the project thus far. Will the project be actively maintained, and developed further, in the future? Or has it been a pet project of yours that soon may be put aside given your current interests and the original rationale for the project? Personally, I was excited to see a project like tantivy that is leaner, potentially more performant, alternative to lucene. I agree with the other comments that have highlighted the issue of clustering. Is this something you could address in the near future? Otherwise, I would recommend thinking of 3-5 key tasks that people might want to accomplish with tantivy as it currently stands, and then make sure you have tutorials which explain as clearly as possible how to carry out those tasks with tantivy.
Funny you should suggest this, b/c my next post is going to be exactly this pattern. I needed this a while ago (maybe even a year ago), and someone on IRC did a great job helping me with it, i'm guessing that someone was probably you :). 
One segment is mostly for curiosity, its not practical in a normal production Lucene system. But I think 4 is a good number, more realistic. While its a relatively low level detail, for the people that know, it would be useful to keep the segment counts published somewhere along side the benches. Thanks
You need to specify the type of the value you're reading, it seems, for the macro to work correctly. Try `let menu_num: u8 = read!();` (also, you don't need the `mut` here).
My teams make heavy use of ElasticSearch, but I will not be able to advise that we switch to Tantivy even in a year or two. At this point it is not technology holding back Tantivy, it is risk takers putting it into production at small scales. My advise do whatever you can to get small users in production. If you also want to target big players quicker, it would be useful to have a Lucene compatibility layer (using the JNI or something), then fork ElasticSearch or Solr to use Tantivy instead. Ideally get the Apache or Elastic orgs to merge your fork as an alternative search kernel that end users can choose between.
Why not use mut of i think it well change in the future? also it did work, although it did show me a warning. ------------------------------------------- --&gt; src\main.rs:8:6 | 8 | let mut menu_num:u8 = read!(); | ----^^^^^^^^ | | | help: remove this `mut` | = note: #[warn(unused_mut)] on by default --------------------------------------------- thank you very much i have been struggling with this a long time Thank you much god bless you. 
Handles do have their own type, [`futures::task::JoinHandle`](https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.3/futures/task/struct.JoinHandle.html). Personally I would say that’s a documentation issue for the client library. Libraries in general should be very explicit if they spawn off new tasks, that’s something that should happen relatively rarely and instead they should mostly be using internal concurrency via combinators like [`join!`](https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.3/futures/macro.join.html). 
Yeah, macros are going to be a big focus of work for 2.0. It's a really big area :-(
I don't have a concrete plan yet, other than 'more frequent than Rust'. I'm, guessing at maybe 6 months, but it will depend a lot on how much changes and on how much appetite there is to update.
They basically don't. There are a few macros we special case (println, etc) and that includes 3rd party macros which follow similar patterns. Macro uses which look like some other Rust expressions get formatted as if they were. Macro declarations are not formatted (or perhaps some very simple cases are formatted, but most are not).
Even ignoring classloaders, raw types, etc. Java generics are unsound thanks to the way null is treated.
I hope this is used in editors and things because Nix could use better tooling.
In general futures-rs examples are written as if they’re in an `async fn`, so there is the implicit `task::Context` available with an instance of `Spawn` to find the executor to send the task to.
I strongly suggest using something like [Swifth libsytax](https://github.com/apple/swift/tree/bc3189a2d265bf7728ea0cfeb55f032bfe5beaf1/lib/Syntax) for representing syntax trees if the end goal here is to create better tooling. I have [an implementation in Rust](https://github.com/matklad/libsyntax2/tree/9f6cf42c5fd0bd98dd3445239f2c6414e8fd9324/crates/libsyntax2/src/yellow), which probably could be extracted into a crate if there's interest. 
I recently explored switching from elasticsearch to tantivy for a relatively small project (full text search on a few fields and faceting). First, tantivy is a fantastic project and fulmicoton is an incredibly responsive and helpful OS project creator/leader. I ended up not switching to tantivy for the following reasons (which I think would be good next steps for tantivy): * The query parser needs a lenient mode (as noted by the author in Issue #5). It is almost okay to use in user-facing contexts, but not quite. It would also be nice if the full Lucene query parsing syntax could be supported in the QueryParser. * Despite the network overhead, Elasticsearch was significantly faster for my use case. My tantivy set up was very similar to the project examples, but consistently added 30-80 ms to my page load times (this was inside an Actix web app). * I ran into a bug where I made a change to a document in the index, and then it seemed to not be updated in other threads (this is inside a multi-threaded Actix web app). I was running into a deadline, so I didn't investigate this further and didn't report it (sorry!). I will definitely follow tantivy's progress and will revisit tantivy for other projects in the future.
Thank you very much,surely the rust community is really grateful for having someone like you in it's sub-Reddit.
Haven't read "Programming Rust" in its entirety, but I found its explanation of ownership and borrowing to be more thorough than the official Rust book. YMMV.
Nix could use better everything. Better documentation, better tests, \*many\* more maintainers (80% of nixpkgs is basically abandoned), and \*so\* many things just go without ever being done because it's a lot of work and we don't have enough people. Source: I do nix consulting and manage operations for a Haskell/Nix shop.
Thanks for this. I've been working on my own parser, but it's a lot easier for me to use a completed one.
id like to encourage you to apply some more self-confidence to your gitlab profile
As others said, C# compiles to a IR and not to native code, i.e. it needs to be run on a virtual machine. Idea: make a C# application and call some sort of Rust entry function from C#, pass a delegate to Rust (not sure if it is possible tho), and then use this delegate to call C# code. By doing this you will be running Rust side by side with a .NET environment. PS: I'm not sure if all these steps are possible.
There's usually a "call for participation" in various projects (including the Rust compiler itself) in [This Week in Rust](https://this-week-in-rust.org/blog/2018/08/14/this-week-in-rust-247/). 
Other people already suggested putting `Fn` into a `Box`, but beware: I expect, that you will want to change the state of the game in that closure. You will be unable to put the state into all closures, as that would require `Arc` or `Rc`, which in turn prevent you from modifying the state. You would have to either use `RefCell` or `Mutex` with obvious performance penalty. There are two ways to resolve this: throw away the idea of putting closures into map and simply match them. This might be problem, if your actions are more dynamic (e.g. if you enable changing controls). The other way is to add a parameter - mutable reference to the game state to the closure.
I'm quite new to rust so I have a genuine question: why are `spawn` and `await` macros and not functions?
You can replace `first.stdout.unwrap()` with `first.stdout.take().unwrap()`, to leave the rest of `first` unmoved. (Then remove all those unwraps later please!) 
Yes, this is possible and reasonably easy to do. Rust supports multiple different platforms and you can install their toolchains with rustup and cross-compile to that target with cargo. You might find these resources useful: - https://blog.rust-lang.org/2016/05/13/rustup.html - https://wiki.archlinux.org/index.php/Rust#Cross_compiling
Of course! If I'm not mistaken, even cross-compilation works. Further, on Windows, both msvc and mingw targets are supported. If I'm not mistaken, RedoxOS is supported too and who know what other architectures...
Wonderful! I like nix and rust, is that a part of something bigger?
If you expect multiple threads to be concurrently accessing and/or mutating the value in the `Cell`, you might want to synchronize those accesses with a `Mutex` or a `RwLock`, so the field would be state: RwLock&lt;Cell&lt;ThrottleState&gt;&gt; Of course, you will have to call `self.state.read()` or `self.state.write()` before accessing/writing to the value in the `Cell`, but it will be threadsafe 
Oh, that makes quite a lot of sense! Thanks! Idiomatically, is it more rustic to try and hide that detail and maybe deadlock someone in a surprising way, or make the whole struct `mut` so that `borrowck` enforces safety in the face of the consumer? I’m curious about the most proper idiom because this struct is basically a paperweight if that field is unchangeable.
Is there any articles that summarize all the changes which will occur in the 2018 edition?
collect is a default function for Iterator. As far as I know, it's a part of the stdlib, so you can expect it to work with any instances.
Programming Rust is excellent, however please note that it does deep dive on quite a few topics (which is a good thing). Therefore it is a good companion for the official Rust book.
Ohh thats nice. Thank you
Oh, a user? Exciting! Make sure to let me know if I break stuff too frequently, yeah? Don't answer this if you don't want to, but I'm curious: What are you planning on using it for?
For whatever reason a lot of Rustaceans use Nix, and it made to me to switch as well. I'm very pleased. The abandonment is not that severe because how easy it is tu just submit a PR to update software one cares about. Do you happen to have donations/patreon for individuals that would like to support the proejct? 
Looking to make a tool that makes it easier to configure a group of systems. I'd like to get it working with NixOps and NixOS configs.
I'd love an example of how to set up a Tonio executor so it's used by `spawn!`.
Googling for Idris was already hard enough, ha!
The Edition Guide has this [https://rust-lang-nursery.github.io/edition-guide/unstable-feature-status.html](Unstable Feature Status) page where you can see a list of all the features that are expected to ship in the 2018 edition. However, note that most of these are not specific to Rust 2018; once they are stabilized they will be usable from both Rust 2015 and Rust 2018 code. The changes that are unique to Rust 2018 are: * [Raw identifiers and new keywords "async"/"await"/"try"](https://rust-lang-nursery.github.io/edition-guide/rust-2018/raw-identifiers.html) * [Importing macros with "use"](https://rust-lang-nursery.github.io/edition-guide/rust-2018/macro-changes.html) * [Changes to module/crate paths](https://rust-lang-nursery.github.io/edition-guide/rust-2018/path-clarity.html)
Looks very promising!
Nice job. I like the simplicity
What does "serverless docker deployment" mean in this blog post?
Lots of work... but I will give it a try, thanks
That's awesome! I'll push a release of rnix to Crates.io including documentation and some bugfixes :)
Interesting, what are the advantages over the current span system? Currently rnix keeps track of the start/end offset of each token, and saves it in the AST. [nix-explorer](https://gitlab.com/jD91mZM2/nix-explorer) demonstrates this, it highlights the original code based on whatever AST node you highlight.
Looks like it already solved the issue I filed.
Are you saying to make the value of the Hash map an enum type and then just store the commands in each possible value of the enum? Then just like match on it? 
&gt; why does the generated Rust documentation not produce links to all the available methods / traits that apply to a type in a given crate? So, in addition to what /u/ehuss said, unless you completely generate your own local documentation for every project individually, this kind of list can never be accurate. This is because you can extend other crates' types with your own traits.
&gt; TRPL is currently at Rust 1.21 IIRC, The second edition is frozen at 1.21, but the new 2018 edition has updates between then and now.
If you want that to be a thing, please comment here: https://github.com/rust-lang/rust/issues/45599 
If you only care about linux, I'm going to promote my own crate: [procfs](https://crates.io/crates/procfs). It's a library that you can use to interact with the [proc](http://man7.org/linux/man-pages/man5/proc.5.html) filesystem on linux. There is a [netstat](https://github.com/eminence/procfs/blob/master/examples/netstat.rs) example that can get you started. I think something like this might do the trick: // get all processes let all_procs = procfs::all_processes(); // build up a map between socket inodes and processes: let mut map: HashMap&lt;u32, &amp;Process&gt; = HashMap::new(); for process in &amp;all_procs { if let ProcResult::Ok(fds) = process.fd() { for fd in fds { if let FDTarget::Socket(inode) = fd.target { map.insert(inode, process); } } } } // get the tcp table let tcp = procfs::tcp().unwrap(); let tcp6 = procfs::tcp6().unwrap(); for entry in tcp.into_iter().chain(tcp6) { if entry.local_address.port() == port_to_find &amp;&amp; entry.state == TcpState::Listen { if let Some(process) = map.get(&amp;entry.inode) { kill(process.pid()); } } } You might not be able to find 1 single library that supports all platforms, so you might have to use several to cover all the platforms you care about. (for example, using procfs for linux, maybe there's something in winapi for Windows) 
So the way I would solve this would be: 1. Define an enum containing all keycodes 2. Use crate like this: [https://docs.rs/enum\_derive/0.1.7/enum\_derive/](https://docs.rs/enum_derive/0.1.7/enum_derive/) to auto generate conversions to/from strings 3. In an impl block (or define a whole Trait if you anticipate multiple implementations) add something like an `.exec` method so you can call it like `&lt;enum_value&gt;.&lt;exec&gt;()` 4. When you get the string for the key code, you can convert it from the string then call exec 
[Disclosure: I work at Amazon.] Typically, “serverless” is marketing speak for “you don't need to manage the underlying virtual machines”. For compute, this often means “pick `n` virtualized CPU cores and `m` megabytes of memory” and the cloud provider will schedule your workload accordingly on a warm pool of virtual machines that _they_ manage.
The `procfs` crate has functions for the [`tcp`](https://docs.rs/procfs/0.3.0/procfs/fn.tcp.html) and [`tcp6`](https://docs.rs/procfs/0.3.0/procfs/fn.tcp6.html) tables. I think you have to use [`all_processes()`](https://docs.rs/procfs/0.3.0/procfs/fn.all_processes.html) to scan for the corresponding PID though. This is definitely specific to Linux though. I don't know if anyone has tried to make a portable wrapper for this kind of functionality. You could just write your own little tool that spawns different `Command`s for each OS though.
1. Keycode is already an enum type. 2. You can impl an enum? (Sorry if this question is dumb, I'm still very new to the language) 
&gt; Is "serverless" a new marketing term for "cloud computing" or is there something more? "cloud computing" is a general term for a lot of things. There's a lot of divisions: * VPSes * Infrastructure as a Service * Platform as a Service * Serverless and probably many more. As you go down this list, you give up control, but get benefits. A *VPS* is a "virtual private server", so you sign up, and you get a thing you can ssh into. It's a virtualized computer, running on the same hardware as a bunch of other computers, but it feels like a full computer. You have full control and can do whatever you want. *Infrastructure as a service* is sort of a broader but similar term. Some people say that VPS has to be real hardware, and Iaas has to be virtualized. IAAS is just a newer term for the same basic idea, as far as I'm concerned. A *Platform as a service* gives you a way to deploy a specific platform. Think Heroku back in the day; they support Ruby on Rails, and you deploy your Ruby on Rails app, and you're done. You give up managing individual servers, but you also don't have to do the work of managing individual servers. Just slide the slider and boom, you've scaled. That slider still controls a number of servers, but maybe they're split between "web server" and "database servers" or whatever. *Serverless computing* changes the abstraction again; instead of thinking about servers at all, you think about "functions". Maybe this is a better way to put it: With PaaS, you're scaling *your whole app*. With Serverless, you can scale *each individual route*. The notion of your app being on a specific server is totally gone; you instead scale specific inputs and outputs. I hope that helps. This space can be confusing.
I really appreciate that cross-language LTO is seen as a first-class usecase in Rust ([PR #53031](https://github.com/rust-lang/rust/pull/53031)). Much like cross-compilation, it's often possible with other native languages, but the hassle is on you, instead of the toolchain just handling it automatically. And Centril just demonstrates how incredible he is at driving consensus in [RFC #2497](https://github.com/rust-lang/rfcs/pull/2497).
Note that the RFC is just about reserving the *possibility* of this syntax so as to be able to experiment; not reserving this particular syntax. I would personally favor `if a is Some(x) &amp;&amp; b is Some(y)` where `EXPR is PAT: bool` because an `is` expression can then be used in many different situations, not just in `if`, such as: fun(a is Some(x) &amp;&amp; x.is_okay()); which is better than: fun(if let Some(x) = a &amp;&amp; x.is_okay() { true } else { false }); 
I'm not positive this solves your issue because I haven't tried it in a DLL, but rust-openssl can link statically to openssl from a [vcpkg](https://github.com/Microsoft/vcpkg) installation. I checked and vcpkg can build a static grpc that depends on openssl. You would need to add support for finding grpc in vcpkg to grps-sys, which is pretty straightforward. You would need to compile the DLL using RUSTFLAGS="-C target-feature=+crt-static" which could be a dealbreaker or exactly what you want. I am the author of the vcpkg-rs crate which helps with emitting the right cargo metadata for pulling in dependencies from a vcpkg installation. If you decide to try this route I'd be happy to talk you through it.
This is great. I also have a small suggestion: In many macros, multiple branches share a lot of similar sub-patterns. It would be easier to visually read the diagrams if the similar sub-patterns were aligned. E.g. aligning all the `macro_name` instances into one column in [this example](https://raw.githubusercontent.com/lukaslueg/macro_railroad/master/examples/convert_args.jpeg). And for macros where a sub-patterns appears verbatim in multiple branches of the macro (or multiple times in one branch), it would make sense to highlight that they are not just similar but the same, by factoring out that sub-pattern into its own sub-rule diagram.
rnix has only one dependency. It's surprising to see a parser that does not use a tool like nom. 
vcpkg is pretty compelling here. I'll give it a shot. thanks!
I think string is simply collection of `char`s so I would expect `Iterator&lt;Item = char&gt;` collect to String but not an iterator of `&amp;str`.
What is the story with vertical alignment for struct fields, enum variants, match arms etc? Last time I checked rustfmt was not supporting any of these and, more than that, it was reverting any manual alignment. This is a coding style requirement at the company where I work now, at least for C and C++ code.
It seems (to me) like the right combination of lifetimes and join handles (the join handle must drop before any borrows/moves/etc that conflict with a borrow made by the Future) should handle the concerns addressed in the article perfectly. Where the article proposes "pass in a nursery", rust has "return a join handle".
Welp, looks like that's definitely what I should use! It's a little unfortunate that I won't have as few dependencies anymore, but it does seem like it's the right thing to do. Just double checking: I don't need to rewrite too much of the logic, right? "Just" all the enum-ing? I'd say separate the crate (and preferably write docs if possible), and I'll look into using it :)
One big missing feature is optimising imports. It is not able to remove unused imports also not able to sort imports correctly.
Check out the example in https://github.com/rust-lang-nursery/futures-rs/blob/master/futures-util/src/compat/tokio.rs (nicest to view if you build the docs with `--features tokio-compat` locally)
This is effectively a blog post summarising a paper. Paper: https://arxiv.org/pdf/1202.4961.pdf Abstract: &gt; We present fast strongly universal string hashing families: they can process data at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that these families—though they require a large buffer of random numbers—are often faster than popular hash functions with weaker theoretical guarantees. Moreover, conventional wisdom is that hash functions with fewer multiplications are faster. Yet we find that they may fail to be faster due to operation pipelining. We present experimental results on several processors including low-power processors. Our tests include hash functions designed for processors with the Carry-Less Multiplication (CLMUL) instruction set. We also prove, using accessible proofs, the strong universality of our families. From what I can tell, this implies that we can have very fast hash functions for keys of up to a moderate length. For keys of greater length, a fallback may be required. This is likely of immense interest for use in Rust's hash tables.
A very well written article. thank you!
Could you open issues so the thought does not get lost?
The RFC addresses this -- in particular, that it doesn't short-circuit. https://github.com/Centril/rfcs/blob/rfc/let-chains-2/text/0000-if-let-chains.md#if-let-chains &gt; It is important to note that this is not generally equivalent to the following expression: &gt; &gt; if let (A(x), B(y)) = (foo(), bar()) { &gt; computation_with(x, y) &gt; }
Just tried it out. It's awesome!
Ohh, it's not accessing the context directly like the `await!` macro itself, that's clever! How is that not `await!(spawn(actual_fut))` though? I guess the `await!` might be confusing?
cool let me try that...
would it work on Mac?
Really? A lib with 20 lines that doesn't do anything you can't do with the stdlib?
No. The proc filesystem is very specific to linux, and so the `procfs` crate will only work on linux.
LLD is the "linker" which comes with LLVM. Every toolchain* which compiles down to native code needs a linking stage where different parts of the output of the compiler are combined together into the final executable (e.g. in Rust this is where pre-compiled crates are combined with your newly compiled code (I believe your code is also separated into many parts in some cases to allow parallel compilation)). As the executable formats on different operating systems are different (for example linux uses the ELF format, and macOS uses the Mach-O format), there is a platform-specific aspect to this. And operating systems typically ship a linker which will works, but only for their platform. The LLD linker is different in that it works across platforms, however it's pretty new and still a little experimental at the moment. You can see the Rust issue for this here: https://github.com/rust-lang/rust/issues/39915 * Fun fact, Go has it's own linker which is why it's cross-compilation story is so good.
And if the field was named properly (e.g., something_version), wouldn’t it be obvious that you should have used a string?
The simplest work-around is a [RefCell](https://doc.rust-lang.org/std/cell/struct.RefCell.html). This does the standard borrow-checker rules at runtime rather than compile time. There are times where you can't prove using rust that your use of multiple mutable borrows is correct, but if you're not sure then this opens you up to crashes if your program attempts to hold two mutable references at the same time.
Having one thread write to player state, and the main loop also update player state is indeed a data race, so you need to overcome that. Your best bet (what I understand most games do) is to synchronize your check for keyboard events with the game mainloop. That is, part of your mainloop is checking for user input. The mainloop would own the GameState, and you would pass a &amp;mut GameState into the function you call to check for input, and also into whatever other functions are done in your mainloop. You can abstract out that strategy hardcore, as well as split the GameState into smaller chunks and enable parallelizing updates to disjoint parts of the state. The result is the Entity-Component-System (ECS) methodology that I hear is all the rage in game development these days. Rust has some crates along these lines (I know of specs, but I think there are others). If you don't like those options, you could always try to go the route of designing a lock-free structure for your game state! I say that so excitedly because it sounds tremendously difficult to get right. I'm not sure if the overhead of using atomics would beat out any gains you'd get from finer-grained parallelization though.
I would instead use MPSC channel and send information from keyboard to the main event loop that would react accordingly to the actions that were performed. This would give you only one place where state can be mutated. You could even have separate thread for state updates and separate for displaying it.
success! I did find a couple of weird things though: * vcpkg advertises all of the libraries included for grpc even though some of them overlap. For example, grpc and grpc\_unsecure ('unsecure' isn't linked to openssl). I solved that by not emitting the cargo metadata when finding the package but filtering out the unwanted libs and then printing the filtered metadata myself. * I had to manually print link-lib directives for user32 and gdi32 to resolve some symbols from openssl. I would expect vcpkg to include those for the openssl package. Did I miss something or is this something Windows developers just know? I noticed rust-openssl does something similar.
You're second paragraph describes what I'm already doing. KeyboardManager is a struct with a method handle_input that is called at the beginning of the frame. My game is single-threaded
Instead of move_right_command::new(&amp;mut player) try storing your players in a Vector (or HashMap), and using move_right_command::new(player_id) where player_id is the index or key. Your move right command will then need push an event to a queue to later be processed by your main loop rather than directly mutating the player object.
Sorry, I didn't read your post sufficiently carefully. Since you're single-threaded, you should be able to structure your code without any fancy synchronization structures. You need to find a way to ensure the &amp;mut you pass to your KeyboardManager expires before you add its twin to your vector of entities. This could be as simple as wrapping the creation of the &amp;mut and the call to KeyboardManager in some more {}'s, but I don't know precisely enough what you're doing to say for sure.
Hey I can help. I'll pm
Curious to hear why you find "thread safe hashmap" to be misleading? I figured it'd be the most accurate description of the properties Memdb has. Or are you objecting because the implementation is too simple, and you were expecting something more complex? Also we emphasize in the README that Memdb is probably most useful during prototyping/experimentation. It's in no means intended to replace a _persisted_ data store like Redis or Level.
(Where both Idris and Whitespace are the brainchildren of Edwin Brady. Of course someone would write something that overlaps...!)
One thing that's weird to me about the new module system is that the module's index does not reside inside the module. i.e. I'd expect foo/a.rs foo/foo.rs instead of foo/a.rs foo.rs But it might make more sense once I use it.
IIUC you still can use the old variant: foo/a.rs foo/mod.rs
Sounds pretty unsafe?
&gt; Curious to hear why you find "thread safe hashmap" to be misleading? I think the expressed opinion was that "thread-safe in-memory key-value store" was misleading and that "thread-safe hashmap" would be better.
&gt; Is the mod keyword still required in foo.rs to declare submodules? Yes.
IIRC, yes, you still need `mod` to "mount" submodules. Initially it was planned to make it optional, but many have argued (and I agree with them), that it's useful to keep it as is, e.g. for "dismounting" whole submodules without deleting or commenting-out whole files or sub-directories.
Yeah that's a good example. Thanks.
Thanks Steve.
[https://zeit.co/zeit/rust-http-microservice/myjlmetcfn/source?f=Dockerfile](https://zeit.co/zeit/rust-http-microservice/myjlmetcfn/source?f=Dockerfile) is rather interesting as well. It looks like it's just a Dockerfile and a now.json file to make it public, then they were off to the races. It's all pretty cool!
See this: https://rust-lang-nursery.github.io/edition-guide/rust-2018/ownership-and-lifetimes/the-anonymous-lifetime.html#more-details
Yes, that's expected. This is https://rust-lang-nursery.github.io/edition-guide/rust-2018/ownership-and-lifetimes/the-anonymous-lifetime.html
Thanks. So is it a matter of taste if I should use it with (simple) formatters? For what it's worth the example on that page uses it in one place, but not with fmt::Formatter.
Could this not be done with 'use' ? Only compile modules if they're used by some other module that is eventually used by main/lib.rs?
&gt; So is it a matter of taste if I should use it with (simple) formatters? The intention is that it's idiomatic to always use it.
Ok, cool. That clears it up for me. I tried to update the docs here: https://github.com/rust-lang-nursery/edition-guide/pull/92
I think it would've been quite confusing. With `mod`/`use` we have a nice separation between "mounting" module and bringing item to the current namespace.
Thanks for the fairly thorough explanation — being comfortable asking questions as a newbie is one of the best parts about Rust!
&gt; Supporting crate as a visibility modifier (tracked here 14. *Given feedback so far, this feature is unlikely to be stabilized for Rust 2018*. (emphasis mine) Glad to see this. My thoughts were expressed in - https://github.com/rust-lang/rust/issues/44660#issuecomment-406013863 - https://github.com/rust-lang/rust/issues/44660#issuecomment-406355868 - https://github.com/rust-lang/rust/issues/44660#issuecomment-408676804 It is good that we keep in mind the problem it is trying to solve: [it isn't clear what parts of a crate are part of the public API](https://github.com/rust-lang/rust/issues/44660#issuecomment-408769188). If something is marked `pub`, you have to trace how it percolates up to see if it actually makes it to the user of your crate. They want to solve that by making `pub` exclusively for your public API. This will make us rely on `pub(crate)` more for cross-module-but-not-public APIs which might get noisy and a shorthand would be nice.
Many Linux distributions' packages have stripped binaries and separate debug symbol files - see [Debian](https://wiki.debian.org/HowToGetABacktrace#Installing_the_debugging_symbols) for example. Ideally the backtrace stuff would support this automatically.
But wouldn’t using u64 mean that this wouldn’t work on a 32 bit machine (and hence be *less* portable, in contrast to what u/po8 suggests?)
Much obliged! :heart: :heart: :heart:
The other main pattern is just to use Vec and index into instead of references. (petgraph)[https://github.com/bluss/petgraph] being the typical library for helping with that. A more complicated solution is to use a memory arena (this blog post)[https://exyr.org/2018/rust-arenas-vs-dropck/] goes into more detail. [too many linked lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/) is also a good resource. It's techniques are applicable to graphs as well as linked lists.
This is great, thanks for sharing! The use of multisets is clever and I love the do-undo recursive approach as well. The 30 minutes I just spent groking your code were definitely educational.
Also check out [cargo-update](https://crates.io/crates/cargo-update). 
&gt; Supporting crate as a visibility modifier (tracked here 17. Given feedback so far, this feature is unlikely to be stabilized for Rust 2018. IIRC, crate as a visibility modifier was added in order to lint for pub declarations that don't end up being pub. Will this unreachable pub lint be introduced with the new edition right away, or will it be stabilized afterwards as well?
Ah rubbish... I feared this would be wasted effort. It's certainly an unfortunately named crate, given its name clashes with the built-in \`cargo update\` subcommand. Thanks for pointing it out anyway.
Serverless to me means that there's some sort of "gateway" service between the internet and the cloud platform. When a request to your service comes in, there isn't a particular server waiting to handle that request but rather the gateway service spawns a brand new "virtual machine" (a docker image in this case) to handle that request, and then kills it when it's done. Another way to think about it is a decentralized /cgi-bin/ but using a docker image instead of an executable process that's run on the same computer.
Yeah, I agree. Its command is named `install-update` which is super clunky. 
Thanks for the feedback! Also you also encountered a bunch of bug with the `FacetCollector` :) . Thanks again for reporting them! Some parts of tantivy are clearly not tested sufficiently. The performance issue vs Elasticsearch could have different causes : - Tantivy does not ship with any cache (as in high level cache). - Tantivy searcher uses only one core for the moment. (that should change rapidly) - If this was with the facet collector: - The facet collector has never been optimized, nor benched - The facet collector is designed for small facet cardinality - The facet collector cannot work over a taxonomy (that one only makes sense for people familiar to Lucene.) &gt; I ran into a bug where I made a change to a document in the index, and then it seemed to not be updated in other threads (this is inside a multi-threaded Actix web app). I was running into a deadline, so I didn't investigate this further and didn't report it (sorry!). The searcher needs to be acquired for every single request (`index.search()`) and release it after the request. Can you confirm that you did that?
&gt; What is the story with vertical alignment for struct fields, enum variants, match arms etc? What specifically do you mean here by "vertical alignment"? Rustfmt uses "block alignment", as contrasted with "visual alignment", is that the same as "vertical"?
I would suggest taking a look at [slotmap](https://docs.rs/slotmap/0.2.0/slotmap/) or [froggy](https://docs.rs/froggy/0.4.4/froggy/).
This looks amazing!
That's a good thought, but the previous one was definitely already via rustfmt-preview as well (I remember struggling a bit to get it working last time for exactly the reason you're suggesting).
I had already done `rustup update` first, as well as `rustup self update`. But thanks for the suggestion!
To answer the first question. ElasticSearch, Solr are dominating. A few sphinx (C++) deployment too. Cloud solutions are on the raise, with Algolia leading the march. Then comes a lot of propriety vendor solutions you have never heard about. Fast, Exalead, Autonomy, Sinequa, etc. Larger companies tend to work directly off a strongly customized Lucene (e.g: Indeed.com, Twitter), or implement their own search engine, most of the time in C++ (all web search engines, Amazon, DropBox, Ahrefs, Ebay). For such a company, any minor annoyance is magnified by their scale, so that always end up customizing their solution easily. For instance, let's say that a tokenizer that is not perfect for your use case is costing a small e-commerce business $10 a month. It's not worth fixing it. At the scale of amazon, that's worth having a software engineer working on it for one year. Tantivy might be interesting for the latter use case, because : - These company end up not using all of the off-the-shelf solution lucene has to offer anyway - Tantivy is not written in Java, have a slow startup time and a low memory footprint.
The cool things with the log search use case is that it is usually about pure muscle, and less about language processing and information subtleties. I look forward to see your effort.
Alternatively you could use ```a.is_some() &amp;&amp; a.unwrap() == x &amp;&amp; x.is_okay();``` Not that pretty but it works.
Thanks. Faster/Lighter than lucene is feasible. Overall better than Lucene is not possible. I could focus on a super specific use case but I don't know which one.
Could cargo fix tell me how to Port my growing number of futures to async/await syntax? It has taken days to turn synchronous code into asynchronous. I shudder at the thought of refactoring again to make it async/await. 
To provide some counter reasoning, I've also implemented many lightweight "executors" that are just delegating to the real one, and it never seemed weird calling those `Executor`s. However, `Spawn` somehow feels less clear to me. ----- On a meta note, I think this case is a good example of the concerns I mentioned weeks ago about discussion directly in Discord (really any chat). I can't keep up with all the messages in all the chat rooms, and trying to scan for the important bits means you're bound to miss something. In this instance, I missed the discussion here. Additionally, since it was a change directly in libstd, it's even harder to watch for. Subscribing to the full rust repo is untenable. Today, I went and found the PR that made this change, and found that it was missing any relevant discussion from Discord. Preferably, the PR description would include more information not only on *what* is changing, but *why*. It's an easy thing to forget, I get that. **Ideally, this sort of thing could have been filed as an issue on the futures repo.** People could have discussed the reasoning, and then there would be record to point at if the change is decided upon.
So when they say “not stabilized for Rust 2018”, does that mean it won’t drop with the first release of Rust 2018, or that the feature is being shelved until the next edition?
I would use it if I knew how to. I learn best from examples.
32-bit machine means that its memory address is 32 bits long, not that it doesn’t support 64-bit integers. 
These links all just broke, sorry :(
Today's rust futures made me cry. Doo Doo Doo Doo Doo. Doo Doo Doo Doo.
What is the Web book about? Every actively maintained Rust web framework has an examples workspace. Some of these are regularly updated. What is the difference between these examples and what the wg aims to achieve? The current setup keeps examples close to the topic of interest. 
[removed]
Actix is also an option for the same pattern.
What's wrong with ``cargo update``?
Thanks! The arena solution along the right lines but I deallocates all storage in one fell swoop whereas I need to be able to drop individual nodes on the fly. It seems like slotmap might be what I'm after.
Slotmap looks like it could work for my use case, thanks! I can try wrapping the Key in a custom Handle type (to be able to implement Drop for it) wrapped with an Rc
You should add redirects. Fixed: - [New keywords](https://rust-lang-nursery.github.io/edition-guide/rust-2018/module-system/raw-identifiers.html#new-keywords) ^(why the heck are keywords nested under "module system"?) - [Macro imports](https://rust-lang-nursery.github.io/edition-guide/rust-2018/macros/macro-changes.html) ^(now, arguably, this one _should_ be under "module system") - [Path clarity](https://rust-lang-nursery.github.io/edition-guide/rust-2018/module-system/path-clarity.html) 
I argued against the change so I’ll let someone else fix it :p
We have to sort that, we’re not sure.
Not really. You might want to consider simply making `Command` a trait with an `execute` method if the main purpose is to execute it.
Are async await supposed to replace futures?
It seems like the underlying issue is that you're having your commands stash `&amp;mut player`. Currently that's conflicting with your entities loop, yes, but won't it also conflict as soon as you have more than one command? Instead, you need to pass down `&amp;mut player` and whatever else in the game world is needed, as arguments to some kind of `Command::invoke(...)` method later. That way the command can do whatever it needs to do to the player, without _storing_ the `&amp;mut`.
If you can pass your `T: Command` around via type parameters or `impl Trait` then there's no dynamic dispatch.
No. You could use a macro_rules or proc_macro (custom derive) to do it for you though through code gen. 
No, just to give you an less callback an more imperative looking way to code against futures.
I’ve made a few attempts to grok this in the last couple months but this is the first time it has clicked (I think). Thanks again for your time and effort.
Something I’m rather lost about is NLL. I’ve heard lots of excitement about NLL, but I’m struggling to find documentation on NLL, and how it helps me compared to before. The documentation seems out of sync with what is happening, as NLL is enabled in the preview in migration mode according to this post, but according to the Edition Guide it is “Implemented but not ready for preview”, and there is no content in the Edition Guide explaining NLL. I’m not exactly sure how to get started with this aspect of the 2018 preview. Does anyone know something I’m missing?
`String` is internally a `Vec&lt;u8&gt;` with a guarantee of UTF-8 encoding, while `char` represents a 21-bit Unicode code-point. It's easy to make a `String` from an iterator of `&amp;str`, but tedious to make one from an iterator of `char` (since you have to UTF-8 encode each `char`).
Since your saying you don't write you don't need mutexes if you construct before any other thread has access. You could also just pass this value once to every thread and prevent concurrent access. This prevents a whole lot of errors and is more explicit? Disclaimer: I am quite new to rust. It's a thought for discussion.
I'm working on an implementation of the [Discworld: Ankh-Morpork - board game](https://boardgamegeek.com/boardgame/91312/discworld-ankh-morpork), however I've still a lot to do. While doing so, I play around and learn how to use Rocket and yew. Not sure if I can release the game to the public once it's finished, die to copyright reasons. In the worst case, I can play the game online with friends at least.
Thanks. I hate the Futures so much. This will truly be welcomed
[removed]
Do you think we should use this data to include a list of resources in the top post? For examples: - Edition guide - TLBoRM - [Syntax index](https://doc.rust-lang.org/book/first-edition/syntax-index.html) - [Iterator cheatsheet](https://danielkeep.github.io/itercheat_baked.html) - 
 struct PlayerId(i32);
Is this already in the `2018-08-15` nightly?
It's "tantivy" :) 
Implementing ToString for your MyType is fine. You are implementing a std trait for a type defined in your own crate. But implementing it for Vec&lt;MyType&gt; is not ok! Here you are breaking the trait coherence rules: the trait [ToString]([https://doc.rust-lang.org/std/string/trait.ToString.html](https://doc.rust-lang.org/std/string/trait.ToString.html) is already defined, and you are trying to implement it for Vec, which is also not defined by you. The coherence rules ensure that nobody can extend, for example, the ToString implementation to always append some text to it's result; which could be funny because Rust supports unicode emoticons. The point of your wrapper type is to enable yourself to implement the ToString trait on it, even though it is just a Vec&lt;MyType&gt;. This is ok because nobody else will unintentionally use `MyWrapperType`. [here is a working version](http://play.rust-lang.org/?gist=d3cb8b0e934d787ae069a7eb5e33eabd&amp;version=stable&amp;mode=debug&amp;edition=2015) 
I tell him that all the time, no luck so far :/ 
I agree to a point, yeah. I definitely try my best to include folks in discussion, but different channels of communication are more effective at reaching different people and I don't always manage to include everyone with a valuable opinion. I think ultimately the idea here is that nothing is final, and this is an experiment aimed at getting experience and iterating towards something that is RFC-ready, at which point we can more formally discuss the decisions that were made and get a review on all of the tradeoffs.
Anything that implements actor pattern. This would also improve debugging as you can store stream of the `n` last events in a cyclic buffer that would allow you to check what caused what change, or you could use persistent data structure that would allow you to do “time travel” debugging. Possibilities are endless. 
It is the first form of the [MIR-based borrow check](https://rust-lang-nursery.github.io/rustc-guide/mir/borrowck.html) that will land on stable. The most important aspect is that MIR-based borrow checking helps [fix many bugs in Rust](https://github.com/rust-lang/rust/issues/47366), both where it rejects valid Rust programs and where fix unsound bugs it allows. Niko made an [update about it not too long ago](http://smallcultfollowing.com/babysteps/blog/2016/04/27/non-lexical-lifetimes-introduction/). Its effects might be subtle to many at first. We will still need to respect borrowship rules, and some of the ergonomic ambitions from the original RFC will not immediately land when its on stable. The ergonomics can then be extended further with borrow checks like polonius, which is mentioned in the blog post. Various updates from this week alone: - [An optimization PR that reduced html5ever's min borrow check by 92%](https://github.com/rust-lang/rust/pulls?q=is%3Apr+author%3Anikomatsakis+is%3Aclosed) - [A PR that could reduce tuple-stress's min check by 40%](https://github.com/rust-lang/rust/pull/53383) - [rustc now bootstraps with NLL enabled](https://github.com/rust-lang/rust/issues/53172) - much more Current [NLL performance dashboard](https://perf.rust-lang.org/nll-dashboard.html) (clean-check is using the current borrow checker). I wouldn't be surprised if the MIR-based borrow checker end up outperforming the current on in the end. 
A "32-bit machine" is a machine with 32-bit registers. The register size of a machine sets what arithmetic operations can be done in a single cycle, and also typically limits the address space of the machine. On a 32-bit machine, `rustc` will generate multiple machine instructions for each manipulation of a 64-bit integer, using multiple registers. This will be slower than on a 64-bit machine, but will work fine and be fast enough for most purposes. 64-bit integers will also take up twice as much RAM as 32-bit integers. For storage-intensive situations, this can be a thing. There are very few 32-bit machines left, and most machines have gigabytes of RAM now. My rule is to use 64-bit integers (`u64`, `i64`) unless I have some specific reason to use something smaller.
Related comment. This syntax strikes me as very non-obvious without knowing it ahead of time. Reminds me a bit of APL with its arcane symbols.
Doesn't work with all enums. I really like the \`is\` pattern!
I don't know your exact circumstances, but maybe you could split the enum into a struct and an enum like this: struct TypedCommand { type: CommandType, command: Command, } enum CommandType { Quit, MoveRight, ... }
Sorry for nor being clear. See [here](https://shkspr.mobi/blog/2014/11/why-i-vertically-align-my-code-and-you-should-too/) what I mean by vertical alignment.
They recently [opened an optimization PR](https://github.com/rust-lang/rust/pull/53304) for proc_macros. It reduces the compile time for Serde derives by **60%**.
I'm told `petgraph` is a nice graph library. I haven't tried it.
That iterator cheatsheet is impressive. Interestingly, it highlight something that I think is missing from the auto-generated documentation- categorising methods by what they do rather than just dumping them all in together. I did something similar for Option and Result - grouping methods by getting the value, setting the value, combining options etc.
It's currently just "known", opened https://github.com/rust-lang-nursery/futures-rs/issues/1203 to try and improve the documentation about the documentation.
I wrote a turtle parser with nom and the code was hard to construct initially in nom 2. However, the grammar rules match up nicely to the code now which makes the code nice to maintain and read. On top of that the parser is crazy fast. The same parser hand-written would be very much like spaghetti I fear. So I agree that nom has a learning curve, but the advantages outweigh the initial difficulty for me.
I think frequently asked questions could bubble up to either of these places: * https://www.rust-lang.org/en-US/faq.html * https://doc.rust-lang.org/rust-by-example/
Sorry, not a native speaker.
What is the problem of a cicle reference if you are going to drop the thing properly? Drop is recursive so you should probably implement it yourself, so the Rcs only leak if the graph is not dropped, or not?
If you have the time , i encourage you to checkout the approach used by [specs](https://github.com/slide-rs/specs) ( and [amethyst](https://github.com/amethyst/amethyst/wiki/Entity-Component-System---Specs) )
Like i said, if the grammar is anywhere close to LL(1) (LL(2) or LL(3) for example) you can just hand-write a predictive parser where each function mmatches up to a production in your grammar. Hand-written parsers will also be faster than parser generators 9 times out of 10. Potentially for more difficult language use nom, but by that point i'd be tempted to break out the more traditional parsers since for thos eyou just write the rules in and all the code is generated for you, rather than having a mess of macros with 0 helpful errors if you're trying to maintain it
The problem is dropping only parts of the graph. The Rc count will never reach 0 in a cycle. So drop wont be called. [The book on Rc cycles](https://doc.rust-lang.org/book/second-edition/ch15-06-reference-cycles.html)
On one hand, it's not the default for threads either, but on the other hand, rayon is *much nicer* than doing fork-join array processing yourself. One difference from other languages is *you can't misuse* the limited default: it just doesn't do hierarchical (other than "await the join handle"). Maybe the detached `spawn!` shouldn't be the default, since I don't see it being that useful? If `spawn_with_handle!` was renamed to `spawn!` it would match `thread::spawn` better, and we can use the name `spawn_detached!` for the other (less common, hopefully) case.
I’ve written about three different approaches to this: `Rc`, an `&amp;_` references with the lifetime of an arena allocator, and numeric IDs: https://github.com/SimonSapin/rust-forest. (The arena might not work for you, since the whole idea is not dropping nodes until the whole graph is dropped at once.) The first two involve interior mutability which can be achieved with `RefCell&lt;Node&gt;`. But I’ve also found more convenient to push cells "more to the interior", on individual fields of `Node`, and to prefer `Cell` over `RefCell` for some fields. As /u/Omniviral mentioned one risk of `Rc` is leaking reference cycles. One way to deal with this, if your graph has some inherent structure, is to replace some references with `Weak` so that there are no cycle of strong references. (For example in a DOM-like tree the link to the parent node and previous sibling should be weak references, if nodes also have a strong reference to their next sibling, first child, and last child.) Finally, there can be advantages to push the IDs case further by keeping your graph in a structure of arrays rather than an array of structures, and structuring your program around data flow rather than control flow. Raph Levien explains this in https://www.youtube.com/watch?v=4YTfxresvS8 better than I could. (It’s a bit long, but the whole talk is well worth watching.)
This sounds similar to the same kind of problems that Raph Levien solved by using data flow rather than control flow and a structure of arrays in this talk: https://www.youtube.com/watch?v=4YTfxresvS8
This pattern is a common source of security vulnerabilities. Both security bugs from last month that I've had a hand in were because of `set_len()`: `inflate` was using `set_len()` exactly as you describe, and the function could be exploited to disclose memory contents. For something that's used in an imaging library (`png` crate), this is [devastating](http://seclists.org/fulldisclosure/2013/Nov/83). I've discussed it in my "[Auditing popular crates](https://www.reddit.com/r/rust/comments/8zpp5f/auditing_popular_crates_how_a_oneline_unsafe_has/))" post. `smallvec` was also using `set_len()` and seemed to do that mostly correctly, unless a panic has occurred, in which case it [performed a double free](https://github.com/servo/rust-smallvec/issues/96). Double free can be exploited to get arbitrary code execution. And that's just the ones I was involved in. There are probably lots of uses of `set_len()` out there that are still vulnerable.
&gt; But it can be safe, and checking whether it is is as easy as adding an assert that the size is smaller than the capacity and making sure that the elements are initialized or destroyed. This is nowhere near as easy as it sounds. Just last month I've had a hand in two security vulnerabilities caused by misuse of `set_len()`. `smallvec` seemed to work fine unless a user-supplied iterator panicked, in which case it had [an exploitable double free](https://github.com/servo/rust-smallvec/issues/96). By contrast, `inflate` crate had *a lot* of missing checks in a not that complex function, and I'm still not sure I've added all the required ones to make it safe. I've even written [a detailed post](https://www.reddit.com/r/rust/comments/8zpp5f/auditing_popular_crates_how_a_oneline_unsafe_has/) discussing the latter.
I wonder why something like [ed25519-dalek](https://github.com/dalek-cryptography/ed25519-dalek) was not fitting for this use case?
&gt;Overall better than Lucene is not possible. Can you explain why you feel this way? In what way do you feel Lucene will always be better than tantivy, and why do you feel that tantivy cannot become at least equal to Lucene in those areas?
If/When Rust gets HKT, those methods could become trait methods, which would keep it in one place. For example, `Option::map` and `Result::map` could just become `Functor::map`
&gt; It is so simple that you do not need any configuration, but if you insist... &gt; `crates.upToDateDecorator`: The text to show when dependency is up to date. Default is 👍 I really prefer an empty string here, to eliminate unnecessary noise. Now it looks much better.
Blog posts have terrible discoverability when you're new to something, how do you know whether it's still up-to-date, whether the author actually knows what he's talking about, if it follows the languages idioms and so on? I'd much prefer if we pooled resources like this into a beginners guide to not-so-uncommon questions sort of thing.
Note for everyone testing out `uniform_paths`: a fix for suboptimal/wrong diagnostics is up: https://github.com/rust-lang/rust/pull/53427.
Right now - no. crates.io database is public (it's in a git repository on github) so in theory anyone can do that. Someone has even put together a Python script to search for dependents of vulnerable versions on crates.io. However, it is not currently run on a regular basis, and uploads of new packages depending on vulnerable versions are not rejected by crates.io.
When I use an enum, it's because there is a limited number of options that type could be *and* it's impossible to know which one it is ahead of time - e.g. maybe I'm deserializing it, or throwing a bunch of them in a collection (enum are a nice way to make lists that hold a items that belong to a predetermined set of types). Using a common trait only works when: a) you can know the type ahead of time. You can still write geneic code: e.g. this function works with any Command object whose concrete type is known at compile time. (static dispatch) b) you don't know the concrete type, so you're passing it around as a boxed trait object (dynamic dispatch). This is what you would use if you need to stick a bunch of commands of different concrete types into a list (vec). Because a vec can only hold one type, you'd box up the various command types so that they all become the same type: a boxed command trait object. What you can do with an enum that you can't do with a trait object is deserializing thing. Basically, when the set of types implementing a common trait are finite and under your control (in a single crate) I think an enum gives you more advantages. 
Yeah, what we do in std is create a type that on drop corrects the length of the vector, so that if something panicks between ‘set_len’ and the moment were all invariants are restored, no memory unsafety can be introduced.
Oh, my bad, I completely glossed over that.
Hey, same here! I was really proud of myself for learning Piston2D in one weekend. However, I spent most of this week getting cross-compiled ports working (particularly a browser version with WASM/Emscripten), which resulted in a lot of agony but also a lot of learning.
Normal build: AST -&gt; HIR -&gt; MIR -&gt; LLVM IR Check: AST -&gt; HIR -&gt; MIR If you have an error, both would stop at the error. If you don’t have an error, check stops earlier.
Ah, okay! I get it now. So yeah, both ways have pros and cons. I believe this was a situation where the majority of code wasn’t doing it, and so we chose to stick with the majority style. It also feels more consistent with block alignment, which is the overall guiding principle. Maybe nick remembers more :)
You could turn this into a AOT compiler for bf pretty easily using \[cranelift\]([https://github.com/cranestation/cranelift](https://github.com/cranestation/cranelift)).
So like many other people posting here, I also participated in LD42 and used SDL/GL/Piston2D to write my game. I *just* finished adding cross-compilation to the CI so now it has also distributions for browser (WASM) and Windows! Lots of pain went into making it work, but I learned a lot along the way. Besides that, my primary project is still the design and initial development of the [Tsukurou](https://gitlab.com/tsukurou/spec) game development framework, but there's new news that's *even more exciting* \- I've added another side-project to my to-do list! (Which, by the way, is completely unrelated to the above two projects and will likely just distract me.) Introducing [RACK](https://gitlab.com/rack), the Rust Audio Connection Kit. The goal for the core is to be a platform-agnostic framework that anyone can use to send, receive, and process digital audio and MIDI signals. In addition to the core library, I plan on maintaining at least a few official backends and some basic utilities built using it. My ultimate goal is to use this to build a digital "matrix mixer" system that mixes multiple input channels to multiple output channels. This is something that I've wanted on an affordable, small-scale level (maybe 4 in 4 out stereo at max), but I can't seem to find anything similar for sale (maybe a few listed items on ebay, but nothing still being manufactured) unless it's integrated into a professional 16+ channel mixer. Also, if you're wondering, yes, I did kind-of rip off the acronym from JACK, but this project doesn't have much in common with it. I'm considering keeping the name "RACK" (that is definitely staying because I love it) and changing its meaning to something else. I'm open to suggestions!
Come on, you can't not include a link with that. I'm particularly interested in the web version, as I've spent a couple of days researching current approaches and so far only got Kiss3d to work.
That's very interesting experience, thank you. I looked through my code for mentions of `set_len` and found them mostly used to allocate memory before passing to `Read::read_exact`, a simple custom byte allocator scheme based on 'pointer bumping' a Vec (this might be suspect as it returns non initialized bytes) and a poor man's memcpy where I moved some chunk of repr(C) structs from one Vec to another. In some cases the call to set_len was followed or preceded by immediately initializing the buffer in a way which trivially shows all the bytes are written to, which is probably fine. The custom allocator is some abstraction where I used a Vec to buffer serialized binary data to disk, now that I look at it that definitely writes the underlying non initialized bytes making an info leak, or at least it's not obvious that it doesn't... I'll fix that, thanks! As for working with uninitialized generic T, that's a whole other can of worms, see [this reddit thread](https://old.reddit.com/r/rust/comments/95vxdy/understanding_ub_with_stdmemuninitialized/)...
Actually you can compile C# down to native code using CoreRT which uses the Roslyn 'Ahead-of-Time' compiler to achieve it. Its in alpha now but it works well enough for reasonably complex applications such as games.
One nit (arrived here from this week in rust) - I don't think you should be using an enum at the end of the article - you could in fact send a bad value through the port by using enums. Using separate wrapper types for each stage would be more type safe, and just as clear.
"Fast optimizing Brainfuck interpreter" is not a phrase I ever expected to see.
The argument for not using nested imports is that if I want to find all the locations where I'm using `std::fs`, I can grep for `std::fs`, and find them all. When it's broken over multiple lines, it's impossible to search for with standard tools, and you need some tool which understands Rust.
Why?
It feels like a bit of a non sequitur. I guess I'm too used to thinking of BF as a purely "toy" language, so the idea of an optimizing interpreter for it catches me off guard in a humorous way.
Thanks for suggestion, but this project was intended to be an interpreter
There's [rust-learning](https://github.com/ctjhoa/rust-learning), for one.
Ah, now I understand you. [You](https://github.com/Wilfred/bfc) [should](https://github.com/brain-lang/brain) [definitely](https://github.com/rdebath/LostKingdom) [see](https://github.com/lifthrasiir/esotope-bfc) [these](https://github.com/AsuMagic/AshBF) [projects](https://github.com/matslina/awib)
Is there any way to check for equality on method pointers? My naive attempt fails to compile... struct Foo { } impl Foo { fn bar() -&gt; String { format!("Foo.bar") } } fn main(){ let a = Foo::bar; let b = Foo::bar; if a == b { println!("Method comparison works!"); } else { println!("So sad..."); } }
Wait, doesn't a == Some(x) work? I mean, it's different to bind the Some data to a variable than it is to compare to one.
I probably really should not be surprised, but I am. Thanks! :D
To be sure, I don't mean people can't chat. It surely does have some uses. I think that copying useful context to issues and pull requests just helps everyone. It's especially helpful to the Author 1 Year From Now, when they can't remember why they made a change anymore (speaking from my own experience ;_;). I just have felt this several times, and figured being able to point at an exact instance is more easily understood than some vague feeling. As for experimenting, I've heard from others (and felt myself) that the std::future RFC is just de-facto accepted, since it's already in libstd, and changes happen there without more details in the RFC. 
Slot map is excellent. 
You can do this: Command::Quit(command) | Command::MoveRight(command) | Command::MoveLeft(command) | ... =&gt; { command.execute(); }
Out of curiosity, why do you want this, and how would you want this to work? One thing to note is that any two different function pointers like this will have an entirely different type, so no runtime confusion is even possible.
You can cast to pointers and compare the values of those: struct Foo { } impl Foo { fn bar() -&gt; String { format!("Foo.bar") } } fn main(){ let a = Foo::bar; let b = Foo::bar; if a as *const fn() -&gt; std::string::String == b as *const fn() -&gt; std::string::String { println!("Method comparison works!"); } else { println!("So sad..."); } } Playground: http://play.rust-lang.org/?gist=c38343483e91eadddf6ce05d18eb0dfb&amp;version=stable&amp;mode=debug&amp;edition=2015
Functions all have different types, but they can be casted to a shared type (eg. `fn() -&gt; String`) so runtime confusion is possible.
I'm trying to build a react-react like library toolkit for rust. It would provide the algorithm and tree management. The current state of it is that if a library writer defines a what the ui tree node types are (for example, like Div, P, and Button), they can invoke a macro that builds the necessary stuff to get the view diffing code. And then a consumer of that code could write custom components (with state and props!) so long as they implement the correct trait. And the diffing code maintains an shadow render tree, and emits render update requests (e.g. node x has changed, or node y has a new child) Right now, I'm trying to add interactivity through events and event handlers. Since managing to hack together something that works with custom components and the boxed component trait, I'm pretty confident that I could pass around function pointers and make them work, even with the unique types. Basically, components define a props type (well, struct - what others can pass in) and a state type (again, struct - their own internal type). Right now you can technically define a fn type on the props, but it won't ever be used. The equality check was important for sending out the diff updates. But as I think about it, that's actually not terribly important for diffing. I'm designing it so that there's a differ object (takes the render request, updates the internal render state and emits the differences to whatever is responsible for rendering it) - so the differ object can actually retain the method handles, and not send them out in the update. It can just send a callback that calls the method handle it's retained. In short, thank you for the question! I don't think I need it :D. Now I'm off to implement node id's, method handles and an indirect method reference type :) :) :) ----- p.s. The goal of the library is to easily enable writing of a view layer that's easy for users to write custom components for, a la react. It's different from react in that it's backend agnostic. It doesn't actually do any drawing - it just maintains an internal tree and emits drawing instructions. One could actually implement different drawing backends for any given library written using my project (once I get it to a point where I feel okay releasing).
Someone I knew once wrote an optimizing native code Brainfuck compiler, that would output an ELF executable. In Bash.
As my edit points out, my understanding was flawed and this technique isn't actually suitable for Rust─TBH I'm a little confused about the point as a whole of the guarantee that actually is provided. But the idea with combining two hashes would just be that you have two parameterizations (so six 64-bit integers) and you use one for the high 32 bits, one for the low 32 bits. You can also do this with three 128 bit integers, which is probably significantly faster if you have the appropriate instructions. You don't need different hash functions because the algorithm is parameterized.
Wow, that's awesome! And if I add a Foo::baz with the same signature as Foo::bar, it reports that the method pointers to them are different. Sweet :D
Doofenshmirtz evil incorporated 🎶🎶
It would be `a.is_some() &amp;&amp; a.unwrap().is_okay()`. However, then you have a use of `unwrap` in the code (or `expect`), and each time you read it you'll spend time (again) convincing yourself that this time it's okay and cannot possibly panic :x
`a is Some(x)` **introduces** the binding `x`. Subtle, I know.
Maybe. It's not always obvious though :(
I see, so it's only about binding with short circuit.
&gt; You don't put anything in the list and it's up to rust teams to be sure that they're still up to date. I feel like a central documentation source would work better for the team. With a blog list, they'd have to watch blog posts, ask for updates, wait for update, and de-list or clone it if not updated. If its in one place, they maintain control over it, propose updates in an RFC, and update as part of RFC tracking issue. Some examples of successes for centralizing logic from blog posts: - [killercup's post on APIs](https://deterministic.space/elegant-apis-in-rust.html) was a precursor, at least from my perspective, to the [API guidelines](https://rust-lang-nursery.github.io/api-guidelines/) - I merged several blog posts and my experience into [crate-ci](https://crate-ci.github.io/)
I don't get that is_okay since it panics, not return an error. And sure, but for that you could just use a == Some(x).
Oh nice! I didn't know this one. That's pretty great!
This just occurred to me, but I wonder whether it's possible to strip out, well, all the debug information that doesn't matter... find every place where a program *can* panic and only keep the debug symbols for panics that can be reached and dependent functions. Anyone know whether this is done already?
Can you describe in a few words for people far from the "subject"?
`x` does not exist prior to `a is Some(x)`.
It's about providing a binding which returns a `bool` as to whether the binding succeeded or not. What you do with the boolean is up to you :)
How did you end up closing the portal to the Dark World which that kind of black magic must have created? 
Isn't `&lt;'_&gt;` about signalling that `Formatter` may not be `'static`? I don't understand why it being borrowed by `&amp;mut` changes anything :'(
I really wish I could distinguish posts other than mine :(
It's actually a common but rarely spoken about job title in the industry
Note for everyone testing out `uniform_paths`: a fix for suboptimal/wrong diagnostics is up at https://github.com/rust-lang/rust/pull/53427. *Kindly brought forward by /u/eddyb at https://www.reddit.com/r/rust/comments/97mpt0/annoucning_rust_2018_preview_2/e4ahcdl*
You can install Rust with some package managers, but rustup is easier to use and always provides the latest version. 
I see, but your code doesn't make sense tho.
Haha ok. Here's the game page. I have links to the source code if you want to look at it. Main weird thing about targeting WASM with my given frameworks and backends is that I had to patch in a [custom rust-sdl2](https://github.com/agausmann/rust-sdl2/tree/emscripten-bundle). The upstream version doesn't currently support `target_os = "emscripten"` so I added support for building [emscripten's port of sdl2](https://github.com/emscripten-ports/SDL2) in those cases. I wish there was a backend that built properly out-of-the-box, but this was the only one I could get to work. If you have any other questions feel free to contact me, I probably missed some other important things. [https://ldjam.com/events/ludum-dare/42/astrohoarders](https://ldjam.com/events/ludum-dare/42/astrohoarders)
Thanks, just added it!
The rust community loves bleeding edge. Most of us develop in nightly. Even the Arch repos don't update fast enough for our liking. 
Yeah, the language isn't complete enough to make developing on stable useful, I get it. But at the very least, install rustup through your package manager, so at least *that* is tracked properly.
I'm pretty sure arch is the only one that packages rustup right now. Maybe debian or ubuntu. Maybe you could make a PR for rustup that manually adds it to different local installed package databases when you curl it. 
You can install rustup with your package manager.
That's false, plenty of food dick to Steele and don't miss anything.
Another idea, which is Windows-specific, is to create some [COM Interfaces](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/interop/example-com-class). You can use [winapi](https://github.com/retep998/winapi-rs)'s [RIDL! macro](https://docs.rs/winapi/*/x86_64-pc-windows-msvc/winapi/macro.RIDL.html) to declare the interfaces in Rust, and use them in a very simple way. 
Thanks for letting me know! I enabled issues and sent a message to GitHub support to make it not a fork. &gt; docs do not generate You need to use a recent nightly. I also just noted it in the README file. 
I don't know too much about rust internals but Rust switched to [LLVM 6 on version 1.25](https://blog.rust-lang.org/2018/03/29/Rust-1.25.html) and [LLVM added support for preventing spectre branch prediction attacks](https://releases.llvm.org/6.0.0/docs/ReleaseNotes.html#non-comprehensive-list-of-changes-in-this-release). It might be related to that.
[Here](https://github.com/CensoredUsername/dynasm-rs/blob/master/doc/examples/bf-jit/src/main.rs), have an optimizing brainfuck JIT compiler written in rust.
rustup updates itself, there's no tracking issue here.
&gt; Most of us develop in nightly. This at least wasn't true in 2017, per [the survey](https://blog.rust-lang.org/2017/09/05/Rust-2017-Survey-Results.html). We'll see what things look like this year. Direct link to the distribution of version usage: https://blog.rust-lang.org/images/2017-09-05-Rust-2017-Survey-Results/which_version.png
Thanks for explanations :) it really helps. Would be good to have this in docs. 
I mean file tracking. As in, does it have a clean uninstall that removes *all* files it created? Can that be done through the package manager? That's the benefit of a package manager.
I knew about LLVM 6 but didn't consider the spectre stuff. That may play a part indeed. Data dependent branches are everywhere in this code.
Cool! However, if I build it with the MSVC toolchain and try to debug with visual studio, I can never step into invoke_main - it just continues execution at that point. Is there any way to fix this?
Nixpkgs packages `rustup` too.
I only glanced at the go article, but it's probably the same reason the Rust standard library removed scoped thread spawns? Mainly that `std::mem::forget()` and similar operations are safe, so a join handle can't be the only thing preventing double mutable references or use-after-drop.
I'm using actix-web (with rust-embed to embed the yew wasm frontend that uses `WebsocketService`), it allows serving the websockets from the same port as the assets. Then I can easily get the ws url in the frontend like this: fn ws_url() -&gt; String { let host = window().location().expect("location").host().expect("host"); format!("ws://{}/ws/", host) }
I have no idea how much visibility https://rustjobs.rs/ gets, but you might want to post this there too just in case.
One benefit is that you will usually start out with `foo.rs`, but then you want submodules, so you have to move to `foo/mod.rs`. With this change, `foo.rs` stays as-is, and all you need to add is `foo/bar.rs`.
I have warmed up to it after seeing all these thought-out examples. As always the Rust community achieves a solid design.
ok, got "built" to run, I misread the documentation and my insufficient Rust knowledge failed me. For the archives: In build.rs the documentation suggested built::write_built_file() does not include DEPENDENCIES in the default Options. Instead, one has to call built::write_built_file_with_opts and set the DEP option to true - so best way is to take the crate source and copy&amp;modify write_built_file_with_opts to obtain a custom build.rs: extern crate built; use std::{env, path}; fn main() { let src = env::var("CARGO_MANIFEST_DIR").unwrap(); let dst = path::Path::new(&amp;env::var("OUT_DIR").unwrap()).join("built.rs"); let mut opt = built::Options::default(); opt.set_dependencies(true); built::write_built_file_with_opts(&amp;opt, &amp;src, &amp;dst); } In the main.rs then do: mod built_info { include!(concat!(env!("OUT_DIR"), "/built.rs")); } fn main() { println!("{:?}", built_info::RUSTC_VERSION); for entry in built_info::DEPENDENCIES.iter() { println!(" {:&gt;22} {:}", entry.0, entry.1); } } Unfortunately the printed list is incorrect, see documentation of set_dependencies for why. I spent quite some time to get this to run and learned Rust while doing so :-) In summary, the problem that a program can self-report the packages and compiler it was build from remains unsolved. I now believe there should be a compiler flag/feature that when enabled adds a well-known data structure at compile time - the precise point when all dependencies are known? 
&gt; does it have a clean uninstall that removes all files it created? I just ran `rustup self uninstall` and it seems to have nuked itself. Is this feature missing some behavior you want?
You're talking about an [eBGP route server](https://tools.ietf.org/html/rfc7947) but this is an [https://tools.ietf.org/html/rfc6480](RPKI) (Resource Public Key Infrastructure). The goal is not to figure out where to send the packet (that is the job of the routing protocol) rather to cryptographically validate that the person advertising e.g. Google's IP space into BGP saying "send packets to Google here" is actually Google and not some malicious attacker or error.
Point is, if you use a package manager, it's both a *standard* way of doing it, and is *always* going to work. If rustup had a bug that made it not actually delete everything, or had a bug that deleted *too much* (see: steam), then you're at the mercy of rustup's uninstaller to properly uninstall everything. With a package manager that knows what files are meant to be where, it can easily just delete the files. It knows there won't be any conflicts. And if your install gets corrupted or something, you can simply install the package again and it'll ensure that the files there are correct. That, and teaching people that running random code from the internet is actually the best way of installing stuff isn't a great idea. With a package manager you can have some level of trust that everything there is maintained by a trusted individual. Even AUR helpers encourage you to read the PKGBUILDs.
&gt; and is always going to work. &gt; if your install gets corrupted or something, you can simply install the package again This has not been my experience with apt. I'm thinking of trying out Nix, or do you have suggestions on a better package manager? 
If you don't trust the Rust developers, why are you running their code on your machine? 
Idk. I use pacman. I just tried deleting `/usr/bin/rustc` (Owned by rustup), and now, obviously, rustc doesn't work, and `cargo build` complains about a missing rustc. But `pacman -S rustup`, ignoring the warning about it being up to date and it reinstalling (We know it's up to date, we want it to explicitly copy over the files again), and now everything works again. It complained about `warning: could not get file information for usr/bin/rustc`, which again, is to be expected. It doesn't exist, but it copied it over again from the version of the package it has locally.
I trust the rust developers. But if their site gets hijacked for 10 minutes before someone notices, *everyone* who installs rustup through that script while it's malicious would be fucked. There's nothing forcing you to actually read the code. With a package in the repo, it's only packaging *releases*. So shit going wrong in terms of malicious code is a lot harder.
Can the macro impl_web be replaced with derive at some point? It would be nice to have a list of pending things for near 1.0 release. I know there are issues already there for most of them. 
I rm'd my `~/.cargo/bin/rustc` and ran `rustup update`. It's back, everything works again.
&gt;It feels like a bit of a non sequitur. I guess I'm too used to thinking of BF as a purely "toy" language, so the idea of an optimizing interpreter for it catches me off guard in a humorous way. U wot m8, I wrote an optimizing Brainfuck to Piet compiler to learn OCaml. [Shameless plug](https://github.com/theindigamer/b2piet)
Congratulations :)
True. But it's a small use case, I belive. And making tools that can achieve this (even by translating rs files, for "legacy" tools to work with) that shouldn't be hard, given how awesome Rust ecosystem is.
I think that there was a discussion about this somewhere in [https://github.com/rust-lang-nursery/fmt-rfcs](https://github.com/rust-lang-nursery/fmt-rfcs). IIRC the consensus was that the vertical alignment should be avoided to reduce the diff churn.
Agree, but if you don't use self-borrowing types, as I guess is the case for the OP on futures-0.1, it is not going to make a difference. I don't have still experience with futures-0.3 (still waiting the dust to settle) but I suppose it will be possible to use futures with self-borrowing types without the async/await syntax - I am used to playing the type-tetris needed. If they make it more ergonomic and easier to reason, I am the fist to switch.
I'd be lying to say I wasn't slightly disappointed. This being reddit, I expected something completely different for a program named 'cat'! Hahahaha... I'm so dumb.
That's the right way to do things. So you were able to "see" your update in one of the threads, but not in the other threads. Is that correct?
Oh wow that's cool. Was this tech always around or was it developed later when there was a need for it? 
There have been various ideas available for a long time but sadly the last nearly 25 years of BGP4 has been maintained by people maintaining accept/reject filters and handling tickets on anything that sneaks through. RPKI by itself doesn't make things foolproof either, there is another extension called BGPsec that builds on RPKI which provides full path authentication. Unfortunately getting the entirety of the decentralized internet to update (even just incrementally) has been and continues to be a slow process. Much as getting the world to switch to IPv6.
Username checks out
You can set a breakpoint on the actual main() function, or somewhere else in your own code.
Good thing the script is hosted on github. If github is compromised we can all kiss secure computing goodbye. 
Are you aware of [https://matthias-endler.de/2018/fastcat/](https://matthias-endler.de/2018/fastcat/) ? How does it compare? 
Did you try middle clicking to paste? That might be putting it into the other buffer.
Perhaps an indicator if something is NOT up-to-date would be better?
You could edit Rust files in Xcode but not much else I imaging. Xcode won't understand how to build a Cargo project.
Oh god why would you want to use Xcode for anything that's not iOS/macOS development? It's so slooooooow :-) I tried it once, but didn't work properly. No RLS support, no cargo support, etc...
What should I use.
Why would you want to? Just use vscode or atom with the rust IDE plugin.
It really doesn't matter. Most big editors work (relatively) fine with rust. Sublime, Atom, VSCode, VIM, etc... Just have a look at this: https://areweideyet.com/
`sudo rm -rf --no-preserve-spacetime`
VSCode looking good so far. Thanks.
Also for consideration IntelliJ Community edition + Rust plugin. Might also check: [https://www.reddit.com/r/rust/comments/941nzc/book\_recommendations\_and\_ide\_to\_learn\_rust\_for/](https://www.reddit.com/r/rust/comments/941nzc/book_recommendations_and_ide_to_learn_rust_for/)
Correct. To be clear, I'm not 100% sure it was an issue with multiple threads—it just seemed that way: I would refresh a search results a few times and it would show the "correct" results, and then I would refresh it again and it would contain terms that I had deleted.
It can be replaced with attribute macros which will be in stable with the 1.30 release. 
Doc PRs gladly accepted. 
Though you didn't say it explicitly, I'm guessing that since you've shared this here you're also looking for feedback. One thing I notice is your `Optionify` trait. It might be simpler to make use of the recently-added [`Option::filter`](https://doc.rust-lang.org/std/option/enum.Option.html#method.filter) method instead. For example, the following code can be rewritten: ```rust let end_char = matches .opts_present(&amp;['A'.to_string(), 'E'.to_string(), 'e'.to_string()]) .as_some("$".to_string()); ``` to ```rust let end_char = Some("$".to_string()) .filter(|| matches.opts_present(&amp;['A'.to_string(), 'E'.to_string(), 'e'.to_string()])); ```
Well it sort of makes sense that someone might make an Xcode plugin for rust at some point since MacOS and iOS are compilation targets, but it looks like nobody has yet. I've been using atom with rust-IDE (plugin) and it works pretty well. The official rust language server plugin was made for vscode though so that one might be a little better somehow or get updated more frequently. There's also a rust plugin for IntelliJ IDEA. Those are the main ones people mention.
You should replace Primary with Clipboard if you want that.
Saw this in the docs: let mut f = File::open("foo.txt")?; let mut buffer = [0; 10]; // read up to 10 bytes f.read(&amp;mut buffer[..])?; Is there any difference between `&amp;mut buffer[..]` and `&amp;mut buffer`?
Building a program for a particular computer involves more than just knowing what CPU it uses: there's a whole bunch of conventions about how parameters are passed when calling a function (including how the operating system calls things like `main()`) and how results are returned and all kinds of different stuff. Popular choices include "gnueabihf", "musleabihf", and "androideabi", but since they're mostly just arbitrary choices there's a lot of potential options, and many (most?) don't even have official names. Somebody making a computer that's designed to be general-purpose and extensible, like a PC, a Raspberry Pi or an Android or iOS smartphone, is likely to stick to one of the most popular options so that developers can use existing tools. On the other hand, somebody making a computer designed to be single-purpose, like a camera, might choose a popular option, or they might completely make up their own incompatible thing to save a few cents per unit (if we teach the compiler to assume register 12 doesn't exist, my cousin's got a bunch of otherwise-fine CPUs we could buy for cheap!). Obviously *somebody somewhere* has a compiler that works for your device, because they built a Linux kernel (and other firmware) for it. But lacking more specific documentation, you might have to copy binaries back off the device, disassemble them, and figure out by inspection what conventions they expect.
`&amp;mut buffer` has type `&amp;mut [u8; 10]` (that is, borrowed 10-item array) `&amp;mut buffer[..]` has type `&amp;mut [u8]` (that is, borrowed slice)
Does the rust plugin work outside CLion? 
Thanks for such a kind and patient response. I ran across https://github.com/hello/kasa/blob/master/ambarella/build/env/armv7ahf-linaro-gcc.env and https://github.com/hello/kasanojo , which look to be a cross compiler setup suggesting that something like a linaro arm-linux-gnueabihf-gcc perhaps ought to work, but I’m not having any luck with that, either.
In that particular situation, where you have a borrowed fixed-length array, and you're trying to use it in a situation that requires a borrowed slice, there's really only one possible way to do the conversion, [so Rust quietly does it for you](https://doc.rust-lang.org/nomicon/coercions.html) and compilation continues. The same kind of thing happens when you have a `&amp;mut T` but you need a `&amp;T`, for example. It's possible that the docs were written before the compiler was taught about that particular coercion, or that the author was used to having to spell things out in other contexts and wrote out the longer form out of habit.
Thank you, makes sense.
It looks like that project builds a whole new environment from scratch, which means they get to pick whatever convention they want (and so they pick a common one). In your situation, you're trying to build off the environment provided by the existing firmware.
Linaro toolchain should work. In .cargo/config you must set path to libraries set ar and linker to linaro toolchain. [target.armv7-unknown-linux-gnueabihf] rustflags = ["-L/path/to/linaro.../lib"] ar = "arm-linux-...-ar" linker = "arm-linux-...-gcc" You should have some makefile or bash script to build your project with. This is what I use (with some additional variables for openssl and such): PATH=/path/to/${TOOLCHAIN}/bin:${PATH} \ CFLAGS="-I/path/to/${TOOLCHAIN}/include "${CFLAGS} \ LDLAGS="-L/path/to/${TOOLCHAIN}/lib "${LDLAGS} \ CC=$arm-linux-....-cc \ PKG_CONFIG_ALLOW_CROSS=1 \ cargo build --release --target=${RUST_TARGET}
I wrote something similar, but it uses Serde to flush to a JSON file on any write actions, it's called MVDB: https://github.com/jamesmunns/mvdb-rs
And then if you refreshed it again, and again, is the result flaky or does it stay permanently in the faulty state? 
Vim with RLS
Indeed. The extension shows the latest available version if a dependency is not up to date (that `Latest: x.y.z` label).
No remote though, right? I'd mailed some time back, and didn't receive any responses! :-)
Ahh yes. I'd forgotten that destructors are not guaranteed to run. Thank you.
I just use vs code and love it, but eclipse does have support for rust. I think it’s called eclipse corrosion. It’s not as fully featured as I’d like from an ide but it seems to work fine. 
What you are doing here is hand-rolled dynamic dispatch.
I know, I don't think I even wrote it. I pasted the link didn't see the r/ until it was posted. Maybe some problem with the new layout? Test: http://www.example.com/
It’s not type tetris that makes it hard, it’s the large amount of unsafe code needed which makes it very easy to accidentally introduce unsoundness.
I believe you're looking for https://diesel.rs/
Hey, thanks, wrapping it in a box worked great, so this is the code: `Box::new(move |data: &amp;[u8]|{...})` I actually thought of using box yesterday, but I didn't create the closure in the box immediately (like I did in that snippet), but tried to box it later on, when it was already created, but that didn't work because of the same reason (that closure didn't have static lifetime). Just one more question, I didn't use `Box&lt;dyn Fn(&amp;[u8])&gt;`, but `Box&lt;Fn(&amp;[u8])&gt;`, and everything seems to work great. So is `dyn` needed here? 
I can absolutely relate to your observations on error handling in rust. I love the way rust treats error handling in general but in detail is really tiresome in the current state. I love how the ergonomics initiative brought good things onto the table but I think error handling was forgotten in that because I think it's currently the single most unergonomic thing for both beginners and intermediate rust users. You can have a multiple hour long conversation about error handling with a beginner without much effort if you try to go through an example that's more involved as a hello world like fetch a file from the web an save it on disk. This should not be the case and is a huge time sinkholes for learning rust. And for intermediate users it's just tiresome boilerplate and a drag while prototyping. Anyway, very cool article. The mongo part was very interesting to me because I never really used it out of a context like node.js/meteor 
`dyn` is never required in current Rust.
Please add cat pictures to the Readme.
You might also be interested in https://crates.io/crates/structopt :)
Can you please add a license, so that people can contribute.
Thanks! So, I should add it now, even though it isn't needed, to make sure my code compiles with future versions of rustc?
Eclipse + RustDT works on any debug-enabled code. Can easily flow between Rust, C, C++.
You can shadow the trait method with an inherent method which always gets called if it's accessible: impl Ipv4Net { pub fn contains&lt;T&gt;(&amp;self, other: &amp;T) -&gt; bool where Self: Contains&lt;T&gt; { Contains::contains(self, other) } } However the more straightforward approach might just be to have two separate inherent methods, one for `Ipv4Net` and `Ipv4Addr`, especially if the semantics are different.
I bet someone will run into troubles in the near future thanks to this crate name
Thank you!
Yup. I'm looking for feedbacks, but I realized only after having posted my link that Reddit doesn't allow to write something for a link-type post. Cool suggestion by the way!! I think I'll make use of it. The more standard features, the better.
I am aware. `fastcat`'s best feature only works if your file doesn't need manipulation, meaning character replacements, adding line number... when your `cat` command doesn't have flags, basically. See my `fast_print` function. In that regard, `fastcat` is some orders of magnitude faster, because it doesn't physically copy a file to the stdout but remaps the memory by means of a syscall.
Just to be sure: your current code will keep compiling on future versions of rustc. That's because rustc will support both Rust 2015 and Rust 2018. But it's a good habit to use dyn from the start, I think.
Done!
Ugh, why does this API require `&amp;[String]`...
Well, I hope not! I have a "FOR LEARNING PURPOSES ONLY" disclaimer right here: https://docs.rs/crate/toykio/0.2.2
Some small feedback items, in no particular order: * `&amp;[b'a', b'b', b'c']` is just `b"abc"`. * `match { x =&gt; print(a)?, y =&gt; print(b)? }` is simpler as `print(match { x =&gt; a, y =&gt; b})?`. * the future is to use `..=` as the inclusive range. * when you `write_all` individual bytes, you'd better use a `BufWriter`. * `if let (b'\t', Some(tab_str)) = (byte, &amp;options.tab_char)`. * `buf[len - 1] == b'\n'` can panic for non-ASCII. * `append_str` should probably chop of the newline, and then just do `push` operations. * no need for taking a reference in `Ok(ref len)`. * `write_all(format!(x).as_bytes())` is just `write!(x)` but with an unnecessary allocation. 
I'm not sure if this is the right place to ask, but are there any examples of IPC on Windows? There's some crates (namely [ipc-channel](https://github.com/servo/ipc-channel)), but there's not much in the way of documentation or examples.
That won't necessarily suffice. For example, both I and another user in this thread have assumed it was a misspelling of tokio.
You can't do this. For example, if `T` is `Drop` and `f` panics, you'll have UB.
You mean getopt? If so, yeah, it's horrible.
What kind of troubles would you imagine that people thinking it was a misspelling would cause?
Thanks. Hmmm, back to the drawing board.
Google seems to think I'm trying to search for `tokyo` whenever I do a search for `tokio`. I gotta put quotes around it as it is!
I disagree with most on this name. The name seems fine to me. When I read the title, I thought right away that it was someone's "Toy" implementation of async IO.
Have a look to: \- [Rust book chapter](https://doc.rust-lang.org/book/second-edition/ch09-00-error-handling.html) \- [Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/errors.html) \- [On the use of then](https://hermanradtke.com/2016/09/12/rust-using-and_then-and-map-combinators-on-result-type.html) \- [Failure](https://github.com/rust-lang-nursery/failure) \- [Andrew's guide](https://blog.burntsushi.net/rust-error-handling/)
It might also be worth looking for crates which let you use ODBC from Rust, so you could use a vendor driver. *Might*.
If you're not into videos, i believe these are the slides: https://lislis.de/talks/fp-rust/
I think something like the following *might* be okay, but the semantics around `::std::mem::uninitialized` are complex, so who knows? At least the `&amp;mut T` parameter ensures that `T` isn't a type without values. pub fn apply_to_mut&lt;T, F: FnOnce(T) -&gt; T&gt;(x: &amp;mut T, f: F) { struct Bomb; impl Drop for Bomb{fn drop(&amp;mut self){::std::process::abort()}} let mut value: T = unsafe { ::std::mem::uninitialized() }; ::std::mem::swap(x, &amp;mut value); let bomb = Bomb; value = f(value); ::std::mem::forget(bomb); ::std::mem::swap(x, &amp;mut value); }
Scalars can be defined quite easily for Juniper, see for example the Uuid scalar implemenation as an example: [https://github.com/graphql-rust/juniper/blob/master/juniper/src/integrations/uuid.rs](https://github.com/graphql-rust/juniper/blob/master/juniper/src/integrations/uuid.rs) graphql_scalar!(Uuid { description: "Uuid" resolve(&amp;self) -&gt; Value { Value::string(self.to_string()) } from_input_value(v: &amp;InputValue) -&gt; Option&lt;Uuid&gt; { v.as_string_value() .and_then(|s| Uuid::parse_str(s).ok()) } });
That *might* be ok if and only if it is not possible for T to contain any self referential borrows/raw-pointers. ([this excellent 6 part series of blog posts seems relevant.](https://boats.gitlab.io/blog/post/2018-01-25-async-i-self-referential-structs/)) /u/ImportantAddress as /u/thiez said, the semantics around `::std::mem::uninitialized` are complex. From what I understand that is mostly because it hasn't been formally decided what those semantics are, exactly. (See https://internals.rust-lang.org/t/blog-post-never-patterns-exhaustive-matching-and-uninhabited-types/8197 and https://github.com/rust-lang/rfcs/pull/1892 for example.)
There's a crate for this: [take_mut](https://docs.rs/take_mut/0.2.2/take_mut/fn.take.html)
I've been meaning to check out [tiberius](https://crates.io/crates/tiberius) properly but just haven't had a chance. I have to work with MS SQL stuff at work, though, and I've found [this ODBC crate](https://crates.io/crates/odbc) to work well generally. I actually use Rust at work specifically for little web applications to snag information from directly from MS SQL databases and some other sources I get json from. I mostly choose to work in Rust because I'm a lot more familiar with it than any other language I might use to write web apps. I work for a tiny college, which means both that we're poor and need to squeeze everything out of what resources we have that we can *and* that I'm the only programmer working with 90% of our databases and stuff, so there's nobody to stop me. Kind of a unique situation.
&gt; For error handling, I would strongly recommend the &gt; https://github.com/rust-lang-nursery/failure &gt; crate. The code in the article is using the failure crate and the critique is targeted at it, too. 
I just encountered an unhelpful error when I tried to clone a struct and pass a mutable reference to a function, but forgot to derive Clone. [Example here](https://play.rust-lang.org/?gist=9534fc6f2d4892d811b842d53a0dcf2e&amp;version=stable&amp;mode=debug&amp;edition=2015). Is this a known issue? It's clearly cloned the reference, hence this error, but it took me a few minutes to figure out why it was giving *this* error and not a missing function error.
That particular special case could also be `a.map_or(false, |x| x.is_okay())`
If you don't like being "boring", you should definitely switch to PHP which is totally not boring: `str_split` vs `strcmp`, `htmlentities` but `html_entity_decode`, `usleep` but `microtime`, `strtoupper` but `deg2rad` it is even less boring that you could think of: `array_filter(array, callback)` vs `array_map(callback, array)` or (I personally love this one): `stristr(haystack, needle)` but `in_array(needle, haystack)` Do you still think that "being boring" is a good idea? Anyway, this crate name shows that lack of namespaces in crates.io hurts. Imagine that we have "regex" and "regexp" crates or "StructOpt" and "Struct0pt"...
&gt; but wasn't able too Why? Did you try to disable the encryption?
In the context of numeric types, `From` is only implemented for lossless conversions. `f32` to `u32` is lossy: std::f32::NEG_INFINITY as u32 as f32; // 0f32 std::f32::INFINITY as u32 as f32; // 4294967300f32 4.23f32 as u32 as f32; // 4f32 Additionally, `as` does not work with generic types. The crate [`num-traits`](https://crates.io/crates/num-traits) might provide you with the desired functionality: extern crate num_traits; fn main() { use num_traits::NumCast; let i: Option&lt;u32&gt; = NumCast::from(4.6f32); // Some(4u32) } 
or prefix the search with 'rust', I find that works well too
"kio" are the last 3 letters of 'tokio' but with 'toy' substituted at the beginning. it's a portmanteau of 'toy' and 'tokio'
Diesel doesn't have SQL Server integration, but if you're interested in working on adding it I am happy to help however I can.
oh yeah, I get it, I just wondered if the k in tokio stands for anything now...
Not sure if I should just remove it from crates, because it's not supposed to be installed / used. The idea is that one can study its source code to learn about executors.
I think this would be difficult to happen accidentally, unless `future` implements `Copy`.
You know what's going on bois. :\^)
It is clever, but the problem is that almost everyone will assume it's a misspelling of Tokio. That was certainly what I thought when I first saw the article.
I didn't even notice the spelling at first.
Have you read [The Rust Programming Language](https://doc.rust-lang.org/stable/book/)? There's chapters on the OOP and functional aspects of Rust that may help. Even if you're an experienced programmer TRPL is a good skim if you're new to Rust.
&gt;It seems like the only reason you have a tasks_done vector is because you can't remove entries from wait_queue as you iterate it Correct. &gt;Is it possible to fix this with some unsafe {}s? Wouldn't it be iterator invalidation? I'd rather stay on the safe side here. 
Since `wait_queue` isn't actually iterated it could be avoided by just removing the tasks as they are 
 Cannot open /dev/zero: Is a directory MY LIFE IS A LIE!
In OOP you tend to think in terms of "IS A". In Rust, with Traits, think more in terms of, "DOES/CAN DO/SUPPORTS". A Trait is a set of verbs. Implementing a trait for a struct means that that set of verbs collectively can be used with that struct. So, an algorithm that needs to operate on things that "Can do" something, just operate on the given Trait or Traits. So, stop thinking in terms of Nouns (objects) and instead think in terms of Verbs (Actions/Sets of Actions). Think of structs as just buckets for structured information that a set of verbs (a Trait or Traits) can operate on. The important part is the traits and the verbs (methods) they define. The structs are secondary.
So Rust is using component-based approach? Thanks! I get it now.
I have read some pages few days ago but I still didn't quite get it. But now I know I should use component-based approach instead of Java-style OOP.
The learning curve is real. I think you're probably feeling the forced composition-over-inheritance. It might help to back up, if only for an afternoon, and really internalize composition-over-inheritance in your favorite OOP language, and then rust (and all functional languages, really) may seem more intuitive.
One could use separate enums and a separate "type" of channel for each stage, as opposed to one type for each stage as is done in the example. I'm not certain what you mean with 'separate wrapper types'. From my perspective, a match on an enum is really as type-safe as anything else, even-though the compiler will not catch logical errors. I am not too worried about writing code that happens to use the wrong variant, as such a logical error, or perhaps typo, is best caught by automated tests(or probably by the compiler after all since you tend to deconstruct and use those values down the road). On the other hand, by choosing a 'one type per variant' approach, you are losing a lot of the expressiveness that Rust has to offer in the form of enums and pattern matching. Also, consider a case where there are really lots of variants of messages, for example https://github.com/servo/servo/blob/ad83faa7452f46fe496487f21d2469616417a7b5/components/embedder_traits/lib.rs#L74 As far as abstractions go, the 'one type per variant' approach doesn't seem to scale as well as enums. 
Good suggestion. I have now added the functions for the not declaring a variable before a calling the functions :)
You should read the whole thing, note what you don't get, then look up on it. Rust is notoriously famous for having a difficult learning curve, and even immediate user sometimes still struggle to get their program to compile (I.e. fighting the borrow checker).
Noted. Thanks for the tip!
Is this for some kind of parallel garbage collection? Why don't you update the `end_pointer` *before* updating `free_pointer`? That way you're guaranteed that `free_pointer` can never be larger than `end_pointer`. Are these values monotonically increasing, or can they get smaller at some point? On what kind of processor are you testing? The x86 / x64 architecture has very strong memory ordering guarantees for non-atomic loads and stores, which can hide concurrency bugs that would trigger on an architecture with weaker guarantees.
&gt; Is this for some kind of parallel garbage collection? Yes, though it is not a _concurrent_ collector. &gt; Why don't you update the end_pointer before updating free_pointer? Further down there is code that will run as long as `free_pointer &lt; end_pointer`. Advancing `end_pointer` first could cause the code to try and allocate into the wrong place. &gt; Are these values monotonically increasing, or can they get smaller at some point? They are monotonically increasing. &gt; On what kind of processor are you testing? I'm using a Ryzen 5 1600X, which is x86-64, though the code doesn't exclusively target x86-64. As such I'd prefer not to depend on x86-64 specific guarantees.
I would consider removing the gl\_ prefixes, which are an artifact of C having wide open namespaces. Also it would be great to have an "easy" wrapper like this for gfx-rs when they finish the rewrite
I wouldn't go around calling ripgrep a "legacy" tool. Using a tool instead of text search is a bit of a hurdle though. It's not like I can hook the tool up to vim's `/`. Yeah, we can use a ctags like system, but it does add friction. Why not add tooling support to make it easier to write the searchable versions, as the only major advantage those have are being shorter, right?
I would say that the trait is the main verb. Then the structure would be any extra verb phrases and nouns that go around it to make a sentence. I am going to the bar "Going" would be the trait. It gives a basic idea of what you're doing. "I am" and "to the bar" is the struct. It gives meaning to the trait, and conversely, the trait gives meaning to the struct. But going can be used with different surroundings. "He should be going to school" You're still using that base "going" and it still has the same idea, but the surrounding stuff changes it completely.
So why should it be there?
It's there for people to find it.
If you're lazy, you could also turn the two points into relative offsets as u32, pack those into an usize and write the whole thing atomicly.
Tokyo Tokio Toykio It's obviously confusing to the passive observer and probably one pun too many. I could suggest toy-kio or toy_kio as a compromise. Does crates.io allow hypens or underscores in crate names?
I have a function that returns a struct. But I want that struct's fields to be immutable or basically whole struct immutable. Is there only option to make getter method for each field an set them private? If so does it cause an extra overhead?
&gt; I would consider removing the gl_ prefixes, which are an artifact of C having wide open namespaces. Yep, if every single one of your functions starts with `prefix_` you can add a module instead so people can use `prefix::thing` or just `use prefix::thing; thing`.
&gt; Is there only option to make getter method for each field an set them private? Yes, pretty much. &gt; If so does it cause an extra overhead? If this struct is only used within your crate, then no: the getters should be easily inlined. If you are writing a library and the getters may be called from other crates, you may want to mark them as `#[inline]`, which enables cross-crate inlining.
Something about tokens since mio uses `Token`, something about I/O, something about Tokio being an alternative spelling for Tokyo the city...
Is this mentioned in the docs somewhere? I wouldn't mind taking a look at those :)
this threads make it seem like the focusing on the name is more important than the actual project. What do you guys think?
Would love any feedback on parts of warp that were weird! One thing I noticed was using the `unstable_pipeline()` call for the server: the cases where that is needed are pretty much never, it's better to not use it (it's *only* purpose is to appease silly pipelined benchmarks).
This is what I use, but I wouldn't recommend it to someone coming form Xcode.
The process of having to refactor your errors that you described is actually what should be done in all languages. Any language where that process isn't required has poor error handling IMO. It sounds like you dealt with exceptions, in whch case feel free to unwrap everything as it is almost equivalent. Or you can do as above suggested and use failure::Error and the ? operator heavily. I haven't found it to be any more obtrusive than Go, and Go forces you to deal with errors on return in a similar way.
Not mine, just something I found awhile back and liked. 
Its what I use for something similar.
&gt; The process of having to refactor your errors that you described is actually what should be done in all languages. Any language where that process isn't required has poor error handling IMO. I don't feel like this is accurate. I can see that it may be true that due to Rust better(explicit) error handling it may get more involved to do it right but due to the fact that error handling is severely flawed in Rust by a mistake in the run to Rust v1.0 and the many attempts to correct this don't indicate that the problem lies truly on other languages doing poor error handling (and me coming with expectations learned in that languages having problems with rusts error handling) &gt;It sounds like you dealt with exceptions, in whch case feel free to unwrap everything as it is almost equivalent. Or you can do as above suggested and use failure::Error and the ? operator heavily. I cannot agree on that. In the case of exceptions you brought up, i can easily build my own without going back to a base exception type and loosing all of the information beside an error message. &gt;I haven't found it to be any more obtrusive than Go, and Go forces you to deal with errors on return in a similar way. Go is not a very good case in handling errors – i can agree on that. 
Is there a stream of rustconf?
1. The easiest way to reproduce things like this is to just put sleeps in your code, maybe between steps 2 and 3 in your case? 2. I do think this interleaving is possible. You might want to use one atomic as a "lock" flag, where you "lock" it with compare-and-swap. Then you do all the atomic operations you need to do before you "unlock" that flag. Since you only hold it very briefly, you can spin trying to acquire it.
I think the most important thing to point out about `failure` is not the crate but the documentation's ["Patterns and Guidance"](https://boats.gitlab.io/failure/guidance.html). Some quick and dirty personal recommendations - `failure::Error` is best for applications and not libraries - People should wait for the error RFC and `failure` hitting 1.0. As-is, so many people using `failure` has limited its evolution / learning because the authors recognize that error reporting is fundamental and breaking the API can cause rifts in the ecosystem for where it is used. For more guidance on `failure`, I highly recommend [this post](https://users.rust-lang.org/t/custom-error-guidelines/19547/11?u=epage)
This got me wondering if you could make a generic Immutable wrapper. Here's my attempt: https://play.rust-lang.org/?gist=5fa1be92544abd36ee7c34f4696e2ca1&amp;version=stable&amp;mode=debug&amp;edition=2015 It lets you do: pub fn example() { let foo = i_return_immutable("you can't modify me"); //foo.a += 3; // This results in an error; uncomment to see! //foo.0.a += 3; // This is also an error; the inner value is private. let c = foo.a / 3; println!("Result: {}, {}, {}", foo.a, c, foo.b); } See the Playground for the complete example. Basically its just a wrapper type that only provides ways to get references to its inner type; you can never get a mutable reference. As far as I can tell that should work; I can't think of any way to "cheat" (besides types that already have interior mutability; can't overwrite that). The only downside is that this is _specifically_ for immutability. i.e. you can't use it to make a struct where the fields are "readonly" to the user, but also has `&amp;mut self` methods (because once the type is wrapped in Immutable you can't get a mutable reference to give them).
`FnMut` has nothing to do with that. It's not `f` that has to be moved, but `x`.
Can you (the author) expand on the role of the Context object
I'm learning Rust since this week as well and mostly just use [https://doc.rust-lang.org/book/2018-edition](https://doc.rust-lang.org/book/2018-edition). Just read chapter per chapter and if something isn't clear at first (which not everything will be), just look back at the specific chapter. My background is Ruby, Elixir and Javascript (learning Elixir helped me a lot).
I like the suggestion about using `sleep()`, so I'll give that a try. Thanks!
Actually, no, I'm in the dark
Especially when posted by aturon ;)
isnt usize of the size of tge system pointer? so on 32bits 4bytes? 
Because of how clipboards work in X, pasting is only possible while the program that modified the clipboard is working. I downloaded the example and tried running it. As is, it did nothing. When I added a `std::thread::sleep_ms(5000)` to it and ran it, for the next 5 seconds middle-clicking would paste `Hello, world!` (then it would do nothing again).
You're right. Changed to atomic u64.
Felt similarly about food/foo.rs thing Once I actually started using it it was much more ergonomic and miles better than food/mod.rs imo
Thanks!
Hey there, thanks for writing this. I'm wondering if you've thought about this at all. Would providing access to various opengl resources that need to be bound/umbound through a lambda be more idiomatic? What I mean is, instead of: gl_bind_buffer(GLTarget::ArrayBuffer, vao); ... gl_bind_buffer(GLTarget::ArrayBuffer, 0); // I'm assuming this is how you would unbind it? you would have an api more-like gl_bind_buffer(GLTarget::ArrayBuffer, vao, []() { // do stuff with bound buffer here }); // the buffer is automatically unbound here
This is good stuff indeed! Thank you!
You could use a lambda like that, but you would actually lose out on performance quite a lot. The reason is, binding/unbinding OpenGL objects actually has a fair amount of overhead, and unbinding buffers (Which is rarely useful as far as I know) would mean double the OpenGL state changes each frame of your application, lowering the FPS quite a bit.
How can I run a system process and than read its output line by line with Rust? I know that I can run something with `std::process::Command` and that I can get all output at once, but for memory reasons I would like to do this line by line. let output = Command::new("zcat").arg("huge.txt.gz").output().unwrap(); let output = output.stdout; // how can I read line by line?
This was an interesting article. I really like the measured / sociological view of undefined behavior you use, I feel like I haven't seen that as well articulated before. I do wonder if undefined behavior is necessary at all. I vaguely remember the CraneLift code generator having "no undefined behavior" as one of its selling points, although [that no longer seems to be the case 🙁](https://github.com/CraneStation/cranelift/blob/master/docs/compare-llvm.rst#undefined-behavior)
I could see myself using a version that only does the unbinding on debug builds, but were getting outside the standard usecase here.. Cheers on your library man!
https://www.reddit.com/r/rust/comments/7wx68g/does_rust_have_a_lot_of_learning_resources_for/du3w82f/
First, you use `.spawn()` instead of `.output()`, which spawns the process as a child so it can run concurrently. Then in [the returned `Child` instance](https://doc.rust-lang.org/std/process/struct.Child.html) you can get the child's stdio handles and read from its stdout/stderr and write to its stdin. However, you need to configure the process to use piped stdio for the streams you want or else the handles will be `None` in the `Child` struct. You do this by setting the stdio streams you want to access into piped mode like this (apologies for formatting issues, I'm on mobile at RustConf): use std::io::BufRead; use std::process::Stdio; let child = Command::new("zcat").arg("huge.txt.gz").stdout(Stdio::piped()).spawn().unwrap(); let mut child_stdout = child.stdout.take().unwrap(); let mut line = String::new(); loop { child_stdout.read_line(&amp;mut line).unwrap(); if line.is_empty() { break; } println!("zcat wrote: {}", line); line.clear(); // `.read_line()` appends }
A couple of basic machine learning algorithms in Rust. These also serve as an practical introduction to numerical computing in Rust, which is quite verbose and tricky compared to say Numpy in Python. I'm quite new to Rust and open to suggestions (and PRs) on all fronts. 
&gt; Can someone explain the big picture, dos and donts of Rust language? There really aren't any. The language lets you do anything you want, provided you take the appropriate path to do it. In terms of learning it, I think you highlighted your issues here: &gt; I came from OOP languages, so I have pretty strong paradigm shock lately. and in the next sentence: &gt; There's too much learning, relearning, and unlearning here and there to the point it feels like I'm learning programming from scratch. Part of what you're experiencing is likely due to bad habits from having to work around language limitations. Rust isn't immune to this either (hello `unsafe { ... }` my old friend). But once you start to get a reliance on bad habits they change how you view solutions to things. Rust *will* feel far more restrictive than C++ (for example), but will feel better than Java (in some ways, though my feelings towards Java, while my own, aren't entirely positive for so many reasons). Rust is pretty basic at its core (in terms of use, it's actually complex interwoven tapestry of magic, but you don't need to stare into the *Untempered Schism* unless you really want). It doesn't have to rely on legacy C functionality to work, so it isn't held back by that. Inheritance, as it comes from OOP, is still available, but you have the option to inherit by capability (trait) rather than the inheritance you know--it's a bit different, but you can achieve the same things (and more!) by virtue of the seemingly tiny distinction. What's really cool about the change is that you can traits to other things (like `u32` and the other primitives), allowing you to selectively apply functionality (if you so choose). I'll go a bit further and say that the need to split template-only classes across multiple files irked me in C++, and Rust's cross-referencing of types is awesome (provided they're in the same (or module?) you can inherit cleanly). For all of the initially perceived 'bad' there's a lot of 'good' lurking around. Remember that there's usually a very good reason that a modern language has made its choices, and, as recently came up on the Rust user forum, [not even C is actually a low level language](https://queue.acm.org/detail.cfm?id=3212479), so it makes perfect sense to provide changes to the memory models and interactions you are used to using!
It's certainly an interesting read. While I'm not sure I agree with all of it, I think the biggest issue is that we're not going to see undefined behaviour disappear while people insist on attempting to optimise unsafely in languages that don't even accurately represent the platform they're on. And I'm not (deliberately) taking a swipe at anyone, this is probably more of an industry problem as much as a social one; until such times as people come together to put safety first, it's always going to be an issue. Also, and only somewhat on point, when I think of a visual representation of "Undefined Behaviour" I actually think of the [Weirdmageddon introduction to Gravity Falls](https://www.youtube.com/watch?v=uWvdqkl2UH4).