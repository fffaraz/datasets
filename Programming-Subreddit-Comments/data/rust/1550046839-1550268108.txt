You still need those checks unless you are sure about the argument types or don't care what happens with the wrong type. This is not a theoretical problem: I once had a python ETL application delete the whole production database on me because of a wrongly typed argument. We could restore from backup, but that wasn't a good day for me. So, do you prefer: fn foo(a: u32, b: &amp;stronger) -&gt; u32 { .. } or def foo(a, b): check_type(a, int) check_type(b, str) .. From a verbosity standpoint, the former wins.
Original author is: /u/z-wei 
The author writes that rust code is often too noisy (and that using unsafe is icky): let font = unsafe { core::ptr::read(FONT.as_ptr() as *const PSFFont) }; The author then highlights how this is much simpler in Zig: const font = @ptrCast(*const PSFFont, &amp;fontEmbed); In general, this might be true, but this is not a good example for two reasons: 1. There is nothing icky about using unsafe. It is a perfectly valid language construct. Some people might feel religious about it, but its core purpose is to keep track of bits of code that require extra attention from the programmer. I'm not familiar with Zig, but if it does not have this feature, that just means that by definition it can't keep track of potentially unsafe areas of the code. There is nothing stopping the programmer from wrapping unsafe code in a safe function. 2. If the author encounters the above incantation too frequently, nothing stops them from writing a simple 1-2 line macro called ptr_cast! that will be just as simple to use as the Zig example. That's what macros are for, when a particular construct gets too noisy for it's own good, a macro can be used to tidy it up. Hell, this particular case could even be handled by a simple generic function.
For C. Do you have any counter-example?
I'd like to get the current memory usage of a rust program, so I can log it periodically. I could poke into /proc/&lt;[PID](https://doc.rust-lang.org/std/process/fn.id.html)\&gt;/status on Linux (portability is not an issue), I just wondered if there is something more convenient. I could not find a crate for it, neither an API to get head size from the rust allocator (which would not measure exactly the same thing, but either would be fine as I expect the heap to dominate).
I just added the `linear/binary/exponential_group` trait methods which return a `Linear/Binary/Exponential/GroupBy` struct using the default `PartialEq` of `T`. You can see it here: https://github.com/Kerollmops/slice-group-by/pull/3/files
The absence of such a macro must mean it's true.
For the lazy_static case: why not rely on the fact that the macro is matching `expr` for the second argument, and we can figure this out from the metadata? (Proc macros could potentially be set up to set an additional structure delineating what parts of the invocation are rust code. Trickier, but I have some ideas on how this might work)
I think the main reason for such difference is `lock.write(&amp;['\n' as u8]);`. Usually stdout is line buffered, meaning your program will flush output for each iteration, which is obviously quite inefficient, try to write in your tests program ~20-40 bytes of zeros and `\n`. One way to solve this problem will be to wrap `Stdout` into `BufWriter`.
This indeed will work for macro-by-example, because we know *both* the input and the output format. Knowing only the input is not enough: `caesar!` could have accepted an `expr`. Another similar example is a macro by example which accepts `expr` and discards it. Arguably, "correct" completion in this case should suggest syntax (keywords, if expressions, etc) and arbitrary identifiers. This also means that IDE has to special-case macro by example macros, it can't say that "any macro is TokenTree -&gt; TokenTree" function. 
Sys means time running system calls. Remember that libraries also calls system calls, so even if you don't think you're calling into the kernel, the libraries does. A for the time in writing, I/O is usually very expensive in terms of time, particularly if you have spinning disk drives (I.e. non ssd).
Then they should use that and you could have influenced them somehow which would not have made this article possible. Because everytime I encounter this discussion, everyone seems to say that existing tools solve this problem yet every major company seems to release similar articles now and then. They both seem contradictory. 
The whole project appears to be less than 1000 lines of code. My suspicion is that for small projects, you might not see some of the benefits of Rust, since you can still hold the whole program in your head fairly easily. Where Rust comes into it's own is when the codebase becomes complex enough that there is a need to raise the level of abstraction. Between enums, traits and ownership, Rust allows you to specify the invariants of an interface more precisely than any other language. Having said that, I enjoyed browsing the codebase and taking a look at zig, which is supposed to be a nice C replacement and very suited to this kind of project.
It‚Äôs because things at every major company are a shit show. Everyone runs around with their heads on fire to please management. It‚Äôs always the fastest thing that gets done, not the correct thing.
I don't know what kinda position /r/MrToolBelt had within MS but I don't necessarily think it's reasonable to assume he was in any kind of position where he could cause change like that. 
&gt; A strictly decorator based form of attribute macros, which can‚Äôt change the source item, but can produce other items. That's what derives are for FWIW (can't be placed on every type of item, but "strictly decorator based" isn't as meaningful for something like a function)
&gt; There is nothing icky about using unsafe. It is a perfectly valid language construct. Some people might feel religious about it, but its core purpose is to keep track of bits of code that require extra attention from the programmer. I'm not familiar with Zig, but if it does not have this feature, that just means that by definition it can't keep track of potentially unsafe areas of the code. There is nothing stopping the programmer from wrapping unsafe code in a safe function. I sometimes, only half-jokingly, wonder if Rust should have used `} ... safe {` instead of `unsafe { ... }`to attempt to draw attention to how other systems languages implicitly put *everything* inside an `unsafe` block.
&gt; This also means that IDE has to special-case macro by example macros, That seems like a good thing for them to do though? These macros provide more static information than proc macros, why not take advantage of that?
Even if it was the case that good programmers wouldn't make these errors, why would you make the life of those better good harder? If the best programmer in the world is using 70% of their mental energy to prevent these errors and 30% to build the tool itself, imagine what amazing code they could write if most or all of that 70% burden was lifted.
Agreed. It's *supposed* to feel icky because manipulating pointers like that is prone to human error, no matter what language you use. All he did was choose a language which made him feel better about doing the same mistake-prone thing.
&gt; The real reason people have an affinity for CLI tools is that they can be automated very easily. In my opinion they are also better for *teaching*: that is, it is easy to precisely describe how to do something with a line of shell script which might otherwise take many words or pictures of GUIs.
IIRC, derives are allowed t change the item they are decorating
Yes, IDE should special case mbe.
Right that's what I'm saying. It sounds like that's what you wanted
It's highly unfortunate to hear that even big companies don't follow proper programming practices. I used to hear lot of good stuff about coding standards in Google. Not really sure now. 
Ok, if you're learning programming for the first time, Python is probably the best choice. It's the only major programming language that is designed based on PL education research (so it's designed specifically to be easy to learn). Another great choice for this is Lua - it's often seen as a bit of an "outsider language" like Lisp but once you know it you'll start seeing it everywhere. It doesn't have anywhere near as good teaching materials as Python does, though. If you want to mess around with bytes and learn how the computer really works, use C but _don't_ compile with optimisations. You can find some great beginner tutorials for C online but compiling with optimisations makes C a lot more mysterious since undefined behaviour is extremely hard to understand and even the best programmers in the world can't avoid it completely. If you want to write fast, correct code and be forced to build a consistent mental model of the execution of your program, Rust is a good choice. For an absolute beginner it's probably not necessary, but Rust is a pretty nice language to learn and has fantastic teaching materials, and if you start with Rust then you will almost certainly write much better code in other languages if and when you start using them. I certainly write better code in other languages since learning Rust.
Looks promising! It would be cool to support `#\[bench\]` too 
Dynamic languages are great for scripting and hackathons because of their flexibility. Dynamic languages are awful for important production software because of their flexibility. 
Both Oracle and Microsoft are known shit shows. 
All the bugs that are blamed on me are in actually made by an imaginary programmer.
&gt; Maybe there's a release vs debug switch I don't know about for wasm-pack builds. I see `--debug`, so the default seems to be release.
Yes, NOAA-APT. doing a WXtoIMG alternative. [link](https://github.com/martinber/noaa-apt)
With Python I always found, when I was a relatively new programmer, that the higher level constructs would actually confuse me more than help me. Like just looping for instance, when you see a three arg 'range' it's not exactly clear what's going on without looking at the documentation. Whereas if you already know what assignment, comparison and increments look like in C, you have a pretty good idea of understanding what the for loop is going to do. You probably also want to have your text editor configured to show you whitespace. I'm tempted to say that Go is probably a good first language. There's very little boilerplate, the language is small on features and high on useful standard library functions if they want to start experimenting with web servers and the like before even having to worry about package management. Mind you, on the opposite side of the coin, Go is so easy to pick up if you know another language you have to wonder if the same applies the other way around.
That's a fair suggestion. I'll work on implementing them today. Along with the new `Lines` rasterizer, that should be enough of a change for a 0.2 release.
Hey, I just want to thank you for writing this. I was thinking about implementing some similar solution on my own (definitively more web-oriented, possibly with more JS thrown into the mix). Your post will be of great help.
safe is the default, so `unsafe` to opt out makes sense. Also it's a-OK to feel icky when writing unsafe code to misuse a pointer cast as a parser.
Notice that saying that integers must be initialized/frozen does not mean that Rust will not have to deal with the problem of uninitialized data. So when you are complaining about the *presence* of something like trap representations, we are not just talking about the invariants for integer values here. We are also talking about what not-yet-written to memory even looks like. I agree that trap representations and indeterminate values in C are awful, but I think that is mostly because they are very poorly specified (if at all). Rust will not have "magic values that change when you look at them multiple times" (aka indeterminate values), and it has a very clear model for what uninitialized memory looks like: every byte (or bit, or value, the granularity is still open) can be either initialized to some fixed value, or not initialized ("poison"). I think that this is a story we can teach reasonably well, and I believe we can do better than 4-5 decades of C because C never had a model that is as clear as this. The C standard does not even explicitly acknowledge that uninitialized memory is different from frozen memory! Patterns like [this](https://research.swtch.com/sparse) are not actually possible in C, because this assumes that reading the same uninitialized value twice will give the same result, which is in contradiction to what compilers do. If C were to acknowledge this mismatch and explicitly incorporate something like "posion", I think the entire store would become much simpler. Of course, a model without "poison" (something where freshly allocated memory is frozen) would be even simpler. But given the space Rust is targeting, I think that is not an option. (Also, having "freeze" in the source language has its own disadvantages, see [some of the links in this post](https://www.reddit.com/r/rust/comments/apreqi/ucgmiri_allhands_2019_recap/egarzl8/).
You are mostly right, just a little correction: &amp;[T] can refer to data stored on the stack (e.g. a [T; 100]) but can also point to some elements stored on the heap. For example if you have a Vec&lt;T&gt; you can get a pointer to its contents as a &amp;[T] (for example using `&amp;myvec[0..]`).
&gt; it seems approximately coherent/defensible for Rust to define reading freed bytes as undefined behaviour/memory unsafety (I will assume you mean uninitialized bytes, because nobody is suggesting it should be allowed to read deallocated bytes.) No, I don't think so. That would make it impossible to e.g. implement `memcpy` in Rust, or to read the memory representation of any struct containing padding bytes. We have to allow unsafe code to read uninitialized data, that's just something low-level code sometimes has to do.
I agreed I felt the author was too quick on judgement for rust
&gt;It really doesn't make sense to me for someone to feel icky about using unsafe, then migrate to a language where the whole thing operates under that ruleset. You put it much more eloquently than I did. Essentially the author's complaint seems to be that they found it cumbersome that Rust forced him to deal with potentially unsafe code explicitly. However, phrasing it that way also doesn't feel kosher so I'm not surprised they didn't elaborate in more detail. It's a bit of a shame because there is a grain of truth hidden in there: it is mentally taxing to have to make choices about safety all the time. It forces one to take a break from the flow and consider the implications. This can get annoying fairly quickly. When doing data analysis, I don't like to think about types, architecture and safety. They are orthogonal to my primary objective, which is to find some value in the data. The code I write is most likely throwaway. I want to read that damn csv file and just let the tools figure out what type should each column have. 99% of the time it works fine, and sometimes I have to make things explicit. Phrased this way, I could see the author's reasoning as valid. If he wants to explore some ideas and experiment before committing to some construct, the constant mental burden of considering the safety upfront is a drag. But then there is a perfectly fine solution for that as well: just wrap frequently repeated unsafe ops in a macro or a function.
&gt; It really doesn't make sense to me for someone to feel icky about using unsafe "Out of sight, out of mind".
On your reduced test case: $ time (for i in {1..10}; do ./target/release/prof &gt;/dev/null; done;) ( for i in {1..10}; do; ./target/release/prof &gt; /dev/null; done; ) 1.35s user 0.03s system 99% cpu 1.388 total Which sounds about the same as what you got. I don't have `pprof`, but I tried `perf`: 32.72% prof prof [.] &lt;std::io::stdio::StdoutLock&lt;'a&gt; as std::io::Write&gt;::write 18.37% prof prof [.] &lt;alloc::vec::Vec&lt;T&gt;&gt;::extend_from_slice 18.01% prof prof [.] &lt;std::io::buffered::BufWriter&lt;W&gt; as std::io::Write&gt;::write 15.36% prof libc-2.28.so [.] __memrchr_avx2 8.49% prof prof [.] prof::main 6.70% prof libc-2.28.so [.] __memmove_avx_unaligned_erms 0.19% prof libpthread-2.28.so [.] __libc_write 0.09% prof ld-2.28.so [.] do_lookup_x 0.06% prof ld-2.28.so [.] _dl_map_object_from_fd 0.02% prof [unknown] [k] 0xffffffffb7e00a87 https://stackoverflow.com/questions/41444097/whats-nss-passwd-lookup-call-that-i-see-in-profiler-output-about seems to indicate it might be an issue with the debug info. Can you install some debug info packages, and do you get the same results with `perf`? What about `perf record --call-graph dwarf`? As for your original code, as someone else mentioned, newlines flush the standard output. If you have a lot of short lines, you could try to avoid that. Finally, the easiest thing to do might be to run it under `strace`.
This looks cool! What sets it apart from [postgres-binary-copy](https://github.com/sfackler/rust-postgres-binary-copy)?
 \#!\[warn(bare\_trait\_objects)\] 
Safe is the default in Rust, but unsafe is the default in the greater world of systems-level programming languages.
It's in general infeasible to write such a macro, because of the recursion limit.
If I understand correctly, I can't give the source iterator to the map function, that way I'll iterate over every element only once. I'm storing values inside a buffer because I need to use the same value more than once. Maybe I'm wrong. What you suggest is: (1..).buf_map(3, |iter| { for x in iter { println!("{}", x); } println!(","); }); As a result I want `1 2 3, 2 3 4, 3 4 5,...`. I understand you suggest that the `iter` available inside the closure should be something like `(1..).take(3)`. But that way the result will be `1 2 3, 4 5 6, 6 7 8,...`
You're looking for /r/playrust
I wish it had more scala-tags like syntax
Exploring here: [https://twitter.com/ryan\_levick/status/1095344395744960512](https://twitter.com/ryan_levick/status/1095344395744960512) :)
And there are only two reasons to cast pointers like this: - C FFI, which is never memory-safe when pointers are involved - To transmute the type of values (e.g. for binary serialisation), which is also not provably memory safe (in part because pointers do not encode the size of the pointed region)
&lt;3
&gt; All he did was choose a language which made him feel better about doing the same mistake-prone thing. Not really? He posted four other reasons he chose Zig (binary size, compile time, custom allocators, minimal language), and then gave *one* example where he felt Rust was cumbersome to use *and* he prefaced it by saying it was a very subjective point. I'm not sure why everyone here is so focused on it.
I think you just need to change your bound `T: MsgType` to `T: MsgType + Clone`.
There was a lot of this back in C vs Pascal days. Pascal bashing using ISO Pascal as comparison, gently forgetting about ISO Extended Pascal, Turbo Pascal, USCD Pascal, Object Pascal and many other dialects, while some C examples weren't really ISO C anyway.
Oh, that's right! Thanks a lot
Weren't they working on bringing Rust functionality to C++?
No, I mean freed bytes: that's what the information leak is from, deallocate and then reallocate. I guess it's pretty close to uninitialised, though.
Yeah. Seems so. I'm getting a 50x to 100x slowdown on the move from native to wasm with 99% shared code. I'll have to do some profiling to figure out what's going on.
I was referring to that specific point of justification for using Zig. The others are perfectly valid and I have no argument with them.
Mind clarifying? I wasn't programming in the C vs Pascal days and my only experience with Pascal (Free Pascal) doesn't help me to guess at your intent.
I didn't look at the code, but one possibility is that Rust promotes static polymorphism, which may lead to lots of copy of the same code when a generic type / function is instantiated with different types. If the other language uses dynamic dispatch by default, that could save lots of code size.
stdout should only be line-buffered if connected to a terminal no?
‚ÄúI‚Äôm a complex programmer. All my features are real, all my bugs are imaginary.‚Äù ‚Äî binkarus, probably
Thanks nox :)
Go - the language is simple so you can focus on learning programming, you still learn a bit about pointers unlike Python. It has strings and garbage collection built it so a lot less hassle than C. I did a video series teaching programming with Go via fun game projects - https://gameswithgo.org/ its free
because it is crazybananabeans land. Imagine a newbie working on it finally figuring how * and &amp; and what they mean and then they see Stroustroup on youtube saying that they are deprecated now. waaat 
His main reason was experiencing 'friction' while coding though. Custom allocators and binary size are basically icing on the cake. Minimal language is a boon when learning a programming language, though, and he mentioned that the compile times in rust broke his flow, which is arguably even more important for achieving that developer satisfaction.
I‚Äôm not aware of any, I was just wondering what exactly you were claiming, since Rust does do this.
He could have created a simple macro: ```rust macro_rules! read { ( $x:expr as $t:ty ) =&gt; { unsafe { core::ptr::read($x.as_ptr() as *const $t } } } ``` "problem" solved. 
&gt; It really doesn't make sense to me for someone to feel icky about using unsafe Look at it this way: After using a language for a while, you kind of develop an intuition for what's permitted and what isn't. Now in an `unsafe` block, the rules are quite different, so unless you use it all the time, you have to check all the new rules manually and might not recognise problems by just reading the code. Compare that to a language where you are trained in `unsafe` rules from day 1. It may not be safer, but it will *feel* safer because it's familiar. There are also some community members who react to `unsafe` blocks the same way some C community members react to `malloc` (i.e. *what the hell did you just bring upon this cursed land*). They were quite vocal, particularly in the actix-web "scandal", and probably managed to convince a bunch of people there.
Why have this when you can do `unsafe` pointers and `impl Sync` trivially, or simply wrap in a `Mutex`? The combination of those three should cover all use-cases absolutely, especially ignoring why `Sync` is even a thing.
This is the most well known example, from Brian Kernighan itself. https://www.lysator.liu.se/c/bwk-on-pascal.html It is stuck in time, focus only on ISO Pascal, which was indeed quite limited, while ignoring the improvements provided by Pascal dialects that render those criticisms invalid.
What is the consensus on using macros to hide `unsafe`? I guess the macro could be named accordingly, `unsafe_read` or something.
The very point of this crate is to contain the unsafety to what happens while the wrappers are alive, for when your data isn‚Äôt Sync and cannot be Sync. For example the DOM in Servo is‚Äôt going to use mutexes everywhere when script execution is always single-threaded and we want Sync values only when doing layout.
Do you happen to know what kind of crawlers? For search engines?
Sure your naming is better! My point was that the Rust's powerful macro system was done to simplify this kind of boilerplatish notations.
Are you running on linux? Do you have access to dtrace? Perhaps you could trace the chunks with which you are trying to do your writes. There is a lot the kernel does to enhance your write performance like buffering and working on layout policies. Often with large or frequent writes buffer management and copying/zeroing pages from user space to kernel space becomes expensive and your I/O bottlenecks. Also you didnt provide any hardware, os or filesystem information which is important for these benchmarks
&gt; script execution is always **single-threaded** and we want Sync values only when doing layout. Then don't pretend it's `Sync`?? Why would you need it to be `Sync`? It sounds like the [xy-problem](http://xyproblem.info/).
Some people say "Rust is cool, but you can also use a static analyser in C". My answer: "I don't think that any C static analyser can guarantee the thread safety".
I doubt there are humans able to remember the 200+ UB cases described in ISO C, and the compiler specific behaviors across all major compilers, including versions of the same one.
Do you mean the style guide? Or are you having trouble with an editor plugin for clang-format?
I agree with your suggestion, but I wanted to hear if this is considered idiomatic rust? As in, does it have any implications apart from putting the burden of naming the macro appropriately, on the programmer.
Start with a base style then look up the documentation for the differ clang-format options. At least in my experience, clang-format's long line wrapping is a bit flexible. Rust has an idiomatic style, but C++ tends to vary a bit, so you have to pick a base style guide that makes sense to you. I personally prefer the LLVM style guide.
&gt; There are also some community members who react to `unsafe` blocks the same way some C community members react to `malloc` (i.e. what the hell did you just bring upon this cursed land). They were quite vocal, particularly in the actix-web "scandal", and probably managed to convince a bunch of people there. That's a complete mischaracterization. We weren't reacting to the "use of unsafe", but rather, "the *incorrect* use of unsafe."
You can hide an unsafe implementation if you are sure that your wrapper is safe (that happen a lot in std, for example). I think that is a bad idea if you hide an unsafe implementation without this guarantee; but in this particular case, this is a small codebase with one developer, so the implementation is not *really* hidden.
What if you disable the display update?
&gt; in more technical terms the very stack is called stack frame Isn't it `call stack`? `stack frame` refers to a single _frame_ of the stack, not the entire stack itself. &gt; Every function that we encounter is then added to the stack with variable, and values defined on the function Unless you're compiling your Rust program against JVM, as many variables as possible are stored inside registers, not stack. &gt; as the stacks on the computer is limited so we tend to store larger sets of data (dynamic data types) over a heap memory and add pointer reference (reference to the heap location) at the stack. This sentence is implying that computer's heap memory is unlimited, which is not true. &gt; This is the reason Rust does not need a garbage collector Stack is not the reason Rust does not need a GC. &gt; The same case can happen here as well, like, what if we need two variables pointing to the same data structure. For this, Rust has a concept called move. Move semantics is the precise reason you _cannot_ have two variables pointing at the same thing. &gt; Rust frees up the memory when the stack frame is popped after the complete execution of the respective function. No, it does not (again: because of the move semantics). If Rust did so, you would not be able to create a new structure inside a function and return it, because it would be immediately freed.
Yeah. I can disable display updates and I still get most of the slowdown so I‚Äôm guessing the problem lies within the emulation itself, not copying the graphics to the html canvas. Maybe some optimization that works great on native just doesn‚Äôt translate to wasm. I need to figure out where the wasm version is spending all its time.
Locking stdout inside a loop isn't cheap though.
For great justice!
I have no idea what you are talking about. This is absolutely not the XY problem (nice name-dropping though). Script and DOM operations all are single-threaded, but layout needs to access those data in a Sync way at specific moments in time, and we know when that happens and we know script doesn‚Äôt run meanwhile. Using Mutex or any kind of lock would be catastrophic for performance, and using unsafe code all over the place makes it impractical to actually reason about whole-program memory safety. Nothing pretends to be Sync here, things are actually Sync when you respect the invariant that you shouldn‚Äôt use the wrapped values while the wrapper exists.
FYI, there's a thread on internals right with some of the discussion touching on that: https://internals.rust-lang.org/t/explicitly-marking-unsafe-macro-expressions/9425
You should read Fuchsia source code properly. The original C code is being rewritten in modern C++, plenty of Gerrit comments about it. The TCP/IP stack and low level file system utilities are in Go. A new UI framework is being written in Rust. The UI is currently in Dart, but I guess they might change it due to Scenic. 
Nice, perfect timing! Thanks for sharing that.
The question makes lots of sense, as proven by Modula-3, D, Sing#, System C#, Oberon and many others. A GC is a convinience, not an obligation to use it for 100% of all memory allocations. GC less in C# is achieved via structs, stackalloc, spans, native interop.
What if I have actual non Sync fields? For example, if one of the fields is an Rc, wouldn't I be able to clone it across threads, creating a segfault time bomb? Or will the derive macro somehow deal with this? Really it seems like this is based on the idea that it's safe to use non Sync values if they won't change, but doesn't Sync mean whether it's safe for multiple threads to _read_ a value?
Rust still needs its Unity/Unreal though. Using plain old SDL feels so 90's.
And many of those ideas landed on WP 8 AOT compiler, .NET Native and the C# 7.x memory related features.
Especially in something inherently intended to be exposed quite directly to the web at large, where any passing ne'er-do-well can poke at it.
If you math code is short, this is expected. Computers are really good at math. You can use `rayon` to speed up, or just don't print so many things to tty (I assume you do).
look at the code, the actual locking happens outside the loop
Both. It is also stated on their presentation, C#, Rust and safe C++'s subset.
&gt; Script and DOM operations all are single-threaded, but layout needs to access those data in a Sync way at specific moments in time, and we know when that happens and we know script doesn‚Äôt run meanwhile. So, what you're telling me is that `mut` is single-threaded, while everything else is multi-threaded? Sounds like a `Mutex` to me. &gt;Using Mutex or any kind of lock would be catastrophic for performance, No, just your naive implementation would be. What's stopping you from wrapping the *entire reference tree* in a single `Mutex`? &gt; and using unsafe code all over the place makes it impractical to actually reason about whole-program memory safety. It's actually unsafe though. You are literally implementing something that is no better than *C* code for safety, but for some reason you want it to be in *Rust* and pretend it's not `unsafe`. Why not mark the single-threaded access functions as `unsafe`, because they *literally are unsafe*? The problem with `unsafe` isn't the word itself, or the syntax, it's the underlying implication that the code loses the protections the compiler provides. &gt;Nothing pretends to be Sync here, things are actually Sync when you respect the invariant that you shouldn‚Äôt use the wrapped values while the wrapper exists. Needing to respect an invariant makes it `unsafe`, and that should *always* be explicit. There's reading you should do about Rust and `unsafe`. Here's a small pertinent [quote](https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html#when-to-use-unsafe-code): &gt; Using `unsafe` to take one of the four actions (superpowers) just discussed isn‚Äôt wrong or even frowned upon. It's wrong to hide `unsafe`, not wrong to use it.
I'm pretty stoked about what feels like all this language innovation that is happening these days. Go, Rust, nim, and zig. They all serve slightly different (but overlapping) niches, too.
I dunno. In my experience with Rust on Cortex-M, the size of the binary is already very small. Static vs dynamic dispatch on my codebase explains about a 10% difference in size. A 4x size reduction is extremely fishy and my gut reaction is that there is significant functionality that is missing (whether the author knows it or not). Back when I did C, this was a common symptom of programmers forgetting to mark certain things volatile (which lead to ugly race conditions later on). I'd be interested in knowing why the binary is so much smaller.
I haven't used `wasm-pack` -- I find it absurd that the docs don't even say what it is. But I didn't notice such a large slowdown (albeit on less complex code, I suspect) with `wasm-bindgen` and the `wasm32-unknown-unknown` target.
What you say !!
I use VecDeque because it's already a ring buffer
This C++ optimization seems like it leaves performance on the table. If your program has threads but the references are all used inside one of the threads the optimization is not used. The Rc/Arc abstraction seems much better. Instead of trying to do optimizations in the background allow the programmer to specify what he wants and have the compiler check that he isn't making mistakes.
&gt; Stabilize TryFrom and TryInto with a convert::Infallible empty enum üéâüéâüéâüéâ
I still have no idea what you are talking about, you probably missed that `Inert::from_ref` is unsafe, which is why I jokingly mentioned the user must swear on the holy baguette that they must not use the wrapped value when their wrapper is alive.
I think I've attempted something similar in Rust. One of the problems I had was that I had scoped \`unsafe\` too large (i.e. on a whole statement), causing it to be silent on another unsafe thing I did which was a bug. I don't fully understand what inert does, but would something that hides a particular class of unsafe behavior on one particular structure not be useful? Provided that the whole API is explicit enough that an auditor would at least notice something fishy is going on? The solution to my problem was to split the statement out into its components, and then apply \`unsafe\` only to the unsafe thing I wanted to do, and then the compiler highlighted my bug.
&gt; This is absolutely not the XY problem (nice name-dropping though). Different note, it's *always* the xy-problem. No one is above it. In this particular case: X: I want `Sync` without everything being marked `unsafe` Y: `Mutex` Y: Anti-pattern; don't do that Y: You don't actually need `Sync` Y: `unsafe impl Sync for ... {}` Y: Something else that if more of the problem was known, it might be obvious
That was the general topic of the event, yes, and the majority represented your position, but there were some who wanted to replace *all* unsafes with safe code, disregarding correctness. There were also discussions on whether the number of unsafe cases in a codebase is inversely related to code quality, with some people proposing to implement that in Cargo. I was referring to the people who hold those opinions, not the actix-web incident in general.
... I really wish someone would write a spec for Rust.
Maybe LTO with Rust was not deleting *every* last bit of libcore?
&gt;I still have no idea what you are talking about, you probably missed that `Inert::from_ref` is unsafe, which is why I jokingly mentioned the user must swear on the holy baguette that they must not use the wrapped value when their wrapper is alive. If it requires the caller to add `unsafe`, I return to my original question of *why are you adding this*? The caller can easily add the trait implementation override themselves. At least make a more formal documentation; instead of swearing on a baguette, explain that this adds XYZ `unsafe` functionalities, marked as such.
Gee, this sounds like a `libstdc++` misfeature or even a bug. You're totally right that you could bypass `pthread_create` with your own system call. &gt; I can see tons of problems caused by using dlopen in statically compiled binaries. While uncommon it seems like a totally legitimate use. Lesson learned: use `libc++`, not `libstdc++`. Would that it were the default for `clang`.
So instead of compile-time guarantees, this relies on a runtime hack that is prone to breaking in mysterious ways if any of the threads sharing the pointer were spawned bypassing the pthreads API*. It's also a single global, irreversible condition: once anything in the program spawns a thread (including any libraries using threads under the hood), all your `shared_ptr`'s become slow and will stay that way. In Rust, an alternative to frequent atomic reference counting is to use `Rc` and create some other ways to pass data between threads, for example with serialization and synchronization channels. * There may be a general assumption in the glibc world that all threads using glibc need to be managed through the pthreads "runtime", so this is not really breaking much.
There is rust tooling for take arm CPU descriptors and generating type safe code for accessing processor features... I don't think zig has that.
That‚Äôs exactly what I do when I explain that this crate allows you to access non-Sync data in a Sync way, as long as the user doesn‚Äôt continue to use the wrapped non-Sync values while the wrapper exists.
This macro comes from the this [hex-literal](https://crates.io/crates/hex-literal) crate and is imported inside this crates lib.rs
I've looked purescript over quickly and thought it looked cool. And I've also talked to several people who really like elm, but I haven't been able to use it directly myself. I've only heard of reasonml, but haven't had the time to check it out at all. &amp;#x200B; Ideally, I would be using something else besides javascript, but I've got a couple of constraints that prevent me from making that choice.
I see. From my perspective, the actual information leak is the freezing -- if we had no freeze operation, there was no leak. And certainly the bytes may not be read *until* they got reallocated. Reading freed bytes *is* UB. But if they get reallocated and then someone freezes them, that's how information can leak anyway.
I appreciate the link and I'll enjoy reading it, but I still don't fully understand the connection to this situation. &gt; There was a lot of this back in C vs Pascal days. &gt; &gt; Pascal bashing using ISO Pascal as comparison, gently forgetting about ISO Extended Pascal, Turbo Pascal, USCD Pascal, Object Pascal and many other dialects, while some C examples weren't really ISO C anyway. What "this" is occurring here and now? Bashing Rust? Bashing Zim? Something else?
Thanks! I knew about the \`hex\` crate, but this one is new to me! :D
&gt;That‚Äôs exactly what I do when I explain that this crate allows you to access non-Sync data in a Sync way, as long as the user doesn‚Äôt continue to use the wrapped non-Sync values while the wrapper exists. What are you accomplishing over `unsafe impl Sync`?
I've been keeping an eye on [RFC: stabilize `std::task` and `std::future::Future`](https://github.com/rust-lang/rfcs/pull/2592). The FCP has finished (with disposition to merge) but no further action has been taken yet. So I suppose it should be on the newsletter somewhere, not sure where though. 
Oh, this is neat. If I understand correctly, it's similar to the pattern of having a `Rc&lt;T&gt;` and taking out the `&amp;T` so you can use it in multiple scoped threads. Except for situations when that doesn't work, because the `&amp;T` itself can't be shared (because it gives asccess to other `Rc&lt;U&gt;`s and such). For those cases this crate would provide a limited set of unsafe operations with wrapper types instead of harder-to-understand bare pointer mechanics. Though I think what I most like about it is that it makes it easy to communicate the intent of the involved unsafe operations.
`Node` stays not-`Sync`, while `LayoutNode` is `Sync`. This is my last reply in this particular thread because it's starting to get me annoyed.
&gt; Add const generics to AST üòçüòç
Yes exactly! If not for the implementation of `Neutralize` for `RefCell&lt;T&gt;`, `Inert::from_ref` could even be safe, because the issue you mention that the `T` may expose other `Rc&lt;U&gt;` values doesn't happen with my stuff, rather `Inert&lt;Rc&lt;T&gt;&gt;` doesn't expose the `Rc&lt;T&gt;` as you said but it also doesn't expose the `T` itself, rather it exposes `&lt;T as Neutralize&gt;::Output`, which itself will neutralise any `Rc&lt;U&gt;` field of `T` and expose `&lt;U as Neutralize&gt;::Output` etc. I plan to introduce a couple of marker traits to introduce safe ways to create the initial `Inert&lt;T&gt;` from values such as `&amp;TypeThatDoesNotUseRefCellAtAll`, or `&amp;mut RefCell&lt;TypeThatDoesNotUseRefCellBehindRefCountedPointers&gt;`.
You are childish. You must not believe in your own code and your intention if you have troubles answering such questions. 
Instead of the critical voice, I want to encourage /u/z-wei to continue on his journey here. Good craftsmen don't let their tools dictate their capabilities, he weighed the options and chose zig. For a personal project that should be the only valid comparison. And moreover, I believe that we should instead learn from this report, not dismiss it so easily. Having worked on C++ kernel code the last months, several points seem familiar and maybe we can learn from their solutions in Genode/L4 or build our own. &gt; That being said, however, it‚Äôs not as if I intend to only ever write code for the RPI3, so if I want to confidently say to people ‚Äú‚ÄòX‚Äô is a good language for embedded development‚Äù, I want to be able to say that knowing that no matter the storage size (within reason) they‚Äôre working with, that language will be able to conform to it. This seems like a feature missing from Rust - complete stripping/compression/etc toolchain coming with cargo where you can specify the maximum size. I imagine this may also be necessary for supporting things as the Ardiuno? [All I can fine](http://jakegoulding.com/blog/2016/01/02/rust-on-an-arduino-uno/) is some manual hacking with rustc and no cargo. &gt; I know this is a hot topic for rust in general, but I can‚Äôt really not talk about it. Even as a daily and convinced user of Rust, I feel the same. It was postulated/promised that structured languages would be faster to compile than unstructured. I can't feel it yet, although the perf team is doing a very nice job. Let's just file this under the maturity goal of 2019 and prioritize it, right? &gt; Allocators and the Standard Library -- Zig features this idea that allocators should be provided as arguments to a function. This is almost definitely not what you‚Äôre used to, Now the most open question of the post, and the comparison with C++. There, several classes have that template argument `Allocator` in the standard library. And it sucks, so much that I wouldn't want to see it in any of my code. For years, standard libraries were buggy (and they still are, MSVC least of all surprisingly), and the specification for this allocator is too narrow for some uses and too wide for efficiency in other.. What do you do instead? Well 'simple', you build your own data structures in those case where it matters. Now, I think that could be a fantastic opportunity for Rust to have a `#[no_std]` crate that offers a more narrow selection of datastructures that come with configurable allocator. But hopefully with a nice trait for this allocator and it probably needs specialization for some optimizations. And I must say that in the general case, the zig standard library allocator signature &gt; `fn (self: *Allocator, byte_count: usize, alignment: u29) -&gt; Error![]u8` (which is kind of like `fn&lt;'a&gt;(&amp;Allocator, usize, u32) -&gt; Result&lt;&amp;'a mut [u8]&gt;`) seems too narrow. For example an allocator that tags its outgoing pointer not by size nor alignment can not do this, the same exact caveat applies to C++ and is the reason why the standard library template arguments don't actually allow you to do really powerful things without scope creap of their contract. And for that reason, I actually like the fact that the standard library types in Rust do not allow you to customize their allocators, or at least not more than by the global attribute. A crate would be nice though.
This. System level calls are always expensive, in every language. 
An unsafe block that can be replaced with a safe one should be replaced. It might be safe today, but it might stop being safe tomorrow. That's why some people are trying to add to e.g. `std` functionality that's generally useful, but requires unsafe code. Consider this question: how much unsafe code does a web framework require, and why?
While this RFC was in "final comment period", it was listed in TWiR issue number 271 and 272.
Does it avoid allocations when it's not required to grow? It shouldn't be (too) slow, then. But it sounds like you're implementing some kind of FIR filter with a large number of taps. It's probably not surprising it's slow.
References and pointer dereferencing exist in Rust also though. I don't see how C++ is any different from rust here.
Yeah, I'll have to play around with it a bit to fully wrap my head around and fully grasp the implications and transitivity :) It certainly seems to allow avoiding *a lot* of boilerplate compared to having to do this all manually in a sensible way. I know I've run into this problem at least twice in the past, and I remember my "*sigh* rework the whole damn tree structure to be `Sync`/`Send` it is then" resignation. So, this is a very welcome area of exploration to me.
&gt;What if I have actual non Sync fields? For example, if one of the fields is an Rc, wouldn't I be able to clone it across threads, creating a segfault time bomb? Or will the derive macro somehow deal with this? `Neutralize` for `Rc&lt;T&gt;` is implemented only if `Neutralize` is implemented for `T`, and that makes `Inert&lt;Rc&lt;T&gt;&gt;` derefs to `&lt;T as Neutralize&gt;::Output`, which means the `Rc` part of `Rc&lt;T&gt;` is never exposed through the `Inert&lt;Rc&lt;T&gt;&gt;` wrapper, which means you can never change the refcount. &gt;So in order to use this library, it must expose a Neutralize impl for every single type in std, I need to derive Neutralize for all of my types I'm using, and I need to make newtype wrappers for all external types I'm using so I can impl Neutralize on them (manually). Yes, except that the creation of the inert wrapper will be done through the companion crate `inert_derive` and its methods will be generated through attributes on the fields you want to expose. &gt;After all of that work, I can still just unintentionally modify the original value that was made inert and produce undefined behavior in my program and the only unsafe keyword is around from\_ref. Not exactly, you won't be able to do that through the new type, because the generated new type will use `Neutralized&lt;T&gt;` which exposes the `T` only through an unsafe method as shown in `LayoutNode::children` in the original post. &amp;#x200B;
nobody has deprecated fundamental operators in rust (yet)
Ah, so you'd need to be able to mark what scopes things live in as well. That makes sense. So would a input format + scopes (on a per input fragment piece) make sense, or would there be yet other things needed? Of course, that alone would be a huge undertaking, but it'd make using proc macros so much easier. That said, I wonder what percentage actually fall into the category of "using things in scope in a well defined transform". I'd suspect it'd be a lot, but I'm could easily be wrong. At the very least, the majority of the ones I'd write (if not all) would fall into that category. I took a bit to look at your other responses in the thread. I like the example of the custom derives. I guess in addition to the input format and scopes, you'd want target types as well (e.g. type for the expressions). Hmmm... lots to consider. Thank you so much for your responses in this thread, and for the feedback given thus far :)
You can find that out by clicking the `[src]` link in the top-right.
There's a [`FIXME`](https://github.com/rust-lang/rust/blob/626e74d5f64cdc820b6c6ac1a5a9a42096cd147a/src/libstd/io/stdio.rs#L354-L359) for this.
ohh, anyone know when const generics will be available in unstable?
&gt; Not exactly, you won't be able to do that through the new type, because the generated new type will use `Neutralized&lt;T&gt;` which exposes the `T` only through an unsafe method as shown in `LayoutNode::children` in the original post. I said the original value. I can swear on all the baguettes I like but that won't stop normal human error. I don't know what servo design aspect made you come up with this library, maybe you could share more info so I understand, but why not just wrap your non Sync fields in a Sync container like https://docs.rs/atomic_refcell/0.1.3/atomic_refcell/ It uses an atomic operation where your solution does not, but it doesn't require users to implement a trait on literally every type and it's actually safe.
The itertools crate has a [tuple_windows](https://docs.rs/itertools/0.8.0/itertools/trait.Itertools.html#method.tuple_windows) method which might be what you want?
It's worth taking a look through the code you're calling. See the [definition of Stdout](https://github.com/rust-lang/rust/blob/626e74d5f64cdc820b6c6ac1a5a9a42096cd147a/src/libstd/io/stdio.rs#L354-L359) and you'll note: pub struct Stdout { // FIXME: this should be LineWriter or BufWriter depending on the state of // stdout (tty or not). Note that if this is not line buffered it // should also flush-on-panic or some form of flush-on-abort. inner: Arc&lt;ReentrantMutex&lt;RefCell&lt;LineWriter&lt;Maybe&lt;StdoutRaw&gt;&gt;&gt;&gt;&gt;, } So it's ultimately a `LineWriter`, and now you should be seeing what `lock()` is protecting, and why [your `write()`](https://github.com/rust-lang/rust/blob/b244f61b77c42d7be695afd7901ee4418559e518/src/libstd/io/buffered.rs#L895-L929) is slow: let i = match memchr::memrchr(b'\n', buf) { Some(i) =&gt; i, None =&gt; return self.inner.write(buf), }; Writing 13 million newlines will cause 13 million calls to the `write()` syscall. Writing 13 million spaces will fill the [8KiB buffer](https://github.com/rust-lang/rust/blob/b244f61b77c42d7be695afd7901ee4418559e518/src/libstd/io/buffered.rs#L54) and then write the lot in one go, causing 1,618 calls to `write()`.
I think that's not the case. The signature is: ``` #[proc_macro_derive(Foo)] pub fn foo_derive(input: TokenStream) -&gt; TokenStream { // etc. } ``` You must return the generated code. If you return `TokenStream::new()`, your proc macro does nothing.
&gt;I said the original value. I can swear on all the baguettes I like but that won't stop normal human error. Yeah that's fair, but that's already the case in Servo, but without the `Neutralized` thing, and without the mechanically implemented trait that limits the sharing to fields that can be neutralized, so you end up with hand-written methods on `LayoutNode` that you need to review to make sure they don't call `Node` methods that aren't safe to call from layout threads. This system will definitely reduce the area where normal human error is possible, and will reduce the probability of reviewers suffering from *unsafe fatigue*, where you have so many unsafe blocks in your face that you don't even know how to reason about them. &gt;I don't know what servo design aspect made you come up with this library, maybe you could share more info so I understand, but why not just wrap your non Sync fields in a Sync container like &gt; &gt;https://docs.rs/atomic\_refcell/0.1.3/atomic\_refcell/ Because that would kill performance, and that would mean using atomic cells for things that aren't even accessed by layout **at all**, just because they are stored in a `Node`. &gt;It uses an atomic operation where your solution does not, but it doesn't require users to implement a trait on literally every type and it's actually safe. That trait implementations are derived, and "literally every type" is a non-issue, that's the case for many **many** traits in Servo already.
Care to share the video?
I don't think there's a precise date, but it's getting quite close. I'd imagine within the next 2-3 months...
I think is a good idea. I'm ok with any option that is easy to join, even mail list. But maybe just a repo and all with issues?
The dbg! macro looks super useful!
dbg macro omg how‚Äôd i miss this
Due to one of the discussions (I think on HN) for this story, I decided to try writing a fibonacci number generator in C that had a cache. The result when I first wrote it was an immediate seg fault. Followed by no output. Followed by all zeros. Followed by a correctly working implementation (although, I don't free anything, so I'm sure there's more room for things to be messed up). &amp;#x200B; Learning, typically, is hard and painful and can easily leave you in a position where you cannot adequately express how you are getting your "correct" answers. In raising chickens for a large scale operation, you want to separate the male and the female chicks (I think they throw away the male chicks). However, a male chick looks basically identical to a female chick. In order to train people how to spot the difference, what you do is you have someone just start trying to determine the sex of the chick. And then you place someone next to them you already knows how to spot the difference. The new person makes a guess and the experienced person corrects them. Then you iterate (sounds suspiciously like how you train a neuro net). The end result is that you have someone who can now tell the difference, but they cannot actually tell anyone how to do this without just sitting down with someone and correcting them as they guess what sex a sequence of chicks have. &amp;#x200B; I think a lot of people end up learning programming the same way. They get skilled at programming, but it was from a sequence of failures. They don't know why they get correct answers. And the only way for them to pass on knowledge is by telling others how they are "wrong". &amp;#x200B; The obvious problem with this is that with a language like C you can get something "correct" that is only wrong when the runtime memory is in a particular configuration. Or when you run your optimizers in a certain order. Or when you get certain malformed inputs. And this is very easy to slip by anyone learning to program in the same manner as someone who learns how to determine the sex of chicks. The failure for incorrect behavior will only rarely show up (sometimes only after careful analysis by security researchers) and it will be separated by large amounts of time between when it was made and when it was determined to be an error. &amp;#x200B; The end result of all this? If you don't know when you're wrong AND you don't know how to explain why you are right AND learning can be difficult AND the only way to pass on your knowledge is to tell others how they're wrong THEN you're going to get a lot of people who are very resistant to try to learn something new and they will fight tooth and nail in order to avoid the new wave of techniques that make things better.
&gt; Because that would kill performance, and that would mean using atomic cells for things that aren't even accessed by layout **at all**, just because they are stored in a `Node`. This seems like more of a design smell than a reason to make a new library, but I imagine it's too late to have that conversation given servo's size. &gt; That trait implementations are derived, and "literally every type" is a non-issue, that's the case for many **many** traits in Servo already. The trait implementations are derived on MY types. I cannot derive implementations for types outside of my crates. I have to wrap those in newtypes and then manually impl Neutralize. That's hardly a non-issue. That might not be a problem in Servo but this isn't a Servo module it's a standalone library.
&gt;This is a mechanism to access non-`Sync` values in `Sync` contexts. `Sync` doesn't restrict what you may do with a value. It restricts what you may do with a location: - you may not access a location, in normal memory, allocated as a `!Sync` type, if that access may be simultaneous with any other access, unless both are made from the same thread. (i.e. the accesses may be *unordered* according to the C-ish memory model - Rust doesn't define this formally, but the expectation is "something C-compatible" and llvm currently implements this. This model has sharp edges. For example, two accesses in the same thread *are not ordered relative to each other* from the perspective of the program as a whole, which means you *cannot* reason very far about what a function does without either ensuring effects stay within the thread or imposing ordering constraints.) - You may not borrow a `!Sync` `Token` with the effect of calling a function whose arguments include `&amp;Token` simultaneously from multiple threads. This is used to enforce a "not reentrant across threads" restriction of a foreign library. &gt;## How is this sound? &gt; &gt;When the user create a `&amp;Inert&lt;T&gt;` value from a `&amp;T`, they must swear on the holy baguette that they won't use the `T` directly until all the `Inert` wrappers go out of scope, De-scoping a value is a no-op. *No-operation does nothing and imposes no memory ordering*. Thread X can write to var_a in scope 'a, choose to spill a temporary value to var_a, leave the scope, do other things visible to thread Y, propagate the spilled temp value to Y, and finally at some indeterminate point hundreds of cycles later propagate the correct value for var_a into thread Y. It gets worse. Thread Y can try to read from var_a in scope 'a, need to wait for a cache fill or address generation, execute other stuff past scope 'a, and at some indeterminate point go back to loading var_a (with a value corrupted by some other thread calling neutralize) and then work its way forward with the corrupt value. (Even on "strongly ordered" x86 architecture, compilers are encouraged to schedule a load later in program order if it depends on long-latency address generation. This has the same delay-and-go-back effect, it's just done at compile time because the CPU can't do it at run-time.) Note that "hundreds of cycles" can mean several iterations of an optimized loop, so the window in which mischief may occur is certainly large enough to hit. Also testing is only somewhat helpful; it's very possible to have a 1/10,000 bug. If you don't like that chaos, you need to tell the compiler explicitly and in *both* threads. If you don't want to deal with those details, you say `unsafe impl !Sync for NotThreadsafeThing {}` and the language *guarantees* that somebody else *will* take care of the details, or at least claim to. So implementing `Neutralize` depends on intimate knowledge of the !Sync type. I think you understand but the details can be *gnarly* and I don't think the current design can take care of them. &gt;The end goal is along those lines: &gt; &gt;For this input: &gt; &gt; #[inert(LayoutNode)] &gt; struct Node { &gt; tag: String, &gt; // Nobody would encode their tree like that but that's just to &gt; // show what it would generate. &gt; children: RefCell&lt;Vec&lt;Node&gt;&gt;, &gt; } `RefCell` can be optimized aggressively. The machine code can neglect to increment the borrow count if it is guaranteed to decrement it when done. &gt;Generate that: &gt; &gt; struct LayoutNode { &gt; // Not directly a Node so that methods we write ourselves on &gt; // LayoutNode cannot access the Node without unsafe code. &gt; value: Neutralized&lt;Node&gt;, &gt; } Newtype pattern is sound. &gt; impl LayoutNode { &gt; fn tag(&amp;self) -&gt; &amp;Inert&lt;String&gt; { &gt; unsafe { Inert::from_ref(&amp;self.value.as_ref().tag) } &gt; } Sound because `String: Sync`. &gt; // Through the Neutralize impl for RefCell&lt;T&gt;, &gt; // the type Inert&lt;RefCell&lt;Vec&lt;Node&gt;&gt;&gt; derefs to Vec&lt;Node&gt;. &gt; fn children(&amp;self) -&gt; &amp;Inert&lt;RefCell&lt;Vec&lt;Node&gt;&gt;&gt; { &gt; unsafe { Inert::from_ref(&amp;self.value.as_ref().children) } &gt; } &gt; } Only sound if you can guarantee: - All accesses that are made through the deref happen-before the neutralize call. - This means you must invoke a Release or SeqCst barrier *below the last use of the `&amp;Vec` reference* in each thread that uses it. However this is impossible because references do not implement Drop and you can't rely on Drop anyway. - and the neutralize call must be synchronized-with the barriers whenever they happen in different threads. That requires inter-thread communication. ---- You can't soundly do what you're trying to do, not exactly. At best `Neutralized` could be written as something like a Mutex minus the ability to park-and-wait. `Neutralized` cannot implement Deref because it needs to put a barrier below the last use - `MutexGuard` has this functionality and it is necessary even without the locking. A sound Neutralized would be something like a RwLock: - `Neutralized&lt;W&lt;T&gt;&gt;` where `W&lt;_&gt;` is a `!Sync` wrapper type with special properties and `T: Sync` - allows `&amp;T` access through a `NeutralGuard&lt;'use, T&gt;` - when `Neutralize` is unwrapped it ensures that the unwrap cross-thread-happens-after every `NeutralGuard` has been dropped. If not it fails. - the overhead is an atomic variable in `Neutralized&lt;T&gt;` - the user is responsible for proper synchronization, otherwise unwrap failures will occur. These failures will happen non-deterministicaly but will not trigger undefined behavior. That's not too appealing, but I could see niche uses. Design-wise I'm not sure it's desirable to factor out `Neutralize` from `W&lt;_&gt;`. The primary use cases I can see are `Cell` and `Rc`. A neutralized `Rc` is simply a less ergonomic `Arc`. A neutralized `Cell` or similar is a `Mutex`/`RwLock` that fails instead of waiting, and I think those could be more ergonomically designed. 
&gt; It lacks visual cues for variables (It is easier to understand the type of a variable when type information is associated with it) Type hints are now a thing. Sure, the language doesn't force you to write them, but there are plenty of ways to choose to make your functioning code look bad (actually fewer in python than in many other languages). &gt; stackoverflow posts about this matter can look like incantations (list-comprehensions is a good example) Eh, the same is *certainly* true for a number of rust's constructs. &gt; Certain functionality you need to unearth quite a bit to figure out what it is implicitly calling (iter call in for loops, str() calling __str__, __exit__ with context handlers using with). On the other hand, you rarely need that, and once you do, it's very easy to understand and replicate. 
&gt;The trait implementations are derived on MY types. I cannot derive implementations for types outside of my crates. I have to wrap those in newtypes and then manually impl Neutralize. That's hardly a non-issue. &gt; &gt;That might not be a problem in Servo but this isn't a Servo module it's a standalone library. The current system in Servo without the trait or the wrapper means that every method you want to call from the layout system has to be implemented by hand, with unsafe code. So having to implement by hand a trait on a few new types instead is still an improvement. I said it's a non-issue because that's just the usual pain of using traits, and the amount of third-party all-encompassing traits such as serde's desensitised us to that pain.
To be clear: You're quoting an RFC in final comment period, not that a stabilization has actually gone through yet. But it's so close! :D
I get that but I'm asking questions about the viability of inert as a general purpose solution, not as a solution to a Servo issue. As a Servo module this is clever and useful. For most applications, though, it's probably papering over design problems that would normally be exposed by an excessive reliance on unsafe code. Basically, I think you yourself have not committed the XY problem as the other commenter suggested because this sounds like servo really needs this. However, I think most people who would use this library would be committing it by doing so.
&gt;and the neutralize call must be synchronized-with the barriers whenever they happen in different threads. That requires inter-thread communication. To send the reference to the inert wrapper to another thread, you must send it over some kind of channel, which must provides its own synchronisation. What you say would indeed be an issue if anything else touches the refcount in `Rc&lt;T&gt;`, but that can't happen through the inert wrapper. &gt;You can't soundly do what you're trying to do, not exactly. I disagree. What is the difference between sharing a `&amp;Foo(Rc&lt;String&gt;)` that only lets you get a reference to the `String` inside, and the `&amp;String` directly? &amp;#x200B;
I'm confused, the link for it goes to a PR in rust-lang/rust, and it has lines like ``` - #[unstable(feature = "try_from", issue = "33417")] + #[stable(feature = "try_from", since = "1.34.0")] pub trait TryFrom&lt;T&gt;: Sized { ``` in it, so I thought it was actually stabilizing the traits?
The C++ standard only recognizes ```std::thread``` as a means of executing multithreaded code. Any other mechanism is undefined behavior and not supported by C++. It would be no different than introducing threads in Rust using a system call and then passing an ```Rc``` to those threads. Rust would be unable to provide any guarantees in much the same way that C++ can't provide any guarantees once you use an unsupported mechanism.
This sounds like an admission that the never type isn't working out?
I think the _existence_ of `Rc` is one of the clearest examples of how Rust's safety story is different from other languages'. A similar type was ["overwhelmingly" rejected](https://stackoverflow.com/a/15140227/823869) for inclusion in C++, with one of the main arguments against being the tricky bugs/vulnerabilities that would result from accidentally using it across threads. In Rust on the other hand, putting `Rc` in the standard library was uncontroversial. (Or if there was any controversy, it was a long time ago and I've never read about it.)
There's actually some research that shows the CLI are good for the elderly who would otherwise have problems with technology. &amp;#x200B; The big takeaways were: 1) you typically don't see massive UI changes with a CLI tool and 2) you can always look at what you did in the past to try to figure out where you are in the present by scrolling up in the terminal.
ah, I missed that the PR wasn't merged yet still, I'm excited
&gt; The Rc/Arc abstraction seems much better. The C++ optimization is an attempt to abstract over the Rc/Arc distinction so that there is no distinction. Calling Rc/Arc an abstraction is not quite correct, as a user you have to make a choice in your API as to whether you use an Rc or an Arc and you're stuck with that choice. C++ decided that such a trade-off wasn't worth the benefits, at least since in C++ ```shared_ptr```s are discouraged outside of multithreaded code.
I read it as "we want to stabilize Try{From,Into} without having to wait for ! to stabilize" 
It is possible - [here is an example](https://github.com/jesskfullwood/frames/blob/master/src/frame.rs#L334-L355). `df.join(df2, leftcol, rightcol)` will compile iff the columns exist in their respective frames and have the same type, else it will fail, with a pretty confusing compiler error. As you can see, the definition is complicated. Because essentially it requires emulating dependent-typing using traits.
When it comes to large codebases a lot of the things that sound like design smells are actually inevitable, and it's somewhat frustrating to see people consistently brush aside the needs of large codebases as patterns nobody will want to use. Large codebases are a great case study of https://thefeedbackloop.xyz/i-bet-that-almost-works/
Wouldn't it be more premature optimization instead of XY in that case? Like using `get_unchecked` in your code even though it makes no performance difference in your situation.
Wrong sub - this sub is for the rust programming language, not the rust video game.
Except that in Rust, you would explicitly have an "unsafe" block in your code where you accept the responsibility not to break things. Otherwise, I agree.
Many people don't realize how expensive atomic operations are on a modern CPU: they can be 100x slower real easy. It's a big exception to the one instruction per cycle rule of thumb. (So are memory accesses, except the miracle of caches often saves you there.) One can imagine adding a `#[require(nothreads)]` feature to Rust. This would enable all kinds of neat optimizations, and would allow the compiler to let you have static mutable global variables for your single-threaded programs, since the compiler would guarantee that you never had multiple threads. Every datatype could be marked `Send` + `Sync` because no sending and no syncing could ever actually take place. The price would be that you would not be able to link any library that wasn't also `#[require(nothreads)]`. Most of the standard library and most other libraries are single-threaded currently, so it wouldn't be super-onerous once all the declarations were put in. I suspect this was considered at some point and rejected because of the ecosystem split it would induce. In current Rust it is transparent whether libraries are multithreaded: you don't have to know or care. This is super-nice for library crate authors, who can choose whether to use threads or not without their clients knowledge. For an example of doing a thread-transparent library in C 20 years ago you can look at our `XCB` X Window System C bindings library. Being thread-transparent was an explicit design goal when I designed it. It ultimately required some formal methods (Z specification and analysis) to show that it was probably OK, and there are still some threading bugs because the implementation doesn't match the design perfectly. You don't pay the performance penalty for using XCB with your single-threaded program, because you link against a stub version of pthreads using weak symbols (much like the libstdc++ plan, but fairly explicitly documented). So‚Ä¶it's complicated. Rust made some choices. libstdc++ made some choices. I *really* wish my binary crates could specify `#[require(single_thread)]` and use global mutable variables, but I'm glad that my library crates can freely use threads where appropriate without breaking their clients. YMMV.
I'm thinking XY because the hypothetical user wants to share non Sync data across threads so they reach for Inert, the X, when what they really should do is something like separating Sync and non Sync data or using Sync containers. It's a bit like when you try to design things like a normal OOP language and end up with everything in Rc and RefCell. It solved the problem you thought you had but your actual problem was trying to use an inappropriate design for Rust.
The buffering I'm talking about is supposed to happen in the libc, it should be independent from the stdlib.
The most recent relevant discussion was added by /u/SimonSapin here: https://github.com/rust-lang/rust/issues/33417#issuecomment-463282164 TL;DR: The current approach will be to try making `Infallible` an alias of `!` (the "never" type) eventually. Theoretically this could cause breakage if somebody implements a trait for both types, since there would then be conflicting implementations. The libs team has judged the breakage to be unlikely and worth trying this strategy at the risk of living with having both of them down the road if the breakages from replacing `Infallible` with `pub type Infallible = !` are too great for some reason.
You're looking for r/playrust
Rust normally uses the system's stdlib, there should be 13 million calls to write(2) which should itself buffer (normally, the libc sets stdout to fully buffered unless it's connected to a tty).
The VecDeque never grows because I always insert and take a sample at the same time (well, the capacity is one element larger than the one I need because I push() before pop() but i should not grow) Yes, I'm doing really long FIR filters but anyways I'm comparing the performance against a similar implementation using loops. Anyway I'm going to do better benchmarks before trying to optimize it. Thank you!
Let's drill down some more. [`sys::Stdout`](https://github.com/rust-lang/rust/blob/b244f61b77c42d7be695afd7901ee4418559e518/src/libstd/sys/unix/stdio.rs#L20-L33) just wraps STDOUT_FILENO with a `FileDesc`. [`FileDesc`](https://github.com/rust-lang/rust/blob/b244f61b77c42d7be695afd7901ee4418559e518/src/libstd/sys/unix/fd.rs#L99-L106) directly calls `write()`. The behaviour you're describing is part of C stdio, using `fwrite()` and friends. Rust doesn't use those, because it's providing its own.
&gt; What was about the Rust code which made it take so much space? Hard to tell. A release build of Rocket Hello world generates a 6 Mb binary, but you can get it to 200Kb by passing the right flags: https://jamesmunns.com/blog/tinyrocket/ .
Pretty sure a C++ shared library using `shared_ptr` could break if called from Go, since bypassing libc and using syscalls directly is exactly what it does for pretty much everything aside from name resolution.
Thanks, I was trying to find the context, but the comment link didn't work on mobile.
Fun fact, the first ever LA Rust meetup talk was on this topic. Seriously!
Not quite: &gt; A strictly decorator based form of attribute macros, **which can‚Äôt change the source item,** but can produce other items. I interpret this as meaning that adding a trait implementation `#[derive(Clone)]` is fine, as it's purely additive, however anything modifying the item it's decorating such as `#[async]` is not.
It is inevitable that your non Sync and Sync data must be stored on the same structure which needs to be used both in non Sync context and in one multithreaded algorithm? I find that hard to believe. Besides, I'm not brushing aside anything. I even said that it sounds like servo really needs this. It rather feels like you two are dismissing the concerns of me and other commentors because they don't apply to servo.
Good luck. You might want to try using iterators and `zip` -- sometimes they generate better code. It would be nice if you could get the auto-vectorization to kick in, but there are some crates for that ([faster](https://github.com/adamniederer/faster)?). 
&gt; It is inevitable that your non Sync and Sync data must be stored on the same structure which needs to be used both in non Sync context and in one multithreaded algorithm? Yes, that structure is the DOM, which has its own design constraints that we have no control over. &gt; It rather feels like you two are dismissing the concerns of me and other commentors because they don't apply to servo. Uh, what? You're the one saying "this is a design smell for servo". This library was created to support servo. The commentors are saying "why do you need this", and nox is explaining how it's necessary in servo, this is hardly brushing concerns aside.
Do you find yourself not using Cargo a lot? What workflows are you using where you aren't using Cargo, and where you aren't able to bring in dependencies?
The `rand` crate has gone through quite a lot of breaking changes, and it's not yet 1.0. Why bake in a poor API when using the crate is just a line of `Cargo.toml` away? It might sound like designing an RNG is simple, but you'll quickly run into all sorts of issues. A simple RNG like you would like would probably work for tiny programs like the guessing game from the book, but you wouldn't want to use it for something serious. And if so, why bake it in the standard library?
&gt; Yes, that structure is the DOM, which has its own design constraints that we have no control over. So then it has nothing to do with the concerns of large codebases, it's actually just the DOM. &gt; Yes, it's a standalone library, but it's a standalone library that's unsafe to use; so other clients of this library need to make the same judgement that servo has made. It's not dangerous. That's not what the post says though is it? It just says it allows you to access non Sync data in Sync contexts. I'm just saying that for most situations that you have that problem, this solution is questionable. Given that, repeatedly informing me that it works well specifically for the DOM in Servo is meaningless.
Just to make sure: did you mean the comment link I just posted or the one in OP's TWiR link?
I think the issue is that this is about accessing non-Sync data using it's Sync-subset, not a block of some Sync and some non-Sync data. The easiest example is `Rc`. You can (theoretically) use it in a Sync environment if all you ever do is deference it, and never clone or drop it. In one part of the code, it needs to be `Rc` because it's a performance critical component that clones the `Rc`s but can only use one thread. In another part, it needs to act like `&amp;` because it's a performance critical multi-threaded environment that doesn't need to clone the `Rc`, only follow the pointers. In that specific case, you could have a derive for some sort of `BorrowedT` that replaces `Rc` with `&amp;`. `Inert` just does this subsetting for more `Sync` types and helps reduce the `unsafe` proof obligation on the user. `Inert` is somewhat an `unsafe` _type_ in that it can't be allowed to escape the current execution context so it won't outlive the parent context that owns (and promised the `unsafe` baguette not to use) the non-`Inert` data structure.
Ah yes, that makes sense. Thanks.
`fn neutralize` should probably take `&amp;mut self` to prevent having non-neutralized access to the same data on the same thread. I realize it's an `unsafe fn`, and probably the servo use case doesn't have the luxury of starting from the `&amp;mut T` in the first place, but it seems more correct. The `&amp;self` version does have to exist for recursively neutralizing, though... `&amp;mut self` just seems more correct for the root.
&gt; So then it has nothing to do with the concerns of large codebases, it's actually just the DOM. The point I'm making is: Most large codebases end up in situations like these where they need to do something somewhat weird. Don't jump the gun on calling things a design smell. &gt; That's not what the post says though is it? The post literally says it's unfinished, you're asking for more documentation than it needs right now. Also, I am not arguing against adding said documentation, criticism of one thing you have said is not criticism of all things you have said. &gt; Given that, repeatedly informing me that it works well specifically for the DOM in Servo is meaningless. I only did this because you claimed it was a design smell. All I was trying to do was point out that it's very common for large codebases to have legitimate weird needs and it's frustrating when people tell you "that's a design smell you should do something else" when "something else" doesn't actually work. It's super condescending, and is a consistent experience I (and others working on large codebases) have had with programming communities in general, including Rust.
I have crossposted this to /r/cpp ([here](https://www.reddit.com/r/cpp/comments/aq9duh/no_the_problem_isnt_bad_coders_sean_griffin_medium/)) -- hope you don't mind! :)
Could `#![require(nothreads)` be done by a proc macro crate in a similar way as the `#[no_panic]` attribute?
This is one of the ways the current futures implementation needs to be improved, this state of affairs is not meant to continue indefinitely. This is not related to finalizing the syntax for creating a generator that takes arguments visibly in its surface syntax. If you use only async/await, you never actually see a Waker.
&gt; C++ decided that such a trade-off wasn't worth the benefits It's not that they made such a decision, it's that a non-thread-safe reference-counted pointer would be completely unsafe in C++ as it could get used in a multi-threaded context and then all hell would break loose. So C++ pretty necessarily defines only a thread-safe version, and implementations are on the hook for trying to optimise it. Rust can get away with both because the type system means Rc literally can't be used in multithreaded scenarios.
&gt; would allow the compiler to let you have static mutable global variables for your single-threaded I am not sure, but I think `static mut X: T` is unsafe even with a single thread, due to reentrancy. Would would be safe is making a `static X: RefCell&lt;T&gt;`. 
The link to this comment in the OP: https://github.com/rust-lang/rust/issues/33417#issuecomment-423073898
There's been progress on the Allocator front in C++: with C++17, the methods to construct/destruct objects were removed from the Allocator interface, so that it can concentrate on... allocating! Unfortunately, it leaves use with an iffy interface: there's really no reason for the Allocator to have to be parameterized by a type. It's just cumbersome. Especially when many collections will `rebind` the Allocator to another (internal) type or several anyway. I wish it had focused on allocating memory instead. And of course, there's the big question on the point of having an Allocator *template* argument. I must admit I prefer the approach from Zig here: given the cost memory allocation, passing a function pointer (or two), seems like a good solution. --- As for the criticism on size/alignment, I am of two minds. Deallocating from just a pointer is simpler on the user, and avoids storing size/alignment with each pointer (potentially), whereas deallocating from the triplet could potentially allow faster deallocation but at the cost of inefficiencies elsewhere. It's not clear to me that one solution performs strictly better than the other.
There is a [discussion](https://github.com/rust-random/rand/issues/648) about introducing a new lang item which will provide a very simple [`getrandom`](https://github.com/rust-random/getrandom)-like interface. There is several good reasons for doing it, but I highly doubt Rust std will ever get anything beyond that, as this functionality can be covered by `rand` project without any problems.
&gt; The post literally says it's unfinished, you're asking for more documentation than it needs right now. I'm not really asking for anything. I am saying my responses are based on the post because that's the info I had. I also don't think it's unreasonable to think if your library has a narrow use case that you would not announce it as a general solution. &gt; that's a design smell you should do something else It was more my intention to suggest that it is a design smell _normally_ and that the same logic should not necessarily be used to justify the use of Inert in other codebases. That's why I said it was too late and did not suggest doing anything else with servo because I knew whatever I thought would be wrong.
I don't think making things inert can be safe while `Rc` is `Neutralize` either. Consider the case of a `!Sync` task executor. If you can make a `Rc`-containing type inert then suspend the current task, another `Rc` can be accessed while the inert version is alive. Part of the `unsafe` contract to the holy baguette is that you won't allow untrusted code to run on this thread until all inert-carying tasks have trustedly reported the destruction of the inert values in a synchronizing way.
Looks like pthread is used directly by the runtime itself: https://github.com/golang/go/blob/50bd1c4d4eb4fac8ddeb5f063c099daccfb71b26/src/runtime/cgo/gcc_linux_amd64.c#L5
On arbitrary sized integers. This seems ripe for misuse by people thinking they are doing some kind of optimization. OTOH I have been toying with re-encoding the RISC-V instruction set into something better (some of the instruction formats are awful). So far I've got something that naturally generalizes to 16,32,48,64 bits. I wondered if anyone would ever design a 48-bit processor and thought the lack of language support would certainly cut off any effort before it got started. And now here we are looking at the possibility of N-bit integers in Rust. I think it's a terrific idea as an enabler but I wouldn't want to see it widely used.
zig doesn't have dynamic dispatch. any dynamic dispatch that went on would have to be userlevel.
As Stack Overflow is found of: 6 to 8 weeks...
That's fair. In the future though I would advise against making comments denigrating others' use cases, even if what you're trying to say is legitimate (as it is here) it comes off as pretty arrogant and folks tend to deal with it a lot, and there are other ways to say that.
In windows, it is possible to get a notification when a thread is created, since a dll's entry point is called with DLL_THREAD_ATTACH. The DLL could have a global variable keeping track of the thread count, and use atomic refcount only if thread count &gt; 1. In addition: * Enumerate threads when the DLL is first loaded, to get a correct initial thread count * When "thread count" is changed from 1 to 2, add some kind of memory fence to make sure the increase is picked up immediately. Would this work or am I missing something? And is there some way to track number of threads on other platforms?
Biggest highlight of this TWiR for me too. I am glad to see progress on const-generics, even in a limited form (no computation on parameters) it already allows specifying array sizes for array-based structures.
There are some professional environments that strictly lock down libraries and put any new library use through architectural review. I understand this completely - libraries are nice because someone else is doing the work for you, but you can have licencing issues or you may lose forward momentum when a bug is found in the version you are using and upgrading would require major, project wide code changes due to braking changes in the library. In the case of a random function though, I think an external lib is the way to go. Especially when there are a dozen different ways to generate pseudorandom numbers and not every method is good for every possible application.
If you only use async/await, you never actually see a Generator either, so I agree, it's a bit of a moot point. But in your "second use case" which the post focuses on, the user is expected to interact with generators directly, so the surface syntax/api for generators does actually matter. How then would you create an async `Stream` from a `Generator` without any way to pass the `Waker` into the generator body on every `poll_next` call? I'm not seeing how "Streams cannot take resumption arguments" can be true when `poll_next` is synonymous with `resume` with a `Waker` resumption argument.
Yeah, but Rust has to think about itself first, and the error condition of allowing unsafe code if someone forgets to write `safe { .. }` is a bad one.
Yeah I'll be careful how I word it in the future. To me "code smell" just means "design that needs a good reason to exist", not "bad code", and I was assuming Servo had a good reason so I wasn't thinking it was a criticism.
I agree there is no need to include `rand` in `std`. So it can be developed independently.
Alright, here's the full problem statement I see this serving: You have an `Rc&lt;Refcell&lt;Node&gt;&gt;`-based tree. The single threaded mutator requires granular shared mutability in this way, and cannot afford to pay the cost of using atomic accesses without tanking performance. There is a separate step guaranteed not to overlap the above step by clear-as-dirt control flow. To get acceptable performance, it needs to be multi-threaded, but it also needs to observe the tree without needing to write to it. Also, swapping between the two modes shouldn't require walking the entire tree. How would you solve this? `inert` solves this by making your `unsafe` proof that you can't run untrusted code on the and you synchronously observe the finalization of access to all of the `inert`-ified access before releasing back to the non-inert type. `inert` then for your trouble exposes the `!Sync` types in a thread-safe manner, through the `unsafe impl Neutralize`. The idea is sound, even if implementations may have bugs or `unsafe` proof burdens aren't always met. I _cannot_ come up with another strategy that meets these requirements. Also, servo's already doing effectively this in a much more error-prone, `unsafe`-everywhere fashion.
That's only for cgo though. You can disable cgo entirely and produce static executables with `CGO_ENABLED=1` and it'll use the go runtime rather than libc implementation. Unless it does some extra magic and still manages to use pthreads?
Beware: I haven't used a proper form of UoW, so I might not understand it properly. Is it usually something like a database transaction? Does it imply (require) change tracking (√† la `dynamic-update` in Hibernate)? One example I've found [here](https://docs.microsoft.com/en-us/aspnet/mvc/overview/older-versions/getting-started-with-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-application) doesn't seem particularly convincing, since it only mentions transactions once, then goes on to not use them. The `UnitOfWork` there is just a wrapper over a database context and some repositories. That being said, I think: - persisting the entities to the database does not require mutable access - (if they want to) the entities can do the dirty checking / marking themselves without needing anything special wrt. mutability (if you update an attribute you already have a mutable reference). - at times you might want to change entities through immutable reference, e.g. to reset the dirty attribute flags: you can use a `Cell` or something similar for "value" (`Copy`) types I'm sure I'm missing something, but perhaps some code or a sketch might be useful here. PS: Are you Oren of ayende.com? :-)
They did make that decision. C++ doesn't prevent a non-thread-safe RC pointer, it's just that the standards committee didn't think having it in the standard library was worth the risk (a trade-off, if you will). In any case, their point still stands that having both Rc and Arc is not the same as having an abstraction over both.
I am actively working on [miniserve](https://github.com/svenstaro/miniserve), it's not my project, though I'm trying to contribute to make it the "simple HTTP server" everyone would want to use. And I think we're not far from that ! 
&gt;Alright, here's the full problem statement I see this serving: &gt; &gt;You have an `Rc&lt;Refcell&lt;Node&gt;&gt;`-based tree. The single threaded mutator requires granular shared mutability in this way, and cannot afford to pay the cost of using atomic accesses without tanking performance. ... &gt;The idea is sound, even if implementations may have bugs or `unsafe` proof burdens aren't always met. This is absolutely wrong on the worst possible level. `RefCell` has an internal lock! You won't borrow it in multiple threads, even if it's only for reading. Burn it with fire; RefCell is not for multiple threads. This is also ignoring the entire Rc accidental stuff you could do. &gt;I _cannot_ come up with another strategy that meets these requirements. Also, servo's already doing effectively this in a much more error-prone, `unsafe`-everywhere fashion. There's an easy one: use pointers. You can reference count it yourself, not share a lock, etc.
Learning Rust fist is not wrong, though it can be challenging.
&gt; I don't know about his code, but casting pointer like that seems quite unsafe though. I think the "icky feeling" might come from semantics. We can't actually tell if it's unsafe or not without more context. What we can tell is that the Rust compiler cannot prove that the code is safe. Just for fun, let's assume we renamed `unsafe` to `trust_me_on_this` so we that `trust_me_on_this`-blocks instead of `unsafe`-blocks for pointer deref, ffi, etc. Maybe that would better reflect what `unsafe`-blocks really are?
Ah, I didn't realize that you wanted the maps to overlap. I think you'd be better off with slice windows, and calling collect once each stage, as others have suggested. That way, you're making a single copy of the output elements for each stage, and you can use them several times, rather than making m copies where m is your window size. src_vals.windows(1000) .map(|xs| xs.iter().sum()) //[1] no need to copy, and good cache behavior .collect() //[2] Each result is written to memory once (apart from any reallocs) .windows(10) .map(convolve) .collect() //etc (Ignoring type handling ~~because I'm lazy~~ for clarity here) Your way, with buf\_map, you need to allocate a buffer for each iteration, and copy elements into it. So if you have a window size of 1000, you're copying each element 1000 times. That gets expensive quickly. This way, you're never copying anything, so you save that expense. You mention above that it's "not online", which I take it to mean that the nth pipeline stage can't start until stage n-1 does. That's true, and it's a throughput-latency trade-off. I'm assuming `src_vals` is a finite sample already, rather than an iterator backed by an input stream or something. If that's not the case, then I think you need a new struct that implements FromIterator in an on-demand streaming fashion. Let's say `WindowBuffer` implements `FromIterator` in such a way that it is backed by several vecs, and lazily fills its backing vecs from the supplied iterator, as it needs them. When does it need them? To fulfill requests to its windowing iterator. This version of window yields a thing that can be indexed like a slice, but is backed by two non-contiguous memory regions (two slices). If the index operation fits in the bounds of the first slice, it uses the first slice. Otherwise, it indexes into the second slice, subtracting the size of the first from the index. So each call to `WindowBufferWindowIter.next` returns one of these pseudo slices, and occasionally, that requires filling a new buffer from the source iterator. You can probably swap between two backing buffers, as long as they're at least as large as the largest window you support. The upshot to this somewhat complicated approach is that you get finite buffering of an infinite source, so latency is manageable, but each value produced anywhere in the chain is stored only once. All this just exists in my head, so there may be issues I'm not thinking of here, but I'd be really interested to see the code if you actually wrote it.
RefCell uses an internal lock; by dereferencing it in multiple threads (mutable or not), you are doing something provably unsound.
&gt;BufWriter Thanks a lot! Program running time was cut in half and practically nothing was cut to practically nothing.
&gt;Thanks a lot! Program running time was cut in half and \`sys\` was cut to practically nothing. &amp;#x200B;
As far as I can tell, the idea is to push the explicit use of `unsafe` to the portions of the code where it matters. For example, let's say you have a program that performs unsafe operations somewhere. Would you put an `unsafe` block around `main` and then merrily write both your safe and unsafe code under the same umbrella, or would you seek to isolate the specific function or block where `unsafe` is needed and place the attribute there? Hopefully it's obvious why the second answer is preferable. So in this case the servo folks must have identified that `unsafe impl Sync` is too broad for this particular use-case. They *could* do it in the same sense that they *could* wrap everything in `main` in an `unsafe` block, but by doing that they would lose the actual benefits of unsafe in controlled isolation.
A discord #wg-dataframes room would be rad. 
&gt; The C++ optimization is an attempt to abstract over the Rc/Arc distinction so that there is no distinction. I know, and my point is that this ends up working sub-optimally. What was apparently done was "figure out if the app is running multi-threaded and do the expensive thing in that case". Rc allows you to specify "this variable is only used in a single thread so do the non-atomic thing" and that's fine in Rust and wouldn't be in C++ because the compiler doesn't allow you to shoot yourself in the foot with that. The only real downside I can see with that is in cross-crate APIs where you will tend to expose Arc instead of Rc in case the caller wants to use threads and so the extra performance is not possible without duplicating the API.
&gt; I assume that this means that I'll have to use Rc&lt;&gt; as the exposed data type to handle that, right? You could also store the entities separately, assign each entity an ID and only store the IDs in the UoW structure. Then when you want to call `commit` or `rollback` on the UoW you'd pass it the collection of entities. The way you'd structure something like that in Rust really depends on your exact design constraints. In general it might not always be possible to directly apply some of the design patterns found in other languages to Rust. Just as you can't code in Haskell like you would code in Python, you also can't write idiomatic Rust like you'd write C++. Idiomatic Rust has a certain flavor which needs to be intuitively understood and internalized; if you're used to a different flavor of structuring your program then I'm afraid that (at first) Rust is going to feel awkward (and maybe frustrating) to use, and many things will seem like there is no good way to implement them in Rust. This does, however, become less of a problem the more time you spend with Rust and actually get a feel on how to write idiomatic code in it.
As also written in the [README.md](https://README.md), this is not an official project, though!
A good comparison point might be the `lazy_static` crate, which is arguably a fundamental piece of infrastructure for Rust, but which isn't in `std`. As others have pointed out, Cargo is one reason Rust coders are more accepting of critical libraries remaining outside of `std`. Another factor is that `lazy_static` and `rand` aren't necessarily API details that everyone has to agree on. If my library handles its global variables and random number generation one way, and your library does those things a different way, that doesn't necessarily cause problems for us.* That's different for big API traits like `Iterator` or `Future`, which is why those things tend to wind up in `std`. \* That might be different if you want to do something like "make a big program's behavior completely reproducible for testing or simulation purposes," where it's critical that every part of your program uses random numbers from the same source. But for a difficult and specific project like that, it's also unlikely that a standardized `rand` API would give you everything you needed out of the box.
Last I checked, highly monomorphic code can bloat the binary quite a bit. Rust (or LLVM?) won't remove the dead monomorphized functions from the resulting binary. There are options to do this in LLVM, however I don't believe they're used (yet) in Rust. Although, TBH I haven't checked recent versions of Rust. The author didn't state this, so I'm unsure how much it affects them, but Rust using jemalloc by default, instead of the system, or a custom allocator will increase the binary size as well.
Hey! Trying to run cargo build in a virtual machine using vagrant, but it keeps freezing after having started X where X is the number of assigned Cpus compilation. I believe the file system freezes because if I try to log in again and navigate to that directory the console freezes. &amp;#x200B; Any ideas? 
Good read! Just thinking out loud. What if you attempt to use the shared pointer in a signal handler ? That could result in concurrent access, without threads ?
The goal is to be able to write an async generator which yields items and operate over that as a Stream. There's no resumption arguments in the surface syntax there - you can't write `let (x, y) = yield z;`
I was referring to this remark. &gt; All he did was choose a language which made him feel better about doing the same mistake-prone thing. Most of the C jabs at safer languages were all about feeling better at coding with no rules, with the consequences we now endure on our IT stacks. 
I have one of those: https://github.com/lnicola/rusty-share/ (beware, terrible code)!
&gt; `RefCell` has an internal lock! That's why you _aren't allowed to access `RefCell` directly_, which is what makes this sound. Instead, you get `Inert&lt;RefCell&lt;T&gt;&gt;`, which uses [`RefCell::as_ptr`](https://doc.rust-lang.org/std/cell/struct.RefCell.html#method.as_ptr) -- _which doesn't touch the reference count / lock_ -- to `Neutralize` the `RefCell` and give you `&lt;T as Neutralize&gt;::Output`. (Actually, you probably won't get a `Inert&lt;RefCell&lt;T&gt;&gt;`, that's just for illustration purposes. Instead when you access the containing neutralized type, it'll go straight through the `RefCell` as if it weren't there using `&lt;RefCell as Neutralize&gt;`.) No one says that this is "just" safe. There's a large proof burden on using it to make it sound. You can only give the inert references to code you trust not to reachable leak them, and to synchronize back with the orchestrating thread once all inert references are no longer in use. &gt; There's an easy one: use pointers. You can reference count it yourself, not share a lock, etc. So, do this, but re-implement the non-`Sync` to expose `Inert&lt;T&gt;` natively? Because that seems to be what you're suggesting. If that's to make it more sound to rely on what may be implementation details of the `!Sync` data types, I'll give that to you. If not, you're just introducing more places to mess up, while this is trying to reduce the `unsafe` surface area. `inert` is based on the idea that `!Sync` types have some `Sync`-capable subset, if you synchronize that `Sync` access to happen "atomically" in respect to `!Sync`-capable access. If we disagree on that point, I guess someone'll have to go write a proof for it, because I intuitively believe it to be true. All `inert` does is make it easier to restrict yourself to that `Sync` subset of `!Sync` types statically.
Is this a different `Stream` from the one in the [futures-preview](https://docs.rs/futures-preview/0.3.0-alpha.12/futures/stream/trait.Stream.html) crate? I would expect the `Stream` impl for `Generator` to look something like impl&lt;G, T&gt; Stream for G where G: Generator&lt;Yield = Poll&lt;T&gt;, Return = ()&gt; { type Item = T; fn poll_next(self: Pin&lt;&amp;mut Self&gt;, w: &amp;Waker) -&gt; Poll&lt;Option&lt;Self::Item&gt;&gt; { match self.resume(w /* can't actually do this, no resume args */) { GeneratorState::Yielded(y) =&gt; y.map(|v| Some(v)), GeneratorState::Complete(_) =&gt; Poll::Ready(None), } } } If you can't actually pass the `Waker` down to the `Generator` ... then what do you do with it?
I'd be willing to wager a couple bucks that the logic in libstdc++ that we're discussing arrived before `std::thread` was specified, maybe even proposed.
Please don't make **rand** part of std. I have a hardware random number generator and having interfaced that to rust, I don't want to use a random number generator, even if its cryptographically secure. 
Does that mean that Unix type signal handlers are unsupported by C++, and that C++ is unsuitable for writing device drivers that that rely on interrupts ?
The std library already has a basic interface to the system random number API for hash map seeding. It would probably make sense to expose a very minimal API for generating random bytes from this. But everything else is better off in the rand crate, I think.
Yes, I probably understated the contract of the holy baguette, I should have been more precise and not make it about scope. The gist of it is that you shouldn't access the damn data when it's currently alive behind an `&amp;Inert&lt;_&gt;`. &amp;#x200B; That being said in the case of `Rc&lt;T&gt;`, the scenario you describe can lead to unsoundness only if the `T` can be mutated, and that can only happen in that case with `RefCell&lt;T&gt;` or another similar type that provides single-threaded interior mutability, in which case we can still encode that with marker traits.
5K is impressively microscopic. I don't think you'd get that by accident. Has the zig designer made a special effort to make binaries as small as possible? That might do it.
Have you checked the number of open file handles vs. the maximum?
By the way, Rust uses the system allocator by default as of 1.32.0!
[https://github.com/rust-lang/rfcs/pull/117](https://github.com/rust-lang/rfcs/pull/117)
Both ```std::shared_ptr``` and ```std::thread``` were introduced in C++11.
Bindings have pros and cons: - Pros: Bindings to good quality libraries are much cheaper to create/maintain than said libraries. - Cons: External dependencies on non-Rust code. There are multiple issues with the latter: - Cumbersome for users: especially for C and C++ code, installation is often a hastle. - API issues: idiomatic Rust code is not idiomatic Library X code; defining a rustic API may not be so easy, and a non-rustic API may go against the grain. - Safety issues: binding to C and C++ means inheriting their safety issues. - Performance issues: lack of cross-language inlining means that small back &amp; forth between Rust and C are more costly than just staying in C where inlining can kick in. --- This does not mean that bindings are a bad idea; far from it. Just that it's not a silver bullet. There are already bindings to a number of libraries, the naming convention for "bare-bone" bindings is that the crate names should end in `-sys`. Then, more elaborate bindings can be built on top, with rustic APIs. See [Kornel's guide](https://kornel.ski/rust-sys-crate).
I'm not sure if you've seen this before, but [arewelearningyet.com](https://www.arewelearningyet.com/) has some info on the Rust ML landscape.
Signal handlers and interrupts are both supported by C++ but they do not require or introduce any additional threads, they are executed in the main thread. You can think of both of them as interrupting the main thread of execution rather than every interrupt or signal spawning its own thread.
It sounds like you want to do change tracking? You can do that via a wrapper type that implements DerefMut
A question about Cargo workspaces: I have a project for which I want to be able to compile a console binary *and* a Windows DLL. It's easy enough to isolate the shared logic into a separate library crate. If I set up a workspace with this "core" library crate, and then two other crates that each on their own would build the binary or the Windows DLL, would `cargo build all` simultaneously produce the binary and the DLL? Or is there a better way to do this that's more Rust-ic? Maybe a way to tell Cargo whether to build the binary or a DLL at compile time with an extra command or flag?
&gt;I suspect this was considered at some point and rejected because of the ecosystem split it would induce. I don't recall anyone ever seriously proposing that. Concurrency was one of the design goals of Rust from the beginning.
But can't signal dispatch (and IRQ for that matter) pre-empt the main thread ? 
How can I specify features using a cargo workspace? I want to run something like: `cargo run --bin bin_name --features feature_name` it works if I run this inside of the bin\_name crate, but it doesn't if I run it from the workspace root. Is that just how cargo workspaces work?
Just for future reference: "code smell" is usually used for code that isn't necessarily bad on its own, but indicates that there might be something undesirable underneath (generating the smell). However, this meaning has been somewhat weakened recently, with "code smell" being used for basically any code where there exists a reason to not like it, and is slowly becoming more synonymous with "bad code" or "code that I wouldn't accept" as people use and abuse the term (as happens to all language).
Ideally `#![require(nothreads)]` would affect the borrow checker behavior, so probably not I think?
But `std` is just another library which anyone can contribute to and could have the same issues as an external library. I'm not saying it doesn't make sense for companies to review their libs, but they should also be reviewing `std` too.
If I do cat /proc/sys/fs/file-max it outputs "18446744073709551615", so I guess not :/ Thanks for the answer!
After reading this, my first line of thought wasn't "Add this optimization to Rust!". It was "Oh my Jesus this is an insanely dirty hack that is a massive footgun just waiting to happen". This is going to blow up in somebody's face one day. I'm sure it already has. The circumstances may be niche since you need to avoid doing multithreading through pthreads, but this has probably already introduced several dangerous data races into production software. I am so glad Rust *does not* do stuff like this. And for what? A minor performance boost for those programs that are written by authors that use the wrong type? Perhaps I'm getting my Rustacean paranoid tin-foil hat on now, but this doesn't seem worth it if the price is likely to be "introducing undefined behaviour into some production codebases".
Yes. But I'm in a small minority for good reasons.
ulimit -Hn =&gt; 524288 and ulimit -Sn 1024 You think that might cause it?
Yes absolutely, signals and interrupts interrupt the main thread but they don't create new threads in the process, the signal/interrupt handler run in the main thread of execution. It's more like "Oh main thread, you're currently computing a factorial? Well now you're going to suspend doing that and instead handle this signal/interrupt and when you're done handling it you can go back and continue doing what you were doing before (or terminate)."
[`RefCell::as_ptr`](https://doc.rust-lang.org/std/cell/struct.RefCell.html#method.as_ptr) doesn't touch the reference count/lock. `&lt;RefCell&lt;_&gt; as Neutralize&gt;` uses `RefCell::as_ptr` to treat it as a `&amp;_`. This doesn't touch the non-thread-safe data. How does that make it unsound?
Honestly while it is useful to have a random number generator in the standard library, that's a really difficult thing to do because inevitably people will use it incorrectly, and the use cases for a random number generator are usually such that fixing a bug would actually break people's programs. Go decided to include a non-cryptographic RNG and it has led to a lot of wasted time due to people using it incorrectly.
&gt; A simple RNG like you would like would probably work for tiny programs like the guessing game from the book, but you wouldn't want to use it for something serious. And if so, why bake it in the standard library? Asked and answered? Most uses of PRNGs aren't "serious" in my experience, and games are a really common consumer. Forcing somebody building a tiny game to buy into the complex and expensive `rand` crate architecture to roll some dice is a bit antithetical to the Rust new-user experience in my humble opinion. I've got an initial oxidization of my [toyrand](http://github.com/BartMassey/toyrand) C library mostly done, but I'm having some last-minute issues I don't understand. I'm planning to post the Rust in the next few days. Maybe this kind of thing is the right compromise, I don't know.
Wow. Didn't know I could do that. What does the &amp;'_ part do? I know ' is for lifetime specification - but I've only so far dealt with it in functions with input references/borrows.
I mean... *technically* it's still a compile-time hack since most decent compilers would inline the weak reference branch and just go straight for the thread-unsafe version of the code. But it's still a really crappy thing to do for a lot of other reasons.
Well right, but you are going to have the standard library most likely as an already vetted, accepted library. It's already reviewed and approved. No need to wait for your architects to check it out. No waiting.
Wow. Learned a ton.. Thanks
I have a [hardware RNG](https://altusmetrum.org/ChaosKey/) too ‚Äî I even helped design and validate it. That said, I would be fine with having a Mersenne Twister or somesuch sitting in std: it's not like it's going to bother me when I'm not using it. Mostly I use PRNGs for games and graphics: fast is way more important than perfect or secure.
There are actually three versions of Appel's book. We were using Java version which I liked, there's also C version. The nice thing abou the tiger books is that they are very approachable and a la carte structured with more advanced topics stored in the second half. 
But doesn't this then break the assumptions that the shared pointer is making ? Couldn't there be an edge case where the main thread gets interrupted when trying to take a reference to the shared pointer, but gets interrupted in the process, by say a signal handler that drops the last reference to the same pointer ?
This comment is really exceptional.
I have a project for which I'd like to build both a console binary *and* a Windows DLL. It's easy enough to isolate the core logic to one library crate; can I put that into a Cargo workspace and then set up a binary crate and a library crate set to output a `cdynlib`, and then expect `cargo build all` to output both the binary and the DLL? Alternatively, is there a more Rust-ic way to do this? Maybe a way to configure my crate so that a flag or something on the `cargo build` command will control whether it outputs a binary or a DLL?
You could always just roll your own rng if you only need it for a dice: time * constant mod 6
&gt; Most uses of PRNGs aren't "serious" in my experience, and games are a really common consumer. I don't know. Someone will want a random value in `[0; 1)` (or `[0; 1]`?), someone else will want a random integer, and they'd better not use the popular `rand() % 100` approach. Yet somebody else will need a salt for their password hash. One might want a random number once in a blue moon, someone will need a billion of them in a loop. It's hard to solve all these use cases.
You want `--package`
At first approximation dangling pointers and null dereferences exceptions manifest similarly to end users so you could group them togeter in some contexts. To add to his answer, anoter advantage is speed and no ConcurrentModificationExceptions that no mainstream language not in pure functinal category will be able to prevent (because you either need immutable data structures or track lifetimes to do that across threads). 
&gt; Many people don't realize how expensive atomic operations are on a modern CPU: they can be 100x slower real easy. It's a big exception to the one instruction per cycle rule of thumb Where did you get the 100x figure and what atomic ops are you talking about? I don't think I've observed anything close to that with cas or fetch-and-add.
rand deserves a cryptographically secure 0-ary function in every standard lib just to trim down on the vulnerable brogrammer apps out there.
It's not any more completely unsafe than array indexing. They could have added it, but didn't because they thought people would misuse it. 
&gt; games Often need a random that isn't 'standard' at all. Biased to make players not go 'it's not fair!'; biased to make your gambling scam product make the house always win; recording to make 'rewind' and 'replay' possible, etc.
&gt; As for the criticism on size/alignment, I am of two minds. Deallocating from just a pointer is simpler on the user, and avoids storing size/alignment with each pointer (potentially), whereas deallocating from the triplet could potentially allow faster deallocation but at the cost of inefficiencies elsewhere. That was also what I was trying to say. Having a pointer obviously provides storage improvements for the consumer of the allocator, while having more data then pointer and potentially *even more* than size/alignment makes allocator patterns possible that would require incredible hacks at first. For example image a combined allocator that multiplexes to two internal ones. If the deallocation method receives just the layout (pointer/size/align), there is not intrinsic way for it to guess which of the two internal ones to call. So, it either has to provide a unique tag in the allocation response and rely on receiving it back at deallocation, or the internal allocators have to support some magically made-up-on-the-spot `is_allocated_by(&amp;self, *const[u8])` which may be hard to implement on its own and can be arbitrarily costly etc.
Just for fun, [this is what](https://gist.github.com/Akira13641/e770be6501654a05acf02be5875f1776) `FrameBuffer.zig`, the file with the quoted "fonts" line, actually looks like rewritten in Free Pascal-compatible Pascal (with bits and pieces from other files in the codebase pulled in for the sake of coherence.) [The original Zig](https://github.com/sjdh02/trOS/blob/master/src/vga/framebuffer.zig), comparison.
Reliability is just another quality of the product. Yes, it is important - very important! - but the ideal reliability is not "infinite" because that would take an infinite amount of resources and you don't have infinite resources. NASA is willing to pay for all that reliability, because they really need it - if a mistake is discovered five years into the mission they can't send someone to fix it, and even a software patch is not that simple with the distances they need to handle. But for normal projects? That kind of reliability does not justify its cost. The benefit of technology is not just enabling things, but also lowering their cost to the point it makes sense to use them. We had books before the invention of the printing press, but they were too expansive to be widely used. Making them cheap allowed everyone to use them. So yes - it may be possible, maybe with NASA rules, to achieve that kind of reliability with C. But you need Rust to be able to afford it.
I agree with you. Having to help someone learn python (even if I don't know myself) I can say that lack of typing didn't really help in trying to explain why some expressions wouldn't work. Unless you just have a problem that you want to be done with as quickly as as possible (that's how I'd imagine most non-cs people will use python) then there's use in learning languages that make "behind the scenes" working more explicit. If he's set on pivoting to Rust then Java, espectially parts before Java 8, would be better as it's much smaller than C# but at higher level works very similarly. Othewrise it obviously depends what he/she might want to use it for besides as a jumping off point. 
Ideally you'd be able to replace it on other crates when you pull it like the allocator. In practice, it's probably a complex way of shooting yourself in the foot from unmet requirements, so i understand why not.
This is something I'd be interested in, as I'm one of the people who posted a WIP that I'm working on. My day job's in data engineering, and I am trying to use Rust where it makes sense. I think a few of us can agree that a dataframe library is too large a project for one person, so a working group sounds great! I recently started contributing to Apache Arrow's Rust implementation, and I'm using it in my dataframe library. Getting more people contributing to Arrow would also be a net positive as scientific computing libraries could leverage this work in future. Perhaps we could hop on a shared Google Doc and come up with ideas and possible things to explore, before setting up something concrete. I find having clear initial directions more welcoming to potential contributors and co-conspirators.
`impl Trait for &amp;'_ Type {}` is a shorthand for `impl&lt;'a&gt; Trait for &amp;'a Type {}`
Didn't know. Thanks
Kudos for the effort, quite nice.
Your scenario is only possible if pointer itself (as opposed to the pointee) is accessible to the main thread and to the signal handler, for example if the pointer is a global variable. But in that case the C++ standard provides no guarantee that it should work in the first place, with or without atomic reference counting. However, if the pointee is shared and the pointers are not (two ```shared_ptr``` objects referencing the same object), then your scenario is not possible to begin with because the reference count is guaranteed to always be at least 1.
So, in this case, it'd be like the story for replacing the allocator?
\&gt; All he did was choose a language which made him feel better about doing the same mistake-prone thing [https://andrewkelley.me/post/unsafe-zig-safer-than-unsafe-rust.html](https://andrewkelley.me/post/unsafe-zig-safer-than-unsafe-rust.html)
I agree with this idea in principle but in practice I find I will usually make more mistakes if the error-prone thing I‚Äôm trying to do is further convoluted by the language. I have the same issue with reflection and unsafe in Go
`"somestr".repeat(n)` would probably be more idiomatic. You could even do it once, at the beginning, to create the long line's output, and then slice the String to print the shorter segments. That would make it pretty easy to have just one `println!`
Would the thread count not need to be atomic, thus cancelling out the benefit?
More like this: * `[u8; 4]` : 4 bytes on the stack * `&amp;[u8; 4]` : pointer on the stack pointing to 4 bytes on the heap * `&amp;[u8]` : pointer and a `usize` on the stack, the pointer to some (not statically-known) number of bytes on the heap (that length is in the `usize`) So `[u8; 4]` and `[u8; 100]` are distinct types with different sizes.
&gt;allocator that tags its outgoing pointer not by size nor alignment can not do this I'm not sure what this means, but if you're talking about having alignment in the resulting pointer type, you do get this from using the allocator interface: `fn alignedAlloc(self: *Allocator, comptime T: type, comptime alignment: u29, n: usize) ![]align(alignment) T` Note that the alignment value has to be a compile-time known value, and then it's part of the return type. There's no function for doing it with size, but it would be trivial to add: `fn allocArray(self: *Allocator, comptime T: type, comptime alignment: u29, comptime n: usize) !*align(alignment) [n] T`
You could have a match clause for the user input that catches the error if a non number value entered and repeats until valid.
I would avoid creating some mut integer and then updating it in a loop. For larger loops it becomes difficult to track down where it is actually updated. You can replace char_fill_count by 2*line+1. You can replace the while in repeat_char by `for _ in 0..times‚Äò.
Yup, hard to forget! Good times. I was delighted to know you work on Rust now. I do as well - [my fuzzing work](https://www.reddit.com/r/rust/comments/8zpp5f/) has received a lot of attention, and I should have two other nice security-related Rust projects published soon.
Are you sure `&amp;[u8; 4]` points to four bytes on the heap?
Or as we say in Rust-time: 2 or 3 releases.
&gt; and would allow the compiler to let you have static mutable global variables for your single-threaded programs We already have mutable globals and they already [don't require sync?](https://doc.rust-lang.org/stable/reference/items/static-items.html#mutable-statics)
&gt; Would this work or am I missing something? Threads can be created before a DLL is loaded, so `DLL_THREAD_ATTACH` wouldn't be called.
An atomic read is far cheaper than an atomic increment/decrement, so I believe it would come out ahead
/u/rabidferret, just wanted to add a [note that somebody from /r/cpp pointed out](https://www.reddit.com/r/cpp/comments/aq9duh/no_the_problem_isnt_bad_coders_sean_griffin_medium/egeqfw5/). From the OP: &gt;With a normal mutex we would be fine, since only one lock can exist and it doesn‚Äôt matter if we unlock it on a thread other than the one we locked it from. This is actually not something you should do, at least in C++. According to the [named requirements for Mutex](https://en.cppreference.com/w/cpp/named_req/Mutex): &gt;The expression `[mutex].unlock()` has the following properties &gt; &gt;.... &gt; &gt;The behavior is undefined if the calling thread does not own the mutex. This weakens the argument for C++ audiences specifically, for whom I assume this article is at least in part. I agree with your post general thrust, though, having experienced the same thing myself, and I hope that this might be helpful feedback
I made a playground with some suggestions : [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a887dc56c6a68e5e2cb03b4e75cd55f1](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a887dc56c6a68e5e2cb03b4e75cd55f1)
What I'm most curious about is the story with stdweb and wasm-bindgen. I've seen in some Github discussion that there's interest in writing stdweb on top of wasm-bindgen. Is there anyone working on that?
Concurrent, but not parallel, right? No atomic primitives are needed for just concurrent code, as far as I'm aware.
A foundational part of `stdweb` is its `js!` macro. The ground work for being able to build the `js!` macro on top of `wasm-bindgen` is happening in this RFC: https://github.com/rustwasm/rfcs/pull/6
I tried implementing something like this for a variant of the Reposotory Pattern in Rust using a Vec of enums indicating events/actions (and the needed data to execute the action), and then commit to the database using a transaction that can Err and roll back if needed. However, making it as generic and abstract as I've seen in C# is something I didn't solve (and didn't try to either) so there was more boilerplate, but I guess that could be solved using macros. I often find that I don't really need UoW, but it is sufficient to work with database transactions written in SQL for the critical parts that needs it. In general, I work more directly with SQL that I build for the database in Rust than I do in C#, and I don't really find that as big of a problem as i thought. I don't know if Diesel (the Rust ORM) can help with this, usually I've used Entity Framework to implement UoW in C#, but I'm not that familiar with Diesel. I would check it out at least. 
&gt; The only real downside I can see with that is in cross-crate APIs where you will tend to expose Arc instead of Rc in case the caller wants to use threads and so the extra performance is not possible without duplicating the API. Is there a crate out there with traits to be generic over Arc/Rc or otherwise offer optimized code for non-sync cases? I think such a library would be very nice to have. &gt; and thinking about it there's nothing actually stopping the rust Arc implementation from having the same optimization so you get the best of both worlds. Other than there not being a portable way to tell how many threads are alive (AFAIK). C++ may assume all threads are created from the standard library, but that breaks down once you start mixing languages. The same would be true of Rust.
*or she
Would it help if there was a set of libraries (like rand and regex perhapd) that were on an officially endorsed crate list. That way you could have your architects approve these at the same time as std. I proposed something like this in my Rust 2019 post.
Being able to replace other libraries random number generation sounds like a recipe for disaster... what if one dep needs a really secure random number, another needs a fast one, and another needs a biased one?
Thank you. I understand the changes except for the unwrap\_or. I know it is somehow taking a u32 integer and making it a usize but I don't understand the syntax of how yet. And how to break out the error handling on it as well. Pointers are always welcome!
It doesn't. It points to 4 bytes which may be either be on the stack or the heap. let foo_stack : [u8; 4] = [0x01, 0x01, 0x01, 0x01]; let foo_stack_ref : &amp;[u8; 4] = &amp;foo_stack; let foo_heap : Box&lt;[u8; 4]&gt; = Box::new([0x01, 0x01, 0x01, 0x01]); let foo_heap_ref : &amp;[u8; 4] = &amp;foo_heap; Here foo_stack_ref and foo_heap_ref both have type &amp;[u8; 4], but foo_stack_ref points to an array on the stack, and foo_heap_ref points to an array stores on the heap.
Thank you. 2 \* line +1 is a better simplification, you're right. Repeat character went away over the string repeat functionality.
Thank you! Repeat works fine, slices might be better though.
Thank you. I have a loop for that and one I figure out the syntax of handing the unwrap\_or result then that'll complete this beginner exercise.
If you're using the jemallocator crate as your global allocator, you can use https://docs.rs/jemalloc-ctl/0.2.0/jemalloc_ctl/ to get information about memory usage from it.
The same thing that the generated `Future` impl does, what ever that happens to be. While the syntax for creating a `Stream` may be a mix of the generator and async syntaxes, it won‚Äôt have anything directly to do with the `Generator` trait (by my understanding). The fun interaction between syntax-level generator arguments and async is in implementations of `Sink`, this is a topic that has really not been explored much since we don‚Äôt have generator arguments to try building prototypes of it yet. Although, I‚Äôm now realising that the workaround I used to implement an `async_block!` macro without TLS or generator arguments may work for this case as well.
These types of measurements aren't always useful, there's a lot of bias in a rewrite. Of course it's going to feel easier, you already know how to structure the project, which algorithms to use, what the corner cases are. I once heard a story from an older programmer (before VCS) who spent a month building a project as a consultant, only to have a head crash that destroyed the code. He then rewrote it in one night, because he had already learned all the lessons and knew what needed to be done. &amp;#x200B; Sure, Zig, might have been a better pick from the beginning. But this isn't proof that it is true.
&gt; While the syntax for creating a Stream may be a mix of the generator and async syntaxes, it won‚Äôt have anything directly to do with the Generator trait (by my understanding). Is that not what this post was about though? The stated use case that it focuses on is &gt; As a means of representing some sort of iterable source using imperative code. The epitomizing example is using a generator to make an Iterator; another similar example is using a generator to make an async Stream. For this use case, the user interacts directly with the generator language feature, and ideally there is as little library code as possible between them and just using the feature.
Well, not system wide though. Even that is risky (and probably inconvenient). If you're aware of the jargon, i was thinking more of a per-crate ServiceLoader/Dependency Injection replacement idea.
[https://stackoverflow.com/questions/2538070/atomic-operation-cost](https://stackoverflow.com/questions/2538070/atomic-operation-cost) &amp;#x200B; My understanding is that atomic cas can really mess with your cache, forcing memory fetches. This is more of an issue under contention. 100x seems like a reasonable statement.
Sorry, *safe* static mutable global variables for single-threaded programs.
Neat. A lot more complete than the wonky little [diffparser](https://github.com/Freaky/rust-diffparser) I wrote over Christmas. `Line::Add(&amp;'a str)` bothers me a bit &amp;mdash; nothing about unified diffs scream "Unicode" to me, I'd much prefer `&amp;[u8]`. Same with filenames &amp;mdash; I used `PathBuf`, though it's a bit awkward on Windows. It's tempting to just leave that as bytes too, let the consumer decide what to do with it. I'd also really, *really* want a stream parser. I don't want to be slurping a whole 3GB diff into memory all at once just to produce a diffstat.
Cool. BTW, i've been looking for a solution to binary diffing that adapts to problem of single file divided into multiple files (but slightly different) for a long long while. The classic case is a cue/bin divided into a cue/bins (multiple tracks). I feel like byte array matching algorithms in order to match parts of the 'original' to a sequence of the 'final' set could get much better results than stuff that just assumes 1 input file == 1 output file, even if the files are stuffed in a uncompressed zip first.
From memory, talking to a friend of mine who studied this stuff. This SO [answer](https://stackoverflow.com/a/16132551) has a 10-year-old microbenchmark that seems to say something similar. It's possible things have changed in the last decade: I'll ask my other friend at Intel who builds simulators for these things. I mean, there's a lot of different figures you can use with some justification here. 100x was OOM for cache hit on ordinary access (3 cycles) vs synchronized cache hit on `FAA`. And again, very OOM, not a study.
Oh my bad. Good point.
&gt;Type hints are now a thing. Sure, the language doesn't force you to write them, but there are plenty of ways to choose to make your functioning code look bad (actually fewer in python than in many other languages) Sure, however when teaching beginners I would prefer not to deviate too far away from textbooks or online resources which do no use type hints. I'd say that I do like that there is indentation consistency required and that style has never been a problem with python but the transformation of data types, globals and variable hoisting are components of the language that result in some unnecessarily complicated and messy code. &amp;#x200B; &gt;Eh, the same is *certainly* true for a number of rust's constructs. I agree! However the compiler does go out of its way to break down these constructs for you when you written them incorrectly and provides feedback for the learner. I'm not advocating Rust as a beginner language btw, without a large amount of persistence you will fall into the trap of not understanding why the compiler rejects your source and possibly become very frustrated with it. Other (mostly compiled and typed) languages provide fairly accurate hints as to why your program was rejected and explain why you have written something incorrectly. I'd say this is why I think Rust's compiler error messages and explanation are fantastic. However, Let's take the following code: def f(x): x + 1 y = f(1) print(y) Python will consider this valid but considering there is no return instruction in f, y will be None. A beginner can be confused why assignment occurs to a function that returns nothing, however this is if they can figure out that an assignment error has occurred, this is harder to trace back when the problem cascades through multiple lines. &amp;#x200B; &gt;On the other hand, you rarely need that, and once you do, it's very easy to understand and replicate. Creating your own iterator or context handler is not exactly trivial for a beginner, but you are right, beginners do not necessarily need know that. However the for loop can obscure how python is iterating through each element in a list. You do not see how it is changing the index to retrieve the next element, it is just accepted that on each iteration you are given the next element in the list type. Sure you can hand-wave the the fact it is using an iterator and just accept you get another element or you can explain what is happening.
If you get started I want to encourage you to check out a few things I‚Äôve been working on. I recently published a few crates for parsing JS https://github.com/Freemasen/ress https://github.com/Freemasen/ressa https://github.com/Freemasen/resw As an experiment I have started working on a project that will convert the Rust ast defined by `syn` into the ast defined by `ressa`. https://github.com/freemasen/jsyn Once the ast was converted it could be passed of to `resw` to take care of the writing. 
I got your comment on YouTube but apparently the YouTube filter filtered it out as spam because of the link. The issues you are running into here are a result of the application being depreciated in many ways. I made this video about 9 months ago and the APIs for most of the libraries used have changed in various ways. I will look into the code and see if I can bring it up to date. Sorry for the inconvenience and hopefully this doesn't put you off on my tutorials. 
I actually find list comprehensions significantly clearer than the alternate construct, which i had trouble comprehending for the damned longest time. 'So these stacked fors are equivalent of the 'Cartesian product'? What's a 'Cartesian product' anyway? When i started writing python i greatly appreciated how the language pushed me to use list comprehensions to make those cartesian products 1 or two lines of immutable transformations, which a nice way of not getting 'intimidated' and focusing on the important parts.
*Hey just noticed..* It's your **2nd Cakeday** greppable777! ^(hug)
On linux the [prometheus](https://crates.io/crates/prometheus), which is used for providing metrics to a prometheus scrapper. Can get's memory usage when the process feature is enabled. It uses the [procinfo](https://crates.io/crates/procinfo) to parse the /proc/&lt;PID&gt;/statm
Cloning could be problematic, but if he is passing ownership into the function then that would be fine. 
I edited the original post with a revised listing. It now has error handling and that user-input function I can see myself reusing as I'm learning. Thanks again, your reply was the one that helped the most.
I would like commercial efforts to be filtered from the calls - who this benefits than the particular company? And it's all busy house-cleaning work that the developers don't want to do themselves. Call for Participation TiKV: Convert trait objects to dyn syntax for Rust 2018. TiKV: Remove all the extern crates for Rust 2018. TiKV: Add tcmalloc support to the tikv_alloc crate.
Yes.
Yes, for gods sake. Multiple times in recent history I've done extern "C" { fn random() -&gt; u64; } unsafe{ random() } to get random ints for debugging/testing purposes. It doesn't need to have all the distributions the rand crate has (which are quite convenient when simulating more complex things), but the ability to generate uniformly random bools/ints/floats should be included.
So here's a fairly stupid quick test running on my 5 year old Intel whatever laptop: import Control.Concurrent import Data.Atomics import Data.Primitive.ByteArray main = do -- an unboxed reference v &lt;- newPinnedByteArray 1 writeByteArray v 0 (0::Int) let cas = casByteArrayInt v 0 end &lt;- newEmptyMVar let count = 1000*1000*1000 job :: Int -&gt; IO () job expecting | expecting &gt; count = putMVar end expecting -- end main thread | otherwise = do -- NORMAL READS AND WRITES -- was &lt;- readByteArray v 0 if expecting == was then writeByteArray v 0 (expecting+1) &gt;&gt; job (expecting+2) else job expecting -- ATOMIC WITH CAS -- -- was &lt;- cas expecting (expecting+1) -- if expecting == was -- then job (expecting+2) -- else job expecting -- busy wait for other thread to cas -- fork two threads which will alternate cas and busy-wait, taking turns -- incrementing: mapM_ forkIO [job 0, job 1] -- wait for a thread to finish takeMVar end &gt;&gt;= print Runs for the normal and atomic (CAS) versions, respectively: Performance counter stats for './main +RTS -N2': 235,181,919,510 cycles 407,463,843,105 instructions # 1.73 insn per cycle 39.742654513 seconds time elapsed Performance counter stats for './main +RTS -N2': 137,038,913,784 cycles 20,135,002,902 instructions # 0.15 insn per cycle 70.433975626 seconds time elapsed So I'm not really sure what you want to conclude from this test. The non-atomic version is not quite twice as fast, the insn/cycle is 10x more productive (though it was doing more busy-spinning; less green!). In both cases I think we're benchmarking cache coherence traffic. I don't think the 100x rule of thumb makes sense at least for my processor though. I should have tested fethc-and-add while I was at it... it behaves better w/rt stalls in my experience.
Do I think it would be nice? Yes. Do I think the rand crate in its current state is ready to be adopted by std? Nope. Haskell added a random API to the standard way back in the 90s and now the standard library provides a random interface that prevents you from writing certain types of RNGs. So based on that experience I think it's good to let the APIs mature before adopting them into the language standard. Now, there is a third option here that we might consider. Having a set of crates that are community backed and tested together. Somewhere between "just a random crate on crates.io" and "part of `std`".
Yeah, I just talked to my processor guy. We had a conversation, and I wrote my own wall-clock single thread refcount increment [benchmark](https://gist.github.com/BartMassey/d82d57bbb6213b6d7e41c2231061fbb2), which agree with his estimates. On my Haswell box it looks like about 6 cycles for an unlocked RMW add and 18 for locked RMW add with everything presumably in L1. Still worth avoiding, but not the dramatic number I remembered. I'll update my post. Of course, if you miss cache things get really gross really fast, since you invoke the whole cache coherency protocol. Hopefully this won't happen almost ever in practice for any kind of active lock. My friend said "i mean, words really cannot express how bad a page that crosses two memory types would be in a multi-socket system." That cracked me up, but is not a thing that's going to be happening here. Thanks! Sorry for the temporary misinformation.
Unfortunately, many-to-many diffing is Difficult, especially in the case where pieces on one side can be duplicated or de-duplicated anywhere on the other. Somehow you need to map all the source files into a flat namespace, so a target file can reference any part of any source file, and that's morally equivalent to concatenation.
&gt;Between enums, traits and ownership, Rust allows you to specify the invariants of an interface more precisely than any other language Maybe you're engaging in a purposeful bit of hyperbole. Maybe not. Note that I think Rust is cool. Regardless, there are a number of languages that are superior to Rust in the ability to 'specify the invariants of an interface'. For example, [Idris](https://www.idris-lang.org/). Idris has [uniqueness types](http://docs.idris-lang.org/en/latest/reference/uniqueness-types.html), like Rust. Idris is dependently typed, unlike Rust, which allows you to specify essentially any invariant you like in the type system. E.g. you can [describe a state machine](http://docs.idris-lang.org/en/latest/st/machines.html), and the valid transitions between states, in the type system. Then, any implementation of that state machine will fail to type check if it does not validly implement the states and transitions described. It's pretty cool. &amp;#x200B;
`cargo_root_dir&gt; cargo run --features feature_name --package package_name --bin bin_name` doesn't seem to work, is that what you meant?
One thing I liked about XNA was how it was more of a framework that gave you a window, drawing surface, inputs, and a math library, and then mostly got out of your way when it came to actually making the game. Of course there will always be demand for editors for quick game design and building, but at the start I see Rust games being written by tying together the good libraries that are out there: gfx-hal (maybe now with rendy?), ncollide, specs, and others. It's so much easier to get going with a library compared to C++, and similarly it's easier to get your code running cross-platform.
I don't mind the order being the responsibility of the user (after all, in the cue example, it's obviously the order of the tracks, in other examples the user can simply 'use their brain'). So basically i'm not looking for a 'now we must solve the packing problem for files' (even if it's solvable), at least not at that level. I'm looking for 'ok this ordered set (or one) file is 'related' to this ordered set (or one) file and i want to create a binary patch that minimizes differences'. It's the 'align the freaking byte strings so that insertions and removals gets ignored' that grinds my gears in the naive use of xdelta for instance.
From a security perspective, is undefined behaviour sufficient though? An optimising compiler could optimize away the read (or do all sorts of horrible things), but a naive one might allow the bytes to be read...
Sooo much writing and not even a sentence fragment describing what Clever-Commit does. It just absolutely stupefies me the way blogs are run sometimes.
I used to think it should be there, but the more I see performance and/or security issues around random number generation the more I think the choice should be on the end developer. By not having a standard rand the onus of understanding rand falls to the one using it.
Contributions are welcome!
I think I found some actual technical details behind this: https://github.com/PapersOfMathieuNls/clever-commit/blob/master/combined.pdf however, still seems like quite a stretch to consider this relevant to /r/rust.
I tend to split my binaries into separate crates, because I want them to have extra dependencies like CLI parsing stuff that shouldn't apply to the lib. But I don't know if there's a "most standard" way .
Maybe things changed as Rust grew up. I remember when any software tool (like Clever-Commit here) supporting Rust (say, Notepad++) was newsworthy. If supporting Rust is no more newsworthy, that's a great sign indeed.
It's an Apache-licensed distributed database. I personally don't think some of the developers getting paid takes away that value.
 I am in my bachelor second semester. So far we had to learn C/C++. I personally felt the [https://www.learncpp.com/](https://www.learncpp.com/) worked better for me to learn C++. I learned namespace, virtual functions etc. for the first time (coz first lang was C) and I really felt the power of control over machines. But soon, I really got frustrated forlearning what not to do rather than learning something about really cool. I think Rust will be best fit for me as I am from C/C++ background. I really want to write good and efficient code in Rust at least for 2 years from now. But I think the rust-programming-book is not suiting for me . If there was a website on Rust like [learncpp.com](https://learncpp.com) , I think noobs like me would get better benefit. I have a little knowledge on data-structures. PLEASE , SHOW ME A WAY TO LEARN basic COMPUTER SCIENCE WITH #rust . Give me advice how I will learn inside the machine with rust. Thanks. :)
Does xdelta *actually* produce a hundred-megabyte patch when removing parity information from a CD image? It's been a while since I played with it, but my impression was that it was a lot smarter than just aligning the start points of the two files and just comparing each byte. Of course, I was playing with relatively small (dozens-of-megabytes) files, maybe xdelta switches to a less intelligent algorithm for hundreds-of-megabyte files. As far as I know, the most scalable way to do this kind of binary diffing is by building a [suffix array](https://github.com/BurntSushi/suffix) of the source file, and using it to look up each chunk of the target. I believe [Flips](https://github.com/Alcaro/Flips) works like this when creating a BPS file, so you might give that a go. Flips uses [libdivsufsort](https://github.com/y-256/libdivsufsort), which builds a suffix array in O(n log n), while the `suffix` crate should work in O(n), so even if Flips is too slow for your use case, faster speeds are theoretically possible. Tools like bsdiff2 are optimised for executables, but still very powerful for arbitrary files. It's a pretty complex algorithm, though; I've never heard of a second, independent implementation.
[removed]
And Crystal and Swift and D and SAC... Would love to see usage converge on just one systems language :P
I'm working on this: ``` #[shell] fn list_modified(dir: &amp;str) -&gt; Result&lt;impl Iterator&lt;Item=String&gt;&gt; { r#" cd $DIR git status | grep '^\s*modified:' | awk '{print $2}' #" } ``` It is an attribute-like proc macro to execute external command and wrap it in strongly typed rust function. Does it look useful?
Looking for a manageable way to use futures and streams in my rust project. Ideally using awake and async but I don't know if that is possible to do or where to get good info on ways to use futures.
Isn't that basically what rust-lang-nursery is? 
&gt; are milliseconds really making all execution times _a lot longer When you‚Äôre trying to run an interactive application at 60, 90, or 144 Hz a GC pause on the order of milliseconds is way yonder too long.
You should look into implicits in Scala. Basically, you can mark function arguments as coming from the current scope, and hence acts to make dependency injection really really common.
That's why I said "jokingly" and `} ... safe {`. It'd be near impossible to write a parser for, but, if it could be done, it'd be like the inside-out house in the Hitchhiker's Guide series which implies that all the rest of the universe is the inside of an asylum.
Yeah that's what I meant. Oh my bad, just assumed it'd work without testing. Unfortunate it's nightly-only
I don't dispute that and do wish unsafe Rust was smarter. My point is that manipulating pointers is an *inherently* unsafe operation, just like cutting down a tree. You can build safer tools and design safer procedures but, at the end of the day, it's still your job to cut in such a way that it doesn't fall on something important.
I agree that manipulating raw pointers is inherently unsafe. But given the new evidence I provided, your quote is not a fair characterization of the situation, given that the language change made it less mistake-prone.
You wouldn't get 22K with jemalloc - it's a couple hundreds of kilos. Plus jemalloc is not available on platform author is developing. 
What is the best way to compare a `Option&lt;(String, String)&gt;` with an `Option&lt;(&amp;String, &amp;String)&gt;` by having rust coerce it properly? The best working solution I have been able to figure out is the following, but it horrifies me. let (x, y) = (String::new(), String::new()); let a: Option&lt;(&amp;String, &amp;String)&gt; = Some((&amp;x, &amp;y)); let b: Option&lt;(String, String)&gt; = Some((String::new(), String::new())); let cmp = a == match &amp;b { Some((b0, b1)) =&gt; Some((b0, b1)), None =&gt; None }; It just seems like there should be a better way than this.
Sounds like something a shadow DOM does, keep track of the changes and then commit the changes to the real DOM. I'm pretty sure there are some Shadow DOM implementations in Rust front end frameworks. Maybe get some inspirations out of those. For the rest im not that familiar with the Unit of Work pattern.
Thanks. I'll be busy for the next few days, so I'm not sure when I'll have time to take a proper look at those, but it definitely looks like the kind of stuff where, if the tasks your codebase performs are mostly comprised of it, there's not much benefit to using a "safe" language.
Firefox is all about hype now. They need to install ublock origin by default if they cared about privacy.
Amazing. I really need this sometimes and installing node with all it's deps is a pain. As for me, the really important featureas are screenshots of the page and browser extension preload.
Thanks! I've created a couple of issues. Feel free to subscribe to them! &amp;#x200B; [https://github.com/atroche/rust-headless-chrome/issues/1](https://github.com/atroche/rust-headless-chrome/issues/1) [https://github.com/atroche/rust-headless-chrome/issues/2](https://github.com/atroche/rust-headless-chrome/issues/2)
Had the post pointed out that Zig has stronger safety guarantees in that context, I would agree with you. Even if it had just said that "This task is inherently `unsafe` and having to throw the keyword around constantly increases the chances of error because it clutters up the code.", I'd agree. ...but the passage in question is talking about "this icky feeling of using the `unsafe` block in rust".
Looks great! Is this multi-threaded by default? I've been tinkering with an ML experiment that requires driving lots of tabs and Puppeteer just doesn't cut it.
Agree with the general sentiment that `rand` architecture feels complex. I regularly need "toy" random numbers (to simulate user's input, when doing some graphics stuff, when I need to [make a decision](https://www.youtube.com/watch?v=T1UAOv4qhII)). In Python I just `import random; random.randrange(100)`, in Rust it takes me quite some time to figure out how to do things, "unusual" bounds like `where Standard: Distribution&lt;T&gt;` are super cute, but make me pause when reading the code (I think `rand` would make for a good homework about traits?). I understand that this complexity is there for a reason, and I appreciate that, when I needed a random point on the sphere, I was able to just get it. However I do wish there was a `simple_rand` crate which exposed a mostly trait-less API which doesn't need a prelude. It's interesting that I get similar feelings when using C++'s `&lt;random&gt;`.
- it might tempt users to write platform-dependent code - will you try to parse the result? - are you planning to invoke the shell, or build the pipeline in Rust code? If it's using the shell, a proc macro might not bring too much to table
[rust-learning](https://github.com/ctjhoa/rust-learning/blob/master/README.md) has a huge list of various learning materials including exercises. Also once you got the basics, look for a project to join. Many have easy mentored issues. https://this-week-in-rust.org has a weekly list.
Diesel has separate structs to represent changes to a database row so I'd expect it would work well with this pattern
Yes, there are a few types of programs that don't play nicely with GC - but author seems to imply that GC is always bad for every kind of application.
Yeah, each tab and browser does JSON (de)serialization of method calls / responses and events in its own thread. &amp;#x200B; Each instance of \`Browser\` has a single WebSocket connection that all tabs share, but it's literally just writing and reading strings and passing them over channels to the relevant tab / browser to handle. &amp;#x200B; From my experience with Puppeteer, it's the (de)serialization where the bottleneck is.
Global mutable variables aren't safe in single threaded rust either due to reentrancy.
This reeeeeally depends on what's going on around the atomic op. What often happens is that it messes up your cache and now you have some memory loads which miss L2 cache and that's kinda slow. (I don't quite recall if it's the rc atomic cas that can cause this)
[`unwrap_or`](https://doc.rust-lang.org/std/result/enum.Result.html#method.unwrap_or) allow you to get the `Ok` value inside a `Result`, or get the provided value. value.trim().parse().unwrap_or(12) is equivalent to match value.trim().parse() { Ok(val) =&gt; val, Err(_) =&gt; 12, } `unwrap_or` doesn't take a u32, it takes the same type as the `Ok` variant, here a usize. Literal integers in Rust doesn't have a type, it's inferred from the context. If you want to give a type to a literal, use `12u32`
1) I think it is not a solution for hardcore production use. It is not fast, it is not covering every failure case and in its simplest form, it may panic. 2) Yes! I currently support 11 variants of result type - from `()` to `T where T: FromStr`, to `impl Iterator` and everything can be wrapped in Result or, in some cases, errors can be ignored 3) I'm invoking the shell with the option to change the interpreted (e.g. `#[shell(cmd = "python -c"`). It doesn't do a lot of magic - just setting args, envs, performing std::command::Command and parsing the result.
I think the relevance could be increased if the information would be provided in the form of "X that does Y support Rust now" instead of "F uses X".
Can you please explain in more detail? I'm obviously confused somehow.
That's the least charitable interpretation of his words you could have made.
Right now it's not possible to properly use allocations in no_std on stable. So having at least something is better than nothing. Can't wait the day when Box&lt;&gt; becomes less magical.
I like that approach and hope it will be possible to use on stable soon :)
How are you using random numbers?
For this feature we even landed a stabilization in Nightly last year, then reverted it before of bugs around never type coercion‚Ä¶
Or you know, maybe if the fucking post had anything about Rust, or talked about what Clever-Commit is, or even just barely mentioned that Clever-Commit is in rust. &amp;#x200B; Or god forbid even LINKED somewhere that mentioned Clever-Commit was written in Rust. I mean, FFS, I tried and honestly had no idea why this was posted here.
The never type is not exactly doomed, but a new set of issues to resolve seems to pop up every time I get my hopes up about its stabilization being very close. As I [commented in the PR](https://github.com/rust-lang/rust/pull/58302#issuecomment-463285061): &gt; `TryFrom` was overdue when we first tried to stabilize it a year ago. There was ‚Äú[not much needed to unblock this one](https://github.com/rust-lang/rust/issues/33417#issuecomment-325797552)‚Äù 18 months ago. I personally feel we‚Äôve done enough of waiting just a little bit longer.
If author didn't want me to interpret those words in such way, he shouldn't have left room for such possibility. If, for instance, author meant `you cannot write RTOS in GC-ed language`, he should have wrote that - not `GC makes executions time a lot longer`, which is simply not true and should be avoided in _technical articles_ meant for _technical readers_. Overall all I'm saying is that this article contains a few mistakes (which I pointed out) and that it'd be worth amending it so that it does not incorporate any easily mis-interpretable phrases. It's really no big deal - I'm not saying that author should drop writing and perform a harakiri.
Oh damn, this is right up my alley. Are you looking for contributors? Also there might be some reusable stuff here: https://github.com/devtools-html/rust-cdp
As far as I know, Clever-Commit is not written in Rust. It just supports codes written in Rust.
Hey, yes I'm definitely looking for contributors :) I also have plenty of spare time at the moment to answer questions, review PRs, chat on IRC, etc., because I'm taking a little break from paid work (and this crate is underpinning a side project I'm working on ). Interested in either of the two issues on GitHub?
P.S. you can capture screenshots at the moment by doing something like \`\`\`let img\_data = tab.call\_method(CaptureScreenshot { format: "png" })?.data;\`\`\`. &amp;#x200B; It just isn't implemented as a method on Tab. See also [https://github.com/atroche/rust-headless-chrome/blob/master/src/cdtp/page.rs#L68-L81](https://github.com/atroche/rust-headless-chrome/blob/master/src/cdtp/page.rs#L68-L81) for details.
Dude. This rocks. I am hardcore with testing. Rust seems to be getting a lotta test love. Props. 
I am more in sync with jackmott2, however I am not speaking against Rust't adoption, rather that such kind of middleware is really critical for Rust to win over many studios, now that middleware has finally become mainstream. And it doesn't need to be done everything from scratch, being able to create Unity/Unreal plugins in Rust in some ergonomic way, could already be a path into that direction.
Please don‚Äôt put Mersenne Twister into `std`. It‚Äôs not even a good PRNG. 99.9% of the time you either want a CSPRNG or you don‚Äôt care whether or not it‚Äôs a CSPRNG and so using one is just fine. Only in a small percentage of use-cases do you generally care about either replayabity or absolutely maximal throughput (without needing cryptographic security). On the other hand, number of times something like MT gets misused in cryptographic contexts is absurdly high. So if there‚Äôs ever an official `rand` in `std`, it should be safe by default. `libc rand` is one of the most widely-misused functions in the entire C library. Please let‚Äôs not repeat that same mistake. 
While I appreciate your enthusiasm and the work you've done, I really dislike it when people put documentation as a todo, because I see so many crates that have been years in the making and still have documentation as a "todo." It isn't the most enjoyable part, but it takes perhaps an hour at this size of crate to document properly (or less, to be honest).
One example is that you can't borrow mutable statics safely across function calls (which would make them pretty much useless): static mut FOO: Option&lt;String&gt; = None; fn evil() { FOO = None; } fn main() { FOO = Some(String::from("foo")); let foo = FOO.as_ref().unwrap(); evil(); println!("{}", foo); }
I'm not arguing against types. Types are good. While I'm mainly a python developer, much of my new work is checked with mypy, and I'd prefer if even more could be checked in a nice way. What I'm arguing is that automated tests are also needed to verify business logic. Does the save function save valid data? Can the load function handle all supported legacy formats? What is the output of the format function? Of the examples you provided I'd prefer to work with the python one, but mostly because I still find string handling in Rust to be very fiddly. :^)
False dichotomy: types + tests, not either types or tests. But with types, you'll have proof of some invariants, so you don't need additional tests to check them. I also worked in Python for some time, and I still like it for smallish scripts, but I no longer want to work on more sizable code bars with it.
From my understanding, the main contention point around `freeze` is that information could leak without causing UB. Programs that cause UB leaking information is, I think, a lost cause. But usually such programs have way bigger problems, such as remote code execution vulnerabilities. If you want guarantees for anything, whether it is information loss or not being exploitable, you have to first make sure you have no UB.
You are amazing. Finally I can throw out nodejs in my work :)
I always thought Rust stdio is a wrapper around the OS I/O syscalls (e.g. open(), read(), write(), lseek() etc. on POSIX), and not a wrapper around C stdio (fopen(), fwrite(), fseek(), etc.). Thus, any buffering would have to be implemented in the Rust libstd, and can't reuse the buffering implementation in C standard library.
TIL that Rust crates can be published without documentation.
docs.rs exists automatically, but some effort should be spent to actually enrich those documents. As a bare minimum, examples that demonstrate every feature should be in existence. 
For everybody who hasn't used rand in a long time, and to give some perspective. You can simply do this for your basic random requirements. use rand::prelude::*; let x : char = random(); let y = random::&lt;f64&gt;(); if random() { ...} else { ... } --- P.S. if you install cargo-edit, you don't even have to open up Cargo.toml. You can just 'cargo add rand' and use the above code.
How does this compare to [fantoccini](https://github.com/jonhoo/fantoccini)?
&gt; At first approximation dangling pointers and null dereferences exceptions manifest similarly to end users One is a possible security issue, the other is an annoyance. &gt; anoter advantage is speed Compared to? &gt; that no mainstream language not in pure functinal category This isn't true. D allows users to pass either immutable or shared (part of the type system) data to other threads and isn't a pure functional language. Pony is imperative and has a type system that statically prevents deadlocks from existing (there are no locks!). My first and only Rust program contained a deadlock in it. I'd never written one before in any other language. 
I published [lyon 0.13.0](https://docs.rs/lyon/0.13.0/lyon/). Lyon is a set of tools to manipulate and tessellate paths, useful to render vector graphics on the GPU. The new version comes with it's usual share of bug fixes and a whole lot of ergonomics improvement, and a handful of new features.
I would advise you to install rls (easy now using *rustup component add rls-preview rust-analysis*) and [autozimu](https://github.com/autozimu)/[LanguageClient-neovim](https://github.com/autozimu/LanguageClient-neovim) It's much better than syntastic. you can even activate clippy lints by adding a #!\[warn(clippy::all)\] in your file.
I can successfully reproduce this on my machine with the latest version of `rust.vim` as well. From the looks of [this GitHub issue](https://github.com/rust-lang/rust.vim/issues/27), it appears that `:RustRun` has been broken for quite a while and is considered outdated. The documentation was supposed to have been updated to reflect this, but it hasn't gotten done. In the meantime, I've found that [`vim-cargo`](https://github.com/timonv/vim-cargo) works reasonably well if you want to call Cargo subcommands from within Vim or Neovim. It features a `:CargoRun` command, which seems to work as expected. Perhaps try it and see if that works better for you.
I really wish we could abstract over Rc/Arc, but I believe its not possible in urrent rust, at least in a nice way. You really need higher kindred types, as Rc/Arc are type constructors rather than simple types (they take another type and give you a type). 
I believe from my testing the naive way of using xdelta (eg: [this](https://github.com/i30817/zxd3/blob/master/zxd3/__main__.py#L162)) does create significantly larger files than necessary yes. (and i think i even tried it in the 'removing' case where theoretically the file size should have been measured in dozens bytes because all the info is there on the original file). My main suspect is that the since the compression works as a window it's consistently picking local maxima as 'different, but close enough to compress'. I don't remember why i didn't use flips but it's probably a combination of 'doesn't have a library' and 'flips tries to read the whole file into runtime memory, which crashes for isos and dvds'.
There are a few important differences I can see: 1. fantoccini uses WebDriver, whereas headless\_chrome is based on the [Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/) (which means that it's not as cross-browser, but does have access to more parts of Chrome -- in my case I'm working on a project based around the JS / CSS coverage metrics DevTools records.) 2. fantoccini uses tokio and futures, whereas headless\_chrome has a synchronous API. I prefer it that way, and I'm happy to go into more detail if you're interested. 3. fantoccini seems more mature and complete!
I strongly believe \`std\` should include \`randint(low, hi)\` and \`random()\`, which would generate \`\[0, 1)\` floats. A lot of people here argue against including a particular crate, \`rand\` in \`std\`. Yeah, it is heavy-weight and potentially unstable. But \`randint\` and \`random\` won't see any breaking changes and often you want to do a single, simple thing, test something, etc. Having to add dependencies to flip a coin feels like a papercut. &amp;#x200B; My case was when I was supposed to write a randomized algorithm for an algorithms class, which required \`randint\` and nothing more. The assignment was hosted as a competition on [codewars.com](https://codewars.com), which did support Rust but not allow using cargo (or otherwise nonstandard dependencies), so I had to go with C++. I totally agree this is a very niche situation, but \`randint\` + \`random\` are as standard as it can get so it still feels ridiculous to not have them available.
I work on the research side in systematic trading in the HFT space so data volumes can get quite chunky. I'd be interested in being part of a working group as an increasing number of people are looking at Rust as a C++ replacement. The safety guarantees offered by Rust and the deterministic performance and allocation behavior are obviously of massive interest.
I'm not sure that I understand how this is supposed to work. Consider this as a representative example of what I want: let mut user = session.load::&lt;User&gt;("users/1-A"); user.Age++; session.save_changes(); Note that in this case, I have a problem. I want to give back an mutable instance, but I also need to keep it around inside the session so I can handle the `save_changes()` call and detect any changes done by the caller and persist them. The problem is that I want to port a library to Rust, and part of what I want to do is make sure that I have a consistent API. If you ever work with Java, then Hibernate is the closest example of what I'm trying to do.
&gt; An unsafe block that can be replaced with a safe one should be replaced. I strongly disagree. If you believe this, you must fork std. Large part of history of std is basically replacing safe code with faster code with unsafe blocks.
You may be correct that boats is planning some way to actually use the `Generator` trait in the definition of `Stream`, I initially read that as referring to the generator syntax and transform only (that being the main "language feature") and not the generated trait implementation. Without some drastic changes to the `Generator` or `Stream` trait I have difficulty seeing how they could be directly coupled, similar to how `Generator` and `Future` can't be directly coupled at the moment. If boats has a plan that doesn't involve resumption arguments (which make this coupling trivial) then I'll be very glad to see it. Referring to another quote from the post: &gt; It‚Äôs also worth noting that futures-async-await library previously mentioned implemented a verison of ‚Äúasync generators‚Äù that compile to streams, and so we can take some lessons from the experience with that library. I provided the initial implementation of this, and the approach taken seems to me to still be a relatively sane way to do it in the compiler as well. Basically this would just require updating the `async` transform to change explicit `yield expr` into `yield Poll::Ready(Some(expr))` (this wasn't possible in the `futures-async-await` implementation because of limitations in what `proc-macro`s can do).
Yes, I'm Oren from `ayende.com`. The scenario is exactly like Hibernate, but no related to `dynamic-update`, actually. Just to mention the reference to the object so I can tell whatever there have been changes, and then process them. Basically, this is what I want to have: ``` let mut user = session.load::&lt;User&gt;("users/1-A"); user.Age++; session.save_changes(); ``` This like the dirty flags aren't held on the entity itself. I'll have a `struct` that will contain all of those and a reference to the actual entity. Thinking about this, I'll _have_ to return back a pointer to the entity's struct, not the entity itself, because I must maintain a reference to it and have the caller's changes in the same place. 
Yes, that is something that I'll have to explore, but it sounds like what I'll need to do. Have an object that I hold that can hand references to entity instances. But isn't this something that just move the work? I'll have to give a mut reference to it to the caller, and hold to the reference myself. I _think_ that I'll have to use a `RefCell` for that and then have a `DeferMut` that would wrap that. Is that an idiomatic way of doing this?
Doesn't seem outdated, but low-level, as the issue states. It's a wrapper around rustc, which is why there's an error message about extern crate. You can simply `:compiler cargo` for your rustfiles, and then `make c`, `make t` etc., will work nicely with quickfix.
I meant "can be replaced with no loss of performance or functionality". If I remember correctly, most unsafe code in `actix-web` was either unsound or superfluous. And I'd rather have unsafe code in `std` than in a web framework.
There is also this paper which presents different benchmark with interesting comments: [https://static1.squarespace.com/static/5a60ec649f8dce866f011db6/t/5ab286da2b6a283afce7d752/1521649372997/Safety-Serialization.pdf](https://static1.squarespace.com/static/5a60ec649f8dce866f011db6/t/5ab286da2b6a283afce7d752/1521649372997/Safety-Serialization.pdf) &amp;#x200B;
Ah, thanks. I based my description on the final comment in the thread describing it as a low-level and old way of building and running Rust code. I had taken "old" to mean outdated.
Actually, I've installed the LanguageClient and the systastic both. Do I have to uninstall one of them?
The question is if the architecture supports something - memory fence, cache flushing, etc - that you can do when the thread count changes from "1" to "&gt; 1", that makes the existing thread to pick up the new thread count value immediately. (This operation can be slow as it is likely to only happen once.)
Thank you very much for recommending `vim-cargo`. It works like charm. I think I can keep using the vim with it for Rust.
&gt; &gt; fantoccini uses tokio and futures, whereas headless_chrome has a synchronous API. I prefer it that way, and I'm happy to go into more detail if you're interested. Can you, please? Is it to make the API simpler?
You're welcome! I would also recommend that you additionally install RLS and use a language client plugin over Syntastic, as u/wagnerf42 had mentioned. It can give you autocompletion along with additional features, such as go-to-definition.
You mean, I don't need the systastic anymore for Rust?
in a way it's cool that people choose between different high perf / safe languages; in the end the goals align
You can use it, but there is little reason to do so if you are already using a language server plugin, since it can already do what Syntastic does and more.
Sorry for my lack of knowledge. I tried that you've said. `:compiler cargo` -&gt; no response `make c` -&gt; shows `Press ENTER or type command to continue Finished dev [unoptimized + debuginfo] target(s) in 0.28s` `make t` -&gt; seems like it runs a test... I'm practicing the example in the official Rust book which find the word in *.txt file. This example makes a command to take an arguments in command line. How can I run it? In bash, I did `cargo run some_word poem.txt`
Got it. Thank you for your advise.
No worries. Wishing you the best of luck in your future projects!
So, what you're describing is exactly fine, works as expected. The first just sets the compiler to `cargo` and does not give a response indeed. The next one then invokes `cargo c`, which is `cargo check` and that finishes without error. Good for you! The last is then `cargo t` which is `cargo test` which does indeed run tests. To run your program the way you did in bash, just use `:make run some_word poem.txt`. Although for running things, I'd recommend opening a terminal in (n)vim.
A few examples wouldn't hurt.
I appreciate your advise. It's very much helpful to me. Have a good day!
This might be easy, but I just cannot see it. I'm trying to define a handler for hyper in which the Payload is defined as a trait object, so I can return responses with different payloads. However the below code fails with: expected type `hyper::Response&lt;std::boxed::Box&lt;hyper::Body&gt;&gt;` found type `hyper::Response&lt;std::boxed::Box&lt;(dyn hyper::body::Payload&lt;Data=hyper::Chunk, Error=hyper::Error&gt; + 'static)&gt;&gt;` Now hyper::Body does actually `impl Response&lt;Data=Chunk, Error=hyper::Error&gt;` so I don't see what's wrong here. extern crate hyper; // 0.12.22 extern crate futures; // 0.1.25 extern crate http; // 0.1.15 use futures::prelude::*; use hyper::{Body, Chunk, body::Payload, Request, Response}; // this works // type RespData = Box&lt;Body&gt;; // this doesn't type RespData = Box&lt;Payload&lt;Data=Chunk, Error=hyper::Error&gt;&gt;; fn handler(_req: Request&lt;Body&gt;) -&gt; impl Future&lt;Item=Response&lt;RespData&gt;, Error=http::Error&gt; { let mut resp = Response::builder(); let body = Body::from("hello\n"); resp.body(Box::new(body)).into_future() }
&gt; Line::Add(&amp;'a str) bothers me a bit ‚Äî nothing about unified diffs scream "Unicode" to me, I'd much prefer &amp;[u8]. OTOH diff and patch are both designed to work with text-ish ascii-compatible data, they absolutely don't support binary data in any useful capacity, so using text makes sense to me. Git actually extends diff with a base85-encoded char-as-length-prefixed mode to display pseudo-binary diffs.
If your backing store is a database, it's probably easiest to update the table directly and make your unit of work a transaction. That might look like let mut user = session.load::&lt;User&gt;("John Doe"); user.age += 1; user.health -= 1; user.update(); session.commit(); An alternative is to store the entities in the session and hand out mutable references to them (`&amp;mut User` instead of `User`);
That is very yucky interface from my point of view, I don't want to place the user with the burden of managing the state,that's the library job
Are you referring to `user.update()`? Then you can return a proxy that contains the same data as the `User`, but updates the store in `Drop` (the equivalent of `Dispose()`. But now you'll probably ask about navigation properties and/or collections, and I don't think I have an answer to that.
Cool! I was thinking just yesterday about type classes/traits being hard to discover and here it is :) Mind readers! 
... and appears to be 100% C‚ôØ?
Interesting. "train data with pre-trained networks": What does this mean actually? How does one train data?
First of all: The Rust implementation of Nimiq is still in development. We're currently in a Hackathon (started last week Monday) and are trying to finish a version which we can then test on the mainnet. &amp;#x200B; About Nimiq: Nimiq is a blockchain running in your browser. It uses NiPoPow to sync really fast. So basically you can pull up the webpage and you pretty much have your wallet (after setting a password and writing down your seed). &amp;#x200B; We're using Rust to get more performance out of standalone nodes (seed nodes, miners, etc.), but we'll eventually compile parts of it to WASM and run that in the browser. Also Rust gives us some nice safety garantuees :)
https://reddit.com/r/rust/comments/a8tlx1/using_c_libraries_un_rust_making_a_sys_crate/
I believe that was actually not merged yet as they split that PR up because it was taking too long. Please correct me if I am mistaken.
I don't even blame Piston *too* much. For over a decade now there's been so much focus on game graphics as the sexy thing to learn and teach that some of the assumptions which work for games have leaked into gui. &gt; "I am the most important task in the foreground and the user will ensure that I have sufficient resources to render a 'playable framerate'. Consistency is more important than efficiency." As a result, it's hard to even track down how the original GUI tech worked: Xerox and X and Apple and Windows. But I found a blog which is rather mind-expanding. https://magcius.github.io/xplain/article/regions.html (And others, but that's a good one to start on.) OpenGL isn't inherently married to the technique of clearing the screen 60 times per second, but any library that says "okay, double-buffer and don't think too hard" will make partial redraw somewhat more difficult. 
Great work as usual
Also, if you are unsure about your proposal and don't feel it to be interesting enough: a) In doubt, submit, it's probably interesting. b) Send an email to james.munns@ferrous-systems.com, he won't be in the committee and will help you with your proposal!
What is your `drop` doing?
text-ish ascii-compatible describes virtually every text encoding there is though. Files on disk are one of the times it makes complete sense to have a file in a non-utf-8 encoding.
Unless I am missing something, Write is something that is implemented for data sinks like files or tcp, not for your data structures. Use [this](https://docs.rs/bincode/1.1.1/bincode/fn.serialize_into.html) to output your binary data to a file.
Unless I am missing something, Write is something that is implemented for data sinks like files or tcp, not for your data structures. Use [this](https://docs.rs/bincode/1.1.1/bincode/fn.serialize_into.html) to output your binary data to a file.
Unless you really need to duplicate truncation-related bugs, it's best to pick one format for your intermediate values. It's both easier to read *and* better for performance. Casting is often an additional instruction (`movsx` / `movzx` on PC) and it can interact poorly with register renaming or require moving a value to a physical register of correct size. So Rust is actually making you more aware of the low-level cost of the code. Convert when you load and store only.
Well you could do this, but I'm not sure it's much better... let cmp = b.map(|(ref x, ref y)| Some((x, y)) == a).unwrap_or(false);
Cool! I noticed this talks about `rust-protobuf`, can anyone comment on their experience between `rust-protobuf` and [Prost](https://github.com/danburkert/prost)? A while back I needed a Twirp RPC implementation and found prost-twirp a bit lacking, so I hacked together my own. With that said it's very immature, and I've been meaning to reconcile that. However, I was not aware of `rust-protobuf`'s existence. Perhaps I should base my RPC library off of `rust-protobuf`? Overall I do like Prost, and don't have many complaints. Though, this Macro is seems nice, since I've been making my own `from/into` data structures.
&gt;One is a possible security issue, the other is an annoyance. You and me know the difference, I'm very confident most people aren't aware of the intricacies of the distiction and do not recognize the difference when their apps crash. &gt; Compared to? Most other languages. Not saying that should be the deciding factor for most people, I'd even say people focus too much time on execution performance. That's why we are seeing security vulnerablilities in applications that should have no business being written in C/C++. If developers won't relinquish speed for safety in conventional memory safe languages then Rust can serve as a safer substitute for them. &gt; D allows users to pass either immutable or shared (part of the type system) data to other threads and isn't a pure functional language. I'm not sure if mainstream qualification applies here. Also I'm not sure how much effort has to be spent to keep this up. People say you can write memorysafe code in C++22, not too sure about that. There are libraries on .NET for immutable data, but you'll be interfacing with mutable data all the time or you perhaps won't be able to control what libraries you use. Rust is inverted in this regard, you'd have to spent effort not to be covered by benefits of lifetimes. Or as they say, you can write cobol in any language and I guess you can write rust in any language as well. &gt; prevents deadlocks ConcurrentModificationException are a different class of problems. Deadlocks are harder to avoid with a language without imposing undue constraints, fairly sure there's a connection to a halting problem. 
[Rust by example?](https://doc.rust-lang.org/rust-by-example/)
&gt;Suggest items that are not in scope while completion... The corresponding extern crate and use items will be inserted automatically This is probably the biggest feature while working with Java in IntelliJ that was missing when working in rust. Awesome!
Hey there thanks for the comment! Posted without proper wording/reference so deleted the post. Re-posting with just a direct link. 
Are you joining your child processes?
Wrong subreddit you‚Äôre looking for playrust 
&gt; dodge This is one of the areas in which Rust has changed and old information is still floating around and confusing people. Here's how it now works: All values are zero or more bytes. The number of bytes is called the size of the value. If all values of type `T` are the same size, we call the type `Sized`. If they may be different sizes, we call the type dynamically sized. Each actual value has a size. `[T]` is zero or more values of type `T` packed contiguously, somewhere in memory. The number of values is called the *length*. Automatic memory allocation - the "stack" - cannot (yet) provide a location for a dynamically sized type. You have to use indirect memory allocation - something analogous to `malloc` which is either written in Rust or accessed via FFI. This restriction applies to `let` local variables, `for` induction variables, `match` pattern variables, `fn` parameters, and maybe something else that I'm forgetting. `[T]` is called "slice of `T`". It is a primitive type constructor. For each sized type in the type system there exists a corresponding slice type. Since `self` is a formal parameter of a method, the methods of slice types cannot take self-by-value. They're all self-by-ref and self-by-mut. Any pointer to data specifies the base address and the size of the data. The type of the pointer identitifies the type of the data, the "target type". When the target type is sized, the size of the target is known. So the pointer is optimized to only hold a base address. When the target type is dynamically sized, the pointer type holds both base address and length. `&amp;[T]` is an (address, length) pair. `*mut [T]` and `Box&lt;[T]&gt;` are also (address, length) pairs. So is `&amp;str`. That's because they're all primitive pointers to a sized value of an unsized type. However, because they are different types they have different methods, traits, and operations. If you have a type `T` and a reference (`&amp;T`), you can call T-by-ref methods. There are two accepted syntaxes: - (*x).foo() - x.foo() The compiler recognizes that they are equivalent when searching for a method. The first syntax works even when `T` is dynamically sized. That's because method calls expect a location-expression. You can't have a temporary value of type `str` - the stack allocator says no - but you can have a location of type `str` and call a by-ref method. Since the compiler won't ever dereference raw pointers for you, you'll see things like (*buffer)[i] = n; in unsafe code. The explicit deref follows `buffer: *mut [T]` to the [T] location. The index operation selects location `i` with bounds checking. Then `=` drops the value at that location and replaces it with `n`. When you write the abbreviation buffer[i] = n; the same steps are followed, but `&amp;mut [T]` to `[T]` is implied. It's still invoking by-mut `&lt;[T] as IndexMut&gt;::index_mut` on the slice.
If `load` returns a `&amp;mut User`, then you should be able to write: { let user = session.load::&lt;User&gt;("users/1-A"); user.Age++; } session.save_changes(); 
Right on, thank you for the explanation!
thankyou for the kind words!
Or from unsafe { to [#hold_my_beverage] do { } üòÅ I'd propose adding a LOOKMA keyword to INTERCAL but something about a safe/unsafe distinction feels antithetical to the language.
How is your program being terminated? `kill -9` will just kill it instantly, nothing you can do. Same with `std::process::abort`. Panics should do a controlled shutdown (calling `drop`, etc) unless you've got `panic = "abort"` in your `Cargo.toml`
Probably there will have to be an `AsyncGenerator` trait. We have to implement resumption arguments in the *core state machine mechanism* that all of these features are built on at some point because futures and streams cant depend on TLS hacks forever.
&gt; if the tasks your codebase performs are mostly comprised of it, there's not much benefit to using a "safe" language. I guess it really depends on how exactly you define "safe" overall. At the very least, Pascal is not "Rust-style safe" as there's of course no concept of separation between "safe methods" and "unsafe methods", so you can do things like switch into inline assembly in the middle of a function and then back again any time you want, and so on. On the other hand, there's a long list of things about it that IMO make it arguably much safer than something like C or C++ in a practical sense.
Also, I believe if you panic during `drop`, you will get abort behavior. Something to maybe check for.
`let cmp = a == b.as_ref().map(|(a, b)| (a,b));` works. Kinda feels like `&amp;(,) -&gt; (&amp;, &amp;)` should be a method on tuples/arrays... 
If this thing supports rust, then it would definitely be newsworthy! But I don't see any reference to Rust in the paper that I linked, and only a couple of tertiary references to it in the OP blog post which doesn't lead me to believe there's anything *really* tying it to Rust. They say that mozilla will provide "expertise" in Rust (along with other languages).... what does that mean? is this tool going to get support for Rust? Does it have it already?
I think it makes sense to define a function for the conversion between `&amp;(T, U)` and `(&amp;T, &amp;U)`: fn pair_refs&lt;T, U&gt;(tuple: &amp;(T, U)) -&gt; (&amp;T, &amp;U) { (&amp;tuple.0, &amp;tuple.1) } Once you've got that, you can do your comparison pretty cleanly: a == b.as_ref().map(pair_refs)
&gt; You can code in C with the correct tools to help ensure safety. Valgrind, clang-sanitize, static analysis and a good coding standard means you essentially have the safety of any of the C alternatives. The key difference being that these are all things you run on a whole program, or maybe a test suite. What makes Rust unique is that it makes the analysis *modular* (or "compositional"). In Rust you can design a good API once, and then basically stop worrying about it (think `Vec`, `RwLock`). If someone finds a bug, you have one place to fix it. In C, you have to constantly check everyone using the API. If someone finds a bug, you have to check every use. It just dosn't scale. Moreover, if you now need another property, you can often code this as a safe-to-use library in Rust. With the "tool" approach, you have to write another tool.
Yes, it's unfortunate, but it's sometimes necessary. I've published crates that lack documentation---and certainly nowhere near the level of "every feature has an example"---because it takes a _ton_ of work and mental energy to add good docs. In some cases, it's not clear to me that the API is actually right, so I'd rather hold off on writing docs until I at least have more experience actually using it (instead of just building it). Writing docs for an API you're unsure about is a daunting task, because you might have to redo it. It's a judgment call. With that said, missing docs should probably be a blocker before one expects others to _use_ their crate productively. I know I generally avoid using crates that aren't well documented, either because it's not clear that the critical issues have actually been thought through or because it, at least, gives the appearance of it being unmaintained. There are definitely some crates in our ecosystem that fit into this slot, and I agree that's unfortunate. But damn, writing docs is _hard_.
I wonder if/when Rust will make it to the big leagues and get its own JetBrains IDE
&gt;quick fix to create lifetime parameter from usage And this is also very cool
I think I'm missing something obvious I want to use `write_hex` from [https://docs.rs/hex/0.3.2/hex/trait.ToHex.html](https://docs.rs/hex/0.3.2/hex/trait.ToHex.html) I'm doing: ```rust let file_name: Cow&lt;'empty, str&gt;; // just to let you know file_name's type let mut intl_hex: Vec&lt;u8&gt; = vec![]; file_name.into_owned().write_hex(&amp;mut intl_hex)?; ``` I got: ``` error[E0277]: the trait bound `std::vec::Vec&lt;u8&gt;: std::fmt::Write` is not satisfied --&gt; src\main.rs:86:36 | 86 | file_name.into_owned().write_hex(&amp;mut intl_hex)?; | ^^^^^^^^^ the trait `std::fmt::Write` is not implemented for `std::vec::Vec&lt;u8&gt;` ``` but as far as I know, std::vec::Vec&lt;u8&gt; does implement `std::fmt::Write`
It's much worse than array indexing because it is much less local. When accessing an array you usually have some way of know the length. When writing a library with ref-counted pointers, you have no way to know if elsewhere in the program this library will be used in a multithreaded way.
Hi, I'm the guy working on this. I'm working on trying to add more documentation and examples. For now, though, there's a little bit more on [the wiki](https://github.com/ni/rebar/wiki).
Yep! Rebar is an addon to LabVIEW NXG, which is an application developed in C#/.NET. I was a little surprised to see this here. I would be super open to feedback on Rebar from this community, but actually using Rebar requires having LabVIEW NXG, which is not free software (and I definitely don't expect everyone to go buy it). The main audience for Rebar is people who use LabVIEW/LabVIEW NXG but are interested in the kinds of benefits that Rust semantics bring.
I agree with your assertion about crates which have an unstable API. I whince a little when I am privately updating a library and forget to have updated the examples, which are now subtly broken. The fatigue of having to keep examples and docs up to date is not trivial. I see it like cooking, though. If I want to cook, I expect to spend the same amount of time as cooking cleaning. Documentation is like cleaning. It's not my favorite thing to do, but I don't like a dirty kitchen more. As a point of personal pride, though, before I announce anything or release, I make a final pass at documentation and proof-reading. This was something that definitely improved as I got older and more experienced (and also got worse as I got more burned out, haha). The alternative is releasing crates early to the public and getting sooner feedback before finalizing anything. Upon reflection, I could see how an API like this would be more unstable based on community feedback, and therefore, as you said, it is a judgement call to wait and write documentation later. As long as it is written at some point.
You can never write `++` in Rust :-p
yes, I would uninstall syntastic. also, be sure to use rustfmt (also available as a component). it's really a must. other tools I use are clippy and racer.
It's executing a command inside a running docker container, telling it to stop
No, I'm not, but it's a nice thing to do, though I don't think it will help me. I'll take a look, thx!
I'm just pressing CTRL+C, it's linux, so it's a SIGINT. My Drop impl is not called at all. At least, putting a println! at its beginning, nothing gets printed after I've pressed CTRL+C
How do i get c++ references in rust? I want to bind a field to variable. fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { let &amp;mut j = &amp;mut self.count; j += 1; if j &gt; 6 { return None; } Some(j) } it says error[E0384]: cannot assign twice to immutable variable `j`
The work on rls-2.0 has been started.
`Vec&lt;u8&gt;` implements `io::Write`, not `fmt::Write`.
This is fmt::Write, not io::Write
The default action for SIGINT is to terminate the process, at the kernel level. You'll need to install a signal handler to do something different, like https://crates.io/crates/ctrlc
Sorry, what false dichotomy? Maybe I misunderstand, but I was just saying that you want tests in both cases. 
You are correct. Right now only the parsing of const parameters and arguments are merged in. The rest of the implementation is on the way soon‚Ñ¢Ô∏è.
You‚Äôre correct. Rust packages safety up nicer. That‚Äôs why I like rust. My point was that it‚Äôs *possible* in C. Without getting ‚Äúbetter‚Äù or ‚Äúhaving godlike skills‚Äù. The tools exist if you use them.
`String` isn't for "text-ish" data, it's for UTF-8, and absolutely nothing else. If the right thing to do when encountering non-UTF-8 data in a file isn't one of "*mangle it irreversibly*" or "*give an annoying error and refuse to work*", you shouldn't be using `String`. --- head/sys/contrib/ncsw/Peripherals/BM/bm.c (rev 0) +++ head/sys/contrib/ncsw/Peripherals/BM/bm.c 2016-02-29 03:38:00 UTC (rev 296177) @@ -0,0 +1,815 @@ +/****************************************************************************** + + ÔøΩ 1995-2003, 2004, 2005-2011 Freescale Semiconductor, Inc. + All rights reserved. Whoops, that was supposed to be `¬©`. As for binary data, diffs actually work just fine. They're certainly not *optimal*, but they do work.
Question: why would you build yet another proof-of-work-based blockchain when it's been shown that mass adoption would [significantly increase the rate of global warming](https://phys.org/news/2018-10-bitcoin-global-couple-decades.amp), which is [already going to kill most of us](https://www.rifters.com/crawl/?p=8433)? Also, do you have any solutions to the rampant fraud that has occurred in every unregulated cryptocurrency thus far? Why should I use your software if it's less secure, more expensive, and more environmentally damaging than boring old money?
Can confirm, I'm happy to answer questions!
&gt; String [is] for UTF-8, and absolutely nothing else. That's completely and absolutely untrue. String is the proper type for any *actual text*. &gt; If the right thing to do when encountering non-UTF-8 data in a file isn't one of "mangle it irreversibly" or "give an annoying error and refuse to work", you shouldn't be using String. If the file should be *text*, then the proper thing is to either generate an error if the file should be UTF-8 encoded, or to decode it using something like encoding-rs otherwise. 
&gt; text-ish ascii-compatible describes virtually every text encoding there is though. If the file can be in a proper but non-UTF8 encoding, decode it beforehand.
So does the container keep running, or does `drop` not get called? On Unix, child processes get terminated by default when the parent stops. Could it be that your `docker stop` command gets killed before it finishes because of that?
Oh god please no JS frontend. old.reddit.com FTW!! :-)
Yes, [quite a lot of companies](https://www.rust-lang.org/production/users) are using it already in production.
Some more Googling unearthed the [Configuring a target](https://doc.rust-lang.org/cargo/reference/manifest.html#configuring-a-target) section of the Cargo reference, which suggests that you can configure both a `[lib]` and a `[[bin]]` target for your crate. From there I also found a [Stack Overflow answer](https://stackoverflow.com/a/26946705/683033) showing that, yes, you can build both a lib and a bin from one crate. For my purposes, this is exactly what I need! I'd imagine this approach would also work for you: If the only place you use those extra dependencies is the files that are targets of each `[[bin]]` target, I can't imagine the linker would pull them into the others, so they shouldn't be included in the final builds.
JS frontend would be fine if a first class non-JS alternative exists. I think if we're going to shoot for the moon, we could write a isomorphic application where based on user config *(like `?no-js=true` or w/e)* it would use server side rendering. Of course, there would be no JS regardless - we're be writing isomorphic Rust, and server rendering with Rust when the client requests so. This is purely hypothetical. I say "we" as a non-contributor to the project.
Indeed. As the title mentions this is just the "to AST", meaning that the parser produces an Abstract Syntax Tree. This has not reached type-checking nor code generation yet.
&gt; This is purely hypothetical. I say "we" as a non-contributor to the project. Love the way you‚Äôre approaching this! ;-)
Try an explicit type annotation: let b: RespData = Box::new(body); resp.body(b).into_future() Rust's type inference cannot infer that `Box::new(body)` is supposed to be a `Box&lt;dyn Payload&lt;...&gt;&gt;` -- and `Box&lt;Body&gt;` isn't a `Box&lt;dyn Payload&lt;...&gt;&gt;`, even if `Payload&lt;...&gt;` is implemented for `Body`. The explicit type annotation makes this work.
Haha, I just didn't want to imply a relationship. I guess I'm used to writing documentation with an inclusive tone, "we need to fix X" and etc. lol
I use [inferno](https://github.com/infernojs/inferno), the whole thing is like 20 KB, it's not too heavy, and extremely fast. I agree we should keep it as light as possible.
For compositing allocators, I've had good results from simply using return types: - `allocate` returns a potentially null pointer, in which case the next allocator in line is called instead. - `deallocate` returns a potentially non-null pointer (its argument), in which case the next allocator in line is called instead. By ordering the allocators from small to big sizes, the slight overhead of checking whether the first could allocate/deallocate was not too much of an issue: it's "normal" that large allocations be costly anyway, so people are careful with them.
Old reddit also uses JavaScript.
String definitely requires that the contents are valid UTF-8: https://doc.rust-lang.org/stable/std/string/struct.String.html. It's unsound to break this invariant (and not possible to do so in safe code).
Hi, can someone explain to me why these outputs of two functions are completely different? I thought they're equivalent (or similar). [https://godbolt.org/z/nkc0N3](https://godbolt.org/z/nkc0N3)
Assuming you know the encoding. If I may be allowed to intentionally bait you, without investigating, can you tell me the encoding of source files in a checkout of a Linux kernel? Hopefully I'll get this crate released soon, but see: https://burntsushi.net/stuff/tmp-do-not-link-me/bstr/bstr/#when-should-i-use-byte-strings
It sounds like you're dealing with a problem tangentially related to what I tried to solve, catching signals and sending commands to a child process in Docker. Maybe my little tool can give you some inspiration: https://gitlab.com/markwetter/mc-wrap If you end up doing something similar, I would use `crossbeam-channel` and `signal-hook` instead of `chan_signal`. /u/burntsushi has deprecated his `chan` crate in favor of `crossbeam`.
VSCode is the recommended Editor, but IntelliJ Rust is by far the best experience IMO. One slightly annoying thing is that their internal formatter and rust-fmt don't result in the same file in all instances. Would love to have the option to use rust-fmt for Ctrl+Alt+L - for now I have mapped it to Ctrl+Alt+K.
&gt; If the file should be text, then the proper thing is to either generate an error if the file should be UTF-8 encoded, or to decode it using something like encoding-rs otherwise. This is true only in the philosophical or ideal sense. If my tools did the "proper" thing, I'd get a never-ending stream of bug reports. This is why Rust's regex engine provides a searching mode that does not require valid UTF-8. PCRE is experimentally [allowing the same thing](https://lists.exim.org/lurker/message/20180917.192454.6e424732.en.html). When given the choice between doing the "proper" thing and _explicitly invoking undefined behavior_, [Julia chooses the latter](https://github.com/JuliaLang/julia/blob/19a0f71656ba86c1361471d7067f0e7fa5c93132/base/regex.jl#L8). (See the [this commit](https://github.com/JuliaLang/julia/commit/1033b881c49e937212556185f6fc6e3635c30284#diff-7422507bb03a8ecee35360dc9380b9ca) if you want to explore why they chose this. See also `man pcre2api` and read the section on `NO_UTF_CHECK`.) The "proper" fix to this would probably require a backwards incompatible change to the way many of our environments work: making the encoding part of file meta data and enforcing it.
&gt; Assuming you know the encoding. True, or that you can run an encoding-detection pipeline which is either specified or doesn't end with an encoding accepting every 8-bit values (aka any iso-8859 encoding amongst others).
The string itself contains UTF-8, what you create the string from has no reason to.
Rocket.RS?
When you do let &amp;mut j = &amp;mut self.count; Its the same as doing let j = self.count; because of destructuring. If what you want is to modify `self.count`, then you probably want to do this. let j = &amp;mut self.count; *j += 1; if *j &gt; 6 { return None; } Some(*j) AFAIK, you must dereference explicitly in all cases expecting not a reference (so methods are different). Also note that under current constraints, iterators cannot do `fn next(&amp;'a mut self) -&gt; Option&lt;&amp;'a mut T&gt;`. This is being worked on and called "streaming iterators".
Doesn't rust compile the code on documentation? How is it even possible for it to get 'out of date' without a major warning at least?
I've used both rocket and actix, and far prefer actix. [Actix is one of the fastest benchmarked frameworks out there](https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=fortune), rocket isn't benchmarked at all AFAIK.
&gt; Would love to have the option to use rust-fmt for Ctrl+Alt+L Not that this might not be as good as it seems: if there are syntax errors in the file, `rustfmt` usually just dies, while IntelliJ Rust produces a pretty sensible formatting. 
Personally, I don't mind if JavaScript is present on the page... ... but I'd really, REALLY, prefer if the page was at least *readable* without JavaScript, and ideally fully usable. Think about people using screen readers, or simply not liking to execute arbitrary code on their machine. If Rust is powering the backend, then surely it can send a rendered (HTML) page rather than compute the HTML in JavaScript :)
Very true, and that is also terribly frustrating. Bringing the two formatters to parity is another choice. I wouldn't mind either.
&gt; Step 4: Manage expectations.
There are regular job postings on this very r/rust. Searching for Engineer reveals quite a few: https://www.reddit.com/r/rust/search?q=Engineer&amp;restrict_sr=1
&gt; This is true only in the philosophical or ideal sense. If my tools did the "proper" thing, I'd get a never-ending stream of bug reports. I mean, I don't know that it would *be* the proper thing for a tool like ripgrep, I'd assume it's perfectly valid to have iso-8859-10 logfiles and want to search them. &gt; The "proper" fix to this would probably require a backwards incompatible change to the way many of our environments work: making the encoding part of file meta data and enforcing it. I think the former already exists (e.g. com.apple.TextEncoding seems to be a thing), the latter would make for interesting times (I don't know that com.apple.TextEncoding is actually used anywhere or by anything).
These component-based frameworks create very readable html, they just allow for re-usability, working nicely with code, etc. Take a look at the html of a site I just wrote with inferno: https://torrents-csv.ml/#/search/torrent/test/1
Ah, I see. Thanks. It's possible that the borrow checker could be modified to treat globals specially, I guess. But maybe not, and in any way it would be a lot of work. Again, thanks for the explanation!
You are looking for r/playrust
Try asking around at a local meetup. I think there‚Äôs a rust group in Tel Aviv. Also try looking for remote jobs.
Yes using some JS is fine. An entire JS frontend (meaning rendering) is a different story 
Yes using some JS is fine. An entire JS frontend (meaning rendering) is a different story
shid.
`r as *const _ ` means we're casting some kind of reference type to a point. If `r` is `&amp;T`, we end up with `*const T`. Then next part casts away `const` so we can do mutation through the pointer and end up with `*mut T` instead.
When the parent stop, child process are adopted by the init process (pid 1) they are not killed. In docker there is an infamous problem with pid 1 see https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/ 
I feel you. (And yes, MT isn't a great PRNG: just well-known.) I wish I could think of a way to feasibly collect a large number of examples of PRNG usage. For libraries it's complicated, but if we had a way to measure I'd bet you a Coke that 95% of the application usages are for games, graphics and simulations. (I'd guess that anybody who is rolling their own security at the application level likely has bigger problems than using a bad PRNG anyhow :-).) For graphics and simulations high throughput is really important; for games and graphics ease of use is pretty important. Anyway, it's kind of moot: it seems clear to me that `std` won't get anything for PRNGs it doesn't already have anytime soon. Thanks much for your comments!
You're right, I don't know what got into me :-).
Nicely explained, thank you.
Wait, is that function safe?
For example, code like [this](https://github.com/actix/actix-web/blob/285c73e95ea4a011673bcd4f84a26d2aee84e592/src/server/helpers.rs#L80) seems weird to me.
I must admit I misread your previous statement. And I agree, types don't completely replace tests. However, types can replace *some* tests that you'd otherwise have to add, so you can concentrate on more higher level concerns like your business logic.
Honestly, people who write unsafe should always include a (short) comment explaining why exactly this unsafe code is "safe" (And perhaps describing other invariants). *sigh*
That code is not actually safe. There was a thread about it a few months ago on reddit.
Supposedly it was fixed at the time.
I don't think so; it hands out an exclusive mutable reference from a shared immutable one without something like a `Cell`/`RefCell` type which is special. I think you could easily show the unsafety of this function by calling \`get\_mut\` twice in a row, at which point you have two separate mutable references to the same location which are both no longer exclusive.
I remember that, though I thought there was a cleanup effort too afterwards. This unsafe might not have been that easy to remove I guess.
It has been removed. The link I posted is from an older commit.
Nothing much, actually :) It was created mostly for learning purposes and the main difference is `pgcopy` is not tightly coupled with `postgres` crate and can be used to write *files* in that format (generic over `io::Write` basically); `postgres-binary-copy` writes data into an inner memory, which is not quite useful when you need to dump few gigabytes of data. That's it, I guess. If you have any thoughts, I would like to hear them!
Although I agree that cryptocurrency mining is an unnecessary waste of energy, I'd like to point out that this Hawaii University research paper on cryptocurrency mining is highly criticized by researchers (the numbers are mostly based on assumptions and some of those are already known to be invalid). Maybe read this article to get insights on that topic: [https://hackernoon.com/the-reports-of-bitcoin-environmental-damage-are-garbage-5a93d32c2d7](https://hackernoon.com/the-reports-of-bitcoin-environmental-damage-are-garbage-5a93d32c2d7) Also when it comes to Nimiq: The Team announced they'd like to switch to Proof-of-Stake soon, which does not have the problem of energy consumption. Regarding regulation of cryptocurrency: Did you consider the same applies to any other currency, including USD and EUR? Most of fraud still happens using USD at the end. Before frauders used Bitcoin or Monero, they used PaySafeCards and uKash (or whatever they were named). Every means of cash is affected by that problem. As long as every "good" person using a currency requires and verifies customer information for larger amounts, risks of these attacks are low. For example exchanges require personal details and also information on source of the funds if you try to convert larger amounts of Bitcoin to USD. Banks will also require the same information. And so on.
Here is one source: https://remote.com/jobs/browse?keyword=rust Also, you can subscribe to https://this-week-in-rust.org/ where almost every newsletter contains part: "Rust Jobs".
One would think this is possible, but actually there is no general way to shutdown the children 100% of the time, unless the children are watching the parent and waiting for it to die.
IIRC you can interact with chrome using any language which has a websocket client. Working with it as such a low level isn't necessarily sexy, but it works.
&gt; I mean, I don't know that it would be the proper thing for a tool like ripgrep, I'd assume it's perfectly valid to have iso-8859-10 logfiles and want to search them. Again, you don't know the encoding up front, and it's often impractical to accurately discover it. So it's most convenient treat files as "something something ASCII compatible, preferably UTF-8."
Will look into it!
The CFP end date in the text is given as Feb 15th, 2018 - no excuses! Just saying, probably a copy paste thing.
Top JS frameworks support server-side rendering now. This means you can write e.g. a fully declarative reactive front-end in React and have the server render it. The browser won't have to execute any JS, it'll get the compiled page. Though you can still include the framework on the front-end to support dynamic features. To do this, the server-side renderer of a given framework needs to be ported to Rust (most existing implementations are for NodeJS).
+1 for needing to handle SIGINT. That crate looks gross, though. `ctrlc::set_handler` should definitely be an `unsafe` function because signal handlers have lots of caveats. It's unsound. I'd recommend [signal-hook](https://docs.rs/signal-hook/0.1.7/signal_hook/iterator/struct.Signals.html) or [tokio-signal](https://crates.io/crates/tokio-signal) instead.
True. In this context, I was defining "safe" as "having a clear separation (`unsafe`, FFI, etc.) between regular code and code which can do raw pointer manipulation." I actually originally planned to use Free Pascal for a DOS hobby project (an installer creator analogous to InnoSetup) but, bit by bit, I wound up convincing myself that, for what it is, Open Watcom C/C++ was a far better choice. (It's something that needs to fit comfortably on a floppy disk without crowding out the actual content. Open Watcom's C runtime stub is only 996 bytes if I use `void main(void)` and it has a much nicer (and more appealingly documented) syntax for writing snips of inline assembly as FFI wrappers around BIOS `int 10h` API calls to avoid the tens of kilobytes that get pulled in just to draw a positioned and coloured "Hello World!" using `graph.h`.)
Oh yes, that's totally a thing. In fact, I'm not sure you even need any configuration in `Cargo.toml`; you can just put source files in `src/bin/` and they automatically become binaries. The main downside there is that any dependencies you need for your binaries have to be dependencies of your library also. Avoiding that requires separate crates.
It looks like a cool project. First thing I could see after taking a glance at the code is that there‚Äôs a lot of logic in `main.rs`, including declaring modules. The idiomatic thing to do would be to extract everything not directly related to the executable into `lib.rs` and declare submodules there. Effectively, this makes it so that you are defining a library and then using it in your `main.rs` file. 
Your case does not fit `PartialEq`, at all. Quoting the docs: &gt; Formally, the equality must be (for all a, b and c): &gt; &gt; * symmetric: a == b implies b == a; and &gt; * transitive: a == b and b == c implies a == c. You should just have a custom trait here, or even just a function on the type itself, that returns a distance. 
Thanks! That works indeed, now the `handler()` function compiles. However if I now actually try to use that function, I get another error: error[E0277]: the size for values of type `(dyn hyper::body::Payload&lt;Data=hyper::Chunk, Error=hyper::Error&gt; + 'static)` cannot be known at compilation time --&gt; src/main.rs:23:10 | 23 | .serve(|| service_fn(handler)) | ^^^^^ doesn't have a size known at compile-time | = help: the trait `std::marker::Sized` is not implemented for `(dyn hyper::body::Payload&lt;Data=hyper::Chunk, Error=hyper::Error&gt; + 'static)` .. with the code below, which is straight from the hyper example server. Again it looks like some type is not inferred, but I'm at a loss. I've written a lot of code using hyper and actix-web, but usually when I encounter something like this I just find a workaround. In the code I am actually writing I added an impl Payload for a Stream I already have, I could just put the Body stream wrapper in front of it and call it a day. But I'm still curious as to why this does not work.... ( [link to playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e8be41eebac684d459c8762dbee27e06) ) fn main() { let addr = ([127, 0, 0, 1], 3000).into(); let server = Server::bind(&amp;addr) .serve(|| service_fn(handler)) .map_err(|e| eprintln!("server error: {}", e)); println!("Listening on http://{}", addr); hyper::rt::run(server); }
Turns out it doesn't actually work for my needs, or at least I can't get it to compile after changing this. The moment I point the `[lib]` section at a file intended to wrap and expose my needed logic in a `cdylib`, neither the lib nor the binary (via a separate `[[bin]]` section) will compile, complaining they can't find the shared module that implements my core logic. I suspect it's either me not setting up the module correctly (although I followed what the book said...), or that changing the library type has repercussions for how the linking works, or both. In any case, some quick testing with a workspace, where the "core" logic is in one library crate and then my two build targets are established via separate crates, suggests that this is the approach I need to build the artifacts I need.
I really like to see technical alternatives to reddit but I guess that even if you'd build the most awesome software it does not help if there are no users. So beside discussing the tech approach I'd recommend to really take effort on how to move/get users to a new place!
&gt; Again, you don't know the encoding up front, and it's often impractical to accurately discover it. Yes that's my understanding, and hence why I said I don't know that it would be "the proper thing" for ripgrep to assume anything about the encoding of what it's given or what it finds.
Why do you even want to use PartialEq? As the other poster has pointed out, your design does not meet the requirements for PartialEq. What generic algorithm are you trying to use with this, that requires PartialEq?
Agree. Mastodon had a really well done launch, and was able to attract a lot of users cause it had good buy in from users and the tech community. 
rust-lang-nursery has all sorts of stuff in it, much of which like Chalk and Polonius is unlikely to be used by end users. On the other hand it doesn't contain critical core library functionality like `rand` and `regex`. I think such a collection would also need to be a bit more official (telegraphed from the Rust website for example) to be useful.
You wouldn't download an auto!
I read it as "we will automatically add the right `use` statement, but also the `extern crate` if the edition is 2015".
Ha, I did the same thing for my universities system (although in node). The interfaces for these things are remarkably bad!
WASM frontend written in Rust, obv.
&gt; rls-2.0 Is this in a completely separate repo on GitHub?
[https://github.com/rust-analyzer/rust-analyzer](https://github.com/rust-analyzer/rust-analyzer)
&gt; If an unsynchronized memory access occurs (aka a data race), then Java will raise a runtime exception‚Äîhowever, this still relies on programmers appropriately using concurrency primitives. Is this true? I've used java for a while and from what I can tell you're on your own w.r.t. data races, there's no runtime checking at all.
This is confusing, because by default child processes you spawn wind up in the same process *group* as their parent, and the default behavior of Ctrl-C in the shell is to send SIGINT to the entire process group. So it often appears that the parent dying "caused" the child to stop, but what actually happened is that the shell killed both. But as soon as you do something interesting like OP did, where your "child" process isn't your real child for some or other reason (in this cause because `dockerd` spawned it for you), the usual relationship is broken.
Well, obviously you know your use case better than me. I don't know enough about Open Watcom to comment on the binary size comparison (although I feel like FPCs are generally quite small), but when you refer to the assembly syntax, do you mean [stuff like this?](https://svn.freepascal.org/cgi-bin/viewvc.cgi/trunk/packages/graph/src/msdos/graph.pp?view=markup#l166) Personally I've always preferred the directly-parsed nature of inline ASM in Pascal to the string-based approach often found elsewhere, but I guess it's a matter of preference.
Eh, it's actually a sync thing, the text doesn't automatically update. Thanks for catching!
Take some inspiration from lobste.rs (see interesting innovations on https://lobste.rs/about)
Damn that site looks beautiful.
This seems to be a limitation of the hyper API, not sure how much that can be changed. In a nutshell: Somewhere, the response data needs to implement `Payload`. If you look at [the documentation](https://docs.rs/hyper/0.12.24/hyper/body/trait.Payload.html#foreign-impls), Payload is implemented for `Body` and for any `Box&lt;E&gt;` where `E` implements payload and `E` is `Sized`. The latter bound is implicit, because the `Sized` bound is opt-out. Thus, `Box&lt;Body&gt;` implements `Payload&lt;...&gt;`, but `Box&lt;dyn Payload&lt;...&gt;&gt;` does not. Might be a bug in hyper. Out of curiosity, I cloned the hyper repo and changed impl&lt;E: Payload&gt; Payload for Box&lt;E&gt; { to impl&lt;E: Payload + ?Sized&gt; Payload for Box&lt;E&gt; { After that, your example compiled fine. Consider opening an issue on the hyper repository and ask whether this restriction is intended?
It already has, kind of. It shares its IDE with C and C++. CLion has the Rust debugging capability that IntelliJ is missing. So you could believe that Jetbrains will use CLion has the premium Rust IDE.
I see it as progressive enhancement. Have the site usable, or as much as usable as reasonably possible, without JavaScript, and sprinkle a little of it on top to make it better. I find those pages that come up blank with JS disabled awful.
&gt; but when you refer to the assembly syntax, do you mean stuff like this? I was referring more to the documentation. I've always found that to be Free Pascal's greatest weakness. (Most notably, how disorganized the wiki feels, that the official way to check which units a target supports is to open up the distribution archive and look at the filenames, and that they can't even auto-generate basic boilerplate documentation for Free Vision. When I wanted to play around with Free Vision and wasn't willing to download a pirated copy of Borland's Turbo Vision reference, I was forced to instead order a used copy via AbeBooks.) That said, from what I see there, Free Pascal appears to be more comparable to Borland Turbo C than Open Watcom as far as inline assembly goes. Here's an example API wrapper from my code: static void clear_rect_asm(row r_top, column c_left, row r_bottom, column c_right, uint8_t _fill); #pragma aux clear_rect_asm = \ "push bp" \ "mov ax, 0600h" \ "int 10h" \ "pop bp" \ parm [ch] [cl] [dh] [dl] [bh] \ modify [ax]; The `push bp`/`pop bp` are just to be absolutely sure that a [bug in some BIOSes](http://www.ctyme.com/intr/rb-0096.htm) which clobbers `bp` is being worked around until I can take a look at the disassembly to verify that Watcom is already doing that. They're probably superfluous. Note that there's no `mov ax,val_ax` because Watcom's syntax allows me to map arguments to registers declaratively, and no pushing and popping of `ds` because Watcom's documentation makes it clear that it's unnecessary for me to do that manually in code like this. The string-based pragma *is* a bit ugly, but has the advantage of being compatible with splint, unlike the aforementioned `far` keyword I have to hide behind an `# ifdef S_SPLINT_S`.
Also has the same name as the Erlang build tool. Not that it matters, really.
Great points, I look forward to seeing how the team responds. I know they discussed in the past that they'd be open to switching to POS or whatever the new innovation is in mining at the time they want to switch, but I believe while the currency is young it's best to use a battle tested consensus mechanism. Regarding the rampant fraud, I'd love if you could elaborate upon what you mean. Fraud happens everyday, everywhere, in every currency. If you're talking about forms of fraud that are specific to cryptocurrency I can only think of a hand full. There are no smart contracts on Nimiq so no Parity wallet issues or ICOs can occur, so the only two big threats I see are: loss of keys and loss of funds. Keys can be lost via simply losing your key (which Nimiq nor any other crypto can protect you against) but this isn't much different from losing your debit card besides the fact that it can't be recovered. Keys could also be lost to scams but as with all cryptos , you should always be careful where you enter your private key and preferably should never give it out. But Nimiq is unique from most other cryptos in that the native wallet is online similar to MyEtherWallet. This means that Nimiq is more susceptible to someone mirroring safe.nimiq.com and putting it up at safe.nlmiq.com or something, but Nimiq as a whole seems dedicated to educating it's users and not just creating a currency so I hope to see users warned against such scams. My extension Nimiq Utilities Extension also helps with this by giving you keybinds to the official nimiq sites ensuring you never have to type the URL and risk getting it wrong. Loss of funds is different from loss of keys, because with Loss of Funds you are sending your funds somewhere and due to how crypto works that transaction is mostly irreversible. This kind of fraud could occur due to a user using sketchy exchanges or storing large amounts of NIM in an application like one of the tip bots. I think most of these fraud opportunities can be beaten simply by keeping as much NIM in your wallet under lock and private key ( ;) ) rather than leaving it in exchange or other application wallets. Overall, "why should [you] use [Nimiq] if it's less secure and more environmentally damaging than boring old money"? The answer is that it's far more secure than boring old money and you (and only you) own your funds. While this comes with drawbacks such as transactions being irreversible and you losing your key being you losing all your funds, it comes with the huge upside that a bank isn't in control of your funds making profits off just holding your money. And while the environmental impacts of Nimiq could grow to be an issue, at it's current size it's effect on the environment is minimal.
Mixing imperative and functional styles of loop iteration has made made encounter some frustrating borrow checking issues. I am hoping there is an obvious solution I am missing but I haven't been able to convince myself that is the case. This example may look a bit contrived but it's a simplified version of what I would like to be able to do. Ultimately, this loop needs to build the odds vector directly from before the filter of multiples of three, as building it inside the for loop body will miss some odd values. I have tried a few things, like .scan() that builds an array that returns internal references, using .inspect() with a closure that reference odd and build it (but can't be read internal to the loop body). fn main() { let my\_array = (0..).take(100); let mut odds = vec!\[\]; for x in my\_array /\* really want .something(build odds array here) \*/ .filter(|x| x % 3 == 0) { /\* might be missing some expected entries in odds \*/ if x % 2 == 1 { odds.push(x); } /\* needs read access odds array in for loop \*/ println!("{}", odds.len()); } } In the real world, this example roughly maps to reading a file in one pass, keeping track of certain tokens that in it (in a vec) and applying filters that potentially remove some of the tracked tokens while having a for loop body that needs to know the information stored in the vector. Any suggestions? &amp;#x200B; &amp;#x200B;
I want to create a Rust application that will allow users to edit documents. By document, I mean an object that will contain fields and collections of other objects. The user interface will be either web-based, iOS Cocoa, or even Flutter (when its C FFI becomes available, hopefully in the near future). I want to write the business logic in Rust, and Rust to manage the state. My question is: what is the best way to keep those documents in memory? I've read that global state is an anti-pattern.
It's true to an extent; for example, Java's HashMap is not thread-safe and will throw if it gets modified while someone else is reading it, but this doesn't prevent all forms of data races. https://docs.oracle.com/javase/10/docs/api/java/util/ConcurrentModificationException.html But: &gt; Note that fail-fast behavior cannot be guaranteed as it is, generally speaking, impossible to make any hard guarantees in the presence of unsynchronized concurrent modification. Fail-fast operations throw ConcurrentModificationException on a best-effort basis.
It's next week and I'd honestly like to hear the answer!
Fair -- it's certainly possible to do better in C than "just write C". But I think even with these tools, you don't arrive at the same level of confidence as in Rust, in particular during development. Basically, "C + these tools" is still harder to get right than "Rust". Of course, once unsafe code is involved, ideally you'd use "Rust + these tools". ;)
Something I'd really like to see, not just in Rust but programming tools in general, is a tool that explains code by leveraging the AST. When I'm working with a language I'm not familiar yet, it can be difficult to figure out what a special character or some particular syntax means. Even if it only links to the relevant reference documentation or has a popup showing an excerpt with an explained example, that would be very neat. I actually worked on something like this years ago while at uni but it was for Eiffel...
I also work on the quant side in HFT :) (I‚Äôm also hacking on [hdf5-rs](https://github.com/aldanor/hdf5-rs) crate which might be hypothetically tied in nicely with dataframe-like API)
You say it's your third day doing Rust, but to be honest I don't believe you :-). In any case, this is *amazing* code for a first Rust project. Thanks for sharing it!
How does that relates to Jetbrain's efforts? 
&gt; I was referring more to the documentation. I've always found that to be Free Pascal's greatest weakness. There's definitely aspects of it that could be a lot better, yeah. It's sort of a double-edged sword in that the developers prefer to write it by hand, which means stuff like the main language reference / e.t.c. are quite high quality I think as they're actually purpose-written guides, but library-level stuff can sometimes be a bit lacking / incomplete. The main thing is that AFAIK none of the core developers like heavily doc-commented source, which is why the FPDoc utility is based around external "descriptor" files instead. &gt;and that they can't even auto-generate basic boilerplate documentation for Free Vision. There is another open-source Pascal documentation tool called PasDoc (designed for both Delphi and Free Pascal) that does work like this (and even picks up "normal" comments) that you can run over anything, FYI.
&gt; I was referring more to the documentation. I've always found that to be Free Pascal's greatest weakness. There's definitely aspects of it that could be a lot better, yeah. It's sort of a double-edged sword in that the developers prefer to write it by hand, which means stuff like the main language reference / e.t.c. are quite high quality I think as they're actually purpose-written guides, but library-level stuff can sometimes be a bit lacking / incomplete. The main thing is that AFAIK none of the core developers *like* heavily doc-commented source at all, which is why the FPDoc utility is based around external "descriptor" files instead. &gt;and that they can't even auto-generate basic boilerplate documentation for Free Vision. There is another open-source Pascal documentation tool called PasDoc (designed for both Delphi and Free Pascal) that does work like this (and even picks up "normal" comments) that you can run over anything, FYI.
And at least a couple of CSS frameworks, looks like. See the comment about the intended audience; within that space, "Rebar" was expected to be fairly novel.
&gt; Support focus mode for Rust files What is the *focus mode*? 
Right. My point here is to just be careful about declaring what's proper. And it's not just ripgrep, xsv falls into this bucket as well. Many Unix-like command line tools do as well.
This code isn't unsafe, it's only unsafe to dereference the pointer. 
Not hard, just 2 letters
Yes, it is. It's not safe to dereference the pointer afterwards though. 
&gt; What Rust features caused that? TBH, crates. Zig compiles things on a per-file basis and generates object files that are literally only *exactly* what you need. There's not much you can do about it without fundamentally changing how Rust linkage works.
Technically correct while not bringing anything relevant to the discussion... nice!
I've had exactly this experience last year when speeding up a hot loop in a rails app I work on. It was even a similar problem: listing all possible times for scheduling given some complex constraints. Re-implementing it in a ruby extension written in rust gave me about a ~30x speedup. But to avoid FFI overhead, you do have to ensure you are giving the extension a nice chunk of work rather than just calling it in a loop. I think there's a lot of room for making things faster in rails apps. Eg, one issue I sometimes see is how slow loading and serializing many ActiveRecord objects is, even if you're smart about only loading what you need etc. I have an idea for using ActiveRecord to still generate the queries (since you presumably have that all modeled nicely already), but execute them from a rust extension that loads the data and has a way to serialize it. Something like this could potentially speed up some endpoints I have that handle a lot of data.
Wrong sub
Well, congrats, because your comment is far worse. I have no idea why that comment is irrelevant to the discussion, it would've been helpful if you would've contributed instead of gleefully taking a chance to denigrate a comment.
Builders are used for scenarios where you have a lot optional parameters. In your usecase, parameters a, b, and c are all mandatory so a builder is not the solution. Instead, I would just create a function `build(a, b, c)`.
Sorry, I misunderstood the question. I thought author asked when Rust as a language will get it's own powerful IDE.
Because we get that it's not the unsafe part but it's the part that breaks unsafe invariants so the unsafe part is activelly broken. Every discussion around unsafe is like this. We are discussing invariants and a smart ass say hey this is not the unsafe part. This doesn't help at all because we don't mean it's unsafe we mean it's what makes unsafe broken. In my experience it's hard to talk about unsafe here because people want to be the technically correct ones instead of helping.
Somewhat related, in some of the projects I've worked on we've moved to postgrest for GET requests, and whenever there's special logic needed for updates or creates in other types of requests we'll do the modifications inside rails and then proxy to postgrest to serialize the underlying records to keep serialization consistent and fast as well as being able to use postgrest parameters like `select`.
Well yeah, the 'y' to 't' is easy. You've just gotta back up 5 letters. But going from 'b' all the way to 's'? Forget it! That's all the way across the damn alphabet!
You're probably right, but I just like the semantics of the builder pattern a bit more than a bundle of variables in a constructor. I think I'd be happier with named parameters or something. The application I'm working on has something like 10\~15 parameters, so it definitely has some readability advantages, but the trade-off I'd be making is for a bunch of boilerplate in the builder.
Thank you, this makes a lot of sense and clearly would get old after a while.
We‚Äôve been doing [similar things](https://twitter.com/DavidPdrsn/status/1076132784639483907) at [Tonsser](tonsser.com)
cc /u/Oxdeac001 thank you. I have changed that and impl a Classify and Distant trait.
ActiveRecord indeed has massive overhead when retrieving a large collection. Rails simply was not made for manipulating large batches of records. I have some good experiences writing plain old SQL and using ruby Struct to get reasonable performance.
&gt; &amp;[T] is an (address, length) pair. *mut [T] and Box&lt;[T]&gt; are also (address, length) pairs. So is &amp;str. Are &amp;[T] and Box&lt;[T]&gt; different? Maybe different on the outside but not the inside? &amp;[T] is a slice of an array, vec or String, correct? So its pretty obvious how to make one, but how does one make a Box&lt;[T]&gt;?
Thinking of this today and looking at the [Anymap crate](https://github.com/chris-morgan/anymap) I got inspired to see if the same abstractions used there allowed for some sort of UoW to be implemented without resorting to refcounting. Granted, it's probably not Idiomatic, but still it uses only safe Rust. It might be a totally dead end but it works and might serve as some sort of inspiration? It allows for this: fn main() { let mut session = UoW::new(); let entry = session.load::&lt;SomeEntity&gt;(); println!("{:?}", entry); entry.value().msg = "Changed".to_string(); println!("{:?}", entry); let other_entry = session.load::&lt;SomeOtherEntity&gt;(); other_entry.value().msg = "Other has also changed".to_string(); session.commit(); } Which gives this debug output: Tracked { id: 0, inner: SomeEntity { id: 1, msg: "Hello" }, state: Dirty } Tracked { id: 0, inner: SomeEntity { id: 1, msg: "Changed" }, state: Dirty } The frist entry will be "updated" to db: Tracked { id: 0, inner: SomeEntity { id: 1, msg: "Changed" }, state: Dirty } The other entry will be "updated" to db: Tracked { id: 1, inner: SomeOtherEntity { id: 1, msg: "Other has also changed" }, state: Dirty } [I put the code sample on the playground if you want to have a look](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e378f2ceb9df1aa314cf9cd100d0a681) Just remember this is something I just wanted to check out while I had a couple of hours today.
Yeah, one of the things that bugged me about Puppeteer (which uses Promises heavily) is that almost every line of your test has "await" before it, and when I experimented with futures on this project I found the same thing. I think it's because the interaction between a user and a web browser usually *is* synchronous ‚Äî people don't click a button at the exact same time as typing in some words while also navigating to a new page. And for weird cases where it is necessary to do two things at once to a tab, you can spawn a thread, because a `Tab` in headless\_chrome is behind an `Arc` and its mutable fields are behind mutexes. But maybe my assumptions about how most people will want to use the library will be wrong and I'll have to reconsider futures / tokio! &amp;#x200B;
&gt; In some cases, it's not clear to me that the API is actually right, so I'd rather hold off on writing docs until I at least have more experience actually using it Even documenting a bad API is not necessarily a waste. The documentation helps people get started, and just by having it you'll get more people giving you feedback right away. Sometimes just the act of writing can bring unforeseen weaknesses to light. Those benefits kick in at a much lower quality/thoroughness than needed for high-quality documentation though. So typically it's good to have some general documentation that introduces the problem space and your solution to it, but not so good so to have a comprehensive quick reference.
this compiles and runs just fine on [play.rust-lang.org](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=85b7cf874d5e351f96c73326e648e789), have you updated your compiler lately? `rustup update` should do the trick if you're using rustup. not sure why you're odds checking with `x % 3 == 0` and `x % 2 == 1`, the only way to check oddness is `x % 2 == 0` and `x % 2 != 0` your question is kind of confusing, but perhaps you're looking for something like this? [(playground)](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d49bcb8e098d8e29e3b140019340b445) ``` my_array.filter(|x| x % 2 != 0).for_each(|odd| odds.push(odd)); ``` that will fill your `Vec` in a nice one-liner. note that `my_array: Iterator`, so if you're using an actual array or slice then you should use `my_array.iter()`. if you're doing much more complicated logic in the closure than that though, like reading or writing other local variables, then I would recommend using a regular `for _ in _` loop. it's a bit more verbose but much more flexible with the lifetimes of the other variables. hope that helps!
Yeah, I know what you mean. I was hoping that there'd be some people who (like me) wanted this crate to exist badly enough that they'd put up with having to read the tests and source code. I'm confident I'll write the docs within the next week ‚Äî I'm interstate for a wedding at the moment, and my extended family would be pretty mad if I spent any more time on my computer than I have already ;) Re. fantoccini, check out my reply here: [https://www.reddit.com/r/rust/comments/aqgsxk/announcing\_the\_headless\_chrome\_crate\_puppeteer/egg5bll/](https://www.reddit.com/r/rust/comments/aqgsxk/announcing_the_headless_chrome_crate_puppeteer/egg5bll/). I actually hadn't come across it before, and I wish I had! Reading through the implementation is high on my list of things to do when I get back to Melbourne.
Hello!! I'm trying to use the [nom](https://docs.rs/nom/4.2.0/nom/) parser to parse a file, line by line. I (painfully) wrote a line parser, but I struggle to use it and to make the transition with the CompleteStr object. Any clear explanation of what a CompleteStr is and how to use it? If possible with simple example? Here is what I come up with so far: #[derive(Debug, PartialEq)] struct Line { list: Vec&lt;String&gt;, } named!( name_parser&lt;String&gt;, map_res!( map_res!(nom::alpha, std::str::from_utf8), std::str::FromStr::from_str ) ); named!( line_parser&lt;Vec&lt;String&gt;&gt;, separated_nonempty_list!(tag!(", "), name_parser) ); fn parse_input(input: &amp;str) -&gt; Vec&lt;Line&gt; { let result = vec![]; for line in input.split('\n') { let line = line_parser(line.as_bytes()); println!("{:?}", line); // I'm stuck here, how to modify my parser to use it line by line? } result } #[test] fn parse_simple_list() { assert_eq!( line_parser(b"ktlj, cntj, xhth "), Ok((&amp;b" "[..], vec_of_strings!["ktlj", "cntj", "xhth"])) ); } 
&gt; I think there's a lot of room for making things faster in rails apps. I'm still hoping for Rocket to reach the stage where it is a Rails competitor.
[https://www.jetbrains.com/help/idea/ide-viewing-modes.html](https://www.jetbrains.com/help/idea/ide-viewing-modes.html) Maybe they mean distraction-free mode?
In general in idiomatic Rust you usually want to have very clear ownership semantics with only one owner. Some of the possibilities are: 1) You can have the `Session` own the entities, and only return an ID through which the entity can be mutably accessed: let mut user_id: Id&lt;User&gt; = session.load::&lt;User&gt;("users/1-A"); session.get_mut(user_id).age += 1; session.save_changes(); 2) You can have the `Session` own the entities, and directly return a mutable reference: let mut user: &amp;mut User = session.load::&lt;User&gt;("users/1-A"); user.age += 1; session.save_changes(); (Although in this case this is probably not what you want as e.g. you won't be able to put the `Session` and `&amp;mut User` inside of the same structure nor you won't be able to access multiple objects at the same time unless you use interior mutability inside of `Session`; for different use cases this is sometimes acceptable though.) 3) You can decouple the objects from the `Session` and have the user own the entities: let mut user: User = db.load::&lt;User&gt;("users/1-A"); user.age += 1; let mut session = Session::new(); session.add(user); session.commit(&amp;db); --- --- Of course sometimes you just *can't* avoid having shared ownership and in those cases it's perfectly acceptable to use things like `Rc`/`Arc`, especially when you already have a preset API from another language in mind that you'd want to exactly match in Rust.
It's a shame Helix had such problems because on the surface it looks really cool. Just open a macro, write Ruby, done.
While it's probably very easy to write docs now, it's maintaining them when the project evolve that's hard. Especially for a young project, which is very likely to evolve
Yes, I didn't say it was a _waste_. I said it was _hard_. It can require a ton of effort and mental energy, especially when it's something that you aren't sure is right. I'm not trying to get into a protracted argument about documentation here. I'm just trying to counter-balance the notion that putting documentation as a TODO is bad. There are many things to balance.
``` Error 1009, Access Denied What happened? The owner of this website (deliveroo.engineering) has banned the country or region your IP address is in (BR) from accessing this website. ``` Damn dawg, what did I do
I don't feel a sense of closure after reading this post. They didn't refactor nor re-engineer the code. If it were made idiomatic and utilized parallelism, this will dramatically improve performance again. Their timeline algorithm seems like a good fit for rayon or crossbeam. The post was published after they finished their appetizers and they haven't even gotten to the main entree.
What types are you passing across the FFI? Json strings?
Yes, my example does compile with comments suggesting my preferred implementation. If we take the comments into account it would look something more like: fn main() { let my_array = (0..).take(100); let mut odds = vec![]; for x in my_array .inspect(|x| if x % 2 == 1 { odds.push(x) } ) .filter(|x| x % 3 == 0) { println!("{}", odds.len()); } } [rust-playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a04f35cceaae083a87f353a01bfc9690) Due to borrowing reasons this approach prevents me from accessing `odds` within the for loop body or anywhere else that the closure borrowing it still exists. Don't get too hung up on the evens/odd stuff as it's just meant to show the general structure. `my_array` is really an iterator over tokens and `odds` is mean to be a subset of specific tokens from the token stream prior to other filter operations. The `odds` tokens represent an vector of tokens needed to process subsequent tokens. For performance reasons, I am trying to do this in one single pass. For aesthetic reasons, it would be nice for something like this example to work. There are workarounds but none are quite as elegant as a full iterator approach would be. A simple workaround is to just do all the filtering in a big match statement in the body, so it's not the end of the world.
Wrong subreddit, you probably meant: https://old.reddit.com/r/playrust/
They also banned us Colombians. This is the wall that other guy was talking about.
Using `ruru` for the FFI, you can pass all basic ruby types like `String`/`Array`/`Fixnum` which turn into Rust types like `RString` and `RArray`, you just have to have code that checks the type given that ruby may allow nil or any other argument type to be passed. And I recommend converting to Rust types like `Vec` etc. You can then do your processing and then return something like an `RArray` of `RString` or similar, which are just regular ruby arrays and numbers on the ruby side. It's actually possible to pass any type and call dynamic ruby methods, but you'd probably get a ton of overhead doing that.
well it's impossible to say without knowing the actual code :p there is no general way to solve a problem. I wouldn't get hung up on trying to do it the pure functional or .adapter chained way. just use a for loop and get the job done, it will probably be more readable anyway.
The pro move here is to go to \`ruSt\`, saves two hops in ascii and 8(!) hops otherwise.
I'd say that's at least a couple years away honestly, for the ecosystem and dev experience to somewhat catch up. Even then, I'd be hesitant to force the borrow checker upon my coworkers, I don't want a mob after me!
Using/abusing `serde` to marshall data back and forth between Rust and a higher-level language turns out to be really powerful and eliminate a ton of boilerplate casting and validation. We've cut out tons of that kind of cruft using [neon-serde](https://github.com/GabrielCastro/neon-serde) in similar fashion in Rust add-ons for Node.
You definitely can! Be warned that an action like "clicking on the element with id "log-in-button" will turn into at least six separate method calls at the protocol level ‚Äî and that's once you already have a handle to a tab.
this won't work because you are trying to use \`&amp;mut odds\` and \`&amp;odds\` in two closures at the same time. the only way around this is to use a \`Cell\` or \`RefCell\`, but that comes with its own horribleness. just use a for loop with nested if statements, it's really not as bad as people make it out to be.
The crate you point to, seems like a good start. Netlink is what command line tools are using to talk with kernel network stack, I believe. It would really be worthwile to have a robust `netlink` protocol crate available.
I would suggest grouping some of the parameters into structs; even if they are immediately destructured on the other side of the function call, you still gain the side effect of having them all be named at or before the call site.
because rustc isn't smart enough :p this works: ``` pub fn baz(a: [u32; 4]) -&gt; [u8; 16] { unsafe { core::mem::transmute(a) } } ```
Wow, thanks a lot for digging into this! I will open an issue tomorrow.
I suppose the non-code-example parts of the docs can get outdated though.
&gt; There's definitely aspects of it that could be a lot better, yeah. It's sort of a double-edged sword in that the developers prefer to write it by hand, which means stuff like the main language reference / e.t.c. are quite high quality (IMO) as they're actually purpose-written guides, but library-level stuff can sometimes be a bit lacking / incomplete if it's not one of the areas that gets a lot of attention. I just find it all to be a mix of disorganized wiki pages and hard-to-find non-wiki pages, poorly indexed and/or cross-referenced, in general, so it's a hassle to find anything and I rely heavily on bookmarks to things I stumbled across by accident or brute force trawling in the past. Open Watcom isn't perfect, and it'd have been nice if their wiki wasn't only available in the Wayback Machine, but, 99%+ of what I need which is specific to Open Watcom responds to "If all else fails, download all of the manuals as a pack of PDFs and use Ctrl+F" and the rest benefits from C having accumulated a much more helpful stable of Google results than non-Delphi-specific stuff about Pascal. (In part, because so much Pascal stuff either predates the web or is in places PageRank doesn't know about, like files in simtelnet archives, or was hosted on sites that changed ownership and didn't preserve their archives.) &gt;The main thing again is that AFAIK none of the core developers like heavily doc-commented source at all, which is why the FPDoc utility is based around external "descriptor" files instead. In other words, they're doing the same thing that makes me hate Sphinx in the Python ecosystem, where I often have to go source-diving because a project neglected to document something and Sphinx's autodoc extension requires that, even at its most automatic, you still have to manually add an `automodule` declaration for each file. As far as I'm concerned, it's negligent to have a tool which allows you to leave something undocumented by accident or through laziness, rather than because you explicitly applied some kind of "Internal stuff. Don't document this." attribute. (Which, ideally, could be disabled with some kind of "document internal stuff" flag for bringing new contributors up to speed.) &gt; There is another open-source Pascal documentation tool called PasDoc (designed for both Delphi and Free Pascal) that does work like this (and even picks up "normal" comments) that you can run over anything, FYI. Thanks for letting me know. I'll have to give that a try next time I'm playing around with Free Vision. I already discovered one mistake that I need to report in the wiki. It's not just the color picker that's unimplemented in Free Vision. `TTerminal` and `TTextDevice` are also missing. &gt; Also the C++ version of Turbo Vision (which Free Vision was translated from) is itself open source, and I'd imagine the general concepts in the docs would translate pretty transparently between both languages as the API itself is identical. I did go looking for documentation on the C++ version of Turbo Vision at first but didn't find any pre-generated stuff and it didn't occur to me to try pointing Doxygen at it before I bought a used copy of Borland's reference for the Turbo Pascal version.
I wouldn't call it a bad practice to just use `main.rs`, but it is nice to switch if there's any chance someone would want to use this as a library.
Thank you Vladimir Levenshtein.
I‚Äôd love to go back to using IntelliJ/Clion for rust projects. The only issue I have is that the last time I used it, I got a warning that &amp;str was private. 
Wonder how things would have gone using something like Z3 for your scheduling? [https://theory.stanford.edu/\~nikolaj/programmingz3.html](https://theory.stanford.edu/~nikolaj/programmingz3.html)
can anyone explain why the following won't compile and offer any suggestions to fix it? fn heads_and_tails&lt;T&gt;(slice: &amp;mut [T]) -&gt; impl Iterator&lt;Item=(&amp;mut T, &amp;mut [T])&gt; { let len = slice.len(); (0..len).map(|index| { let (heads, tails) = slice.split_at_mut(index + 1); (&amp;mut heads[index], tails) }) } [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6d3ed2db71fcd8f9a22104ed2257e49c) the iterator is supposed to iterate over each element in the slice as well as a slice of the subsequent elements. surely the iterator and the references it yields should all have the same lifetime as `slice`?
The wiki isn't even an a official thing, just so you know. It's entirely community maintained. &gt; (In part, because so much Pascal stuff either predates the web or is in places PageRank doesn't know about, like files in simtelnet archives, or was hosted on sites that changed ownership and didn't preserve their archives.) I mean, that's true in some case certainly, but I imagine most people aren't specifically looking for DOS-related stuff. Most use of FPC is on the typical "current" mainstream targets (Linux, Windows, Mac, Arm, e.t.c.) People on [the forums](http://forum.lazarus.freepascal.org/), which are quite active, are usually willing to help also.
Really looking forward to this conference. &amp;#x200B; It is both my first conference and first overseas trip.
The article mentions doing a naive reimplementation lacking several optimisations, so I assume there's a lot of performance left on the table.
Levenstein distance doesn't actually take into account the distance between individual characters. A single replace of any one character with any other (as well as remove/insert of a single character) is considered an O(1) operation.
Since when is this considered RLS 2.0? I haven't seen anyone saying such a thing before.
You can't do this with iterators. What would happen if someone did a `collect::Vec&lt;_&gt;` on it? You'd potentially end up with multiple mutable references to the same data.
Even if that particular study had some flawed assumptions, we can do a back of the envelope calculation for PoW energy consumption. Bitcoin is currently running at [5.6*10^19 hashes/second](https://www.blockchain.com/en/charts/hash-rate). A [top of the line ASIC mining rig](https://99bitcoins.com/bitcoin-mining/bitcoin-mining-hardware/) uses 2000W to calculate 4.4 * 1^12 H/S according to its specs. Let‚Äôs assume all miners are this efficient. Then, we can multiply it out: 5.6*10^19 H / s * 2000 W / (4.4 * 10^13 / s) = 2.2 * 10^9 W = 2.2 gigawatts of power or approximately the energy consumption of [Puerto Rico](https://en.wikipedia.org/wiki/List_of_countries_by_electricity_consumption), 19 TWh/year = 2.17 gigawatts. Even if a large chunk of that energy comes from renewables, that‚Äôs still an absolutely frivolous waste of energy to accomplish *tens of transactions per second*. I can beat you by 9 orders of magnitude by installing postgres. 9! As for Proof of Stake, I‚Äôve always been confused how it‚Äôs supposed to play out long term. Proof of stake is a system where capital automatically accumulates more capital (via staking) and redistribution is impossible (by design). Crypto is supposed to be freeing, right? How can anyone be free when a chunk of their money leaves their pocket and enters the pockets of the rich whenever they use money? Why should I trust a system built on the assumption that money is evenly divided -- because if anyone has too large a stake, they can break the system! -- where money automatically becomes *less* evenly divided, simply by being used? Finally, RE: fraud, I‚Äôm not talking about the currency being used for criminal transactions -- although a currency whose primary uses are speculation and money laundering is slightly alarming, to say the least. I‚Äôm talking more about the [$1 billion worth of crypto stolen by hackers in 2018](https://www.forbes.com/sites/daveywinder/2018/12/31/how-hackers-stole-1b-from-cryptocurrency-exchanges-in-2018/), taking advantage of shoddy coding and simple mistakes. Terrifying, especially when there‚Äôs often no legal recourse to recover your money if its lost. I‚Äôm not letting my grandma anywhere near a crypto wallet any time soon, and I‚Äôm curious why you‚Äôd dedicate any more time to an obviously failed technology. 
Looks sick!
It's a shame. Btw, is this safer than transmute? pub fn qux(a: [u32;4]) -&gt; [u8;16] { unsafe { *(&amp;a as *const [u32;4] as *const [u8;16])} } &amp;#x200B;
Crossbeam provides a MPMC channel, as well as many other useful primitives.
They have the same shape, meaning they look the same to the processor or if you use a machine level tool like a debugger or core dump. But they have different meanings. When `Box` is dropped it frees memory. So we say that it is an "owning" pointer that owns the memory it points to. To prevent double-free and bad aliasing, `Box` also has to be unique. Whenever a `Box` pointer exists it must be the only pointer which owns its target memory. `Box&lt;[T]&gt;` is created by calling `Vec::into_boxed_slice`. It may also be created by `collect`ing an iterator or the `into` generic conversion. It may be useful when you need something like a C `malloc`ated array or a BASIC or PASCAL dimensioned array. The program decides how large the array needs to be and doesn't change the size later. However the elements of the array are mutable. `Vec` may be used, but `Box&lt;[T]&gt;` discourages resizing accidentally. The conversion may re-allocate and move the contents, so for large\* arrays it may be meaningful to use `Vec::with_capacity` to guarantee that the standard library won't reallocate. (\* "Large" probably means at least "larger than half of the L2 cache" and possibly even larger than that depending on performance characteristics and requirements. PC memory bandwidth is now tens of gigabytes per second so worrying about reallocating a `Vec`is likely a premature optimization unless the reallocation happens frequently.) 
CompleteStr is just a wrapper around &amp;str, so you can construct one like this: `CompleteStr("foo")` but you have to also add `use nom::types::CompleteStr` at the top. Modify your parsers to take a CompleteStr: named!( name_parser&lt;CompleteStr, String&gt;, map_res!( map_res!(nom::alpha, std::str::from_utf8), std::str::FromStr::from_str ) ); Currently you're calling your line_parser with a &amp;[u8], not a &amp;str. Just pass &amp;lines instead (or `CompleteStr(&amp;lines)`). nom's parsers return a Result&lt;(I, O), E&gt; so to get the result out, use something like this: let res = line_parser(CompleteStr(&amp;lines)); match res { Ok((leftover, output)) =&gt; println!("parsed line: {}", output), Err(_) =&gt; println!("Error occured") }
&gt; The wiki specifically isn't even an official thing (whereas the actual docs are, obviously), just so you know. It's entirely community maintained. That only makes it worse. I rely heavily on the Wiki to know what I actually need to search for to turn up useful results. If the wiki weren't there, I'd consider the Free Pascal documentation unusable. &gt; I mean, that's true in some cases certainly, but I imagine most people aren't really looking for DOS-related stuff most of the time anymore. I'm not specifically talking about DOS-related stuff... just learning to effectively use Free Pascal without the intent to use Lazarus and LCL. My initial intent was to do something "good enough" that would work on any 32-bit platform so it'd be easy to develop (ie. target DPMI but do quick-iteration test runs natively on Linux), and would target CDs and self-extracting downloads but not floppies. The idea of using OpenWatcom and writing BIOS wrappers came about when I started to realize how quickly a Free Pascal project's size grew as I started adding the necessary units and how un-optimized the `Crt` unit is. Aside from the documentation issues, I found the state of build automation with Free Pascal to be surprising (eg. The non-Lazarus automation seems cripplingly geared toward building units rather than final binaries, the Lazarus automation seems to not support cross-compilation, etc.) and the selection of pre-built cross-compilation binaries for targeting DOS to be uncomfortable at best. (If I remember correctly, at the time, I had to compile my i8086 binaries by running the GO32v2 builds under DOSEMU if I didn't want to build my own FPC from source because there were only i8086 overlay archives for Win32 and GO32v2.) That's an area where Open Watcom is best-in-class. A single install for whatever host you want can build for every supported target and, while it's not as versatile as GNU Make in some ways, WMake allows me to have a portable Makefile which works for cross-building or DOS-native building thanks to constructs like `!ifdef __MSDOS__` and `!ifdef __UNIX__`... there are whole StackOverflow answers on how to approximate that with GNU Make by switching on environment variables which just *happen* to exist or not exist based on Windows vs. POSIX... let alone trying to detect DOS vs. Windows. &gt; People on the forums, which are quite active, are usually willing to help with pretty much anything also. Maybe it's just me, but I see it as being impolite and an imposition on their time if I'm asking as many questions as I anticipated I would need to ask for that.
Build a wall and make VPNs get paid.
Yes, but `crossbeam::channel`'s channel implementation doesn't broadcast a message to all receivers; it picks randomly from whichever receivers are available.
&gt; I could also use an RwLock with a queue of messages, but then there's no way to block a thread until a message is sent. What about a Mutex and a [CondVar](https://doc.rust-lang.org/std/sync/struct.Condvar.html#method.notify_all)?
Today, I learned
I mean, I'm not necessarily married to JetBrains. Thanks for the info
You are looking for /r/playrust
I was reading the CondVar documentation just now and it seems like it would work, but it might take some time.
https://crates.rs/ https://cargofox.io/
```chmod -777``` sets your intelect to that number (don't try it, it's *cursed*).
&gt;If the wiki weren't there, I'd consider the Free Pascal documentation unusable. You couldn't find what you needed in [any of these?](https://www.freepascal.org/docs.var) &gt;Aside from the documentation issues, I found the state of build automation with Free Pascal to be surprising I mean, for example, assuming you had everything installed properly, with a directory layout where the main program file was adjacent to three folders called `source`, `lib`, and `bin`, for a simple command line build you'd just do: `fpc -Tmsdos -Pi8086 -WmHuge -Fusource -FUlib -obin/ProgramName ProgramName.pas` `WmHuge` there referring to the DOS memory model, with Tiny, Small, Medium, Large, or Compact also being valid options. FPC does also come with a tool called FPCMake that generates GNU make files from a pretty straightforward INI-ish format with lots of configuration options, which is [described here]. LazBuild just builds Lazarus project files, which do support any number of build modes with any combination of targets, so I'm not quite sure what you're referring to there. As far as Lazarus in general, it's in no way only for building LCL GUI apps, if that's what you might have been thinking. At it's core it's just an IDE (with project templates for console applications and all that) with lots of code-completion stuff you won't find elsewhere. &gt;(If I remember correctly, at the time, I had to compile my i8086 binaries by running the GO32v2 builds under DOSEMU if I didn't want to build my own FPC from source because there were only i8086 overlay archives for Win32 and GO32v2.) [This](ftp://ftp.freepascal.org/pub/fpc/dist/3.0.4/i8086-msdos/) is the latest stable release directory, and [this](ftp://ftp.freepascal.org/pub/fpc/snapshot/trunk/i8086-msdos/) is trunk. Both have Linux binaries available. I also highly recommend [FPCUpDeluxe](https://github.com/LongDirtyAnimAlf/fpcupdeluxe/releases), which is along the lines of something like Rustup, but as a GUI app (cross-platform of course.) It can build you any configuration of FPC you want quite easily (and keep in mind building FPC only takes like 5 minutes.) &gt;Maybe it's just me, but I see it as being impolite and an imposition on their time if I'm asking as many questions as I anticipated I would need to ask for that. To each his own, but I'm quite certain it wouldn't bother anyone. The description of the main forum category is literally "for general programming questions", actually.
Uruguay ain‚Äôt working either. 
Maybe it's because you can't even figure out what subreddit to rant in?
&gt; iterate over each element in the slice as well as a slice of the subsequent elements. Although you can't do that for the reason /u/asymmetrikon identified, it's possible to fix with two levels of sophistication. Let's start with the simple one that works. The idea is to iterate over indices (safe enough) and only give out a mut-ref when the iterator is borrowed. fn heads_and_tails&lt;'iter, T&gt;(slice &amp;'iter mut [T]) -&gt; HeadsAndTails&lt;'iter, T&gt; struct HeadsAndTails&lt;'iter, T&gt; { slice: &amp;'iter mut [T], index: usize } impl&lt;'i, T&gt; Iterator for HeadsAndTails&lt;'i, T&gt; { type Item = HeadAndTail&lt;'i, T&gt;; } impl&lt;'i, T&gt; HeadsAndTails&lt;'i, T&gt; { fn get_mut(&amp;mut self, ht: HeadAndTail) -&gt; (&amp;mut T, &amp;mut [T]) { let a = &amp;mut self.slice[ht.index..]; let (h, t) = a.split_at_mut(1); let h = &amp;mut h[0]; return (h, t) } } struct HeadAndTail { index: usize } This won't work with a `for` loop because you can't borrow the iterator in the loop body. (`for` takes ownership of the iterator.) But it works fine for a `while` loop. The only problem is that the `HeadAndTail` isn't linked to a particular iteration. You could collect them and apply them to something else - while this won't be unsafe it won't be right either. It's possible with a type-system hack to use lifetimes as a watermark, guaranteeing that `HeadAndTail&lt;'iter&gt;` is *only* used with the iteration which created it. If you want to dig deeper, the [source code for GhostCell](https://github.com/pythonesque/kravanenn/blob/wip/src/util/ghost_cell.rs) is where I learned them. However, that builds on concepts from the Rustonomocon: PhantomData and lifetime subtyping variance, so for now I'm only going to tease it.
I think this is a great point, actually, if I understand what you mean correctly. Here's how I'd put it: you really need to program in C (or something similarly low level), and spend time debugging confusing memory errors before you really appreciate why there's a borrow checker and why you're spending time fighting it. For one thing, this helps you understand what's going on internally when you write safe Rust. But also I think people who are experienced in Rust come to like the borrow checker, but I don't know if that can happen if they've never seen what it's like programming without it. This doesn't necessary imply that someone should learn C *first*, I think. But they probably should at some point. I mean, in some sense the answer to "what language should I learn" is "all of them, or as close as you can get" since each has its own advantages and/or teaches you a new way to think about problems. But in my option, C would be fairly high on the list. Even if paradoxically I might suggest rarely actually using C (for new projects).
r/lostredditors
What operating system(s) are you using? For CPU profiling, Instruments.app works quite well on macOS, and `perf` on Linux.
I don't see this working for actually composing *arbitrary* allocators. For example if one of them is the system allocator, trying to free an unknown block will simple be undefined behaviour -- and is correctly currently marked unsafe. &gt; Safety &gt; This function is unsafe because undefined behavior can result if the caller does not ensure all of the following: &gt; ptr must denote a block of memory currently allocated via this allocator,
I think the test has to happen at runtime because you don't know until link-load whether the process uses pthreads. It should be pretty fast regardless because the test always takes the same branch.
The additional thread must be explicitly created by the first thread, so there is no risk of an memory ordering issue.
&gt; You couldn't find what you needed in any of these? For most of them, I didn't know what I was looking for until I'd read the wiki and it didn't help that I'd run into things like broken links that really shouldn't be broken, like the current 404ing state of the "FPCMake" link in the sidebar of the Free Pascal website. The RTL and FCL references were useful, but I wound up doing the following for each of them: 1. Browse through the entire manual, taking notes on what useful stuff was where, under which names. (This is what I referred to as "brute-force") 2. Download the GO32v2 and i8086 install bundles and put together a shell pipeline to list out which units were available for which targets. &gt; I mean, for example, assuming you had everything installed properly, [...] I didn't have a problem with *that*. `fpc [...] my_root_unit` does enough and I was relying heavily enough on units that come with FPC that I was looking for something a little more versatile than `./configure` mixed with dependency handling. The problems I had were things like: 1. A piece of documentation somewhere on the site which discouraged using FPCMake "because it's really only meant for building Free Pascal itself". 2. Confusion over how to use FPCMake to cross-compile and not install a DOS executable, made worse by an inability to find a good supply of examples to learn from. 3. Difficulty finding any documentation for FPMake aside from the [wiki page](http://wiki.freepascal.org/FPMake), which doesn't cover my use cases. 4. No way I could find to either get a copy of lazbuild for GO32v2 or have the Linux version emit a Makefile that could be run using the GO32v2 build of GNU Make. 5. No clear answer from *any* of them for a more portable way to script tasks other than the basic "compile this stuff". &gt; As far as Lazarus in general, it's in no way only for building LCL GUI apps, if that's what you might have been thinking. At it's core it's just an IDE (with project templates for console applications and all that) with lots of code-completion stuff you won't find elsewhere. I'm well aware of that. My problem is that I'm very iffy about the feasibility of using `lazbuild` to target DOS and I'm willing to sacrifice the IDE if that's what's necessary to get the kind of build automation experience I'm aiming for. &gt; This is the latest stable release directory, and this is trunk. Both have Linux binaries available. Sorry about that. I was poking through it and I realized that the actual problem wasn't the i8086 binaries, but, rather, that I can't find a pre-built Linux-to-GO32v2 cross-compiler. The i8086 was fine all along but, initially, I'd intended to save myself time by using various units not ported to i8086 like the unzipper. &gt; I also highly recommend using FPCUpDeluxe, which is along the lines of something like Rustup, but as a GUI app (cross-platform of course.) It can build you any configuration of FPC you want quite easily (and keep in mind building FPC only takes like 5 minutes.) Now *that* I didn't know about and will definitely be taking advantage of next time I do something in Pascal. Thanks. :) &gt; To each his own, but I'm quite certain it wouldn't bother anyone. The description of the main forum category is literally "for general programming questions", even. Questions are one thing. Having enough of them that each answer prompts a new question thread is another.
I am on linux
This is true. However, I think that unsafe blocks should sometimes extend beyond the operations that absolutely require it, so that they contain all reasonable sources of dragons. This code snippet (which looks like it's casting away const-ness) probably qualifies. The unsafe operation would be mutating through the pointer, but the _mistake_ would be making it `mut` in the first place, and that's where I'd want my "here be dragons" sign with appropriate justification.
Any idea why?
I'm trying to understand why the compiler isn't able/willing to infer default types in some of these cases. (They are trivial to fix with explicit annotations, but I want to understand *why* this doesn't work as I expect.) // this works as I expect: the default of f64 is chosen and 8 is printed fn main() { let x = -1.0; println!("{}", std::mem::size_of_val(&amp;x)); } // this fails because x is still in the float "superposition" and rustc can't find float::abs // I don't know why it refuses to choose f64 and instead fails fn main() { let x = -1.0; println!("{}", x.abs()); } // sample output from above: error[E0599]: no method named `abs` found for type `{float}` in the current scope --&gt; src\main.rs:3:26 | 3 | println!("{}", width.abs()); | ^^^ // this fails in essentially the same way, just with "integer" instead of "float" // not sure why it doesn't just choose i32, but presumably the same reasoning applies fn main() { let x = -1; println!("{}", x.abs()); } // this also fails, which follows from the above, since size_of_val is happy to work with // whatever it gets, but doesn't have any information to further constrain x fn main() { let x = -1.0; println!("{}", std::mem::size\_of\_val(&amp;x)); println!("{}", x.abs()); } // I thought maybe this was related to macros and the order in which things are expanded? // no dice, this also fails in the same way fn main() { let x = -1.0; let y = x.abs(); println!("{}", y); } // finally a success example, which also makes sense: using the variable in a way that // explicitly requires one type works as expected fn main() { let x = -1.0; be_f64(x); println!("{}", x.abs()); } fn be_f64(_x: f64) {} In the above cases where rustc declines to infer the default type prior to doing method lookup, why is that?
Chandler Carruth is a pretty good resource: https://youtu.be/nXaxk27zwlk In Q&amp;A he said he's had bad experience with using `--call-graph=dwarf` but for the work I've done in Rust it produces much more helpful perf output. It may be inaccurate for very short functions, but that's usually not what I'm measuring.
Or just good old-fashioned progressive enhancement!
no, it's just an uglier transmute. it's even less "safe" because of the raw pointer deref, even though we know the pointer is valid; it's still just worse style. but I try not to worry too much about what people think is "more safe". if you want to transmute, and have good reason for it, then just transmute! in this case it's clearly less error prone than the original code you had.
ah, yeah that never actually occurred to me! I guess I assumed that there would be a way to make an iterator where calling \`next\` borrowed the iterator, but that isn't possible is it?
interesting. tbh this seems like way more effort than its worth; thankfully I don't actually need a fully fledged iterator lol. thanks for the detailed reply though!
`export RUSTFLAGS="-Z self-profile"` or `export RUSTFLAGS="-Z time-passes"` might be illuminating. Turn off incremental compilation `export CARGO_INCREMENTAL=0` would also be interesting if it isn't helpful.
What's the point of a raw pointer if you don't mean to de-reference it later, other than comparing it to a pointer you probably plan to de-reference? 
&gt; My problem is that I'm very iffy about the feasibility of using lazbuild to target DOS and I'm willing to sacrifice the IDE if that's what's necessary to get the kind of build automation experience I'm aiming for. If you what you mean is that you ultimately want to develop on Linux and generate binaries that run on GO32v2 DOS, you could definitely use the Linux version of LazBuild, with a Lazarus project file where the build target was set to i386-GO32v2. I'm not sure why you'd need LazBuild on the DOS end? Also, I just remembered, LazBuild actually itself has the capability to generate GNU makefiles from Lazarus project files as well, which might also be useful depending on exactly what you were doing.
If all else fails, you could even use `SetThreadContext` to have the original thread set the new thread count :-D
You may already be aware, but there are already some existing typed builder crates - [this one for example](https://crates.io/crates/typed-builder). If you get really stuck maybe look to them for inspiration.
This code is unsafe as it breaks the rule "casting shared reference to mutable is UB" Yes, it will probably work ok unless you try to use two mutable references to the same location. But compiler is free to assume that this can never happen and complitelly eliminate branch where this function is called. Yes, it doesn't do this right now, but it can.
This is not true unless you rely on implementation detail of the compiler.
&gt; If you what you mean is that you ultimately want to develop on Linux and generate binaries that run on GO32v2 DOS, you could definitely use the Linux version of LazBuild, with a Lazarus project file where the build target was set to i386-GO32v2. That's one of the things I was unsure about. Combined with FPCUpDeluxe to easily build the necessary cross-compiling builds of FPC, I think that covers my concerns for DPMI-based Pascal apps. &gt; I'm not sure why you'd need LazBuild on the DOS end? It's not strictly necessary but, for a project intended for retro enthusiasts, being able to develop on retro hardware is a nice option to have. For the Open Watcom version, in addition to using WMake as the build automation, I've rewrapped all of the lines to 78 columns to account for a maximized document having window borders in Watcom Vi's TUI.
Works fine if you live in Costa Rica. An acquaintance from Venezuela faces the same issue thou.
Why not to use serde?
Your calculation on energy usage might be completely correct, but that doesn't say anything about the impact of it. There are certain common fallacies when it comes to what people think how power production, consumption and energy prices work: * The cheapest energy is not from coal or nuclear plants. In fact, the cheapest energy sources are hydroelectric and geothermal. * The hard (and expensive) part about power supply is not the production, it's the transportation. Often enough, the required infrastructure is not present and building it up is a costly and lengthy process. This leads to the absurd situation that cheap hydroelectric power stations turn off some of their engines or run them at limited capacity, as it wouldn't be possible to transport the produced energy, even though it is way cheaper than the energy from the next nuclear plant. Mining farm operators have some very special situation: their efficiency in mining is directly related to their power costs, so they are way more incentivized to optimize on them. Over-capacity stations are extremely interesting targets for mining farms, as the produced power is cheap if you just can get access to it. As mining farms are very portable and the exact location doesn't really matter (you only need a decent internet connection), they will just move directly next to power stations with over-capacity. Now, something interesting happens: the power station increased their degree of capacity utilization. As the costs are mostly defined by one-time costs of building the station as well as costs of maintaining the engines (which also are mostly the same no matter if used on high or low capacity), this means that the average power price at the power station can be reduced. This will make it more attractive for investors to build infrastructure for power transportation, as even after transportation costs, the power might be cheaper than those of other sources. Thus the bitcoin mining would actually help society by subsidizing energy infrastructure to increase overall usage of renewable energy sources - and this happened with only very few additional damage to the environment. I am not saying that cryptocurrency mining is primarily done using such energy source with net positive outcome for the society. But research indicates that significant amounts (about 50%) of crypto-currency mining can actually be sourced to the region of Sichuan, China, where the described situation is in fact common. 90% of the energy in Sichuan is renewable. In Western world, cryptocurrency mining can be sources to regions like Canada and northern US states - again regions with high penetration of renewable energies. Some studies estimate that about 75% of the power used for cryptocurrency mining is from renewable energy sources. The more professional the mining operations get, the higher the usage of renewable energy, which also means this number could further improve. If we add the positive side effect of supporting the renewable energy industry through extra income, one could actually try to argue that cryptocurrency mining is net positive for the environment. Again I'm not going so far to say that this is true and cryptocurrency mining is not a huge waste of resources. I only want to say: It is not that easy to say that based on some basic estimates of power consumption. &amp;#x200B;
Seems to be a new upcoming feature of IntelliJ Platform: https://github.com/JetBrains/intellij-community/commit/4b735c89ed137fb56bc4dad2fc87c8025f881fcf#diff-1d80e0fcf84cb2d03cbc7668f433fc37R304
I'm writing a HashMap backed cache that can store multiple data types, for which I have an enum: enum CachedEntry { SimpleString(String), List(Vec&lt;String&gt;), } I'm using the `entry` API to return existing values, and look up and return new ones. This works, but after I insert the enum I have to match it again against the type I already know it is, which feels redundant. fn get(&amp;mut self, key: &amp;str) -&gt; Result&lt;&amp;String, ()&gt; { match self.entries.entry(key.to_owned()) { Entry::Occupied(e) =&gt; match e.into_mut() { /* snip*/ }, Entry::Vacant(v) =&gt; { /* Contrived string (real code does db lookup) */ let new = format!("{}", key); /* Is this the right way to return a reference to the just inserted value, or is there a better way? */ let entry = CachedEntry::SimpleString(new); match v.insert(entry) { CachedEntry::SimpleString(str_ref) =&gt; Ok(str_ref), _ =&gt; unreachable!(), } } } Is there a better way to get at the inner reference without matching against the item I just created and using an `unreachable!()` call? [link to playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0f0d55209bd502c481c57491a3ee67b6) Thanks for any help!
Side-comment: I don't know anything about ML or Rust, but I have this insane project in mind that I would like to use as an excuse to learn the language... building an AI to beat a friend in a three-in-a-row game, she is too good, I will never be able to beat her current score which is the double than my current. Feels like cheating, but damn, I'm bringing my own abilities to the table too lol.
How would you compare this to prost?
Would they need to though? I think that in order for any Rust framework to out-Rails Rails/out-Django Django/... would be to implement the actual framework in Rust and allow business logic to be implemented in Ruby/Python/...
As far as my understanding goes, C++ concepts solve this. Yes, not in a heavy weight (as the first iteration that never saw C++11), but I expect quality of implementation of libraries to be much better since then. 
You can mark code as @safe because there is a safe subset. But it uses a GC. You can mark things both @nogc and @safe as well. It feels more flexible in general but to achieve full safety in a general sense the GC is necessary. Also there is a proposal to be able to mark references that escape a function if the parameter is meant to be returned. I think that with a lightweight, non-full system you can get 90% of what you want without friction. I am not talking about multithreaded code. That is an area I have not investigated well. But for single-threaded code, it is enough most of the time. A bit of engineering can solve the GC problem: use and when it gets on the way, refactor to non-GC as much as possible. I do not think most errors need a full type-system check for lifetime like Rust's in practical terms. It loses a lot of flexibility. I am sold on the metaprogramming power of D and the genericity of its algorithms. It has a lot of practical uses, some focused on performance, others on convenience: - type introspection to choose the best algorithm - generate optimized code from regex or grammars (you could embed a python piece of code, for example, as a resource, and have it compile optimized). - create language bindings easily by generating boiler-plate through type-system driven metaprogramming. - design by introspection: https://dconf.org/2017/talks/alexandrescu.pdf I really think that what you can do with template metaprogramming + mixins is, in most cases, more valuable that having a lifetime prover, because, anyway, the performance-critical part of your code (in most applications, but this does not prevent D from being used without a GC) is going to be small and can be solved by engineering better, but that list of things that I can do in D can only achieved by D, and C++ is painfully close but missing for example compile-time reflection, so the bindings use case is not as ergonomic (you need macros to generate metadata).
Where else should i post it? Dude I legit never use this plattform so i have no clue ffs. Also pretty helpful since u r Not helping me either and just being funny oof
Would u help me then instead lol
Probably the most important point.
The only help I can give is to tell you to look at what a subreddit is about before posting. This one is NOT about your video game.
It is quite possible to program without the GC in D, though it is convenient and not a problem most of the time. You can subset as much code as you want to not use GC, so with better engineering it is easy to get what you want most of the time. There is also the betterC mode, that one directly strips the runtime (RTTI and GC, among others), but I think it is not necessary for most uses, anyway.
Bruh its about rust And an admin who fucking bans ppl cuz he doesnt like them personally
1) I'm not your 'bruh', and 2) Rust has no admins. It's a *programming language*, not that you'd know what that is. 
1. It means as much as *Facepalm* 2. What is this bs? Rust, the game, is developing by itsself? What r u talking about 3. I didnt got banned but i m trying to help q friend 
Alright nevermind rust is a programming language......
Never heard of that before i ll delete it asap
Awesome!
&gt;This error is fairly cryptic because the problem is fairly cryptic. To call a FnOnce closure that is stored in a Box&lt;T&gt; (which is what our Job type alias is), the closure needs to move itself out of the Box&lt;T&gt; because the closure takes ownership of self when we call it. In general, Rust doesn‚Äôt allow us to move a value out of a Box&lt;T&gt; because Rust doesn‚Äôt know how big the value inside the Box&lt;T&gt; will be: recall in Chapter 15 that we used Box&lt;T&gt; precisely because we had something of an unknown size that we wanted to store in a Box&lt;T&gt; to get a value of a known size. &gt; &gt;As you saw in Listing 17-15, we can write methods that use the syntax self: Box&lt;Self&gt;, which allows the method to take ownership of a Self value stored in a Box&lt;T&gt;. That‚Äôs exactly what we want to do here, but unfortunately Rust won‚Äôt let us: the part of Rust that implements behavior when a closure is called isn‚Äôt implemented using self: Box&lt;Self&gt;. So Rust doesn‚Äôt yet understand that it could use self: Box&lt;Self&gt; in this situation to take ownership of the closure and move the closure out of the Box&lt;T&gt;. I was doing so well in Chapter 20 of the book until I got to this. ([https://doc.rust-lang.org/book/ch20-02-multithreaded.html#implementing-the-execute-method](https://doc.rust-lang.org/book/ch20-02-multithreaded.html#implementing-the-execute-method)) A few questions: 1) Why does "the closure \[take\] ownership of `self` when we call it"? What `self` are they referring to? 2) Is `self: Box&lt;Self&gt;` special syntax only available for `Box`? 3) Are there any tickets or nightly features related to this I should look into?
So, are you trolling, or are you really that dense? YOU'RE IN THE WRONG SUBREDDIT.
Masto what? 
I'm trying to benchmark my code, so I unfortunately have to use nightly rust. &amp;#x200B; But outside of that, everything works in stable. &amp;#x200B; Is there a way to get the stable compiler to just ignore the benchmarking code, so in release mode it's using stable, but in benchmarking it's using nightly?
I am blown away by some of the stuff people make with macros. This is awesome.
There is really very little macro magic. If you are interested, look at https://github.com/synek317/shellfn/blob/master/shellfn-core/src/lib.rs Macro is only interpreting fn signature and calls one of fn from shellfn-core.
&gt; I'm trying to benchmark my code, so I unfortunately have to use nightly rust. You don't, though. Check out criterion. Other than that, put your benchmarks into a `benches` directory, that way they'll be ignored unless you run `cargo bench`, and then you only do that when you have the right toolchain. 
Thank you for the advice! I was considering splitting it into a library and then a CLI that uses the library, but was too lazy to do it. Maybe I'll do it when I have time
Thank you for the compliment! Rust has actually been pretty pleasant to use, and I actually already wrote a similar downloader in Elixir, which is a functional language, so I guess it makes the transition more seamless. The only thing that I still have to keep on watching out for is the compiler's borrow checker.
The unfortunate answer is "it depends". If the resulting assembly instruction is `inc [mem]` or `dec [mem]`, then you're probably safe. But if it's `load [mem] into reg`, `inc reg`, `store reg into [mem]`, then a signal handler can interrupt in between and you end up with the wrong refcount. 
I am using `valgrind`/`callgrind` + `kcachgrind` and its good enaugh for me.
Doesn't sound insane to be, in fact it sounds absolutely doable. Do it.
I have a custom type `LineNr(u32)` which simply wraps an integer. Is there a way to construct a `Range` of it to iterate over? As far as I can see, I'd need to implement `Step` for it, but it's [nightly only](https://doc.rust-lang.org/std/ops/struct.Range.html#impl-Iterator). I don't want that. I'm feeling like I'm missing something, though. Is there another way to do this? I could introduce another struct `LineRange` and then impl `Iterator` for it, I guess... anything better? Thanks for any comments :)
&gt; Even if a large chunk of that energy comes from renewables, that‚Äôs still an absolutely frivolous waste of energy It's not just the energy, but also the amount of heat you're putting out.
For something that simple you're probably looking at making an engine (at that level potentially one that can hold all possible states) as opposed to ML. It'll be faster and be more exhaustive because you, again, are likely going to be able to iterate over all possible states.
&gt; by default, the script is added as a last argument. You can change it using special variable PROGRAM in the cmd parameter. That's odd. I'm used ot other defaults for replacement parameters such as `{}` in `find` and `xargs`. Otherwise, that looks highly useful for interop with on-shot programs. Plans to make a `stream` variant `async` is available or a plan to provide additional dynamic parameters similar to format strings?
1) `self` refers to the first parameter of `call_once`: pub trait FnOnce&lt;Args&gt; { type Output; extern "rust-call" fn call_once(self, args: Args) -&gt; Self::Output; } https://doc.rust-lang.org/std/ops/trait.FnOnce.html 2) In the context of parameter lists, `self` is a shorthand for `self: Self`, `&amp;self` -&gt; `self: &amp;Self` and so on. The type on the right is called a _self type_ and currently there are only a selected few 3) [Arbitrary self types](https://github.com/rust-lang/rust/issues/44874) is probably what is refered to in the book. Btw, `FnBox` is [present in the std library](https://doc.rust-lang.org/std/boxed/trait.FnBox.html) and will be deprecated once the type system catches on
Maybe a suggestion could be that a callback returns a "diff" of what it wants to do with the `Dispatcher`. This will work if you just want to enable a callback to create or modify entries, but you won't be able to call other callbacks. The main benefit is that you can have a `FnMut` as the callback because you don't need a mutable pointer to the dispatcher when your calling the callback. E.g ``` type Callback = FnMut() -&gt; Diff; enum Diff { Empty, Entries(Box&lt;dyn Iterator&lt;Item = Callback&gt;), } ```
You probably don't need to do anything AI wise. A brute force approach where you search for long runs and the resulting setups (with even more runs), would possible. If the range of possibilities is too large then it will be too slow. I doubt that would be the case. Rust would fit this well because it's fast. So you can search for more moved per second. If you really want to do it AI based there are libraries out there for Rust. However the main language for AI is Python.
Love this plugin. Thanks for all the great work! I'll toss a random feature request in here: I use protobuf files a lot and they get compiled into Cargo's output dir. I load them like so, but the IDE doesn't pick them up. Would kill to have that work. ```rust include!(concat!(env!("OUT_DIR"), "/dc.rs")); ```
I wish it had a community edition.
Thanks. That crossed my mind as well, just forgot to write it down. I might be more useful for the `Fn(&amp;Dispatcher)` rather than the `FnMut()` one, as you probably want to know the current state of the dispatcher before deciding how you want to change it. However, the real dispatcher is a bit larger than the simplified example above, and having to implement all method calls as instructions is a bit daunting. I was hoping for something simpler :-)
Interesting, is [the main website for the company](https://deliveroo.co.uk) also blocked or is it only the engineering blog?
&gt;However I do wish there was a `simple_rand` crate which exposed a mostly trait-less API which doesn't need a prelude. &amp;#x200B; What would be the advantage over `rand::prelude::*`? You can import just that and use `random()` which will work for all the primitives in the standard library. And forget about rand traits and such because everything required is imported as well.
For me, Denis Bakhvalov's blog has been extremely helpful, eg.: https://dendibakh.github.io/blog/2019/02/09/Top-Down-performance-analysis-methodology
I think I agree that this library isn't really intented for running dozens of chrome instances in parallel, and if it was, then they would want to perhaps use an async library instead (although dozens of threads isn't a ludicrous amount). As a typical test runner you are only testing one instance of a Tab usually, and threads are very easy to worth with in Rust. I agree with your overall design choice. Not everything has to be tokio/async. Even with `async/await` landed, this API is probably simpler to use and reason about.
Technically you can have both `&amp;mut Dispatcher` and `&amp;mut Callback` if you will guarantee that this callback is inaccessible during the call.
Great idea! The `PROGRAM` const is likely to change, it is one of the reasons why the version is `0.0.1` and the crate is not published on crates.io :) I was experimenting with Generators but I'm not yet familiar with them. There is a huge issue currently when I return `Result&lt;impl Iterator&lt;Item=Result&lt;String, E&gt;&gt;, E&gt;`, after I start yielding lines, I have no good way to return error from the whole function when for example script ends with exit code 1. I thought that Generator, which is iterator + return as far as I understand, could solve the problem, but no success yet. Currently, I want to provide a separate implementation for return type `Vec&lt;T&gt;`. Next, I will definitely jump into your idea.
On some job listing sites you get some decent hits: * [https://indeed.com/Rust-Jobs](https://indeed.com/Rust-Jobs) * [https://stackoverflow.com/jobs/developer-jobs-using-rust](https://stackoverflow.com/jobs/developer-jobs-using-rust) * [https://www.remote.com/jobs/browse?keyword=rust](https://www.remote.com/jobs/browse?keyword=rust)
That's the `Option&lt;Box&lt;FnMut(&amp;mut Dispatcher)&gt;&gt;` version: remove the callback while you call it, then put it back in again. Or is there a better way of providing this guarantee, preferably without using unsafe code?
&gt; Easier is to not remove it, but mark as borrowed, but this way require unsafe block with lifetime trickery. I e, `RefCell&lt;Box&lt;FnMut(&amp;mut Dispatcher)&gt;&gt;`?
I know nothing about it, but https://crates.io/crates/multiqueue seems to fit your requirements? You might also want to try searching for the term "bus".
Check out unsized_locals
This sounds hard to get right. The callback could even do a `mem::swap` on the `&amp;mut Dispatcher` and destroy the old dispatcher completely. (And AFAICT pinning won't help: either the dispatcher implements `Unpin` and then you can swap it away, or it does not and then it's essentially the same as a `&amp;Dispatcher`.)
You could have the Callback take a &amp;Dispatcher and return a (optional) closure that takes the &amp;mut Dispatch. But that's not simpler. I would suggest you re-evaluate your use case. Creating lots of possibilities for what a callback can do sounds like a plus. BUT having dynamic functions that can modify the table that they are stored in is basically a poor mans scripting language. For a vaguely similar situation i am using: `Map &lt;Key , Box&lt;CallbackTrait&gt; &gt; ` `CallbackTrait` is a `fn cb(&amp;self , event ) -&gt; Result` An Error removes it from the Map. And i implement callback for things like `NewType(Rc&lt;RefCell&lt;Thing&gt;&gt;)` and `mpsc::Sender`. 
Creating it is safe. Using a raw ptr in any form is not 
Interesting question! I am definitely OK with std's or rayon's prelude, but I don't like rand's prelude for some reasons I don't quite understand. I think I am ok with std prelude, because it is ubiquitous: every crate uses `.as_ref` somewhere. I am ok with rayon's prelude because it only exports traits used to enhance std types (and it needs to enhance std types) rand's prelude, in addition to `SliceRandom` and `IteratorRandom`, exports types, functions, and traits, required to working with types from `Rand` themselves. That somehow doesn't seem necessary? Like, python's API where everything is a method on an rng and the module re-rexports methods of the default rng instance as free-standing functions seems easier to use?
No, panicing during Drop will just panic. Panicing while already panicing is an abort and that is often triggered through a panic in Drop. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=30e0c5908c64171568c6908fdc5dc6e5 (Remove the comment on the second panic to get the abort behaviour)
No, I did not know that crates.io does not refuse crates without documentation.
Thank you, I was confused by the `hex` doc saying `ToHex` trait was implemented for `Vec&lt;u8&gt;`
Thank you, I was confused by the `hex` doc saying `ToHex` trait was implemented for `Vec&lt;u8&gt;`
True. I guess this depends on whether the program is statically compiled or not, although who knows if the linker is smart enough to inline code like this? Is it even possible for it to? Traditionally, no... but linkers are getting ever smarter.
From the post it sounds like they're serializing a Ruby object to JSON then having Rust deserialize JSON to a Rust struct. However, your comment doesn't mention this so am I misunderstanding the post? If I do understand things correctly, doesn't this require you to keep your Rust struct and the target language object in sync?
Thanks for you reply!! However, the modified parser you suggested doesn't seem to work for me, and I struggle to understand why: named!( name_parser&lt;CompleteStr, String&gt;, map_res!( map_res!(nom::alpha, std::str::from_utf8), std::str::FromStr::from_str ) ); returns the following error: expected &amp;[u8], found struct `nom::types::CompleteStr` 
In brief when we talk about 'sugar' here we're talking about a syntactic construct to make things easier to write. Desugaring is the act of reducing this sugar by expanding it out to the syntax it replaces. If syntax is `sugar` then there's some other (more complex, or less pleasant) way to write it out without the sugar. For example, you might think of `?` as 'sugar' for the full-on `match` expression it replaces: ```rust // Sugary: foo? // Desugared (approx): (match foo { Ok(v) =&gt; v, Err(e) =&gt; return Err(e.into()) }) ``` 
&gt; If developers won't relinquish speed for safety in conventional memory safe languages False dichotomy that relies on the assumption that memory safe languages are slower than C or C++. One can have one's cake and eat it too, and right now the best choices for that IMHO are Rust and D. But there might be others I'm not familiar with. &gt; Or as they say, you can write cobol in any language and I guess you can write rust in any language as well. Which is why I wrote [this](https://atilanevesoncode.wordpress.com/2018/11/06/implementing-rusts-stdsyncmutex-in-d/). It's possible in D due to lifetimes as well, otherwise it wouldn't be safe. &gt; Deadlocks are harder to avoid with a language without imposing undue constraints, fairly sure there's a connection to a halting problem. Depends on what you consider to be "undue". Pony has no deadlocks because it has no locks. The type system prevents you from shooting yourself in the foot when it comes to concurrency.
Rust and C take different approaches. Rust I consider beginner friendly good error messages and immutable by default when you want to write C go unsafe.
So useful!
[Crystal](https://crystal-lang.org) is a good alternative to Ruby, why haven't you tried it?
Thanks. There needs to be some way for a callback to initiate changes of the dispatcher, and those changes are not limited to removing the current callback. Returning an optional closure that takes `&amp;mut Dispatcher` is an interesting idea, but comes with its own complications, because I assume the return type would be a `Box&lt;FnOnce(&amp;mut Dispatcher)&gt;` which does not only mean extra boxing for every call, but also it's difficult to call such a function in stable Rust. 
I am no "fanboy" but it doesn't get higher level than Rust ... I can express everything with Rust that I do with Scala with code within 1.2x words.
&gt; Is there a better way to get at the inner reference without matching against the item I just created and using an unreachable!() call? You can use `if let` instead of `match` but other than that, not really - the compiler, given the function definition of `Vacant::insert`, cannot prove that `Vacant::insert` will return a mutable reference to exactly what you inserted and nothing else, so I think you'll still need that `unreachable!()` somehow.
Building an API wrapper using reqwest. When receiving a response from the API, there may be some headers specifying ratelimits. How do I best intercept the response? Essentially what I want to do is this: fn run_request(&amp;self, request: RequestBuilder) { let response = request.send() .and_then(|response| self.update_ratelimit(&amp;response)) /// Maybe some more combinators .into_future(); } fn update_ratelimit&lt;'a&gt;(&amp;self, response: &amp;'a Response) -&gt; Result&lt;&amp;'a Response, Error&gt; { /// Do some parsing of the headers Ok(response) } This is giving me some issues with lifetimes, since I'm returning the reference, but `and_then` is requiring that I return a `Result`. Is there a more appropriate combinator I'm not aware of? Really want I want is just an intercept. I don't want to transform the `Result` I received from `.send()` in any way.
It looks like xunitüëç
[https://github.com/SunDoge/torch.rs](https://github.com/SunDoge/torch.rs) I start to write the binding for the c headers generated from ATen and c10. I plan to implement the `tensor` first, then `autograd` and model serialization, follow the same path as [torchrs](https://github.com/torchrs/torchrs), which is not in development now.
Become a competent consultant and start using Rust on a greenfield project.
Game tree search is one of the classical AI techniques. &amp;#x200B; On the other hand, frameworks for learning (and especially deep learning for neural networks) is definetly Python-centered. I think it is important to not make the msitake that AI means that it needs to be (deep) learning.
[This](https://www.arewewebyet.org/topics/templating/) will probably take you in the direction you want to go. I'm not familiar with how react-bootstrap works but are you talking about isomorphic rendering where you're rendering HTML on the server as well as sending a SPA to the client? I'm not aware of anything in rust that does this automatically, but that's not to say it doesn't exist.
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 515.5 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about job search advice. But, I'm only ~18% sure of this. Let me know if I'm wrong! Have you checked out Forbes, LiveCareer, TalentWorks? They've got some great resources: * https://www.forbes.com/sites/karstenstrauss/2017/03/07/job-hunting-tips-for-2017/#794febea5c12 * https://www.livecareer.com/quintessential/15-job-hunting-tips * https://talent.works/automate-your-job-search
It blocks
[Syntactic sugar](https://en.wikipedia.org/wiki/Syntactic_sugar) means a convenience syntax that does not add new features to the core language. You could do the same things without the "sugar", but it would involve more typing and less readability. The compiler will first parse your source code and build an abstract syntax tree. Then it will "desugar" it to turn the convenience features into something the rest of the compiler understands. Rust has some "syntactic sugar" features but a more extreme example is Scheme (and other Lisp-style languages) where the core language is very simple but a lot of "sugar" features have been added (with Lisp macros) to improve the ergonomics for the programmer.
You want r/playrust.
Thanks!
Haha you Rust game people are so funny. So first of all: This is the subreddit for the programming language called Rust. You're looking for /r/playrust. But back to topic: So you basically want to play a game that is so toxic that you choose to be a slave for other players, just so you don't get killed instantly. lol. Why do you even playing such a game?
Just guessing (would need to see more code) but I think you're still passing a &amp;[u8], hence there's a difference between signature and how you invoke the function. You have to change both parts at once. Also it's currently returning a &amp;str, I think, not a String, so your return types don't seem to match up either. 
Ever seen \`Eve Online\`
No, I choose to contribute to a group while reaping the benefits because I don't have the time span I normally could play so that I can set up multiple safe houses, workshops, and build a moderate supply network which would enable me to fight against other players at a competitive level once the server reached 2 days old and everyone is packing SA Rifles. &amp;#x200B; Rust is nothing compared to some games out there. Unfortunately it still is a time spent = reward gained which means if I have 1/3rd the time as other players I'm going to be a significant handicap. &amp;#x200B; Now, what games do you play, so I can collectively pour salt on them while making myself look like an egotistical ass?
A long time ago, I wrote [a binary patching tool](https://gitlab.com/Screwtapello/python-bps) for the BPS file format that Flips uses, but because I wrote it in Python, it's not nearly as fast or memory-efficient as Flips, and I never got around to trying out the suffix array approach. It might be a useful reference for such a thing written in Rust, though. I also set up [a binary patching benchmark suite](https://gitlab.com/Screwtapello/binary-diff-comparison-tests). For obvious copyright reasons I could only put the most basic tests in the repo, but when I actually ran them [I included a bunch of more practical tests](https://docs.google.com/spreadsheets/d/1_z5p4yXWM3cd5kk8bskbzvacZhpwJ36e-JiHuDqJWQU/edit?usp=sharing). I'd love to have the kind of test you propose in the suite, although none of the diff tools I tested support multiple files, so it'd have to be "an uncompressed tarball of files in format A" ‚Üí "an uncompressed tarball of files in format B", but that'd still be a useful test. Maybe even tests with the files in "correct" order and tests with the files in "incorrect" order, just for completeness. &gt; flips tries to read the whole file into runtime memory, which crashes for isos and dvds A CD-ROM ISO is less than 4GB, so byte-offsets can be stored in a `u32`, so a suffix array would take 700MB * 4 ~= 2.8GB of RAM. That's not very much these days. Of course, DVDs are much larger, and would still present quite a difficulty.
Dude i Just found that out Relax. I basically have never used it Thats why i deletes this article before 
[Sciter](https://sciter.com/) sounds just like what you need. It has [Rust bindings](https://github.com/sciter-sdk/rust-sciter).
I personally have had a lot of fun with [https://github.com/Stebalien/horrorshow-rs](https://github.com/Stebalien/horrorshow-rs)
Sciter is for HTML rendering in an application, /u/rhapsodhy was asking for HTML rendering for web servers. Yesod is a Haskell web framework/library.
Ah, my bad. Sorry.
Honestly thought this was by someone wanting to maintain an existing crate for a moment.
The explanation of mut==uniq is really useful! 
Game? Game? We are programmers! Mua ha ha ha! We need no games to sustain ourselves. We have only one long, unended struggle with the borrow checker!
It's clear now, thank you so much!
Think in sugar as "auto-rewrite of code" that is made by the compiler/interpreter automatically for you. Why it do it? To simplify (*to the compiler*) and/or optimize the code! &amp;#x200B; For example, this: for item in items { } Is "sugar" for iterating on an object that implement a *iterator protocol.* Most of the time, is translated internally into something like: iter = items.iter() // return a implement of a iterator trait while let Some(item) = iter.next() { } And how do this? Because the compiler work with AST (abstract syntax tree). So for example, you can define a programing language as: #[derive(Debug, Clone, PartialEq)] //Incomplete, just to show how is done: pub enum Expr { //Values Break, Continue, Pass, Value(Value), //Control flow While(BoolExpr, ExprList), ForI(String, TT::Range, ExprList), } And you "desugar" each **ForI(String, TT::Range, ExprList)** (that only work with iterators) into **While(BoolExpr, ExprList)** that is the most general form. &amp;#x200B; This is everywhere. For example, some languages define a object as just a HashMap of *Map&lt;string, function&gt;* or alike, so each time you do: customer.save(...) is desugared into call = obj["customer"] call(...) &amp;#x200B;
Streaming zip files in python and transforming them into byte array iterators if you don't have to go back is not that hard, the code i linked has a approach for that. I rather doubt the same approach could be used for a suffix array though...
That's nice! I was actually considering trying to make something similar when I get to macros, but it's good to know that a (presumably much better) solution is already out there!
I agree that it's at least a couple of years away. But I'm not sure that you'd actually hit into the borrow checker much in this kind of app... everything tends to be request scoped and used once anyway...
There are frameworks like this for PHP (written in C/C++), and they're ok, but they run into issues when you need to extend them...
That's a good compromise; I'll probably end up just doing this for the sake of keeping the code simple.
The thing about the crate is that apparently it only does ip link stuff, and i need to do ip route and ip rule, so i'm not sure how i'd go
That just seems like the worst of both worlds. I don't understand why people continually implement html-inside-language-of-choice. Let my rust files live in rust files, and my html templates in html files. These always break syntax highlighting as well. Html templating a la Tera or Slim makes sense, but this doesn't.
&gt; Unfortunately the cef-rs bindings are immature, old (doesn't build with latest Rust) and abandoned I wouldn't throw out the CEF idea just because some bindings are old. You can write your own bindings (i.e. add a function here and there as you need it). Also, why embedded? Why can't you call out to something else for e2e tests? You aren't getting much by embedding.
Really? What game is worse than Rust? It really seems like an uber-toxic cesspool. Don't get me wrong. If you enjoy this, go for it. I just can't imagine a game being even more toxic than Rust. Posting in the wrong sub like you did today happens multiple times a day here and sometimes they post gameplay videos. And literally every single video is about owning much much weaker players, killing "naked" beginners, etc. It's never something like "Hey look how cool my new base is", "yo check out what cool things I crafted", etc. 
Would it make sense to have a `Vec&lt;crossbeam::channel::Sender&lt;T&gt;&gt;` and then send a clone of each message to every sender? ``` for s in &amp;senders { let _ = s.send(msg.clone()); } ```
Yes precalculating the optimal policy for all States is totally doable. There are only about 15000 states iirc
That's my main source of inspiration, indeed ;)
Thanks!
Sounds like there's enough interest; I'll get a github repo up for discussion (as a starting point), I'll update this thread at that point.
You can look into Jetbrain's special license program for crate developers. Otherwise IntelliJ + Rust plugin is just as good, minus debugging which may not be essential. Sometimes a few `println!()`go a long way!
I was wondering how to compare Enum variants. ``` pub enum MyEnum { A, B(something), ... and many more } pub fn get_first_occurrence(value: MyEnum, vector: Vec&lt;MyEnum&gt;) { for x in vector { if x == value { return x; } } } ``` The code above doesn't work, but shows what I am trying to achieve. I don't want to have to list every case manually, but just try to match against the first matching value.
I would make the parameters mandatory on your builder struct that are mandatory. This can either be done when the builder is created or when the builder is built. I have seen both approaches.
It allows you to enter a Boolean array by typing 0's and 1's which shorter to type compared to false and true. &amp;#x200B; byte\_map!\[1,0,0,1\] \~\~\~ \[true,false,false,true\] &amp;#x200B; It really just replaces each 0 with false and each 1 with true (technically not replacing 1's explicitly, replacing non-zero values with true).
It seems to be generating an array of `bool` values, based on whether `flag` is `0` or not. the `$flag:expr` syntax says the macro accepts an expression at that location, followed by a comma(`,`). The `$(...)*` syntax surrounding it specifies that it repeats zero or more times, see [The Little Book Of Macros](https://danielkeep.github.io/tlborm/book/mbe-macro-rules.html#repetitions) for details. In the body, `$($flag != 0,)*` expands the macro zero or more times, generating an array of true/false. As for why, no idea.
It is more efficient than templating, as it generates what would otherwise be hand-rolled code.
Ah, yes that's what I meant, thanks. Panic during a drop which is called while already panicking.
Rust library is booming and I like it.
That sounds like premature optimization. It's very unlikely that template rendering speed is that important to your application and it's much easier to maintain actual HTML with some `{}` or `&lt;% %&gt;` or whatever in it than it is to maintain some custom Rust/HTML macro soup. It isn't even readable like [yew's `html!`](https://github.com/DenisKolodin/yew). Plus, with ordinary text files you can change the templates while your server is running to adjust the HTML without recompilation, something Rust is pretty slow at doing.
horrowshow and horrorshow-like crates don't have to be used to render entire HTML pages. They can be conveniently used in an API to render snippets which can be dynamically joined together. I've often used a combination of templates and horrorshow.
To compare values of a certain type, said type needs to implement `PartialEq`/`Eq`. You can derive that automatically: #[derive(PartialEq, Eq)] pub enum MyEnum { ‚Ä¶ } Furthermore, your `get_first_occurrence` is not well-typed: What should happen if there is _no_ occurence? You need to return an `Option&lt;MyEnum&gt;`. Better yet, you don't need to define this function at all (unless you use it more than twice) because Rust's standard library has some good stuff in it! // for some x vector.into_iter().find(|value| value == x) Also, you instead might want to use `position` instead of `find`, makes more sense since you know that the found value equals `x` (in case you are not sure, there is `any`).
See also the discussion here: https://www.reddit.com/r/rust/comments/ao94tb.
Dear god, this sounds like equivalent to /r/ProgrammerHumor haha guys I have no clue what I'm doing my hands are shaking rn and my code somehow just works, what a struggle to be a programmer haha. Try debugging a C++ program that segfaults so hard even debugger doesn't know what the fuck is going on.
Is that not also true of templates?
A headless chrome crate was announced just a couple of days ago https://www.reddit.com/r/rust/comments/aqgsxk/announcing_the_headless_chrome_crate_puppeteer/ I think you have to install chrome seperately, but I believe headless chrome is far more reliable than most other solutions for e2e testing.
That depends on what you're looking for. A good list is available at https://github.com/djc/template-benchmarks-rs but is missing https://github.com/lfairy/maud As you can see pretty much all libraries with the exceptions of handlebars are not likely to be the bottleneck. I don't know why handlebars is so slow in big loops, it must be a low hanging fruit to fix. In short it depends whether you want type safety in your templates (at the cost of compile times/no dynamic templates) or whether you're ok with a templating engine like the ones in dynamic languages (no type safety/slower to render). Have a look at the documentation of the libraries in the links above and pick the one you prefer.
From Witnet developer Tomasz: "We needed something which supports schema files, as the serialization is defined by the protocol."
Also from Witnet developer Tomasz: "This macro is a complement to rust-protobuf, which generates Rust code from a protobuf schema. Prost also generates Rust code from a schema, but in both cases there are some limitations on how to handle type conversions. For example, we need to check that the "Hash" message is a 32-byte array, and as far as we know prost does not provide that functionality."
To add to what /u/kevmoc said, this only applies to methods which actually specify that they throw this exception. If you have a data race on a variable, you get no warning by the compiler (though there are commercial static analysis tools like ThreadSafe from Contemplate, which can help), and anything that is happens-before consistent can happen. This is still better than the C and C++ world, where a data race on a non-atomic variable is UB, but happens-before consistency still allows for behaviour which is surprising to most people.
When I read "HTML Rendering in Rust" my first thought was "That's what Servo does". This just shows how much the web has changed since I had my own "homepage" at the university back in the 1990's. HTML was something you wrote and the web browser rendered it. I still feel the words "generation", or "composing" are more fitting in the context of the post than "rendering". We don't normally say the compiler renders an executable do we?
&gt; isomorphic rendering where you're rendering HTML on the server as well as sending a SPA to the client? I'm not aware of anything in rust that does this automatically, but that's not to say it doesn't exist. https://github.com/chinedufn/percy 
I don't find the example very convincing. Having information about aliasing is nice for some optimizations, but I think it's much *more* important for correctness. E.g. without borrow checking, the following would be legal: let opt = &amp;mut Some(String::new()); if let &amp;mut Some(ref s) = opt { *opt = None; println!("Use after free: {}", s); } Similarly: let mut v = vec![1,2,3]; let ptr = &amp;v[1]; v.clear(); println!("Use after free: {}", ptr); And the classic case of iterator invalidation... Of all the arguments in favor of the importance of aliasing, I think "slightly better optimization" is one of the weakest.
This is definitely not an issue, at least not anymore.
They tried JSON, but ended up doing what I'm proposing instead: &gt; You _could_ instead serialize objects in Ruby to JSON and then parse it in Rust, and it works mostly OK, but you still need to implement JSON serializers in Ruby. Then we were curious, what if we implement serde deserializer for AnyObject itself: it will take ruties‚Äôs AnyObject and go over each field defined in the type and call the corresponding method on that ruby object to get it‚Äôs value. It worked! So it's "serializing" from a Rust struct into a set of Ruby objects representing Ruby-runtime-native strings, objects, lists, etc. Serde provides the machinery to automatically walk a complex/nested Rust struct and build an equivalent complex object in an arbitrary serialization format (or, conversely, to walk something in an arbitrary format and produce a complex/nested Rust struct). It happens most often to be used for JSON, but it can instead be YAML or MsgPack, or V8 or Ruby objects, or whatever else. &gt; doesn't this require you to keep your Rust struct and the target language object in sync? Yeah, for usecases where that's desired, you'd probably want to operate on the Ruby or JS object directly via whatever introspection mechanisms are offered by ruru or neon or whatnot, and serde probably wouldn't be a good approach. For lots of cases, though, you want to communicate some set of inputs that might be complex, do some set of complex/slow calculations, and produce some set of results that might also be complex (I pass in two points, and I want the geometry of the most efficient route between them, or whatever). In those cases, you're not mutating some object that persists, so there's nothing to keep in sync. You can just copy back and forth.
Nah, this one is safest. To prevent loosing callback due to panic you can craft a type that will put it back on drop.
Thank you for the explanation! I just wanted to make sure I wasn't missing something.
I can't see those example in the link I shared. Is that another page?
Do you have any docs to back up the claim that this code is UB? I thought const-cast was well-defined.
Sorry, I wasn't clear about what I thought had to stay in sync: I meant the schema has to stay the same (so struct members can't change their type or get renamed etc. without updating your target language object. New members might be ok if they're optional otherwise also problematic)
Yes, multiqueue is what I ended up using. As I mentioned in the post, the bus crate didn't fit my requirements because it only permitted a single sender. I don't know about busses in other contexts.
Just expand it in your head like a template: for each expression followed by a comma (`0,`) generate `{} != 0,`: static TOKEN_MAP: [bool; 256] = [0 != 0, 0 != 0, 0 != 0, ..., 1 != 0, ...]; As to what it's doing, it's returning true for bytes corresponding to any of: !%'*+-.0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyz~ Using a lookup table is probably faster than the branchy logic you'd be using otherwise.
I think you might have mistaken the example for the point. OP is about correctness. It just uses optimization as an example of where aliasing could make two seemingly equivalent versions of a function mean different things.
Ditto, I enjoyed using it for not-too-complex HTML structures.
Ah I see. For deserialization, serde can be configured both with default values to fill in missing fields, and optionally to ignore extra/unexpected fields. If you rename fields, though, or change their type, yes, you'd need to update the definition. At some point, no matter what the mechanism is, the two things that are talking to each other have to agree on format -- there's not really any getting out of that whether you do it manually or using a helper library like this. This just cuts down on some of the busywork.
It's mostly due to the LLVM (LLVM IR -&gt; binary) phase. Which is partly due to LLVM being relatively slow, and partly due to Rust handing LLVM relatively bloated LLVM IR compared to other languages (consider the LLVM IR for an iterator chain vs. a classic for loop). Using trait objects rather than generics is one thing that you can do to speed up compile times a lot.
You're looking for /r/playrust
oh sorry lol. thanks for the headsup
Also, IMHO you probably want to avoid reentrancy (i.e. nested handler calls), which opens up a huge can of worms all by itself. Better to have a queue to which further call requests will be deferred until you get back from this callback. Perhaps not everyone agrees with this, but having debugged complex nested handler bugs several times in several languages, I now avoid the pattern.
In the case where one is working with a heterogeneous collection, is dynamic dispatch via trait objects always going to be slower compared to monomorphization via an enum with variants for each type in the collection, or is there a tipping point at which dynamic dispatch becomes more performant?
Last commit in cranelift is month ago. Is it alive? Does someone know the current state of things?
Creating two mutable references to the same thing is unsound. It's way way worse than just using a raw pointer.
Then the optimization would be incorrect. But C allows aliasing (for pointers of the same type, without `restrict`) and still has a reputation for being amazingly fast. Dont get me wrong, I think it's amazing that Rust has this information in the type system, but LLVM already does a great job and can often use analysis to obtain sufficient aliasing information to make this optimization correctly. Indeed, we currently [do not](https://github.com/rust-lang/rust/issues/54878) communicate aliasing information to LLVM due to bugs, so the example in the documentation is currently theoretical.
The last commit was 4 days ago (see the second commit in the list https://github.com/CraneStation/cranelift/commits/master). I suspect that the "latest" commit shows as 28 days ago because a commit that was sitting around in a pull request for a while was recently merged. For progress on CraneLift as a Rustc backend, you may be interested in following this MVP milestone: https://github.com/bjorn3/rustc_codegen_cranelift/milestone/1
I'm not the op, and I haven't had a similar case, but for what I know, Crystal is not a letter to letter equivalent of ruby. So you can't just run your ruby code on crystal and get a speed burst. So you're back to using crystal side to side with ruby, if you want to avoid a rewrite at all cost. It's true that the similarities between crystal and ruby would help the said rewrite, but it still won't be an incremental approach. Writing another service in crystal is not possible, based on what they said in the article. So your last option is writing Ruby extensions in Crystal (maybe that's what you meant from the beginning). And from what I can gather, there [some](https://github.com/manastech/crystal_ruby) [POC](https://www.slideshare.net/mobile/AnnaKazakova/how-to-write-ruby-extensions-with-crystal), but nothing as efficient as ruru or plain old rust C ffi bindings. And more, Crystal has concurrency with coroutines, but no parallelism, everything runs under one thread. All that said, I think Crystal looks really good, it just needs more time to mature and gain more features, it's just not yet here for deliveroo needs. 
This formatting doesn't work on old reddit. You gotta put 4 spaces before each line // Sugary: foo? // Desugared (approx): (match foo { Ok(v) =&gt; v, Err(e) =&gt; return Err(e.into()) })
What do you mean? There was a commit 3 days ago
It depends, if you do certain things with traits and trait bounds all of the time is spent in verifying soundness of trait bounds. Other times almost all of the time is spent in LLVM. If you have a slow build and lots of trait bounds on things, you can sometimes speed it up (a ton) by grouping common sets of trait bounds on an empty trait, and use that as your trait bound. 
&gt;It's mostly due to the LLVM (LLVM IR -&gt; binary) phase. sometimes! sometimes its 90% in rustc 
Sorry if this sounds stupid, but would it be possible to write a compiler for Rust in Rust that was faster? Isn't the Go compiler written in Go for example? It's blazingly fast.
&gt; i have no clue Some things are obvious. :lol:
Go compiler does not optimize as much as rust ones does, also it's not using LLVM.
 I feel like rust will dominate embedded systems programming, game engines and other heavy duty tasks. Elixir/Phoenix will be the next Ruby on Rails and there will always be JS because it dominates the search engines/clear web. 
This helped a lot. Have a much better understanding now.
https://docs.oracle.com/javase/7/docs/api/java/util/ConcurrentModificationException.html
I don't remember the page, but IIRC even creating `&amp;mut T` out of `&amp;T` is considered UB. Casting pointers is OK though.
The rust compiler (rustc) is written in Rust. Most of the compilation is spent in LLVM which would be a huge investment to rewrite (while retaining the excellent optimization LLVM offers). What's currently being done is adding a layer to rustc (MIR) that enables doing some heavy lifting before LLVM does it's thing.
Read the `netlink` protocol, and add the missing bits for routing. Might be a bit challenging, but I'm guessing a lot of the basics might have been taken care of already.
True, at this point it almost feels like the meme about JavaScript developers jumping on the next great framework every other month, but I'm this case it's another programming language.
As someone who spends a lot of time on variable naming, clone.clone() pains me.
What kind of other methods does the Dispatcher have? It seems like that is pretty important info for this design question. You're asking how to pass a `&amp;mut Dispatcher` to the callbacks, but maybe the better answer is to split up Dispatcher so you can more easily pass its state mutably to the callbacks? It's hard to say without seeing it.
r/playrust
Not a big fan of prefixing the functions with `calc_`.
I've tried https://github.com/DenisKolodin/yew and it looks what you are looking for. I've saw other projects but did not try them I've build a multi player game with it and actix and websocket. [the code is here](https://github.com/mardiros/wasm-othello) but I don't use yew intensivelly because the game is in a canvas. [demo](https://othello.gauvr.it/) Good luck
&gt; I don't see this working for actually composing arbitrary allocators. I never said it did. I don't even see why this is relevant: if you are writing your own allocator, certainly you control how you write it. &gt; For example if one of them is the system allocator, trying to free an unknown block will simply be undefined behaviour -- and is correctly currently marked unsafe. In my case, `malloc` is the last allocator of the chain, so whenever it gets a non-NULL pointer, it's its own to handle. &gt; Not regarding the potential overhead of having to check arbitrary blocks [...] Actually, this strategy is used in a low-latency trading application *specifically* to avoid arbitrary overhead on memory requests (including deallocation). It indeed requires the underlying allocators (except for the last) to have a cheap way of identifying whether the block to be freed is under their responsibility, or not. How such allocators achieve this is up to them; the ones I wrote make it very cheap (few CPU cycles).
For my information, is it worth working with [uom](https://crates.io/crates/uom) (dimensional analysis) to contribute at physical correctness proof of your code?
`clone_shark` would have been somewhat more readable I think.
Yes, what I'm saying is that is a normal property of examples. For another case, the jukebox application featured in code examples in TRPL is also purely theoretical.
Nice, I've always wondered how the sunrise and sunset times were computed. One small remark: you should probably exclude your `.idea` folder from the crate (there's a `Cargo.toml` option for that).
 &gt; What kind of other methods does the Dispatcher have? As planned, there are three or four different maps in the dispatcher, and one of the maps also contain information on how two different maps relate to each other. There is a string interner as well. For every map, the Dispatcher contains methods to add and remove callbcks as well as get/update the relation information. &gt; maybe the better answer is to split up Dispatcher so you can more easily pass its state mutably to the callbacks? Hmm, but I still want a callback to be able to, e g, add another callback in the same map, so just splitting the Dispatcher into, say, the three different maps will just create more boiler-plate without solving much. 
[cranelift](https://github.com/CraneStation/cranelift) is written in Rust
Type safety would be fantastic. I've started (but not finished nor published) an attempt at creating code for writing and reading HTML and XML from XML Schema, Relax NG or DTD.
What do you mean by "P2P computing", exactly?
clone_shark doo doo doo doo doo doo clone_shark doo doo doo doo doo doo clone_shark doo doo doo doo doo doo clone_shark doo doo doo doo doo doo clone_shark! It had to be done.
https://ggez.rs
A commenter on r/programming was kind enough to copy/paste the content for another user barred from seeing it. Content at: https://www.reddit.com/r/programming/comments/aqonpk/moving_from_ruby_to_rust/eghv5yi
Content at: https://www.reddit.com/r/programming/comments/aqonpk/moving_from_ruby_to_rust/eghv5yi
Content at: https://www.reddit.com/r/programming/comments/aqonpk/moving_from_ruby_to_rust/eghv5yi
Note that's 17x including all the overhead of converting from Ruby to Rust, and then from Rust to Ruby on the way back. Depending on the frequency of conversions, this may be non-negligible.
On mobile right now, but couldn't the `.map(..).flatten()` be shortened into one `.flat_map(..)`?
`callgrind` is nice for CPU-bound applications, but for memory-bound applications it's quite unfortunate that it doesn't model 3 layers of cache.
For benchmarking, I can only recommend the [criterion crate](https://crates.io/crates/criterion): it's simple to use, and produces quality results. For profiling, you'll want a dual approach: - timing, - profiling. Then it's a back and forth: identify bottleneck with profiling, smooth it out, check impact on timings, do it all over again until performance is satisfactory. --- Timing matters because sometimes you can mis-identify a bottleneck, say optimize for CPU cycles when cache misses where the bottleneck, etc... by carefully going back and forth, you ensure that any "optimization" actually yields substantial improvements. Also, remember: algorithm improvements generally trump micro-optimizations.
Erm, or just `shark`, since variable rebinding is a thing. 
Thanks but I don't think articles like this should be accepted here. If it's excluding a huge part of the userbase it shouldn't have a place here.
Regarding ML, TF &amp; PyTorch have C/C++ backends that use CUDA &amp; ROCM. I read in an article (I don't remember where) that it's possible or easier to create a framework that uses the graph way of TF in languages other than Python like Javascript. Is it easier to do this and reuse CUDA kernels?
True. I'm just saying that this kind of pattern is difficult to implement with Rust's mutability rules and when I've encountered it in the past it indicated that I had a type with too many responsibilities. I think some form of lazy updating, whether it's returning operations or a closure to do the mutation, is your only real option with this type.
&gt; But C allows aliasing (for pointers of the same type, without restrict) and still has a reputation for being amazingly fast. Languages without explicit handling of aliasing at the type level are only fast due to ugly workarounds. As an example, the Eigen C++ matrix library requires you to manually annotate operations with [`.noalias()`](https://eigen.tuxfamily.org/dox/classEigen_1_1MatrixBase.html#a2c1085de7645f23f240876388457da0b) so that it [can lazily evaluate an expression rather than allocating a temporary](https://eigen.tuxfamily.org/dox/TopicLazyEvaluation.html). Omitting `.noalias()` can lead to a several-times slowdown.
I would not call that a queue; it's a bit confusing. For polling readers, the easiest way to implement what you want is to take a look at the LMAX Disruptor pattern: - A ring buffer of (generation, item). - Multiple writers, using an atomic "generation", which doubles as index (= generation % size). - Multiple readers, each keeping the last read generation locally. If your items are too small to fill a cache line, you may want to look at interleaving streams to avoid as much contention as possible.
Yep, but I ain't OP.
&gt; Sometimes a few `dbg!()`~~`println!()`~~ go a long way! [`dbg!()` macro](https://doc.rust-lang.org/beta/std/macro.dbg.html) was stabilized in 1.32.0! Spread the awareness! 
I have no idea what the official guidance on the matter is; I've polled my fellow moderators about the case. Unfortunately, I checked both r/programming and r/ruby threads and could not find a single reply from someone at deliveroo who could explain the reason. If someone has a twitter account, it may be possible to ask https://twitter.com/Deliveroo?lang=en
Tbh the whole triple map is pretty overkill. I'd much rather have a simple .flat_map(|kind| { let shark = kind.to_string() + " shark"; // ... 
There's just not much to do when compiling Go. No monomorphization (generics), no borrow checking‚Ä¶ there's nothing interesting happening, Go is basically "C with concurrency and GC".
I have a huge amount of experience in Python, but I lack the knowledge in ML. if there is a lack of libraries in Rust, no problem, my main goal is to grasp Rust anyway.
&gt; Rebar is a graphical dataflow language I was thinking about the Erlang thing too, was a bit confused there.
Thanks, but I think we should make a rule that if that happens the post is removed, if they fix it they can post it again. /r/Brasil has a bot that uses https://outline.com/ to break paywalls and blocks like this, maybe we should think about it?
The board look like this: [https://i.imgur.com/QAep6MM.png](https://i.imgur.com/QAep6MM.png) It's a 9 by 9 with 5 distinctive figures, if my math is correct, the amount of possible states are 12021.4658873087
&gt; I had a type with too many responsibilities. Yeah, it's hard to get it right. But it's also easy to fall into the other pit, where you have to wade through generics and boiler-plate types to get to some code that actually does something...
If you run a Nightly rust with the `-Ztime-passes` option you can see how long each phase is. E.g. `cargo +nightly rustc -- -Ztime-passes` (I think that's the right invocation). Note that some phases have measured sub-phases, in which case the sub-phases are indented and show up *before* their parent phase. Also, code generation is parallelized and so those phases can show up in a confusing order. As others say, code generation (done by LLVM) is often a big part of the time. Within the front end of the compiler, macro expansion, type checking and borrow checking are often (but not always) the slowest phases. Conversion between the different IRs usually doesn't show up as significant.
I like Askama. It does jinja templating, I have my html files, and my Rust files. But it does compile the html templates into the binary and checks any types used in the template. Compile time verified templates has saved me some headache. 
And this is what LLVM does in Rust? I'm not saying that Go compilation is as complex as for Rust, I'm just asking if it could all be done with Rust without any external tools.
&gt; It's possible things have changed in the last decade. If anything, the penalty has gotten *more* expensive, because core counts have increased, and so the overhead of cache / memory coherency across all of those cores has increased. I do this kind of analysis all day long. The effects of interlocked access, and of cache lines bouncing between different cores with high frequency, are probably *the* dominant effect on CPU performance. Not clock rates, not DRAM bandwidth, but cache locality and interlocked access. 
I think there is too much cloning, and the into\_inner is not necessary, I tried to improved the code, but probably could be better: use std::iter; fn main() { ["Baby", "Daddy", "Mommy", "Grampa", "Grandma"] .iter() .flat_map(|kind| { let shark = format!("{} shark", kind); let shark_clone = shark.clone(); iter::repeat_with(move || format!("{} {}", shark_clone, &amp;" doo".repeat(6))) .take(3) .chain(iter::once(format!("{}!", &amp;shark))) }) .for_each(|shark| println!("{}", shark)); }
Yeah but all the other sharks have two-syllable descriptors. Purely looking at the meter, `clone_clone shark` is an objectively better fit.
I think some of this is a little pedantic. But most of it is entirely valid. Sorry OP, if you‚Äôre going to be technical it needs to be right. 
It sucks thou because I feel like cheating, she has played like 4 or 5 times and has a score of 510025 (mine is 243325 with hundreds of sessions under my belt), it's hard for me to focus (was diagnosed with hyperactivity when I was a kid). My other option is to build this thing, ensure it works and can get high scores, and then use it to highlight the best moves, so I can sit down and train with an aid, after that I can try by my own and beat my friend's ass in this game; It feels fairer. I'm excited by the challenge, nevertheless, haha.
It seems like each `"_ shark"` string is cloned 4 times, even though they'd only be cloned 3 times ideally. Can that easily be fixed (other than translating the inner `iter::repeat_with` to a for loop)?
&gt;Game tree search is one of the classical AI techniques. Ah, this is going to serve well for my research before starting. Thank you.
`shark` is already used in the `iter_once` later on, so I don't think that would work.
`hyper` is a really low-level library. You should look at `tide`, `tower`, `gotham`, `rocket` or `actix-web` if you want something easier to use.
Not sure what you search but there is a rus implementation of https://libp2p.io at https://github.com/libp2p/rust-libp2p
Further on top of that people in /r/ruby pointed out that most will be DB access, and that ActiveRecord is really inefficient.
I only want to have a single https request and return the body as &amp;str that*s all. It seems impossible.
You should try reqwest, it's made expressly for that purpose.
Client or server? For client, try `reqwest`, as someone else said.
didn*t even compile when I tried. some cc: linker errors.
Thanks for your answer! The only thing is that I want to compare the variants only, not the potential contents that the variants might hold. implementing PartialEq would require PartialEq being implemented on the contents as well, which is what I don't want to compare. Hope you can help!
I felt the same until I found reqwest. A mention of that should be on hypers site. 
It only needs to be implemented on the contents if you derive it automatically. You can implement it yourself manually and do something like: ``` match (a, b) { (A, A) =&gt; true, (B(_), B(_)) =&gt; true, _ =&gt; false, } ```
Why wont this compile: `fn main() {` `let answer = run_it(Box::new(|| 1 + 1));` `println!("Answer: {}", answer);` `}` &amp;#x200B; `trait FnBox {` `type Output;` &amp;#x200B; `fn call(self: Box&lt;Self&gt;) -&gt; Self::Output;` `}` &amp;#x200B; `impl&lt;F: FnOnce()&gt; FnBox for F {` `type Output = F::Output;` &amp;#x200B; `fn call(self: Box&lt;F&gt;) -&gt; F::Output {` `self()` `}` `}` &amp;#x200B; `fn run_it(f: Box&lt;dyn FnBox&lt;Output = i64&gt;&gt;) -&gt; i64 {` [`f.call`](https://f.call)`()` `}`
Got it, that makes sense, thanks! A question you might be able to help with: type-specific encoding looks to be the same as [ToSql::to_sql](https://docs.rs/postgres-shared/0.4.2/postgres_shared/types/trait.ToSql.html). Is it? Is that useful at all to help you be compatible with more types, including [arrays](https://github.com/sfackler/rust-postgres-array), [ranges](https://github.com/sfackler/rust-postgres-range), [enums, domains and composite types](https://github.com/sfackler/rust-postgres-derive)? It's academic as I don't actually have a use for that but just wondering!
Having a tough time getting this to work with WSL on windows. Ctrl commands are just not recognized by tmux. I also don't know how to get it to just launch bash.exe instead of powershell.
The clone_shark gave me a bunch of clones of myself and now wants to repo me because i can't pay it back
I've been having great fun with gtk-rs: [https://github.com/gtk-rs/examples](https://github.com/gtk-rs/examples) The [cairotest.rs](https://cairotest.rs) is where I started. IMHO they should not have put all the drawing code right inside the closures. Pull those out into separate functions first and then you'll be drawing what you like. By going this route, you'll also be preparing yourself to build full desktop applications in Rust. 
It doesn't matter what language compiler is written. rustc is already written in rust, but it's just a front-end to LLVM (just like go). Problem is rustc has to spend a lot of time expanding macros, generating functions from traits, running borrow checker, generating all the abstractions in the world, then LLVM will try to optimize that mess. That's why rust is so slow in non-release builds.
Already exists, with some caveats: https://github.com/thepowersgang/mrustc
That was awful.
I'm new to Rust, and I'm looking to switch over to it at some point away from C++. I really admire the idea of an incredibly stable program. One of the biggest benefits to Rust (imho) is Cargo. I love the tool and how easy it is to use. So I was hoping that Rust would be easy to use for Android development, but a lot of the learning materials I've found online seem to be based in Ubuntu; which I'm not opposed to, but is there any simple guide for Windows on getting a Rust JNI app to my phone?
It should compile -- what errors, and what operating system and toolchain are you on?
I got teleported back to my summer camp years.
(Go is not a front end to llvm)
No, rustc does that. LLVM just gets *a lot* more code to compile. Rust and C++ both prefer static dispatch for generics (because performance, zero overhead yay), so what happens is monomorphization ‚Äî instantiating all the generics/templates into actual code. That's usually done by literally copying it and substituting the type parameters. So if you use `Vec&lt;A&gt;`, `Vec&lt;B&gt;` and `Vec&lt;C&gt;`, that's three copies of the `Vec` code. As others have already mentioned here: LLVM is not always the bottleneck; there will be a Cranelift backend for debug builds (as an alternative to LLVM); Rust will *never* compile as quickly as C or Go because it's an inherently more complex language, like C++ and D. It trades off compilation time for runtime performance.
What have we unleashed... 
This is a rust compiler written in c. OP was wondering about a source to source compiler from rust to c. 
Here's my take based on what you wrote: use std::iter; fn main() { let doo = " doo".repeat(6); ["Baby", "Daddy", "Mommy", "Grampa", "Grandma"] .iter() .flat_map(|kind| { let shark = format!("{} shark", kind); iter::repeat(format!("{}{}", shark, doo)) .take(3) .chain(iter::once(format!("{}!", shark))) }) .for_each(|shark| println!("{}", shark)); }
I am trying to write a simple reddit bot. I have created a bot account and an app. Now the bot user needs to authorize my app to read subscribed subredddits. For that I need to load a page GET request and I want to display that page in a rocket server, but that's not possible. Even with reqwest it's not possible. #[get("/auth/&lt;client_id&gt;/&lt;secret&gt;")] fn auth(client_id: String, secret: String) -&gt; &amp;'static str { let mut link = String::new(); link.push_str("&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;&lt;a href="); link.push_str(&amp;uri(&amp;client_id)); link.push_str("\"&gt;Authorize App.&lt;/a&gt;"); link.push_str("&lt;/div&gt;&lt;/body&gt;&lt;html&gt;"); &amp;link } Reqwest could be used for the next POST request. But I have acutally no clue, how in the world I am supposed to write a useful app with rocket and reqwest. The error now is &amp;link.clone() | ^------------ | || | |temporary value created here | returns a reference to data owned by the current function &amp;#x200B;
Okay, that can be done easily, too: #[derive(Debug, Eq)] enum Foo { Alpha, Beta(u32), } use std::mem::discriminant; impl PartialEq for Foo { fn eq(&amp;self, other: &amp;Self) -&gt; bool { discriminant(self) == discriminant(other) } } #[test] fn equality() { assert_eq!(Foo::Alpha, Foo::Alpha); assert_eq!(Foo::Beta(100), Foo::Beta(23)); assert_ne!(Foo::Alpha, Foo::Beta(47)); } /u/asymmetrikon: No need to do it manually thanks to `std::mem::discriminant` :)
mrustc compiles to C.
Thanks, I think I get it now.
You should return a String, not str.
&gt; it's an inherently more complex language, like C++ and D Well D compile times are pretty damn fast, so I don't think it's impossible...
My kid is obsessed with this song, and now I find it here. I guess I have to just embrace it, for there is no escape...
&gt; micro controllers that they can't run rust on I can't remember the name of the architecture, but someone was telling me about a chip that used a DSP for floating point math. And gcc was the only compiler they knew of that could compile floating point code for it. They wanted to use Rust and that turned out to be the thing that prevented them from doing it.
I'm really hopeful for [azul](https://azul.rs/). Still no releases because the core is still being heavily worked on, but I think they're hoping to have a first release sometime soon. 
ip route is supported to some degree in my pnetlink crate.
You can append `?ts=4` or `?ts=2` to Github URLs, and it'll change the rendered tab width: https://github.com/jquery/jquery/blob/master/src/core.js?ts=2