Run "cargo doc --all --open" inside the root folder of your rust project and jou should get the documentation local build.
Yep. In the code I linked, I use: unsafe fn generic_cb(obj: *mut lv_obj_t, cb_type: Action) -&gt; u32 { let tr = pros_sys::lv_obj_get_free_ptr(obj) as *mut Callbacks; if let Some(cb) = ptr.as_mut() { cb.call(cb_type); } 0 }
&gt;Primarily measure postfix (what .await provides) vs prefix They should've allowed both syntax, so that people could test what is missing in both syntax. The way it is now is just so that we can "get used to it" by force. And we will, because that's how humans are. But it doesn't mean it is a better syntax. We will probably see posts like "I prefer prefix, but postfix is okayish". It is not clearly better to break the consistency of the language. It just unnecessary add more weirdness of the language .
Thanks, I will give that a try today when I get on the VPN. For info sake I had to install rust via yum (using RHEL 7) and due to restrictions I cant even run the rust install script. So I hope this works.
For this solution ou should not need an VPN. Find packages via https://crates.io/ (no rs domain), add them to our rust project, run the command I posted above, profit! :)
I don't want to admit it, but I would use Python or Typescript simply because you can code way faster. With exceptions of course, wouldn't use them (at least JS) for financial products etc.
My bad, I'll flesh out the examples. ### Partial Polymorpherization The goal is to be able to polymorphize only part of the arguments, as some may be required to remain generic: // #[momo(x, y)] fn function&lt;X = Into&lt;String&gt;, Y = AsRef&lt;str&gt;, Z&gt;(x: X, y: Y, z: Z) -&gt; Vec&lt;Z&gt; { inner(x.into(), y.as_ref(), z) } fn inner&lt;Z&gt;(x: String, y: &amp;str, z: Z) -&gt; Vec&lt;Z&gt; { // } ### Arbitrary Trait Polymorpherization The goal is to be able to require that two arguments be of the exact same type (T) even when the function can be written only against a trait, without paying the cost of monomorphization (on those arguments): // #[momo(t0, t1)] fn function&lt;T: Trait, Z&gt;(t0: &amp;T, t1: &amp;T, z: Z) { inner(t0, t1, z) } fn inner&lt;Z&gt;(t0: &amp;Trait, t1: &amp;Trait, z) { // }
As cool as that project is, it's such a shame it's not GPL licensed...
Sloth is a one-of-a-kind command-line rasterizer for the command line. It can render real-time, static images, and now javascript for the web of 3D models (obj/stl). It is a pure rust rasterizer, and renders the models itself, and using crossterm, works on Windows, Mac, and Linux. You can compile and run it here. https://github.com/ecumene/rust-sloth/
Awesome!
There's an implementation of `cat` written in rust called [bat](https://github.com/sharkdp/bat)
We're the one that's not for official business, so it's more fun. like the "internals" forum and the "users" forum on rust-lang.org
Nah, just a typo. Correcting it now.
This looks cleaner than what I was doing. Thanks.
That would be awesome!!! You free to send pull requests!!! :3
It builds the docs locally from the source code; no internet required, let alone a VPN.
I could add GPL as second license, but would be GPLv2 only. MPLv2 and GPLv2 are quite similar in many ways though
I have to use my org's VPN to log into my dev servers. From there many sites are blocked.
Ah, gotcha.
Crazy man, love it. Great work :) Now all we need is to make 3D listing of directories and we can have interfaces like the old 80s hacker/nerd movies. [It will be unix, and we know this.](https://giphy.com/gifs/scene-park-unix-nhKW2pvXwI8mc)
That looks like the most annoying way to operate a computer... I'm on it ;)
The main thing I would worry about is availability and maturity of drivers for Cassandra etc. If nothing else, you might end up doing some FFI work to adapt a C/C++ library to Rust.
As long as the crates can be downloaded, it should work.
My guess is it's a company VPN, that they are required to use for work, and within that network there are certain firewalls in place. Lots of companies do this.
Have you tried to force DNS over HTTPS?
Nice. It seems that there is very much space to scroll on the right in the demo. Is there a reason for that?
This is cool! Is the renderer translated into javascript and ran directly in the browser, or is the scene prerendered?
Might I ask why a company would block a whole TLD range?
Government, lots of stuff is locked down.
Including all of Serbia? What other countries are blocked?
I dont know specifically, but many TLD outside of US are blocked. I am not in NETOPS or security so I cant answer why or how many sites.
Hi there! I'm working on a pure-Rust crypto lib (https://github.com/brycx/orion) and would gladly welcome any contribution. There are a few tasks on the issue tracker, but if you're looking to work on something bigger, I'm open to discuss that too. I'd love help with anything from testing, docs, implementations and reviews. We could also discuss of course, if there's something more specific you want to know about or work on.
It's pre-rendered, sloth does that. You input how many frames you would like it to export, a width+height, and it prints them in a javascript array to the console. See src-webify for the nitty-gritty
Unfortunatly I dont think its going to work. I am not a network expert but I get lots of errors just trying to download the crate. $ cargo run Updating crates.io index error: failed to load source for a dependency on `toml` Caused by: Unable to update registry `https://github.com/rust-lang/crates.io-index` Caused by: failed to fetch `https://github.com/rust-lang/crates.io-index` Caused by: the SSL certificate is invalid; class=Ssl (16); code=Certificate (-17) $ cargo doc --all --open Updating crates.io index error: failed to load source for a dependency on `toml` Caused by: Unable to update registry `https://github.com/rust-lang/crates.io-index` Caused by: failed to fetch `https://github.com/rust-lang/crates.io-index` Caused by: the SSL certificate is invalid; class=Ssl (16); code=Certificate (-17) $ cat Cargo.toml [package] name = "rprocd" version = "0.1.0" authors = ["Ronaldo Nascimento &lt;**********&gt;"] edition = "2018" [dependencies] toml = "0.5"
I noticed that too! It's probably because every time a color changes a &lt;span&gt; tag is there. The DOM probably sizes the page based on plain text. To fix this, I can certainly print &lt;span&gt; with a new line.. I'll probably make an issue about this
Try https://github.com/rust-lang/cargo/issues/1180#issuecomment-235153322
I thought this was the other coreutils written in rust which are MIT licensed. My bad, MPL is a great choice!
That's the point! If you make that happen you'll have the deepest respect of my inner teenager.
I need help with regular expressions and/or patterns. I'm trying to split a string in the form of xdy+z (i.e., a dice roll) into three separate variables. So, number before 'd' is number of rolls, number after 'd' is number of sides, and number after +/- is the modifier (that takes its sign with it). Well, I've looked at the [Regex](https://docs.rs/regex/1.1.6/regex/) documentation, but I still can't seem to wrap my head around how it works.
I made a `wc` clone called [`cw`](https://github.com/Freaky/cw). Fun little project. Main thing lacking is proper locale support and a decent test suite.
Which is quite the statement: It is not common, therefore you need to be "protected" from it... If it's not sufficiently normal, let's be suspicious about it.
If you want something more challenging, check out [Purple](https://github.com/purpleprotocol/purple). You can also contribute to [graphlib](https://github.com/purpleprotocol/graphlib) which is a graph library written for general use case. If you are interested or have any other questions, give us a message on our [discord server](https://discord.gg/v9v7wnE).
This summarizing article has peaqued my interest for the original talk. Is it available as a video somewhere?
Hi, I have several opensource projects that are not focused in 1 direction, and I'm updating all of them at the same time :) [diwata](https://github.com/ivanceras/diwata) - dynamic database GUI aimed to be backend to most CRUD apps and application, e-commerce sites. [rustorm](https://github.com/ivanceras/rustorm) - the orm behind diwata project. [sauron](https://github.com/ivanceras/sauron) - elm,react like web framework for purely rust code for client side webapps. [svgbob](https://github.com/ivanceras/svgbob) - it has sophisticated algorithms to convert ascii diagrams into a pretty svg image.
Looks awesome! Back in college we had to make a game that ran in the terminal, but we cheated a bit using WriteConsoleOutput on windows, which let us write colored rectangles, and we resized the font size to 2x2, and changed the color palette with some barely documented Windows function, so it was effectively just a pixel buffer with a limited color palette. I always wanted to get into extending the idea into 3d, maybe even making a console video player. Youtube did this a few years back for april fools I think? Yours is a lot nicer than [what we made](https://youtu.be/d2nwgMV77Ug), but it was a fun experiment! Nice work!
&gt; True, it might be late complain, but talk about await stabilization was sudden, and .await is not way was initially proposed in RFC. There was no RFC to adopt .await, it was one sided decision of lang team. So there is no way for community to be even involved in first place. I'm having trouble squaring this with what I saw, which was that while you are right that the particular syntax was not proposed in the rfc, the fact that they wanted to stabilize a syntax was well publicized and then vigorously debated among a wide section of the community, and that the lang team used that debate in coming to their conclusion that .await was a good option.
There have been proposals that would allow both to be implemented (mostly with the postfix variant using hypothetical postfix macros to wrap a "standard" prefix variant), but no-one really seems to have responded to these specifically. The debate seems to have essentially revolved around postfix vs prefix, but in my experience writing `async`/`await` code in other languages, they're both pretty useful for different situations. Some code is fairly imperatively written - the prefix variant is pretty ideal in these cases, given the word's use in the English language, and the way programmers seem to think about the `await` action. On the other hand, particularly in very idiomatic Rust code, you also get long chaining syntaxes. In these cases, there's no doubt that the postfix variant will make this sort of code significantly easier to write. Unfortunately, I haven't seen much response to these proposals, although a few people have made them in various ways to a few people.
inclusiveOr?
The way you would do this would be to compile a regular expression with `Regex::new`, and then use the `captures` method to match with captures. [Playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a7c9e39d77bf3a617d188edd8c1d477a)
Nowadays I'd probably use the AGPL instead of the GPL because of how much the internet is used, companies like Google *can* use GPL code, but they definitely can't use AGPL code for their closed source software.
I mean, that was literally just them using the file manager [fsn](https://en.wikipedia.org/wiki/Fsn_(file_manager\)), it was and still is a real program, although maybe a bit hard to track down given what happened to SGI and all the dead links floating around.
My justification for posting this even though it's a Python article is that it lays out a lot of reasons why Rust should (and has been) avoid growing too large a standard library. I think the point about how "asyncio shouldn't be in the standard library if you can't do any IO without an external library" might actually be something Rust should consider.
For examples, there will be no compilation time improvement, because `momo` drags `syn` with it.
C++-style placement-new is not really the point when we talk about "placement" in Rust, but rather C++-style constructors doing in-place initialization. That is, `box` is equivalent to *non-placement* new, in that it enables the allocation to happen before the initialization- Rust can't even do *that* consistently, let alone placement-new.
At worst you can just cheat and use CSS to hide it. CSS is great for that.
I work on a small tool called [Topgrade](https://github.com/r-darwish/topgrade). While most of the code consists of tasks to execute, there's a small framework for providing utilities such as the ability to perform dry run, detection of the current Linux distribution and such. I mostly work on this alone and I never got any real criticism. Surely there are things that I can do better.
My development environment has been on CentOS for the last 3 years. GLib on this has been always older than Ubuntu and other distros. I couldn't even start their server in my VM because of this. They have precompiled binaries that simplify their process, but greatly reduce the compatibility. Currently under VSCode, there is a plugin called remote development which for now works only in insiders builds. The whole experience of using this has been waaÃ y better than the code-server. I would any day recommend VSCode with remote development plugins as opposed to this.
I'm working in a relational language (not to be confused with a RDBMS) to be used alike Lua/python: [https://bitbucket.org/tablam/tablam/src/default/](https://bitbucket.org/tablam/tablam/src/default/) &amp;#x200B; The idea is to provide a better semantics for make data-intense code, business logic, etc.
The "official" Discord also has users and beginners sections, so the internals vs. users comparison doesn't really work (they are for separate discussions regarding different parts of Rust). Why was there a need for 2 servers in the first place?
This is something I hear quite often. Usually those languages are faster at setting up the initial endpoints. But that this is not where most of the time during the lifetime of a REST service goes. It is the business logic that takes by far most of the time. It doesn't matter whether you can setup your initial skeleton in 10 minutes or 8 hours. When it comes to implementing the internals of a service I would dare to say Rust is as fast as other languages if not faster. Furthermore Rusts "if it compiles it runs" brings a lot of speed on the table. Typescript/Python/etc services have a much higher chance of bugs materializing as critical incidents at runtime.
So this is another one of my side projects at work, and I've finally got the approval to release it publicly! ðŸŽ‰ Hopefully it can be useful to someone.
Ehh, I'd definitely say that Erlang is quite practical, but it's absolutely a product of its time. I would not say it's fantastic though, but it gets the job done. Anything you can do in Erlang you can do in Elixir either as a direct translation or with more ergonomic syntax. There are also constructs in Elixir that don't exist in Erlang.
There's also a ls implementation in Rust: exa
&gt; For info sake I had to install rust via yum (using RHEL 7) and due to restrictions I cant even run the rust install script. If by "rust install script" you mean `rustup` then you might want to check out [the standalone installers from here](https://forge.rust-lang.org/other-installation-methods.html) which contain a fully offline installer that doesn't require a network connection nor any special priviledges.
It's a good thing to keep in mind. Streams in Node.js is another good example. It's been through 4 major revisions, and being bundle in the standard library has been a huge hindrance.
I think the problem (when monomorphize and when not) is important, and `momo` makes sense as an exploration project. But I don't think anybody should use it for serious stuff. I don't mean it as an attack, this is just my opinion, but I will state it. I apologize in advance if it comes out too negative. If there is nicer approach to saying stuff that I mean to say, please let me know. I'm not good at Internet conversations. 1. The problem of monomorphizing (or not monomorphizing) appears very similar to the problem of inlining (or not inlining). Inlining should be addressed at the level of CFG representation, not AST. I have a hunch (but no proof) that it's the same for monomorphization. 2. The crate is too complicated for what it does. Its two hundred lines of code to do a transformation that can be described in one paragraph and performed by hand. I agree that doing it manually adds accidental complexity, but so does depending on \`momo\` to do it implicitly. The only way accidental complexity can be sort of avoided is if it's done automatically in a standardized way with no corner cases (say, by a compiler). 3. It has large `syn` crate as a dependency (it specifies the exact version even). So if you add `momo` as a dependency, chances are you'll have to compile yet another copy of `syn` in your project, even if you have a bunch of "syns" in your dependency graph already. 4. Maybe you don't need it in the first place. Unless you have external interface requirements that you should accept as a given, what's the point of making a function polymorphic if the only polymorphic thing it does is calling `.into()`? In many cases, if not all, it would be cleaner to move `.into()` to call site. And if we are talking about `.as_ref()` or `.as_mut()`, they might even disappear entirely thanks to deref coercion.
Async I/O is a separate create in rust. The standard library has the bare minimum to support the async/await syntax. If you want more, then you need futures(-preview) and something like `tokio`, `romio`, or `runtime`.
Hmmm...I'm curious how the second example would play out. This would mean that Momo needs to accept arbitrary traits, iff the respective arguments are named, right? Those traits would need to be object-safe, of course, else we'd get a compile error. But I guess that UX could be made acceptable with documentation.
&gt; I also think the point about how "asyncio shouldn't be in the standard library if you can't do anything with it without an external library" might actually be something to think about with respect to futures. I think there's a delicate balance to be reached. On the one hand, as Rust has been cautious about and as is noted in the article, a `std` that is too large is a burden. On the other hand, inter-operating libraries is made easier when they can communicate with a common set of types, and since `std` stands at the root of the dependency graph, it is in the best position to provide such types. I think that having `std` provide *traits*, such as `std::future::Future`, and let libraries implement those traits is a good way of balancing the two concerns: - The API of [`Future`](https://doc.rust-lang.org/std/future/trait.Future.html) is small and well-defined: one output type, one method. - Libraries can return `Future` or consume `Future` without caring much for the actual implementation, enabling inter-operability. I would be leery of `std` attempting to provide a full-blown HTTP/3 implementation, or multiple database drivers: too large, too ephemeral, ... however I see value in a few small, well-defined interfaces.
Is this discord about the Rust game? Because this subreddit is about the programming language.
If your app is I/O bound which most biz apps are, node.js will be fine and by the time it becomes a scaling issue you've got lots of cash coming in and can rewrite it IMO. JS is a very succinct language. Ive rewrote a c# app in JS and it was 10x less code. GO is good for API's, but if im doing any kind of HTML rendering on the server, node would be a better choice. I'm not saying rust doesn't have it's use cases, but a web app may not be the best one.
As far as the microkernel itself having a wasm interpreter, that's basically the goal of Nebulet. Which, in principle, should help reduce some of the overhead a microkernel introduces since all the services are in ring 0.
I had thought that the entire futures crate was going to be stdlib'd eventually, but that might not actually be the case. My apologies if I am incorrect on that.
I really like the Future/Waker in std but everything else in crates model. It allows for standardized interoperability between different reactors and event loops, but plenty of flexibility.
&gt; side projects This looks pretty awesome! Certainly puts the *systems* in *systems programming*. &gt; Uses a custom, tailor-made stack unwinding implementation which makes it a lot cheaper than other similar tools, potentially up to orders of magnitude faster in some cases Two questions: - I have only ever used massif for tracking memory consumption; do you have any idea of the relative performance of your memory profiler compared to massif? - How is the support -- and performance -- for multi-threaded programs?
You sort of missed the point of this crate: Why do something by hand (and possibly add bugs in the process) when you can have the computer do it? Also, 200 LoC for a proc macro isn't really that big. I've written larger proc macro crates. And while I agree that having good support in the compiler would be preferable, having a simple painless way to experiment is still a win. If you want, you can even try it and use `cargo expand` to get rid of the dependency once you're done.
&gt;Like, how does one become proficient enough to be able to, for eg, comfortably discern all of the pieces in the method above? If your goal is to understand *all* rust code, the answer to "how much rust do you need to know" is "all of it". Which is the same answer as any other programming language. But you can understand a lot with less experience. That particular method is complicated enough that I think it would take a while of staring at it for anyone to "comfortably discern" exactly what all of it means.
I cloned your project all the way back when you first posted it here, and I loved using it. The funny thing is, I once left it running on another desktop and forgot about it. After something like 15 minutes, turns out that's what was heating up my CPU.
I mentioned this elsewhere but, I had thought that the entire futures crate was going to be stdlib'd eventually, but that might not actually be the case. My apologies if I am incorrect on that.
I wonder if there's a way to make curated sets of packages? Perhaps even community maintained, and grouped with them? Imagine something like: ``` # Cargo.toml edition = 2018 packages = ["database", "web"] ``` Or maybe some installation defaults, ala: ``` cargo new --bin "foo" --with "database" --with "orm" --with "web" ``` The goal of this, in my view, would be to replace a large standard library with a similarly easy and consistent set of packages. Mind you, there would be no "database" package, it would translate to automatically including database dependencies like Diesel for an ORM, or whatever. I'm not saying this is a good idea at all. Just brainstorming on ways to have a lean stdlib, but still help bootstrap new Rust-devs with common choices, ie making it feel more like batteries included. And because this would effectively just be a thin wrapper around sane package defaults for common use cases, it would not be a big deal to change what "orm" translates to in a future edition. Something like `cargo new --bin "foo" --with "orm"` would just simply install the normal dependencies. Anyway, likely a bad idea - but I do think there is merit to trying to improve common library discovery. Smoothing the new developer workflow.
Hum... that wasn't my understanding; well, at the very least, I share your concerns and I hope `std` remains relatively lean.
You don't actually need "packages", inasmuch as a regular `crate` can simply be a facade crate which re-exports other crates. For example Diesel comes with multiple backends for a variety of databases. What is missing, though, are: - Discoverability: as a newcomer, what is the current community consensus on the best crate/set of crates to use for connecting to a database? - Durability: when adopting a new large dependency, with a potential for "infecting" the codebase, there is generally concern about the long-term maintenance of the dependency.
The regular expression syntax itself is what was confusing me. What you have looks similar to what I was working out, but I couldn't get it to split properly for some reason... Thanks!
Oh man. That should totally be the splash screen for your ransomware
For this very reason I'm a proponent of a thin standard library, combined with a official list of recommended "batteries".
I feel like the total cost should only be a single unconditional JMP, no? Pseudo assembly: PROC thisA: ; do the conversion JMP @impl PROC thisB: ; do the conversion JMP @impl ; ... @impl: ; rest of the method ret or is there a secret need for the separate `_impl` method?
&gt; I have only ever used massif for tracking memory consumption; do you have any idea of the relative performance of your memory profiler compared to massif? I never really measured this exactly, but it should be several orders of magnitude faster than massif purely due to the fact that it's running fully natively, unlike massif/Valgrind which is essentially a full blown CPU emulator. Basically, what this profiler does is that it only replaces `malloc`/`realloc`/`free`/etc. leaving everything else intact. Inside of one of those replaced functions it grabs a stack trace, calls the original function, and sends that stack trace along with the information about the allocation itself to another thread for processing. The [Heaptrack](https://github.com/KDE/heaptrack) profiler works in a similar way, although it uses `libgcc` for stack unwinding; my implementation of stack unwinding is fully custom, and it uses a few tricks to be even faster - e.g. if you have a stack trace 100 frames long and you call `malloc` twice then my profiler will only end up unwinding ~101 frames instead of ~200 frames as it keeps track of how many stack frames were popped since the last allocation so it doesn't have to fully unwind the stack every time. &gt; How is the support -- and performance -- for multi-threaded programs? It fully supports profiling multi-threaded programs - one of the applications with which we use this profiler at work launches something around ~40 threads IIRC.
There is still the cost of dynamic dispatch which you don't have with monomophized code. In most cases, this cost is negligible, but in your hottest code, every extra instruction will count.
It would be pretty cool to build a WASM version of the renderer so that you could do this in realtime in the browser. :-)
In about 1980 I had a CP/M S100 box with a normal 80x24 character display: it had a 3Ã—2 subpixel font in the upper bits of the character cell. So I wrote a graphics library for rendering 160Ã—64 text in UCSD Pascal; wrote a graphics editor on top of that; and then later wrote a print driver for my daisy-wheel printer. The printer had microspacing, so I rendered graphics using just dots by positioning a "`.`" character. Those were fun projects.
The more I learn about it, the more promising it looks. &gt; as it keeps track of how many stack frames were popped since the last allocation so it doesn't have to fully unwind the stack every time. I am curious how you do this without further implementation; if you have a link or something, I'd be interested in understanding how you manage this as this looks like a technique that could be useful for a variety of purposes.
Seeing the turning ASCII Pikachu in your last post really made my day \^\^ &amp;#x200B; I've even made a small shortcut so that I can just type `pikachu` in the terminal and smile :) It's awesome to see that you continue improving your awesome program.
of course a dynamic language is going to be shorter and more terse, I just don't like having to walk up at 3AM in the morning to find some stupid bug has crashed the site, a stupid bug that simply would not have compiled in any static language. Also refactoring any non trivial sized application in a dynamic language is a huge huge pain. I've worked with almost every language (except prolog and Idris), and worked in both dynamic and static languages, and for server side, static just works out better in the long run.
&gt; If your app is I/O bound which most biz apps are, node.js will be fine and by the time it becomes a scaling issue you've got lots of cash coming in and can rewrite it IMO. I've seen this argument being made quite often. However, I'm not sure if it holds. In my personal experience (part-time webdev in a medium-sized start-up mainly working with TypeScript Angular frontends and node backends), there never is time for rewriting such a thing. I know I don't have so much experience yet as I'm still attending university, but do tech people generally think that above argument is valid? I could only see this being true in larger companies if not corporations that just don't care.
for front end? 100% agree with TypeScript, backend, erh not so much! :) Python in small doses is ok, I've been bitten in production a few times with Python, that static languages is a huge relief
I'm planning to write a command-line first-person shooter in a game jam some time using [Euc](https://crates.io/crates/euc). If I end up doing that, I'll post it on this sub.
How'd you know?? ðŸ¤£
It does that sometimes, I should probably implement a limited framerate ðŸ‘€
I also end up needing to write a lot more tests in JS and Python, and the error/exception cases especially usually don't work the way they should the first time around. That's what really bugs me in JS and Python, the long tail of things that I need to test before I feel kind of confident it won't blow up due to a random typo in an exception handler somewhere.
Thanks!
It's likely that Redox, or other new and uncommon OSes, is less secure than Linux, which has quite a lot of people looking for (and fixing) security issues. In principle, certain kinds of bugs should be reduced/eliminated by Rust, though a kernel still needs a lot of unsafe code. It's really difficult to estimate what benefit a kernel as heavily developed as the Linux kernel would gain if it had been in Rust.
&gt;having a simple painless way to experiment is still a win I'm grateful that you are working on this. It means that somebody qualified is thinking about the problem and possible solutions. But I'm not going to use `momo` and will be a little bit upset if any of my dependencies start using it. Because I don't consider it *the* solution, just an exploration. &amp;#x200B; &gt;Why do something by hand (and possibly add bugs in the process) when you can have the computer do it? First, why do it at all? What's the advantage of fn some_function&lt;I: Into&lt;String&gt;&gt;(i: I) ... some_function("hi") over fn some_function(s: String) ... some_function("hi".into()) I think usually second style is preferable. Even if the function is called in 1000 places and it's tempting to trim those `.into()`, I don't think that's a net saving. Because the signature becomes more complicated, and it has to be looked up every time you write or read function invocation. There could be exceptions, of course. One that comes to mind is that it's actually nice to be able to write any of the File::new(str) File::new(path) File::new(path_buf) But that works because I already internalized that `File::new()` accepts vaguely path-like stuff, so I don't bother deciphering what the hell `AsRef&lt;Path&gt;` means every time I use it. It's ok because it's in the standard library and the programmer is expected to be familiar with it. For third-party stuff, explicit conversion at call site is better. Another exception is if `&lt;I: Into&lt;String&gt;&gt;(i: I)` signature is forced on you. Say, you are implementing a trait trait SomeTrait { fn some_function&lt;I: Into&lt;String&gt;&gt;(i: I); } But then, why does such trait exist in the first place? I think it would be better if "into" version was made into a default implementation: trait SomeTrait { fn some_function_for_real(s: String); fn some_function&lt;I: Into&lt;String&gt;&gt;(i: I) { some_function_for_real(i.into()) } } And then, unnecessary monomorphization will be avoided for all implementers automatically. &amp;#x200B; &gt;Why do something by hand (and possibly add bugs in the process) when you can have the computer do it? Second, not that simple. It's "bugs added by me vs bugs, limitations, and corner cases added by you", not "bugs added by me vs no bugs". It's also "explicit vs implicit". It's also an additional dependency with [all costs associated with dependencies](https://research.swtch.com/deps). Sometimes it's clearly better to use a library than do something yourself (for example, parsing JSON). Sometimes it's clearly better to do something yourself (like `x % 2 != 0` instead of [is-odd 3.0.1](https://www.reddit.com/r/programming/comments/886zji/why_has_there_been_nearly_3_million_installs_of/)). It's a trade-off, and some considerations here are case-specific or even subjective. So I'm only evaluating things in my context. I think that `momo` is closer to `is-odd 3.0.1` than to JSON. And just like I have nothing against checking numbers for parity, I'm not against `momo`. But I don't think it should be intended to be used as a dependency in "responsible" library projects.
&gt; I am curious how you do this without further implementation; if you have a link or something, I'd be interested in understanding how you manage this as this looks like a technique that could be useful for a variety of purposes. I've come up with this technique myself and I don't have it written up anywhere, although it's not really rocket science - it's more or less the most obvious way to answer the question of "how can I not have to always unwind the full stack even though I want to always know what the full stack trace looks like?". Basically, normally if you call a function the compiler will push a return address onto the stack so that the function you've just called knows where it's supposed to return to after it finishes. What I do is that I grab that address (and I know where exactly those are saved on the stack since I'm unwinding the stack), save it to my own shadow stack in thread-local-storage, and replace it with the address of a trampoline. The trampoline, when called, increments a counter of how many stack frames were popped (so I know how many stack frames are still valid from the previous unwind), pops the real return address from the shadow stack and jumps to it.
Hey, which nokia department are you from, because I talked to the routing departement in Antwerp, and none of the 5-8 engineers i talked to had ever heard of rust, including the local head of the ion department. They did offer me an internship in Rust after my evangelizing, but i didn't take it because it was unpaid, which is a bit sad.
They did record it but I haven't seen them post it yet. I'm still waiting.
True. With all the SaaS stuff out there now, GPL's niche has shrunk, but it really depends on what you're trying to accomplish. For example, you might go as far as BSD if you feel the public good of getting the benefits of your code into as many SaaS offerings as possible is greater than the public good of having improvements made public.
What kind of FPS can you crank out? I was thinking it would be cool to use for some game project :) I guess I should dig into the code!
Well, Nokia has over 100k employees, so like any other huge company I'm sure it has a lot of variability across departments and locations. (: I'm from Mobile Networks which is responsible for the base stations your cell phone talks to, and at the location I work all the internships are paid AFAIK.
[Are we web yet?](http://www.arewewebyet.org/) The filters in [warp](https://docs.rs/warp/0.1.15/warp/) are quite nice -- [routing example](https://github.com/seanmonstar/warp/blob/master/examples/routing.rs).
Thanks for the response. That's more or less the answer I've been looking for
It can pull maybe 30-40 fps, and that's through the WSL terminal! If you _really_ want it to be game ready, fork sloth and port it to a real rasterizer like https://crates.io/crates/euc/0.1.0. (I will also merge your PR if you do, no questions asked)
Or I just render and use it as sprites :) Got some thinking to do. Definitely cool project! Thanks for sharing!
Well in antwerp, it was policy to pay â‚¬9/day I believe, i assume to pay for lunch.
You could try adding it manually to your /etc/hosts file if you have root.
I agree that momo comes with its own cost, and I may remove the minor versions from its dependencies to ease the cost a bit (at least for crates using other proc macros). However I disagree that `some_func(mystr.into())` is preferable style. In my experience, library users cope well with such generics (especially when well-documented), and duplicating the same piece of code again and again doesn't help readability either. If anything else, momo helps boosting interest in more clever monomophization. And if we're lucky, it won't be needed anymore in a not-too-far-future rust version. Then I can deprecate the crate and we'll enjoy both improved compile time and code size without any cost at all.
Honestly, I just want to write stuff and to not help businesses. Making things AGPL can't *harm* businesses, but at least not helping them. In an ideal world copyright wouldn't need to exist, and everyone would be able to share code and stuff freely. But that's a long way off. I don't think that my work being used as a way of making profit is good. The AGPL is *in effect* at least a non-commercial license.
[Are we web yet?](http://www.arewewebyet.org/)
&gt; I don't think that my work being used as a way of making profit is good. The AGPL is in effect at least a non-commercial license. Not quite, but I understand the thrust of your point. I make a similar argument when encouraging non-programmers to prefer the Creative Commons BY-SA (ShareAlike) licenses over the BY-NC (NonCommercial) ones, because of how much their intuition about what "commercial use" is differs so much from what the law says and because BY-SA and BY-SA-NC are incompatible if someone wants to make a combined work.
Oh my god, they're all nested. #sloth-frame &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span &gt; span And now I have learned Chrome Devtools doesn't let you scroll the elements window when you overflow it. The fix here is obviously to close the tag before starting a new one. This won't fix the scrolling issue, though. That's because you have a ton of whitespace at the end of every line.
Currently on the same boat as you. I wonder if Rust will always be too hard to pick up and be productive.
And yet, python's batteries-included stdlib is a huge contributor to its success. At work, we have several large Python projects; from a quick `grep import` of one of them, we use the following functionality from the stdlib (just directly, not including transitive dependencies): - FTP handling (`ftplib`) (don't ask why) - OS abstractions (`os`, `pathlib`) - Date-time handling (`datetime`, `time`, `calendar`) - CSV parser (`csv`) - Base64 decoding/encoding (`base64`) - Cryptographic hashing (`hashlib`, `hmac`) - Compression (`zlib`, `zip`) - Logging (`logging`, `syslog`) - Random numbers (`random`) - Unit testing framework (`unittest`, `unittest.mock`) - Regular expressions (`re`) - JSON parsing (`json`) - URL parsing (`urllib.parse`) - UUID type (`uuid`) - XML (`xml.etree.ElementTree`) - Emails (`smtplib`, `email`) - Translation (`gettext`) - Text encoding/decoding (`codecs`) - Profiler (`cProfile`) - Command line argument parsing (`argparse`) - Image format detection (`imghdr` - yeah that's an odd one) - Tar file handing (`tarfile`) - SSL handling (`ssl`) - AST/parser (`ast`) Now, for almost each of these, there exists a 3rd-party package that does the job better. And in some cases, I use them. But having to do so all the time would be extremely unpleasant. In my opinion and from my experience, for the working programmer using a language that is meant for "getting shit done" (like Python, C#/.NET, Java/JVM and Go), the advantages of batteries-included far outweigh the disadvantages.
 &gt; (He didnâ€™t record the talk, so we have the slides to go by) Also, it's "piqued".
One caveat of Nodeâ€™s concurrency is that itâ€™s *only* over IO. This means something computation heavy (which can even just be some templating stuff) can block your entire server from responding/serving to other IO requests. AirBnBâ€™s engineering blog has a good write-up on their challenges with this https://medium.com/airbnb-engineering/operationalizing-node-js-for-server-side-rendering-c5ba718acfc9.
Iâ€™ve finally got a lower-level project with IoT devices that screams Rust. Pretty happy to get my hands dirty with that in the coming months. I think for certain workloads itâ€™s just inherently easier to use other languages. Like Node.js... as much as I dislike JavaScript at times, I admit itâ€™s super easy to build certain things like REST APIs. And some of that just comes down to familiarity. Iâ€™ve spent the last 4 years working in server-side JS. If I had that same experience with Rust Iâ€™d like have equal proficiency. Iâ€™m hoping the async/await additions in nightly start to make things easier. As for REST, Iâ€™ll probably look into Rocket more. Needing nightly doesnâ€™t bother me since itâ€™ll all be containerized with Docker anyhow.
I see it the same as Java EE. The official implementation is just a collection of extensible classes and interfaces (traits), but each servlet container (runtime) is free to provide its own implementation of the API. If one doesn't work well enough for your use case, you can switch to another implementation with very few code changes.
Lol
It's all those zero cost abstractions.. Everyone knows you get what you pay for!
It was a good summary of a thing that a *lot* of people wanted. I don't know if I'd call it "influential" because it was more reporting an existing hope than spreading a new idea. NaCl was released in 2011 - that was the thing that really got the ball rolling in my opinion. I remember around when PNaCl was announced people complaining that it would never work because LLVM IR isnt fully portable (and obviously things can never change). Fortunately the naysayers didn't win.
That's fair! I think if I found the compiler accepted calls to `fn x(a: &amp;dyn MyTrait, y: &amp;dyn MyTrait)` and `fn y&lt;T: MyTrait, U: MyTrait&gt;(a: &amp;T, b: &amp;U)` but rejected calling `fn z&lt;T: MyTrait&gt;(a: &amp;T, b: &amp;T)`, then I'd be a bit... confused, at least. It makes sense from an implementation perspective, but I'm not sure how sound it is from a theoretical one. For instance, this call would have to be denied: fn func&lt;T: MyTrait + ?Sized&gt;(a: Option&lt;&amp;T&gt;); let x: Option&lt;&amp;dyn MyTrait&gt; = ...; func(x); But this one... accepted? fn func&lt;T: MyTrait + ?Sized&gt;(a: Box&lt;&amp;T&gt;); let x: Option&lt;&amp;dyn MyTrait&gt; = ...; func(x); What if I create a `struct&lt;T: MyTrait + ?Sized&gt; { ... }` struct: does the compiler allow or deny creating it with `T = dyn MyTrait`? Since (hopefully) most rules of what's allowed with structs can be determined solely by their signature, I'm inclined to believe this would be denied. Or maybe the compiler would insert an implied vtable property? That's a lot of magic though, if it does. I guess what I'm saying is that as far as I can reason, we'd end up with a mess of implementation-driven behavior for this. Like, we could allow it, but I can't find any overarching rules to reason with that we could rely on. I'm possibly unimaginative, though. This could be a super nice feature, if we figured out underlying rules and hashed out what exactly the compiler would and would not allow. With sane rules, this could definitely be the kind of thing that gets accepted as an RFC.
Actually, rust is built upon the LLVM infrastructure, which allows for a crazy amount of optimization. Perhaps you didn't specify the correct flags?
Did you compile with --release?
Pretty sure this post was meant for r/playrust
My dude i feel like you might want r/playrust, as this one is about a programming language.
Are you maybe looking for /r/playrust/?
I thought MIR was supposed to help in this regard?
Why dont you ask your IT to get an exception for rust related domains? Speak with your boss first and ask them nicely, and explain why you need access and that its a trustworthy site..
Python's batteries-included philosophy stems from a pre-internet mentality; if I had no connection, I'd rather have a full-featured unittest in the standard library than nothing at all. However, nowadays for the majority of people in the developed world, having no connection at all is abnormal. Even a slow connection can download stuff from crates.io, so there's less need for it to be bundled with Rust directly.
I agree that Python's stdlib is a huge part of its success - same with Go. There is one major factor to consider though; package management in Python and Go is considerably worse than rust. I still think that moving things back into std from a crate is a reasonable idea though, as has happened with futures, and I think as should happen eventually with serde, regex, and rand.
(He's in the wrong subreddit, I think)
IntelliJ has pretty good autocomplete.
I get what you're saying there, but with modern pipelined and look-ahead CPU architectures, that should only be a single clock, I'd think. Maybe that's a `-Os` vs `-O2 thing at that point? :-) --- Maybe another option is to have Rust make it the caller's responsibility to call `.into()` et al. in the correct cases? Then dynamic dispatch isn't needed any more. Not sure whether Rust (or any compiler for that matter) could do that though.
I agree. For Python, and JS. For Rust the team (might) choose the best one.
Thank you so much! Most needed!
Really nice. How about inlining the outer function ?
Well, at very least languages like D, Nim, Crystal and etc has no stupid game named the same. Good for them.
Nope, it is definitely goes to r/rustcirclejerk
There's a Linux clone called fsv http://fsv.sourceforge.net/
HTML automatically closes them at the &lt;/p&gt; tag, and chrome won't let you look into the &lt;p&gt; tag because it's being updated very quickly, which chrome isn't used to There *is* some weird whitespace in the js file for the actual frames! It's before and after every frame. I'll look into this some time, it's definitely from the command line
This is one of the more entertaining ones I've seen. I hope the mods don't kill it, it gave me a grin :)
You can pause the Javascript in Chrome in the sources tab
&gt; especially if we ever end up discussing a `.match` or `.if` The possibility of other keywords getting a postfix syntax really makes me wish for another sigil. Seeing the builder pattern mixed together with .match/.ifs in an example someone posted was really jarring
Correct, it isn't official in the sense that the users forum is. The servers were created at different times by different people for different purposes. The unofficial server has been around for two years longer. Both are nice places to chat :)
All I know about is the Espressif llvm repo https://github.com/espressif/llvm-xtensa . Doesn't seem very active, but it's supposed to be functional. There's an open issue that's blocking rust support currently, so I'd track that. https://github.com/espressif/llvm-xtensa/issues/2
(And you didn't get the joke, I think)
Take a list of all the dependencies you will need (Redis, Cassandra, etc). Check if exist a crate for Rust, then decide if this is ok. &amp;#x200B; \&gt; Somebody please convince me I should use Node.js &amp;#x200B; Pick node if: &amp;#x200B; \- You hate to use a good package manager \- You love to code on a [wat](https://www.destroyallsoftware.com/talks/wat)? language that need another (saner) language that transpile to that. Using the above. \- You hate to have a decent dynamic language. Python, Ruby or Elixir are better and have good tooling, but choses JS, why not? \- You love callback hell, but not worry, you can transpile a solution. \- You need a good ecosystem, that provide highly valuable implementations of drivers to Cassandra, Redis and Leftpad &amp;#x200B; I can play this game all day! &amp;#x200B; But seriously, the ONLY reason: &amp;#x200B; \- You need this to be on JS. End.
Your understanding is correct; the stuff thatâ€™s intended to go in has already gone in. The futures crate will still exist for the other stuff. (That said, Iâ€™m not on the libs team... but thatâ€™s my understanding)
We pitched this idea back in 2016 and most people did not like it https://aturon.github.io/blog/2016/07/27/rust-platform/ There have also been attempts like the stdx crate, but they didnâ€™t go anywhere either.
Yes, but this is the rust subreddit and you must succumb to the wasm circlejerk! In all seriousness though, this is an absolutely terrible idea. I'm all for doing insane stuff of the sake of research, but there comes a point where it becomes one of those "they've become too preoccupied with whether or not they could, they didn't stop to think if they should" situations. I implore people to go look up some of the JIT bugs that have been published for Chrome/Firefox so they can learn just how complicated and subtle they are. Fun story, Microsoft used to do their font rendering in the kernel because it was such an expensive operation. It wasn't until after suffering from browser to kernel exploits for YEARS that they finally moved font rendering into user space (in like, 2017).
Was that actually a joke? Just looks like the guy trying to be helpful. But just downvote and be snarky. I'm sure that helps.
Ah I see, I didn't know the community server is older
`serde` and `rand` are decent contenders, as they expose core interop traits: `Serialize` and `Deserialize` for the former, `RngCore` for the latter. `rand` still needs a good deal of maturing before it has a legitimate call for going into std, though: it's not even confident enough in itself to call itself "1.0" yet. There is a proposal to include "`getrandom`" into std, though. `regex` isn't, really, as while it is a very widely useful crate, it doesn't expose an interop trait. (There is an unstable pattern trait in std for searching strings, though!) And there are still tradeoffs in choosing `regex`; there's a reason ripgrep offers an RE3 mode.
My recommendation (and what I do) is to set-up VPN (your network configuration) to only route the organisationâ€™s IPâ€™s through the VPN and everything else through your regular network interface.
I started the formal process, so I hope it will get approved. Specifically I asked for access to the docs.rs domain. But the other problem is downloading crates from github etc. I tried adding our organizations CA certs to the RHEL machine, but I still get invalid cert errors (see below). I am no expert on networks or certs, but it seems our network has some sort of intermediary that fails in the cert chain somehow. This only happens on our internal WAN when trying to access specific sites.
This is awesome! :-) By a strange coincidence, I spent this afternoon reworking my FactoryBot-like library to use proc-macros. It doesn't quite have the same goals, and aims to be used just for constructing Rust types: [https://github.com/mjkillough/factori](https://github.com/mjkillough/factori)
`slowlyGetBeginIndex.....endIndex` vs `slowlyGetBeginIndex......endIndex` is hellish.
Thanks for the laugh.
Why is the [awaitðŸ•‘](https://www.reddit.com/r/rustjerk/comments/ajxn2n/with_all_the_heated_discussion_on_what_the_new/) syntax not included?
But for those of us not in the developed world, being able to work while offline is still a great benefit. As a resident of Germany, I frequently find myself stuck without a connection and lacking some essential crate. Cargo's newly stabilized offline mode mitigates this problem somewhat. The ability to view the docs for all my dependencies is also great.
That is something that would probably get me into a lot of trouble with NSOC.
There was a blog post just yesterday that outlined how to do it in detail. I'm too lazy to look it up for you, but you can find it in *This Week In Rust.*
It's always nice to be able to buy new batteries, when your device supports removable batteries. As a large community of battery users, we've learned that non-removeable batteries aren't great. It's convenient for devices to come with batteries included, so you don't have to run to the store to buy more, but when things sit on shelves for years, and their batteries starting leaking and corroding, you gotta be able to replace them. So it sucks when these old products are vital, yet you can't swap out new batteries. (I really like the battery metaphor, for whatever reason. ðŸ˜„)
There is already at least one well known JIT in the kernel! Look at eBPF, it's currently a pretty hot topic. Although more restrictive (max 4096 instructions, no unbounded loop, max stack size 512 bytes), it's no less than a JIT.
I can definitely see the value in including serde in a standard library, and serde is a great crate. Even so, I would be reluctant to do so, because it has downsides, notably code bloat. It is entirely possible that somebody comes up with a better approach (perhaps continuing ideas from miniserde), at which point having serde in the standard library could hold back progress. I believe there are many similar situations in the Python stdlib. Basically I'm arguing that the crates.io status quo is a pretty good place to be, not that we shouldn't always be looking for ways to improve it. One thing I would find interesting is a package of precompiled libs as a way of reducing cold-start compile times.
Imagine if Python, though, had a lean std library, i.e. one that only changes on language update. Then, it had several libraries already included and installed on language installation. However, it's just that those libraries could actually receive updates and act independently from the std library. It's not 100% perfect, because the versions that come with certain installations but get stuck being supported. However, if the installer bundles could be updated, even for older versions, that might help, even if they can't connect to the internet normally. What hurts Rust, I think, is that blessed crates aren't so obvious on the Rust website. With lots or crates not officially stabilizing themselves, it can be hard to tell what to trust. There is definitely some stuff, but it's not a super 100% obvious named tour. You can see, here's networks or here's CLI, but the crates themselves aren't as immediately visible. They don't come with an installation quite so easily. It's fine for most people, who can develop and use the internet fine, but there is a small majority that can't or don't want to install anything else. I'm not sure to what extent the community should support them, however. Monolithic downloads/installations just aren't a great idea.
Can you be a bit more specific? 1. If it was posted just yesterday, would that be the upcoming This Week in Rust? If so, how do we read the unfinished draft? 2. There's certainly nothing about it that I can see in the current most recent TWIR from four days ago. 3. I didn't remember seeing any mention of ESP32 or Xtensa stuff recently (aside from this) in the /r/rust/ RSS feed and, after going back through the last three days of the "sorted by newest" listing here on the web interface, I also didn't find any evidence of such a post.
Every time you call `next` on the iterator, it will give you a mutable reference to `self.item_behaviour`. If I called let v: Vec&lt;_&gt; = factory.collect(); then all of the elements of `v` would contain a mutable reference to `self.item_behaviour`, which is forbidden. As for the particular error that you're getting, the problem is that the `&amp;mut self` argument to `next` has an implicit lifetime that's not `'a`. You can see this more clearly if you explicitly name its lifetime, e.g. `fn next&lt;'b&gt;(&amp;'b mut self) -&gt; Option&lt;Self::Item&gt;`: the error messages will now tell you that `'b` might not live as long as `'a`. We can then try telling the borrow checker that `'b` will last as long as `'a` with `fn next&lt;'b: 'a&gt;(&amp;'b mut self) -&gt; Option&lt;Self::Item&gt;`, and that gets further, but now it's unhappy because we've now changed the signature for `next` so that it no longer matches that required by `Iterator`. You'd probably be fine if you could get away with avoiding having to name an explicit lifetime when you're defining `Item`, but that feature (Generic Associated Types) isn't in Rust yet. At this point you can try waiting for [Generic Associated Types](https://github.com/rust-lang/rust/issues/44265) to be added to the language, but that could be a very long time. Alternatively, you can give up on using an iterator here, and just implement a `next` method on your `Factory` that you can use in something like `while let Some(foo) = factory.next()`. It's not as useful as an iterator, but the borrow checker can make sure that you don't have two `&amp;mut self.item_behaviour` at the same time (by preventing you from calling `next()` again while `&amp;mut self.item_behaviour` is still live). Now, even ignoring all that you'd run into another problem with the way you're trying to return `Item`s: if the borrow checker got much further it would start complaining about you trying to move items out of a borrowed context. If `Item` was `Copy`, it would work, but only because it was implicitly making a copy of each item. You can't just *move* an item out of a `Vec` without something to replace it, because there's no way for the compiler to say "these elements can no longer be accessed". When you call `IntoIter` on a `Vec`, it's different since now the only way to access those elements is through the Iterator, and it's no longer the responsibility of the `Vec` to call `Drop` on them when the `Vec` is dropped. There are three ways, then, to work around this. The first and simplest is to return references to the `Item`s, mutable or otherwise. Second is to call `self.items.pop()` to remove items from the end of the `Vec`, which it can do in constant time. Third is to call `self.items.remove(0)` to remove (and return) the first element in the `Vec`. The downside is that it then needs to shuffle all the other elements back by one to fill in the gap, which takes time proportionate to the size of the `Vec`. (Another annoyance is that you need to make sure that `self.items` is non-empty first or it will panic.) The advantage to both the second and third approaches is that you don't need `index` anymore since you're modifying the `Vec`. I've [modified your code](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4f7ea02f62c4719833ea7e8579166fb6) to use a non-Iterator `next` method, and the costly `self.item.remove(0)` approach, since it was closest to the original approach, and I don't know what your actual use-case is.
This blog post is the result of my puzzling over how auto-complete works in the shell, and subsequently whether shell completion scripts could be written in Rust. It also introduces a new crate, [shell_completion](https://github.com/JoshMcguigan/shell_completion), which is a library that exposes some low level primitives for writing shell completion scripts in Rust. `shell_completion` is certainly in its infancy, so I'd be happy to have contributors of any kind (both code and opinions are welcome). I've created a few [issues](https://github.com/JoshMcguigan/shell_completion/issues) to get the discussions started.
It doesn't matter what language you're using to generate HTML. You don't want to put CSS and Javascript inline within the HTML because it prevents you from using the most powerful tool for preventing XSS (Cross-Site Scripting) yet devised: [Content Security Policy (CSP)](https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP) lets a website instruct the browser (via HTML headers, so an attacker getting access to your web-based template editor doesn't mean they have access to edit your security policy) to only allow certain operations from a whitelist of domains. In this case, the key is the `script-src` and `style-src` directives. If you omit `'unsafe-inline'` and `'unsafe-eval'` from them, then any CSP-aware browser will simply ignore scripts and styling directives embedded in HTML like that... which stops XSS via holes in your HTML sanitizers dead in its tracks. (There's even a `'self'` option for your whitelists which matches "external files on whatever domain this was loaded from, but not inline or `eval()`'d stuff. `Content-Security-Policy: default-src 'self'` is a great starting point for a new project.)
You will also want rustup.rs.
&gt;due to restrictions Did you try to manually download the shell script and then run it there or do they block that too? How do they expect you to get any work done if you can't do that?
Huge
How tolerant is this of other kinds of appended data? (eg. What if I wanted to use it to define the default config for a self-extractor stub that was going to have a Zip file or some other kind of archive appended onto it?)
Upvoted because I am amused.
I think this is good advice. Even real life can have XY engineering problems.
Yep, +1 to that. It becomes a nightmare if there isn't a standard type that interops between all these libs. It forces you to create a shitton of conversion code.
Hacker News thread: https://news.ycombinator.com/item?id=19950563
Imo a batteries excluded std lib works great in rust due to everything being built from source. You don't get binary dependency hell with the Rust approach with transitive dependencies. I'd be very reluctant to expand rust's stdlib beyond the bare necessities.
I wrote a program that `println`s some messages on an interval. On my Mac it works without any issues. I pulled the repo on a DigitalOcean VPS and when it ran it didn't print anything to stdout, so I figured it may be a Linux/Mac issue. I went into a Docker Ubuntu container from my Mac and tried the same thing and it printed to stdout without any issues, so I figured maybe it was a DigitalOcean thing. I tried the same thing on an AWS EC2 instance and it did not print anything, so it sounds like it is related to SSH / remote servers? I have no idea why that is happening. Any ideas?
Hey bro, this is the rust programming language not the rust game.
What are my options for threads without depending on all of std? Iâ€™m writing a runtime for a toy language and I want minimal dependencies. So I settled on allocating memory and creating threads (and a few synchronization primitives). I was originally planning on doing this in C, but I like rust. I know I can use the alloc crate to get access to memory allocation without requiring all of std. Is there something similar for threads?
How have you installed your companies CA? On my Ubuntu-based machine, I had to put the .crt file into /etc/ssl/certs (I _think_, not entirely sure oj that path), then I ran c_rehash (as root)
PNaCl did fail, and they did encounter all the problems naysayers pointed out. Naysayers were correct.
Yeah, regex is definitely the one I felt least strong about.
As someone who worked &gt;10 years in the industry, yes I think the argument is valid, and yes, it is generally accepted as valid. I also have witnessed such rewrites myself.
trying to get my generate tress function to work I don't understand why I am not getting the correct type. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2554f10427b6428d60338ad475b2cf04
also try this for other rnaging infromation https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2554f10427b6428d60338ad475b2cf04
www.regex101.com is a wonderful site to progressively build up a regex, with a lot of visual support and a quite comprehensive explanation for basically everything in regular expressions.
[Build Rust environment for ESP32](http://quickhack.net/nom/blog/2019-05-14-build-rust-environment-for-esp32.html)
When the parameters of a function are split across multiple lines, you put a comma after each one even the last. Typescript can also do that. It reduces lines changed when you add it remove a parameter when viewing diffs.
there totally is a delicate balance to strike, but imo at the moment we're \_way\_ too light on core functionality like networking and cryptography, and the projects that implement them are technically pretty excellent but come with nightmarish complexity and contagious execution models. &amp;#x200B; i'd love to see more functionality provided as standard (things like poll, which imho is a glaring omission) to the point that it is reasonable to write a safe communicating application using std, without having to adopt an enormously complex runtime or compile in a bunch of c and hope you picked the right library. either through inclusion in std, or by providing an expanded set of \_standard\_ libraries like std-net, std-crypto or something. learning go \_and\_ writing a reasonably performant networking app with working TLS and HTTP (or any other networking or socket communication) takes about 30 minutes and you have to learn about channels and goprocs. sending anyone new (or who values their time over unreal performance) into the depths of tokio is an incomparibly terrible experience, that imo is a reflection of our too-lightweight std library.
So you're saying that since this is the **only** parameter, it is also the last parameter. And since it is split across multiple lines, `rustfmt` puts a comma after it?
Do you mean creating threads through an operating system (e.g. by doing a syscall) or doing concurrent programming on a bare metal/embedded environment?
Can you compile a hello world example on your remote servers that runs? Or is it an issue with printing on an interval, over time?
Yes, the default hello world does work on the servers.
I havenâ€™t tested it this far yet, but every config you wish to save will be saved as a string slice. Iâ€™m planning on changing this to a byte array meaning if you want to use it to store some data that will later be re-encoded to something else that would work however you please.
Yep, see `ducc.exec("Arithmetic.add(4.5, 5)", ...)` in the code snippet. You can also [compile JS code and evaluate it later](https://github.com/SkylerLipthay/ducc/blob/master/ducc/src/tests/function.rs#L7).
The former. The idea is that the runtime does require some sort of operating system to run, but that even a hobbyist kernel would be enough as long as it can provide memory allocation and basic threading.
It's also been a source of serious vulnerabilities in the kernel.
but not russia... hmm.
What type do you see `p` as having?
How we are suppose to set our \`.vimrc\` to enable it ? I have \`\`\` let b:ale\_linters = { \\ 'javascript': \[ 'eslint', 'standard' \], \\ 'typescript': \[ 'eslint', 'tsserver' \], \\ 'graphql': \[ 'eslint '\], \\ 'rust': \[ 'cargo', 'rls', 'rustc', 'clippy', 'rustfmt' \] \\ } let g:ale\_linters\_explicit = 1 \`\`\` &amp;#x200B; But Ale seems to report me only error when I save the file, and the linter is \`cargo\`.
This looks amazing! Question: can you profile memory of programs that run forever? Or would you have to stop them like with Valgrind?
You can use whatever API your operating system provides for spawning threads. If you're in a POSIX environment, you could use [libc](https://docs.rs/libc/0.2.55/libc/) to create threads using `fork` or `execve`.
My guess would be that your ssh session is timing out, different servers have different timeouts depending on their sshd configuration. If you want to be sure that it's not rust that's the problem, write a simple bash script that does the same kind of intermittent printing and test that.
I don't think it's a timeout. I have the interval set to every second and it starts right away. I can still ctrl+c the program and start it again, run other commands, etc.
Thank you.
The survey is definitely biased to certain factors. The OP writes: &gt;I want to clarify one thing up front. Just like a poll on an internet forum should be taken with a grain of salt, a thread like this isnâ€™t a real substitute for a â€œusability studyâ€ or anything like that. Still, I think that language design being informed by the people who are using it is a good thing.
I was more hoping for an existing crate to provide a platform independent API that I could use.
As far as I know no crate like that exists. If you're looking for a project, you could write one ðŸ™‚ The needs of the community for threading are met pretty well between the standard library and platform-specific libraries. Writing an application that can't use the standard library but is on top of an operating system but isn't able to use the POSIX API but does want to be independent over multiple platforms is a pretty rare case.
I don't think I've ever seen an article telling the story of someone giving a talk before. Pretty cool concept and an interesting read.
Would it be possible to demo the second stack today using a thread local `Vec&lt;u8&gt;`?
Yes, because should you ever decide to add a second parameter, the diff will be one line and not two.
Interesting sample choice
If you *are* talking about the game, I feel you. I tried it out on my MacBook Air, and as soon as I move the mouse it gets blurry. Maybe someday hardware will catch up and make it playable.
Very nice. I hope cargo-sandbox will one day be the default when invoking build operations :)
I would like to think one of the goals of the project is to prototype what an on-by-default sandbox might look like
A blast from my past. I wrote a blog post about the Ruby library back in 2012... Iâ€™ll have to check this out!
I have a TODO about this somewhere.
You've got one more issue now.
This **has** been discussed before, but I do feel like a feature is needed here, regardless of shape. We have a fantastic type system. We shouldn't have to rely on documentation to determine which among 10's of errors a function returns.
I misremembered. It was five days ago in a blog post that appeared in my RSS feed somehow. Anyway, [here you go](http://quickhack.net/nom/blog/2019-05-14-build-rust-environment-for-esp32.html).
BTW, `get(n).unwrap().as_str()` is just `[n]`.
I've had really large `target` dirs when tracking nightly... rustc doesn't seem to remove caches from older versions.
I don't think the proposals above involve dynamic dispatch, but instead automatically splitting out small monomorphised wrappers for the core non-generic (and non-trait-object) code, exactly like `#[momo]`. The pseudo-code you're replying to is just a way to completely minimise the cost (it's effectively doing a tail-call of a the main code).
The first compilation won't be faster, but later iterative ones might be, especially when downstream code which doesn't need to recompile the crate that uses `#[momo]`. The downstream code will just see much smaller generic code when importing from the already-compiled parent crate, and this will translate into less time monomorphising and less time optimising.
Been following this project for a while. I'm really looking forward to the 0.3 release as that would be close to a full GUI library in Rust. Hope you figure out the 3d drawing too, to make it even more performant
Additionally, and for the same reason, the closing parenthesis is on its own line.
There are plenty of Russians among top Rust developers, you know. So please stop with the conspiracy nonsense. It is plain disrespectful to all of us.
The pseudocode I wrote contains a single JMP as overhead, so I suppose you can call it dynamic dispatch. But if you inline the outer call, then I don't think you lose anything!
I see. Agreed, the outlining itself is pretty simple. The question is when to do it, and I'm not sure there is a simple answer here. Anyone knows what C# does? AFAIK, they also monomorphize generics.
But with tests is not all about code not crashing, it's making sure it does and returns what you want. My latest projects (Django) have over 99% test coverage, if I were to accidentally break an API I know immediately. No language can save you from breaking logic.
In Haskell, with just 2 errors to combine, I would use `Either`, which is equivalent to `Result`, but with more generic names of constructors `Left` and `Right`. Problem here is that `Result` seems to be the only such type in `std` and it's use could be misleading - none of the errors is `Ok`... And with more possible errors, syntax(es) you proposed seem to be more natural anyway.
When it comes to projects I work with, the speed of development matters a lot. They would often give 1 backend dev and 3 android + 3 iOS, and you need to be ahead of them (yeah it sucks, but it's not that hard with Django). But our API logic is 90% quite simple, for any features that are compute heavy I already plan using Rust. Unfortunately we don't have such projects yet.
I wanted this for a very long time especially for error handling as you described. I am not 100% happy with the error handling story in rust today. I feel like many crates that exist around error handling are trying to tackle the boilerplate problem of **Second method** which sounds like the way you want to go. But there are other fields in which this could be helpful. I think Rust has a great story ahead around `Futures` and this would help in cases we currently have to use `futures::future::Either` All in all this and [structural records](https://github.com/rust-lang/rfcs/pull/2584) are the things that would really help me in my day to day usage of Rust and would help me introduce error handling to newcomers. I always stumble over explaining error handling to newcomers when it comes to return multiple error types in one function. I always feel like "can we skip this to a later point, because its hard to explain now â€“ just box the error or make a string out of it for the time being until we have time to do it right". Where "right" is mostly **Second method** with all the boilerplate involved.
Why would a command line tool like ripgrep benefit from having full implementations of TLS, TCP and HTTP bundled into it? I can understand having `std` define traits for futures, for collections, even for encryption primitives, to allow interior between libraries. I can also empathise with the problem of knowing what are the best/recommended packages on crates.io. For that reason I could endorse the idea of a Rust-batteries meta-crate issued by the core team that bundled up the current best library versions. But I donâ€™t think itâ€™s sensible to push this all into `std`.
I'm fairly sure every one of us newbies has made this mistake. It was around a month before I found out --release wasn't the default (and it should be!).
This looks pretty good. Rust needs more gui libraries in my opinion. I'd like to give it a try for a pet project of mine.
There's a CotW thread on rust-users where everyone can suggest and vote.
This is why I did the runner crate - stuff frequently used stuff in the cache, and write little programs off-line without having to go through the Cargo dance. Plus, get offline docs for everything in the cache.
The whole 'blessed crates' thing encountered great opposition. I can understand this from the point of the core team; they can't afford to play favourites. (Rather like with Lua, which has a very minimal stdlib; the Lua core team at PUC Rio feel it's their job to deliver the language, and the community's job to come up with the ecosystem). Alas, so what happens is that newcomers depend on 'tribal knowledge'. I hung out here for quite a while before I knew what everyone was using. Some choices are fairly easy (regex!) but e.g. the HTTP(S) space is so fragmented.
Any chance of intercepting mmap and co?
I see! This makes so much more sense now. Didn't know about implicit lifetimes. Thanks a lot for your elaborate answer! &amp;#x200B; The API I'm using requires an Iterator that yields tuples of type \`(T, &amp;'a mut V)\`, so I somehow need to make this work using the Iterator trait.
I can't tell if this is an argument for or against this feature
Interesting. So the allocator is just as much a source of puzzling behaviour as a GC would be, but to certainly a smaller magnitude.
In a sense, yes, it introduces some hard-to-predict latencies. After all, it is pretty complex problem it tries to solve. But I see several differences: * The amount of time you spend in GC depends on the allocation throughput (how many new objects you allocate and how many die over unit of time) and how much memory you still use. With the allocator, you pay just for the throughput, it doesn't have to scan the memory in use. * With GC, the whole application is stopped for the GC to run. With allocator like jemalloc I think it just makes one thread busy doing some upkeep and the others can run. While there's some locking inside, it is more granular and probably only for really short times. But yes, if you have some soft-realtime requirements, you might have to tweak some of the knobs it has to get good characteristics. Hard-realtime programs AFAIK usually avoid using dynamic allocation on the hot path altogether.
Try printing to stderr instead (`eprintln!`), or manually flushing stdout, to see if itâ€™s a buffering issue!
I think it's a tad complicated to move DSTs around, although I seem to remember someone managing it. After that, remember that the addresses should be stable, so you would actually be using something more akin to `Vec&lt;Box[u8]&gt;&gt;` to create a segmented stack, plus some additional meta-data. Also... this wouldn't work well with coroutines (as any TLS). In stackless coroutines all stack content need be in the coroutine, so you need compiler help. This means it wouldn't work well with `async` functions. With those limitations in mind, I think it should be possible to create a POC.
That's... that's... devious! And delicious! It's obviously very architecture dependent, and will probably trip up all kinds of ROP protections in a hardened binary, but I really like the simplicity of it :)
&gt; With GC, the whole application is stopped for the GC to run AFAIK this depends. For example, Nim has separate heaps on each thread, so there is effectively a separate GC running on each thread which means your whole application is never stopped.
This was an interesting read. A couple of comments: - "The pre-processing is also parallelized by [rayon]" -- you missed a link here - the glibc allocator has gotten quite better a couple of years ago (in 2.26, IIRC). Have you tested the system allocator to see if `jemalloc` is still worth using? - you touch on this, but it sounds like a large part of the improvements come from doing less (e.g. building the automaton outside of the service), and using better algorithms. There are a couple of factors: the language, the absence of a GC, the different algorithms and sets of features, and also the developer culture. It's harder to tell which how much each of these contribute. - returning 3 alternatives in an `impl Trait` -- in async code, I suspect `await` will make things much better - even without a GC, some people pointed out that deterministic destruction can introduce large pauses and/or even overflow the stack (imagine freeing a large tree) - have you considered using a client-side hash or bloom filter instead of hitting `urlinfo` on each web request, similar to Google's Safe Browsing? I haven't looked into the privacy policy of Avast, but I dislike the idea of sending every URL I'm loading to a company, regardless of how well-intended they are.
JVM also has multiple GC implementations, and they can be tuned to some extent. It's hard to find a stop-the-world GC these days.
Its sound pro "this feature" for me. To paraphrase it: Most lib authors tend to use **Third method** which takes of the "burden" from the author to the user. Less boilerplate to write for the author but more investigative work for the user to determine the exact error types. Whereas **Second method** leans more on the work of the author to have a more convenient experience for the user â€“ but still have to go back and forth for the enum definitions. So with **Third method** we â€“ as a user â€“ have to rely on the documentation which errors to expect but we shouldn't.
&gt; Unless I am missing something obvious, the technique also works even when alternating Rust and C stack frames are on the stack, no? Yes it does.
&gt; Nim has separate heaps on each thread, so there is effectively a separate GC running on each thread which means your whole application is never stopped. That sounds interesting. What happens when the object is shared or moves between threads?
It does intercept `mmap`s too, however right now those are not treated as allocations nor analyzed in any way; it just saves them as-is. You can use the REST API to fetch them yourself through the `/data/&lt;id&gt;/mmaps` endpoint.
Yes. In fact, this is our primary use case at work. It continuously writes the profiling data to disk (or streams it to another machine) so you can just grab that data file any time you want. One caveat is that you can't inject this at runtime. You have to start your application with the profiler `LD_PRELOAD`ed. It would be possible (and not that hard) to make a runtime loader for it where you could just inject it into an already running process, but I haven't got around to that since there wasn't really a need for it.
Sharing can only be done explicitly either via a manually managed block of memory or by copying.
Thanks for the post, I agree with the problem you exposed and that there's no perfect solution right now. In this case, I see a 4th solution: refined enums. I don't remember the exact names but I saw some RFcs about it. The idea is to derive a type from an enum by picking only a few of its variants. This would address the cons of the 3rd solution. ``` enum MyLibErrors { ErrorFirstKind(...), ErrorSecondKind(...), ErrorThirdKind(...), ErrorFourthKind(...), } // Really not sure about the syntax here. The idea is that you signal that the result is a `MyLibError` // but only the `ErrorFirstKind` and `ErrorSecondKind` are allowed (the other variants act like `!`) fn my_func() -&gt; Result&lt;(), MyLibErrors::{ErrorFirstKind, ErrorSecondKind}&gt; { if someCondition { let err: ErrorFirstKind = error_from_first_source(); return Err(MyLibErrors::ErrorFirstKind(err)); } else if otherCondition { let err: ErrorSecondKind = error_from_second_source(); return Err(MyLibErrors::ErrorSecondKind(err)); } // ... } ```
Thank you
There exist applications other than http(s) servers.
Option 4 and 5? use std::fmt; use std::error; #[derive(Debug)] enum Either&lt;T, U&gt; { // we can reuse this for other error types. Left(T), Right(U), } #[derive(Debug)] enum Triplet&lt;T, U, V&gt; { Left(T), Centre(V), Right(U), } #[derive(Debug)] enum Many&lt;A=(), B=(), C=(), D=(), E=(), F=()&gt; { A(A), B(B), C(C), D(D), E(E), F(F), } use Many::{A, B, C, D, E, F}; #[derive(Debug)] struct PrintError(); impl fmt::Display for PrintError { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { write!(f, "The document failed to print.") } } impl error::Error for PrintError {} #[derive(Debug)] struct MemoryError(); impl fmt::Display for MemoryError { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { write!(f, "Insufficient Memory") } } impl error::Error for MemoryError {} #[derive(Debug)] struct DisplayError(); impl fmt::Display for DisplayError { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { write!(f, "The output did not display correctly") } } impl error::Error for DisplayError {} fn my_func() -&gt; Result&lt;(), Either&lt;PrintError, MemoryError&gt;&gt; { // your code Ok(()) } fn my_func2() -&gt; Result&lt;(), Triplet&lt;PrintError, MemoryError, DisplayError&gt;&gt; { // your code Err(Triplet::Centre(DisplayError())) } fn my_func3() -&gt; Result&lt;(), Many&lt;PrintError, MemoryError, DisplayError&gt;&gt; { // your code Err(Many::C(DisplayError())) } fn main() { println!("my_func: {:?}", my_func()); println!("my_func2: {:?}", my_func2()); match my_func3() { Ok(unit) =&gt; println!("OK!"), Err(A(PrintError())) =&gt; println!("Print Error."), Err(B(MemoryError())) =&gt; println!("Memory Error."), Err(C(DisplayError())) =&gt; println!("Display Error."), Err(D(())) =&gt; panic!("Makes this not compile if we add new cases"), _ =&gt; panic!("Unknown Error."), } println!("my_func3: {:?}", my_func3()); } **Option 4 (Using `Either` and `Triplet`, `Quartet` etc.)** Advantages: * Code will not match if you add new errors, or take errors away. * Errors are clearly part of every type signature. * Always able to downcast to concrete type. Disadvantages: * You have to write a new enum for every new number of types, so one enum for two errors, one for three errors, etc. * Pattern matching is more verbose, but not more verbose than option 2. **Option 4 (Using `Many`)** Advantages: * Errors are clearly part of every type signature. * Always able to downcast to concrete type. * No boilerplate whatsoever. Disadvantages: * Slightly verbose pattern matching * Code may compile if you add new errors, (But not if you remove them!). There is a workaround (see `main`), but the consumer has to remember to write this.
Your question is already answered, but just to add to this: in general if my lines get too long ant rustfmt starts going weird, I take this as a hint to try to break things up into multiple bindings. Not really sure what to call `f64::exp((x - self.mu).powi(2) / self.neg_two_sigma_squared)` though...
&gt; the glibc allocator has gotten quite better a couple of years ago (in 2.26, IIRC). Have you tested the system allocator to see if jemalloc is still worth using? I might try it once we get some more of real traffic. But the servers are running some oldish version of Fedora, so there's a high chance the improvements are not there yet. &gt; you touch on this, but it sounds like a large part of the improvements come from doing less (e.g. building the automaton outside of the service), and using better algorithms. Certainly. But the Rust library ecosystem is quite an enabler in this area too. For example, I haven't seen a library for Java that would allow building the image and just transferring it into another process. But even the Java implementation is heavily optimized code â€’ for example, the updates go to a great length to avoid allocation as much as possible, as that was just outright killing the GC back in the days it was implemented. That's also why nobody wants to touch that code base any more ðŸ˜ˆ. &gt; even without a GC, some people pointed out that deterministic destruction can introduce large pauses and/or even overflow the stack (imagine freeing a large tree). Yes, but I can control when and where they happen. If the data sources weren't just one big allocation but a tree or so, I would have to make sure they are destroyed outside of the hot path. Controlling when or where the GC hits is much harder. &gt; have you considered using a client-side hash or bloom filter instead of hitting urlinfo on each web request, similar to Google's Safe Browsing? Actually, yes. There are some problems with it. For one, even as the hashes this database does not fit. We could allocate few MBs for such database on the device only. Furthermore, this check against list of known-bad URLs is only one of the checks. Machine learning models to detect other things (eg your device has been hacked and sends spam) which definitely don't fit (too CPU intensive) are being added. However, I expect we'll try to move into that direction in the future, at least in part. Lowering the latencies and lowering the load of our servers is something everybody in the company agrees is a good thing. Increasing privacy of customers (and therefore trust) is something at least the engineering department mostly agrees is a good thing. There are actually some pieces in place already. However, there's only so much that can be done for the first release.
nope, that didn't do anything
manually flushing stdout - i haven't done this before. do you mean using a `flush()` method in the code? wouldn't any stdout buffers be released after i stopped my program?
&gt; Increasing privacy of customers (and therefore trust) is something at least the engineering department mostly agrees is a good thing. That's a nice way to phrase it :-). I skimmed https://www.avast.com/privacy-policy in the meanwhile, and it seems there's a certain conflict between the services offered and client-side approaches that transfer no private data to the servers.
I would vigorously oppose the addition of regex to std, personally.
&gt; even without a GC, some people pointed out that deterministic destruction can introduce large pauses and/or even overflow the stack (imagine freeing a large tree) For latency-sensitive services, building a large tree would be a problem by itself. Still, the nice thing about control, is that if necessary you can move the destruction to a background thread. &gt; have you considered using a client-side hash or bloom filter instead of hitting urlinfo on each web request, similar to Google's Safe Browsing? I would note that a Bloom Filter would not eliminate the need to send the URL every time; it's a probabilistic data-structure. The answer to "Is it in the set?" is either "No" or "Maybe", and in the "Maybe" case you need an actual source of truth anyway. That being said, I use Firefox without Avast exactly because I see no reason why a random entity should have a list of all the URLs I visited, so I certainly understand your privacy concerns.
From Rust 1.32, jemalloc is disabled by default. Why keeping it in your binary ?
AFAIK they try really hard to make the stop-the-world as short as possible, but so far I haven't seen any algorithm that is able to avoid it completely. I admit, I haven't been following that branch of research lately, so if you have a link to an article how to avoid it completely, I'd be interested. And I'm pretty sure if you really wanted and started to write the application from scratch in Java with the requirements in mind, you most probably could get significantly better results than the current old implementation has. But it probably wouldn't be *idiomatic* Java code would probably take more effort to write than it took in Rust. I'm just stating that these challenges play nicely to the strengths Rust offers.
I thought that even the most parallel GCs still had a small stop-the-world period for coordination?
Also, it's notable that most memory allocators, much like GCs, are configured for throughput by default. It seems that keeping tail-latencies under control is always an afterthought :( For example, the issue you spotted in jemalloc could be avoided by having jemalloc release the memory little by little, rather than all at once. It could "queue" the pages to be released, and only release one every 16/64/256 allocations for example to minimize the spikes, before even considering a background thread.
From what I've read, the system allocator is supposed to lock more often than jemalloc, which was specifically designed to handle multi-threaded loads well. The service is multi-threaded. So mostly just to be on the safe side of thing, there wasn't that much research into if the claim is really true. Also, jemalloc was disabled for two reasons. One is, it supposedly gave trouble on certain OSes/architectures (which isn't our case) and that many mostly single-threaded loads don't need it, so it's better to opt-in than opt-out.
Congratulations on introducing Rust in your workplace! That's definitely an example I'd like to follow :) --- I see that Avast isn't listed on the Users of Rust page: https://www.rust-lang.org/production/users Since from your post it appears it's using Rust in production; maybe it should get its entry?
As I understand, this is something that Jemalloc actually does, but not based on number of allocations, but on time. So if you keep the allocator busy, it does release by small bits or, even better, just reuses the memory. I think the spikes would just naturally disappear once the service got some actual load.
I want to have it included there once the product is actually out of the doors and reaches some users. Until then, claiming it's in production would be a bit of a lie.
This is interesting. Thanks for the insight! It's nice to see additional ways of solving this problem, though as you've pointed out, both of these also have some drawbacks.
Ooh yes I forgot about it, thanks guys !
Structural records definitely sound like something that could also be immensely useful, and would pair nicely with union types as well, I feel. Unions would be the counterpart to enums, and structural records would be the counterpart to structs/tuples. &amp;#x200B; I do agree that there are definitely a lot of other compelling use cases for this. I feel like, in a way, it could render **some** uses of the \`Into\` and \`From\` traits obsolete, especially when passing arguments to functions. Error handling just happened to be the one that I found most compelling at a glance. I think that if the community wants to pursue this feature in the RFC-land, then the more use-cases we can think of the better. Do you think you could come up with additional concrete scenarios where this feature would improve ergonomics? Thanks!
&gt;adding modules to the standard library stifles innovation, by discouraging programmers from using or contributing to competing PyPI packages... now that data classes are in the standard library Hynek Schlawack must defend his attrs package So competition stifles innovation? Hmm.
&gt; AFAIK they try really hard to make the stop-the-world as short as possible, but so far I haven't seen any algorithm that is able to avoid it completely. Yeah, you're probably right. But I believe that pauses of hundreds of ms are often avoidable by changing or tuning the GC (not that your article contradicts this). &gt; I'm pretty sure if you really wanted and started to write the application from scratch in Java with the requirements in mind, you most probably could get significantly better results than the current old implementation has. But it probably wouldn't be idiomatic Java code It would probably be idiomatic allocation-free Java code, which is mostly an ugly C with good lock-free collections. But people have implemented [cool stuff](https://martinfowler.com/articles/lmax.html) in that subset of Java. At this level you're transcending language features and limitations, I suppose.
Well, there's a history section on [Wikipedia](https://en.wikipedia.org/wiki/Rust_(programming_language)#History) which has some sources. I can't recall if Graydon or anyone else made a historical article, though.
The same that gets put into test.data.box. the whole idea is to somehow store the type to use it later to down cast without needing to tell the type later. In the playground I have ways to do it but I can't think of a way to store it dynamically with the box.any. In other langs, I would prob just create a function and save a pointer in the struct so the data would always be the same size no matter the function. But I can't find a way to do that in rust with a generic function either. Wonder if there is a way to recreate box where it'll have a method that generates with new that holds onto T when its created, like the struct example in playground.
I guess it's not a problem if the game is obscure and requires thought: [https://en.wikipedia.org/wiki/Nim](https://en.wikipedia.org/wiki/Nim)
From what I've read it's often more performant. Could be wrong though !
I was more wondering about the mechanism by which the information is embedded. I've seen various approaches to accomplishing this which work by appending data onto the end of the binary, and some of them break if they're not the last thing in the file.
Thanks for the insight! &amp;#x200B; This is an interesting one. It definitely looks like a far more "lightweight" feature which can still see some usage, but the question is if both union types and refined enums (?) should be in the language at the same time. &amp;#x200B; Since enum variants are not considered full types, I don't think it'd be a good idea to allow them be used as parts of a union, e.g. \`(io::Error | MyLibErrors::ErrorFirstKind | MyLibErrors::ErrorSecondKind)\`. My core reasoning for this is that enum variants can't be used like that elsewhere in Rust, and trying to tackle them in this way onto unions is, in my opinion, inconsistent. That is to say, union types, if implemented, would not (should not?) be a "superset" of refined enums. &amp;#x200B; I wonder, however, if they could complement each other, i.e. in a hypothetical scenario of \`(ErrorFirstKind | ErrorSecondKind | MyLibErrors::{SomeError, OtherError})\`. &amp;#x200B; let some: (ErrorFirstKind | ErrorSecondKind | MyLibError::{SomeError, OtherError}) = ... &amp;#x200B; match some { err: ErrorFirstKind =&gt; ..., err: ErrorSecondKind =&gt; ..., MyLibError::SomeError: MyLibError =&gt; ... MyLibError::OtherError: MyLibError =&gt; ... // this could be a sugar MyLibError::SomeError =&gt; ... // type automatically inferred } &amp;#x200B; This looks like it would make these 2 features even better together!
It's a call/jump to a single (statically-known) function/label, so I don't think it is particularly similar to what is usually called dynamic dispatch. For instance, the compiler can easily see what that target function is, and so, for instance, decide to inline it if it seems beneficial (the inability to inline, and thus inability to do most other optimisations, is one of the biggest problems of dynamic dispatch, beyond just the cost of doing a jump/call to a dynamic location).
If the nominal benefit of `momo` is compile time, shouldn't some sort of metrics (even something basic/"best case") be collected to validate that it's functioning as expected? It seems it would make sense to do this before putting a lot of effort into testing and documenting something that may or may not solve its target problem (and potentially even before a publishing a blog post titled "get back some compile time", so that the title can be justified/"proved"...).
For reference, here's what I consider to be the analysis of anonymous sum types. Advantages: * Zero boilerplate * Error types are clearly part of every type signature. * Easy syntactical representation. * Creates tuples for sum types, giving them an equivalent representation. * No need to create new types every time you want to return from a function. Disadvantages: * Type information is not available for the user (even though any implementation will require an internal type marker) * An invariant of anonymous types (or it should be) is that any given type appears only once, but this cannot be guaranteed with the current type system. * There is confusion as to whether or not the two isomorphisms `(A | B)` and `(B | A)` should be equal. * If they're not equal, then I would argue that you should use a regular `enum` because that makes the ordering explicit. You also wouldn't be able to make a function generic over both `(A | B)` and `(B | A)` without some kind of additional syntax if they're not equal, even though both functions would have identical syntax. * If they are equal, then how do we guarantee that in the type system with generics? * Unlike tuples, the ordering is unimportant, so they are not truly isomorphic concepts. These are probably nitpicking, but the combination of generics and anonymous sum types seems to always be a hacky compromise.
Erlang does that as well. Problem is, you still need to request memory from the OS or give it back to the OS at some point, which will block the thread no matter what. So hard-realtime systems usually avoid dynamic allocations altogether.
&gt; It has large &gt; syn &gt; crate as a dependency (it specifies the exact version even). So if you add &gt; momo &gt; as a dependency, chances are you'll have to compile yet another copy of &gt; syn &gt; in your project, even if you have a bunch of "syns" in your dependency graph already. It's not depending on an exact version. Cargo defaults to semver-compatible version ranges by default, and an exact version would be `"=0.15.34"` (https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html discusses the specifics). Anything else that depends on `syn` using `version = "0.15"` or `"0.15.1"` (or any other `"0.15.*"` version) will be able to use the same `syn` version as `momo`, and, similarly, if there's a new `syn` `0.15.35`, `momo` will be able to use/share that version.
I thought about this for a little while and this is what I came up with as what I consider ideal: // anonymous sum type with some ordering. Names for constructors are implied. let x : (FirstType | SecondType | ThirdType) = ... match x { 0(a) =&gt; ... // a: FirstType 1(b) =&gt; ... // b: SecondType 2(c) =&gt; ... // c: ThirdType } // structural product type analogue. Inline type definition. no need for boilerplate. let x : (Name1(FirstType) | Name2(SecondType) | Name3(ThirdType)) = ... match x { Name1(a) =&gt; ... Name2(b) =&gt; ... Name3(c) =&gt; ... }
It also bloats the generated binary and interferes with memory profilers and other allocator-related tooling, making them output nonsensical results. After a glibc update made it obsolete for most workloads it just didn't make a sensible default anymore, so it got dropped. Jemalloc also has some nasty corner cases, e.g. it makes writing to memory after fork() really expensive and drops performance ~10x on Linux in fork-heavy workloads. It also interacts poorly with huge pages, sometimes resulting either in [memory leaks](https://www.nuodb.com/techblog/linux-transparent-huge-pages-jemalloc-and-nuodb) or in yet another ~10x slowdown.
Related: https://github.com/not-yet-awesome-rust/not-yet-awesome-rust, https://github.com/dtolnay/request-for-implementation.
First slide deck about rust http://venge.net/graydon/talks/intro-talk-2.pdf
A talk about rust features that were removed: https://youtu.be/olbTX95hdbg?list=PL85XCvVPmGQh8nWR_Z-fTmPGsUWuzb-dn
Your approach is wrong. Rust is statically typed, so `p` must have a single type that the compiler can deduce. You should probably be using traits and dynamic dispatch (`Box&gt;dyn Trait&gt;`) to solve your problem. However, you should allso explain what you want, as this seems very much like an [XY problem](http://xyproblem.info/).
Graydon Hoare put up a repo of his early design notes here: https://github.com/graydon/rust-prehistory Between 2010 until around 2013-ish, most of the documented discussions around Rust development took place in #rust (and later #rust-dev) on irc.mozilla.org (public logs for which are hard to come by, but it's possible that some long-time lurker was keeping their own logs), the rust-dev mailing list (which is now no longer used, but an archive is available at https://mail.mozilla.org/pipermail/rust-dev/ ), and the issues tracker in the rust-lang/rust repo.
I'm not a computer scientist or a language designer, so when faced with questions like these, I usually try to approach them from a position of "what benefits does this particular decision or choice grant us, if picked?". I also don't fully understand all of the tricky implications when it comes to type theory, so please forgive me my ineptitude :) &gt; Type information is not available for the user (even though any implementation will require an internal type marker) Could you please elaborate what the practical implications of this are, when using the feature? &gt; An invariant of anonymous types (or it should be) is that any given type appears only once, but this cannot be guaranteed with the current type system. Should it? I don't know what the practical implications of not upholding this invariant are, but can't we just collapse a type of `(A | A | B)` into `(A | B)`? I think this would be the most ergonomic and pragmatic thing to do. Besides, the only case where I can see that occurring in the real world by design is when generics are involved - but couldn't this be solved by applying the solution I outlined in my post? &gt; There is confusion as to whether or not the two isomorphisms `(A | B)` and `(B | A)` should be equal. &gt;If they're not equal, then I would argue that you should use a regular enum because that makes the ordering explicit. You also wouldn't be able to make a function generic over both `(A | B)` and `(B | A)` without some kind of additional syntax if they're not equal, even though both functions would have identical syntax. My personal take on this is that they should be equal - for the same reason of practicality. What would be a use-case where you would want two types `(A | B)` and `(B | A)` to not be equal? I think that just introduces additional complexity without obvious benefits for the common use-cases. &gt; If they are equal, then how do we guarantee that in the type system with generics? Once again I'm not sure I understand. Why should the ordering matter? Conceptually this could be represented as a *set of types*, which has no inherent ordering. If two generic types in a union turn out to be the same concrete type, then we arrive at the problem outlined in specimen one, but I don't think it's a *huge* deal-breaker, since there's ways to resolve the ambiguity in a way that is relatively uncomplicated, unless I'm missing something here. I think at this point it's worth asking what should the **idiomatic** use of generics with union/sum/whatever types look like? Is it even a good idea to encourage using them with generics? The most compelling use cases are those where the types are known and concrete, not generic. Since generics introduce so many potential problems to the feature, maybe we could settle on a less-than-ideal solution, just to resolve the possible ambiguities, and discourage use of union types in scenarios where a lot of complexity is involved, and encourage using normal enums instead. Just like most of the times, it's better to use a struct than a tuple, even though there are cases where tuples win in terms of ergonomics. &gt; Unlike tuples, the ordering is unimportant, so they are not truly isomorphic concepts. Is that a problem? It's true that while tuples are like unnamed structs, the same analogy does not hold up for enums and unions, but I think that's fine. It doesn't have to hold up if the benefits are worth it (which in my opinion, they are).
Ah! I replied to your other comment, but since you're suggesting sum types with ordering, I'm wondering, what are the benefits of that over unordered ones?
Thanks for posting! What did your coworkers think of the Rust language / ecosystem?
Maybe just use two wrapper types around the `Rc&lt;T&gt;`: * `Unfinished&lt;T&gt;`: where `new()` takes `T` by value, moves it into an `Rc&lt;T&gt;`, implements `DerefMut` for use during initialization, and implements `Into&lt;Finished&lt;T&gt;&gt;` which destroys the first wrapper and puts the `Rc&lt;T&gt;` inside the second wrapper. * `Finished&lt;T&gt;`: which only implements `Deref` (not `DerefMut`) and implements `Clone` in the obvious way.
I'd at least want to document how to use momo and how to use `cargo expand` to get rid of it, saving even more compile time.
You're right, their main downside is that it only allows to create a subset of an existing enum, for cases where you need to merge distinct types you'd need full anonymous types. Compared to anonymous unions, refined enums could allow to reuse the same tag/discriminant in the full enum enum and refinements. I'm not sure how Rust optimizes the various casts, but by specifying the explicit subset relation, the compiler may avoid unnecessary conversions.
Arc has a get_mut method which returns a unique reference to contents if the ref count is one. Using this method, you can implement struct ArcBox(Arc&lt;T&gt;), which implements DerefMut&lt;T&gt; and Into&lt;Arc&lt;T&gt;&gt;. The memory layout would be essentially the same as with intrusive refcounts.
I can't see any better solution than matklad's or perhaps rolling your own Atomically-referenced smart-pointer. The problem is is that Arc::from_raw() does some clever magic to preserve the refcount so it doesn't exactly work with a raw pointer obtained from a Box or such.
The only issue would be that get_mut will check ref count every time, although we know for sure that it is one. Thereâ€™s no unsafe get_mut_unchecked method in Arc, but if you really need to make sure that know check happens, you can store a raw pointer in ArcBox and use Arc::into_raw/from_raw.
There is an `impl&lt;T: ?Sized&gt; From&lt;Box&lt;T&gt;&gt; for Rc&lt;T&gt;` which allows you to convert a `Box&lt;T&gt;` into an `Rc&lt;T&gt;` ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ffd0046495be2ac11b931ecbe5d7f69a)): use std::rc::Rc; fn main() { let mut x = Box::new(5); *x += 1; let y = Rc::&lt;i32&gt;::from(x); println!("{}", y) } Some people find [the `into()` form](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e1eecfebb375574a6c087a59a3a3e945) easier to read and work with, especially if the type has already been specified in your data structure so you can avoid the extra inline type annotation: let y: Rc&lt;i32&gt; = x.into();
I *think* this does reallocation and copying: you canâ€™t just reuse Box allocation because you need to store the reference count inline.
I have a talk for the ACM about it. Iâ€™m on mobile so I canâ€™t grab a link for you right now.
[`Rc::from_box`](https://doc.rust-lang.org/src/alloc/rc.rs.html#701-720) is a tad different though, it makes another allocation, move the object, and drop the first allocation.
You mention `one line change in Cargo.toml` with regards to the jemalloc change but not what it was. Could you share the change? The results seemed interesting.
SOAP (and related standards): not difficult, just tedious.
Adding the `background_threads` feature to jemalloc. ``` jemallocator = { version = "~0.3", features = ["background_threads"] } ```
Excellent. Thank you and great write up.
Funny, we're _(not me personally)_ using SOAP at work in Python, and soon Go. As we've been using Rust more and more lately, I'm curious where the pain points are in relation to SOAP, so I don't wrongly advise someone to use Rust in SOAP projects if I shouldn't. Mind going into more detail? I appreciate the heads up regardless :)
I'd first ask if such creating T on stack first and then moving into the Rc has a measurable cost. What I'm thinking: &amp;#x200B; Rustc/llvm might be clever enough to see through your code and construct the object inside the allocation even if you haven't written it that way. &amp;#x200B; Even if it does move, it's one memcpy of an object you probably have in the L1 cache. If the object is not huge, the cost could be really small.
Ah, this stores the data as a constant in the program which goes in the data section. It allocates a string of text which is used to identify that section, followed by your config, then it fills the rest with 0s to keep the byte size the same. Since the section always must have the exact same number of bytes, say you input "hello" as the config, since that is 5 bytes, and I allow 2024 bytes to be allocated, it would append 2019 0s to that, then replace it with the previous one.
You should post it on r/playrust
Well, it's not explicitly *non-commercial* but any company who wants to make money off of selling a service falls into one of two categories 1. they're making profit off of it, and the only reason they are doing that is because they own the software, and no one else can run it. 2. They're making profit off of it because of some other reason (They're trusted, they have the resources or staff or the user base to make it usable). Not *all* companies fall in the top category, but a fair few do. Like, wolfram alpha, even the pro features, would probably run well enough on a home server. The reason you need to pay for pro is not because they have massive servers to handle the load, it's because they own the software. (They *sell* mathematica for use on home machines) And as for the second group, I don't really mind. Okay, yeah, maybe non-commercial isn't the best term, but it makes it hard or impossible for people to profit purely based on copyright law.
I only used SOAP in a former life as a dark matter java developer, so Iâ€™m not in a position to go into detail with respect to rust. From my memory, though, oneâ€™s experience depends heavily on the endpoint you have to hit. Trying to work with one that required a lot of WS-* standards in my java days left me with PTSD.
How are you arranging to do things "on an interval"? There's right ways and totally wrong ways to do that.
Using Tokio and polling a Stream which contains a Tokio Interval with a `println` after it.
Servo has a fork of Arc that includes a separate [UniqueArc](https://docs.rs/servo_arc/0.1.1/servo_arc/struct.UniqueArc.html) type for this purpose.
Yeah, that's going to require sharing the complete code for anyone to figure out. That's a bizarrely complex way to achieve the stated outcome but presumably there's more goals which are being accomplished by doing it that way.
https://www.youtube.com/watch?v=79PSagCD_AY
I don't plan on ever learning await, but I feel now I have to learn it, and teach it to new people very early, so they, and I, don't mistake it for variable access.
I'd say this is pretty accurate. I've used SOAP a few times, on different projects. For some, if you have the WSDL, they're trivial, and on others, it's horrendous. I dealt with one that required asynchronous callbacks, and that one had me wanting to shoot someone. (Either myself or the guy who designed it.)
&lt;3
Great points! Itâ€™s fairly easy to implement, too: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=23ffe0e104668bc2d2e7391bcbaeb294
&gt; This crate isn't deciding to monomorphise or not This crate splits type conversions from the rest of the function body. This is a very special case of something that in general amounts to understanding what part of the function body does not depend on type parameters and could be reused across all monomorphizations. I have a feeling that it's better to think about this part in terms of CFGs, not ASTs. Probably at MIR level. &gt; Cargo defaults to semver-compatible version ranges by default Thanks. I didn't know.
i have not asked anything about particular language, it can be any decent type safe language , like typescript for example. But right now rust fast is replacing JS and i am getting into the trend so its natural to see what others think about the issue.
I wouldn't consider numbering the variants "ideal". If this is used for error handling, then we'll get many different subsets of a relatively small pool of types (the errors used in that app/lib) and the numbers will always be different. The same error can be number 2 when returned from one function, number 4 from another, number 1 from a third function, and so on... This can become very confusing very quickly. Having named variants is better, because then you can give the same name to the same type of error everywhere, and it'll be much less confusing. (I actually prefer to refer to the variants by their type, but if the choice is between positional and named I prefer named).
Are we talking about regex like \*any\* regex or regex like specifically the one crate called regex?
It's not a hello world or anything. It's a program that needs a runtime I've been working on.
people with mediocre intellectual levels love JS, dont waste time conveniencing them what they cannot handle.
So, to be clear, intrusive refcounting is orthogonal to the problem of having unique mutation. You can have intrusive refcounting without unique mutation, and you can have unique mutation without intrusive refcounting. Intrusive refcounting is annoying in Rust because you essentially have to implement an unsafe trait for each type you wish to put inside `IRc&lt;T&gt;`. You can smooth this out with some custom derives (for structs), but it's not quite clean. It's still a very useful pattern. For unique mutation, we do have `Rc::get_mut()`, and if you call it shortly after constructing an `Rc` it _may_ optimize to avoid the uniqueness check. Exposing `RcBox` is indeed the solution here. Stylo has a fork of `Arc` for FFI reasons, and I maintain a copy of it on crates.io [here](https://docs.rs/triomphe). It has `UniqueArc`. It doesn't have the same abstractions for `Rc` however.
servo_arc is internal, it's better to use [triomphe](https://docs.rs/triomphe/) here, which is a fork of ServoArc, but better documented and intended for public consumption.
&gt;So with **Third method** we â€“ as a user â€“ have to rely on the documentation which errors to expect but we shouldn't. It's a dangerous option. What if it doesn't return a third kind of error now but does so in the future?The type system would not help you?
Both I guess?
Pure Rust MP3 decoder. A largely automated translation of minimp3 codebase has been started and made very good progress but is not finished: https://github.com/icefoxen/rinimp3
I'm glad :)
No, and you will still need plenty of tests in Rust, but you can use the language to encode more of the business logic as part of the type system and prevent errors from sneaking in. The explicit error handling is also a major factor with this imo. This can also become very boilerplate heavy though, so finding the right balance is important. In general, your typical crud backend will benefit much less from Rust s type system than other areas though , so the benefit is limited. Combined with the immaturity of the ecosystem and lack of proficient devs I have to say that Rust will rarely be the right choice in this field at the moment, unless you really need max performance.
Not sure this counts as "easy" tho.
Interesting. I reach for regex so often in Python it feels like a reasonable thing to have there, and I don't really expect breaking changes for a regex lib to be super common either. Why do you feel that way?
Aren't there privacy issues with sending every URL that someone visits to your servers? Do you log these requests? Have you ever sold this data?
Donâ€™t forget `Drop`
Just to illustrate this pattern of multi-line function call arguments: function_call( arg1, arg2, arg3, ); That last comma doesn't just compile, it's usually seen as a good thing.
Friends don't let friends use SOAP
I tried. I _really_ tried this time. Some issue with GL on the Mac platform is giving me quite the headache. I have an issue [linked here](https://github.com/KenSuenobu/rust-pushrod/issues/113) with an article that describes how to do the buffer swap. This _was_ working at one point, but it no longer wants to run. I'd love eyes on it - the code verbatim doesn't seem to want to run on OS X 10.14.5. For the record, current code runs at about 33-38% CPU usage on my system. When I ran the same GL code on my Linux desktop, it dropped to a whopping ... wait for it ... 6% usage. And I think I can get that down even farther. But if I can't figure out this bug, it's going to take a longer time to solve. That's why I've backlogged it for now.
I agree that this is how I would format this with a function that truly takes multiple arguments. I just through it was strange that `rustfmt` added the comma to a function that only takes one argument. &amp;#x200B; But I understand why now.
I agree. I was a GUI API developer on the Atari ST back in the day, and a lot of my design principles (KISS) come from that. They have been applied to this library. My old friend Karl also seemed interested in the project, so I might have an additional old friend working on the project. I am in the process of separating out the project's widgets into its own crate, which will allow me to create a whole bunch more types of widgets. If you have suggestions of widgets you'd like to see, please feel free to visit the Issues page on my project, and add a suggestion. I'm all ears, as I want this to be the "defacto" GUI library!
God, this is so true. One only uses it when someone youâ€™re required to integrate with demands it. No one chooses it, unless trapped.
Well, SOAP is more difficult than necessary in *any* language. It's a little less difficult when WSDL tooling is available, but that's not a language thing at all, but a community thing. People just don't do SOAP in Rust AFAIK, and I'd like to keep it that way.
You mean [this](https://rust-lang.github.io/async-book/)?
Given that the feature is fairly new, I'm sure people will write up more detailed blog posts and maybe even books once they have time to play around with it extensively.
Looks promising. I'll look into it once it's complete.
OpenGL on Mac is deprecated, and Apple actively doesn't do anything to fix it... e.g, Dolphin (the emulator) recently had a breakage where you end up just having to use [MoltenGL](https://moltengl.com).
This is the reason. With so much change, nobody has wanted to really write stuff up. Now that itâ€™s settling down it makes more sense to.
Iâ€™m always eager to dogpile on SOAP, but I wouldnâ€™t go so far as to wish that someone who wants to choose rust cannot, because it makes the integration they need to perform difficult.
Quick and dirty solution, can you change CustomError to take a (u32, T) tuple as its only argument? Something like this: ``` struct CustomError&lt;T&gt;((u32, T)); fn example1&lt;T: Hash + Eq&gt;( map: &amp;HashMap&lt;(u32, T), u32&gt;, value1: u32, value2: T, ) -&gt; Result&lt;(), CustomError&lt;T&gt;&gt; { let key = (value1, value2); match map.get(&amp;key) { Some(_) =&gt; { /* do_something */ } None =&gt; return Err(CustomError(key)), } Ok(()) } ```
I wonder if a macro could be created to achieve something like this.
Ah yeah. That is interesting. I think that could work. I did not consider the idea of making a tuple struct have a tuple.
I updated that with a better option that doesn't require changing the Error struct.
Oh my gosh, is it really that simple? This is a facepalm moment for me. That seems blatantly obvious after actually seeing it.
I have a lot of coworkers, so it's hard to generalize. (: Some like it, some are indifferent, some even took it for a spin in a recent Advent of Code.
I know that I'm guilty of this. I feel a constant need to reinvent the wheel: to write things that have already been written, to try new ideas and then publish them - without providing long-term support for them as projects in their own right. Does anybody else feel the same way? How can this be resolved? Perhaps, like the platform support tier system, we should have certain Rust crates that have "tiered" official support in the Rust ecosystem and become officially-recommended projects?
If you're stuck on older glibc, jemalloc can be MUCH faster. I have Rust code that's tries to be nice to the allocator that's 20% faster with jemalloc. Some older production C code that was very mean to the allocator and had way too many threads was 100 times faster on jemalloc (on production traffic). I would like to test on a newer glibc but I'm limited to older OSes at work. &amp;#x200B; Jemalloc contains memory profilers and leak detectors that works better than any of the external tooling I've tried. The key feature being that I can enable tracing of memory allocations on production servers with only a 1% impact to performance during the tracing. Then I can find leaks and hot allocations immediately on the troublesome instances. Valgrinding production would be insane, all the LD\_PRELOADs of tracing tools don't seem to provide easy activation/deactivation and add an additional layer of atomics/locking which just slows it down too much for live production tracing. It's been a while since I've looked for tools (since jemalloc solved it for our C code), so if someone knows of an equivalent for the newer glibc I would be interested. Sadly I don't think there is a good Rust API for the memory tracing/arena control in Rust's Jemalloc. &amp;#x200B; I think the huge pages issue was worked around years ago, so I doubt it affects the version in the jemallocator crate. Also I would think most Rust code would prefer threads to forking if at all possible.
It's not a book, but [This video by Jon Gjengset](https://www.youtube.com/watch?v=9_3krAQtD2k) goes into quite a bit of detail about what's going on under the hood. It's 4 hours long, so probably best consumed in multiple sittings.
Indeed! The impl body should be the same as in `Into&lt;Arc&lt;T&gt;&gt;`.
On that note ... pure rust Opus decoder. I think this is very important. Opus is currently the best lossy audio codec technology we have and it is a web standard that is getting adopted widely. Most VoIP software and many streaming services use Opus now and people are increasingly encoding their music into it. This is not something "easy", however. I've seen a couple of projects being started for this, but they all seemed abandoned.
Challenge: Dedicate an hour of your game time each weekend to reading [This intro book](https://doc.rust-lang.org/book/).
The [Rustonomicon](https://doc.rust-lang.org/nomicon/README.html) was really good for me. It goes over all the nitty-gritty stuff about borrow checking, PhantomData, edge cases, and especially `unsafe`. Even if you don't use unsafe Rust, it was really helpful for me.
Guilty as charged. But so far at least two projects are not yet committed to `crates.io`, except that the first-come, first-serve basis for crate names made it very tempting to publish some of them. I've convinced myself that their names will stay unused for a year or so until I find out if I can actually support the projects. In general though, the article presents the lone wolf syndrom as being overwhelmingly motivated by ego of the developer or social issues but I think that's too simplified. My own, first impression of other libraries solving related problems is *almost always* that I have little to no influence in their development. That is, my instict tells me to start my own project before even having been in a social situation and this is true even for crates that have not reach any stable (`1.0` and upwards) version yet. While certainly true for some projects, it doesn't need to be this way. I don't think that offical crates or a recommendation list would combat this issue. What about adding more indications to the willingness of crate maintainers to design changes? Being explicit in the code of conduct and `Contributing.md` is a good start but I think it comes too late sometimes. But `crates.io` already has a flag in `Cargo.toml` [namely maintenance](https://doc.rust-lang.org/cargo/reference/manifest.html#package-metadata), and neither `actively-developed` nor `looking-for-maintainer` accurate fits the bill, so maybe something to the effect of `searching-for-fresh-ideas`.
OOP? Meh.
It's pretty common for people to work with somewhat locked down hosts that they can't install stuff on, but can copy a script to. A large stdlib is a blessing in that case. If you're developing python applications, the large stdlib may be a downside, but for scripting it's a big plus IME. Irrelevant for rust given compilation of course :-)
webm
I mentioned this in a sibling comment. But you can search for the privacy policy.
Async programming is just programming using the Future Monad, but with the boilerplate of the Future Monad removed. &amp;#x200B; For me to understand it, I first understood what a Monad is, then I understood how a future or promise (async) is a Monad. In my opinion, the most thorough way to learn what a Monad is is by learning functional programming such as [https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653](https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653) . The very short explanation is that a monad is a chain of computations. The chain typically looks like this: &amp;#x200B; Foo.andThen( someLambda andThen ( someLamdbda and Bar ) ) &amp;#x200B; Basically, the "andThen"'s continue the chain of computation and the chain typically ends with an "and" or something to that effect instead of an "andThen". The async keyword in say JavaScript (there is a library for it in Scala) just hides the boilerplate so you don't have to write "andThen andThen andThen" &amp;#x200B; I don't know if you'll understand this, but I made a video explaining it in Scala a while back: &amp;#x200B; Monads in Scala: [https://youtu.be/mXUze0vH-PQ](https://youtu.be/mXUze0vH-PQ) Futures in Scala: [https://youtu.be/JPbeFZn3OV0](https://youtu.be/JPbeFZn3OV0) &amp;#x200B; From this series that I made: [https://www.youtube.com/playlist?list=PLXcr3tdUCbQb6zjN6kw4s20joId2jygUe](https://www.youtube.com/playlist?list=PLXcr3tdUCbQb6zjN6kw4s20joId2jygUe)
Not terribly impressed with the web design... http://0x0.st/zm1P.png
That's a fair criticism :D
Good idea - thanks! Have you thought about doing videos to complement the blog posts?
The thing that is getting in my way is that I code because of love, not money. If I just wanted to get money, I could get a job so fast.
I didn't want to read that and would have preferred if you had never said it. What do you expect me to do, change who I am to make you happy?
Why the hell did you even tell this to me? Seriously, do you expect me to go: "Oh. Well kod from reddit doesn't want to interview me because he doesn't like my personality. I can and should change who I am as a person to make kod happy." Really?
Because it would kill development on it and its internals. There's too much overhead to contributing to std. And it would remove the possibility of ever making any breaking changes. In fact, Python's `re` module in the standard library has suffered because of this. Take a look at the `regex` package on PyPI for something more "modern" that cannot be applied to `re` because it would be a massive breaking change. I think people just need to accept that Rust's standard library is thin and all the costs and benefits that come with that. Using a crate from the ecosystem should be banal. That is, "because you use it a lot" isn't really a good enough reason. &gt; it feels like a reasonable thing to have there Of course. A fat standard library is a reasonable thing to have. But this is a matter on which reasonable people can disagree, so "reasonableness" isn't really the right litmus test to apply here.
Your playground link doesn't work. There's [no gist with that hash](https://gist.github.com/rust-play/2554f10427b6428d60338ad475b2cf04).
What an utterly useless comment. Why even bother commenting?
Please be sure to dispell the idea that classical inheritance == OOP and explain why Rust is great at OOP and why inheritance is often such a bad idea.
\&gt; " I'm not sure if this comes out in the interviews or communications, but if it does, when you leave out an impression of very annoying type of beginner/junior who probably gonna be resistant to learning anything." &amp;#x200B; And the impression that I get from you is you can work with people who are like you, but you can't work with people who are different the way I am different. That's not actually a good thing. &amp;#x200B; \&gt; "Provided it showed up in somewhat generally condescending context." &amp;#x200B; I don't try to be condescending, my ego is just big, and I don't do it on purpose. Most people are, on average, less intelligent and technologically competent than I am. That's all I was saying. I wasn't deliberately trying to be condescending. &amp;#x200B; \&gt; "It's to late to go deep into this one, even if I could. But .. there are a lot of competing theories and practices." &amp;#x200B; I don't give a fuck. There is one theory that I like most, and it's immutability first functional programming. OOP is file if there is also functional programming (like with Scala), but I want functional programming. I don't give a fuck what the pay is - I want what I want and I want functional programming like I got in Scala. &amp;#x200B; \&gt; "This is evident in martial arts, which was a snake oil salesmen paradise until MMA came out and put all of them to a test in a brutal survival game. Business is MMA to software engineering. If theory is not applied in wide spread manner, when it probably doesn't work or if it does work, when it doesn't scale or fails at cost" &amp;#x200B; Amazon and Facebook were originally built on Perl and PHP respectively, and they made lots of money. The choice of technology isn't that important from a money perspective. You can use bad technology and make lots of money or you can use good technology and lose money. &amp;#x200B; \&gt; "But being fixated on being dogmatically correct over being effective is generally not a good quality of a software developer." &amp;#x200B; That is not about being dogmatically correct. It is about being happy. Amazon Web Services was a 4.4 star employer on glassdoor, but I was not happy. Bank of America had a lower rating, but I was happy, and the thing that really made the difference for me was the technology that I got to work with. I want to be happy working at my job. &amp;#x200B; \&gt; "And even if you end up in R n D department of some sort, realizing that your mission is to provide value to others first will get you there faster. Currently, from the way you put it, I can't feel even a trace of this." &amp;#x200B; I provide value to people and to things that I love. Your inability to understand and appreciate people who are different than you is apparent. This is something that you as a person should work on because there are all different kind of people in this world.
The SOAP problem has been solved - don't use SOAP
I'd be very interested in this series and how you bridge from OOP to Rust. As someone who writes a lot of object oriented code I find it really hard to write Rust code that doesn't suck, just because I think very differently about code structure than is best used in Rust.
webm is a container, not an encoder/decoder
DM me if you want to join the clan TrungSJ#2852
**The fragile base class** is a good example of why inheritance in OOP isn't a fantastic idea. From Wikipedia: &gt;The **fragile base class problem** is a fundamental architectural problem of [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming) systems where base classes ([superclasses](https://en.wikipedia.org/wiki/Superclass_(computer_science))) are considered "fragile" because seemingly safe modifications to a base class, when inherited by the [derived classes](https://en.wikipedia.org/wiki/Subclass_(computer_science)), may cause the derived classes to malfunction. The programmer cannot determine whether a base class change is safe simply by examining in isolation the methods of the base class. [A Study of The Fragile Base Class Problem \[PDF\]](http://www.cas.mcmaster.ca/~emil/Publications_files/MikhajlovSekerinski98FragileBaseClassProblem.pdf)
75% of the APIs that we need to integrate at work are external soap apis. Supporting soap is not a choice, it's a hard requirement
What do you do if one of those â€œbatteriesâ€ has a license that is incompatible with your project, or if the license changes in a future release? This isnâ€™t a hypothetical question, just curious to know how to deal with this problem if it arises.
I think it is more than just this. I know some developers whose employers donâ€™t let them rely on any dependencies outside of their standard library (language agnostic). This is for reasons of security, legality, and compatibility. Maybe it is overkill to outlaw so many open source libraries, but it can also be a blessing to know that your dependencies are all either hassle-free or owned by you.
For anyone that doesn't know what Webrender is [The whole web at maximum FPS: How WebRender gets rid of jank](https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/)
One requirement to be a "battery" is to have a liberal license that fits every use-case.
Is there evidence Rust is suffering from the same problems? The article doesn't actually mention it, it only talks about LISP.
The article doesn't reference Rust, and doesn't intend to. I posted it here because I thought it was interesting to consider given the context of the many competing crates that exist in the ecosystem. I think it would be useful to think about solving this problem by perhaps tiering crates according to ecosystem support, or to find some way to push more standard interface crates to the top of crates.io searches.
I'm curious, what it look like? Like if we already had the `DSTStack` data structure (give it whatever API you like), what would this code desugar to? fn add_one&lt;'a&gt;(f: dyn Fn() -&gt; i32 + 'a) -&gt; dyn Fn() -&gt; i32 + 'a { move || f() + 1 } fn main() { let x = 1; let g = add_one(|| x); assert_eq!(2, g()); }
I'd love some built-in clap completion!
Excellent, `triomphe::UniqueArc` is exactly what I was envisioning, right down to the name. Thanks!
How can I recreate a recursively connected tree with many different classes?
regarding the docs thing, and assuming the people you're teaching are using rustup, `rustup doc` will open a menu in your browser that has descriptions of and links to the newest versions of all the main documentation (std, the book, by example, cargo book, etc).
One thing we noted in our study of novice Rust reports is the problem of borrow checking issues that can't be solved without unsafe (Sec 3, Hypothesis 1): https://arxiv.org/pdf/1901.01001.pdf Some of these (e.g. [split_at_mut](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.split_at_mut) for vectors) are implemented in the standard library, but I think there's a potential broader effort of cataloguing these kinds of issues and producing a centralized index of mid-level borrow check idioms. This could both help users discover these patterns when they encounter them, link to relevant libraries/blog posts/RFCs, as well as highlight yet unsolved issues. Part of the underlying issue is that as a Rust programmer, when you encounter a borrow check issue, you have to guess: do I need to restructure my code, because I don't know borrow check semantics well enough? Or do I need to use an external library, because this is fundamentally hard for the borrow checker? (Or both?) And hopefully such a resource could help with that question.
I was a Be developer way back in the day, and this was an enormous problem in BeOS. The entire application framework was C++, and relied heavily on inheritance, leading to big issues whenever a base class changed. It was a Very 90s Ordeal. This does not change my nostalgia for BeOS, but it is an important lesson that we can't forget.
I knew Java and some C# before Rust. Now I'm trying to get my coworkers to see the value of using interfaces over inheritance.
Makes sense. If a crate were to change to a more restrictive license for a new release then, I suppose it would lose its status as a â€œrecommended battery?â€
Looks great. Quick fix for your readme. "See the examples, specifically [with\_powerset\_enums.rs](https://github.com/idanarye/rust-powerset-enum/blob/master/blob/master/powerset-enum/examples/with_powerset_enums.rs) vs [without\_powerset\_enums.rs](https://github.com/idanarye/rust-powerset-enum/blob/master/blob/master/powerset-enum/examples/without_powerset_enums.rs), to understand how this works." contains dead links. &amp;#x200B; [https://github.com/idanarye/rust-powerset-enum/blob/master/powerset-enum/examples/with\_powerset\_enums.rs](https://github.com/idanarye/rust-powerset-enum/blob/master/powerset-enum/examples/with_powerset_enums.rs) [https://github.com/idanarye/rust-powerset-enum/blob/master/powerset-enum/examples/without\_powerset\_enums.rs](https://github.com/idanarye/rust-powerset-enum/blob/master/powerset-enum/examples/without_powerset_enums.rs)
What kind of non-beginner are you in this case? I think the answer will depend on whether you're an advanced rust user looking to learn a particular topic, or, for instance, advanced in another low-level like C++, or in a high-level language like Python. I'm not finding it entirely clear what exactly you're looking for, and if your non-beginner status is in Rust or programming in general?
mypy is a must for Python
Awesome explanation!
What an utterly useless comment. Why even bother commenting?
This is awesome. What are the next big pieces of oxidation to look forward to?
As far as better API goes, instead of \`pub fn r\`/\`pub fn w\`, you could implement \`Deref\` and \`DerefMut\`. You might also want to make the other methods on \`Reader\`/\`Editor\` associated functions instead, to avoid ambiguity with the \`Deref\`ed type's own methods. I'd also rename \`Editor\` to \`Writer\`.
Author here; a quick note about this crate: I created it for another project ([Rosy](https://docs.rs/rosy)) where I'm writing high level two-way bindings for Ruby that aims for: - 0 or low cost abstraction against the C API, exposing `unsafe` alternatives where there may be some cost - Uses Rust's powerful type system such as generics to maximize code reuse and reduce unsafety When I feel that it's somewhere comfortable to use without much headache, I'll write a full post with details about it.
If you'll excuse the self-promotion, [Rust in Action](https://www.manning.com/mcnamara) is intended as "the book you read next" once you have exhausted the free resources.
Have you ever taken Haiku for a spin? It even has a Rust port these days...
What would `let x: u32 = !0` mean? (I am clear on every bit, except for the `!`)
I'm by no means an expert, but one idea is to put the objects in a flat array and then store their indices in a tree with the recursive structure you want.
&gt; with many different classes This just in: If you frame your problem as an OOP coding challenge, it may be more difficult to figure out a ~~not totally broken~~ non-OOP way to solve it.
It's the [not](https://doc.rust-lang.org/stable/std/ops/trait.Not.html) operator, which for integer types is implemented as a bitwise not. `0_u32` represented in binary would be 0b00000000000000000000000000000000, so when the bits are flipped it's 0b11111111111111111111111111111111, which is `std::u32::MAX`, which is 4294967295.
&gt; you have to guess Please don't spread this everyone-is-as-clueless-as-I-am FUD. Everyone has limits but believing *your* limits are universal is the height of arrogance. Point being, here it's entirely unnecessary to guess as long as you understand lifetimes, and the sad truth is that understanding lifetimes is as much a prerequisite in other languages as rust.
It's a disease, don't blame user, they are sick
Thanks!
Note that function arity is not a part of rust syntax and thus rustfmt doesn't and shouldn't know it.
One of the things that has *really* drawn me to rust is that all libraries are semi-documented *by default*. It is *literally* impossible to publish a library to crates.io without also publishing a set of reference documentation that is guaranteed to be both *correct* and *comprehensive*. Now, you can obviously write no content for your docs (or worse, bad or out of date content), but the list of public types, traits, etc is *always* correct. I didn't realize what an absurdly big deal this was until I went back to do some React programming this weekend and discovered that, while Typescript is great, most libraries don't ship a comprehensive listing of their public interface.
Maybe you meant to post this to r/PlayRust? This sub is for the programming language of the same name. It's fun to learn though, if you want to get involved!
So excited. When will start shipping?
I imagine after async/await is stabilized that better documentation will slowly accumulate, once it is being used consistently in the wild.
Oh sorry D:
The module system is a bit troubling, especially when using the main/lib pattern, i always have to look up how to do it when i start a new project.
It's not like WebRender is "finished" in any way, but probably replacing the font renderer with Pathfinder.
Is there a way to get an iterative list of clap args? For example, if I run program as "MyProg A --B=C --D=E" and something is wrong with "--B=C", I want to still get back that it was called with A.
This brings up a good point, once async has fully stabilized, will it be in the rust book? Iâ€™m assuming it must be.
Trying to figure out how to implement a Language Server Protocol server for my little grammar and writing utility tool. A lot more concepts I don't understand yet.
I've worked on my neofetch-like info tool a lot with a couple other people. It's been improved so much due to all the recommendations and tips I got from my other comments/posts about it. It's also been renamed from "fetch" to "rsfetch" to avoid confusion with the BSD and Mac tool also called "fetch". I think my favorite thing about Rust is how fast it is. There has been at least triple the amount of code added since I first shared my tool, but it's execution time doesn't even go above 1.5 ms. It's just insane to think about. [Here's](https://github.com/rsfetch/rsfetch) a link to the Github repo for it. I'd love to get feedback on it's current state.
That's awesome! What does the function signature looks like in the generated documentation for code that uses it?
&gt; when you encounter a borrow check issue, you have to guess: do I need to restructure my code, because I don't know borrow check semantics well enough The issues I've encountered as a novice were all easily solved by adding a `.clone()` or adding the `Copy` trait to a data structure.
You can't just "change to more restrictive license" just like that. Any decent library will have many contributors, anyone can fork it etc. making it not really feasible to change license except for rare cases when there's a unanimous agreement.
Reading the MEAP version every now and then and loving it! Topic selection and writing style really appeals to my personal taste. Thanks for doing the hard work, you rock!
Eagerly waiting for Linux+Nvidia support.
Is there anyone here aware of a projector similar to CEF but relying on Firefox instead?
Rust's native asynchronous functionality does not chain `and_then()` calls undercover, but uses generators - so it works in a completely different way than manually-composed futures.
I guess such severe restrictions may make it a lot easier to avoid the many problems, though still extremely difficult to get right. Without having any deep knowledge of eBPF or the demands of network router features implemented on top of it - it seems to me that similar performance could be achieved by going in the other direction, i.e. implementing the network interface driver and TCP stack in userspace, without compromising kernel stability.
[Here](https://developer.mozilla.org/en-US/docs/Mozilla/Gecko/Embedding_Mozilla), but last time I looked at it, this seem rather inactive.
Can I forcefully enable this in nightly somehow?
You can set the pref gfx.webrender.all to true in about:config.
`cargo run --example` might fit your needs!
21st of May
The "Unlicense" is a bit problematic, see https://softwareengineering.stackexchange.com/questions/147111/what-is-wrong-with-the-unlicense. For a "do whatever you want with it" license I'd recommend CC0 instead.
But now there's the question: "What is w liberal license?" Personally, for me it's something like "MIT" or "Apache 2". They do not really restrict me in what I can do with code licensed with them. Otoh personally I dislike things like the "GPL" for trying to force restrictions on me. So I wouldn't consider it a good "liberal" license for "batteries". At the same time I feel like calling "GPL" not liberal is not right. So that'd be a big talking point imo.
thats how Scala died a slow silent death, they were dragging the dead horse and were also boasting about it, until it was too late. REPL and SBT killed Scala, and if Rust dont focus on 100% open source IDE it will die too , just like JetBrains drew the final stab into the guts of Scala in hope of making billions which it could not do with Kotlin. Learn from C# instead. History repeats itself, BEWARE !
Thank you for the link. It was pretty interesting, especially with Unlicense being banned in Germany. Are you allowed to change your license after it is already set on Github?
Thanks! Enabled
Sauron is an html web framework closely adhering to the elm architecture. This week, I've added implementation for `Cmd` for triggering commands when you app is initialized or updated which can optionally trigger some other commands. The example for how this is one is found in https://github.com/ivanceras/sauron/tree/master/examples/fetch_data
Itâ€™s not by default yet, but you can already flip the `gfx.webrender.all` preference in `about:config`. After a restart, look whether â€œCompositingâ€ says â€œWebRenderâ€ in `about:support` to check.
Thereâ€™s https://wiki.mozilla.org/Mobile/GeckoView on Android.
i use HSTR
Thanks! It works but not ideal... I tried to create a folder in example, then cargo does not recognize it.
Thanks! Tried on my work laptop with Intel HD Graphics 620, getting CPU load average of 3-5 depending on content. The main process seems to hog a single core at all times. I wonder if it's actually using HW acceleration? Or is the integrated GPU just so horrible? Will need to try with my main desktop at home to compare.
[https://wiki.mozilla.org/Oxidation#Rust\_Components](https://wiki.mozilla.org/Oxidation#Rust_Components)
Hi, I'm going through the substrate crypto kitties tutorial. I am confused by the trait syntax used in the first example. Can someone please explain this line for me: trait Store for Module&lt;T: Trait&gt; as KittyStorage { // ... } Is this a trait definition and implementation in the same block? Here is the link [https://shawntabrizi.com/substrate-collectables-workshop/#/1/creating-a-module](https://shawntabrizi.com/substrate-collectables-workshop/#/1/creating-a-module)
I feel like I really don't experience the re-building thing much, particularly _because of crates.io_. We can really easily recreate the wheel, but also really easily depend on work that other people have done! I can pull in a crate by adding one line in my `Cargo.toml` and immediately explore the documentation. This makes me much, much less motivated to re-invent things. The other factor is the consistency between different crates in the rust community. We have many nice patterns, things like builders, ways of using visibility, etc.. These make it really do make it easy to adopt other people's work, and remove a lot of the need for in-house development. I don't have to learn new paradigms every time I use a library. I usually don't even need to memorize any method names! Most things are consistent, and we have nice guidelines to enforce that. There's also the fact that rust doesn't have things like Higher-Kinded types, and that macros definitely have limitations and costs. The rust community implements a lot of functionality in user libraries, but very little of that functionality is language-level - things like object oriented programming support. It's mostly all things which really can be abstracted well as libraries, and that can be depended on well because of well-guided and consistent APIs.
How is this in any way relevant to this post?
I think you're getting tricked by Non Lexical Lifetimes! It's not the fact that you are passing `foo[0]` in, but rather the fact _that you are using foo after altering smaller_. If you don't use `foo` at all, then the borrow "foo" starts at line 6 ends at line 6, and doesn't conflict with your mutable borrow. For example, this is legal: let a = smaller.iter().collect::&lt;Vec&lt;_&gt;&gt;(); smaller.clear(); But this isn't: let a = smaller.iter().collect::&lt;Vec&lt;_&gt;&gt;(); smaller.clear(); println!("{:?}", a[0]); The reason this, and `.remove(foo[0])`, error, is that you haven't truly cloned the data. You've collected into a `Vec`, but that's still a `Vec&lt;&amp;T&gt;`. The items in the vec still reference data inside the hashset, so clearing or removing that data from the hashset will invalidate those references. Instead, I would recommend either requiring `T: Clone` and using `.iter().cloned().collect::&lt;Vec&lt;T&gt;&gt;();`, or preferably using the dedicated method [`HashSet::retain`](https://doc.rust-lang.org/std/collections/struct.HashSet.html#method.retain). This is a hard problem to get right (even in GC'd languages like Java, removing from something that you're iterating from is an error), so using the specialized method built for this is recommended.
I don't think the source of the problems is the same. I think that Rust is still in the gold rush phase: a shiny new language to do things in and everyone wants to do their thing.
I don't exactly have a list. What I meant is things like this: * You want logging. And you want your logging to be configurable by a config file. There really wasn't a ready-made solution for that. So I wrote [spirit-log](https://docs.rs/spirit-log). That one still is not great and would need a lot of work which I really don't have time to do (I'll probably find some time to list tasks around spirit and try to attract some people with a promise of help and mentoring). * Metrics inside your application. Eventually, we've chosen to go with [dipstick](https://docs.rs/dipstick), and while the library looks really promising, it also doesn't feel finished. * And others. What I feel all these have in common is: * They are not inherently *hard*, they are just lot of work. I mean, they are not easy either, but it's more of choosing the right API than clever optimisations to shave off 10% of RAM consumption. * You probably don't need them when doing a small home side project or little utility. They are however needed in a corporate world. * If you stick with the very corporate Java, you just have a library to throw at the problem. I think the problem will be simply solved by having enough corporate users and time.
Did you follow [this layout](https://doc.rust-lang.org/cargo/guide/project-layout.html) ?
It's not a valid rust syntax, but it's used inside the `decl_storage` macro, so I guess the macro expend to separated definition and implementation blocks.
Just [released](https://github.com/ivanceras/sauron/releases) sauron 0.7.0 today, with bunch of improvements of the api, most notably added Http for simple fetching of data in a rest api and injecting the data into the view seamlessly. Hit a milestone in [diwata](https://github.com/ivanceras/diwata) rewriting the elm-client to sauron, with minimal changes to the existing html structure.
Have a one Vec per type, and use newtyped indices (FooId, BarId) to refer to the different types. You can use Slab-s instead of Vec-s if you need stable IDs with deletion. Or use idcontain.rs to also prevent id confusion when ids are reused. This method is also more efficient with much better memory locality.
Thanks for the explanation. I've come to more wired syntax in the examples. Guess they are making a DSL with macros.
Preparing a new release for the ics crate (writing iCalendar files). I've been trying to make things more convenient by having const structs and several constructor methods instead of dumb derived defaults. I've finally figured out how to add doc comments in macros. I only need to clean-up my macros to make them even more convenient. Once that is done, I will try to tackle a small type system for the input. Probably numbers first. For the 1.0 release I will also change the edition to 2018 to make it more ergonomic plus a small parser. This way the last 0.x release should have all the features for rustc 1.26.
You can already do that in stable!
Force enabled it and what a shame this did not arrive sooner. There is a clear change in smoothness.
you are trying to undermine Rust by polluting it with old Unix traditions, soon the Emacs, Vim and such sick editors will start flocking to Rust choking its growth and then strangling it to DEATH way before it replaces JS. So how is it not Criminal hmm ?
Hello, heaptrack author here. I don't use libgcc for unwinding, instead i use libunwind: [https://github.com/libunwind/libunwind](https://github.com/libunwind/libunwind) &amp;#x200B; The name libunwind is pretty widely used, afaik gcc, llvm and other projects have a libunwind too ;-) &amp;#x200B; Cool project btw!
You can also add many `[[bin]]` sections to your cargo.toml: [[bin]] name = "my-cool-binary" path = "src/my-cool-binary.rs" Then you do `cargo run --bin my-cool-binary` I think.
Not pretty: ```rust pub fn load_data_file_or_default( path: &amp;Path ) -&gt; Result&lt;Data, &lt;&lt;Error&lt;!, !, !&gt; as WithVariant&lt;Error&gt;&gt;::With as WithVariant&lt;FieldError&gt;&gt;::With&gt; ```
sorry will repost in the new question section thanks for letting me know
Wow, this is great! I really, really hope that anonymous enums are going to be a thing in rust some time. I _love_ the error handling capabilities they enable. Thank you, this is indeed a great start to discuss possible language integrations without _too_ much bikeshedding about syntax :D
trying to get my generate tress function to work I don't understand why I am not getting the correct type. https://gist.github.com/rust-play/03f6b0e240b5983f4138d7af8adb28f7 https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0e15cd79b8a86a930f5a9563d9e925f5 not sure which link is more helpful
thank god, please do that, i am seriously interested in such since my near future goal is to encapsulate all the html and its events into reusable objects/ Rust classes. :-)
Fixed. Thanks!
Hey, I'd just like to thank you for asking this question. This is the kind of content that helps people who are complete beginners. It's nice to see where your brain went to try to fix things and what kind of problems I can expect to make in the future.
check these links [https://en.wikipedia.org/wiki/Composition\_over\_inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance) &amp;#x200B; [https://riptutorial.com/rust/example/22917/inheritance-with-traits](https://riptutorial.com/rust/example/22917/inheritance-with-traits)
Remove the semicolon from line 88. Then you're gonna get another error, to fix that change line 67 to `.collect::&lt;Vec&lt;_&gt;&gt;();`.
Yep, github renders instantly!
Forgive my essay. If you create anonymous sum types, I imagine there will be plenty of people to use it to return from a function which may give multiple types. &gt; Type information is not available for the user (even though any implementation will require an internal type marker) So I'm no expert either, but the basic idea is that `enum` is like a dictionary (which has some kind of ordering), and anonymous sum types are like a set (which have no ordering, and presumably no duplicates). That means that there is strictly less information in an anonymous sum type than there is in an equivalent `enum`. That means we're going to be ok converting an `enum` to an anonymous sum type, but converting it back is going to be undefined unless we specify additional information. #[derive(Debug)] enum Triplet&lt;T, U, V&gt; { Left(T), Centre(V), Right(U), } fn from_triplet&lt;T, U, V&gt; (input: Triplet&lt;T, U, V&gt;) -&gt; (T | U | V) { match input { Left(x) =&gt; x, Centre(y) =&gt; y, Right(z) =&gt; z, } // OK } fn from_anonymous&lt;T, U, V&gt; (input: (T | U | V)) -&gt; Triplet&lt;T, U, V&gt; { match input { x: T =&gt; Left(x), y: U =&gt; Centre(y), z: V =&gt; Right(z), } // The behaviour of this is undefined. } `from_anonymous` is undefined. If we have no ordering, then `(T | U | V)` is the same as `(V | U | T)`. How does the compiler know which type to assign to `Left`? Is it `T` from `(T | U | V)`, or `V` from `(V | U | T)`. In the general case, this is undecidable. We simply don't have enough information to cast an anonymous sum type to an `enum`, so you would have to provide ordering information. If you think you can use the order of the `match` arms to resolve this ambiguity, this may not be sufficient: fn from_anonymous2&lt;T, U, V&gt; (input: (T | U | V)) -&gt; Triplet&lt;T, U, V&gt; { if let x: T = input { return Left(T); } if let y: U = input { return Centre(y); } if let z: v = input { return Left(z); } } Here is a particularly perplexing function: fn unholy&lt;F, S&gt;(a: (F | S), b: (F | S)) { match a { first_a: F =&gt; { match b { first_b: F =&gt; feed_the_world(), second_b: S =&gt; fire_missiles(), } }, second_a: S =&gt; { match b { first_b: F =&gt; feed_the_world(), second_b: S =&gt; fire_missiles(), } } } } fn main() { a: (u32 | &amp;str) = 10; b: (&amp;str | u32) = "Hello."; unholy(a, b); } Do we feed the world or do we fire missiles? if `F` is `&amp;str` and `S` is `u32`, we feed the world. if `F` is `u32` and `S` is `&amp;str`, we fire missiles. The compiler has no way of knowing which is which. Creating an ordering based on earlier branches of a `match` won't solve the problem, because we could access the data through a series of `if let` expressions. &gt; An invariant of anonymous types (or it should be) is that any given type appears only once, but this cannot be guaranteed with the current type system. Here's an example: trait MyTrait { fn do_something(&amp;self) { println!("Defalt implementation."); } } impl&lt;A, B&gt; MyTrait for (A | B) { fn do_something(&amp;self) { fire_missiles(); } } impl&lt;A, B, C&gt; MyTrait for (A | B | C) { fn do_something(&amp;self) { feed_the_world(); } } fn blissfully_ignorant() { let x: (u32, u32, &amp;str) = 15; x.do_something(); // confusing to say the least. } if we call `blissfully_ignorant`, do we fire missiles or feed the world? `x` will collapse into an `(A | B)`. Do we select to fire missiles at monomorphisation? Imagine being the guy that has to track down the bug where only some of their usages of a generic function break because it turns out another of their generic functions would spit out a result with two types the same. On the other hand, we could choose not to collapse it. If we do that then have another look at the `from anonymous` function I wrote above, and consider the implications of adding a duplicate type to already undefined behaviour. Essentially I believe that you would have to restrict anonymous sum types to concrete types only, and enforce no duplicate types at compile time. Note that forbidding matching on anything except a concrete type essentially means you can only put data inside a union and not remove it when you need it.
all of the problems you list are valid and frustrating, in my experience: 1. if you find yourself combining a lot of things (or moving through states) itâ€™s often cleaner to wrap everything in a [pollable object](https://tokio.rs/docs/futures/basic/), that contains the interesting items / state, and has the added bonus of saving you from a compiler bug that occurs if you chain to many futures. 2. this should be helped by the pin API. if youâ€™re method chaining you can often `move |a| a.do().map(move |r| (a, r) )` and pass the actual object down the chain, and if youâ€™re writing anything you intend to consume from somewhere else (for example, library implementations) or you have to do this more than three or four times, see option one. 3. yep
also clone like the wind imo. tokio provides wild performance at the cost of incredible complexity which you may or may not need. itâ€™s totally reasonable to trade memory and performance for both progress and sanity, and for most applications, it doesnâ€™t matter at all.
I feel dumb for asking this, butâ€¦ Is there an idiomatic Rust way to split a single module across several files? If so, what is it? Right now I'm using one file per module with `use` and `pub use` statements, but there are times when I really just want to break a module into several pieces, I think? For example, in my [woodpieces](https://gitlab.com/BartMassey/woodpieces/tree/master/src/game) minichess code there are a bunch of modules that have fairly tight coupling. It's not obvious to me that I'm buying much by splitting the core game code up. I'd prefer to put the core game code in a single `game` module.
I figure unless you can introduce dependent types and GADT's in rust, then unordered sum types will break generics completely. My "essay" is a reply to one of your other comments.
Maybe add some whitespaces in-between ?
Working on the 0.3.0 release of [crin](https://crates.io/crates/crin), (a CLI for crates.io) which will add nesting support for your custom lists of crates.
There are so many insane statements in this comment it's hard to know where to start. What are the old Unix traditions discussed here? Why does emacs and vim "flocking" anywhere mean? And why would that choke a language's growth? Why would you assume that rust is destined to replace JS, and why would you want it to?
Thanks for downvoting without explanation, I was actually curious why not.
All good points, and great examples. We don't want to fire the missiles :) Thanks for explaining. With all that said, it looks like there are a lot of compelling reasons against using union types with "naked" generics, but if we put all that aside for a moment, what would the benefits even be in unholy generic scenarios using union types? Would it be a huge drawback if they were, indeed, restricted as to avoid precisely this situation? That's what I'm wondering. I also like your proposal suggesting named union variants. Though, I do think it comes at somewhat of an ergonomics penalty, which in my opinion significantly reduces the appeal. Obviously, that heavily depends on what you're looking to gain from this feature, too. I'm going to stop here for now since I'm on a phone and sleep-deprived. Going to need more time to process all of this.
Please please don't use justified text unless decent hyphenation is enabled. This typography is beyond terrible.
Still what most people do when trying to solve things is to google it and it is really annoying when most hits are stubs of the old book Is there way to tell google to remove the old book or priorities the new book first?
Using just the types to identify the variant is the least work. Using names requires the developer to come up with names and the user to learn them. If you introduce names or ordered types, can you have one error type more than ones? e.g. `let x: (FirstType | FirstType);`. Having no order and no names is easiest. `let x: (FirstType | SecondType)` and `let x: (SecondType | FirstType)` would be equivalent.
You proposal gives Rust a way to do function overloading. ```rust pub fn foo(bar: (First | Second)) { match bar { s: First =&gt; foo_first(s), s: Second =&gt; foo_second(s), } } fn foo_first(bar: First) { ... } fn foo_second(bar: Second) { ... } ```
Cool can't wait to try, last time I enabled was getting crashes often so disabled it
At some point, yes.
I hate to add more criticism to the website, but Tachyons seems like a poor choice for this style of website. Tachyons takes a "custom styles for every page/element" approach, which means that modifying styles in one place won't change them anywhere else. In some cases that can be an advantage as it allows you to get deep into complex styling without worrying about making it generic. But in the rust website's case, where the styling is relatively simple but ought to be consistent, it seems like this would be a pretty big disadvantage. (And I believe that there were a number of issues opened around inconsistent styling when the website first went live, so this isn't just a hypothetical issue).
i know what you are doing. but i am wise and old, so i will hit you right where it hurts by ignoring all the irrelevant questions. Look up some videos of old IDEs from the 90s , check if they have anything equivalent to SBT or cargo, and find out how they made new projects using the IDE UI. That was in the 90s , thats 30 years ago, thats how far behind you want us to be; you might be able to silence me everywhere but i wont go down in silence.
I appreciate the criticism, but from a project perspective, I inherited a project that is 50% in a migration from A to B. You are proposing moving to C, where C is undefined. I have inspected the problem and decided that the best way forward is pulling through the migration. Additionally, closely looking at the website, though, there's not _so_ many commonalities between all the pages and about every page has a custom section somewhere. Avoiding "action at distance" in a web sense here is actually useful.
I'm not the author of the post.
On the off chance you're not trolling: Why would someone building a tool for whatever have any impact whatsoever on the direction the language is taking?
This could become a defacto to read after reading the official Rust book, for us experienced guys stepping into Rusted murky waters ;-)
So long as you are the sole author of all the code you can relicensing it at your own whim, otherwise the contributors needs to sign off that they acknowledge and agree to relicensing their contributions under the new license. For the most part, the projects I've seen on Github that have done it do it with a checklist for all contributors and then ask each of them to reply with something like `"I approve the license change to MIT/Apache-2.0"`, once everyone agrees you're good to go.
Is there a way to disable lints from toml file?
As far as I remember, the previous time this was posted, the poster never tried to justify it as anything but off-topic spam.
I haven't used Nom is quite a while, but I think you want the [preceded](https://docs.rs/nom/5.0.0-beta1/nom/sequence/fn.preceded.html) macro, with [many0](https://docs.rs/nom/5.0.0-beta1/nom/macro.many0.html)!( [anychar](https://docs.rs/nom/5.0.0-beta1/nom/character/complete/fn.anychar.html) ) as your first argument, and your date\_parser as the second argument.
The arguments against using types is that it causes problems with generics: ```rust fn foo&lt;S, T&gt;(x: (S | T)) { match x { _: S =&gt; println!("Fizz"), _: T =&gt; println!("Buzz"), } } foo::&lt;u32, u32&gt;(42); ``` Should this print "Fizz" or "Buzz"? I still prefer a type-oriented solution, and I have [my own suggestion on how to solve this problem](https://www.reddit.com/r/rust/comments/9tpb77/anonymous_enums_type_collapsing_and_generic_match/), but this still is a problem that needs to be addressed.
Servo's wiki has a 2019 road map as well https://github.com/servo/servo/wiki/Roadmap
I neither upvoted nor downvoted, but keep in mind that, when people downvote the original post, there's a strong psychological element of "A downvote means I don't think these should be showing up in my feed". It could just be people saying "Ugh. *Another* person arguing about the chosen await syntax? I don't care anymore. Just stop talking about await*"
You *can* use `include!(somefile.rs)` but I'd generally suggest trying to find ways to decouple logic if you can.
Nope. Totally not a requirement (prerequisite). I've made several cool things in Rust and managed to not really understand lifetimes. Don't spread that FUD. To actually address your dissatisfaction with the root comment though - it's possible to understand the root of lifetime mechanics without having a good intuition about how they affect program structure. You can understand lifetimes and paint yourself into a corner. There's understanding lifetimes, and then there's knowing how to effectively work with lifetime imposed constraints.
Thank you very much for this, as someone who mostly writes Java this is an awesome resource!
I strongly recommend adding a [ring](https://github.com/briansmith/ring) dependency to it.
ah...didn't know about that so far...will give it a try!
Why would I need ring, when this a client side webapp fetching data from a server. The encryption is already handled by the browser's https/ssl.
I imagine it's a Lord of the Rings joke.
Oh, I was seriously thinking about the technicality, /r/woooosh/ just past my head :)
Functional CSS is something I originally hated, but am coming around to. Itâ€™s not really about â€œnot making it genericâ€, it is a different philosophy around how you use styles that many front end people (which I am not, to be clear) are finding significantly more maintainable. Iâ€™d chalk up those inconsistencies to the â€œthereâ€™s two CSS frameworksâ€ than tachyons.
I'm suspecting my idea isn't good but I don't know why, what's the best way I can find out why?
endtest spam, user is affiliated look at post history, also uses multiple accounts to push this crap.
&gt; That being said, if you have an alternative path, would like to own and be responsible for it it &gt; and &gt; make sure it is implemented on a short timescale (say, a month), I'd be happy to have you on board. (and I seriously mean that) Is it required to use rust? I see that's used fairly heavily in the repo, and I can understand why that is. If you're ok with Gatsby(React.js based), I might be open to it. I share a similar concern about the Tachyons approach.
I'd like to avoid javascript styling. The website is semi-static, it's a rocket app that mainly serves static content, but serves as a host to all kinds of functionality (like redirects and i18n [upcoming]).
It appears smoother and faster, but are there any benchmarks yet for rendering popular pages? Github, youtube, amazon, etc...?
Generally you make a folder with your module name, inside the folder you have a bunch of other modules/files and in a mod.rs you `mod` and `pub use` those other files.
Gatsby can handle i18n afaik, by js styling do you mean instead of css? You can still use css, or you can use a library like Linaria which has benefits of css-in-js but can still export a css file during static build. I understand the reluctance of css-in-js as well as react itself with its mix of markup with js, it's a hard sell initially, I remember feeling quite uncomfortable about both of these for quite some time. Once you actually give them a proper go though, it can cause a change of heart ;) Gatsby is really great and can provide you with a lot of wins for free. The current iteration of the site loads a bit slow(I've not got great internet here), and I'm rather sure that Gatsby would make a difference with it's focus on performance. It has other perks too, I may be a tad biased as I contribute to the gatsby-image module. Let me know if you're open to the idea, I could setup a few pages and host them for you to try and look at the source. If what you see makes you happy, then we can go from there.
Also, I'd love to see not only benchmarks but also CPU load comparisons.
&gt; Gatsby can handle i18n afaik. We're currently integrating fluent, preferably through the Rust implementation, the same goes for the templating engine (handlebars). &gt;Gatsby can handle i18n afaik, by js styling do you mean instead of css? You can still use css, or you can use a library like Linaria which has benefits of css-in-js but can still export a css file during static build. I must admit that I don't have a full overview of how gatsby would operate and it seems like a whole departure from what we currently have. Mind that the current page is known and understood by the current WGs and I don't want to reteach them. &gt; Gatsby is really great and can provide you with a lot of wins for free. The current iteration of the site loads a bit slow(I've not got great internet here), and I'm rather sure that Gatsby would make a difference with it's focus on performance. It has other perks too, I may be a tad biased as I contribute to the gatsby-image module. This is mostly due to unoptimised images that are actually not cached: https://github.com/rust-lang/www.rust-lang.org/issues/319 There's an easy fix to be had there once we got cache-busting for CSS.
If I understand what NaCl is properly, it's more like a predecessor to asm.js/wasm. The primary concept that I'd never seen before The Birth and Death of JavaScript was the concept of running sandboxed application code on Ring 0, allowing the language/runtime to do virtualization rather than relying on the OS to do it. It's basically the acknowledgement that the abstractions provided by compilers and interpreters provide the same safety but with more utility than the abstractions provided by the operating system. Whether it uses bytecode or just Ruby or something isn't really that relevant. I actually would love to see an OS that only accepts safe Rust code as programs and can run it on Ring 0 to avoid all virtualization.
I'm pretty sure Rust should have been written in Rust.
Iâ€™d like to know more about total power load as well. What impact is expected on overall battery life?
IMHO, the number of lines is not the only way to gauge a compiler complexity. Rust has the advantage to be modulable: the code can be splited in self-contained crates. If Iâ€™m not mistaken, that's the direction that the compiler teams follow.
We can set `rel="canonical"` for all old books _under our control_. We used to have serious issues with a US university that hosted the Rust `0.11` documentation as part of a workshop and ranked really well due to their huge pagerank.
Hooray, RBR is within driving distance of me (Indy) this year!
&gt; Mind that the current page is known and understood by the current WGs and I don't want to reteach them. What do you mean by that? The code for it? If they want to make changes? With Gatsby, you separate the data from the view. You've done that only for users from what I can see. If your concern is with making changes elsewhere, that data can be extracted out to a variety of formats that can be edited directly in a git repo, or via a CMS like Contentful and others. Upon build, Gatsby queries the data sources(files/APIs/etc), and applies it. A common setup is a CMS like Contentful(free tier), and a static host like Netlify(free tier). Whenever the CMS or git repo update, they use webhooks to tell Netlify to do a new build/update. You can get a staging branch preview before deploy if you desire, among other features via Netlify, upon deploy you benefit from it's CDN. It's a nice workflow. If you were referring to modifying the actual site(style/markup) rather than the content, I completely understand. Gatsby would be quite a bit different, light changes aren't likely to be a problem, they have pretty great documentation and active helpful maintainer community. &gt; This is mostly due to unoptimised images that are actually not cached I wasn't referring to caching, but first load. Images didn't have placeholders, there was that 56k experience of progressive downloads for rather small images(at least display size I didn't inspect the source) when viewing the team pages. Text was also a bit delayed, web fonts I guess. Gatsby offers other benefits here though, responsive images(it'll create different resolutions from the input, setup picture markup to handle them all, add placeholder, provide webp and other format as fallback, optimize your images, etc) and lazy-loading support. Preloads and prefetches to prioritize loading, the prefetching especially, as the static pages rehydrate into a React website/app when JS is available, content for pages you'll navigate to can be fetched in advanced, so that when you click the link, instead of loading a new page that incurs a refresh, the DOM is updated with new data and you get that SPA like experience, navigating between the two pages is very fast. In addition to that you can get offline support as well as PWA. There's more but it'll drag on :) Gatsby saves time in the long run personally considering the value it provides. &gt; May I ask where you are based, just to log that issue? Auckland, New Zealand. My neighbourhood will get access to fiber later in the year.
Hi, I have few questions: * Do you want an employee or a freelancer? * When do you want to begin the project(s)? * Is it possible to work remotely during a part of the week? Thanks!
I'm a student in Bristol at the moment, looking for an internship this summer. A rust-based role sounds ideal!
I expect you would run into issues with compile times, tooling, ecosystem, libraries, frameworks, and interoperability. New programming languages have huge hurdles to overcome.
About the missing `dyn`s, there are a few "disabled by default" lints which can be useful if enabled as warnings/errors. You can find them in the rustc book
What is the cost of this jemalloc feature? Curious whether it ought to be opt-out rather than opt-in. Your learning the hard way how to use a straightforward setting that maybe ought to be activated by default is something worth considering..
&gt; Hello, heaptrack author here. I don't use libgcc for unwinding, instead i use libunwind: https://github.com/libunwind/libunwind Oops! Sorry for the confusion! I stand corrected. &gt; The name libunwind is pretty widely used, afaik gcc, llvm and other projects have a libunwind too ;-) So, the `libunwind` used by GCC is not the same as `libunwind` the standalone library, and there are multiple unwinding libraries, and they are all available on the same platform, and they are all called `libunwind`, and they all expose the same API...? (: (`_Unwind_RaiseException`, etc) Although now that I think about it then it does make some sense as the API is part of the C++ EH ABI apparently, but it doesn't make it any less confusing.
Full time employee based in London but some flexible working is ok. We're already building but we have one role left to fill.
Do you have Rust experience? Have you got a portfolio or github?
Thank you for your answer. What is the salary range for this position?
Perhaps that was a bit hyperbolic. I certainly don't want Rust to _exclude_ someone because they need a particular technology, I just hope that the community chooses alternatives that I _think_ are better. However, if SOAP is well supported, that means people are using it, and that makes me a little sad. You can get most of what you want with Serde, so it's doable, just not full-featured.
Wow this is an awesome tool! Perhaps I missed it in the documentation but it doesn't measure the time taken on allocations/deallocations over time does it? I see a timestamp for allocation events but I'm assuming those are collected after the allocations are done.
That would be backwards incompatible :) Maybe in a future edition.
Oh, i know about that, i didn't mean that they should be enable by the compiler by default, but new projects should use them (and, maybe cargo should add the `#![deny(...)]`s by default, similarly to how it sets the edition automatically for new projects)
Great.. that's exactly what I'm looking for.
Ah! What opportune timing for me to have posted my thread just a few days ago! &amp;#x200B; This is great, and definitely a move in the right direction. &amp;#x200B; Still, after some discussion and thinking, I'm starting to wonder if we can ever fully support anonymous union types when generics are involved. /u/pilotInPyjamas made a [particularly compelling post](https://www.reddit.com/r/rust/comments/bqcgt7/anonymous_union_types_good_idea_or_bad_idea/eo7nuen?utm_source=share&amp;utm_medium=web2x) describing the very interesting implications of one possible implementation, which is, probably, also the implementation most people are expecting for this feature. &amp;#x200B; If you had to pick, which one would you prefer?: 1) Resolve the ambiguity in some way, perhaps at an ergonomic cost - force variants to be named and/or numbered, do not collapse types, etc. 2) Save ergonomics for most common use-cases (error handling etc.) at the cost of explicitly forbidding users from using non-concrete types in their unions. 3) A third option?
Messaged you
I don't think this is good because it can become confusing how something is bound in more complex expressions, also the `-&gt;` makes me think of functions, not a variant of `.`
Check out [https://www.reddit.com/r/rust/comments/bqn9e6/announcing\_the\_powersetenum\_crate\_a\_poor\_mans/](https://www.reddit.com/r/rust/comments/bqn9e6/announcing_the_powersetenum_crate_a_poor_mans/) ! Looks like someone else already did it :)
I propose to also remove the dot: `get_connection()!await.call()!await`
50k to 70k dependant on experience. 32 days holiday.
After some thought and consideration, and having had several (very important, and very confusing) caveats pointed out to me, I'm tempted to arrive at a conclusion that Rust simply isn't prepared to adopt this feature in a way that would be feature complete and work well with generics. There's a lot of reasons for that, a lot of which I personally don't understand well, but my interpretation is that Rust today simply isn't equipped to reason about types in a way that would be necessary to resolve some of these questions. &amp;#x200B; One way to resolve this would be to screw generics entirely. Only allow concrete types to appear within a union type, and check at compile-time that they satisfy all the other invariants necessary to guarantee that we don't get ***the weird behaviour.*** This has the benefit of retaining the nice-looking, short syntax for usage in error handling and possibly function overloading. The downside, is, it seems very weird to have a composite datatype where generics are not supported. Another way to solve this would be to force the users to either enumerate/name their variants. That can solve a lot of the issues stemming from trying to resolve questions like "if `(T | U | V) == (U | V | T)` and `T == U` then what happens?", but I personally feel is less than ideal, and trying to accomodate for a use-case that might not (or should not?) be widely-used, at the expense of the common one.
I got soft approval from a couple of key people in my organization to release as open source a tool I wrote at work. I'm just waiting on CEO approval. So hopefully, I'll be setting up some automated builds of a personal code coverage tool this week.
So, here's the problem: you are basically asking us to move our whole stack. While I wouldn't say that I'm happy with all components of it, it's not like we randomly just selected stuff. Concerning the move to a CMS system: we are kind of doing this, by moving to fluent and pontoon, so most of the text will vanish. But for that, I don't need to pull in a whole new base framework. &gt; If you were referring to modifying the actual site(style/markup) rather than the content, I completely understand. Yes, that's the point. Concerning responsive images, we can totally move towards them, but we don't need gatsby for that. A better font loading strategy is also a totally valid request, I'd like to go incremental there. Finally: we can't assume people to have a node environment around and we're pretty strict on that. People that want to hack on Rust stuff should be able to do it with the things that _we_ provide and have under control.
Isn't there a concern that allowing anonymous enums and similar features would hurt the quality of the ecosystem in the long run? It would encourage people to also use them as ad-hoc function return types in cases where a function might return one of two similar types, instead of thinking a bit harder about what types a function is really supposed to return. I feel like you'll start seeing, especially in applications, developers being lazy and just returning (i32|f32) or (Vec&lt;T&gt;|HashaSet&lt;T&gt;) and similar instead of dealing with conversations internally. It's a feature that might enable accumulation of technical debt in some projects.
How do you handle multi trait objects? (e.g. `&amp;dyn Foo + Bar + Baz`)
So, the solution is to make a database. Hmm
Currently my pseudo-trait object is actually a struct, not a real trait object, and its behavior is defined by a single trait, so for the moment I would say I don't support that just yet
I donâ€™t think starting from scratch with an entirely new second technology stack to generate the website is something that fits the criteria u/fgilcher has outlined: &gt;We need to stay rather conservative with our tech stack, we'd like to have as little non-Rust as possible to enable the community to run the page easily &gt; &gt;a shorter time frame, better manageability and a future plan Fortunately, none of the improvements youâ€™ve enumerated require Gatsby specifically or Node.JS in general, nor do they need to be baked in wholesale from the beginning, so perhaps, once the migration to Tachyons is complete, we could incrementally implement those features through Rust and regular client-side JavaScript.
How do you handle panic=unwind? unwinding in a `extern "C"` invokes undefined behavior - these have the `nounwind` LLVM attribute, and in some cases results in mis-optimizations.
Yeah no worries :) like I said, fully understand why you'd want to stick with rust where possible. You can re-implement all of the things Gatsby offers that are relevant to you, nothing wrong with that. You said you were open to alternatives that could be delivered in a short time frame, so I thought I'd offer my time doing such if it were acceptable for me to use what I believe is more appropriate at achieving that along with all the other benefits it brings. I was not expecting that offer to go down well if showcasing rust and/or prioritizing contribution from the rust community were of high importance for the website. Otherwise it'd not be an issue to switch the stack considering the benefits elsewhere.
Currently I assume it doesn't, but once the memory leak is patched I intend to throw a call to `panic::catch_unwind` inside the `extern "C"` functions and convert the unwinding panic into an abort
This looks nice; Will you ever support other common archive formats like zip/7z?
Without a minimal working example (without dependencies) of what you are actually doing, helping you requires investment time that I (and probably many others don't have). I don't think it is reasonable to expect that people will pull your crate, come up with an example that should fail (really), pull your proc macro dependencies, figure what the example expands to, etc.
I'm not really concerned about that, because we still have compile-time enforced static typing, and our typing system is stronger than most. If your function returns `(i32|f32)` any user of that function (including you, when writing tests or other functions that use it) will have to **address both** possibilities - or it won't compile. So it's easier to just set one return type and use `into()` to convert the other type.
Why doesn't this work for you? impl&lt;'a&gt; Drop #obj &lt;'a&gt; { fn drop(&amp;mut self) { Box::from_raw(self.reference); } }
It's 4am here, so I either missed the part you quoted or it was edited in since, probably the former. No nothing requires Gatsby, nodejs or anything else out there. It's more of a avoid NIH, all these things are done(and there's quite a bit), take advantage of it. By all means re-implement all that however you see fit if that's more appropriate for the project. I was looking at it as purely a website, the stack I'm familiar with, and the mentioned openness towards alternatives that can be delivered within a reasonable time that are worthwhile. Regarding the requirements, rust isn't required to manage content, style changes are using css and markup handlebars. I'm not really seeing how JS was that big of a leap there and from what I saw of the rust source for the project. A shorter time frame I'd argue you'd get with Gatsby, as well as better manageability, but I know the community I'm discussing this in and how most people tend to feel against the suggestions I'm discussing here(the downvotes on my comments are further indication of that). It's barely starting from scratch if there's an existing site that just needs to be ported. I was happy to take on the whole thing anyhow, but because it doesn't sit well with others here that my approach would move the website away from rust, that's irrelevant. Sort of the opposite RIIR, ha. It's all good, I understand the community and maintainers reasons, all the best! :)
I see, that response was edited after I replied. I don't think I missed it, the part you quoted just wasn't there at the time.
Using a different data structure to build your graph in no way makes it a "database."
I'd definitely consider gatsby for my next greenfield project :). BTW: logged the responsive images thing here. https://github.com/rust-lang/www.rust-lang.org/issues/783 Could you please open an issue describing your font issues exactly? Though, if it's just late loading fonts and the text is visible, I would stick to that, because the other option is having nothing visible or just parts, which I hate even more.
What's the use-case for this? The only time you care about whether there's an internet connection available is if you want to connect to something specific, so why not just do that? What's the benefit of doing if online(None) == Ok(true) { /* do whatever it is that needs internet access */ } over just /* do whatever it is that needs internet access */
You can also create a bin directory under src, and anything under it gets compiled as a binary e.g. src/bin/hello.rs becomes the binary hello. This works even for lib crates
What are the main benefits you gain by dropping GNU compatibility? Is it mostly about simplifying things, or are there any particular mis-features that it's better to be rid of?
You're not alone, I will be replacing it with a macro as soon as possible too.
Can you post a test case that demonstrates the memory leak?
That doesn't work because casting it back to the right type requires passing it back over the FFI boundary to the library, which then requires a function pointer in the struct for the job. If you look at the snippet I linked above, I tried to do that and it results in a segmentation fault for reasons I'm not sure of.
Trying to learn pest so I can convert the dotenv parser to use that instead of a regex.
I would just like to add: [https://tokio.rs/docs/getting-started/futures/](https://tokio.rs/docs/getting-started/futures/) as a source to understand futures better. Reading this, seeing Jon Gjengsets video linked to by [u/codeallthethings](https://www.reddit.com/user/codeallthethings/) and reading [The Async Book](https://rust-lang.github.io/async-book/) is a pretty good start, but I agree there is a huge room for improvements in the documentation.
&gt; Though, if it's just late loading fonts and the text is visible, I would stick to that, because the other option is having nothing visible or just parts, which I hate even more. I recall several pages loading up with blank areas, and text later showing up. One in particular had bullet points, but no text content for a few seconds. Seems like what you hate even more is the current behaviour? I agree, the fallback font should be visible from the start and then replaced by the webfont once loaded in. Out of curiousity, what sort of ETA do you expect the current approach to take? **EDIT:** &gt; Could you please open an issue describing your font issues exactly? [Done](https://github.com/rust-lang/www.rust-lang.org/issues/784). Btw, some pages might have a page weight of 120KB, with 70KB of that being your webfonts, and another 20KB the tiny favicon. That's 75% of the payload.
Sorry, I missed your snippet link. Perhaps I'm not invoking the undefined behavior reliably here, but I can't seem to reproduce the issue in the playground: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=303b8c93d6a6e61d074b583c4b1dff58 If you attach a debugger are you able to get a call stack?
I've added an example of each component to the post. I would provide Playground links, but that doesn't work well for testing for reasons I explained in the edit.
I just did a few minutes ago
I wish we had Rust event in Iran.
The first step doing OOP with Rust is to forget OOP. :D . Go Actors + functional instead.
It looks like invoking UB requires operating over an FFI boundary. I posted some more snippets, think you could do some more testing locally with those?
TL;DR: the problem is not with generic types in the union, the problem is with generic types at the `match` - so it should be addressed there. I already wrote my opinion on the matter in https://www.reddit.com/r/rust/comments/9tpb77, but basically I think we should change the way we think about anonymous enums. I think the key is to have collapsing work recursively - so `(Q | (R | (S | T)))` will be the same as `(Q | R | S | T)`. This means that `fn foo&lt;S, T&gt;(x: (S | T))` could not be called without turbofish, because: ```rust let x: (u32 | f32) = 12; foo(x); ``` Can be either one of: ```rust foo::&lt;u32, f32&gt;(x); // or foo::&lt;f32, u32&gt;(x); ``` But it could also be: ```rust foo::&lt;(u32 | f32), !&gt;(x); // or foo::&lt;!, (u32 | f32)&gt;(x); ``` Or even: ```rust foo::&lt;(u32 | f32), u32&gt;(x); foo::&lt;(u32 | f32), f32&gt;(x); foo::&lt;u32, (u32 | f32)&gt;(x); foo::&lt;f32, (u32 | f32)&gt;(x); foo::&lt;(u32 | f32), (u32 | f32)&gt;(x) ``` And so on - so the problem is not just switching `S` and `T`, it's much bigger. Also, consider this: ```rust let x: (u32, f32, bool) = 3.14; match x { _: (u32 | f32) =&gt; println!("Something"), _: (f32 | bool) =&gt; println!("Something else"), } ``` What should be printed? The only sane thing to print here is a compilation error. But now consider this: ```rust fn foo&lt;S, T&gt;(x: (S | T)) { match x { _: S =&gt; println!("Something"), _: T =&gt; println!("Something else"), } } foo::&lt;(u32 | f32), (f32 | bool)&gt;(3.14); ``` What should be printed in this case? Is it still legal to make it a compilation error? Because of this inherent problem, I think it is meaningless to `match` against multiple generic types. I think we should "limit" ourselves to the following ways of treating anonymous enums: 1. Don't treat them as anonymous enums - just treat them as a single type based on some traits the compiler demended to be implement on them. 2. Match against concrete types. 3. _Head-tail_ (or _first-rest_) matching - match against **one** generic type, and have another arm for everything in the annoynmous enum that's not that type (that arm will be removed by the compiler in case there are no such types). 4. Use special syntax for generating match arms for each type at runtime. I suggested a syntax for that in https://www.reddit.com/r/rust/comments/9tpb77.
We can already do overloading with traits.
Finshir is a stress-tester which is trying to fill a whole thread pool of a web server, opening tens of thousands TCP-based connections simultaneously. The main feature is that it is protocol-agnostic, which means that you can test both raw TCP, HTTP, HTTPS and other protocols with ease. It is also coroutines-driven for performance reasons.
Mostly simplicity at first, but what I meant was that it's not in the main goal, but one tool could have less, same or more features of GNU coreutils. For example, the `groups` tool have one feature more than GNU one, `basedir` have the same features and are 100% compatible, while there is one, that I don't remember right now, that have one or two features less than GNU counterpart. One "mis-feature" (not really one), is that sometimes GNU tools have a "useless" parameter that "enable" features that are the default behaviour, which I, personally, find useless, just use without parameters.
Nice one! This bug ended up killing one of my side projects. I'll have to give your crate a try.
The segfault is caused by [this line][1]: *(obj as *mut #obj) = #obj::__new__::&lt;#ident&gt;(); The assignment operator drops the old value before assigning the new one. The old value in this case is uninitialized memory. You should change this to: std::ptr::write(obj as *mut #obj, #obj::__new__::&lt;#ident&gt;()); I have a working test case with this fix that seems to run the destructors correctly. [1]: https://gitlab.com/zack-emmert/abi_trait/blob/609bdd70a6cddf797ae5f878cfdc6a56052ec79d/abi_trait_macro/src/lib.rs#L223
Generally speaking flag/bug compatibility is pretty important when making coreutil/buildutil items. I learned this the hardway by making a `tar` clone that uses a different flag/subcommand structure. While a bit more verbose it declares what you are doing `car extract xz -f $myfile -d $mydir` but then replacing every instance of `tar -xJf $myfile -C $mydir` is a pain nobody was interested in.
I appreciate the transparency in salary. I'm currently employed but just wanted to say this. Good luck!
That's what I've been doing, but all those submodules and reexports are kind of a mess when I really just want to write a single large module.
Ok, figured out a potential solution: &amp;#x200B; `[[bin]]` `name = "experiment"` `path = "scratch/experiment.rs"` &amp;#x200B; `cargo run --bin experiment`
About that, that is a thing I thought about. And every short flag/parameter I maintain compatibility with GNU ones. Like, if there is a feature equal to GNU one, the short flag/parameter is guaranteed to be compatible
Considered that, decided "ugh". Thanks!
That, or you can stick it under `examples`.
&gt;One "mis-feature" (not really one), is that sometimes GNU tools have a "useless" parameter that "enable" features that are the default behaviour, which I, personally, find useless, just use without parameters. Often those parameters have a "last one wins", which I believe is so that aliases don't become less powerful. Consider a simple example of \`ls\`, which has no color by default: # .profile alias ls "command ls --color" When using this alias, you might want to get non-colored output, so you use the "useless" parameter: /home/Lucretiel/&gt; ls --no-color # translates to "command ls --color --no-color
Keep in mind actix is currently in the process of making some changes as they are working on the 1.0 release (no idea when that'll be ready). The docs on their main site are using the 0.7 version, while the examples on their github are 1.0
That's mostly due to Rust's unorthodox borrowing and lifetime system, as well as the combinators returning different types rather than the same type. If it weren't for that, there would be no practical difference. Maybe a few years of refinement will make closures and the type system powerful enough to support lowering arbitrary monads to and_then chains while still allowing everything you'd expect from a traditional functional language, but it would be a long ways away.
You sir are a life-saver. If I understand this correctly, assigning the mutable pointer tries to drop the old value as though it is a `#obj`, which segfaults because it's uninitialized? And `ptr::write` simply overwrites without running destructors.
Yes, exactly.
That's why I put it between quotes ;)
What is CEF?
I don't think any widespread programming language will emerge that expose the details of speculative execution because that feature is so hardware specific that exposing it (in my opinion) would cause the language to cease being generalized because it would rely on hardware specific features.
Wow I spent like a week staring at my Drop impl trying to figure out what was going wrong. I should've taken the hint when the debugger showed me that the segfault was happening before the object finished initializing. Well, perspective helps. And judging from your flair, so do experience and expertise.
You have stated the advantages of regex being a separate crate. But I want to say what you are missing out on. Since regex is a separate crate, it becomes a dependency. The common wisdom is that the less dependencies you have, the better. In addition, adding a dependency adds more friction compared to using something from std, that's just how it is. Together, this means that people avoid using regex when they otherwise would have. The threshold is higher. By not being in std, less people use regular expressions. Though when it comes to regular expressions, maybe a little friction is not so bad :) &gt; There's too much overhead to contributing to std std can depend on the regex crate. So regex can still be developed as it is now? &gt; Take a look at the regex package on PyPI Anyone's free to add it to their `requirements.txt` - why don't they? :)
&gt;These psuedo-trait objects work across an FFI-boundary (assuming both sides are written in Rust), are clonable, and (as far as I can tell) are memory-safe. *** Written in the same version of rustc, and compiled with the same version of rustc on a feature compatible processor model. Rust doesn't have a standard ABI, so between versions it is unreasonable to expect V-Table layout, or call semantics to remain identical. --- What platform are you testing you? Windows/OSX/Linux have different guarantees about how allocations are handled when you allocate _within_ a dynamically linked object, and when you de-allocate outside of that dynamically linked object.
In the most traditional definition, no portable language is low-level. In some more recent definitions, Rust's facilities for manual memory management make it a low-level language. The distinction is blurred and I'm not sure if it's massively useful.
That and it gives the API some flexibility to change the default. (Depending on how strict you want to be with backwards compatibility.) For scripts that want behavior equal to the default, they use the redundant flag, and then when the default changes they're ok. But if there's no such flag, there's no way for scripts to protect themselves, and changes to the default break everyone.
What's the difference between sauron and something like [https://github.com/David-OConnor/seed](https://github.com/David-OConnor/seed) ? I ask that because there are many similar projects in this area: [https://github.com/flosse/rust-web-framework-comparison#frontend-frameworks-wasm](https://github.com/flosse/rust-web-framework-comparison#frontend-frameworks-wasm)
Where do crates like sdl2, glutin, glfw, winit and minifb fit into all of this /u/icefoxen ?
I'm not too familiar with Specializations so hopefully someone that know more on that subject will respond, however I'm not sure I understand the problem of solving your problem like this: [https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2015&amp;gist=225a8dc75a00626c53b4204e1b038bb5](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2015&amp;gist=225a8dc75a00626c53b4204e1b038bb5)
I don't think the distinction is really even that useful, but Alan Perlis may have since he wrote this in his "Epigrams In Programming": &gt; A programming language is low level when its programs require attention to the irrelevant. I think parts of Rust qualify based on that definition.
I thought about it. Fortunately, I don't have enough time for it at the moment. I do hope to prepare a talk on this series when I'll be done. With some luck, I will be able to give the talk at meetups and conferences, and we will probably have a recording from one of those sessions.
By the author's definition (and I like that paper) there is no such thing as a low-level-language outside of assembly. Which the author's analysis deeply breaks down as modern assembly is re-encoded into micro-operations, and cached. Which resembles the execution flow of a Just-In-Time-Compiler (JIT) not really hardware assembly. &gt; I wonder, is there a language or is it even possible to have a language that takes into account modern paradigms such as speculative execution etc. or will there be forever a veil, that cannot be fully unlifted. Unlikely. Cache Coherency, and memory bus latency are not something you want to deal with. Even when trying to write the absolutely fastest code. You can optimize around them, but explicit control may lead to violations of the underlying hardware protocols, and cause one-or-more cores to get into illegal states.
You made a good point, so good that made me change my opinion, I may include those redundant flags now
Chromium Embedded Framework. I use it to render a webapp into an offscreen buffer I present as part of an UI on a game I've been working on.
Good observation. I agree that the problem is definitely within the match statement, but since union types are fairly useless without the ability to extract the underlying value, I think both are inherently linked. Another interesting case is a type that mixes generic and concrete types, for example, (u32 | T), and how those should be handled.
I could see three: \- Checking the the internet connectivity and warn if it's down on a specific line (if you have other equipment relying on this that might not warn you, some industrial equipment does not have a local line or mechanism to warn when they're down and rely on a 3. party to notice the lack of communication and warn you) \- A service that only rarely relies on an internet connection, but you would like to get a warning that it's offline so you can fix it immediately instead of getting an error at 3am :) \- Connectivity status for an internal dashboard or something similar
The same can be said of a lot of stuff that *is* being exposed, though; SIMD comes immediately to mind.
No events in Southeast Asia!
Imo, the most important thesis of this article is this: &gt; A low level language is one whose abstract machine maps cleanly to the underlying hardware. The case being made is not that C *isn't* a low level language, but that it's *no longer* a low level language because its virtual machine is no longer a (near) 1:1 mapping of the underlying hardware.
A somewhat similar project is mesabox, located at https://github.com/mesalock-linux/mesabox, although its aim is to provide an alternative to busybox.
Yeah, I can vouch for this. It's really good! Especially the examples used. I'm amazed about the amount of thought that has gone in to them.
A language is not low-level or high-level. I've found that low-level and high-level have become more and more difficult to define as languages evolved. Once upon a time, assembly was low-level because it forced you to reason at the machine while SQL was high-level because you described what you wanted and something would figure out how to achieve it. This was a typical control vs expressiveness switch. Nowadays, however, languages like C++ or Rust, which offer a great degree of control, also allow programming at a higher level of abstraction; see `&lt;range&gt;` and `Iterator`. And languages such as Haskell, based on mathematical definitions and computer science theorem, nonetheless feature pragmas to finely control strict vs lazy evaluation, vectorization, etc... As a result, while some languages may remain strictly low-level (assembly) or high-level (SQL, Python), many languages blur the lines and are *both* low-level and high-level... to a degree. --- Thus, to answer your question, I will rephrase it: **How low-level can Rust be?** Rust offers **perfect control over the memory layout of structs**. By default it reserves itself the right to lay out memory as it wishes, however you can take control at any point by simply dropping a `#[repr(C)]`. Rust offers **tight control over SIMD instructions**. When push comes to shove, and the auto-vectorizer mocks you, you can once again take control over your tight loop and specify exactly how to compute things using compiler intrinsics. Those are platform specific, so beware. Rust does NOT offer **perfect control over instructions**. There is not, yet, a possibility to specify specific sequences of instructions on stable Rust. This is desirable. Assembly is not THAT low-level either, for that matter. Once upon time, assembly would exactly describe what happens. That was a long time ago. Nowadays, out-of-order processors reign on consumer and server markets alike, speculation abounds, and we have little control over exactly what is executed. So there you have it, Rust is **two** levels removed from hardware execution: - The compiler (and optimizer) will not faithfully transcribe your Rust code to assembly in a 1-to-1 mapping. Instead it will be transformed: inlined, pre-computed, turned on its head. - The CPU will not faithfully execute the emitted assembly. Instead it will pre-fetch memory, speculate on branches, and re-order the execution of instructions. Note that both of those are governed by *semantic rules* which finely constrain what they can and cannot do, so that *observable* effects are preserved throughout the whole chain. The definition of *observable*, though, does not include number of cycles, number of cache/RAM accesses, number of inter-core messages, ... and of course, whether the content of caches is invalidated or not. These intentional omissions have led to advances in optimizations allowing software to run faster; unfortunately they also present an opportunity for side-channel attacks of varying danger.
Because I don't actually know the type of `Gen&lt;T&gt;`, I just want it to be able to give me a reference of B in one implementation, or a reference of A in another implementation. I'm asking for `Gen&lt;AsRef&lt;B&gt;&gt;` and etc. As I mentioned I believe I could use dynamic dispatch to allow for two different implementations, eg `Gen&lt;Box&lt;AsRef&lt;B&gt;&gt;&gt;`, but I'm trying to stick with static dispatch.
That's a fair point.
Are there any rust events in new england? i guess i could go to Rust Belt Rust
I am in Vietnam btw. Please let me know if you guys hiring any remote devs. Thanks.
&gt;2 Thanks for the feedback! It's my first crate. I selected an easy one to get comfortable with the environment :). I use this one in different Node.js projects, and it seems I'm not the unique. [https://github.com/sindresorhus/is-online](https://github.com/sindresorhus/is-online)
40 years ago C was a high level language. Enough said.
Huh. I find the article to be a bit wandering in its coverage of various issues affecting performance of the modern program-compiler-processor system. The last serious example I know of to expose much of the CPU to the compiler in order to improve efficiency was the [Itanium](https://en.wikipedia.org/wiki/Itanium) (AKA "Itanic") debacle. One of the key findings there was that there often just isn't enough information available at compile time to do a good job of planning execution. That leads to building fancy runtimes that effectively do a lot of the scheduling and execution work in software that a more traditional CPU would do in hardware. This is kind of an obvious performance loss. Maybe the problem is solvable â€” maybe it isn't. The jury is still out. My student and I got good results on Itanium with a fancy state-space search based scheduler: this approach is coming back into vogue. AMD processors apparently [manage to avoid](https://www.engadget.com/2019/05/20/amd-chips-immune-to-mds-vulnerabilities/) Spectre, Meltdown, and related security issues while still maintaining fast execution of sequential code. This is evidence that the Intel problems are just processor security bugs â€” I believe that Intel views them the same way. Hyperthreading, a shortcut designed to squeeze more performance out of each core rather than replicating cores, turns out to probably have been a security mistake: this has been well known [for a long time](https://geekz.co.uk/lovesraymond/archive/hyper-threading). In any case, the article's claim that &gt; The proposed fixes for Spectre and Meltdown impose significant performance penalties, largely offsetting the advances in microarchitecure in the past decade. seems really exaggerated. About 15% on typical non-hyperthreaded workloads, according to recent Phoronix measurements. &gt; There is a common myth in software development that parallel programming is hard. This would come as a surprise to Alan Kay, who was able to teach an actor-model language to young children, with which they wrote working programs with more than 200 threads. It comes as a surprise to Erlang programmers, who commonly write programs with thousands of parallel components. Writing programs with parallel components can be made easier than C or Rust running on a modern CPU makes it. As somebody who has an MS thesis in parallel runtimes for an exotic family of languages, I reject the premise that writing *efficient* parallel programs is easy. For anything but simple manipulations of huge quantities of data, efficient parallel algorithms are just harder to find than efficient sequential algorithms. The standard result for new parallel programmers is an algorithm that uses dozens of threads, each of which runs several dozen times slower than a good sequential algorithm would. Worse, for some fundamental problems â€” alpha-beta pruned game-tree search, many dynamic programming problems â€” the best known parallel algorithms require an exponential number of processors to improve on the performance of the best known sequential algorithms. Of course, not all problems are fundamentally algorithmic, and for data-intensive, IO-intensive or embarrassingly-parallel tasks it is important to provide easy access to parallelism. Rust is trying super-hard to do this in an efficient but usable way: the jury is still out on how well it will succeed. I already find it easier to write efficient parallel code in Rust than Erlang: in my [toy benchmark](https://github.com/BartMassey/ttt-bench) the sequential slowdown for Erlang is about 100x in any case, so I'm not too excited about it as a solution to performance problems. A lot of the article talks about defects in C and its specification, especially in modern versions. Much of what's described there doesn't really apply to Rust, I think. Undefined behavior is a huge performance problem in modern C, which is ironic since it was originally mostly put there to deal with processor disparities. I think the burden of proof is on those who advocate "higher-level languages with pared-down processors" as the solution to improving performance while retaining program correctness. It's crazy hard to do, because you have to alter all three parts of the program-compiler-hardware ecosystem at once. "C is not a low-level language" is one of those slogans that sounds cool, but in my humble opinion it doesn't really provide much of a prescription for accomplishing the difficult task of changing everything.
&gt; The features that led to these vulnerabilities, along with several others, were added to let C programmers continue to believe they were programming in a low-level language, when this hasn't been the case for decades. Excuse me what? This requires some citation. Speculative execution avoids processor stalling when we hit branches. This is entirely invisible in assembly as well, and has nothing to do with C. The same thing is true for data caches.
&gt; The common wisdom is that the less dependencies you have, the better. In addition, adding a dependency adds more friction compared to using something from std, that's just how it is. I agree. Which is why I'm constantly applying pressure to crates I depend on to keep their dependency graphs small. Here's my [most recent example](https://github.com/dguo/strsim-rs/issues/34). It's also why I want to provide a lighter weight regex crate going forward that compiles more quickly, but has slower runtime performance. &gt; std can depend on the regex crate. So regex can still be developed as it is now? No, it can't. Putting out a new release would be a pain. Today it's very easy because I personally own the full pipeline. I'd like to note that friction isn't the only consideration here. It's just part of it. If friction _were_ the only consideration, maybe it wouldn't be enough on its own to prevent regex from moving into std, so I don't see the point in squabbling over this. &gt; Anyone's free to add it to their `requirements.txt` - why don't they? :) I can't tell if this is a serious question or not. The `re` module in Python's standard library is often "good enough" for many use cases, especially when it's only tested on ASCII text. This is an argument _against_ putting regexes in the standard library, because you wind up with two completely distinct implementations where one is strictly inferior and rotting away.
Nebraskan, here, planning to go to the one in Colorado. Is anybody else planning to hit that one?
That's a good point.
With method #3 - _head-tail_ matching: ```rust let x: (u32 | T) = ... match x { num: u32 =&gt; println!("x is a u32"), other =&gt; println!("x is not a u32"), } ``` Note that even if `u32` is one of the types included in `T` - `other` will still not match it, because it does not match `T`, it matches everything that's not a `u32`. We could also go the other way around: ``` match x { t: T =&gt; println!("x is a T"), other =&gt; println!("x is not a T"), } ``` In this case `other` will only match if `x` is a `u32` - or never, if `T` happens to contain `u32`. But this case will probably be much less useful and we'll see it much less.
Why not just use a test? Even if you don't have a set of inputs/outputs that you're trying to verify, "smoke" tests where you just ensure that your code can indeed be used the way you intend can still be valuable to have. If there are other tests that you don't want to run while playing with this particular case, you can always use `cargo test &lt;name_of_test_fn&gt;` to just run that one test.
Don't think so - only configuration of some lints.
The cost is several more threads started automatically, behind the programmers back. Starting threads you haven't asked for is certainly not something a nicely behaved library does and it could be quite surprising. I believe this default is taken from jemalloc itself, not overriden by the wrapper crate.
You're welcome. I have become a fan of making my learning experience public whenever possible. Posts like these that other people make help me out, too; and even though I've been programming in Rust for (almost) a year at this point, there are frequent occasions when I still feel like a beginner. &amp;#x200B; The risk of potentially feeling like I may have asked a stupid question for everyone to see is a small price to pay for getting the help I need and for helping others who may have similar questions. &amp;#x200B; The Rust community has been really great throughout my journey, and I'm happy to participate in it.
I'm not an expert on that front. You could try asking in an appropriate *.rust-lang.org forum but that's more a generalized language design question than a Rust question and you're still likely to run into the problem of people feeling that they've already burned enough time discussing this. I suspect it's already been addressed in the reams of discussion that went on surrounding the `await` syntax and there's already a history of people coming in and proposing syntaxes that they already considered in great detail and rejected. That said, what I can say is this: 1. By design, Rust has a high barrier for new syntax constructs and other aspects of the core language to try to stave off the march toward C++-like difficulty to master. 2. Rust's design takes certain lessons from Python and "There should be one-- and preferably only one --obvious way to do it." is one of those lessons. 3. By design, Rust is not an adventurous language. That's left up to languages like Haskell. 4. Rust is reluctant to add new sigils (operators made only of punctuation) because learners can't google them. (eg. The devs try to stick to what people already know from other languages when possible. The *current* `await` syntax had to be justified for being a break from what other languages use and your proposed `-&gt;` would spread that alienness to the language as a whole.) My perception is that the behaviour of your proposed `-&gt;` is going to introduce confusion in more complex examples because weak binding is a tricky thing to get right and make intuitive enough for the user to feel they've grasped the rules for. At the same time, it feels like a case of "I *really* don't want to accept this await syntax. Can't we change the core language instead. I'm sure we'll find another use for it if we keep looking." (And that's not the Rust way. It's the job of the person proposing a feature to justify its necessity.)
Implementing the Lockstep core of a complex Space game using Laminar, Serde and Specs. I love how I can define an enum for message types, where each enum value "holds" the relevant message struct. I can then pass that enum around, store it in vectors, serialize it and so on. Right this minute, I'm struggling with synchronization of ticks and state.
just depends how you want to define low level language!
A couple of easier ways that using custom \`bin\` entries in your \`Cargo.toml\` might be: * Add a file to `src/bin/` -- Cargo automatically picks this up and obviates the need to write a manual TOML entry. * Use a [`cargo-script`](https://github.com/DanielKeep/cargo-script) file -- this lets things be really self-contained. * Use [an entry in the `examples/` folder](https://doc.rust-lang.org/cargo/commands/cargo-run.html#cargo_run_target_selection).
Good amount of holiday but that seems a little low salary for London.
this requires lattice specialization, with a third impl covering the overlap - an impl providing what to do when a type implements both AsRef&lt;A&gt; and AsRef&lt;B&gt;.
I'm confused - can you explain this lattice specialization? [Here](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2015&amp;gist=3a3b6a968f165c61a962e36430e86f61)'s the 3rd impl, fwiw - though I suspect that's not what you meant
Hi, I'm surprised I just found out about this just as you posted this link. I wish the author posted more about it. The project has been around longer as well. Just scanning their example code, it looks like we have more or less the same style and goal. Both project design is heavily taken from elm. Seed syntax relies heavily on macros(which has an advantage for flexible optimizations), while sauron are just plain function calls. The big advantage of using plain function calls, is that the compiler is accurate in telling you which line you may have missed to pair the parenthesis and braces. Another advantage is that the formatter `cargo fmt` can do very well with it, and does a pretty good job aligning the blocks like in a tree structure, [example](https://twitter.com/ivanceras/status/1130482961982169093). I created sauron for me to primarily translate my existing elm code without too much visual changes. Greatest strength sauron has it it's core vdom which is designed to be generic as possible and didn't assume to be used only for html. You can also build server side view using sauron by calling `app.view().to_string()` and serve it in the server. I have some experimental code which uses sauron vdom to apply the node tree diffing in native controls and text-ui widgets, and it looks promising.
AFAIK, that is exactly, what it is.
Yes, it doesn't measure how long the allocations themselves take; it only saves *when* the allocations are made. In general I find that if you want to measure *performance* you are probably better off just using a normal CPU profiler which should tell you how long you're spending allocating memory (that is - *if* the allocations are indeed a bottleneck).
The other exampleâ€“ though this might actually be consistent with your point about languages ceasing to deserve the term "generalist"â€“ is GPU programming. GPU languages (like OpenGL Shading Langauge and OpenCL) don't assume a "sequential instruction, flat memory space" model; they instead operate on a virtual machine that is much more similar to the underlying GPU hardware.
Indeed, and this is not because C has changed (it has not) but because the underlying hardware has.
But it still fails to build - hence my confusion haha.
Even so, it was lower-level than many of its contemporaneous languages, and even more venerable ones such as FORTRAN and COBOL. Goes to show that it's all relative.
I often write unit tests for this sort of thing, just abusing the test env to run my little one-offs. Sometimes, they even stick around as actual tests.
this extension to specialization has not been implemented has not been implemented
Yea, this seemed weird to me as well. I can't imagine there is any feature that was added to C which enabled speculative execution bugs. Maybe I'm wrong about that... but I do know enough to understand it is a hardware bug, and even if you hand-write your code in assembly, you are STILL going to be vulnerable to hardware attacks. It would be *great* if everyone could just disable a feature of C and all these spectre-type bugs just go away, but I don't think that is even possible.
Do you need slices or an `Iterator&lt;Item=u8&gt;`? If the latter, you can use `a.iter().chain(b)` and accept an `impl IntoIterator&lt;Item=u8&gt;` instead of the concrete slice, `&amp;[u8]`.
Ah hah, thank you for clarifying. Good to know at least! [Link for anyone curious](https://github.com/rust-lang/rfcs/blob/master/text/1210-impl-specialization.md#the-lattice-rule)
If you take a peek under the hood at how all this works, you'll find quite a bit of shenanigans with extern "C"` function pointers, and you'll also find that when I say FFI-safe, I mean it. The vtable is a struct with C representation, so I would expect it to remain stable across different compiler versions. As for testing, I'll admit I've only thoroughly tested on Linux, because that's what I have available. If you'd like to contribute, I'd appreciate it if you could run some tests on a Mac (once I've pushed my latest changes, of course), as I don't have any Apple products available. Or a less popular operating system such as a BSD variant.
Could you explain how? I've tried in a simple way but failed. struct O {} trait Overloaded1 { fn overloaded(&amp;self, _: i32); fn o(_: i32); } impl Overloaded1 for O { fn overloaded(&amp;self, _: i32) { println!("2"); } fn o(_: i32) { println!("2"); } } trait Overloaded2 { fn overloaded(&amp;self, _: String); fn o(_: String); } impl Overloaded2 for O { fn overloaded(&amp;self, _: String) { println!("3"); } fn o(_: String) { println!("3"); } } fn main() { let i = 1; let s = String::new(); let o = O{}; o.overloaded(i); // error o.overloaded(s.clone()); // error O::o(i); // error O::o(s); // error }
SIMD is still higher-level than the place where things like speculative execution and the cache hierarchy exist. Only things like prefetch instructions and memory barriers interact directly with those layers, and even then there are many imaginable implementations, depending on the microarchitecture. A prefetch is only a hint, no guarantees, and the implementation of a memory barrier would depend entirely on the precise configuration of the memory bus. RISC-esque architectures advocated for the opposite approach back in the day, and mostly lost out to CISC-esque architectures such as x86, arguably because the more high-level instruction format allowed the CPU to make better decisions in the moment than an ahead-of-time compiler. Facetiously, you can compare modern CPUs to JIT-compilers and not be entirely wrong. :-)
Would making them warn by default still be considered backwards incompatible?
You don't need two traits - only one, which you `impl` twice: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=df2a3353dd9371e5416587594db1b55e
I know of several decent crates out there with only 1 or 2 contributors, and even some large projects have changed licenses in the past. (QT is notorious for switching to a more restrictive licensing model, for example) It isn't a hypothetical, there is a lot of movement back and forth between copyfree/copyleft/copyright in the OSS world.
\&gt; **Is Rust really a low level language?** Yes. TOTALLY. &amp;#x200B; But you need to understand that "low" and "high" requiere something to compare against. ie: Rust is low-level language in contrast to Python, but is high-level against assembler. &amp;#x200B; Is like say "X is fast". Faster, then what? A turtle is fast... against a snail. &amp;#x200B; \---- &amp;#x200B; However in general terms, without much qualifications Rust, C, C++, Pascal are part of the "low-level" languages, because MOST development is not system development, ie, is higher-level. &amp;#x200B; What the paper explain is the same point about "relative to..." because some developers think in more absolute terms, and can't imagine that you could get more low-level or that you could do low-level development in higher-than-C language. And also, that with the complexity of modern CPUs, the idea that anyone can actually do "low-level" AT THE FLOOR OF THE CPU is not possible anymore... (ie: some actually believe that using C mean can work at the level of the CPU).
Not that I know of. All we've got is the Boston Rust meetup I think. I did make the drive out to Columbus, OH for RBR in 2017. Indy is just a it too far for me.
Isnt SIMD more of a concept? Followed by hardware standards implementing that concept, which is a bit more general/portable/abstracted than speculative execution, an implementation detail of the cpu design? And those hardware standards being mostly a compiler/optimization implementation detail the programmer doesnt need to know?(Dont quote me on this though)
aye Rust Dev here, not looking though
Definitely looking forward to Oxidize 2! :-)
Unless you work in finance from what I've seen it's about normal
Hey,you might want to look at my [abi\_stable](https://crates.io/crates/abi_stable) crate for defining Rust types that have a stable layout.
What does the role actually entail? What is the company? Where is it based?
Finishing up the 0.4 release of [abi\_stable,](https://github.com/rodrimati1992/abi_stable_crates) With the addition of ffi-safe wrappers of crossbeam channels(without select)/Mutex/RwLock/Once,some changes to how libraries are loaded,and a minor support for reflection.
Was there ever consensus that it is something that will get added?
I reviewed the code. This is why I say what I've said. You're just relying on an external library to handle all the `dlopen`, `dlsym`, and `dlinfo` operations into another library, and it is just exposing raw pointers to you, while not managing the os specific details of _how_ those calls and allocations are managed, as those details are unsafe the side effects maybe unsafe, which, maybe the result of your memory leak.
Qt is special - it's basically commercial project and requires https://www.qt.io/legal-contribution-agreement-qt . Sure. A smaller project with not many contributors could do it. But then, being a "battery" crate means bigger attention and responsibility. Anyone would be able to fork and maintain it under the old license, making it rather pointless. I think it is a hypothetical, not a practical problem.
You want to have the user pass you a &amp;mut to a [https://doc.rust-lang.org/std/io/trait.Read.html](https://doc.rust-lang.org/std/io/trait.Read.html) (std::io::read) In your buffers case, they can use [https://doc.rust-lang.org/std/io/trait.Read.html#method.chain](https://doc.rust-lang.org/std/io/trait.Read.html#method.chain) to collect together all the bodies of multiple messages into one reader (Read is implemented for &amp;\[u8\]).
Me too! (Detroit ish)
What I've heard is that it's typically better for power consumption to finish the computation ASAP so you can put the CPU to sleep sooner.
TypeScript is very good.
One excellent feature that GNU lacks is a `cp` progress bar option that works as you'd expect (maybe similar to `wget`'s), except when the source file is infinitely long (`/dev/zero`, for example), the progress is determined by how much of the destination has been filled, which is a cool feature that `pv` has.
Reading this book right now, thanks! For those who bought it a long time ago and almost forgot about it and even missing emails from the publisher (that was me!), it's updated recently on 2019-05-15
&gt; but then replacing every instance of `tar -xJf $myfile -C $mydir` is a pain nobody was interested in. Personally, i would be. I really hate the trend of "backwards compatibility." Sure, backwards compatibility is nice, but not for 40 damn years. Not when it means you can never improve, or even fix bugs because they're now relied upon. Not when everything is so interconnected and interdependent on specific versions and bugs that you're locked into them forever. For a OS preaching choice and user customization it sure locks you in a lot.. Imagine, if you will, a world where you can install two *different* versions of a program on linux, without having to [resort to weird hacks](http://gcc.gnu.org/faq.html#multiple). Imagine not having a global system-wide directory for installing all software, for all users, making upgrades downright dangerous. Imagine something sane, a folder for programs, and each program in its own folder. Hell, another folder for versions! Imagine software installation as simple as moving into `/Programs/$Name/$Version` or `/Users/$User/Programs/$Name/$Version`, dependency management as simple as one little file saying "I depend on $Version of $Name", build systems as simple as adding those directories to the PATH(You dont even have to rewrite the build systems! Backwards compatibility!) [Also imagine static linking](http://harmful.cat-v.org/software/dynamic-linking/) &gt; Basically Unix is an extreme mess, and we're stuck with it. :(
Speculative execution is also a concept with hardware standards implementing that concept.
&gt; Imagine, if you will, a world where you can install two *different* versions of a program on linux, without having to [resort to weird hacks](http://gcc.gnu.org/faq.html#multiple). Imagine not having a global system-wide directory for installing all software, for all users, making upgrades downright dangerous. Check out https://nixos.org.
&gt;The CPU will not faithfully execute the emitted assembly. Instead it will pre-fetch memory, speculate on branches, and re-order the execution of instructions. It will also not allow direct access to the hardware gates, this is implemented via microcode, its own interpreter sitting below the assembly level. That being said, assembly is the lowest level a programmer can interact with, so we can call it low-level for that reason alone.
This is really cool, but since you're not compatible with GNU, how do I install these binaries for my own use without breaking GNU tools? For example, a while ago I installed uutils to `~/.cargo/bin` which is on my `PATH` but since they don't support `cp -a` I couldn't run autotools configure scripts. Does anyone have a fix for this?
I've tried it on occasion, but it's too different/odd for my tastes. There is something more my style, [GoboLinux](https://gobolinux.org/), but it's a tad dead.
Not sure how to solve that with Uutils, but if -a flag is usefull, my coreutils cp will probably have it.
Your code is very clean for the case of one argument: trait Overloaded { fn overloaded(self); } impl Overloaded for i32 { fn overloaded(self) { println!("1"); } } impl Overloaded for String { fn overloaded(self) { println!("2"); } } fn overloaded&lt;T: Overloaded&gt;(value: T) { value.overloaded(); } fn main() { let i = 1; let s = String::new(); overloaded(i); overloaded(s); } Has this technique been extended to functions with multiple arguments or differing number of arguments (`overloaded(a)` vs `overloaded(a, b)`)?
You can use crates `error-chain` or `failure` to manage errors.
That's what enums are for. Use them like this: ``` enum MyError { E1(Error1), E2(Error2), E3(Error3), } // Impls of From trait so that you can use your type with the ? operator. impl From&lt;Error1&gt; for MyError { fn from(err: Error1) -&gt; MyError { MyError::E1(err) } } impl From&lt;Error2&gt; for MyError { fn from(err: Error2) -&gt; MyError { MyError::E2(err) } } impl From&lt;Error3&gt; for MyError { fn from(err: Error3) -&gt; MyError { MyError::E3(err) } } ```
Nice work, interesting.
&gt; hardware standards implementing that concept There are standards, cross-processor, for speculative execution and it's implementation? It's my understanding that SSE for example will work the same regardless of processor, if it implements the standard, whereas speculative execution is a concept, sure, but it's implemented however cpu makers want?
are you using these imports in tests? If so, tests don't compile at all when you use `cargo build` so the compiler is right, `json` isn't being used. But then when you run tests you do use them, so it errors. Simply put `#[cfg(test)]` before your import.
Nope, they're used in the real code. I'm not even running `cargo test` in this case, the errors are on the build.
That's strange, can you make a reproducible example? Or can we see the code?
Not that I am aware of, I think it was proposed but no consensus was made about it, so it got postponed to another RFC to work it out after specialization lands.
Is there no macro that could write all this out automatically? It seems like you should be able to just say \`enum\_errors! MyError {Error1, Error2, Error3};\` and get all that.
A macro's scope is decided earlier than other imports so you need to tell the compiler what you're doing. Put a ```#[macro_use]``` directive in your crate before trying to import macros.
&gt;error-chain Thanks for the code sample, I initially thought about enum's but wasn't aware of the From trait which deterred me from looking into it
Cool! I'll look into those crates too to see if they make my life any easier
How can one use features for conditional benchmark compilation? I have a feature that disables a chunk of my code, and I want to conditionally disabled the same chunk of code in my benchmark.
Differing number of arguments is currently impossible in Rust, but multiple arguments is easy - just use a tuple: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=bcecccc384d229b4f25c723afbc17451
 pub struct Passwd { name: String, Make `OsString`'s using the Unix [extension traits](https://doc.rust-lang.org/std/os/unix/ffi/trait.OsStrExt.html). Usernames, directories etc are not UTF-8 strings.
&gt;Windows/OSX/Linux have different guarantees about how allocations are handled when you allocate within a dynamically linked object, and when you de-allocate outside of that dynamically linked object (or in a different dynamically linked object). Oh,so my handling of allocations within abi\_stable(by using virtual dispatch)is necessary then?. What I do in abi\_stable is that anything that allocates and deallocates has a vtable with a deallocate function(including RVec,RBox,RArc,etc) that deallocates the value when dropped.
Had an absolute blast meeting all of you folks, thanks a ton for organizing it; I will definitely revisit.
The snafu crate will do it.
My blog post on error handling might help: https://blog.burntsushi.net/rust-error-handling/
Tests produce too much noise in their output, run unrelated code(if you have actual tests), and on top of that won't show any of your output anyway if the test passes, and throwaway code often prints.
Also might be worth looking at: https://github.com/rust-lang-nursery/error-chain
you gotta indent with 4 spaces ``` doesn't work(as far as I know) so it gets this style
If you have binary/lib targets that use that library, they'll also need to be marked `no_std` otherwise they'll pull in the standard library.
I like the [failure](https://github.com/rust-lang-nursery/failure) crate for handling this. It simplifies both Boxed errors and enum errors.
I made sure to convert to common Rust types so would be easier to manipulating it without loading more thing to the tools. There a good reason to not converting it? Thanks for the reply :)
you're overwritting core::alloc?
Triple backticks work in "New Reddit" but not in "Old Reddit" and various apps. (Yes, they're using different Markdown parsers in the two different sides of the site itself.)
Yeah, but then we built a shit load of stuff on top of it, leaving it far closer to the bottom than the top. What are you even trying to say?
No,what I am doing is defining an `extern fn` to deallocate,delegating the deallocation to whatever the allocator is in the library where the value is instantiated. Ie:If I construct an [RVec&lt;\_&gt;](https://docs.rs/abi_stable/0.3.1/abi_stable/std_types/struct.RVec.html) in a dynamic library,and drop it in the binary/another library,it will use the function in the dynamic library that constructed the `RVec&lt;_&gt;` to deallocate it`.` If you look into it's source code you'll see that it uses a vtable constructed along with the value.
&gt; [On Unix systems](https://doc.rust-lang.org/std/ffi/struct.OsString.html), strings are often *arbitrary sequences* of non-zero bytes, in many cases interpreted as UTF-8. That you're calling `to_string_lossy()` should be a good hint - you're *explicitly* creating a path through which data will be corrupted during load because it's not valid UTF-8. Which is a somewhat odd expectation to be made of a platform that dates from several decades before UTF-8 existed :)
I wont be that slayer again. The website has clearly been poorly-designed and kept systematically broken for so long that I've simply given up on the website. The way maintainers treated (or mistreated) feedback, closed down discussions, and shot down genuine contribution attempts (that were completely adhered to the design and style constraints of the new site) left a very bitter aftertaste in my mouth. I bet it's the same for many others. If you want slayers, you'll have to replace the bridge that was burnt.
That post was one of the early things that got me excited about rust. Thanks for writing it!
The member (also standalone) doesn't depends on parent module, also \`cargo nono check --package &lt;XXX&gt;\` returned all success, however, the toplevel (parent) module does depends on \`std\`, not sure if that pull the \`std\` dependencies for the \`member\` module (\`no\_std\`).
You seem to be talking about two different kinds of implementation. If you mean what's implemented in the machine code specification, then, yes, all SSE-supporting chips will implement it. Similarly, all IA-64 processors will have ways to tell the processor which branch is most likely and so on to help its speculative execution function more effectively: &gt;[Additional instructions for speculative loads and hints for branches and cache are difficult to generate optimally, even with modern compilers.](https://en.wikipedia.org/wiki/IA-64#Instruction_execution) If you mean they implement it the same way in hardware across chips, that doesn't apply to SSE or to speculative execution control in IA-64. For example, some chips might have one particular SSE operation take a certain number of clock cycles and another might implement it so that it takes a different number of clock cycles. There can be other variations as well.
[removed]
By the author's definitions, assembly is not low-level. Therefore, I actually wonder the point the author wants to have. Specifically the parts about a supposed partial C guilt in Spectre, and the "non-C" processor trip is weird. Processors are not *that* much connected to C already, especially in the way they address the performance needs. And different other approaches have been tried, in more exotic directions, and basically failed (e.g. Itanium, Cell, explicit handling of caches and/or too depth memory hierarchy). So only thing different that remain and is consistently useful and in extremely broad use are GPGPU, but even if called GP there are not actually suitable for GP loads best addressed by CPUs, merely for math compute kernels. So where is the actual dep to "C" for CPUs from a technical point of view? The sequential model of execution; BUT that is actually shared by tons of languages; actually most of what the author cited as drawback (or impact on proc) because of C also exist in most other programming languages. The thing about restrict and object aliasing really has an impact on compilers, but mostly not on processors. In a nutshell the OOO (for high perf) +kind of JIT engine (for pretty much everybody now) with cache coherency (also: everybody) that hides its internals is an *extremely good* way to design and evolve processors and simultaneously have multiple models for various needs and a stable ISA, that will be extremely hard to beat, and that is quite lightly linked to C. Actually, it also serves pretty well most modern languages that appeared far after C; native or not; low-level or not (whatever that means). Now the *only* question that remains is: do we want more parallelism. And the answer is yes. *BUT* it can be done gradually using the current approach, that is not incompatible with "C" at all (although some other languages are better to handle the risks it can introduce / organize / structure programs better, etc.) -- actually extremely fruitful parallelizations have been done using C derivatives on GPU.
At the end of the day, Rust is significantly lower-level than, say, C#. They are categorically different. That difference is described as high- vs low-level. Regardless of his points, the practical application of distinguishing between the two is valid. It just sounds like it's not low-level *enough* for him. Honestly, it sounds like he's gatekeeping low-level languages, which seems odd to me.
You can always `Box` the error (which returns a pointer to the error), so the size can be dynamically determined. Adds additional overhead, but is a major improvement in terms of flexibility.
By the way, you can just have files in a `bin/` dir without needing to edit your `Cargo.toml`. `cargo run --bin scratch` runs `bin/scratch.rs` if it exists.
Good point, but if I don't use lossy version and handle the Result, would be ok to convert, or would be better to receive as OsString and after that convert to String?
Are you using `edition = "2018"`?
Sounds like you maybe want something similar to .NET `System.IO.Pipelines`: https://devblogs.microsoft.com/dotnet/system-io-pipelines-high-performance-io-in-net/ But there's no magic there. Just a lot of bookkeeping under the covers. I'm not sure how well the general idea would translate into Rust.
RustConf last year was an incredible experience! Highly recommend 10/10
While not a higher-level language, the [IA-64](https://en.wikipedia.org/wiki/IA-64) instruction set does just that.
&gt; The website has clearly been poorly-designed and kept systematically broken for so long that I've simply given up on it. Can you elaborate a bit more about that(or link to the discussions detailing those issues)? I recall quite a few liked the prior site more, what was the reasoning for replacing it, do you know?
As a general rule of thumb for the same amount of computation to be done on the same core, with clock speed being the main knob to turn, that often holds (but even there, not always - for example I have a system I work on where for various reasons the most efficient run speed is actually 1/5 its maximum, so unless it is doing work that it canâ€™t keep up with at the lower rate, it always stays at that rate). It doesnâ€™t automatically apply for moving work to different cores or drastically changing the way the work is structured. From the (probably oversimplified) description linked, it sounds like WebRenderâ€™s approach would involve a much larger amount of computation overall for a lot of cases.
The issue isn't whether the flag is _useful_ it's if any existing software relies on a flag existing. Which some probably does, in which case you're bound to at least the GNU features.
You have to start from a completely different point. If I start an OOP project I start with a few classes. Write some getters, setters, some unit tests. I end up writing functions in Rust first. I operate on data I don't even have yet. At this point there are no tests. My functions are so simple they obviously work if they compile. Then I write integration tests between all the components. I don't like fragile tests. I don't like ifs or booleans. I try to match only when necessary (hopefully just once). I like chaining combinators over looping. It's kind of a post-functional style. Functional, but with safe mutation.
This is a prototype cellular automata climate simulation processor for games that i'm making. it works with base parameters: altitude, temperature, air humidity and surface water - the missing one that i'm implementing next is sea currents and winds to produce climate.
Interesting! &amp;#x200B; From the post, this library looks very similar to Twisted or asyncio in Python. Is that indeed the case? &amp;#x200B; Twisted and asyncio are both conceptually pretty simple, in my opinion. The problem is, once you start to do anything more involved, the complexity spikes through the roof. Most significantly, implementing back-pressure or cancellation is extremely hard and many Twisted / asyncio libraries just punt on it and use unlimited buffers with the hope that memory doesn't run out. This contrasts with something like Rust's futures library, which may have a much higher initial complexity, but where I feel like the difficulty curve is much gentler as the interfaces compose much better than the Twisted / asyncio ones do. So, I'd be interested in hearing more about how things like back-pressure and cancellation will work with this library and your thoughts on how composable its interfaces are.
Hey, you might want to post this on r/playRust. This sub is for the programming language of the same name. We'd love to have you if you're interested! Either way, hope you find a solution to your issue.
Did you check the memory usage? What sort of script were you running? This is very general question.. Could be anything. No way to reproduce..
I use error chain but some feel that it is rather boilerplate-y compared to failure, which I've not used. error-chain is pretty good though.
One thing to note: It is better to think of `&amp;T` as a *shared* reference rather than a *read* reference. Similarly it is better to think of `&amp;mut T` as a *unique* reference rather than a *write* reference. So what you want is a data-structure that is shared and mutable. Well, that's what interior mutability is for. You can use interior mutability with `Cell`, `RefCell`, `RwLock`, `Mutex` or something similar to shift the checks to runtime, but that's as good as it gets. You can then build up data-structures from there. There are already data-structures like this on crates.io, just search for persistent data structures.
It depends on the context. Does the data need to remain intact end-to-end? Is a script going to read the output and plug it into something else? Then the right thing to do is probably to keep it as an OsString and pass through the bytes unchanged - that is, after all, what coreutils will be doing. Or if that's too fiddly, yeah, handle the `Result` and error out rather than pass through corrupt data. A good example is `find(1)`, which is commonly used in shell pipelines. It'd be natural to `println!("{}", path.display())` each result, but really it should be using `as_bytes()` and writing those out directly, string encoding be damned - filenames are *bytes*. And if you can't do that, then yeah, better printing an error than a mangled filename.
I just checked out Rust for the first time. Are named/labeled arguments something that could come to Rust?
Does this handle "thinks it's in a workspace, but it isn't"? (i.e. does adding a `[workspace]` table to `example/bin-crate/Cargo.toml` break it?) If this weren't constrained by the need for zero dependencies, this would be amazing for another unrelated problem: bootstrapping! Here's Pest's example: we have our package, `pest`, which requires `pest` to build. We ship the prebuilt file on crates-io to avoid the cyclic dependency. For local development, we have the `pest-bootstrap` package which depends on the crates-io version of Pest and generates the output source file for `pest` to build. When building locally, we want to use `pest-bootstrap` so that we don't have to check in the generated source file. The problem with this is that the buildscript cannot use `cargo run --package pest-bootstrap` as that attempts to recursively lock the target directory. If we do a similar trick to you and build it in a temp directory, this shouldn't happen. (We currently do a very fragile "dynamic link" that relies on the `pest-bootstrap` crate having been build previously.)
This was my biggest sticking point through the paper and I was beginning to wonder if I had misunderstood something. By this definition, there are no low level languages useful on more than one machine, which is kind of stupid.
You could be running into this: [https://github.com/rust-lang/cargo/issues/5730](https://github.com/rust-lang/cargo/issues/5730) (Depending on how your build-dependencies look like)
There was a nice discussion two weeks ago that revolved around the design and (mis)handling of the site here: https://www.reddit.com/r/rust/comments/blkh9i/halfway_into_2019_already_and_has_the_promised/ Many of that thread's comments link to external discussions and issues that better illustrate some of the problems that plague the site and the excuses being made to justify postpone or outright deny fixing them. That thread is by no means exhaustive list of all issues, but it is a good representation of the nature of the problems with the site.
&gt; Imagine, if you will, a world where you can install two different versions of a program on linux Nixos has already been mentioned but it's not a general solution. A general solution is to write your program and name it differently, then make it behave like something standard if it's called with an appropriate `argv[0]`. Ripgrep has no use for the `grep` name, `rg` is just fine, and we can have both modern features and non-insane cli as well as backwards compatibility without installing each and every program twice. That said, in those cases I'd argue that the `grep` feature set should be "POSIX me harder": Provide exactly what's in POSIX, not more, not less. Something something people expecting `/bin/sh` to be `bash`, that's not even true on Linux (hello, Debian and Ubuntu), much less BSDs or Illumos or whatever arcanae exist out there. And, as it has been mentioned: According to POSIX, `groups` does not exist. Oh, and `sccs` is *still* part of the spec. Maybe it's time for a new one...
*Even `dd` can do that*.
I hate it when linux asks for a user/password tuple in the commandline, that if your computer is really slow, there is a danger of you typing your password in the clear after you type the username but before the password is requested. That's just... incredibly bad. Fix that plz.
Lol dude ya missed it.
File a bug against autohell, [`-a` is not portable](https://pubs.opengroup.org/onlinepubs/9699919799.2018edition/utilities/cp.html).
That's how you get scripts that only run on GNU. The world is larger than that.
Thanks for that! :) I know I got shut down with my advice to consider leveraging the existing web ecosystem, but I'm going to port some of the existing site over anyhow to further illustrate why it's worth really thinking about. I understand the desire to use rust for the site itself and for contributors, but looking over the source repo, that point seems a bit moot, especially with all the extra work that needs to be done just to avoid adopting a non-rust stack. Many of the issues are simple fixes that I could take care of. If I'm able to make a good case for it, it could be adopted and the maintainers can RIIR if they really insist, while providing a better experience in the mean-time until they can reach parity.
I think I struggle with just functions. In the oop world objects are about responsibilities so I naturally group functionality and data together. But that works terribly in rust. I guess what I would love to see is a good on how to translate oop code into good rust code if that makes sense.
Well, makes sense, for now, all the utilities done, so far worked fine with pipelines, at least with grep and riggrep. But your argument is great, handle the conversion was already on my plans, but I may change that to OSString if I find (or anyone else finds) any problems with that
I don't agree with this line of thinking. For example, if a tool is made with BSDs flags/features in mind, should GNU be bound to ar leas the BSDs features? I think that it's a decision of the developer to support or require a specific implementation of the tool or support many.
Aw shit again. Change this language name to Rusty or Rusto
I am currently trying to build my first web app using Actix-Web. Looking at the docs I feel pretty frustrated. I don't know what to look for and where to look for stuffs. There are so many trait and generic jumping hoops there. I tried to look at the source code and feel hopeless as well. I can't wrap my head around it. &amp;#x200B; Any tips?
&gt; Make the right decisions about mixed types, e.g. the average over integrals is not integral, it's floating point Would you consider relaxing this requirement? I suspect that this isn't actually a valid assumption for most use cases. Instead, I'd assume the output type should be the same as the input, and require converting on the way in. E.g. if I'm asking for the average size of MP3 files in my music collection, I almost certainly don't want to receive a fractional number of bytes. Furthermore, there is no "globally obvious" output floating point type. Sometimes I'll want more precision (`f64`), sometimes I'll want less (`f32`). Sometimes I might even expect [`f16b`](https://github.com/rust-lang/rfcs/pull/2690)!
&gt;Would you consider relaxing this requirement? I suspect that this isn't actually a valid assumption for most use cases. &gt;Instead, I'd assume the output type should be the same as the input, and require converting on the way in. Most definitely not! If the input is a built-in integral type, summation should be carried in i64 (if signed) or u64 (if unsigned) - one cycle per addition. (You don't want to compute the sum of bytes as a byte; you want a 64-bit integral.) Then division should be done in f64. Then sure, if floating point addition is what's needed, it's easy to convert the stream of integral to a stream of floating-point numbers on the fly. So the function supports both scenarios. The larger point here is that there needs to be a certain amount of smarts the function needs to do in order to choose the right types for the summation and division. It's not a trivial matter, which makes the problem interesting. &gt; E.g. if I'm asking for the average size of MP3 files in my music collection, I almost certainly don't want to receive a fractional number of bytes. What if you compute in megabytes? :o) On a serious note, the general going is you don't care about fractions when the inputs are assumed to be numerous and large, which only applies to a fraction of the cases. &gt;Furthermore, there is no "globally obvious" output floating point type. Sometimes I'll want more precision (f64), sometimes I'll want less (f32). Sometimes I might even expect f16b! Good point. I think f64 is an obvious default - about as fast as f32 on modern hardware. f16b is exotic.
I feel like cargo/crates need to support the concept of a shipping manifest. A meta package that's just a pointer to a fixed set of other projects/revisions. I know you can just make an empty crate to do this but that feels wasteful somehow.
It's more than that; I'd argue there are no practical programming languages that avoid the creation of the features behind spectre/meltdown. These are inherently necessary for anything that has control flow (which admittedly not every possible language has, but certainly every language that fully exposes the hardware details of basically any processor). I'd take that there are no low level languages that are useful on more than one machine. It's fine to define "low level" as the actual CPU-specific machine code, even if few people ever use it. The "level" of a language seems to mostly be relevant for bragging rights anyway. I won't take that using high level languages is somehow the cause of hardware vulnerabilities, though.
&gt; If the input is a built-in integral type, summation should be carried in i64 (if signed) or u64 (if unsigned) - one cycle per addition. Why not `i128` or `u128`? And what if my sum doesn't _fit_ in a `u64` but another mean algorithm would handle the challenge just fine â€” maybe I would assume that a super-generic mean function would handle this just fine. And then this "most average function there is" would have let me down. I think you're trying to define an overly clever function that "just knows" what the user wants, when what the user actually wants will vary radically between use cases. That is to say, there is no such thing as "one mean function to rule them all", and if you insist on trying to implement that function you're going to end up making a function that meets _your_ needs, and just confuses/misleads other people who try to use it. :) If you can provide some more details about the use cases you imagine for this function (from the description, it doesn't actually meet any of mine!) then you might get some more useful feedback. Otherwise, if you're actually just looking for a function that can calculate a mean of an iterator of "most numeric types" as an `f64`, your needs might be met by something like [this](https://docs.rs/streaming-stats/0.2.2/stats/fn.mean.html).
How much overhead does rayon have over straight threads? In my case, I don't really need work-stealing. I'm collecting the results from a function on all values from 0 to 1,000,000 and the function is run in fixed (and relatively short) time, calling f(1) takes as long as f(1000000), so I can just spawn 'num\_cpus::get()' threads and do it all manually. It's easier to let rayon do the work, but I'd rather avoid a slowdown.
Looks like this event loop is Linux/glibc only as it relies on `epoll` and `eventfd`.
&gt; Why not i128 or u128? Because it's uncommon and slower. Apply common sense and sensibility. If someone does need the summation to be carried in a 128 bits, they can transform the input on the fly. The default is 64 bits - it's a sweet spot. Reasonably people may disagree so stating one's assumptions when writing the code is helpful. &gt;If you can provide some more details about the use cases you imagine for this function (from the description, it doesn't actually meet any of mine!) then you might get some more useful feedback. I did my best to list the requirements. Basically a generic mean function that wouldn't need to be revisited for most cases. It's not rocket science. &gt; Otherwise, if you're actually just looking for a function that can calculate a mean of an iterator of "most numeric types" as an f64, your needs might be met by something like this. What types of inputs does that support? From what I see on line 72 in the [source code](https://docs.rs/streaming-stats/0.2.2/src/stats/online.rs.html), the mean is computed in a "running" way, i.e. with each sample addition: self.mean += (sample - oldmean) / (self.size as f64); That's a terrible way to compute the mean of anything. Grossly inefficient and marred by accumulating errors. So bad in fact, I omitted it from a review of solutions I found on the Net in a recent talk. (A better way to go would be to keep the accumulated sum, and do the division upon the mean being requested.)
GNU compatibility would be near impossible. I've seen the source https://www.gnu.org/software/coreutils/ It's not necessarily a bad thing, but GNU in general has rarely followed the Unix principle of "do one thing and do it well" but taken a more organic "do things if they make sense and out would benefit some kind of architecture." This leads to code where `ls` alone is a sizeable undertaking to understand what its even doing at all
https://github.com/georgewfraser/vscode-tree-sitter/releases helps here, except for the brackets.
I have `get` and `get_mut` that do exactly the same thing except the latter is `&amp;mut self` and returns a `&amp;mut` result to a member of `self`, vs. the former that just returns a shared reference. The contents are entirely duplicated. How do I avoid that duplication?
Is there even a doc for the standard js API in ts?
You could also make a post about containers with similar elements. When do you use 'Vec&lt;Box&lt;dyn trait&gt;&gt;'? When do you use a struct with an enum field? When do you use an enum with struct variants? This seems to be one of the big stumbling blocks.
Note that you can also do this by setting environment variable MALLOC_CONF to background_thread:true.
It's broken for 2/3rd of the OSes tho...
C# does not have any unorthodox borrowing and lifetime system and yet it's async transformation mechanism is very similar to the Rust's one.
Not that I've been able to find. Generally I make do with editor completions and "jump to definition"
Anyone have any impressions?
there's a screenshot comparing the result of tree-sitter highlighting. I think it's indeed better, not perfect though
I appreciate your anger. I'm sorry for that. Neither me nor anyone currently involved in the website is responsible for it. There's literally no one involved in the website who was involved in the original implementation in a major capacity. The only exception being somewhat me, who did content for the community page, but nothing more. There's also the other side of the coin: people who really love the website and the fact that it is frequently mentioned as a benchmark, even in it's admittedly broken state. Example? https://twitter.com/thorstenball/status/1130708763768434688 That discourse is exhausting and takes time and is at its end already. I'm forming a new team and would really love if the community gave those people some breathing room. We need to take a broken project out of the ice-box that was not moving for precisely the bad mood around it. No one wants to do that as their hobby. This post is an honest and straight-forward call for support. Please treat it as such, even if you have fundamental problems with the website.
You might consider using byte strings. You get the full power of a string API without sacrificing correctness on Unix. See: https://docs.rs/bstr/0.1.3/bstr/#file-paths-and-os-strings
Sorry if I missed this from just skimming the examples, but what about error handling? :-)
It's not.
While I appreciate your enthusiasm, I think the issues the website suffers from go way beyond what stack is used. When you can't fix a [glaring critical issue](https://github.com/rust-lang/www.rust-lang.org/issues/612) without a lengthy 4-month discussion (with multiple volunteers offering to do the work) where maintainers make ridiculous and dubious excuses and ultimately close the issue without addressing anything, the problem is organizational. (FFS, just add a temporary "ugly" fix until your ideal solution materializes). In any case, I wish you luck. You'll need it.
Make the function generic on the inputs and return an f64. This way all numeric types are supported and the user can use the result as he wants.
I tried it with a couple themes; IMO it's a thumbs down out of the box, but I it seems like it's just the way they specified the grammar so the project as a whole might be great. Too many items have been put in the same/unfortunately selected sorts, so some stuff that I think most people would really want contrast on (like visibility and mutability modifiers) have none. example : https://imgur.com/a/n7CPpHT It also carries over the (IMO) most unfortunate thing about the current highlighting grammar, which is that structs, enums, and traits are all displayed with the same highlighting. Probably all fixable though!
[Screenshot from the page.](https://github.com/georgewfraser/vscode-tree-sitter/raw/master/screenshots/rust.png) It makes the `struct` field names a different color, and gives a single color for all types and traits, but it loses the macro color and function color. It also throws away the color for stdlib default imports like Vec (and probably Option and Result), which is probably a plus for some people. Looking at my current VS Code, I have a separate color for `derive` lines, `::` (when not in function parameters), and `&amp;`, which it doesn't have in the before screenshot. I don't think I have any extra extensions that might affect this besides possibly RLS (but I would guess a lot of people are using that already). So I'd say this offers improvement in type definitions, and not much else.
I apologize. I didn't mean anyone specifically in my post, and I certainly wasn't directing it at you or anyone else who wasn't involved in the current website or subsequent fallout. Also, **thank you** for stepping up to handle this mess. It's a big task with all that has happened. I'll refrain from making any overly harsh comments in the future and will edit my original post. I wish you luck in your task.
&gt; I think the issues the website suffers from go way beyond what stack is used. Technical issues is what I'm wanting to address. I'm not particularly great at UX/design or what content is presented. What I can do is resolve the issues that the technical choices are impeding/delaying, which also can assist with other issues to some degree. &gt; the problem is organizational. I thought rust was more community driven? Either or, I can understand the viewpoints of both. Bit surprised no one was referencing the sites for other languages to back up their statements. &gt; (FFS, just add a temporary "ugly" fix until your ideal solution materializes). It was a bit different from a bug or lack of a feature though? This issue seemed to have conflicting bias on the purpose, I'd assume there was some way of the community or certain members beyond maintainers (as it's not really a code thing) could bring this up for debate/voting? Isn't that how the language moves forward with long-winded RFC processes? &gt; In any case, I wish you luck. You'll need it. I have no expectations of it being accepted as the new way forward, but since it should be much more accessible/flexible, if the community wants to send in PRs, they could shape an unofficial site how they see fit.
Also *thanks you* for the response, it really lightens the mood. :)
In my benchmarks, the overhead has been negligible. But the right way to tell is: Write your algorithm with rayon, do the measurement and look at the profile.
If you get an unused_imports warning, and get a compile error after removing it, it is a bug. Please report an issue with a reproducible example.
 That is a nice recommendation!!! I will use that, that seems like exactly what I was looking for. Would be better to just re-export the byte string types or make alias types?
Thanks, for the input. I've been ruminating on this for literal hours over the past few days now. I don't think it is so much as the non lexical lifetimes as it is this (would appreciate a sanity check): I understand why this is valid: fn main() { let mut foo = vec![1,2,3,4,5]; let mut bar = &amp;mut foo; // first mutable borrow. bar is not used again however, so it is essentially dropped immediately foo.clear(); // another mutable borrow, but because bar is dropped this is ok } And I understand why this is invalid: fn main2() { let mut foo = vec![1,2,3,4,5]; let mut bar = &amp;mut foo; // first mutable borrow. bar is used again, so we can't drop it immediately foo.clear(); // another mutable borrow, in this case it is a second mutable borrow as bar is still live bar.clear(); // using first mutable borrow } What was (is?) throwing me was why something like this would be invalid: fn main3() { let mut foo = vec![1,2,3,4,5]; let mut foo_reflist = foo.iter_mut().collect::&lt;Vec&lt;_&gt;&gt;(); foo.clear(); // E0499 second mutable borrow for x in foo_reflist {} } So my current thinking after much rumination is that foo_reflist borrows foo, and `collects` it into a vector. The borrow of `foo` is returned immediately. HOWEVER - foo_relist also has a lifetime that is bound to foo. And the borrow checker for multiple borrows is based on lifetimes and not simply direct references. Is this the right line of thinking? If so I know this is probably a _duh_ moment. I guess what has thrown me is 4.2 discusses the double borrowing problem but it's done in the context of direct references. Chapter 10.3 discusses lifetimes but it's mainly in the context of ensuring that cleanup doesn't invalidate references. I'm starting to realise that these things are much more deeply intertwined. At least I hope so - if my thinking isn't right it's probably back to the drawing board.
&gt;It's not rocket science. Indeed â€” in rocket science, the goal is obvious: deliver payload X into orbit Y. And if you want to move into rocket engineering, then you might want to think about minimizing cost, too. Rocket science is pretty easy! :) Here, the goal is not so obvious. You say... &gt; Basically a generic mean function that wouldn't need to be revisited for most cases. ...and yet while I have many use cases for a `mean` function, the one you describe meets none of my needs. I think you might be assuming too much that other people's use cases match your own, and thereby overestimating the need or even possibility of building a "one size fits all" mean function. To take another of your own requirements: &gt; Must have a form that mathematician with little programming expertise [...] Your mathematician with little programming expertise does _not_ think that the use of 64-bit integers internally is obvious or "common sense and sensibility". Your mathematician probably thinks that this is utterly bananas and arbitrary, because they don't know anything about the internals of popular processor architectures or why they should need to care about that. The problem in defining a "one size fits all" `mean` function based on the requirements you describe is that there is so much tension between them (I might even say _contradiction_ between them) that no matter what you come up with will be grossly surprising to half the people whose needs you propose to meet with the function. I'd suggest finding something to meet the needs of your specific use case, or if you're really interested in building something super-generic, spend some time researching the target audiences; the real challenge here is not in building the solution, but figuring out what the question should be in the first place.
Also, there a easy way to accept any kind of string? Like using `impl` in parameters?
You could try using \`--manifest-path &lt;XXX&gt;/Cargo.toml\` instead. Feature resolution also seems to be affected by which manifest is selected as the "primary manifest" (I've mostly noticed this making a difference related to intra-workspace dev-dependencies, but depending on what is causing the issue for you it may help).
I just fixed a bug in [momo](https://github.com/llogiq/momo) and added an example. I'll add more tests and benchmarks later this week, and then it's hopefully ready for release. Also TWiR and some other stuff.
Wonderful ! Which model are you using for meteo ? I've been eying [this](https://github.com/igarciad/weather_simulation) for a while but didn't have the courage to RIIR ...
I'm holding off trying to understand async/await so I can apply it to my usecase, but I'd like to ask you about it for your lib: &amp;#x200B; Similarly to you chat server example, suppose you're handling messages from a client. Now suppose that handling a certain message involves sending something to the client, which will answer with a message, but you need to have that second message to handle the first one (as a made up example, the first message could be "give me a user list", and then the handling could involve asking back "what's your identifier", and the second message contains that identifier, which you need for the list). If you loop over messages received from a channel, and your handling is blocking, this will not work, because the loop won't go on while handling the first message, and therefore will never start handling the second. &amp;#x200B; I don't know the right terminology, maybe "nested communication" or something? I presume this comes up a lot in client-server communications and looks like a prime use for async to me (but I don't know a lot about this). &amp;#x200B; So, my question would be, can this be handled by your lib? Could the \`update\` function just block on certain messages, and others will keep on being handled? &amp;#x200B; Sorry if that is a trivial question...
That's a good observation! In the terminology I'm suggesting, I would probably refer to things solvable by clone as a "low-level" borrow check issue, one that's solvable without any significant code re-architecture. But I don't think that applies to the kinds of issues cited in the paper, particularly when cloning is not an option.
&gt;What struck me the most, is that the author asserts the Spectre and Meltdown CPU exploits were almost not avoidable, as C itself hid a lot of the complexity involved in hardware design. The thing is, many of the behaviors involved in the recent CPU vulnerabilities weren't just hidden by C, but by the processor itself. In particular the worst bits (which allow the separation between protection levels to be violated) such as Meltdown and the recent Intel bugs were very much undocumented behavior. So if you want to go down that road you end up arguing that "x86 assembly is not a low-level language"...
"WINDOWS AND LINUX USERS: Please download the appropriate .vsix file from here and install manually until https://github.com/microsoft/vscode/issues/23251 is complete." What's the use of an ~app~ plugin store when you have to install stuff manually?
Idem *for applications*. Nothing is sweet like a `Result&lt;(), failure::Error&gt;` everywhere I don't care. For libraries though I prefer to implement my own `enum`.
Thabk you
Very nice intro! I've found such a sentence in the post about project management ([https://oribenshir.github.io/afternoon\_rusting/blog/project-management](https://oribenshir.github.io/afternoon_rusting/blog/project-management)), in the Packages section: "The package can hold at most one library crate and as many binary crates as you would like" Is this correct? Isn't it the opposite?
AFAIK avoiding this kind of duplication is impossible in rust
Those are possible with macros. The std println macro uses something like that: println!("{foo}, {bar}", foo=1, bar="bar");
This sounds interesting - I have sent you a private message.
There is no documentation yet, so I'm not sure about it, but I think you keep a state associated with each TcpConnection in the handler, and build a state machine to keep track of the responses of the connection without having to wait for it
i'm producing my own simplified model (it must be lightweight) ;o
You're not low level unless you're directly moving the electrons.
I'm a bit confused about the `impl Trait` syntax. What is the difference between these two function definitions? ``` fn foo(bar: impl MyTrait) {...} ``` and ``` fn foo&lt;T&gt;(bar: T) where T: MyTrait {...} ```
What's your result for `average([std::u64::MAX; 2])`? What's about `[std::u64::MAX]`? f64 is less precise than `u64` and you loose presicion in this case.
You have to download the plugin manually.
The latter lets you use the [turbofish syntax](https://matematikaadit.github.io/posts/rust-turbofish.html) (`foo::&lt;u32&gt;(123)`), if you needed to for some reason - but functionally they are equivalent. I usually choose the first one, because it's more readable IMO.
I just added [FBmTexture](https://www.janwalter.org/doc/rust/pbrt/textures/fbm/struct.FBmTexture.html) to [rs-pbrt](https://www.rs-pbrt.org/about/) and fixed a [bug](https://github.com/wahn/rs_pbrt/issues/95) ...
It's not `What's everyone working on this week`, but glad to hear it!
You can try to avoid duplication with macros, but apart of that - there's no way.
[This thread](https://www.reddit.com/r/rust/comments/919ief/whats_the_status_of_the_named_parameters_rfc/) from 10 months ago suggests that no such thing has been planned as for now. I presume there's just not much pressure on this functionality, since its main use case (skipping optional parameters) is not a problem in Rust (since it does not support default parameters anyway).
https://github.com/georgewfraser/vscode-tree-sitter/issues Couples of issues atm but the author is working on it
And even on Linux it would be nice if there was something which used to new `io_uring` API so we can get async disk IO too.
We're based in Fitzrovia, London. The company is Enhance ([www.enhance.com](https://www.enhance.com)) and we are building a product that enables shared and managed hosting companies to create distributed, fault tolerant, modern application hosting. It's basically a modern and scalable alternative to cPanel which is the industry leader. It is a microservice architecture and most services are written in Rust.
&gt; For example, some chips might have one particular SSE operation take a certain number of clock cycles and another might implement it so that it takes a different number of clock cycles. There can be other variations in implementation as well. Enough of a difference to make using SSE *slower* than not? If not then that would just be extreme micro-optimizing for implementation details?
Depends on the definition of "low level". Some people argue that basically only "low-level" assembly languages should be low level, others argue that any language that exposes hardware details is low level. It's a sliding scale that's only useful in context.
Strange, must've misclicked.
For the case where you want a `shared_ptr` without atomics even in a program that does use pthreads, see [std::__shared_ptr&lt;T, __gnu_cxx::_S_single&gt;](https://stackoverflow.com/a/15141844/981959). The default is for `std::shared_ptr` to be safe in the presence of threads, but if you know what you're doing there's a way around it.
&gt; A general solution is to write your program and name it differently, then make it behave like something standard if it's called with an appropriate argv[0] Ugly hacks, yeah. Imagine a world where you can just install any two versions of a program and it just *works*, other programs can reliably find and call it, you can set whichever version you want as default by putting it in the PATH, you dont need to mess with symlinks, it Just Worksâ„¢ &gt; Ripgrep has no use for the grep name, rg is just fine, and we can have both modern features and non-insane cli as well as backwards compatibility without installing each and every program twice. Oh, I more meant two versions of the *same* program. You can install Ripgrep and Grep, sure, but they're different programs and it'd be weird if you couldnt. If you want to install different versions of them, however... you'll need to rename everything yourself, probably manually(i dont think package managers support this?), and there is no reliable or standard way to know the existence of or use different versions of arbitrary software.
Lots of nice things in this release, sweet. ---- &gt; Removed the Read trait bounds on the BufReader::{get_ref, get_mut, into_inner} methods. I'm confused on what purpose that serves?
No, the struct doesn't hold the bounds. Most of the methods are hidden behind `R: Read` (including the constructor). I've found this to be a far better pattern because then transparent use of the struct (e.g., where a reference is shuffled around) doesn't need to mention the `R: Read` bound itself. These methods don't care about `R` at all, so it makes no sense to hold them to the `R: Read` bound too.
Please remember to update the links to the standalone installers.
What use is there in shuffling it around if you can't use it though?
Ah yes, Rust 1.25 before match (write-)ergonomics, much nostalgic.
My DD doesn't do this. What distribution are you on?
Can anyone elaborate on the need for `Option::copied`? I was under the impression that `cloned` would function exactly like that for types that are `Copy`. Doesn't `.clone()` on a type that is `Copy` just copy it?
Exactly as he said. Also, if you want to block the client because you're processing the current request asynchronously and want to delay processing the next request, you can call the `mute()` method.
[The traits have multiple methods for error handling.](https://github.com/adgear/mini-rs/blob/master/src/aio/net.rs#L648-L655)
How endtest is spamming up tech subs every day with [multiple accounts](https://www.reddit.com/user/boss_scarbos) and [focused spam posting](https://www.reddit.com/user/dragnea_presedinte) that clearly breaks [reddits self promotion rules](https://www.reddit.com/wiki/selfpromotion.)
See [this question](https://github.com/rust-lang/rust/issues/57126#issuecomment-458314544) and its reply in the corresponding reply. In short: `Option::copied` was provided, because `Clone` might have side-effects, which `Copy` is guaranteed to not have.
I don't think I understand your question. If you're writing a library, then I'd try to make the API in terms of `Vec&lt;u8&gt;`/`&amp;[u8]`. If you're writing an application, then just do whatever you want? Note also: https://github.com/BurntSushi/bstr/issues/5
Have you looked at the `bstr` API? You'll note that many APIs, like [`BStr::find`](https://docs.rs/bstr/0.1.3/bstr/struct.BStr.html#method.find), accept a `B: AsRef&lt;[u8]&gt;`, which permits callers to provide any of `String`, `&amp;str`, `Vec&lt;u8&gt;`, `&amp;[u8]`, `BString` or `&amp;BStr`. This does not of course include other types of strings, like `Path` or `OsStr` because it's not possible to implement zero cost conversions for those types to `&amp;[u8]` on all platforms. (For Windows, it's not even clear what a `&amp;[u8]` representation of an `OsStr` would be. WTF-8 is a natural candidate, but that's an internal implementation detail of `OsStr`.)
What does the `Box&lt;Fn&gt;` imply?
Right... With a default implementation doing nothing, if I'm understanding correctly? So it looks like this style of API is inspired by Objective-C and Java (ultimately, Smalltalk, I suppose), with message-passing based on virtual method calls. This approach would probably feel more natural in those languages, or even something like C++. The issue is that if a user of the API wants to handle and communicate errors, or perhaps react to them in some way, they will have to manually specify some communication channel to whatever component is issuing events on the event loop, and that will be hard to do without running into lifetime issues or using shared ownership. In short, `Rc&lt;RefCell&lt;T&gt;&gt;` all over the place. I could be wrong, but this is at least part of the reason why this API feels not quite "Rust-y". :-)
Box&lt;Fn&gt; is a pointer to a function, and if I understood the release notes correctly, you can now call it directly without having to dereference it first. ie. let f = Box::new(|| println!("Hello world!")); *f(); // before f(); // 1.35.0 and up
Sure, I understand that because this is the regular `Box` utilization. But what does that imply for this particular case?
How does that sort of thing work for people on older Kernels?
You can send messages to another `Handler` via a `Stream` as the example in the post shows. Isn't this enough?
For back-pressure, it will work the same as Pony. [More details here.](https://github.com/ponylang/rfcs/blob/master/text/0017-tcp-backpressure.md) The user will be able to react to backpressure with the [`throttled()` and `unthrottled()` methods](https://github.com/adgear/mini-rs/blob/master/src/aio/net.rs#L673-L677) and will be able to [`mute()`](https://github.com/adgear/mini-rs/blob/master/src/aio/net.rs#L470-L472) and [`unmute()`](https://github.com/adgear/mini-rs/blob/master/src/aio/net.rs#L496-L498) reading from a socket.
If I were to do that, it would be behind a feature.
To me the most obvious case is that you can now use a `Box&lt;Fn&gt;` in places that have some kind of `Fn` type bound, so functions taking a function of in associated types, etc. Maybe there is something else?
So what would be a low level language? Assembly and machine code do not allow controlling those details, so they arent low-level either?
Thanks. Do you know if this is related to the feature `fn_traits`?
I would so much prefer that https://rust-lang.github.io/rfcs/2089-implied-bounds.html be implemented so that you only need to specify it on the data structure once and never in any impls. (Although there might be back-compat concerns to add the bounds to `std` data structures that previously only applied the bounds to operations).
Is there a smart fast way to calculate the length of all vectors within a vector? let vdad: Vec&lt;Vec&lt;i32&gt;&gt; = Vec::with_capacity(1000); vdad.push(vector1); vdad.push(vector2); I want to know the combined amount of members of vector1 and 2, using only vdad. I use a helper-function so far, which loops over the vector of vectors and then adds up the v.len()
Like i said before history repeats itself, and i been there seen that, which is why i can predict it all, and if you are really interested in proof (the basis of my claim), just look around . Do you see anyone creating cargo plugins for any Rust IDE, by the time someone starts working on it out of despair without funds, it will be too late and then the corporate interests of evil marketing from likes of intelliJ will over shadow thier efforts(which is what i meant by strangling Rust to death).
For some odd reason, Ubuntu Bionic, with coreutils 8.28. The option you're looking for is `status=progress`.
Does anyone know if there is similar a (n)vim plugin?
&gt;Make the function generic on the inputs and return an f64. Yes, but how would the deduction of the summation type work? (Summing integrals should be carried in 64 bits etc). Some code would be helpful. Thanks!
&gt;What's your answer for average([std::u64::MAX; 2])? That would overflow in the u64 implementation. If the caller knows the inputs push against the boundaries of u64, they'd need to convert the stream to a floating point type prior to feeding it to the function. &gt;What's about [std::u64::MAX]? f64 is less precise than u64 and you loose presicion in this case. BTW are there custom variable-sized integrals and infinite precision reals in Rust?
I honestly wouldn't know. As you have probably already observed, the RFCs/issues don't reference each other. So taken together with what `fn_traits` seem to be about, I would say that these things are orthogonal.
https://azul.rs/ http://tomsik.cz/stain/ https://github.com/mozilla-mobile/android-components https://github.com/mozilla-mobile/reference-browser/
I see. thank you.
&gt;The problem in defining a "one size fits all" More like "one size fits most" so the function would need to be revisited/redefined only for niche cases (frequent overflows, unusual types involved, etc). There will of course be such. But thinking of e.g. a general library function, it should cover most meaningful cases. (BTW I see a bunch of downvotes... which parts of what I said were uncool by community's standards? Thanks.)
They're an orthogonal issue. They're in charge of talking to the OS to actually create and manage windows and such. Ideally, you should be able to use any one of these window-creation libraries with any graphics library.
Read down a bit, but https://doc.rust-lang.org/stable/book/ch20-02-multithreaded.html#implementing-the-execute-method
Add a shout out in this week in rust - that's sure to get noticed. (I'm all for a rust role but alas am a contractor... no contract rust roles yet but this could very well be the year...)
Writing a pass compatible password manager that also encrypts the file names in the repo. It also does not depend on gpg but uses openssl aes to en_decrypt everything
Writing a pass compatible password manager that also encrypts the file names in the repo. It also does not depend on gpg but uses openssl aes to en_decrypt everything
You could use the various [iterator](https://doc.rust-lang.org/std/iter/trait.Iterator.html) methods: `vdad.iter().map(|v| v.len()).sum()`
I made something exactly like this for my science fair project. Here's a couple things that came from it that might be useful to you. Looks like you're trying to make worlds (as in globes), so it'd be nice if the edges if your map lined up. Check these out: A program for wrapping voronoi heightmaps here (msg me if you need some explanation): https://github.com/ecumene/java-voronoi-height-maps Turn your perlin map into a seamless noise function https://www.gamedev.net/blogs/entry/2138456-seamless-noise/
"Low level" is always said in a relative context. It *is* a low level language compared to most other languages.
He relies on dependencies (tree-sitter) which has to be compiled for each platform but MS does not support distributing extensions with different deps yet (issue linked in your quote)
For a safe implementation I would just use a lazy_static! RwLock&lt;Box&lt;dyn Database&gt;&gt;: [like so](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7d32977de248e72cb2906326bac8f746) You could maybe look at how [log](https://github.com/rust-lang-nursery/log/blob/master/src/lib.rs#L1249-L1307) does it. but that boils down to the same thing really, it just saves a little indirection by using unsafe
`#[cfg(test)]`/`#[cfg(not(test))]`?
Is it possible to [make this work]( https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=3677afe1e4b56588d6dbc3c0c972f4a5) with new release?
`#[macro_use]` isn't required for importing macros from other crates in the 2018 edition of rust, you simply import it like anything else with `use crate::macro`
Same with `example/` and `--example`.
You can use those methods in a generic function however. Before this change you would need to needlessly specify `R: Read` in every caller as well. fn redundant_fn&lt;R&gt;(reader: BufReader&lt;R&gt;) where R: Read { reader.into_inner(); } fn nice_fn&lt;R&gt;(reader: BufReader&lt;R&gt;) { reader.into_inner(); }
For libraries you can still use `failure_derive` to automatically implement `Fail` for structs/enums
Yes, for now i don't have time, but in future can implement
What if you disable the progress bar? To me, it looks like it's spending some 60% of the time matching regexes. &gt; Wahrscheinlichkeit Sigh ðŸ˜¶.
Technically, that works. *Technically.* But, now what? Do something with it. You can't, and will never be able to. By not having a `R: Read` bound, the fact that R is read is forever lost, you can't get it back. [So you can get some generic type R back, but you can't use it for *anything*.](https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=c1b4970ab96df0f6f6ae3f59f42e12ec) After calling ~~bad~~nice_fn, R is useless. R is some generic type R that the compiler knows nothing about, and you can do nothing with.
I feel stupid now -.- Thanks
Unfortunately not. [`impl FnOnce&lt;A&gt; for Box&lt;F&gt;`](https://github.com/rust-lang/rust/blob/50a0defd5a93523067ef239936cc2e0755220904/src/liballoc/boxed.rs#L699-L707) relies on moving a value out of a `Box`, which is only possible because of a special rule in the language that says it is. To do the same with a container that isnâ€™t `Box` weâ€™d need something like a `DerefMove` trait, which doesnâ€™t exist yet. This impl even works with `F: ?Sized`, in particular `F = dyn FnOnce(â€¦)`. This relies on the unstable feature [`unsized_locals`](https://doc.rust-lang.org/unstable-book/language-features/unsized-locals.html). However the only way to produce an unsized local today, even on Nightly, is by moving out of a `Box`.
Not right now, as far as I know. Box is still special. Someday!
Can you file an issue on the `indicatif` crate with a progress bar that just counts up? I've never used it, but it seems surprisingly slow.
Can reproduce; the slowness is due to the progress bar. It seems that indicatif is not really meant to follow such minuscule tasks; you will get much more meaningful results if you do e.g. for _ in bar.wrap_iter(0..total/10000) for _ in 0..10000 { // check if won... } } (And just in case, `cargo run --release`.)
OMG, how did I not think to look into the base logging package. Facepalm.
Also release notes say that &gt; You can now coerce *closures* into unsafe function pointers. But the feature doesn't work for [all closures](https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=bec2e04c9ed0b6acec24e78376d2d17c). Maybe if it did work for this use case it could have been possible to call closure without moving it out.
This is the exact opposite of how generic types work. The whole point of generic types is that *somebody* knows what they are, and that identity is threaded through intermediate layers which don't.
I await your proof that, despite all evidence to the contrary, this code compiles. https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=f70348506cd662aa4d054e94c6f87d3e
Don't put things into my mouth.
There's also a new contender in this space, snafu.
You literally just argued that code should compile, and the fact that `R` is `Read` is lot is the opposite of how generics work. So go on, prove it.
If you only wanted to have fun with iterator helper methods, and didn't care about good performance, you could also write `vdad.iter().flatten().count()` :-D
The latter also makes it possible to have more than one parameter of type `T`. That's important if you want to do something like push them both into the same `Vec`, where the compiler needs to know not only that both parameter types implement `Trait`, but also that they're the _same_ type.
Yes, it looks like `cargo new` added that automatically.
Sure, I'll try.
It's dependent on a lot of other stuff, I'll see if I can reproduce in a simpler example.
Can you submit an issue for this for uutils? Iâ€™m slowly trying to go through the code and refactor the utilities. It would be great to have a list of missing features people need so I can add them as I go.
This is unrelated. [RFC 1558 Allow coercing non-capturing closures to function pointers](https://rust-lang.github.io/rfcs/1558-closure-to-fn-coercion.html) has been implemented for a while now. In 1.35 it is extended to support coercing directly to `unsafe fn()` function pointer types, not just `fn()`: https://github.com/rust-lang/rust/issues/57883 `fn(â€¦)` and `unsafe fn(â€¦)` (lower-case f) function pointer types are a single pointer, pointing to the code to execute. Closure syntax `|â€¦| { foo }` creates a value of an anonymous struct type that implements the `FnOnce` trait, and possible `FnMut` and `Fn` as well. (Upper-case F.) That struct contains fields for each of the variable that the closure captures from the environment. To do dynamic dispatch with traits, you can for example have a value of type `&amp;dyn Fn()`. However this value is made of two pointers: one for the data (the value of the anonymous struct type, for closures), and one for the vtable (which itself contains a funciton pointer to the `Fn::call` implementation.) If the anonymous struct for a closure happens to be zero-size (because the closure captures nothings, or only zero-size values), then this data pointer in `&amp;dyn Fn()` is useless. RFC 1558 allows coercing to a (single) function pointer in that case. In your example, the closure captures `x` so its anonymous struct is non-zero-size and it cannot be coerced to a `fn()` or `unsafe fn()` pointer type.
&gt; (BTW I see a bunch of downvotes... which parts of what I said were uncool by community's standards? Thanks.) I can't speak for others, but I will happily downvote people who I think are obnoxious, even if they strictly haven't "violated" any particular standard... For instance, I think your complaint about the stats library is very rude. It's clearly a *streaming* operation (as the crate name 'streaming-stats' would suggest), so you can run and query it forever, using only a constant amount of memory (as opposed to keeping all numbers in memory). Of course sacrifices must be made to achieve this, in this case the algorithm perform more computation and accumulates more rounding errors than *other implementations with different constraints*. Perhaps you don't think this implementation fits *your* idea of what a mean-calculating function should do, but that is not a reason to belittle this implementation. *I* think it's silly that you don't want to do Kahan summation, but you don't hear me calling your suggested implementation 'terrible' and 'bad'. In addition, I don't appreciate comments like "Apply common sense and sensibility" either, when used to imply that someone didn't. Your requirements might make sense to *you*, but clearly the people responding on your post disagree. Are they all morons, or would it be nicer to assume that maybe your own ideas are not as convincing to others? So yes, I totally downvoted you.
makes sense, thank you
Whenever you want to reduce a collection of items into a single value, think [`fold`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.fold): let sum_of_lens = vdad.iter().fold(0, |sum, vec| sum + vec.len()); In this case, though, the map+sum way is probably more readable.
Of course it doesn't; `nice_fn` doesn't know that `R` is `Read` on input. If you make it `nice_fn&lt;R: Read&gt;`, it will compile. If there was a `R: Read` bound on `BufReader`, you'd have to write that in the first place.
We already have some of those - like [log](https://crates.io/crates/log) and [regex](https://crates.io/crates/regex).
You might want to take a look at this website: [https://areweguiyet.com/](https://areweguiyet.com/) . I don't have any personal experience with this, so maybe other people can give a more in depth answer.
Yes? And? The fact that you can make code that does nothing is.. useless. What purpose does it serve? What purpose does removing the `R: Read` bound on those methods serve? You can't *do* anything with it without that bound. Why would anyone ever *want* to write nice_fn, a function that, by definition, can never do anything?
They *literally* never said that. &amp;#x200B; Here's what this enables, even though you're being incredibly rude right now: [https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=53778ce4b1e99ca3ee1dc60c6975e3ec](https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=53778ce4b1e99ca3ee1dc60c6975e3ec)
&gt;I can't use Box, because of no_std. If you're able to implement an allocator for your no_std environment, you can use the `alloc` crate (which will be stable in 1.36) to get `Box`.
"asking for an example" -&gt; "rude" ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤”
In the grand scheme of things, there are better things to get tilted at than a programming language.
This might be useful for future reference but I don't use C++ at all. My point was exactly that the Rc/Arc abstractions get you the same performance improvements without the instances of "if you know what you're doing" and "so you're on your own" that you had to use.
Let's just hope Gatsby is still en vogue by the time your next project comes up
Where in god's name does one learn about this? I've barely even understood closures.
The only way I can see to do this while keeping the function somewhat simple is to have the caller decide what type to use for summation. Otherwise, you'd need some complex type checking and overflow checking to know when to switch to a bigger sum type, and even then you won't know if the caller would prefer to use a float or int in any given situation.
An example of why it's good to move trait bounds to only the method that need them: [https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=081bddd82037acce928f715b43928ead](https://play.rust-lang.org/?version=beta&amp;mode=debug&amp;edition=2018&amp;gist=081bddd82037acce928f715b43928ead) &amp;#x200B; Long story short, there's two code paths for the same struct, one that doesn't require any io.
How familiar are you with Rust? If you don't know the basics, starting with the [rust book](https://doc.rust-lang.org/book/) and writing a few simple command-line apps will prepare you much better than jumping straight into the web will. Look at the [getting started](https://actix.rs/docs/getting-started/) guide and [examples](https://github.com/actix/actix-web/tree/master/examples) from the project. Try to write a very simple hello world and then start adding features from there. If you have any specific questions about the framework or error messages that you're not sure about, feel free to ask.
wow, this is beautiful. Love this kind of resource
You would _almost_ be wrong about that, but currently you aren't. `slice::Iter` does override `count()` to return the remaining elements count in O(1), however `flatten().count()` unfortunately doesn't forward to that implementation so it will still run in `O(n * m)`. With enough inlining the optimizer may still reduce it to sums of pointer math but I wouldn't bet on it.
&gt;I can't speak for others, but I will happily downvote people who I think are obnoxious, even if they strictly haven't "violated" any particular standard... Thanks for answering! &gt;For instance, I think your complaint about the stats library is very rude. It's clearly a streaming operation (as the crate name 'streaming-stats' would suggest), so you can run and query it forever, using only a constant amount of memory (as opposed to keeping all numbers in memory). Of course sacrifices must be made to achieve this, in this case the algorithm perform more computation and accumulates more rounding errors than other implementations with different constraints. Perhaps you don't think this implementation fits your idea of what a mean-calculating function should do, but that is not a reason to belittle this implementation. This may be a simple misunderstanding. Yes, a moving average implementation in constant space is what I had in mind as well. Requiring accumulation would be onerous. So here's how the current algorithm does it: Strategy: maintain the average at all times For each sample: compute the new mean with the recurrence: newmean = oldmean + (sample - oldmean) / samples; When queried: just return the running mean The cost of this strategy is two additions and one division per sample. A secondary problem is that as the number of samples grows, the relative losses caused by division to a large number grow as well. A better solution would go as follows. Strategy: maintain the sum all times, compute average on request For each sample: compute the new sum with the recurrence: newsum = oldsum + sample; (better yet use Kahan at the cost of a bit of extra state) When queried: return the running sum divided by the count The cost of this strategy is only one addition per sample, and one division per query (which can be helped in case of repeated queries by caching). There are known ways to address accumulating error. I think it's difficult to argue that the first version is superior. &gt; I think it's silly that you don't want to do Kahan summation, but you don't hear me calling your suggested implementation 'terrible' and 'bad'. I totally agree it would be silly. That's why the Rust stats library should use Kahan, too. My point in the initial message was "No need for advanced algos (Kahan etc) as long as it's clear how they could be adapted." Meaning, no need to write down the algorithm as long as it's clear where it would be inserted. For simplicity of exposition only. &gt;In addition, I don't appreciate comments like "Apply common sense and sensibility" either, when used to imply that someone didn't. Hmm, I didn't mean that but I see how it could interpreted. The continuation was, "Reasonable people may disagree so stating one's assumptions when writing the code is helpful." For example here's what a possible answer would be: * If summing 8, 16, or 32-bit integrals: the summation is done in 64 bits * If summing 64-bit integrals: the summation is done in 64 bits * If summing 128-bit integrals: the summation is done in f64 (or not supported altogether) * If summing f32 or f64: summation is done in f32 or f64, respectively That would be a good set of choices. &gt;Your requirements might make sense to you, but clearly the people responding on your post disagree. Are they all morons, or would it be nicer to assume that maybe your own ideas are not as convincing to others? The nice thing about this challenge is it's an ideal to aspire to, not a hard and fast set of requirements or rules. The overall desiderata are easy to agree to: generality, good code reuse, cleanliness of implementation are unlike to foster controversy.
If you want to write a GUI program in Rust, you're gonna have a bit of a rough time right now. No UI library that is pure Rust is really usable right now (this means like, azul-rs and what not). The only libraries that you could make substantial UI projects with are things built ontop of pre-existing frameworks like GTK. An example of this would be like, [relm](https://github.com/antoyo/relm) for example.
Is that any different from just using a `BufReader&lt;String&gt;`? `String` implements `Read` after all.
&gt;The only way I can see to do this while keeping the function somewhat simple is to have the caller decide what type to use for summation. That would be a good option to have, but there are simple obvious cases: all i8, i16, i32 sum as i64; all u8, u16, u32 sum as u64. All floating points sum as their own floating point type. For everything else, let the user choose. Is there a simple way to make these decisions?
Just looking at the first one... &gt;Private field, public getter. This would be the textbook solution. [...] &gt;I felt that the additional method call parentheses from the getter would be noisy and provide zero benefit. Rust users already understand how struct fields work and would be happy to access this value as a field if I can let them. This is just a textbook example of having and easy and simple solution, and throwing it away in favor of something complicated and obscure. Rust users also understand how method calls work, and would be happy to access a field through a getter if it helps uphold important invariants. Using undocumented code and Deref impls to avoid a pair of parens is madness.
See `num` family crates
Madness is very common in bored software jockeys sadly. I've sometimes done insane things before taking a step back and go 'but is it worth it?' (Such as creating a ActionListener/Action factory using reflection in Java --which was not worth it btw, because it was difficult to make lazy, which was fundamental to not get weird object creation timeline problems).
Minor personal nitpick, I would prefer if you showed the futures 0.3 version as it will soon be stabilized with futures 0.1 becoming obsolete. Async/await only natively works with futures 0.3.
Hi! We support anything that implements the [Handler](https://docs.rs/lambda_runtime/0.2.1/lambda_runtime/trait.Handler.html) trait, so you don't need to anything involving `lazy_static!` if you don't want to. As for avoiding unsafe, remember that *most of* `std` is implemented atop of unsafe code. It's meant to provide a safe abstraction. `lazy_static!` is one of them!
I'm using stable so I can't use alloc at the moment, but I'm looking forward to. Will it be possible to have more than one allocator? I have one for kernel objects which never deallocates and a normal one for user code.
I feel the aversion to lazy_static has less to do with unsafe, and more to do with the fact that it feels like a bandaid on the lack of functionality in `const fn`s.
Like rust playground, for example
It just breaks my heart a little whenever I see someone practically dive into traffic to avoid a papercut.
The maddest scenarios make terrific case studies. ;) If this had been written about all the times I wrote a getter method, it wouldn't help anybody. I suspect that you are pointing out that handwriting the underlying implementation would be confusing and troublesome -- I agree. But once packaged up into a macro that exposes the pattern safely and clearly and correctly with attention toward how it appears in documentation, it can become a reasonable building block.
[The GTK bindings](https://gtk-rs.org/) are by far the most complete option, unfortunately.
It's the newest version
One of the advantages to wrapping incoming error types in your own error enum values is that you can add additional context to the error, to enable better debugging or error reporting. In a program I'm currently writing, I intentionally wrap my errors so that I can much more clearly distinguish between io errors that arise from reading the config file versus io errors that arise from interacting with stdout/stdin. &amp;#x200B; There are numerous situations where I have multiple enum members that all wrap the same incoming error type, just to give better context for the cause of the issue, which has the unfortunate downside of meaning I can't really implement the Into trait to get easy conversions, but I really think it will help in the long term both in terms of debugging as well as error reporting to the end user.
i rewrote my [bash prompt in rust](https://github.com/NerdyPepper/pista) for the shiggles! it was enjoyable, working with libgit and libc. i plan on reading the rust nomicon this summer, and maybe do something useful with my newfound knowledge!
Cool, thanks! It would be great if average() would be general enough to work with those types without repeated specialized implementations.
Right now you can only have one global allocator that `Box` and friends will use. There is work being done to make collections parametric wrt allocators but I have no idea what the timeline is for that becoming stable.
i rewrote my bash prompt in rust, with a couple new features! i wanted to take up a small program to familiarize myself with git2-rs and libc. &amp;#x200B; the prompt itself is fully configurable. maybe ill extend it to a library down the line! feel free to ask me any questions or critique my implementation! &lt;3
It's the newest version
I still don't understand why so many people seem to dis GTK. I have my frustrations with the gnome desktop, but GTK seems reasonable.
Here probably because gtk is not written in rust.
It doesn't look native on Windows (it pretends but doesn't) which cuts 99% of the desktop market for desktop application.
`fold` is a really common idiom in functional languages like Haskell. For folks coming from those sorts of languages, it's often the first thing they look for. But if you're coming from the usual imperative languages C or Python, it might be less familiar.
Shouldn't it be `Uniform::from(1..=6)` instead of `1..6`?
It is! I follow your thinking, and it sounds completely right to me. If you were to expand the full type of `foo_reflist`, then it'd be something like let mut foo_reflist: Vec&lt;&amp;'__ i32&gt; = ... where `'__` is the lifetime of foo.
Yup, I realized that a few hours ago and changed it to the "stupid" approach ( [1, 2, 3, 4, 5, 6] .choose(rng.into()) .unwrap() + [1, 2, 3, 4, 5, 6] .choose(rng.into()) .unwrap() ) as usize
Typically, by studying functional programming basics. The three arguably most fundamental tools for manipulationg collections functionally are the functions `map`, `filter`, and `fold` (the latter is sometimes called `reduce`).
About a month ago I was desperately searching for good teaching material about (procedural) macros in Rust. Then I discovered your brilliant workshop and really learned a lot from it. Now a case study of tricky Rust code combined with macros. &amp;#x200B; Thanks so much for all the work you done for the Rust community!
Nothing else looks like native in windows nowadays either, unless you wander into the really old parts of control panel.
Isn't this when programming becomes art?
i wonder how many users care anymore now that almost everything is web/electron?
Qt looks native and consistent across all platforms, is very easy to develop and bundle and has permissive license. I would love to use Rust for GUI part and get rid of C++ completely but alas, no way for now :( GTK unfortunately is only usable on Xorg from my experience.
It's a contrived example. In this case, you *could* think of it as a second layer of buffering, but a cache is different from a buffer.
I care. I resent the fact I have three different web browsers installed because of my choice of text editor and the fact people stopped using IRC. Native is fast; web browsers not used for browsing the web are bloat in every sense of the word.
Notice that by that definition there doesn't exist any real low-level language for x86 anymore. At least not a public one. Even the assembly has layers of abstraction over microcode, which is the one that is bound to the details of the hardware.
&gt; RISC-esque architectures advocated for the opposite approach back in the day, and mostly lost out to CISC-esque architectures such as x86 I would disagree with this. Most CISC CPUs are RISC behind the scenes with a layer made to appear like the CISC one. Moreover the rise of ARM in power-limited areas, a RISC architecture, shows that RISC is still out there. CISC remained in PCs and Servers because there was a huge inertia to keep x86 going. The only way to move to 64 bit with 64x86 which is x86 with some extensions, only keeping the inertia could they keep the benefit.
I think it's a bit of misconception that nobody cares anymore. It probably comes from the variety of 'free' apps that can be downloaded and installed without much support or obligations. Corporate users certainly do because they pay for it and they don't like 300MB runtime to be deployed on 10k+ workstations which takes 400MB of memory and 10% of CPU just idling. And then there is a development aspect of it, maintaining a large client-side web app written in some dynamic language is a nightmare. Hell, even on a server side I would rather stay away from any of the fancy JS frameworks. To give an example, we have created a relatively large solution with variety of languages involved (Scala, Java, Rust, C++). For the server web part it's a Wicket framework, and just a very little portion of hand-made Javascript, and it's been amazingly great to develop and maintain, it's testable, super reliable, looks good (for a business app, so no endless scrolling and smartphone-optimized controls), super easy to refactor. And for desktop client part it's middle-sized Rust portion (30k loc) + small part in Qt/C++, which I'd love to get rid of eventually (because even with little C++ code you have to be super careful). Also a joy to develop without any electron crap.
People may have seen this news via the crosspost here yesterday (https://www.reddit.com/r/firefox/comments/bqg49u/after_3_years_and_8_months_of_work_webrender_will/), but this official announcement has much more information about the rollout and future development of WebRender. Be sure not to miss the list of Firefox bugs accidentally fixed by WebRender: https://bugzilla.mozilla.org/show_bug.cgi?id=1464426
Qt is not native and consistent across all platforms. macOS notably does not look consistent; they draw their own controls. When people call Qt 'native', they're overloading the term to mean something else entirely (e.g, not running in an interpreter).
&gt;macOS notably does not look consistent; they draw their own controls &amp;#x200B; From their own documentation: Qt's widgets use HIThemes to implement the look and feel. In other words, we use Apple's own APIs for doing the rendering. &gt;When people call Qt 'native', they're overloading the term to mean something else entirely (e.g, not running in an interpreter) I have used the term 'native look and feel', not 'Qt native'.
Is it Strong?
hmm, that naming...