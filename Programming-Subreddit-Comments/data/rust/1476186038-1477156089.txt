Please share the URL of your blog. Would like to subscribe by RSS.
&gt; Rust's type system is neat, but it can't guarantee correctness or even simple invariants (e.g. this list is always sorted). I keep seeing a claim like this but I never understand it. As far as I can see you encode the invariant into the type system (e.g. you make a sorted list type, and ensure the only way you can construct it is by providing evidence that the list is in fact sorted and/or by sorting) the same way you would in Coq et al. It's impractically cumbersome to do this in a language that lacks both HKTs and dependent types, but it's not impossible.
&gt; On Linux, go binaries* do not use the C library, depending only on kernel version 2.6.23 or above. This means you won't be able to run a go binary on an old kernel, which may exclude some enterprise Linux distros. My knowledge may be out of date, but I think it's a little more complicated than that. The Go toolchain won't produce fully static binaries in all cases, for example, if you use the `net` package. You can force the issue by setting `CGO_ENABLED=0`, which should give fully static binaries.
please please do!
I looked a bit into svgcleaner, libsvgdom and libsvgparser and have some remarks: - You just convert the path into relative form. This is far from optimal. Just think of `M 0 0.995 L 0 1`. With relative paths the line would be a `l 0 0.005` This happens quite often in real svg files. In svg_util I get a bit closer to the optimum by [checking wether the relative or the absolute path segment is longer for each segment](https://github.com/hannni/svg_util/blob/master/src/primitive.rs#L279). This can be done in a streaming manner without heap allocations (I have [a struct](https://github.com/hannni/svg_util/blob/master/src/util.rs#L247) that implements `Writer` and just counts the characters written without allocating anything). The only case where this might not be optimal is when the absolute and relative segment are the same length. You might save one char in later segments with one version, but that needs lookahead so I didn't implement that. - According to the svg spec you only need `f32` except for intermediatary steps when applying transforms for path data. - You don't need spaces after the arc flag since the *second version* of SVG 1.1 (IMO a bit bad to have such a change so hidden). [In the *first version* of SVG 1.1 and before you always need a space following a flag](https://www.w3.org/TR/2003/REC-SVG11-20030114/paths.html#PathDataBNF). So you should probably remove `&amp;&amp; x.is_sign_positive()` from [here](https://github.com/RazrFalcon/libsvgdom/blob/master/src/types/path.rs#L593) if you want to gain compatibility with older SVG versions with `join_arc_to_flags`. There's an [inkscape bug](https://bugs.launchpad.net/inkscape/+bug/1284409), showcasing exactly the case that can happen with your code when it hits a parser with the old grammar. Test [`gen_path_11`](https://github.com/RazrFalcon/libsvgdom/blob/master/src/types/path.rs#L831) would be affected by this. Probably [the documentation for `join_arc_to_flags`](https://github.com/RazrFalcon/libsvgdom/blob/master/src/write_options.rs#L18) should be updated as well to better describe the situation. - You don't seem to round the numbers for printing. You need to do this otherwise you end up with `M0.1 0.1 l 0.2 0.2` being converted to `M0.1 0.1 L 0.30000000000000004 0.30000000000000004`, because binary floats can't represent most decimal fractional numbers exactly. I solved this by counting [the amount of decimals after the dot](https://github.com/hannni/svg_util/blob/master/src/path.rs#L240) when parsing, and then [rounding](https://github.com/hannni/svg_util/blob/master/src/util.rs#L21) when [converting to relative or absolute path segments](https://github.com/hannni/svg_util/blob/master/src/primitive.rs#L230) &gt; It's looks like a very simple alternative I tried to keep it as minimalist as possible to keep code complexity low. Just two printing options, pretty printing for (imo insane) people that want to work by hand on path strings and an optimized version for small file size. Two representations of paths, low level [`PathSeg`](https://github.com/hannni/svg_util/blob/master/src/path.rs#L114) for e.g. browsers that need to support the javascript SVG dom manipulation api, and high-level [`Primitive`](https://github.com/hannni/svg_util/blob/master/src/primitive.rs#L10) that is as simple as possible for renderers, optimizers (combining path segments, cubic to quadratic segments, ...), use in anything that wants to output SVG. Still I think my code does the same optimizations as yours, plus additional ones (trying both relative and absolute path segments, rounding when converting to absolute/relative for writing, trying to convert into smooth beziers, trying to convert into horizontal/vertical lines). So far my code really only touches a tiny part of SVG, while your code is pretty all-encompasing and I like your code :) Nice work
Please file bugs. rust-gdb should still work, and in fact should be used for types like Vec that aren't treated specially by the compiler.
Here's a basic one that works. If you need more robust enum parsing, there's the [`parse-macros`](https://crates.io/crates/parse-macros) crate. macro_rules! IntoCode { (@as_expr $e:expr) =&gt; { $e }; ( ($dst_name:ident) enum $src_name:ident { $( $var_name:ident $_var_payload:tt, )* } ) =&gt; { impl $src_name { pub fn into_code(&amp;self) -&gt; $dst_name { IntoCode! { @as_expr match *self { $( $src_name::$var_name(..) =&gt; $dst_name::$var_name, )* } } } } }; } #[derive(Debug)] #[repr(u32)] enum DataCode { Message = 1, Error = 2, } enum Data { Message(Message), Error(Error), } IntoCode! { (DataCode) enum Data { Message(Message), Error(Error), } } struct Message; struct Error; fn main() { println!("{:?}", Data::Message(Message).into_code()); } 
Yeah, sum types and traits make this stuff really nice. Especially tree traversals are nice to work with. But this doesn't cover all, or even the majority of programming problems. Sometimes you have, say, a big bunch of floating point data you have to make some sense out of. There's no structure or clearly defined base/inductive cases that would work out neatly and invariants have to be enforced with plain old asserts and unit tests. I'm pretty sure the Rust compiler magic would reduce the amount of time that I spend in the debugger but it doesn't magically make it go away. For easy stuff, yeah perhaps (but I probably wouldn't need a debugger even if I was writing C) but the kind of difficult problems I work with it's a tool I want to have.
When I was 11, my father taught me programming in a BASIC dialect by letting me write a connect four game with minimal guidance. I even managed to implement a minimax-based computer opponent. The code was ugly as heck but it worked. In hindsight, his patience and reluctance to give more information than needed plus the fact that I liked playing games ~~spurned~~ spurred my interest in programming, an interest that hasn't waned for almost three decades. Thank you for giving me a reason to be thankful for his awesome gift to me and a new appreciation of what it takes to teach.
I think it's possible to learn a very simple set of rules for which things are safe and which are not, and if you stick to the safe set then your programs will always be correct. You may find the safe set inadequate to express the program you want (particularly with a limited type system like Rust's), but you'll always at least know when you're stepping outside the safe zone. (This dovetails with thinking about the halting problem: there is no algorithm that can always determine whether a program will halt, but there is a large class of programs that provably halt). And IME with experience it becomes very possible to write your programs in sympathy with the type system such that most or all of them remain within the safe subset - you ask yourself why you believe the code is correct, and then translate that into the type system. (I'm basing this on my Scala experience here - obv. it's a bit harder to express everything in Rust, but I'm not convinced it's *that* much harder - replacing all my HKTs with concrete first-order types would be tedious and bloat the code a bit, but it seems like a small constant factor overhead).
Can't wait until versions with that bug that requires the `as_expr` trick go out of use.
I look forward to reading this blog post!
A (what I'm guessing is a) similar story is making rounds on Facebook right now. Apparently you're required to orally declare any cash (up to 10,000 Euro) you're carrying or forfeit it. I imagine if you're not aware of this rule or make a mistake the experience would resemble a robbery.
Yes, that's entirely true. I was trying for simple-as-possible-but-no-simpler, but that point covers an important category of binaries for go and Rust hackers. :-)
Huh, I never thought of it that way. Interesting, I can see that.
`println!("{:#?}", my_variable);` I *think* I saw it once mentioned in IRC but nowhere else
My friend and I applied for Rails Girls Summer of Code, and one of the projects we could apply to was Servo. We mentioned it to our friends (who happen to work at Mozilla) and they very excitedly told us we should seriously consider Servo. We didn't even know what a web browser engine was at that time, but after hearing them talk about it so passionately, we got excited too! Our project mentor `jdm` also listed the project as appropriate for beginners, which helped us just jump in. Edit: Fix typo.
Tip I found recently (maybe everybody knows it already but well): on Linux you can use `ldd your_binary` to display the list of libs it has been dynamically linked to. I find it useful because sometimes a third party crate will link to a library dynamically without advertising it that much and if you have the library installed you might not notice anything until someone (e.g. Travis) complains it doesn't work on their computer.
This is making me smile, because the exact same thing happened to me and my partner. Later when I started learning how to program, my partner has been the best learning resource though!
The alternative is to do something more like this: `readelf -d YOUR_FILE | grep --line-buffered NEEDED | cut -d [ -f 2 | cut -d ] -f 1` I keep it around as a function in my shell configuration: function deplibs() { readelf -d "$1" | grep --line-buffered NEEDED | cut -d [ -f 2 | cut -d ] -f 1 } Unlike `ldd` (which, yes, executes various bits of the binary it's run on), this is safe :D
I'm not sure what gitter is but those two libraries you linked look about right. Thanks for your help.
Is there also a way to figure out the `repr` parameter in the macro?
Awesome! Your words bring hope to me.
I'm programming since 11 too and also created some games at that time. :)
I know this isn't a direct solution, but you can always enable one level of optimizations in debug/test mode.
I think that's both awesome and all the more reason to pay the gift forward to those who are currently learning. Alas, I see too often self-appointed programming 'experts' who are impatient and want learners to fail to bolster their ego with how they mastered this 'hard' topic (because if it wasn't hard, why would so many drop out?).
You can use box syntax if you're ok with nightlies https://is.gd/ghUmU6
*spurred.
What if you need to modify the option in the else of the if let Some()
I've submitted a PR to crates.io to use it: https://github.com/rust-lang/crates.io/pull/457
Good point. Although, I'm not sure how much plugins differ from actual rustc. If they differ too much, it might be better to just fork rustc and experiment with it. I'm not an expert in compiler, so I don't know. I tried to look at plugin API and as far as I understood, codegen was allowed only using `#[derive(...)]` which may be insufficient for this case. (But maybe changing syntax to something like `#[derive(Trait from self.field)]` could work.)
I wish rust would swap the roles of musl and glibc, making glibc the one you have to opt-in for. 
I don't remember seeing such thing. I've seen teachers who weren't able to teach and some even knew less than students (sad result of government "education"). I sometimes don't understand how programming can be hard for anyone. Somewhere deep I feel like it should be possible to learn for anyone. I'm not sure whether that is just my wish or reality. Anyway, there's probably something into it, because my very good friend does game dev courses for children (not just programming but also graphics, storytelling, etc) and even younger children (less than 11) get most of the stuff and are highly motivated. The difference is that he does it very well. He works hard to use modern teaching concepts (gamification, project-based learning, freedom for children, ...) and make the courses interesting. He also occasionally takes the children out to meet real game devs and learn from them. I consider it being one of the most influential projects that I can observe closely. So I see some potential here. :)
I'm not too worried about it; if debug output is suddenly multi-line, the worst-case scenario is that I have to mine the information out manually. It's just worth mentioning that there's more ways to treat the output format than "either you're parsing it, in which case shame on you, or you're capable of handling any arbitrary UTF-8, in which case you're fine."
If it fails then that means it's a none value. If you need there to always be a value there are functions like unwrap_or() that deal with it. You can see all the different ways you can get values out in the docs [here](https://doc.rust-lang.org/std/option/enum.Option.html)
The same thing as if you'd do it manually. If you have to traits like this: trait Tr1 { fn foo(&amp;self) -&gt; u32; } trait Tr2 { fn foo(&amp;self) -&gt; u32; } You can already write both `impl Tr1 for Bar` and `impl Tr2 for Bar`. It will work but you will have to manually tell the compiler which one you want to use (I think it's something like `(x as Tr1).foo()`) So if you used `#[delegate(Tr1, Tr2)]` it would just do the same thing as if you wrote the code yourself. The result would be the same. The behavior is the same as with `#[derive(...)]` - it just writes the code for you, so you don't have to write boilerplate. The difference would be of course when delegating just `impl Type` instead of `impl Trait for Type`. And again, the behavior would be same as if you wrote two functions with same name: hard compile error. In that rare case, you could still specify individual methods. Is that understood better now?
That's terrible, why would it do that? Does it have to execute code from the binary to determine all linked libraries?
On Windows both Rust and Go have to link to Windows system DLLs to access Windows API. This means they both end up depending on DLLs that every Windows computer has. However, Rust by default will typically link to `msvcrt.lib` which ends up in a dependency on DLL versions of the VC++ CRT, which are not guaranteed to be available on Windows. You can always bundle those DLLs however, resolving any portability concerns. Furthermore there is work towards making it easy to link to `libcmt.lib` instead, which would statically link the CRT and resolve those concerns entirely. 
`Box::new` is a regular Rust function, so semantically the item is created on the stack then copied to the heap. The stack creation will usually be elided out by optimisations, but may not always be, and probably won't be when running in debug mode (so you might overflow the stack in debug mode and have the code run fine in release). On the other hand the box syntax is a [placement new](https://en.wikipedia.org/wiki/Placement_syntax), the boxed item is semantically created directly on the heap (or somewhere else, `box` is a trait based operator so you can implement boxing for custom allocators, or collections) without the intermediate semantic step through the stack. Importantly, `box` can also reach "inside" functions so if you `box foo()`, `box` will not apply to the return value of the function, instead it will allocate the relevant space for the function's return value and the function will store that return value there directly (IIRC `Box::new` *may* do the same… depending on the optimisations being run)
AFAIK, coroutines are affected, but mioco is not, since it does not allow such a freedom of "jumping". Mioco exposes a model that feels and behaves like threads (just you can have millions of them, and they will schedule in more on-readiness fashion). 
&gt; Sure some things might break This is why it's unacceptable to change it now. Crater helps us make sure we don't break things, but that doesn't mean we can just make breaking changes even if they pass creater without an extremely good reason. Specifically RFC 1122 explains exactly what may be done as a breaking change. It states: &gt;"minor releases may only contain breaking changes that fix compiler bugs or other type-system issues." edit: I've changed my mind about this minor change to debug semantics. It might well be acceptable to change, pending a crater run and discussion of possible cases, and pending a discussion with the community, obviously.
Well you can't e.g. write a generic function that works on lists and sorted lists, because you don't have higher-kinded types. And you don't have dependent types so you have to pass invariants around in a kind of continuation passing style (i.e. there's no way to have a type that represents a+b, you have to instead carry around an unknown type c and a certificate that c=a+b).
Also of note http://yehudakatz.com/2016/10/11/im-excited-to-work-on-yarn-the-new-js-package-manager-2/
&gt; It's patterned after Cargo, and @wycats Like actually? Someone was like "Oh, cargo is great, let's do this for js" ? That's pretty cool, if so. I'm ignorant of Cargo's history, though. edit: Ah, so Yehuda worked on Cargo.
I wish Rust devs named `&amp;mut` as `&amp;uniq`, as was occasionally discussed, eg. [here](https://www.reddit.com/r/rust/comments/25i544/babysteps_focusing_on_ownership_or_removing_let/).
I just use this: function deplibs-tree() { local -A SEEN_LIBS function deplibs-recursive() { local LIB="$1" PARENT="$2" if [[ -z "$PARENT" ]]; then SEEN_LIBS[$LIB]=" " echo -n "${LIB}" else echo -n "${SEEN_LIBS[$PARENT]}" echo -n "${LIB}" if [[ -n "${SEEN_LIBS[$LIB]}" ]]; then echo " (already seen)" return else SEEN_LIBS[$LIB]=" ${SEEN_LIBS[$PARENT]}" fi fi echo "" for CHILD in $(deplibs "$(cc -print-file-name="$LIB")"); do deplibs-recursive "$CHILD" "$LIB" done } deplibs-recursive "$1" } Output: $ deplibs-tree /bin/bash /bin/bash libc.so.6 ld-linux-x86-64.so.2 libdl.so.2 ld-linux-x86-64.so.2 (already seen) libc.so.6 (already seen) libncurses.so.6 libc.so.6 (already seen) libdl.so.2 (already seen) Easier to read IMO, and still avoids `ldd` (with it _executing_ the files it is run on). Speaking of: &gt; You can also do this safely with a forced interpreter and `LD_VERBOSE=1` This is no safer than `ldd` - in fact, `ldd` is a shell script that does exactly that. The problem is that the dynamic loader actually executes parts of the libraries loaded this way.
`npm shrinkwrap `
&gt; (IIRC `Box::new` may do the same… depending on the optimisations being run) The general term for that optimization is "return value optimization". Even in debug mode, [this](https://is.gd/2pT6Ll) example does use it. Look at the LLVM IR, where `fn test() -&gt; [u8; 15]` is translated into `void @test([15 x i8]* noalias nocapture sret dereferenceable(15))`, equivalent to `fn test(out: &amp;mut [u8; 15])`, which means that it's constructing the value in its caller's memory instead of on its own stack.
Gitter is a chat platform for Github projects. Just replace "github.com" with "gitter.im" in the project URL ([here](https://gitter.im/dpc/mioco) is the mioco gitter).
&gt; `(x as Tr1).foo()` There's also the `Tr1::foo(&amp;x)` syntax, which is easier on the eyes IMHO. 
That's going to work when returning the value to the caller through the stack, but wether it works when the value is immediately passed to `Box::new` (and the value is immediately constructed in boxed memory) is the real question. While I really suck at LLVM IR, comparing the IR between `Box::new(test())` and `box test()`, I assume no: in the latter case there's an allocation block *before* the function call. In fact it looks like RVO to heap doesn't even happen in release mode with `Box::new`.
You might get more feedback in /r/playrust. This subreddit is for the programming language called Rust. It has the same name as the game, but they're not related.
No problem. &gt; I'm ignorant of Cargo's history, Gather 'round the campfire kids, it's time for some old war stories. ---------------------------- Way back in October of 1995, [CPAN](http://www.cpan.org/) was born. Written for Perl, it truly was a comprehensive network of archives. Especially notable was [CPAN testers](http://www.cpantesters.org/), which tested packages on tons of platforms that you probably haven't even heard of. EDIT: A whole bunch of people said I should give a shout out to [CTAN](https://en.wikipedia.org/wiki/CTAN) here. I never used it. --------------------------------- Fast forward to 2003. The Ruby programming language is becoming more and more popular outside of Japan. Ruby and Perl are closely related; Perl was a huge inspiration for Ruby. And so, the idea was had to create `rubygems`, a tool that would let you easily install packages of Ruby code. It would download from [RubyForge](https://gems.rubyforge.org/) at first, but then, trouble was brewing. This next part is important. In 2008, as more and more projects moved to the newly-formed GitHub, and away from RubyForge, people still had to host their projects on RubyForge, because it was running the gem server. In April of that year, GitHub announced that [they'd be running a Rubygems server too](https://github.com/blog/51-github-s-rubygem-server), for any gem hosted on GitHub. The blog post contains this text: &gt; One concept regarding our server that bears repeating is that your gem will always be prefixed with your username. Installing mojombo’s grit gem is done via the following: &gt; &gt; $ sudo gem install mojombo-grit &gt; Successfully installed mojombo-grit-0.8.1 &gt; &gt; Using said gem works a couple of ways. First the regular require: &gt; &gt; $ irb -rubygems &gt; &gt;&gt; require 'grit' &gt; =&gt; true &gt; &gt; Update: The following also works if you have competing versions of the same gem: &gt; &gt; $ irb -rubygems &gt; &gt;&gt; gem 'mojombo-grit' &gt; =&gt; true &gt; &gt;&gt; require 'grit' &gt; =&gt; true &gt; &gt; The namespacing may feel awkward as first, but it really lends itself to the distributed nature of the service we provide. Forking a RubyGem project shouldn’t be any more complicated than forking any other type of project. At first, we all rejoiced! And indeed, this was the beginning of the end for Rubyforge. -------------------------------------------------- But all was not well in package manager land. While you _can_ install `mojombo-grit` and `grit` at the same time... do you want to? For those of you who don't know Ruby, it has no real namespacing, so... yeah. It got messy. Furthermore, since you could so easily use these forks, people didn't bother contributing fixes back upstream. They'd just say "oh cool I pushed a patch to `steveklabnik/grit` so use that instead." And now you had hundreds of forks of popular projects with their own little incompatible extensions. It was madness. It was not a good time. Nobody was happy. This lasted for about a year. In August of 2009, a new project, [Gemcutter](http://www.rubyinside.com/gemcutter-a-fast-and-easy-approach-to-ruby-gem-hosting-2281.html), came onto the scene. &gt; As part of the plan to get everyone using it as their main gem repository, Gemcutter has already imported all of the gems that were on RubyForge - meaning there are over 5000 gems on there already. &gt; &gt; Other than ease of publishing, other benefits of Gemcutter include the way it sidesteps the confusion surrounding Github's naming policy for gems (i.e. prepending the Github username to the gem), and that it provides easy, obvious access to project-pages for the gems. **Six weeks** later, GitHub announced that they were [retiring their gem server](https://github.com/blog/515-gem-building-is-defunct), and Gemcutter was re-named [RubyGems](http://rubygems.org/). -------------------------------------- But all was not well. As the ecosystem _exploded_, so did the number of gems you'd need to get going with a project. What was at first one or two gems became ten or twenty. And those ten or twenty depended on two or three of their own. Say you're building a rails app. Here's what you'd do: 1. Run `rake server` 2. Watch it say "hey you don't have library x installed" 3. Curse. 4. Install some version of library x. 5. Pray. 6. Goto step 1. This was.... brutal. So along game a new feature of Rails: you could add lines like these to a config file: config.gem "rspec" config.gem "rspec-rails" config.gem "mocha" And then `rake gems:install`. This would then automatically install all of those gems. This made it more convenient, but nothing enforced it. So you'd end up accidentally installing a new gem, and then completely forget to add it to the config file, and then the next person's build would just break. Ugh. This is where two people you may know, Yehuda and Carl, step up. They created a tool called [Bundler](http://bundler.io/), which lets you create a file, the `Gemfile`, where you can list your gems and gem versions. It will also automatically figure out all of your dependencies for you. Running `bundle install` figures all this out, installs the gem, and creates a `Gemfile.lock`, which records all the versions of all the stuff it installs. Rails can then use bundler as a library, and ask to load all those gems and versions. This changed everything. Everyone has to add their gems to the file, so you always know that it's up to date. It won't write out the lockfile unless all the versions of all of the dependencies work together, so you're never stuck with conflicts. A simple `bundle install` gets you all set up. (Also worth noting: Bundler was _very_ controversial at the beginning. But that's another story....) (This is all late 2009-mid 2010) ----------------------------------------------------------------------------------- [Node.js](http://nodejs.org/) hits the scene. It also needs to figure out how to load modules. The person who writes the module loader for node also writes a tool, npm, to manage your versions for you. Like Bundler, it has a `package.json` file, but unlike Bundler, no lockfile mechanism. This is January 2010 But where npm really shines is something that was a major criticism of Bundler/Rubygems: it can install and use _two_ versions of a package. If you are in this situation: * Package A depends on C package v 1.0. * Package B depends on C package v 2.0. With Bundler, you're stuck. With npm, it Just Works. The reasons here have to do with the underlying language, and I won't get into them here. ----------------------------------------------------------------------------------- Now, we're building Rust. It's clear that Rust needs _something_ to manage packages. (Well, clear to some of us... that's another story.) We tried two or three different things. They didn't work out. it's 2014. Mozilla decides to hire Yehuda and Carl [to make a package manager for Rust, Cargo](https://mail.mozilla.org/pipermail/rust-dev/2014-March/009087.html). Cargo learns very heavily both from Yehuda and Carl's experiences with Bundler, but also with what npm did right. Cargo has the lockfile feature of Bundler _and_ the multiple versions feature of npm. We also remember the namespace hell of the GitHub gem server days, and repeat what Rubygems did. Which also has controversy.... ------------------------------------------------------------------------------------ Now it's 2016. Yarn hits the scene, learning from Cargo's lockfile-based approach and bringing it to JavaScript. It cites Cargo explicitly as an influence. ------------------------------------------------------------------------------------- So, as you can see, there's a long lineage here. Some parts of this are shorter, because this is already too damn long. You'll also notice that this is focused on dynamic language, language-specific package managers. It's woefully incomplete. Such is life. You'll notice two big themes: 1. Every new tool builds upon the successes of the older tools, and learns from them. 2. Somebody gets mad at every step of the way. Hope that helps.
What was the controversy around Bundler, exactly? (or vaguely if need be)
I get a similar output, thanks for sharing that. I notice that there are multiple versions of GLIBC included for many of the dependencies; does that mean that all of those libraries' code is included in the binary or that the included code is compatible with them all?
From what I know, EXEs on Windows will always look for DLL files in the local folder the EXE is running in. So you can just look for `msvcrt.dll` on your machine, copy it into your app's folder, and zip them up together when you do a release.
Which is the [universal function call syntax](https://doc.rust-lang.org/stable/book/ufcs.html) or UFCS for short.
Right, the [`Box::new()`version ](https://is.gd/42Fbma) doesn't look like it's doing that. In release mode, the IR for `main` has %_3 = alloca [15 x i8], align 1 call fastcc void @test([15 x i8]* noalias nocapture nonnull dereferenceable(15) %_3) which appears to be storing the result of `test()` on the stack.
src/appendix_04-history-of-package-managers.md | 96 ++++++++++++++++++++++
Lol thank you for the unabridged history
 error[E0277]: the trait bound `&amp;std::collections::HashMap&lt;T, &amp;str&gt;: std::iter::IntoIterator` is not satisfied --&gt; &lt;anon&gt;:4:9 | 4 | for (key, value) in h { | ^ | = help: consider adding a `where &amp;std::collections::HashMap&lt;T, &amp;str&gt;: std::iter::IntoIterator` bound = note: required by `std::iter::IntoIterator::into_iter` So the problem is specifically when you try to iterate over the `HashMap`. But that's strange, why *wouldn't* you be able to iterate over a `HashMap`? Check [the docs for `HashMap`](https://doc.rust-lang.org/std/collections/struct.HashMap.html). Scroll down and, *ah-ha!* impl&lt;'a, K, V, S&gt; IntoIterator for &amp;'a HashMap&lt;K, V, S&gt; where K: Eq + Hash, S: BuildHasher type Item = (&amp;'a K, &amp;'a V) type IntoIter = Iter&lt;'a, K, V&gt; So it *does* implement `IntoIterator`, and on a reference to boot. But doing so *requires* an extra constraint on `K` (they key) to hold. If it doesn't, then this `impl` doesn't apply. Specifically, it needs `K: Eq + Hash`. In the case of your function, `K` is `T`. Does `T: Eq + Hash`? No. What happens if you add that? use std::hash::Hash; fn tst&lt;T&gt;(h : &amp;HashMap&lt;T, &amp;str&gt;) where T: Eq + Hash { for (key, value) in h { println!("{} = {}", key, value); } } gives error[E0277]: the trait bound `T: std::fmt::Display` is not satisfied --&gt; &lt;anon&gt;:7:33 | 7 | println!("{} = {}", key, value); | ^^^ &lt;std macros&gt;:2:27: 2:58 note: in this expansion of format_args! &lt;std macros&gt;:3:1: 3:54 note: in this expansion of print! (defined in &lt;std macros&gt;) &lt;anon&gt;:7:13: 7:45 note: in this expansion of println! (defined in &lt;std macros&gt;) | = help: consider adding a `where T: std::fmt::Display` bound = note: required because of the requirements on the impl of `std::fmt::Display` for `&amp;T` = note: required by `std::fmt::Display::fmt` Ok, add *that* use std::fmt::Display; use std::hash::Hash; fn tst&lt;T&gt;(h : &amp;HashMap&lt;T, &amp;str&gt;) where T: Eq + Hash + Display { for (key, value) in h { println!("{} = {}", key, value); } } Which now compiles. Moral of the story: don't just blindly follow the compiler's suggestions. They're just that: *suggestions*. They aren't always correct, because the compiler can't always identify the correct source of the problem. Also, *check the docs*: if the compiler says a trait isn't implemented, check to see *why*.
You're right, and I strace'd stuff myself to be sure. It seems that the old misbehavior of some versions of `ldd` (directly executing the file under certain circumstances) hybridized with [this post](http://lcamtuf.blogspot.com/2014/10/psa-dont-run-strings-on-untrusted-files.html) in my head at some point; it then proceeded to grow a moustache to show how evil it was.
box syntax has been around since the before 1.0 if I'm not mistaken. What's taking it so long to be stable?
What is npm used for on `crates.io`? I thought it was written in Rust? I'm just curious.
It's Rust on the backend, Ember on the frontend. So it's used as a build tool, not as a web server.
Ah, makes sense.
You got: Error: Unsupported CSS at 1:109. And that's ok. svgcleaner doesn't support full CSS2 spec. It doesn't mentioned in the Readme though... Only in the libsvgdom readme. I should add more items to limitations section. Anyway, svgcleaner is more suited for cleaning SVG files created in vector editing applications, which is stated in the Readme. There are no benefits from cleaning handwritten files.
&gt;You'll also notice that this is focused on dynamic language, language-specific package managers. Such a shame, too - there's plenty of stuff happening elsewhere too. In the Haskell world, `cabal-install` vs. `stack` is an endless fountain of drama. There's also NuGet, Paket, maven...
Yeah, it might be nice to have an option to have that (or also to check that all strings are translated). I'd personally prefer to have warnings instead of blocking the compilation, but I don't think it's possible in a `build.rs` file.
Another big tradeoff is that static linking leads to bloated binaries. Which can be a pretty big deal on low end mobile devices, not that you have much choice in that case.
They were removed before 1.0 because they'd be much nicer with higher kinded types, which are not yet in the language. See the section ["Removing the traits" in the collections conventions rfc](https://github.com/rust-lang/rfcs/blob/master/text/0235-collections-conventions.md#removing-the-traits) (Edit: now with direct link to section)
Does rust not provide an option for dynamic linking? I've never tried or looked in to it. And yes, my comment wasn't terribly on topic. I just wanted to bring up an FYI sort of point.
Or when working on code that has very complicated invariants that can't easily be tested.
You still need a debugger for debugging logical errors.
Reduce, not eliminate.
Feel free to ask any follow up questions, this concept is very exotic to people without a background in statically typed functional programming languages like Haskell or Scala.
Can I bikeshed a bit ? That html! macro looks very complex, Have you tried handlebars ,it lets you write html in html :) .
Funnily enough, I've actually used Haskell a fair bit and monads without fully understanding them; I've just come to learn typical usage and apply it to my own code. I haven't had a chance to carefully read the RFC (I won't skim over it because I won't pick much of it up) but will follow up once I've had a chance.
Your comment makes me wonder why we couldn't just have a tower of traits for operations based on required complexity. *i.e.* have an "*O(n)* index" trait, as well as an "*O(I don't care)* index" trait with a default impl for *O(n)* types. If you *care* about the complexity, you can use the trait corresponding to the maximum reasonable cost version of operations. ... oh wait, I know why: because that's complicated and no one would bother, and the whole thing would be chucked out the window for something less hideously over-engineered in the next major version. :P
&gt;This feature is not full-blown higher-kinded polymorphism, and does not allow for the forms of abstraction that are so popular in Haskell dang, was really hoping we'd be getting a `Monad` trait soon this is an exciting RFC, and I hope it gets moving. just last week I needed a lifetime parameter on an associated type and ended up replacing the trait with a concrete implementation instead. (there was only one implementation of the trait, and I was only using it to provide more separation between modules.)
Identify which DLLs your program depends on that are not system DLLs. `msvcrt.dll`, which mingw programs depend on, is a system DLL and does not need to be sitributed. Msvc programs however will either depend on `msvcrt120.dll` if you're using VS 2013, or if you're using VS 2015 `vcruntime140.dll` along with a whole pile of DLLs from `C:\Program Files (x86)\Windows Kits\10\Redist\ucrt\DLLs`. Once you've identified the DLLs that are not system DLLs and thus need to be bundled with your program, you copy them to the same directory as your EXE and shove it all together in your archive format of choice (7z) and then distribute. Or just statically link the CRT.
It does (`-C prefer-dynamic` parameter to rustc, iirc)
&gt; Beyond that there are main useful traits that rust could use from functional languages like haskell that can only be described with higher kinded types that would allow Rust to more more generically describe Iterators, Option, Result, and Futures (Functor, Applicative, Monad). This is a common statement, but its not really true. Unfortunately, the interfaces of the relevant methods (e.g. `map`, `and_then`/`flat_map`) are not consistent, and there's probably no real way to bridge the gap. In particular, `Iterator` and `Future` don't return types of "the self constructor" because a) they're traits, b) their lazy semantics 'leak' into their type signature.
The basic situation is that `Monad` is a trait (type classes in Haskell and traits in Rust are the same concept), but `Monad` uses higher kinded polymorphism in its definition, so you can't define the `Monad` trait in Rust. Rust does have types which implement the `Monad` interface (particularly `Option` and `Result`) but you can't abstract between them using that interface. Similarly, Rust has a bunch of collection types which implement `iter` with the same basic interface, but you would need some amount of higher kinded polymorphism to define the trait that contains the `iter` method (I would say *less* higher kindedness than `Monad` would need, but YMMV).
The problem is that `RandomAccessContainer` takes a parameter. So there are no types that implement `RandomAccessContainer`, they all implement `RandomAccessContainer&lt;String&gt;` or `RandomAccessContainer&lt;i32&gt;` or maybe `RandomAccessContainer&lt;T&gt; where T: BufRead`. Even implementing `RandomAccessContainer&lt;T&gt;` is different from "implementing RandomAccessContainer." Its actually very much like how if your function returns `|x| x * 2`, you get a function, but if it returns `x * 2` you get an int.
If you implement a trait for sorted and unsorted lists, you can be generic over both, no higher kinded types needed. Of course there are examples that need higher kinded or dependent types, but I don't think sorted lists qualify.
&gt; I didn't see this in the spec https://www.w3.org/TR/SVG11/types.html#Precision It's a bit complexly worded, but I think it says you should use single precision floats and convert to double for a few intermediary steps like applying `&lt;transform&gt;`s. &gt; So current version is correct, if I understand you correctly. You also need a space after the second flag according to the old spec. If a negative number follows, you don't output one with your code. So if you have `a50 50 0 0 1 100 0 50 50 0 0 1 -100 0` your code will output `a50 50 0 0 1 100 0 50 50 0 0 1-100 0` and break with programs using the old grammar. SVGo seems to behave the same way as your code, which lead to someone reporting it on the inkscape bugtracker: https://bugs.launchpad.net/inkscape/+bug/1284409 In the end they adopted the new grammar, so future versions of inkscape should support to leave of all spaces after flags. 
Nice step by step answer! &gt; don't just blindly follow the compiler's suggestions Good advice! We should still try to make them better, though. :) I'd've expected the error message to list the available implementations of `IntoIterator` (with the bounds on `K`) for example, so I didn't have to look at the docs. I unterstand that might be hard to do though (since it depends on `K` not the hashmap itself).
You could return a trait object or an enum, couldn't you?
X-Post referenced from /r/programming by /u/EdWilkinson [The Strange Details of std::string at Facebook](https://www.reddit.com/r/programming/comments/56xxmb/the_strange_details_of_stdstring_at_facebook/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
I never liked handlebars to be honest. It feels too weird to use. Besides the html! Macro writes a lot like haml so I like that aspect
Experimental ones: https://github.com/apasel422/eclectic As its docs explain, there's no good solution for iterators with current rust features. Edit: Hrm, even the Entry api uses boxed trait objects, that's a bit costly. Must be to allow everything to use used through dynamic dispatch.
Smaller and probably faster seems like a good benefit to me. But yeah correctness seems more important. My simple guess is that you need to convert to `f64` during the rounding, to not lose precision in those calculations. &gt; So it's an Inkscape bug, not libsvgdom? Maybe I didn't explain the issue good enough. In the [`join_arc_to_flags` documentation](https://github.com/RazrFalcon/libsvgdom/blob/master/src/write_options.rs#L18) you write that some viewers don't support this. This is because they have only implemented the old version of the specification, that does require spaces after the flags. However even if that option is off (in the case of a negative number following a flag) the path strings you produce are not compliant with the old version of the specification. So older viewers might choke on the svgs produced by you, even if you have `join_arc_to_flags` turned off. The inkscape bug is just that svgo behaves exactly like your code does currently. So inkscape, using the old grammar, couldn't read the file. So inkscape's behaviour was correct according to the old spec, but not according to the new spec (which always allows dropping spaces after flags, like you do when `join_arc_to_flags` is turned on). The people in that bug report didn't find out that the reason for this was that inkscape followed an old version of the specification, so it might be a bit confusing. &gt; And I should add a space after flags only for compatibility reasons? Exactly
&gt; &gt; Yes I do. https://github.com/RazrFalcon/libsvgdom/blob/master/src/types/number.rs#L27 Doesn't `format!("{}", number)` already do this? I remember a post about revamping the float pretty-printing so that it would print the smallest decimal number that maps to a given floating point number.
Stupid question -&gt; what benefit could this have over the current situation where data structures take generic type parameters like Vec&lt;T&gt;? Is it just the implementation method (maybe for efficiency) or is there something more?
Because the full placement stuff was being worked upon. Specifically, the placer traits and the `&lt;-` operator.
Thanks, that was really helpful!
But I'm using write!, which is used by format!.
This RFC looks really great; I especially like the idea of starting with only lifetimes parameterization, which does not require extending HRTB and yet opens up a lot of usecases. Has it been proposed?
Actually, it is desired that such traits exist, but we are stranded at the moment as the current generics cannot express the required constraints over lifetimes. I do not think that performance characteristics were considered. From the point of view of the implementer, this doesn't really matter; it's up to the user to pick a container which is good enough for their data and the algorithm (linear indexing is fine on 5-elements, for example).
I disagree, it is not duck typing, I still defined exactly what interface I need and want to use.
Dużo leweli można nabić?
Thanks for your help, I couldn't get your version with explicit lifetimes to work in my non-reduced version but instead I managed to: Since I use these templates to construct a struct, I can constrain the constructor directly by the Fn* trait, and then rely on the compiler to figure out that that implies it also implements Callback: [playground](https://play.rust-lang.org/?gist=4432d9a543c919e51a4ef92624306534&amp;version=stable&amp;backtrace=0). Thanks again!
Because you're writing code that talks of taking a VecLike object. It'd be nice to write code once &amp; afterwards be able to swap it out for a SmallVec in some subset of cases (nb smallvec crate includes a VecLike trait for SmallVec/Vec). If I have a data structure that can offer VecLike behavior, I shouldn't have to convert it to a Vec to pass it to some API See Higher Kinded Types. Instead of being generic over Vec&lt;T&gt; where T: Thing I want to be generic over U where U: VecLike&lt;Thing&gt;
I've been using Tendril as a SSO/COW Vec&lt;u8&gt;/String with some success. The API isn't a drop in but works well enough.
Please keep your criticism constructive (as in pointing out flaws with the JVM). Also as someone staying on the JVM for some decades, I'm very happy with my choice so far.
Premature sendage. Happens to the best of us...
Well my point was certainly not to try to expose algorithmic complexity in trait interfaces. 
Thanks, corrected.
Wouldn't it be possible that `from_raw_parts` does the conversion internally?
It means many Rust packages are *transitively* dependent on `libc` and/or `gcc`. Without `libc`, it's hard to talk to the OS or other native libraries, and `gcc` is used to build libraries or shims for binding purposes. So, yes, many Rust packages are transitively dependent on either the operating system, or some library written in C/C++. This is to be expected.
&gt; Actually, it is desired that such traits exist, but we are stranded at the moment as the current generics cannot express the required constraints over lifetimes. Makes sense. I thought I'd remembered that being a reason, but it wouldn't be a great one. I must have been confused with something else.
The `smallvec` crate has a `VecLike` trait for exactly this reason. https://docs.rs/smallvec/*/smallvec/trait.VecLike.html It works out without HKT because `SmallVec&lt;T&gt;` and `Vec&lt;T&gt;` are similar enough: they both dereference to `&amp;[T]` and get most of their functionality from that.
Oh, we can't change String/Vec at all, however there can be external crates which provide new string types. The perf impact of these things varies depending on the codebase, so a blanket change doesn't make sense as much. If someone needs it, they can use their own string type.
This isn't a full or proper answer, but seeing as nobody else has responded yet I'll give it a shot. Basically you'll need two things. One is the cross-tools targeting Linux. Those will just be special versions of gcc, ld, etc. that are prefixed with the target triple. The triple will probably be something like x86_64-unknown-linux-gnu. I would use brew to install crosstool-ng, and then use it to install the tool chain. The second thing you'll need will be the Rust cross-compiler and runtime. That's easy thanks to rust-up. Google for and download rust-up then use it along with your target triple to install the proper rust compiler and you should be mostly good to go.
I'd say the default string implementation in rust should be similarly optimised to that of c++, would be nice if String had SSO as well, as long as that interacts well with str. In fact, until now I had assumed that's what rust did
My information may be out of date but, last I heard, musl didn't support [/etc/nsswitch.conf](https://en.wikipedia.org/wiki/Name_Service_Switch) plugins (because of the way in which they are linked against glibc), which could produce confusing failures on configurations different from the defaults developers are likely to keep.
I was thinking of making a crate with functions for doing combinatorial stuff (that would be useful for solving Project Euler problems, amongst other things) but there are several things that I don't how to begin doing: * Deciding what types to use and how to make a decent/good API: Any suggestions for a good, beginner-friendly, online resource? * What kind of tests to write: AFAIU, I just need unit tests for different functions. * How to find optimal algorithms for many things: I could just search DuckDuckGo for good algorithms or look for papers/answers on StackOverflow etc. Is there any standard book/online resource for this? Alternatively, should I just try to emulate a C/C++ library for this? Also a peripheral question: there's no support yet for tail call optimization, right? I read the relevant discussion [here](https://github.com/rust-lang/rust/issues/217) but didn't understand much. As a toy example: consider the function `choose(n,r) := n!/(r! (n-r)!)`. * The input types should both be of type T ∈ {`u8`,...,`u64`} (or maybe even `BigUInt`). * Should the output type be `Result&lt;u64&gt;` (where `Err` indicates number is too big) or `BigUInt` from the `num` crate or something else? * How do I find the fastest way to compute it? I found one solution for Haskell [here](https://www.quora.com/What-are-some-efficient-algorithms-to-compute-nCr) on Quora but I don't think the same method will be fastest for Rust as Rust does not have TCO.
Yeah, thank you for reminding me! :)
Macros can't do reflection, so I highly doubt it'd be possible.
I used to think that there were 2 ways to represent strings: null byte terminated and length prefixed. Both have their upsides and downsides, but neither beats the other in all ways. And as it seems (at the time) null byte terminated won. When I started working with Rust I realized that, yes there is a way to combined the upsides of both implementations and have basically no downsides: fat pointers! 1. easy access to the string length 2. no annoying null byte terminator or any length in the string data This enables super efficient arbitrary slicing (substrings, really), which is super amazingly awesome when working on strings. Now I realize C++ can unfortunately never ditch null termination due to backwards compat but it makes me wish it could. A C++ standard library modeled after Rust's would be pretty damn nifty, but then again I might as well write Rust, which I do :)
On macOS, libc is the only officially supported interface. Apple does not document the syscall API. The same is true for FreeBSD. So unless you want to play rogue, you have to use libc on POSIXy OSes other than Linux anyway. On Windows, using the syscall interface is unsupported and completely infeasible. Look it up; the shear effort Microsoft had to go through to make sure everybody went through kernel32.dll is almost sad, but considering how many other "undocumented interfaces" have become de-facto stable, it makes sense that they have to randomly assign all the syscall numbers at install time, patching them into kernel32.dll and ntoskrnl.exe. Go for Linux uses the system call API. Nothing wrong there; it's documented. Rust uses libc. Go for macOS and BSDs also uses the system call interface directly, which is not supported by the vendors and can break at any time. :no_good: Rust again uses libc. On Windows, Go programs use kernel32.dll, as Microsoft recommends. As ugly as the number-patching is (it makes updates a PITA from what I understand), it apparently is necessary. In this case, I'd be happy if other OS vendors learned from Microsoft. Rust on Windows uses memcpy and a couple of floating point functions from libc, because calls to these functions are inserted by LLVM. All the system calls are done through kernel32.dll, just like Go does.
Yes, small string optimizations are one of those subjects that come up repeatedly. We've made the decision many times now that Rust's default types do not do this and likely will not ever. We value the predictability, simplicity, and utility of the flat representation. If profiling reveals this is important for an app-specific reason there are crates that provide such types. Should be a FAQ entry. It would be interesting to implement SSO in std and get numbers of how it affects the ecosystem globally. I would imagine that the predictability of Rust's types (especially lack of copying) leads programmers to be less sloppy and there wouldn't be a huge impact in the general case. One could imagine implementing instrumentation that detected excessive String allocation and suggested switching to a different type. In general I wish std was instrumentable...
What's the problem? 1. Rustup puts its binaries in /usr/bin. It conflicts and provides both rustc and cargo 2. There are system packages called `rust-toolchain-stable` maintained in the default repo 3. There's a system for building system packages from archive specifications (e.g. `rust-toolchain-stable-x86_64-pc-windows-msvc`), or alternative, everything is available in the official repos (which might be a bit much) 4. Rustup’s install and update functionality either is neutered and just tells you what packages to install, or it invokes the system package manager in the right way
There has been quite a bit of discussion of SSO in Rust before (I'm on my phone so no links but searching Github and the discourses for SSO or small string should find it), although most of it was focused on `std`.
You may be interested to know that C++ is getting a `string_view` type that, as you say is needed, is not nul terminated to allow arbitrary slicing.
It would be completely busted to do it in std; people rely on interior pointers to be stable across moves for unsafe code. And they are correct to do so.
It doesn't necessarily need to be an intrusive SSO.
The `futures::collect` function mentions in its [documentation](https://docs.rs/futures/0.1.2/futures/fn.collect.html): &gt; Note that this function does not attempt to execute each future in parallel, they are all executed in sequence. Is there an idiomatic way to collect a list of futures in parallel? I see the `buffered` and `buffer_unordered` methods on `Stream` that might be related, but turning a list of futures into a stream seems to involve a lot of type gymnastics?
Ok then whatever you mean by intrusive is something insane and not what I was talking about. This code is correct, and shouldn't break: unsafe { let x = String::from("hello"); let y = (&amp;*x) as *const str; let z = x; println!("{}", &amp;*x); }
You are looking for /r/playrust. This subreddit is about [the programming language Rust](https://www.rust-lang.org/en-US/).
By intrusive I meant something with a pointer to itself. Which doesn't work at all in Rust (but is something C++ does often, one of the string reprs mentioned in the video does it). Yeah, this guarantee is broken. Not sure if it is a guarantee, but folks would expect it to work.
We effectively decided to guarantee it in the thread I linked.
 stack.push(stack.drain(stack.len()-2..).product()); thankfully this horror show didn't compile (run afoul of borrowing etc) does rust code tend towards more readable / maintainable code as a side effect of this? 
In addition to what was already mentioned here, there are also some Docker images with rust compilers for Linux target (glib, musl) you could use. Sorry, I don't know of a guide or tutorial on that topic specifically. Maybe you could weite something about it when you get it to work? :)
Not really, since unsafe code may rely on the pointer still working.
Rust is very readable/maintainable. The borrow checker is a big reason for this, it prevents *nearly* all memory error related problems C/C++ can have. The borrow checker *should* happily accept let prod = stack.drain(stack.len() - 2..).product(); stack.push(prod); The issue is that your mutating stack twice in the same line. You can *alias* mutable pointers. So you can only mutate *something* in one place at a time. Having 2 mutable references to stack on the same line is a no-no. The reason for this is when draining the stack vector, the underlying memory in the vector *may* move (as it gets freed). So when you push to a vector starting at the old location, you *may* get a segfault. So the compiler refuses to compile your code. 
Rust and C++ have taken fundamentally different approaches to generics; C++ treats templates as a code generation mechanism and then type checks the resulting code. Rust has a type system with parametric polymorphism. We couldn't switch between them at this point.
/u/sigma914 and you both brought up C++ but I believe that the approach I described is different. My approach would not do the typechecking after instantiation, it would still use the trait as an interface. I admit that I don't know enough about the details to seriously talk about this subject and maybe I can't convince the ones who know the details due to this. So, if you feel like explaining why my way is not a valid way, please do so:) I'd appreciate it:) 
Note that while it _would_ make collection traits feasible, that's not that the only motivation. :-) This little bit of higher kindedness is the kind that I see needed most often by a long distance.
Not a robust example, but we've recently switched a large project to use error-chain. It's not perfect, but I think it's the best way to deal with errors in Rust atm. 
Ahh this is a good start, I hadn't even known about this crate. Looking inside the crate would also be a great way to look at error handling.
Darn I think you're right. I tried sketching out my thoughts on this more fully and they both run into problems. ConstraintKinds doesn't help because we'd need to versions of our higher kinded trait one over the trait versions (`Future`, `Iterator`) and one over the struct versions (`Option`, `Result`). As for two you could probably get rid of `IntoIterator` if we had HKT but it doesn't get really help us implement `Monad` or whatever for `Iterator`, aside from providing something that could be used a `pure`/`return` impl. 
The `new` method is only provided in the context of an `impl RandomAccessContainer&lt;_&gt; for _`. We need to fill in both types to find the `new` method, even though the `new` method only needs one of those types. You can't "not care" about the parameter, because we need the parameter to find the impl.
Yes, I understand that we need a specific type to find a specific impl but why do you need to know exactly which new method will be called? From the trait, you already know that the type will have such a method, why does it matter which one it will be? For checking the other types and the expressions in the templated function, it is irrelevant, I think. 
Request for Comment; to propose a significant change to Rust, its standard library, or its tooling, you write up a document and submit it through a pull request to [this repo](https://github.com/rust-lang/rfcs)
&gt; From the trait, you already know that the type will have such a method, why does it matter which one it will be? For checking the other types and the expressions in the templated function, it is irrelevant, I think. I _don't_ know that the type will have such a method, because I haven't found an impl, because I can't find an impl without all the type parameters. One thing that may help you understand is that Rust's syntax is sort of sugary about the `Self` type. In reality, every trait has at least one type parameter. That is, you should think of `ToString` as `ToString&lt;Self&gt;`. When you call a method from `ToString`, you figure out what the `Self` parameter is and then you look up the impl for that parameter to find the method - or raise a type error because there isn't an impl. Multi-parameter traits work the same way. You need to know all of the type parameters to look up the impl.
This is all part of the "non-lexical lifetimes" thing. There's been a lot of discussion on how to handle that - search the subreddit if you want the full story - but it's definitely on the roadmap.
One thing that may make this example more confusing is that it is not really well motivated. For example, this totally works, and I think its a lot like the semantics you're thinking of: trait Container&lt;T&gt; { fn new() -&gt; Self; } fn foo&lt;C: Container&lt;T&gt;, T&gt;() -&gt; C { C::new() }
It would be interesting if you could tag a struct with some kind of `#[thin_ptr(MyTraitObj)]` and then construct some kind of`ThinPtr&lt;MyTraitObj&gt;` from the struct that would only be a single pointer but Deref to a `&amp;MyTraitObj`
&gt; which it almost certainly won't! Which ones do?
There’s at least one crate relying explicitly on that guarantee: https://docs.rs/owning_ref/*/owning_ref/trait.StableAddress.html
For brew, go to brew.sh and follow the install instructions. Once brew is installed, do brew update, then brew install crosstools-ng. For crosstools-ng see crosstool-ng.org. Beyond that I don't know much as I haven't really used crosstools much.
For next week's FotF, I'd like to nominate Yehuda Katz, the lord of package managers. 
Thank you!
For context, the last time this worked was with a hacked up Rust 1.0.0-beta with even older instructions from tomaka and AerialX. Thanks to brson for the [update to support emscripten in nightly](https://users.rust-lang.org/t/compiling-to-the-web-with-rust-and-emscripten/7627) and jer for his talks on cross-compiling Rust!
Thanks, I didn't know that.
Interesting! This is very similar to [this PR](https://github.com/colin-kiegel/rust-derive-builder/pull/28) for the derive-builder crate but might go a few step further. Would be awesome to collaborate on this!
I agree. On the other hand, some things (generics) can't be dynamically linked. And dynamic linking doesn't allow inline optimizations. :(
Oh wow, very nice tutorial! I made a [game of life](https://github.com/Luke-Nukem/rusty_life) in rust with sdl2 a while ago. There might be something of interest in my code for you?
Hello. I try to implement my own unchecked slice, because boundary checking in default one is a bottleneck. I have a Slice struct with start pointer, end pointer and the length, and I try to implement core slice API including indexing with Range. As far as I understand, original slice returns &amp;[T], which is slice too. Here is my code: impl&lt;T&gt; ops::Index&lt;Range&lt;usize&gt;&gt; for Slice&lt;T&gt; { type Output = Slice&lt;T&gt;; fn index(&amp;self, range: Range&lt;usize&gt;) -&gt; &amp;Self::Output { /* Constructing new Slice */ &amp;new_slice } } Of cause, I get borowwing error, but I cannot see any way how to return the new Slice struct itself from ```fn index```. How can I override Index operator and return new slice? Is it possible to fully disable boundary checks in release build? (why there is no unchecked! macro or keyword or something like that to eliminate checks?)
This is amazing. Thanks for bringing it up to date and creating rust-webplatform.
That would be great, actually! There's already an issue [here](https://github.com/rust-lang/rfcs/issues/1762), and it seemed like most people found it worthwhile (or at least not overly burdensome).
I think the goal was to prevent users explicitly pattern-matching on the fields or constructing the struct themselves. However, that only requires that one field be private, not all of them. It might make sense to just have a single field `_make_private: ()` to prevent breakage when adding new fields.
Technically, if you also want to support struct initialization with `..Default::default()`, this would need to be a public hidden field, because purely private fields break struct init syntax even if they're never mentioned. Preserving semver with public fields is not my favorite aspect of Rust.
Seems about right. One angle here though: I've done nothing to minify this. So like, run that js through the closure compiler or something, then gzip it. It'll be much smaller. 
Grok failure! I really see the power of the borrow checker i do get it... but following the rust book, the life time chapter seems use a lot of words to say that lifetimes are scopes? But how does some ascii soup in a function definition let you say anything other than the scope of the function? fn bar&lt;'a, 'b&gt;(...) How is a and b any different they are both in the scope of the bar function?, given that i completly have lifetimes all wrong i think that whole chapter does a far worse job of explaining compared with the quality of what ive read so far... And if you can "elide" whatever that word means, why would you ever need to be specific?
Yes. I'm also looking for a better solution.
I wrote it as piece of documentation for `slog`, but I think what I describe is universal for any Rust code, and might be interesting to other Rust users as well.
Ah, I see, so you'd rather derive all the accessors for now and if the need for validation pops up then you'll re-implement the appropriate setter by hand?
Why does returning a mutable reference here require me to have the closure be `move`? And why doesn't that apply to the non-mutable reference one? https://is.gd/RgyEES EDIT: Is it because it owns the mutable reference, and it can't have multiple ones (and Rust thinks this is possible, because the borrow checker is "dumb") so it requires us to "move" the mutable reference? Does this incur any more overhead than usual? It should just be copying a pointer, correct?
It's worth noting that IntoIterator is actually implemented for each of `Vec`, `&amp;Vec` and `&amp;mut Vec` separately (scroll down to see https://doc.rust-lang.org/std/vec/struct.Vec.html).
C
Great post! I haven't read any of your code, but &gt; placed on heap with Box and then reference counted with Arc. Arc already boxes its contents. You might have extra allocations. If RecordStatic is always used as a static, I'd make those lifetimes 'static rather than 'a.
Your edit is correct. I wouldn't say it's that the borrow checker is dumb though; this is just the type system working as intended. &amp;mut T isn't Copy, for exacty the reason you state, so this just falls out of that. 
While I understand your sentiment, and understand that Go is not Rust, I think gofmt has shown that generalized automatic formatting can work. While not everyone will ever agree on a format, I think that it's much better than everyone in the ecosystem doing it differently. The biggest win is not when your whole team/company use the same style (you can easily do that today with lots), it's when the entire ecosystem uses the exact same style that it really shines. 
&gt; How can I override Index operator and return new slice? You can only return `&amp;T`s from `Index`, there's no way to return a value here. &gt; Is it possible to fully disable boundary checks in release build? There's no global switch to disable bounds checks. &gt; (why there is no unchecked! macro or keyword or something like that to eliminate checks?) You can ignore the checks with [get_unchecked](https://doc.rust-lang.org/stable/std/primitive.slice.html#method.get_unchecked) whenever you need to eliminate a check. Dropbox wrote about another, related strategy, [here](https://blogs.dropbox.com/tech/2016/06/lossless-compression-with-brotli/). &gt; However, many of the bounds checks cannot be elided like one of the two in the above example. To measure their effect, we made a macro `fast!((array)[index])` to toggle between the safe operator[] on slices or else the unsafe get_unchecked() method, depending on whether a --features=unsafe flag was passed to the rust-brotli build. 
&gt; there's no support yet for tail call optimization, right? There's no guaranteed TCO yet, no. LLVM may do it if the stars align.
Yes. I mean, I don't actually _like_ getters and setters, either! But I want to have a library API that can remain stable even when the implementation changes. Unfortunately, because of the way Rust is designed, putting a `pub` field in a public struct interface is a big commitment. I've been following the `docker-compose.yml` format for a couple of years now, and I know how it typically evolves: Simple string fields are replaced with complex struct fields. New fields get added. Old fields are subtly reinterpreted. Fields which never supported variable interpolation now do. The whole point of my library is to hide all these horrible details from the user, and to offer the most stable API possible. We have about half a dozen internal Ruby tools that _think_ they know how to parse `docker-compose.yml` correctly. Every one of them is wrong, and no two of them agree. I wrote the `compose_yml` library for Rust as an experiment, to see if I could bring some type-safe order to this mess. Overall, it's working great—except that it exposes way too many fields as `pub` when what it really should have is an API. Once I expose a `pub` field in a struct, I'm basically saying, "This is the way the format works. I won't ever need to validate this argument, or automatically 'upgrade' this string field to a struct" (and store the string in a subfield, like I did when `build: "foo"` became `build: { context: "foo" }`). A language like C# offers `get` and `set` methods that allow me create "fake" fields that do some processing behind the scenes. But a `pub` field in Rust is a permanent commitment. Plus I have to add all those dummy private (or hidden) fields to allow future extensibility.
Ah, good point. I was too quick to call it dumb. If anything this is a beautiful fusion of the excellent type system and the excellent borrowck
Naw, it is dumb in ways, this just isn't one of them. (Oh and I'm not sure if I was explicit enough, but all aspects of your edit are correct; it's just copying a pointer.)
I thought the whole idea of wasm was it was native instructions for the browser so it wouldn't need overhead?
To be honest, Id rather see section "Blog posts" disappear. They're linked everywhere, even before TWiR comes out.
So, following that, why do "New Crates" should disappear?
It's equivalent to let var = v.pop().unwrap(); v.push(var); Which is valid. `v` doesn't _need_ to be evaluated, it's already a value. `v.pop().unwrap()` needs to be evaluated and stored somewhere, and with MIR it can now be expressed that the above code is equivalent to the OP's and thus the original code would be accepted. `v.pop().some_mutating_fn(v.pop())` would be more problematic, however.
Awesome, bring on the (sensible, incremental, well thought out progress toward) HKTs!
You *could* do that, but like you say, there are situations where it becomes more problematic. This works today: let mut v = vec![1]; let val = v.pop().unwrap(); v.push(val + 1); let mut v = Box::new(vec![1]); let val = v.pop().unwrap(); v.push(val + 1); Do we want to get into a situation where you can do this: let mut v = vec![1]; v.push(v.pop().unwrap() + 1); But not this: let mut v = Box::new(vec![1]); v.push(v.pop().unwrap() + 1); Such non-lexical lifetimes would break smart pointers.
It's a separate issue, but you can hardly find uses of from raw parts for Vec that are actually correct. Sized deallocation and other things makes it nonintuitive to use for most, they don't realize all the constraints involved.
This statement of principle seems to rule out the sting optimization Niko floated in the rusting keynote, too.
I don't understand your reasoning. MIR essentially mechanically transforms method calls of the form a.b(expr) into let intermediate = expr; a.b(intermediate) (it does a bunch of other things too, but this is the transformation that is important to non-lexical lifetimes). Both of your examples work equally under this scheme. Or equally, if you allow `v.push(v.pop().unwrap() + 1)` to work under non-lexical lifetimes, why would `v.deref_mut().push(v.deref_mut().pop().unwrap() + 1)` not work? Since that's what the `Box` example desugars to.
That is a pretty nice macro https://github.com/tcr/rust-todomvc/blob/master/src/main.rs#L34-L41 that I haven't seen or thought about before. 
`deref_mut` and `deref` can have side effects, so wouldn't it be possible to 'see' the difference in evaluation order between `T::foo(t, T::bar(t))` and `T.foo(t.bar())`? Edit: Here is an example where the semantics of the code have changed when the MIR transformation is applied: https://play.rust-lang.org/?gist=94c2b3843c42c75ea85c148f29c4ada5&amp;version=stable&amp;backtrace=0 I'll admit the example is a bit far-fetched, but it shows how the change is breaking.
Can you pinpoint me to a SVG that gets distorted, so I can also do some testing?
Using Write for composable formatting is great. I want to attract attention to Display trait based formatting which is very compsoable. It's not perfect since it has some dynamic dispatch overhead, but working with it is very nice. One simple example is https://docs.rs/simplesvg/0.4.0/simplesvg/ where each element implements Display. Also, I'll shamelessly plug itertools' .format() method which lazily formats iterator elements.
So /u/zonyitoo and me are or rather were working on context-rs, coio-rs and so on, but the development basically halted as soon as we noticed one serious flaw: *Whatever* you do the stdlib and the exception model in Rust is currently absolutely incompatible with all suspend-down solutions (and coio-rs was using those), which was a really big bummer for us. Suspend-down coroutines are those which look like ordinary function calls but actually yield to a runtime like in Go. Suspend-down is a lot more heavy-weight than suspend-up ("async" &amp; "await") though, but opens up the very useful capability of implementing a userland scheduler in a runtime. That runtime can then shuffle coroutines between threads to optimally balance the workload between threads. This greatly reduces the risk of latency jitter etc. The only other alternative is to use one thread per job and is seemingly what [some vocal Rust-developers are aiming for](https://news.ycombinator.com/item?id=12270911) (which really really saddens me). So of you choose to use suspend-down the reason it doesn't work is that TLS addresses are cached between repeated access inside a single call frame. But since it's possible with suspend-down that a coroutine gets transferred to another thread during a yield to the runtime (for e.g. work-balancing) this can cause invalid references to the TLS of the previous thread. Now you could say: Just make yields to the runtime `#[inline(never)]` and we tried that and it worked first, but then we noticed that the Rust stdlib is actually using TLS (!) to implement exceptions, which... _really_ boggles my mind. But that's it: we can't change exceptions without changing the stdlib. You can use suspend-up coroutines like context-rs (very fast!) or coroutine-rs freely though, but developers like me, who mostly work with server systems seem to be stuck with stuff like Go, and bleaching my eyes with that syntax every day is not fun. 😐 P.S.: This is quite a sad story btw. coio-rs contains a replica of the Go scheduler and showed a very stable and good performance and would have been a good alternative to Go actually. I'll wait and hope for alternatives now though but I fear those will never come because suspend-down seems to be something the Rust core team views as ultra-low-priority.
This is the first time I've seen slog. Looks very nice! Good tips, too.
That sounds dangerously close to making Cargo into a cross-platform package manager, which is a can of worms I hope Cargo never opens. Write your software, document how to manually set it up for maximum usefulness, then let Homebrew/Debian/FreeBSD decide how they would like to automate those steps for their users. And maybe make a Windows installer, because poor Windows users don't get any love otherwise.
Many weeks there are no 'new crates' in TWIR because there aren't really suggestions. Beyond that, lately, they tend to be fairly niche. It was initially good to have because you could learn about really big crates like hyper, serde, itertools, etc. But I think as time has passed there's been recognition that the issue of visibility into the crate ecosystem is best solved elsewhere.
Well, you could express why you disagree. Someone who is involved in TWIR may be interested in your feedback.
This one comes from my OO hammer (I have a hammer so every problem needs a good hammering ;) ) if I have a struct method is it possible from within the method itself to gain a mutable reference to the structs instance, I've seen a number libs passing the struct instance as a parameter are all methods (in effect) "static" (as in the java static) I can see the borrow checker no liking a method getting a reference to the calling instance but was wondering if there was some magic I'm not yet aware of...
If I'm understanding you correctly, you take &amp;mut self or plain old self in this case, rather than &amp;self. I'm not quite sure what you mean by "static" here, if it takes some form of self, it's not like static. Can you point me to an example?
I don't have a specific file. You can create an image in Inkscape and save it with precision 16 (yes, Inkscape support such precision). I''m not saying that your implementations is wrong. I don't know. You are right according to the spec, but in the real world there are too mach files with numbers bigger than float.
I actually need both if I want to specify `into`: #[derive(setters)] #[setters(into)] The trick is that the derive code for `setters` actually looks for `#[setters(into)]`, parses it, and removes it from the AST. The relevant code is still really ugly but you can [find it here](https://github.com/emk/accessors/blob/b614791d63dad026dd6a8a989788ed61182631ec/src/lib.rs#L121-L156). I have some ideas about how to make a pretty, general-purpose API for this, but they still need some work.
I'm not quite sure I follow. Doesn't trait Foo { fn foo(&amp;self); } impl Foo for Vec&lt;f64&gt; { ... } allow me to perform the call on a reference to a vec regardless?
This is fantastic. I'd definitely like to see more posts like this one. The bulk of my experience programming is in high level languages (Java mostly) where allocations are frankly done without much of a second thought. I sometimes struggle moving away from that mindset because I'm not familiar with strategies on how to prevent this. Will read through this in more detail later tonight, but any other similar docs anybody can recommend? 
Well, I basically look through "PRs merged into Rust", "new crates", "new/fcp/approved rfcs" in that order sorted by importance _to me_. But that feedback is exactly countered by TWiR mantainers saying, that they want to move "New crates" section _out_ of TWiR. Even niche crates are interesting to me and, currently, __there is no weekly issue__ like TWiR, that covers "New crates" section. I understand, that there is a work that should be put into this section. Maybe the problem is there is noone suggesting crates to that section? Can I help with this? I, personally, do not think that the "(_new_) crate visibility" problem is solved by now.
But,It's still very hard to write complex logic as a newbie.
In rust the self parameter is written explicitly in the method signature (like Python, unlike C++), but you don't write it when you call the method.
What exactly are you looking for? There are some crates for linear algebra, special functions, root finding and lots of other things that would qualify as numerical analysis.
On the top of my head: https://github.com/indigits/scirust https://github.com/boxtown/statrs http://www.arewelearningyet.com/, http://www.arewelearningyet.com/scientific-computing/ Unfortunately AFAIK nothing as complete as GSL. But if you just need a subset of the features it may be already implemented. There is also an ongoing discussion about numeric / scientific crates for Rust: https://users.rust-lang.org/t/numerics-math-foundation/7247/22 So if you are missing some important features post them here (on reddit) or in the above thread in the Rust user forum.
The benefit of having a `&amp;'static str` as a struct member is that you can copy it eg. to other thread. However I do want all these `&amp;'static str` to become `&amp;'a str`. Unfortunately slog mainstans good level of backward compatibility with more established `log`crate, which demands `&amp;'static str`: https://github.com/rust-lang-nursery/log/blob/master/src/lib.rs#L575 I was actually asking could it be changed: https://github.com/rust-lang-nursery/log/issues/94
Scanned the thread but didn't find this bit. Was there a compelling reason to offer such a guarantee? I personally wouldn't expect interior pointers to remain valid after a move.
I'm currently researching numerical analysis, and I was wondering if there were any scientific computing crates to which I could contribute. So, I'm not looking for any numerical methods in particular, since I simply seek to help flesh out Rust's scientific computing libraries. You make a good point below about modularity; I'll look into some of the crates specific to special functions. Thank you!
This is a very nice little post regarding micro-optimization (in particular I didn't know that `perf` could be used to track cache misses!). My one criticism is that the first paragraph doesn't immediately make it clear what the project is supposed to be. I assumed it was some sort of noise generation library, but the Github page says it is "a lazy, zero-allocation and data-agnostic Information Retrieval Library", which is still a bit cryptic ("information retrieval", so like a database?). https://github.com/JDemler/perlin
I imagine whatever Rust uses (especially in the beginning) will be tailored, design wise, strictly to reduce FFI access. Since everyone seems to believe that to be the bottleneck currently. Benchmarks will of course be all that matters, but yea. My hunch is that the react model will be best. Eg, two main things: 1. Load up all your data/modifications in batches on the Rust side 2. Pass the data to a react-like library on the JS side, which will do the actual JS handling.
Make Rust Trait Again.
Don't know if it's the case here, but sometimes you can move validation into the method(s) that actually do something with the data.
&gt; ("information retrieval", so like a database?) Kinda. A database's primary purpose is to store and retrieve "stuff" where "stuff" can be anything. An information retrieval engine's primary purpose is to store text and provide a way to search that text such that the results returned are the ones *most relevant* to your query. A typical IR query fetches candidates, ranks them and provides the best N results. The scales are large enough here that you don't often see databases and IR engines using the same underlying techniques, although there is certainly some similarity and even some exceptions. For example, postgres has fulltext search capability, but its relevance ranking is quite primitive compared to, say, Lucene's. (It's essentially TF without IDF in [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).) There's another Rust IR engine in the works too: https://crates.io/crates/tantivy
If we get something like [`DerefPure`](https://github.com/rust-lang/rfcs/pull/1646) (which would be an `unsafe trait` and promise to have no side effects) then that could alleviate this problem (and enable other nice things, like pattern matching through smart pointers).
What is a simple way to *steal* the pointer from a `Vec`? Effectively I want the pointer from a vector to outlive the `Vec`. Yes I'm aware this is unsafe. I'm passing an array from Rust to C via the FFI. And building with array with Rust's `Vec` is a lot easier then making a ton of `unsafe{ write() }` calls.
Use `vec.into_boxed_slice().into_raw()` to leak the `Vec` and get a raw pointer to its contents. Note: To safely de-allocate the contents later, you will need to use `Box::from_raw`.
Leaking memory isn't actually unsafe, see [here](https://huonw.github.io/blog/2016/04/memory-leaks-are-memory-safe/). TL;DR: You can leak memory in safe code by creating rc cycles, this is difficult to stop without paying a heafty price, so making leaking memory safe was chosen and abstractions using destructors to enforce safety of unsafe code were revisited.
Dozens (likely hundreds in near future) of new crates are published every week, and it's quite hard to decide on which to pick. We can only select few, so we must have some guidelines in place. One way to do it to let community submit proposals and vote on them. We already do that and chose to call the section "Crate(s) of the Week". I'm open to suggestions.
There are some transformations happening with the plugin right now, the plugin was renamed to "Rust Enhanced" maybe couple of days ago. The "Rust" package built into Sublime is a snapshot of rust-lang/sublime-rust from several months ago. I used this plugin for last couple of years and implemented significant part of it, but now I have no idea what's happening and what are the plans. I'd suggest to open an issue and ask all the questions there - the maintainer is quite active now.
The correct syntax should be: `Box::into_raw(vec.into_boxed_slice()) as *const bpf_insn`. Note this resizes the allocation to make the boxed slice. Instead you can do: let (p, len, cap) = (vec.as_ptr(), vec.len(), vec.capacity()); mem::forget(vec); // later drop(unsafe { Vec::from_raw_parts(p, len, cap) }); 
https://crates.io/crates/webplatform :)
Middle button - drag will also give multiple carets.
aaah right.... clang.... (that was a large penny dropping...) thanks
I think statically linked binaries are usually smaller.
I installed Sublime Text 3 which comes with Rust support built-in. Didn't try the Rust plugin with Sublime Text, sorry. :(
It will still need to interact with JavaScript at some point 
Will it? I thought the idea was that it would replace JavaScript and all the equivalent IO and DOM access would be possible?
&gt; pub struct Record&lt;'a&gt; { &gt; meta: &amp;'a RecordStatic&lt;'a&gt;, &gt; msg: fmt::Arguments&lt;'a&gt;, &gt; values: &amp;'a [BorrowedKeyValue&lt;'a&gt;], &gt; } Can you defer formatting to another thread, with this implementation? It seems to me that since you rely on borrowing, you necessarily mandate that formatting must occur on the same thread, and synchronously, since the arguments might otherwise die or change value. For low-latency systems, the only solution I know of is to copy the arguments into the `Record` (along with the log format), and ship that off to another thread. Obviously, said arguments better be small enough (in C++, `static_assert` helps ensuring that). In Rust, I wonder what possibilities there would be. Keeping in mind the explicit nature of Rust, maybe a dual set of macros (one `sync`, for when an argument cannot be copied over, and one `async`, for when all arguments can be copied over) could be interesting? The user would be encouraged to use the default (`async`) set of macros by default, and the `sync` variant would be provided when an argument requires synchronous formatting.
I use your advice. Checkout the update in the OP.
I believe this is the issue you are looking for: [std::thread::JoinGuard (and scoped) are unsound because of reference cycles](https://github.com/rust-lang/rust/issues/24292)
You can also use the [thread-scoped](https://crates.io/crates/thread-scoped) and [scoped_threadpool](https://crates.io/crates/scoped_threadpool) crates.
Wow! That's really neat. I'm not sure if it isn't too magical for my tastes, but given that the introduction of non-lexical lifetimes would presumably just allow you to erase the macro invocation, it's not so bad. The source code is really interesting -- I'd never thought to do that sort of lisp-style recursion on vararg macro arguments to process them. I've always just used the same `$()*` expansion syntax and been frustrated by its limitations. That use of the `@` sigil -- is that just being interpreted literally, and the use of an uncommon character is just to help hide it? That's really clever. I didn't even know `@` was legal like that, but it seems like a nifty convention for "private" macro patterns.
I should probably include the caveat about deref-with-side-effects that was mentioned elsewhere in this thread. It is really unlikely to matter though. Yep, `@` used to be much more common in Rust syntax, when it was a pointer type (sorta like `RefCell` I think), but that got removed. Now it's only used in match pattern syntax, and so it's often used like this to denote internal macro rules.
Thanks! This is quite the rabbit hole. Seems it basically is just a matter of tightening the scope of borrow regions, and that implementing it is indeed more complicated than "just tighten the scope of borrow regions", as it always is with software design, if I understand correctly.
I prefer [scoped-pool](https://crates.io/crates/scoped-pool). Its better than thread-scoped because it doesn't require `unsafe`. Its better than `scoped_threadpool` because it has less life limetime paramaters and has some neat methods on `Scope` to make it easier to pass the scope around.
How do other (non-Rust) programs that copy behave?
Nice writeup - I like the use of the playground, I'm going to give that a shot next time I write about rust. I think state machines are a hugely useful pattern for writing safe code - SMACK attacks against TLS are all state machine related, as an example. Seems like a great place for rust. &gt; The memory required to represent the state machine is only the size of the largest state. This is because a fat enum is only as big as its biggest variant. + 1 byte for the tag, unless the null optimization takes effect, which I don't think it would.
Do all the various crates operate on a standard set of base types or traits? 
The last section about an enum wrapper is a solution to this. You could also write a common trait that has some method to downcast to a state type and store a trait object in the vector. I think the former is generally more type safe, convenient, and generally faster. 
Oh my god I cannot thank you enough for this. I have been working on a lexer and the state machine is getting very complex and subtle bugs are popping up. I have been so dreading refactoring because I haven't thought of a solid abstraction to make the state transitions safe. Now I am excited to go and refactor! 
Thanks for the response! I *would* do that, but I'm trying to benchmark some private parts of the library, which is why I tried to include them in the library itself. Any ideas? It might be that I just have to include a copy of the implementation that I want to benchmark against in the library repository. I could feature-gate it to prevent normal compilation. That would work, but wouldn't be ideal/convenient.
&gt; I would do that, but I'm trying to benchmark some private parts of the library, which is why I tried to include them in the library itself. Any ideas? There's fundamentally no way to do what you want as you've specified because of the dependency cycle. Your question (I think) boils down to, "How can I cause crate A to depend on crate B which depends on crate A?" An unsatisfying answer to your problem is to actually export the things you want to benchmark/test, but mark them with `#[doc(hidden)]` so that they aren't part of the public API. (Sadly, the `regex` crate actually does just this, for $reasons, so you'd be in good company.) Another option is to put the benchmark in the implementation crate, although I'm assuming there's a reason why you can't do that. &gt; It might be that I just have to include a copy of the implementation that I want to benchmark against in the library repository. I could feature-gate it to prevent normal compilation. That would work, but wouldn't be ideal/convenient. Well, you don't need to feature gate it. You can just put it behind a `#[cfg(test)]` so that it only compiles when running tests.
Glad to see statrs up there :) The focus is statistical utilities so the scope is a lot more narrow than GSL but I'm hoping to relieve at least a small pain point for rust scientific computing
`&amp;Vec&lt;T&gt;` implements `IntoIterator&lt;&amp;T&gt;`. The implementer in this case is `&amp;Vec`, _not_ `Vec`.
Ah I see what you're driving at. Appreciate you being so patient with me
You should be using [version 1.12.0](https://www.rust-lang.org/downloads.html) right now---otherwise you are in the wrong subreddit, try again at /r/playrust :)
Wow the wrapper was exactly what I was missing
That's good stuff, thanks for sharing! I think you meant to respond to /u/JDemler though :-)
I'm working something like that with https://github.com/tmzt/incrust rendering the page on the server and instancing on the browser, I'm generating the js with an ast tranform approach rather than an arraybuffer vm like enscripten. I like your thinking though, it would really cool to render a serialized vdom from react-enscripten and render in pure js, I don't know how it would compare performance wise with my approach. 
I'll admit I'm not really up on all the IR stuff, though the book looks like a good resource. What I'm wondering is if there is a good redis-like engine for full text searching, document indexing, and information retrieval built in a native language, or if these libraries would work well in such an engine. By redis-like I mean having a simplified api, exposing some of the primitives or low level features not a complex dsl like elastic or lucene.
Take a look at the implementation of `fs::copy` on unix. pub fn copy(from: &amp;Path, to: &amp;Path) -&gt; io::Result&lt;u64&gt; { use fs::{File, set_permissions}; if !from.is_file() { return Err(Error::new(ErrorKind::InvalidInput, "the source path is not an existing regular file")) } let mut reader = File::open(from)?; let mut writer = File::create(to)?; let perm = reader.metadata()?.permissions(); let ret = io::copy(&amp;mut reader, &amp;mut writer)?; set_permissions(to, perm)?; Ok(ret) } Observe how it manually tries to copy permissions over. Because it sets the permissions _after_ copying the file, this means that the file can be successfully copied yet still return an error due to the inability to assign permissions. NTFS does not have chmod permissions, therefore attempting to set chmod permissions on a samba share will fail. For comparison on Windows we simply call `CopyFileExW` and that handles everything for us.
Hey! You're totally right. :) I'll update that.
Yes you totally could! I didn't include that for space reasons. Something like `StateMachine&lt;S: State&gt;` would probably be better in "bigger" implementations.
Ohh I see, well that settled it then haha!
I'm a total rust n00b, but I'm making such slow progress :( Now stuck on a new error which doesn't make sense considering I'm using a `Box` so it should be `Sized`: error[E0277]: the trait bound `futures::Future&lt;Item=std::string::String, Error=tokio_curl::PerformError&gt; + Send: std::marker::Sized` is not satisfied --&gt; src/lib.rs:84:34 | 84 | let (r1, r2) = lp.run(s1.join(s2)).unwrap(); | ^^^^ | = note: `futures::Future&lt;Item=std::string::String, Error=tokio_curl::PerformError&gt; + Send` does not have a constant size known at compile-time error[E0277]: the trait bound `futures::Future&lt;Item=std::string::String, Error=tokio_curl::PerformError&gt; + Send: std::marker::Sized` is not satisfied --&gt; src/lib.rs:84:27 | 84 | let (r1, r2) = lp.run(s1.join(s2)).unwrap(); | ^^^ | = note: `futures::Future&lt;Item=std::string::String, Error=tokio_curl::PerformError&gt; + Send` does not have a constant size known at compile-time = note: required because of the requirements on the impl of `futures::Future` for `futures::Join&lt;futures::Future&lt;Item=std::string::String, Error=tokio_curl::PerformError&gt; + Send, Box&lt;futures::Future&lt;Item=std::string::String, Error=tokio_curl::PerformError&gt; + Send&gt;&gt;` error: aborting due to 2 previous errors https://play.rust-lang.org/?gist=e44a30012cc5a78d2d764aa35f24499e&amp;version=stable&amp;backtrace=0
`Box&lt;Future&gt;` might be useful, but in many cases you can get away with an enum that implements `Future`. Disclaimer: I haven't played with this much so there may be a much better way to do it. 
Great work! This is a good example of *mature* optimization! While optimizing the used algorithms (for better code gen and incr comp) will make usual compilation a great deal faster, the kinds of optimizations you describe here are important as well -- and even improve the performance of std and thus user code. Awesome! Btw, you mention that malloc/free take up a bunch of time *and* that you use stage1: Is this faster with jemalloc? Also note that IIRC using codegen-units prevents some inlining.
One small thing: Don't clone one git repository, especially a big one, twice in the row from an online URL. You can just git clone https://github.com/$user/rust rust0 git clone rust0 rust1
I'm currently trying to write an event loop built on `futures`. [This is how I currently view the crate](https://c2.staticflickr.com/6/5205/5232013153_7808b471a2_b.jpg). I mean, I've written a coroutine implementation *and* a UI-focused coroutine framework in two other languages in the past. It's not like I haven't done this before. I really, *really* wish the docs explained what the various components did *in context.* I mean, I *think* I've seen something like four different ways to "wake" a task, all of which appear to do radically different things, and I still have little to no idea what the *actual* differences are. It doesn't help that every time I try to trace the execution of anything from `tokio`, it just bounces around back and forth through the layers of abstraction and multiple crates, to say nothing of the damn global variables that make it exponentially harder to tell what's going on. It feels like there's nothing in there that you can understand in isolation: there are no leaves to the abstraction tree, just a cyclic graph that keeps branching. I *still* haven't worked out what `task::park` does. It doesn't park *anything!* It clones a handle! No, wait, it clones an handle, *and* a `Vec` of events, but I'm not sure what those even do, and why does it just snapshot them? What happens if someone calls `park` twice? Does it explode? Does it collapse into a black hole? Does my computer turn into a reindeer and ride away on a rainbow? Is `park` only compatible with *some* of the waking methods? All of them? Do I *have* to use `park` for `task_local!` to work? &amp;nbsp; ... I don't have anything useful to add: just letting you know you aren't the only one confused :P
LOL, thanks for the reply and that's generally my thought when I'm reading the documentation. Now I feel like in a team (who cannot understand mozilla docs), which helps a lot....
What do you mean with "my own heap"? Did you allocate n bytes using rust's internals and then wrote your own malloc that allocates within this space? Or do you mean that you split the work into multiple threads + their own heaps? Unrolled linked lists are a good idea I think. I want to combine them with compressed postings and inlined positions though :)
The stage2 compiler is faster than the stage1 compiler, sometimes significantly. I don't know how much of this is because of jemalloc, and how much is because the stage1 compiler is built with an older compiler that generates worse code. Having said that, IME the improvements that reduce malloc counts tend to speed up the stage2 compiler almost as much as the stage1 compiler.
`.join(foo).join(bar)....` perhaps ? You might need to box it to do it in a loop. I'm not really familiar with this API like I said.
Yup. But I'd argue that usually when you copy / clone a repo, you want a clean working directory without any of the local changes. So while it won't make a difference here, it's just generally good to know you can clone locally IMHO.
I allocated a huge Vec&lt;u8&gt;, and I use u32 as addresses in it. Malloc(some_size) is just maintaining a value with the number of bytes allocated so far, incrementing with the some_size and returning the old value. 
That's right. On strace, all read/writes were fine, but in the end there's this: chmod("/mnt/irony-e/hohoho/copytest.txt", 0775) = -1 EPERM (Operation not permitted)
That's the case here. I guess I'll stay on my copy-implementation without setting permissions. Thanks!
Thanks for taking time to write up all of this thinking. The use of `From`/`Into` is really neat. It's such a shame that all of the states are not described in a single place - the `Enum` option looks so neat and tidy.
&gt; But polling a future will block the thread. Polling a future should never block the task, the implementation is considered bad if it does.
Sorry, I mean polling a future on an event loop. You're right as polling the future will just return the current state.
I like the way you did it. I am working on a fairly complex FSM myself currently and did it slightly different. Some things I did different: * I also modeled the input for the state machine. That way you can model your transitions as a match over (State, Event) every invalid combination is handled by the 'default' pattern * Instead of using panic for invalid transitions I used a Failure state, So every invalid combination transitions to that Failure state [Example](https://play.rust-lang.org/?gist=ee3e4df093c136ced7b394dc7ffb78e1&amp;version=stable&amp;backtrace=0) 
Hey this is a cool strategy! I'm going to add a [link to this](https://hoverbear.org/2016/10/12/rust-state-machine-pattern/#alternatives-from-feedback) into my post! Thanks!
Thanks for taking the time to write that up for everyone! Quite a beast of a task.
IIRC, doing a clone will hard-link object content, likely saving space.
I feel similarly. I've only glanced over the API, but there's some things which feel off to me: for example, the `Future` trait entails `Item` and... `Error`? Wait, what? And I recall acrichton saying in his youtube speech "And if an error case isn't necessary for your future, we can just optimize that away!" That's... very backwards logic. Why not just have `Future&lt;T&gt;` and if I need an error case, then I'll use `T U Err` for the parameter? It's not merely a matter of performance of the resulting output; it's a matter of the semantics I intend to express when I write the code in the first place! As far as I can see, that forced `Error` on the trait sounds like mixing abstraction layers. It's like defining `(&gt;&gt;=) : IO a -&gt; (a -&gt; IO (Maybe b)) -&gt; IO (Maybe b)` in Haskell, which besides looking ugly, suddenly loses all sorts of nice mathematical properties which allow you to reason fluently about your code. So yeah, I'm skeptical of what I understand of it, which makes me doubly skeptical of the parts I don't understand.
I have a full implementation of that idea here: http://laze.rs/lazers-replicator/src/verify_peers/ (this is one part of the replication protocol of CouchDB). In general, single-sized structs and generics as markers have the drawback that you cannot replace the state of a machine in memory without involving the user.
Nice work! Also TIL about the DHAT profiler, I was not aware of this particular valgrind feature. This will be immensely useful.
Have you see this? http://www.ishbir.com/post/2016-08-14-futures-in-rust/ Not sure of the soundness of the approach but it solves your problem I think.
The programming language Rust has always been free! For the game please ask in /r/playrust! :)
Oh crap! Sorry!
Setting permissions after the copy looks really dubious to me. That way somebody might open the file before it finishes copying while it still has more permissive settings. (Also WTF at Linux not shipping a file copying function with the OS, forcing each programming language to re-implement it)
&gt; In an ideal situation we’d be able to make enums with restricted transitions between variants, but that’s not the case. I haven't written many Rust macros but do you think this could be implemented with them?
Well, there's still `--reference`, which will stop it from downloading objects you have locally.
Oh, that's interesting, I think I saw that option before but didn't understand what it did. Neat!
I don't think there are many hot `HashMap`s in rustc that don't use FNV
I would really love to see more documentation and examples on futures.rs and tokio. I wanted to switch EdgeDNS to using these instead of raw mio, but eventually gave up due to the lack of documentation.
Cool! Will have a look this weekend :)
A Question about Crate Attributes. In reading http://stackoverflow.com/questions/27454761/what-is-a-crate-attribute-and-where-do-i-add-it I know to put my crate attributes at the "entry" point of my code. If I have a "project" with multiple crates (main.rs, lib.rs) - must I put the same attributes in both the main.rs AND the lib.rs? 
Yea that's my question. This looks neat, but honestly i'd never do it unless: A. Rust became so slow i have no choice, or: B. It was added to Rustup to do these compile steps for me (or i could write a little script to automate this, i suppose)
Why do that at all? Just make a branch.
Probably because you don't want to switch branches + rebuild the original compiler to do the comparison benchmarks. You need two working directories.
Rust doesn't allow calling trait methods directly, you must have some kind of type that implements that trait first. It's not that it would be unsound to do so, it's just that you'd have to special-case this to be allowed _just_ for trait methods that have default implementations and that don't use `self` at all. There's really no reason to put the time and effort in to implement this, since you could just extract it out to a free function. Also, the standard for Rust types is CamelCase, so you would call your struct TheStruct in this case.
There's a lot that's wrong with this code, and without a longer example it's difficult to see what you're trying to do. However: - The trait has not been implemented for any types, so will not be usable - The method implementation is just a "default" implementation, you don't normally provide method implementations when defining a trait unless you specifically need this defaulting behaviour. - The explicit way to call a trait method is: `&lt;Type as Trait&gt;::method()` - the syntax you're using `Trait::method()` can only be used when `Type` can be inferred from the context (and this type *must* implement the trait). For this short example, you don't need a trait at all: https://is.gd/tq0pFU
Hm yeah, might work.
Hashing pointers as in literally pointer values, a `HashMap&lt;*const, T&gt;`? Maybe a dedicated integer hash (Thomas Wang's, or at least Knuth's multiplicative) would be a good idea for these?
I've done a lot of benchmarking of `HashSet` and `HashMap` in Rust and the performance sucks unless you have a really well tuned hash function which balances random distribution (because the pathological worst case behaviour of the data structure is awful) against the speed of the hash function itself (which gets called *a lot*). Even FNV is way too slow, often running several times slower than, for example, a .NET `Dictionary`. &gt; malloc and free are still the two hottest functions in most benchmarks. Avoiding heap allocations can be a win. However, my gut feel is that the performance of a compiler would be vastly better with a classic generational garbage collector. The fact that people are talking in this thread about optimising `malloc` would seem to back this up. &gt; it’s surprisingly common in large programs to create HashMaps that are never used And this indicates to me that a `HashMap` is the wrong tool for the job. If most collections are small or empty then a dense tree would be more efficient than a sparse hash table. 
Rust code already is never pure, so you didn't have those properties to begin with. (And I mean purity in general here not the `pure` function)
&gt; Rust code already is never pure That is not an accurate statement at all. Rust cannot *enforce* purity, but that does not mean that there are no pure functions. `+` is pure, `*` is pure, `id` is pure, etc. Just because Rust cannot enforce a property doesn't mean it doesn't exist or isn't true. Haskell isn't smart enough to actually understand and verify the Functor laws, either, but that doesn't mean there are no valid Functors in Haskell!
SipHash::write and finalize are still showing at the top of the profile. So..
No, you should just use `git worktree`! They then share a `.git` folder (and thus share commits, upstream, etc), and are just different workdirs.
Murmur3 and XXHash finalizers are probably (one of) the best hashers for arbitrary numeric inputs.
Huh. We do use them for the symbol hash, maybe moving to something like C++ name mangling is a better idea. Although I wouldn't expect it to be *faster*, that one is cacheable.
Perfect, thank you. :)
So far's I know, neither function is for numeric inputs, they're high-throughput bytes hashes. Wang's hash has roughly the same number of operations as murmur3's *finisher*.
Now now, don't be so negative. Be pleasantly surprised that half the comments are on-topic! :-)
But `futures::collect` is not running in parallel. That's why I really nead a join...
By that logic, Haskell devs can't use Functors or Semigroups or Monads since their compiler can't verify those, either.
True, but what I'm saying is a bit different. Since the input has a known size (intX) you just have to mix the bits, and the finalizers (the last step) of those hashers do an amazing job on that.
Well, this is where things get more subtle. In this situation, you have two choices: 1. Not rely on it because well, you can't be sure. 2. Rely on it and say "it's your fault if you don't uphold the invariants in your implementation." In my understanding, Haskell mostly chooses #2. This is also how Rust deals with `unsafe` code. I think this choice makes a lot of sense for Haskell, as the people who use it are going to know to satisfy these properties, and the properties overall make sense, that is, the majority of the time, you'd just do it correctly already. However, the situation with Rust and purity is very different: while Haskell has a culture of understanding the monad (etc) laws, Rust culture very much doesn't care about purity at all. So much so that we removed it from the language entirely! So while the chances of taking a random `FooM` package off of Hackage and having it obey the monad laws are high, the chances of taking a random `foo()` function in Rust and having it be pure is very low.
Not sure what you mean, I guess.
You're welcome! And if you're interested in Rust, you're welcome here too!
Writing an event loop is going to require advanced knowledge no matter what. Writing custom event loops is also not something that is being focused on documentation wise. futures-rs &amp; Tokio are both very ambitious libraries and we (alex, aaron, and I) are doing what we can on that front. Given that, `futures::task` is not really polished. It's only at the stage necessary to get Tokio &amp; cpu-pool working.
The improvements mentioned in the article are all in Rust nightly builds already. You don't need to compile Rust yourself to benefit from them; you can just download the nightly toolchain. (The instructions for compiling and profiling are for people who want to contribute to the speed-up efforts.)
Some of nnethercote's improvements are in Rust 1.13, which will be released to the stable channel on November 10. And some are in Rust 1.14, which will be released on December 22.
The futures-rs library introduces some new ideas around futures that (as far as I know) have not been done in a future library before. The design decisions were made to enable **much** greater performance than the traditional approach taken with a futures library. If you come to the library expecting it to behave like futures/promise libraries in node.js, you are going to end up being confused. That being said, yes, the state of docs is lacking. This is a known issue. The libraries are all very young and are still changing a lot (though the rate has just recently started to slow). Docs will follow, but the authors only have so much time on their hands. As for the specific question in your post "But polling a future will block the thread". `Future::poll` should never block the thread. As documented on the function: https://docs.rs/futures/0.1.2/futures/trait.Future.html#tymethod.poll &gt; Returning quickly prevents unnecessarily clogging up threads and/or event loops while a poll function call, for example, takes up compute resources to perform some expensive computation. If it is known ahead of time that a call to poll may end up taking awhile, the work should be offloaded to a thread pool (or something similar) to ensure that poll can return quickly. I think the rest of your confusion may fall out of this.
Note that Vec&lt;Vec&lt;_&gt;&gt; is pretty inefficient, as each column is a different allocation. Use a Vec&lt;_&gt; that's x\*y long, and index with y\*width+x.
I might give it a shot when I have time. Having a clean and quick way to declare these state machines would be awesome.
I believe what you want is this: trait the_trait { // trait methods here can only be called when you have a type // implementing the trait } impl the_trait { // an impl block can be used to add inherent methods to a trait, // and these can be called directly (independently of the type // implementing the trait) fn new() -&gt; the_struct { the_struct {} } }
You are right. Polling a future should return its current state. It's not blocking. Future is defined as _its value not available now, but some time later_. So I think I have to continuously poll the future to get its value when available ( the only way I come up with is a loop). As that loop could block the thread, I have to set up another thread to run the event loop. Is there a better way or a common pattern or something? I hope this time my confusion is expressed better....
Sorry for the delay. Here you go: [script](http://pastie.org/10943132), [benchmark driver](http://pastie.org/10943133)
The way you do this currently is with `tokio_core`: that is, running futures are placed in an event loop. You select on your future and https://tokio-rs.github.io/tokio-core/tokio_core/reactor/struct.Timeout.html At least, that's my understanding.
In other words, [do what DMD does](http://www.drdobbs.com/cpp/increasing-compiler-speed-by-over-75/240158941).
I use geany, too, but I wouldn't compare it to sublime. The autocomplete is IMHO worse than in vim, the editing functionality is basic at best (no multi-cursor, slow seek/replace compared to sublime), and there's no jump-to-error.
&gt; What happens if the thread producing the future value panics? That's similar to contending that `+` should return `Option&lt;i32&gt;` because the universe might knock an electron out of orbit somewhere. That may happen, yes, but when it does, the computer is *no longer running the program you wrote*. &gt; Future is an async Result And it shouldn't be! Making it a `Result&lt;T&gt;` instead of a `T` means you have no "trivial" element. If I already have a `x:i32`, why can't I put it into `Future&lt;i32&gt;`? But that's silly, you say. Why would you ever want to do that? Well, for the same reason you want a `0` in your number system: because of its algebraic properties. But I don't give a F*ture about algebra! I write real software! Indeed. Watch: I have `getProfile : UserId -&gt; Future&lt;Maybe Profile&gt;` and `someIds : [UserId]`. Because in my world, `Future&lt;T&gt;` has `pure : T -&gt; Future&lt;T&gt;`, that means it's an Applicative! And `[T]` is a `Traversable`, and any time I have a `Traversable` and an `Applicative` I can use `traverse`. `traverse(getProfile,someIds)` has type `Future&lt;[Maybe Profile]&gt;`. Without that `Applicative` instance, I can't use `traverse`, which means I have to do god knows what to get those profiles together. That's why algebraic properties are *useful*. EDIT: fixed `sequence` / `traverse` mistake
https://crates.io/crates/webplatform
Proper way to do it is like this: struct TheStruct { } // Traits only define signatures trait TheTrait { fn new_struct() -&gt; TheStruct; } // implement the trait for your struct impl TheTrait for TheStruct { fn new_struct() -&gt; TheStruct { TheStruct {} } } fn main() { let v = TheStruct::new_struct(); } Limitation in Rust is that you are unable to specify generic return types in the function signatures of Traits. FWIW this limitation has prevented me from doing anything serious with the language. Others just deal with it though.
&gt; Rust may not often explicitly think about "purity" in those terms, and &gt; we just found that most of the benefits of it can be gained in other ways I think we're in violent agreement here. This is what I mean: we don't have purity. We do have a lot of other guarantees, and those are useful, and they're sorta similar to purity in ways. But they're not the same thing, and so require a different way of reasoning about code. &gt; I've certainly found that libraries are cleaner when concerns are separated: e.g. data processing in dedicated functions with IO in a glue layer above Agreed, for sure. I'd argue that's less about purity and more about general effects, though.
The Haskell `IO` monad does encode an error case, though. It works exactly the same as `Future` does, in that sense, but it's less flexible since it hardcodes `IO::Error` for the error case ([docs here](https://hackage.haskell.org/package/base-4.5.0.0/docs/System-IO.html)). In fact, out of the two, `futures-rs` is the one that is actually able to incode an infallible result (via the type `Future&lt;T, !&gt;` (`!` is the unstable `Never` type).
Believe me, Haskell's IO errors have caused far more of a storm than I'll ever manage to cause here :)
What's wrong with `Future&lt;Item=T, Error=!&gt;` for your can't-fail case?
Oh ok, for some reason I had it in my head that it was yielding a `Future&lt;Result&lt;T&gt;&gt;`. If that's not the case, then my only contention is that the `Error` is unnecessary, but it doesn't break any important properties or directly impede anything, so it's not a big deal.
Okaay. So next someone is going to tell me that I can somehow do all this in a single clone command, or what? git is seriously feature-packed! :D
Just indent each line of your code by four spaces.
`futures-rs` and libraries on top of it is not ready yet for mass consumption. I've been investigating myself, and while I understand more or less what is going on, I am constantly confused and don't know what to do next. It will take a while to iron-out confusing bits, write good documentation, and educate enough users. But it seems to me it's totally worth it, and the performance and flexibility it gives is going to be spectacular. So for now, I'd say: if you can spare 10% of performance to get convininace try `mioco` (I'm the author, BTW). If not: go with `mio` directly - `mio` is quite easy to understand and work with, and the initial boilerplate is not that big.
I'm new to programming and rust/cargo and used the following commands to build a new project, now I'm having trouble running my executable. $ cargo new 1 --name I wrote my program in the resulting src/lib.rs that was created, and compiled using cargo build. When I run cargo run I get the following error error: a bin target must be available for 'cargo run' Judging from man, I need to use: $ cargo run --bin name What do I use for name?
That's a better situation than I thought the language was in. I wonder...are you still able to do this if your i16 is of type T?
It's partially what /u/RustMeUp is saying, but it's also that nobody has put the effort into determining if this really is the API that we want to have forever. That's why you haven't seen much discussion, there just... hasn't been any. It is true that one path towards this might be custom harnesses, and then we'd never need to make a commitment here. Part of the reason why is that there isn't a ton of pressure to do so: with `rustup`, you can `rustup run nightly cargo bench`, and given that `benches/` are a separate directory, relying on nightly for just that while keeping the rest of your code stable isn't the largest deal. That said, I would like to see a stable path for them eventually, personally.
I'm going to try hashing multiple bytes at a time on Monday.
That's a very good point. Having everything modular allows for much more expansion as the system grows up. [edit: typo]
Is that the same as casting from a concrete type to a trait object? I have been trying to get back into rust the last couple days but still am a tad shaky on the details.
I'm pretty sure I used cargo new correctly, I just didn't look at the usage when I posted my original comment. My Cargo.toml looks like this: [package] name = "generic_name" version = "0.1.0" authors = ["Swasly"] [dependencies]
You inspired me to make a [set of unit tests](https://github.com/djzin/chrono-tz/blob/master/tests/numberphile.rs) covering the points made in the video. With my local patched copy of zoneinfo_parse, 5 are failing - the julian_to_gregorian ones, the mar 25th one and a couple of leap second ones. But then, I don't think pytz handles any of those cases correctly either!
Cargo currently creates libraries by default. That's why you got a `lib.rs`. You probably wanted an executable, which would have a `main.rs`, and you get that with `--bin`. $ cargo new --bin hello Created binary (application) `hello` project $ cd hello $ cargo run Compiling hello v0.1.0 (file:///../hello) Finished debug [unoptimized + debuginfo] target(s) in 3.75 secs Running `target/debug/hello` Hello, world!
Yes. From [the documentation of `Future::join`](http://alexcrichton.com/futures-rs/futures/trait.Future.html#method.join): &gt; If either future is canceled or panics, the other is canceled and the original error is propagated upwards. This is a useful behavior and probably justifies the design decision, given that you can just supply an uninhabited type for the error if you absolutely need to algebraically express its impossibility.
As far as I can tell, in the ~1.4 years since Rust 1.0 there haven't been any serious attempts at making a new benchmark library. That suggests that the existing one is good enough. Why not embrace `#[bench]` as working rather than hold out hope for something better? I'm really glad the built-in `#[test]` framework is on stable. It works pretty well too. I could imagine there being a "more perfect" test harness, but the built-in works well enough that I haven't bothered looking. I feel similarly about `#[bench]` other than having to tell people that they need to be managing their rustc install via `rustup` and using a different command to run benchmarks. Long story short - let's just mark it as stable! If we don't know what's wrong with it after using it so much, it's not that bad.
The Arc should be destroyed appropriately because the only things holding a reference to it are the closures owned by the value you return. Why do you need atomicity and a mutex, though? I don't see any multithreading here.
There's a big difference between "maybe someday someone will dream up something better" and "we haven't put in the work to determine if this is even what we want." The consequences for stabilizing too early here are very high, and the rewards for doing so are very low. That's a bad tradeoff, imho. That said, if you think it's good and want to get it on stable, what that takes is doing the work of writing up the RFC! Like I said, there hasn't been a champion yet. That champion could be you.
&gt; This is a useful behavior and probably justifies the design decision Say I have an emergency alert system setup so that when some emergency happens, I send out an emergency instructions video to hundreds of base stations around the country. I do this with my `sendEmergencyVideo : BaseID -&gt; Future&lt;(),Error&gt;`. I take my `bases : [BaseID]` and traverse sending emergency video to them. In the Haskell world, the `sendEmergencyVideo` would be executed for each `BaseID`, and I'd get back a `Future&lt;[() U Error]&gt;`, and I could iterate through the results to see if anyone had problems and maybe try to resend it to them. In the futures-rs world, if *any one* of my hundreds of base stations isn't available and immediately yields a connection error, all my other transmissions are cancelled! Ohno!
`i32` would never have that method, it would be on reading from the file. [and that does have an error case, with Result](https://doc.rust-lang.org/stable/std/io/trait.Read.html#tymethod.read).
&gt; I'm specifically referring to using a bump pointer allocator and a no-op free. I'm going to try and test this out to see what kind of a difference (memory, time) it makes. I am a bit worried about: &gt; I suppose rustc can't do that globally, because parts of the compiler unavoidably allocate short-lived objects (the optimizer, as it compares strategies, and borrowck, as it computes path information that doesn't get used by any later passes, come to mind). but the compiler seems to use Arenas in many places where this is the case. Either way, it's worth trying out, and it looks like it shouldn't be that hard (we shall see).
`some_future.then(Ok)` will turn a `Future&lt;T, E&gt;` into a `Future&lt;Result&lt;T, E&gt;, !&gt;`. I think it's what you want.
I had not found that page very enlightening. Working with tokio, I don't seem to need to be aware of tasks at all, which makes the primacy futures-rs gives them confusing. If they're an implementation detail relevant only to people implementing core event loops, that could be made clearer.
You can use [bencher](https://bluss.github.io/bencher/) to run the same benchmarks on stable, but it's entirely rudimentary.
I would be more tempted to say let's mark it deprecated. Nothing new will come out of it while it's available to use, so we need to drive people away from it. Rust's maturity should soon drive people to need benchmarks on stable though. That's what I needed, so I use a rudimentary port of the crate to stable.
It works, it's just kind of ugly. `Future` effectively has a `Result` type baked into it and in the cases where you don't actually want that you have to awkwardly disable the unused case. It's debatable whether that's better than writing `Future&lt;Result&lt;T, E&gt;&gt;` for failure-prone IO operations.
&gt; It makes perfect sense, imho, that futures can fail: you're saying that sometime in the future, you might get a value. This doesn't seem fundamentally different than `let x = f();` saying that sometime after being invoked, `f()` will return a value. The universe can change while `f()` is being evaluated, too. `Future`s typically concern IO which is more sensitive to that sort of thing, yes, but even then, errors can be eliminated. Consider also `tokio_core::reactor::Timeout`, which as implemented is guaranteed never to return an error, but which does not express that in its type.
What eddyb is saying is that a GC won't help most of the allocations in the compiler, since they're *already* arena-allocated.
So there's two known problems here: * allocating `Substs` even when the interner cache is hit, which is not about arenas and can be removed in the common case * having one arena per type instead of a single arena - the general case is expensive because of destructors but we're moving towards a future where none of the arena-allocated types need drop, at all (this is where arena-allocated slices come in - they replace `Vec`)
There's more awesome changes than this too, for example https://github.com/rust-lang/rust/pull/36524
&gt; Writing custom event loops is also not something that is being focused on documentation wise. I get that, but it doesn't make the situation as it exists *right now* less painful. To be clear, I'm not seeking to ascribe blame to anyone; I'm just frustrated that the bit I *thought* was going to be relatively easy has turned out to be the hardest part... and I'm working with raw Win32 UI calls. &gt; It's only at the stage necessary to get Tokio &amp; cpu-pool working. I got that impression when I noticed the docs for (I *think* it was) `Task` referring to `notify`, which turned out to be an undocumented, private method buried inside `tokio`. :P
&gt; Only if you specifically select the method that has those semantics I specifically searched through several keywords on the documentation to try to find an `and` combinator that behaved the normal way before I made that post. I couldn't even find one with the normal semantics. This pathological behavior isn't even constructable in my world, much less constructable by accident! The `join` combinator allowing that construction is a direct result of including `Error` on `Future`. You cannot construct this combinator without it. The whole premise of Rust is safety via avoiding *shared* mutable state, and this `join` combinator is violating that: the completion of one future is magically dependent on unrelated futures elsewhere. Rust's claim to fame is banning this kind of sloppy logic from memory management, so what is it doing in a core Future lib?
I dunno... even when I was looking at writing some code that just used `futures` and `tokio`, the fact that I couldn't see how stuff was being communicated around the system put me off. Not to say that this is, in fact, how raindroppe feels, but even if I see a nicely documented, clean interface, I *need* to have *some* idea of what's going on under the hood. I have to have some kind of mental model in place to explain how all the bits will fit together in a wider context. `Async::NotReady` having no payload *really* screwed me over, because without that, I couldn't work out how anything in the stack was supposed to function. At which point I *had* to look at the implementation, at which point I realised Cthulhu R'lyeh wgah'nagl fhtagn. Or to phrase it another way: "That's a nice looking pie. Smells great, too." "Enjoy!" "Tell me how it's made, first." "... you don't wanna know that." "I know I don't; *I need to.*"
[Doesn't look that way to me](https://tokio-rs.github.io/tokio-core/src/tokio_core/src/reactor/timeout.rs.html#51-65). Even following up on that `update_timeout` call, I can't see any way for this would fail for any reason but a bug in tokio.
The bug would be not using the type system to express that it can't fail.
It's more like a conversion function between concrete types than it is the type erasure of a trait object.
I need something that satisfies the `Send` trait right? I assume even if it isn't the case here that threads aren't involved, the futures system want that flexibility. I tried making it work with an `Rc` but failed: error[E0277]: the trait bound `std::rc::Rc&lt;std::vec::Vec&lt;u8&gt;&gt;: std::marker::Send` is not satisfied --&gt; src/lib.rs:51:17 | 51 | req.write_function(move |data| { | ^^^^^^^^^^^^^^ | = note: `std::rc::Rc&lt;std::vec::Vec&lt;u8&gt;&gt;` cannot be sent between threads safely = note: required because it appears within the type `[closure@src/lib.rs:51:32: 55:14 body:std::rc::Rc&lt;std::vec::Vec&lt;u8&gt;&gt;]` 
That's fair. This is the tough thing about library design! It's quite possible that the `Future` trait will be an eventual candidate for inclusion in the standard library someday in the future; that'd be an excellent time to make the case for that.
It looks like `Send` is a requirement imposed by [the curl bindings](http://alexcrichton.com/curl-rust/curl/easy/struct.Easy.html#method.write_function), not futures. Perhaps it (or curl itself) uses threads internally? In light of that, it seems like your current implementation is reasonable. It's unfortunate that libcurl forces that level of indirection and complexity on you; this could perfectly well be singlethreaded and implemented entirely with futures, perhaps with a native tokio HTTP client library.
`&amp;&amp;` is bash sugar. git has nothing to do with it.
The way I'd normally do this is to put all of your trie nodes into a single vec, and then reference them using indices instead of pointers. This approach is much easier to prove safe in the eyes of the borrow checker. :-)
For what it is worth, the fact that tokio's Timeout doesn't return an error right now is due to an incomplete implementation. Requesting a timeout is definitely a fallible operation. See tokio-timer (github.com/tokio-rs/tokio-timer) where I covered some (but not even close to all) of the error cases.
&gt; I still haven't worked out what task::park does. It doesn't park anything! It clones a handle! No, wait, it clones an handle, and a Vec of events, but I'm not sure what those even do, and why does it just snapshot them? What happens if someone calls park twice? Does it explode? Does it collapse into a black hole? Does my computer turn into a reindeer and ride away on a rainbow? Is park only compatible with some of the waking methods? All of them? Do I have to use park for task_local! to work? I too was totally bewildered by this and ended up creating an issue (https://github.com/alexcrichton/futures-rs/issues/136) which basically turned into "help me, I don't get it!" (which Alex then very ably did). One thing I think is very missing from the documentation at the moment is a mention of the connection required between event loops and futures to be able to support futures both returning notready without a payload (as you mention) and then running themselves when ready later on. Without this crucial connection, users are just left seeing it as black magic. I'm not convinced the current design is ideal, but I don't have a good feeling for the needs of users in this space. I do plan to write a tutorial for implementing an event loop to properly demonstrate how futures and an event loop might need to talk to each other - I think a high level understanding of the being the scenes workings is important and that documentation is a bit thin right now.
How can tracing the arenas be faster than not tracing the arenas?
The fact that most (all?) *primitive* asynchronous operations can fail does not mean that *all* asynchronous operations can fail. For example, I might be interested in a future which resolves to a status message when an IO operation terminates for any reason. Such a future will never produce an error, because it is based purely on a consumed future, and handles both error and success cases as successes. I don't see any reason we can't have our cake and eat it too with type aliases, e.g. `type Future&lt;T, E&gt; = PureFuture&lt;Result&lt;T, E&gt;&gt;`.
Unfortunately, a type alias would not work in this case because Future is a trait. I personally believe that the library should make it easy to handle the common case (errors). I have personally not once hit a case where a future did not have an associated error type and I have not seen a solid argument to make the common case less ergonomic in favor of the minority case. That being said, the futures library is decoupled enough to enable you to implement PureFuture in your own crate. Maybe you should try it out and report back what you find. If there are enough legitimate cases indicating that `Future` should not have a baked in error, this fact should be discovered sooner than later. The futures library is still very early and changes can still happen.
How is this different to just passing `&amp;Rc&lt;T&gt;`? Passing that around doesn't involve touching the reference count, and you can clone a new `Rc&lt;T&gt;` from it.
&gt; which understands how to block on that type This would require that every executor understands all forms of blocking. Aka, all potential blocking reasons would need to be known to the executor ahead of time, which is not really extensible. Instead of doing that, we said that there was a concept of a `Task` and this `Task` is how anything can control blocking. This matches up with how thread blocking / unblocking is handled with `thread::park` / `thread::unpark`. Already, I've seen people implement futures in ways that I didn't anticipate in advance, and this is possible due to the "blocking" framework provided. That being said, the difference between what futures-rs does today isn't really that different than what you are describing. There is an implicit reason that is returned with all futures. This "reason" is a gate that anything can toggle, which then unblocks the future.
&gt; Why don't arrays implement iterator traits themselves? [...] Or because we currently can't parameterize over length [T; N]? I attempted this in [PR32871](https://github.com/rust-lang/rust/pull/32871), but yes the lack of integer generics halted it. My iterator type looked like `IntoIter&lt;T, [T; N]&gt;`, but it would ideally be just `IntoIter&lt;T, N&gt;`, and we couldn't change to that later. &gt; If so, would this be implemented once we can since that would actually be a breaking change? That was addressed in my PR too -- it's considered an allowable *minor* breaking change, and anyone who really wanted the slice iterator can just use the `iter()` method. Alex also suggested we might be able to use an `impl Trait` syntax to actually hide the ugly `IntoIter&lt;T, [T;N]&gt;` type. However, you can't yet use that for associated types, and we would need to assign `IntoIterator::IntoIter`. Edit to add, [arrayvec](https://crates.io/crates/arrayvec) and [generic-array](https://crates.io/crates/generic-array) are both wrappers on immediate arrays (no heap) and have `IntoIterator`. They might even work directly in your `test_arr()` implementation.
&gt; Why don't arrays implement iterator traits themselves? ... Is it because the arrays would be stored on the stack while the iterator is stored in the heap? Or because we currently can't parameterize over length [T; N]? Structs are never stored on the heap unless placed in `Box`, `Vec`, `Rc`, etc. It's partially because we don't have generic integral parameters yet, and partially because we don't have a good solution for preventing the array from dropping its contents if they aren't `Copy`. There's plans to implement both those things, but neither is very far down the pipeline AFAIK. 
It took a while to realise how tokio adds things to mio. During the call to `Future::poll` if a source is not ready yet it will register interest. (Then poll won't be called again until it is ready)
...and have an empty line before and after.
That is indeed how it works.
Double indirection though. With Stylo we sort of needed something like this because in Gecko (And many other C++ codebases), the pattern is that reference counted objects maintain the refcount internally, and their version of `Rc&lt;T&gt;` is just a pointer to T (since this is more interoperable with raw pointers and the way sharing is usually handled in C++). So a "borrowed" form of `Rc&lt;T&gt;` is useful from the POV of this pattern, and FFIing with codebases using it. So overall not _that_ useful, but nice-to-have. 
&gt; Does it? Shouldn't Rc be able to offset the pointer so that it points directly to T, then offset back when doing Rc::from_raw? I guess that works.
That's very cool. How is the `black_box(_)` implemented? I thought this was the one method that could not be stabilized, because it relies on inline assembly, which is itself unstable, not to mention unportable.
It's using `read_volatile`. There's a big warning text about black box not being equivalent, but for me it hasn't been a problem in practice. Good benchmarks don't overuse black_box. (And the project that I made this for didn't need black box at all).
*Note*: I was about to answer OPs question, but got confused half way through. So... now *I* have a question about my explanation :P Here it is: --- &gt; Why don't arrays implement iterator traits themselves? Consider this code: let a = vec![1, 2, 3]; let b = vec![4, 5, 6]; let c = vec![7, 8, 9]; let arr = [a, b, c]; for (i, v) in arr.into_iter().enumerate() { if i == 1 { panic!(":("); } } Let's assume that `[T; 2]` would implement `IntoIterator` directly and would return an iterator that would return the elements by value (meaning: `v` has the type `Vec&lt;_&gt;` and not `&amp;Vec&lt;_&gt;`). Let's step through: - In the first iteration the combined iterator yields `(0, a)` (where `a` is the first vector). The loop body does nothing and after the loop body ends, the vector is dropped (aka. "the destructor is called"). This means the elements `1, 2, 3` are freed. - In the next iteration, `(1, b)` is yielded (where `b` is the second vector). But now the loop body decides to panic! In the case of panic, Rust, by default, unwinds the stack to run all destructors (`drop()` impls). - First the scope "loop body" is left; `b` goes out of scope and is dropped. - *But what then?* --- At this point I was about to write: now it's difficult to decide what elements to drop, because a fixed size array doesn't carry a `len` field. However, as far as I understand this is only true for `drain()`-like methods. But `into_iterator()` can take ownership of the array (so the original array `arr` isn't dropped anymore anyway). And then the `Iter` can track the length of the array and what elements to drop (which it does already anyway). So it should work, right? Or why am I wrong? 
&gt; I specifically searched through several keywords on the documentation to try to find an and combinator that behaved the normal way before I made that post. Use `a.or_else(Done).join(b.or_else(Done))` or something. It can be constructed. The behavior of `join` is in the documentation, don't use it if you don't need cancellation to work that way. &gt; the completion of one future is magically dependent on unrelated futures elsewhere. Rust's claim to fame is banning this kind of sloppy logic from memory management, This has nothing to do with memory management? As Steve said, this is no more magic than any other combinator. It just makes the other futures stop polling, which has nothing to do with memory management or mutable state. The caller is the one responsible for polling them in the first place, it's not "shared mutable state" here. The caller previously could have made the decision to stop polling on an individual future which it owned. Now, `join()` (which owns the future) is able to make that decision for the caller. The completion of futures has _always_ been dependent on the owner polling them. That's ... how this whole thing works. There's nothing special being done with join.
Probably https://github.com/rust-lang/rfcs/blob/master/text/0141-lifetime-elision.md It's called "lifetime elision"
To continue on what /u/zzyzzyxx said, if you are expected to create a library but you want to be able to run some code for testing purposes, consider `cargo test` which works for library crates. For more information see the book: https://doc.rust-lang.org/book/testing.html
A thing that occured to me: any `T` is also `From&lt;T&gt;`, which might or might not be what you want.
Will there be a native way? SIgnals are pretty much in every OS right?
&gt; SIgnals are pretty much in every OS right? To an extent, but for example Ctrl+C handling is very different on Windows.
*exactly* why I prefer geany - it *doesn't* have these features....
&gt; This sidesteps all the complicated synchronization issues, but it's not possible for a compiled language like Rust. Actually... a compiled language could also check for various flags (signal pending, thread-interrupt required, ...) at a number of boundaries (I/O functions, system calls, back-edge of loops, ...). However this would incur an overhead.
A struct with 100 members could represent a table in a database.
If you have a table with 100 columns, you should probably also reconsider your design.
In certain applications, it's pretty easy to wind up with 100-column tables, especially if you're using a column store (instead of a row-oriented database) and cataloging things in the real world. A large retailer like Amazon, for example, sells hundreds of kinds of products, and many of these kinds of products have specialized metadata. So the `products` table will accumulate columns. Similar, a search engine's `web_pages` table will tend to accumulate lots of columns, many of them used as ranking signals in a machine-learning algorithm. I've definitely seen legitimate real-world applications, especially in machine learning or data warehousing, where you really do know hundreds of individual facts about an entity. If you really do have one of these objects in your problem domain, a Rust struct with a bunch of traits _can_ work reasonably well. But you should definitely stop and ask yourself design questions first.
Yeah, that definitely came off as "your design is shit". Sorry. My intended point was that if you have a table with 100 columns, you should really stop and ask yourself "hold on, am I sure a 100 column table is the right thing to do"? There are definitely legitimate cases where the answer is "yes".
Building off of this article—what libraries do you use for signal handling in Rust? I've used [chan-signal](https://github.com/BurntSushi/chan-signal) before, are there others which are commonly used?
&gt; Part of the reason why is that there isn't a ton of pressure to do so: with rustup, you can rustup run nightly cargo bench, and given that benches/ are a separate directory, relying on nightly for just that while keeping the rest of your code stable isn't the largest deal. Benchmarking your code with an other compiler version than the one you're releasing the code with, doesn't seem like such a great idea. Doing meaningful benchmarking is already by itself a quite nasty topic, so using different compiler versions isn't that helpful.
I think a global variable is the best thing to use there. The function pointer variable that SetConsoleCtrlHandler itself sets is a global variable, no?
At least jump-to-error would be very welcome here.
Is there a way to catch other things which are signals in POSIX? e.g. access violations, process aborts, exceeded CPU time limit, process termination request
It might refer to calling unwrap without being sure the value is there let x = opt.unwrap(); instead of if opt.is_some() { let x = opt.unwrap(); } or using pattern matching. But that UncheckedOptionExt is really interesting.
First you should split it into smaller structs, then you can think about adding traits. For example, you can turn struct Map { player_x: f32, player_y: f32, enemy_x: f32, enemy_y: f32, } into a much cleaner struct Position(x: f32, y: f32); struct Map { player: Position, enemy: Position, } without using traits. 
So are there any traitofs performance-wise when seperating the interface with traits? Or maybe any other tradeofs I didn't even think about?
Splitting methods into traits should not have a runtime overhead due to static dispatch. Splitting a struct into substructs can be a helpful pattern when dealing with borrowing: https://github.com/rust-unofficial/patterns/blob/master/patterns/compose-structs.md
Thank god it doesn't have the same behavior. Having any line of code potentially throw an exception makes it very hard to reason about in large applications.
&gt; As far as I can tell, in the ~1.4 years since Rust 1.0 there haven't been any serious attempts at making a new benchmark library. What about [criterion](https://github.com/japaric/criterion.rs)?
Bah thought this was about ECS 
https://github.com/nix-rust/nix
If you use traits with static dispatch (with generics), then there is no runtime performance cost. However, if you use trait objects, then there is a slight dynamic dispatch cost - https://doc.rust-lang.org/book/trait-objects.html
That lint has two sources of false positives: `'static` defaults you don't want have to be declared away, e.g. `Box&lt;T&gt;&gt; + 'a` and functions bound as `fn` values.
&gt; You can make more complex solutions such as having a separate thread that will get woken up when a signal happens and which does all the actual signal handler work, but that gets away from the native concept of signals. Another (linux-only) solution is to use the signalfd functionality to handle signals in your regular event loop, if your program has one. Is there anything about the "native concept of signals" that is worth preserving, compared to the alternative of sending a message over a channel and returning, and then handling the signal in a thread that can use the entire Rust library? Apart from signals that are caused by one of the threads (e.g. division by zero).
What if you call a function? How do you know that an opaque function that you call doesn't access globals? I think you'd have to backwards incompatibly introduce an effects system to Rust to make that work.
&gt; Is there anything about the "native concept of signals" that is worth preserving, compared to the alternative of sending a message over a channel and returning, and then handling the signal in a thread that can use the entire Rust library? Honestly, I don't think it's worth trying to make a "native" solution. Most programs that want to do anything with signals just use the "write a single byte to a pipe" trick, because signal handlers can do so little safely.
Yeah, that's even better (like Manishearth also mentioned below).
&gt; No, you still have to trace during the mark phase. That's how a tracing GC works. Yes. You must trace all reachable arenas. If the arenas are short lived then there are probably no reachable arenas in the entire nursery generation. Hence you must trace no arenas. That's how short lived garbage works. &gt; If deallocation were really the problem then we could fix that by merging arenas into larger allocations. A GC is overkill here. A GC is one possible solution to the problem. It sounds like other solutions are already being considered. 
Ah, so by "unchecked unwrap" you actually mean "unannotated unwrap"? Btw, it is not difficult to find examples of non-annotated unwraps in the stdlib; e g [here](https://doc.rust-lang.org/src/collections/up/src/libcollections/vec.rs.html#669) and [here](https://doc.rust-lang.org/src/std/up/src/libstd/collections/hash/map.rs.html#2018).
Pretty sure that is what unchecked is trying to say. Unwrap is totally fine if it encodes an invariant or in quick and dirty code. Using it as error handling in a library less so.
Normally you'd use a tokio `Core` rather than a thread pool, especially for effectively zero-CPU-cost stuff like timeouts, if that wasn't clear.
This seems to imply unwinding is part of the Windows ABI? I assume Rust needs to be compatible with this system as you can still generate seg faults etc with unsafe code. I read that it is UB to unwind across FFI barriers; but this seems to imply that, at least on Windows, there is no problem as unwinding is part of the Windows ABI? Does this mean that on Windows it *is* safe to unwind across FFI barriers?
&gt; This seems to imply unwinding is part of the Windows ABI? Sure is. &gt; I read that it is UB to unwind across FFI barriers; but this seems to imply that, at least on Windows, there is no problem as unwinding is part of the Windows ABI? Does this mean that on Windows it is safe to unwind across FFI barriers? Only if Rust's unwinding system is identical to SEH. Admittedly I'm not sure off the top of my head whether that is the case.
&gt; You probably shouldn't have to catch SIGSEGV in Rust Besides, the Rust runtime already sets up a SIGSEGV handler. So if you don't want to break its functionality, you'll have to be very careful about calling into the Rust one. (this handler is part of how Rust detects stack overflows cheaply)
Hey, that update is really great, thanks! Although, I would have loved to see examples of usage in macro docs as well. Though, I guess, I can make such a PR myself. 
Please don't. Thoughts of OS superiority is what got the Windows/Linux situation into this mess to begin with.
It's not a reimplementation, but I'm writing idiomatic Rust bindings for TensorFlow (https://github.com/google/tensorflow-rust). I think better integration with the larger software community will be better for Rust in the long run than reimplementing specific pieces of software, not to mention being more efficient in terms of programmer time.
Well you can also create alias `type InfallibleFuture&lt;T&gt; = Future&lt;T, Void&gt;`... (maybe something like that could be in the Futures crate itself)
&gt; Rust culture very much doesn't care about purity at all. Well, most of code I've read or written was either pure, or involved obvious IO (obvious from the name and context, like `File::open()`, `Read`, etc), or some logging. What I mean is that even when people don't care much, they naturally tend to do things that make sense. Futures might be less obvious but after reading an article about them, I got them.
Just found out about this. Excellent news!
The boilerplate with using raw mio shouldn't be understated. In one project, I eliminated 4,000+ LOC going from raw mio to tokio.
Since everyone is only commenting on the "unchecked `unwrap()`", I just want to say: very nice article. It's nice to see good explanation about "old stuff". I'd like to see more like this!
Serious question. What were the changes in Macros1.1 and where can I read about them? Has the Macro book been updated with them? I see a lot of crates *using* these changes but were they? Edit 1: thanks for the links. But RFC/Diffs aren't docs 
I'm afraid I don't follow.
That's true...
 you want /r/playrust
Yes, this work too (https://is.gd/WmFyeM): struct TheStruct&lt;T&gt; { valor: T } // Traits only define signatures trait TheTrait&lt;T&gt; { fn new_struct(valor: T) -&gt; Self; } // implement the trait for your struct impl&lt;T&gt; TheTrait&lt;T&gt; for TheStruct&lt;T&gt; { fn new_struct(valor: T) -&gt; Self { TheStruct { valor: valor } } } fn main() { let v = TheStruct::new_struct(20); print!("{0}", v.valor); }
[Here's the num changeset](https://github.com/rust-num/num/commit/97551ade5ba6872b436b6b4e117007c47181503f)
From readme: Spans cannot be boxed, cannot appear as a field of a non-stack-only type, and cannot be used as a generic argument. 
From readme: Safe code cannot create dangling pointers by storing it on the heap when Span&lt;T&gt; points to unmanaged memory or stack memory. So it can be used to avoid messing with GC in .NET
Sadly the concept of stack slices doesn't work really well in GCd languages. Useful in some cases though. Now, if you could make the slice trace the original object that might make it very useful.
&gt; But RFC/Diffs aren't docs These changes aren't stable yet, and so they will be under-doc'd until then. With so much work to do, docs focus on stable stuff more than unstable stuff right now.
That is what `Span` does when tracking GCed objects like arrays: there is a special "ref field" that the GC will update if it moves any object around.
That will be better! Thank you!
Interesting crate ... should try it out ... thanks.
You can create a trait describing the operations you need from either `Arc` or `Rc`: // Sized bound required to return `Self`. // Can also use `Borrow&lt;T&gt;`, but they're semantically the same in this case. pub trait Container&lt;T&gt;: AsRef&lt;T&gt; + Clone + Sized { fn new(val: T) -&gt; Self; } impl&lt;T&gt; Container&lt;T&gt; for Arc&lt;T&gt; { // Compiler is smart enough to realize you don't want an instant stack overflow and so calls the base method. // You may want to choose a different name for readability's sake, though. fn new(val: T) -&gt; Self { Arc::new(val) } } impl&lt;T&gt; Container&lt;T&gt; for Rc&lt;T&gt; { fn new(val: T) -&gt; Self { Rc::new(val) } } Then parameterize your datastructure over this: // Default parameter so the user doesn't have to explicitly pick one if they don't care. pub struct MyDataStructure&lt;T, C = Rc&lt;T&gt;&gt; { // Or however you use it root: C, // Compiler complains if `T` isn't used in the struct definition. // Underscore prefix implicitly silences "unused field" lint. _marker: PhantomData&lt;T&gt;, } impl&lt;T, C: Container&lt;T&gt;&gt; MyDataStructure&lt;T, C&gt; { pub fn new(root_val: T) -&gt; Self { MyDataStructure { root: C::new(root_val), _marker: PhantomData, } } } Then `MyDataStructure` is automatically `Send + Sync` if `T: Send + Sync` and `C = Arc&lt;T&gt;`, which the user can set by calling: MyDataStructure::&lt;T, Arc&lt;T&gt;&gt;::new(my_val) Or you can create a method in a separate `impl` block impl&lt;T&gt; MyDataStructure&lt;T, Arc&lt;T&gt;&gt; { pub fn new_send(root_val: T) -&gt; Self { Self::new(root_val) } } So they can call it like MyDataStructure::new_send(my_val) 
The link will take you to the GitHub issue where we are compiling the list of crates - feel free to add your contribution as a comment to that issue. Alternatively, add your suggestion to the comments here and I or someone like me will get them added to the master list.
Tldr 
Thanks! But what if you need it to be recursive? I.e. Node contains an A/Rc&lt;Node&gt;
Table based SEH is an explicit part of the Windows x64 ABI (and almost all other Windows architectures). For Windows x86 it is a bit more complicated however in that it uses stack based exception chaining, which means that SEH isn't dictated and you end up with things like MinGW using DWARF and SJLJ unwinding on 32bit Windows. Rust uses SEH for its unwinding system (except for `i686-pc-windows-gnu`). It borrows the C++ personality function though, so that destructors are fired during Rust panic unwinding but _not_ during other SEH exceptions such as access violations. Rust _used_ to use the general personality function which resulted in a rather fun [issue](https://github.com/rust-lang/rust/issues/33112). A side of effect of using the C++ personality function is that Rust will also fire destructors during C++ exception unwinding and vice versa. At least for msvc, unwinding between Rust and C++ has sane behavior (although Rust doesn't guarantee it will stay that way). Note that unwinding into code which doesn't know how to deal with exceptions, such as C, can still result in nasal demons. For evidence of C++ and Rust exception/panic unwinding compatibility, observe my test results: * Rust destructor firing during C++ exception https://gist.github.com/retep998/1a257d815a22f2b69f48ed20c19134ab * C++ destructor firing during Rust panic https://gist.github.com/retep998/b9689de1e810ea40ec61ad0e6fb57a02
Then you would put that parameter on your `Node` struct too: struct Node&lt;T, C&gt; { next: Node&lt;T, C&gt;, _marker: PhantomData&lt;T&gt;, }
Your struct directly contains itself, which won't work. You'd need something like next: C&lt;Node&lt;T, C&gt;, but it's not clear to me that that's possible.
[removed]
Oh, right, it works with stack arrays too. I knew .net didn't have array value types, but the examples for `Span` use stackalloc to get them, so that needs to work. Hm.
How did you figure out what options to put in build.rs ?
And maybe it can be abstract enough to also work with [other types of UIs](https://scribbles.pascalhertleif.de/impl-virtual-dom-cli-libui.html) :)
Yes, that's what I intended.
Check yo subreddit
r/playrust
Legitimate question here, why would you want to do this? Seems like there many better options for languages to transpile to js or even to compile to webasm. Rust is awesome but it's clearly designed for systems programming.
How do you envision that? The same interface for different systems that guarantee the same rendering? The devil is in the details. There are so many arcane rules in the DOM (transparency, z-index, attribute inheritance, text aligning) that it would be a shame to lower other systems to that level, or a lot of work to get around the DOM's problems. Finding a common ground would seriously hamper the usefulness of a shared interface.
It's the same problem as golang hashes not being memory-safe w.r.t threads. They describe it as "tearing" (by analogy to screen refreshes). Other thread assigns a different (pointer+length) pair, you observe it when one field has been changed but not the other -&gt; memory unsafety. If you can only store (pointer + length) on the stack, it prevents any access by another thread.
[I was working on this for a bit](https://github.com/zvxy/rust-dom), but was waiting for wasm support and also stable macros. Maybe I'll pick it up again. It was a pain in the ass to get [component style traits](https://github.com/zvxy/rust-dom/blob/master/src/dom.rs) working though. Had to do some voodoo to get trait objects to play nicely with cloning. Looked a bit like this: struct App { n: usize } impl App { fn onclick(&amp;mut self) { self.n += 1; } fn onload(&amp;self) { println!("{:?}", self); } } impl Component for App { fn render(&amp;self) -&gt; Node { dom!(&lt;div onclick=|| { this.onclick(); } onload=|| { this.onload(); } /&gt;) } }
Thank you! A follow-up is in the works.
&gt; BTW does anyone know how to compile with optimization in a way, that would not optimize whole program to nothing? :D (I use xargo.) That sounds as if you have not set up entry points and linker script correctly. I know there is a decent write-up out there, but I lost the link, maybe someone can pitch in? While I do not think that the following is what you need; there's a way to force the compiler to keep at least parts of a program segment by adding inline assembly to it: for i in 0..1_000_000 { asm!("nop"); } This will result in a million nops and is a hack to get a `delay()`-style function (remember to check how many instructions it actually compiles down to, if you're estimating its execution time). It also works with an empty `asm!("")`. That being said, I would assume you need to fix the underlying problem first, `asm!` should not be used for these hacks just to light up an LED (sans blinking ;)).
High-performance Web apps.
Please try HOWTO in the OP and let me know if it works for you. /cc /u/steveklabnik1
I kinda see your point in general, but isn't Rust's slice syntax much nicer for this case?
&gt; That feel when no keyword arguments in Rust. See how readable that is? I am not quite sure what you are getting to; is this sarcastic or heartfelt? For comparison, the Rust syntax would be `let slice = array[1..3];`
&gt; The same interface for different systems that guarantee the same rendering? Sorry, I don't mean the DOM interface, but the vdom API: Defining trees of 'components' (just a name) and how to determine the minimal set of updates to transform one version of tree into another. How a component is rendered and how it can update itself is defined in the component itself (but may of course be shared), but how to _use_ the component is the same regardless of whether you are dealing with DOM components, or libui components, or components for the custom rendering stuff of a text adventure. The insanity of the HTML5 DOM would just be an implementation detail (a huge one, but a more or less isolated one) of the DOM components. (I tried writing something like that in Rust once, based on traits and trait objects, but it got way too complicated way too fast. Maybe I'll try again with a more concrete implementation first.) &gt; Finding a common ground would seriously hamper the usefulness of a shared interface. *Edit:* Or did I misunderstand you? The examples you give for things for which "arcane rules" exist ("transparency, z-index, attribute inheritance, text aligning") are all things that other UI abstractions may need to solve, but this should not be part of an vdom-style abstraction IMHO.
In this case, yeah. But that's a very specific case. In most other cases the Rust version would be `o.drawWindow("Calculator", 100, 200, 400, 600)` If the API designer wants to make his intentions more clear: `o.drawWindow("Calculator", Point(100, 200), 400, 600)`
Note that this was a feature a year ago, and also already in Rust 1.0. So it might not seem like a big deal, but something that was part of all of Rust's modern history. As usual, This week in Rust and every release announcement is the best source of news and improvements, but I guess it's a bit high volume. I think there are also improvements that simply slip in unnoticed. Here's one in Rust 1.12 that I noticed: In the Itertools trait, to compile in Rust 1.11, this method's associated type bounds need to be written with a lot of `&gt;`'s: fn kmerge(self) -&gt; KMerge&lt;&lt;&lt;Self as Iterator&gt;::Item as IntoIterator&gt;::IntoIter&gt; where Self: Sized, Self::Item: IntoIterator, &lt;&lt;Self as Iterator&gt;::Item as IntoIterator&gt;::Item: Ord, { In Rust 1.12 you can shorten it to: fn kmerge(self) -&gt; KMerge&lt;&lt;Self::Item as IntoIterator&gt;::IntoIter&gt; where Self: Sized, Self::Item: IntoIterator, &lt;Self::Item as IntoIterator&gt;::Item: Ord, {
&gt; The insanity of the HTML5 DOM would just be an implementation detail (a huge one That's indeed what I was referring to. It can be a real PITA to implement an arbitrary UI interface using HTML and CSS. The thing I work with simply doesn't use any of the built-in functionality like scroll bars: only single line text input is done using the DOM. So a unified API with different underlying implementations is not something I see happening, nor being used. The current trend seems to be the other way around: avoid native UI libraries and use Electron.
&gt; I was working on this for a bit. I was trying to do something similar, started with the diff'ing algo first[https://github.com/thysultan/VR](https://github.com/thysultan/VR/blob/master/Rust/implementation.rs)
Just push a new minor version.
Trial and error mostly. After looking at the docs I realized I needed to tell where the library files were located and what their names were and while using rustc you could just pass -L and -l flags it would be a pain getting the flags passed to cargo. I looked it up and the options in build.rs correspond to those flags. Did tgat make sense?
I end up wrote my own version of parallel collecting. As Ralith said, it is not as hard as it sounds like.
I wonder what they actually said. I woke up after the comment was posted. I use Haskell professionally (which was after I had started using Rust) and I've been toying with the idea of using the languages together for a while. They make up for the other's weaknesses quite nicely and they have similar ideas of how they work. Will this be needed all the time? No, but when you do need it why not leverage the best tool for the job.
That doesn't run on stable, alas.
This. I did a build version push, however. Had to learn it the hard way, that Crates.io doesn't render Markdown for very short example codes.
Sounds like a ton for work. Thanks for the effort! 
You're welcome! It was hard, but it was a fun problem to work on rather than doing homework!
&gt; Using named parameters, it is easy to make a mistake and accidentally swap width and height. One might argue that the names should make it obvious... but I'd rather the types make it not compile. What? What's the difference between `Width(400)` and `width: 400`? If you meant that to be the height, you're going to make the same mistake in either case. There's literally no way for the compiler to save you from yourself if you ACTUALLY typed `width: 400` as a named parameter.
I just realized that since this is a website I can trivially add a html redirect and I don't need any symlinks. I still think some sort of guide detailing common mistakes and how to avoid, like a checklist of what to do before publishing, should be a thing. I'll make a new repo for how I see this and post it here later for discussion.
Funny thing is that your example already violates your rule. Instead it should be: o.drawWindow("Calculator", Point(X(100), Y(200)), Width(400), Height(600)); And then I'd go all the way: o.drawWindow("Calculator", Point(X(100), Y(200)), Size(Width(400), Height(600))); API design be difficult.
At least in C# the parameter names are optional. Weight the `Width(200)` your actually specifying a type. In the case of the named parameter `400` would be just as valid as `width: 400`.
&gt; I still think some sort of guide detailing common mistakes and how to avoid, like a checklist of what to do before publishing, should be a thing. We have some of this, but I'd like more as well.
&gt; Had to learn it the hard way, that Crates.io doesn't render Markdown for very short example codes. Showing the README on a crates' page with rendered markdown is a very common feature request, nobody has stepped up to implement it yet though.
Would someone be kind enough to ELI5 the importance (or handiness) of array slicing?
I agree with the upfront cost. On the other hand, my experience suggests that I read code much more than I write it, and therefore the extra writing time is well spent :)
Good points :)
For the most part I wield react and JS as if they were rust, partly because I subjectively believe it results in more maintainable systems. I want the ability to declaratively express a given scene, but much more efficiently than JS can express with current tools. If things work as I imagine, LLVM should be able to statically resolve/elide diffs (and subsequently, traversal bailouts) in a large amount of cases, but we will see.
Even better, a staging area where crates and generated pages are only visible to the developer (using some additional, random temporary key), then a separate commit step to publish them to the world. Sonatype OSS does this in the Java world and it is immensely useful.
This might be a solution. The only problem is I want my library to work on stable Rust. Since this requires a plugin it's a no go unfortunately. Though it might work in this example for someone who doesn't care about that. I might play around with it and see what works.
Oops! I even read over my comment before submitting and was certain I read "cloned" haha. Thanks for catching that.
While that would indeed be useful, it would rely on human checks which means you could still miss some issues. Pinning down the exact version of the software seems useful, to avoid two users referring to the same version for different content, but in the case of documentation why not allow editing?
you're looking for /r/playrust, unless you want to stay awhile and learn some programming
Ty
Not usually, no. The only time that it is useful is if you change the type in between, something like fn into_bytes(s: String) -&gt; Vec&lt;u8&gt; { Here, you'd consume ownership of the `String`, and return a vector of its bytes. This is useful because you wouldn't need to do any reallocation. It's still "take ownership of the argument and then return something owned ", but it's not the same type on each side.
Personally I'm addicted to the way the Rust type system expresses ownership. I find myself wishing I had it even in relatively small Python libraries. I know Java and others often use immutable containers to do things like this, and that does help, but there's nothing quite like "I'm giving you a Vec and I promise it is All Yours now."
The whole point of wasm is that you actually can program for a low level virtual machine. Unlike compiling directly to javascript, you actually get the performance benefits from using a systems language. Also, I feel as if rust's safety features make it way more general purpose than a lot of mainstream systems languages (that is, ignoring a lot of less popular systems languages that actually did pioneer safe systems programming language). I wouldn't feel comfortable writing code for something complex like a web application using a language like C/C++, but when `unsafe` isn't used, rust feels a lot like a higher level language that guarantees things wont blow up. You get the best of both worlds: amazing performance and low level tuning, and code that is easy to read and write because of the guarantees of safe code.
I think the x and y are very redundant and don't carry their weight.
The code works correctly in debug. I have used `ptr::(read|write)_volatile`, so it should not be optimized away. I have one warning though: warning: static item is never used: `RESET`, #[warn(dead_code)] on by default --&gt; src/main.rs:78:9 | 78 | static RESET: extern "C" fn() -&gt; ! = ::start; | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Any idea how to make compiler (or linker) think it's used?
Thank you very much for developing this tool! I decided to try ARM + Rust yesterday and I have already a LED shining. :)
Ok, thanks!
`Span::new(array, range: 1..3)`?
Cool, I've been using it for my GBA stuff too. Is there a good introduction to the 3DS stuff?
Funny you mention state diagrams and protocols. I actually started with an UML state diagram for my actual application which is also a network protocol handling application. I have had a lot of issues with the shared data. My thoughts on that: * pattern guards don't really work well, because the move into the guard of non-copy variables. So while they map nice onto a uml state diagram, using if else within the match gives less trouble. * My State consists of more but 'simpler' data. All the stuff that is read only and known beforehand I pass as argument to 'next' and 'run' and all the state that is dynamic is handed from one state to the next and modified if applicable. * In my run method I send packets to outside but I have a separate receive function to get the response back. Every time it goes through the loop it either gets Event::Response variant or Event::NoResponse. But that is a valid thing in my protocol so for me it made sense. I don't think it makes much of a difference though. * I don't modify the state internal data in run at all, I only use the transitions to modify states or their internal data. * I don't think you actually need the states to be PartialEq. So having TcpStream in there is not that bad.
You can also fork the example library and give it a shot! I'll try it out sometime, but this week is quite packed for me so I won't be able to do too much work on it.
Even better, you can link to docs.rs, then your docs will always be uptodate.
That's true, I could maybe be convinced of situations where it makes sense to use Rust with wasm, not transpiling to js though. Also my opinion might change when Rust gets a built in garbage collector. Maybe I'm just not comfortable enough using Rust yet but from my experience it seems like there are many things that will always take more code and more thought to implement in Rust than in Kotlin or Swift for example. Right now I generally use Rust for only very low level stuff, personally use it for embedded projects. I use Kotlin for just about everything else (including transpiling to js), except super light scripts which I use Python for. For example I would rather implement a wasm VM in Rust but at current I'd rather write code that runs in it in Kotlin. I tend to be a pretty big proponent of abstraction and GC though, like I don't honestly think it makes sense to make an office suite that doesn't use a garbage collector. Like I said in my previous post though I think there is a big gray area and I can understand arguments to switch some of the middle ground stuff that I currently do in Kotlin over to Rust especially when Rust gets a GC. Although when Kotlin is updated for targeting native compilation my choices will be even harder. I like both languages a lot although I might be bias because I would say I am an expert Kotlin developer and an intermediate Rust developer, probably on the low end of intermediate at that. Regardless it's nice to see progress being made in production languages though, a few years ago I would have been choosing between C/++ and Java or JS which for me is like choosing my preferred form of torture as, at best, I don't like working with any of those languages. Now I get to choose between languages that don't constantly frustrate me with their design decisions.
Agree. The same could be said for width and height, but for some reason those I keep having to look up...
Xargo is brilliant. You can start out by depending on a magic libcore-providing crate, but it quickly goes wrong when you try and depend on other crates. Xargo just works. 
This doesn't quite solve the bigger problem of small mistakes in publishing and rust docs also still can't handle feature gated documentation (building docs on a linux machine means all windows stuff goes poof). While automatic docs hosting is nice, I'll still host them myself for now.
[removed]
I better switch Redox to Xargo then
So, docs.rs will show per-platform docs, so it has windows docs for every crate. It doesn't apply to the standard library yet though, which is tough. Features themselves are a bit harder; I'm not sure that's been sorted out.
If you check out the mainline branch of intermezzOS, I also incorporated some other work /u/japaric did exploring how to essentially eliminate all uses of `make` for building, https://github.com/intermezzOS/kernel/issues/69 is one problem I've had with that approach, though. (I shell out to nasm in a build script, basically. Ideally I think I'm going to end up porting to GCC so that I can just eliminate nasm entirely, last I checked, redox uses external asm files much more heavily than I do; for example, its interrupt handler stuff is all nasm macros, whereas I do it in Rust in intermezzOS.)
That would be cool. Redox no longer uses asm for kernel features like interrupt handling - see https://github.com/redox-os/kernel for our redesigned kernel
Nice! I knew you were re-doing it but I haven't had the chance to check things out.
This is not really on topic, but is the title a Nine Inch Nails reference? Just occurred to me... Pretty state machines rhymes with their debut album. :)
It will basically be Gentoo for isomorphic web applications. 
That seems very close to using `-rc1`, `-rc2`, ... prefixes, Which can be done already, but have a looser and unenforceable semantics.
There's only four methods on `Core`. None of them are called "spawn", but [`run`](https://docs.rs/tokio-core/0.1.0/tokio_core/reactor/struct.Core.html#method.run) does what you want. Of course, you normally shouldn't be trying to "get some value" out of a future at all; you just create a new future around it using one of the `Future` methods.
I left `Option` out for simplicity but that's probably what you'd use: pub struct MyDataStructure&lt;T, C: Container&lt;Node&lt;T, C&gt;&gt;&gt; { root: C, } struct Node&lt;T, C: Container&lt;Node&lt;T, C&gt;&gt;&gt; { data: T, next: Option&lt;C&gt;, } impl&lt;T, C: Container&lt;Node&lt;T, C&gt;&gt;&gt; MyDataStructure&lt;T, C&gt; { pub fn new(root_val: T) -&gt; Self { MyDatastructure { root: C::new(Node { item: root_val, next: None }), } } } 
Haven't heard of Kotlin, i'll have to give it a look! Also, i can definitely understand your comment about hesitant to make changes. Though i think there are tradeoffs there that you can make with Rust. Eg, using generics more often, traits, etc - the nice thing about Interface languages like Go and Rust is that i think you can have your cake and eat it to in regards to "duck typing" and the benefits duck typing brings. Rust is not quite there in ease of this though, but in time and with better tooling, i think it will be on par with Go. _(i brought Go up, becuase i find Go quite easy on the Interface front. Lovely, infact)_ Regarding concurrency, i think this is one area where Rust has a huge advantage - i believe the spec for WASM includes a thread-like feature. Meaning, we could hypothetically be writing concurrent code in the browser. At this point i think it's too early to say what they will end up like though, but i'm sure most non-Javascript applications (C/C++/Rust/Go especially) will want concurrency, asap. Highly desired features haha &gt; Feels very strange being on this side of the argument. I've spend the last 3 months convincing everyone at my company we should migrate all our c code to Rust. haha, yea - but i get where you're coming from. Personally, i've just been so tired by dynamic languages and the problems therein that i welcome Rust on the web. Furthermore, it is a bit of a dream of a lot of programmers to have a single language for both frontend and backend. Being able to run the same code in both places (of course within limitations of security) would be pretty nifty. In my days of NodeJS, i can definitely say it was fun, and i welcome that with Rust :)
I'll check it out, thanks!
Thanks! I'm currently working through ArcadeRS, I'll take a look at your other link too
I'm a bit more tied up this week then I'd like but I'm prepping my talk on Curryrs and FFI between Haskell and Rust for Tuesday's Rust meetup in Boston!
Thanks for the links! I didn't know ArcadeRS!
For width and height the order is not obvious, but for x and y it is.
You need to remember to add all type constraints when using typenum: impl&lt;Level&gt; NodeStats&lt;Level&gt; for Node&lt;Level&gt; where Level : Unsigned, U1 : Shl&lt;Level&gt;, &lt;U1 as Shl&lt;Level&gt;&gt;::Output : Unsigned { type Size = &lt;U1 as Shl&lt;Level&gt;&gt;::Output; } fn main() { println!("Size of Node Level 2: {}", &lt;Node&lt;U2&gt; as NodeStats&gt;::Size::to_u32()); } 
"simple 4-dimentional game written in Rust"
There's a lot of good answers here. If anyone is so inclined, we'd be happy about a pull request to http://community.rs https://github.com/rust-community/rust-community.github.io
Thank you for linking ArcadeRS, that one had completely flown under my radar! The macro example in 1.3 was especially helpful - a lot of the tutorials/documentation that area are very abstract and I've struggled to get my head around it.
I agree, I really hope we get this at some point. What's the argument against it?
To expand on this: It is possible to shift left any `Unsigned` by any other `Unsigned`, but the compiler doesn't know that, and it's not even certain what the output is, even though it will also always be `Unsigned`. So, you need to tell it that you can shift left and that the output will be `Unsigned`, and it's okay for the impl to only apply in these cases (which happens to be all cases, but again, the compiler doesn't know that). I *think* once we can use specialization with associated types (which, as far as I'm aware, isn't being actively worked toward so it may be a while), we can make it so a lot of these where clauses are unnecessary. Until then, unfortunately, working with typenum will require these verbose where clauses.
I intentionally didn't post here because I thought it doesn't worth it. Still happy that you crossposted here.
hi all, i'm using handlebars-iron. can i separate template? like header and footer in another separate template. thanks.
[Let's build a browser engine!](https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html).
Kind of. X is the horizontal axis, and Y is the vertical axis. In the end, width and height are the same. Still I always reverse them for some reason :). The point is that even though it may seem obvious, it ends up not being obvious, and that's a very big challenge in API design.
Honestly, it looks pretty hacky, and since it's really not the default, obvious way of doing stuff, I'm not expecting much APIs to use this form.
The first chunk of code just outright fails for me on Win7 with an error msg "error: process didn't exit successfully 'target/debug/arcade-rs.exe' (exit code: 3221225781)". No real explanation of the error or anything. EDIT: I apparently missed copying a .dll file from the SDL2 library directory into the project's root directory.
Automate all the things. Make your CI to publish crates if and only if it pass all checks on tag. You could also add additional checks like checking for typos, etc. Less work and less errors.
Wow that's quite the diff on that PR. Awesome stuff :)
People are hostile about this because there's countless areas you should see before posting that mention that this is *not* /r/playrust, and yet we still get so many of these mistakes.
I have a problem like this making a game. I have one struct for the top-level "World" data structure of the game, that basically provides access to the whole game state, and gameplay functions take World as context since they can basically involve interactions between any subsets of game state contents. I ended up writing a bunch of free-standing functions that take `&amp;World` as the first parameter instead of cramming everything into struct methods, since it's a sort of different situation from your usual datatype. Normal datatypes have a specific set of duties and you can usually circumscribe the amount of methods you need, but for the game the amount of rule complexity can grow very large, something of the order of a thousand functions dealing with gameplay wouldn't be out of question with something of the complexity of NetHack, and all of them need to get the game state context from somewhere. Top reason I'm doing things as functions is that I want them split into several modules, and if I did the traits approach I'd need to write the signature of every function twice. (Hm, maybe I could write a macro for this that would let me write the functions once and that would emit the convenience trait with the signature blocks and the impl block, with me writing only what amounted to the impl block... Sounds kinda nasty though.) Anyway, interested in how other people would do something NetHack-like, where you have a ginormous amount of gameplay logic code split into dozens of modules worth of code, and all of it needs access to one "central database" of current game state. Maybe I should treat of the database moniker as less metaphorical and look into how actual database programming manages database connection context handling?
Can someone just have a panic handler that shows a GUI dialog?
Yeah I saw it wasn't getting any comments on the other subreddit, and thought it deserved some, so cross-posted it here in case it would get more :) The only very minor thing I have to add is that instead of cloning the context (a `HashMap`) every time you need to locally extend it with some binding to type check something, it would *presumably* be more efficient to just insert the binding beforehand, and then remove it afterwards. But I assume efficiency wasn't the primary goal in any case.
Yes, this one's great. 
Released [watchexec](https://github.com/mattgreen/watchexec) 1.0 last week, which is a small Rust-based file watcher for all major OSes. Got CI up and running (copied a lot from ripgrep :) ) which is really nice. I'd like to get it into Homebrew on OS X, but I think that requires a certain amount of Internet Points (er, Github stars) to do so, so I'll mostly be publicizing it and writing it up.
It's possible to post from the Reddit frontpage and, if you type rust into the "choose a subreddit" box, the completion popup covers up the description of the chosen subreddit long enough for you to immediately click "Submit" without seeing anything. https://imgur.com/a/BYVwu
Getting Rust to work on [Fuchsia](https://fuchsia.googlesource.com/manifest/). Have a binary pure code running but not stdio yet. In order to land these patches, Rust will need to upgrade to LLVM 4.0 to access the fuchsia triple.
I had actually come across this one a while ago and ended up skipping it due to its age. Is the ~2 years of difference less significant than I assumed? Thanks!
Is there a rust lib (or app) that will parse rust code and give me a list of functions in a module and their parameters (with types)?
Haha I was gonna write this whenever I got some free time. Nice on you for beating me too it :p
I've decided to start working on an NES emulator. Mostly for fun and the learning experience, and I'm really excited to get it working eventually.
I would love to be able to recommend a book like this to people! I think the big challenge would be introducing references and ownership (and to the extent necessary, memory) without overwhelming the novice with details about how the machine actually works too soon in the learning process. Ownership is just _everywhere_ in Rust code and you can't ignore it for very long. It even shows up in the common slice types, like `&amp;str` and `&amp;[_]`. The [How Rust Do](http://blog.jfo.click/how-rust-do/) tutorial posted earlier today does a surprisingly nice job of explaining all of these concerns in terms of ownership, and barely mentions memory at all.
Nice article! `println!("Hello, World!\n");` will print an extra `\n`, the closest equivalent would be `print!("Hello, World!\n");`, though `println!("Hello, World!");` is more idiomatic and probably what's intended.
You could use the syntex-syntax crate on crates.io, it's an export of libsyntax, witch is used by rustc for parsing. However it might be overkill if you only want function names/parameters. 
Oh fantastic! I'll definitely be running through it then. Thanks for the response
Based on [rsnotify](https://github.com/passcod/rsnotify), which supports native file watching protocols such as inotify (a ton better performance than polling). Great!
Current book author and new book co-author here. &gt; Are there any tutorials or learning materials targeted at total non-programmers? In general, no. _However_, I think that a particularly intrepid individual who is willing to put up with some frustration and ask a lot of questions in `#rust-beginners` or elsewhere could probably make do with the new book. Probably. It just depends. But I think it really only fits a certain type of person, at best. I would love for such a thing to exist, but I have to prioritize people who already know programming at the moment. Hopefully someone else will take up the torch.
Any chance for a more readable font for mobile devices? Grey font and very thin letters is to hard too read my android phone.
I don't mind either way, but just wanted to say it read unite easily on my phone.
Very nice article! Just one thing, when you were talking about Result&lt;T,E&gt;: &gt;This has something to do / a lot in common with the maybe monad in Haskell, and the Option type in Scala and the option datatype in ML. I don’t really know about how those things work other than to mention them as probably pertinent here! Actually it is closer to the Either monad in Haskell, and Maybe is actually Rust's own Option&lt;T&gt;.
Oh I agree totally. JS has some weird language pitfalls, but it is 100% a legitimate programming language. Just, sometimes a misinformed jerk starts trying to swing their ego around. I like the idea of my friends being able to turn it around on them. Not trying to insult frontend devs, people who use JS on the backend, dynamic language programmers, or anyone else.
I haven't tried it out yet, but do you know if it would work on NFS under Linux? NFS, of course, being implemented well before the idea of FS notification was a thing.
You're looking for /r/playrust
How does it compare to [entr](http://entrproject.org/)?
good point! I will adjust that
I'll adjust that, thanks.
I have not tried this. Let me know how it goes. My guess is it won't get notifications and you'd have to force polling in order for this to work. Edit: yeah, this won't work; inotify has no way of knowing that a file has changed on another system, see this [SO answer](http://stackoverflow.com/questions/4231243/inotify-with-nfs) I've added an issue to add a `--force-poll` option for this case. It'd be good to auto-detect it, too.
&gt; JS has some weird language pitfalls Every language does. Considering its 20-something year history, the sometimes insane backwards compability they've had to deal with and that it was thrown together in like two weeks, it's awesome. Personal favorite, and I'm a bi fan of the prototypical inheritance model and the semi-functional approach as well. But yeah, I am part of a fair share of programming communities, and JS regularly takes a beating. Rust is a nice language as well. Really looking forward to getting in to it at some point. 
Thanks! A common issue I have with file watches is that some editors create temporary files, then delete and move the temporary one into the original’s place instead of overwriting the original. This way, they ensure atomicity, but break file level watches. How is this handled?
Cool, I wasn't sure if rs-notify did some magic to bridge file systems or not.
I use windows a lot and I have yet to find a terminal emulator I like. If your emulator worked on windows and supported ligatures, I'd probably love using it. Anyway, sounds like a fun project.
How far along are you? I think the 6502 portion of the emulation is probably the easiest. I started a project like this, but in haskell, and I remember getting sort of lost learning about the graphics hardware. I didn't test my 6502 emulation very much, but it was a fun and interesting project. I ended up taking what I had learned about the history of the 6502 and giving this talk: https://www.youtube.com/watch?v=wOJj-IdYZxI
`std::thread::sleep` blocks the entire thread. Don't use it with futures.
Yes, that's the usual approach.
Note: `read_line` also exists, and can be more efficient.
I've only started a couple days ago so I'm not really far along. Right now I'm on the 6502 portion which does seem relatively simple so far. Also I loved the talk!
&gt; Also I loved the talk! Thanks! :)
I found it helpful to take an entry from the [PL Zoo](http://plzoo.andrej.com/) and translate it to rust. That was after doing some of the other things recommended here like too many lists.
He's probably saying the idea of a 4-dimensional game is not simple.
I wrote this, I think it's faster than bufread/lines but honestly I may have just gone off the rails: https://is.gd/12Oy1K. I started doing this just to experiment with memchr, and while I thought I was only going to be dealing with ascii text. I guess I can't use memchr on playground, so I'm not 100% sure that compiles, but it should be very close.
Yes, that's basically how it works! It's a replacement for the llvm backend. SPIR-V for shader code has a few restricitions, so only a Rust subset can be compiled. Here is an example code if you are interested (currently not working due to transition to crates): https://github.com/msiglreith/inspirv-rust/blob/master/rust-examples/quad.rs 
That's what I'm guessing, too, but I wanted to be sure ;)
I'd expect the same would be true of Linux's inotify, and probably Windows' API (although I've never used that before).
If you already have a `str`, you can just call `str::lines` to get a noncopying iterator.
You want /r/playrust
Hmmm, I haven't seen anyone get annoyed with /u/nikomatsakis...
I know this is the wrong rust forum, but it might be danielfromsl. If it's not, go watch all of danielfromsl's videos anyway.
/u/japaric, kixunil wants to send you a tip for 1 High-five (7,787 bits/$5.00). Follow me to **[collect it](https://www.changetip.com/collect/953378)**. -- [^^what ^^is ^^ChangeTip?](https://www.reddit.com/r/changetip/wiki/tipping-on-reddit)
Is there a way to search for a group of elements in a Vec? I've got a program where I'm reading and writing bytes from memory and I need to be able to search for groups of sequential bytes, specifically opcodes. Right now I'm going over the bytes sequentially and if I see the first byte I check the second byte I'm looking for. Below is a sample of the code I'm currently using, mem_bytes is the byte vector, segment.0 is the starting memory address, and chunk is the 4096 byte chunk of memory I'm currently iterating over. while count &lt; (mem_bytes.len() - 1) if mem_bytes[count] == 15 { if mem_bytes[count + 1] == 5 { return Ok(segment.0 + (4096 * (chunk - 1) as u64) as u64 + count as u64) } else { count = count + 1; continue } } else { count = count + 1; continue } } I don't know a lot about searching algorithms, or if there's a better way to do this, but the faster I can get my code to run the better, even if it temporarily would use more memory.
Awesome, tyvm. Looks like I'll be able to use that to speed up some other searches as well :)
My time to shine! I had this issue trying to get event notifications from my Mac into a docker VM that had my files mounted using NFS. Womp womp, it didn't work. So, I wrote my first tiny rust app. It's called [FS-EventBridge](https://github.com/TechnologyAdvice/fs_eventbridge). It's a TCP server you run on the machine that's mounting the NFS shares, which a client on the host can connect to and stream in the absolute paths of files that have changed. Think of it as an NFS out-of-band add-on to handle fsevents propagation. The client is [fsbridge](https://github.com/TechnologyAdvice/fsbridge) -- it's pretty self-explanatory. It's in Node, but using rs-notify as a baseline, it wouldn't be a challenge to implement in Rust!
I've been working on a pet project (originally started out as a testing project for dynasm-rs) of mine, a whitespace JIT-compiler. The structure of the thing is actually quite interesting. Internally, there are actually 3 different interpreters/compilers at work. A jit compiler, a simple interpreter and a bignum interpreter. Execution starts in the simple interpreter, while in another thread the jit compiler starts compiling code. Once a location where control flow joins is reached, the interpreter looks up if the next block is compiled. If it is, it switches over to native execution. Meanwhile, the compiler continues compiling further blocks, and stitching them together where possible. If an uncompiled block is reached, or an error occurs, the native execution returns with the state as it was at the start of the failing operation. This allows the interpreter to repeat the error if one occurred, allowing it to give detailed explanations of what went wrong (essentially allowing the native execution to not bother with errors). Lastly, if the error concerned an arithmetic overflow, the interpreter can fall back to the slow bignum-based interpreter. There are also various other tricks used to keep it fast. First, the jit-compiler attempts to keep the top of the stack in registers where possible, only spilling them when it runs out of temp regs or when an error occurs. To keep everything safe, the compiled code checks if the stack is large enough for everything in the following block only once, after which any stack lookups are nothing more than a `mov` instruction. While the jit compiler is busy compiling, it will modify previously compiled code to directly jump into newly compiled code where possible. Furthermore, it uses an extremely optimized map structure for the heap. Profiling showed that for heap-heavy code it would spend almost 90% of the time in heap management functions (at the time, the heap was a simple FnvHasher-based HashMap. Older whitespace implementations tend to use a resizing vector as a fast heap but this of course has problems where it runs OOM when you store a really big key). To combat this, I added a cache in front of the HashMap. The cache is a very simple static buffer addressed by the first 16 bits of the key. It is lazy with eviction, and doesn't even bother to put a value back into the cache if a read missed the cache. However, as in whitespace the largest part of the heap usage is writes followed quickly afterwards by multiple reads it served its goal. An additional benefit was that the cache retrieval code could be fully inlined in the JIT code eliminating any function call overhead on a cache hit. Now what kind of speeds does this result in? Benchmarking has shown that it is able to execute about one billion whitespace instructions per second on my 2.6GHz machine in general purpose, heap-heavy code. In code that only relies on the stack it was able to reach even 4 billion whitespace instructions per second which kind of surprised me as it was executing more than one whitespace instruction per clock cycle, where on average each instruction results in at least one instruction. Modern out-of-order superscalar processors are weird\*. This has certainly been an interesting project. It's a lot of fun exploring the more unsafe aspects of rust code (both calling into run-time generated code and calling back into rust from said code). It also was a lot of fun exploring ways to optimize the thing through profiling. It also taught me a lot about debugging (can't just call println!() from assembly after all) and the variety of ways in which a program can crash. I've also gained a lot more respect for compiler writers. x64 is kind of insane. (For instance, x64 `idiv` can overflow if you divide i64::MIN by -1. All other arithmetic instructions set the Overflow flag on overflow. But not `idiv`, it will cause a Division Error fault. So to check if you can safely even perform an integer division on x64 you must first check if the denominator is zero, and then if the denominator is -1 and the numerator is i64::MIN. \*one such example of weirdness occurred when I optimized code generation to reduce register moves when a duplicate value was immediately consumed. While as far as the code was concerned it simply removed a `mov` instruction between two registers this actually made it execute slower. 
So splitting on lines and converting to strings is 1 pass. While converting to string and splitting on lines is probably 2 passes. So the first method probably has better cache behavior. 
Continuing to read the new version of the Rust Book. I figure I'll just pause at chapter 8.3 and try to work on some small script things for practice. http://rust-lang.github.io/book/ch05-01-method-syntax.html
Thanks, now that I have a handle on how to use typenum I'll figure out how to express a child's size in terms of it's parent
Great! Glad to be of help.
Thanks for keeping the updates coming! [Here's a neat post about the architecture of TiDB and TiKV](http://weekly.pingcap.com/2016/10/17/how-we-build-tidb/).
Servo appears to have CoreGraphics bindings already: https://crates.io/crates/core-graphics If that supports what you need it would be a lot simpler to just use that as a Cargo dependency. Rust doesn't directly understand header files, so all the `#[link]` attribute does is link the framework into your output binary. You still need to manually declare all the structures, functions, constants, etc. you plan to use (which is what that `core-graphics` crate does).
Thought about the traits approach a bit more, and the repeat signature problem might not actually be that bad. Only traits that need to access the underlying world struct need a specialized implementation. The traits for higher level game logic can just require that the trait inherits the lower-level access trait and have all the high-level methods implemented directly in the trait in terms of the lower-level methods. Then all I need to do for the world type with those is `impl HighlevelLogicTrait for World {}`
Just started a side-project to learn about databases from the groundup. Nearly done with my B+Tree implementation. https://github.com/bbatha/bplustree
Ok, cool. This doesn't have what I need, but it has a lot of core stuff. What would be the best approach to add to this? Fork and link my fork as a dependency? Include this lib in my bin and add onto it? 
As part of my efforts to write a Rust-based XNB unpacker for the Stardew Valley resources, I'm in the middle of reverse-engineering the tIDE map binary format (and possibly getting sidetracked by writing a fancy binary file analysis tool using Conrod). This project is made of yaks, it turns out.
Just started a rewrite of a ptrace-based program sandbox I've written in C++ in Rust. I haven't gotten very far though; you can see my progress here: https://github.com/chaosagent/confine-rust. I also refactored much the the (non-compiling and abandoned) [ptrace](https://crates.io/crates/ptrace) ([my version](https://github.com/chaosagent/rust-ptrace)) crate for rust-nightly and the nix library.
I would not be shy to call JavaScript a bad programming language (I've used it fairly extensively both professionally and unprofessionally), but I would _definitely_ not call programmers who use JavaScript mainly or exclusively "not programmers", in the same way that Java and C++ programmers are still programmers despite those languages being towers of sorrow held together with spit and duct tape
Are you trying to disassemble x86? If so some time ago I wrote a simple length disassembler that gives you an iterator over the opcodes: [crate](https://crates.io/crates/lde), [docs](https://docs.rs/lde/0.1.1/lde/). Obvious disclaimer: I am the author. extern crate lde; use lde::{InsnSet, OpCode, x64}; if let Some((op, va)) = x64::iter(mem_bytes, segment.0).find(|(OpCode(bytes), va)| bytes.starts_with(b"\x0F\x05")) { // Found it at virtual address `va`. } Obviously won't be as 'fast' as `memchr` but may be helpful...
I think it's hard to answer your question without sharing more details about the actual problem you're trying to solve. For example, you may not want to actually read line-by-line at all!
How would i go about sorting a vector of structs in rust by one items inside the struct? 
&gt; A batchmate of mine was getting interested in it, and it sounded hella neat. Awesome, that's me! Glad I got you interested Jeff!
Well, 4-dimensionality itself might not be simple (although it really is simpler than it looks! ;) ), but the game certainly is - maybe overly simple, even :p
Very nice so far. Found a small error: in "Not just stdout, pls.", the very first function signature is fn write_header&lt;T:Write&gt;(seconds: u32, mut handle: StdoutLock) But you probably didn't mean for that &lt;T:Write&gt; to be there.
thanks!
I have a question on whether `Result&lt;(), Error &gt;` is idiomatic. I've seen it in places. Why isn't there an equivalent to `Option&lt;T&gt;` but more like `Try&lt;T&gt;` where you either get a `Success` or `Failure&lt;T&gt;`?
You probably want to post in /r/playrust
Looks like someone else screwed up if you're getting a segfault without using `unsafe` :-) Could you compile in debug (so not release) and run `gdb -ex run target/debug/hash_test`, then type `bt` (which will get you a stacktrace)? Edit: also you can escape gdb using `q` :D
It is idiomatic. () is the return type of a function called only for its side effects. Why make a new type when we already have a good one to represent this case? Making more than one "succeed or fail" type would mean that things that work with a computation that errors would have to handle both kinds, significantly increasing complexity.
The thing that panicked _is_ in result.rs, not main.rs. I agree that it's less than ideal, but I am not sure how you'd go about fixing it.
Agreed that this is annoying, but the very next line mentions to set `RUST_BACKTRACE=1` which will give you a nice backtrace including the line that called the unwrap. I just export that in my `~/.profile`. Wish it was on by default though.
what's up alex! How have you been? :D Yeah it's been in the back of my mind for years
The debugger won't detect overflows in the C library, only in the Rust code. If the Rust code relies on overflow, it should have been using `Wrapping&lt;T&gt;` or `x.wrapping_add(y)`; failure to do so in another bug (in addition to the segfault).
Pretty awesome to see some big companies other than Mozilla take up rust. Definitely makes rust feel like it has some strong staying power.
Great timing! Now I have another argument of type "That big, well-known company uses Rust too." :P
Actually, usually it does. Yet I have never seen `Err(())`
I couldn't agree more.
Option is about "absence vs presence", Result is about "success vs failure." Yes, these are in some ways similar, but that doesn't mean they're the same.
I doubt the performance of Linux. AFAIK you would have to add each file to the list of watched files. :(
I understand that argument, but that means that if you have a failure without an error message, it should be `Err(())` instead of `None`. But I have universally seen `None` as an indicator of single-point failure!
I'm on mobile and so I can't watch this right now, but I really really want systemd folks to stop using unsafe languages and start using something else (preferably rust, but go is fine too). I hope this is a step in that direction.
You use `sort_by`. Something like this: #[derive(Debug)] struct Point { x: i32, y: i32, } fn main() { let mut v1 = [Point { x: 0, y: 0 }, Point { x: -1, y: 1 }]; v1.sort_by(|a, b| a.x.cmp(&amp;b.x)); let mut v2 = [Point { x: 0, y: 0 }, Point { x: -1, y: 1 }]; v2.sort_by(|a, b| a.y.cmp(&amp;b.y)); println!("v1: {:?}", v1); println!("v2: {:?}", v2); }
I'll be glad to be proven wrong, but if you don't understand the need for the ownership and lifetime idioms, they look like hazing. If you've already had your hazing in the form of experience in C/C++ battles, then the need for them becomes intuitively obvious. But for a new programmer, "write this in C, so you'll see the need for writing in Rust with mutability and ownership decided a priori" comes pretty close to saying "Wax on, wax off, Daniel-san!" I'll be happy to eat my words though. 
This is super interesting. If I'm not mistaken, one of the reasons Facebook took up Mercurial (aside from its ability to scale) was that it was written in an accessible language: Python. The implications for Rust are pretty good here!
While that's true, it's also the least exciting place to be looking at.
Forking would probably be the easiest, as far as dependency management goes. It would also let you submit pull requests back upstream.
When was the last time you tried it? There has been a ton of perf work going into Mercurial the last few years.
Around four years ago, but git was so much faster that I switched. I also found git's concept of branch easier to use, but it may be me failing to grok something.
After reading chapter 6.1, this is where my skills are at: https://play.rust-lang.org/?gist=35ae2df75c47fd3aa6177a29da04ba07&amp;version=stable&amp;backtrace=0 As you can see, I clearly am not getting the hang of borrowing yet (even though I thought that I understood it)
This sounds incredibly ambitious... without knowing how many resources they are putting behind this I have to say I'm a bit skeptical. 
Thanks! (You're running into a weakness of ordering that we couldn't figure out how to best handle; the answer is in chapter 10. Using String instead of `&amp;str` and `Vec` instead of `&amp;mut [T]` solves 99% of your problems. The last one is using `{:?}` instead of `{}` to get debug formatting instead of display. With those changes, you're good: https://is.gd/DMp48i The lesson here is "for now, stick to types that have ownership rather than borrowing in compound structures." I'm going to open an issue on the book repo about this.)
Tomorrow please do https://github.com/erickt/stateful ! :-)
&gt; aside from its ability to scale Actually, if I remember correctly, Facebook made Mercurial scale. They have been the one pushing its limits even since they started.
What worked for me was *24 Days of Rust*.
Them and Mozilla...
Nope, just a talk about bindings to libsystemd... and mostly abound C FFI in Go and Rust.
&gt; But for a new programmer, "write this in C, so you'll see the need for writing in Rust with mutability and ownership decided a priori" comes pretty close to saying "Wax on, wax off, Daniel-san!" I'm saying new programmers don't *have* to learn C though.
I was at Facebook at the time, and I think the reasons behind the switch to Mercurial were a little more complicated than that. Neither git nor hg was able to scale to what FB needed at first. FB engineering is happy to write C when it makes sense*, but the problem with git (or what's great about it, depending on your perspective) is that all the guts are exposed. If you want to replace the `objects` folder with cloud storage, for example, you have to change code all over the place, and you'll break a lot of wrapper scripts (both official and unofficial) that make assumptions about how everything is laid out. I think Mercurial had a few things going for it: its internals were more abstracted away, Python made it possible to monkey patch things without rewriting as much, and the Mercurial team was more willing to merge these sorts of changes. \* In fact, a big part of how they made Mercurial scale was rewriting a lot of performance sensitive code in C.
A little bit of PowerShell (ran as administrator) goes a long ways: # (system wide) [System.Environment]::SetEnvironmentVariable("RUST_BACKTRACE", "1", "Machine") #(user) [System.Environment]::SetEnvironmentVariable("RUST_BACKTRACE", "1", "User") 
So far this mercurial thing is still under development though, and it's not clear if they will continue to use Rust -- currently it's a project worked on by mostly one person who did it in Rust, but they might decide to switch if they're getting more people involved. IIRC the friends page is only for "production" Rust code, i.e. Rust code you're betting money on. They'll get there :)
Personally, I'd just write a Go implementation of the systemd wire protocol. It'll probably perform better than a C binding, because a native Go library can avoid switching stacks. Also, you're less likely to invoke UB in your bindings. Rust doesn't have the performance problems, but I'd still write the wire protocol in Rust to avoid the UB.
You'll also want to fix the styling for the code boxes. It looks like [this](http://imgur.com/8gkhriT.jpg) this when on mobile. That being said I enjoyed your article. Useful, short, sweet and to the point!
In war, victory is never absolute. The conquerors take the spoils: the vast bitfields of the Proprietary, the READMEs of the Open, even the Handbook of the Novice. After all of these are taken, what remains? The rebels, the dissidents, the Emacs users! More insidious even than these remains NIHilists. They walk unseen among the reformed; in dark corners, they whisper of the evils of those things Not Invented Here. They do not show their faces, but rather sow the seed of doubt. "If we rely on this Conqueror's gift, what will we do in times of greatest desperation?" Even after the rebels, the dissidents, all of the others have been crushed under the weight of popularity, the NIHilists remain. Sometimes they have sown the seeds of life into barren, motionless fields, but more often than not they have sown the seeds of relapse into the times of war. Let us all keep careful watch and stand fast against such dangerous people.
Ugh, sorry. Yeah it looks different on different devices. I want it to be horizontally scrolling for the code box overflow on everything, but I haven't cracked that nut yet. Thanks for reading anyway!
Sorry if I wasn't clear. The overflows is happening in the Rust code...well it was but I offloaded the hash calculation to the AF C API.. Thanks for mentioning the wrapping type and method, I'll look into that.
The usual recommendation is "every type should implement Debug", but I'm not aware if there's some reasons they should be an exception.
So, digital only ;)
If you don't want to fix the wrapping issue right now I think you can *probably* get away with adding the following to your Cargo.toml: [profile.release] opt-level = 0 debug = true I haven't actually tried but I suspect that it should disable optimizations and enable debug-info when creating a release build, while still disabling overflow checking. Of course you would want to remove this again when you have obtained a stacktrace because you probably want optimizations when compiling with `--release`.
All of the JS engine, all of the DOM implementation, the vast majority of the media stack, all of the networking code, all of the HTML parser, off the top of my head. There are lots of parts that make up a web browser!
¿What the community thinks of async iterators like the proposed for javascript: https://github.com/tc39/proposal-async-iteration? How high is the priority in this kind of syntactical beauties?
Title in the post (and this I suppose) should be Serv*ice* should it? Like in the github repo
Oi, don't go tossing us emacs users in with them ruffians! ;D
I'm not sure what requirements you have in mind for a document database, but in principle you can write anything in Rust. I think it's more of a question of how much work you're willing to take on versus relying on libraries. On the one hand, Rust being a young language may not have all the libraries you need. On the other hand, you can bind to C and C++.
No worries! Its a heads up in case you didn't know :D
Yeah, it looks like that's the case. That would explain why a lot of inotify tools want you to increase the limit for watches, it doesn't support directories.
Yeay! This was one of the things I was most excited about at rustconf. 
Sure enough the first This week in Servo in 2 weeks just happens to have 9 screenshots of Firefox and Stylo side by side, to my untrained eye there doesn't seem to be much difference between them anyway.
Do you think this is worth opening an issue?...I'd be willing to work on it when I get some free time.
&gt; Ironically, in the Visual SourceSafe era, Microsoft used a different in house SCM. If you've ever had to use Visual SourceSafe, this is not surprising at all.
Maybe. There is a language server niece (Omnisharp) just for C# which runs on vim, emacs, atom, sublime and others, but I don't know how much effort one need to port it.
I'm implementing my own language server for a different language and I have a question about your`http` mode. The lsp is itself `jsonrpc` over *http-ish*, so why do you have two?
&gt; Power of C/C++ with safety whist not using a garbage collector. Hard to argue with that statement; that's the best of both worlds IMO. My biggest fear of getting engulfed by Rust is that there may be a shortage of much needed libraries. Glad to hear this is not the case! 
I would suggest taking a look at TiDB/TiKV, the low-level component is in rust and the higher-level component is in Go. You might be able to build on top of TiKV and build a document database.
Requirements are basic read/write and basic lookups/filtering. I plan on implementing it as a toy mostly for learning purposes. However, if I feel like it is viable, I could make it an actually package/application.
&gt; TiDB Thanks. Regardless of the fact that it uses Rust, that project seems awesome!
Though in the past year a lot of great libraries have been produced by the ecosystem. The difference in solid libraries in my experience from 1.0 to now is nothing short of amazing
Great, now I'll never be able to tell the difference between an /r/rust submission and an /r/playrust one :p
Not to mention that cargo makes it easy to cross-compile or build a 100% static Linux binary. rustup target list rustup target install i686-unknown-linux-musl cargo build --target=i686-unknown-linux-musl --release
The rust book: https://doc.rust-lang.org/book/ is probably a good place to start. Also, found this, which seems to be aimed at cpp programmers: https://github.com/nrc/r4cppp Good luck!
Ah, found the answer: https://internals.rust-lang.org/t/introducing-rust-language-server-source-release/4209/7
The big innovation over C++ is the concept of lifetimes. You already know that code like this char* get_ptr(std::string&amp; s) { return &amp;s[0]; } ... char* name; { string name_string = "hello"; name = get_ptr(name_string); } cout &lt;&lt; name[0]; is invalid, right? Rust adds extra annotations to let the compiler stop you from doing that sort of thing. Think of the difference between the following two functions: const char&amp; first_letter(std::string&amp; s) { return s[0]; } and const char&amp; first_letter(std::string&amp; s) { static const char* letters = "abcdefghijklmnopqrstuvwxyz"; return letters[s[0] - 'a']; // Pretend I know it starts with a lowercase letter } . They have identical signatures, but very different behavior with respect to when it's safe to call them. With the first one, you need to make sure you never dereference the returned pointer after reallocating or freeing the string you gave it. With the second one, you can do whatever you want with it, since it's pointing to static memory instead. In Rust, these two functions have different signatures. fn first_letter&lt;'a&gt;(s: &amp;'a String) -&gt; &amp;'a char vs fn first_letter&lt;'a&gt;(s: &amp;'a String) -&gt; &amp;'static char . The `'a` is the name of a lifetime, which is some span of code between a constructor and a destructor. The first says "for all lifetimes, if you give me a reference to a String that is valid for that long, I'll give you a reference to a character that is valid for that same period". The second says "I'll give you a reference to a character that is valid for the entire length of the program". If the compiler sees that a caller is violating the contract of the first function signature, that the returned reference is forgotten before the string given to it is destroyed, it will prevent the program from compiling. On top of that, it's a much nicer language to code in than C++ thanks to not having decades of cruft and borrowing nice features from other languages.
Statement attributes were stabilized. There's still debate over what kinds of expressions should support attributes.
Thanks for that, looking into it now. First time I've needed to do something like this.
https://github.com/rust-lang-nursery/regex/tree/master/regex-capi is a full project; http://jakegoulding.com/rust-ffi-omnibus/ should be very useful to you as well.
Since no one else has mentioned it, the main reason I use Rust is because of the ML-style type system. Once you get used to good generics and sum types (enum in Rust) and type inference, you never want to go back.
The typescript language server also works (for some value of "works") with vim, using some kind of JSON protocol, which I suppose is the same.
ಠ_ಠ ... This would be talking about a person's personal lifetime work on low level multi-disk OS drivers using Rust ... right... ?
I have no idea whether this is likely to be of interest to many people here, so I figured I'd just throw it out there and find out! The code is on GitHub: https://github.com/jeffparsons/planetkit
I sure hope so.
Did you try rustdoc?
This. After you use Rust for a few months and go back to C/C++, you'll write a lot less segfault errors, because the borrowing rules are basically engraved into your fingers.
Really not an issue at all, but you have a raw string in `main.rs` which which doesn't contain double quotes inside it but is still prefixed with a hash, i.e.`r#"..."#`. A raw string can have 0 or more hash marks, so if you don't have any double quotes in your string, you can just use `r"..."`.
Well, it does support directories, but not trees.
That sub-repo cloning sounds awesome for large index-like repos like Cargo or the Arch User Repository
They should be the same in fact. Stylo replace other components so it needs to do at least the same things as before
&gt; pcwalton improved google.com He’s not working for Mozilla any more? :)
Thanks for the feedback! I see, I wasn't aware `r"..."` was possible, that's good to know. In this specific case, I'm pretty sure it's going to bite me some day if I change that though. Unless the compiler will tell me?
Maybe an other option would have been to use [syntex_syntax](https://github.com/serde-rs/syntex)? Using that myself on an project which requires parsing source code but want to be able to run on stable channel.
libsystemd is part DBus calls, part direct filesystem access (usually state in /run – faster than an IPC call). And "DBus calls" themselves consist of wire protocol (binary format) and API (method names &amp;c). The [D-Bus wire protocol](https://dbus.freedesktop.org/doc/dbus-specification.html#message-protocol-marshaling) has been stable for many years, with several independent implementations. (Though kdbus did try to introduce a new syntax, based on the GVariant wire format.) The systemd API is another question, but _most_ of it is covered by the "interface stability promise".
Rule #2.
If you've ever spent two days debugging memory problems, then you can imagine experience with Rust as someone telling you exactly where that bug is before you ben know that there is one. Rust doesn't allow you to compile code which contains memory bug. (If you really have to interface with C libraries or hardware, you can use `unsafe` marker to allow doing so. Then if your software crashes, you know you can find it there.)
It's actually a bad graph because it also included workers of a different type there. But there will be a blog post soon with details :)
I don’t know the situation for this specific gradient interpolation issue, but for web features in general: If there is consensus (interoperability) among rendering engines (especially those with market share) and it is against the spec, the spec should be changed. If there is and it is according to spec, many websites probably rely on that behavior no matter how "wrong" it is, and "fixing" it might break these sites. Sometimes, enough browser vendors can be convinced to change their engine’s behavior because the breakage is likely to not be very significant. But that’s generally not easy to pull off. In any case, it doesn’t help anyone if Servo unilaterally goes against consensus. I’ll be just one more web-compatibility bug in Servo.
I think this idea of a common library of gameplay tools (like pre built character controllers and such) is an interesting idea. How would you compare your *idea* to the Unity and UE4 asset stores? Arguably they have the same purpose, though obviously they don't help Rust game devs much. 
Yeah that's a pain, hoping incremental compilation comes out soon enough!
&gt; The RLS gets its source data from the compiler and from Racer. Where possible it uses data from the compiler which is precise and complete. Where its not possible, (for example for code completion and where building is too slow), it uses Racer. Questionable decision. Racer failed to provide proper autocompletion for several of the _introductory_ programs in the Rust Book back in January. I [reported](https://github.com/phildawes/racer/issues/482) one of those. Has it really improved that much? EDIT: I love that RLS is happening though. (=
I have to get it to work first!
&gt; A large part of why other communities are so toxic is because they're mostly white men. Having a highly diverse community means that you have a community that will fight against misogyny and racism, and is more welcoming to people of all walks of life. This implies that the dominant form of hostility in other forums is sexism and racism. I find this hard to believe. I frequently see attacks and blind zealotry against all manner of groups; I think it's more likely that the toxicity caused the prevalence of white men than the other way around. If people frequently turn disagreements into fights, it makes sense that minority parties would get excluded. This isn't to say that diversity efforts are unhelpful, but I feel they do more to treat the symptom than the cause of a damaged community.
It would be great if this allowed Vim to have the same features as IDEs have.
Right now I have this fn that I want to add some safety checks: pub fn get_pixel(&amp;self, x: i32, y:i32) -&gt; &amp;[u8] { let rgba = 4; let sx = (y * &amp;self.width) + x; let start = sx * rgba; let end = start + rgba; &amp;self.pixels[start as usize..end as usize] } How do I check if the vector slice is valid to prevent out of bounds error? (I want an actual check on the vector rather than just checking if the x and y are within the image dimension.)
&gt; The trade-off is that the compiler's "borrow-checker" is brutal. I'd have to qualify that observation, because the compiler's persnicketiness is nowhere near as annoying as the experience of watching your code be rejected by a continuous integration server over the same issues. 
Well, if Rust is used as the Mercurial server by Facebook, given how it hosts their whole codebase, I would argue it qualifies as production use ^^ But yes, long road ahead...
I think one aspect may be the technical level required to use the language. Someone who might consider themselves genius level in JS or Go soon learns humility when faced with Rust and the Rust compiler. Either they give up, or persevere and learn something, which means they have more respect for other newcomers.
&gt; assertive, aggressive, and are taught to argue loudly for what they believe. These are negatives? o.0 How else do you ensure the correct technical decision is reached?
This is quite clever. I imagine that one would want to extend this further to prevent the containers from having to be running apriori, but I doubt that is in the scope of the utility you've built.
I really think this package should get a new name... I got excited that it was an enhancement to a perlin noise algorithm.
Me too, always gets me.
I think this is the crux of the idea behind a language server: going from a combinatorial explosion (M languages * N editors * X features), the complexity is somewhat reduced to (M languages * X features) + (N editors * X features), where M and N are the leading factors.
I've has some lousy experiences with MySQL in the past, [like this](https://grimoire.ca/mysql/choose-something-else). Are these concerns valid for TiDB?
It's great to see some other Rust people working on Docker-related projects! We just announced [cage](http://blog.faraday.io/announcing-cage-develop-and-deploy-complex-docker-applications/), which tries to make it easier to develop large Docker applications with lots of services. Is there any Docker-specific code that we might be able to spin out into a library and share? Right now, we have `docker-compose.yml` parsing, and we're thinking about adding `docker ps` support for querying service status. I don't know whether either of those would be useful for Habitat.
A lot has been open sourced, for example https://www.mercurial-scm.org/wiki/FsMonitorExtension. Not sure about all. There might be a lot of tweaks that only make sense in the FB environment?
I thought about a Perl wrapper. *(Why would anyone want such a thing?)*
&gt; A large part of why other communities are so toxic is because they're mostly white men &gt; Having a highly diverse community means that you have a community that will fight against misogyny and racism You mean like fighting you for racism?
This was interesting for sure, maybe you can use some of its ideas https://www.reddit.com/r/rust/comments/55ns2m/safe_and_efficient_bidirectional_trees/ 
I mean, it's not being used _yet_ :)
openocd supports semihosting without any of the extra python work here: 'arm semihosting enable' is the monitor command that turns it on.
Sadly `iron` is still based on a thread-per-connection model (last I checked anyways). If absolute throughput is necessary for your usecase, you might need to look into building your app on top of `tokio` or one of the other async IO models.
My main goal here is to understand the right way to do this in Rust so while PetGraph isnt directly useful to me, I think reading its source might be very enlightening for me. Thanks for the pointer!
To avoid repeating myself, [here's another comment where I address the topic of "racism against white guys"](https://www.reddit.com/r/rust/comments/589563/how_can_we_preserve_our_friendly_rust_community/d8z5pdi).
&gt; As I understand it, "racism" doesn't just mean discrimination or prejudice. "Racism" implies prejudice and the power to act on it If I understand that right you're saying that racism can only be done by people in power? Is this the Tumblr thing where blacks can't be racists because they are not in power? &gt; in the United States and Europe, white men hold a lot of social, political, and economic power Yes because Europe is mostly white and so is the US because it was founded by Europeans. So of course the amount of white people in power is high(er). &gt; I'm suggesting that we have an emphasis on brining a more diverse crowd of people into the community as we grow. Diversity only to have diversity is useless. Diversity that brings verifiably/provable improvements to a community is useful. Also I would like to see evidence that * 1. the majority of people in the Rust community are white males * 2. that those white males are the reason for a toxic social environment.
Whoa, I didn't know that (and am using st-link anyway). I'll make sure to add this later. Edit: I've added a section referring to the functionality. Thanks.
Some traits are in the core library, some in the standard libraries. Traits are namespaced, and if you want to use a trait in a context, you need to `use` that trait (unless it's a part of the standard prelude or already in your scope).
Surely referring to another community as 'pure cancer' runs afoul of rule #2 as well? I find it ironic that someone could refer to a community that they personally belong to as helpful and welcoming, while at the same time comparing another community to a slow and deadly disease that will kill about 1 in 7 of us. When you find yourself referring to others as 'pure cancer' perhaps you need not look far to find that 'occasional asshole'.
No, there will be no conflict. The existing trait will simply be shadowed. Thus you will have to address it explicitly (e.g. use `::std::cmp::PartialEq` instead of `PartialEq` if you want to use the builtin trait given you have a custom trait named `PartialEq` in the same module).
Awesome, exactly what I was looking for. I always forget about shadowing. I should make an effort to use it more.
I would guess that you can use [Universal Call Syntax](https://doc.rust-lang.org/stable/book/ufcs.html) to disambiguate between any traits using their full import path.
&gt; That's not the community. There is no data about the distribution of the Rust community and there never will be one because it's impossible to ask every Rust user on the planet. Well that's convenient. So we can never make any statement about the demographics of the Rust community because we can't ask *everyone*? &gt; So "everybody is white male" is useless. I never said that everybody is a white male, what a lousy attempt at a strawman. You can even see multiple women when you follow the link I posted. &gt; Because non-whites and non-males aren't racist/sexist. Again, I never said such a thing. &gt; In fact, your comment is sexist and racist. Please do explain which part, so that I might better understand myself and enjoy personal growth.
Isn't this to disambiguate function names, not traits?
/u/khronnuz , AFAIK iron is based on hyper and hyper is already using asyncio in latest version. I'd ask first in iron github issue about that.
&gt; Well that's convenient. So we can never make any statement about the demographics of the Rust community because we can't ask everyone? You can make assumptions about the community by aggregating data and, if the data seems to represent a valid extract of the community, you can then claim generally valid assumptions about the community. That's how it's done in science. &gt; You can even see multiple women when you follow the link I posted. Did you just assumed their gender? Triggered. &gt; Again, I never said such a thing. Right, you said "fact that its elite consists primarily of white males could be used to support the argument that the community is racist / sexist" so you didn't say it but it implies that statement. &gt; Please do explain which part, so that I might better understand myself and enjoy personal growth. The part above where, from my point of view, you implied that because the majority of the elite in power is white and male it leads to a racist / sexist community because white males behave this way.
Not sure about that. The master branch is, but the version on crates.io doesn't look like it is.
[Also, the rust book has a nice chapter about FFI.](https://doc.rust-lang.org/book/ffi.html)
Indeed, let me fix it right away!
Let me know how it goes!
&gt; Did you just assumed their gender? Triggered. Please keep comments constructive, and keep in mind community openness. Joking about "triggering" is disrespectful to people have deal with post-traumatic stress, and doesn't encourage community engagement.
I feel you have conflated me with /u/DroidLogician. I would not have chosen the same words as him, but, at the same time, I do not think he intended to be hostile.
This is the more detailed follow up to the graph that was submitted here [yesterday](https://www.reddit.com/r/rust/comments/588kft/current_status_deploying_rust_into_production/?st=iuhg3p49&amp;sh=af978ff7). Ask me anything!
I have not, I was just surprised to see you remind /u/kodifies of the rules, but not /u/DroidLogician. The 'you' in my second paragraph was more a general 'you' than *you* in particular. Perhaps I should have used 'When one finds oneself...'.
&gt; I want to encourage that we make it easy for other groups of people to join the community. This sounds like a good base for the Rust community, I'm in.
Perhaps your beliefs would be better stated as "everyone has unconscious biases/prejudices based on their experiences. Because of this a more diverse community will be better at being welcoming to a wider demographic." This, I think, is a fairly uncontroversial statement and it has less assumptions that go unstated.
Absolutely. I think that's a good way of putting it.
Wow, bravo, I loved reading this. I have heard so many good things about Rust and as of today I have started this long journey. Truly motivational!
If someone tries to be constructive, I tend not to be fussy over the details. It's hard to say nothing offensive, especially when talking about behaviour you dislike. If you escalate too soon, you risk causing strife rather than preventing it. This is especially true through lossy mediums like language. 
Just started learning Rust. I'm still trying to work my way through "The Rust Programming Language". I've been learning C++ for a while now but Rust looks interesting so I'm taking some time to learn it. I get the feeling though that it feels more similar to C or Haskell than C++. 
Beaten to the punch by 15 min: https://www.reddit.com/r/rust/comments/58d0lu/fixing_python_performance_with_rust/
I wrote this code for fun, it's a Sieve of Eratosthenes (finding primes). I was originally using an AtomicBool, but then I realized I shouldn't have to as it wouldn't have any concurrent issues (if written in a language where it would compile, say C). I'm assuming AtomicBool adds overhead. However, I'm having a hard time getting it to compile in Rust without AtomicBool: use std::env; extern crate rayon; use rayon::prelude::*; fn main() { let nums: usize = env::args().nth(1).unwrap().parse().unwrap(); let mut numbers = Vec::with_capacity(nums - 1); for _ in 2..(nums + 1) { numbers.push(true); } (2..nums).into_par_iter().for_each(|i| { let mut num = i; while num + i &lt;= nums { num += i; numbers[num-2] = false; } }); let mut sum = 0; for i in 2..nums { if numbers[i - 2] { sum += 1; } } println!("{}", sum); } The problem here is obvious, I can't have a mutable borrow there. That would make sense since I could also read from it or write multiple kinds of values. However, in this case I'm not. Is there any way to create a reduced access reference for this situation?
His definition is the definition used by sociologists who stupidly agreed it made sense to conflate prejudice with classic racism and power dynamics to define a new definition for 'racism'. I agree it's stupid.
Do you expect to continue using Rust to improve the performance of your application? What's next? The top comment on HN is about Cython. Can you expand (here or there) why you didn't choose to use that?
And by Armin himself :)
Gah! I expected the duplicate checker to catch it, I was suprised he didn't.
It seems they may have given the wrong impression by using a dumb definition of racism.
Working on the same problem, (although starting out with a serial version) I assumed that the compiler was simply not intelligent enough to catch that your "data race" won't cause an issue because it's idempotent. I can't think of a non-atomic solution beyond opening the plastic cover and pressing the big red `unsafe` button, since any wrapper struct that did some sort of guarded-write would have to touch the value at some point, which would then trigger the same issue. (I'm by no means an expert at Rust, though, so I'd love to see a better method)
Hmm, yeah a struct with an unsafe implementation `OneWayBool` could definitely solve the issue.
I am also working on a prime sieve, and have this: fn main() { let mut values : Box&lt;_&gt; = Box::new([true;1_000_000]); let len = (*values).len(); let step_limit = (len as f64).sqrt() as usize; let mut step_size = 2; let mut cur_pos = 2*step_size; let start = Instant::now(); while step_size &lt; step_limit { while cur_pos &lt; len { values[cur_pos] = false; cur_pos += step_size; } let finish = Instant::now(); let duration = finish-start; println!("Finished {:} in {:?}", step_size, duration); step_size += 1; while !values[step_size] { step_size += 1; } cur_pos = step_size*2; } } ...which blows the stack if the length of `values` becomes much greater than a million. I'm confused to why, since there doesn't appear to be any enormous stack allocations. (Have I misunderstood how `Box` works?) Any help?
I *think* this is safe, it seems to work: https://gist.github.com/anonymous/b916593f4c0f4258f16b5e1e73a1d046
We were just talking about this at work, awesome to see this article! One of the most likely ways i can get Rust into our camp is by dropping in very specific bottleneck improvements via this exact scenario. With that said though, arguably this still seems very complex. Is this likely to improve in time at all with Rust? Or is this as good as it gets?
I think with Higher Ranked Trait bounds, and some sort of trait currying we could make a wrapper struct that we stick `IntoIterators` into. trait Functor { fn map&lt;A, B, F: Fn(A) -&gt; B&gt;(self, F) -&gt; Self&lt;B&gt; } struct LazySeq&lt;A, I: IntoIterator&lt;Item=A&gt;&gt;(I); impl&lt;for&lt;I&gt; I: IntoIterator&lt;Item=A&gt;, A&gt; Functor&lt;LazySeq&lt;I&lt;Item=A&gt;&gt;&gt; for LazySeq&lt;I&lt;Item=B&gt;&gt; { fn map&lt;F: Fn(A) -&gt; B&gt;(&amp;mut self, f: F) -&gt; LazySeq&lt;I&lt;Item=B&gt;&gt; { LazySeq(self.0.into_iterator().map(f)) } } I feel like I might be cheating with the associated type stuff, but I think there might be a solution along this path. It could also implement Iterator and IntoIterator so that it would be backwards compatible. And maybe have an IntoMonad trait that LazySeq implements, so you can use existing iterators generically.
The first sentence of the comment you're replying to seems to be an answer to the "Did you consider using cython?" question, maybe you could clarify what more you're looking for?
Segger have an "educational" version of a JTAG debugger: https://www.segger.com/j-link-edu.html?page_request=j-link-edu.html It seems to be a J-Link Base without really any restrictions, and, for $65 from Digi-Key, it's not worth farting around with things that kinda, sorta work if you're just doing hobbyist stuff. They released this because the Chinese clones were eating them alive. That being said, the J-Link Base isn't out of line at $400 for a professional tool if you're using it for work. And it works with just about anything that needs JTAG.
I like how the biggest issue was tooling instead of endlessly chasing pointer crashes and using valgrind
Well, this seems to work: struct A { f: Box&lt;for&lt;'a&gt; Fn(&amp;'a i32)&gt;, data: i32, } impl A { fn m(&amp;self) { (self.f)(&amp;self.data); } } fn main() { let a = A { f: Box::new(|x| println!("{:?}", x)), data: 42, }; a.m(); } 
That is one use, but it's sometimes useful at other times.
Dude that's awesome. Thanks so much.
&gt;His definition is the definition used by sociologists who stupidly agreed it made sense to conflate prejudice with classic racism and power dynamics to define a new definition for 'racism'. This would be a valid line of reasoning if the concept of institutional racism didn't predate the concept of individual racism. But even putting that fact aside, words have complex meanings that are defined by countless historical, political, and social events. It's not a "stupid" definition, it's just one you're not familiar with.
Insisting that they're racist for wanting more diversity in Rust's community is absurd. Nobody with any amount of sense buys the whole "anti-racists are the real racists" argument. You're being incredibly uncharitable if you're aware of what they're trying to say and still insisting that they're racist. 
&gt; Using ancient CentOS versions to build somewhat portable Linux wheels with Docker. While this process is tedious, the difference in stability between different Linux flavors and kernels make Docker and CentOS an acceptable build solution. Would musl help on that? IIUC if you compile to a musl target and statically link it, the resulting binary should work on any linux distro, since the translation from libc to syscalls is included in the binary.
Your syntax is unclear to me (and internally inconsistent I believe). I'm not sure exactly what you're trying to express, but it seems that you want `LazySeq` to be a type with a higher rank type parameter, so that you can have a `LazySeq&lt;T&gt;` contain any arbitrary type which implements `IntoIterator&lt;Item = T&gt;`. Then `Functor` would be implemented by the `LazySeq` constructor. To define such a type doesn't only require a higher rank trait bound, it requires an actual higher rank type. Such types are not possible in Rust as far as I know - because of monomorphization &amp; Rust's memory model, every value of a type has to occupy the same number of bytes.
It's kind of unintuitive, but that array is stack-allocated before being moved to the heap. It's kind-of unavoidable; sometimes LLVM will elide the stack-allocated copy, sometimes it won't. You can go straight to the heap with the unstable `box` keyword, but instead, you should use `vec![true; 1_000_000]` which won't overflow the stack, and can grow dynamically. However, since `bool` is a glorified `u8` that only stores one value, that's a lot of wasted bits. If you want even more compact representation for your sieve, and don't mind importing a crate, I recommend [`bit-set`](https://crates.io/crates/bit-set). It'll cut the memory usage of your boolean array by a factor of `8`, give or take--since it allocates in amounts of `u32` by default, it'll overallocate a few bytes here an there, but it's still a massive savings overall. Starting with all bits set to `true` is as easy as let mut set: BitSet = (0 .. len).collect();
That is exactly my holy grail too. Let me know when you start building it, so I can start using it ;)
I'm not very familiar with the UE4 asset stores, but I imagine the biggest difference is going to come from Epic Games being a giant AAA studio, and me being one person who is also very passionate about his day job... leaving not much time for this sort of thing. David and Goliath, except that I guess in this story both David and Goliath are both basically cool people doing their own thing. :) I agree with you that they have essentially the same purpose. So the purpose of my idea then becomes trying to find out how you might be able to apply that to a free software community. If what I'm doing gains any traction, then ideally I'd try to find a home for it in an existing community, e.g., Piston. In practice I think the most obvious difference would be that even in the best case, the bits I work on are probably going to be applicable to a much narrower set of games. You may notice a common theme in the kinds of games I'd like to build using this thing: - Block Dude, but 3D. (Most easily achievable.) - A family of small Minecraft-esque games, each with an extremely narrow focus: - Complex organic worlds at universe scale tech demo - Playable death-match in tiny arenas on the surface of random planets with somewhere between 1-3 weapons - P2P MMO tech demo Each of these has a massive amount of overlap with the others, so they're most likely to help people build other simple games that are also based around procedurally generated voxel words/universes with first-person control of a single character.
Thanks for your comment. I already have both those articles bookmarked, and intend to liberally steal ideas from them. :)
Howdy folks, I'm locking this lovely feast of irony as it's veered way off the rails. To answer OP's question myself: don't bother fretting about it. Strive to view everyone that you interact with as a fellow human being, and hope that they afford you the same courtesy.
Hi steve. Anything to add some safety checks, and I'm ok with changing the function return to Result. Do you mean something like this? pub fn get_pixel(&amp;self, x: i32, y:i32) -&gt; Result&lt;&amp;[u8], Error&gt; { let rgba = 4; let sx = (y * &amp;self.width) + x; let start = sx * rgba; let end = start + rgba; try!(&amp;self.pixels[start as usize..end as usize]) } I was thinking something worst like: if &amp;self.pixels.is_valid_slice(start as usize, end as usize) { &amp;self.pixels[start as usize..end as usize] } but the try! Result looks better. 
No, currently it's definitely not optimal. Especially because fuzzy search is not explicitly implemented. But one could think of implementing fuzzy operators (at the moment only less than or greater than come to mind) corresponding with fuzzy query atoms which don't need to search the whole vocabulary and still return all the matching term ids. With that additional abstraction one could use optimal fuzzy search for arbitrary types. 
As much as I'd love to do everything in Rust, I'm not willing to sacrifice progressive enhancement, so I've been sticking to Python+Django+Bootstrap+jQuery as fads come and go. (eg. I always patch my Bootstrap so the drop-down menus operate on hover so they'll work with JS disabled.) However, if a "transparent data synchronization" Rust framework came about which made it easy to produce apps which fell back to doing everything server-side and had Django's SQLite+PostgreSQL-compatible schema migration support, I'd definitely be interested.
It would feel petty to open a PR just for this, but there's a typo in the `random_field_for_optimiziations_only` field name. Cool project!
If you want `MyTrait` to act like `Any` as well as having its own functionality, you want [mopa](https://crates.io/crates/mopa).
A bit of a n00b question, but how does the thread-per-connection model inhibit performance? My intuition tells me that having each connection running in its own separate thread would give the best performance possible. Is that not the case? Is it the os-level thread context switching that kills the performance? My experience with using libraries like `libuv` for async programming (albeit not in rust), restricts the whole uvloop to running in a single thread, thus eliminating the thread context switching, but not taking advantage of multiple processor cores/hyperthreading. Does `tokio` work differently? 
Would love to hear a little bit about how the JIT works!
Thanks for the detailed and clear reply. I guess I will start experimenting with at least the 32-bit ARM and maybe write regression tests myself for my use case that can run also on one of the desktop platforms as a additional sanity check. 
Yeah, might be an option. I just went with the setup i have for another library an there i already went down the centos path.
I have also done it. "maybe_result" type for C++ that is similar to rust result: https://github.com/trafi/maybe-result-cpp Currently using it in prod on both iOS and Android.
Better to be petty than leave it as is.
Thank you :)
Blocking I/O and context switching involve a lot of bookkeeping, cache misses and pipeline stalls. Jumping from userspace to kernel space and back, and all the associated operations, is very expensive. On top of that, spawning extra threads has diminishing returns: each thread requires memory for its stack space and other associated data structures, and each new thread competes for timeslices in the kernel scheduler, reducing the amount of time that can be dedicated to your application code and increasing the amount of time the kernel has to take to keep track of all of this. This makes 1 connection/thread models highly self-limiting. The big deal with async I/O is that you can handle many connections per thread. Instead of blocking while waiting for data from one client, you can be reading bytes from another connection that are already ready and processing them, or writing out bytes to other connections, or polling for new connections. The syscalls for these operations are comparatively inexpensive, because they're just checking the kernel for messages queued for these specific connections, or asking the kernel to queue messages. It is true that these event loops don't parallelize well. But that's mostly just so they don't require expensive synchronization. `libuv` [can have multiple threads, each with their own event loop](https://github.com/kristrev/libuv-multiple-loops/blob/master/libuv-test.c), though I'm not sure if the design of the library prevents them from running concurrently or not. I actually had a look through `tokio`'s API, and the story with threading is an interesting one. When a TCP socket is bound for listening connections, it can only accept connections from one event loop. However, accepted connections can be sent to other threads (i.e. other event loops) to be processed, and these threads can be pooled so the connection/thread ratio can be dynamically controlled. So `tokio` is truly scalable. The other great thing about `tokio` is that you can have multiple protocols sharing an event loop, so you can wait for TCP connections on the same thread that [handles interactions with your Redis database](https://github.com/tokio-rs/tokio-redis), while also waiting for bytes to be received or sent from some number of clients.
I'm not terribly familiar with the UE4 asset store, but I have used Unity's which is similar. The resources on the store aren't all officially made or curated, they're open to the community and a lot of different people upload a wide variety of stock assets for other people to use. Overall it sounds like you're aiming to help fix one of the major missing pieces of the Rust game development story, which sounds like a good idea to me :) I'd be interested to see if it'd be possible to build portable (with regards to game engines) game dev tools. I know I don't personally use Piston but it'd still be nice to be able to use existing tools in my hacked together personal engine. I don't have high hopes that it's possible, but who knows maybe someone will figure out something clever.
As mentioned elsewhere this links against libpython which comes with its own sets of problems. I intentionally decided against that.
Your first bit is close, but `[]` will still panic, so `try!` doesn't actually work there. You could do one of two things: * check `start` and `end` and return an `Err` * use `get` instead of `[]` and convert the `Option` to an `Err`.
A popular form of fuzziness is edit distance, and a popular implementation of that is Levenshtein. Lucene will actually build a Levenshtein automaton for edit distances within `N` edits from the query and intersect it with its term index (which is also a finite state machine). This approach is nice because it will completely skip keys that can't possible match within a certain edit distance.
Thank you! I was pleasantly surprised by both Rust and rayon. Rayon was dead-simple to use, although I did puzzle over handling `Result` until the rayon team helped me out on gitter. They're going to add some convenience methods to make this easier! But `rustc` really stole the show. It actually walked me, step-by-step, through (1) converting all my affected traits to require parallel-safe implementations, and (2) fixing the one bug that didn't allow parallel code. Rust makes me feel like a genius. And the libraries are awesome.
I'll probably do a bigger writeup later but here's a summary. The jit compiles in extended basic blocks. It compiles starting after a label or call, until it hits an instruction where control flow join is possible (labels, unconditional jumps, calls, returns). Each of these blocks has two entry points. The first entry point is for when the interpreter calls into the jit. It will organize the stack and several registers for the jit. After this is the second entry point, for when another jit block jumps into this jit block. It then calls JitState::get_stack with the following arguments: a pointer to the JitState, the minimum amount of items there should be on the whitespace stack to execute this block safely and the maximum amount of items this block will add onto the whitespace stack. JitState::get_stack will check if the stack meets these requirements (and possibly reserve more stack space) and return a pointer to the top. Additionally, a pointer to the start of the stack is passed in memory. In case the stack size was not correct, the block will return to the interpreter. Returning from the jit is actually quite simple. If an instruction errors, the instruction needs do do three things. First, it needs to spill any values in registers back to the whitespace stack. Second, it sets JitState::stack_change to the change in stack size with respect to the start of the jit block. and Third, it returns the index of the instruction that triggered the error. The interpreter will then attempt to execute the faulting instruction again and provide a clear error if necessary. A return from the interpreter doesn't always have to cause an error as some things just can't be verified at the start of the block. The execution of code after the stack checks is relatively simple. Whitespace is a stack-based language, so in extended basic blocks we know the addresses of all stack variables statically with respect to the top of the stack at the start. This means compilation is rather straight-forward when naively just loading values from memory at the start of the instruction, and spilling them back afterwards. As all this memory traffic tends to slow execution down though, the compiler attempts to keep the most-recently used items from the stack in registers, only spilling them when errors occur or when the jit calls back into rust code. There are a few instructions which allowed for further optimization/trickery though. * Adds, subtracts and multiplies with a value pushed on the stack just before get compiled into add immediate instructions completely. * The handling of the heap access instructions is special due to the used datastructure for the heap (the heap is a HashMap using a FnvHasher, but this still proved to be rather slow in heap-heavy programs. Therefore, a simple cache was added in front of the HashMap, that accelerates reads quickly followed by writes greatly. Additionally, a cache hit is completely inlined into the jit block, it only needs to call out to rust code on a cache miss or a cache eviction. * Calls/returns are also special. As the jit doesn't use the native stack for calls, a call instruction will call out to rust code and push two values onto the JitState callstack. The first is the index of the instruction it should return to (which has to be there for interpreter fallback), the second will be the address of the jit entry point of the block it should return to, if this is known. When a return executes, it reads these values (erroring out in case no values are on the stack), and in case the return jit entry point is known, it will directly jump to it. Otherwise, it would return to the interpreter. * All I/O functions are just calls to rust code, with the exception of inum. As inum can fail in annoying ways (when someone enters a number that doesn't fit in a 64-bit value), the interpreter needs to fall back to bignum-based interpretation. However, as the values have already been read from the input stream, we can't return with the state as it was before the instruction executed. Therefore, the jit compiler doesn't even try to handle inum, and just returns to the interpreter when it is encountered. The final pieces of magic lie in how the jit blocks are chained together. As the jit compiler was engineered to be able to compile while executing, it will at first generate code that on any flow control jumps returns to the interpreter to figure out how to continue. However, as more pieces of the program become compiled, the jit compiler rewrites the code for previous flow control exits to jump directly to the relevant blocks. The only problem then is adjusting the stack size for each block, which is cleverly handled by setting JitState::stack_change before the jump, which JitState::get_stack then checks at the start of the new block. The result of this is that after compilation is finished, execution will almost completely be native code. You can check the generated assembly by using the --dump option in the command line tool, which will write the complete jit buffer to a file which you can then load into a disassembler. As the way it does control flow is very different form usual programs though, control flow graphs can be rather chaotic, [for example, here's part of the control flow graph for a whitespace interpreter written in whitespace](https://i.imgur.com/YASlLzp.png). If you want to have a more detailed look into how it works, I'd advice just checking out the [source code for the jit compiler](https://github.com/CensoredUsername/whitespace-rs/blob/master/src/core/mod.rs#L796). The actual code generation is only about 600 lines long.
Lol, go ahead, that field is already one of the weirder things in the project anyways. It exists solely to stop LLVM from inlining a function call which is both marked as #[inline(never)] and #[cold]. I have no idea why it keeps trying to inline it.
Just after I finished converting this loop to parallel, I said, "The was just a _perfect_ experience of what Rust is intended to be." And so I dug desperately back through the scrollback buffer of the terminal running `cargo watch`, and happily enough, all the error messages were still there. So I wrote it up! :-)
Can you somehow quantify the gain of the conversion? Something along the lines of "50% speedup on 4 cores in our most relevant use case, 10% reduction on 1 core"?
Ha, true. I did not think about that, rather as fuzzy in a "starts_with" way. Thanks for the input. Hm. Yes, perlin would need an abstraction to allow similarily efficient Levenshtein implementations for Strings to be competitive. Though Levenshtein is for strings only, right? So probably the best way to approach this would be to define a trait for edit distance between two terms. Could a Levenshtein automaton be generalized for other types that implement edit distance? Or what would these types need to provide to generate similar automatons for them? Well, this is part of the future anyways. I'll put it on my todo list.
Using `_` in general is the right way to do nothing as a default case. You can just use it, not `&amp;_`, and that'd be the better way to write it. That said, this _exact_ code would be better written as if x == "String" { println!("matched"); }
&gt; So, the _ =&gt; () is required minimal? Yup! &gt; what's the difference between &amp;_ and _? `&amp;_` will match against any reference, and `_` will match against anything. So `_` is more general. There's no reason to actually write `&amp;_`. I'd argue this is a poor diagnostic.
&gt;I'd argue this is a poor diagnostic. I must say, as a newcomer I spend two days until my code compiled… On one hand it is obvious that I just needed default case. On the other I've seen error with `&amp;_` not `_` for the first time, so I was convinced that I just forgot to put reference symbol somewhere at my `String`. I couldn't find a working, complete and minimal example of pattern-matching on `String`s (not `&amp;str`) anywhere.
Good question! The original version of this loop was already too fast to measure for our use case—typically something like 0.01 second from startup to shutdown for the `cage` CLI tool for this code path. But we do have other, more complicated loops that are slower. Now that I've proven that rayon works, I'm hoping I can just use it "by default" everywhere I have a potentially expensive loop. But I need to define some custom reductions to handle more complicated `Result` types (which I'll propose upstream). And I need a solution for merging line-oriented I/O from multiple `std::process::Command` objects.
When do `Vec` or `String` resize? Is it when they're full, or slightly before they are full? If it is the later, what is the percentage when it re-sizes. :.:.: If I have a `Iter&lt;A&gt;` I want to map it to a `B` type. But each `A` can return multiple `B` type. So a pure 1:1 map would be `Iter&lt;A&gt; -&gt; Iter&lt;Iter&lt;B&gt;&gt;`, is there a way I can chain those? so it the signature is more like `Iter&lt;A&gt; -&gt; Iter&lt;B&gt;`
If you were to implement your own std lib, it's possible to just import it as a crate, and re-name it in the cargo file, like in [rust.ko][rust.ko]. It'd be nice if there was an easy way to re-use libcollections and such though. But having libcollections in a separate repo would likely mean using git-submodules, and I wouldn't wish that upon anyone. [rust.ko]: https://github.com/tbelaire/rust.ko/blob/master/Cargo.toml
Cool, thanks!
&gt; provide safer/rusty wrappers around the native API of the underlying system, and thus very tightly bound to said underlying system. I'm very unfamiliar with this topic domain. Mind going into a bit of high level detail about what implications this might have? I'm trying to understand some bulletpoints so i can justify spending some time on this. Fwiw, our main usage would probably be calling Rust funcs/etc from Python, and these would need to have a very natural UX.
What kind of tree are you making? `petgraph` is only relevant if you want a "monolithic" data structure *that describes a tree*. If you want an “actual” tree, (or graph), i.e. were the nodes are loose and separately allocated, then it requires a completely different approach.
That gives hope :) I've looked at your crate before and will probably read more of its code once I tackle this task. Thanks anyway.
&gt; Oxidation is nearly "complete." &gt; ironed out I see what you did there. ;)
Loved this. As someone who is currently in a C++ project with no plans to make a switch, but who's also learning Rust (and loving it!), debugging segfaults and invalid indexes and dangling pointers and having to thoroughly think about lifetimes because there's no one to enforce them... It's sad. Besides the borrowck, I've found that rusts semantics (immutability by default, move by default with explicit copy) and the iterators API are the other two thins that I like the most. ADTs and pattern matching are awesome two. Those 4 make it possible to write funcional-like code with C++ speed, and that's amazing.
A short description for it would be a prefix tree with variable sized nodes. There is a bounded number of sizes that the nodes can take. As the number of children of a node changes, it switches its internal representation from a smaller node to a larger one. Since the whole point of having different sized nodes is to have a space optimization, approaches which create the node to be the max of all nodes would not help me. 
Nice tool! Fish support would be awesome. I also have some general questions: 1. Do you accept MRs? (Asked because you mention patches in the README) 2. Do / Will you provide AUR package for Arch? 3. Why Gitlab? 
Pretty cool stuff. I'm writing a library ([rustypy](https://goo.gl/ZVKtWd)) that handles FFI conversions and generates bindings automatically in Python which would be good for quick prototyping etc. in case someone wants to quickly try some improvements without having to bother with writing CFFI, CTypes etc. boilerplate (also adds interfaces for working with analogs to python lists, dicts and tuples). 
I'm new to VS Code, installed it just to test this. The README says: "Then you can just open the extension in VSCode and run it (F5)." How do I do this? I copied the two extensions in the extension directory, and they show up on the VSCode extension tab, but nothing is happening when I'm editing Rust code. I tried pressing F5 (I think the "F5" on the readme refers to the button? What does "running it" refer to? Running the extension?) but nothing happens. RLS is running on the background. Btw. if I try to run the rls binary directly, without using `cargo run`, I'll get the following error: dyld: Library not loaded: @rpath/librustc_driver-6eb85298.dylib Referenced from: /Users/drasa/repo/rustls/target/debug/rls Reason: image not found Trace/BPT trap: 5
Adding an element to a Vec or String will resize the buffer if it's full. [Currently it will double the size each time](https://github.com/rust-lang/rust/blob/7bccb829d0fe9a733bd6efcf6f7313186ae237ab/src/libcollections/vec.rs#L692-L695) but this is not guaranteed. The [docs](https://doc.rust-lang.org/std/vec/struct.Vec.html) say: &gt; `push` and `insert` will never (re)allocate if the reported capacity is sufficient. push and insert will (re)allocate if `len() == capacity()`. That is, the reported capacity is completely accurate, and can be relied on. It can even be used to manually free the memory allocated by a Vec if desired. Bulk insertion methods may reallocate, even when not necessary. &gt; &gt; Vec does not guarantee any particular growth strategy when reallocating when full, nor when reserve is called. The current strategy is basic and it may prove desirable to use a non-constant growth factor. Whatever strategy is used will of course guarantee O(1) amortized push.
An update: Silly me, I missed that according to the README of the VS Code extensions, I have to run `npm install`. I expect that I have to run that in the directory of those extensions? (I have zero experience of npm too) I'm not really sure what the following even means: "Open the client in VSCode and run with 'debug' to open a new VSCode with the plugin installed" So I run the client (=the extension?) IN VSCode (how?), and then run with 'debug' (what do I run with debug?) to open a new VSCode (another instance of VSCode???)?
The error has to do with the fact that `&lt;Self as Foo&lt;O&gt;&gt;::Type` prefers the `Self: Foo&lt;O&gt;` bound and ignores the fact that `&lt;i32 as Foo&lt;_&gt;&gt;::Type` is *always* `i32`, which is why you can even leave out the bound in the first place: it's unnecessarily generic.
actually, I managed to do with a different approach using an atomic bool
Awesome writeup! Thank you ^_^
I used to have to do this. I used sublime locally and just scpd it over.
It will do whatever you configure it to do for lots of types. Fields, mutables, primitives, structures, traits, runs, etc can all be colored or otherwise displayed differently.
Welcome to Rust! 
Thanks for the suggestion of bit-set, it does indeed cut down the memory usage enormously. However, I've found that flipping the conditionals around (so that _composites_ are added to the set) and then initializing with `with_capacity` was literally around 25x faster than `collect()`. (It just means it takes a bit longer to reconstruct the list of primes rather than composite numbers.) Also, I noticed when compiling the original version with an array, the compile took an enormous amount of memory (&gt;4GB) for a large sieve. Is it possible that the compiler was doing some sort of failed optimization and was allocating stuff on the stack for that? 
Whoa, I got it to work! Gonna send a PR with clarifications that would had made it easier to understand!
Are there any conventions yet around source file layout? Like structs before methods? test module at the bottom, traits go ??? etc. Curious if the answer is, whatever works - or if any opinions are starting to develop. Thank you.
12 years ago I was using kwrite from KDE 3 with the fish KIO slave. Native browsing, opening and saving files over an SSH connection (or indeed FTP). I presume they still have something like that in KDE 4.
It did get linked in as the standard library, but it was a huge hack job and probably doesn't even compile anymore due to being several nighties out of date. If I were to try going that route again, I would probably do it from scratch instead of trying to resuscitate the current implementation.
I saw [this GIF](https://www.reddit.com/r/gifs/comments/4xdfa9/timescape_halls_harbour_nova_scotia/) and I wanted to create my own. Inspired by [this PHP script](https://github.com/lgommans/TimeCircle) from the reddit thread, and motivated by insomnia, I threw this together in a few hours last night. The crates.io ecosystem makes Rust really compelling, even for little scripty things like this. Try it out! `cargo install spiralizer`
Hi! The link is broken.
https://github.com/rust-lang-nursery/fmt-rfcs is where conventions are being laid out; there isn't yet something at that level, though.
It won't be stable for quite some time unfortunately, and before procedural macros become stable I'll probably have to rewrite the codegen parts of dynasm-rs as the plan is to change procedural macro output from ast to tokens (shouldn't be too significant though, just somewhat annoying). You could use syntex though if you really want to use dynasm-rs on stable. It'll make compile-time error reporting a bit worse, but it should be possible. (Although some change need to be made as it also uses const fn's due to an annoying limitation of bitflags in the assembly data. Due to plugin development reasons I'm basically always on the nightlies so I didn't consider it a problem for myself. But I can see how it can be annoying.
Is it possible to achieve this in Vim?
Wait, is that because the crate name was actually just `std`?
That is mesmerizing
Have you heard about X-forwarding? Since the most used window-system-protocol-something (called "X") on Linux is basically a network protocol, you can send it via ssh easily. You just need a local SSH Server (which you probably have if you are using Linux). Then you do: ssh -XC server `-X` says "forward X stuff" and `-C` just says "compress the stream". After the connection was established, just open some application that will open a window and you should see that window locally on your PC. Now you are not restricted to the terminal anymore. Personally, I use Sublime Text.
Real cool, worked great, tried it on a set of progress images from my last project. https://gfycat.com/WhimsicalCheapDrafthorse
Wow this is amazing. Better than the original I'd say!
That's good. The underlining was bothering me style wise even though highlighting it somehow is a good idea.
&gt; Very cool. It's weird seeing how fast day/night transition into each other. I figured those bands would be a bit wider. Yeah, I think it would look nicer if the sunrise/sunset were slowed down a bit from real-time.
Aren't most java updates just security fixes to the plugin?
Would you care to elaborate? Nothing is allowed to break inference that works on stable today. If it does, that's a bug. The issues addressed by this point release don't have anything to do with type inference either. I also am not sure what you mean about "touting stability a bit hard"; the post is merely saying there was a huge amount of changes, some accidental breakage occurred, and we're issuing the first point release to address it. Maybe I'm just dense, but I can't see any evidence to support your statement.
It seems to me more like it's because Rust already has a new release every 6 weeks. That's not a particularly long time to wait, so there would have to be a fairly significant bug to justify putting out a point release sooner.
Makes sense. Thx.
I can't speak to what the op is thinking of, but there are plenty of situations where we do knowingly (and of course unknowingly) break downstream code. In the language, this is almost always for fixing soundness holes or otherwise egregious design mistakes. There are also aspects of the type system that make breakage due to library changes technically unavoidable, though we put great effort into minimizing the impact. For example, adding any implementation of any trait _can_ cause breakage due to method resolution failure. For specifics, if you look at the [release notes](https://github.com/brson/rust/blob/relnotes/RELEASES.md) there are sections on "compatibility notes" and "breaking changes" that provide details of _most_ things we've knowingly broken. Here are the [full details](https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md) of the policy toward library breakage. [And language](https://github.com/rust-lang/rfcs/blob/master/text/1122-language-semver.md). We have a number of mitigation strategies for avoiding breakage, and even when desired changes are technically within the letter of the policy, we always weigh them against the practical downstream impact. If there are incidents we've overlooked that have had notable negative impact, I'd love to hear about them, especially if they are accompanied with ideas for how to do better next time. So it's true that Rust doesn't guarantee perfect stability. But we do try really hard. 
Just did a release of [oscpad](https://github.com/bburdette/oscpad) this last week after modernizing the elm part of it, which forced some rewriting of the rust server too. Now I've gone through and gotten rid of all the rust warnings, and next up is to split it into a lib and an app instead of just an app. 
Hmmm... yep that totally fixes the error. Although I'm now confused again ;) When I first ran the build I saw this output: Compiling itoa v0.1.1 Compiling num-traits v0.1.36 Compiling dtoa v0.2.2 Compiling quote v0.3.3 Compiling unicode-xid v0.0.3 Compiling serde v0.8.15 Compiling syn v0.9.2 Compiling post-expansion v0.1.0 Compiling serde_codegen_internals v0.10.0 Compiling serde_codegen v0.8.15 Compiling serde_json v0.8.3 Compiling serde_derive v0.8.15 Which implied to me that `serde` was in fact installed for the project. I suppose I have a two new questions now. Am I as the developer responsible for determining the dependencies of my dependencies and explicitly enumerating all of them in `Cargo.toml` and the other question is this: Any idea why it was complaining about `_serde` rather than `serde`? Thanks for the help and the super quick answer BTW!!
I've definitely had breakages before, but I do things at the fairly extreme end of the type system. Both [typenum](https://github.com/paholg/typenum/) and [peano](https://github.com/paholg/peano/) have broken due to rust updates. Compiler changes were the reason for version 1.3.1 of typenum, and peano is still broken (the fix is nontrivial and I haven't looked into it much as it's not a library anyone should be using anyway). I understand, though. Never breaking any code was never one of their promises.
Very nice! Articles like this are quite valuable. My one remark is that you use `if let` in the "Taming Options" example without prior explanation, and I tend to think that that's one of Rust's features that's particularly head-scratching to newcomers (even if it's not hard to understand once it's been explained).
All good points, thanks! I've incorporated the changes to the article, and credited you in the commit message. I also linked back to your comment from the first of the points above. I'd completely forgotten about the `_y` trick! As an aside, the `assert!` was initially there to highlight that you would only do this if you truly *knew* that the `Option` in question was `Some`. But I realize it's redundant with `.unwrap()`. Along similar lines, I had `.into_iter()` there to clarify that it was an owned iterator, but I guess that's not really important in that context.
Atom supports this just fine. I use it to edit source code on my desktop. Just mount the share with sshfs, and get the Tokamak terminal.
If you depend on something that depends on e.g. `serde`, then `serde` will be compiled *solely for the purpose of that dependency working.* If you want to use `serde` *in your own code*, you have to specify the dependency explicitly. So no, you don't need to track down your deps' deps; but you do have to include any that you want to use directly :)
That sounds very similar to the [official playground](https://play.rust-lang.org/), or the [alternate playground](http://play.integer32.com/) which allows some crates. AIUI, both of these compile and run their code in external processes. Your design sounds like you want to run within the same process, which is pretty scary to me. Note for instance your ban on `unsafe` -- we're only talking memory safety here, not security, and there are *many* ways to subvert that.
I've seen various Rust sandboxing projects but I'm not sure how ready they are. Generally, for a sandbox to be reliably secure, you need to adopt a two-process model where the inner process is as un-privileged as you can get away with. (Ideally, nothing except receiving/sending messages to the parent process and/or reading/writing file pre-opened file descriptors given to it) Python 2.3 actually threw out a sandboxing approach which sounds vaguely similar to your idea because they found it to be too difficult to guarantee safety. PyPy later reintroduced sandboxing using the two-process model I described and it's also how Chrome's sandbox works.
Here is my thought for now: 1. parse code to ast (use librustc's function) 2. verify ast (e.g. emit error when use 'unsafe' crate) 3. do some instrumentation on ast (e.g. add time_check to loop/function/closure) 4. transform ast to mir/llvm-ir/machine code (use librustc's function) 5. load &amp; invoke compiled code (use llvm mcjit/orc) is seemed reasonable. but this will use rustc's internal functions, and for now, there is no clear document on it (please let me know if there are some). 
I don't think your approach is viable, because of the existing soundness bugs in the Rust compiler. You can cause undefined behavior in safe Rust, for example by casting large floats to integers. It seems to me that the only viable option is to have the script run in a separate process with limited privileges. I would probably look at what the sandstorm.io folks are doing, they basically made sandboxing their business.
On stdlib changes: I wish that whenever a function changed its run-time semantics, it instead were deprecated and a function with another name were created. It may be inconvenient, but it's the right thing to do IMO. I'm okay with compile-time breakage though (if not widespread), that's what type systems are for.
Yes, I want the user code to run in the same process, because I need the user code has the ability to access the data of hosted process. It mostly like 'store procedure' in RDBMS. For now, in my project, I'm using a lua VM to do the same thing. But there is huge overhead between host C &amp; lua VM (vm-native boundry, dynamic dispatching, object ownership transforming between GC and C), and the syntax of lua is very ugly. So, I am trying to use rust to reimplement the project. I know it is very hard to implement a general-purpose-sandbox, but in my project, only very limited feature is required: no thread, no lock, no unsafe code, no filesystem access, and even loop is not necessary (can be replaced by map/filter/fold/... and tail call), mostly a pure functional style code.
 &gt; One thing that you, as a user of Rust, can do to help us fix these issues sooner: test your code against the beta channel! To what degree is this actively being taken care of, for code on crates.io at least? I've heard about crater but I'm not sure if it runs "cargo test" or just "cargo build", and I'm unsure of how the process works w r t how often it is run, and who's responsible for fixing the regressions that crater discovers. Not to point fingers at anyone, just curious how the process works (in case there is one).
it seemed casting is a problem, is there "undefined behavior" list of rust and rustc?
No, there are also changes to the runtime and compilers. For example, when you see something like *Com 8u20* on the [JEP](http://openjdk.java.net/jeps/0) list, it means completed and delivered with Java 8, update 20. This is only relevant to the OpenJDK, then you have the other compiler vendors as well.
I believe it's a ground-up rewrite, so it would be more of a replacement than a merge. I don't know any plans to actually do that though. The crates are the top 100 downloaded from crates.io, minus a few that are blacklisted. You can find that code [here](https://github.com/integer32llc/rust-playground/blob/master/top-crates/src/main.rs), and here's the actual [Config.toml](https://github.com/integer32llc/rust-playground/blob/master/compiler/base/Cargo.toml) that's generated.
The problem with a single-process model is that it's like a bubble. A single flaw, and **pop**. See, for example, [JIT spraying](https://en.wikipedia.org/wiki/JIT_spraying). The reason the two-process model works as well as it does is that you're relying on battle-tested kernel code to lock down the boundary between trusted and untrusted code far more tightly than is possible when running trusted and untrusted code in the same process.
&gt; We’ve also gotten rid of the v variable entirely. Pretty neat! reads a little awkward because you use `v =&gt; v` as the catch-all. anyhow, great article! thank you
https://github.com/rust-lang/rust/issues?utf8=%E2%9C%93&amp;q=is%3Aissue%20is%3Aopen%20label%3AI-unsound%20
Wow, where did you learn all of this? This seems really cool, but I don't understand a lot about your post ([mrw](https://www.youtube.com/watch?v=Ccoj5lhLmSQ))... 
Nice, thanks. Question, though. Are let mut v = foo(); match v { Enum::Bar(ref mut x) =&gt; { x += 1; }, Enum::Baz(ref mut y) =&gt; { y -= 1; }, _ =&gt; (), // needed to make pattern exhaustive }; v and match foo() { Enum::Bar(x) =&gt; Enum::Bar(x + 1), Enum::Baz(x) =&gt; Enum::Baz(x - 1), v =&gt; v, } really the same? The first seems to modify an `Enum::Bar` inplace (by taking a mutable reference to x), the second seems to construct a new `Enum::Bar`. This might not really be what one wants, or did I miss something. A small nitpick: In "Updating or inserting into a map" in the first example, you're shadowing `vals` inside the `for` loop. I assume that works, but it's quite a bit confusing imho.
From the [reference](https://doc.rust-lang.org/beta/reference.html#attributes): &gt; cold - The function is unlikely to be executed, so optimize it (and calls to it) differently.
Same here, though I haven't learned C++, I did try some Haskell.
Right. My question was more about if there is a routine of doing crater runs regularly during beta in order to catch stable-&gt;beta regressions, and if so, what would happen next if a regression is found.
Any reason you're not using lalrpop?
I could give you a number of reasons, but by the end of the day I wanted to write the parser from scratch myself because learning about it and doing it was a lot of fun :). I'm also _very_ happy about the performance of it. Doing [json-rust](https://github.com/maciejhirsz/json-rust) on the side and taking all the learnings from it back to Ratel helped a lot.
I think the article Sounds like you turned Off mir again. But you Just fixed it, right?
Does Macro 1.1 help with making clippy work on stable rust?
Is that unsound though? If the loop never returns, then I don't see how it can break memory safety.
I think it can also be useful to have information on what this variable could be used for. E.g. if I see some default impl of a trait that looks like fn hello(name: &amp;str, _: bool) { ... } I'll have less info to implement my own version than if it is fn hello(name:&amp;str, _capitalize_last_name: bool) { ... } 
The point is LLVM optimizes out the loop entirely, so the function does return and everything explodes: https://is.gd/E0jBow
Oops, my bad.
You could also construct functions in a way that makes it clear regardless, like using an enum instead of a bool there.
Coming from C++ and Python, Haskell was a bit too weird to me but Rust for some reason makes sense. It felt hard to get anything done in Haskell. And debugging C++ takes patience. And modern C++ is fantastic. You should try it. The people who bash C++ usually haven't seen it in the last 20ish years or they work with it daily and are bored of it. The modern version of the language, called C++ 14, is as sleek and shiny as any other modern programming language. Just think of it as a lower level Python and you'll grow to like it. C++ is one of the few language with true object oriented programming inspired by Simula and not whatever it is that Java has with every file being a class and all those restrictions that it sets on you. 
Basically (if I get this right and explain it correctly ^^), if you binnd the variable let stdin = io::stdin(); it will live until the end of the scope, so you can then use it (in this block) without problem. If you don't do it, the value that `io::stdin()` returns is dropped (freed) immediately, which means that you can't use it (it's a dangling pointer, so it would be undefined behaviour, and the compiler won't let you do that) in future function/method calls.
Thanks for the answers! As someone who uses webmail (gmail) and google calendar I guess these this project is not for me :-)
Just as neat: correctly handling shadowing of variables when highlighting usages. I was quite impressed by that when I discovered it. http://i.imgur.com/Fb43Zcb.png
These runs are done regularly, see for example https://internals.rust-lang.org/t/regression-report-stable-2016-08-16-vs-beta-2016-09-21/4119 And we look at all potential regressions. Yes, it only looks at "cargo build".
I'm not sure about the _serde/serde stuff :-/
&gt; The drawback of rust-python is hard to use and because it tries to use Rust's lifetime system to manage incrementing and decrementing reference counts for python objects, you may quickly run into lifetime management issues. I you're introducing a team to Rust, they will quickly learn to hate it if they have to run into these lifetime issues. Really appreciate that line. Last thing i want is lifetime problems for new rust devs haha.
This looks really cool, i appreciate your work on it!
Even more I welcome your contribution and your efforts! But yes, its target audience is power-commandline-users ... maybe there will be a GUI some time, but I do not plan it... I want it to be scriptable and everything, curses UI beeing the one thing I might integrate... but CLI is the only interface I plan for 1.0 (which isn't even in my timeline... I hope I get to 1.0 within the next two years... it really all depends on community effort as this is a monster project).
Oh, thanks for the `Entry API` tip, this was quite the unpleasant thing about working with maps.
Ah, but how come the former works, but the latter does not: use std::io::BufRead; fn make_str() -&gt; String { "hello\nworld".into() } let s = make_str() // std::io::stdin .as_bytes() // .lock() .lines() .collect::&lt;Vec&lt;_&gt;&gt;(); let s = std::io::stdin() .lock() .lines() .collect::&lt;Vec&lt;_&gt;&gt;(); I *think* it has something to do with the result of `lock` implementing `Drop`.
Added a link to both! Thanks.
I've added a sentence pointing out that they're not technically the same. Also addressed your comment about `vals`. Thanks!
No. 1.1 stabilized custom derive which things like serde or diesel use. Clippy is a compiler plugin so it's not stable.
Is there a list of tools you're planning to support? I'd really love a unified PIM experience, but most of the times things just don't fit together really. 
Really great article and I learned a few things too like the Entry API. I will say though that instead of using unwrap it's better to use expect. Even if you know it's always going to work expect allows you to know where it failed based off a custom message you pass it. Unwrap is nice for tutorials but unadvised for actual Rust code if possible.
I agree that it's unfortunate to get new Rustaceans into the habit of using `unwrap`. Unfortunately, using expect also often just looks dumb. For example, I occasionally write code like: fn ... { if x.is_none() { return ...; } let x = x.unwrap(); ... } To avoid the extra indentation that a `if let Some` would introduce. In cases like that, using `.expect()` just seems silly. In the original post, the use of `unwrap` might be questionable, though I do explicitly say it's for when you *know* it's `Some`. It's also not entirely clear what useful message I should put in the `expect` in that context. Maybe the argument is simply that I should mention `expect` at all?
&gt; To avoid the extra indentation that a if let Some would introduce. Swift's guards work pretty nicely for that. A cleaner alternative is to use a diverging match or `if let`, though the repetition is annoying: let x = if let Some(v) = x { v } else { return … }; alternatively, convert the option to a `Result&lt;_, …&gt;` and try!, though that requires that the enclosing function returns a Result.
Documentation on docs.rs: https://docs.rs/combine/2.0.0/combine/
We are currently preparing to RFC clippy so it can get stabilized (by using a clippy-special compiler feature) – fingers crossed.
Hello, I've wrote a little tool for myself lately and thought that someone else might find it useful as well so decided to upload it to Crates/Github. Code review(it's real tiny app) is greatly appreciated!
Direct gif output shouldn't be too hard to implement, I'll take a look. The images are decoded to a temporary folder, and then mmaped open. The hope was to improve performance when you have more images than RAM. It seems to work...
Unfortunately, yes. Fixing that is what "The Update Framework" stuff is about.
&gt; it is possible to use rust in such an embedded space (limited amount of RAM)? Definitively. Yes. There quite a few Rust projects in this space. To name a few: - [Tock], an OS for embedded devices. - [f3][] (disclaimer: I'm the author), a crate to play with the [STM32F3DISCOVERY] development board. - [teensy3-rs], Rust on the [Teensy3] [Tock]: http://www.tockos.org/ [f3]: https://docs.rs/f3/0.1.0/f3/ [STM32F3DISCOVERY]: http://www.st.com/en/evaluation-tools/stm32f3discovery.html [teensy3-rs]: https://github.com/jamesmunns/teensy3-rs [Teensy3]: https://www.pjrc.com/store/teensy3.html All these crates/projects target the ARM Cortex-M architecture which is the same architecture the RTL8710 is using. &gt; Is there a was to flash a rust program somehow to a device like this? Yes. In general, you can use OpenOCD plus a "programmer" like the ST-LINK to flash Rust programs. Both rustc and gcc ultimately produce an ELF file and that's what OpenOCD flashes into the microcontroller. The [copper] book (disclaimer: I'm the author) has instructions about how flash and debug Rust programs using OpenOCD + GDB. [copper]: http://japaric.github.io/copper/ --- The hard part of getting Rust (or C) to work with this board is that there's no documentation (as it's usual with Chinese devices) so one has to reverse engineering the board to find out where the registers are, what the pins actually do, etc. Luckily, it seems people are already hacking this thing and are documenting their progress in [this forum]. [This thread] is particularly interesting as they have reported that they've managed to flash a program into the board using OpenOCD. [this forum]: https://rtl8710forum.com/ [This thread]: https://www.rtl8710forum.com/viewtopic.php?f=10&amp;t=12 I'm going to order one and, when I receive the board (probably, in like 2 months), I'm going to try to put some Rust into it. No promises though. 
&gt; After the bugs in this release I have a mission to massively expand that coverage. There's a lot more we can do. *thumbs up* Thanks for keeping up the good work. :-)
&gt; how much more work it would take to completely replace crates.io I think /u/carols10cents would love to work towards having a crate server that is a transparent cache and/or allows locally-hosted crates. To me, completely *replacing* crates.io seems like a misstep, as we'd be throwing away all the good community work and sharing.
You wanted /r/playrust.
I do like passing UFCS methods like this, but the stars have to align just right. For instance, any small change in reference level might force you back to closure form. Or perhaps if these results had large `Ok` types, and you wanted to dump that for just the possible `Err`, then you might write `result1.and(result2).and(Ok(()))`.
Ah thanks. 
I [wrote up a post detailing how I set my mirror up](http://www.integer32.com/2016/10/08/bare-minimum-crates-io-mirror-plus-one.html), including how it stays up to date with crates.io. At the end is a list of next steps-- definitely not all the things that could possibly be done for more redundancy, but next possible things. If anyone wants to work on any of these and would like advice or pointers, let me know!
What about updating a crate / installing a new crate, or any other way of getting a crate I didn't already have? Is there some out-of-band method of trusting the files? This would apply equally to crates.io, of course.
&gt; If this program had multiple files, I’d search through them next; but, as there are none, I’m going straight to the Internet. Turns out, it does just what you’d expect – it checks if a given integer represents whitespace or not. This definition simply allows calling it directly on char values without writing out an explicit cast every time. With github inaccessible I can't check the analysis documents but… was that for the benefit of some old weird-ass compiler? Because C will normally promote chars to ints implicitly without the need for a cast.
Because originally they were going to be in another format. Thanks for the feedback, though - I've consolidated them. I'm planning that the next post will encompass the entirety of the rest of the deconstruction, and the final post will cover the entirety of building the Rust version. I might make a followup that covers some enhancements in functionality. 
Rust on Teensy 3? Outstanding! I'm *just* reading/learning Rust (wrapping my head around the way it works) and also tinker with these boards. Perhaps one day, I'll get to see them converge! Thank you.
Cool!! Thank you for sharing this. I'm learning Rust from the ground up, and this is a great little project to analyze (and use!) I use i3wm, so something like this is perfect. I've recently made a little one-liner curl/grep utility to get the current spot price of silver and output it on my status bar. Perhaps I'll try building a Rust/GTK version of it as a simple early project!! Thank you for sharing! 
Can I get a tl;dr for what this means for Rust developers / end-users?
As gitlab and github are likely on pretty different infrastructure (as you mention in your post), I wonder if it makes sense for there to be an official mirror on gitlab? Something that is managed by the rust team, and always kept in sync with the github repo
play-by-email board gaming system? Like, basically they play a game using emails? Could you please elaborate?, Very curious.
Essentially there's a game server which can parse commands from emails, updates game state, and emails the next player to take their turn. [This is what it looks like](http://i.imgur.com/wcX7VJU.png). The original system was written in Go but I found it really difficult to implement the more complex games. I've been porting the system to Rust and have finished 2 of the ~25 games.
Is there anything I would need to play such games? Does it cost money to play? Sorry If I'm pushing too far.
The parser combinator style you're using here is very similar to what I've been doing with [termpose](https://github.com/makoConstruct/termpose/blob/master/cppintro.md). I've been holding off from doing a rust port, in part because I don't need it yet, but mainly because I think I might want to take a radically different approach, which, I'm now thinking, is integrating it into combine(or something very similar). Basically, the termpose library is, first, a function that takes files to structured trees of strings- parsed under a white-space sensitive syntax for s-expressions- and second, a set of parser combinators for taking the resultant trees of strings to whatever else- EG: structs of dates and numbers. I'm getting a strong impression that the tree parser could and should be a Parser for combine, and that it would make sense for the second stage, typing, to use combine's float/date/char/optional/struct parsers, because there doesn't seem to be a substantial difference between the way we do things there. Am I overlooking anything? Would that work?
Ah, I do see one thing. Combine doesn't seem to aspire to be bidirectional. A parser takes a string to a Try&lt;structure&gt;, but it can't take that structure back to a string. In termpose, there are Checkers, similar to parsers, there are also Translators, which go both ways. Would you have any interest in doing that with combine? I think it could work.
Yes, thank you! There we go.
&gt; * Rust should integrate easily with C++ code This is new. Is this new? I'm excited. &gt; In other words, it should be possible to directly include C++ headers (e.g., include! {myproject.hpp}) and have the extern declarations, glue code, and so forth get generated automatically. !!!! 🎉🎉🎉🎉 That is all.
&gt; Rust should have a lower learning curve Does this mean they're going to focus more on language ergonomics and less on this whole "explicit is good" thing that results in verbose and noisy code?
HKT isn't a direct roadmap item, but it may well turn out to be important for accomplishing some of the items that are. I also think that [withoutboats's RFC](https://github.com/rust-lang/rfcs/pull/1598) is a very promising step that achieves much of what we'd want from HKT. Non-lexical lifetimes, of course, fall directly into the learning curve bucket.
[A blog post about that](https://bawk.space/2016/10/06/c-to-rust.html) showed up in This Week in Rust a couple of weeks ago. 
FWIW, "explicit &gt; implicit" is a poor design principle IMO, and has never been a core tenet of Rust's design. Whether an explicit or implicit treatment of something is better is highly context-dependent, and should be driven by other goals (speed, reliability, productivity, etc). The existing language and standard library pretty carefully balances these choices. For example, the fact that borrowing happens implicitly in `self` position but explicitly for arguments was a calculated choice about balancing between one's ability to reason about code and one's ability to read/write it productively. Similarly, the fact that we require explicit type signatures for functions, but not for `let`.
ty, I'll check it out...
Because `u8` and `char` are not the same size I don't think there's any idiomatic way but to use an array of `u8` and copy to an array of `char` by way of `Read::read` or `Read::read_exact` and a for loop. https://is.gd/nmSqaL I suppose if you can call `Read::bytes` then you might not need to manage the `[u8; 10]` yourself. Note too that `&amp;str` in Rust is not null-terminated so you may need to account for that if you intend to expose `my_string` as `&amp;str`.
Usually, you don't need `'a`, so don't worry about it too much yet. Normally, they're 'elided,' which means that the compiler has a set of common patterns, and if you don't provide the `'a`s it just fills them in according to those patterns (which in practice are what you want nearly all of the time). So this function: fn foo(x: &amp;i32) { } Is the elided form of this: fn foo&lt;'a&gt;(x: &amp;'a i32) { } Here, `'a` is the lifetime of the borrow that the function takes. One thing that's important about this is that `'a` is not _any particular_ lifetime. That, is, when you call that function, it will be whatever lifetime is actually appropriate. For example: { let x = 0; foo(&amp;x); } let y = 1; foo(&amp;y); The lifetimes here are totally difference, because `x` doesn't exist anymore by the time you call `foo` with `y`. The `'a` is how `foo` is able to be abstract over references which have different lifetimes. Its actually very similar to how `Vec&lt;T&gt;` could be a vector of any type. Again, though, you don't need to worry about this most of the times. There are a few times you will need to be explicit about it. For example, lets say you have a function that takes two references, and returns a reference: fn foo(x: &amp;i32, y: &amp;i32) -&gt; &amp;i32 The compiler doesn't know if the return type has the same lifetime as `x`, the same lifetime as `y`, or if they all have to have the same lifetime. So you have to be explicit: fn foo&lt;'a&gt;(x: &amp;'a i32, y: &amp;'a i32) -&gt; &amp;'a i32 This means that the return type can't outlive either `x` or `y`, and both `x` and `y` will be borrowed for as long as the return type exists.
&gt; (pronounced "lifetime a") I've heard Rust core team members pronouncing it "tick a". I personally pronounce it by not making a sound at all but leaving a gap in its place :P
The trick is that with `Fn` traits we have "obvious" inputs and outputs for the elision algorithm (which is, roughly, "enter a scope where missing lifetimes are HRTB just one layer out, and then if there's only one lifetime, that's what gets implied in the return"). Given `Fn&lt;(&amp;T,), Output=&amp;U&gt;` one could argue that the arguments are the inputs and the associated type bindings are the outputs so it could elide like `Fn(&amp;T) -&gt; &amp;U`. ... [except we already do elision](https://play.rust-lang.org/?gist=07a0649ea9ddf16b2734d8553455d45d&amp;version=stable&amp;backtrace=1) in such cases, from an outer scope (in this case, the `fn` pointers), and changing the behavior would be a breaking change (you seem to suggest that we could make the two examples in my link equivalent, which is not the case). There's more interesting stuff around bounds, but it suffers from similar problems: what scope do you get the lifetimes from? Some cases (e.g. `I: Iterator&lt;Item=&amp;'a T&gt;`) *require* that the lifetimes come from the function definition (HRTB would mean that the caller of `.next()` can choose the lifetime which is wrong), others (such as the `Fn` traits) usually deal with HRTB around the bound itself. This is where /u/nikomatsakis and /u/pnkfelix might have materials to explain this better, but, I don't see how the various usecases are easily reconcilable. That said, I was rather hoping for something more clever than eligion, e.g. some syntax that allows describing HRTB in "relative relationships" rather than named lifetime parameters introduced by `for&lt;...&gt;` - but maybe that's my implementation-aware bias.
This has been implemented: https://github.com/slog-rs/slog/issues/69 /u/matthieum thank you so much for pointing it out. It was actually easier to implement than I expected and I was able to create a whole generic `Async` drain, that allows offloading anything to a worker thread.
Huh, weird. Have you talked to your professor?
I'm afraid it's the latter.
I'd like to add one rust-specific thing to the list of basic IDE features: scaffolding `impl`s. The IntelliJ Rust plugin does this and it's great. I find that getting the type signatures of an impl correct is one of the simplest things that breaks my code flow when I'm writing Rust since it means jumping over to the browser and pulling up the docs.
I'm the speaker and author of this paper. AMA!
After giving this talk I had a chat with Ralf Jung (who's working on formalising Rust, including unsafe bits). It seems like we're all solving similar problems.
If you've seen the RustBelt project, they're aiming to give a formal semantics to rust, so that is indeed an interesting direction. One of the things that we get with Cogent that I don't think Rust can get is an equational semantics. Mostly, this is due to the fact that in Rust, a mutable borrow need not be returned from a function, and the original value becomes accessible after the function is called, getting rid of the equational theory which allows Cogent to have some lovely HOL embeddings that I don't think Rust can match.
The ultimate test is whether or not it will integrate nicely with Windows API, especially whether it can handle COM nicely.
A Read-Write Lock is a synchronization device (primitive?) which can be called upon to create two things: read handles, and write handles. The Read-Write Lock is programmed to have the following invariant: there can exist and unlimited amount of read handles, or one write handle, but never both kinds. Any request for a handle that would violate this invariant will lock the thread. A Read-Write Lock will of course be rendered invalid if the data it protects becomes invalid (deallocated, reallocated elsewhere, etc.) The rust reference system are a Read-Write Locks, only instead of working over periods of time to synchronize multi-threaded data access, they work over lifetimes (scopes) to ensure memory integrity of stack data. At any point in stack allocated data's lifetime, you can create mutable or immutable references to it. The invariant is that there can be an unlimited amount of immutable references, or one mutable reference, but never both. And also references cannot outlive the data they refer to, or if it is moved (called by value.) The moving semantics are necessary to ensure that a piece of data will only have its destructor called once. Essentially calling by value means that the callee is now responsible for deallocating.
 I would like to call into the kernel. ioctl() and the like. i found an ioctl library, but it could only handle basic types like ints and such. My question is, how do i treat C structs from rust? How do make space for them? How do i pass a pointer to them to ioctl? How do i from rust access their contents? Stuff like that.
racer's jump to definition is beautiful too, and it lets you jump to the trait definition easily and you can copy from there (works fine across crates and jumping into the Rust libstd). Concretely, do something like `use std::error::Error;` then use jump to definition on the Error ident.
I’m guessing servers is where we’ve seen more early adoption in production (whatever that means to who says it) because it’s easier to deploy a new language where you control the machines that run it. So it’s understandable to make it a 2017 focus IMO, but I don’t think it’s meant to be exclusive.
&gt; fn f(x: &amp;for&lt;'a&gt; Test&lt;'a, O=&amp;'a i32&gt;) { x.f(&amp;1) } TIL `for&lt;'a&gt;` could be used in a type like this. Thanks!
Good point, thank you!
You're welcome. :)
Well, given the little work I did, you're welcome :) Did you get much performance out of it?
[The Rustonomicon has a great section on ownership and lifetimes](https://doc.rust-lang.org/nightly/nomicon/ownership.html).
Argh. On the inner have I'd love to have sorry for clippy on stable especially for new users who might do some anti patterns with their code. On the other hand I'm not too fond of specializing compiler features for one plugin that won't help others. I feel conflicted here
&gt; and far far uglier and noisier than Go, Swift, or D or Nim. Slightly unfair comparison. You have specifically presented here a *lifetime*, which means borrowing, which none of the above languages feature. By borrowing: - Rust does not need a GC, allowing easy integration with languages that do need one - Rust offers more guarantees about what can (and cannot) be done with `x` I've debugged Java applications with layers of Visitors/Decorators/... and while the syntax is simple (readable), *understanding* what is happening to the data is a nightmare due the spaghetti nature of the call graph and the pervasive mutability. Rust has immutability by default, and enforces in type signatures whether the callee can retain references to the piece of data, which is extremely helpful in this space.
There's a new round now: - https://internals.rust-lang.org/t/regression-report-stable-2016-10-20-vs-beta-2016-10-20/4254 - https://internals.rust-lang.org/t/regression-report-stable-2016-10-20-vs-nightly-2016-10-21/4255 It looks "bad", but there's a lot of them that are just from warnings, but these projects are using `#[deny]` (so opting in to being broken).
If there's a way to stabilize some base items that could be reused possibly I won't be that against it. Some reusability is better than none you know?
I would be willing to contribute to a rust port of this!
&gt; How many people have you met that shared your opinion on the matter? Many, at least 10 (since you asked for a number). It's two primary things that put those people off -- the explicit lifetime syntax (`'a` everywhere looks terrible), and the [appallingly ugly macro syntax](https://github.com/Marwes/combine/blob/master/src/combinator.rs#L7-L15) littered with the `$` character.
Heh, this makes me feel a bit guilty, since I'm working on a parser combinator library using `impl Trait` :D
OK, I feel compelled to chime in here, even though I have no constructive ideas about changing the syntax. I'm surprised you've never heard it described as ugly. As someone who has dabbled in Rust and used it for a few personal projects, I think it is very ugly -- and it only gets uglier the more I play with it. (I'm more used to Ruby/Haskell/C.) I've always assumed you would need to come from C++ to appreciate the syntax.
What's the difference between Tox and bitmessage?
[Another post](https://www.reddit.com/r/rust/comments/587ecn/looking_to_possibly_learn_rust_i_come_from_a/d8y9gxl/) on the topic. Essentially, a lifetime is a name for the span of time between a value's creation and its destruction. Each `'whatever` is the name of one of these spans. Function signatures that are parameterized by `'a` (e.g. `fn split_words&lt;'a&gt;(sentence: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt;`) describe a requirement for how the lifetimes of the inputs and output relate to one another, generally saying that one must outlive another.
Well, sure, a simple function type looks OK. (Still, I prefer Haskell there!) I really don't like the `&lt;&gt;` for parameterized types and constraints, especially when things get nested. For instance, taking an example from the standard library, impl&lt;T&gt; PartialOrd&lt;Cell&lt;T&gt;&gt; for Cell&lt;T&gt; where T: Copy + PartialOrd&lt;T&gt; in Haskell might be, instance (Copy t, PartialOrd t) =&gt; PartialOrd (Cell t) where `PartialOrd&lt;Cell&lt;T&gt;&gt;` especially looks ugly to me. I'm not often writing these myself, but I *am* often reading them in the documentation. Lifetime specifiers are gross when they aren't adding any information. And I ran into needing them pretty quickly in my structs. And not only are they in the struct definition, they are in the implementation block and all the function types. A lot of line noise without actually giving me any information. Here is a snippet from my little project: pub struct Engine &lt;'a&gt; { pub doodads: LinkedList&lt;&amp;'a RefCell&lt;Doodad&gt;&gt;, pub player: &amp;'a RefCell&lt;Player&gt;, pub center: &amp;'a RefCell&lt;(i32, i32)&gt;, pub map: Map, } edit:formatting
&gt; One uses macros to construct parsers and the other uses tuples. Are there other differences that aren't obvious? Macros vs traits are the biggest difference and before 1.11, macros were definitely superior due to compile time issues with the crazy types combine creates (`Iterator` adapters have the same problem, only they were rarely nested to the extent parsers are). With compile times mostly fixed (compile times are still slower than they could be but its getting better every release! ) I think traits is the better alternative (but I am obviously biased :) ). Since traits stay within the normal syntax of rust they offer better error messages from rustc and are easier to get started with as there is no extra syntax to learn. &gt; I guess, I'm just wondering why not converge efforts? Nothing against that, but I am unsure if there is anything that is possible to share. &gt; Ideally, regardless of how parsers are constructed (macros or tuples), why not allow them to interoperate? It would be trivial to use a parser from one library in the other (I guess combine lacks a concept of incomplete results but that could just be mapped to end of input). Other than doing a rewrite to use one of the libraries over the other I don't see much reason for doing it however. Anything one of the can do the other can as well.
`combine` is of course also completely compatible with `impl Trait` as well. ;) Got a link to the library?
It doesn't even build right now, and is only a couple hundred lines, so unfortunately not yet. Maybe after the weekend, if I manage to finish the input refactor I'm planning :)
Wouldn't this require a full C++-to-Rust translator, for those headers which contain function bodies? I believe that's not an uncommon thing in the C++ world.
I'm going to start pushing for `rustup component add clippy` (and clippy 1.0) very soon. Just .... need to get this move over with. November sometime. Remind me if I forget.
`rustup component add clippy` basically. It will cheat to work on stable, much like libstd does.
Yeah. The two-pass approach is what the blogger I linked to was using and I'd think it'd make refactoring easier since you'd have fewer translation hops to keep in mind at any one time.
I understand that you are one of the people you speak of. If so, don't fret – in practice generics like that aren't too common. I've been writing Rust code for over a year now and only once did I have to write a `for&lt;'a&gt; ..` type – in example code for my last RFC.
The kernel rewrite sounds promising. I have one question. You write: &gt; There are very few drivers in kernel space now. Those include the PIT and RTC drivers, for clock functions, and a small ACPI driver that handles just the MADT table to bring up other processors. [Wikipedia's entry](https://en.wikipedia.org/wiki/Microkernel) on microkernels states: &gt; Start up (booting) of a microkernel-based system requires device drivers, which are not part of the kernel. Typically this means that they are packaged with the kernel in the boot image, and the kernel supports a bootstrap protocol that defines how the drivers are located and started; this is the traditional bootstrap procedure of L4 microkernels. Some microkernels simplify this by placing some key drivers inside the kernel (in violation of the minimality principle) [...] I am curious why you chose to include these drivers instead of implementing a bootstrap procedure like the L4 kernels did it?