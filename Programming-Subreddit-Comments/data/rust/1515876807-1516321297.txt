&gt; Hello world seems good candidate for this purpose. It really isn't. There are many articles out there which explain why this is the case. For example, most frameworks will parse the HTTP headers into a hash map. However, I could also implement this by parsing the headers when they're actually requested, by just searching the string, turning lookup into a linear time operation but improving my "Hello world" performance (I've seen several frameworks *actually* do this in the real world). Even if you're just talking about "lazily parsing headers" vs not, making something lazy has a non-zero cost associated with it, and *all* real world use cases are looking at the headers. Hello world does not. This is just one example. There are plenty of articles out there on this subject if you're interested in seeing more.
Well, I respectfully disagree. I think the Rust Book is one of the clearest examples of a reference type of resource, instead of something taking you from A to Z teaching you new techniques along the way as needed (I'd call it a chronological approach, not sure if there's a good name for it). Even the structure of the Rust Book makes it pretty clear it falls in the first category. Of course, that's not a value judgment, the resources that do exist are pretty good, including the Book. They're just almost all of a certain type.
&gt; You can do the same for gotham. Then we can see independent results. I've never used gotham, so I'm really not the person to do that. This is misdirection though. Saying "well you should implement this other benchmark for this other framework" does not change the fact that the benchmarks you've linked to are misleading at best.
A "reference" resource wouldn't work through example projects step by step. There are several different project-based chapters in both editions of The Rust Book. It's *meant* to be read cover to cover. [RusyByExample](https://rustbyexample.com/) is an example of a reference resource, in my opinion.
Merely expecting people to go through something from start to finish does not make it *not* a reference, but we clearly don't agree on that.
I made them because they favor actix in performance, and that was my goal, nothing else :) But if they are so misleading it should be easy to proof.
Is there a way to find the start index of a str in the owning String? I want to use `String.split`but instead of `&amp;str` I want `(start_offset, end_offset)` tuples.
This is the best article from the Rust 2018 series by now. Awesome work, thank you very much!
First of all, truly amazing work. Has any research been done to determine its resilience to quantum computing based attacks?
I've never learned C or C++, but am a web dev and have suffered greatly due to null/undefined errors + race conditions in javascript. Tried to learn C++ a few times in the past but never really got into it. Rust on the other hand feels natural. The tooling around cargo is really well thought out and it is really easy to get going.
IMO putting http in the kernel is the wrong approach. I'd rather put TCP in userspace. So you just get all IP packets with a specific local port DMAed into a userspace ring-buffer and handle it there. And batch send a lot of IP packets from another buffer.
Have you tried CLion with the Solarus theme?
When using for-each on a HashMap, is there a deterministic order?
Check out https://github.com/dtolnay/objekt. extern crate objekt; trait MyTrait: objekt::Clone { fn recite(&amp;self); } impl MyTrait for String { fn recite(&amp;self) { println!("{} ‚ô´", self); } } fn main() { let line = "The slithy structs did gyre and gimble the namespace"; // Build a trait object holding a String. // This requires String to implement MyTrait and std::clone::Clone. let x: Box&lt;MyTrait&gt; = Box::new(String::from(line)); x.recite(); // The type of x2 is a Box&lt;MyTrait&gt; cloned from x. let x2 = objekt::clone_box(&amp;*x); x2.recite(); }
I think you want `find`, which returns the start index, and you can add the `len` to get the end index. Something like s.find(sub_str) .map(|start| (start, start + sub_str.len()))
Would there be more breaking changes in a "0.12 with h2" than requiring a slightly more recent Rust? If not, that seems like a small cost, with the benefit of having h2 by default in a nice separate version. (The alternative is 0.11.14 introducing a major new feature behind a cargo feature.)
&gt; let base = match c { 'a'...'z' =&gt; 'a' as u32, 'A'...'Z' =&gt; 'A' as u32, _ =&gt; return c }; This is not safe against timing attacks.
Eh... we all follow our patterns. Version numbers are a bit arbitrary and as /u/GTB3NW mentions, it isn't production ready. I have yet to release a 1.0 crate (mio is still at 0.6). I'm personally reserving 1.0 for when I'm reasonably confident that no APIs will break for a long time (if ever).
I'm not sure that will work. If I split the string `"there are words".split(" ")`, I get the slices `"There" "are" "words"`, but I want the tuples `(0, 5) (6, 9) (10, 15)`.
No, and it's rather important that there isn't, lest HashMaps be mishandled to copy their values in O(n¬≤) (earlier versions were accidentally quadratic in that regard).
I believe one big thing that rust hadn't got yet is const generics, AKA being able to use a boolean, integer, etc in a template instead of just types, for example: fn foo&lt;const b: bool&gt;(a: i32) { ... } This is already possible in C++. Apart from that, there are probably some other slightly hacky things that rust doesn't allow but C++ does. This is because C++ doesn't really check validity of the code in a generic template until you actually use an instance of it, whereas in rust you need to use traits to require that the types you use in generics support all the stuff you doe with it (e.g. addition using the Add trait).
Note that there are some hacks you can use to get around that, e.g. [typenum](https://github.com/paholg/typenum).
Care to elaborate? What should the world use instead? I am sincerely interested in how Rust and maybe wasm could be used to make platform-universal applications. HTML+CSS+JS has proven to be a very effective UI creator. It's also got the largest developer base by far. It seems that the ability to leverage this to build Desktop or even mobile apps would be a big win. 
Forgive my ignorance, but I've never quite understood what that was good for. Especially in rust where everything is immutable by default, why would you need const generics?
Thank you -- much appreciated!
Oh I misunderstood. Maybe you can use [`match_indices`](https://doc.rust-lang.org/std/string/struct.String.html#method.match_indices)? That'll give you an iterator yielding 5, 9, ... which you could use to create those tuples.
Ahh ok
Why not ? fn human_readable(byte: u8) -&gt; char { match byte { b' '...b'~' =&gt; { byte as char } _ =&gt; '.' }
Ahh, yes, that's where the underlying misunderstanding is occuring. I do not believe that to be the case. Rather, `str` is UTF-8 encoded unicode. This means the length of a character is variable. `h` is one byte, but `Â•Ω` is (I think?) three. Hence, you cannot jump a fixed multiple of bytes to get to a given character, as you might jump into the middle of a multi-byte character. Thus why `.n(...)` has overhead compared to `[...]` - it has to scan through the string to find the given character. Unicode is quite complicated, but UTF-8 is relatively simple. Query your prefered search engine about it.
Sometimes, features aren't about what you can do, but what you can't do.
This is a very good point
If that's the case, one of the best features of Rust generics is you can't generate megabytes of error messages because you try to use a type which doesn't satisfy the required trait! (see also [The Grand C++ Error Explosion Competition](https://tgceec.tumblr.com/))
You're not safe against timing attacks!
As far as I know, the compiler itself won't ban you, but you should check the license of the code editor/IDE you're using.
This is effectively what I'm doing when wrapping a large C library. I've got a unit struct which initializes the library on creation and destroys it on drop, then *all* types which work with that library must contain a reference to the guard. You get a bunch of lifetime annotations attached to your types, but I like how this design makes the library really safe. I remember wasting hours upon hours at work trying to debug what were effectively lifetime errors when using this library in C++.
I tried Mac builds for a while, but couldn't get gtk to compile. I was able to build cli programs though. I'll have to look at adding flatpack, haven't tried packaging anything that way before.
Recursive trait bounds are problematic in Rust: https://internals.rust-lang.org/t/recursive-trait-bounds/5265 This isn't a problem in Rust where generics need trait bounds, but not in C++ since it's basically "untyped".
Another good example would be the BTreeMap in the standard library. The branching factor "B" of a B-tree can affect cache locality, so it should be possible to determine it at compile time based on the type that's stored in the map. This is not possible without const generics so it's currently set to 6 (a good compromise).
Sorry, but this isn't the right subreddit. You probably wanted r/playrust
Ok thanks
\#notsureiftrolling
I have an [open PR](https://github.com/rust-lang/rust/pull/47228) implementing a related functionality, and I'm thinking about implementing a whole Cursor based API. Most of the required fuctionality (including the lookup of the first/last element) is already implemented internally, so the next step would be the API design. /r/arthurprs if you have ideas, can you reference it in my PR? Thanks.
Ok I looked into HashSet. I dont think I would want to use HashSet coz if I want to get the EDGE that I am looking for I should be able to update the EDGE with the uppdated value. The example I have here for my question is a mere representation of my problem and is not the problem itself :).. Thanks to you I now know a new collection type.
&gt; A popular project will increase languages adoption. This is unlikely to be popular, but my main point there is it's easier to increase language adoption with a framework or such which motivates code in that language (e.g. using rails means writing ruby) than with a standalone program (e.g. using nginx doesn't mean you write C). No one really cares what language a kernel module or static binary is in; they just care how to use it. It's frameworks and libraries which can more easily help with adoption. What you describe doesn't help rust more than it helps C, doesn't help rust more than it helps ruby, etc etc. As such, I don't see how this would have a significant impact on adoption, even if it were to be a good idea. &gt; &gt; in fact, I think not being written in C is basically instantly disqualifies a kernel module from existing currently, and probably for the next 10 years at least. &gt; Again, why? Linus doesn't accept C++ or rust into the kernel. The linux kernel isn't going to fork from linus's or greg-kh's branches any time soon. Linux distros are allergic to shipping out-of-tree modules (with nvidia and zfs being the two biggest exceptions I think), and companies are also allergic to using them. It's a massive security risk to use a module which must be updated independently of the kernel and may prevent you from updating your kernel. I won't get into this more since your response is misinterpreting my point and I think we really just aren't going to see eye to eye on this. I would be happy to be proved wrong and for you to make this project, make it successful, and to see an upswing in rust adoption as a result... I really doubt I'll be wrong though. Good luck.
For the Fn/FnOnce - could you create a `struct ServerCreator(Option&lt;ThingWhichYouWouldReturn&gt;`, implement `NewService` on it manually, and have it return a server-implementing `struct NoResponse;` if it's already returned the other thing once? As for exiting, hyper's `Server::run_until` should work alright if you have some kind of "two sided" future. I think a `futures::sync::oneshot`channel should work for this: you can give hyper the 'receiver' end, and keep the 'sender' in your server. (https://docs.rs/futures/0.1.17/futures/sync/oneshot/index.html) Let me know if that helps - I haven't implemented this before, so this is mostly just me dumping information that might be relevant.
I'd personally love to see a full-stack framework in rust like MeteorJS sometime in the future!
I believe you're looking for /r/playrust.
Last I checked, you can't generate additional items that depend on your generic parameters. That is, you can't have a generic function which contains a `const` that depends on the function's parameters.
Uh, woops. Let me format this right..
This is something I want to support in [`cargo-web`](https://github.com/koute/cargo-web/), so please create an issue on its issue tracker describing what you need exactly and we'll see what can be done. (:
So what are we missing?
are you an idiot? re-read my question: **I DON'T WANT TO CREATE A SECOND MODEL WITH THE ONLY DIFFERENCE BEING THAT 2ND ONE WILL BE WITHOUT "ID" SO THAT POSTGRESQL WILL GENERATE ID BY ITSELF.**
some idiot, who's unable to read the same thing at diesel.rs
In an embedded environment it's common to want things that have various buffer sizes. For example, I'd love to have one circular buffer libraries where I can template out the size of each block and how many blocks there are. This is done with macros in C, where I've done this before, but would be quite nice when using Rust const integer generics.
In C++ parlance, I believe the feature you‚Äôre talking about is called ‚Äúnon-type template parameters‚Äù, non-type because they allow you to declare a parameter to be a specific type &amp; take a value rather than a type as the template parameter
check out 'programming rust' by o'reilly it's legit af https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283
For anyone who cares about this, today I wrote some Rust bindings to this webview lib and translated the minimal and timer example, and they are working. The bindings code is still messy but I will clean in up and release them soon..
They work somewhat differently, I'm not a compiler writer so I'll probably use the wrong terminology :) C++ templates are effectively duck typed. So I just write templated code to access things that I expect to be there, if I used a type that doesn't have the expected thing then I get one a famously large C++ template compile error. Rust generics however are trait based. This is similar in essence to C++ concepts. The generic type will need to support the appropriate traits for the type of operation you are trying to perform. For example this won't work in Rust: fn add&lt;T&gt;(a: T, b: T) -&gt; T { a + b } Because Rust doesn't know anything about T, including if T supports add. To tell Rust T must support adding you need to specify a trait fn add&lt;T: std::ops::Add&gt;(a: T, b: T) -&gt; &lt;T as std::ops::Add&gt;::Output { a + b } Note the output type is because the Add trait output isn't necessarily a type T. In C++ you could write a similar template template &lt;typename T&gt; T add(T a, T b) { return a + b; } You can pass any type to this and it will only cause a compile error when the compiler attempts the + on a type that doesn't support it.
This isn't lisp, where code is data
You gain bad habits learning C and it's hard to forget them. You become aware of them learning Rust and learn to avoid them going from Rust to C. 
https://www.geeksforgeeks.org/memory-layout-of-c-program/ Can't tell if you're joking...
Because I tried learning C many times in the past, and had much difficulty getting anywhere with the language, so every attempt failed. Rust was fairly straightforward in comparison, and there wasn't any kind of knowledge gap to worry about. Only after learning Rust could I actually write C.
I am not in favor of this. Creating Strings is an allocation, and I like it being implicit.
For example you can have a String representing "Hello" and call .into_boxed_slice to get a Box&lt;str&gt;. (The method may be wrong I'm on mobile, but it's a thing). 
Really the same? Didn't someone say that generic associated types are not as powerful as full HKTs, e.g. not enough to implement monads?
An example that comes to mind is, in C++ you can express a generic function that takes its arg either by ref or by val, depending on the size of the type of the arg. But in Rust you can't, because ref-ness is a property of a mem location, not of a type.
Well I'm the opposite. I found C relatively straight forward but struggle with rust and all its ownership rules 
Or `(b' '...b'~').contains(byte)`
Wasm dom access. And old browser support.
No there are legit papers. On mobile right now but like if you google around you‚Äôll find plenty of resources. In fact the main cat of the switch is indirect. It‚Äôs it flush that‚Äôs expensive.
The key is that GATs and TQHRTBs *together* are enough to give you the full power of HKTs. Right now, all we have is NGATs and LQHRTBs, which clearly aren't enough, but even by including one of those two enhancements, we still don't have enough power; only by including *both* do we reach the same level.
New to rust! What is the easiest way to reverse a vector?
Its a joke... Lispers are always talking about how "lisp code is data", I was just going off that
So how would you define [`Monad`](https://github.com/mjepronk/wiwinwl-purescript#monad-purescript-prelude) with GATs+TQHRTBs?
Yeah, last time I used that crosshair my Visual Studio Code license expired: https://media.discordapp.net/attachments/274215136414400513/401187488141672448/image.jpg?width=428&amp;height=570
Welcome! vector has its own handy reverse function: fn main() { // note `mut` because reversing a vector will mutate it! let mut reversible = vec!["/r/rust", "to", "welcome"]; reversible.reverse(); println!("{:?}", reversible); // ["welcome", "to", "/r/rust"] }
Thanks! I did some googling and somehow didn't manage to find the built in reverse function
 trait MonadFamily: Sized { type Instance&lt;T&gt;: Monad&lt;T&gt;; } trait Monad&lt;T&gt;: Sized where for&lt;U&gt; &lt;Self::Family as MonadFamily&gt;::Instance&lt;U&gt;: Monad&lt;U, Family = Self::Family&gt; { type Family: MonadFamily&lt;Instance&lt;T&gt; = Self&gt;; fn pure(x: T) -&gt; Self; fn bind&lt;U, F&gt;(self, f: F) -&gt; &lt;Self::Family as MonadFamily&gt;::Instance&lt;U&gt; where F: Fn(T) -&gt; &lt;Self::Family as MonadFamily&gt;::Instance&lt;U&gt;; } 
They don't work for everyone, but small programming challenges help languages click with me as a beginner. There are a few sites to choose from but [codewars](https://www.codewars.com) is one. As a regular user of the site, I would recommend just sorting challenges from easiest first, and making sure to set "Status" to "Approved" to filter out the "In Progress". And even then, you may stumble upon a few questions you don't think are well designed. That's fine, just skip those.
Why not just put a crosshair sticker in the middle of your screen? You‚Äôre definitely not banned that way. 
No problem! If you want more "real world" issues, the community marks [starter github issues](https://www.rustaceans.org/findwork/starters) and the weekly newsletter and other places have calls for participation
I've been really enjoying reading all the #Rust2018 posts but I wanted to be able to go to one place to read them all and figured others might too. So I've collected a bunch of posts into a feed you can subscribe to and will continue to add new ones as the month progresses. The website was built with the [Cobalt static site compiler](http://cobalt-org.github.io/), which is written in Rust. I also wrote a couple of Rust tools to add new URLs to the feed and generate the RSS feed from the JSON feed. All the [source is on GitHub](https://github.com/wezm/read-rust).
The biggest one not mentioned so far is variadic templates, both classes and functions. It'd be nice to have the power to write a tuple class in rust (though you shouldn't), or a fold over a tuple of an arbitrary size, which leads to the next nice to have. Generic closures! It feels like an arbitrary restriction to prevent `|x: impl Debug| { print!("{:?}", x) }` in rust.
* `p1` is a mutable pointer to a `Point`. The pointer is allocated on the stack and stored in `p1`. The pointee is allocated on the stack and stored in implicit storage that exists just *before* `p1` does. * `p2` is an immutable `Point`, allocated on the stack. * `p3` is an immutable `Box&lt;Point&gt;`. The `Box&lt;Point&gt;` is allocated on the stack and stored in `p3`. The `Box&lt;Point&gt;` contains a pointer which points to a `Point`, which is allocated on a heap. By default, this is the Rust heap, but it could also be the C heap, or a system heap of some kind. * `p4` is a mutable pointer to a `Box&lt;Point&gt;`. So you have `&amp;mut Box&lt;Point&gt;` on the stack pointing to `Box&lt;Point&gt;` on the stack, pointing to `Point` on a heap. * `p5` is a mutable `Point`, allocated on the stack. * `p6` is a mutable `Box&lt;Point&gt;`. The `Box&lt;Point&gt;` is on the stack and points to a `Point` on a heap. I keep saying "a heap" rather than "the heap" because there's more than one heap. That said, it generally doesn't matter which one you're using, unless you're trying to transfer ownership between different bits of code that might disagree on what "heap" means.
Not sure if I got p1 correctly: - A mutable pointer to Point points to p1, which itself is allocated on the stack. - The Point is also allocated on the stack, but in implicit storage. Is that it? What implicit storage means in fact? 
There are two storage locations. One is named `p1` and contains the pointer. The pointer points to the other one, which doesn't have a name, and is also on the stack. If it was explicitly written in the code, it would be just before `p1`, and be written something like `let p1_pointee = origin();`. You told Rust to make a pointer to the result of `origin()`, so it has to *store* the result of `origin()` somewhere so the pointer has something to point at.
If I do something like this: println!("p1 = {}\tp2 = {}\np3 = {}\tp4 = {}\np5 = {}\tp6 = {}", mem::size_of_val(&amp;p1), mem::size_of_val(&amp;p2), mem::size_of_val(&amp;p3), mem::size_of_val(&amp;p4), mem::size_of_val(&amp;p5), mem::size_of_val(&amp;p6)); Then my results are: p1 = 8 p2 = 16 p3 = 8 p4 = 8 p5 = 16 p6 = 8 Which make sense, since p2 and p5 are the size of the struct itself, and the rest are the size of the pointers in my 64-bit system. But when dereferencing then, by doing \*p1 and \*\*p4 gives me the following error: let p7 = *p1; let p8 = p2; let p9 = *p3; let p10= **p4; let p11= p5; let p12= *p6; println!("\np7 = {}\tp8 = {}\np9 = {}\tp10= {}\np11= {}\tp12 = {}", mem::size_of_val(&amp;p7), mem::size_of_val(&amp;p8), mem::size_of_val(&amp;p9), mem::size_of_val(&amp;p10), mem::size_of_val(&amp;p11),mem::size_of_val(&amp;p12)); I get the following errors: error[E0507]: cannot move out of borrowed content --&gt; sh.rs:36:14 | 36 | let p7 = *p1; | ^^^ | | | cannot move out of borrowed content | help: consider using a reference instead: `&amp;*p1` error[E0614]: type `sh::Point` cannot be dereferenced --&gt; sh.rs:39:14 | 39 | let p10= **p4; | ^^^^ 
The first error is just what the compiler says. If you don't understand what it means, you need to read up on [ownership](https://doc.rust-lang.org/book/second-edition/ch04-00-understanding-ownership.html). The second error is because I messed up. I was looking at the initialiser, not the explicit type you added. What I said was correct for this: let p4 = &amp;mut Box::new(origin()); What you wrote was this: let p4: &amp;mut Point = &amp;mut Box::new(origin()); That is slightly different. The `&amp;mut Box&lt;Point&gt;` doesn't fit in a `&amp;mut Point` variable, so the compiler resorts to "deref coercsion": it repeatedly dereferences the right hand side until it matches the left hand side's type. In effect, it reaches inside to the `Point`, and takes a pointer to that. So `p4` as you wrote it is a mutable pointer to a `Point`. `p4` is on the stack and points to a `Point` that is allocated on a heap. This allocation is owned by a `Box&lt;Point&gt;` in implicit storage.
&gt; Rust is for people who wish they could write C/C++ programs but found those languages too unapproachable. &gt; &gt; maybe Rust is not for people who are already C++ experts and who are happy with C++? I really don't want Rust to be seen as a toy version of C++ or "dumbed down C++" or something. To put it bluntly: I don't want the thought to become popular that people write Rust because they are too stupid to write C++, the "proper" systems programming language. Instead, I want Rust to be its heir. The thought that I want to become popular is that C++ is legacy and that Rust is the future. Yes, Rust is in many ways easier than C++. That's one of its advantages. Yes Rust attracts people from dynamic languages. And this helps growing the community so that is great as well. But I think that Rust's main goal should always be to get C++ developers to switch over to Rust. Because if Rust becomes uninteresting for C++ developers then most likely because Rust stopped being a systems language. vorner [has put this very nicely](https://vorner.github.io/rust-2018.html): Rust shouldn't become another compiled python or compiled ruby. There is already a ton of these languages that are closer to this idea. But there aren't many true replacements for C++.
In fact I haven't gotten to ownership yet, I was messing with it. But thanks for the explanation regarding the allocations :-) I will get on the latter part later!
Dunno. Ask /r/playrust
I think you are reading a little much into that. Given that _roughly a third_ of newcomers come from dynamic languages (another third from functional compiled languages and another third from systems languages), shows that Rust does something more interesting: it leads to a re-surged interest in systems programming from all sides. &gt; maybe Rust is not for people who are already C++ experts and who are happy with C++? I don't quite agree with that statement from the post of the OP as well, that's definitely not the goal of the language, _but_, jvns is right in the sense that those that are very happy and productive with C++ are not the ones to spend extreme amount of time convincing. Let's try those uncomfortable and unhappy first and boost them up.
I head the Diesel team tried to enter that competition once ;).
This post makes me so happy, and not because it quotes me approvingly. Everything Julia says about her experience with Rust is what I want to hear is happening for people :-D.
`str` does not implement `Sized`, thus it's required in order to pass `&amp;str`.
Sorry, didn't mean to imply that with my comment. I meant to say: Rust's preferred tool for metaprogramming is proc macros, whereas templates do it for C++. They both have advantages and disadvantages, and neither cover all the things you can do with the others.
I think you won't get the best answer about templates in the Rust community. People don't like them here, and there's... not an insignificant amount of FUD going around. You can do most things with templates, and I think they're an elegant solution to the problem of creating generic code in an un-GC'd language. However, C++'s templates are hard to understand without the context of the design of C++'s types. C++'s class system is about the closest thing you can get to a duck typed ML module system. I dunno, I'm not sure exactly where I'm going with this. There's a lot about the philosophy of C++'s type system that I think would be good to talk about, but it really requires a full on blog post or a talk or something. I don't think you'll get a good answer on reddit. Learning an ML will get you pretty close to understanding the philosophy behind the C++ type system though - functors are equivalent to templates, modules equivalent to classes.
Thinking of C++ as ML is a very good way of thinking about things: template &lt;typename T&gt; class optional { private: ... public: auto is_valid() const&amp; -&gt; bool; auto get() &amp; -&gt; T&amp;; } becomes module Option(T: Type): sig type t val is_valid: t -&gt; bool val get: t -&gt; T.t end
I would argue that they're different enough that saying both of them are "metaprogramming" is misleading. Functions on types/modules are quite different from functions on the ast - they're both useful in their own way, but they're apples and oranges, imo.
The crab book is fantastic.
I share the view that Rust is a less intimidating systems programming language‚Äîit has been a tremendous win for my self-confidence if nothing else‚Äîbut don't believe that it is a language with training wheels. I feel that Rust provides liberation rather than constraint. You know you're safe so you can go for a walk in the dark.
You're making a greater distinction than is necessary. Aside from/given const generics, SFINAE and `impl` `where` clauses have similar power, and monomorphization *is* substitution. Both C++ templates and Rust generics are turing-complete (via associated types), the difference lies in the explicitness of bounds and ad-hoc polymorphism. In Rust we have implicit bounds out of the "WF" (well-formed) requirements of signatures, so you can imagine C++ as having WF over the entire body of a function (even if I don't think current-generation C++ compilers take advantage of this). While the template expansion may seem more like a macro-by-example, it's *still* type/value-driven, just in a more ad-hoc and implicit way.
Just one small disagreement about terminology, in Rust, we call `&amp;x` a *reference* to `x`. A *pointer* would be `*x`. That one is nullable and primarily used for interfacing with C.
Could you give an example where you would need such a closure?
And yet I remember people arguing for the exact opposite. I remember because I *used* to call them references until I was convinced to call them pointers and *it's too late now, no backsies.*
&gt; Since there seems to be no roadmap of how and when to remove that limitation, it does not look that temporary to me, in practice. I would consider it as a sufficient reason to not stabilise `impl Trait` and put additional effort into researching the matter but of course this is not for me to decide. &gt; A newtype does allow the library author to change how the trait is implemented I have missed the fact that one can define internal type as private and this prohibit destructuring by API user, so I stand corrected here. I guess it boils down to definition of elegance in that case. To me it is a property of a language allowing to write _idiomatic_ code in the language with minimal amount of irrelevant boilerplate. If one agrees that type erasure for returned traits is something that should be widespread, it is hard to sell if one needs to provide proxy implementation for that trait each time for each named result type. If it is not something that should be expected common in the library code, solution doesn't really matter and the one of least effort should be chosen. To certain extent this is a general problem for me in Rust - there is more focus on ergonomics of micro-snippets than higher level idioms. Probably only exception was addition of `?` which was a real deal breaker.
üí∞prideüí∞andüí∞accomplishmentüí∞
Paging /u/steveklabnik1 ‚Äì please clarify!
I don't think there's a specific *tooling* for that. In general, I think it boils down to two areas: * Tooling and infrastructure you probably want anyway, not just because of new people. All the usual best practices ‚Äí have a version control system, have documentation, have some tests. And making sure it is easy to find (doing it in a similar way that everyone else helps, unless you have good reasons to make something else). * Being nice and welcoming. And invest the time to help them. You know, there's no silver bullet: https://techcrunch.com/2011/10/25/lead-bullets/
That's not exactly true. You still need to use specialized primitives (eg. different TCP stream than the one from std). You just can use them in more procedural way than futures. And I wouldn't recommend may to a beginner for two reasons: first, it's separate universe from the rest of the ecosystem ‚Äí most async things today return futures and there's no easy way to combine them with may. So you have to go full may or not at all. The second is, may wants to mimic go in its N:M threading. Which breaks assumptions in Rust about thread safety. This is left to the user to solve. This includes not using thread local storage in a coroutine, but some other things too. And when many things in std and in whatever dependencies you might have use TLS, it's kind of hard to make sure it is not used. If you look for stack-full coroutines (I believe there's some place for them even after the language-level generators are stabilized), there are other options without these problems (yes, that's actually a dirty ad for https://crates.io/crates/corona).
Yes, that worked, thank you! I went with `loop`, `if` and `break` because I found that somewhat clearer :)
Why wouldn't a logging library detect which module logs what? For instance: With (simple_logger)[https://github.com/borntyping/rust-simple_logger] instead of `2015-02-24 01:05:20 WARN [logging_example] This is an example message.` I get `2015-02-24 01:05:20 WARN [&lt;unknown&gt;] This is an example message.` With [simplelog](https://github.com/drakulix/simplelog.rs) instead of `11:13:03 [ERROR] usage: Bright red error` I get `11:13:03 [ERROR] Bright red error` (no module name at all).
Paste this function's and deserialize's signatures.
const generics/ 'non-type template parameters', whatever you call them; HKT ('template template parameters'). I do also miss the ability to nest (sharing template parameters through nested classes) ... I hope rust eventually gets module-level type parameters, that would be superior.
In addition to googling, the docs for the specific trait (in this case std::Vec) are a great place to look: https://doc.rust-lang.org/std/vec/struct.Vec.html
pub fn variable_get&lt;'de, T: ::serde::Deserialize&lt;'de&gt;&gt;(key: &amp;str, default_value: T, conn: &amp;PgConnection) -&gt; T { use schema::config::dsl::*; let ret = config.select(data) .filter(name.eq(key)) .first::&lt;Vec&lt;u8&gt;&gt;(conn); if let Ok(val) = ret { return ::bincode::derialize(&amp;val).unwrap() } return default_value }
What is `config` here?
Diesel schema dsl.
This is really cool! Some people have played with lints that do similar things in Rust, like https://github.com/mcarton/rust-herbie-lint But I think people haven't played with this enough. Landing these kinds of things in clippy would be great.
Is `::serde::Deserialize&lt;'de&gt;` really the type you want to return from your function? You're deserializing something into a T which can be deserialized again. I suspect this is not what you want.
Indeed. You need to write 1u32.add(2).add(3) Kinda lame, actually.
&gt; I think you won't get the best answer about templates in the Rust community. Is that a challenge :D ?
 return deserialize(&amp;val).unwrap() | ^^^ borrowed value does not live long enough
&gt; You're better off rewriting Nginx, Varnish, and/or HAProxy to use it We're already on it with [s≈çzu](https://github.com/sozu-proxy/sozu) :)
&gt; I feel that Rust provides liberation rather than constraint. Indeed. **Fearless** is the key term here :). With C++ I had to use gdb from time to time. Sometimes because I was debugging someone else's code, sometimes it was my code which was triggering UB. With Rust, that kind of thing is not happening unless unsafe code is involved which I fortunately don't have much to do with.
*Disclaimer: I do not purport to have all answers. C++ is a complicated beast, and Rust is moving fast. If you happen to spot a mistake, please point it out and I'll edit this post to fix it.* There are many differences between Rust generics and C++ templates. The bulk of the functionality is similar, if reached through different means, but the edge cases vary a lot. I think it's easier to just examine the various axes one at a time. --- **Variadics** Rust has built-in support for tuples and function types, and that's it. It otherwise does not feature variadic generics inherently. Like the good ole days of C++ (pre-C++11), it can be somewhat emulated with *type-lists*. The idea is to encode a list of a types in a cons-list of types as in `Cons&lt;T, Cons&lt;U, Cons&lt;V, Null&gt;&gt;&gt;` and use recursion over this list to iterate over the types. It's not pretty, especially in terms of error messages, and it stresses the compiler, but it should optimize well. *Unresolved question: will Rust one day allow manipulating variadics?* --- **Higher-Kinded Types** In C++, those appear in the form of template template parameters, or nested template types. In Rust, the latter is going to be available under the name Generic Associated Type (GAT). GAT should allow the full power of template template parameters: where a C++ parameter is `template &lt;typename&gt; T`, you will in Rust pass a `T` with a nested `Template&lt;U&gt;` type. --- **Genericity over values** In C++, you can use integrals, enums, as well as several kinds of pointers, as template parameters. Rust should soon allow to parameterize with integrals, it's unclear how much more it will allow. They are likely to differ in how generic code is type-checked. In C++, type-checking of non-type template parameters occur when the template is instantiated, which allows returning an array with size `N` which is produced by a function which derives `N` from some other sources/computations. Rust traditionally attempts to type-check generic code *once*, which would here require a proof that the function will derive the same `N` (in the bounds). *Unresolved questions: what will bounds on const parameters be like? What will they allow?* --- **Specialization** In C++, any struct/class or function/method can be specialized for a specific set of types with no limit: - struct/class need not expose the same interface, contain the same members, etc... - function/method need not return the same type. This is extremely flexible, though once again the error messages can suffer a *bit*... In Rust, for now, struct/class cannot be specialized, only trait implementation (methods) can. Furthermore, a more specialized trait implementation must follow the interface of the original trait to allow generic code to be checked against the original trait interface (including lifetimes). *Unresolved question: will it be possible to specialize `struct`/`enum` data?*
Right now, you can't match "through" an Rc. I'd dereference the rc to a reference to match. For example: match (&amp;*f, &amp;*g) { (&amp;Int(m), &amp;Int(n)) =&gt; Int(m + n), _ =&gt; unimplemented!() }
&gt; I think what I'd really like in 2018, is to be able to use a 'known good' nightly; rather than just rolling the dice on whatever happened to come out of the CI system last night, I'd like to be able to take stable Rust, and turn on the experimental features I need. I think that's a very good idea. I am not sure how to about it though. It seems simple enough to add two more channels: `beta-dev` and `stable-dev`, however we would not want users start shipping services to production with nightly features enabled and then complaining about breakage. Maybe we should work on fixing the root cause, and work off those nightly features instead. Easier said than done of course. I would note, though, that just because you use nightly you should not feel compelled to update on a ~~nightly~~ daily basis. You can perfectly only update nightly every 6 weeks. &gt; And in an ideal world, I'd like to see some of these features go away - the syntax for inserting assembly instructions into the compiler output for example, shouldn't be too difficult to fix, for example, as C has been doing it for decades. It's being actively discussed. Unfortunately the ASM syntax is closely coupled with LLVM at this stage, which isn't too nice. It is also important to consider cross-compilation and platform availability: personally I would wish for ASM bits to only be usable in functions declaring which target they are for (using `cfg`) and if necessary which augmented CPU features they enable, to be able to: 1. Have different ASM bits depending on which target/CPU features we are compiling for, 2. Being able to tell at a glance (aka with tooling) which targets/CPUs the crate can be compiled for, and ideally extract *that* information and present it on crates.io. It's frustrating to start depending on a crate, and then fail to deliver for windows or ARM because the new dependency doesn't actually support them :(
&gt; How about making Rust the first language with a slogan "Zero-overhead made easy"? I like this slogan :D
&gt; I'd like to see some of these features go away - the syntax for inserting assembly instructions into the compiler output for example, shouldn't be too difficult to fix, for example, as C has been doing it for decades. Inline assembly is not part of the C specification. Instead, some C compilers provide pragmas with inline assembly. GCC and Clang pragmas are mostly equivalent to each other but the MSVC inline assembly follows an entirely different approach. The equivalent of a pragma in Rust is an unstable feature. The difference is only that C compilers per default allow pragmas while Rust requires you to be on nightly (or to use environment variables NOT intended for end users) in order to enable them. So stabilizing `asm!` isn't catching up to C but instead getting beyond C :). Other than that, great post. I definitely agree that embedded should be one of the places Rust is strong in and where I hope Rust to have future growth.
You *won't believe* number 3!
&gt; I really don't want Rust to be seen as a toy version of C++ or "dumbed down C++" or something. The reverse is a lot closer to the truth: C and C++ *are* toy languages that will break horribly as soon as you try to do anything that's at all challenging with them, and Rust (as soon as it gains the proper support) will ultimately be regarded as the 'pro', 'industry-standard', 'enterprise' counterpart to those languages. This isn't even theoretical, as we *have* already seen a memory-safe language (albeit not an especially high-performing one, compared to what can be achieved by Rust) largely supplant C/C++ in the enterprise, specifically due to the increased robustness that memory safety implies.
You are correct.
All references are pointers, but not all pointers are references.
Try forgetting an ampersand when calling a diesel function if you dare.
&gt;I think what I'd really like in 2018, is to be able to use a 'known good' nightly; rather than just rolling the dice on whatever happened to come out of the CI system last night, I'd like to be able to take stable Rust, and turn on the experimental features I need. That way, my integration exercises only need to take place every six weeks rather than every time I do a rustup update, and we can all 'settle' on one compiler version rather than working on five projects requiring five different specific versions of nightly. That's stable rustc with unstable features unlocked with `RUSTC_BOOTSTRAP=1`. This way you have a well-defined version (not a random nightly) + all backported regression fixes (nightly regresses regularly) + inline asm and other necessary features are still available.
lol, you can help me with this question?
It's completely subjective, and you haven't provided any sort of basis for ordering. Important *how?* If you just want five traits... Copy, From, Iterator, Fn, and Itertools?
Thank you for answer. That's it. To you, probably, this traits are mostly important in Rust.
You could specify that the iterator item needs to implement `AsRef&lt;str&gt;`, like [this](https://play.rust-lang.org/?gist=a0c65768885d6ee1c92a91bf2b7faebb&amp;version=stable). That will allow you to work with the item as a `&amp;str`.
number 3 is the trait that c++ developers don't want you to know!
lol, you can help me with this question?
This function should [work](https://play.rust-lang.org/?gist=d1538f091e801c0e32f3d36cf40c7bf3&amp;version=stable): fn foo&lt;T: AsRef&lt;str&gt;, I: Iterator&lt;Item=T&gt;&gt;(val: I) { // .. } 
Help us by defining "important". Do you go for the "most used" traits? Or traits that enable some advanced techniques less seen in other programming languages without traits? Is it more like ‚Äì what advantages do we get with traits rather than OOP? "Important" is really hard to get a feeling of what you're expecting an answer could look like and is really subjective.
How do I handle the `(f, Add(Int(n), g))` case? 
Yes you need to do the unpleasant break up into submatches for the nesting. Shepmaster is technically correct but forgets to say it doesn't apply to `Rc`, nested matching works fine in Rust. Not for Rc, because it is opaque. Nested matching works fine for accessing public fields.
How can we know these "bugs" aren't some sort of government ordered backdoors? Efficient and secure crypto must be their worst enemy.
Dear /u/ThomasdH, Citing from the [official ROT26 page](http://rot26.org/), &gt; WARNING!! we have not yet been contacted by NSA/USA/UK to provide a backdoor to our encryption service... but its only a matter of time until it happens. Note: we will keep this notice up-to-date so please check this website regularly. Regards, The Rust ROT26 Team
Those methods are defined on `&amp;str` - they are not part of a trait. You already have the required bound: `&lt;I as Iterator&gt;::Item: AsRef&lt;str&gt;`, you just need to call it: arg.as_ref().parse() arg.as_ref().chars() 
Thank god, I am so happy it seems like Rust is getting some embedded love this year. I really hope Rust for embedded systems becomes more approachable for people. The process seems fairly lengthy and annoying as it is 
thanks for this -- I wrote the post and edited it to say I actually think Rust is both for people who don't know C/C++ at all and C/C++ experts. IMO that's one of the coolest things about Rust!
ah awesome, thank you
Volatile [reads](https://doc.rust-lang.org/core/ptr/fn.read_volatile.html) and [writes](https://doc.rust-lang.org/core/ptr/fn.write_volatile.html) have been stabilized.
An answer to most of the "Why isn't X available by default?" questions is "Because we want to do this right and we are not there yet." 2018 should be there year we _get_ there!
Traits are great. Someone should make [a talk](https://www.youtube.com/watch?v=0zOg8_B71gE) promising to show idiomatic Rust but then ramble on about awesome traits for half an hour!
Interesting - cargo usually downloads as soon as you do a cargo build. If you don't change versions, you can work completely offline. I once had an internet outage and I could work on, until I needed to look something up (via Google). This wasn't the failure of cargo, though. And no, usually you don't need to connect to the internet if you run an application, not even for the first time. Only when downloading packages, which happens the first time you do `cargo build` or `cargo run`. And for documentation, is `cargo doc --open` not good enough? I mean, I can't speak for the quality of documentation of certain libraries. But Rust has offline documentation, just as man pages. I'm not sure what you experienced there. &gt; call Cargo programmatically not sure what you mean with "programmatically". You can always do `system("cargo build")` (or whatever the command for your language is). You can automatically download cargo and rust. I'm not sure what you are missing. 
`&amp;Vec&lt;u8&gt;` coerces to `&amp;[u8]` just fine ([because it implements `AsRef`](https://doc.rust-lang.org/std/vec/struct.Vec.html#impl-AsRef%3C%5BT%5D%3E)). That's not your problem.
I do like it, but it reminds me of last year‚Äôs fireflower discussion. It focuses on the what, not in what that lets you do.
Haha, yeah I figured as much. The tone of this post was intended to be: "Oh hey, these are all the things I've struggled with, I hope 2018 might be the right year to make this better!" Hope that came out right haha ‚ú®
I *am* too stupid to write C++. I also think *everyone* is too stupid to write C++, and the people who insist most that they aren‚Äôt, or who would look down on someone for choosing Rust, are the ones I‚Äôm most worried about. How much more pointless damage has to be wrought before we set aside this macho bullshit and actually take steps to make things better?
Heya, thanks for your reply! Let me try my best to answer a few of your statements: &gt; cargo usually downloads as soon as you do a cargo build So, if you do `cargo install`, then apparently you also need to run `cargo build` before you can work offline. This was unexpected behavior, and felt it was worth bringing up. --- &gt; And for documentation, is `cargo doc --open` not good enough? (...) Your development environment isn't a raw kernel, is it? (...) I don't really want to trade in rustdoc for man pages So I'm seeing two statements here: 1. Do we _really_ need docs available from the command line? 2. I like `cargo doc`, and don't want to give it up. So to answer the first statement: it isn't crucial, but it's pretty darn useful. CLI docs are fast to open, searchable, and work even when there's no browser available (like when you SSH into a machine, or indeed run on a raw kernel). I was surprised there weren't any docs available for any of Rust's stdlib crates. It'd be cool if there were. To answer your second statement: I'm not suggesting dropping HTML docs. Instead it seems like a useful addition. --- &gt; not sure what you mean with "programmatically" By "programmatically" I mean having direct bindings to the program. Being able to call functions on it, getting proper errors back, all without needing to rely on string parsing. Because command line output can change, and status codes convey little information, needing to shell out to programs isn't quite ideal.
[removed]
Delighted to hear! :D
Agreed, like everything it really depends on the audience. "Zero-overhead" and "easy" in the same sentence sounds really good to a C++ programmer like me, because I already know the *what*, it's the *how* which cuts me deep and bleeds me dry :)
We have chosen to support all of our embedded SDKs as Rust. It's getting increasingly difficult to defend that choice, due to problems as seen in the linked post.
Thank you! ^^
I believe SQLite also supports pluggable filesystem backends, which would provide an alternative to Emscripten's VFS emulation. (Whether SQLite is "designed to be embedded" to a significant enough degree to be usable in an Emscripten-less WASM project, on the other hand, I don't know.)
Sadly no, not really. The features determine the OpenGL version to use. If the users of the library forgot to set the feature for their right OpenGL version, they can encounter really weird compilation / linking errors. `cargo doc` has a `--features` flag, for example. `cargo publish` sadly doesn't. :/
Just some comments with pointers to ongoing projects, as I think people will be interested in this even if all I can say is "stuff's being worked on but it's not done" :) &gt; using cargo while offline Yeah, there's been some talk about that, and you can use more of cargo while offline than last year (I guess), but it's nowhere near done. You can, however, use &lt;https://github.com/alexcrichton/cargo-local-registry&gt; to manually create a local mirror. &gt; [offline docs, man pages] Interesting use case. There is a rewrite of rustdoc in the works, that basically uses the same interface as RLS (Rust language server, used for IDE/editor plugins). In theory, you could write a frontend that targets a CLI instead of HTML. Relatedly, an editor plugin using RLS will be able to offer not just autocomplete stuff but also show docs for items, [like this](https://imgur.com/a/eMBGZ). &gt; By "programmatically" I mean having direct bindings to the program. In addition to `--message-format=json`, you can also use cargo as a library! https://crates.io/crates/cargo
I disagree strongly with the sentiments expressed in this comment. I would say I'm trying hard to lead the Rust community in almost the opposite direction! Let me start with the idea that "Rust's main goal should always be to get C++ developers to switch over to Rust". My view is that the people with systems programming needs/desires goes *far* beyond the people writing C++ today. In other words, the market for systems programming is much, much bigger than the C++ developer base. I see Rust as *the* language poised to "unlock" this much larger market for systems programming. The reason that programmers don't choose C++ today isn't because they're "too stupid" to write it. It's rather because they've taken a clear-eyed look at the *costs* involved. C++ is so expensive to use that it only makes sense when it's truly required. This is part of why the Rust core team has been emphasizing productivity *in addition* to performance and reliability -- it presses our advantage. For example, the Rust core team has talked extensively with Dropbox engineers about their decision to use Rust in various contexts. These engineers are not "too stupid" to use C++; many of them have extensive C++ backgrounds. But they are well aware of the pitfalls, and what it means to have a *team* of people productively work on a C++ codebase. For that reason, they were also considering Go, a language that wasn't as good a match for the problem space but would be far less expensive (and more productive) for the team. The reason that Rust won out wasn't that it was a "proper" systems language. Rather, it was because it offered the *right mix* of performance characteristics (notably, predictable and low memory usage) together with high productivity. Besides, what is a "proper systems language" anyway? Your comment seems to essentially equate it with C/C++. I think, rather than acting as gatekeepers for the term and concept, we should be talking in much more concrete terms about the concerns that matter to people: speed, reliability, predictability -- *and* productivity. I also object to the "zero sum" analysis that pits productivity and learnability improvements directly against being a "true" systems language. Can you give any concrete examples of the risk you see for Rust becoming a "toy"? Did you look at the examples in Julia's post, which were things like NLL, improved error messages, and the new match semantics? In short, I think Rust should be *far* more ambitious than merely being the heir to C++. We should aim to empower a much larger base of programmers to write code that is performant, reliable, and pleasant.
The way the "compile time" problem is usually solved is by doing prebuilds. At install time you get the right prebuild, and tada - it now works. In the case of Node.js it'd even be easier, as WASM might (eventually) be an option to allow it to run on every platform.
I like `IntoIterator`, `Clone`, `Eq`, `Ord`, and `Display`
My top 5 "traits that make Rust good": - [`Deserialize`](https://docs.serde.rs/serde/trait.Deserialize.html) - [`Iterator`](https://doc.rust-lang.org/std/iter/trait.Iterator.html) - [`Clone`](https://doc.rust-lang.org/std/clone/trait.Clone.html) - [`Sync`](https://doc.rust-lang.org/std/marker/trait.Sync.html) - [`Sized`](https://doc.rust-lang.org/std/marker/trait.Sized.html)
Oh, okay. That solves it.
Correct, it is rather bad as a general purpose slogan for this reason. At the same time it can be a very good pitch for a specific target audience who is already familiar with zero-overhead principle and cares about it. And embedded devs tend to make exactly that kind of audience.
&gt; I really don't want Rust to be seen as a toy version of C++ or "dumbed down C++" or something. I *think* you're talking about rust losing its heavy focus on systems-level programming? This isn't going to happen. The whole reason Rust exists is to make systems-level programming a good fit for a wide range of programmers. If you go back and read the post that kicked off the ergonomics work: https://blog.rust-lang.org/2017/03/02/lang-ergonomics.html there are some good quotes about this: "make productivity a core value, without sacrificing the others." Meaning "fast, reliable, productive‚Äîpick three", as Aaron says in a different post. Rust will always be systems-level. It will always focus on being as fast as possible and as reliable as possible. Those two goals can stay true *and* we can go after being as productive/ergonomic as possible.
Just for the case you assumed the opposite: I don't look down on people who claim they can't write C++. Writing proper C++ is hard and coordinating this in a team even harder. I am too stupid to write C++ and so is my wife :p. But that doesn't mean that Rust should be mainly marketed as "C++ for the stupid" because that would make C++ devs stay away.
&gt; So, if you do cargo install, then apparently you also need to run cargo build before you can work offline. I may be misunderstanding you but you don't need to do a cargo install. I think maybe you're just used to how npm works? If you're working with a source project you only need to run cargo build, it will download the dependencies and as long as you haven't added any deps or changed versions you can work offline. https://doc.rust-lang.org/cargo/guide/working-on-an-existing-project.html
I'll second the documentation comment. I just spent a few days without a wm or internet connection and it was not fun.
&gt; I would love for Neon users to be able to combine lifetime elision and the impl trait shorthand syntax to write something like: Nonononono, `impl Trait` will not replace generics! It's meant like a type inference for complicated types.
&gt; to be able to use a 'known good' nightly; rather than just rolling the dice on whatever happened to come out of the CI system last night If you have a `rust-toolchain` file in your repository that contains for example `nightly-2018-01-04`, rustup will automatically use that version. This is what Servo does, I‚Äôd recommend it to any application that uses Nightly. (Maybe not to libraries whose users are not all also maintainers?) Servo also uses a number of unstable features. And even in stable code, occasionally something breaks when a compiler bug slips into Nightly, or when a new optimization uncovers a bug in some `unsafe` code. Pinning to a specific Nightly and having a regular CI job that check the latest one allows use to deal with all this on our own schedule without blocking other work. (That CI job is daily for us, but other projects might want it less frequent.)
Who is ‚Äúwe‚Äù? I‚Äôm not sure the rust team is aware anyone is doing this. We tend to prioritize production users‚Äô needs, if we can.
Frankly, I‚Äôm happy to let the C++ devs who would be kept away by that message have their way. Nobody benefits from a culture of worshiping complexity and equating smart with difficult.
https://github.com/geeny/linux-hub-sdk
I don't know anything about cpupool or futures but it [seems to work](https://play.rust-lang.org/?gist=c7e0db8bc7a5b175fb8a5ef8fe4c2efc&amp;version=stable) if you follow the hint by changing both `H: Handler` trait bounds to `H: Handler + 'static`.
I have the following data structure: ``` enum MaybePair&lt;T1, T2&gt; { Pair(T1, T2), Nothing, } impl&lt;T1, T2&gt; Drop for MaybePair&lt;T1, T2&gt; { fn drop(&amp;mut self) {} } ``` I want to implement the function ``` fn deconstruct&lt;T1, T2&gt;(p: MaybePair&lt;T1, T2&gt;) -&gt; Option&lt;(T1, T2)&gt; { match p { MaybePair::Pair(x, y) =&gt; Some((x, y)), Nothing =&gt; None, } } ``` I get "E0509: cannot move out of type which implements the `Drop` trait". The error message makes perfect sense, this is to avoid situation where only some of the fields are moved, but I'm moving all fields. I want to convince the compiler that in this branch of the match statement `p.drop()` should not be called. Or maybe that `p` should be replaced with `Nothing`, and only then `drop()` should be called, if that's easier, I don't care. Is there safe way to express it?
Is there a lowest common denominator that you could make a non-feature (I.e. `#[cfg(not(and(feature = "opengl-2.0", feature = "opengl-2.2", ..)))]`)
[catching functions](https://internals.rust-lang.org/t/pre-rfc-catching-functions/6505) is one of the very few rfcs that I really hope will not be merged. It adds so much magic and mess and does not bring any new functionality. I really cannot understand why people are so excited about it. Maybe because its author removes comments that tries to criticize this RFC?
&gt; I find the road taken with the new module system interesting: clear deprecation of the old style and actually replacing it in a new epoch. Yes, me too. I really like the epoch idea. Removing stuff in the new epoch shouldn't be done light heartedly, only if there is a reason, but if the old system does nothing else than standing in the way and making the language more complex, then you should break it.
&gt; Maybe because its author removes comments that tries to criticize this RFC? Clearly not, as that thread has plenty of non-removed criticism.
Completely agree, I've mentioned it a few times here (with some other things). I should probably write a proper blog post about it 
If anyone got contact details, I'd be happy to.
Basically what that error is saying "The `H` type could be a reference, and so you could spawn a worker, give it that reference, and then drop the thing the reference is borrowing while the worker is still running". Adding `H: Handler + 'static` means "H must be something that doesn't depend on another object's lifetime." I don't really like `'static` for a name because it feels like it gets used for three different meanings. But oh well.
...and thanks for drawing my attention to the fact that I typo'd "lynx" there.
https://github.com/rust-lang/rfcs/blob/master/text/1951-expand-impl-trait.md#motivation-for-expanding-to-argument-position
&gt; catching functions is one of the very few rfcs that I really hope will not be merged. It's not even an RFC yet. &gt; Maybe because its author removes comments that tries to criticize this RFC? The author is not on the mod team, and so doesn't really have the power to do that. &gt; I really cannot understand why people are so excited about it. I would suggest that maybe you ask, rather than casting aspersions on its author.
I disagree with 99,99% of this document
Actually, once `universal_impl_trait` and `in_band_lifetimes` are stabilized, you'll be able to write something remarkably similar to the example: fn get_foo_bar(scope: &amp;mut impl Scope&lt;'a&gt;, obj: Handle&lt;'a, JsObject&gt;) -&gt; JsResult&lt;'a, JsValue&gt; You can't quite get the rest of the way because there's not a clear way to infer that, for example, `Scope` should have a `'a` parameter while `&amp;mut` should not.
Interestingly there's a similar controversy in the C++ world about the "short-hand" syntax for concepts. Ie. whether the following should be allowed: void foo(MyConcept x); as sugar for template&lt;MyConcept MC&gt; void foo(MC x); There, of course, the problem is that there's nothing that syntactically differentiates the short form from a regular monomorphic function signature. On the other hand in Rust the `impl` keyword is there which is in my opinion enough to signal that the function in question is polymorphic on the parameter type in question (or the result type). 
I'll start with your example, as I think it's a good starting point for thinking about the problem. The same reasons you wouldn't write the above function as use std::fmt::Display; ```fn do_something(f: &amp;Fn(&amp;Display), a: &amp;Display, b: &amp;Display) { f(&amp;a); f(&amp;b); } do_something(&amp;|d|println!("{}", d), &amp;5, &amp;"hello");``` are the same reasons you wouldn't want to have your closure only work with trait Objects. I'm no rust expert, but performance and preservation of the original type (Sizedness), are two examples I can think of. This is impossible to call with a closure: https://play.rust-lang.org/?gist=59390aa939d65a11e8e591630dd7f67e&amp;version=nightly ```fn do_something&lt;F, A, B&gt;(f: F, a: A, b: B) -&gt; (A, B) where F: Fn(A, A) -&gt; A + Fn(A, B) -&gt; B, A: Copy { (&lt;F as Fn&lt;(A,A)&gt;&gt;::call(&amp;f, (a,a)), &lt;F as Fn&lt;(A,B)&gt;&gt;::call(&amp;f, (a, b))) }```
I use Rust for embedded work at my job. One of the things that prevent me from recommending Rust to other people doing the same is the lack of const generics. The [RFC](https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md) merged, but I have no idea how long it'll be until we see it `rustc`.
&gt; because programs in C can use asm on a stable compiler No, they can use asm in some C compilers that offer inline assembly as an extension (not even in all of them, since syntax and behavior varies from one compiler vendor to another).
&gt; but I cannot understand why it will be allowed as a parameter type. Because rust is a consistent language, and it makes no sense to allow `impl trait` in some cases but not in others. I like to be able to use impl trait to document code: let foo: impl Iterator&lt;Item = u8&gt; = some.iter().chain().with().an().impossible(|v| v).type();
Thanks for the clippy shout-out! I almost feel like a big deal now üòé ‚Äì that said, clippy only moved to the nursery a few weeks ago (it started as /u/Manishearth's personal project and I joined shortly after, and we've been at it for about two years), and we're in the process of 'riding the trains', so next beta should include a clippy component for rustup, until we finally land in stable.
Solid writeup, many of your points were ones that I had not seen in other rust2018 posts. Definitely think those are interesting points to consider.
Yeah, this mentioned in the article - adding packages from crates.io should be part of Cargo out of the box, and not something requires several steps to install.
You might try putting the data inside `Pair` into its own struct and implement `Drop` on that instead.
That's amazing, I'm excited for how clippy is going to evolve! :D
Aplogizes for pointing the author. I didn't realize that it was someone else. Anyway, I wrote there that I'm against this feature and I pointed out the reason (added complexity and hard to guess behaviour in some cases), but still my post has been hidden. Other person wrote even longer post with more thoughts and some criticism and it got hidden too. So sorry, I just quit this discussion and pray to all gods for this rfc not being merged :)
Okay, this minimizes my question even further: struct Pair&lt;T1, T2&gt;(T1, T2); impl&lt;T1, T2&gt; Drop for Pair&lt;T1, T2&gt; { fn drop(&amp;mut self) {} } fn deconstruct&lt;T1, T2&gt;(p: Pair&lt;T1, T2&gt;) -&gt; (T1, T2) { let Pair(x, y) = p; // E0509: cannot move out of type which implements the Drop trait (x, y) // don't want p.drop() call, stuff is already moved out of p } But I still don't understand how to get rid of the error.
Have you ever worked on a project for 5+ years? It's nice to change things up. Has Apple ever offered you unimaginable sums of money? It's hard to imagine refusing the kind of offer Apple would make to someone like Graydon. No, Graydon does not think Rust sucks and Swift is so much better. [He's not the only one from the early days working for Apple these days.](https://news.ycombinator.com/item?id=13533701)
My unscientific opinion is that he probably saw how much the Rust community had matured and didn't think he was needed anymore. And working on Swift == $$$$ (probably)
It's a bit of a pain to use them, though; at least when you're working on a MCU and want to perform plenty of combined reads/writes. `volatile_write(PORTA, volatile_read(PORTB) | volatile_read(PORTC))` just isn't as nice as e.g. `PORTA = PORTB | PORTC` in C.
Thank you dtolnay
I guess you can write an API that hides it?
We got Gankro back so that's a thing at least :).
How would placement new help? I've been trying to figure out if it's just an optimization or something more.
Plus, I could see how it might be attractive to not work in a project where you have to wear the responsibility of being the big inventor/BDFL/oracle type, after so many years.
This is great! There also a .NET implementation (Minisign-Net) and I just started a Go library (jedisct1/go-minisign). The Minisign format is pretty simple, and if your Ed25519 implementation is sane, your Minisign implementation is likely to be secure.
&gt; Yes you need to do the unpleasant break up into submatches for the nesting. Ugh, ok. Thanks for the help! 
Here is the discussion I started on the forum for the competition: https://www.kaggle.com/c/santa-gift-matching/discussion/47460 One response at the time of this posting from the winner. 
Graydon left the project far before the Rust community could be called mature by any measure.
Sorry, I didn't initially understand your problem. I think in this case, you want a struct that wraps `Option`, as that's typically the best way to move out of a type that has a `Drop` impl because you can still `.take()` the `Option`: struct MaybePair&lt;T1, T2&gt; { pair: Option&lt;(T1, T2)&gt;, }
This is a crate that I worked up today. I have been dissatisfied with the rate that discussions around `asm!` have been proceeding, so I decided to see what could be done with current stable Rust. This is a prototype, and I don't recommend using it in production, as reflected in the version number. But, it is published on crates.io as `libasm`, if anyone wants to experiment with it. I don't know enough about assembly programming in a userspace environment to go much further than this, but I'm happy to work with anyone who does to improve the crate.
Thanks so much for doing this! And of course, thanks to the whole community for creating this problem in the first place :D
BTW, you may want to use https://twitter.com/search?f=tweets&amp;vertical=default&amp;q=rust2018&amp;src=typd as an additional source of data. There are some posts there that haven't made your list yet.
I really like this write-up. I've definitely started to get a sense that some parts of Rust (including big-name community projects) hit a bit of momentum loss in 2017. I could be completely wrong, but from my perspective, I felt like a lot of things hit big milestones and then development kind of dropped off once they hit a done-ish state. I can definitely support a proposal to jump back on backlog and maintenance, and finishing up things that have been stagnating. 
Was not aware.
Yup. And that's pretty similar to the horror I have in my code wrapped with a macro right now.
To my knowledge, that definitely has to do with. Graydon is around and can be seen on this subreddit from time to time, but usually doesn't comment on language details if they are not historic.
Is there a way to create a pseudo form of Abstract Base Classes that OOP languages use? For example, right now I'm working on a toy program that watches classifieds and alerts me when something I'm interested in pops up. My intent was to have something along the lines of: let mut sources = vec![CraigslistSource::new("motorcycle"), DropzoneSource("parachute"), ...] // ... fn run(sources: Vec&lt;Source&gt;){ for src in sources{ let matches = src.find_matches(); ... } } I implemented a "Source" struct that works for a single site. All the code is generic (e.g. fetching the page, parsing the DOM, etc) except a single method responsible for parsing HTML for that particular site. I'm trying to refactor all the generic code so that I can define new sources by implementing just a single ::find_matches() method. I tried doing it by defining a trait: pub trait WatcherSource { fn find_matches(&amp;self); } This allows me to treat each different type of Source in a generic manner, but I'm still left with having to copy and paste 90% of my code over (the generic parts) each time I want to create a new type of Source. I tried putting the generic code under the trait definition to get around this, e.g.: pub trait WatcherSource { fn find_matches(&amp;self); fn fetch_page(&amp;self){ // Default implementation } fn parse_dom(&amp;self){ // Default implementation } } but that didn't work. Namely because I can't seem to use "self" in the default implementations (e.g. "reqwest::get(self.url)"). Even if I did I would still need to copy/paste code (such as the struct definitions), and the internal methods now public so it doesn't seem ideal. Is there a better way to do this in Rust?
I think what's missing is an actual, hardware environment for continuous integration of a ```no_std``` MCU application. Grab a development board, a STM32F4Discovery or something similar, attach a debug interface, serial console listener and some sensors and stick it to the end of a CI pipeline. Establish a minimal set of functionality for each milestone and treat it as a first-rate citizen. Potential milestones: - Serial console - Memory allocator - OS with tasks - HAL for various MCU peripherals - Ethernet or WIFI - ```std``` environment as a library I'm emulating this by using [QEMU for STM32](https://github.com/hashmismatch/freertos.rs/tree/master/qemu_stm32_tests/src) for my FreeRTOS Rust shim. I've just spent an entire day bringing it up to speed for latest nightly, and it's really not fun [cobbling together](https://github.com/hashmismatch/freertos.rs/pull/9/commits) the body of knowledge to embed a nightly Rust library into a C firmware image.
Shouldn't marketing to different audiences use different slogans anyway?
Nightly feature are not unused inventory in the same sense because they can _already_ be consumed. This is different from features that are completely in house and no user can touch or experiment with. For that reason I don't buy the unused inventory analogy at all.
Not sure what your trying to do. You cannot get 2 mutable borrows at the same time (`&amp;mut page` and `table[...]=` both mutably borrow table). Don't you want a mutable page then modify it directly (no return)?
Great hack! Do you think it will work for cross-compiling to microcontrollers?
Cool. I was more wondering about non-x86 arches, since I saw e.g. [hardcoded register names](https://github.com/coder543/libasm/blob/master/src/generate.rs#L13) which probably wouldn't be right for ARM or something.
yeah, that's something which really could be deleted. Initially, I was going to have the `libasm` functions take a set of registers and return a particular register, and then `libasm` would handle the monotonous task of `mov`ing input values from the ABI into those registers, and then handle `mov`ing the value from the output register to wherever the ABI is expecting the value to go for a return value. The hardcoded `rax` register name doesn't actually affect anything, except for that one conditional check.
I just remembered there was an interesting talk about inline assembly in Rust: https://youtu.be/7Mzl2HA3TuU
The hard-coded `rax` instruction has now been deleted.
Yay, excited about that one!
I tried to install and use `xargo` on [an example project](http://blog.japaric.io/quickstart/) using Linux, but I couldn't even get things to compile following the directions, so I didn't attempt to add `libasm` into the mix. I am curious to see what hiccups might happen so I can fix them.
This is for encoding data that is *almost always* utf8 text, called STFU8 text. I'm going to build on this crate for a new crate that has serializable `Path` and `PathBuf` objects that can even be editable on the frontend. Stay tuned!
Woah, this is cool! It kinda reminds me of how serde used to work before custom derive was stabilized - use a build script to process the file ahead of time into whatever the rest of the program expects. Hopefully this can catch on and get some traction. 
future based system also have the thread-local access issue. even it runs on a single thread. https://particular.net/blog/the-dangers-of-threadlocal 
I'm really excited to see this coming along as I've been following the discussions on i.r-l.o as I do embedded dev and will likely need this functionality before too long. One thing I noticed is that it uses a unique way of specifying the target triple. I would recommend reusing the standard `#[cfg(...)]` format if possible, as then there's no extra learning on the user it works like most Rust code does.
it's possible! the format being used now is actually the standard toolchain target triple.
Ooh this is useful to know :)
The reason floats don't implement `Ord` and `Eq` in Rust is that they are inexact. Have a look at [this example](https://play.rust-lang.org/?gist=c1c7007fa4cef98116605a4d4ac3225d&amp;version=stable). It means that if you were to say use the value `0.1 + 0.2` as a key to insert something into a map, you wouldn't then be able to retrieve said thing by using `0.3` as a key, because those two values are ever-so-slightly different due to floating point inaccuracy. Here is a good explanation of [how floats work](http://fabiensanglard.net/floating_point_visually_explained/) and why these limits exist.
I believe the issue is that you're using a 0.3 `log` crate with an 0.4 `log` logger: https://github.com/rust-lang-nursery/log/blob/0.3.x/src/lib.rs#L899 The issue should be fixed if you make sure to use the same version throughout the whole executable. [cargo outdated](https://github.com/kbknapp/cargo-outdated) can help with this.
&gt; Out of curiosity, what are you doing that you want to be able to Eq and Ord for floats I am building order matching lib. The orders have to be sorted based on price then time of entry. I could probably use some fixed point arithmetic but decimals are more natural. And for matching prices I have to do equal. I was aware of issues with float, but I am hoping that `decimal` will be better for a reasonable precision. 
Exactly, the compiler could infer the types from the strict field. I would say both are idiomatic, though I'd prefer the former if the type was sufficiently clear from context.
You're probably better off using [BigInt](https://crates.io/crates/num-bigint) for currency
It's actually the opposite problem: `capture()` *doesn't* take ownership. It returns an array of subslices pointing into the `&amp;str` argument. The issue, then, is that the `line` is dropped at the end of each iteration, when you still have references into it. I'd tell you how to fix this but I haven't figured it out :P
&gt; In one case I wanted to use Vec::resize_default(), which has been waiting for stabilization for about 8 months now without any signs of progress. Library stabilization is to a large extent push-driven. If you think an unstable API is useful and would like it stabilized, please comment on the tracking issue!
Maybe check /r/playrust ,because this reddit is about the programming language Rust
Oh sorry, thanks
I don't think `BigInt` is right. With `BigInt` I will need some king of fixed point arithmetic.
&gt; Make cargo more extensible Everything in this section sounds like Neon just needs a cargo subcommand. See [cargo-web](https://github.com/koute/cargo-web) as an example. You could make it so new users simply do `cargo install cargo-neon` once, and then they can do `cargo neon` commands like `cargo neon new my-first-neon-project` or `cargo neon run`. Another option would just be make `neon` a top-level command. `cargo install neon-cli` and then the user would just say `neon new my-first-neon-project`, etc.
Like it or not, using a big int library is the proper way to implement programs that deal with currencies. If you aren't dealing with fractional amount you could maybe get away with just using a big int to represent the amount in the smallest currency denomination (pence/cents/...)
They _can_ be used, but as a casual Rust user, I feel like they are generally more difficult to use, not only because they are only available on nightly, but also because they are not as well documented, and are potentially buggy or come with some limitations. As a result, I don't use them even if I'm aware of some of them, so I do buy the unused inventory analogy.
I think the unused inventory comparison is flawed. &gt; If you think about that in terms of unused inventory: how much time has the community spent on designing and implementing these features? A _lot_ of these are features that exist only for the compiler, and many more are pre-1.0 APIs that just existed. Neither are things folks have spent much time on. Thinking of actually-RFCd features that are stalled, only one comes to mind: placement new.
I tried using cargo outdated, but it didn't find anything: $ cargo outdated All dependencies are up to date, yay!
Aren't "unprintable ASCII" codepoints (less than 0x20) still valid UTF-8 characters (unspecified CONTROL characters)? An unmodified UTF-8 encoding is already able to represent numbers from 0x0000 to 0x10FFFF (though surrogates are not well-formed), and it's possible to go further if you use leading bytes higher than 0b11110xxx. I guess this is about a human-readable form of UTF-8 plus bytes 0b11111000+? I don't quite get what the purpose of STFU-8 is. (But then again I'm not quite a Linux person, either.)
https://feedback.unity3d.com/ Website where people can vote for ideas for rust projects/libraries. It would be great to have one place for sharing ideas or implementing them! Actually started a project like this but it is not finished nor published yet...
I'm a Linux person and I don't get it either. What is this crate for, exactly? What does it provide that standard UTF representations don't?
It sounds like it's for embedding binary data in your text. If you mostly have text but want to throw just a few bytes of data in there then you might prefer to use this over base64 or whatever. The bytestring "Here's some data: `X`" where `X` is a byte with hexadecimal value 0x80 is not valid UTF-8. So to encode it in a text stream you can use base64 and get "SGVyZSdzIHNvbWUgZGF0YToggA==" or use STFU-8 and get "Here's some data: \x80"
Sorry. Implying that is a shit move. There‚Äôs plenty of opposition in the thread, even from project members and it is a pre-RFC. Also, if you followed the module RFC last year, these discussions do lead to major changes regularly. Implying that we remove criticism and only allow supportive comments undermines the very reason _why_ we have adopted this process.
Yeah, no, that's not what happened. Exactly two posts were removed, and that was because they weren't saying anything new and were mostly "no no no" with a bit of extremely basic reasoning that had been brought up before multiple times. They're not being removed due to being criticism, they're being removed for not bringing anything new to the table and only adding to the noise. That thread is like 90% criticism, to say criticism is being suppressed is a bit disingenuous. 
I'm not judging RFC process, only this one particular pre-RFC where I have seen that my and at least one other person's criticism with arguments has been removed.
But you have to use something like virtual methods to implement ad-hoc polymorphism supporting such cases. In this case I would prefer to implement sort of special object of trait supporting virtual methods instead of all traits being virtual (which hurts performance significantly, just look at c++ std::lib that ignores virtual methods completely).
I ran into a case where `try!(...)` worked, but `...?` failed type inference. Is this categorically a bug which I should try to make a MCVE for, or is there something more subtle going on? Commit: https://github.com/roblabla/cargo-travis/pull/24/commits/2d95ec220eab05b38b0b23c82918767fb32e85b3 Broken build before with `?`: https://travis-ci.org/roblabla/cargo-travis/builds/328885450 Fixed build after with `try!`: https://travis-ci.org/roblabla/cargo-travis/builds/328888548
Please also include a section how this is different to WTF-8 (what OsString uses)
&gt; I could be completely wrong, but from my perspective, I felt like a lot of things hit big milestones and then development kind of dropped off once they hit a done-ish state. Getting something to a somehow working state often only looks like most of the work is already done. 
This is fascinating. You're also tackling the issue from the opposite of what I've been doing lately. I've found myself straight translating a c++ project into rust with almost every single function that's not pure math doing something unsafe.
I don't see how your second point follows from the first. Yes, nightlies are usable. On the other hand, in the 2017 Rust community survey about 80% of Rust users were using some form of stable Rust. As such, I still argue that only stable Rust is viable way to realize the value for Rust features.
I guess it depends on what kind of embedded work you do, but in a lot of cases it is the stability of the compiler, not the language that is important. In some cases the compiler does not even support the whole language, but the vendor leaves guarantees that the compiler will produce correct output for what it does support. Picking a random nightly version would be a hard sell in quite a lot of organisations.
It's an analogy, it was never going to be perfect. Your comment responds only to design and implementation cost -- they might not even be the largest cost of unused inventory. I'm also not sure what "these" features you are referring to or what your definition of "stalled" is. Are you really saying that all of these 268 in-flight unstable things were implemented in the pre-1.0 days, and that they're progressing on schedule? From my perspective, just the `Vec::resize_default()` seems like a counterexample to that.
any particular reason you're using so much unsafe? Just the nature of the c++ code or what?
I'm struggling to wrap my head around generics. I have `Request&lt;K, V&gt;` struct and these `K` and `V` are stored in struct fields. At the end that `Request&lt;K, V&gt;` should be serialized as a stream of bytes. Here it is: https://github.com/svartalf/rust-memcache-proto/blob/0.2/src/request/mod.rs#L27 I want to add an optional support for serde over K and V types like that: use serde::{Serializer, Serialize}; impl&lt;K, V&gt; for Request&lt;K, V&gt; where K: Serialize, V: Serialize { fn to_writer(&amp;self, writer: &amp;mut [u8]) -&gt; io::Result&lt;()&gt; { writer.put_u8(self.magic)?; writer.put_u8(self.opcode)?; // `self.key` here have the `K` type writer.write(self.key.as_bytes()); writer.write(self.value.as_bytes()); Ok(()) } } My problem here is that `serde` itself does not provide any methods for serialization into bytes (or into writer) and all implementations (ex. serde_json / serde_cbor / etc) have their own (very different) methods for that. I was thinking about some kind of visitor pattern maybe, so library users will be forced to do smth like that: impl&lt;K, V&gt; for Request&lt;K, V&gt; where K: Serialize, V: Serialize { fn to_writer(&amp;self, writer: &amp;mut [u8], visitor: F) -&gt; io::Result&lt;()&gt; where F: Fn(&amp;Serialize, &amp;mut [u8]) -&gt; io::Result&lt;()&gt; { unimplemented!() } } let request = Request::new(..); // I'm gonna skip struct init let mut result: Vec&lt;u8&gt; = vec![]; request.to_writer(&amp;mut result, |field, writer| { serde_json::to_writer(field, writer)?; }) Any thoughts about it? Also I hope that borrow checker will allow me to do that.
&gt; Are you really saying that all of these 268 in-flight unstable things were implemented in the pre-1.0 days, and that they're progressing on schedule? No. I'm saying most are not RFCd features. This can mean: - features added to support users who are already using nightly for other reasons (there are a whole bunch of features for OS stuff, like the abi features and the naked_function one, among other) - features added pre-1.0 that we never made a decision on - features that exist for the compiler itself (`rustc_on_unimplemented`,`lang_item`, etc) - APIs that were added without an RFC. `vec_resize_default` is an example of this. Same situation as pre-1.0 stuff, really -- minor thought put into the API, no major discussion, not much "community time spent" on the whole thing. Then, there are also a lot of: - Partially stabilized features where we're still waiting for the dust to settle on the stuff that was already stabilized before making decisions on moving forward (e.g. `untagged_unions`, `universal_impl_trait`) - In-flight features that are actively being worked on I count features as "stalled" when there _was_ an RFC (i.e. the feature is on track for stabilization) and it's not moving forward. &gt; From my perspective, just the Vec::resize_default() seems like a counterexample to that. `resize_default` hasn't had an RFC, it's unreasonable to expect any progress to happen on that front.
&gt; Picking a random nightly version would be a hard sell in quite a lot of organisations. A lot of organizations follow clang-trunk or gcc-trunk closely. You can decide to manually switch to a particular nightly every 6 weeks, or every 2 weeks, or once per year... With nightly it's up to you. In particular, you want nightly not to compile stable Rust, but to compile nightly Rust. Even if we released every six weeks you can't rely on your code compiling with a new "officially-released-nightly" because chances are that in those 6 weeks many people have tuned, removed, reworked, ... a nightly feature you are using in a way that it breaks your code. For example, inline assembly is in the process of being completely reworked, the old syntax might not even parse with a nightly in 6 weeks...
&gt; This library could probably be used to provide SIMD functionality on stable Rust today, Many SIMD functionality relies on compiler intrinsics actually being inlined, also, LLVM cannot reason about inline assembly, so it wouldn't be able to optimize the SIMD code you generate with it, which LLVM is great at btw.
You want to post this on /r/playrust, although honestly I don't think they'll appreciate this post there either.
Yeah, mostly. Converting between languages is always a chore to begin with, and also the project is over 2 years old and ongoing even now. My strategy is to work my way up to the present by picking out good "checkpoints" in the timeline to convert over, test that things are working, and then jump to the next checkpoint down the timestream. This lets me skip past some of the churn as new features settle in, but it still means that as soon as I'm done "here" I just throw out some of the code as things update into the next iteration. It's very much not the normal rust "build for stability", it's a lot more "build quick so that some day I might even catch up". The "real' biggest reason for unsafe is that if I use &amp;mut then `rustc` will tell LLVM to optimize as if there is exclusive access all over the place, which will soon enough be not necessarily true and I'd rather not lie to LLVM about that one. I could try to figure out when that is true or not and use &amp;mut over *mut some of the time, but being inconsistent about it is actually worse. The other reason is that rust is currently very bad at the "container problem" for lifetimes: rust usually can't tell when a sub-reference should lock the whole outer struct or not, so a lot of code that should be valid isn't under the lifetime system unless you jump though hoops. Hoops that I don't care to bother with because when I hit bugs I don't really hit memory bugs in the first place. Some day I will collect these and other thoughts about the experience into a big blog post.
&gt; Like, integers never live on even addresses (since there‚Äôs actually no legal way to put them there in the first place, so it makes no sense to prescribe what happens if one gets there nevertheless). Do you mean odd addresses?
Along with 242 others http://www.openculture.com/2017/01/download-243-free-ebooks-on-design-data-software.html
&gt;Unspecified ‚Äí it still acts somehow sane. int x = 0; printf("%d\n", ++x + ++x); might turn out to be either 3 or 4, because the compiler is free to either increment x twice and than take its value, or increment once, take the first value, then increment second time and take the value again. Isn't this still UB? There are two unsequenced side effects to the same variable. Annex J2 of the C11 standard seems to list it under undefined behaviours: &gt;A side effect on a scalar object is unsequenced relative to either a different side effect on the same scalar object or a value computation using the value of the same scalar. Maybe the author was thinking about the order in which functions arguments are evaluated, which is unspecified. So `printf("%d %d\n", ++x, ++x)` instead of `printf("%d\n", ++x + ++x)`. Am I missing something? 
In many projects the tooling is never updated, it would add more work and risk to the project without adding much value. Suggesting that the project starts off with a random nightly version would be a really tough sell in those cases. Like I wrote, it depends on what kind of embedded work you do, it is a really wide field.
Heterogeneous lists require an extra level of indirection, such as Vec&lt;Box&lt;Observable&gt;&gt; This is because `Vec` requires all elements to have the same size, but different `Observable`s might have different sizes. It will also need to store the run-time type information somewhere, which `Box` takes care of for you. Unfortunately for your particular case, associated types don't work with this. The type `list[0]::Element` cannot be known at compile-time, for obvious reasons. You can do dynamic dispatch into a function inside the trait, such as `list[0].some_func()`, but you cannot refer to its associated type from the outside in a meaningful way. 
No, the limited precision is not the reason `Ord` and `Eq` are not implemented. Those traits require a few properties that floats don't share: * Reflexivity: a == a for all a * Totality: (a &lt; b || a &gt; b || a == b) == true both are violated by `NaN` which isn't greater, less or equal to anything including itself.
The problem is that different `Observable`s may have different `Element` types: if you were able to put any `Observable` in a trait object, what type would `Element` have for the trait object? The solution depends on your exact requirements: do you need to work with `Element`s of the observables once they are in the `Vec`? If so, then you need to define a second trait (`Element`) which encapsulates the operations that are possible on an `Element` associated type, and then make an `ObservableBase` trait that has no associated type: each method that deals with elements should take or return an `Element` trait object. Now you can store a `Vec&lt;ObservableBase&gt;`. If not, then you still need to add an `ObservableBase` trait that has no associated type, but this time you should just omit any methods that require knowledge of the `Element` type, and leave those methods on the `Observable` trait. 
Stab in the dark: I believe `?` now also works on `Option`, whilst `try!` only ever results in `Result`. Might be the compiler can't tell what you want because of the closure boundary.
Good ~bot~ turtle.
Does [this playground](https://play.rust-lang.org/?gist=8b95d82b12147ddf9f80864bc5eea3b3&amp;version=stable) reflects the problem? 
neat!
&gt; The absolutely worst thing about undefined behavior is that it is allowed to pretent to work as expected. This is sooo correct. I hate bugs that are essentially invisible because they can't be reproduced in a development environment. But you ship the code and 1% of users have it crash their system about once per month.
Of course odd, my mistake :-). Will fix that. You know, this is how all the nasty bugs come to be.
[`SliceDeque`](https://github.com/gnzlbg/slice_deque) is a `VecDeque` implementation that derefs into a slice (basically a `Vec` that also has amortized constant time `push_front` and constant time `pop_front`) without using any additional memory. It implements ~100% of the `Vec`, `VecDeque`, and slice APIs (the only API missing are `from_raw_parts` and `into_boxed_slice`). Over the weekend I finished Windows, support, meaning that it currently supports all tier 1 platforms (Windows, Linux and MacOSX). This week I plan to finish setting up CI and ironing bugs to do a 0.1 release.
&gt; In many projects the tooling is never updated, it would add more work and risk to the project without adding much value Nobody forces anybody to upgrade. &gt; Suggesting that the project starts off with a random nightly version would be a really tough sell in those cases. Well they are all "equally good". Naming one random nightly "official" is only going to create confusion, because if you find a bug the answer will always be "try with the next nightly then". 
Why? The documents has arguments, you didn't bring any.
Most of these are one-off features without much architectural impact on the compiler. I highly doubt this has a significant cost. We could remove some of the APIs, but most of the compiler features seem useful (e.g. all the ones needed for doing OS impls; those are actually used)
I didn't get much done on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) last week. I'm still dealing with getting tests to pass for non-floating point storage types. Currently running into issues because conversion factors are originally represented as floating point numbers and may lose precision. Likely going to put that on hold this week and try to knock out some easy issues.
I did - it feels unnecessary to me. It adds complexity (most of the time more features = more complexity) and adds only very little readability comparing to existing solution. It also makes providing types on fn invocation harder or impossible - where will br `impl` params placed? As first fn tyle parameter or last or? E.g. fn&lt;T&gt; foo(t: T, bar: impl Bar) Should I call IT foo::&lt;u32, MyBar&gt;(...) Or foo::&lt;MyBar, u32&gt;(...) Solving this problem leads to adding exceptions like "providing types arguments is not allowed when impl param od used" or "mixing templates and impl params os not allowed" or some other rules that's make rust harder to learn and master.
I was *really confused* by the documentation for this crate. The stuff about ANSI escape sequences is weird, for example. [Technically, an ANSI escape code can be invalid UTF-8](https://en.wikipedia.org/wiki/ANSI_escape_code#Escape_sequences), but in practice, all actual uses of ANSI escape codes stick to ASCII, and are therefore valid UTF-8. I don't just mean to be a pedant here, but seeing that really threw me off. And then there are things like "Windows paths are not necessarily UTF8 compliant," which is also really weird. Windows paths are UTF-16. The Rust standard library---as an implementation detail---does a UTF-8-like transcoding of said paths internally (using WTF-8). From looking more carefully at the documentation in the crate itself, it looks like this provides a convenience wrapper around [`std::ascii::escape_default`](https://doc.rust-lang.org/std/ascii/fn.escape_default.html) (or something similar) and then also provides a routine to reverse the operation, which I believe is not in std. The actual implementation doesn't appear to use `escape_default` though. This is actually useful stuff. I use helper functions (usually around `std::ascii::escape_default`) all the time, particularly when trying to visualize a `&amp;[u8]`. I guess I think of this crate as converting binary data to its escaped form represented by a `String`. I don't even know if I'd mention UTF-8 at all.
create an enum for each possible associated type and fill your vector with enum instances
For me, the pain of Rust's minimal IDE support is cushioned by the fact that the main alternative to Rust for my purposes is C++, which has comparably minimal support! 
I don't think you understand my point, my point is that we're not putting that much _new_ work into these nightly only things. The blog post makes it sound like we're putting a bunch of effort into implementing and maintaining these, I just disagree. I generally keep an eye on what's new in the compiler and I don't really think it's true that much effort goes into perma-nightly stuff. The blog post just cites the number of unstable features to back it up, but again, a lot of those are perma-nightly for legit reasons (e.g. compiler support) or for "was there since 1.0" reasons. It also cites Rocket which is weird because the stuff Rocket uses _isn't_ perma-nightly, it's just that proc-macro is something that's going to take a while to happen. But that's being actively worked on! &gt; it's just a big dumping ground for 'whatever we want' and 'people who need other features we can't stabilize yet', I'm pretty sure it stopped being that years ago; there's a pretty high bar for adding features. Stuff like reserve_default seems to fall through the cracks, but usually you can't just go and add an API or feature without there being a decent reason. &gt; If that's important, those things should be important to stabilize. Everything is important to stabilize; importance is relative. We don't currently prioritize OS impl stuff because there are more pressing things, however this doesn't mean we shouldn't help those pioneering Rust in OS. Making Rust work for a new use case is always a chicken and egg problem: you need to know what people _need_, but folks won't know exactly what they need until they start building and maintaining things, but they can't if the language doesn't have the features. Helping OS folks experiment is still important for Rust. &gt; Actively supporting people who need features outside of stable means that you are actively diverting resources to that effort. For a pretty liberal definition of "actively supporting". These are generally pretty self-contained and rarely take any extra maintenance efforts. Nor are folks obligated to keep these working; if the compiler breaks these it's more likely that the folks using these features will be the ones who must fix it (and it will get removed if folks aren't using it).
I wonder if valuable metrics that would help inform this discussion are: 1. Number of nightly-only features over time 2. Age of nightly-only features FWIW, I‚Äôm sympathetic to the OP‚Äôs point: given that community surveys indicate that a huge fraction uses the stable compiler, nightly-only features are inventory. That said, if that inventory is *turning over* and most of the features are in for 12 weeks or less before making it to stable (as Manishearth has indicated) then it‚Äôs not an issue.
'cargo install' is not really the equivalent of 'npm install'. It's specifically for installing command-line tools from crates - and the docs are pretty clear about that: ``` $ cargo install --help Install a Rust binary [...] This command manages Cargo's local set of installed binary crates. Only packages which have [[bin]] targets can be installed [...] As a special convenience, omitting the &lt;crate&gt; specification entirely will install the crate in the current directory. That is, `install` is equivalent to the more explicit `install --path .`. ``` I'm not sure what was going on in your project, with the downloading but then needing the internet again. But i wouldn't have expected 'cargo install' to do what you needed anyway. I'm all in favour of making the tools easy to use, but if you come from another language, and don't bother to read the docs for the new language's tools, that's on you! 
&gt; I have seen that my Ah, I see.
You should look into scoped thread-pools.
&gt; I'm all in favour of making the tools easy to use, but if you come from another language, and don't bother to read the docs for the new language's tools, that's on you! That sounds like a friendly way of saying [RTFM](https://en.wikipedia.org/wiki/RTFM). I don't think that's a particularly useful stance to have, as it shuts down any conversation as to _why_ something may be unexpected. If you want to create a more friendly UX, then blaming the user for getting something wrong is not the way to go. Instead, perhaps we can look at what happened here. Apparently `cargo install` doesn't work the same way as in other package managers (e.g. `npm`, `pacman`). Why is that? - It only installs things globally. Okay, that's unexpected. It doesn't make this explicit during installation. You kind of have to know before you install it. - Apparently the `install` and `build` phases are bundled into the same command in Cargo. This is also quite different from other package managers (the "install" phase is usually separate from the "interaction" phase). If you expect them to be separate steps, then Cargo's setup can be quite confusing. Is this the right separation to have? If it is, how can this be made less surprising? It might be easy to look at a situation and say: "you're wrong". It might be more useful to instead think about what led to the "wrong" behavior instead, and find ways to do better.
**RTFM** RTFM is an initialism for the expression "read the fucking manual". In expurgated texts, substitutions such as "read the frickin' manual", "read the flaming manual", "read the fine manual", "read the friendly manual", or similar variants are used. Initialisms similar to RTFM include "STFW" ("search the fucking web"), "GIYF" ("Google is your friend") and "LMGTFY" ("let me Google that for you"). These indicate that the questioner could have easily found the answer to their question on the World Wide Web. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I‚Äôm really sorry for the delayed reply. In Thrift you have the concept of a channel (socket, pipe), transport (buffered, framed) and protocol (multi, binary, compact). Often the user has to choose - at runtime - a combination of those components with which to communicate with remote endpoints. The ideal approach is to get these three component choices up front and construct a Thrift client that instantiates them in a single expression (thus preserving the types and allowing use of static dispatch). This gets annoying because the space of possible enumerations is i x j x k. You‚Äôd have to have a match arm for every possible combination. Alternatively, you simply instantiate a boxed type for each component (Box&lt;IOChannel&gt;, Box&lt;Protocol&gt;, ...) and assemble those boxed types into a Thrift client. Of course, IIRC when you do that you lose the original component‚Äôs type information, and can‚Äôt upcast later. It‚Äôs a little less straightforward than I‚Äôve described because you can‚Äôt access a channel directly in a Thrift client: it‚Äôs wrapped by a protocol. Anyways - why does the concrete type matter? In this specific case: testing. Imagine that I‚Äôm trying to test out a binary protocol. Ideally I‚Äôd pass in an InMemoryChannel, invoke Protocol operations on it and check its internal buffer to ensure that the bytes look correct. To do this I‚Äôd naively upcast the IOChannel into the specific type I wanted and invoke ‚Äúget_buffer‚Äù to introspect the in-memory buffer. I can‚Äôt do that. Instead, I had to wrap my buffer in an Rc, share access to it and introspect the data that way. It was surprising and frustrating. Hopefully that all made sense?
that's an interesting statement. I believe proper SIMD intrinsics would be better than raw SIMD ops, but raw SIMD is better than code which LLVM is failing to generate SIMD for automatically. If and when stdsimd reaches stable, that will be a good day.
Finally published 0.1.0 of [```packed_struct```](https://crates.io/crates/packed_struct), my packing library for byte structures. Works on stable rust and generates documentation, debug output and more. I'm planning on writing a visual blog post to illustrate the points. The first consumer of it is a little helper for the [Multwii Serial Protocol](https://crates.io/crates/multiwii_serial_protocol), a protocol for popular acrobatic quadcopter flight controllers like Cleanflight.
&gt; I guess JavaScript and Ruby share this idea of "easy", where every task should require a single command or method call. But isn't one-line installs the case for all package managers? Linux subscribes to this idea too: ```sh $ apt-get install tmux $ pacman -S git ``` Even Rust does this sometimes: ```sh $ cargo install clippy ``` Is this not the same? Or is it different because the version is saved to a file? The main reason why i want this is because it's faster. Finding the right package versions takes a lot of time, and typing things is prone to errors. Workflow matters a lot for people, and being able to reduce friction between "I'd like to install package X" and "package X is installed" is important UX. Also: this wouldn't require you to change your own workflow - it would simply improve other people's workflow. Or are you saying people are wrong for wanting to work a particular way?
This is exactly what I've been looking for! Benchmarking stuff is kind of difficult when you don't actually care all that much. Just being able to run a command is awesome!
what about map_produce?
&gt; I did - it feels unnecessary to me. I wasn't aware of any previous discussion, so I was referring to this thread only. Thank you for clarifying! &gt; Solving this problem leads to adding exceptions like "providing types arguments is not allowed when impl param od used" or "mixing templates and impl params os not allowed" or some other rules that's make rust harder to learn and master. Not necessarily, I think it is quite natural to assume it's in the order of first occurrence. In your example, that would be `foo::&lt;u32, MyBar&gt;`.
FYI, Itertools has a [`map_results`](https://docs.rs/itertools/0.7.6/itertools/trait.Itertools.html#method.map_results) method that seems similar to your `map_ok` and its name was discussed [here](https://github.com/bluss/rust-itertools/issues/162). If you don't want to use `flat_map_ok` because it sounds like it returns an `impl IntoIterator`, maybe go with something akin to [`filter_map`](https://doc.rust-lang.org/1.23.0/std/iter/trait.Iterator.html#method.filter_map)?
Nice blog. On the bit about upcasting trait objects: I personally hope we never see this, and we'll find a better common solution. One negative thing about working in UE4 is the fact that you're expected to upcast in any function that takes an Actor or Component which makes refactoring expensive and dangerous.
&gt; Not necessarily, I think it is quite natural to assume it's in the order of first occurrence This way cases like this: fn foo&lt;T, S&gt;(t: T, bar: impl Bar, s: S) will be very confusing, because I write one thing (`T,S`), and the result/use is different (`T,MyBar,S`). As far as I understand, the current solution is to deny providing types when there are `impl` params. Any solution causes 'branches' in the learning process and in the usage
Even more confusing would be fn foo&lt;S, T&gt;(t: T, bar: impl Bar, s: S) but that is not idiomatic. I'm not sure what the correct order should be in this case.
In Advent of Code there are no bad inputs, so I always used `flat_map(|id|id)` everywhere. `Result&lt;T,E&gt;` implements `IntoIterator&lt;T&gt;`, so flap mapping the identity function just unwraps all those lines.
You can add `&amp;download=true` to avoid giving away a name and email address: http://www.oreilly.com/programming/free/why-rust.csp?intcmp=il-prog-free-product-lgen_why_rust&amp;download=true
&gt; all actual uses of ANSI escape codes stick to ASCII, and are therefore valid UTF-8 I'm pretty confused. The link you posted states: &gt; Sequences have different lengths. All sequences start with ESC (27 / hex 0x1B), followed by a second byte 0x1B is outside the range of ASCII 0x00 - 0x7F http://www.asciitable.com/ &gt; Looks like this provides a convenience wrapper around std::ascii::escape_default Basically, except that it converts *all* non printable characters (except in pretty when it doesn't convert tab, newline and form-feed) and also intends to support representing WTF-8 (i.e. UTF-16) as human readable/writeable UTF-8 (see [this issue](t looks like this provides a convenience wrapper around std::ascii::escape_default)) UTF-8 is important as one of it's main purposes is to be an interop layer between UTF-16 and UTF-8
I will soon. Once I finish [this issue](https://github.com/vitiral/stfu8/issues/1) it will easy to give an example of converting to/from OsString. In a nutshell: STFU-8 is *always* UTF-8 whereas OsString is sometimes UTF-16. However, you can use STFU-8 to read and write UTF-16 *as UTF-8 text* (with non-conforming characters escaped as `"\uXXXXXX"`)
Yes, it's about the "human readable form" -- and for being able to represent/edit it on a web-ui.
... or, generally (because in my actual problem there are more enum variants), have enum that doesn't impl Drop trait, and a wrapper struct that does. Thank you! This indeed looks like a purely safe Rust solution, and in theory it does not even have any runtime overhead. enum MaybePair&lt;T1, T2&gt; { Pair(T1, T2), Nothing, } struct MaybePairWithDrop&lt;T1, T2&gt;(MaybePair&lt;T1, T2&gt;); impl&lt;T1, T2&gt; Drop for MaybePairWithDrop&lt;T1, T2&gt; { fn drop(&amp;mut self) {} } fn deconstruct&lt;T1, T2&gt;(mut p: MaybePairWithDrop&lt;T1, T2&gt;) -&gt; Option&lt;(T1, T2)&gt; { match std::mem::replace(&amp;mut p.0, MaybePair::Nothing) { MaybePair::Pair(x, y) =&gt; Some((x, y)), Nothing =&gt; None, } } However, this wrapper type introduces syntactic overhead for pattern matching everywhere, so I decided to go with unsafe approach instead: fn deconstruct&lt;T1, T2&gt;(mut p: MaybePair&lt;T1, T2&gt;) -&gt; Option&lt;(T1, T2)&gt; { unsafe { let result = match p { MaybePair::Pair(ref mut x, ref mut y) =&gt; Some(( std::mem::replace(x, std::mem::uninitialized()), std::mem::replace(y, std::mem::uninitialized()))), MaybePair::Nothing =&gt; None, }; std::mem::forget(p); result } }
I recommend the reading of [What every C programmer should know about Undefined Behavior](http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html) LLVM blogposts and [A Guide to Undefined Behavior in C and C++](https://blog.regehr.org/archives/213) by Regehr, both article series are very informative and might help you out with understanding the implications of undefined behavior.
There is no 'pacman install' FYI. Also, if you use cargo as a package manager (to install packages) then 'install' works exactly that way, it installs to ~/.cargo/bin. You didn't have to read a manual, you could have just read `cargo help` &gt; It only installs things globally. It doesn't, it installs them locally in your user home dir. &gt; Apparently the install and build phases are bundled into the same command in Cargo. They aren't. Build downloads and compiles a source project. If you're in a source directory and you want to install it locally, install will do that. During development you don't need install, just build, there is no 'install phase'. There are no 'separate steps'.
I'm not advocating to alienate any existing users, or anything equally extreme. All I'm saying is that it might be useful to think of how to make the differences in tooling less... unexpected. Nothing concrete. Just think about how to make things more friendly.
In terms of API docs, I would focus a lot less on the specific encodings and focus more in terms of the transformation you're performing. As you say, the transformation is about converting non-printable characters. That transformation isn't in and of itself coupled with encoding, but rather, the semantics assigned to each character in text. But take it as you will. :-) As someone who works deeply in text, I will say that your current README is very confusing. I'd be happy to work with you on it.
I am working on [voxel-rs](https://github.com/voxel-rs/voxel-rs), a voxel engine in Rust. It will soon be moddable, but for now I am laying the basics required to make the game playable.
I'm correcting you because I don't want to spread any misinformation, sorry. &gt; Downloading and installation are very different phases to me I don't understand what you're proposing. Do you want a separate download command? When you run 'npm i -g &lt;pkg&gt;' you don't have to explicitly dl deps, it's all handled for you, that's the closest thing to what `cargo install` is (except it installs per user). I think this is still sort of missing the point though. Regardless how you view installation, the command you are looking for in your blog post is 'build'. It doesn't install anything, it just compiles your source code (and if dependencies aren't available will fetch them for you). This is the workflow for normal development: - clone project - cargo build
Maybe replace that with a recent screen play adaptation of Shakespeare ;)
Sorry to respond to you in multiple spots, but consider that you have certain expectations solely because you're used to how npm works. Other people don't have these same expectations. For example, if you use stack, then cargo works almost exactly the same. You edit a file, add a dependency, and stack build. You're biased to a specific workflow, and cargo doesn't work exactly the same as npm, so that's unexpected to you. In terms of user friendliness, you only need to learn one command to compile a source project: build.
&gt; whenever you write unsafe into your Rust code, you should make a proof (well, not necessary a formal proof, but at least a reasoning) why this use of unsafe is actually safe to do. I am curious about people‚Äôs uses for unsafe. I‚Äôve only needed to turn to unsafe for FFI in all of the Rust code I‚Äôve written. I personally haven‚Äôt needed to write any custom data structures, yet. And realize it might be necessary there. So a qq: when are people most frequently turning to unsafe?
This is basically the same api as writing inline assembly in c and building them with a build.rs, what is the advantage of using a macro?
As somebody who uses CLI in my day-to-day, `vim` for editing Rust, and develops lots of low-level software...I think both are important! It's all about flow, right? It's definitely very valuable to reason about a Cargo manifest by hand; I want to make sure I can do things the manual way without it being a total pain. Even so, I still find it more than mildly convenient to add or remove dependencies with a single command! It fits very well into my `tmux` workflow, where project management, building, and running happen in a window adjacent to my code editing. Typing `extern crate blah`, switching to my runner pane, and then running `cargo add blah` is way more flowy than `vim Cargo.toml` and loading more file editing details into my head.
Oh, I guess cargo outdated doesn't work like I expected. Just checked, and `log = "0.3"` doesn't report being outdated (I suppose because you have the most up-to-date version that matches the constraint). https://github.com/sfackler/cargo-tree should actually help.
Me too! Exact same thought process here. :)
`printf("%d %d\n", ++x, ++x)` Also undefined behaviour
FYI, Rust doesn't actually protect against memory/resource leaks, mostly because that turns out to be infeasible in practice: &gt; Rust's safety guarantees do not include a guarantee that destructors will always run. For example, a program can create a reference cycle using `Rc`, or call `process::exit` to exit without running destructors. ... Because `mem::forget`ting a value is allowed, any `unsafe` code you write must allow for this possibility. You cannot return a value and expect that the caller will necessarily run the value's destructor. It is still the case though that idiomatic Rust code will be very unlikely to leak.
you can "represent" binary data as properly encoded utf-8 and then decode it again. The UTF-8 part of the binary is human readable in the representation.
Yep, exactly!
&gt; create an enum to hold each possible associated type and fill your vector with enum instances This is a good idea. But, then, whenever a new element type is needed and an observable uses it, then the enum needs to be changed an any matches on that enum need to be completed. I am quite new to Rust, is this a good programming practice? Creating an enum that is growing along as you need more elements? It seems to get easily out of hand in bigger projects. Still I guess this is the best solution for my project, I don't think I will need to scale it up and risk running into such problems.
I've worked in UDK (UE3 Scripting) for many years and i find the casting they encourage you to do in that whole architecture is pretty heinous, but it was the downcasting that scared me. What about upcasting makes refactoring expensive and dangerous?
Oooh, these are some good points. I have thought about creating a trait, but then, elements may be i32 or [i32; 3] or things like these. I can't just define a Trait for them, can I? I need to do type aliasing or something, and creating type aliases or new structs for every possible (well, used) basic type like i32, usize seems odd. But it might be an idea if most elements are not basic types. In the second scenario, is there a way to say that Observable is an ObservableBase, but with more methods? Or like defining a struct as both Observable and ObservableBase?
Personally I like enum-based polymorphism better than the classic OOP-based one. Of course, it depends on the context. With enum you get compile-time error if you have not matched everything. I'd call it an advantage :)
I think `impl Trait` doesn't support trait methods yet (I'm not sure though, maybe it does?). Anyway, you can achieve what you want using `Box&lt;Iterator&gt;`: [playground](https://play.rust-lang.org/?gist=d4f8273f71c060bd1056ec7de5acb1fe&amp;version=nightly)
You can implement traits for any type, as long as your crate defined either the trait or the type. I.e. you can't implement for example `std::io::Write` for `i32`, since your crate didn't define either of them. This is to avoid having 2 crates implement the same trait for the same type in 2 different ways. But if you have your own trait, you can implement it for e.g. `i32` or `[i32; 3]`.
You can also give them garbage data. 
- it will automatically select the correct version of the assembly based on the target triple, which I don't think is easy to do or portable in C, but maybe I'm just misinformed. - you don't have to write any C code - presumably, a version of `libasm` which has had more than 3 hours of work put into it could provide optional ways of writing assembly that include more syntactic sugar, so you don't have to memorize the platform ABI to know which registers are going to hold the input values - this uses no C compiler at any time. it is just dependent on using the assembler for the target, so with additional work, `libasm` could use a native-Rust assembler library (if/when one exists) to convert the assembly into a linkable static object, which would make it so that there are _no_ dependencies on anything outside of `cargo` and `rustc`. I mean, there is a lot of potential to this concept. `libasm` right now is just a minimal proof of concept demonstrating the fundamental leap required to show that everything else described is within reach, if someone wants to reach out and exploit this opportunity.
hmm. The state I want is per-client not per-server but if I change things around and make a new instance of the server instead of cloning the server for each connection I can do what you suggest. Is it weird to do that? I've only ever seen examples that call `from_server` once and then clone the result. I just pushed changes so you could see how that looks.
Nice! Just benchmarked [zr](https://github.com/jedahan/zr), my little rush zsh plugin manager and got very similar numbers to my manual benchmark. 26¬±4ms to generate a pretty hefty (14 plugin) zshrc. The time to load said plugins is 180¬±11ms, so a 14¬±4% penalty is reasonable for not having to manually manage plugins. First is zr making the zshrc, second is making and loading all the plugins. [benches](https://i.imgur.com/Ap8LruN.png) function zrinit { zr load sorin-ionescu/prezto/modules/git/alias.zsh \ sorin-ionescu/prezto/modules/history/init.zsh \ sorin-ionescu/prezto/modules/osx/init.zsh \ sorin-ionescu/prezto/modules/homebrew/init.zsh \ junegunn/fzf/shell/key-bindings.zsh \ zsh-users/zsh-autosuggestions \ zdharma/fast-syntax-highlighting \ molovo/tipz \ changyuheng/zsh-interactive-cd \ geometry-zsh/geometry \ jedahan/geometry-hydrate \ jedahan/geometry-todo \ ael-code/zsh-colored-man-pages \ momo-lab/zsh-abbrev-alias } 
Even if this text is well written, you could have similar information in the book or other documents. It‚Äôs more about grasping customer data than providing novel approach / information. Moreover it dates back to 2015. Buzz?
Yes, you can implement traits for any types, including built-in ones as long as you defined the trait. Yes, you can specify that one trait extends/implies another like so: trait Observable: ObservableBase { ... } This is just shorthand for: trait Observable where Self: ObservableBase { ... } And you can put almost any constraints you want on, for example: trait Observable where &lt;Self as TraitA&gt;::AssociatedType: TraitB
&gt; We could adopt a rule that features cannot stay in nightly if no progress on stabilization is being made for more than 4 cycles. Or agree as a community that no more than 25 language and 25 library features can be in-flight at any time. I agree with the general sentiment, though not with the proposed limits. As the number of contributors grow, it seems likely that the number of in-flight features will grow, and that's *fine* (of course, it creates management issues, but that's a nice problem to have). The 4 cycles limit seems better, however some features are inherently hard (specialization anyone?) and require a lot of experimentation. Maybe it should be ownership based? As in: - each feature must have a shepherd, - a shepherd may only have maximum of N features to its name, - if a shepherd needs to pick up a N+1 th feature, they need to drop one of the N currently to their name, - if no shepherd is willing to take over, sorry to those who worked on it, but the code is cleaned-up and the feature removed. It'd probably be an unpopular decision of course, socially it's hard to say NO and even harder to tell someone you'll have to drop their work in the bin. It may even burn bridges. At the same time, though, it's a reality that one can only juggle so many balls at a given point in time. So if no progress is made, the ball drops...
That and we should *encourage* users to use stable Rust as much as possible. *nightly* should become a hacker's thing, were you toy around with Rust, push the compiler to its limits, etc...
In [optional](https://github.com/llogiq/optional), I use one unsafe `slice::from_raw_parts(..)` call to get rid of one branch. As the length can only be 0 or 1, this is provably safe. In [bytecount](https://github.com/llogiq/bytecount), I have one unsafe block to extract one suitably aligned slice of a larger type from a byte slice. The code has an assert statement to ensure correct alignment, so it *should* work on all valid byte slices.
https://www.reddit.com/r/rust/comments/61lnyr/polymorphism_in_rust_enum_vs_trait_struct/ I think `Box` is the simplest solution here. If performance matters, try to implement both and measure but I'm guessing there will not be big differences.
You should show examples in the README for better adoption - https://www.makeareadme.com/#usage 
I dunno, maybe I misunderstood something but where is the C# version of your workflow?
Thank you for the feedback! Is this your color scheme or am I (or `ansi_term`) doing something wrong? The mean time is the most important output and it's in dark gray instead of bold+green. (https://github.com/sharkdp/hyperfine/blob/b0bab45e515b14e399b8f6d913f56f9b3a88bcf4/src/hyperfine/benchmark.rs#L189)
Or don't be evil and use a savvy programmer's trick.
Aha, I see. Thank you very much!
No, I thought about `impl` **in** my library, so users can just pass a closure into a `to_writer` method and provide a serialization routine in it, as in latest 5 lines of my example above. Yet I'm still thinking that this design looks ridiculous and probably serde should provide that methods, as was discussed here: https://github.com/serde-rs/serde/issues/644 
Had you looked at `newtype_derive` (https://crates.io/crates/newtype_derive)?
Ah, didn't realize the cc crate wasn't automatically a c compiler dependency. Hopefully inline assembly rfc makes progress before we need to do all of this work to get to it instead.
It seems like the content of the file you're parsing isn't a number. Could it be a number followed by a newline? What happens when you do `contents.trim().parse::&lt;i32&gt;()`?
Both are viable options in my opinion. I don‚Äôt personally care if they support the community or not. The current age of the web where user data is aggressively collected needs to end.
/u/thiez thanks. I fixed it already. let mut contents = String::new(); f.read_to_string(&amp;mut contents).expect("something went wrong reading the file"); let len_withoutcrlf = contents.trim_right().len(); contents.truncate(len_withoutcrlf); temps.push(contents.parse().unwrap()); //into vector The fn returns a vector with all values.. 
You're declaring a static array of 8GB. Obviously the poor rust playground server either doesn't have that much memory available or, if it does, it has been sensibly configured not to allow rustc to use that much memory. Perhaps you could make your static array a few orders of magnitude smaller?
I'm not certain about this, but a 1000000000-element array of `u64`s amounts to 8GB, which might be more than the compiler is willing to handle as a static variable. edit: obligatory acknowledgement of thiez's quicker response!
I'm trying to create a clone of what I believe is an `Iterator` of what I believe are `String`s, but the compiler is telling me that it expected `String`s when instead it found `&amp;_`. let sentence: &amp;str = "Help me I am so lost"; let words = sentence.split(' '); let owned_words = words.map(|word| word.to_owned()) .cloned(); results in | .cloned(); | ^^^^^^ expected struct `std::string::String`, found reference | = note: expected type `std::string::String` found type `&amp;_` Ultimately, I'd like to be able to perform mappings and filters on an `Iterator` of strings while still being able to use the original `Iterator` later. But neither `&amp;_` nor "ampersand underscore" are effective search queries, I'm not sure how else to approach this! Guidance would be greatly appreciated :) [Here's a playground!](https://play.rust-lang.org/?gist=8be9919338d7fa6b5774f755a0fbbe7b&amp;version=stable)
I'm trying to create a clone of what I believe is an `Iterator` of what I believe are `String`s, but the compiler is telling me that it expected `String`s when instead it found `&amp;_`. let sentence: &amp;str = "Help me I am so lost"; let words = sentence.split(' '); let owned_words = words.map(|word| word.to_owned()) .cloned(); results in | .cloned(); | ^^^^^^ expected struct `std::string::String`, found reference | = note: expected type `std::string::String` found type `&amp;_` Ultimately, I'd like to be able to perform mappings and filters on an `Iterator` of strings while still being able to use the original `Iterator` later. But neither `&amp;_` nor "ampersand underscore" are effective search queries, I'm not sure how else to approach this! Guidance would be greatly appreciated :) [Here's a playground!](https://play.rust-lang.org/?gist=8be9919338d7fa6b5774f755a0fbbe7b&amp;version=stable)
uuuh, that is very nice, good to know! thank you!
Oh wow. Well, this seems really versatile, I need to play around more with Rust. Thank you for your answers.
I made corrections, I hope this time I'm inside the proper unspecified behavior (eg. one that is unspecified but yet not undefined). Thanks once again for pointing that out.
 pub fn worker_create(p_create_settings: worker_create_t) -&gt; worker_instance_t; is not the same as worker_instance_t worker_create(const worker_create_t* p_create_settings); You're missing one level of indirection in the argument.
Oops, sorry got my terms mixed. It's the down casting that bothers me. I guess I haven't actually had a conversation about it in 5+ years.
Nice. I built [utime](https://github.com/coder543/utime) to just apply `criterion-rs` to external tools directly. Simply, utime my-command and its args to use it. Yours has a nicer UI, for sure!
I've found that creating a separate server object per connected client is often a useful pattern with capnproto RPC. Hm... if you do go this way, do you even need the `Rc&lt;RefCell&lt;...&gt;&gt;`? Can't you wait until you know the email address before constructing the server object?
As far as I remember, I've written unsafe on three occasions: * When learning Rust a year ago I thought I absolutely needed to do a zero-cost abstraction. I never finished that code and now I know that was pointless (for one, it was premature optimisation, for another, there's a crate that already implements what I needed). * Doing FFI. * In `corona` (a library for stackfull coroutines), I need to juggle the stacks and `context` (the library for switching stacks I use) presents unsafe API ‚Äí which makes perfect sense, it can cause havoc if used incorrectly. I also allow waiting for a future that is not `'static`. But I need to plug it into `tokio_core::reactor`, which takes `'static`. The future has all its data somewhere on the current stack and when suspended, owns the stack, so it really is `'static` in conceptual sense ‚Äí but Rust doesn't reason that far, so I had to cheat with `unsafe`. So this is in the category of expressing a new abstraction Rust doesn't know about. I hope my reasoning there is correct and I haven't overlooked anything. Though I probably should ask for a review by someone else ‚Äí but I plan some more changes first.
That's strange, because it's [supposed to work](https://play.rust-lang.org/?gist=4d24fbeba13610645921f9c01952ac01&amp;version=stable). If I had to guess, your `settings` is not actually the same type that the function expects to receive.
No, it's Shakespeare on purpose. With undefined behavior, you never know what may happen ‚Äí even being liable for copyright infringement for something in public domain ;-)
And we don't know that because a macro operates at the syntax level and they isn't type aware ? (this is a genuine question, I know nothing about macros).
I didn't. At least, not the way that sentence characterizes the timeline. I burnt out; ran out of emotional energy to be effective in my role as technical lead for the project mid way through 2013 (at the tail end of my divorce, and while recovering from a surgery -- not a great time in my life), so I took a break, switched off the Rust team, took a year to work on lower-profile and less-time-sensitive projects inside Mozilla (test-farm automation for Firefox-on-Android at first; later the [wifi-and-cell geolocation service](https://location.services.mozilla.com/)), eventually quit Mozilla and worked for a completely unrelated payment network (Stellar) doing a distributed transaction processor for another year and a half, then finally in early 2016 got a call from someone at Apple saying they were looking for some folks to help with Swift (in a non-leadership position, which I prefer). It's got nothing at all to do with an assessment of the relative merits of the languages. I like Rust a lot, and still consider it a very important technical contribution to the landscape (in the sense of a successful technology transfer from research to industry, prioritizing memory safety and data-race safety for systems programmers -- see [my comments on this matter here](https://graydon2.dreamwidth.org/247406.html)). I'm also thrilled to see the community develop to such a broad and healthy extent: both the wide ecosystem of libraries, the quite broad ownership of the language and compiler codebase itself, and the extent to which the community emphasizes beginner-friendliness, simplicity, helpfulness, approachability, mentoring, documentation, outreach, and yes even its battle-weary code of conduct (which you can blame me for if you are looking for someone to blame). IMO these are all great things, and I think Rust will always have a special place in my heart given the unusually intense effort I put into its first 7 or so years. But: I don't think Rust is the last or only language that needs to exist. Indeed, I think there's [quite a lot of work left to do on languages](https://graydon2.dreamwidth.org/253769.html) before anyone could credibly argue such things about any language. I've always been a language pluralist -- picture my relationship towards languages like a kid enjoying a wide variety of building blocks, musical instruments or plastic dinosaurs -- and I don't think evangelism or single-language puritanism is especially helpful. More specifically: I like Swift too! I even [said so when it was released](https://graydon2.dreamwidth.org/5785.html). It has a bunch of qualities that Rust lacks (the clang importer, reflection, a repl and playgrounds, runtime-dynamic generics, keyword arguments, cleanly-integrated reference-typed classes, user-extensible pattern matching, simplified local borrow-like alias control, compiler-supported ARC, generally much lower cognitive load) and an overall different area of focus (mostly user-facing, UI-centric app development, so far). In many ways, it took things that Rust tried to do early in its life and ran with them, rather than changing course in the same places Rust did; there's a lot of familiar pieces. I'm happy Swift exists too, and I'm happy to be working on it. Various members of the Rust and Swift teams know each other, talk to one another, trade ideas and implementation insights, and generally coexist peacefully; and they're both fantastic groups to work with. I feel very lucky to have had the chance to work in both projects. 
If you're using itertools, you can shorten that slightly as `flatten()`.
I'm not sure that backing out features because they are stalled is a good idea - the main reason a feature is stalled is because stabilizing it turns out to be more complicated than it appears at first glance, so the effort/cost ratio is lower than it appeared to be. 
The `.cloned()` adapter does not clone the iterator. It clones the *items*, as if you did `.map(|x| x.clone())`. To clone the iterator itself, you just call `.clone()` as with any other type you want to clone. However, the iterator you've created is not `Clone` because of the closure passed to `.map()`; closures currently do not implement `Clone` even if they could. However, replacing the closure with passing `.to_owned()` directly as a first-class function works (because first-class function values implement `Clone`): let owned_words = words.map(ToOwned::to_owned).clone();
Looks like you accidentally posted twice.
Yea, macros run before type checking, so a macro isn't aware of any types in a program.
Alternatively, they could just `.collect::&lt;Vec&lt;_&gt;&gt;()`, and iterate over the resulting `Vec` as often as they want.
 $ cargo tree enigma v0.0.2 (file:///D:/src/enigma) [dependencies] ‚îú‚îÄ‚îÄ getopts v0.2.15 ‚îú‚îÄ‚îÄ log v0.4.1 ‚îÇ [dependencies] ‚îÇ ‚îî‚îÄ‚îÄ cfg-if v0.1.2 ‚îú‚îÄ‚îÄ simplelog v0.4.4 ‚îÇ [dependencies] ‚îÇ ‚îú‚îÄ‚îÄ log v0.3.9 ‚îÇ ‚îÇ [dependencies] ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ log v0.4.1 (*) ‚îÇ ‚îú‚îÄ‚îÄ term v0.4.6 ‚îÇ ‚îÇ [dependencies] ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kernel32-sys v0.2.2 ‚îÇ ‚îÇ ‚îÇ [dependencies] ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ winapi v0.2.8 ‚îÇ ‚îÇ ‚îÇ [build-dependencies] ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ winapi-build v0.1.1 ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ winapi v0.2.8 (*) ‚îÇ ‚îî‚îÄ‚îÄ time v0.1.39 ‚îÇ [dependencies] ‚îÇ ‚îú‚îÄ‚îÄ libc v0.2.36 ‚îÇ ‚îî‚îÄ‚îÄ winapi v0.3.3 ‚îÇ [dev-dependencies] ‚îÇ ‚îî‚îÄ‚îÄ winapi v0.3.3 (*) ‚îî‚îÄ‚îÄ yaml-rust v0.4.0 [dependencies] ‚îî‚îÄ‚îÄ linked-hash-map v0.5.0 I hope this helps.
Exactly. Though I'm not sure if we could at least get the other derives...but I'm on mobile and too tired to check.
Thanks for taking the time to review my problem I've written a smaller version of my problem: I have a `Table` that contains `Page`s, and `Page`s contain `Row`s. I don't want to initialise my `Page`s and my `Row`s at compile time so I use an array of `Option` (maybe I shouldn't be doing this in the first place) Then I want to check if a `Page` is `None` or `Some` before either assigning it a new `Page` with the `Row` or the `Row` in the existing page extern crate arrayvec; use arrayvec::ArrayString; #[derive(Debug, Clone)] struct Row { id: u32, username: ArrayString&lt;[u8; 32]&gt;, email: ArrayString&lt;[u8; 256]&gt;, } #[derive(Default, Debug)] struct Page { rows: [Option&lt;Row&gt;; 15], } #[derive(Default, Debug)] struct Table { pages: [Option&lt;Page&gt;; 32], num_rows: usize, } fn main() { println!("Hello"); let mut table: Table = Default::default(); let row: Row = Row { id: 1, username: ArrayString::&lt;[u8; 32]&gt;::from("Alex").unwrap(), email: ArrayString::&lt;[u8; 256]&gt;::from("alex@bjr.com").unwrap() }; match table.pages[1] { Some(mut p) =&gt; p.rows[0] = Some(row), None =&gt; { let mut page: Page = Default::default(); page.rows[0] = Some(row); table.pages[1] = Some(page); } } }
That's really cool!
Iterate over lines in the file, parsing each line. The .lines() iterator method strips the newline and gives you just the text
Is there safe Rust equivalent of the following function? fn modify_in_place&lt;T, F: FnOnce(T) -&gt; T&gt;(x: &amp;mut T, f: F) { unsafe { let old = std::mem::replace(x, std::mem::uninitialized()); std::mem::forget(std::mem::replace(x, f(old))); } }
That's a good approach as well. It depends on the actual use-case, I guess.
No, and there cannot be, because it is not panic-safe :)
&gt; It would be cool to be able to define the trait bounds only once for implementation of all structures, like in the following pseudo-code: I really like this idea. This seems potentially related to a stronger module definition than exists today. That is modules could be a location to define the `scope` discussed here.
Most of this is good except the large source files one. Even keeping a file to just one struct and the related code you can get over 1k lines easily.
That's actually not true due to the signature of `Deref`. The returned type must always be a reference, the only case where we could ever do this is when the inner type is a reference. It would be nice to ask if the inner type implemented `Deref` though, so we could deref to `&amp;str` instead of `&amp;String`.
We used to be able to back when it was unstable. We can't in 1.15
I have to import std::str::FromStr to be able to call mytype::from_str() outside of where mytype is defined. It is in scope inside its own module for certain but without the import I get the error: no function or associated item named `from_str` found for type `mytype` in the current scope. Also, I have a macro that works like this: ni32!("-0.1234") but I want to change it to ni32!(-0.1234). Is there any way to do this via (0.$e:tt) or the like? I would prefer if it could be done without floating point getting involved, because some of my types have a precision higher than f64. 
It looks like that long predates 1.15 and hasn't been updated for custom derive.
I guess the content is not collected a big `String` to split but rather read in a loop one at a time and processed in the same loop. `trim` or subslicing (if he trusts the sensor to give him ASCII characters only) is probably the best thing to do.
The `str::parse` function uses the `FromStr` trait to allow parsing strs into arbitrary types.
Ah, I see. You're right.
If you haven't discovered it yet, `.zip()` is the iterator adapter that handles running two iterators in lockstep for you. If you want to collect to another vec while leaving out some elements, you can use either `.filter()` or `.filter_map()` (which also lets you transform those elements).
Occasionally, nightly builds will lack the `rustfmt-preview`/`rls-preview` components for a few nights as they get fixed for compiler refactors. I recall seeing a post a while back about a (cargo subcommand?) utility for checking that none of your installed components are lacking in a toolchain before updating, but can't find it again. Does anybody recall what it is?
Then the related code can typically be moved to submodules. Shorter files are pretty much better all around. Less to keep track of at once (for you and the editor) and more encapsulation (by moving deep implementation details to submodules and limiting the parent modules to the broader strokes), making code easier to reason about. We don't need more monsters like https://github.com/rust-lang/rust/blob/master/src/libcore/iter/mod.rs, which is all the iterator types in one file; going to one file (def + impls) per type would make this much easier to navigate.
Ooh these definitely sound useful! Thank you!
&gt; The current age of the web where user data is aggressively collected needs to end. I think only some collection of data is problematic. Microsoft shouldn't be collecting telemetry because I already paid for the license. I would be okay with pervasive cloud integration in Androids if there was any meaningful alternative on smartphones. Intel shouldn't require registering an account for software to use hardware I lready bought. But I haven't paid anything to O'Riley for that book previously, so I'm completely fine if they wish to know my email in exchange for that book. 
sorry to hijack this - but it's at least related to d128/f64 ... how do you guys get back and forth between d128 and f64 in your code? is there some place in the decNumber ecosystem (I've only used the rust version) where there is a performant conversion function? I don't know enough C to really navigate that codebase. 
[swc] is a project to port [closure compiler][] / [babel](https://babeljs.io/) / [postcss][] / [webpack][] / [rollup][] to rust. Project is under active development and far to be done. Currently only ecmascript parser is implemented. I'm currently porting control flow graph simplifier from google closure compiler. When it's done, I will do some ast design work or implement early errors for parser. ----- EDIT: Use inline link for babel. ----- [swc]:https://github.com/swc-project/swc [closure compiler]:https://github.com/google/closure-compiler/ [postcss]:https://github.com/postcss/postcss [webpack]:https://webpack.js.org/ [rollup]:https://rollupjs.org/
To be fair, the documentation to code ratio in that file is an order of magnitude higher than most
Yes, but it also contains all the iterator types together even though they don't share a whole lot of code (except trait imports, I guess). This is the kind of situation the author was talking about; the high LOC count is more of a symptom than anything. Each type can pretty easily be broken out into its own file, which also gives an instant improvement to navigation because you can easily spot the file for the iterator type you want to look at, whereas now you basically just have to grep for it.
I think it would make it far, far worse to navigate. Half the time RLS is crashed and dead, so all I have is Ctrl+F to guide me. Sticking to a small number of files is a lot better.
&gt; I think only some collection of data is problematic. Microsoft shouldn't be collecting telemetry because I already paid for the license. Agreed. &gt; I would be okay with pervasive cloud integration in Androids if there was any meaningful alternative on smartphones. I don‚Äôt understand what you mean. Don‚Äôt Google services already count as pervasive cloud integration? And if privacy and data collection (at the OS level) is a concern then iOS is a meaningful alternative. Claiming otherwise is irrational. Or use some other form of Linux. I am more concerned with native apps like Facebook and Uber which have proved to disrespect user privacy. &gt; Intel shouldn't require registering an account for software to use hardware I lready bought. Agreed. &gt; But I haven't paid anything to O'Riley for that book previously, so I'm completely fine if they wish to know my email in exchange for that book. I‚Äôm not fine with this or these kinds of practices. I don‚Äôt want your newsletter, I don‚Äôt want you to know my email even if you‚Äôre product is free, and I don‚Äôt want you to sell my email to your ad partners. Ultimately this is a minor transgression compared to the tracking advertisements that have poisoned the web, and the dominance of Facebook (and maybe Google, Microsoft, and Amazon depending on when your trusts lie). But the whole model must end. 
The main issue the author was talking about is large files with many barely-related types, like `iter/mod.rs`. It's also quite a bit exacerbated when some project maintainers insist on listing all typedefs together before any impls though fortunately `iter/mod.rs` doesn't do that. I personally use the IntelliJ Rust plugin which is generally pretty reliable with the only current issue I have being that goto-definition doesn't work on items created by macros, but that's understandably difficult to implement.
it would be nice to have optionally define documentation in separate specialized files.
I don't personally see any value in that. Changing code often requires changing docs. Better to keep them close
Sure, except when i said "just one struct and its related code can get above a thousand lines" your reply was about using sub-modules and more encapsulation layers, which sounds anti-helpful to me.
It depends, right? You don't need a submodule for every function's implementation details but a struct with a very large API surface should probably have its implementation broken into at least a couple of parts to make them easier to digest. If the filename very clearly suggests what part of the implementation it's involved with, then it's not much more inconvenient. It would even be easier to jump back and forth between use and definition since individual buffers/tabs in your given editor will retain the lines you were looking at whereas having multiple buffers pointing to the same file will be varying levels of painful depending on your editor (IntelliJ doesn't seem to allow these at all). 
Two things: I wrote a cool, interactive git client in Rust where I'm working on new features and fixing bugs: https://github.com/spacekookie/fool And a schema building API, using Diesel for queries during schema migrations and which should eventually be embedded into a cool migration toolkit (super early in development) https://github.com/spacekookie/barrel Especially regarding the second one, I'd love to talk to people and get feedback if what they want üòú
Nice, just switched to it.
&gt; Don‚Äôt Google services already count as pervasive cloud integration? They could be integrated in less pervasive way. It'd be fine if every service prior to its activation obtained actual informed consent from me instead of UI being littered with dark patterns. I'm still salty google took liberties by "helpfully" siphoning my contacts. &gt; then iOS is a meaningful alternative. They are pricier and a walled garden but I more and more regret now not going with them. They are both bad in their own ways. &gt; even if you‚Äôre product is free Maybe agree that it is less egregious because the choice to get that book is much more voluntary than in other examples? 
The second and third proposed improvements are [const generics](https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md) and [trait aliases](https://github.com/rust-lang/rfcs/blob/master/text/1733-trait-alias.md). RFCs have been accepted for both.
Two thoughts here: 1. code removed is still in the repository's history, the very nature of a version control system keeps the code around 2. If a feature is in nightly, but stalled on the way to stabilization, and no immediate progress can be made, is it worth keeping around? What criteria would lead to a feature being removed? 
If a feature's nightly implementation is just a (small) fraction of the work needed to stabilize the feature, is there a point where it's time to re-evaluate whether or not a feature is really worth it to continue/keep it? I'm thinking of [escalation of commitment](https://en.wikipedia.org/wiki/Escalation_of_commitment)
**Escalation of commitment** Escalation of commitment is a human behavior pattern in which an individual or group facing increasingly negative outcomes from some decision, action, or investment nevertheless continues the same behavior rather than alter course. The actor maintains behaviors that are irrational, but align with previous decisions and actions. Economists and behavioral scientists use a related term, sunk-cost fallacy, to describe the justification of increased investment of money, time, lives, etc. in a decision, based on the cumulative prior investment ("sunk cost"); despite new evidence suggesting that the cost, beginning immediately, of continuing the decision outweighs the expected benefit. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Multi-parameter type classes are already possible directly in Rust as-is; no further extensions are necessary for them. Take a look at the [`std::convert::From`](https://doc.rust-lang.org/std/convert/trait.From.html) trait, for example, and ask yourself how you would write it in Haskell or Purescript (or any other language of that lineage). As for the type class hierarchy on that page, I'm pretty sure it's possible, but I will work out the details for you in a little bit. Expect this comment to be edited.
Ok I see This is working for me: match table.pages[1] { Some(mut p) =&gt; p.rows[0] = Some(row), None =&gt; { let mut page = Page::default(); page.rows[0] = Some(row); table.pages[1] = Some(page); } } https://play.rust-lang.org/?gist=85d984647b0ac70c8ea5df6205f47727&amp;version=stable To come back to the initial question, you do not need this line: table.pages[page_num] = updated_page; because the page has already been updated in the match branches.
I'd suggest looking at the 'volatile' or 'volatile_register' crates. Both are zero cost easy wrappers for volatile memory locations. I'd make links, but I'm on mobile.
perhaps #[derive(Deref)] #[deref_target="&amp;mut &amp;str"] to change the default?
If I understand everything correctly, most of what you mention _already is_ more general than Result / Option / etc. In my understanding of `async/await`, the plan is to have the compiler really only support [generators](https://doc.rust-lang.org/unstable-book/language-features/generators.html). Then an external compiler plugin will add `async/await` on top of that (but Generators will still be usable). As for `?`: this.. already works for any type? I mean it's not yet stable because of naming bikeshedding, but the https://doc.rust-lang.org/std/ops/trait.Try.html trait will allow anything implementing it to be used with `?`. If you're already aware of those generalizations: could you elaborate on your concerns? In my understanding both `yield` and `?` will work for any types you want, and have no plan to be special for `Result`, `Option`, or any other monad-like type. With monads: I understand the general idea of them, but I have no idea how they relate to `yield` nor to `?`. async/await, or more generally, generators &amp; `yield`, are a way of having a function transformed into a state machine which is fully stored as an enum on the stack (rather than having its own suspended stack). This doesn't seem similar to monads at all? `?` is a way of doing early return with a transformation on the return. Nothing inherently related to transforming over types, nor related to `Result` besides that being the only stable usage of it. Could you explain more closely the relation between these features and monads?
It seems like you're talking about the implementation details of async/await in rust, whereas I read the OP as talking about the syntax in general (in rust and other languages). Is that right?
Do I understand correctly, then, that struct Wrapper&lt;'a, T&gt;(&amp;'a [T]); impl&lt;'a, T&gt; Deref for Wrapper&lt;T&gt; { type Target = &amp;'a T; ... } // ... no impl&lt;'a, T&gt; DerefMut for Wrapper&lt;T&gt; ... ... presents (from outside module perspective) an immutable view of a borrowed slice of `T`s? (unless `T` has interior mutability) And if so, then would the immutability be lost if `Wrapper` contained/dereffed-to a `Box&lt;[T]&gt;` or a `Vec&lt;T&gt;`?
No? async/await produce a generator. This isn't an implementation detail, they are sugar for producing generators that implement Future. do-notation doesn't help here.
Can generators not have a monad instance written for them? Async/await (in JavaScript) is the equivalent of Haskell's do notation, specifically for the Promise (pseudo-)monad. I think OP is wondering if, assuming the behavior of Rust's async/await is analogous and is also the equivalent of do notation, we ought to generalize it to work with any monad, not just the generator monad (if such a thing exists).
E.g. in this example from the mdo-futures crate: let get_num = ok::&lt;u32, String&gt;(42); let get_factor = ok::&lt;u32, String&gt;(2); let res = mdo! { arg =&lt;&lt; get_num; fact =&lt;&lt; get_factor; ret ret(arg * fact) }; That's a monadic pattern and it works like async/await. Every statement can depend on results from previous statements in an async way. The problem with `Try` is, it doesn't have the api of a real monad, it requires two associated types and is tied to the Ok/Error dichotomy. A read monad only has two functions: pure and bind, e.g. [here](https://github.com/TeXitoi/rust-mdo/blob/master/src/lib.rs#L111-L123) is the monad impl for `Result`. And monads support composing multiple monads into a stack and lifting computations through these monad stacks (to support different kinds of side-effects combined). Try only provides early return as the "behind the scenes" functionality, real monads can provide any "behind the scenes" functionality, e.g. logging between each step/bind(), async.. The monad impl basically provides an interpreter for the DSL that is do-notation and can do much more than early return, e.g. look at the [monad impl for Iterator](https://github.com/TeXitoi/rust-mdo/blob/master/src/lib.rs#L131-L140) and the [example](https://github.com/TeXitoi/rust-mdo/blob/master/examples/iter_mdo.rs).
I don't know exactly what it would look like, but it would be cool if the end-user could teach the compiler new intrinsics. Blocks of asm with extra properties attached.
Right. When using mdo-future, you still have to create your core to run the futures chain on. But the actual syntax for chaining is purely monadic and independent of the implementation details.
I'm interested in writing a compiler from PureScript (it's like Haskell but strict instead of lazy) to Rust, would this crate be useful for the generated code?
[Themes for JetBrains IDEs](http://color-themes.com/?view=index). My personal favourite: Roboticket.
Async-await isn't quite analogous to `do` notation because of it respects control flow operations like loops, and may (someday soon) allow borrowing across yields, which isn't (easily?) achievable using monad patterns.
Thanks for your work on xi, I'm looking forward to use try it (on Windows)!
Just because Rust is easier to write correct/bug-free code in, doesn't mean you have to use your brain less than in C++, but the way you use your brain is more rewarding in Rust: You can focus your brain efforts on building higher skyscrapers (abstractions) on top of each other instead of being bogged down by debugging footguns. So it frees your mind up to ascend to higher levels..
Thoughtful post, and a fantastic illustration of survivorship bias. Thank you!
&gt; Because rust is a consistent language &gt; The weird thing is that depending on where you use it, impl Trait has a different meaning. hm..
&gt; Because rust is a consistent language, and it makes no sense to allow impl trait in some cases but not in others. Actually, it does make sense! And in fact it makes even more sense to not allow `impl Trait` for func args, to preserve orthogonality (only one iodiomatic syntactical way to express `T: Trait`). Why does it make sense to only allow `impl Trait` for return types, you ask? Because we **need** `impl Trait` for when we return e.g. `T: Iterator` where the T in question is some Map or FilterMap containing a closure type, which can't be named! So it would be wrong to be able to write fn foo&lt;C, R: Map&lt;std::slice::Iter&lt;'_, {integer}&gt;, C&gt;&gt;(..) -&gt; R Because `C` the closure type is not universally quantified, but fixed by the body of `foo`! So we **need** `impl Trait` for return types but not for arg types! And IMO we shouldn't allow it for arg types, because then we have two idiomatic ways to express the same thing and none is more preferable than the other, which destroys orthogonality.
This shouldn't be too difficult to support, though, right? `rustc` could just float out all the inner items (as it conceptually does anyway) and in doing so copy all the generic parameters to the newly extracted item. Does this just need someone to do the work of actually making the compiler do that, or am I missing some deeper problem here?
Probably the assumption that compiler authors won't add new optimizations or change existing ones in way permitted by the spec.
Not only that, it's a novel interpretation of the phrase "off-by-one error".
This is a great tutorial - I've gotten very confused about how to deal with overeager matching before and this helps a lot.
One thing I say is that some of my gamedev concerns (there's 2 contrasting issues) might re-surface in different ways in the potential for rust-in-the-browser (i.e. new use-cases): half my concerns are about low level efficiency whilst the other half are about rapid-iteration. My 'dream language' has the efficiency of C++ as its pillar, but doesn't go out of it's way to cripple opposite use cases (UI/rapid iteration/quick scripts..) .. and actually has concessions to enhance these. There's currently some cases where c++ can feel a bit faster than rust for quick experiments, despite being quite abysmal (possibly because of single-source stuff dodging the awkwardness of header files). With tweaks, rust could be unambiguously superior in every domain. **rust is a contender to be shaped into my dream language, but doesn't target the space I describe** Just incase my concerns sound insane (describing something that doesn't exist) there are 2 other efforts to do **exactly** what I want * JAI * Apple Swift- if it eventually gets [move-semantics](https://github.com/apple/swift/blob/master/docs/OwnershipManifesto.md) retrofitted, it would become suitable. it started from the middle ground of course ('application'). wouldn't it be great if Rust stole there thunder? It is so frustrating that this community is, overall, actively hostile to my ideas :( These goals might sound insane but recently what I was doing highlighted the need. I was messing with a quick tool in JS to collaborate with someone elses 'image annotation'(for AI) platform. I want to connect this with game-dev toolchains, e.g. but I dont want to have to switch languages so much. I'm 'rewriting in rust' but doing something like this is 4x as slow (devtime), i.e. i'm back to being bogged down in compulsory traits etc. I want to move data structures between them. Here's a discussion about part of a tool that starts as UI but turns into something computationally intense https://github.com/bbernhard/imagemonkey-core/issues/59. **The point is I dont want language boundaries between the different use cases**. you can start something in one domain then need to add performance, or vica versa add UI/tests to another. I remember something that happened in the development of rust which reminds me of your bulletholes anecdote. They were removing features based on measurements of use.. *using the rust compiler for reference* (the potential mistake being assuming *all* potential use cases would mimic the patterns in a compiler sourcebase ... features of great promise and use to *me* were removed because the compiler didn't use them. 
Is this related to Rust?
That blog post is the latest progress on this front. It wasn't really a priority so it didn't move forward from there, but it should not be hard for others to pick up from where we left off. So yeah, stalled.
The community is not hostile per se. Rust is a great language for building large applications and some of the things you propose (not all) to make it better for quick experimentation would allow things that make large applications harder to manage. This doesn‚Äôt mean that your use cases shouldn‚Äôt be addressed, but that doing so without allowing people to make large applications harder to refactor is very hard. Backwards compatibility make things even harder, and that the arguments deviate from constructiveness does not really help with these issues. Still I think that some of the RFCs you wrote do address real problems. Yet your RFCs are not even at the level of many pre-pre-RFCs. Many have encouraged you to make these better to which you respond with a tone like ‚Äúit‚Äôs not my job, that‚Äôs your job‚Äù. Sorry but it is not. So the mixture of the things you want being hard together the lack of willingness by the interested party to put in the work and the tendency to deviate from constructive criticism are what might be giving you an hostility feeling.
Thank you! This is good to know.
[removed]
I didn't know but as Taymon posted, there is already an RFC for trait aliases: https://github.com/rust-lang/rfcs/blob/master/text/1733-trait-alias.md They're supposed to solve the same problem :)
Maybe, but for me personally (remember I come from Ruby world) it would be a sign of smell. It should not count blank lines, comments or tests.
Thanks for the update! It's a bit disappointing, having GC hooks like that would be an extremely useful feature for using Rust as the host for scripting languages.
It turns out there are fewer pre-1.0 unstable features left than I thought before counting: https://github.com/rust-lang/rust/issues/39954
&gt; And in fact it makes even more sense to not allow impl Trait for func args, impl Trait in argument type does not mean the same thing as in return type, one is an existential type and one is an universal type. It would make sense to allow `impl Trait` in argument type to denote existential types (which is not the same thing as `T: Trait`).
Yep, that was discussed ad-nauseum in the RFC. The places where it has a different meaning are places were it doesn't really make much sense to use its real meaning anyways. Purist gave a good fight, but pragmatist won: if its useless, either don't allow it, or if we allow it, we allow it for something useful at least.
Turns out that assembly is incredibly hard to reason about formally (e.g. such that a compiler could understand it). For example, many assembly instructions can have many side-effects (setting flags) and their behavior is affected by global state (like flags set). 
I also wrote one using criterion-rs https://github.com/AlexanderThaller/bench-rs but this one really has a nice output.
I feel like Rust needs a way for corporations or individuals to sponsor the resources needed for Tier 1 / Tier 2 support for other architectures.
Outside of ARM, AVR has the biggest support in the embedded space. Lots of beginner programmers are learning C/C++ via Arduino and dealing with hardware is hard enough without having software bugs that are hard to debug. The best part about embedded systems and Rust is the Trait system. You can connect completely different items together without having to understand what is happening. A good example is a HTTP library. Now this needs a network connection. This connection could be via a SPI interface, UART, USB or something else. Rather than develop a HTTP library for each use case, you could just use a tcpPacket trait. Personally, I am working on an embedded project that I would love to use Rust on. It uses ARM, but the Arduino structure. By targeting AVR and ARM, the same code could work exactly like the Arduino IDE and compile to both targets as required.
&gt; How should a language allow the quick and dirty, I dont advocate anything that breaks correctness; it's really just more options for inference , and syntactic tweaks, and a couple more cases - C++ references are safer than raw pointers, but not as syntactically/mentally intense to use as pointers with lifetimes; also there's the whole issue of traits vs duck-typing , which isn't about correctness 
&gt; to make it better for quick experimentation would allow things that make large applications harder to manage. some compromise ideas- the haskell-esque 'dont repeat the types in the impl - once they've been defined in the trait' - and whole-program inference *jjust within a module* (e.g. your test functions can infer directly from the calls to the 'real' functions); &gt;&gt; "Yet your RFCs are not even at the level of many pre-pre-RFCs" you're nit-picking ... forget the beaurocracy/procedure, the blocking point is the fact the community doesn't like the idea, not 'how it's written up'. the prior art describing the idea is usually clear. 
There is a work on an avr backend in llvm now. People have programed rust on avr chips (Arduino). https://github.com/avr-rust/ https://github.com/rust-lang/rust/issues/44052 I also read good RFC for inline assembly and amid this month getting a lot of attention. So I would say people are working on.
Many people develop for multiple platforms. And if you have to use C/C++ for half your work it's simpler to just use it for everything rather than switching back and forth. It's the same as why bus companies maintain a lot of far-flung and late-night lines that will never make money. A lot of people riding the busy commuter lines that make money depend on one or other of those other lines. If they didn't exist they'd just use a car, and those busy lines would no longer be very busy, or very profitable.
Alas it's not really extendable to other effects, the way a more general effect system, like OCaml's would be. Would be really lovely to see a more comprehensive effect system for Rust - one that would tie together panics, console out, async io, results, rng, etc. Alas it might be a bit tricky given the time frame. Hopefully generators+associated type constructors+const generics can give us enough tools to retrofit something, but it might not be the prettiest. :/
However there is still the issue of having to maintain an AVR backend for LLVM -- who else would be using it and thus invest resources? This is not trivial. It it not _just_ more of the same, it is an entirely different problem of its own.
Completely agreed, Rust could bring a lot on the table for embedded development -- at least compared to C and C++. However, maintaining a backend for LLVM is non-trivial and would, in my opinion, only remove focus from solving the actual showstoppers at hand: making no_std effortless, working on non-nightly, having reliable library/platform definitions, support for remote debugging etc. etc.
&gt; And monads support composing multiple monads into a stack and lifting computations through these monad stacks (to support different kinds of side-effects combined). I would really hate to see MTL-style (ie. monad transformers) come to Rust, and all the clunkiness of juggling transformer stacks... Hoping that we can figure out a zero-cost version of row polymorphic effects, but it might be a few more years yet. It's a tricky nut to crack, and there's already a huge amount of Rust's ecosystem falling into place.
That way is already there. Address the team, pay someone for the maintenance, maybe find a way to do that out of Mozilla (basically: find someone with a bank account and management). That could also be your company. There's a couple of external engineers and consultancies on the Rust teams already, paid for specific jobs. If someone wants to form and fund an embedded group or team, we can happily provide. Integration into the wider project is possible. Feel free to approach either asquera or integer32 if they can manage such a thing under their umbrella.
looks like an awesome project, I can't wait to read the source code, will you notify us when it is published?
Thanks!
Thank you!
I like every post which mentions const generics as an important to-do. Crates like ndarray feel so awkward to use, compared with a dimensionality-generic version, it feels almost pointless to develop in this direction until the feature comes. I imagine async without async/await is in a similar spot, however, as far as I understand, the proposed async/await implementation is built explicitly to be compatible with futures-rs. Linalg and numerics, on the other hand, would have to be seriously redesigned, including multiple breaking changes to the APIs, if we want to make a proper use of the new lang features. One shouldn't think of it just in terms of a language feature - the const generics - being unstable. From my POV, it's more like the whole *linear algebra* is unstable. It feels a bit awkward to cry to rust-lang devs for features, since this is an open-source project and, theoretically, if you want it - go ahead and implement it. The issue is I (and probably many people with numerics background) have 0 competences to hack on the compiler.
The Unions 1.2 RFC is not merged, and the union 1.0 RFC implementation is not finished. So I wouldn't expect everything that is not currently covered by a passing rustc test to not work :/ Maybe check out the compiler tests and see if something there does what you want and how?
You can't match on a union, because there's no way to tell which field of a union is set. If you want a *tagged union* you should be using a normal rust enum.
Let's say we assume for a minute that in the eyes of most Rust users, some of your suggestions (e.g. C++ style "better-than-raw-pointers-but-a-far-cry-from-memory-safe" references) would fundamentally break the premise of the language, or at least make it materially worse for them (harder to reason about code they're reading, because there are fewer guarantees provided in the language). A lot of what remains, e.g. global inference for the sake of smashing out code quickly, could be served really well by IDE features. And some other ideas, e.g. your preferred syntax tweaks, could be implemented in crates, especially when the full "Macros 2.0" story stabilises. These seem to me to be the ideal places to implement features that most users don't want to have to worry about, but some do. Or it may be that another language would better suit your needs for some of your projects (e.g. Go embraces static duck typing) and that better FFI in Rust would help you reuse code between the two. What I'm getting at is that there are likely more realistic options to meeting your needs than trying to convince the Rust team/community to go back against very deliberate design choices in Rust to help your use cases. And some of them may still involve changes to Rust ‚Äî just not necessarily the ones you have in mind right now.
&gt; So if you are a newbie impl Trait just works. If you are a newbie, nothing "just works", you have to read the book and learn the one idiomatic way to express it, and it doesn't help that we then have two ways, which way should one choose? This will lead to more confusion! It's very inelegant to have two non-orthogonal ways to do the same thing. Newbies have to learn about type params anyway so what's the problem? And who would write `impl Trait` for args by chance, to see if it would work? I never did when I was learning Rust. Instead I read the book and memorized the syntax while noticing that it's similar to C++. Why can't newbies nowadays be expected to read the book?
Or we compile PureScript to Rust and allow writing PureScript bindings to Rust crates.
&gt;&gt; "would fundamentally break the premise of the language" I never said I wanted them *everywhere*, or by default. Please dont assume I'm *so* stupid. There is a middle ground that is simply missing in rust at the moment. .. you have the extra markup of safe, or deliberate extra verbosity in unsafe (like they want to discourage people *beyond* merely having to write unsafe{}) &gt; Or it may be that another language would better suit your needs for some of your projects (e.g. Go embraces static duck typing) and that better FFI in Rust would help you reuse code between the two. sure I was impressed by Go and even the experience of using JS , in their correct contexts; The question I found myself asking was.. "well why not just stick with C++ for engines, and use these other languages for tools/UI" ... which is the status quo. But it seems the potential is there to do better - and I've put invested time here.. forcing myself to get over the hurdles, thinking as the language grows it'll be worth it. There's nothing I want that can't be non-destructively retrofitted to Rust.. whereas changing C++ at this late stage is far more difficult (getting rid of headers etc.. ). I do fundamentally like how most of Rusts syntax-space is used, immutable by default, the whole idea of the unsafe block, having a decent lambda syntax, etc..
Huh, TIL about `catch`. I've been using a macro with the same name to wrap a block in a closure and accomplish essentially the same thing: ``` macro_rules! catch { ( move $b:block ) =&gt; { (move || $b)() }; ( $b:block ) =&gt; { (|| $b)() }; } ``` It works pretty well, though I have no idea if there's a performance penalty using this method.
&gt;That's actually not true. It is actually true. I never said a C++ reference was safe. I said it was *safer than a raw pointer*. It communicates more than a raw pointer, you have to do more to break it.. as such it *is* a step forward, whilst not being *as* restrictive to use (e.g. we need extra abstractions to do some safe things like indexing 2 parts of an array in rust, 'split_at_mut' etc..) &gt;&gt; "Well, error messages of duck-typed templates are such a pleasure to work with. " but we'll have the best of both. There ARE situations where the duck typing is simpler. Some of the things with intense use of computed types get out of hand when you try to do them in rust - the complexity of the trait-bounds. &gt;&gt; "Alone these two points are huge benefits of Rust compared to C++." there's plenty of things I like about rust, do often I list them in my critiques. It's just I find myself doubting that attempting a switch was worthwhile, in the context of feedback over the past few years. &gt;&gt;"and I really don't hate C++, I even like it for its own properties." well I keep using it's features over C, I do like the fact you can pick and choose a subset. I *do* hate certain things about it though. e.g. back in C it's easier to make a header generator. The way classes and headers interact is horrible. If C++ got UFCS I'd be a lot happier.
You may be interested in rust-update script in clippy repository. e.g. ```shell #!/bin/sh export RUSTUP_HOME=$HOME/.rustup-attempt test -d $RUSTUP_HOME || (rustup toolchain add nightly &amp;&amp; rustup default nightly) &amp;&gt;/dev/null rustup update &amp;&gt;/dev/null rustup component add rustfmt-preview rls-preview &amp;&gt;/dev/null if [ $? -ne 0 ]; then &gt;&amp;2 echo "rls or rustfmt not found" exit 1 fi unset RUSTUP_HOME export RUSTUP_HOME rustup update ```
If it is that straightforward then it makes sense -- exactly because of the potential ecosystem.
Also, this lacking should be less frequent after `rustfmt 0.3.5-nightly`, since `rustfmt` will start using `rustc-ap-syntax` crate over `libsyntax` tied with the compiler.
There's a simple fix for that. Don't update the compiler, pin its commit.
&gt; How is that simplifying it for newbies to use the same syntax for both? Newbies don't know the difference between existentially and universally quantified types. They can just use `impl Trait` for generics in many simple cases, and learn the more powerful syntax when they need to. Anyways, making your points here doesn't lead anywhere. There is an RFC thread in the repo where this is being discussed (not merged yet IIRC). The people in favour have made better arguments than the people against. I'd recommend you to read the whole RFC and the discussion from the beginning before you comment because what you are saying has already be said, and most of the people against have failed to make new points, but have constantly repeated points that have already been made and debunked. 
&gt; every large open-source project will have a style-guide to enforce one style We have `rustfmt` which can let you choose which style your project should follow. So all of this can happen automatically. 
&gt; I said it was safer than a raw pointer. It communicates more than a raw pointer, you have to do more to break it. I don't believe this at all. If you've broken a raw pointer, it's because somebody has dereferenced an uninitialized, or explicitly nulled or deleted pointer. References, though, can appear, break themselves and disappear without anyone even knowing they're there. It's crazy how frequently temporary references end up being the butt of yet another gotcha. 
But reference can still leak memory and you can get the same ffect with rust already with rc and arc. 
You have c style refs already. Called rc and arc. Use them to your heart's content but you can trigger runtime errors by mutably borrowing twice. You can't both have the big benefit of rust and then not have it.
I'll give you a big one: doc comments are impossible to i18n.
&gt; so that I can set a flag in a struct depending on the type of the variable that is being used from the union. Is there any reason you're not using an enum instead?
&gt; But reference can still leak memory and, yes, but this isn't black and white. there are shades of grey. it's like asking if our taxes/public services should be 100% or 0% (communism or uncap) without having the options of 30%, 50% etc to consider. a C++ reference is dangerous but *less* dangerous than a raw pointer, and less verbose than anything in rust. 
Re: your last paragraph That's what I've always intensely disliked about the "fork it and do it yourself" attitude that pervades OSS. We're an ecosystem of diverse specialists and generalists, making tools to serve each other by teaching the computer how to carry our skills so we don't have to show up in person. "If you want this language feature, add it yourself" is a dangerous concept and I'm glad Ruat tries to avoid it. The compiler and Lang teams are heavily specialized in writing the Rust compiler, and imo it's their *job* to implement features for which we ask for us, so that we in turn can use their work to do our work that they can't do. (In exchange, it's our job to follow their structure on requesting features and try to help to the best of our abilities, but that can mean as little as writing a clean RFC and not getting unpleasant on obstacles.) That's how healthy ecosystems work ‚Äì "fork it and do it yourself; I can't be bothered" is the opposite. But yeah I'm definitely waiting on const generics something fierce as well, plus better handling of unsized types.
Just cause quacks like a duck doesn't mean it's a duck. Duck typing is unsound.
Did you write a RFC for this proposal?
Started writing [nanowasm](https://github.com/icefoxen/nanowasm/), a small standalone WebAssembly interpreter. I hope for it to be a nice tool for playing with WebAssembly code, as well as a library for embedding wasm programs into Rust. ...but really I wanted to learn about WebAssembly and I like writing interpreters. Shout-out to the [parity-wasm](https://crates.io/crates/parity-wasm) crate, without which I probably wouldn't be bothering.
Great resource, especially going through the steps of debugging (btw: to make bugs like that easier to find, I always delay adding the catch-all entry point rule until the rest of the macro is completely finished). The refreshed `trace_macro` feature and new `compile_error!` are really helpful here (before that we used void enums, uphill both ways, etc). This could be a chapter in [TLBoRM](https://danielkeep.github.io/tlborm/book/index.html)!
given the existing verbosity is deliberate (I followed the language as verbosity was added) .. I'm doubtful it would be accepted. there's more ideas like a ```'temp``` lifetime (perhaps controversial because it's not strictly a lifetime, rather a modifier of the pointer to be substituted with a context sensitive shortest-possible-lifetime.. 'no escaping' .. ).. there's ways to do that already but it's fiddlier (the 'for&lt;&gt;' notation which means making symbols in each context, and having to touch 2 locations in the source to do it)
You don't have to do *anything* (explicitly) with references to break them.
Fundamentally, it is about breaking a larger task into a bunch of smaller tasks that can occur independently. Then, putting them in an in-box and then allowing your pool of workers to grab the next task from the in-box and perform the task and put it in the out-box. You might have 1, 2, 3, 4 or more workers (CPU Threads) available and the number of individual tasks that need completed might number in the multitudes. When all the tasks are done and in the "out-box" you can then call the over-all work "done".
I haven't tracked related RFCs, so I would like to ask some questions regarding generators. Do we have plans for integrating generators with `for` loops? Python folks naturally will expect to be able to write something like: for value in my_generator() { // .. } Maybe generators could also make the following pattern more natural, which can be useful e.g. for data processing: // Raise error on first error, process yielded values in the loop, // `ok_value` is unwrapped return generator result, can be skipped // if generator returns `Result&lt;(), E&gt;` let ok_value = for value in my_generator()? { // .. } Currently we have to write a bit of unpleasant boilerplate.
I think no, because generators have a return value as well, so it's a bit weird. But we could have an explicit `GeneratorIter()` adaptor. You're right that that syntax would work but that would lead to two kinds of iterators. We could genericify all iterators to be generators. but that might be weird too and also would be hard to make backwards-compatible wrt the traits involved. FWIW you can do `while let GeneratorResult::Yield(foo) = generator.resume()` which isn't too bad.
(yes, the blog post linked to above is an attempt to answer this question; I'm not asking the question)
[removed]
Definitely "Food for thought"!
We could do desugaring of `for` loops for both `Generator` and `Iterator`, with prioritization of `Iterator` if type (for some reason) implements both traits. Or we could forbid implementation of both traits (by `Generator: !Iterator` bound perhaps?). But, yeah, those solutions feel a bit dirty.
What exactly would you expect that to do?
I haven't used PureScript too much, so I can't say for sure, but my intuition says "yes, probably". This crate‚Äîor one like it‚Äîmight make writing your *compiler* a easier, in that it allows you store and access previous states. I haven't implemented such a sophisticated compiler, so I can't really comment on that.
The generator is similar to a closure in that it just turns the whole thing into an anonymous type that stores everything it needs. So initially it's just all stored on the stack. However when you treat it as a type with a Future implementation and spawn it into an executor (like Tokio's reactor), then it's up to that executor on what it does with that generic type. Most executors will just turn it (and I'm pretty sure Tokio's does too) into a heap allocated trait object (Arc&lt;Future&lt;...&gt;&gt;) that it can then use to queue up with the other futures.
What is the cost of this feature (async/await)? In .Net/C#, that is not free, it creates a state machine which more memory/CPU.
This is what [`take_mut`](https://crates.io/crates/take_mut) does and it indeed aborts on panic.
Why use Electron at all if WebGL will be used for rendering?
This can't be done because Deref::Target is an associated type.
The state machine is stack allocated here (usually). Pretty cheap / zero-cost (i.e. you can't do it with less cost manually) Tokio probably heap allocates it, but most lightweight thread impls need to heap allocate _somewhere_
The crate already supports `DerefMut`
I personally don't like this pattern (I think it's even considered an anti-pattern in some Rust guidelines). Why not just use `.0` which is more obvious when reading the code? The fact that you'll have auto-coercion will allow you to silently bypass the type safety a newtype wrapper provides.
I'm still really confused by what you're asking for exactly. I figured if you'd written up an RFC, it'd be a bit clearer.
Yeah, it's possible, but tricky.
Oops! Sorry 'bout that. Just saw a question.
this thread is just high level discussion; there's other posts describing my ideas in more detail (some in the discourse forum). Gauging feedback I figure it's probably not worth me putting all my ideas into RFCs
Can you put up some links to those detailed discussions?
Not gonna lie, I'm not excited about another text editor based on Electron.
Does one need the whole Electron for that ?
This seems like a bit of a strange beast to me. Many of the design decisions are similar to xi. Those that are different (such as the choice of Electron as the primary GUI) are "implementation details", and of course there's an Electron front-end for xi. There's no obligation to contribute to existing projects rather than NIH'ing new ones, but I don't yet understand the case for this one.
&gt; A crucial difference is that Tokio is single threaded, whereas the Go scheduler can use multiple OS threads for execution. So anything based on Tokio will be single-threaded by default?
Yep. I mean, rust is single threaded by default too :)
I think `impl Trait` always means "something implements Trait".
My understanding is that, since it can know the "stack" at compile-time, there's one single allocation, at startup, with the whole size of the stack.
Yep, pretty much. Sufficiently recursive futures will probably heap allocate subfutures.
We can probably learn a lot async/await stuff from the discussions going on in TC39 -- e.g. https://github.com/tc39/proposal-async-iteration
You can implement `IntoIterator` for generators (not too hard to do), but it has to be opinionated about what to do with the return value, so I don't think we'll see such an implementation in std. We could also desugar the for loop to handle `Generator`, but there's still the question of the return value.
Yeah, but is there a way to have Tokio behave like the Go runtime and distribute the work over multiple CPU threads?
for await exists in Rust! `#[async] for value in stream {}` It's just that this is different from iterating over a generator; because this will yield back control on each iteration whereas for iterating over a generator you just want to get values till it's done.
Not currently. I believe there are some challenges at the mio level, I saw an issue about this.
I'm no fan of Electron myself (or many other JS related things), but I still find this exciting. They are using Rust for the core instead of C++ or Go or Swift! What is even more exciting is that this is by the Atom team (notice the atom github org)... maybe one day we'll see Atom switching towards Xray.
I see. I take it's being worked on?
Not at all.
Is there an ongoing bikeshed thread about (ab)using `?` for all things await? I want to participate :)
_shrug_ last I checked it didn't seem to be high priority, because you can offload heavy work to other threads anyway. Doesn't solve the issue completely, but it helps.
Oh, I didn't notice. Yeah I guess it's noteworthy.
I don't think so :) Await is kinda different but the connection makes sense. Though an async function already can use ? to mean the normal thing so that may get confusing.
Here: https://github.com/tokio-rs/tokio-core/issues/127 Running futures on a threadpool is supported, but the IO event loop is single threaded only.
Yeah I guess the problem is that we'd need to overload `?` to work for "yield generator" and "return error". I'm pretty sure in &gt;50% of the cases you don't want to deal with either, though.
IMO the return value should be the value of the `for` expression.
Mh, yeah, that would make sense.
What's the timeline for stabilizing generators? The tracking issue doesn't have a lot of commentary.
Yeah after giving it some thought this sounds like the most logical solution, and it ties everything up together. Worth writing an RFC about once generators are stabilized.
No idea. /u/desiringmachines has some neat ideas for alternate syntax which may relate with their proposals for throw/catch sugar, so it at least is waiting for that. But also waiting on folks to use it more I think.
it is really common to do this when exporting a [newtype](https://github.com/rust-unofficial/patterns/blob/master/patterns/newtype.md). This makes the newtype effectively implement all the same methods as the original type (although I'm not sure how it affects whether it implements the same traits... I don't think so.)
I guess another reason is that they want to render some GUI components in HTML... The general question "Does one need the whole Electron for that" applies to many apps that use electron... often apps don't need one set of features that electron offers yet they always ship with the full electron copy because JavaScript is a dynamic language, you never know which features you really don't need.
Can someone explain why some platforms _aren't_ supported? Doesn't rust compile to LLVM which compiles down to metal on its own, so every platform with LLVM support should automatically work with rust?
Quick Poll: If you (all replies welcome) could pick only one to promote, which one would it be?
Here's the steps: 1. internals post 2. experimental RFC accepted 3. implementation and experimentation in nightly **WE ARE HERE** 4. Write a "real" RFC 5. Accept RFC 6. Stabilize Too early to lay out a true timeline here, IMO.
Its my color scheme - using base16-eighties generated by a script and it seems the bright colors are messed up. Switched to https://github.com/taniarascia/new-moon for now, looks good [new moon](https://i.imgur.com/s0UUWVz.png) 
Rust isn't exactly a functional language, though it can be written in a functional style if you want to. Immutable types aren't really idiomatic from what I've seen, but the borrow checker makes working with mutable state a lot less painful, and you can provide immutability guarantees where needed simply by passing around immutable references.
Not at all. It was an assumption about a distance piece of code in the same codebase.
Very nice explanation, thank you! I think the first (mainstream at least) language with async/await was C# and the mechanism has recently been introduced into javascript..
I love this post / article. Thank you!
In "modern" C++, raw pointers are also non-owning. Yes, "it's not supposed to be null" is useful to know, but it's much more sensible to just make a `nonnull&lt;T&gt;` type that doesn't have all of the gotchas of a reference and "not null" doesn't buy you all that much in C++ anyway since basically every type has multiple illegal states. I know what you're saying, I just *strongly* disagree. Hidden, unchecked lifetime relations are insanely dangerous, and C++ would be a heck of a lot easier to use safely without them.
There might be a small penalty in debug mode, but compiling in release mode should inline all those.
As far as I know the problem with promoting Tier 2 is simply one of limited resources. That is, a sponsor needs to step up and make appropriate servers available.
Ahh... just another example of the tunnel vision that makes programming anything other than safe Rust while tired an exercise in overconfidence. (I wrote that around 2AM local time, just before I went to bed.)
It still won't work in 1.2. Unions explicitly do not know what type they have internally, so you will never be able to match over them like this. Whenever unions are used, the user needs to know the type that is currently stored in the union and access with it with the right accessor. The fact that they do not know is the difference between an union and an enum, and the reason unions need unsafe. To do what you want to do, you can either change the union to an enum, or you can store a bit somewhere and use that to decide which accessor to use.
So, I'm thinking about whether or not generators are a suitable alternative to do-notation, so I tried implementing Haskell's `Maybe`+do-notation with generators and `Option`: https://play.rust-lang.org/?gist=d83c47e149ab0e6bf49071e6ee8dfa4a&amp;version=nightly (yes, I know, it's easier to use the `?` operator - I'm trying to make a point here) Generators are similar to do-notation in the sense that in both the caller is controlling the execution of the callee. Generators, however, are weaker than do-notation in 3 ways: 1. The monads in do-notation accept a value - with generators we can't pass an external value from the caller to the callee via the `yield`. 2. A generator is stateful - and once you resume it you lose the state. Monads are immutable - you can call them multiple times to advance the do-notation from the same place. 3. A do-notation can "yield" different types, and the graph of possible "yielded types" is encapsulated into it's type. A generator must always yield the same type. I managed to work around the first limitation in my `option_do`, but it was quite hacky. Still - generators are far from stable and if this limitation will prove to be an actual problem their API can be changed to support passing values to the `yield`. As for the second problem - with Rust's ownership system it should be possible to `impl Clone` on generators. This is not without it's complications, of course, and I'm not sure if this limitation is an actual problem. The third limitation is the hardest - and probably the most interesting.
To begin with, production-quality C wrappers should suffice. Communities will evolve and come up with competing pure-Rust impls later. It‚Äôs more about availability of usable and documented packages for a variety of technologies at this point in time. I totally realize that the core team must be quite busy with a road full of challenges. They‚Äôve already been doing am amazing job. I wanted to bring to their notice the lack of necessary libs which may be hindering Rust adoption elsewhere like my scenario. 
Racket is great language for beginners! You could also try Elixir(https://elixir-lang.org/ which will give you great insight with concurrency). Also I have to say that Scala is a big language but there are excellent resources(and a really nice community like Rust) that help you learn the language. 
One of the Chef Habitat guys was talking about how nice it'd be to get x86_64-sun-solaris working properly while we were at Rust Belt Rust. Which is also one I'd like to see (as well as x86_64-unknown-freebsd), but for myself it's purely a hobby project at this point and nothing production.
&gt; The api will simple with few database operations and business logic [‚Ä¶] business logic is quite complex [‚Ä¶] application will be huge Errrr, okay? Did you know that SQL is a functional programming language as well? I'm personally a fan of moving as much logic as possible to the database (views, custom functions, triggers). I'm also surprised you don't have Erlang/Elixir in that list. &gt; We are currently serving our api in nodejs. How's that going for you? If rewriting this is a big endeavor, maybe sticking with Node but using TypeScript and restrictive lints are good a way forward?
Thanks for checking!
&gt;Yet Numpy for Python, Tensorflow for Python/C++, R, Julia and Matlab do not have compile-time bounds-checks, and have been massively successful. Let me make two assertions about those languages: * I don't know about c++ tensorflow bindings, but all other languages you've mentioned are interpreted. So, they can't have compile-time bounds check by definition - I am not just nitpicking, this is important because the compilation phase can help find bugs. * The other thing common to those languages is *very* overloaded syntax, which means you often do not what you think. Its not instantly obvious what A @ B.T does in python, when A is 4-dimensional and B is 3-dimensional. Broadcasting is very cool for conciseness, but can result in bugs that are very hard to track down, if your code should do element-wise matrix multiplication, you (mistakenly) pass in a vector, but it gets broadcasted anyway, and silently produces nonsensical numbers. Those problems can be largely resolved by a good type system, instead of a big test suite. Emulating Numpy experience in Rust isn't an exciting prospect for me, whereas statical dimensional analysis would save me quite some time if I had it in my past projects. So I agree SIMD is important for performance, and I agree performance is important for numerics, but it's not performance what makes me want const generics in Rust. The mottos of Rust are (amongst others): performance, type-safety, explicitness. These are also very useful in scientific computing, and it's where Rust can make a difference. I think my point is similar to what I have seen in comparisons of Tensorflow and PyTorch: Tensorflow complains in your with tf.Session() as session: code about errors you made somewhere else, while specifying the dataflow graph. PyTorch is an improvement, because it throws at the line the error happens. But static analysis goes a step further: it shows you where you *make* the error, not merely where the runtime ran out of ideas how to interpret your code and gave up.
You can run multiple event loops, like one per core. Fpr something like a webserver, that's just fine, they can all listen/accept on the same socket if it was opened with the O_REUSEPORT flag and the kernel will loadbalance the connections over the eventloop-threads. (O_REUSEPORT - see https://lwn.net/Articles/542629/ )
&gt; I know what you're saying, I just strongly disagree. no, you're arguing for the sake of arguing. I never said they were *safe*. I know Rust is 'safest'. I said they were *safer*, whilst *not being as verbose*. They're another point of compromise , and as such sometimes they're actually more pleasant to use (there's the possibility of keeping them inside a class boundary, etc.) Writing ```nonnull&lt;T&gt;``` has a cost, over writing *T. Sometimes *T is a nicer tradeoff.
&gt; no, you're arguing for the sake of arguing. Please don't go this route.
Thanks for the details on Tokio and AsyncIO u/Manishearth. It clarifies lot of things. Isn't the [May](https://github.com/Xudong-Huang/may) stackful coroutine essentially does the similar to what you describe? In fact it supports multi-core as well. Any pros/cons with this approach compared to `Tokio`?
A state machine doesn't automatically mean "more memory/CPU." Normal code is already full of state machines. The cost in C# comes from allocating the storage for the state machine, and from the need to save/load its state across suspension points. --- In both C# and Rust, the storage for a state machine is essentially just the stack frame, turned into a struct. However, in C#, a lot of locals are allocated on the heap, particularly nested state machines. In Rust, they can all stay in a single block of memory given an anonymous type, much like a Rust closure. (Except in the case of recursion, which must deal with the same issue as recursive data structures.) This anonymously-typed block of data can be allocated on the stack, just like a Rust closure. However, an event loop like Tokio is going to be dealing with some dynamic and unbounded number of them at once, so they'll be allocated in some kind of collection on the heap. This is still much cheaper than a collection of thread handles tied to 8MB stacks. --- Saving and loading state across suspension points is very similar to saving and loading registers across function calls. In either case, the data the function/generator is working with is written to the stack/block-of-data and then read back afterwards. This cost is really tiny, though there are a couple of optimizations that function calls can do that generator suspension can't. First, when function calls are inlined the compiler can skip the saving/loading and just throw all the state into the same run of the register allocator. Nested generators can also be inlined, but top-level generator suspension can't because the event loop has to dispatch them dynamically. Second, calling conventions have rules about "caller-saved" and "callee-saved" registers. Functions are allowed to scribble over caller-saved registers but must save and restore callee-saved registers to use them. So, if a caller has data in a callee-saved register and its callee never uses it, it never needs to be saved/restored. Generators instead have to put everything in their state block because they may be resumed from an entirely different context. --- Overall the cost of the state machine approach is pretty minimal, definitely cheaper than N:N or M:N threads. The one way that they can be more expensive is recursive generators, which wind up looking like segmented stacks with each frame allocated separately.
What I would really like is a solid actor library, like Akka, built on top of Tokio. I know there have been several attempts at creating actor libraries. It would be nice to have one that becomes the standard and develops an active community.
There is a world of difference between "If you want this language feature, add it yourself" and "If you want this language feature, add it yourself, **and we'll help you.**" where I feel the Rust community is more in the latter.
I like the idea of turning all iterators into generators. I think it could be done backwards-compatibly if we just treated `Iterator` as a `Generator` that yields `Item` and returns `()`. That would also solve the problem of how to give `for` loops a value in expressions, I think? All existing `for` loops work with `Iterator`s which all "return" `()`, and all existing `break`s in `for` loops also "return" `()`. So, `for` loops with `break value` just need to unify the type of `value` with the return type of their `Generator`, with no need to introduce `for..else`. Maybe that is too weird though. :)
you already did
Can you describe what game dev libraries you want to see?
I feel like I'm getting red flags here. You seem to have dived into this paradigm first and it doesn't sound like a small side project for learning. You or your team have people new to functional programming; why force it? Functional Programming is not the be all and end all of maintainable code. Look at how you can optimise or improve what is already there (there are plenty of tools to lock down JS into something more sane) or use Golang or something. It's a REST API serving 3-500 requests per second, you don't need anything too out there.
You can't implement traits on other traits, and you can't implement foreign traits *generically* (`impl&lt;T: Common trait&gt; Display for T`). Trait coherency rules are deliberately strict in order to prevent potential downstream collisions. I've been hitting this issue a bit recently and tbh I just use a macro to generate the Display impl
That's pretty neat to use a macro, but what are you generating specifically? Are you implementing Display for the Trait and then having the structs that implement that Trait provide a method for the Display implementation (on the Trait) to call?
I've been super impressed with [yew](https://github.com/DenisKolodin/yew) and now that it has components merged in I want to start messing around with that. Since I don't really have any projects that I want to do right now that involve web development I am going to start working on porting [semantic-ui](https://semantic-ui.com/) to yew. I figure that if we're going to ever get the wasm/rust combo to be a solid solution for web development then we need some component libraries. I've worked on the React port of semantic-ui so I figure it shouldn't be too hard. Obviously the alpha status of yew means that this won't be ready for production use anytime soon and will liekyl change to match.
It is also pretty new in Python...
Updated - thanks.
Note that this repo is owned by the Atom team. If you still want JS plugins that use HTML, then yeah, you do need Electron.
Personally, I want to see some maturity here. Hit any pure rust gaming library and you're likely to see something like &gt; X is undergoing a lot of changes at the moment Not that it is a bad thing that these libraries are taking their time to hammer out a good API, but at the moment it doesn't feel like you can rely on them. You end up looking instead at just using SDL2 bindings, which is less than ideal. But further, the forest of available libraries is a little daunting to navigate here. Should I use gfx, vulcano, glutin? And then, the documentation, just isn't really there for these game libraries. Again, they are changing fast right now but at the same time you are somewhat left in the dust if you want to give any of them a try.
My personal choice would be for the FreeBSD target to be promoted, but I believe the most beneficial target would be wasm32. Rust has the opportunity to become the default language for high-performance in-browser code. Its combination of sane package management, focus on enabling performance, and grok-ability put it far ahead of any other language I can think of for wasm development. I think that if rust became the default in that domain it would help guarantee the continued relevancy of the language.
&gt; Should I use gfx, vulcano, glutin? As I understand: use vulkano if you target only modern enough platforms and/or don't care about users without Vulkan support, glutin for legacy OpenGL (probably not the best idea for from the scratch project), gfx if you want to abstract over several APIs, which sounds neat, but can have non-obvious costs. Personally I've used vulkano for my small point cloud visualization project and was more or less happy with it. (although I would prefer if API have used passing of &amp;Arc&lt;T&gt; and implicit cloning inside methods instead of explicit cloning everywhere) If in future we'll get SPIR-V target it would be a blessing!
I mean, not necessarily using this crate in the compiler code but in the generated code, since PureScript would otherwise clone for every mutation if implemented naively. E. g. list operations would clone the whole list.. So by using this crate this overhead can be eliminated, right? 
awesome, i've actually been working on [something similar](https://github.com/jawm/jumpjet) for the past few months, mostly just to learn rust, and because, like you say, WebAssembly seems like a great platform for running 3rd party code in a rust program, for example a plugin system, or smart contracts on blockchains :) I've only recently gotten to the point where it can parse full wasm files (haven't tested it much tho) so there's a long way to go yet. I too, have taken the occasional peek at parity-wasm and various other repos when the official docs left me confused :P
Sure, if an architecture is supported by LLVM, then one can compile #![no_std] code for it. AVR was brought up precisely because its LLVM support is still a WIP, but it's pretty close to being complete from what I understand.
The part they mention only at the end of the gfx sales pitch is that they don't yet have a way to abstract shaders over different backends :/
I see, and `::item` is common to each struct implementing Typename?
It's part of my trait, which is why I have &lt;Typename **as Trait**&gt; first. Using the turbofish there allows us to clearly disambiguate exactly what is meant, and means Typename can have an item with the same name that doesn't collide.
Ok I'm sorry in advance, I'm very new to Rust. So the turbofish is the `&lt;&gt;::` syntax? And can you explain what `&lt;Typename as Trait&gt;` is actually doing? `Typename` is a struct or a trait? If this is covered in documentation I must've missed it, I read the whole book but didn't encounter this.
We don't have for loops with break value, because there's nothing that can be done in case of an empty loop. Need a for else block for this.
Weeeell, there is `StructTask&lt;T&gt;`, if it's an `await` that you may have reason to believe that it will return quickly enough that it's not worth paying the cost of the state machine (and being a struct, will also avoid the cost of heap allocation and GC deallocation, as it will be stack allocated), but yeah, otherwise this is certainly the case (the default `Task&lt;T&gt;`) and this alternative is pretty new
Yes. More my point is that the epoll/os handoff is really fast. It is unlikely that mechanism would be CPU bound.
Rust requires that generic code have a known, deterministic, final form. Generally, it can figure things out by itself, but sometimes it can't, and requires that we explicitly tell it what types are going on and how they're being used. If you have: struct SomeStruct; impl SomeStruct { const ASSOCIATED: &amp;'static str = "Struct inherent"; } trait Marker { const ASSOCIATED: &amp;'static str = "Trait inherent"; } impl Marker for SomeStruct {} then you have created a type (`struct SomeStruct`) which has some data associated with it. I am using constants instead of functions solely because it's less to type; you can have inherent (`impl SomeStruct`) and trait (`impl SomeTrait for SomeStruct`) methods/functions that share names just like you can functions. But what is the value of `SomeStruct::ASSOCIATED`? Traits are transparent when invoked. This is one of the two moments when the "turbofish" syntax appears. We can inform the compiler to use the struct type directly, or use its trait implementation, by casting the type: `&lt;SomeStruct as SomeStruct&gt;` or `&lt;SomeStruct as Marker&gt;` instruct the compiler how to resolve the `::ASSOCIATED` constant, which exists on `SomeStruct` in two different places. The other place you find the turbofish is when you have a function that returns a generic result, and you must collect it into a single known type. let group = some_iterator().collect(); What is `group`? The `.collect()` method can return all kinds of collected results. let group: Vec&lt;_&gt; = some_iter().collect(); is one means of specifying the type (`collect()` must emit some kind of `Vec`, and the inner type will be filled in by the type of `some_iter()`), but you can also do let group = some_iter().collect::&lt;Vec&lt;_&gt;&gt;(); or this (where we explicitly tell `Vec::new` what kind of `Vec` to build) let mut group = Vec::new::&lt;f64&gt;(); some_iter().for_each(|f| group.push(f)); The `::&lt;&gt;` or `&lt;&gt;::` sigils are the "turbofish" and it's just how you give the compiler more explicit and absolute knowledge of types at a specific moment in the code.
No, I understand that- I'm saying generators are an alternative solution without `else`. For example: let _: () = for x in iter { } // iterator returns (); loop returns () let _: () = for x in iter { ... break ... } // iterator returns (); loop breaks with () let _: () = for x in iter { ... break s ... } // ERROR: iterator's type R=() and break's type S don't unify let _: R = for x in gen { } // generator returns R; loop returns R let _: R = for x in gen { ... break r ... } // generator returns R; loop can break with R let _: R = for x in gen { ... break s ... } // ERROR: generator's type R and break's type S don't unify
Oh, yeah, I agree.
There definitely exist several attempts at actor libraries, but I'm a fan of [actix](https://github.com/actix/actix) at the moment filling that niche. It uses the Tokio event loop.
Unfortunately, this makes it harder to write than the equivalent Go code (and a naive implementation will be slower than Go is many cases). That's not a critical issue, but it kind of harm the marketing of Rust as a good tool for back-end stuff. 
Go was single threaded by default for years and did fine with it, I don't consider this a major issue.
Thank you! That was great!
Async/Await started as a Haskell library, was ported to F# (which also has monads) and from there to C# (which required compiler changes) If Rust stabilised this and the Rocket nightly features by year end it would be amazing. A decent showing on Techempower with clean code would get a lot of attention 
Definitely not a major issue. But having moved from Node to Go, getting back to actively thinking about not blocking the event-loop with too much CPU work seems like a downgrade.
Is your use case here just stability?
Quite unfortunately it looks like this feature will be [blocked](https://github.com/rust-lang/rfcs/pull/1148#issuecomment-358143130) on mutually exclusive traits (or some equivalent functionality).
Have you tried looking on [crates.io](https://crates.io)?
&gt; I always like to post this article when the subject of do-notation in Rust comes up: http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/ It goes into a lot more detail on why monads don't work well for our use case and how you could define an alternative framework that does. Interesting! I see it mentions effect handlers in Addendum 1. Would be super interested to see how that could apply to Rust. I want to be able to express effects in my type signatures, but I don't necessarily want Monads for that. Trouble is that the way languages like Koka do it isn't really compatible with the zero-cost abstractions we demand in systems programming.
Do closures have type-erasure semantics by default? Also does it make sense to assume type-erasure semantics implies less runtime overhead (since the run time doesn't need to also include the type information when data is being passed around. 
Rust promises memory safety in safe code, which includes no data races and no stale pointers. By adding C++ references to the language, you break that promise, unless you restrict the references to `unsafe` blocks, which instantly makes using them more verbose than Rust references with proper lifetimes.
The network poller (the epoll event loop, basically) in the Go runtime is single-threaded, it wakes up goroutines on other threads to do the work. This does not scale. I wrote an nntp proxy in Go that proxies about 10.000 nntp cmds/sec, throughput a few Gbit/s, and I had to run multiple Go processes - 1 per core - to let it scale. One of these days I'll RIR it :)
That's what Go does and it doesn't scale for network-heavy processing..
Then why not only allow the `&lt;T: Trait&gt;` syntax for args if code will have to be converted to that anyway?
This is probably a bug. Generators aren't really supported by NLL in the latest nightly-- I know of at least [one PR](https://github.com/rust-lang/rust/pull/47353) fixing some issues between the interaction of NLLs and generators, and Niko mentions in that PR that there is more work to do WRT properly handling `Suspend`.
But then how to provide the actual type for `bar` when calling `foo()`? fn foo&lt;S, T&gt;(t: T, bar: impl Bar, s: S) 
Maturity and a long-term vision. 
This is indeed a bug fixed with [this PR](https://github.com/rust-lang/rust/pull/45337). With that applied you get this error: error[E0626]: borrow may still be in use when generator yields --&gt; allowed-self-borrow.rs:19:22 | 19 | for i in &amp;v { | ^^ 20 | yield *i; | -------- possible yield occurs here error: aborting due to previous error
That looks interesting, I was considering using some kind of compiler plugin/macro to do some custom form of integration, but I wasn't quite sure how to proceed. I'll have to take some time to try to understand how this works, thanks!
That's sounds good, but doesn't work. I learnt a bit of PHP instead of Rails for a small Software Engineering project and the only takeaway was not using PHP ever again. I foresee that the students will get a similar experience between using C or Rust. While C is relevant now, it won't make much sense to keep using it even if it's still being used on active projects and it's not hard to find people backing it. What really matters at this point is that the language has a replacement that is about to be strictly better 'soon'. Existing codebases are not appealing enough to learn a language, and even if you need to, it won't be hard after learning Rust, it'll just make them hate the codebase a bit every segfault or so, after all imperative languages are pretty similar.
They aren't higher priority, they're just not something you can mock up easily and add later. You have to design the whole API with them in mind, or don't bother at all. SIMD and BLAS can be hidden behind function calls. Const generics cannot.
for fast calculations where precision isn't a priority
I'm using Rocket+diesel+postgres at my job for a single page app with a REST Json API and would recommend this "stack". If you don't want to use rustc-nightly in production, I'd recommend using iron instead of rocket. If you want to get into functional programming, I recommend learning PureScript and writing the frontend in it! The free [PureScript By Example](https://leanpub.com/purescript/read) book contains several practical projects for PureScript beginners. With [purescript-waterslide-rs](https://github.com/tomhoule/purescript-waterslide-rs) you can easily generate your PureScript types from your Rust types to keep them in sync (and in PureScript you don't have to write your Json encoders/decoders, which you'd have to do in Elm). PureScript is like Haskell but strict, not lazy. Or you could learn Elm but you won't learn much more if you already know Rust.. (E.g. it has no type classes so you end up writing a lot of boilerplate code.)
I think they might be planning on rendering the core text editor in WebGL but not all of the surrounding UI like menus, tabs, etc.
&gt; You can use async/await without using Tokio. For example, I think this would be useful for Servo‚Äôs networking stack. It doesn‚Äôt need to do much parallel I/O (not at the order of thousands of threads), so it can just use multiplexed OS threads. However, we‚Äôd still want to pool threads and pipeline data well, and async/await would help here. Does this mean no event loop at all? Or just a different event loop, perhaps one that uses multiple OS threads?
3\. A useable embedded scene
A different event loop. Currently we feed incoming messages to an event loop which spawns a thread to handle each message. These threads often spawn further threads, e.g. we have a different thread that does the actual fetch, for reasons. The second thread could be eliminated with futures and async, and the architecture could be pipelined better overall.
Meh. I'll stick to Sublime Text 3.
Just piling on, since I've had a nagging question about generators that I haven't found a good opportunity to ask it. In implementations of generators in other languages, yield can actually return a value enabling values to be passed both from the coroutine and also back to it (i.e. a `resume_with` method). I'm curious whether this was discussed for Rust and, if so, why it was decided to not have the Generator trait support this?
Yes, this has been discussed. Currently generators don't support this, but there are plans to make it work. The current generator impl I suspect is "just enough for futures to work well". (futures don't need yield-as-expression) 
Cool...thanks for the quick reply!
It looks like they don't even realize that Neon exists and instead wrote their own library for JS/Rust code called covalent.
One thing I don‚Äôt quite understand is why can‚Äôt we just grow the OS stack? Surely we can do this on x64 because virtual memory is abundant! You can start with one page of stack and allocate the next virtual page on demand. You‚Äôll need to make sure that you don‚Äôt map addresses below the stack, but that should be easy if address space is large. In fact, I suspect that something similar actually happens in practice for OSes which do overcommit. So why can‚Äôt an OS thread start as one page of memory and grow from there as needed?
Ah, I don't think I was clear enough (also I originally intended to write an explicit C# equivalent but then decided not to bother). The C# workflow is implicit from the comments in brackets. So the same but without any of the hunting for types/definitions bits, and using the debugger to investigate variables rather than print statements.
Could you elaborate the last point? I‚Äôm not quite getting it :-) is it that having more developers/users will make ‚Äúembedded‚Äù as a broad concept more relevant?
You're probably looking for /r/playrust
My thought is: if you add, say, AVR as a target, you'll get new users that develop for AVR. But you'll also get some new users from people that develop for both AVR and ARM that didn't want to use two separate languages for their different projects. With that said, the bulk of the responsibility - and work - should lie with the compiler backend developers. It's reasonable for Rust to try to target whatever backends LLVM offers; it would not be very reasonable for Rust to try to go it alone, I think. If AVR becomes a fully supported target architecture, then Rust can (and, I think, should) make the effort to support it but otherwise not. 
The siren song of performance is hard to ignore. Especially for less experienced developers. Sacrificing maintainability, readability, debugability and simplicity in the process. One of the 2018 threads had a point on survivor bias. Those that remain working with Tokio are those that stick with its mess and don't mind, care or they pretend it's not there. How many Rust/developer newbies abandoned the language altogether after trying to get something reasonable made with Tokio we don't know. Tokio evangelism is strong and there were lots of threads about Rust newbies being stuck on some Tokio issue. 
Yeah, you can do this. I think the idea is to allocate less than a page. Also, threads have other overhead, it's not just memory. I think with OS threads typically only the first page is physically allocated; and the rest will be allocated when you try accessing them. (but all of it is allocated up front beforehand)
I'm not sure if there's currently a way to say you're waiting for one of two IO actions, if that's what you're asking. Such an abstraction could be built, however.
AFAIK wasm target is nostd, so it could potentially force ecosystem to lean towards nostd as much as possible. Which is great!
&gt; if code will have to be converted to that anyway? Code doesn't have to be converted to that.
I don't know look up the RFC.
Hopefully a usable AVR backend would encourage other languages‚Äô teams and ecosystems to also work on this backend. In some sense it is possibly a lot like with traditionally statically compiled languages vs interpreters languages for moving to Rust. People working in C, C++ and Ada are probably mostly satisfied with their tools, but people working with interpreted languages see something particularly useful in Rust hence more switch. Now that Rust has gained a lot of tracktion, it is less one sided. The same might happen for the embedded ecosystem. In order to widely support embedded platforms you need to track PIC, AVR and many other architectures in addition to ARM. However, people who only deal with Arduinos have a lot to gain from switching and a probably more willing to switch to another language. However, I don‚Äôt see this as actually improving Rust for production use on embedded systems. But maybe it is just a few years off and I should just roll up my sleeves and chip in with some work ^^ 
I'm guessing what he means is Tokio right now is unmaintainable and a lot of projects are going in head first. Async/await is unstable and quite far from being stabilized. 
Well, I am finally working on my GUI framework [azul](https://github.com/maps4print/azul) (following the concepts I listed [here](https://codeburst.io/my-journey-with-rust-in-2017-the-good-the-bad-the-weird-f07aa918f4f8)). I am standing on the shoulders of giants here - (webrender, limn-layout and kuchiki (for html)). I've followed and contributed to limn, but the documentation is just so nonexistent and the styling so buggy that I can't really use it. So I just thought, why not make a proper GUI framework (Rust + CSS). What I've learned so far is that strongly-typed API don't work for GUIs - you need some kind of ID system / stringly typed APIs. So in the end, [this](https://gist.github.com/fschutt/f912bed7402df1ec05e5dd77891d876d) is roughly how I want the API to look like. A drawback is that every item has access to everything (the whole AppState), but I think in a strongly typed language, the effect is negligible, at least in my experience.
Interesting! Googling around revealed that goroutines start with a 2kb stack size, which is not that much less than your typical 4kb page. Is there any great benchmark comparing goroutines with Linux threads with small stacks? I read a loot that threads are **much** heavier weight than goroutines, but I am not entirely convinced by the arguments typically presented. One argument is about memory usage, but both models can start with small stacks and grow them on demand. Another argument is the cost of context switch, but you typically context switch on epoll, and that happens the same number of times in both systems. Perhaps I shouldn't have read that much of pcwalton's comments on HN? (-:
&gt; async/await gets rid of callback spaghetti And until that's on stable, and every async library supports it, we're just going to have to suffer callback-hell? &gt; type errors are reasonable That's just *wrong*. I'm not entirely stupid, and twice I ended up throwing my hands in the air and giving up writing extremely simple async-based UDP-servers using tokio, because the type-errors we're completely inscrutable, and the documentation helped me understand nothing. &gt; hell, most of the userbase of the Go language has this problem. I don't believe this to be true for a second, I believe people choose Go for a variety of reasons, and only a very small subset of them are because it helps them solve a C10K-problem. In what world do you live, where anything but a tiny fraction of web-apps need to scale to thousands of connections? In what world do you live, where scaling one piece of your infrastructure helps you scale anything else? Serving 10K HTTP clients is fun and all, but if you have the money to serve the same load on your database, you have the money to just throw hardware at it instead of development time. Async is a problem looking for a solution. And while it's looking it's making a lot of developers lives miserable. This makes me sad :(
&gt; Interesting! Googling around revealed that goroutines start with a 2kb stack size, which is not that much less than your typical 4kb page. Huh, in which case I was wrong :) But yeah, threads are quite good for most use cases.
&gt; And until that's on stable, and every async library supports it, we're just going to have to suffer callback-hell? Everything's being developed in lockstep, so we'd probably see this stuff stabilizing around the time Tokio gets finished.
I use this plugin, and it's brilliant. It changes Sublime into basically a Rust IDE, and has saved me so much time with inline error-checking.
off course, a lot of OpenGL and Vulkan related stuff, but this is AMD specific
I never suggested that a nightly version is labeled as official. I described how it works in many organisations when it comes to embedded work. Nobody will touch a "nightly" labeled compiler there, the risks and/or costs are too high. 
Reddit is written in python. Ruby on rails and php are the most common web stacks. They all have garbage performance.
Well, the current `futures-await` doesn't need yield-as-expression for its `-&gt; impl Future` and `-&gt; impl Stream` support, but if generators supported passing in arguments it might be possible to extend `#[async]` to also support functions returning `-&gt; impl Sink`. ([My first attempt](https://gist.github.com/Nemo157/092bc8ccd258ec07911ee2ee88ff2c47) at how this could work looks pretty horrible, but I'm sure this could be improved substantially).
Yeah. I suspect this is something folks wish to resolve before moving forward, but idk if there have been designs for it yet.
Immutable types are idiomatic. So much so that immutable is the default!
Can RLS suggest iter() now?
That's not how Haskell and friends use immutable types. In Haskell, immutable types are completely immutable at all times, and can be "changed" by creating a copy with the parts you need changed. I believe the compiler is smart enough to optimize that into in-memory modifications, too. I haven't seen the same approach used in Rust at all. 
Yeah, that's clearly a bug: https://play.rust-lang.org/?gist=ae6ddabbc8ddc411dcedd3fc48a8ee28&amp;version=nightly I will report it.
Hi, I want to use a 2D array to represent a table and this table gets passed around to various functions. I can't seem to pin down a clean way of doing this. Are my options: - Vec of Vec - ndarray (Don't want to use this) - Use Box to create my own table struct? Thank you.
Conceptually at least, I think it should be possible to replace the tokio executor with an OS-thread based executor, i.e. every task is executed on a blocking thread from a pool. At least that's what I understood from the answer to my question [here](https://github.com/alexcrichton/futures-rs/pull/455#issuecomment-298922234). It will be interesting to benchmark.
sweet. This sounds useful. I assume tokio allows tasks to be blocked on multiple IO events at once so this will just work in tokio.
Yes! I wasn't aware of yinz having a bunch of problems like described in the article though, is that true?
If the table is square, you can also use a 1d vec and do the math yourself, treating it like a 2d table.
&gt; since the run time doesn't need to also include the type information Rust doesn't really have a runtime, and even without erasure, there's no type information stored with stuff. Types are a compile-time construct.
&gt; If you are testing on Windows and WebRender makes rendering flicker horribly, worry not Kvark has landed a fix in WebRender that will make it in nightly soon. It's been fixed for me! At the work week late last year, I had this issue, but had gotten used to it, and so I just left it on. Some very strange looks from people when they noticed my UI blinking...
Mmh, idk how to respond to that. The thing is that Tasks are completely unrelated to how Tokio does its epoll loop. And that‚Äòs in my opinion the most problematic part about Tokio. Tokio‚Äòs reactor is both an executor for Futures, but also an epool loop. Both of these things are unrelated but mixed into the same type in an absolutely hidden way, that confuses everyone, completely failing at the Single Responsibility Principle. The executor part of the Tokio‚Äòs reactor is just that, an executor. It takes futures and executes them (by running them between the epoll calls). It can take any kind of future (even futures that have nothing to do with tokio) and vice versa, you don‚Äòt need to use tokio‚Äòs reactor at all. You can spawn those futures onto any kind of executor, it doesn‚Äòt need to be Tokio‚Äòs reactor. So that part of Tokio‚Äòs API is just a little nice gimmick I would say. However the other part of Tokio‚Äòs reactor is that is running an epoll loop and EventedFutures that you create have an implicit communication channel with the reactor they have been created with. So the important part here is that these futures can only ever make any progress if the event loop is running. They however completely communicate through this implicit communication channel and in no way need to be used with Tokio‚Äòs reactor at all. So all you need to ensure is that Tokio‚Äòs reactor is running in at least some way, and those futures automatically make progress then. So when you spawn a selection of 2 futures into the executor part of the reactor, then it doesn‚Äòt know anything about whether those futures implicitly communicate with the epoll part of the reactor at all.
I'm not sure I get what you mean here. If a future/task is blocked on IO, tokio will know not to poll it again until after epoll completes. The question is, does this apply to a selected pair of futures?
What does "usable" mean to you?
I've been using packed_struct to handle the packing of network protocol headers for a bit of experimental code I'm writing at work. Congrats on getting a version published!
&gt; (actually, it had happened in the past and continues to happen even now), I'd be interested in hearing more about this, if you wanna PM me. We try to treat Mozilla the same as any production user. There has been some stuff Servo has wanted for a really long time that we still haven't done, because other things are more important!
Let me just show an example control flow: - 2 EventedFutures get created, they establish a hidden communication channel with the reactor they have been created with - 1 KeyboardFuture gets created (let's say it communicates with a keyboard polling thread) - All 3 of them get selected together into a combined future - The combined future gets spawned into the executor part of the reactor. - The executor part of the reactor polls the future. The selection combinator polls all 3 sub futures. - They all notice they are not done yet, so they get a notifier handle. The 2 EventedFutures send this handle to the epoll part of the reactor, queuing up interest in being polled. The keyboard future somehow communicates that interest to its thread too. - Since we are now done with the future execution, the epoll part of the reactor starts running. It received interest of the 2 EventedFutures being polled, so it runs epoll with those 2 events. Once epoll unblocks, it takes the notifier of the associated EventedFuture and starts the notification. - The executor part now executes the associated task again, doing all this over and over until the Task finishes executing. The important part is that this communication with the epoll part and the notification back to the executor happened completely opaquely. The executor part of the reactor never knew about any of this at all. Just like it didn't know that it also communicated with some keyboard polling thread. That's quite nice, as this means you can use any kind of executor. You don't need the executor part of Tokio's reactor at all, the epoll loop (and the keyboard thread, ...) just automatically indirectly gets notified properly. However this also means that all of these semantics that I just told you about are completely hidden away from the API surface as the reactor is both an executor and the one that manages the epoll loop.
I've had this in my mental model for years now, but it's always hard to convert that deeply ingrained understanding into a clear explanation. There's a true art to that there! Thanks for writing this blog post for the benefit of others - I'll be sure to direct them over if I get any future questions on this. Thankfully there if a move to elide the `ref`s in the future. Can't remember where it was posted though - I think it was one of /u/aturon's.
Yes, slices are what I was thinking of (without realizing it). I'm trying to come up with a way of how to possibly use a slice as a 2D array passed to a function. I suppose Vec of Vec is the way to go then? I would rather avoid any calculations on indices to represent a 1D array as a 2D array. Thanks!
&gt; Nobody will touch a "nightly" labeled compiler there, the risks and/or costs are too high. Then those organizations will just have to wait till they can use stable rust. If they can't do right now because feature X is unstable, they should push for stabilization of that particular feature. 
Yeah, I guess some of this could be exposed.
Are you looking for [module-level doc comments](https://doc.rust-lang.org/book/first-edition/comments.html)? You can use `//!` like a normal doc comment, and it'll render at the module-level page. [Here's](https://github.com/AdamNiederer/faster/blob/master/src/lib.rs) an example, and its [output](https://docs.rs/faster/0.3.0/faster/).
This is interesting. I've been toying with the idea of using IPFS the storage layer for a database. Given what you've written here, I'm not sure its ready for that yet.. 
This is what I do already do with the `futures-cpupool`. Database and API calls called run on a futures thread pool.
I'm not certain that's true. I think AVR has the biggest support in the hobbyist embedded space, but I'd be surprised if it was true in the commercial space. I'd expect PIC to be the largest in that space based on what I've seen. Either way, having either platform officially supported by Rust would make sure that the ecosystem continues to support it. Part of the problem with some of these platforms is that they're &lt; 32-bit word architectures and have tiny amounts of RAM/onboard storage, so many assumptions made for x86_64 doesn't apply. It's important that any platform is supported so they all can eventually be supported.
In the second edition: https://doc.rust-lang.org/book/second-edition/ch14-02-publishing-to-crates-io.html#commenting-contained-items
&gt; Kvark fixed edge mask decoding in the shaders. The link there is malformed. Currently it is: &gt; https://github.com/servo/webrender/pull2254 But it needs to be: &gt; https://github.com/servo/webrender/pull/2254
I'd like to say that the editor I choose for Rust is almost entirely based on the plugin support. I have been using VSCode primarily because its plugin (in my opinion) has been much better than the competition. But I am always ready to re-evaluate, and I think plugins like this one are crucial. I'd love to have sublime text be my go-to editor for Rust.
`let _ = _a` is a no-op here. Since it doesn't bind anything, it doesn't move out of `_a` *or* borrow `_a`. But compare [this](https://play.rust-lang.org/?gist=03c58ea73959a7492af18c424429098f&amp;version=stable).
&gt; And until [async/await is] on stable, and every async library supports it, we're just going to have to suffer callback-hell? The term 'callback hell' refers to nested callbacks and the whole point of futures is to chain or flatten callbacks instead of nesting them, so if you have callback hell with futures, you're doing it completely wrong, or you don't understand the term. Chained futures still have one level of nesting in the combinator calls (e.g., `and_then`), but generators allow making the calls implicit and write async code in a more sync style. async/await is just syntactic sugar for generators combined with futures, so the control flow with async/await is still exactly the same, so it's no less "spaghetti", just cleaner or more terse. &gt; Async is a problem looking for a solution. Async or evented I/O is a popular concurrency control model for I/O bound programs for a reason; it's scalable, understandable and predictable, especially compared to preemptive approaches like threads that have problems like deadlocks and don't scale as well for I/O. If your program is computation bound, not I/O bound like servers or UI, then async is the wrong model, but that doesn't seem like what you're saying; instead, you're blaming the async model for Rust's incomplete implementation/documentation of it.
I'm pretty sure you're going to want to use glutin regardless of what OpenGL back end you use. I mean, vulkano, gfx, glium all use glutin or support glutin, to my knowledge? I'd personally go for gfx or glium right now if you want good hardware support. Vulkano is the future, but vulkan isn't supported by everything like OpenGL is.
This is exactly how markets are supposed to work, Rust does well to promote a market of skills. Relating this now back to the thread post. Rust would be in a much better place if we could leverage low level embedded people to lay a foundation that is suitable for embedded use cases. Which would further help other low level use cases like HPC and scientific computing work.
Is it possible to require in a Trait that it can be only implemented on Enums? Or is there an abstraction for Enums? I need to work with generic objects that might be enums. Like, I have a trait object MyTrait, that I know for sure that is a variant of some enum. Does this make sense?
+1. Couldn't agree more.
Yeah thanks, I'm sort of new to reddit.
Try [Deref](https://doc.rust-lang.org/std/ops/trait.Deref.html), it's a little more automatic.
This post is loaded with useful information. Where do I read up more on Rust's internals like this?
GUI libraries are hard in any language. The only options I've really seen suggested have been to wrap an existing api like Win32 (only on Windows), Cocoa (only on mac), GTK (looks like crap on everything other than Gnome), and QT (looks like crap on everything other than KDE), or to use Electron or the browser, which is slow and has its own problems. Neither of what you asked for are "little" things.
Thank you very much for this writeup. I think I understand macros much better now. I had an idea for a high-performance message logging and filtering library I wanted to write, which relied heavily on macros to do its magic with good performance. That idea got stalled a couple of months ago, because I was lost and confused (due to not really having much experience with macros in Rust) and the macros started getting too complicated for me to reason about them easily. Your writeup gave me some ideas for how to restructure the whole thing to make it better/cleaner. I might revisit that idea now.
My pros/cons is that May has some ruff unsafe edges : https://github.com/Xudong-Huang/may/issues/6
pcwalton's HN comments tend to be about green threads vs Linux OS threads. x64 address space *is* huge, 4Kb stacks *are* pretty cheap (you can use a much cheaper allocator too!), and the Linux scheduler *is* pretty good so he's right about that comparison AFAIK. However, OS threads still have more overhead than their 4Kb stack. They require a kernel stack, which on x86_64 is 8Kb that *can't* be allocated on-demand. They also *always* go through the kernel when task switching, which means a general purpose scheduler. Green threads don't have kernel stacks- they all share the host threads'. Task switching doesn't always go through the kernel- epoll can wake up the event loop with multiple events, tasks can be communicating through user-space channels, etc. This means you can often get through several tasks without touching the kernel at all- basically a form of batching. Further, Rust can't use Go's method of stack growth, because it doesn't have a precise tracing GC, and likely never will have one for *all* pointers. Thus, its green threads would have to use segmented stacks, plus stack switching for FFI. (This part is key to pcwalton's HN comments.) Besides, as far as performance goes, that comparison is pretty much moot given how futures handle those issues. They have fixed-size "stacks" that can be *way* smaller than even 2Kb. Switching between them is even cheaper than green threads (though they aren't as easily preemptible). And, you can even imagine a special-purpose event loop that only handles one (or a small number) of top-level future types to get an even simpler scheduler.
There's [this issue](https://github.com/alexcrichton/futures-await/issues/3) on `futures-await`, and also [this internals thread](https://internals.rust-lang.org/t/pre-rfc-catching-functions/6505) on "catching functions," which is relevant because withoutboats seems to want to make the two syntaxes parallel each other. Personally I think await and try need to remain separate since they're orthogonal effects. I've been throwing around the idea of making await implicit (and get-a-future the annotated case) within async contexts as my preferred alternative to reducing the noise.
I am currently using webrender and a simple CSS parser to make a GUI framework for Rust. Things like this remind me why I shouldn't roll my own renderer, I wouldn't have the time to debug all of these clipping and blending issues. That said, I'd really, really appreciate some more documentation (e.g. what some functions do). I mean, it can't be that hard to just write one sentence on what the purpose of a struct or a function is, come on. webrender has almost zero documentation, this could really be improved.
&gt; And until that's on stable, and every async library supports it, we're just going to have to suffer callback-hell? ...or you could just not use Tokio until then.
Most of this isn't Rust-specific. It's mostly standard C/C++/Rust/etc-level compiler techniques or C# internals, which I learned from LLVM and assorted blog posts. The Rust generator implementation is basically what you'd expect given the combination of a) those standard compiler techniques and Rust's C-like approach to objects and memory, and b) C#'s state machine approach to async/await. Zoxc is the one who's been implementing it if you want to follow the PRs.
Yes, that's it, thank you
&gt; E. g. list operations would clone the whole list.. So by using this crate this overhead can be eliminated, right? Probably, yeah? I guess what I'm trying to get at is: get your compiler working *first* and then worry about performance issues like that. If it's gonna be an issue then, *you're gonna know that it's an issue*.
I always find it baffling when people claim the Rust community is so great. The "community" is one of the biggest things putting me off Rust, and I've seen others share this sentiment.
Is this an opportunity for contribution? Yes, ish. hdf5 is a commonly requested crate. See my previous comments at: https://users.rust-lang.org/t/bioinformatics-tools-that-could-use-speeding-up/13788/7?u=eh2406 Or at the repo: https://github.com/aldanor/hdf5-rs/issues/17
I don't have any great recommendations here, sorry. You can definitely learn more about the register save/load stuff by looking at some specific platform ABIs (for Linux on x86_64, it's confusingly called the Itanium C++ ABI; Windows has its own), and reading some compiler-generated assembly. http://gcc.godbolt.org/ is a great way to experiment with that. Looking into C# memory layout and how tracing GC works would probably be a useful contrast with the C ABIs, to explain the stuff about struct layout. As far as compiler techniques in general, I think one central concept for me is the pipeline from AST, to an IR that models data- and control-flow for optimization (often SSA or sea-of-nodes these days), through register allocation and instruction selection to machine code. LLVM has some documentation about how it does this stuff, and there are a lot of relatively approachable papers about it as well.
Agree with all points except the stack growth. I think, at least in theory, it should be possible to grow stacks by just allocating new pages of virtual memory? Like, I don't understand *why* one needs segmented or relocatable stacks if virtual memory + large address spaces gives you ability to extend stack by pages for free? Or is there some obstacle on this path? Where can I read about these kernel stacks :) ? I haven't heard about them before, sounds really interesting! 
&gt; virtual memory + large address spaces gives you ability to extend stack by pages for free It would definitely be interesting to see a green threading library do that. I'm not totally sure why they don't, maybe it's because 4Kb granularity is too high physical memory-wise? &gt; Where can I read about these kernel stacks :) ? The basic idea is the kernel needs *some* stack to handle interrupts and system calls, and it can't share the userspace stack because it can't trust it, so it needs its own. You could use one per core, but then you couldn't preempt a thread in the middle of handling an interrupt or servicing a system call, so most monolithic kernels that spend a lot of time doing those things give each thread its own kernel stack. Here's the Linux kernel's documentation on the subject, with some more specifics about how they get used: https://www.kernel.org/doc/Documentation/x86/kernel-stacks
You might enjoy https://github.com/rust-lang-nursery/rustc-guide Not much there yet! I'm sure they'd appreciate help from someone who is interested but "outside", if you will.
There's a "Borrowing in match patterns" section in https://blog.rust-lang.org/2017/03/02/lang-ergonomics.html. That might be what you're thinking of?
Which VSCode plugin are you using?
For vscode you need: * https://marketplace.visualstudio.com/items?itemName=rust-lang.rust * https://marketplace.visualstudio.com/items?itemName=be5invis.toml Not sure about debuggers plugins though. RLS plugin will install all the necessary dependencies should you be using rustup. For the RLS plugin you might want to set "rust-client.channel" to "stable" in user setting because sometimes rls or some of its dependencies gets missed in nightly releases. I think its a issue the RLS team is tracking in their github issues.
So much to do...so little time... I think I might have wasted too much time these last 4 days trying to get emacs to work. Time to go back to notepad++, and get working.
I agree that type errors can be inscrutable when using futures/tokio/futures-await. I imagine support in the editor could help here. Can we expect RLS will (or perhaps it does already) show the expected argument and return types of, e.g. a closure in called in a stream.and_then()? Any other tips to figure out what will make the compiler happy? Relatedly, I‚Äôve also wished that I could make the compiler print my code with all inferred types shown. Is that possible somehow? I generally find the compiler‚Äôs error messages fairly helpful but when I use futures they are often much less helpful.
 pub struct GraphicObject&lt;S: Shape&gt; you mean?
enum variants are not distinct types, so you can't say that you'll only return one variant.
sorta; this is being worked out now.
"fix off-by-one error in BufWriter" turned out to be less drastic than it sounds!
While its more automatic, its not ideomatic. Rust tends to favor full delegation rather than "deref up-casting". The deref traits are more for smart pointers, than full on delegation sugar because they give out a reference to the inner object not the outer object. That means, particularly in generic contexts that it can be difficult to tell whether or not you're pass the full object or just parts of it. If you implement the same trait for the inner and outer object this can make it really hard to see which one you're ultimately calling the method on. To make matters worse, deref coercions hide the disjoint borrows of fields from the compiler making it as if you had borrowed the full outer object without the benefits. And finally you can't downcast an object you got through a `Deref` or `AsRef` back to your original object.
OO isn't ideomatic to begin with so whatever floats your boat.
More of a head's up or discussion rather than a tip: NetCDF is sort of converging with hdf5 as a standard IIUC, and I've been starting to use [netcdf](https://crates.io/crates/netcdf) lately (but haven't gotten so far yet). It works, it has lots of features but is not mature, use the git version of the crate.
Don‚Äôt have any comments, just a üëç for building the crate. Great addition to the ecosystem and no doubt will come in handy in some embedded settings.
I agree. I believe that rust's success, in spite of its community, speaks volumes about the technical merits of Rust and the advantages it brings to the field. I expect most people use Rust because it solves a very important set of problems exceptionally well. I know we do. With the influx of new rust users, I hope we see a gradual shift in community consciousness to be more aligned with other more established language projects.
More than happy to put this on [`Not-Yet-Awesome Rust`](https://github.com/erichdongubler/not-yet-awesome-rust) for more awareness! Made a PR [here](https://github.com/ErichDonGubler/not-yet-awesome-rust/pull/27) against the current list -- mind validating what I've got there? Maybe /u/Eh2406 could also help. :)
you can format code on Reddit by placing 4 spaces in front of each line of code. The website and some apps also have a button which can put 4 spaces in front of each line of a selected block of text for you. As it is, this is basically unreadable. But, generally the reason Rust uses more RAM is because `jemalloc` is used instead of `malloc`, and `jemalloc` holds onto a larger chunk of memory just to avoid needing to pay the penalty to do a system call very often. It's possible to use the system allocator directly, but that currently requires `nightly` rust. Custom allocators (like system) are supposed to reach `stable` rust before too much longer. But, does 12MB of RAM really matter? The `jemalloc` buffer of allocated-but-not-yet-used RAM is only ever a couple of megabytes, from what I've seen, which usually doesn't matter, especially once an application starts using gigabytes of RAM. If it does matter, then you'll have to use `nightly` and use the system allocator. I believe things have recently changed with the system allocator as things are being prepared to land on stable, so the documentation I would link you to is probably out of date. [This is the tracking issue for custom allocators.](https://github.com/rust-lang/rust/issues/32838)
Hoping to see you there! The RSVP count is one of our highest ever, though it's totally up in the air as to how many will actually show since we had to reschedule. :P Be enticed by Amazon's much more generous catering budget!
Yeah, the error is in a doc example, not the implementation. I've made a pull request to make the link text clearer.
Thanks. I fixed the formatting. Rust is supposed to be memory efficient and potentially be used in embedded systems, so yes 14.5 MB matters.
On those embedded targets, Rust doesn't use `jemalloc`, so it won't be using the extra 12MB of memory. If there is an allocator at all, it's going to be whatever super-simple one is provided. With microcontrollers and other truly embedded targets, it's a bad idea to dynamically allocate memory at all, usually. So my question still stands: does it really matter on a desktop system with gigs of memory? Those extra few megabytes are not accidental. They improve the CPU performance of the code at the cost of a little memory. _Most_ applications would rather have faster code execution than worry about losing 12MB of memory, _especially_ once the application itself is taking up gigs of memory. **If it does matter**, then you can either wait until the system allocator reaches stable Rust (recommended) or use nightly Rust. But if a constant 12MB of RAM _really_ matters on a desktop or a server, I would love to know what your use case is.
That only covers the use case of the user already having a working version of the components installed for nightly, or indeed already has the nightly toolchain. If you do a clean install of rust via rustup with defaults, and then add the rls-plugin you will find (as of today according to the tool-state tracker) it will break being unable to find rls-preview after attempting to install nightly. To get it to work you either have to manually input the nightly build that last worked or "stable" in the rust-client.channel field. The last working version of nightly with rls-preview (as of today) was 2018-01-13.
Rust: #[macro_use] extern crate error_chain; extern crate reqwest; use std::thread; use std::time::Duration; error_chain! { foreign_links { Io(std::io::Error); Reqwest(reqwest::Error); } } fn run() -&gt; Result&lt;()&gt; { let url = "https://www.google.ca/"; let client = reqwest::Client::builder() .proxy(reqwest::Proxy::http("https://user:pass@aproxy.xxx.com:80")?) .build()?; let _response = client.head(url).send()?; Ok(()) } fn main() { loop { run().unwrap_or_default(); thread::sleep(Duration::from_secs(600)); } } Go: package main import ( "crypto/tls" "net/http" "net/url" "time" ) func main() { req, _ := http.NewRequest("HEAD", "https://www.google.ca/", nil) proxyURL, _ := url.Parse("https://user:pass@aproxy.xxx.com:80") for _ = range time.Tick(10 * time.Minute) { client := &amp;http.Client{Transport: &amp;http.Transport{TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true}, Proxy: http.ProxyURL(proxyURL)}} resp, err := client.Do(req) if err != nil { continue } resp.Body.Close() } } 
That's my point though. Not only is OO a bad fit for Rust, but `Deref` doesn't behave like OO. You can't up or down cast a `Deref` object. If I have a function that takes an argument generically, you may get a hidden "up cast" that you didn't intend, whereas in OO the interface implementation is inherited all the way down. /u/nrc [explains better than I](https://github.com/rust-unofficial/patterns/blob/master/anti_patterns/deref.md#disadvantages)
To put this into context, here are some things which consume *less* memory than 14.5 MB on my system right now: - Sublime Text (with two windows and four files open) - Explorer - Firefox's plugin host - WinCompose (written in C#) - My IRC client - All three VLC instances I have open put together. - The VirtualBox management UI and background service together. 14.5 MB for something that just does HTTPS requests is a lot. Not "you're so fat you have six months to live" a lot, but certainly enough for your doctor to be concerned for your health going forward. Using a little more memory to go a lot faster is fine, but if all anyone cared about was speed at all costs, we wouldn't be using multitasking operating systems in the first place, we'd be back in DOS running one program at a time.
Interesting post! And I think that's a very, *very* difficult question to answer. Rust already has such a wide breadth of reach, and I personally believe that has contributed much to its success. Now that we've established that Rust is useful for many areas of programming, the problem comes down to *what* exactly should be focused on. There are alot of improvements coming down the pipeline ( &lt;3 const generics ), but focusing on them all at one time would be spreading the language too thin. I believe that's why there was a call for community blog posts, so some yearly goals could be set :) 
Did you change any part of the code?
They should add a wipe event to get more players on for the wipe day. E.g a giant nuke/explosion that could be dropped from a plane. In future this could also be a way of keeping bps if you escape the blast on a raft or some kind of designated area that would get you to safety. This could be done anytime in the wipe but would mean that if you wanted to keep playing then you would either have to lose all bps or there could be a character system in place where you could have multiple characters to play as. This would bring more players to the wipe as it would be seen as the "main event".
Please refer to the updated post.
So the error message from serde is: Message("invalid type: integer `36000`, expected struct Sensor") I looked up the definition of the `Sensor` struct that you're trying to deserialize: pub struct Sensor { pub id: String, pub description: String, pub profile: SensorProfile, pub temperature: i32 } But what you have from the file is just `36000`. Where do you expect all this other information to come from?
&gt;Off topic question: I cannot run the produced binary from Nautilus. It gives me this error: &gt; &gt; `There is no application installed for ‚Äúshared library‚Äù files.` &gt; &gt;any suggestion? file command gives this: &gt; &gt; `ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, not stripped` Your crate might be a library. Did you make it using `cargo new project_name` or `cargo new --bin project_name`?
ah thanks :) stupid me. the temperature value comes from a different file than the rest, that's part of the config file. 
Thanks for writing this ‚Äî I communicate with several Modbus devices at work, I‚Äôll try and have a play with this when I get a chance!
that is --bin and obviously I can run it (trough terminal).
Nicely explained for us beginners
Thanks a lot!
Does Rust use jemalloc on Raspberry Pi Zero and Nano Pi?
Both of those have 512MB of RAM, and they run full Linux, so I still say 12MB of RAM wouldn't matter, even though it's hypothetical since OP updated their post to say it's only using 1.3MB of RAM. With full Linux, it assigns pages of virtual memory without actually allocating physical memory to back them. Until each page is hit, it is purely virtual, so the extra megabytes discussed here would be imaginary on a Raspberry Pi Zero, until they actually got used. Even then, Linux has a really intelligent memory management system. If you decided (for whatever reason) to run 100 of these processes simultaneously, and they _touched_ 15MB of pages of memory at startup, but then had a steady-state memory usage of 1MB, Linux could easily swap those unused pages of memory to disk, and only swap them back in if the application ever tried to use them (which we assume isn't happening since we're steady state). Where memory usage _really_ matters is on microcontrollers where you have between a few kilobytes of RAM and up to _maybe_ 1MB of RAM if you're really lucky. There is no OS to swap unused memory pages to disk, and the memory management is purely physical... every page allocated has to be backed by a real page of memory, and so many other limitations that a full computer like a Pi Zero just doesn't have.
This plus `cargo watch -x run` (in [Cmder](http://cmder.net/)) is my preferred workflow. 
A pattern I often find myself using for things like this is two structs: in your situation, one for the 'sensor configuration' and one for the read temperature. Something like this maybe? pub struct SensorConfig { pub id: String, pub description: String, pub profile: SensorProfile, } impl SensorConfig { pub fn with_read_value(self, temperature: i32) -&gt; Sensor [ Sensor { id: self.id, description: self.description, profile: self.profile, temperature: temperature } } }
In this line: impl&lt;'a&gt; fmt::Display for Flobbles&lt;'a&gt; { There are actually two lifetimes. The first is what you've named, 'a, representing whatever lifetime Fobbles is supposed to return. The second, implied to be 'static, represents how long this trait object lasts. To fix it, mention 'a in both, or introduce a new lifetime 'b. Similar to the stackoverflow question you linked, the solution is `Fobbles + 'b`: impl&lt;'a, 'b&gt; fmt::Display for Flobbles&lt;'a&gt; + 'b { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { write!(f, "Trait Flobbles with speed={}", self.flobble_speed()) } }
&gt; Async/Await started as a Haskell library If you're referring to [async](https://hackage.haskell.org/package/async), then it's just a monadic "helper wrapper" around Haskell's normal threads. The modern variant of the library is from [2012](https://github.com/simonmar/async/commit/ea3dd78ec4a7fff21841f629424e6e4f8c5edd7b). (The pre-modern variant is from [2010](https://hackage.haskell.org/package/async-1.0). That makes F# Async older by [a couple years](https://blogs.msdn.microsoft.com/dsyme/2007/10/10/introducing-f-asynchronous-workflows/). Similarly to Haskell async, it's also a callback based system. C# async/await come in [2012](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)#Versions), but are based on state machines (not callbacks), similarly to the Rust futures.
Thank you. Yeah, I am not sure exactly, how I want to do that. The recet push to github works, but I have identified some issues already, that don't make sense or at least coulld be done better.. Also, I'd like to refactor the String concat parts into a macro, but still don't know how to do that.
what is the "-sys crate situation"?
No problem! I'm not sure exactly how to help with doing the struct patterns besides just writing out both kinds, but I can help with concatenation! From what I can tell, it looks like https://github.com/aspera-non-spernit/fancy/blob/master/src/lib.rs#L79 is a large manual `format!()`: it would be much clearer (and more efficient) if done in one step like this: let full_path = format!( "{}{}{}{}", fan.profile.path, fan.profile.prefix, fan.id, fan.profile.manual_suffix ); As for abstracting that into another macro, I can't think of any way which doesn't just make the code more convoluted. I mean, at some point you do have to write out all of the things you want in the order you want them. I'd definitely support putting that code into a fan-specific function so it isn't repeated, maybe in an `impl Fan` block, but I don't think that a macro would help. speaking of macros: it looks like https://github.com/aspera-non-spernit/fancy/blob/master/src/lib.rs#L26 could also be made from a macro into a function if you wanted to do that. Is there something stopping `adjust!()` from being a function like fn adjust&lt;P, V&gt;(path: &amp;P, value: &amp;V) where P: AsRef&lt;Path&gt; + ?Sized, V: Display { // ... } A function would, in my opinion, be clearer than a macro here, and as a bonus it has more useful rustdoc and compiler error support. If you wanted to make it more efficient, using `write!(f, "{}", $value)` rather than `f.write_all($value.to_string())` would avoid allocation of an additional String which I don't *think* needs to exist.
I agree. Rust is not an object-oriented language, despite what the proponents say to try to appeal to a larger audience; they aren't being honest. If you like OOP, Rust is just going to piss you off.
Really wish O'Reilly would start selling DRM-free e-books again.
I am really impressed by the use of the type system to restrict incorrect code as well as reduce unnecessary code duplication. Well done!
Concurrency is about running things in parallel without races - allowing exact timing synchronizations is not a property of "good support for multicore programming". Instead, how about using async IO API? Not an async framework - directly using the async IO API. Never did it myself, but I'd try something like this: * Set your process to maximum priority in the scheduler to guarantee you won't get switched out (use [the scheduler crate](https://crates.io/crates/scheduler)) * Launch the first IO using [`aio_write`](http://man7.org/linux/man-pages/man7/aio.7.html) - note that this operation doesn't block. I couldn't find a Rust crate for it... * Instead of `sleep`ing, busy-wait on the [TSC](https://en.wikipedia.org/wiki/Time_Stamp_Counter). You can use [the clocksource crate](https://crates.io/crates/clocksource). * Launch the second IO.
**Time Stamp Counter** The Time Stamp Counter (TSC) is a 64-bit register present on all x86 processors since the Pentium. It counts the number of cycles since reset. The instruction RDTSC returns the TSC in EDX:EAX. In x86-64 mode, RDTSC also clears the higher 32 bits of RAX and RDX. Its opcode is 0F 31. Pentium competitors such as the Cyrix 6x86 did not always have a TSC and may consider RDTSC an illegal instruction. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
If you try writing it in Rust, there's a good chance it won't even compile because of the potential race condition. Of course, this depends on whether sending the input just refers to calling an external global procedure, or if the receiver is itself written in Rust.
I would have expected MinGW and MSVC to produce compatible binaries (dll). I think you're right though, but I gave up and instead used ```cc-rs``` which did exactly what I wanted 
I see a *lot* of people share that sentiment, but the Rust team doesn't seem to care. They rationalize it with the belief that anyone who takes issue with the current culture is one of the Bad People the code of conduct is meant to exclude. If you don After a while, the whole "we're so friendly" idea becomes an article of faith rather than an empirically observed fact. Rust is not the first language this has happened to. I'm guessing this comment will be deleted within minutes, but I wanted to get that off my chest.
I'll be honest: the moderation is more than a bit authoritarian. On the whole, though, I haven't found that negatively impacting the community very much. My perception is likely warped by my background and alignment on various issues, but I've felt pretty at home and welcomed into the rust community. I mean, yes, there are somethings that the mod teams literally won't let you say, with punishment of deleting posts. In my experience, that doesn't seem to counteract the friendlyness toward friendly new people learning that language nor to most questions though. It seems like those are two orthogonal axis: the mod team is strict, and the community is welcoming, I don't *think* that conflicts.
I realize you've probably talked a lot about it before, or at least I know I've heard accounts of the community, bu it would be interesting if you'd give a short brief on what you find missing and/or bad about this community. Not that I can do anything about it, I would just find it really interesting to see more of the other viewpoint, given that I've found the community fairly nice.
&gt; it would be interesting if you'd give a short brief on what you find missing and/or bad about this community For the love of God, don't use an account that can be traced to your real identity.
Good! Ever since I read the *Why Rust* pamphlet, I've been looking forward to the release of this book. The style of writing is clear and straight-forward and I think it's going to become the go-to book for people who want to learn Rust.
Can you link the pamphlet?
True, not sure how much the OP knows about what all Rust's memory safety entails. However, there is always `unsafe {}`. Using unsafe rust to model race conditions in order to do some sort of frame-worked profiling is quite an interesting prospect. That separation could prove quite handy.
Wow, this is really well done.
Thanks this is cool!
https://www.reddit.com/r/rust/comments/7qivpu/why_rust_ebook_free_on_oreilly_media/
Do you have to have programming knowledge/background to read this book?
Looks like the certificate's been renewed as of two hours ago.
/u/japaric continues to be a force of nature in advancing the state of rust on embedded hardware
Ah, no, look, when you have an enum Car for example, when you say a function will return Car, the actual object returned at runtime will be a variant of Car, Car::BMW or whatever. What I want is to say that it will return an enum in general, whatever that may be. So at runtime the function will return SomeEnum:SomeVariantOfIt. Would it be possible to do that with generic parameters like T: Enum or something?
Your specific example can be solved by introducing traits pub trait Employee { ..} pub trait WorkHarder { pub fn work_harder(&amp;self); } pub trait Blame { pub fn blame(&amp;self); } The next step is to introduce struct named OfficeWorker, OpenOfficeWorker and Manager and implement the relevant traits for each struct. The beauty of this approach is that you can also describe an employee that can blame but not work hard.
Missed opportunity to title this post "Brave new Hello World".
Yes. They don't start you over with "what is a for-loop?".
[removed]
Interesting, didn't see that RFC yet.
Would it be possible to use a specialized decorator instead? Perhaps it could look like this: pub trait InnerMut { type P; fn inner_mut(&amp;mut self) -&gt; &amp;mut P } #[derive(InnerMut)] struct OfficeWorker { #[inner] inner: Employee, //... stuff } This decorator would not obey the "Liskov substitution principle", but it would make some problems simpler. Alternatively there is the [Fields-in-traits-RFC](https://github.com/nikomatsakis/fields-in-traits-rfc/blob/master/0000-fields-in-traits.md#detailed-design)
https://github.com/rust-lang/rfcs/blob/master/text/2089-implied-bounds.md
Thanks.
This is great. I went out and bought a stm32f4discovery board to try embedded rust out 
On the I/O side, I found that bit-banding and bit-banding like registers (like BSRR) on the STM32 could be very helpful. If you use BSRR, then you don't have to worry about synchronization between different "tasks", as long as you agree to modify only bits which "belong" to you. Bit-banding provides the same sort of functionality for non GPIO data bits, for example, for configuring GPIO itself. By using bit-banding you can replace read-modify-write code with write-only code, which is be more tolerant to data-races (although if you modify multiple bits, you have to be careful about the order you modify them, so you don't leave peripherals in dangerous states if you are interrupted while reconfiguring them). It gives you share-nothing design when working with shareable peripherals, like GPIO, where one set of pins might belong to one subsystem and another set of pins to another. Ownership of resources (passing down all these references) was a major pain point for me in cortex-m-rtfm.
Do they sell digital copies? I can't find on the O'Reilly's site... 
I'm planning to buy this book as well.
Missed opportunity to use the rust monster from D&amp;D on the cover.
Seems to be a Kindle version on Amazon
As I said in another reply, I want to create an algorithm that works with enums (called element), but what are the actual variants of the enum depends on the actual use of the algorithm. I don't need to match the element, as that is clearly impossible, but I want to be able to differentiate between them, as in differentiate between the different variants of the element, whichever they might be. And I also need to store the element, being able to recover it and differentiate one variant from another. Basically if I create a trait with PartialEq and Eq requirements, it might work, as this trait on an enum would enforce that it can make difference between them. So if I create a function to get as argument a trait object of that kind, I will get (at runtime) an enum variant of an enum that implements that trait, right? If I have an enum Cars, and implement the Element trait on it, a function that takes Element trait object as argument will accept Car::BMW at runtime as argument. And I can use in the function Element == Element and expect Car::BMW == Car::Audi to work properly. Right? But, is there a way to enforce that the trait can only be implemented on enums? Or do I have to live it like that and just hope the algorithm will be used properly?
They used to, but they've since pulled out of the no-DRM eBook market, and the only way to get their books (that you didn't already get) is through their Safari service, AFAIK. That means the only way you'll get a digital copy to own is if you bought it pre-release. :(
Inspiring code, carefully conceived, clearly explained. Thank you!
I do have to ask, isn't ROS a dominant platform to develop robotics solutions? It uses c++ and python and it has become quite the standard for prototyping quickly. What is Rust's position in relation to ROS?
&gt; They don't start you over with "what is a for-loop?" That's because Rust doesn't *have* for-loops - in spite of being requested over and over again. Instead Rust has *iterators* that dress up as for-loops. But they're really not for-loops. Rust is welcome to add true for-loops any time it wants to, they are *far* superior to the easy-to-spin while-loops which Rust, for some reason, *does* support.
For those curious about the state of (on-chip) peripheral devices [the issue tracker lists a bunch of discussuons](https://github.com/japaric/embedded-hal/issues). Anyhow this is incredibly interesting ‚Äì when I get the time I would love to test this out on the MSP32 series. However, I‚Äôm also wondering, what if I don‚Äôt want to to use the HAL? Say I have a microcontroller where I want access to unique features on that peripheral, could I then just design my own types and assign them to the specific memory locations I need?
By `for` loop do you mean something like `for(int i = 0; i &lt; 10; i++)`? If so, You can achieve the same affect in rust via: `for i in (0..10)` I'm fairly certain that any trivial loop header (where you'd actually control a counter variable) would compile down to the same thing in release mode. (I am aware you said "iterator that dresses up as a for loop" (albeit a little differently) but in the end, they end up doing the same thing, and there isn't much of a difference with typing, so I don't see an issue)
I wouldn't call that "very obvious". Sure, _right now_, only `Result` implements `MyDefault`, but it's a non-breaking change to add more implementations for that trait. additionally: `?` does not require the "input" and "output" type to be the same. I mean, once the trait is stabilized, you'll be able to make a `struct MyDefaultStruct;`, implement `MyDefault` for it, and: impl Try for MyDefaultStruct { type Ok = (); type Error = Result&lt;(), ()&gt;; ... } At this point, it will be fully ambiguous whether `MyDefault::default()` should refer to `Result` or `MyDefaultStruct`. As I said earlier, adding structs and trait impls is a non-breaking change, so this should still fail inference today (as it does).
Meh. I'll stick to ed. 
Meh. I'll stick to butterflies.
Indeed, I'm glad that rust stops me from (somewhat) stupid stuff like `for(*z&gt;&gt;w&gt;&gt;w&gt;&gt;w&gt;&gt;w&gt;&gt;E[w]&gt;&gt;w&gt;&gt;w&gt;&gt;i;i--;)*z&gt;&gt;w&gt;&gt;E[w];` (Actual code that I wrote, it's code golf though, so it's okay :p )
Amazing work! It seems that it is time to update this: https://github.com/yuri91/stm32f40x-rs I also have a ili9341 display that I would like to write a driver for. Hopefully I will have some time to dedicate to this in the next weeks.
`armv7-unknown-linux-gnueabihf` (Raspberry Pi) Although, TBH I didn't have any problem with Rust on RPi apart from the fact that it doesn't have enough memory for release build...
libgen has the pdf btw
I'm totally uninvested in the actual numbers I posted -- more in the principles of how we change this. I think having clear owners of nightly-only features is probably a very good step forward, that also gives people someone to contact if there is a lack of forward movement.
This is utterly fantastic news. It will hopefully provide publishers a little bit of confidence that the Rust community will support their investment :)
Every feature in the language has a cost in learnability, error messages, allowance for growth in the language, and more. As such, every feature should be worth more than the cost imposed by having it. Given that iterator loops solve the vast majority of what a C for loop does, and that the minority is often confusing to understand, and can be done with a while loop, C's for loop just isn't worth it. Moreso, until you actually memorize the arbitrariness of C's for loop, it's pretty unreadable. And the use cases are also more unreadable as the reader has to jump around to figure out the order of assignments and predicates. Even though you might think them superior, many many disagree with that.
They do sell DRM-free eBooks (including Programming Rust) on Google Play https://play.google.com/store/books/details/Jim_Blandy_Programming_Rust?id=hcc_DwAAQBAJ
&gt; 4 days trying to get emacs to work. What did not work? I just installed emacs for mac, initialized spacemacs (one git command), and added the Rust layer (one line in the config file), and everything "just works" (auto-complete, seeing errors as I type, etc.).
/u/SimonSapin, since you've apparently done some of this analysis before (manually), how hard do you think this would be to get, based on some simple text-based analysis of Git history?
&gt; Moreso, until you actually memorize the arbitrariness of C's for loop, it's pretty unreadable. First day programming? Is Rust designed for beginners?
Does it need to be low level? Or can I start coming from Python as well?
[removed]
I have the book. As long as you understand basic pointer and memory management, you would have no problem reading the book. I don't come from C/C++ background, I can perfectly understand the material although it might help if you know them.
 for i in (0..32) { let mask: u32 = 0xFFFFFFFF &gt;&gt; i; // Code after here. } `for mask in (0..32).map(|i| 0xFFFFFFFFu32 &gt;&gt; i)` Granted, neither of those are likely anywhere near the best code. I'm currently writing this at ~02:30 (local time) and am not even sure if they compile, but the point is you can do it, without any weird loopholes. _opinion incoming_ #1 looks clearer to me than the original loop you showed me, reasons being: It's _still_ shorter than the header you gave me, despite having more than one statement (under the assumption that you would actually write it like that), And it's clear exactly how many times the loop will run at a glance. I can think of an actual downsides to #1, such as the fact that you have an extra variable, and not all the "work" for calculating the next iteration is done in the header.
Is it useful if you already read the online Rust book from the official Rust documentation?
Have a look at the url crate
&gt; That means the only way you'll get a digital copy to own is if you bought it pre-release. :( I did it, what a lucky decision! (coming from C++)
&gt; However, I‚Äôm also wondering, what if I don‚Äôt want to to use the HAL? Say I have a microcontroller where I want access to unique features on that peripheral, could I then just design my own types and assign them to the specific memory locations I need? Based on my understanding of the article, yes. You write the driver for your peripheral (as a Rust type, generic over lower-level HAL abstractions like which pin it should use). Then you impl the HAL traits relevant to your peripheral, to implement the common interface for common generic functionality. Nothing prevents you from also adding additional methods to your driver for implementing specialised functionality that is not covered by the HAL traits. Say, for example, you wrote a driver for some I2C controller. You implement the relevant HAL traits for common functionality and some extra methods for some specialised stuff for your exact I2C controller. Then, since you implemented the HAL traits, you can use it as a building block for higher-level abstractions, possibly written by someone else (say, a higher-level driver that depends on I2C and is generic over any I2C controller (since it only relies on common I2C functionality). In your application, you can create an instance of your I2C driver. Then you can call some of the specialised methods to do some specific thing. Then you can create an instance of the high-level driver and pass it your I2C driver instance and it will work via the HAL. /u/japaric please correct me if I am wrong
It makes sense. The svd file provides the definitions of the memory mapped registers and a HAL could then provide the trait for an I2C driver/interface. If I need to use specific registers I simply pull away the HAL and implement something of my own. Thanks!
In general a lot of development boards are cheaper or at least competitively priced compared to (genuine) Arduino boards -- and that's with all the extra niceties of it being ARM and a much, much more modern and full-featured set of peripherals. Yet the simplicity and network effect of Arduino is ofc. hard to beat. 
The STM32 F3 discovery series which were mentioned in the blog are quite good and not expensive at all.
If you're going to be doing that kind of for loop a lot, I would make an Iterator that gives you each mask, and provide a method for it. Then you can do `for mask in ipv4.masks() {}`. Now the code for determining the masks are isolated from what operates on them.
/u/japaric since you discussed the different layers of abstraction, do you think there would be any use for a layer to be generic of different add on parts? As in a stepper trait that you can implement for a specific stepper driver, or an accelerometer trait. This would let you swap out parts easily, but since parts can have differing capabilities it may be hard to write the traits generic enough to be useful.
For an example of how an async state machine can work, you might see the MSDN post here: https://blogs.msdn.microsoft.com/seteplia/2017/11/30/dissecting-the-async-methods-in-c/ It's about C#, but it should give an example of the details involved in async/await transformations.
You can use [`https://docs.rs/url/1.6.0/url/form_urlencoded/fn.parse.html`](https://docs.rs/url/1.6.0/url/form_urlencoded/fn.parse.html) to parse the querystring into pairs. Or use one of the following crates to deserialize it to your own struct: * [serde_urlencoded](https://crates.io/crates/serde_urlencoded) * [serde_qs](https://crates.io/crates/serde_qs) * [nested_qs](https://crates.io/crates/nested_qs)
If you want something like enum Car { BMW } fn foo&lt;C: Car&gt;() { ... } then no, that's not possible, since enum variants are values and not types. I think this could work with const-generics to some extend.
&gt; share-nothing design when working with shareable peripherals Ownership at the peripheral level (what the new I/O model is all about) lets you do this. In the case of I/O pins you can, for example, `split` the `GPIO` peripheral into 16 individual pins (`PA0`, `PA1`, ...). The `split` operator drops the original `GPIO` but in exchange you can operate each pin independently, i.e. from different execution contexts. Even if each pin is operating on the same peripheral or even on the exact same register the implementation, which can use bit banding, as you say, or a set / clear register (see BSRR vs ODR on the stm32), guarantees that operation is data race / race condition free. You can extend this idea to other peripherals like USART to split a `Serial` abstraction in two `Tx` and `Rx` halves (like tokio_net::TcpStream.split does) that operate indepedently. Vendors usually implement their hardware to operate like this so bit banding is not even required. &gt; Ownership of resources (passing down all these references) was a major pain point for me in cortex-m-rtfm. That was in v0.2.0. In v0.3.0 peripherals are no longer special and with the new I/O model you don't have all these references floating around; it's much cleaner. I'll soon post a blog about v0.3.0.
Unrelated, but did you by chance already report that? Is there a public report/ticket describing it? Because I think I've recently experienced the same or something close.
Other languages had C-style loops and have removed them. See for example https://github.com/apple/swift-evolution/blob/master/proposals/0007-remove-c-style-for-loops.md for a discussion.
&gt; /u/japaric please correct me if I am wrong What you said is correct. You implement the HAL traits for your, e.g. I2C, type; you get access to the generic drivers on crates.io (accelerometers, gyroscopes, etc.). But nothing stops you from adding more device specific functionality (inherent methods) to your type.
I don't see why the future has to be _only_ Rust. If the C/C++ monopoly does get broken then I'd expect to see other system languages arise to explore other new ideas.
With lots of build systems you can build custom commands for a project. Like the commands in a `package.json` file. As far as I can tell, this is not supported by Cargo. What do most Rust projects tend to use for this?
Rust was actually named after [Rust](https://en.wikipedia.org/wiki/Rust_(video_game)).
You need to `set omnifunc=LanguageClient#complete`, and then you need to use `&lt;C-x&gt;&lt;C-o&gt;` in insert mode to pop up the completions list.
Awesome! That's really great to hear and hopefully 2018 will be the year where I give Rust for microcontrollers an honest shot! Thanks for the great work!
Yes! They complement each other wonderfully, imo. I read it after having read both versions of TRPL, and I still enjoyed it. I would guess that going the other way is probably also good, they have different focuses.
Nope: https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md#minor-change-adding-defaulted-type-parameters
I really appreciated this write up! It made me want to skim the original paper. Also, there's a reference to a change made to the standard library as a result of this work. This is referencing [this issue](https://github.com/rust-lang/rust/issues/41622) with MutexGuard's API.
&gt; do you think there would be any use for a layer to be generic for different add on parts? Perhaps, eventually? I'd say the usefulness boils down to how often the trait is used. If you have an I2C trait and you implement it for N platforms and write M drivers to interface I2C slaves on top of it you support the N x M combinations. The alternative is to write N x M implementations. The trait saves you writing (N x M - N - M) implementations. &gt; As in a stepper trait Does this one even need a trait? I'm not familiar with stepper ICs but in the case of motor driver (full H brigde) ICs you pretty much always need 1 PWM and 2 digital outputs (IN1 and IN2). The variation comes from what do the states of the digital outputs (the 4 digital states) mean so I was thinking that a single generic type `Motor&lt;PWM, IN1, IN2, MODE&gt;` may do -- the `MODE` generic parameter parametrizes the meaning of the 4 digital states. &gt; an accelerometer trait. This one *does* sound useful but I guess we need to have more accelerometer crates first :sweat_smile:. Now, how useful it is ... it depends on how often you think you'll be switching out the accelerometer during application development; or perhaps you want to write a generic application that will be flashed in different board models that have different accelerometers (ones are more precise than others so you can sell them at more $$$) -- in that case an ad hoc, application specific trait may do a better job; or perhaps you want to create a `Vec&lt;Box&lt;Accelerometer&gt;&gt;` in your code because your board has different accelerometers on board (for some reason). So, it depends, really; different people will answer differently.
I looked into that as well but didn't have the required knowledge to see that as a problem. What I had found on my system was % wc -l trace_c++ trace_rust 970774 trace_c++ 742281 trace_orig 1713055 total The main differences that I saw were. * C++ made more calls. * C++ called `fchdir()` a lot. * C++ called `lstat()` rather than `stat()`, but according to the man page `lstat() is identical to stat()` This, in my limited knowledge, insinuated that the C++ should have been a tad slower in system calls so I looked elsewhere. But of course with filesystem access system calls would be important. Thanks to code from /u/pftbest in https://www.reddit.com/r/rust/comments/7pongm/speed_of_counting_files_in_a_directory/dslhwm8/ there's a good speed improvement from using `entry.file_type()` to avoid system calls to `stat()`. There's a little more information in my reply to that post. 
Looks like you linked to the wrong wikipedia page. [This](https://en.wikipedia.org/wiki/Rust,_Burgenland) is the correct one.
Yes, I am the author. I did not update it for some time, but I am willing to put some effort into it again now. I also patched it a bit (and some guy made some PRs too), but there are still some enumerated values missing I think.
the display that I have uses SPI for communication.
I'm not talking about the extra cargo utilities you can install. I am asking in regards to commands that are project specific, and specific to that project only. Having these as binaries would also be very annoying. For example. I'm currently building a game and it has maps you can run it with. So I have to run things like `cargo run --bin fortress --map ./map/castle.map`. With npm I could stick it in the `package.json`, or I'd use a bash script.
Oh I see. I don't know if `cargo` has this functionality (doesn't look like it from googling) so personally I would just use a makefile that invokes cargo with those arguments.
Main features: * Fast * Makes use of EFF wordlists which were designed to contain human-friendly words as opposed to the arcane and obscure and strange words found in e.g. /usr/share/dict/words. * Single binary with wordlists compiled into it. * It can tell you the password entropy if you'd like it to. * You can use physical dice if you like. By default it will use the OS provided CSPRNG. Installation: `cargo install pgen`
You seem to be defining a for loop as "what C does". There is no reason to do so, plenty of languages have Rust-like for loops. Perhaps your argument is that "C had it first". Also wrong, [multiple languages have had for loops before C](https://en.wikipedia.org/wiki/For_loop#1964:_BASIC) and these have generally had syntax of the form "for i ranges from 1 to 100 step 2", which Rust lets you do easily.
I think you may be looking for r/playrust
/r/playrust
Err I don't see how that is related to my comment? If an impl block for a struct assumes that the struct definition has a `Foo` bound, then removing the bound will break the code.
The loosening bounds section doesn't apply anymore once the ergonomics RFC above is implemented.
It's possible that it's early in the morning, and so I'm wrong. It's not 100% clear to me how making something accept more types is breaking.
... it's not fixed for me! Apparently somehow my config got wiped out and webrender was turned off. :/
A great post! one thing: &gt; I previously talked about the difficulty with helping improve crate docs. I wonder if there's an opportunity to run a purely docs-focussed blitz. The docs team did one a long time ago, and we'd like to do another this year. We were thinking a bit later, as it'd be really nice to have the new doc tooling first.
Right, but in what way? Could you maybe show me some code? Sorry for being so dense. I'm chugging more coffee as we speak :)
&gt; set omnifunc=LanguageClient#complete No way to make this automatic?
I *love* this, I've so often seen weird things in embedded software, this is a refreshign take on the problem. However I'm a bit put off by `Peripherals::take().unwrap()`; there are some times in a main loop on an MCU where I wouldn't know what to do if `.take()` fails except trying again until it works.
This is very impressive. I've been working a lot in the same area, and I really like how it is possible to split ownership of resources. The whole application framework is very cool, too. And of course you always have excellent blog posts and documentation! The constraints that you get when working with SVDs are real, and have been my focus. Probably the biggest issues that I see have to do with the fact that SVD files are MCU-centric and have limited structure to indicate similarity of peripherals both within a MCU and between MCUs in a processor family. As far as I can tell, SVD was designed mainly to make visual debuggers better, not for implementing peripheral libraries. It looks like you use a macro-based approach to deal with this in stm32f30x-hal, but if one wanted to write (for instance) a single DMA-based SPI driver crate that could work with the compatible SPI peripherals on several different STM32 MCUs, how would you go about doing this? Would you need to add a dependency for each supported MCU, meaning that someone wanting to use that crate might now pull in 20+ MCU crates?
 fn main() { let p = stm32f30x::Peripherals::take().unwrap(); ... } This "ensure a single use" case reminded me of an idea I had in the past for a similar issue with things like GUI libraries. The idea was that you could mark a type as only being constructable by requesting it as `main` parameter: fn main(peripherals: stm32f30x::Peripherals) { ... } It would also work for other use-cases: fn main(unmutexed_stdout: std::io::MainStdout) { ... } fn main(gui_runtime: mygui::Runtime) { ... } It would also mean that the types can be non-`Sync` and the application writer must put them in a `Mutex` or similar themselves if they need to. The types themselves would have to implement something like a pub trait MainResource { fn main_resource() -&gt; Self; } which wouldn't be manually callable, like `Drop::drop`.
I don't work on it so I don't know exactly, but this is one tracker anyway https://bugzilla.mozilla.org/show_bug.cgi?id=1386665
Thanks. So if I'm reading that right there's 123 bugs to go before it's on by default in Nightly.
Please try not to refer to traits as superior to oop; They are just different design paradigms which are in fact quite similar. (Except when you're on r/rustjerk) As an example where traits and expecially tagged enums are great is [json](https://docs.serde.rs/serde_json/value/enum.Value.html). Traits alone a really neat for expressing units their conversions as e.g. shown [here](http://blog.jeffsmits.net/compsci/2017/02/15/physical-quantity-as-type/).
I cleaned up the language a bit, thanks for the comment!
&gt; No way to make this automatic? What do you mean by "automatic"? * If you want to automatically set `omnifunc` - just put it in your `init.vim`. * If you want to automatically pop the list - you'll need a plugin for that ([Supertab](https://github.com/ervandew/supertab)? [ [AutoComplPop](https://github.com/othree/vim-autocomplpop)?) &gt; Also, how do I select an option from the list? Hitting enter or tab doesn't do what I "expect"? Use `&lt;C-p&gt;` and `&lt;C-n&gt;` to scroll the list, and type any non-word character(`&lt;Space&gt;` is also OK) to finish the cpmpletion. See `:help ins-completion`. 
You are looking for something like [deoplete](https://github.com/Shougo/deoplete.nvim) or [nvim-completion-manager](https://github.com/roxma/nvim-completion-manager). They should both work out of the box with LanguageClient-Nvim
What's wrong with `&lt;C-x&gt;&lt;C-o&gt;`?
You would write it against the embedded hal spi trait, then a user of your driver can choose whichever board implements that trait.
I created a small example, the comments explain what I mean: https://play.rust-lang.org/?gist=74c4a710a034f2db564f1c37d55b114f&amp;version=stable
Still in stock on Amazon Canada. I just ordered my copy today. 
Ah ha! Thank you. I 100% understand now. Also, embarrassingly, I went back and read the RFC again. The drawbacks section says: &gt; Removing a bound from a struct becomes a breaking change (note: this can already be the case for functions and traits).
Probably that it looks very emacsy
I think making this a breaking change isn't a good thing. The unresolved questions include what I've been thinking: &gt; Should we try to limit the range of implied bounds to be crate-local (or module-local, etc)? That would be a good compromise IMO.
Isn't it [this one](https://en.wikipedia.org/wiki/Rust,_Baden-W%C3%BCrttemberg)?
[removed]
Let's just hope it's not [this one](https://en.wikipedia.org/wiki/Bernhard_Rust).
Cool write up! I like the enthusiasm. :-) &gt; Probably the most significant paper cut is that rust‚Äôs error handling semantics of Result&lt;T, E&gt; don‚Äôt work well for the CLI use case. In most cases a CLI works like a compiler: I want to show the user all the things that are wrong, not fail on the first error I find! Not sure I quite buy this. `Result&lt;T, E&gt;` works perfectly fine, and use of it does not imply "one error and completely fail" semantics. e.g., for result in some_iter_that_yields_results { let val = match result { Ok(val) =&gt; val, Err(err) =&gt; { eprintln!("{}", err); continue; } }; // do something with val } This lets you design the internals of your CLI tool using standard error handling, and simultaneously let's you pick when to decide whether an error should be fatal or not. With that said, one area where `Result&lt;T, E&gt;` is harder to use is dealing with partial success or partial failure. That is, the partial nature of it implies you might have both a `T` and an `E`, which is antithetical to a sum. I've taken to using `(T, Option&lt;E&gt;)` to represent partial success. In fact, `E` can be a standard error type that might even roll up many other types of errors: https://docs.rs/ignore/0.3.1/ignore/enum.Error.html#variant.Partial &gt; It exports types which guarantee that all paths exist and are absolute. I think this line alone basically ensures that I can never use this crate precisely because such a guarantee cannot be made in the type system (at least, as far as Linux/Mac/Windows is concerned). What happens when I have a path and the file doesn't exist? &gt; Oh and did I mention that you can serialize the paths with serde? That was one of my biggest pain points ‚Äì if I have a path I can‚Äôt stick it in a struct which I want to send anywhere. This library solves that and more. I'm curious to hear what you specific use case is that led you to need this. &gt; I‚Äôm currently working on a CLI testing framework built in rust, using the new crate I wrote called termstyle to aid in being able to easily express your styles (when you write your app) and then make it easy to test them as well. The goal is to be able to write your CLI tests in a simple YAML file and get clear diffs against the expected vs result. One of the benefits: while you are writing your tests there is no compilation time if you are not touching your source code ‚Äì and running the tests takes almost no time at all! This sounds pretty cool! Especially the bit about testing colors. I'm excited to see how this turns out! But yeah, nice post. I love writing CLI apps in Rust too. I think the major difference between me and you is that I'm basically happy with what we have now. :-) But I am quite likely blind to many problems!
How does the code size look for these examples? Are we paying for the abstractions in code space?
Oh wow. Someone should buy japaric a pizza for all that. o_O
Right, as the driver author I would be implementing the SPI trait. What I'm asking is, if there are N different MCUs in the device family that share the same SPI peripheral (not just MCU variants that have different amounts of RAM / Flash), do I have to implement and maintain a separate driver for each of those MCUs, even if they are otherwise identical?
On the serialization, I can imagine maybe if you want to let the user point to an options file it might be convenient for the developer to be able to just deserialize it? 
Might be worth weighing in on the tracking issue!
Cargo says that I'm missing the VS C++ files. But when I tried to re-download and install from Microsoft, it tells me that it's already there. And `rustup toolchain install stable` doesn't offer to install the C++ files anymore; it just completes and is good to go. So how can I tell rustc and cargo that the C++ files indeed are installed?
Thanks for the great comments! I love your work. &gt; Error stuff First off: I want to avoid printing/logging the errors as it is not extensible. For one thing would like to sort and format them by relevant severity/path-origin/etc. Secondly, I would like to have a generic method of handling that they happened at all. I would also _like_ to avoid allocating a vector for every function and then flattening that vector for the relevant results (i.e. I'm parsing lots of files, each file can have a vector of errors and warnings). The problem space is still somewhat vauge for me, but essentially I would like a way to efficiently mix logging with error and warning reporting. I'm not sure there is a good solution to this in _any_ language though! &gt; I think this line alone basically ensures that I can never use this crate precisely because such a guarantee cannot be made in the type system You can guarantee that when the file was _discovered_ it was that type. In most applications the files are not changing out from under you after that -- in the rare case where they _do_ you will still get the same `io::Error`, but those cases are rare. Also, did you see `PathFile::create`? That creates the file at the path if it doesn't already exist (assuming you _can_ create the file there). The point is that if you _can't_ find/create the file, then you get an error _where you are instantiating the type_ -- you fail fast with a helpful error. Obviously it is still possible the path goes away from another thread/user/machine-reboot -- but those issues are comparatively rare to "whoops, I forgot to actually create the file at that path" or "whoops, that was actually a directory". **Edit**: also I should note that `path_abs` has some amount of performance penalty for which applications like ripgrep probably don't want to incur. It represents a trade off for ergonomics+(almost)type safety vs speed. &gt; I'm curious to hear what you specific use case is that led you to need [serialization of paths]. I'm doing a rewrite of [artifact](https://github.com/vitiral/artifact) which allows you to write design documents in text files. I want the user to be able to edit (in a web-ui) where the file will get stored but it is currently (practically) impossible to serialize paths as UTF-8 for my webapp to consume in a cross-platform manner. When I first wrote it I was just learning rust and I basically called `path.to_string().unwrap()` everywhere, which will obviously bite me _eventually_.
Looks like the link to [embeded-hal](https://docs.rs/embedded-hal/0.1.0/) is broken: it points to https://docs.rs/embedded-hal/0.1.0/embedded-hal/ (which 404s) rather than https://docs.rs/embedded-hal/0.1.0/embedded_hal/. (The seconded embedded-hal should be embedded_hal). I also have a question about pin configuration. How efficient is the pin configuration HAL? For example, if I'm trying to charlieplex a bunch of LEDs I may want to change a large number of pin directions in a single atomic write. Even without required atomicity, writing 16 lines of code to change 16 pin states seems not particularly fun. Even if I do write those lines of code, will they be fused in to a single bit operation on the direction register? In C (and on an MSP430 since that's the platform I'm familiar with) one can write to the `PxDIR` registers and change the direction of every pin on the port at once. Is something similar possible with this API? EDIT: Another question: Do you have a vision for how device drivers would interact with asynchronous peripherals? For example (again going back to the MSP430) the I2C peripheral can operate while the CPU is off, so you can get some power savings by kicking off a register read and then only waking up every few bits to execute the like 4 instructions required to pull data out of the RX register, bump a pointer, and go back to sleep. Is there a good way to express that kind of interaction today? Regardless, great work on all of this stuff /u/japaric, your work on getting rust in to the embedded space is super exciting and inspiring.
Hi! Happy to answer any questions about our experience that the document didn't answer :) I've also been [filing issues about some of our difficulties in `aturon/rust-wasm`](https://github.com/aturon/rust-wasm/issues) to spark discussion and collaboration with other folks interested rust+wasm.
[removed]
You know what, I swear I read the whole post but I must have missed that section when skimming back to write my question. Thanks for pointing that out. That brings out a different issue, which is whether it's possible or even desirable for svd2rust to attempt to automatically consolidate peripherals into crates. I spent some time on this exact problem and don't think it's the right approach. 
I believe, the best place to start would be this blog: http://blog.japaric.io/ Author is on the bleeding edge of embedded Rust progress and, in some sense, he _is_ the progress :)
Wow! Assuming a pizza costs around 10 USD in average, japaric is getting almost 40 USP (United States Pizzas) per month! That's enough for an extended month! üò± 
haha, this looks great. maybe now hollywood hacker scenes can be slightly less outlandish looking
whats up with the unnecessary emphasis in your post, this isnt a 1990s mailing list
I don't have specific recommendations for you, but other people might give you more precise answers if they'd know what platforms you target (avr, arm, intel?). If you want to know how bare bones/low-level dev looks like in Rust, [intermezzo-os](https://intermezzos.github.io//) might be a good start. Unlike intermezzo-os, which is meant for education, [redox-os](https://www.redox-os.org/) would be a codebase for a serious OS written in Rust. [Tock-Os](https://www.tockos.org/) is an IOT / Realtime platform written in Rust. [Japaric](http://blog.japaric.io/) is basically the vanguard considering Rust on ARM. Definitely worth checking out! AVR isn't officially supported yet, but [it is being worked on](https://github.com/rust-lang/rust/issues/44052).
Same, I kinda forgot about it. What a great gift from past me.
It's so awesome that we can take normal rust and run it in a browser. Everytime I think about it, I get blown away.
Mine arrived at the office while I was away ill. Picked it up today. Amazingly, it arrived unharmed from the USA despite only being shipped in a large, flimsy bubble wrap envelope. Amazon UK needs to have a word with Amazon USA on their wrapping procedures!
 fn read_line_stdin() -&gt; String { ... } let start = read_line_stdin().trim().parse::&lt;i32&gt;().unwrap(); let stop = read_line_stdin().trim().parse::&lt;i32&gt;().unwrap(); for _ in start .. stop { ... } Or if you don't actually mean for loops, let mut cond = check(); while cond { ...; cond = check(); }
this is amazing and wonderful
A lot of the problems that people point out with inheritance (especially multiple inheritance) aren't really limitations in what it can represent. They're more like complexities that come up when you do interesting things with it. For example https://en.wikipedia.org/wiki/Multiple_inheritance#The_diamond_problem and https://en.wikipedia.org/wiki/Object_slicing and https://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science). Avoiding inheritance lets you avoid some of those thorny questions. That said, one of the (subjective, sometimes) advantages of traits/interfaces is that they're not as "closed" as inheritance. If I'm importing some type from another library, it's usually impossible for me to "add more inheritance" to it in my own program. The types it inherits from are fixed in its definition. But in many languages that use traits/interfaces, it's possible to add new interface implementations to someone else's type after the fact.
I'd contend that this is an example of bad code that became idiomatic because C has bad syntax, but alright.
&gt; The idea was that you could mark a type as only being constructable by requesting it as main parameter: Yeah, I have thought about something similar: custom entry points. Today the entry point of a user program is always main with signature `fn() -&gt; T where T: Termination`. My idea was to have the crate that provides the `start` lang item *define the full signature of the user entry point*. For example: // crate: cortex-m-rt struct Once { _0: () } // Basically the feature is that I can pick any signature for `main` here. Today `main` must be `fn() -&gt; T where T: Termination` #[lang = "start"] extern "C" fn start(main: fn(Once) -&gt; !, _argc: isize, _argv: *const *const u8) -&gt; isize { main(Once { _0: () }) } Then the caller can exchange the `Once` token for the peripherals without incurring in a runtime check: #![no_std] extern crate cortex_m_rt; // provides the `start` lang item fn main(once_: Once) -&gt; ! { let p = Peripherals::take(once); } This feature is not enough for RTFM though. There the user entry point is called `init` and the signature is *application specific*. The signature of `init` often contains types which are *defined in the application crate* and depend on whatever the user specified in the `app!` macro. The above feature is not enough because the signature of `main` can only contain types that, following the above example, the cortex-m-rt crate has access to. EDIT: syntax
There is a `with_body()` method on `Response` that takes `Into&lt;Body&gt;`. And `From&lt;Vec&lt;u8&gt;&gt;` is implemented for `Body`. So you should be able to do: Response::new() .with_body(source) As simple as that.
&gt; what platforms you target (avr, arm, intel?). At this point it's still largely a hobby interest. I could, hypothetically convince my employer that we could move at least some of our stuff to it but I wouldn't do that before I had gotten my own hands dirty first. I largely deal in ARM systems but I would be happy starting off on whatever has the best support currently and/or is easiest to start learning.
That's not a dynamic condition that changes _in the loop_. That can be expressed reasonably in a for loop, but cannot (reasonably!) with iterators.
The here we need.
&gt; However I'm a bit put off by Peripherals::take().unwrap() *spoilers* (This will be in the next blog post) You don't need this when using [RTFM](https://docs.rs/cortex-m-rtfm/0.3.1/cortex_m_rtfm/) v0.3.0. There you don't have to define `main` but a function called `init` which signature is `fn(p: $device::Peripherals, ..)` so it has access to the peripherals out of the box -- no runtime check or `unwrap` required. If you are not using RTFM then I think it's perfectly fine to write: fn main() { if let Some(peripherals) = stm32f30x::Peripherals { // do stuff with `peripherals` } } Since main is usually called once and that's where you usually write your initialization code. You can even wrap it up in a macro: main!(stm32f30x, init); fn init(peripherals: stm32f30x::Peripherals) { .. } That expands into: fn main() { if let Some(peripherals) = stm32f30x::Peripherals { init(peripherals); } } 
Thanks for the great tool! I should have been more clear. That issue will *allow* you to write tests for binary/styled/formatted output, but it will still be non ergonomic (requiring you to basically edit a binary string by hand for any changes). I hope to improve that situation by making a CLI testing framework (probably on top of assert_cli!)
I recently discovered [cargo make](https://sagiegurari.github.io/cargo-make/) and played around with it a little and so far it seems really good. 
Yup, that will unblock the ability to do it (this being the first step that we're being slow on). Our adding support for custom string assertions is another important step (this is part of the PR blocking binary support).
I can attest to the fact that developing a cli app in Rust is a very comfortable experience. I love it a lot ‚ù§Ô∏è and I will keep using Rust for all of my private projects where it is a good fit üòä 
This is fantastic! How does this compare to the existing Rust Source Map parsing libraries? Why did you choose to write a new one vs contributing to them? Should existing users switch to yours? Past success in Rust Source Map world included at sentry.io: https://blog.sentry.io/2016/10/19/fixing-python-performance-with-rust.html
&gt; Probably the biggest issues that I see have to do with the fact that SVD files are MCU-centric and have limited structure to indicate similarity of peripherals both within a MCU and between MCUs in a processor family. The SVD spec actually *has* features to indicate similarity at different levels: (a) there's the `derivedFrom` attribute for "inheritance" between elements, where elements can be whole peripherals, registers or bitfields (enumeratedValues); this can be used to indicate that, for example, the USART1 and USART2 peripherals are exactly the same *except* for their base address. (b) there's also support for arrays at the register and bitfield level; for instance, you can indicate that a register contains 32 1-bit bitfields that are exactly the same *except* for their offsets. svd2rust already exploits these features at different degrees. Now, not all the vendor make full use of these features. Some SVD files contain a lot of repetition where they could have been compact if the features I mentioned above had been used. That doesn't mean that us, users, can't transform SVD files to make use of the similarity features so svd2rust can exploit; we can -- in fact, we already do, albeit minimally and by hand. Ideally, we can build a tool to do these transform automatically or semi-automatically (with some user intervention). Regarding similarities across devices: the SVD spec doesn't have features for that and I don't think we should customize SVD files out of the spec to add that kind of information. What we can do instead is have svd2rust find the similarities and produce crate hierarchies as I described in the blog post. There have been [explorations](https://github.com/japaric/svd2rust/issues/96#issuecomment-315506630) in SVD file similarities and there's a lot of duplicated peripherals within a device line. &gt; I spent some time on this exact problem and don't think it's the right approach. If I may ask, what makes you say so?
I could do that? I have to admit, i am mostly clueless about how to properly use macros. Thank you, I will look into it.
Thanks for updating it!
If you are a student and your university uses "EzProxy", you can get access for free. 
Yeah the hex view one is the only one I'd really complain about. The others are quite good. Also, I'd speed up the bootlog one. The messages are too slow. I'd also make the timing more inconsistent. Output bursts of messages quickly from time to time.
You‚Äôve lost me, the point of TOTP is to keep others out of my account if my password gets/leaked found out. https://i.imgur.com/aBtEbyT.jpg 
Just yesterday changed my deoplete + racer setup into the one in these instructions, using completion manager + rls. It worked well for me. I probably need more time to find and iron out all the small details, but so far so good.
The paths ergonomics is my biggest problem with the std. And the error handling is the second one.
Tensor Programming YouTube channel is pretty good. https://m.youtube.com/channel/UCYqCZOwHbnPwyjawKfE21wg
Check out my `init.vim`, I have nvim-completion manager.
Ah yeah, I wanted to automatically pop the list! Let me try one of the plugins you suggested.
I'm working on rewriting the part of `relm` that parses the contents of the `view!` macro (the part that lets you write the GUI layout and event handling in a declarative way). The beginning has been slow going learning how to use the parsers provided by `syn`, so I intend to write a blog post (once I'm more comfortable with the topic) explaining how to get up and running doing the same thing.
what do you think of `path_abs`? Does it solve all/most of the ergonomic issues? I'm currently considering adding `move`, `delete` and `delete_all` methods onto the relevant types, but not sure if the API should consume `self` or take `&amp;self`. Would appreciate feedback. https://github.com/vitiral/path_abs
I've been interested in this as well. You might want to look at the supported [targets](https://forge.rust-lang.org/platform-support.html). Also things like [inline asmbely](https://internals.rust-lang.org/t/pre-rfc-inline-assembly/6443) is limmited to the nightly comilper for now. What I'm looking at setting up is an Arduino. As pointed out it's not officially supported yet, but I already have lots of them. So setting up the fork is worth it for me.
&gt; Also shoutout to the "{:#?}" pretty printing formatter. When I discovered it I felt like the largest pain point of rust just vanished. I still think pretty printing should be made the default.
Any thoughts on what a next wasm oxidation project might be?
unfortunately I think that would be considered backwards-incompatible.
There are some ideas in the "Conclusions and Future Work", but no proposals to rewrite any other particular JS libraries in Rust/WebAssembly. As with any suggestion to rewrite some library in a new language, it helps if you already have a relationship with the maintainers and are willing to put the work in yourself. So, I'm *probably* not going to hop on LKML and suggest they rewrite the Linux kernel in Rust that's been compiled to WebAssembly :-P
There will be a [book about wasm](https://github.com/aturon/rust-wasm/pull/12). I'm not aware of general docs on this topic, [filed an issue](https://github.com/aturon/rust-wasm/issues/28) for writing some, but for now I think the main point can be summarized with "not depending on system resources". E.g. the rand crate has seeded its RNG with some value from the OS. Created an unresolved import error when wasm code was using it. Or take some crate which parses some binary format. If it takes a path as its argument, its function depends on the file system. Thats not good for wasm as the std provided file system is not available. Better is providing an API for implementations of the Read/Write traits. Of course you can offer both APIs. This suggestion is useful in the non wasm world as well as sometimes you want to download something from the network or want to do unit testing etc. Then stuff isn't present as a file so here the generic Read/Write API is very helpful.
&gt; The relatively large code size footprint of the allocator suggests that writing or adapting an allocator that is focused on tiny code size could provide considerable utility for the WebAssembly ecosystem. At least for our use case, allocator performance is not a concern, and we only make a small handful of dynamic allocations. For an allocator, we would choose small code size over performance in a heartbeat. Custom allocators would be nice to have on stable eventually. \*fingers crossed\*
Yep, that's why I pointed that explicitly. I'm going to add encryption in the next release. Currently, it's pretty safe if you don't share your database file in the internet. (I'm not saying about Trojans - if you have them, it's really bad)
&gt; I would also like to avoid allocating a vector for every function and then flattening that vector for the relevant results (i.e. I'm parsing lots of files, each file can have a vector of errors and warnings). Why not just use traits and pass around the an error collection `Vec` around? Obviously this is a very high-level and handwavy observation. You're more familiar with the devils in the details than I am, of course. :)
Because I want to be able to make it parallel over multiple threads. BTW a `Vec` (actually a Queue) that can be pushed to by multiple threads is _basically_ what a `Sender` is!
Couldn't you do something like fn main(once: Once) -&gt; ! { let outside_world = OutsideWorld::new(once); let (peripherals, outside_world) = outside_world.init::&lt;Peripherals&gt;(); let (resources, outside_world) = outside_world.init::&lt;Resources&gt;(); ... } Except maybe made prettier behind a `let_init!` macro, and maybe with `init` going through a trait that can take parameters. The idea would be to pass the `Once` token down an initialization chain like `Init&lt;Peripherals, Init&lt;Resources, ()&gt;&gt;` that's generated in the user crate. It would set a fixed initialization order, which might be good or bad, or maybe worked around with some type/trait-shenanigans. Haven't had a lot of time to think about this exact use-case though :)
Will it be enough? Macbook Air 2015: https://gist.github.com/svartalf/116868007a0c23ceb7dc9d00d8592d19
Nah, the binary is mostly supposed to look cool. But I'll see what I can do.
Hey, I made this. Awesome to see it posted here! I'm super open to all feedback.
I don't think this will ever conflict, but you should know there is an old Sega Mega Drive/Genesis emulator also called "pgen".
The simple answer is that it's just a rule that the compiler requires you to have a string literal as the first argument to `println!`. A variable name is not a literal. Literals are like these: "test" 3 4.5 If you do `let name = "Steve Jobs"`, then `name` is **not** a literal. It is an *identifier.* With `println!`, it is actually generating the code needed to do the formatting _at compile time_. So, the format string needs to be a format string that can be dissected at compile time, not at runtime. When you pass in an identifier like `name`, the `println!` macro can't see _inside_ the string, so it can't figure out what formatting code to generate. Of course, the _compiler_ knows that the variable here is actually a reference to a literal string, but `println!` doesn't. Let's look at the code generated for a more complex example. `main.rs`: fn main() { let x = 3; println!("look: {} + 2 = {}", x, x + 2); } The expanded version is the code generated from the macro. It's one of the first steps the compiler takes, long before it generates machine code. The expanded version looks like this: fn main() { let x = 3; ::io::_print(::std::fmt::Arguments::new_v1_formatted( &amp;["look: ", " + 2 = ", "\n"], &amp;match (&amp;x, &amp;(x + 2)) { (__arg0, __arg1) =&gt; [ ::std::fmt::ArgumentV1::new(__arg0, ::std::fmt::Display::fmt), ::std::fmt::ArgumentV1::new(__arg1, ::std::fmt::Display::fmt), ], }, &amp;[ ::std::fmt::rt::v1::Argument { position: ::std::fmt::rt::v1::Position::At(0usize), format: ::std::fmt::rt::v1::FormatSpec { fill: ' ', align: ::std::fmt::rt::v1::Alignment::Unknown, flags: 0u32, precision: ::std::fmt::rt::v1::Count::Implied, width: ::std::fmt::rt::v1::Count::Implied, }, }, ::std::fmt::rt::v1::Argument { position: ::std::fmt::rt::v1::Position::At(1usize), format: ::std::fmt::rt::v1::FormatSpec { fill: ' ', align: ::std::fmt::rt::v1::Alignment::Unknown, flags: 0u32, precision: ::std::fmt::rt::v1::Count::Implied, width: ::std::fmt::rt::v1::Count::Implied, }, }, ], )); } Now, that's scary! If we clean it up a little, we can imagine the code looks like this: fn main() { let x = 3; print(Arguments::new_v1_formatted( &amp;["look: ", " + 2 = ", "\n"], &amp;[ ArgumentV1::new(&amp;x, Display::fmt), ArgumentV1::new(&amp;(x + 2), Display::fmt), ], &amp;[ Argument { position: Position::At(0usize), format: FormatSpec { fill: ' ', align: Alignment::Unknown, flags: 0u32, precision: Count::Implied, width: Count::Implied, }, }, Argument { position: Position::At(1usize), format: FormatSpec { fill: ' ', align: Alignment::Unknown, flags: 0u32, precision: Count::Implied, width: Count::Implied, }, }, ], )); } This code is being *generated* from the `println!` macro *at the moment of compilation*. The code generated here changes based on what the format string is. If the first argument to `println!` changed to different things at runtime, then this code *couldn't* exist. If you look at this code, you can see the "look: " and the " + 2 = " strings and the newline at the end, these are the static (unchanging) pieces of the format string that were dissected. It then creates `ArgumentV1` values which contain the dynamic parts, `x` and `x + 2`, and then it creates `Argument` values which define how to format those dynamic pieces, such as with floating point numbers if you specified that it should only emit 2 digits after the decimal point. The `print` function can just go down the list: emit the first static chunk of the string, then emit the first dynamic argument, then emit the next static piece of string, and so on. It knows all of this at compile time. Since this code is generated at compile time, it also gives us useful error messages. If you try to do `println!("X is {}");`, then you will get an error at compile time about the unused formatting item. Or if you try to do `println!("X is {}", x, y, z);` you will get an error message about providing too many arguments. This allows the compiler to catch errors before you even run your code, and theoretically allows for better formatting performance too.
I see one fan and many sensors. Do you have the program sensors on your MacBook Air? That gives out some useful info about the sensors (where the measure) and fans (min / max rpm). I can't make a profile without testing it. I'd have to guess the temperatures under various workloads.
Weird... you shouldn't need to set `omnifunc` if you have nvim-completion-manager...
I missed this question: &gt; why does it work for example number 2? It's the same non-mutable variable in both cases and the "{}" is just a placeholder. `{}` *is* a placeholder. `{:?}` is a debug placeholder that will print additional debugging information. For a floating point number, `{:.2}` would specify how many digits to print, etc. &gt; Or does "{}" do any form of type-checking? I would be surprised if it did. Well, I guess it's your lucky day to be surprised, because it absolutely **does** do type checking. It ensures that the type you're trying to print *can* be printed, and it checks to make sure that you're supplying the right number of things to be printed, and several other things. This is the fundamental reason that the first argument has to be a string literal. It *could not* do these things if it can't peer into the formatting string and know what to do at compile time.
&gt; that can act as a substitution of TRPL? partially: http://intorust.com &gt; that can act as a [wonderful addition to] TRPL? https://www.youtube.com/channel/UCaYhcUwRBNscFNUKTjgPFiA ;)
I hear where you're coming from, but I respectfully disagree, and I freely admit bias due to the domain I work in (not Rust). We have a lot of devs and a lot of logs -- which can roll over -- and if everybody prettified up their output, the logs would just roll over more quickly. For the majority of people who don't care about the FrobNoz you just displayed, it's also less distracting to skip past. I further think that it's easier to enforce a rule like "always do scrunched representation" when it's the default behavior anyway. IOW I think it scales better. Cheers!
`slog` author here. What you want is doable with `slog`, and exactly a use-case that would fit `slog` well. You can implement your own `slog::Drain` that can react in any ways to all logging messages: looks for stuff and sets flags etc. If you're looking for easy to use wrapper over `slog-rs` ecosystem, use `sloggers`. I'm aware that getting started with `slog` might seem difficult, but it's because `slog` is very, very flexible. Internally it's very simple, so after you get the right mental model, it's a stroll in a park. If you need more help, come join our gitter channel. :) BTW. I'm a big fan of NixOS, and there is more rustaceans that are combining the power of the two: https://pijul.org/2017/12/12/buildrustcrate/
[Europa Park ](https://en.wikipedia.org/wiki/Europa-Park) :D
That `init` pipeline looks ... intriguing. In any case, even if we had this feature I think RTFM user interface would remain the same because we need this `init` + `idle` split which is required for memory safety of some features. A single user provided `main`, even with custom signature, wouldn't be enough. So, yeah. This is a hypothetical feature I wouldn't prioritize at the moment.
`use handlers::index::foo;` (if `handlers` and `index` are declared with `pub mod` or `pub(crate) mod`).
Cool, good to know. I'm thinking of rewriting [nedb](https://github.com/louischatriot/nedb) in rust as it looks like it is _only_ 1500 lines of javascript code. It would be a fun side project and would be an awesome target for structured logs from `slog` (plus, I could see it being useful compiled to webassembly). The basic structure would be: - before calling a function you would create a unique context ID which somehow gets passed into the function - the function logs using that context - you can query what kind of logs happened during that context. I know from the docs that contexts are `slog`'s bread and butter, so there is just querying left and it sounds like you have a good solution. Thanks the for the great responses!
Thank you! &gt; The docs team did one a long time ago, and we'd like to do another this year. I'd love to help out with this.
Do you mind explaining how? Just in the library? 
I've had this discussion before and I don't think there are any stated guarantees about `Debug` formatting, the same way there weren't guarantees about error message formatting and we changed those. Copy/pasting from my other comment: The `Debug` trait only says &gt; Debug should format the output in a programmer-facing, debugging context. It doesn't say how it should format it. Only that it's for active debugging (not tracing production events like the usual info-level logging from most libraries), and that it's supposed to be programmer facing. The guarantees are little more than "some bytes that humans should read are to be written". I suggest that pretty prettying by default is a much more useful default for humans. Absolutely, anything that relies on compact formatting should have a way to specify that's what they require; I just think the default formatting should be human-friendly first. Moreover, "default" does not mean "only". You could, for example, have a formatting sigil like `:$?` that specifies compact formatting. You could have a setting for rustc like `default-format=compact` which forces the bare `:?` one way or the other. You might even have a cargo option so that you can have compact in release mode but pretty in debug mode.
What is the best supported board for Rust currently? I'm looking to build a midi sequencer and fancied using the project to learn Rust. Bluepill looks far too small for my needs. I presume the STM32F4Discovery is the next best option?
This is fantastic.
So technically you are probably correct. However, I don't think the rust team is likely to be on board flipping a switch like that. Some people might be logging debug messages to a file in their legacy application, it would not be good to suddenly have those logs be multiline because the rust version changed. That could make some people very angry! Rust is in step with other popular languages like python, which requires "opt in" for their pretty prints. I would support a `default-format` flag in cargo though, I think it should be added! However I think the default should probably be compact, and someone can specify `pretty`. You would need to add the `:$?` or something to force compact though.
did you copy paste this to the other thread? Did you know about the `permalink` button at the bottom of comments? It's better to just redirect everyone to the same thread. [here](https://www.reddit.com/r/rust/comments/7r9xoo/rust2018_and_the_great_cli_awakening/dsw3hwb/) is the other thread where I left a comment.
What is the reason behind `Vec::split_off(&amp;mut self, at: usize)` panicking if `at &gt; len`? Why doesn't it return a result like nearly every other method that may fail. I don't think it is because of performance: the check has to be done anyway and the memory overhead of a `Result&lt;Vec, Error&gt;` would probably be neglectable.
Haha, surprised indeed. :-) Before we get into this, thanks so much for your help. Your exhaustive answer(s) are appreciated! So let me get to this point first, because it is easier. Once we cleared that up, I can work through your other example. It has received lots of upvotes, so you must be right! ;-) I actually understand everything you wrote about "{}". "{}" is not just a placeholder, but also a type checker. Easy. The following question is phrased very layman-y and not because I want to disrespect the Rust team or Rust istelf, but because I need to feed my brain something it can understand. So: For some reason println (or is it Macros in general?) isn't just incapable of type-checking, but it also prevents the compiler from type-checking for it? That's why "{}" is necessary to do the type-checking. Is that correct? If so, this sounds like a horrible design flaw :-) (I'm just joking, obviously it is just something I don't understand yet) So why is it done that way? If it's too complicated to explain to a beginner, then I don't need to know the detailled answer, but at least some sort of hint ("it has to do with how the stack works", "it's a performance issue", etc...) I can come back to this at a later point in time and with more Rust knowledge/skills to dig deeper into it. The thing I'm so confused about is that why this is even a problem? I'm coming from Python and if I have a Python variable called "name" that was initially a string "Steve", that variable can easily and without any error be changed to an integer of 5 for example. Since Rust ist statically typed, in Rust "name" will always and forever be a string. If "name" is always and forever a string, then why is the Rust compiler not smart enough to remember that? In what possible scenario could "name" not be a string? Or in other words: Why did Rust/rustc forget that "name" is a string? Let me phrase this in other words one final time to make my cognitive problem more apparent: Let's say the Rust compiler could think out loud: "Hmmm ok, he wants to print a variable called "name", let's see what type it is. Ahhh ok it is already a string and will forever remain a string, awesome, so let's just go and print it out for him". -&gt; S-t-e-v-e However the compiler seems to be not sure enough, so he asks/needs the "{}" placeholder to please double-check if it is really a string. And that part is the part I'm not getting. Why? How is there any confusion if it's a statically typed language?
&gt; The first pass for that particular API has been focused on correctness rather than code efficiency. An entirely reasonable approach =) &gt; The answer is no because each line will turn into a volatile write operation and those are not mergeable by the compiler so you'll end with 16 register writes. Yep, makes sense. That was my gut feeling but I was hoping for something clever =P. Your proposed updated API seems like a step in the right direction, especially if there's a way to make that 0-cost. &gt; Async reactive drivers, i.e. drivers that integrate with interrupts, are on my TODO list. I have done zero exploration on that area so I can't comment at the moment. Sounds good. It's definitely a hard problem. Perhaps a similar approach to how pins are now handled would work? That would probably necessitate either a layer of indirection (via a bunch of handlers in the board support crate calling closures or similar) or doing tricky stuff like dynamically rewriting the interrupt table. I don't even know if the latter is feasible though. On platforms like the MSP430x this could cause problems if the handlers end up located in high memory since their address would no longer fit in the vector slots. Plus it would potentially require users to give up RAM which they may not appreciate. &gt; What's described in the "what's next" part of the blog post are generic async cooperative drivers and exploration is blocked on having impl Trait in trait methods. Makes sense. &gt; What you can already do today is writing non-generic cooperative code using futures / generators; that works fine except that I have not been able to properly integrate sleep into the equation so those programs always end with 100% CPU usage (when no task can advance the CPU continuously context switches between tasks instead of going to sleep). I got a potential, but imperfect, solution for Cortex-M that I have not tried yet but I don't know if it will work on MSP430 as it depends on the sleep and interrupt mechanics of the architecture. I think MSP430 has problems with sleeping in Rust since we're still (AFAIK) missing the `__{bic,bis}_SR_register_on_exit` intrinsics ([ref](https://github.com/rust-embedded/rfcs/issues/20)). This unfortunately means I can't really use Rust for my current embedded project but I'm definitely keeping an eye on the space. Thanks again for all the work you have been doing to get rust into the embedded space, and shout out to /u/pftbest et. al. for their hard work on getting MSP430 support together =D. If there's anything I can do to help you guys out I would love to lend a hand.
One other thing that hasn't been mentioned because it doesn't directly relate to your question: &gt; It is a non-mutable variable, so the compiler should know that whatever is passed into println is definitely a string! I mean it is as easy as "If name is string -&gt; OK, else -&gt; error. I could understand the error, if I had used a mutable variable. But I didn't. You've misunderstood "mutable" in this context. `name` in your code still has a type (`&amp;'static str`) even though you didn't type it - it was *inferred* by the compiler. let mut name = "Steve Jobs"; name = 69; // nice ‚û° error[E0308]: mismatched types --&gt; src/main.rs:3:12 | 3 | name = 69; // nice | ^^ expected &amp;str, found integral variable | = note: expected type `&amp;str` found type `{integer}`
Was following https://github.com/redox-os and saw thing cool thing pop up
I did say "Copy/pasting from my other comment" :D I considered linking but I had some minor additional things to say individually and figured you'd each have different replies and it'd devolve into different threads anyway so I said "eh - it barely matters - Ctrl+C/Ctrl+V"
&gt; Makes use of EFF wordlists which were designed to contain human-friendly words Is there any kind of internationalization there ? Or could it be added in future versions ? Because ¬´native english speaker-friendly¬ª is pretty restrictive view of ¬´human-friendly¬ª ;). 
Ah yes, you're right of course. I should have said "statically typed" instead of non-mutable. I actually clarified that in my reply to "coder543". And this is also why I'm confused. If "name" is always a string, why do I need "{}" as a type-checker-safety-mechanism, if name is always and forever a string anyway? Something the compiler should remember.
Well, you can try going to https://www.safaribooksonline.com/ . Sign in with just your school email ID. No password. Click the link in confirmation email to activate your account (create password etc.). That should be it. 
&gt; Currently, it's pretty safe if you don't share your database file in the internet. Well, no, because the threat model that 2FA protects against is full compromise of one system but not two. That said, there's no reason why system 1 == desktop/laptop, system 2 == smartphone/tablet, so there's niche value in having this kind of tool available. Also, some websites force some form of flawed 2FA on you like password + email when you're using strong, unique per website passwords with a password manager and a local email client. Humblebundle for example lets you choose between email or otp but not nothing. Switching that to a local otp generator doesn't help your security but it doesn't lower it either.
Hey thanks for the response. I must admit I am not well versed in Rust yet, but I have been keeping an eye on it. Working on low cost motor control solutions with M0+ cores, how Rust handles access concurrency issues and HAL's are of particular interest to me. Our memory sizes are low enough that we don't typically consider pre-emptive or cooperative threading though. Reading the post, I see lines such as: let ok = Peripherals::take().unwrap(); ... let clocks: Clocks = rcc.cfgr.sysclk(64.mhz()).pclk1(32.mhz()).freeze(); and I think there is some object oriented features providing this... which means vtables. Perhaps I should go learn Rust, but is my understanding correct here? I've written some C++ code targeting embedded environments, and you can still write rather small code, but things like vtables start to add up over time. Time spent jumping to virtual functions is also something that cant be ignored in realtime systems, which marginalizes their use a bit in tight loops. I wonder if Rust will find a place in low cost, bare bones MCU projects that have typically used simple time division based schedulers. 
To put a different spin on the answer given by /u/coder543 ... &gt; why does it work for example number 2? It's the same non-mutable variable in both cases and the "{}" is just a placeholder. Unlike in other languages, Rust's `print!` is a macro, rather than a function call. Macros allow defining custom syntax rules between the parens, so "You cannot write `print!(name);`" is checked at the same level as "You cannot write `let foo = - + -;`" As for *why* it's requiring a literal rather than "any valid expression"... &gt; It is a non-mutable variable, so the compiler should know that whatever is passed into println is definitely a string! I mean it is as easy as "If name is string -&gt; OK, else -&gt; error. I could understand the error, if I had used a mutable variable. But I didn't. So why is the compiler complaining? After all it isn't just a warning, but an error. Macros get expanded at compile time and what `print!` expands to depends on the contents of that string, so, if it accepted a variable as the first argument, the compiler would have to run part of your code at compile time to figure out what to expand it to... that gets complicated and confusing. (Not to mention, it starts to feel more like the "just do what I mean" aspects of type systems in scripting languages like PHP or Python, which are simpler and more comfortable... until they unexpectedly break or silently do the wrong thing because they guessed the wrong intentions from what little you gave them.) It's much simpler to implement and produces more maintainable code if it just requires a string literal. ...and the reason the expansion of `print!` depends on that first string's contents is... &gt; If this is some Rust memory-safety feature, then why does it work for example number 2? It's the same non-mutable variable in both cases and the "{}" is just a placeholder. Or does "{}" do any form of type-checking? I would be surprised if it did. `print!` actually *does* do type checking. `print!` is a "procedural macro" built into the compiler and, as such, it's code that runs at compile time, to generate new code which will them be compiled. In this case, `print!` parses the string literal's contents, type-checks them, and then generates optimizer-friendly code substituting variables into that specific string pattern. (As far as type-checking goes, the gist is that every `{}` must bed fed a type that implements the `Display` trait and every `{:?}` must be fed a value of a type that implements the `Debug` trait.) &gt; How should a beginner in Rust possibly understand this? Understanding how the innards of `print!` work is a bit of an ambitious task for a beginner because... 1. While friendlier than most, it's still machine-generated code of a sort. (A comparable example would be [this CoffeeScript example](http://coffeescript.org/#try:cubes%20%3D%20%28math.cube%20num%20for%20num%20in%20list%29)). 2. The innards of the standard library are written to be efficient by people who are likely the foremost experts on the language, even if that means that they're somewhat difficult for a beginner to follow. (You wouldn't expect the innards of C's `printf` to be easy to read, would you?) Also, I believe that's the old `println!` code, since I'm only seeing `std::old_io::stdio::println_args` in my search results. That said, since it's the more compact example, I'll use it to explain a few things: #[inline] #[allow(dead_code)] These are [attributes](https://doc.rust-lang.org/reference/attributes.html). They're basically post-it notes used to pass instructions to the compiler. `inline` suggests to the compiler that it should make a copy of whatever it is here, rather than referencing a copy elsewhere. ([When in doubt, trust the compiler's default behaviour.](https://internals.rust-lang.org/t/when-should-i-use-inline/598)) `allow(dead_code)` tells the compiler to not fire off a warning if the generated code contains bits that are just going to get optimized away without doing anything. static __STATIC_FMTSTR: &amp;'static [&amp;'static str] = &amp;[""]; This creates a [static](https://doc.rust-lang.org/reference/items/static-items.html) variable named `__STATIC_FMTSTR` which is a reference (`&amp;`) (with [`'static` lifetime](https://rustbyexample.com/scope/lifetime/static_lifetime.html)) to an array `[...]` of string [slices](https://rustbyexample.com/primitives/array.html) (`&amp;str`) (each with `'static` lifetime) and initializes it to `&amp;[""]` (a reference to an array containing a single empty string slice). (A string slice `&amp;str` is used because it can be a reference to a string literal, part of a string literal, a `String` object, or part of a `String` object.) ::std::io::stdio::println_args(&amp;::std::fmt::Arguments::new( ... call `::str::io::stdio::println_args` and feed it a reference to a freshly-created `::std::fmt::Arguments` object... __STATIC_FMTSTR, &amp;[::std::fmt::argument(::std::fmt::Show::fmt, __arg0)] ...which was created by feeding it the arguments you passed to `print!`. (I won't go into more detail since I think you'll be able to work out what's going on based on my previous comments.)
I've never met any for loop that had its condition mutated by the loop body that was worth keeping as such Considering the well known isomorphism between C for and while loops, I'd even say that the iteration/repetition divide such as Rust used is a definite good.
It's not about whether Rust can type-check the statement `println!(name)` or not, it's about the expectation that the first argument is always the format string, since that is the *documented* purpose of the first argument of `println!`. If you do `println!("Steve Jobs")` that's fine, because "Steve Jobs" is now a format string that contains no substitutions. When you do `println!(name)` you're saying the variable `name` contains a format string... but I'm **not** going to tell you what it is. You're going to have to wait until later to find out! **It could be anything!** The `println!` *macro* doesn't have full access to everything the compiler knows. It asked you to give it a format string, and you gave it an identifier. This makes it sad. If `println!` allowed you to pass in a format string which is *not* a string literal, as an identifier, then everything would have to be done dynamically at run time, which would be less efficient than determining exactly how to print the string efficiently at compile time, when it doesn't matter how long it would take. When done dynamically, it also wouldn't be able to type check the substitutions in that format string, or check how many placeholders are in the string versus how many arguments are provided. Rust likes to catch mistakes at compile time, before your code is deployed to production. [Python has several kinds of format strings](https://zerokspot.com/weblog/2015/12/31/new-string-formatting-in-python/) these days. Some Python examples: '{0}, {1}, {2}'.format('a', 'b', 'c') or 'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W') In the case of your string "Steve Jobs", it's *still* a format string, just one that has no placeholders to be substituted with real values. But, `println!` *still* needs to know that there are no substitutions to be made, and it can only know that if the literal string "Steve Jobs" is the first argument. If it isn't, the `println!` just sees a blackbox. `println!` *inherently* uses the first argument as a formatting string. `println!("{}", name)` provides a formatting string that will have one substitution, and then you provide the substitution as name. You could do something like `println!("{} lives at {} in the great city of {}", name, address, city)` and it would substitute in those three parameters at the locations of the three placeholders. If you don't want to use a formatting string, you can write the string directly to the standard output doing something (generally considered unidiomatic) like this: use std::io::{stdout, Write}; let name = "Steve Jobs"; stdout().write(name.as_bytes()).unwrap(); The format strings are largely handled at compile time, so there's no advantage to doing it this way, it's just a lot more verbose. There *are* reasons to use `stdout` directly, but I don't think they're important right here.
I also believe I confused you with one quote: I said: &gt;&gt; because it absolutely does do type checking. then you replied: &gt; However the compiler seems to be not sure enough, so he asks/needs the "{}" placeholder to please double-check if it is really a string. And that part is the part I'm not getting. Why? How is there any confusion if it's a statically typed language? In my statement, "it" was referring to `println!`, not `{}`. Using `println!` ensures that your print statement is type checked at compile time, rather than doing it dynamically at run time like a lot of languages. `{}` itself *doesn't* indicate a particular type, and it's not there to "reassure" the compiler that it is a string. It is a statement that I have a value that needs to go here. `println!("{}", name);` says replace the `{}` with the value of `name`. The first argument of `println!` *must* be a string literal that determines where arguments get substituted in. `"{}"` is a format string that consists *only* of a substitution. But, as I've said in other places, there are usually other things going on. `"The {} walked to the {}"` is a format string that contains two placeholders, which *require* values to be substituted in at some later point in time.
An important distinction is that traits are type driven so you can do return type dispatch. Traits also can express a Self type so you don't need an f-bounded type or runtime checks to abstract over things like equality. Traits with superclass constraints also effectively assemble on demand which avoids a lot of potential boilerplate you might have in java. That's one of the reason the collections framework in java has optional methods that may crash if they can't be implemented for some type.
oh this isn't the game's subreddit whoops...
It's not that the compiler doesn't know that `name` is a string; rather, it's that `println!` doesn't know what `name`'s _value_ might be at runtime if it's not a literal. For example, try compiling the following code: fn main() { println!("{} {}", value1); // missing the second parameter } `rustc` will blow up and complain at you about forgetting to give another parameter to `println!`. It can do that because it knows how many placeholders are in the format string. Now look at the following code: extern crate rand; fn main() { let x: u32 = rand::random(); let fmt = if (x % 2 == 0) { "{}" } else { "{} {}" }; println!(fmt, "hello"); } Let's say we could actually compile and run this. How would `println!` know, at compile time, whether you passed it the right amount of arguments or not? Half of the time it'd have the right amount of arguments, half the time it wouldn't, and there's no way to tell the difference short of running your program. So `println!` needs a literal because it can't tell what the format string's value would be in _arbitrary_ code. But in your case, shouldn't the compiler be able to tell that `name = "Steve Jobs"`, since it hasn't changed between a literal assignment and printing it out? Well, yes, theoretically it can, and this is where it becomes design philosophy instead of can/cannot. This specific case would be handled by a compiler optimization called [constant propagation](https://en.wikipedia.org/wiki/Constant_folding#Constant_propagation), substituting the string "Steve Jobs" for the variable `name` where it appears later. However, this usually happens _after_ macros (like `println!`) have already done their code generation. To make this case work, constant propagation would have to run _before_ macro expansion, complicating the design of the compiler for not a whole lot of benefit, especially since you can always just write `println!("{}", name)`.