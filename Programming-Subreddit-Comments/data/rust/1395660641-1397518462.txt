I have many draws to rust besides the safety... you can broaden your audience for this language.. surely that would be a good thing. r.e. 'signalling that you know what you're doing with 'lifetimes' - you are specifying this in the function prototype, where the lifetimes usually need to be explicitely annotated
&gt; I have many draws to rust besides the safety... you can broaden your audience for this language.. surely that would be a good thing. I don't believe this is significantly narrowing the audience. Any really ugly `unsafe` code will theoretically/hopefully be a very small proportion of any codebase e.g. just the very deepest parts of the structure loader module, and everything that uses that module doesn't need to see it... and if loading a binary blob is the most important part of your video game, and avoiding crashes as much as possible isn't (and no, testing doesn't work in general: crashes can happen due to weird input, rare interactions, etc. that testing never sees)... well, that seems peculiar. Frankly the thing that will narrow the audience the most is how annoying the compiler is: it's really hard to get something to compile. (But once it does, it's probably pretty close to correct.) &gt; r.e. 'signalling that you know what you're doing with 'lifetimes' - you are specifying this in the function prototype, where the lifetimes usually need to be explicitely annotated I don't understand were this came from? I haven't mentioned lifetimes at all?
&gt;&gt;I don't understand were this came from? I haven't mentioned lifetimes at all? others were telling me you need to signal lifetime &amp; type change seperately (perhaps i should have targetted that reply broader) &gt;&gt;Any really ugly unsafe code will theoretically/hopefully be a very small proportion of any codebase yes, but, *some peoples entire job is the ugly code* *some peoples entire business is the ugly code* (middleware). desktop GL is the tip of the iceberg. On consoles you exploit unified memory. The GPU doesn't speak your language, and you exploit the efficiency of unified memory for graphics resources. Some engine programmers go as far as saying, "openGL has no place on a modern console". They made a GL variant available on the PS3 but you'd be the subject of ridicule if you used it, its performance was so poor. Surprising , huh? just the pointer chasing work abstracting objects behind safe handles and so on killed it. On the current gen, CELL is ditched and you have GPGPU instead. again, something outside of the langauge. &gt;&gt; " just the very deepest parts of the structure loader module, " but this is *changing all the time.* the whole point of tools is to offload work to *simplify* the runtime. There's not much to crash in such a loader, you just get the blob and thats it. The runtime data is immutable. The point is to eliminate all the pointer chasing and unpredictable alllocations building it. The tools did the hard work of clustering and layout that will make it fast. But, the datastruture itself is changing from one day to the next as the design &amp; demands change. The difficult bit there was keeping rapidly changing tools &amp; runtime 'view' in sync. Keeping tools code and engine code in sync. (might even be written by seperate people) Rust macros would be incredibly helpful - rolling a "tools view" of a datastructure (dynamic collections) and a static,immutable 'runtime' (non dynamic). Someone on IRC expressed surprise that you might want to instantiate different versions of the same datastructure.. we did it all the time. &gt;&gt;"and no, testing doesn't work in general" but the language can't fix these issues either. The data goes out of the langauge and back in again at a seperate point, out of one program and into another, via binary. the type system cannot verify whats going on. The only way the language can help is macros helping me to roll a "validator". (I spent a lot of time writing debug "visualizers" of intermediate data.). The rust macro system is a big plus point. 
In your example, if `borrow` wasn't mutable, wouldn't Rust still cause an error for an invalid memory read?
Yeah, it's still invalid if it were `let borrow: &amp;int = v.get(0);`. The problem is actually the mutable borrow of `push`: a `&amp;mut` pointer has to be the unique usable pointer to a piece of data, so if you attempt to create one when either a `&amp;` or `&amp;mut` already exists, it's bad. So I should've been clearer: the actual rule is "a `mut` borrow cannot alias/overlap with any other pointers/borrows". The fact that the compiler emits an error message like "can't borrow `x` as mutable twice" is just to make the error messages nicer, it's not the fundamental rule; i.e. the compiler sees that the programmer has written two `&amp;mut`s; if they have written one `&amp;mut` and `&amp;`, the same rule triggers, but a different error message is shown.
I guess it's nice if you find these fubctions helpful but I do not see them ending up in rusts libc. You could always try by creating a PR on github but it's probably best to just write a library. 
"create a library", its more likely i'll cut paste it around. a library (dependancy if i want to submit to someone else) for a few lines of code is overkill. how about something simple like .to_void_ptr() , i know Vec has .to_ptr(). even that would help. If you had that at least, I'd just need a few lines elsewhere Part of the pain comes from so much casting of integers and it just adds up with the voids :( The helpers could do a useful job, asserting that information is not lost (it's almost always an upcast the whole point is compresion, but I have had a situation with 64bit int and 32bit adressing). perhaps you could make offset(..) do that; You've got issues with pointer-size in servo, the issue of compressed-pointers can't be completely alien, right? pub extern fn offset&lt;T&gt;(dst: *T, offset: int) -&gt; *T -&gt; pub extern fn offset&lt;T,I:Int&gt;(dst: *T, offset: Int) -&gt; *T /* performs upcast, and compile-time asserts size_of&lt;Int&gt; &lt;= size_of(int), and that sign information isn't going to cause a problem... usually its going to be i32 -&gt; int, u32 -&gt; int, etc. maybe even you could have a trait for types that are safely coercable to offset.*/ I have wondered if there's a place for a 31bit,63 type etc.. which specifies that sign bit wont be a problem when you go from unsigned-&gt;signed 
You might get more discussion about your ideas on IRC, if nothing else your writing style would suit it well :).
To me the primary reason why they can't be relied on is that when you run them, they only catch the bugs that *occur* and often, bugs that are found incrementaly in a software's development (just see how many libc bugs and exploits were found throughout the years) just couldn't be caught with Valgrind and similar tools because they were just either not happening or not causing any harm (i.e. not accesing a sensible chunk of memory and so forth) to be actually detected. Valgrind is a great tool but it cannot give you a safety guaranties. And safety has become a crucial matter since the raise of Internet where we need everything to be reliable and secure. This nearly as much the case before.
By the way, semantic tradeoff (which you will minimize by experience) is way better than performance tradeoff or safety tradeoff.
Claiming "I guess I'm just too stupid, I mean, I'm only smart enough to program C++ without error at all times" comes across as trolling, to be honest. :/
Know I'm coming in late, but do you plan on using librustc or libsyntax to accomplish this? I looked through on mobile, and didn't see it used. I want to work on a plugin that takes in text and a section of code, and returns its type, and I wanna know how to get started. I really really want to be part of the IDE 'effort' in rust, and I hope this takes off.
Sorry, i just didnt understand......
size_of() in Rust is the same as in C, that is, it will return a size that will be valid for C-style pointer arithmetic. So the FillsBytes struct will always have a size of 16 bytes, because that's the smallest size that can ensure that two consecutive FillsBytes in memory (e.g. an array or a vector) both have valid alignment (which is 8 bytes because of the u64). The [wikipedia article](http://en.wikipedia.org/wiki/Data_structure_alignment) on the topic is pretty good as far as I remember. And you can rely on Rust structs being binary compatible with C structs (i.e. they'll follow the same padding and alignment rules).
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Data structure alignment**](http://en.wikipedia.org/wiki/Data%20structure%20alignment): [](#sfw) --- &gt;__Data structure alignment__ is the way data is arranged and accessed in [computer memory](http://en.wikipedia.org/wiki/Computer_memory). It consists of two separate but related issues: *data alignment* and *data structure padding*. When a modern computer reads from or writes to a memory address, it will do this in [word](http://en.wikipedia.org/wiki/Word_(data_type\)) sized chunks (e.g. 4 [byte](http://en.wikipedia.org/wiki/Byte) chunks on a [32-bit](http://en.wikipedia.org/wiki/32-bit) system). *Data alignment* means putting the data at a memory offset equal to some multiple of the word size, which increases the system's performance due to the way the [CPU](http://en.wikipedia.org/wiki/Central_processing_unit) handles memory. To align the data, it may be necessary to insert some meaningless bytes between the end of the last data structure and the start of the next, which is *data structure padding*. &gt; --- ^Interesting: [^Computer ^science](http://en.wikipedia.org/wiki/Computer_science) ^| [^Hash ^table](http://en.wikipedia.org/wiki/Hash_table) ^| [^Pointer ^\(computer ^programming)](http://en.wikipedia.org/wiki/Pointer_\(computer_programming\)) ^| [^Scapegoat ^tree](http://en.wikipedia.org/wiki/Scapegoat_tree) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cgawbpm) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cgawbpm)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; I don't think we should go back on the Rust's "make safe code easier to write" principles by relaxing rules that help people do the right thing. I was just remarking that the argument "your case isn't common enough" is not a very good one. "Your case isn't common enough in _my field_/_the code I write_" is probably a better one. For a language to program from toasters to supercomputers you can be damn sure that those not so common cases in your field are the bread and butter of every day in a different field. I'm not saying "optimize the language for those programmers". My point was just "don't forget them". 
They haven't stopped the sea of exploits. In general it's because unit tests and fuzzing can't cover all the possible input spaces in huge complex software. The only approach that's really been shown to scale is to have the compiler verify that memory safety bugs can't happen, via a type system—a type system makes the problem tractable by constraining the problem space in a way that testing can't.
Minor point: action: *ActionFunc, I believe that could be just `action: ActionFunc` with no difference, except the code being safe. (A `fn(...) -&gt; ...` type is already a pointer to the function, not the chunk of code itself.) At the very least, it could be `&amp;'static ActionFunc` so that it is safe.
These human language-like DSLs remind me of Cucumber (http://cukes.info/). This can be helpful if you have less technical users assisting with testing, business logic, or game logic. I'm very interested in seeing what cool DSLs come out. I'm too focused on porting some other work to Rust, but I want to work on a more significant DSL at some point. I'd love to see a DSL for reactive programming (Microsoft Rx, Haskell FRP, or Objective-C ReactiveCocoa). Technically FRP does not need special syntax, but it could look more natural. FRP could be a great compliment to tasks since it naturally works well with pipes. A DSL for creating parsers from Bison-like BNF would be great. Maybe a DSL for representing Objective-C objects would be cool–although currently that will require some hacks without a few FFI extensions (trampoline support for dynamic dispatch, linker support for selectors so they don't need dynamic lookup, and a way to call Objective-C blocks as true closures). A SQL DSL that can check syntax has been suggested. These would require the more powerful procedural form of syntax extension then mentioned in this article, but it is cool that you can do it. I think Rust could go way beyond other meta-programming languages. Low-level access opens up a lot of domains that are not possible in Ruby or various functional programming languages.
Although I've been following Rust development closely, I've not yet had time to dive in. But this looks amazing. So incredibly intuitive. Sharing my code with others will make it so much more readable this way.
Does [the `getopts` crate](http://static.rust-lang.org/doc/master/getopts/index.html) not work for your argument parsing? (Awesome to see that you're highlighting properly by using `syntax`. Also, the highlighting algorithm looks pretty similar to that used in `rustdoc`, did you borrow it from there?)
&gt; Does the getopts crate not work for your argument parsing? I think it may work, I haven't used it because when I started the project, I didn't know what arguments the program will take, so I wrote my own arguments parsing until I'll have a cleaner thing. &gt; (Awesome to see that you're highlighting properly by using syntax. Also, the highlighting algorithm looks pretty similar to that used in rustdoc, did you borrow it from there?) Ah, yes indeed, I forgot to mention it. I'll do it in the next commit. 
&gt; I think it may work, I haven't used it because when I started the project, I didn't know what arguments the program will take, so I wrote my own arguments parsing until I'll have a cleaner thing. I think `getopts` is reasonably easy to use... you certainly don't need a clean code-base for it. (I would even suggest that it's easier to add/adjust args with `getopts` than with manual parsing. :) Certainly makes the `--help` string easier to make (i.e. no effort).)
The point of storing it as an unsafe pointer was to be able to have a null `ActionFunc` without using `Option`. I'm currently exploring an alternative representation that would eliminate the need for it, though.
I had no idea this was possible.
I think `Option` does what you want here. `fn` is known to be a non-nullable pointer, so `Option&lt;fn(...) -&gt; ...&gt;` optimises to a null pointer, so action: Option&lt;ActionFunc&gt; /* ... */ action: Some(do_smithy)
This is a bug I've never even considered, I must admit. I don't think I've ever run across a bug like this, although thinking about it I can image this appearing in C++ code pretty easily, if you aren't careful. for( auto &amp; e : big_vector ) { // do stuff with e /// ... // oh, somewhere in code, realize that we need to add another element to big_vector if( some_condtion ) big_vector.push(new_element) // ....? } I guess I'm having a hard time imagining a less contrived example. @/u/strcat: Do you have a good example from your code where the modified collection was done on purpose, where you forgot that it invalidates the collection variable (possibly).
I just posted this on another discussion on the same link. &gt;Now think about code that receives a collection and a function to apply to each element of that collection. &gt; So, I'm a very nice user and pass to your nice little code above a collection and a function that happens to have a closure over the same collection and modifies it with each call (like add members to the front of it or something...) &gt; I'm not saying this is a sane scenario, but if you're a library writer writing the first function, you have to think about it. 
In many of the cases where I encountered this defect it wasn't immediately apparent that it was a defect. Indeed, I've gone through long discussions telling people why it is. The reason why people may not see it often is because it often works despite the defect. Invalidating iterators doesn't mean the code will always crash. In many cases it doesn't even mean something bad will happen. So this defect could be lurking in a lot of code and just not manifest itself in a visible way. In a case where I was handling connections, the bug only appeared if an incoming connection was accepted while inside the loop. It's a concurrency issue that rarely happened. But it wasn't a race condition, it was this issue. The example in my post was also real. The list was a set of pending blocks to be typed by the compiler. Typing can, in some situations, result in a new blocks being added to the list. It was generally working, but sometimes I'd be left with an unprocessed block.
The author should look at Obj-C. The `NSFastEnumeration` protocol, which powers `for (id foo in bar) { ... }`, checks on each pass through the loop that the collection was not modified, and throws an exception if it was. This does require the collection to implement `NSFastEnumeration` in a way that enables this behavior, but all of the built-in collections do. Granted, it's not as nice as Rust's ability to statically prevent the collection from being modified, but it is a guaranteed exception rather than undefined behavior.
There's been some talk of a more general `FromSequence` trait and `seq!` macro that would allow for e.g. let m: HashMap&lt;int, int&gt; = seq!((1, 2), (3, 4)); let v: Vec&lt;int&gt; = seq!(1, 2, 3, 4); let s: HashSet&lt;int&gt; = seq!(1, 2, 3, 4);
I would prefer let m = map!((1, 2), (3, 4)); let v = vec!(1, 2, 3, 4); let s = set!(1, 2, 3, 4);
 spawn! { foo(); bar(); } is the canonical example for `{}` delimited macro invocations, making it feel more like a control-flow structure. And, `vec![1, 2, 3]` feels more vector-y than `vec!(1, 2, 3)` does. (Also, the simd proposal uses `simd![...]` for various things, and mirroring the fixed length vector syntax for that makes sense.)
What about `treemap!((1, 2), (3, 4))` `concurrent_hashmap!((1, 2), (3, 4))`, `ringbuf!(1, 2, 3)`, etc.? I guess we could have a subset of these defined in the standard library (and then each person defining a data structure can define their own macros), but it seems far more general to just use a trait.
Sounds good. My motivation was letting the compiler infer the container's type from the macro. Edit: that said, it does seem a little odd to use macros for initializing containers.
`[1, 2, 3]` is currently a fixed sized vector, so, if we were able to have generic integers in types, we could have a trait like trait FromFixedVec&lt;T&gt; { fn from_fixed_vec&lt;n: uint&gt;(data: [T, .. n]) -&gt; Self; } and then, e.g., `Vec` would allocate enough memory and then memcpy `data` into it. An alternative is the `&amp;move` pointers that have been suggested occasionally, with a `&amp;move [T]` instead of the generic `from_fixed_vec`. (The `&amp;move` is required to be able to initialise something with e.g. `~"foo"`, where the type needs to be moved around.) (I guess this is basically your suggestion translated into Rust-y terms?)
Same thing in Java.
&gt; Sounds good It's not clear: are you saying that having a wild proliferation of macros sounds good? (With the new type placeholders, it's now less of a bother to give explicit hints, e.g. let m: HashMap&lt;_, _&gt; = seq!((things, with), (complicated, types)); theoretically works.)
Once you're doing something like that, the map syntax might as well be `map!(1: 2, 3: 4)` or `map!(1 =&gt; 2, 3 =&gt; 4)` or some such thing.
Yes, any type. (Does C++ have restrictions on the contents of initializer lists?)
No, the initializer_list is generic I'm pretty sure. It also comes with some extra sugar; http://www.cplusplus.com/reference/initializer_list/initializer_list/ struct myclass { myclass (int,int); myclass (initializer_list&lt;int&gt;); /* definitions ... */ }; myclass foo {10,20}; // calls initializer_list ctor &lt;- notice the lack of parenthesises myclass bar (10,20); // calls first constructor Maybe something like: let foo: Vec&lt;int&gt; = [1, 2, 3]; Could work in Rust? Or would that be too messy/confusing? Or maybe hard to implement? Just a suggestion! :)
I wasn't aware that macros had omnipresent scope. This is a good argument against macro proliferation.
Um, what?
I think this is one of the clearer explanations of segmented vs. contiguous stacks, and the stack thrashing problem of the former. What can I say, pictures help! This page is Go-focused, but has a shout-out to Rust, precisely because the problem is analogous on both platforms.
http://knowyourmeme.com/memes/weird-twitter
And with which one is Rust planing to go with ATM?
Rust uses contiguous stacks now. It does not do the relocation trick that Go is doing, as it lacks precise garbage collection and will likely never have complete knowledge about all pointers. It would break a lot of what makes it a systems language, as you couldn't simply write straightforward `unsafe` code anymore. It also calls into native code too much for it to save any memory - Go does the system calls itself.
rust-ci documentation is currently broken: http://www.rust-ci.org/arjantop/rust-tabular/doc/tabular/index.html
I remember discussing the issue in the case of Rust, the problem is that your ability to move the stack to another memory location is affected by your ability to *re-write every pointer to the previous memory location*. I had proposed at the time to instead turn the problem around and have two "kinds" of pointers: - pointers to heap: real pointers - pointers to the stack: offset from the beginning of the stack Then, whenever you wish to use a pointer, you first have to compute it: `ptr = is_heap_ptr(intptr) ? (T*)intptr : stack_ptr + intptr;` which should be transformed (hopefully) into a conditional move and thus end up being branch-less. There is, unfortunately, one little issue: when you pass the pointer to an unsafe API (`unsafe` block, C call, etc...) you would have to pin it (temporarily...). They say they manage to use Escape Analysis in Go to avoid the issue but I must admit being somewhat skeptic, you generally end-up making the conservative choice and placing the item on the heap.
Java doesn't guarantee this. It says it is strictly a debugging exception that may or may not be thrown. My guess is that it is normally thrown, but the check likely has a data-race which is costly to fix.
IANARD, but I think that rust should promote macros that looks somewhat similar to standard syntax.
No decision has been made yet. It's not a 1.0-blocker so it is not a priority. Also, we try to let things run their course in the RFC repo in terms of discussion before discussing them in the meeting. That only just seems to be happening with virtual structs, etc.
The element type must be a non-abstract object type, which is the same rules as for making arrays. I should note that the rules for `std::initializer_list` have very questionable design choices behind them, although I’d be surprised if those mistakes were repeated. (You would need to do that on purpose…)
I think it's true for all the collections in the stdlib. I don't know about arrays, but nobody really uses them anyway.
Offset pointers would be interesting outside of the stack. They would be ideal for data structures that are stored in a database or transferred over a network without serialization. They could be used as a form of pointer compression for arena allocated data structures since the pointers would only need to be large enough for the arena. 
Rust might use fixed stacks by default, and have resizable stacks as compile-time option. If resizable stacks are chosen, then calls to native functions are translated to invocation of these function on per-os-thread (not per-task) large fixed size stacks.
I get the impression that it would require increased complexity in low-level code along with new restrictions, making Rust less appealing as a systems language. It would also introduce a second ABI to support in parallel, by building two versions of libraries. I think precise garbage collection will fragment the community like this too, so it ends up being a combinatorial mess of optional features - unless it can be done with *zero* code-size and performance overhead for code not using it.
Well the first example at least matches struct initialisation (though it does annoy me that : is used for type annotations everywhere else...)
I should note that I have a PR under review for the first stage of the virtual structs work, but that is feature gated and does not imply we will go down that road.
Has (either in Go or in Rust) some kind of [hysteresis](https://en.wikipedia.org/wiki/Hysteresis) behavior been considered, to deal with hot splits / stack trashing? For example: when a stack segment becomes unused, don’t free it immediately, but keep one (or N) "spare" in case it’ll grow again soon.
Do you happen to be able to give a rough estimate for the maximum performance impairment that *not* having this 'relocation trick' could cause?
Not relocating stacks will improve performance. Go does it because they care more about memory usage per-goroutine than performance, since there's no asynchronous I/O, non-blocking sockets or lightweight task trees/graphs.
Really nice! How do you run the bench and test code though?
Thanks. I abused `Make` for that. If you have a checkout of the source, then in the project directory just run either make test make bench But they are really just aliases for the standard way of running tests/benchmarks in Rust. For example, it sets `RUST_LOG=csv` so that `debug!` statements do something. And for benchmarks, it uses `-O`.
We do that in Rust.
That's not abusing make; it's standard practice. Still, you should have at the end a thing that specifies that those rules are not expected to produce files: .PHONY: compile install ctags docs test test-examples bench test-clean clean push [rust-http's Makefile](https://github.com/chris-morgan/rust-http/blob/master/Makefile) demonstrates a few slightly nifty things for Rust makefiles. (Actually, it's just shifting to a [configure](https://github.com/chris-morgan/rust-http/blob/ssl/configure) + [Makefile.in](https://github.com/chris-morgan/rust-http/blob/ssl/Makefile.in) step with a couple of other improvements as part of adding SSL support.)
Ah, neat! Thanks for the link. I like your `Makefile`. I'll definitely crib some stuff from it. Good call on `.PHONY`. I had forgotten about that. &gt; That's not abusing make; it's standard practice. Mmhmm, yes, I suppose. I'm so used to using `Make` with just a bunch of aliases that don't actually rely much on a static dependency graph that it seems like an abuse. Granted, everyone does it... Although, my `bench` and `test` rules actually do benefit from specifying dependencies, so I guess it's not so much of an abuse after all. :-)
&gt; I'm so used to using Make with just a bunch of aliases that don't actually rely much on a static dependency graph that it seems like an abuse. Think them as entry points to the actual dependency graph (or epsilon transitions in the middle of the graph).
You could write a macro that does something like pipe!( foo(1), bar(2), baz('a')) that destructures into baz(bar(foo(1), 2), 'a')
Warning: rust newbie. I don't believe so. I think the idiomatic way to represent this would be with some combination of "method" chaining. let css = fopen("file.less") .lessc() .autoprefix() .minify() .fwrite("style.css"); You can add new "methods" to an existing type by defining a trait and then implementing it. For example, I needed a version of `fold` that didn't use an initialiser, and added it like so: trait IteratorFoldDefault&lt;A&gt; { fn fold_def(&amp;mut self, f: |A, A| -&gt; A) -&gt; Option&lt;A&gt;; } impl&lt;A, T: Iterator&lt;A&gt;&gt; IteratorFoldDefault&lt;A&gt; for T { fn fold_def(&amp;mut self, f: |A, A| -&gt; A) -&gt; Option&lt;A&gt; { match self.next() { None =&gt; None, Some(x) =&gt; { let mut accum = x; for y in *self { accum = f(accum, y); } Some(accum) } } } } At that point, any type that satisfies `Iterator&lt;A&gt;` gains a `fold_def` method. 
I think pcwalton is referring to [the `green` crate caching some (large) stacks it allocates](http://static.rust-lang.org/doc/master/green/stack/struct.StackPool.html).
Is it already possible to recursively build libraries?
I wonder if you could whip up something interesting with macros, channels and tasks.
By some good fortune, the CS Labs at my uni received a [MakerBot Replicator 2](http://store.makerbot.com/replicator2). I've had a [3d version](https://github.com/cmr/rust-logo-3d) of the logo for a while, but now I've finally printed one! Not pictured is the sticker above and to the right, or the group of students whose laptops are adorned with a Rust sticker.
&gt; While the chaining pattern may seem useful, Rust does not have a currying nor a full support for monads There's need for neither for something like this to be useful. It's very useful in Clojure (through the threading macros `-&gt;`, `-&gt;&gt;` &amp;al) for instance. &gt; so that the actual benefit of this pattern would be small. Any time there's a processing pipeline, there's a use for this. I'm currently on a project which makes extensive use of iterators (and iterator transformers) in Python, and I'm using a variant of this (since it's dynamically typed I could get away with `thread(obj, *fn)`)
What would be the advantage of the `try!` macro over Result.and/and_then?
* Something like that was my "vision" for tabular, but in the end i just left it as an iterator over rows that can be extended with any encoder/decoder in the future (as you probably realized current api requires a lot of hacks to implement anything not really 1:1 translation) * You should compile your lib with -O (makefile) * I'm "stealing" you example csv files for benchmarks
Could be more 3d!
On 64-bit, why can't this just be handled by the VM system? Allocate a continuous "large enough" area of address space for the stack. Initially, only the areas used will be actually provided by the os. During GC, locate the stack top and punch out any real pages beyond that.
`try!` returns from the function with the Result on failure. It is good as an escape hatch out of the function. `and`/`and_then` are good if you want to continue executing the function or take corrective action if there was a failure. There may actually be cases you would want to use `try!` and `and`/`and_then` together.
C syntax strikes again. I really dislike that I get a million parens for calling functions on things.
If careful programming is so easy, how come Firefox crashes on my system from time to time? Surely, they could have just been careful. Remember, this is a language for writing Servo on. A browser engine that will never crash on you while being just as fast.
A hybrid solution is what you want here I think. You don't care about 2ns if you're in a "coordination function" that's doing a lot of blocking and message passing etc. anyway, but you *do* care once you crop into the meaty computationally heavy ones. Thus, use fully co-operative scheduling (to make yields predictable) and switch to a shared big stack for anything that doesn't yield (so that any function that doesn't yield, or call a function that yields, will never "hog the stack" by being swapped out while in the middle of using it). This way most code runs on big stacks (fast), but you still get low per-task overhead because the big stacks are shared (one per CPU thread). Only the first few layers of functions that actually do the asynchronous stuff have to deal with segmented stack overhead.
Have a thousand upvotes, I missed the `return` in the macro. Yes, that makes sense. Well, in that case, `and` makes a lot of sense if you are working on results of another function.
sorry for maybe being not clear, I want to have examples in the generated docs but dont want to duplicate the code (having runneble examples in the examples/ dir and the same example pasted in the doc)
Actually, there was another point that at a later date FFI calls could be tagged with a `stack_size` attribute that would define how much stack (at most) they required; with this simple C functions would not necessarily require an expensive task switch. I believe Patrick Walton had chimed in and said that unfortunately generic computation of a stack size bound was impossible (or close to it) because even if one used stack size attributes on Traits/Pointers to functions, soon enough a recursive function could stall you. And therefore, it was nigh impossible to implemented safe stack growth automatically. I had argued that maybe the simplest thing would be (coupled with tags on the FFI calls) to give the user a `rt::require_stack_size(2048)` function that would take care of extending the stack *if necessary* and that the user could perfectly place this outside its tight loops.
&gt; (as you probably realized current api requires a lot of hacks to implement anything not really 1:1 translation) Indeed! It simplified things to restrict encoding/decoding in the public API to records only. That wasn't so bad. It was the *error handling* that is terrifying. Thankfully, [this PR](https://github.com/mozilla/rust/pull/13107) is about the land which should make this much better. &gt; You should compile your lib with -O (makefile) Thanks! Fixed. &gt; I'm "stealing" you example csv files for benchmarks :-)
wow, didn't even know someone was working on it since there was no activity in [12292](https://github.com/mozilla/rust/issues/12292) since my proposal. But still mapping a list of bytes to something else that other types is a pain.
Even C has types.
Obviously you could just mix `or` with `unwrap`. I think it would be nice to have that in another utility function because `or` requires the default to be wrapped in an `Ok` constructor. I think `unwrap_or` feels a little more natural.
Can't do it with anything built-in. It would be similar to the `include!`, `include_str!`, and `include_bin!` syntax extensions. It might be possible to implement something like `include_doc!` using one of those syntax extensions for inspiration.
AIUI there aren't any immediate plans for servo to do this, but if it's a win it may someday. While jitting selector matching is clever and probably a good optimization, it's also not all that interesting from a research perspective (for servo at least) - this optimization is already open to all existing engines. The stuff servo is doing, even on selector matching, is much more radical. 
Performance benefits for `safe` code? Didn't expect that, that's nice to hear!
I would definitely like to someday if indeed it is a win on large pages (and in fact I have ideas about how to do it safely), but I think parallelism is a much larger win because it has dramatically reduced fixed overhead compared to compilation. That is, parallelism wins on virtually all pages, not just small ones.
[**@pcwalton**](https://twitter.com/pcwalton): &gt;[2014-03-27 21:40:05 UTC](https://twitter.com/pcwalton/status/449299846873108480) &gt;Servo passes Acid2 in my branch: [*i.imgur.com*](http://i.imgur.com/CsLkgLl.png) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/21jjac%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
awsome! nice job.
Yes, Go will still be making a significant performance sacrifice compared to using fixed-size stacks. They have a warm-up period where the stack is being reallocating/copied over and over as it grows. Rust just allocates it all up-front like C, and relies on virtual memory to allocate it lazily as it is touched. This is faster, but means the initial overhead is ~4-8K. It also means creating a green thread will always involve at least 2 system calls, so it loses the advantage of not requiring a system call to create a thread.
I'd like to see screenshots which compare this version with the version that first passed acis1, for reddit/wikipedia/twitter/google.
glibc caches stacks for POSIX threads too.
Rust supports 1:1 threading and uses it by default. That's not going to change.
It's already handled well by virtual memory. Stacks are allocated lazily as they are used, as is any very large allocation. &gt; During GC, locate the stack top and punch out any real pages beyond that. Rust does not have a garbage collector. It will have garbage collected smart pointers *in the future*, but it will never require using a garbage collector so a garbage collection cycle can't be relied upon to do something like this. It seems like a bad idea to shrink stack usage by default, just like it would be a bad idea to shrink vector capacity by default. The current usage is a very strong indicator of future usage. There's `shrink_to_fit()` for vectors and there could be the same thing for stacks.
Nice work, pcwalton. I anticipate vacation time in your future.
Excellent!
The form widget rendering is not part of the test AFAICT.
Oh, it's not. I'm however mixing up in my head Acid 1 with some similar period test that was basically 90% layout of form widgets… (The widgets themselves didn't matter, of course, but it was all about laying them out in the right place.)
The thing I don't like about this is that you're adding a lot of methods to the string class that you wouldn't normally need/use, and you can't pipe it through existing functions...you'd have to write custom chainable versions of each, which kind of kills the convenience.
&gt; the current iterator design indeed uses this pattern sans the operator. Can you give an example? I'm new to Rust. A [Result](http://static.rust-lang.org/doc/0.9/std/result/enum.Result.html) object could capture the 'state', no? Or isn't there something like exceptions so that it can bail out halfway through?
what's "HKT"?
"Higher Kinded Types"
Consider this Rust-like pseudocode: // count the number of non-empty lines let lines: ~[~str] = read_lines("foo.txt"); let filtered: ~[~str] = filter_lines(|x| !x.is_empty(), lines); let nlines: uint = count_lines(filtered); This is not a good way to count non-empty lines since it reads the entire file into the memory. What we really want to do is to increase the internal counter once the non-empty line is detected (and the line itself would be discarded); the pipe operator itself (as like `read_lines("foo.txt") |&gt; filter_lines(|x| !x.is_empty()) |&gt; count_lines`) does not guarantee this behavior either, as it is just a syntactic sugar at best. The proper way would be like this: let lines: Lines = lines_iter("foo.txt"); let filtered: Filter&lt;Lines&gt; = filter_lines(|x| !x.is_empty(), lines); let nlines: uint = count_lines(filtered); `Lines` and `Filter` then would have the necessary state to perform the task, but wouldn't start the actual processing until `count_lines` is called. In the other words, `lines` and `filtered` is not a result but the *promise* to process things. This is actually how the current iterator is implemented, and the actual Rust code is not very different from the pseudocode above (WARNING: never tested): use std::io::{File, BufferedReader}; let buffer = BufferedReader::new(File::open("foo.txt").unwrap()); // we need to keep this alive til `count_lines` let lines = buffer.lines(); let filtered = lines.filter(|x| !x.unwrap().is_empty()); let nlines = filtered.len(); More general way to describe this behavior would be the monad and it will benefit from separate operators, but Rust don't have it yet and thus there is not much need for operators.
&gt; Sure, I get that. It's just a bit of a shame that the "millions of lightweight tasks" scenario isn't being well served by Rust. In fact, the "task" name is pretty confusing now - "thread" is closer to what they actually are. It was never really lighter than it is now. The red zone was always bigger than 8K, which is close to the initial cost of a large stack. It may even consume less memory now than it did before, but it will be significantly slower to spawn them as it needs to call `mmap` and `mprotect`. &gt; I'd like to have something like "goroutines" where I use it for performance and coordination, and take on the responsibility of making sure everything yields and plays nice (it's really not that hard IMO). In return I would get tiny stacks (with switching to big stacks for any "bulk" work, i.e. as soon as you know there's no blocking) so that I don't have to worry about memory when spawning. Go uses a special calling convention and does system calls itself in order to make green threads more viable. As long as we are using libc and libuv, there is going to be a *significant* cost that doesn't exist for this design in Go. Switching to another stack wipes out the cache and destroys branch prediction. Green threads sacrifice a lot of performance over asynchronous and non-blocking designs for the sake of expressiveness. There are other ways to build a high-level abstraction like the C# async/await without making this sacrifice and without requiring invasive changes for people who aren't using the feature. Green threads are not a pay-for-what-you-use feature even now, as they turn all concurrency and I/O calls into dynamic dispatch and raise the initial size of a binary built with link-time optimization from 5-10kiB to over 300kiB *even when you aren't using any*. The segmented stack preludes are also not an insignificant cost to force on all code. In my opinion, these issues seriously hurt Rust's credibility as a viable replacement for C and C++. I hope precise garbage collection doesn't become another of these issues, but I expect that it will.
Will it ever? Or do they intend to drop forms and/or support them with a different layout browser when needed?
Maybe you're thinking of fonts? If I remember correctly, ACID1 on Servo looked different from the gold because fonts rendered differently, but fonts were not meant to be part of the test.
We will need to support forms. Currently we are thinking about building them on top of Web Components instead of directly hardcoding them into the engine though.
That's definitely an interesting approach. What you guys are doing is very cool. I definitely need to start programming some rust.
Thanks to you, I now have [a physical logo of my very own](http://imgur.com/GjM5Wts)!
Awesome.
I didn't say the pipe operator does not work with the iterators: it is a very trivial syntactic sugar, and such syntactic sugar would be better expressed with the macros and not in the core language. There was a discussion about [chaining macros](http://www.reddit.com/r/rust/comments/20ys8i/a_rust_monad_macro/) (the original title was misleading) that exactly do this.
This is great! I guess this means I no longer need 3h build times on my GNU/Linux netbook. :)
Both should be possible, though. :)
You can have a low-level iterator providing a view (no copying) of the data in a fixed-length vector of slices of bytes and use it to build an interpreting iterator on top.
Now only also sign the binaries and everything is perfect
to which part of my post is this referring to?
You said you were just using an iterator over rows for now; I believe it can still be useful as a low-level primitive (assuming it's cheap) and that you can always add a higher-level interface on top of it.
FYI, this is pretty much what is exposed in my OP. One iterator for just raw strings and another for fancy decoding.
that's pretty much what im doing and was the plan. I also have lazy iterator over columns as a lower level primitive (not exported) for (maybe) more efficient decoding (if implemented).
you have it the other way around, Decoder that can produce raw strings (wuth correct type provided), mine would be a primitve iterator that produces strings and a Decoder from vec of strings to T mapped over it.
do rusts' more solid immutability/aliasing rules help with all that though (caching temporaries where possible), if you have immutable objects and immutable vtables. I suppose macros could make declaring a vtable instance that way look very similar to a trait (still grep for fn.. ), and vcall would be just as easy to do following the suggestion of wrapping the trait, it seems like you could make creating a struct with this hack safe, and the 'vcall' could just call a method of that struct, that might allow confining the unsafe to one place 
I assure you I don't. The [`record` method](https://github.com/BurntSushi/rust-csv/blob/master/src/lib.rs#L959) completely bypasses the decoder and returns the raw strings parsed. Of course, the decoder can *also* produce raw strings given the right type. The public API therefore provides two different ways of retrieving records. My decoder is nearly divorced from the internal state of the parser. It hides the fact that it's really just decoding `Vec&lt;~str&gt;` for you.
I'm happy it's working for you!
&gt; In sum, the type system allows Hehe, I'm sure this was an unintentional pun, but I giggled anyway.
I hate to be *that* guy, but there are a few clerical errors about Go here. Firstly, Go doesn't have duck typing. It's structural sub-typing. Similar, but not quite the same. Secondly, Go doesn't have type inference. The term used in the spec is type [deduction](http://golang.org/ref/spec#Variable_declarations). This is important because the type "inference" in Go and Rust are very different beasts. Finally, I don't think the Go authors have ever been on record as saying "generics aren't that important." What [they've said](http://golang.org/doc/faq#generics) is that they haven't found an implementation with a set of trade offs they're comfortable with yet. [Andrew Gerrand confirms it.](https://news.ycombinator.com/item?id=7221110) &gt; I won't try to fully explain the type system here; because it's so much more powerful than many of us procedural folks are used to, it can take some time to wrap your head around it. Don't be discouraged, though. It's really awesome once you get to know it. I do have to agree with this though. Notably, moving `@` pointers into the libraries was a stroke of genius. Made things much simpler and easier to write code IMO. Honestly, from your introduction, I had been hoping for something that described the differences between your solutions to the same problem written in different languages.
Awesome, that is going to fit well for Windows users on my workshop!
Thanks for being that guy! I enjoy being educated. &gt; Honestly, from your introduction, ... That post is in the works.
thanks for catching that
For your first two questions (the "how"), the tutorial does have a section on it: [Crates and the module system](http://static.rust-lang.org/doc/master/tutorial.html#crates-and-the-module-system). I wrote a [SO](http://stackoverflow.com/questions/22596920/rust-splitting-a-single-module-across-several-files/22597760#22597760) answer that should be a complete working example. The rest of your questions are prescriptive and I agree, it'd be nice to have something written down for them. One of the great things about Rust's module system is that its structure can be completely independent from the structure in the file system, which means it's really flexible in what it will let you do. But this also means there are a lot of different ways to organize a crate's structure...
Go has a specific purpose &amp;ndash; it favors a relative simplicity and fast compilation, hence why it is said to be suitable for “webapps” (for instance). Go needs GC to ensure memory-safety and doesn't protect against data races when working in a multithreaded environment. It's not the same to being a successor to C++ if you ask me, which I think Rust really is. Both languages should fit a need through and they have specific purposes &amp;ndash; they are being incorrectly portrayed as direct competitors (imo) because as you said, they are both recent C-like languages backed by big industry names. So instinctively, it feels like they are direct competitors. I think that it is not the case.
Given the nature of this topic, I'd like to remind everyone to keep in mind Rule #4 from the sidebar.
Can anyone who has been working on Servo comment on what it's been like to use Rust for a large and challenging project so far? I am quite interested to know if you have found that your code is really a lot more reliable, or easier to modify and reason about.
I write go for a living, but have been keeping a very close eye on rust and started actually spending some time to learn it over the last week. I feel that rust has kinda ruined me (in the good way) because now I find myself wanting features that go doesn't provide. Enums are brilliant. I absolutely love the type safety of an enum over packing random crap into a interface{} and hoping I don't forget or miss a type assertion. I also prefer how the match statement requires all possible situations to be handled (or to explicitly use _ as a catchall). Go's switch may let you forget to handle new types you introduce later into the code. A common idiom in go (and I would assume rust as well) is to send messages that must keep their order over the same channel and use the type to disginguish what kind of message it is. To me, this is the perfect situation for an enum. Now, because I want the same kind of type safety, I find myself creating code like type MyMessage interface { ImplementsMyMessage() } type MyDataMessage int func (MyDataMessage) ImplementsMyMessage() {} type MyEventMessage struct{} func (MyEventMessage) ImplementsMyMessage() {} func HandleMessages(c &lt;-chan MyMessage) { for msg := range c { switch e := msg.(type) { case MyDataMessage: fmt.Printf("data: %d\n", e) case MyEventMessage: fmt.Println("event") } } } This prevents me from sending random things over the MyMessage channel, but there's still no guarantee that the type switch handles every case. The bare methods are pretty boilerplate too. Immutability and pointer ownership are also very tempting. It's silly for me to have to copy data before returning internal data structures with reference types (namely pointers, slices, and maps). In cases where the caller doesn't need a mutable copy, this is unnecessary overhead (and more garbage). Returning without a copy can make my code unsafe if the caller modifies anything when they shouldn't have. It's too bad that rust doesn't run on my main operating system. Maybe some cross compiling is in the future for me...
What's your main OS?
Bitrig, an OpenBSD fork.
Answered to brson already, but I'm running Bitrig.
What is the difference between "*type inference*" and "*type deduction*"?
&gt; Go needs GC to ensure thread-safety. I think you mean *memory safety* here.
/me whistles, ok then ... I'm afraid you're on your own with that for a while longer :p
Indeed. In applications such as embedded development go isn't even a contender (nor is it intended to be), while Rust is looking very attractive.
&gt; Go needs GC to ensure thread-safety Go has data races, so it's not thread-safe. You could say that it meets a narrow definition of memory safety, but there are few guarantees provided by libraries when you race on data - so I wouldn't call it safe, just safer than C++.
I'm actually pretty new myself (only a few weeks in, but had been casually following since Rust was made public), so I'm not exactly an authority. After a quick search, [this blog post by /u/pcwalton](http://pcwalton.github.io/blog/2013/06/02/removing-garbage-collection-from-the-rust-language/) talks about what I had in mind. Indeed, Patrick mentions this as a critical point (and I agree): &gt; Programmers don’t know which to use, since some operations are available with ~ and some operations are available with @. Actually, we were confused on this point for a long time as well—it wasn’t clear whether ~ or @ would become dominant. We debated for a long time which to present first, ~ or @. However, as the language and community evolved, and coding standards became more settled, a clear winner emerged: the owning pointer ~. In practice, the rule has been that programmers should use ~ to allocate in all circumstances except when they have no way of knowing precisely when the object in question should be freed. 
If you're going to be *that* guy, I'm going to be *this* guy. You're making a distinction between terms that don't have any real difference in meaning. The differences between the terms "deduction" and "inference" (they are similar enough to be synonyms in a thesaurus) are not related to the differences between type inference in Go and Rust. Both "deduce" the types of things via a process of "inference" based on contextual information in the program. The specific things that the inference processes take into account differ somewhat, but the reasoning process is similar, and the effect of static type checking without explicit annotation is similar as well. Also, "duck typing" is an informal term that applies just fine to structural sub-typing, as the essence of Go's interface system is such that everything that "quacks like a duck" can be used as a "duck" without explicitly calling it one. Go just ensures that the parameters required to be "ducks" do indeed "quack like ducks"; at compile time when possible, or at run-time otherwise.
Note that Go isn't memory safe if it's multithreaded (GOMAXPROCS &gt; 1), per the "off to the races" article.
&gt; The differences between the terms "deduction" and "inference" (they are similar enough to be synonyms in a thesaurus) are not related to the differences between type inference in Go and Rust. Where I come from, *type inference* is a term of art and Go simply doesn't do it. The Go authors seem to be aware of this; the word *inference* never appears in Go's language specification. &gt; Go just ensures that the parameters required to be "ducks" do indeed "quack like ducks"; **at compile time** when possible Yes, I see this as the primary distinction between duck typing and structural subtyping. It seems like an important one to me.
oops sorry
Neat! Can you use the `.pkg` files with brew?
The negatives: * We spend a lot of time dealing with Rust upgrades and compiler bugs. * Every so often your code fails the borrow check and you have to tweak it a little bit from the way you first wanted to write it. (I'll gladly take this over segfaults though! See below.) * Build times can be an issue, though compared to Gecko Servo is a breath of fresh air. * Modeling classical OO in Rust can be trickier than in C++. The positives: * Having segfaults isolated to the unsafe code is amazing. It's a very different experience from C++ hacking: I don't remember the last time I had to debug a segfault in the Rust parts of Servo, safe or unsafe. All the segfaults I can remember were somewhere in the C libraries we're linking against (OpenGL, most infamously). * The module system is really nice too: seeing the full path to every imported name right at the top of every file makes it much easier to find where stuff is defined without having to use a code indexing tool. For example, if you see `use layout::block::Foo;` at the top of your file and you want to find the definition of `Foo`, you know it's going to be in `layout/block.rs`. * Having safe concurrency is great: since there can be no data races outside the unsafe code, when there *are* races we immediately know where to look (`layout/wrapper.rs`, most notably.) We have plans to use the Rust type system more effectively and make those much less likely to happen. * Pattern matching is really good for layout code, since much of the CSS spec is of the form "if special case 1, then do this; if special case 2, then do that; if special case 3, …" It lets you transcribe the spec in a very natural way and much of the code in Servo is shorter than it would be in C++ as a result. All in all, I'm very happy with the way it is turning out.
&gt; Where I come from, type inference is a term of art and Go simply doesn't do it. The Go authors seem to be aware of this; the word inference never appears in Go's language specification. You need better language theory textbooks where you come from, apparently. "Type inference" is simply the process of automatically deducing the type of an expression in a programming language. It provides to the compiler type information that the programmer omitted. The opposite process is "Type erasure"; this removes type information that the compiler knows about from the run-time representation of a value. Who knows why the writers of Go's specification chose not to use the term "type inference"? But they do *describe* it when they say: &gt; If the type is present, each variable is given that type. Otherwise, the types are deduced from the assignment of the expression list. So, apparently Go does infer types, because that's pretty much what the term "type inference" means. Yeah, it's not Hindley-Damas-Milner inference, but... neither is Rust's! Yes, Go and Rust have very different type systems, but you're simply wrong to say Go does not do *type inference*. It does, the manual says it does (they assume we are smart enough to recognize what is being described), and you made a fool of yourself by being *that guy* and being wrong yourself. Nice job. &gt; Yes, I see this as the primary distinction between duck typing and structural subtyping. It seems like an important one to me. And it seems like an important distinction to me that Go does not perform type erasure; the interfaces are tagged just like in a dynamically typed language, so that casts from the empty interface can be checked at runtime. And tagged, dynamic compatibility testing at runtime is where the term "duck typing" entered the programming lexicon in the first place. So, yeah. Duck typing in Go. Both your distinction and mine are important, and are worth sharing, but neither warrants calling out the OP as being wrong. Share good info, share different perspectives, but don't be *that guy*.
I don't think macros are expanded when generating documentation? 
AFAIK none of the Go developers are focussing on making it work well in embedded circumstances, so I guess you could say *implementation* wise it isn't? Although I supppose this depends on your definition of "embedded" - Go runs on an RPi, which I wouldn't call embedded but have heard some people do.
&gt; Go needs GC to ensure thread-safety. I believe you meant *memory-safety* here, given that Go is as thread-safe as C: if conventions are followed, it should go well...
At the end of the article, they hint that using a SSE instruction would be sufficient on Intel compilers (if the memory is aligned) to solve the issue with interfaces (and possibly some others). Still, this won't solve data-races in user land...
No idea why you are being down-voted so heavily for pointing these things out. :/
I think that what RefCell is there for, but I'm not quite sure, as I can't seem to use them in the way I think they are supposed to be used. Some code examples would be appreciated.
What about separators other than commas and newlines? Like [ASCII 30 and 31](https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/) ;-)
Well, the field delimiter can be changed to any character you want. As of right now, the record separator can't be changed. I'm not opposed to changing this in the future, but it's probably not a priority for me.
&gt; Who knows why the writers of Go's specification chose not to use the term "type inference"? But they do describe it when they say: Yes. That is precisely the place that I linked. &gt; Yes, Go and Rust have very different type systems, but you're simply wrong to say Go does not do type inference. Generally speaking, sure. But my guess is that they avoided the term *type inference* for a good reason. (To distinguish what Go does with what type inference algorithms traditionally do.) &gt; the interfaces are tagged just like in a dynamically typed language, so that casts from the empty interface can be checked at runtime. I wasn't talking about coercing values at runtime. I was talking about Go's only form of compile time safe polymorphism, and I suspect the OP was too when referencing "duck-typing traits." Also, I could do without the aggression. It's not like I made a big show of things. I acknowledged these things as clerical errors.
&gt;How are errors handled? I couldn't find any exception handling. Does Rust use something like Go's error object? Errors can be handled in various ways. The current approach used in the `io` module is a Result enum, which is kinda similar to the error object in go (in that it is a return from a function instead of an exception or similar), however it is a single return instead of two, so you much explicitly unpack the returned value (and either handle the error or cause a panic if an error occurs), and a warning will be emitted if it is not used (I'm not sure if go does this). &gt;How complete will the standard library be in 1.0? The biggest gap I could find is no http package. &gt;How similar is rustdoc to godoc? Don't know the answer to these. &gt;Does rustfmt or similar exist? The compiler will pretty-print source code but I don't think it's currently guaranteed to produce compilable source code nor is it intended to be used in the same way as `gofmt` &gt; Is Rust statically compiled? Rust currently statically links in external crates by default (but dynamic is supported). glibc must be dynamically linked in currently though. &gt;The standard distribution for Go contains a .vim file with syntax highlighting, can I expect the same for Rust? Rust does include a vim syntax highlighting file.
Haven't written a lot of rust code recently and I haven't used Go at all so I can't give too-in-depth answers, but maybe this is a start: * Unrecoverable errors are handled via "failure" that terminates the current task, unwinding the stack and calling destructors along the way, but without a "catch" mechanic. You can spawn tasks to isolate failure within them, but sharing data between tasks is limited. Other errors are handled "manually", by returning an object that contains either the operation's result or optionally an error description. Between automatic destructors, utility functions on `Option&lt;T&gt;` or `Result&lt;T, U&gt;` objects and pattern matching, it's more convenient than manual error handling in C, but not quite as care-free as, say, a dynamic language with exceptions and gc. * There's a third-party http package at https://github.com/chris-morgan/rust-http "in progress;" "basic, low-level implementations in place." Isn't "complete nor yet compliant in any way." I think this package is used by servo, so I'd assume at least the client bits are somewhat functioning. I don't think there's currently plans to have a http library in the core rust distribution, and I personally am not expecting feature major additions to the standard library, just fleshing out the current APIs and maybe some additions to support planned language changes, but I could be wrong on either point. * Dunno about `rustdoc`/`godoc`. * There's a pretty-printing mode similar to `gofmt`, but it's not receiving as much attention and I'm not sure it's adequate for `gofmt`'s use cases. (`rustc --pretty=normal filename.rs`) I just ran it on a manually-mangled source file and the result was perfectly readable, but not something I'd want to commit. There were way fewer blank lines and I think some comments got shuffled around, and I might have liked some more linebreaks. * Rust is statically compiled and supports static and dynamic linking, and being linked as a library from a C binary or similar. * There's a bunch of vim stuff in https://github.com/mozilla/rust/tree/master/src/etc/vim and I dunno what all of it does, but rust code in vim is certainly very colorful. Edit: In general, even ignoring the standard library and lacking gofmt equivalent etc, I think if you come at Rust from the perspective of "I hope it's like Go, with a few new toys" you might not enjoy it too much. The emphasis on safety and manual memory management makes for some weird idioms and verbosity that might seem completely ridiculous, but are necessary to satisfy the compile-time checks that prevent dangling pointers and mutation of aliased values.
&gt; How are errors handled? I couldn't find any exception handling. Does Rust use something like Go's error object? The [Option](http://static.rust-lang.org/doc/master/std/option/index.html) and [Result](http://static.rust-lang.org/doc/master/std/result/index.html) value is typically used to signal a possible error. Unlike Go, the error has to be somehow handled explicitly either by: * Failing on the error (`result.unwrap()`); * Using another value on the error (`result.ok().unwrap_or(value_on_error)`); * Delaying the error handling until needed (`result.and_then(|v| next_operation(v))`); * Delegating the error handling to the caller (`try!(result)`). The task failure can be also caught by other tasks, but it is mostly for the catch-all situation (e.g. graceful cleanup for the helpful error message) and normally you'll use `Option` or `Result`. It is one of many safety features built in Rust. &gt; How complete will the standard library be in 1.0? The biggest gap I could find is no http package. The standard library will be mostly *stable* in 1.0, but it won't look like the standard library of Python or Go ("batteries included"). Rather, it will provide the standard library for any purpose and many curated but separate libraries that will (hopefully) ship with the Rust compiler or at least will be easy to install from the default distribution. There is already [rust-http](https://github.com/chris-morgan/rust-http) in the active development, so I guess it will mature enough to be included to the set of curated libraries. &gt; How similar is rustdoc to godoc? If you meant the [godoc](http://godoc.org/code.google.com/p/go.tools/cmd/godoc) command line tool, the current rustdoc is able to generate the good-looking [HTML documentation](http://static.rust-lang.org/doc/master/index.html#libraries) out of the source code but unable to serve the HTTP request (like pydoc). I don't know if the rustdoc in 1.0 will support the further feature. &gt; Does rustfmt or similar exist? Not yet. Rust has been under the rapid development and both the coding convention and the surface syntax has been changed multiple times so that rustfmt would be quickly obsoleted at this stage. Once the language is stablized I expect there would be a similar tool though. &gt; Is Rust statically compiled? Yes. It compiles down to the single binary file much like C and C++. &gt; The standard distribution for Go contains a .vim file with syntax highlighting, can I expect the same for Rust? [Yes](https://github.com/mozilla/rust/tree/master/src/etc/vim). Anyway, welcome to /r/rust! Rust and Go have many things in common (static compilation for example) and many other things not in common (Rust: strong memory safety, Go: broad standard library coverage for example), so keep that in mind.
Chris Morgan maintains the [rust-http library](https://github.com/chris-morgan/rust-http), which is what the Servo project uses for its HTTP requests. I'm not sure of whether/when/how it will be merged into the standard library.
Alright, maybe I'll just have to use my memory overhead hack then, miiight work for now.
http://www.reddit.com/r/playrust/ Did you know that without our dumb ass coding shit, you'd never have played a video game in your life?
You're correct, it is a conscious design decision to not allow `=` to be overloaded. This could probably stand to be documented somewhere! As for using a method, that sounds great. Idiomatic Rust code prefers to make costs explicit (which is why, for example, Rust will never have C#-style setters that can call arbitrary code, or Ruby-style paren-less function invocations).
Rust should hopefully allow devs to make games better as well. 
Because the way he says it comes across as confrontational and rude, which is just unessceassary? 
In addition how do you integrate this structure into a build system would be nice. Especially one that deals with cross language/cross platform like cmake, etc.
I see the current `rustc --pretty` as a debugging tool, rather than a reformatting tool. :p It gives the compiler view of the source code and nothing else currently.
This is quite a comprehensive post, great job! However, to nitpick: &gt; &gt; Is Rust statically compiled? &gt; Yes. It compiles down to the single binary file much like C and C++. This isn't quite true in two ways: 1. it dynamically links to libc, and 2. you *can* use dynamic linking if you wish.
Ah, I should've been more specific. By "standard library" I meant "crates included with the language" rather than libstd specifically. You raise a good question though: are all crates outside of libstd going to eventually stop being distributed with the language and fall under cargo's purview?
I'm oversimplifying in the ground that the OP was already familiar to Go and asking for any difference between Rust and Go in the compilation model. Though yeah, libc is inevitable.
It looks like the Q1 goal this year is rendering Wikipedia correctly. Are there any big features left for that or mostly minor things at this point?
Yet to be determined. Some crates will surely be "blessed" by the Rust developers to indicate their faith in their quality and stability, and these will likely be included in any standard Rust install. A de-facto "standard library", even if not all living in the `std` namespace (which may not even exist by that point).
Minor things. Wikipedia is already looking OK. There are definitely bugs and rendering glitches, but we have most of the functionality there (unless you count the forms or multicol, but they aren't necessary for it to look OK).
Titter titter
I thought the original "correction" was confrontational and rude, which I thought was unnecessary. And also wrong. Usually I do not adopt such a confrontational tone, but it was meant to mirror the tone of what I was replying to. Maybe I turned it up a bit too much?
Just to be clear, the tone I adopted originally was meant to mirror yours as a bit of a tongue-in-cheek gesture. I guess I did a lousy job of that, so please accept my apologies for rudeness. I acknowledge that there is a distinction between the type systems of Go and Rust, but I object to the characterization of the OP's usage of the terms "type inference" and "duck typing" as errors, clerical or otherwise. &gt; Generally speaking, sure. But my guess is that they avoided the term type inference for a good reason. (To distinguish what Go does with what type inference algorithms traditionally do.) My point is that you have no definitive evidence of any error, just an assumption on your part as to a deeper meaning behind some word choices. That, to me, is really flimsy grounds for putting on the *pedantic error correcting man* hat. My donning of said hat in response was meant as an ironic gesture to underline the point, but instead seems to have backfired. Again, I apologize for that. You presented information that was largely correct and showed valuable distinctions between the Go and Rust type systems (though personally I feel their forms of type inference are more similar to one another than they are to the full polymorphic type inference that you get in ML), and valuable distinction between Go's structural typing and what is commonly called "duck typing". If you had presented these as informational points rather than corrections of errors, I would have had no cause to reply. They are only errors according to your personal understanding of terms, and not according to any universal consensus as to the meanings of terms.
It would be hilarious if it took one of the "significant" issue numbers (10000, 12345, etc.)
I'm glad we could bring this discussion to an amiable conclusion! Again, sorry to come off so abrasively in my replies. I need to choose words more carefully myself.
I think `priv` may be enough, unsafe code can do anything anyway.
`|` in match arms means "either this pattern or that pattern". `1..2 | 4..6` is two separate patterns. When you bind a variable in one pattern in a match arm, it needs to be bound in all, so you get `y @ 1..2 | y @ 4..6`. `y @ 1..2 | 4..6` violates this binding rule. We'd have to change what `|` means to allow that, or modify how `@` works. I think ideally we'd support `y @ (1..2 | 4..6)`, but this would still be a hack for `match` and not general-case patterns. Given how rare matching against ranges is, I'm inclined to say "oh well" for this slight ugliness.
This would be an interesting feature, it'd be nice to see what additional abstractions it enables.
Thanks.
Alternatively x if [2,3,5,7,11].contains(&amp;x) =&gt; println!("{}",x), or let mut numbers = range(1,10).chain(range(20,30)).chain(range(99,123)); match x { x if numbers.any(|y| y == x) =&gt; println!("Found {}",x), x =&gt; println!("Not found {}",x) }
`RefCell&lt;T&gt;` allows you share aliasing `&amp;RefCell`s around, and to temporary borrow a dynamically `&amp;mut T`. This still implies using references and being bound to the callstack though. For more freedom, you can wrap the value into a reference counted smartpointer: `Rc&lt;RefCell&lt;T&gt;&gt;`. That way, you can have two lifetime-disconnected parts of the code share the same value safely.
of course global data is already unsafe; (and thats' great) that might indicate unsafe data is a useful concept. 
&gt; glibc must be dynamically linked in currently though. You can statically link against a C standard library as you can with C itself. However, glibc doesn't support full static linking - it will always dynamically load stuff like locale support.
I don't really feel like there's much (if any) influence from Go with segmented stacks gone. It inspired the design of the `rust` tool along with the *old* `cargo` and `rustpkg` but they're gone too.
Oh, that makes sense. Thanks for the tip!
Does the second snippet really compile to optimal code? At any rate, it takes a lot of faith in the compiler to even consider using something like that (well, or stupidity...). 
Rather then overloading `=`, I think it would be cool to have special getter/setter syntax. I've never been a fan of using naming conventions for properties. Maybe `foo[bar] = 123` or `foo#bar = 123`.
It works fine without any external libraries or a runtime at all. On Linux, the standard library does currently depend on the glibc ABI due to the lack of libclang integration for reading the ABI from headers but I think musl's ABI compatibility is good enough to use it.
The way privacy works (module-level, not class-level) is straight from Go. I know because I introduced it into Rust :)
Ah, that's also true of other languages like Haskell though so it's not obvious where it came from.
Hmmm. Maybe overloading the index operator could come in handy. Don't think there's a way to differentiate between getting and setting though? 
It would be great to have a blog post with diagrams of some complicated datastructures and how to make them legal using RefCell, WeakPointer, Rc, Arc, Cell, Mutex etc. (But I don't have the skills to write it) 
Overloading the index operator by the current method is useless for this purpose, but will be somewhat possible with [#6515](https://github.com/mozilla/rust/issues/6515).
Can anyone explain a little bit more what he's talking about? It sounds like he's suggesting skipping a step in the CSS JIT context where they have some intermediate representation of the CSS, as an AST, and instead compile as it's parsed. If something changes the CSS and original source is then required, decompilation can be done trivially with pattern matching. Maybe I didn't read the WebKit post on CSS JITing, but why would the original source CSS ever be required after parsing again?
When i saw the tilte I thought it was about the thread "Compiling with no bounds checking for vectors?" on the mailing list.
Not really; lifetimes are indeed involved however the limitation here is the borrow checker as I understand since `&amp;mut` should never be aliased and I am looking at sharing this reference among multiple observees.
Actually, I was wondering if maybe the Rosetta Code site could not be put to use here. At the very least, I suspect there would be interest in an example implementation of each of the GoF patterns.
Ah thanks. I was not worried about the lifetime, this I understood easily thanks to working in C++ day in day out but about the borrowing aspect. So, looking at the [documentation](http://static.rust-lang.org/doc/0.9/std/cell/struct.RefCell.html) it would imply using either `try_borrow_mut` or `borrow_mut` (with automatic freeing!) or even `with_mut` and pass a closure. And as a bonus, I get detection of races: if the observer while dealing with an event triggers an event to itself I'll either get a failure or (if using `try_borrow_mut`) `None`.
Hmm… I wonder what would happen if you instead simply allowed private field access inside `unsafe`.
Thanks for writing!
Made a mostly pure-rust application that can talk to my LCD, this is the first hello world demo of it. Works: CPU and peripheral clocking, GPIOs, UART, SPI, also C12832 LCD driver. Had a few issues with pointers and mutability, but got there in the end. The code doesn't use heap at all and it's really awesome to see how rust minimizes stack usage as well. C code: memset/memcpy only.
3. No memes. :P
Is this a meme?
Great job as always but one minor correction: rust-tabular is not limited to comma and tab separators, any unicode character can be used but CSV (RFC) and TSV (IANA) are "premade" configurations
that wouldn't solve the problem; i think private and unsafe are orthogonal concepts. See the example I'm posting that demonstrates what I think the issue is
i don't think it is; see the example i've posted. i think usafe is supposed to be a level above 'priv' in alarm/restriction. any 'segfault' or 'exploit' behaviour is supposed to be contained in 'unsafe'. 'priv' can currently work around it :)
Cool! What architecture is this?
Fixed, thanks!
Wouldn't this rather be fixed by making `set_index` an `unsafe fn`?
Fixed thanks.
According to the screen shot, it's an mbed microcontroller with an ARM CPU: https://mbed.org/platforms/mbed-LPC1768/
This one is mbed lpc1768 as correctly noted above, though I'm now porting the code to stm32f4 discovery to see how good can rust be with multi-platform support. I would love to try doing something tiny with atmega-based arduinos as well, but piping rustc to llvm-avr seems to be non-trivial. The output binary size is 10k, 3k of that is font blob. I can go smaller with C, but rust code seems to be more readable.
As I mentioned though, a channel brings both overhead and asynchronous delivery (which you can resynchronize with another channel...), so it might not be desirable. Rust is not made after the Actor model.
There used to be a feature-request issue: https://github.com/mozilla/rust/issues/9663 But I guess there's not enough interest.
Are your rustc modifications and code available? Any thoughts on what would be needed to support targeting embedded microcontrollers like this upstream? (i.e.: how hacky is your current solution)
Ooi, would you say porting this too the frdm kl46z and it's ilk would be a particularly challenging task?
I have FRDM-KL05Z lying around, if I can fit that it will surely run on KL46Z as well :-)
Didn't work for me for some reason. Tried to include this code: https://gist.github.com/farcaller/cdb206ada35a18818035, but still failed to link. I'm pretty sure it's my bad understanding of rust (mind that this is my second rust application after 'hello world' running on os x), and it should be totally possible to write that code in rust. Also, when rust supports different sections, it should be possible to write ARM ISR table in rust as well.
That looks nice! Maybe I should finally buy the armish Arduino Due and play with Rust on it… Edit: I just checked out the lpc1768. Why am I even bothering about the Arduino? This thing is just so much better. Thanks for calling my attention to it.
Maybe we can tempt them with type inference, defined integer overflow, and lack of integer promotion?
Yay for progress towards #5121. (My code stopped ICE-ing)
Servo is more or less doing the same thing, except that it's (a) thread-local (so you don't have to stop all threads); (b) precise instead of conservative; (c) safe, with compiler-generated trace hooks instead of manually written ones.
I've re-written drawing routines of C12832 C++ driver into rust as I was kind of lazy, everything else was written from scratch. I didn't want to constrain myself with CMSIS or mbed code as I wanted to get a feeling of how good can rust code be. Also, I didn't use rust-core as it doesn't seem to compile for me, so the code effectively lacks most of the std runtime (in addition to missing support for owned ~pointers).
lpc1768 isn't actually the best thing, I used it just because I had the mbed application board and the fact that driving b/w lcd is simpler than a dedicated rgb lcd I have. If you're looking for something arduino-like, I can suggest you to look at LPC1114 (avaliable in DIP, the most awesome ARM for breadboarding), STM32F4 discovery board (it's very popular and has a decent debugger) or TI Stellaris Launchpad (has quite a good documentation and lots of demo code, also basic functions for peripherals are baked into flash so it's very easy to start using it).
are you using cmsis or other such library, or is the whole stack pure rust? either way, very cool, makes me again want to figure out how to program the lpc1769 i have laying around.
Whole stack not including memset and memcpy (and ISRs: https://github.com/farcaller/arm-demos/blob/master/04-hello-world/platform/common/isr.c) is rust. I've commented on the runtime [here](http://www.reddit.com/r/rust/comments/21qogc/im_making_a_note_here_huge_embedded_success/cgfshj6).
http://www.mail-archive.com/dev-servo@lists.mozilla.org/msg00662.html
If i got something like a beagleboard, how hard do you think it would be to get rust running on it? Ive been looking for a project, and this sounds fun!
How mature is llvm-avr?
I can't speak for the beagleboard, but I've been experimenting with running rust freestanding on the Raspberry Pi. It seems to work pretty well. I managed to turn an LED on the board on, but I'm still trying to work out why function calls seem to crash it. I had assumed something to do with the stack, but I can create variables on the stack which makes me think otherwise.
My only problem is that I am completely unknowledgable about low level programming, but I want to learn. I want to get a board like a Pi or a BB, but I think I would be way in over my head.
It seems that a lot of features that have a huge impact on good library design (uniform function call syntax, higher kinded types, variadics, compile-time function evaluation,..) are targeted post 1.0. I hope that rust libraries don't have to suffer the same split as C++03/C++11 libraries do suffer.
i hope type inference would be tempting , defined overflow great, but lack of integer promotion can be irritating IMO; promotion plus any other extra types to catch all the cases one might mean would be nicer IMO. "a type that is no more than 32bits, no more than can be coerced safely to poitner types" (usually 32bits everywhere, but if you get a crazy microcontroller with smaller adress size it would be less) find myself wanting a 32bit type most of the time since but its' a pain having to manually coerce to array indices :( .etc.
Awesome! I sorely would like to use rust with my Tiva C. Cortex M4.
Rust (game) is but a faint flash in the history of computing, when Rust goes 1.0 and starts it long life as a productive stable language, the game might already be gone.
It supports them now, via the `#[link_section]` attribute.
yeah, it's probably good enough.
Nah, it's a different test altogether. One with a grey background, covered in form widgets (arranged with floats).
I'd never seen "HULK comments on random subjects" before.
https://twitter.com/PLT_Hulk is fairly well known in the programming languages on twitter circle https://twitter.com/securityhulk https://twitter.com/FilmCritHULK
Wrong sub... http://www.reddit.com/r/playrust/
I don't think I'll ever understand the mindset of someone who is willing to invest the time to log into GitHub, find a repo, and create an issue, but not check that the they're submitting to the correct repo. Similar with the occasional Rust (Game) posts here. It's a particular kind of dedication combined with a particular kind of carelessness.
How are you doing precise tracing on the stack?
I know pretty much nothing as well. I've been learning as I go along. I've started with [this](http://www.cl.cam.ac.uk/projects/raspberrypi/tutorials/os/ok01.html) trying to port the assembly code to rust, looking at [IronKernel](https://github.com/wbthomason/ironkernel)'s arch/arm section for guidance. The basic idea is to simply compile to llvm bitcode (rustc --emit=bc) and then convert that into assembly with llc. I then threw that into the source folder in the example project for the Baking Pi tutorial and modified the assembly file to branch into a rust function.
First you probably want to understand the difference between Cortex M based systems (like OPs mbed) and Cortex A based ones (like RasPi/BBB). The latter is is designed to run a full Linux (or other "big" OS), while the former is designed to run your code on bare metal or possibly with a lightweight RTOS. The difference makes quite a different programming experience. If you have Linux then your application code can be basically indistinguishable from code targeting regular x86 desktop, of course depending on what you are doing. Meanwhile the experience on bare metal will be quite different. You don't have the niceties that an OS provides like multitasking, memory protection, file systems, abstracted drivers etc.
Using smart pointer classes that maintain exact rooting for all JavaScript objects.
Thanks for pointing that out, I'll try to port my C stubs over.
It's really simple to run rust in linux running on beaglebone :) as for freestanding, as noted below, cortex-a (application) architecture is a bit more complex than cortex-m (microcontroller), but that mostly additional features. Cortex-a has fully featured memory management unit, two instruction sets (arm and thumb), a number of CPU modes. The problem with cortex-a is that you actually need some 'bootloader' code, one that would initialize the CPU to a state enough to run 'C-like compiled code' (i.e. you need to configure stack, caches, branch predictors, and do that in assembly code). Cortex-m is much more user friendly in this regard as it sets up the stack for you and ISR calling mechanism is very C-friendly.
&gt; C code: memset/memcpy only. Why not replace them with inline assembly and be done with C dependencies? I find every line of not written C code as positive.
You are looking for /r/playrustservers.
I understand that much (via the `JS` type, right?), but without move constructors, how are the exact stack locations that are roots maintained as values are moved around (I guess this applies to GC'd values inside reallocating containers like `Vec` too)? Is it done manually?
I've noticed a weird error, but not sure what caused it. I called my function `is_restricted` from external crate, and apparently it confused it with something, since it claimed that it had 0 instead of 1 parameter (which was definitely not the case). Changing its name to `is_restricted_char` fixed the problem, but I have no idea why Rust would behave like that. Nothing about it was changed, except maybe the type of crate (from lib to rlib or similar).
The problem will be hard to debug without seeing the before and after source.
I was trying to make a test case, but it kinda didn't work... It's basically this commit - https://github.com/DanielFath/xml-parser/commit/428cb74e5833ce3bd7a8ebddf4a82c2a4c8da684 I assumed there was a `is_restricted` function in std, but I might be wrong on that.
ok but maye there's a bit of "Throwing out the baby with the bathwater."; I'm not suggesting you replicate the exact model of C. Requiring a cast between i32-&gt;u32 , or int -&gt; i32 .. thats fine. Rust can clean up mistakes in C. I'm just suggesting *safe* promotions are ok - whenever the sign bit is not lost, and there are sufficient bits. eg (if int=64ibits) i32 -&gt; int -ok, u32-&gt;int ok, no confusing behaviour. if (int=32bits) i32-&gt;int - fine; but u32-&gt;int - NOT allowed as you can't represent +0x80000000 as i32. etc. this is why I also wondered if another type along the lines of 'short/long' might also be useful - short/long originated in an era when 16/32bit values were very common, and today we have 32/64bit values The new value i have in mind is 'min of 32bits, pointersize, natural machine word size' it would be 32bit most of the time, but on some obscure microcontrollers/DSPs with small adress space it might be less.. the idea is its a compact value and suitable for indexing I realise you will only want to allow coersions that are portable
Well then I think I want the cortex-m in that case, do you think you know how I could get a hold of one on a highschoolers budget, along with some resources? Thanks so much!
I think ST discovery boards are probably the *cheapest* way to get into ARM, eg [32F401CDISCOVERY](http://www.st.com/web/catalog/tools/FM116/SC959/SS1532/LN1848/PF259098), which costs fifteen bucks and can be ordered directly from their website. Tiva Launchpads from TI would be another cheap option, eg [TM4C123G LaunchPad](http://www.ti.com/ww/en/launchpad/launchpads-tivac-ek-tm4c123gxl.html), $12.99 from their webstore.
Okay: let x = 1u32; let y = 2; let z: i64 = x - y; What is `z`? * It is `U32_MAX_VALUE`: `y` is inferred to be `u32` because it is involved in an operation with another `u32 (x)`. The subtraction is in `u32`, and the result is then promoted to `i64`. * It is `-1`: `y` is inferred to be `i64` because it is involved in an operation that yields a `i64 (z)`. `x` is promoted to `i64`, and the subtraction takes place, giving us `-1`. We could pick one option and make it the general rule, but it's still going to be confusing and so easy to get wrong. A cast is easy, explicit, and most of the time you won't be adding numbers of different types anyway. As far as I'm concerned integer promotion is not worth it: the mental strain of constantly keeping the rules (which, as you suggest, will vary between platforms) in mind is worse than having to insert a cast every now and then.
This is a very simple example and there are probably better ways to do it. The reason we need a C stub is because Asterisk modules depend on the constructor / destructor attributes which Rust doesn't support. FYI, the reason those attributes are not supported is because Rust avoids life before and after main.
well r.e. your example above, can you leverage HN type inference to give you more clarity and safety differentiating the language from C, and pick option (ii). asside from that i'd just say its your fault doing overflow arithmetic. you've always got that hazard. the problem i'm having is that i32 really is my deafult type**, and i'm using indexing where i used to use pointer-arithmetic - so I'm having to cast *alot* , not a little. Looking for another solution what do you think of the 'compact safe indexable type' idea (i've made another post) (* for similar reasons to why x32 exists. 64bits is overkill in so many situations. if you've got a 4gb,8gb,16 gb system mostly holding texture data, 32bit indices are fine for coordinates, indexing the textures/pixels, references between elements.. whatever) throwing another idea in there, Perhaps you could make the standard library/languge indexing operator accept &lt;I:Int&gt; or whatever with a static assertion catching the appropriate subset of types.
So I've actually got a bug due to no auto-promotion while writing lcd drawing code, where I converted a result of calculation to u32 with ( ... ) as u32. The problem was that actual variables involved were u8, so all offsets were cropped to 256. It made me re-think my variables capacity and in the end write a more effective code, so I, personally, am totally fine with no auto promotions.
Sure, but don't I need to isolate a test case in order to file it? It's kinda unhelpful to say, well, here is this thing that doesn't work in one commit I made in my 2k+ source code.
No, JS&lt;T&gt; types are unrooted. I literally just implemented this on Friday last week on a [branch](https://github.com/jdm/servo/commits/newroot). We root the SpiderMonkey JS pointers, and have dynamic assertions that catch when the root smart pointer instances change order (which will throw off the stack-based ordering when we switch to a precise GC when upgrading SM).
Rust's new package manager "cargo" will make it so no dependencies are shared. ... from what I understand.
It starts off as a copy of older material, which dates back to at least April 1st, 2013. http\:\/\/parasail-programming-language.blogspot.com\/2013_04_01_archive.html
Shouldn't have closed the issue, maybe he had an actual patch for it.
Indeed. They still talk about managed (@) pointers.
You can also just write them in Rust, and rely on LLVM to unroll and vectorize them. It can even do a better job than most standard C libraries do if you target a specific x86_64/ARM CPU. The `loop-vectorize` pass is pretty smart, and includes vector-aware unrolling.
Perhaps an upcoming version of Rust would be written in Rust ;)
Please keep us posted about your progress! I am looking forward to a no-C system.
To be quite honest, I prefer Rust's simple and straightforward approach where a u32 is always a u32 without surprises or magic, and there aren't a gazillion different number types for which one then wants coercion. I also prefer Rust's current notation over undescriptive names like "word" and "long" etc. C/C++ handled this area poorly.
&gt; I'd be nice if you did Then I'll try (it was late yesterday and I was looking for a hack). I'd like to try my due diligence, before submitting a bug like that. I hate when I'm on a receiving end of an info dump and I'd like to not do that to anyone. Anyway as per your wish I've added this whittled version of the test case that has the wrong first commit and the second 'correct' commit. Hope it helps :) https://github.com/mozilla/rust/issues/13229
I'm now totally lost when it comes to Arc, RWLock, Mutex, etc. Can someone please explain when one would want to use which combination of those?
Or not at all implemented :P.
Rust's `int` and `uint` are defined as being pointer-size so they will be 32-bit on x32.
what's asterisk?
&gt; Rust's int and uint are defined as being pointer-size so they will be 32-bit on x32. ok great. Is it safe to assume that i32/u32 can always coerce upward to int/uint without information loss? (that would work just about everywhere now, but require the deinfition to be 'max(32bits, pointersize)' ) 
The situation is actually much more straightforward and compositional now: * `Arc` - Use this to share all types of data across threads - this is the bread and butter of shared-memory programming in Rust. Many things in Rust are `Share`, including immutable 'plain old data' types, but also more complex concurrent types like `Mutex`, and a million future concurrent types yet to be written. * `Mutex` - This makes things sharable that are *not* Share - particularly things that are mutable. If something needs to be shared but can't be put into an `Arc`, then (unless it's also unsendable) put it into an `Arc&lt;Mutex&lt;T&gt;&gt;`. These two types are enough to do *a lot* of shared memory programming in Rust. `RWLock` is just a special case of `Mutex` for when there are a lot of readers but not a lot of writers because it lets all readers access the contents in parallel. `RWLock` though requires that the type be `Share` so that it's safe to take the aliasable `&amp;` pointer in parallel. A writer that takes an `&amp;mut` pointer has exclusive access to the contents of an `RWLock`.
Thank you! That makes a lot of sense!
This subreddit is for the rust programming language, not the game. That's over at /r/playrust, or for servers, /r/playrustservers.
From asterisk's website: &gt;Asterisk is an open source framework for building communications applications. Asterisk turns an ordinary computer into a communications server. Asterisk powers IP PBX systems, VoIP gateways, conference servers and other custom solutions. It is used by small businesses, large businesses, call centers, carriers and government agencies, worldwide. Asterisk is free and open source. You can find more info about it here: http://www.asterisk.org/
But current version of rust already written in Rust. The first version of rust is written by Ocaml i think :)
(Presumably /u/azth was talking about Rust-the-game being rewritten into Rust-the-language.)
I meant Rust the game :) Edit: /u/dbaupp has it right!
Can Rust run on an Arduino (Atmega AVR chips)?
&gt; let mut uglies = Vec::new(); &gt; &gt; for path in paths { &gt; uglies.push(UProc::new([path.as_str().unwrap().to_owned()])); &gt; } This could be written as let mut uglies = paths.iter().map(|path| UProc::new([path.as_str().unwrap().to_owned()]))); (And then drop the `.mut_iter()` part in the subsequent loop.)
Freescale FRDM as well
FRDM boards don't have a reasonable way to debug code, do they? JTAG, SWD or windows-only option is unreasonable :-) Some of the boards are supported by mbed, so GDB might be an option though.
Well, at least the mbed firmware supports CMSIS-DAP, so yeah, it does support USB debugging from what i can see.
While I agree that `map` is a clearer way of doing this, I think it would be much easier to read if it was spread over a few couple lines.
Thanks for the hint. Given my background with Wirth's languages and safe systems programming, I tend to touch plain C only when required to do so.
Isn't there a problem with writing them in Rust? (LLVM recognises them as `memcpy` etc. and "optimises" them to direct memcpy calls, causing infinite recursion?)
It means you need to define them in a separate crate: https://github.com/thestinger/rust-core#freestanding-usage
About your splitting problem: If you have 6 files and 4 cores, `6 % 4 = 2`, so you need to `+= 1` for 2 cores. First you take integer division and then increment.
I was looking into that too. Homebrew itself is all about compiling from source (and of course rust 0.9 and rust HEAD are both in there), but it seems like this would be perfect for [homebrew-cask](https://github.com/phinze/homebrew-cask). As far as I can tell it isn't in there yet though.
Submitted [a pull request](https://github.com/caskroom/homebrew-versions/pull/198) to add this to caskroom/homebrew-versions, which seems like the right place for binary nightlies. Assuming they accept it, you'll be able to do this to install: brew tap phinze/cask brew tap caskroom/versions brew cask install rust-nightly I'm not sure yet how well it will work for upgrades (you might have to just do uninstall+install), but it works well for installing!
Rust's alias properties are flow-sensitive, i.e. they depend on the program point being considered. For example, at a given program point I know that a mutable reference doesn't alias any immutable reference that is usable at that program point, but that doesn't imply anything about those two pointer values at another program point. LLVM's TBAA tags, on the other hand, are flow-insensitive. Given two LLVM memory instructions with non-aliasing TBAA tags, I can rearrange the two instructions with no concern for any other property. If you use separate TBAA tags for mutable and immutable references, you are going to run into problems. Consider this Rust code: fn f(a: &amp;int) -&gt; int { a + 1 } fn g(a: &amp;mut int) { *a = 0; println!("{}", f(a)); } If we give the load in f and the store in g separate TBAA tags, then after inlining f into g LLVM will be able to reorder the load in f before the store in g, which is incorrect. Strictly speaking, LLVM's optimizer will generally look for obvious violations of TBAA rules and avoid them, to deal with C programs that cast things between pointer types in the same function, but there is no guarantee of what this entails. Please correct me if this is not what your proposed TBAA tagging actually does.
Yeah, the use of inlining doesn't really change anything. I just felt that inlining makes the problem more obvious, because you can't use any potential solution (which would ultimately be a nonsolution) that relies on an intraprocedural analysis of borrowing.
Or you can simulate generics using interfaces like the [btree package](http://godoc.org/bitbucket.org/santucco/btree#Key). For some reason this style hasn't caught on in the Go community, but IMO it's much nicer than using interface{}, explicit casts, and reflection everywhere. Kinda reminiscent of the C way, but not as painful.
That's the least ugly solution I've seen, but it still requires modifying or wrapping every type you use in a collection.
&gt; f and the store in g separate TBAA tags, th would the 'TBAA tags' effectively have to change when lifetimes/mutables go in and out of scope? .. at function call boundaries? is that even possible (I dont know the intermediate details of how compilers handle this - but I've had to deal with restrict in C/C++ and analyzing asm. We knew exactly what we wanted to happen as the asm level, but restrict still scared people a bit. A similar effect could be acheived by creating local temporaries for intermediate calcualtions more often - but this was prone to register overspill, so it defeated the point. so long as a function body can assume its outputs dont overlap its inputs.. the optimization we need can be done
git submodules aren't *supposed* to point to subfolders. If there's something in a subfolder that you want to point to, it deserves to be refactored out into its own repository. Even without a package manager, the procedure for 'build this package' is: for each dependency, build dependency, then install dependency, and finally actually build this package. For your case, there should be 4 repositories: A1, B1, B2, A2. A1 probably has B1 and B2 as submodules because it needs them in any case, but - and this is *absolutely* *critical* - it does not *rely* on B1 and B2 being in submodules. It can work just as well if an appropriate version of B1 and B2 were previously installed. If A2 is intended to stand alone, it might declare B1 and B2 as submodules, but for people using it as an addon to A1, they wouldn't use its submodules at all, since for that case, they would already have A1, B1, and B2 installed. As far as version upgrades? At some point, B1 and B2 get some additions. Some time later, A1 is adjusted to use the newer versions and updates its submodules. When A2 is adjusted to use the newer versions, it updates its submodules. If A2 makes it a priority to keep compatibility with A1, then that only affects which versions of B1 and B2 it can have as submodules.
Will ~[T] and [T,..n] remain distinct types, or be merged into one type? It seems odd to have two distinct fixed size array like containers. What about pattern matching on Vec types? 
`[T, .. n]` is very different to `~[T]`. The former is an unboxed fixed-at-compile-time array, the latter is boxed (a heap allocation) and can have size determined at runtime. `~[T]` isn't actually a special type, it just falls out naturally from `~` being a pointer and `[T]` existing. `[T]` will become an "existential" over `[T, .. n]`, i.e. in pseudo-Rust `type [T] = exists n. [T, .. n];`. There is a missing piece of information for `[T]`: the length. To keep memory safety, this has to be stored somewhere, and is done by requiring that unsized types are behind pointers, and then any metadata is put next to the pointer, i.e. `p: &amp;amp;([T, .. n])` can be coerced to `s: &amp;amp;([T])` by the compiler effectively creating `(p, n)` (i.e. making `p` a fat pointer). Similarly `~([T, .. n])` can become `~([T])` by creating a fat pointer. (I've bracketed things to make it clearer that the `~` and `[]`'s are separate.) (Frankly, I'm with /u/strncat that the fact that DST `~[T]` exists is just an unfortunate consequence of DST, and is sucking people into wanting to use it even when `Vec` would be better.)
They will remain distinct types. A `~[T]` would be a `~[T, ..N]` where the statically known size `N` has been turned into a runtime-only known value and stored as part of the pointer. Although the allocation size would remain fixed, it would no longer restrict the type to a specific one.
Would it be worth implementing a lint to nudge people away from `~[T]` and towards `Vec&lt;T&gt;`?
There is one, on allow by default (`deprecated_owned_vector `); its currently deemed too noisy to be active. In any case, it's looking increasingly like we're not going to remove `~[]` entirely because people are wanting to use the DST `~[]` in many places , which I and some others think is misguided for reasons mentioned on the issue that was discussed in the meeting (I'm on my phone, so no link for now); unfortunately it seems most of those concerns were not mentioned in the meeting. :( The big one is passing the size of the allocation to `free`, which allows it to be far more efficient. (I'm waiting for Alex's email to the list to write my thoughts out properly.)
Looks like w3.org is having a fun April Fools....
I agree that having [T] is a bad idea. It seems like the reasoning for it boils down to "because we can." IMO it introduces needless complexity for no benefit. Having it solves no problems.
From the perspective of the casual observer, reading the meeting notes in the OP makes it sound like `Vec&lt;T&gt;` should *only* be used when `push` is required---and `~[T]` for all other cases. Is that what you're disagreeing with or is it some other issue that I'm missing? (Just trying to make sure I understand things right.)
I didn't actually say `[]` was a bad idea, and I don't think that at all. I just said `~[]` (i.e. with the twiddle) was unfortunate because it's sucking people into using it in (what I think are) inappropriate places. `[]` makes certain things more uniform, especially if we are getting DST for traits anyway. I.e. `&amp;amp;*vec` will be an equivalent (or replacement) for `Vec.as_slice`, so library defined vector/array types can act more like the builtin slices (which auto borrow to `&amp;amp;[]`). I guess you might think this is pointless.
I'm disagreeing with doing it in library code, i.e. where someone else will be dictating how the returned `Vec`/`~[]` is used. I can't think of a scheme in which returning a `~[]` is free/cheaper relative to just returning the `Vec` that was used to build the `~[]` (since almost all functions will be building the vector incrementally). (Some schemes only have second order effects rather than making the actual return slower, e.g. forcing our allocators to be slower.)
It's unsafe to adventure alone! [Take this](http://static.rust-lang.org/doc/master/index.html)
As long as you don't place any grenades in the stack, you may count on me
I would like an explanation as to the benefit of actually using `~[T]` in a post-DST world. The only plausible scenario I can think of is if you need to heap-allocate a vector that is a) dynamically sized, but b) the size is known before you do the allocation. In that scenario you could allocate a `~[T]`, mutate the individual elements, and return it, although this would either require unsafe code (to do the element initialization) or doing some form of 0-initializing (e.g. if it's a `~[uint]` it could zero out all memory). But this is kind of an edge case. In the general case `Vec&lt;T&gt;` will be used to construct these things. And I don't see the benefit of translating a `Vec&lt;T&gt;` to a `~[T]`. The only reason I can think of to do that is if you're going to have hundreds of thousands of vectors and want to save one word per vector. But that should be something that client code explicitly chooses to do rather than being done for you by libstd functions. My two biggest concerns with having library functions use a `Vec&lt;T&gt;` internally and convert to `~[T]` before returning are 1. The `~[T]` makes it impossible to see how much memory is wasted as capacity that is no longer accessible, and there's no way to shrink to fit without converting back to a `Vec&lt;T&gt;`, and 2. Without special allocator support, converting a `~[T]` back to a `Vec&lt;T&gt;` still leaves the capacity inaccessible, requiring a reallocation upon pushing anything at all to the vector. Basically, doing this transformation will waste an unknown amount of space (depending on how much unused capacity there is in the various vectors returned from libstd), with no solution in client code short of forcing a reallocation in an attempt to shrink to fit. Based on this, I think that we should convert all uses of `~[T]` over to `Vec&lt;T&gt;` in the standard libraries. We should continue to support all the methods on `~[T]` that make sense (e.g. everything today except for push/reserve/shrink), but we shouldn't ever be returning a `~[T]` from a function. Besides avoiding the concerns listed above, this would also serve to make vectors much more consistent. If some functions return `Vec&lt;T&gt;` and some return `~[T]` that would be somewhat confusing.
It seems to me this would be better served by just wrapping the field in a type that requires `unsafe` to peer through. Kind of like `*T` except not with the pointer indirection pub struct UnsafeField&lt;T&gt; { data: T } impl&lt;T&gt; UnsafeField&lt;T&gt; { pub fn new(v: T) -&gt; UnsafeField&lt;T&gt; { UnsafeField{ data: v } } pub unsafe fn get&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a T { &amp;self.data } pub unsafe fn get_mut&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut T { &amp;mut self.data } } You could then use it like: pub struct Foo { pub x: int, pub y: UnsafeField&lt;int&gt; } fn bar(f: Foo) { // x can be accessed without unsafe println!("x: {}", f.x); // y requires unsafe println!("y: {}", unsafe { *f.y.get() }); } The biggest quirk here is the `UnsafeField&lt;T&gt;` doesn't implement `Clone`/`Show`/`Eq`/etc. You could easily derive them, but these methods would not be unsafe, and I don't know how strict you want to be on that.
Sorry /u/xGeorgeous, but this is not the subreddit you are looking for. Look at /r/playrust.
You and I already discussed this, but for the public record: One benefit of `~[T]`, post-DST, is that it can be coerced in-place to a `&amp;[T]`. This implies for example that a `~[~T]]` could be safely borrowed as `&amp;[&amp;[T]]`, which does seem useful to me. You cannot do the same with `Vec&lt;T&gt;` because its layout (and size) differs from that of `&amp;[T]`, so while one can create a new `&amp;[T]` from a `Vec&lt;T&gt;`, one cannot coerce it in place, and thus you cannot borrow `~[Vec&lt;T&gt;]` (nor `Vec&lt;Vec&lt;T&gt;&gt;`) as `&amp;[&amp;[T]]`. 
This is the same problem any static linking language has. You have a project with depends on N libraries, each of which has N' dependencies itself. 1) How do you fetch the artifact and it's dependencies? 2) How do you determine what order to build dependencies in? 3) How do you 'subcontract' to each sub-dependencies unique build system from a single top level? 4) How do you determine what order to link said dependencies in? It's a good question; these are horribly managed in the C ecosystem, with (3) an especially frustrating cross platform problem. I'm not sure there's any magic bullet here that lets you get away without *some* kind of package manager and solve these problems. Rustc should be able to help with (4) at least... 
&gt;LLVM's TBAA tags, on the other hand, are flow-insensitive. (so are these designed around C++'s aliasing rules - i beleive above C, C++ can assume different types dont alias , is that right, but the reason we still needed restrict is that still isn't enough for all potential optimizations..) 
ok thats work,nice option. I was becoming happier with the general idea that 'unsafe' makes the *file* unsafe, I suspect thats how it would be used on projects.. becaue its so easy to enforce (grep unsafe in a checkin script) ... any changes to a file with 'unsafe' require special attention, and encourage people to divide up modules such that unsafe-containing modules are minimized 
&gt; Without special allocator support, converting a ~[T] back to a Vec&lt;T&gt; still leaves the capacity inaccessible, requiring a reallocation upon pushing anything at all to the vector. Wouldn't this be a really cheap reallocation where the allocator goes "oh hey the requested size is way smaller than this bin, you're good to go!" and does nothing beyond figuring that out?
aatch has an [old but once-working](https://github.com/Aatch/grease-bench/tree/master/src/sys) syscall wrapper. Pure Rust, of course.
&gt; This implementation is only for Linux on x86-64 architectures. We could easily do the same thing for other architectures and OS. Not on Windows, you can't. The syscalls change from OS version to OS version. `kernel32.dll` is the only supported way to call the NT kernel.
They have to conform to backwards compatibility (to the 16-bit era!), which is often worse :)
Nice typography. I would probably raise a bit the font-size for large screens and decrease the line-width further (it helps with readability). Something like: body { max-width: 750px; } @media (min-width: 992px) { body { font-size: 22px; } code { font-size: 18px; } } Raising that font-size is a significant improvement on large screens where you will read from further away (especially since Crimson has a quite tight x-height, like Times).
I think it's more comparable to getting rid of msvcrt, which is actually quite within reach. For the records, most if not all console functions in Windows API live in kernel32.dll, and libterm would need them anyway.
Thanks! I have fixed it as you suggested. It is a CSS that I borrowed from Hakyll example sites (I don't recall which one), I haven't tweaked it much yet (I am no so good at site design anyway).
`errno` is thread-safe in the context of OS threads, but for green threads like ghc implements, they probably need to take extra precautions to make sure that they read out the errno variable of the OS thread that actually called into libc, before that OS thread gets to do anything else, even if the green thread gets scheduled elsewhere or whatever.
One would hope the allocator could figure that out, although I can imagine allocator designs that make even that rather expensive to calculate.
With a custom AliasAnalysis you can implement any sort of alias oracle that you like, but the tricky part is updating your analysis. The usual kinds of alias analyses that people have implemented in the past rediscover aliases by doing a conservative program analysis on the IR, but a Rust alias analysis would be trying to preserve information from the Rust type system throughout optimization, and none of those transformations would be aware of the Rust type system. I think the other implementations of AliasAnalysis were used more for program analysis than traditional compilation, and I'm not even sure if they ever actually worked correctly in the compiler optimization pipeline. For quite some time (and it may even still be there, I'd have to use a custom AliasAnalysis to see) it was the case that after invalidating a custom alias analysis by performing an optimization you would just get BasicAA instead of recomputing the custom AliasAnalysis you specified, or it just wouldn't get recomputed. And recomputing it after every transformation would be too expensive anyways. I think that you probably want a variant of memory SSA to take full advantage of Rust's alias properties in a compiler, but that's not something that can easily be bolted onto LLVM. It will be particularly tricky to come up with a system that allows for some usage of unsafe casts to be compiled in a sound manner.
Yes, they're designed to support C/C++ aliasing rules, but they would also support the aliasing rules of most other type systems. LLVM IR has the 'noalias' property of function parameters that gets lowered from 'restrict' in C. The biggest limitation with 'noalias' is that it is not preserved upon inlining, and solving this is a pretty difficult problem. Currently the LLVM loop vectorizer runs after the inliner / SCC pass manager runs, but the straight-line code vectorizer runs as part of the SCC pass manager. This means that inlining might break loop vectorization that relies on 'restrict', but it shouldn't break vectorization of straight-line code.
What about pattern matching on a Vec? Right now it's not possible, .as_slice() is needed.
This, quite honestly, makes no sense at all to me. Shouldn't ~[T] just be a pointer to a [T]? Couldn't we just have a StaticVec&lt;T&gt; or something? Having ~[T] doing something totally unrelated to [T] just seems, to me, messy and confusing.
~[T] _is_ just a (unique) pointer to [T] (its just that [T], as opposed to [T, ..n] will not be available in the future). It does no more. The discussion is around having both Vec and ~[T] around, since they are very similar types.
Yes, we had a couple, but they bitrotted due to lack of maintenance. It is totally doable; we just need someone to pick it up and maintain it.
All is well, then! Excuse my silliness! 
&gt; In a post-DST world, Vec&lt;T&gt; will be the "vector builder" type... For those of us who aren't following development carefully: I'm guessing that "DST" is "dynamically sized types"? 
Unlikely.
Great writeup, thanks!
[There](https://mail.mozilla.org/pipermail/rust-dev/2014-April/009353.html) is [some resistance](https://mail.mozilla.org/pipermail/rust-dev/2014-April/009367.html) to using ~[] too much.
/r/playrust
It is better in pretty much any real-world case. Android is switching from a JIT compiler to AOT compilation for *Java* in order to improve resource usage, performance, latency and battery life. Unlike Rust, Java actually uses a very large number of virtual function calls. In Rust, they're a rare exception.
Personally, I would love to see higher-kinded types in Rust. Lots of terse-but-readable (after a brief initiation) and inherently-safe transformations can be done with those, even by people without a detailed background on category theory (like me).
If someone wanted to, I'd love to see an article or blog post actually motivating HKTs. Especially if it were Rust specific. I.e., show what improvements it would bring to code, and especially libraries. What practical things could we do if we had HKT that we can't do today (that is proper, useful examples, not toy examples which just exercise the type system).
Maybe via generalised patterns using the `Deref` traits + an impl of `Deref&lt;[T]&gt;` for `Vec&lt;T&gt;` (this last step requires DST). I don't think there are any concrete plans, but some loose discussion about this kind of thing has happened on IRC.
Posting in the wrong subreddit won't get people to join.
The JIT has the entire life-time of the program to perform optimizations, which in case of a long running program is days/months etc. It could very easily remove dead code from runtime constant discovery over such a long period, and I think in the case of the HotSpot compiler, it does use these techniques. One can also use an AOT compiled binary with a JIT "enhancer" to get rid of some of the warmup time needed for Java. I think C# and some closed source JVMs do something similar too.
/r/playrust ? /r/playrustservers ?
&gt;&gt; they ~~got rid of~~ will be getting rid of If I'm not mistaken, they haven't actually released anything, so past tense is premature here. 
I don't really understand what the point would be of optimizing over a long period. As a systems language, Rust needs to provide predictable performance. It has a strong focus on static dispatch (generics) and predictable/controllable memory allocation/layout. I can see why optimizing over the course of a day might be useful for scientific computing, but for almost anything else it's going to be unacceptable to waste so much power and lose all of the gains every time the process needs to be restarted.
I still don't understand what HKT allows that we can't do already. All I see from that is that it allows you to omit the `&lt;T&gt;` in favour of `&lt;_&gt;`, thus that it's just a little bit of extra verbosity. And yet I gather that there is something new that it allows, so I presume I missed something in this article?
One thing I want to avoid is Rust becoming an academic language. I do not want monads to be a standard concept in the language—they're hard things that Rust's current primary audience don't understand and dismiss; for the moment, I think using those words will be bad marketing for the primary audience that Rust is aiming it. Later after Rust has traction, maybe these things will come along, but I think we've got to be careful not to make Rust Haskell, at present at least.
 It's one thing to be wary of very experimental ideas. And indeed the place for those is academia. It's another to refuse to learn tried and tested concepts, that have been proven to be very useful in practice. 
But we don't want them: they have scary names! /s
They dropped 16-bit support with 64-bit Windows. One time, for the fun of it I decided to install MS Works for Windows 3.1 on my Win7 laptop. Everything worked reasonably other than printing. Microsoft takes backwards compatibility seriously. Most people would never notice that XP and later was a completely different kernel than previous home editions.
I'm the first to agree *monads* and *monoids* are useful. Still, if the point being made is HKT's are needed to define traits over type constructors, why isn't *functor* the motivating example? 
Definitely a good idea. I'm still thinking about the best way HKTs would be implemented (I know Scala has a limited inference system, so it needs things like currying, and super weird type aliasing to partially apply type parameters). So I think HKTs in Rust would be *much* simpler than in Scala where, well, it gets quite messy. I'll probably write up a few follow up posts. I haven't touched the bounding system, and how HKTs would extend it and further, more practical examples.
You're welcome :)
Currently in Rust, we only have the ability to express first-order types (type constructors) such as `Vec&lt;T&gt;`, `Foo&lt;T, K&gt;`, etc... That's great! We get to abstract over proper types (values). So `Vec&lt;int&gt;`, `Foo&lt;int, ~str&gt;`. However, HKTs presents a new abstraction. It allows you to abstract over type constructors themselves, which abstraction over proper types. Thus, when you have `Foo&lt;F&lt;_&gt;&gt;` (let's say), `F` is a type constructor, not a proper type. `Traversable` might be a good example. It allows you to abstract over say `Vec&lt;T&gt;` or `HashMap&lt;K,V&gt;`, etc... without specifying the type parameter. You might have: trait Traversable&lt;CC&lt;x&gt;, T&gt; { fn filter(&amp;self, p: |T| -&gt; bool) -&gt; CC&lt;T&gt;; fn retain(&amp;self, p: |T| -&gt; bool) -&gt; CC&lt;T&gt; { self.filter(!p(_)) } } Then: ``` impl&lt;T&gt; Traversable&lt;Vec, T&gt; for Vec&lt;T&gt; {} ``` That would be the ideal implementation. However, currently, you can't have `&lt;Vec, T&gt;` and have the `Vec` not be a type parameter, but the actual type `Vec` *and* that infers it's type `T`. So, higher-kinded types abstract over types that abstract over types. This let's you define these more abstract concepts like `Monad`, `Monoid`, `Functor`, etc... without having an implementation per type constructor. Not sure if that made any sense or not.
I think the naming convention the community is settling on is - `Vec&lt;T&gt;`: vector - `[T]`: "slice", e.g. `&amp;[T]` borrowed slice, `Rc&lt;[T]&gt;` reference-counted slice, `~[T]` owned/unique slice - `[T, .. n]`: (fixed size) array
&gt; The reason you have to learn them in Haskell is because Monads are the only form to escape purity This is false. I can only guess what you mean, but let me begin by stating that Haskell didn't always have a Monad class. That only happened with the move to version 1.3. What allows you to enter the world of side effects is the IO type. You construct and sequence IO actions. Of course you can simulate side effects in a pure context, e.g. you can use sum types (i.e. Maybe or Either) to simulate failure, or you can use functions (s,a) -&gt; (s,b) to simulate functions with state. What happens is that in all of the above cases, you will want to use operations that are similar in spirit. E.g., you'll want to compose effectful functions, or lift pure functions to work with effects. For this reason it is useful to have overloaded operators, and it is useful to have an interface (i.e. class/trait) you can abstract over, to avoid duplicating code. And that's what the class Monad provides. You don't *need* the class Monad for anything. Much like you don't need any other class. All they do is provide you with ad hoc polymorphism and abstraction over the corresponding interface. 
Last October? That's over five months old! Wonder how much it's changed now? What changes would be required?
So if I read this right ~[T] =&gt; java array vec&lt;T&gt; =&gt; java ArrayList&lt;T&gt;?
Note that all Monads are Functors. The former are just Functors with two extra methods. It seems to me the whole point being made is that it's useful to define traits over certain type constructors. Well traits are there to provide you with an (ad-hoc) overloaded interface, over which you can abstract your code. So right now you have e.g. a trait Set that provides you with an interface for sets. You can check if an element is in the set, and check if two sets are disjoint or if one contains the other. But you cannot, e.g. turn a set of elements of type A into a set of elements of type B, using a function f: A-&gt;B. Functor is the abstraction that allows you to do the latter. *EDIT*: As a side note, why is the method in Set, `fn contains(&amp;self, value: &amp;T) -&gt; bool;` called contains and not "belongs"? Isn't "contains" typically used to refer to the superset relation? 
(And so is `Option`, `Result`, `Future`, (most of) the container types, etc.) I believe the "interesting" bit here is being able to express the Monad trait/type class, not just having monadic types/operations.
&gt; EDIT: As a side note, why is the method in Set, fn contains(&amp;self, value: &amp;T) -&gt; bool; called contains and not "belongs"? Isn't "contains" typically used to refer to the superset relation? My guess is to be [consistent with the rest of the standard lib](http://static.rust-lang.org/doc/master/std/index.html?search=contains). It seems like `contains` is used whenever one wants to test if a member is in a collection.
Thanks for the correction.
Type abstraction does not an academic language make. They're well-understood and proven in the literature. IMO this is no different than the region system. They expand the amount of code you can express safely. HKT just exist at the type level, which I think are dealt with less by most programmers today. Keep in mind that when Microsoft Research was working on C#, they almost axed generics entirely, thinking that only type theorists would be able to understand and use them. And yet, today .NET has Monad(Plus), applicative functors, and more, via LINQ. I find that some of the documentation produced by that community is the best source of "theoretical features thrust upon practical programmers" stories. I don't find the marketing argument pursuasive, since we *just don't need to advertise the feature heavily*. We can just offer "Higher Kinded Types" as a cookie to those who appreciate them, while the rest of us can reap the benefits of the libraries they produce. (*evil cackle*)
That said I don't completely understand them, but a few in `#rust` have said the lack of them prevents a clean API in some instances. I also don't want to turn this conversation into a circlejerk. I just find this particular argument entirely ineffective. Yes, if we used this as a cornerstone of our pitch, that'd be bad. But clearly no-one wants to do this. I think this is apt: Rust offers a modern type system combined with a model of memory safety that collaborate to provide zero-overhead abstractions that maintain software reliability. Wording could use some work...
That's not better nor worse. That's different. It removes the requirement that both structures (that holding the A's and that holding the B's) must be the same, and adds the requirement that both must be iterators. To make it clear, note my point was not simply that you could then have mappings between sets only. That would be an example of an implementation of the trait Functor trait, which provides a single map method: fn map&lt;A, B, F: Functor&gt;(xs: F&lt;A&gt;, f: |A| -&gt; B)-&gt; F&lt;B&gt; 
Ok, perhaps. What would the advantage of this be, then? Especially since the rest of the language trends *hard* towards iterators, since they are what's efficient. I agree that we can't express this today. But I don't see how this specific example helps us. Though, I can imagine (but not give concrete examples) cases where you would want this that can't be expressed with iterators. Some tree or graph structures, perhaps.
&gt; I don't really understand what the point would be of optimizing over a long period. As a systems language, Rust needs to provide predictable performance. It has a strong focus on static dispatch (generics) and predictable/controllable memory allocation/layout. &gt; The lack of predictable performance is a good point. Predictable/controllable memory layout does not need to be given up to get JIT benefits. All the gains need not disappear when the process needs to be restarted, unless the assumptions have changed. If they do change then the JIT would have to go through a de-optimize/re-optimize cycle. You can just start with a binary that has been optimized from the long run in a JIT environment. 
&gt; Ok, perhaps. What would the advantage of this be, then? Especially since the rest of the language trends hard towards iterators, since they are what's efficient. Well, what's the advantage of having a Set trait? 
&gt; Iterator: Functor, Applicative, Monad, MonadPlus, Foldable, Traversable, Alternative(?) I guess `None.move_iter()` + `.chain` form an `Alternative` instance. &gt; To me, the interesting bit is that entire call chains of these can be optimized away before LLVM I think Rust would need a reasonably powerful effect system before being able to do sane rewrite rules. 
&gt; Especially since the rest of the language trends hard towards iterators, since they are what's efficient. A functor is more general than an `Iterator`. e.g. `Future` is a functor, but the iterator equivalent isn't really the same. I'd guess it would be pretty hard to fit a parsec-like interface into an Iterator too. (And yeah, an "in-place" fmap on a tree isn't really an iterator-able thing.)
To allow one to abstract over multiple implementations of a set... I suppose this makes sense.
&gt; Though, I can imagine (but not give concrete examples) cases where you would want this that can't be expressed with iterators. Some tree or graph structures, perhaps. Not entirely sure about this, (not sure how collect/from_iterator works) but also maybe cases where the elements are not 'held' but 'generated'. E.g., you could conceivably map a random number generator with a function, resulting in a new generator that returns the mapped numbers. 
&gt;So if I read this right ~[T] =&gt; java array vec&lt;T&gt; =&gt; java ArrayList&lt;T&gt;? Sort of, for `Vec&lt;T&gt;` you are correct. DST (Dynamically Sized Types) says that `[T]` is an unsized type. This means it cannot be used by itself at compile time (as the compiler needs to know the size of all types at compile time). As such, it must be embedded in another type in such a way that makes the type "sized" again. In short, hidden behind a pointer type, like `~`. The similarity to Java arrays is only that it is a fixed-sized array with a length known only at runtime. In post-DST Rust, `~[T]` will not be special in any significant way. 
This is not ivory tower stuff, it has very real practical benefits. Our current monadic APIs are a mess, with lots of duplicated functionality. HKTs would go a long way to reigning in the madness, and making it much easier for folks to keep the APIs in their head without heading back to the docs at every turn.
Correct. Daylight saving time does not affect type usage.
This thread has been linked to from elsewhere on reddit. - [/r/haskell] [HKT discussion in /r/rust](http://np.reddit.com/r/haskell/comments/222yw8/hkt_discussion_in_rrust/) *^I ^am ^a ^bot. ^Comments? ^Complaints? [^Send ^them ^to ^my ^inbox!](http://www.reddit.com/message/compose/?to=totes_meta_bot)* 
&gt; Also, proper code reuse. Once upon a time, some guy proved his proxy system was isomorphic to monad transformers, and cut his public API to 1/3rd. Keeping or current monadic APIs in my head at once, without having to refer to the docs all the time, is a constant cause of frustration. Plus they are not easily composable. :(
https://mail.mozilla.org/pipermail/rust-dev/2014-April/009352.html yeah... I have no idea what I just read
I find this interesting historically.
On the 29th, Rust will shift forwards to 1-based indexing as is suitable for a new moon cycle. Have a safe flight.
I wasn't meaning the input,but rather the API exposed: `Parsec` is a functor, but `f &lt;$&gt; parsec_object` I don't think one wants to be writing that as `parsec_object.map(f)` (using `Iterator.map`), since that returns fixed `std::iter::Map&lt;...&gt;` type. (As an aside: backtracking can also be achieved just by taking `Clone`able iterators, and `.clone`ing a copy at points that might need backtracing; this allows for arbitrary look-ahead, and doesn't require the peculiar `unnext` operation, although this has other drawbacks.)
In Haskell parlance, a Set is a kind of Functor. And the defining operator of Functor is the 'map' operator that applies a function "evenly" across the Functor. Lists, Sets, Vectors, and even types like Maybe (like Rust's Option type) are instances of the Functor type class. The advantage to this is that you capture the idea of 'lifting' a function A-&gt;B to any Functor[A]-&gt;Functor[B] (or however the notation would look...) and use the same 'map' function to do this on all Functor-implementing types. So, in this case, your operation f:A-&gt;B would more generally belong to the Functor higher-kinded trait, and Set would be a normal parameterized type that implements the Functor trait.
So it's basically C++'s template templates? template&lt;template&lt;typename _&gt; T&gt; struct Foo {};
According to http://stackoverflow.com/questions/2565097/higher-kinded-types-with-c HKT can be expressed with them.
The syntax `List&lt;T&gt;` to refer to the type constructor `List` is confusing as hell. This is the same kind of fuzziness that mathematicians use when they say "the derivative of `x^2`"), and it is best avoided, especially when the subject matter is not clear yet. There is a very important subtlety about higher-kinded type operators that you didn't mention in this post, and matters a lot of the design and implementation: are you talking about higher-kinded type operators in general, or higher-kinded type *constructors*? `List` is a (one parameter) type-level constructor just as `Cons` (the constructor of linked list cells) is a (two parameters) term-level constructor: once fully applied (`List&lt;Int&gt;`, `Cons(1, Nil)`, it is its own result and doesn't reduce any further. On the contrary, `Dup := ΛT.Tuple2&lt;T,T&gt;` is a type-level function of kind `★→★` that is not a constructor. `Dup&lt;Int&gt;`, computes one step to reduce to `Tuple2&lt;Int,Int&gt;`. Some operators may do more computation, such as `App := Λ(F:★→★)ΛT.F&lt;T&gt;`: `App(List,Int)` reduces to `List&lt;Int&gt;`. In Haskell there are only type-level *constructors*, and not type-level operators in their full generality (type families are a way to emulate them in many situation, which makes seeing the difference a bit tricky), as they exist in theoretical languages like Fω, or (heavily encoded) in some ML module systems. Restricting oneself to type-level constructors simplifies type inference (I believe that's the main justification for this restriction) as it lets you assume injectivity: if you know that `C&lt;T&gt; = C&lt;U&gt;`, then you can deduce `T = U` (modulo some assumptions on your datatypes). On the contrary, in the general case of type-level functions, this assumption is incorrect, see for example the type-level function `Ignore := ΛT.Int`: you always have `Ignore&lt;T&gt; = Ignore&lt;U&gt;` even for `T ≠ U`. If your language has less type-inference expectations than Haskell, maybe this design choice could be revisited. It seems that the restriction has not been a problem in the practice of Haskell programming (and even less so with type families, but Rust isn't going to get something like that anytime soon), but Rust has fairly different usage cases. 
unfortunately `Set` is not a functor in Haskell, but might well be, whenever Haskell has better support for constraints (`ConstraintsKinds`) in the standard.
&gt; Plus they [Monads] are not easily composable. well, but you cannot compose arbitrary types at all, so this is not worse than before
Thanks for pointing that out. I'll fix the article to be more explicit.
It's not monads that are not easily composable, but our ad-hoc monadic types each with their own interfaces.
while Set is most definitely a functor in mathematics, there are very sound reasons why it is a very dangerous thing in pretty much every programming language in existence, even ones with extremely powerful type systems such as Agda/Idris etc. It comes down to the nature of equality, and any functor in programming cannot work if it every adds some equality that considers additional structure that an intermediate mapping doesn't consider. In some sense this comes down to the very definition of a function in mathematics, where a function must produce the same result for the same input. Unfortunately in programming we have the ability to have different extrinsic notions of equality, so even our definition of something as basic as a function is subjective and based on forces outside the particular function itself! Even with constraint kinds we can't prove that the orderings used do not consider any additional structure, so while ConstraintKinds may give us some hope of expressing Functor for Sets, it doesn't solve its intrinsic danger. I did a preso about this one time, slides are here (pdf download): http://jwesleysmith.bitbucket.org/set-functor.pdf
I’d like to make a point that template template parameters are never truly necessary in C++, as type template parameters in tandem with associated types (and alias templates, meaning associated type operators) can do everything they do. (Partial specialization also helps, as it allows easy ‘rebinding’ of template specializations — altough I’m not sure what that translates to in language-agnostic terms.) [ edit: [here’s an example](http://coliru.stacked-crooked.com/a/31326d2a7bdfb917) which I’m not too sure to express as well with template template parameters ] In the Haskell world this translates to the fact that some multiparameters typeclasses can indifferently (I believe) be encoded with functional dependencies, or type families. In either case the exact set of tradeoffs is subtle (e.g. type inference).
There isn't one yet. I guess you could hack around it for now with something like #[cfg(not(GL_ES_2), not(GL_ES_3))] fn compile_time() { unreachable!("must define GL_ES_2 or GL_ES_3 to select rendering path") } If neither `GL_ES_2` nor `GL_ES_3` is defined (via a `--cfg ...` flag), then it emits an error like compiletime-error.rs:3:18: 3:75 error: no rules expected the token `"must define GL_ES_2 or GL_ES_3 to select rendering path"` compiletime-error.rs:3 unreachable!("must define GL_ES_2 or GL_ES_3 to select rendering path") ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (Please indent code blocks by four spaces to make it easier to read.)
&gt; This is faster, but means the initial overhead is ~4-8K. If I'm not mistaken that's the initial stack size in Go as well?
&gt; We'll, Rust currently doesn't support higher-kinded polymorphism. But, I'm in the process of writing up an RFC for it and experimenting with various implementations. &gt; Within Rust, you typically use a lot of Option&lt;T&gt; and Result&lt;T&gt;, and may end up with quite a bit of pattern matching pyramids. This can get quite crazy and messy. &gt; Luckily, Monads are the perfect tool for the job. Haskell uses them extensively for working with their equivalent option type and it reduces the ugliness quite a bit. In the context of managing lots of option and result types (say when parsing a text file), how does a monad approach look like in comparison to chaining option types with .map() or .and_then()? Any code examples?
I guess that would have to look like #[cfg(not(GL_ES_2), not(GL_ES_2))] #[static_assert] static must_define_GL_ES_2_or_GL_ES_3_to_select_rendering_path: bool = false; (The static assert checks happen much latter in the compilation phase than C's `#error`, fwiw. Well after name resolution/type checking, so if anything like that depends on at least one of those `cfg`s being defined to compile successfully, then the asserts will never be hit.)
This would be fairly easy to put in a syntax extension, though they're currently a little awkward to use.
I think parsers would probably be better served with syntax extensions. You can then define your own syntax (to an extent), and also give more meaningful error messages (when defining the grammar).
Is anyone out there chomping at the bit for a systems language in which to write parser combinators? If you can already do it excellently in Haskell, why would anyone bother to do it in Rust? I believe that we need better examples of monad usage in a systems programming context in order to justify HKT. Rust is not a language that values expressiveness for the sake of expressiveness. To wit, from the official Project FAQ (linked in the sidebar): &gt; **What are some non-goals?** &gt; &gt; * To prize expressiveness, minimalism or elegance above other goals. These are desirable but subordinate goals.
I know I'm describing functors. I used sets as an example to keep it simple (even as others pointed out Set really don't tend to be Functors due to equality, or in practise even ord, restrictions on its elements). My operation f:A-&gt;B is just that, a function from A to B. One that we could be interested in lifting to map sets with elements in A. Much like we could do with arrays, trees, or any other more concrete functorial structure. 
There was a [previous conversation](http://www.reddit.com/r/rust/comments/1vutg0/rust_newbie_questions_regarding_types/cex11i6) about enforcing a kind of physical-unit-safety. Someone suggested that HKT would be necessary for a full system.
&gt; What is the "monadic IO system"? Are you referring to the do notation? I don't mean *just* the *do* notation. I mean the idea of using a monad to structure the interaction with the runtime system. Previously, interaction happened with an entirely different model. While it's true that there was experimentation with monads in Haskell before higher-kinded type classes and the Monad type class were introduced, they were somewhat awkward to use. Probably less awkward than in more syntax-heavy languages like Rust, but still more awkward than you'd want for something as integral to the language as the interaction with the runtime. So my emphasis is on *usable*, in the sense of the human interface design of the programming language, rather than the sense of technical implementability. The monadic IO system was an elegant *idea*, but in order to have a *usable* implementation it needed some syntactic sugar. In order to get syntactic sugar, there had to be a uniform interface. In order to get the uniform interface, "constructor classes" were standardized from their home in `hugs` and the Monad type class was created for the *do* notation to de-sugar to. So these things aren't bound together out of *necessity*, but they ended up coming around together due to *pragmatism*. And because that pragmatic hurdle was jumped, using monads in Haskell became much easier and more natural. And as a result of that, people found a lot more uses for monadic structure than they might have otherwise, and now we get to have these interesting discussions in the subreddits for other languages where adding nicer pragmatics for using monads is debated. :)
So did this come to pass as described, with respect to the hot-reloading boundary now being the process-boundary? That is, if one wanted to create something like an application server where one could un/re-deploy something like servlets, would one have to go through std::io::process to kill &amp; create child processes?
&gt; I believe that we need better examples of monad usage in a systems programming context in order to justify HKT. May be nitpicking here but, you don't need examples of monad usage at all to justify HKT. What you need are examples of HTK's to justify HTK's. I also don't see why you'd need them to be specific to systems programming as a motivation. Most traits you have in Rust aren't specific to systems programming. Traits provide you with abstractions that allow you to produce code that is generic and in some instances, much easier to understand. That is true in the context of both, types and type constructors. If you want a generic map operation, that works on arrays, trees, graphs, and any other polymorphic structure capable of holding (or generating) elements of any type, you'd need something like: fn map&lt;A, B, F: Functor&gt;(xs: F&lt;A&gt;, f: |A| -&gt; B)-&gt; F&lt;B&gt; Now you can either have a Functor trait on type constructors, providing you with nice abstraction and overloading, or you can implement specific map-like functions for each use case. Not unlike what happens with traits of types. Looking at how simple Functor is, and despite not being specific in any way to systems programming, I think we all are pretty certain it'd be used. After all it's such a basic abstraction, it's just about mapping structures. Likewise I don't see the situation with Monad and other such classes as any different. A monad is in particular a functor that can be flattened. So consider e.g. an abstract syntax, tree polymorphic over the type of its variables. You could use `map` to, for instance, rename the variables. But to perform substitution, you need to map them to other trees and then flatten them (so you don't have a tree of trees). That's a monad. Instances of these abstractions appear everywhere. Applications in the context of systems programming is merely a chicken/egg problem. Once people know the abstraction they'll start identifying it in the structures they implement. 
I'm not arguing against the expressiveness or usefulness of HKT in general, not at all. What I need is assurance that they can be implemented in Rust specifically, given our specific design constraints and bare-metal zero-cost-abstractions, in a way that does not implicitly encourage people to adopt programming practices that will compromise performance or memory efficiency. I don't trust myself enough to know if this is possible, and thus these are the concerns that I would like someone to address. It's much the same concern as the argument over guaranteed tail-call elimination. We'd all love to have it, but design constraints mean that it just isn't feasible in the context of Rust.
I can't answer your question specifically, but Rust has changed so much since then (almost *four years ago*, wow!) that by default I doubt that anything said there has any bearing on modern Rust.
Right, I just wanted to clarify that the idea of 'mapping' was something that applied to all 'containerish' things, as well as some things that aren't usually thought of as 'containerish' like *Option*. And higher-kinded traits let you capture that commonality in the type system, which turns out to actually be handy. Again, I'm not trying to disagree or criticize; just saw this as a good jumping-off point for answering questions that people seem to be having. It's not always obvious that there's some commonality between the relationship between 'A' and 'List of A' compared to the relationship between 'B' and 'Option of B'. You can view both as *type transformations* that add some new structure to a base type; multiplicity/grouping in the case of List and possible absence in the case of Option. And the power of *map* is that you don't have to write new functions to operate on these new, more structured types; you just *lift* functions from the base type via its *map* operation. This is a different and more general way of looking at *map* than how you might see it if you were introduced to it simply as a List or Set operation. If you can define a general *lifting* operator like *map*, then when you come across a "structure-enhanced" type of any sort, you don't have to consult its documentation to figure out how to apply a function of the base values to it. You just reach for the general lifting operator! It doesn't matter what the flavor of the extra structure is; those details are abstracted away in the higher-kinded trait implementation for the specific type constructor you're using at that point. This provides a nice separation of concerns, makes the code more expressive by re-using a common general term rather than a specific one, and makes it easier to refactor if you need to change the specific type constructor that's being used. *If* you can make it work cleanly in Rust's type system, it'd be well worth adding.
If you want to write a server you want it to be fast, but also to parse requests correctly and in a bug-free manner.
This is a pretty good writeup (looking forward to that RFC), but there's one fairly important error: Monoid does not require HKTs. In fact only concrete types (i.e. *) can be instances of Monoid because an instance of Monoid must provide an identity element. You can actually write Monoid today in Rust (I wrote it during my internship last summer just to be sure).
I'm amused that this is the first response, given that chris-morgan, the author of Rust's best HTTP library, and who definitely cares about parsing requests quickly and correctly, has been unjustly downvoted to oblivion in this thread.
Do you know if there's anything written down about how to use syntax extensions aside from [#11151](https://github.com/mozilla/rust/pull/11151)? Or maybe there are some good examples in the code somewhere?
That's more-or-less it AFAIK. You could look at the fourcc and hexfloat crates in the main repo for simple examples.
Simple example is `sequence` from Haskell: sequence :: Monad m =&gt; [m a] -&gt; m [a] `m` is higher-kinded and it is polymorphic, and the above `sequence` function works with any type constructor that implements `Monad` (and there are lots of these in Haskell).
&gt; Is anyone out there chomping at the bit for a systems language in which to write parser combinators? There's already a systems language with good support for parser combinators: http://www.boost.org/doc/libs/1_55_0/libs/spirit/doc/html/index.html
As much as I love Boost.Spirit (and Boost.Karma) I certainly hope that should Rust get to it... it will compile **faster**.
You should probably just use something like a PEG via a syntax extension then. Parser combinators in a highly optimized systems language are bound to compile slowly because they're creating a huge number of layers the compiler needs to strip away. It's also a lot harder to debug parser combinators since they're missing the same debug tools you have available with a parser generator.
I would point out `rebind` in C++ allocators: template &lt;typename T&gt; struct SomeAllocator { template &lt;typename U&gt; struct rebind { typedef SomeAllocator&lt;U&gt; type; }; }; With parametric associated types, you could achieve this data-structure thing.
I just wanted to say I've really enjoyed reading your replies in this thread. I've never written a lick of Haskell, and I suppose you could say I'm afraid of it. I'm afraid of it mainly because I've never really tried to understand it. For e.g: the only thing I know about monads is that they're [supposedly really hard to learn.](http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/) But this reply, plus one in another comment thread: &gt; Interesting question. &gt; .map() is the Functor fmap &gt; .and_then() is the monadic bind &gt; .and() is the Applicative (*&gt;) &gt; .or() is the Alternative (&lt;|&gt;) ... well that just makes monads seem downright approachable. I mean -- we just accidentally invented functors in less than 10 lines of code. And that's 10 lines of clean, readable, and easily digested Rust code. --- I realize I've only gleaned a superficial understanding of what monads can do and why they're desirable. However you really have a knack for making these "academic" ( / "scary") topics seem approachable, so thanks for that. 
The problem with HKT in your example, is that it does not play well with a *variable* number of parameters: I want `Traversable` both for `Map&lt;T,V&gt;` and `Map&lt;K,T&gt;`! In C++, you often meet the problem with people (ab-)using the template template parameters: I have a template taking 3 parameters, its template template assume only 2 were needed, now I need an adapter...
Sure. We would need to inline the uses of `Monad` (`Maybe` for the use of `MonadComprehensions` and `IO` for `mapM_`) of course, but the `Monoid` aspect is possible.
One could argue there are many logical next steps to choose from, and that for complexity budget reason not all of them will be chosen from.
Well to solve fizzbuzz we just need Maybe and Monoid, right? How would it look like in Rust? I asked on IRC a few weeks ago and I got a C++ port that looked kind of ugly. I don't have enough experience with Rust to write it myself.
I've seen and heared "contains" among mathematicians to mean both direct (single) membership and "is a subset of". Notably though, whenever I've seen an author make a distinction between membership and the subset relationship, they used "contains" for single membership and "includes" for the subset relationship. The MIT mathematician and logician George Boolos was one of those (and adamant about it).
I think this is the most important point. We need someone with the chops to actually put together a plan of how it'd be made, and only THEN can we have this discussion.
&gt; Dynamically-linked libraries still present a barrier to AoT optimization, though, unless there's futher LLVM magic to solve that problem. There is none, indeed, because no compilation occurs at load-time.
I am really happy about this release! The split of `libextra` is a great start at creating a modular distribution, and I made my first contribution! I can not wait to see where Cargo goes, and the new RFC process is a great place to see possible language changes at a glance. Glad that this language is progressing so fast, and that I could be a part of it!
Highlights (no particular order, partially reflects my personal interests): - Managed (GC-ed) pointers are moved to the standard library. There is no special syntax (formerly `@`) nor special header (for vectors containing managed pointers) required. - Special treatments on `str` and `[T]` vectors are being generalized ("Dynamically Sized Types"). - Lifetimes of temporary expressions have changed in somewhat more intuitive way. - Syntax extensions are now exportable, documentable and much more flexible ("procedural macro"). - Language simplifications: `do` syntactic sugar is removed, `priv` is now default, no trait bounds by default (e.g. formerly `:Send` was implied for `~Trait`). - The smart pointer usage has been improved with new `Deref` and `DerefMut` traits. - There are now many auxiliary standard libraries instead of a single `extra` library. - Usual library fixes, redesigns and influx took place. Most prominent change is an introduction of growable vector type, `Vec&lt;T&gt;`, which subsumes the original `~[T]` type. - Rustpkg is gone, long live Cargo! Rustc has also combined many related options into umbrella flags (e.g. `-C`). As prior point releases of Rust did, Rust 0.10 does not represent a significant milestone. It is always recommended to use the most recent development version (master) of Rust, but it had been a great undertaking to compile Rust from scratch. From 0.10 onwards, however, there are official nightly versions [1] and brave souls can play with master more conveniently now. (They are currently not signed yet, so take that in mind.) [1] https://mail.mozilla.org/pipermail/rust-dev/2014-March/009223.html
How did 0.9 become 0.1? :\ (sorry newb)
You could argue that the "." operator has been overloaded. And your question is testament to why Rust shouldn't support it :)
Parametric associated types are likely to take a similar or greater amount of effort to implement, since they are just support for having trait items with an higher-kinded type. 
The [iter mod](http://static.rust-lang.org/doc/0.10/std/iter/index.html) provides something similar to LINQ, but implemented as a standard module and not requiring any Special Compiler Magic.
It's not 0 point 1, it's 0 point ten. It's not a decimal number; the dot is a separator. It could just as easily be "0:10" or something.
Versioning doesn't work that way. Gnome, for example is on version 3.12 - which proceeded from 3.8, 3.9, 3.10 and 3.11. Pretend it's version 0.A instead, and that it's in whatever base is most convenient.
I'm a bit cynical, and I tend to view things on what they can't do rather than what they can. Or what it does wrong rather than what it does right. The thing that most excites me about Rust is that it does very little wrong. Fast. Static typing. No mandatory garbage collector. No "special" types. Functional enough. What can I moan about? This is a big deal (for me). When investing time and effort into becoming proficient in a language, I want the comfort that it'll be suitable for most problems that are thrown at it. And I can't think of many places it wouldn't be a great fit. I know there's the whole "use the right tool" etc., but who's got the time and brain power to become experts in several languages? Computers hate context switches. I hate context switches. Rust looks like I can use it for almost anything.
http://semver.org/
Yeah, I just tested it, it compiles! I made a repository: https://bitbucket.org/iopq/fizzbuzz-in-rust one issue: if I don't give a parameter the program crashes, how can I put the array access into a maybe and have 100 as the default value?
Is the compiler able to do fusion? Say, if you map and then map again, that it can bring these together into the same loop?
Essentially, yes. All iterators are "lazy". Iterators have a next() method, which gets the next item in the iterator. range(0,100000000).take(5) will only get the first 5 elements. This is not any smart-compiler-magic-stuff; this is just how they work. The type of [1,2,3].map(...).map(...) is something like `Map&lt;Map&lt;int&gt;&gt;` and not [int].
Congratulations and thanks for your help :D
I agree with the shell script part. I don't agree with "You're going to go crazy trying to use it for things where you don't particularly care about the last few percent of performance."
The closest thing I can think of is: // note the added mut let mut args = std::os::args(); match args.remove(1).map_or(Some(100), |s| from_str(s)) { .... //same thing as before `remove` removes an element of a ~[] at a provided index and returns `Option` (i.e. `None` if the index is out of bounds). `map_or` maps a function into an `Option` if it is `Some` and returns a default value if it is `None`. Here our default value is `Some(100)` because the match excepts an `Option`. Also note that we no longer need to specify the type for `from_str` because inference can figure it out for us.
Brendan's resignation as CEO will have no effect on Rust or Servo. Many people at all levels of the organization are huge supporters of Rust, Servo, and Mozilla Research in general.
Memory safety is a pain, and the borrow checker is merciless.
be hasn't done any work on Rust since before 0.1 iirc.
Well, if you don't want to deal with that you could just Gc&lt;&gt; or Rc&lt;&gt; everything, though I guess the syntax would get a little awkward.
The current `~[T]` is bad because it's [slow (among other problems)](https://github.com/mozilla/rust/issues/8981). The DST `~[T]` is considered "bad" for some things because converting from a resizable `Vec&lt;T&gt;` to a non-resizable `~[T]` throws away information/has other drawbacks. This means that user that wishes to resize a a `~[T]` returned from a library function is forced to convert back to a `Vec`, and it's not possible to recover the information that was lost, making things slower. (And it can be slower even without the user doing the conversion, since converting from `Vec&lt;T&gt;` to `~[T]` is unlikely to be a noop too, it requires a `shrink_to_fit` to ensure that sized deallocation works (i.e. passing the size of the allocation to `free`, which can make `free`ing faster).)
Not all bad. You know you typed what you thought. Other languages can't guarantee that.
Sorry for the newbish question but how exactly does this nightly installer work? I got the tar and it seems to have the rustc binary built already, do I just run the install.sh script? Just recently built from master trying to get this installed a couple days ago, so if this nightly is just binaries that would be much easier for my setup.
It integrates well with most scripting languages. I've seen a few people propose creating a scripting language as a syntax extension.
The building blocks are there. That type of thing is what syntax extensions are for. There will probably be a LINQ clone eventually. 
We are very fortunate that he had the foresight to cherry-pick Rust as a project that deserved to have resources invested in it, but as far as I know he has been very hands off ever since. And even in the exceedingly unlikely event that Mozilla pulled out of the project now, I believe that the community would be strong enough to stand on its own two feet.
On my Arch linux box I just had to run `sudo ./install.sh` I don't know if there are any flags, but by default it put libraries in `/usr/local/lib/rustlib` and binaries in `/usr/local/bin/` (If you have a copy from master already built &amp; installed you can run `make uninstall` to clean it up.)
An iterator like `range(0, 10).cycle()` is also simply performing a copy of of the iterator state (a few integers for `range`, a pair of pointers for vector iterators) rather than doing any memory allocation. The iterator module never allocates or fails.
it works, but I'd prefer if I didn't have to modify the args directly, but just do a safe array index access
Arch has 0.10 in the official repositories and a nightly build repository on pkgbuild.com. There's not much point in using the installers, since it won't update automatically and lacks the same integration (mime database, editor support, etc.).
&gt; They are currently not signed yet, so take that in mind. The snapshots are not signed either, so building manually is no safer.
&gt; It is also closer to what C programmer expect. I don't really see why. C programmers understand why a capacity field is necessary for amortized O(1) append, and `std::vector&lt;T&gt;` in C++ uses the same representation as `Vec&lt;T&gt;`.
Ah, I think that's perfect. All I need is a single string literal, so I think I can work off that one.
Rust isn't the only language with that property e.g. Haskell, but it doesn't have the same low-level guarantees. (Things in Haskell generally compose better, due to having a GC etc. e.g. closures work nicely.) I.e. One might say: use Rust if you need correctness *and* the last few percent of performance, Haskell if you only need correctness.
Couldn't you, if you really wanted to, write a macro to fix the syntax awkwardness? *If you really wanted to.*
Yeah, quite likely, though it would be a bit tricky to do in a way which doesn't add its own syntactic noise.
Yeah, that bothered me too. Easiest way to do it is probably: let args = std::os::args(); let n = if args.len() &gt;= 2 { args[1] } else { ~"100" }; match from_str::&lt;int&gt;(n) { ... // same as before } You can inline `n` if you like.
True. I think in general not fighting the compiler is a good idea.
you don't want to write a parser, you want to write a library that makes writing parsers a trivial process.
I meant it is closer to T[] in C, which is kind of the standard sequence type. I guess vector or other custom objects get used more in large projects though.
Pretty much, yes.
OK, I would prefer to pattern-match on the args let n = match args { [_, n] =&gt; n, _ =&gt; ~"100" }; match from_str::&lt;int&gt;(n) { but that's a little silly since we have to specify `~"100"` and then we convert it to int Edit: strcat helped me in IRC so I changed it to let n = match args { [_, n] =&gt; from_str::&lt;int&gt;(n).expect("I need a real number"), _ =&gt; 100 }; for i in std::iter::range_inclusive(1, n) { println!("{}", fizzbuzz(i)); } 
It's a bad thing when you just want to bang out a program quickly, but don't care particularly for performance.
See https://github.com/carlhuda/cargo. It isn't a build system.
Higher kinded constructors is I think "the right thing." In addition to inference, it makes pattern matching on higher kinds decidable type family Strip (x :: *) :: * type instance Strip (f x) = x has worked in some versions of GHC. My reasoning for saying it is the "right thing" though is different: lambda isn't the ultimate. Making type *operators* magical and built in seems like a bad idea for the same reason the function type constructor shouldn't be magical and built in. After all, functions are just one example of the more general notion of ``codata with shifts." Even more radically: one could imagine a language without any sort of codata since it is the *shift* part not the *codata* part that gives rise to computation. Why have implication when you can have subtraction? But this is way too theory-ish and cutting edge for r/rust.
Well, in theory, source tarball is signed, which includes src/snapshots.txt, which includes SHA1 hash of snapshots, so there *is* a trust path to snapshots.
In a word: No. Mozilla is fully committed to Rust, because it's just too damn good. ;)
I like Haskell, but integrating with C libraries is much nicer in Rust.
`jmpStack` should probably be changed from `~[uint]` to `Vec&lt;uint&gt;`.
Rather, that dependencies *need not* be shared. If we both declare on crate `foo`, version 1.0 or greater, then we will share the crate. If I depend on version 1.1 or greater and you depend on version 1.2 or earlier, we will share the crate, probably at 1.2. If I depend on version 1.3 or greater and you depend on exactly version 1.2, we will use different versions of the crate. (Of course, you can only have interoperability between these two crates passing objects from the `foo` crate if they are using the same version of `foo`. But often that interoperability is not necessary, if the dependency is purely internal.)
It's… *slowly* starting to penetrate into my mind.
You can't put macro invocations into just any syntactic position, so I think that is gonna be pretty limited until you rewire the whole grammar on top of a macro.
Yeah, that's what I figured.
iIm not on a machine with rust installed, but what would happen if you advanced the tape to 1025, and then incremented that cell? Maybe another bounds check is in order? Also i think the Wikipedia page on BF says that the regular tape size is around 30,000 Looks great though, I usually write simple BF interpreters to get a hold of a new language, but i guess i never did that with Rust...
Rust does bounds checks on all indexing.
Wouldn't it still crash though? He does bounds checking for &lt; 0, but not &gt;1024, AFAICT
He was the CTO before that.
That doesn't compile I messed around with it and for some reason I needed to do [_, ref n, ..rest] =&gt; from_str::&lt;int&gt;(*n).expect("I need a real number"), EDIT: I changed it to [_, n, ..] =&gt; from_str::&lt;int&gt;(n).expect("I need a real number"), and it worked
It would cause task failure, which would exit the application gracefully. Depends on what you call a "crash"
Oh, I forgot the `,` between `n` and `..rest`. Whoops. So has Rust replicated the Haskell FizzBuzz to your satisfaction?
Some accepted pull requests: - https://github.com/sebcrozet/nphysics/pull/7 - https://github.com/erickt/rust-zmq/pull/33
I think your idea is sound, modulo casts in unsafe code. After thinking about it some more, I think there is a way to encode some of the flow-sensitive aliasing into the LLVM TBAA system (or a slight variant of it). The idea is to break the entire program (all functions in an LLVM translation unit) into 'lifetime scopes', which can be nested inside of a single function. Then you abstract each group of nonaliasing sets of pointers as (S, i) where S is a scope identifier and i is an index into that region. Values (S1, i) and (S2, j) would be defined to alias when (S1 is not an ancestor of S2 and S2 is not an ancestor of S1) or i == j I think you could encode the individual scopes as TBAA trees and the IDs as leaves of these trees, but this means you'd be wasting your entire TBAA forest on this notion of aliasing, and wouldn't be able to also rely on types. You'd really want to be able to specify a pair of TBAA tags for the same mem ops, one for the traditional type-based aliasing and another for the region-based aliasing. Given two mem ops with tags (S1, T1) and (S2, T2) check whether S1 aliases S2 and T1 aliases T2 to determine whether the two tags alias. This means that two loads of different types would never alias, even across regions (and inlining, although see Edit2 at the end of the comment for a potential caveat here). I brought this up with some of the LLVM people at work, and they pointed me towards this previous proposal for handling the inlining of 'restrict' that's pretty much the same idea: http://lists.cs.uiuc.edu/pipermail/llvmdev/2012-December/056586.html I'm still not sure how to safely handle uses of unsafe casts when using type-based aliasing. There are some obvious things you can do with whole program compilation (e.g. merge the TBAA tags of two types linked by a transmute), but it's trickier when calling a function in another translation unit that does an unsafe cast and returns the result. It would be unfortunate to only be able to do sound optimizations on a whole program, or rely on the C/C++ "I hope the compiler isn't smart enough to see through this cast" style of unsafe programming. Edit: This isn't as powerful as something based on memory SSA could be, since it can't refine alias information after inlining; the aliases are fixed upon IR construction. But it does avoid the soundness problems while ensuring that you don't lose alias information in the inlined code after inlining. Edit2: I think you actually need to clone the scope trees upon inlining, so that distinct inlined copies of the same function don't have the same scope tree.
No, many more people would have to be bullied into resigning for their "incorrect" personal beliefs before that happens.
I appreciate this effort, though there are some issues: * The priv removal cannot be accomplished with a simple find-and-replace, since the fixer would have to *add* `pub` as needed. (This is slightly more complicated that `pub` would be only added to the public structs.) In my experience it was much easier to search for `struct` (not `priv`) and update them accordingly. * `std::comm::channel` ([0.10 doc](http://static.rust-lang.org/doc/0.10/std/comm/fn.channel.html)) is not a direct replacement for `std::comm::Chan::new` ([0.9 doc](http://static.rust-lang.org/doc/0.9/std/comm/struct.Chan.html#method.new)). The return type is swapped from each other. * You actually don't need `use std::vec::Vec;`, it is in the prelude. (Maybe the replacement function can check for `#!?\[no_implicit_prelude\]` and remove or update the item accordingly.)
I toyed briefly with the idea of this "TBAA forest" in the first revision of this, but found it unworkable because of the exact reason you point out -- you don't have types anymore! I stopped that line of thought because actual TBAA is going to be more useful I think. Thanks for the detailed response! I'll come back to some of the finer points if/when I get to the point of implementing more advanced alias analysis.
Yeah, you really need to do both for it to be worth it. I think other users of LLVM are pretty interested in making 'restrict' work with inlining, so hopefully a general mechanism for doing both kinds of alias tagging will go into LLVM soon. I'm particularly interested on any ideas for making this work with uses of unsafe casts (beyond relying on LLVM's backup local must-alias check to ensure you're not shooting yourself in the foot). If you can restrict the effects of an unsafe cast to be purely local, which would mean not returning the result or storing it into memory, then I think you could do a simple intraprocedural analysis before generating IR to be pessimistic in the presence of unsafe casts. If you want to return the value or store it, then it's going to introduce a more serious soundness problem. The case of returning a value might be able to be solved with an annotation, but not the case where it escapes into memory.
I hope you mean "It's not very nice to write a shell script in", yet, since the D programming language shows that a systems programming language can be used to write very nice shell scripts on it.
If we were programming in linear logic, I would sure agree that arrow isn't the ultimate. But you can't deny that lambda-abstraction maps well with the central programming activity of factoring out code redundancies. I don't know whether we'll keep functional abstraction as a primitive long-term, and you may very well be right that a more general system (eg. lambda-mu-mutilda / System L) could take over the programming fundamentals. But to this day and age, the term language is heavily based on lambdas, and I think it's very reasonable to ask for the type layer to have the same structure (at least if you ignore the technical problems with higher-order unification). For you pattern matching example, (I'm not expert on higher-order unification but) couldn't you allow the full power of type-level functions, and yet allow definitions which avoid non-linear variable occurences, that is restricted to the subset of higher-order patterns? That would precisely allow what you describe. I thought above a relatively convincing example of desirable higher-order lambda-abstraction in practical programming (summary: `\f (x,y) -&gt; (f x, f y)`); I'll try to write a blog post about that.
Not sure if you would class [this as an improvement](https://gist.github.com/bjz/9970322). It would be nice to see if iterators could be used for this.
I am not the author. I agree this is not optimal, but a first step in an interesting direction. This might evolve into something more robust, more useful ... maybe using librustc. It could be a generic lint/refactoring tool.
It expands to a `Vec::new()`. Type inference does the rest. Edit: you could actually remove the trailing `u`s for `pointer` and `instr` as well. Not for `memory` though.
Very true.
Whoa, so the rust compiler is smart enough to look ahead in the code and infer the type based on methods called later on?
Yes. See https://en.wikipedia.org/wiki/Hindley-Milner
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Hindley-Milner**](http://en.wikipedia.org/wiki/Hindley-Milner): [](#sfw) --- &gt; &gt;In [type theory](http://en.wikipedia.org/wiki/Type_theory) and [functional programming](http://en.wikipedia.org/wiki/Functional_programming), __Hindley–Milner__ (__HM__) (also known as __Damas–Milner__ or __Damas–Hindley–Milner__) is a classical [type system](http://en.wikipedia.org/wiki/Type_system) for the [lambda calculus](http://en.wikipedia.org/wiki/Lambda_calculus) with [parametric polymorphism](http://en.wikipedia.org/wiki/Parametric_polymorphism), first described by [J. Roger Hindley](http://en.wikipedia.org/wiki/J._Roger_Hindley) and later rediscovered by [Robin Milner](http://en.wikipedia.org/wiki/Robin_Milner). Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis. &gt;Among HM's more notable properties is completeness and its ability to deduce the [most general type](http://en.wikipedia.org/wiki/Principal_type) of a given program without the need of any [type annotations](http://en.wikipedia.org/wiki/Type_annotation) or other hints supplied by the programmer. __Algorithm W__ is a fast algorithm, performing [type inference](http://en.wikipedia.org/wiki/Type_inference) in almost [linear time](http://en.wikipedia.org/wiki/Linear_time) with respect to the size of the source, making it practically usable to type large programs. HM is preferably used for [functional languages](http://en.wikipedia.org/wiki/Functional_language). It was first implemented as part of the type system of the programming language [ML](http://en.wikipedia.org/wiki/ML_(programming_language\)). Since then, HM has been extended in various ways, most notably by [constrained types](http://en.wikipedia.org/wiki/Bounded_types) as used in [Haskell](http://en.wikipedia.org/wiki/Haskell_(programming_language\)). &gt; --- ^Interesting: [^Hindley–Milner ^type ^system](http://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system) ^| [^Type ^inference](http://en.wikipedia.org/wiki/Type_inference) ^| [^Polymorphic ^recursion](http://en.wikipedia.org/wiki/Polymorphic_recursion) ^| [^Simply ^typed ^lambda ^calculus](http://en.wikipedia.org/wiki/Simply_typed_lambda_calculus) ^| [^Strong ^and ^weak ^typing](http://en.wikipedia.org/wiki/Strong_and_weak_typing) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cgjs6nf) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cgjs6nf)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
11 days.
That doesn't really say much about its suitability for shell scripts, though.
doesn't work if I put in ..rest, gives me some kind of a borrow check issue (unless I do ref n) works if I just do .. without the ref n but yeah, I'm satisfied How would I rewrite it if I wanted to always default to 100 even if the first argument is not an int? I mean how does "ignore errors, assume 100 in the end" work in Rust without pattern matching twice?
Ah yes I forgot to include how to deal with unsafe casts. It's quite simple actually! Casting to `&amp;` or `&amp;mut` is always unsafe, and is actually undefined behavior right now, with the one exception of the `Unsafe&lt;T&gt;` type, which doesn't matter for TBAA, since it doesn't actually cross the type boundary. One possible workaround is keeping a "dirty map", which would be a simple effect system. When a pointer is returned by a function marked `#[no_tbaa]` we can add the ValueRef to a map that says "loads or stores with this get no tbaa". And then any other operations would first check if any of the ValueRefs are dirty and propagate that forward. This would only work intraprocedurally, but could help some usecases I think. I'm willing to strike it all down as "undefined behavior" though. If you want to break type safety, just use raw pointers.
I am the author of rustfix. I know it is not perfect, and it was never meant to be. As an example, adding pub to structs would have needed something more complex than a search and replace, more like a real parser. The goal is to make it easier to port code, because if you forget to update for a few weeks, the number of compilation errors can be daunting. For the cases that do not compile, I now provide the reference (commit number or pull request in mozilla/rust) that triggered the fix. In the future, it may be a good idea to do it with a real parser, but a parser that handles multiple different syntaxes may be hard to do. If you have other ideas about how to improve it, I am listening, I just began developing it :) 
I was thinking of casts between struct types, e.g. a transmute from &amp;PhantomType&lt;A&gt; to &amp;PhantomType&lt;B&gt;. This case could be solved by adding a special rule that allows instantiations of a phantom type to alias, but are there any other important cases?
Yes, it's constraint solving program that is run at compile time. If a solution is found then the type checking succeeds, if not a compile time error is triggered.
Hm, I didn't consider phantom types. Does this actually need a transmute? Servo also does some unholy stuff with using transmutes to achieve inheritance, though I think I could work around that by just not using tbaa for trait objects. Since their actual type is erased, it's hard to define the exact rules to use.
Ah right, I remember implementing `Succ` :) Gives quite a hard time to the compiler though when you implement exponentiation though.
Well uniform function call syntax (UFCS) + a nice range-based std library is what really do allow you to write nice shell scripts in D (GC plays a role here, but the role is not major). There are a couple of proposals for UFCS in Rust, so I think that with UFCS and the right libraries Rust could be nice for writing shell scripts too.
I'll believe it when I see it. And I would be very, *very* happy to see it :D
Very good point. I think in the end getting consistent is the next option though? Choose one way?
IMO if there is something in Rust preventing it to be nicely used for shell scripting then it is fundamentally flawed, since the only thing that it is actually required are powerful-enough abstraction capabilities and syntax sugar (which is what UFCS is). The D compiler also allows you to write "#!/usr/bin/env rdmd" on top of a file, such that the script code is automatically compiled when you run it as a script, but this is just more sugar.
What is your output? String? If so, I think it would be easier to just make an `Vec&lt;~str&gt;` that has size `Vec1.length * Vec2.length*...*VecX.length` and find a way to add all those in a single pass.
It should be a generic type. Anyway, I'm mainly interested in seeing how an idiomatic solution using iterators looks like to learn something new.
You still need to be something specific in mind. Tuple? Vector? ~~~If you want something similar to your solution I think Macros would be good start.~~~ Have you considered writing some kind of MultiplyMap, that returns an iterator that permutes over it? Like a recursive permutation. Basically you make a trait that works over `[T].multiply_map([T]) ` (i.e. `multi_map(&amp;self, add:[T])` ) and it would work like this. Given a `[1,2,3]` and `[4,5]` it returns `[14, 24, 34, 15, 25, 35]`. Then your code would look like: use std::iter::Repeat; let vec = vec!(vec!('A','E'),vec!('F','H'),vec!('2','4')); let mut perm_vec = vec.get(0); for x in (1, vec.lenght()) let p = perm_vec.multiply_map(vec.get(x); perm_vec = p.collect(); } Can't help you much on the whole idiomatic using iterator/mapping. EDIT: This sounds fun little exercise, I'm gonna try writting it when I get home.
Python and regexp, because that was a goo way to do a quick and easy experiment, and the Git and Github libraries worked reasonably well (I tried Ruby first but was disappointed by the Git libraries). libsyntax is cool, but I do not know how to use it yet, and I'm worried that it would not recognize old code patterns, since it represent a way to parse at onepoint in time. If you have a good introduction to libsyntax somewhere, I could try to rewrite rustfix in Rust (and keep the auto-PR feature in a separate script). A linter would definitely be useful too!
Don't know why you got downvoted, this was a question that a lot of people had, and I'm sure they are happy that they didn't have to ask it. Thanks you :)
Are you asking for an explanation of how we detect syntax errors while parsing, or is there something more?
Well sorry, that was a good question, and I'm glad you asked it.
T[] is just another way of saying "pointer" in C. It does not include a length or anything. It's equivalent to write `char *arg` and `char arg[]` for a function parameter as far as I know, and it's not usable elsewhere (except in C99 at the end of a struct) due to a missing size.
Basically how does Rust detect and recover from error while parsing. It seem to be able to gulp several errors before coming to a halt. I don't know which parsing type does Rust uses, but I've heard that with (and I'll probably misinform you about this) LALR parser can be 'trained' to spot specific errors, by giving erroneous text and looking in which state the parser ends up, and then just emitting appropriate error for that state. 
What's wrong with the obvious way of doing this in any language? That is, either using recursion, or keeping an array of iterators and "adding 1" to it as if it was a number every time, or in the specific case of size 2 vectors using an actual integer and indexing them according to the value of each bit. 
Regardless of the specific politics in play here, a CEO who cannot weather a firestorm of criticism cannot function effectively. Eich's attempts to manage the situation had been consistently, laughably inept. It would have taken only a trivial gesture to address the situation, but his stubborn refusal to employ the necessary diplomacy was what ultimately drove him out. Eich refused to compromise his personal beliefs in order to mollify his critics. That may be admirable in certain contexts, but not when you are the public face of an organization. If he wasn't able to put the well-being of Mozilla above his own irrelevant convictions, then we're better off without him as CEO.
Monads aren't hard to learn at all. They're so simple that people think there must be more to the topic when there isn't.
Leaders who fail to inspire faith in their followers compromise their ability to effectively lead. It's illogical to expect employees to devote themselves to an executive who is attempting to deny them equal rights under the law. Whether or not that executive pinky-swears that he won't disrespect them during work hours is irrelevant to the issue at hand. In the role of CEO specifically, this concern is especially acute.
People weren't scared off by all the bullshit that's in the C++ kitchen sink. The bracket&lt;T&gt; notation is an ugly C++ism that Java and Rust have fallen into. Borrowing from Haskell instead would give you cleaner syntax and pattern matching.
Rust has shebang support, just no actual program to put in there. There is an issue open to add support for something like that to the Rust compiler ( https://github.com/mozilla/rust/issues/9826 ) but in principle it need not be in the same program, or even be official (although being bundled means it'll actually be useful).
It's possible to make a nice list comprehension macro with similar syntax, wrapping `flat_map`, `map` and `filter`.
Rust's `enum` is not meant to be used in the same places as a C `enum`. The chosen naming is unfortunate, because they're really tagged unions. It would be undefined behaviour to store a value not directly listed in the `enum`, since this would break the exhaustiveness of `match` and also the range assertions we put on loads of the discriminant. If you want a flag type you can XOR together, what you really want is a `struct` with the various bitwise operators overloaded. There's an initial attempt at making a generic type for this with `EnumSet` in the Rust standard libraries. It might be better to simply make a macro for defining flag types with the relevant operators though.
I've wanted to mention this before, but Rust actually has *three* namespaces, not two: values, types and modules, e.g.: pub type Foo = &amp;'static int; pub static Foo: Foo = &amp;0; pub mod Foo {fn new() -&gt; super::Foo {super::Foo}}
A **tagged union**! In that context it makes a lot more sense, and the name **enum** is indeed quite ill-chosen. Apart from the C-legacy, doesn't enum = enumeration mean that all contents are of the same type, or if not, that each is given a subsequent number and that this numbering *is* the enumeration?
Rust uses a hand-written recursive descent parser.
There's an RFC to change this (https://github.com/rust-lang/rfcs/pull/27), though consensus is currently in favor of the status quo for various reasons mentioned in that thread.
So it looks like the compiler's modules are part of the core library? This seems not only great, but crucial for a modern language. If you look at C++, people are really using it not because the language design is the best out there, but because to tool chain is so extremely solid. Really I think Rust will take off when a couple of different pieces are there, none of them having to do with the language design (it looks pretty great to me so far as long as managed pointers can be made to be reference counted to keep binaries small, modular, and deterministic if required). So basically when you can download an installer and the installer installs a whole IDE that has syntax highlighting, type information, code completion etc. It should come with debugging facilities integrated into the GUI, and it should make compiling and integrating with C libraries and even pre-compiled code as easy as possible. Once all that stuff happens I would expect an explosion of use. I don't think it needs a profiler right away since that can be done with timer classes although it is certainly a luxury. I see Nimrod, D, and Rust and really love them all, but now it will come down to the tool chain, and being able to parse the whole language with a canonical library is such a huge part of that. The languages are all designed well enough. I would give up visual studio for new projects if I had an equivalent with the important basics. 
D essentially relies on garbage collection. That's the fundamental difference between Rust and D here. That said, I think it's quite possible to write shell scripts in Rust, once the libraries mature a bit.
&gt; IMO if there is something in Rust preventing it to be nicely used for shell scripting then it is fundamentally flawed Manual memory management isn't a fundamental flaw. It's just a different design choice than D made. D is essentially fully garbage-collected and that limits its usability for some domains, while making it more useful for others (e.g. shell scripting).
Right, but essentially, even LALR parser can be hand written, albeit hard to figure out. I assume it's a LL(1) or LL(k) parser, but that doesn't tell me much how does it discovers and recovers from errors.
You would know better than I would, I guess I was talking about stand alone binaries that have no dependence on a run time or garbage collector. Haskell's hello world compiles to 500KB, D's is 100KB, etc. So with someone knowledgeable about Rust and languages I suppose it is more of a question. (Any insight or is there a link I should read?) The binary size is really the lest important thing, it is more dependencies and using the minimum amount of memory possible I am concerned about. I've been writing a lot of C++11 and haven't needed nor wanted to use shared_ptr so my feeling is that even if reference counting a managed pointer is slower than garbage collection, it actually is a rare occurrence, since that vast majority (and all of my current) memory usage is on the stack or owned with unique_ptr and rvalue semantics, and a const reference or raw pointer here and there for borrowed. Anyway the bigger point I was trying to make, although a bit off topic is that I think at this point there are multiple new languages that have designs that are awesome and I want to use them, I just can't be more productive than C++11 without a solid tool chain. TL;DR; I think language design is flourishing although tool chains will dictate the next popular systems language.
With a recent Haskell build, you only get Hello World down to those sizes if you use dynamic linkage. Haskell's Hello World on my machine is 1.4MB because, afaik, it still statically links by default. With a dynamic compile, the binary is 12K.
I don't know about Rust's case, but typically you define some means to resynchronize (quite often this is "wait until we reach the symbol that ends the expression (i.e., ';')"; so you keep lexing as you hit a ";" token and then you start parsing again).
It's not easy to find a reasonable entry point (there's a web of papers that inter-reference each other, none of them being detailed enough to be easy to understand); you should have a look at [Paul Blain-Levy's work](http://www.cs.bham.ac.uk/~pbl/papers/) on Call-By-Push-Value. My first ideas were to look at Noam Zeilberger's work, or Herbelin, Curien, Munch-Maccagnoni, or the work on interaction by [Ulrich Schöpp](http://www.informatik.uni-trier.de/~ley/pers/hd/s/Sch=ouml=pp:Ulrich).
Yes, that was my point.
Bitfields are the way to do this: http://static.rust-lang.org/doc/master/collections/enum_set/index.html
So, guys what do you say of this way https://gist.github.com/DanielFath/9982209? It's very hacky, couldn't get the lifetime parameters to get along with rest of my stuff, nor could I get the closures to work properly :? 
Yes because hounding the losing side of a political process is the right way to go about things. Allowing people with dissenting/wrong opinions to earn their livelihood (based on their professional capabilities) is the price we pay for freedom. Prop 8 had a lot of donors and supporters. The list is public. Let's go campaign every employer of people on that list till they either change their stance or are forced to resign. I apologize for hijacking this thread - it wasn't the right thing to do. But there are way bigger obstacles in the path of true LGBT equality than Brendan Eich, a man who donated $1000 , 5 years ago to a cause that has been won. On the other hand there are far fewer people who have consistently shown the ability to stand up to the bullying efforts of big companies while ensuring that the interests of the common person are preserved while advancing the web standards. One of his many favorable credentials was having the foresight to pick Rust as a worthy investment.
For phantom types, you could get around the problems by always allowing distinct instantiations of the same phantom type constructor to alias, assuming that the definition of the type would be visible to all translation units relying on type-based aliasing. You could then replace transmute with a 'branding_cast' (or something else along those lines) that only casts between instantiations of the same phantom type constructor, and you could limit the use of this cast to modules where the type is not hidden via priv, etc. Servo's unholy implementation of DOM inheritance would be unsound under type-based aliasing, unless all reads went through unsafe blocks. I guess that's another argument for adding some form of single inheritance. ;-) There's nothing you can do for trait object references based on types, besides marking the memory referenced by the vptr as constant and always safe for speculation. I guess you could have a TBAA tree that encodes which types implement which traits, but that would require whole-program knowledge and might not be that useful in practice. Would raw pointers have any TBAA tags, or would you treat them all as aliasing everything? I feel like it might be useful to have both kinds of raw pointers, those that are trusted to be 'typesafe' (but grossly violating any ownership rules of ~ and &amp;) and those that are not even typesafe and are used to cast between pointers to different types.
I seem to have fixed this, but i'm not entirely sure what actually broke it. The change was one set of tags opened with an iteration lines.get(i), but closed with lines[i] - perhaps these were actually broken, perhaps the compiler was optimizing something out.
This claim of being "flow sensitive" seems quite confusing, since it obviously makes no sense to say that whether two values can alias depends on the point in the program being considered. The rules are simply that an &amp;mut and an &amp;/&amp;mut definitely do not alias if their lifetimes overlap, and no pointer is "borrowed out" for the whole duration of the other pointer lifetime. Plus of course normal TBAA on T for &amp;T and &amp;mut T. 
You could try converting this haskell solution into iterator magic: foo :: [[a]] -&gt; [[a]] foo = foldr (\heads tails -&gt; concat $ map (\head -&gt; map (head:) tails) heads) [[]]
Slide 56 is erroneous: `"Sparticus"` is of type &amp;'static str, so it *is* a pointer; the string is not copied multiple times.
"Unjustly downvoted to oblivion?" Well, my relevant posts both got down to 0, but then headed back up a bit and down a bit, with (currently) a net positive vote. Higher-kinded types, monads and such things are the most controversial things you'll find in the Rust community at present. I'm genuinely not fussed about it; I'm interested to see the lie of the community with regards to them.
We've already got quite a lot of the tools from Python's `itertools` module; we should purloin the rest of it! e.g. this is `itertools.product`. &gt;&gt;&gt; from itertools import product &gt;&gt;&gt; list(itertools.product('AE', 'FH', '24')) [('A', 'F', '2'), ('A', 'F', '4'), ('A', 'H', '2'), ('A', 'H', '4'), ('E', 'F', '2'), ('E', 'F', '4'), ('E', 'H', '2'), ('E', 'H', '4')] &gt;&gt;&gt; [''.join(e) for e in itertools.product('AE', 'FH', '24')] ['AF2', 'AF4', 'AH2', 'AH4', 'EF2', 'EF4', 'EH2', 'EH4']
Because it's written in Ocaml? 
Rust itself already depends on C and C is everywhere. Most devs are already using Git. No need to add another dependency on a whole separate language. In any case Cargo might not even require Git, chances are they will setup some centralized package repository that can send out snapshots. The package maintainers might require Git (but it could be agnostic with hg, svn and so on as options) but end users shouldn't need it. Ocaml on the other hand isn't used for much. The change of someone knowing both Rust and Ocaml isn't great, C on the other hand is popular. Of course you wouldn't need to know C since you aren't likely to have to patch Git in order to improve the Rust package manager. But if the package manager itself is written in another language then only people who know both can work on it. There's also the potential need to provide language bindings to the package manager itself. Not sure if Ocaml allows for C bindings to be created for its libraries. Even if it does going Ocaml-&gt;C-&gt;Rust seems like a pain point. It should also be possible to write a simple version of Git in Rust should it be desired. Just need enough to do a checkout.
Raw pointers have no TBAA information. It might be useful to have a split between those two forms of unsafe pointers, but... I'm going to go out on a limb and say "no" to that complexity.
Examples beyond a grammar would have been nice. An informative braindump, though.
All contents **are** of the same type. enum Foo { X(int), Y(uint), } X and Y are constructors for the type Foo, and are called variants. Rust's enums are sum types.
I'm not too unhappy with Rust having more kinds of data types than C because C doesn't have enough machinery to make unions safe. All typed assembly languages that I've seen, as well as Cyclone, ended up adding existentials of some sort to a C-like type system. Scala had an interesting experiment with unifying classes and enums into case classes. I thought about going this direction a couple of years ago for Rust as well. The ugly part about it is that case classes have a kind of arbitrary restriction that all the subclasses have to be defined in the same module. It just seems better to have all the variants within the same set of `{}` braces.
Yeah, doesn't bother me too much either, like I say, more an itch than a pain. In my RFC for nested structs/enums as a solution to single inheritance, I propose putting everything in one set of `{}`. JDM thought that would be unwieldy for things the size of the DOM. So, I'll probably need to add an extension. Not sure how I feel about it though. From the work week, Niko and I ended up thinking the single module restriction for virtual structs would be useful for downcasting (iirc) and OK design-wise since it highlighted the different use cases of single inheritance (closed) vs traits (open). Do you know what became of the Scala experiment?
slides an commentary right? umm how exactly do I get this to play?
I don't know if that will be good enough long-term. That means that no invariant loads can be hoisted out of a loop that modifies a doubly linked list implemented with raw pointers.
The commentary is given in text below the slides.
`enum` is not like C `enum`s: https://github.com/rust-lang/rfcs/pull/27 I had a PR regarding a `bitflags!` macro (not merged) - that might help you: https://github.com/mozilla/rust/pull/13072/files#r10861951 Just copy-paste it into your lib: https://raw.githubusercontent.com/bjz/rust/bitset/src/libcollections/bitflags.rs I should probably just make it an external library.
I have always been a fan of a unified `data` keyword for declaring algebraic data types. Structural types are super handy for throw-away stuff though, I would hope they were retained.
I think he means that Scala case class/case object can be used for object deconstrution and pattern matching, and it's frequently used to implement enumerations.
Niko has stated that all known soundness issues with closures have been fixed, since https://github.com/mozilla/rust/pull/12158.
It's also spelled "Spartacus"! ;)
rust-iteratorcomprehensions lets you nest, filter and map iterators via macros that implement a comprehension syntax not entirely unlike Python’s list comprehensions. This has been done before in some form or another: * http://www.reddit.com/r/rust/comments/1gag3t/list_comprehensions_in_rust_iterator/ * https://gist.github.com/hanny24/5749688 * http://en.wikipedia.org/wiki/List_comprehension#Rust However, none of these seem to have followed recent development of the language and do not compile on a current compiler. Some questions that came up along the way: * Interaction between (exported) macros and the visibility / path system, i.e. how to refer to items in the iteratorcomprehensions crate from within macros that are used in other crates and the iteratorcomprehensions crate itself, see also [issue #12088](https://github.com/mozilla/rust/issues/12088). * Filtering is supported via an if-phrase that is made optional using the `$(…)*` pattern of `macro_rules!` syntax. It would make sense to also have a `$(…)?` pattern that allows for zero or one matches. * Crates using the comprehension macros are at the moment best compiled with `allow(unused_variable)` set so compilation does not become too noisy. Ideally this attribute should only be applied to the expression generated by the macros but if I understood correctly, attributes can only be applied to items such as functions and higher up. At the moment, the comprehensions only support up to three nested iterators. I plan to increase that number oncy I wrap my head around how to do that with macros. `std::tuple` seems to be a good starting point here. This is my first project in rust and I heartily welcome all feedback.
There's no formal list being maintained, but I'd say that basically all the current syntax is going to be unchanged, except perhaps around closure types.
A really good explanation for newbies, good stuff. It is a bit evasive on the fact that `{:?}` is expensive and should be avoided when possible. It's not that it doesn't cast the variable by doing so but rather a form of raw printing the variable, which makes it expensive (I don't know the details).
`{:?}` should never be used in production code. It should be for debugging only.
heh .. perhaps you could say the naming works to *introduce* C/C++ programmers to this feature. They come looking for constants and discover the powerful tagged union... they might not be looking for it, having been in a culture where classes are seen as the best way to do polymorphism, and switch/union is usually considered primitive/obselete
An enum is a datatype with several variants. I'm not sure what your second question is, but an enum doesn't have functions, only variants. enum Vector { V2 (f32, f32), V3 (f32, f32, f32), V4 (f32, f32, f32, f32) } This code declares an enum of type Vector, with the variants V2, V3, V4. {:?} shows you the variant and its values. I recommend reading the tutorial: http://static.rust-lang.org/doc/master/tutorial.html#enums 
&gt; enum doesn't have functions, only variants. I sort of think of the variant declarations as similar to data constructors, being functions that return the enum type with a value corresponding to the variant.
 enum X { Y(int), Z(char) } This makes `X` an enum type with two variants, `Y` and `Z` that can hold an `int` or a `char` respectively. It *also* creates two functions `Y` and `Z` that take an `int` or a `char` respectively, and return a value of type `X` that is of the variant with the same name as the function, with the function's argument stored within that value. Enums with `struct`-like variants work the same way, but you use names for each field when constructing or matching a value of that enum type, it's just for better readability.
How would the syntax for closure types be changing? I mean, would Rust be adding any new closure types in particular? I was a little disappointed to learn that there was only one owned closure type and that the owned closure `proc` can only be used once. I am wondering if an owned closure that can be used multiple times is possible and if not, what is the reason it isn't possible (or I could just be misunderstanding the whole situation)
I thought anonymous records did exist in Rust and were explicitly removed by design, not "just kind of holes".
An owned closure that can be used multiple times is surely possible, as Rust actually had that in the past. It was removed because multiple closure types were confusing and you can use objects instead of closures anyway.
&gt; where the `u8` being 0 indicating `V2` So we "only" have 256 max possible enum entries then. Guess that should be enough for everyone ;) 
An object is just some state and functions that operate on that state. A closure is also just some state and a function that operates on that state.
In C++ it is possible to use bitpacking to make enums very space efficient. e.g. enum Foo { FOO = 0, BAR = 1, BAZ = 2, }; Foo x:2; Since Foo only needs 2 bits to be represented. Will Rust allow for anything like this?
Pretty horrible example, but might get the idea across: trait Callback { fn call(&amp;self, x: i32, y: i32) -&gt; i32; } struct Adder; impl Callback for Adder { fn call(&amp;self, x: i32, y: i32) -&gt; i32 { x + y } } struct Data { vals: Vec&lt;i32&gt;, f: ~Callback:Send, } fn main() { let (s, r) = channel(); let data = Data { vals: vec!(1, 2, 3, 4), f: ~Adder, }; spawn(proc() { s.send(data.vals.iter().fold(0, |x, y| { data.f.call(x, *y) })) }); println!("{}", r.recv()) }
By default (as far as I'm aware), the discriminant is actually a u32 -- hopefully 4294967296 discriminants are enough for you!
Probably using a macro. (Servo already has a `bitfield!` macro.)
Is `bitfield!` like this? https://github.com/mozilla/rust/pull/13072/
 enum Foo { FOO = 0, BAR = 1, BAZ = 2 } fn main() { println!("{}", std::mem::size_of::&lt;Foo&gt;()) // 1 } Not sure if that is good enough for you. :/
For fun, here's a macro for it: #![feature(macro_rules)] trait Callable&lt;Args, Result&gt; { fn call(&amp;self, args: Args) -&gt; Result; } macro_rules! lambda( (($($x:ident : $T:ty),*) -&gt; $U:ty { $body:expr }) =&gt; ({ struct Object; type Args = ($($T),*,); type Result = $U; impl Callable&lt;Args, Result&gt; for Object { fn call(&amp;self, ($($x),*,): Args) -&gt; Result { $body } } Object }) ) fn main() { let f = lambda!((x: int, y: int) -&gt; int { x + y }); println!("{}", f.call((1, 2))); let succ: ~Callable&lt;(int,), int&gt; = ~lambda!((x: int) -&gt; int { x + 1 }); println!("{}", succ.call((1,))); } It won't do type inference though, and polymorphic unboxed closures aren't supported either.
You two are both wrong :P. The discriminant is chosen with the smallest size that fits all variants, and it can go up to `u64`.
Yes, I actually was testing this. Rust is smart enough to allocate space with minimum requirements. So for example, while your enum gives size as 1, if you start FOO = 1000 or FOO =100000, the size increases
Actually, that let err statement was based off one of the comment here - assign wrong type so that compiler outputs the proper type. My doubt was actually - Some(T) is a function which takes params but keeps that value (like a data structure). How is it being done. 
So it's in fact metadata about the enum itself
Exactly. This is what I was thinking based on how smart rust is to assign memory for an enum. For ex: enum X { E1 = 1.. } will use size of 1 where as enum X{ E1= 1000 } will use size of 2 or more etc as and when required.
Oops, some corrections needed here. Can't believe the audience noticed that Spartacus predates the color chartreuse, and yet failed to spot the spelling error (which got copied and pasted all over the twittersphere) :D
Yeah, was an odd choice using {:?} on the Spartacus demos I know. I actually said it was for debugging in the talk, but I failed to mention it in the commentary. The part about it being expensive is news to me though. I used it here just to show it can be done. I figured the tilde characters in the output made it pretty clear that it's not for normal presentations purposes. Maybe I'll shift the {:?} to a better place in the talk where it looks more like debug info than presentation text.
A size of 1 means it uses 1 byte -- which is greater than the 2 bits needed in my example.
While semantically intelligent, I am scared of anyone thinking more then 255 enumerations in a single type is a wise idea, let alone 2^64. I can think of very very few instances where it would be. 
This potential expansion comes to mind: #[repr(uint)] enum SmallVec&lt;T&gt; { Zero = 0, One(T) = 1, Many(*T) = 2..uint::MAX }
http://c2.com/cgi/wiki?ClosuresAndObjectsAreEquivalent
&gt; you can use objects instead of closures anyway Isn't succinctness and elegance at the syntax level valuable, though? Using objects vs closures is so much more verbose.
I hope eventually rust gets the ability for immutable enums to be created which have the size of the actual variant used(+tag); (the icing on the cake would be to allow immutable variant with mutable fields) .. the sort of thing that has us going back to c-like techniques, casting &amp; raw pointer manipulation
By the end of the year is the current plan, I believe.
(Note it's not a hard deadline.)
Is this wise? There might be a function defined in an library that returns an enum that is defined in another library. If the other library changes and adds a new enum value and so the size of the type suddenly changes compatibility is broken. If just a value is added and the size stays the same such a change does not break binary compatibility.
Is the code available?
A hard deadline would make no sense. Why would anyone want to release the stable version prematurely?
That date is the "speculation" of the core team; at least, the most recent "speculation" stated publicly. (You're not gonna get better than "speculation" without a time machine.)
Oh, I didn't know the core team predicted that v1.0 will be released by the end of 2014. A Google search yielded many different dates by different people, so I thought it was just wild speculation. I am not saying that the core team's current prediction isn't speculation, but it is probably more "educated".
[Here's an example that includes capturing outside values](http://www.reddit.com/r/rust/comments/20yhm9/request_for_explanation_the_deprecation_of/cg80ax8?context=1).
Any new closures we come up with are almost certainly going to just be sugar for objects. I assume you're saying: &gt; Ugh, the syntax of objects is painful. Special closure syntax is far more elegant in many situations.
Correct, should have added that too.
Yeah, that's right. Would there be a literal syntax, so that you don't have to declare a new type every time you want different functionality? Will you be able to use normal declared functions in the place of unboxed closures?
Wouldn't this be better in a page on rust-lang.or, linked from the manual, instead of the docs pages? 
In this form it is versioned with the rest of the code (since it's just the doc comment of a module in `librustc`), and is right next to the code it describes. A formal paper will probably be written at some point.
I meant Rust seems to start with int as minimum and not byte and expands as and when required. For the c++ styled bit size enums, sorry I just started with Rust yesterday, I still don't completely understand enums :P
pathetic; can't even read, let alone play games properly.
 use std::io; let mut reader = io::stdin(); let string = reader.read_line().ok().unwrap_or(~"invalid string); print!("{}",string); Can't beleive I wrote printf instead of println. For added fun, if you want to get args from the command line, let args = std::os::args(); if args.len() &gt; 1 { println!("{}", args[1]); } Edit 1: Ah, I didn't read your post fully, OP. I think your issue is the same one that had affected me in the change to 0.10. Edit 2: this works for me as a loop: for line in io::stdin().lines() { io::print(line.ok().unwrap()); } 
I'm not sure why it's not working for you (it is for me), but you could start by taking a look at the error: use std::io; fn main() { for line in io::stdin().lines() { match line { Ok(line) =&gt; io::print(line), Err(err) =&gt; fail!("IO error: {}", err), } } } 
`ToStr` is now automatically implemented in terms of `std::fmt::Show`; you should implement that trait instead.
Excellent, thank you for the example of getting arguments from the command line. That was the next thing I going to try to do :) As for my original question, I am still running into a problem. I used the example you posted in your link. use std::io; fn main() { let args = std::os::args(); if args.len() &gt; 1 { println!("{}", args[1]); } for line in io::stdin().lines() { io::print(line.ok().unwrap()); } let mut reader = io::stdin(); let string = reader.read_line().ok().unwrap_or(~"invalid string"); print!("{}", string); } ... and now I get this error: task '&lt;main&gt;' failed at 'called `Option::unwrap()` on a `None` value', C:\bot\slave\dist2-win\build\src\libstd\option.rs:264 EDIT: I am running Rust 0.10 on Windows (i686-pc-mingw32) if that helps
I personally avoid importing bare functions from modules, and prefer the `io::...` style. Types can/should be imported unqualified (e.g. `use std::io::File;` + `File` instead of using `use std::io;` + `io::File`), unless I'm only mentioning it once or twice, in which case I'll module-qualify those types too. (This is approximately what [the styleguide](https://github.com/mozilla/rust/wiki/Note-style-guide#imports) recommends.) Of course this is a matter of taste.
There are a few things I'm thinking about. One is the differences between linux and windows. Another is what version of rust you're using. I know it's 0.10, but... rustc --version for me returns 0.10-pre-nightly (7bda3df 2014-04-02 17:51:48 -0700) Did you build this yourself, or did you install it from one of the .exe installers? When you say 'pause and wait for user input', what are you doing with the cmd.exe window (which I think you're using - I never tried to develop on windows and haven't used it in years, and wasn't a power user on it). Like are you inputting something in particular, or are you pressing enter on an empty string? I kind of wonder if it was a bad install or if there's a bigger picture thing that I'm missing here. Edit: What happens with the following examples: fn main() { print!("A string") } fn main() { let astr = "another string"; print!("{}",astr); } fn main() { let mut reader = std::io::stdin(); let astr = reader.read_line().ok(); } that last one should print Some(YOUR_STRING)
Thanks! Didn't know that.
Sorry, I'm not sure what else to say but maybe seek help from #rust@irc.mozilla.org. Those guys have helped me out a lot. That or maybe someone else here already has the answer. Hope you get this figured out soon, I've got to go to sleep.
Ok, thank you for your help this far, cheers :)
This example gives me the following error ... task '&lt;main&gt;' failed at 'IO error: unknown error (OS Error 8 (FormatMessageW() returned error 15100))', test.rs:7
Looks like you're hitting a bug: [#13304](https://github.com/mozilla/rust/issues/13304). :(
I would say that with functions it depends on whether they're clear by themselves or not; with one like `vec::from_fn` (as it was), I would never import the function, but rather `std::vec`. On the other hand, `stdin` and `print` are both very clear names; the `io::` part is just noise.
Regarding 255 (and more), I suspect that generated state machines could easily reach it.
I wonder if it would be interesting to switch the representation to `(f32, f32, f32, f32, u8)`. The idea is that it is much easier to reuse the space of an unused tail than to reuse padding within the object. For example, some C++ compilers do so: considering `struct Base { int a; char b; }; struct Derived: Base { char c; };` a viable layout is to put `c` right after `b` in what is normally the tail padding of `Base`.
That'd be interesting to experiment with. I think some people have thought about it, not sure what the conclusion was (maybe the extra offsets required for every check of the discriminants caused a (large) slow down? I don't know). I guess one could investigate other languages with tagged unions and see if they've considered it, before diving in and changing rustc.
"r+" comments on the last commit on a pull request indicates that the poster has commit access and has reviewed and approved the pull request. It triggers bors, the continuous integration bot, who will merge that pull request into a temporary branch, point the buildbots at it to run the test suite, and then push that branch to master if everything is okay (or complain if it isn't). The idea is that only bors ever pushes directly to master, and bors only pushes commits that are known to merge into something that passes tests, master is never broken. (also, "r? @someoneelse" is basically "hey please review my code, you who i assume is responsible for this area of the compiler and/or happens to be around right now") "cc"/"cc me" is just someone commenting because they want to get notifications for new comments on that issue. I think there's a button to do that without commenting, but I'm not sure it has the same effect and anyway doesn't let people know they're interested in that commit. "cc @someoneelse" is intended to give someone else a notification because the commenter thinks the other person should be aware of this issue. There's no deeper functionality behind these comments, anyway.
`r+` is a mozilla-ism (might predate mozilla, I don't know), meaning "[code] review approved". (You'll sometimes see someone asking for a review by someone specific, e.g. `r? @huonw`, and very rarely see a rejection in the form `r-`) The integration bot 'bors' looks for comments from a whitelist of reviewers saying `r+` on the HEAD of a pull request as signs that it should test (and hopefully merge) that PR. (It also looks for `r=&lt;name&gt;`, which allows people to make minor fixups (e.g. if they forgot to update a test) but still have the review under someone else's name, without having to wait for that person specifically to re-r+.) (I believe bors also supports vetoing a build via an `r-` comment.) The cc similar to its use in email; it's just "I think this person is interested in/is knowledgeable about this issue" (if it's someone cc-ing themselves, they're doing it either to get notifications from the issue, or to change the tags of the emails that github sends, so it ends up in a more specific folder (general "watching a repo" emails are different to "involved in this issue" emails)).
https://github.com/mozilla/rust/pull/13268 Nothing new, just syntax changes. An owned closure could be used multiple times, we called that type `~fn()` in the past. The story for closures is nuanced and I don't think I can faithfully reproduce all of it, but the end plan aiui is to have some traits, `Fn`, `OnceFn`, etc, and desugar into trait objects of those. See in/around https://github.com/mozilla/rust/issues/8622
 struct Option&lt;T&gt; { tag: u8, data: T } fn Some&lt;T&gt;(value: T) -&gt; Option&lt;T&gt; { Option { tag: 0, data: value } } fn None&lt;T&gt;() -&gt; Option&lt;T&gt; { Option { tag: 1, data: unsafe{ std::mem::uninit() } } } It's similar to this.
Behold the majesty of the [buildbot waterfall](http://buildbot.rust-lang.org/waterfall?refresh=15). And the [list of pending PRs](http://buildbot.rust-lang.org/bors/bors.html). Note that although the are a lot of them, the turnover is quite high.
Thanks. One quick question - Is this behavior possible because Rust allows implicit return values? (Like we don't need an explicit return statement)
Well a function evaluates to its body, this is entirely unrelated to early returns being possible. It could just the same be: fn None&lt;T&gt;() -&gt; Option&lt;T&gt; { return Option { tag: 1, data: unsafe{ std::mem::uninit() } }; } but that's poor style.
I didn't run into such a situation, I just remembered the reason why gcc doesn't do the same from enum in C anymore. This was a change years ago in gcc. Can't find this in the changelog, though.
I also updated Rust for Rubyists to 0.10: https://github.com/steveklabnik/rust_for_rubyists/commit/9e55c5702915d6f514c2ee493b174c891219a70c
Oh, nice! Should have announced it on /r/rust, I would have seen it ;)
Yes, you can write an operating system. Many have. Someone even ported their C++ multitasking OS to Rust, and reported a very pleasant experience (and less bugs!). People run Rust on ARM mcus etc. See: - https://github.com/pcmattman/rustic - https://github.com/pczarn/rustboot/ - http://jvns.ca/blog/2014/03/12/the-rust-os-story/
* yes, you can write an operating system * rustc, which uses a LLVM backend * Rust has benefit of hindsight * it helps to advocate the language * yes, it is fast (at level of C++ with same safety assumptions) * depends on the available libraries, but is a nice systems programming language in general * the community is very friendly and growing
Oh yeah thats right. Got it. Thanks for the input!
&gt; Can I write an operating system? Yes! &gt; What sort of compiler is used? (Dlang uses different ones) There is only one compiler at the moment, `rustc`, that uses LLVM as a backend. &gt; Does that age matter that much? (Dlang is much older) Regions and linear types as far as I know were still in their infancy when D was conceived. Rust can take advantage of them now that they have been explored in languages like MLKit and Cyclone. Rust is also new, exciting, and has lots of momentum. Immaturity is an issue though because the language is still developing, and the library APIs are still changing. I would not recommend Rust for production use at this time (although I believe a few companies already are). [This Week in Rust](http://cmr.github.io/blog/2014/04/05/this-week-in-rust/) is a good resource to keep abreast of the latest developments. &gt; Does the fact a company is backing it really that much better? Rust has a number of full time developers working on the core language, and others working on Servo (some work on both). This makes a big difference to the speed of development, and it is very rare that pull requests are ignored. This also gives them funding to run [the buildbots](http://buildbot.rust-lang.org/waterfall?refresh=15), which is essential for keeping the tree in compiling for everyone. &gt; Is it fast? Yes. Should be in the same league as C or C++. Future work could theoretically even result in faster code than C because Rust's type system allows the programmer to express many more invariants. &gt; Does it have a "place" in the world where using it is the best choice I would say its "place" is close to that of C and C++. So yes, operating systems, embedded development, soft realtime applications, game development, etc. &gt; How active is the community/What is the community like? The community is extremely friendly, active, and growing at a rapid pace. There are a number of IRC channels on [irc.mozilla.org](http://irc.mozilla.org/). They are probably the best place to ask for advice: * [#rust](http://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust) - general discussion * [#rust-gamedev](http://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust-gamedev) - game development * [#rust-internals](http://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust-internals) - compiler and libraries * [#rust-osdev](http://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust-osdev) - operating system development [Rust CI](http://rust-ci.org/) has a list of active projects. There is a [mailing list](https://mail.mozilla.org/pipermail/rust-dev/). And of course there is this subreddit. &gt; Also, if anyone can add how easy concurrency is that would be cool. Rust was built for concurrent applications. The language is now powerful enough that all(?) concurrency primitives can be expressed in libraries, but are safe due to Rust's powerful static type system (immutability, linear types, regions, etc.) &gt; And what is meant by "not having a sense of humor"? Just one of our mods being funny (he thought we were all being too serious in a reddit thread). ◔ᴗ◔
Taking a look at some stuff now. And thanks for the xxHash link. It is definitely a close one, but I was thinking more about a well optimized compiler taking different languages. Can LLVM/rustc compiled binaries be as fast as something written in a comparable style but in C++, for example? Something like [this](http://benchmarksgame.alioth.debian.org/u32/benchmark.php?test=all&amp;lang=rust&amp;lang2=gpp&amp;data=u32) shows that Rust can compete with C++ and be about 1/3 of the speed, worse case 30x slower. Is there any comparison like this that uses a non-trivial benchmark?
Awesome! Thanks for the references!
Thanks for writing!
I love the friendliness! And the fact you mentioned gamedev: do you know of any source code for games to check out written in rust? Also, can you elaborate on the "faster than C because you can express more invariants" part? I know static typing helps the compiler and speed of the binary as well, so are you just saying there are more types? (Not claiming this is the case, but an idea to see if my thinking is correct) if C only had a type for linked list (with access time O(n)), someone discovered random-access lists that allows rust to access elements in O(1) time? I know this is not actually the case, but is this what you meant by "more invariants can mean even faster"?
It's too early to be using those benchmarks to mean anything useful. They are not maintained, and certain features (like the bignum lib, as you can see from the pidigits benchmark) are not optimized yet. Also Rust's codegen isn't perfect yet. Rust compiles to LLVM, and clang does too. In theory they should be the same speed. Rust doesn't have a lot of magic or anything like that which would disallow it from generating the same LLVM bytecode that clang does (from C or C++). I would just take the devs' word for it when they say it will compete with C/C++. 
Yeah, I should have. Oops!
I would read that post. Just so you know. :o)
I dislike this approach. It means you need to take extra steps in order to check for errors (if this is even possible to do cleanly with this approach), else it fails silently. With the fail! at least the failure is loud, and with IoResult you have to handle it in some form, even if it's just calling fail! or returning a default on error.
In case anyone is curious, the branch where I tried doing explicit error handling is here: https://github.com/dwrensha/capnproto-rust/tree/decoderesult Here's what the addressbook example looks like: https://github.com/dwrensha/capnproto-rust/blob/decoderesult/examples/addressbook/addressbook.rs
To be clear: I still use IoResult when I'm actually doing I/O. See, for example, serialize.rs: https://github.com/dwrensha/capnproto-rust/blob/master/capnp/serialize.rs 
I'm pretty sure that code doesn't compile. fromMaybe takes a Maybe a for its second parameter, not [a]
1. It compiles on my GHC, so you should probably check before commenting 2. listToMaybe gives you Maybe first element of the list
 let fizzbuzz d i = fromMaybe (d i) $ ["fizz" | i `rem` 3 == 0] &lt;&gt; ["buzz" | i `rem` 5 == 0] &lt;&gt; ["bazz" | i `rem` 7 == 0] &lt;interactive&gt;:25:38: Couldn't match expected type `Maybe a0' with actual type `[t0]' In the first argument of `(&lt;&gt;)', namely `["fizz" | i `rem` 3 == 0]' In the second argument of `($)', namely `["fizz" | i `rem` 3 == 0] &lt;&gt; ["buzz" | i `rem` 5 == 0] &lt;&gt; ["bazz" | i `rem` 7 == 0]' In the expression: fromMaybe (d i) $ ["fizz" | i `rem` 3 == 0] &lt;&gt; ["buzz" | i `rem` 5 == 0] &lt;&gt; ["bazz" | i `rem` 7 == 0] ... edit: oh, you're using the MonadComprehensions extension. One of the reason I don't like using too many extensions, it makes code harder to read.
The biggest difference is that in D, safety is achieved through garbage collection, while in Rust, safety is achieved through a sophisticated series of compile-time checks (the lifetime system and borrow check). This eliminates the overhead at runtime, at the cost of a more complex type system. This means that Rust brings safety to domains that previously couldn't be safe because the overhead of GC was too high.
Some very good responses here, but what about the braces-for-scope and assert statements?
Static type checking helps you check programs against some constraints. For example, if you have a constraint that a reference can never be null, you never have to do `if(x != null)`
Any comment sends Emails for new comments even if you have notification Emails disabled. The subscribe button follows your notification settings.
Not just that, but the Notifications section in GitHub has a separate area for things you're participating in, as opposed to things you're merely subscribed to. Rust Committers are subscribed to the entire mozilla/rust repository by default, so they have to actually participate in a thread (e.g. by saying "cc me") to have the notifications stand out.
I have thought about this before. From my limited understanding, it seems to encroach on the territory of [ML-style module systems](http://en.wikipedia.org/wiki/Standard_ML#Module_system). The issue is this tends overlap with the functionality of type classes somewhat, so if it is not done right it could end up feeling a little awkward. There has been some research on unifying the type classes and modules: - [ML Modules and Haskell Type Classes: A Constructive Comparison](http://www.cse.unsw.edu.au/~chak/papers/modules-classes.pdf) - [Modular Type Classes](http://www.mpi-sws.org/~dreyer/papers/mtc/main-long.pdf) - [Backpack: Retrofitting Haskell with Interfaces](http://plv.mpi-sws.org/backpack/) But we might be too far along the road for doing any major surgery like that. I haven't had a great deal of experience with using ML modules though, so take this comment with a grain of salt.
I would really appreciate seeing a summary of the issues surrounding owned closures at some point. Unbureaucratic currying/partial application is one of the functional programming idioms I miss in my everyday C/C++ programming life the most, and while my impression of it might be somewhat outdated or incorrect due to blind dabbling, it seemed like Rust was still falling visibly short of the freedom languages like *ML give you in that regard.
As for your first paragraph: - Yes, curly braces create scope. - Yes, we have assert, as `assert!(expr)`. - Yes, tests are easy—mark test functions with the `#[test]` attribute, compile with `--test` and you can then run all your tests with a ncie test runner. We've got benchmarking too in much the same way (with `#[bench]` and `--bench`). - I don't know what D's compilation time is like so I can't comment on that. I consider your Google trends results to suspect. "D language" has a strong focus in Nigeria and India with the US coming after that, which just doesn't match what I would expect. [Substitute dlang in its place](http://www.google.com/trends/explore#q=dlang%2C%20Rust%20language&amp;cmpt=q) and you get a different set of results, substitute "d programming" and you get another (showing a headline that demonstrates that it includes "3-d programming"!), go for the topic D (programming language) and you get another. But they're all minor enough queries that you can't get a lot of useful information out of it.
I wonder if this differs much from something like pub struct Foo&lt;'l&gt;; impl&lt;'l&gt; Foo&lt;'l&gt; { fn Bar(x: &amp;'l T) { ... } static BAZ: uint = 1; // ... } Foo::&lt;'a&gt;::Bar(...) This does require associated items for full generality.
(And the reason is a (lazily executed) format string too, e.g. `assert!(condition, "foo {} bar", some_variable);` is fine.)
With this approach, how do you handle cases where there isn't a sensible default to fall back to? Just curious. I've never used capnproto-rust or capnproto at all, but it does sound very interesting.
&gt; Also, can you elaborate on the "faster than C because you can express more invariants" part? _Note: I’m not an expert on these things, so take this with a grain of salt:_ As far as I know, LLVM performs optimizations on the IR (intermediate representation) provided by a compiler frontend, like `clang`, `ldc`, or `rustc`. The issue is that the IR was designed around languages with C-like type systems, and this limits the amount of program transformations that can be performed. Rust has the potential to provide the optimiser with far more wriggle-room than in a C-like language, given the fact that the type system provides information regarding regions, linear types and immutability (those are the invariants I was mentioning before). Putting aside LLVM's optimizations however, Rust's type system already allows you to be far more cavalier when throwing around references than you would in C because the type system will catch your mistakes. This reduces some of the overhead that you might have had to have done in an equivalent C program in the name of safety.
`BAZ` is an associated constant (associated items are not yet implemented - I'm waiting for them to be implemented with baited breath).
but how does it relate to the lifetimes/module stuff in the rest of the example?
Not sure. Perhaps he was showing that the `impl Foo&lt;'l&gt;` is also some sort of module thing because you will be able you put any kind of item in it. (not just functions)
It was just an example of a thing that can go in a `mod` that currently can't go in an `impl`, but will be able to be placed there with associated items.
rustache
&gt; I think the best solution for the Rust implementation is to log an error and fall back to a default value when invalid input is detected. I have had bad exeperience with this. The used code gave no control over what/how much was logged. This gets ampliefied the more such libraries you use. Lots of logging output means there is effectively no logging at all.
&gt; When Haskell is good enough for your purposes, it's a strictly better choice. Can you expand on this a bit? As a lover of Haskell who's also interested in Rust, I'm interested in this.
I actually ran into this today, working on a freestanding libstd. Took me a little while to work out exactly what I had to import and where so I could get the length of a string. One unintuitive aspect of it was that I was expecting to have to use the module that contained the `impl`, *not* the one containing the implemented trait itself. Definitely like the sound of it from a convenience point of view, although it'll be another thing in Rust that appears at first to work by magic (which is bad). On a related note, it'd be tops if the documentation had a linear list of the available methods on a given type and where they come from. There's been more than a few times I've had to bum around the docs for a while trying to figure out whether or not there's a method for doing X on a value.
Thanks!
Thanks for the explanation! That certainly fits in with my experiences with Haskell.
I'd never been tempted to try D for some reason - but Rust, strangely, was (a) close enough to C++ in the areas I *need* but (b) further away enough in the areas I wanted *changed* for me to try it. not entirely sure why that is; i *did* go through a phase of trying out different languages (haskell, clojure, common lisp, scala) even though they were all ultimately ruled out by being garbage-collected.. in that phase of curiosity, I still didn't try D. Its possibly because I like the idea of functional programming, even though I need multi-paradigm. I describe rust as having a slightly more functional flavour, with its expression blocks and immutable default. One thing I've read about in D I like the sound of alot is Uniform Call Syntax. That in itself is another , possibly better, direct solution to one of the things that bugs me in C++, the asymetry between methods and functions and bouncing between them. Its possible rust will get something like UFCS aswell, and traits are good enough (one can still make a new trait to extend someone elses' type). in some ways rust makes them more asymetrical because it only dispatches on the first parameter.. the first paramter is *more* special.. but its' extentable so I dont mind as much - but in my 'perfect langauge', there would be no "methods" at all, just functions, and you just have sugar for prefixing a parameter if you like
&gt; Do the data structures give types (yes, except for the variants of enums) What does this mean? Can anyone explain a bit more?
Now I can't unsee it.
Noob questions - &gt; let mut memory = [0u8, ..30000]; What is this statment doing? &gt; while instr &lt; program.char_len() Since program is not mutable will compiler cache the length or call the function for every iteration? 
I think what is being proposed is a bit different then how ML uses modules. This is more of an extension to generics. I don't think it would compete with type classes.
If you have any suggestions/improvements don't hesitate to post them here or to open a pull request!
I use Notepad++ with a custom syntax highlighting pack. There is an emacs mode (https://github.com/mozilla/rust/tree/master/src/etc/emacs) and a vim plugin (https://github.com/wting/rust.vim), but I am not building any serious application and Notepad++ is enough for me.
The first link is for all projects that use Rust and D. Not sure if it is only those registered on Ohloh, our whether they to pre-emptive statistics gathering. And it seems you are right, D has a separate repository for Phobos. Here's the same page [with Phobos beside Rust and DMD](https://www.ohloh.net/p/compare?project_0=Rust+%28programming+language%29&amp;project_1=DMD&amp;project_2=Phobos+%28D+standard+library%29).
&gt; Generics behave much like C++ templates To clarify, they behave the same in respect to the code they generate, but they have rather different semantics in terms of type checking.
I use Sublime Text with the Rust Highlighting plugin (https://github.com/jhasse/sublime-rust)
Vim here.
Vim works fine. Syntax highlighting is in rust codebase.
Emacs, with the rust-mode extension.
I think I use different Highlight plugin, but overall still Sublime.
Note that the rust.vim repo is just a mirror of [the in-tree vim mode](https://github.com/mozilla/rust/tree/master/src/etc/vim) to make it integrate with the vim package manager(s) nicely. Also, speaking of package managers, the in-tree emacs mode is actually available on [MELPA](http://melpa.milkbox.net/#/rust-mode) (updates regularly, installable via `M-x list-packages` for sufficiently recent versions of emacs).
What's the plugin you use?
Thanks for nitpick, will add to my fix list :) I got interested in Rust at Codemash conference in January after watching Steve Klabnik's presentation. I was looking something new and exotic to hack on for a bit, and Rust really got my interest. I've spent some time in the C++ trenches in the past, so the whole compile time safety really grabbed me. And then combine it with the concurrency and functional features and you have something really intersting. Submitting the talk to StirTrek forced me to get off my butt and get on with learning it. :)
I just use Joe. It works. 
~~It doesn't compile for me. I might have done something wrong. Rust v0.10 on Arch Linux.~~ Accidentally used v0.11-pre, I could swear I had been using v0.10. Sorry if I had wasted your time. Anyway, the game looks good. Though it might be better if the instructions were printed. I could only find them in `console_input.rs`.
&gt; worse case 30x slower. The 'worse case' there is pidigits. This benchmark is almost entirely a test of the bignum library. The C++ implementation links to libgmp— which isn't part of the C++ language. I believe the rust bignum implementation is just some rust standard library bignums which are not fast. Since Rust's bindings to call C code are fast I'd expect a rust implementation which used GMP to be equally fast.
The code is for an older version of Rust, so you'll have to wait for someone to upgrade it (or upgrade it yourself).
I primarily use Sublime Text, but I converted the syntax highlighting stuff from the sublime plugin to work with Text Mate or Textastic (Text Mate compatible editor for iOS). Sublime copied their syntax highlighting from Text Mate, it is just wrapped up a little different. I just use it to review some code on my iPad with Textastic. https://github.com/esummers/rust-tmbundle
README.md states: "A basic implementation of the Mine Sweeper game using the Rust programming language 0.10." ~~I am using v0.10 right now.~~ Sorry, I am using v0.11-pre-nightly. I will try to compile it with the v0.10 compiler.
I use Kate but I only do occasional tests with the state of the language.
I've used (and still interact with the code I wrote) D for 5 years now, using both D1 and D2. Switched to Rust because I was discouraged by D's community, and lack of a coherent vision for D's future. &gt; Does that age matter that much? (Dlang is much older) D2 supporters like to ignore that fact and claim that D2 is a completely different language from D1. D2 has been around for probably 5-6 years now, so it's about the same age as Rust. While Rust is changing rapidly, D2 certainly is not immune to change and backwards incompatible language and library changes are introduced with every compiler release. The fact that this still happens despite D's age (I'll now ignore the difference between D2 and D1, as D2 inherits quite a few features/misfeatures of D1 and it is not impossible to have code compiling for both in principle) should tell you something about their development process. Rust is on track to stabilize a sizeable subset of it within a year's time given the current estimates... I don't think D2 has any sort of timeline and I fully expect it to be unstable for years to come. &gt; Does the fact a company is backing it really that much better? Absolutely yes. D is community driven, but not everybody in that community is equal. Whenever some company of a certain size asks for a feature, D's leaders bend over backwards to accomodate them so that they /do/ get some company backing. Consider the example of user defined attributes (UDAs). The documented syntax for them is this: @(3) int a; pragma(msg, __traits(getAttributes, a)); // prints 'tuple(3)' That is not the only syntax, however. This also works: [3] int a; Why? Because that's the preview, pre-release, syntax of UDAs that got used extensively by a certain company and therefore was kept in the language despite wide discontent in the community. See these [two posts](http://forum.dlang.org/post/kadpfc$2vtj$1@digitalmars.com) from Walter and Andrei. There was a very similar event on a library level with a focus on Thrift bindings (some IPC thing from Facebook, I am meant to understand). I don't think the degree of independence gained from being backed by a company from the start should be discounted here. Rust doesn't have the necessity of trying to appeal to a major company, and thus can focus on being a good language first and foremost (a byproduct of Mozilla wanting it to implement Servo well). &gt; How active is the community/What is the community like? I think the community is one of the best parts of Rust. They are friendly, knowlegeable and welcoming to ideas from academia and practical usage. They seem somewhat inactive from a glance at the mailing list, but that's because a very big chunk of it is chatting away in IRC.
Thank you! I thought that they actually were printed... I will fix it now
I sound just like you, going through haskell, clojure, CLISP, but not scala and only briefly into rust. For me, it was that the language wasn't stable enough and examples that worked in 0.7 didn't work when I tested them in 0.8. There was also an issue getting a compiler that stayed up to date, and I ended up with D as a result. I can say I like D more than C++, but it doesn't have the big company behind it that Rust does, and seems to be less "hyped" at the moment. That can kill something, but D seems to be a "leaner" dinosaur, C++ is the dinosaur that is just so big and has so much extra junk it is hard to keep track of, but is popular. C is the little lizard that can imitate a good dinosaur, but only the good traits. It's too easy to end up imitating bad traits though, or doing it so poorly that even a "lean" but slow dinosaur (D) can beat it by not changing stuff. Rust to me seems like a very quickly evolving "lean'd up C++" similar to D but will be faster moving and more popular. The fact it is sort of "ADD" about how it wants to do stuff though it making it hard to "train." On that note, I'm very excited for Rust to settle down, get a 1.0 out, and hopefully a book. A stable, but fast language that is popular and has a lot of support from a company behind it will be great. So, anybody, correct my analogy (Dinosaurs to programming languages? Yeah it won't be perfect.) and if you know of a target ETA (I know pinning one down is a bad idea, but if there is a goal I'd love to hear so) or any book deal going on I'd love to hear about those. A book and examples online are a great way for me to get into a language. Off topic, but the D compiler is written (in harmony with the D syntax) to only have to look at each character 7 times. There is no ambiguity for the parser. I think C++ the minimum IS 7 times, sometimes more. Is there any info on this for Rust?
#1 Is a fixed vector
The last bit sounds sort of like Haskell's "If it compiles, it is probably right" because of the very strict type checking. And as far as I know clang IS (part of) LLVM, and LDC of D compiler fame definitely is LLVM. Both of these just put code into LLVM bytecode, sort of like the java compiler. The difference is that the LLVM bytecode is then further transformed into specific machine code. This lets people optimize the heck out of the LLVM -&gt; machine code bit and as long as something compiles to LLVM bytecode, you'll get fairly quick bytecode. I believe emscripten actually compiles anything that CAN become LLVM bytecode into that bytecode, then does a bytecode to Javascript conversion. It uses the ASM subset of javascript which Firefox (and maybe others down the road) will give more security clearance to, letting things like that UE3 citadel demo be compiled to LLVM, then to ASM.js and run at near-realtime speeds. ---- So now that I went off on my own tangent, it sounds like Rust is more specific about what the code is doing, so something that in C might say "Well, we're going to do this OP to every element of a list" and then later finds out OP was multiply by two, Rust would already have specified that and would quickly just say "Well, we're going to bitshift every element of the list to the left one place" which is faster as a result. I hope I'm interpreting that correctly. If I'm not, and you have an analogy, I'd love to hear it. If you saw the "Dinosaur" analogy I made, you can see I think in analogies :P. Even an example of what C does vs what Rust does can help.
Perhaps you could run Linux in a VM, and then run Rust there. That's what I do on my Windows box :)
as far as i'm aware Rust is 'context-free-grammar', no ambiguity, and strict limit on lookahead.. its one of my wishlist features, should make tools easier in future.. being able to manipulate source without needing the entire program context.. at the minute this is a theoretical benefit. The only real thing I miss from C++ is the mature tooling. Its quite possible 'autocompletion after a dot' (eg intellisense and so on..) is more useful for navigating than any features of the language itself. Chicken/egg situation there, there aren't enough Rust programmers in the world to implement this it seems yet. I got as far as making a rust html code browser but it isn't anywhere as useful as an IDE 
Ok good point at the bottom. Honestly I don't care as much about compilation time as run-time for the user. I usually turn on all optimizations because I want to do lots of things quickly and even a small gain at the expense of compile time will add up in runtime. Thanks for the first 3 points. Having a language with these built in offer a lot than languages that need libraries to do this (making it that much more "meh I'll just test on the fly. Or QA will catch it"). And if you ever do hear about compilation time again, I don't need a rebut or anything, just letting you know that it was designed so that syntactically each symbol of code would only need to be looked at once. In C++ it is a minimum of 7 times I believe, so this translates to compilation times that have that "Hm that compiled so fast I think I did something wrong." feel. Indeed, compiling in D can be so fast you wouldn't have time for sword fights in the hallways. It was part of the design.
&gt; They are not maintained… Do you mean the Github [rust/src/test/bench/](https://github.com/mozilla/rust/tree/master/src/test/bench) programs are not maintained? shootout-meteor.rs latest commit a day ago
Woah the autocomplete thing jumped out at me. Does this mean you aren't sure if Rust will ever have it? Or do you know it will? Or know it won't? I would also rather have autocomplete (and something that does autocomplete can look ahead further, I don't care, it just impacts things if you need to compile fast) than not. If anyone who is pulling for D is in this thread, get autocomplete in my IDE (either a Vim plugin, Sublime plugin, or IntelliJ IDEA plugin) and that language will automaticallly get more support.
Textadept. It's open source and rather usable.
emacs , its the only editor that I've been able to sufficiently bend to my will on windowing and hotkeys. If my fingers didn't have certain things permanently burned in by breif/codewright in 1995 and visual studio 2000 onward.. I probably wouldn't choose it. I have only managed to use basic 'words from same files' autocomplete usefully, and inaccurate etags jump to def. I've made it fire up a 'rustdoc' search URL from a hotkey on a symbol. I wish it was possible for emacs to bring up a disambuguation menu when you ask to jump to a tag... it knows there are multiple ones and you can walk them with some key chord i can never remember google for 'racer' , someone is trying to make an emacs completion plugin that understands modules at least tooling is a big deal it seems; things like intellisense in visual studio are immensely useful IMO, I hope the rust community grows big enough to get this kind of support... being easier to parse it should be easier in some ways? but you still have complexity to deal with. As an outsider I've found it hard to work with rustc &amp; its' AST... clang for C++ tools is much easier for me - despite the fact I much prefer the rust language.
I just meant, in general, they are not being currently worked on and improved. There are many benchmarks missing from the shootout and there is a lot of `FIXME: Inefficient` throughout some of them. The commits you see are just find/replace syntax changes so they are still buildable as the language changes.
Ok. I just mention age because D1 did its thing, D2 learned from D1, and it continues to learn from itself. It is at a point that a book can (and was) written on it. I don't think Rust is at that point quite yet. This post alone though has shown me how friendly the community is. I am not saying that D doesn't have a friendly community or brains behind the language (it does. Both Walter and Andrei are very smart. Walter worked on C++ I think and already learned what to improve from that experience) but I'm glad this community is very active, and is not getting stale (try finding an OS in D that is currently being developed. Even just a kernel). The only place Rust is lacking for me right now is in stability, which will come with time, but that future just hasn't arrived yet.
&gt; Python uses gmpy More than one Python program is shown -- http://benchmarksgame.alioth.debian.org/u64/program.php?test=pidigits&amp;lang=python3&amp;id=1
To compare the goal final speed in relation to C++ and Python. One is compiled, one is not. Rust is compiled. I was just checking if it would even be in contention for "language olympics 100 meter dash." I want speed with rust to be dependent on the programmer, not the language. I'd hate to write a very fast, optimized rust program and have a naiive implementation poorly written in C++ still be faster.
Vim, with [Syntastic](https://github.com/scrooloose/syntastic) for error highlighting.
Well, it couldn't be class-level because neither Rust nor Go have Java/C++ style classes, right? :-)
So if rustc shovels libstd with your program into LLVM... can LLVM remove unused libstd functions to make the binary smaller?
There doesn't happen to be syntax hilighting for nano does there?
Here are some potential issues I thought of: * The most natural interpretation of the syntax is that Foo&lt;'a&gt; and Foo&lt;'b&gt; are distinct modules when 'a and 'b are distinct variables, but this makes it impossible to use the resulting module in more than one place. * It decreases the amount of local reasoning that I can do given a function definition, since I have to look at the enclosing module to determine whether there is a lifetime parameter. * Does this descend to trait impl blocks? To the individual function definitions in the trait impl themselves? It does seem a bit strange to apply this autoparameterization to external traits whose type parameters are already defined.
...which is 3 times slower than the gmpy version (program 2) anyway. It is almost irrevalent that there are several programs which do not use GMP and beats the baseline Rust program, since both the Rust program and libnum has yet seen the serious optimizations (and in that sense OCaml would be very *slow*, though it is merely a fault of the naive `Big_int` implementation). It is very relevant that people didn't try to optimize the programs without GMP but instead chose to use GMP, since it is what is available, legitimate and fastest.
&gt; "language olympics 100 meter dash." But pi-digits is the "language olympics" shot put.
You'd better use `rand::task_rng()` instead of `rand::StdRng::new()` (expensive) for [populating mines](https://github.com/aochagavia/RusticMineSweeper/blob/841dd75/src/board/mod.rs#L57). I also think `Square` is a bit redundant and can be desynchronized to neighbors due to the sloppy coding; you'd better have some tests to counter that. ;) Asides from the code, I suggest some more commands: `q` for exiting from the game, `ss` (well, better mnemonic anyone?) for conditionally opening neighbor cells only when the number of marked neighbors equals to the current cell (equivalent to the middle button in Windows Minesweeper). Also, `m` to toggle the mark (instead of setting it).
&gt; That's not fair to Rust, now is it? *Really.* I don't think I've seen the "not fair" card played since Ruby 1.8 -- probably because there's no down-side to shrugging and saying arbitrary precision arithmetic isn't a current priority.
Geany (I compile from source to get the highlighting + ctags support, in principle a nightly works too).
Sublime
Thanks for your thoughts! Like I said I haven't thought this through much, so these are not proper answers (i'm not sure if they'll work): 1. You given `Bar` in `Foo` you could call `Foo&lt;'a&gt;::Bar` and `Foo&lt;'b&gt;::Bar` or use two aliases: `use FooA = Foo&lt;'a&gt;` and `use FooB = Foo&lt;'b&gt;`. 2. True, I guess that is just a trade-off. 3. Module-level params would be available to use in an impl block, but external traits would not be 'auto-parameterised'. E.g., mod m1 { trait T&lt;'l&gt; { ...} } mod m2&lt;'k&gt; { struct S { ... } impl m1::T&lt;'k&gt; for S { ... } } 
It means that when you define any of the data structures (other than enum variants), that definition creates a type. E.g., when you write `struct S { ... }` you can then use `S` (and `&amp;S`, `~S`, etc.) as a type, e.g., `fn f(x: S) { ... }`. Tuples are a bit different in that they introduce structural rather than nominal types and there is no definition of a tuple. But, tuples still give a type as well as a data structure. In contrast, if you write `enum E { variant1 }`, you can initialise data using `variant1`, but you cannot use `variant1` as a type.
I've thought about this some more, and I think I agree with you; in many typical cases, the fail-fast behavior is exactly what one wants. I've now made that the default behavior. Importantly, though, you now also have the option to opt-in to the default-value fallback mode that I describe in my original post. (I've updated the post with a note about this.)
I like this, as just a general marketing thing.
As someone who's in the 'will probably write (another) book on Rust' camp, I'm also waiting for it to be closer to 1.0: no reason to go through all that effort just to have everything change out from under me!
LLVM can and does, otherwise our binaries would be at least as large as the libstd library (and they're not). (Our current method of providing IO does mean not every unused function is removed, leaving our binaries larger than strictly necessary; but this is just a bug, and people are working on it.)
Also, flycheck for error checking. 
This is the kind of thing Rust should be very good at. We just need to reimplement TLS in Rust to prove it. This is a project that sounds ridiculous at first but nobody is particularly happy with any of the TLS implementations.
Tooling will come! I'm pretty sure Rust will be much easier to tool than C++ because the language is much easier to parse. Not sure whether we would need incremental compilation though. But yeah, I think it's just a matter of priorities at the moment.
Something thats been suggested is that it might be possible to write something that submits code fragments to the existing ast &amp; compiler. Doesn't sound trivial though. I was worried that HM type inference might actually make things harder - but for complete-friendly code , I think people would be happy to annotate a few more types. One sticking point i can see with the idea of just 'fixing up a current incomplete code fragment' is .. the return type would have to match? Perhaps the 'butchered' version of the current function could have the return type removed, and any scope blocks get an added ';' to ensure they're not trying to return :) Seems there's a lot of people interested in this, I've had a look, can the community organize itself to build this i was interested in adding more "ask-the-compiler"-friendly error messages too. eg, when you dont find a symbol in scope, look for it in other scopes. When you dont find a method, list the supported traits and show a link to the struct def ("see definition of..") 
Now that I think about it, if the `mod Foo&lt;'l&gt;` syntax expands to a module whose individual items have lifetime parameters, then Foo::Bar would just have additional lifetime parameters, and you would use it just like any function that has explicit lifetime parameters.
Shucks, it took months to even _get_ TLS support in rust-http... (not that I'm contributing patches or anything, just saying that it's true, and not trivial :/)
No, A rust based library for this would be awesome if rust was stable and rustc thoroughly tested. Neither of those are true yet though so we can't realistically build a production library yet.
KATE or vim, depending on how I feel.
I'm no expert, but I doubt the compiler optimizes that out. I think it gets called every iteration. 
https://twitter.com/OhMeadhbh/status/453295192989130753 I'll note that she used to work on the privacy team at Mozilla, too.
I'm looking at this more from a PR perspective. This demonstrates an important use case - I'm not suggesting the world drop OpenSSL right now and replace it with a Rust library written overnight. This bug demonstrates that there actually is a very real need for Rust, and that maybe it's time for everyone else to think about eventually migrating to it. We could let the opportunity pass by, but that seems like a [wasted crisis](http://www.brainyquote.com/quotes/quotes/r/rahmemanue409199.html). I'm sure this seems obvious to the people in this sub, but I'd be shocked if even 10% of /r/programming knows what Rust is about at this point. Among that group, I presume there's a decent chunk of people who are so used to the C languages that any effort to move away seems like a waste of effort. The sooner we get some examples out there *why* Rust is important, the sooner Rust will become the mainstream language to use.
This is a receptive venue for us, and I hope to see multiple talks about Rust.
Thanks! You just saved me some work from doing that myself. PS: Sublime Text &amp; Textastic here as well
Emacs.
Well, it seems the prediction in my other reply to you [may not have been wrong](http://www.reddit.com/r/netsec/comments/22gaar/heartbleed_attack_allows_for_stealing_server/cgmwzc5) (currently at (6|9). Even in /r/netsec, a security sub, a suggestion to move to a safer systems language wasn't exactly well-received. I suspect people don't even realize that languages can catch such errors at compile time, leading to an expectation that languages with thorough compile-time checks lead to lower performance in terms of execution speed, memory usage or both. I'm pretty sure this doesn't bode well.
I've started work on a rust plugin for intellij. Currently only does syntax highlighting, but I'm hoping to add autocomplete and type inference soon™. It handles the rust source pretty well, even some of the more extreme string test cases. [preview](http://imgur.com/mDl0Qu6) [plugin](http://plugins.jetbrains.com/plugin/7438?pr=) [github](https://github.com/Vektah/idea-rust) 
Just saw both of these replies, from a PR perspective it probably is beneficial, but as we can't offer a solution, I doubt it will make much of an impact on the other '90%' of /r/programming. But rather push the '10%' who have heard of rust to be more in favor of it. You could probably have worded your post in /r/netsec more effectively (said why rust eliminates bugs, and that it would have eliminated this bug specifically), but that is a disappointing result. 
I believe that the actual cryptographic libraries are the smallest part of the problem. Not that we shouldn't have high quality cryptography in rust, but TLS is such a complex protocol that I believe that a TLS stack written in rust with bindings to high quality C cryptographic libraries would be a great win. These projects are orthogonal to each other though. Whenever a cryptographic library written in rust is stable enough we could easily switch to using that in the TLS library. I don't mean this as an attack on rust-crypto though, good work. The devil is (as always) in the details, and even more so in cryptographic code. 
I feel the same. Although I probably wouldn't notice if they didn't feel like special cases. It would be nice to allow data structures that are not built-in to use patterns so if/else is needed less frequently. I could see something similar to Haskell Pattern Synonyms except with the power of a syntax extension so they could use accessors in the object they are matching against. Pattern matching could be extended for a type instead of built in to the language: fn match_example(my_map: HashMap&lt;i8, (i8, i8)&gt;) -&gt; i8 { match my_map { { 123 =&gt; (a, b) } =&gt; a + b, _ =&gt; 0 } } I think the implementation would look like `macro_rules!` except instead of expansion you succeed and bind to variables on the left side or fail and bind to nothing. I think an existing type would be specified instead of a macro name.
There's also a Python program that is [5 times slower](http://benchmarksgame.alioth.debian.org/u32/program.php?test=pidigits&amp;lang=python3&amp;id=1). There's also a Python program that completed in [0.061 seconds](http://benchmarksgame.alioth.debian.org/u32/program.php?test=pidigits&amp;lang=python3&amp;id=4) (but it doesn't seem to have survived the update to `gmpy2`. Neither of those things make winning the *language olympics shot put* relevant when you only need to do *the 100 meter dash*. It's kind-of important to look at what programs do.
Well, that's fair to Rust now, isn't it :-) Almost as if pi-digits had become a test of FFI overhead…
&gt; as we can't offer a solution Not today, but we can create awareness, which will spread safer software sooner. The other 90% doesn't even know Rust solves some of their problems. They may not even realize they have a problem, if they're overconfident in the less safe systems languages they use now. All we need to do is jump up and say "Well, isn't it a real shame there's no production Rust TLS library yet!". Knowing that it's safer without any significant performance impairment may well spark some interest in Rust. I agree my phrasing was not optimal, but as I'm not a Rust expert, the more I say the higher the chance I'm just plain lying. There's a reason I often just lurk in this sub.
As long as you accept to have the tag just before the variable-length part, you're still golden :) But indeed there can easily be interactions with lots of stuff.
I remember reading somewhere that one of the main Haskell guys (possibly SPJ) said that if he was starting from scratch Haskell's laziness would be explicit (and opt-in). Too unintended space leaks.
There are a lot of "C or bust" types in the security community. I think they aren't really being rational: the hard statistics surely show that there are far more *memory safety* violations in C or C++ apps than in apps written in memory-safe languages like Java, even if (as is always true) all programs can have vulnerabilities. To make a somewhat tortured analogy, just because we haven't cured cancer yet doesn't mean that eradicating smallpox wasn't a massive human medical advance. There are also plenty of people in the security community who do appreciate the value of language-based solutions. Coverity is well-regarded in the security community, for example.
This is several times slower than the Haskell and pypy versions in the same repository; if anyone knows why (or has comments on the code generally) I'm all ears.
I think you're just more optimistic about the interest of the average /r/programming reader then I am, or maybe about the effectiveness of this sort of PR. I'm certainly not an expert, or even moderately experienced, in this area though, I could easily be too pessimistic. Edit: To clarify, I think rust is big enough now that (with a few exceptions) people on /r/programming who you will actually care to consider the benefits of rust have already heard of it, and the rest will simply not care what you are saying. There are various reasons I think people would not care, they have heard of lots of new languages, and none of them have really amounted to anything better then c++. A good programmer won't make these mistakes, it's unnecessary. C++ is already so entrenched (library/tool wise) that working with new low level languages is just a waste of time. C represents what's going on at the hardware level, no memory safe language will ever do this properly. Etc, etc. Yes I know these aren't all true, but it's why I would expect most of /r/programming people to dismiss rust.
&gt; the *only* plausible reason Try to falsify your hypothesis. Have you noticed that for some tasks, some c++ programs are 10x slower than other c++ programs? &gt; another problematic problem Say what you consider to be "problematic" about it. &gt; But it does not imply that those implementations are slow in general… [*In general?*](http://benchmarksgame.alioth.debian.org/dont-jump-to-conclusions.php) "These are just 10 tiny examples." 
Awesome! Associated items are definitely on the wish list, but I'm not sure what the time frame on them is. Did you see this? [Not-an-RFC - type and lifetime parametric modules](http://www.reddit.com/r/rust/comments/22dwuy/notanrfc_type_and_lifetime_parametric_modules/)
i think they have a plan for associated types ? i've been after a 'typeof()' or 'decltype()' equivalent, ... personally i like the cases where you can express another required type in terms of type-param .. but whenever i've brought that up they talk about associated types which seem more like what you're on about here. I'm not sure how much overlap there is of that with the cases you're talking about. I'm curious what the plan or direction is
My associated types would always determined at compile time. (In particular, this idea does not play well with trait objects, because then the associated types themselves would be determined at runtime.)
everything i've been interested in is compile time; its just where and how they are declared; 'decltype' /'typeof' is something i find myself missing from c++ occasionally - simple examples being, a dot product of 2 vector inputs returns a scalar; the scalar type wants to be derived from the product type of the input vector component types (write a full expression for that, and when you input your fixed point maths types, it would propogate their shift value correctly.. template&lt;typename V1,typename V2&gt; auto dot(const V1&amp; a,const V2&amp; b)-&gt;decltype(a.x*b.x) { return a.x*b.x+a.y*b.y+a.z*b.z; /* could do N-D aswell*/ } ) its possible some of the use cases I have in mind get solved by 2-way inference, i still think predominantly in C++ terms unfortunately.
Yes, I saw that. :-) In fact, my post was originally only going to be about associated types, but when I saw that post, I found an opportunity to make some of the code clearer by also using parametric modules.
(are you pyon from freenode #programming ? i see parameterized mod in there, is this toward some of what you're missing from ML, but its similar to what we just do with classes in c++)
&gt; everything i've been interested in is compile time Oh, my bad. &gt; its just where and how they are declared; 'decltype' /'typeof' is something i find myself missing from c++ occasionally I like what you can express in C++. But I definitely do not like the way it is expressed in C++.
Yeah, that's fine... if it's marketed as such.
The first thing to do would be to profile (if you're on Linux `perf` works well, on Mac OSX Instruments.app is apparently good). At a guess `malloc` and `free` will be high on the profile due to the use of a lot of `Rc`. (I don't really have a great solution, though. If it is in fact the allocations, maybe you could use [some `TypedArena`s](http://static.rust-lang.org/doc/master/arena/struct.TypedArena.html) for `Term` and `Cont`.)
Looks like she'll work together with /u/dagenix on this, nice.
Looks nice! Just a minor comment: I recommend adding a section on licensing directly to `README.md`.
actually compiling with -O makes the time difference negligible! (All the `Rc` used to be owned pointers, per the suggestion in the rust pointer guide that they were suitable for recursive data structures, but all the cloning that seems to be required made things *very* slow.)
Glad we agree on word of mouth. It doesn't particularly matter how useful it is; the potential upside is extremely high, the potential problems near nonexistent. There should be no reason not to mention Rust in appropriate contexts. &gt; The problem with this logic, is most people don't end up knowing they ever caused a security issue, let alone one that rust could mitigate. Even if they don't realize it, at least if they can *imagine* causing a security issue, there'll still be an important use case for Rust. If they can't *imagine* causing a security issue in a language that doesn't try to save them from themselves, I hope that they're either using their genius to do good or that their wellbeing is looked after by others.
&gt; (All the Rc used to be owned pointers, per the suggestion in the rust pointer guide that they were suitable for recursive data structures, but all the cloning that seems to be required made things very slow.) If you're happy to share substructures of your recursive data structure, then, yes, `Rc` is good. (`~` is the appropriate thing for modelling something that's designed to not be shared (for such types, cloning is unavoidably expensive).)
/r/playrust
Awesome! Thanks for maintaining this!
&gt; Try to falsify your hypothesis. Have you noticed that for some tasks, some c++ programs are 10x slower than other c++ programs? Of course. And I exactly tried to avoid that question by saying "it *suggests* that Java is at most 4 times slower for any task *when given enough optimization efforts*". O(n^2 ) sorting algorithm in C++ is slower than O(n log n) sorting algorithm in Python when the input is large enough, I don't doubt it (and I would've been lying if I question it). You can still compare the fastest programs in given languages, e.g. O(n log n) LSB radix sort with OpenMP in C++ vs. O(n log n) [built-in adaptive mergesort](https://en.wikipedia.org/wiki/Timsort) in Python. &gt; Say what you consider to be "problematic" about it. The main problem with these problems is that in most cases the benchmark would measure the performance of high-performance libraries or bindings available to the target implementation, not the performance of the implementation itself. Imagine that icc (known to be more performant than gcc/clang sometimes) can't compile GMP correctly somehow and is marked several times slower than gcc/clang; it would be quite problematic (but still representative). This is a main limitation, or characterstics if you prefer, of the Shootout benchmark which people don't seem to grasp well, and which is bad for PR anyway. My original comment argued to make it undeniably fast by using GMP since it is the only feasible way to do that. &gt; "These are just 10 tiny examples." (I should first mention that I've negated the "in general" clause.) Shootout explicitly says so because it is *in general* dangerous to draw a concrete conclusion (e.g. the language X beats C++!). But Shootout is still a good way to see the general trend and characterstics of language implementations (e.g. the performance characterstics of the implementation X1 is similar to that of g++, let's look at why some programs are particularly slow) and that's why it is useful. Rust is still an evolving language, but its design goal and implementation implies that it should share the same performance characterstics as C and C++ in order to be widely used. Shootout is just one way to test this (and also a PR for masses).
They're open to this kind of thing depending on how you phrase it. Exhibit [A](http://www.reddit.com/r/netsec/comments/22gaar/heartbleed_attack_allows_for_stealing_server/cgn8foo) and [B](http://www.reddit.com/r/netsec/comments/22gaar/heartbleed_attack_allows_for_stealing_server/cgnb4ia).
Yes, `call_twice` and `function` inside `main` can be only used within `main`. (But they can't refer to other variables in `main`, unlike `closure`.)
Yeah, no difference.
This thread contains some interesting comments that suggest the primitives Rust would need to put in place to enable an implementation built with timing attacks in mind... https://news.ycombinator.com/item?id=7557089
Bitwise operations on types smaller than `int'.
One thing to keep in mind that might not be immediately apparent. Note how, at the outermost scope, you can refer to functions before they are defined: fn first() { second(); } fn second() { // It's legal to call me before I'm defined! } Well, this rule also applies to functions defined within other functions: fn outer() { defined_later(); fn defined_later() { // It's legal to call me before I'm defined! } } ...but this does **not** apply to closures: fn outer() { my_closure(); // error: what the heck is my_closure? let my_closure = || { // I have an environment, so I have to be defined before I'm called! }; }
Related question: is there any performance penalty for defining a function within a function? I would *imagine* not, since they aren't closures, but just figured I'd ask.
Looking through my comment history, I saw this, and felt like mentioning that there was a post on [pythonic iterator comprehensions](https://github.com/bsteinb/rust-iteratorcomprehensions) a couple days ago. I have not stepped into the magical lang of macros, so im not sure if you can implement the way Haskell has it, but it might be fun to try :)
You should try Rust sonner than 1.0. It's more stable than when it began and the fact that its last version is 0.10 does not mean it's far from the 1.0 milestone. Try it a bit to compare with go, that's why I moved from Go to Rust. I hated a few things in Go: the weird "slice" syntax, the fact that public methods had to begin with an uppercase letter, and the mandatory garbage collector. All those things that were not in Rust.
Why does a closure need a virtual call?
From the sound of it in that link, Rust is already way better suited for crypto than the other languages mentioned. If you believe the thread has added value and you have Twitter, you may want to consider tweeting the link to https://twitter.com/OhMeadhbh/status/453295192989130753 .
yikes, is that a double-dereference instead of a single dereference? i.e. in C terms, (closure-&gt;vtable-&gt;function)(closure-&gt;data,args..) instead of (closure-&gt;function)(closure-&gt;data,args..) if so is that permanent or something that can be optimized in a later implementation
When the compiler says `args[..]` instead of `args` it means it has created a slice of the owned vector. This happens in the match statement in order to pattern match. The slice borrows the vector, but is valid for the whole arm. You can not move `n` since you could (in theory) modify n in the same arm and break the rules of borrowing. Edit: To clarify, the lifetime of the slice includes the arms of the match statement, not just the pattern matching.
No, a closure is the latter, but it's essentially equivalent to the trait object (semantically).
&gt;&gt;"and stuff like type inference/checking is going to be a lot harder. " I guess one potential plus over C++ tooling is, with Traits you are going have the potential for 'dot-completion' in generic code 
Updated formatting (for clearer error messages): &gt; https://bitbucket.org/iopq/fizzbuzz-in-rust &gt; &gt; the code works as written, but when I change it to: &gt; &gt; [_, n, ..rest] =&gt; from_str::&lt;int&gt;(n).expect("I need a real number"), &gt; &gt; it gives me a borrow check error &gt; &gt; &gt; fizzbuzz.rs:46:7: 46:8 error: cannot move out of `args[..]` because it is borrowed &gt; &gt; fizzbuzz.rs:46 [_, n, ..rest] =&gt; from_str::&lt;int&gt;(n).expect("I need a real number"), &gt; &gt; ^ &gt; &gt; fizzbuzz.rs:46:3: 46:17 note: borrow of `args[..]` occurs here &gt; &gt; fizzbuzz.rs:46 [_, n, ..rest] =&gt; from_str::&lt;int&gt;(n).expect("I need a real number"), &gt; &gt; ^~~~~~~~~~~~~~ &gt; &gt; error: aborting due to previous error &gt; &gt; but when I change it to &gt; &gt; [_, ref n, ..rest] =&gt; from_str::&lt;int&gt;(*n).expect("I need a real number"), &gt; &gt; it just works (but warns about rest not being used) &gt; &gt; Why do I have to use ref n in the second case, but not the first? &gt; &gt; I'm running 0.10 on Windows 7: &gt; &gt; C:\Chocolatey\bin\..\lib\rust.0.10.20140218\bin\rustc.exe 0.10-pre (cae5999 2014-02-18 19:26:50 -0800) &gt; host: i686-pc-mingw32 &gt; 
Thanks for responding. I'm still confused. Can you demonstrate what the compiler does with Rust code? So the reason it works with `ref` is because it decides to use "move" semantics when it's not `ref n`. But after that I'm still very cloudy about what happens. Where can I change `n` and what borrow does it break?
`ref` creates a reference, so it doesn't move. Edit: 'reference', not 'pointer'.
I think it is fine to post here. A lot of people who are new to the language (myself included) follow the rust reddit feed. I think stack overflow may be more appropriate then IRC if you are going to do a big writeup with examples. It is nice to have this stuff preserved for the next person. http://stackoverflow.com/questions/tagged/rust I like how some people are tagging the version of Rust they are using since these posts may confuse someone in the future if the language changes before 1.0.
Whoa! Just realized.. how would you diff decimal point from full stop without operator overloading? :P
(`ref` creates a *reference*. Creating a pointer like `~` or `Rc` (if that were possible) would move.)
Surely an intentional shout-out to https://www.youtube.com/watch?v=Vppbdf-qtGU
was this an April Fools tweak? if so, it's April 9....
oh right, I forgot, it's Mozilla, just like the bugzilla notice about [Zarro Boogs](https://bugzilla.mozilla.org/buglist.cgi?quicksearch=i_hate_zarro_boogs).
quine00.unl from http://www.filewatcher.com/b/ftp/quatramaran.ens.fr/pub/madore/unlambda/CUAN/quine-0.html 
&gt; The main problem with these problems *Batteries included.* &gt; Shootout explicitly says Shootout? That was 7 years ago. &gt; a good way to see the general trend and … performance characterstics It's convenient (but can also be inconvenient). &gt; PR for masses It's a billboard which you can use to show-off Rust programs.
(/r/rust is entirely community run, fwiw. None of the mods are moz employees.)
At this moment there are people working on Cargo. You can see the official announcement here: https://mail.mozilla.org/pipermail/rust-dev/2014-March/009090.html
I'm having trouble even finding the source to install cargo. Is the source and an install/compile guide somewhere? Even just knowing where cargo is would be helpful. Right now I only know it is planned, but might not exist.
&gt; 6.&amp;nbsp;Leonhard Euler is infallible. Now .. &gt; 6.&amp;nbsp;Genndy Tartakovsky is infallible. Edit: relevant? &amp;#3232;_&amp;#3232; &gt; 3.&amp;nbsp;No memes.
Cargo is here: https://github.com/carlhuda/cargo Yes it's still active because it's supposed to replace the old rustpkg. But I'm just a newbie, I can't help you more. Rust is beginning and I wouldn't know how to write a server with it.
Ok. Thanks! I can get into building it myself then.
So I'm one of the people that responded to you in netsec and I just found this thread from HN. People are largely ignoring your statements because it requires a significant amount of time/money in a relatively unknown/unused language when compared to C. Or the fact that there is no existing crypto library that can be swapped for OpenSSL within Rust that has been vetted and okay'd. On top of the fact that unless someone who is seriously educated in crypto writes the library, it will most likely have to be rewritten for security concerns. I'm sure you read the comments above where someone remarked on trying to implement AES with only a couple options and it took them several months. To say nothing of their skill, it is more proof that crypto is not easy and claiming that things will be so much better in Rust, while ignoring everything else about the problem except memory corruption bugs, is ridiculous. How many platforms can rust compile to? How many cryptographers have experience with rust? Has anyone done research on other attacks Rust might enable? Just because it cures one, doesn't mean it doesn't introduce/cures more/other bugs. Just running around and acting evangelical for something like this does no one no good. This isn't some hip js lib, this is a lib that millions of people rely on, from their business to their lives. Just saying "OpenSSL should be rewritten in rust!" helps no one. Why don't you either design a crypto library or help someone design one, and then have it audited by several different firms, and then you can start saying let's replace OpenSSL with this new vetted library that has no memory corruption bugs. Because until then, to me, it's just ridiculous. 
Also to install Rust itself you can either find a package for your distro, or run the install.sh script from the nightly tarball (http://www.rust-lang.org/install.html)
While this seems counterintuitive at first, is this because the closure follows standard variable scoping and name resolution?
I've gone ahead and fixed this. Thanks for the heads up!
April Fools is passé. We here at /r/rust are far too dignified to partake in such tawdry foolishness.
yeah, the closure does just create a value, not a definition, so it follows the same rules like any variables in a function body.
Right. In fact, it might be easier to just think of this behavior as `fn` declarations being automatically hoisted to the top of their scope, rather than anything special with closures (which are just following the same rules as you'd expect).
Mmmm... programming cheese.
Omelette du fromage!
Note that it's still very, very new, so it's not going to be super functional at this point. But Yehuda built Bundler, so if you like the Ruby ecosystem, you will probably like what he has come up with. (We have also learned a lot from the failures of Ruby's ecosystem...)
Do we have a good explanation of `ref` anywhere?
I looked for it in the documentation and it said the keyword `ref` makes it a reference instead of moving the value. What I don't get is why I can't move `n` in that case and what kind of a slice it creates that prevents this since I never use `rest`.
I saw your other post, and while I agree that OpenSSL should be rebuilt/exchanged for another library in popular usage, there currently isn't one. People have been calling for the disuse of OpenSSL for a long time. It started as a person's personal learn-to-code project. It is rather that creating a replacement that is as "battle-tested" is really hard. Telling everyone that brings up the heart bleed discussion isn't necesarily doing good. Not everyone has coding skills, let alone the crypto skills and time necessary. I feel that if you really want to make something happen, you're going to need a lot of money or a lot of influence. As for using Rust specifically, it is not a stable language. It even says so on the homepage that it is a work in progress. Building a massive library on a rolling release is as I said, ridiculous.
[#11455](https://github.com/mozilla/rust/issues/11455) is a bug about a constrained form of return type inference.
I found another typo in the sidebar under "Discuss": &gt; \#rust (general questions) \#rust-internals (compiler dev) \#servo (web browser dev) \#rust-gamedev (**vidja** games) \#rust-osdev (operating systems) Although it looks intentional.
Thanks, that's a good suggestion, but I just checked on the bug and it's been resolved. I downloaded the latest Windows Installer and it has the update too. The code examples in this thread work now, happy days!
The code `[_, n, ..]` works because it's moving ownership of a `~str` out of an owned vector `~[~str]`, this "invalidates" the vector, i.e. the compiler will not allow you to read anything from the vector after that. In particular, capturing a slice into this now-invalidated vector (as `..rest` does) allows you to read from it. Theoretically the compiler could be taught that the are disjoint, that is, the slice never includes the invalidated element but we no longer have pattern matches on `~[T]`, only on `&amp;[T]` and fixed length vector. (I guess fixed length vectors may have the same problem.) &gt; C:\Chocolatey\bin..\lib\rust.0.10.20140218\bin\rustc.exe 0.10-pre (cae5999 2014 -02-18 19:26:50 -0800) host: i686-pc-mingw32 (This isn't actually 0.10, it's a compiler build from master at some point between 0.9 and 0.10. It is nearly 2 months old now; if possible, I would recommend [updating](http://www.rust-lang.org/install.html) to 0.10 or, even better, the nightly builds.)
I updated to 0.11-pre and of course it broke my code. &gt; fizzbuzz.rs:46:3: 46:13 error: unique vector patterns are no longer supported fizzbuzz.rs:46 [_, n, ..] =&gt; from_str::&lt;int&gt;(n).expect("I need a real number"), &gt; ^~~~~~~~~~~~~~ &gt; error: aborting due to previous error
Sounds good.. I'm wondering.. for a new pet-project, should i use zmq or nanomsg? Nanomsg seems to be the successor of zmq but also it seems to be relatively unmaintained?
Yup, need to explicitly coerce the `~[~str]` to an `&amp;[~str]` via `.as_slice()`.
OpenSSL has been audited by several different firms? Perhaps you should avoid those firms then.
zmq is pretty battle tested at this point. However, the original author of zmq went on to make nanomsg (https://github.com/nanomsg/nanomsg) which is definitely being maintained. (unless you're talking about the rust bindings for nanomsg)
Is there any way to call `u32::default()` or something similar?
Yes mod u32 { pub fn default() -&gt; u32 { 0 } } fn main() { println!("{}", u32::default()); } (Seriously though, [it's covered by this RFC](https://github.com/rust-lang/rfcs/pull/4).)
Ha! Cunningham's law strikes again! That's what I get for not checking beforehand ;)
Deliberate indeed, though I cannot let such a keen eye go unrewarded. CSS hast been delivered unto thee!
I know what nanomsg is but the last time i checked whether i should go with zmq or nanomsg the nanomsg project looked amost dead, people asked on the mailinglist if Martin Sustrik is still active... The commit activity in Dec. 2013 - Feb. 2014 is like dead: https://github.com/nanomsg/nanomsg/graphs/commit-activity But it seems to have increase again, good! 
I don't quite understand how reducing the amount of information immediately available to the compiler (by adding *more* inference) helps an IDE/autocompletion.
&gt; I don't understand how reducing the amount of information immediately available to the compiler (by adding more The kind of autocomplete I'm thinking of here works forward, the compiler needs to compile something incomplete - the issue is it wont compile if it doesn't have a complete source fragment at the minute (your function doesn't yet produce the right output..)? Also *you* dont know the types yet :) thats why you wanted complete to help :) Autocomplete helps you find them in the first place, by bringing up the calls you might need, and showing you what they return.. its about navigation and discovery, not reducing keystrokes. The time autocomplete is useful is when you have to dig into nested structures.. it's looking these up as you write, saving you a lot of navigation time. My worry is that 2 way inference will make this kind of autocomplete harder.* of course there might be better ways to solve this, a flag that allows it to compile more without all the types actually matching up, and still return as much as it could figure out I realise inference itself does some of the work for you - I still miss autocomplete when navigating someone elses large codebase.. the IDE is performing a smart search on the fly as you type, its incredibly useful. I think the power of "dot-completion" its actually the main reason OOP languages get used, write a parameter then the IDE tells you the available functions.. I've not extensively used languages with 2 way inference, if there *are* IDEs that exploit it it would be interesting to see. I imagine that autocomplete and 2 way inference help in different situations. I have thought of trying other partial hacks like looking for all the other functions that just contain types from the function signature, rank all the functions based on how many of the signature types they have. Of course that would be a different way to leverage the information you've specified in the signature. And its nice that trait bounds mean we could potentially get autocomplete in generic code where we wouldn't in C++
Ahh, my mistake.
There's a cross-platform inotify wrapper is in libuv: https://github.com/joyent/libuv/blob/master/include/uv.h#L2030-L2037 So it would be better, if librustuv had these functions, rather than having inotify wrapper in a separate library.
There is some file-related stuff in rustuv that I'm aware of: http://static.rust-lang.org/doc/master/rustuv/file/struct.FileWatcher.html Before writing inotify-rs, I tried to figure out how to use it, but I got the impression that rustuv is really a part of the Rust runtime and not intended for stand-alone use (please correct me, if I'm wrong). I considered using libuv directly, but ultimately decided that using inotify is much quicker and easier for my immediate use case. I'd like something cross-platform eventually, but for now this is good enough.
You need to create a vector of trait objects. It is covered in [this section of tutorial](http://static.rust-lang.org/doc/master/tutorial.html#trait-objects-and-dynamic-method-dispatch). For example: let obj_collection = vec!(&amp;a_obj as &amp;GetSharedValue, &amp;b_obj as &amp;GetSharedValue, &amp;c_obj as &amp;GetSharedValue); It's a little bit verbose. I hope that it will change.
Personally way above me. But I agree. Both will have a place. Heck even brainfuck has a place. Looking at it, rust seems to embrace the "pure and functional" idea more. Static types. Look at the fizzbuzz example that sets a variable to the output of a few if/elses. D realizes it won't be prefect and is a little less pure for added syntactic sugar. Of course rust is younger too. Look at the examples on Rosetta code to compare the two. You can see generally idiomatic ways in each language to do things. I'm less concerned with concise and fast code (but speed is still up there) and more with ease of installation on a brand new VM or something. 
Aha! A lightbulb just turned on above my head. Thank you. That section of tutorial and [its corresponding section in the reference guide](http://static.rust-lang.org/doc/master/rust.html#traits) are densely packed with information. I remember reading that but it never really registered until now. To anyone else coming along with a similar issue, casting it like this also means you can only call methods available in the trait. Other methods attached to the object, such as those on the base object or another trait, will fail: let obj_collection = vec!(&amp;a_obj as &amp;GetSharedValue, &amp;b_obj as &amp;GetSharedValue, &amp;c_obj as &amp;GetSharedValue); for collection_object in obj_collection.iter() { println!("collection_object.base_object_method: {}", collection_object.base_object_method()); // fails }
It was even worse when you had: impl&lt;T, U&gt; Foo&lt;T&gt; for Bar&lt;U&gt; { fn bar&lt;A&gt;() -&gt; Bar&lt;U&gt; { ... } } Foo::bar::&lt;T, A, (┛◉Д◉)┛┻━┻&gt;()
/r/playrust
That's not how you'd do it in Ruby, there are tab characters there! :p ;) I, like /u/krdln, also hope that this could change. I don't see why it's impossible to infer this type information, but I am also not a type theory master...
I think there should be a way to get the file descriptor to use it with poll/select.
Thanks for developing this! I'll probably use it in the VCS I'm writing (to avoid needing to write a "moved" heuristic).
I've seen a couple of places where people have resisted calling this sort of thing 'inference', preferring to call it 'deduction' instead. I don't want to start an argument, but if you fall in this camp and would care to explain why you make this distinction in terminology, I'd appreciate knowing where this comes from.
Eventually I'd add a heuristic for it, but if the user is running a daemon to watch the respository, worst case is I'd miss a move and call it a delete/create/modify, or make a way-too-big diff (if two unrelated files were swapped), right? If I can't trust modification times and the user doesn't want everything to be re-hashed all the time, then there aren't really any options. Are there some issues I haven't considered? I'd love to know, since I only have so much time to do this project.
At the very least we could define a second macro that wraps around `vec!` and does away with the verbosity. Somethig like `macro_rules! vec2(($cls:ident, [ $($obj:expr),+ ]) =&gt; vec!($( $obj as &amp;$cls,)+) )`?
I don't know what the worst is that can happen; I've just seen others advising that *depending* on inotify is hazardous.
Thank you so much for this. I stumbled across it a few days ago when I picked up Rust and it made my life much easier.
I'm not the guy you asked directly, but SublimeLinter works well with highlighting and autocomplete.
&gt; As an experiment to see if a safer systems programming language could have prevented the bug I tried rewriting the problematic function in the ATS programming language. It would be interesting to see how rustc would react to an 1:1 translatation.
I don't think it'd be possible without a lot of unsafe code, which sort of defeats the purpose.
I'm not *that* familiar with Servo's development, but this talk seems quite a bit outdated, despite the very recent posting. There's no date I can see, but this talk must have been given months ago. Among the things that have changed are the style system, Acid 2 support, and even more work that has gone into parallelism throughout, especially in layout.
A direct one-to-one translation using raw pointers can't avoid `unsafe`, yeah; but going even just one step up (to a slice, which is just a raw pointer + the buffer length) would likely reduce the `unsafe` required dramatically.
Thanks for pointing that out. I think it's a recording from Strange Loop in September 2013.
Makes sense, thanks! I don't think I'll get to it today, but I'll make it public.
A rust version would be interesting. I suspect the borrowing capability of Rust would make some things nicer. In particular code like: prval (pf, pff) = extract_hbtype_proof (pf_data) val hbtype = $UN.cast2int (!p_data) prval pf_data = pff (pf) In Rust you don't need to obtain the proof and dispose of it. ATS provides more type machinery than Rust at the cost of more syntax and annotations. Rust has features like borrowing built in. One of the main reasons I went for ATS in the article was being able to integrate with C easily and demonstrate being able to replace a C library function with a safe version. You can do this in Rust but it's a bit more work.
Yeah, it's a shame infoq posts these talks so late, especially for an emerging language like Rust. Thanks for pointing this out.
Rust also supports 2 byte and 4 byte Unicode escape sequences. e.g., println!("wat\U00000008wat"); println!("wat\u0008wat"); 
Excellent, that works, thanks! I didn't think of that.
Thanks, this works too, as long as the counter is on the left side of the screen.
A great name for a Rust fork with a faster development pace.
Here's my stab at reproducing it as faithfully as possible. I didn't include the ClassPerson since I think you would have to use an reference counted or garbage collected pointer I think. I'm newish to rust so if anyone sees any bad habits or mistakes please point them out. As for when rust free objects, everything in rust has a unique owner (unless you use a different pointer than ~), everything is freed as soon as that unique owner falls out of scope. I believe that anything is not a pointer is allocated on the stack. extern crate sync; use sync::DuplexStream; #[deriving(Clone)] struct StructPerson { name: ~str, age: f64 } enum PersonMessage { Person(StructPerson), Finished } fn main() { let mut sp = StructPerson { name: ~"Struct Person", age: 10f64 }; let mut sp_dup = sp.clone(); println!("Struct Person 1: {}, {}", sp.name, sp.age); println!("Struct Person 2: {}, {}", sp_dup.name, sp_dup.age); sp.name = ~"Changed Name"; sp.age = 100f64; sp_dup.name = ~"Changed Dup Name"; sp_dup.age = 200f64; println!("Struct Person 1: {}, {}", sp.name, sp.age); println!("Struct Person 2: {}, {}", sp_dup.name, sp_dup.age); let (people, child) = sync::duplex(); spawn(proc() { adder(&amp;child); }); for _ in range(0, 10000) { // I'm just cloning here for simplicity, since sp is 'moved' // if you don't want to do a clone, you could use an arc people.send(Person(sp.clone())); } people.send(Finished); let total = people.recv(); println!("Sum: {}", total); } fn adder(people: &amp;DuplexStream&lt;f64, PersonMessage&gt;) { let mut sum = 0f64; loop { let message: PersonMessage = people.recv(); let finished = match message { Person(p) =&gt; { sum += p.age; false }, Finished =&gt; { people.send(sum); true } }; if finished { break;} } } 
&gt; // Gc collects heap data here &gt; &gt; // When will rust free class-based sP? Rust doesn't use a GC, the memory is free'd as soon as it goes out of scope. If you want garbage collection you can use std::gc::Gc&lt;StructPerson&gt;. There is also std::rc::Rc&lt;T&gt; for reference counting. If you want mutability with these I think you have to use std::cell::RefCell as well.
Op, for the ClassPerson example, you could do let mut sp = StructPerson { name: ~"Struct Person", age: 10f64}; let sp_dup = &amp;mut sp; as long as sp_dup doesn't outlive sp.
OK, thanks for explaining! I think this is actually different than other situations I've seen, and (assuming I understood you correctly) I agree that this isn't what you'd call type deduction or inference (and I understand those to mean basically the same thing). So, I'm going to re-state what you explained, to see if I understood correctly. `fn foo() -&gt; _: Iterator&lt;Bar&gt;` is a function with no parameters that returns a value of *any* type that implements the trait `Iterator&lt;Bar&gt;`. So, what you're really describing is a parametrically polymorphic return type with an additional type constraint requiring that the trait is implemented. I think an analogous Haskell type would look something like: `foo :: (Iterator Bar a) =&gt; () -&gt; a` which I think makes it a bit clearer that the type itself is fully specified, but it's a constrained polymorphic type rather than an uncertain concrete type. I'm still learning the Rust type syntax, so I can't offer any suggestion as to another way to express that in a Rust-like syntax. I'm not sure if "type erasure" is really going on in Rust at all as far as I understand it. Possibly it is for reference types, but it's my understanding that pretty much all polymorphic types are monomorphised during compilation. Type erasure would imply that the generated code itself remained polymorphic; i.e. the same object code works for all type-correct values that could be passed to it. This is doable for 'everything is boxed by default' languages, since all values are represented with the same run-time representation, i.e. a pointer; it's not so straightforward in a language that deals with unboxed values by default. I would call it a 'trait-constrained polymorphic return type' instead, though that's a bit of a mouthful.
If I even add let sp_test = &amp;mut sp; I end up with 29:60 error cannot borrow `sp.age` as immutable because `sp` is also borrowed as mutable and a couple of other borrow errors how would I fix this?
Thank you.. couple of questions - * name: ~str : Why is the ~ required? * Shouldn't for loop be outside spawn? * Arc was what I was looking for. So in Rust, whenever I want shared I need to specifically use one of these? * This send-recv is like a light weight WCF ? Is it like tcp or mem shared end points? * One major difference I see is - adder is like a thread that keeps running until it gets finished (as opposed to just a function that can be called whenever I like. Is there any other way to do this?) Thanks.. you've been very helpful :)
I think I get it now. The pointer which points to heap is owner. When the stack based pointer goes out of scope, the underlying heap is freed. Major shift in how I think, must get used to this... 
It works for [this](https://gist.github.com/anonymous/3b1d36078ccd8ffb3937) minimal example. I'm not really sure whats wrong with the rest. I'm quite new to rust too. What's line 29?
The struct takes a unique pointer to a string, I could make it take a reference you would just need to change the struct and enum declarations to: struct PersonStruct&lt;'a&gt; { name: &amp;'a str, age: f64 } enum Message&lt;'a&gt; { Person(PersonStruct&lt;'a&gt;), Finished } then you wouldn't need the ~ I think this way is far uglier though. The 'a is a lifetime, it means the string must be a reference that must last as long as the struct's lifetime, 'static is a special lifetime that means the reference must be valid for all of the program. Any string literal such as "hello world" is by default of type &amp;'static str I believe. spawn is effectively creating a new thread (rust provides m:n threading as well as m:m). So the for loop will start executing straight away after the call to spawn. I hope this clears up your question. Arc and unsafe are the only way to share memory between tasks I believe. I think it is better to share memory by communicating unless you have performance/memory issues however, as I think it is clearer. I'm not sure what WCF is, they are effectively equivelant to go channels, I recommend reading the rust tasks and communications [doc](http://static.rust-lang.org/doc/master/guide-tasks.html) or searching for tutorials on go channels since they are very similar. I'm not aware of any other ways to do adder, perhaps someone else does however.
I think it's because sp is borrowed if you create a reference to it, so trying to access sp after that until the reference is out of scope is a compile time error. Taking a reference and modifying the reference is perfectly fine, just modifying the original at the same time is not, hence why your example has no compile errors. Line 29 is just the first error I read, any line trying to access sp or one of it's fields gives a compile error.
Yep. Similar to how std::unique_ptr&lt;T&gt; works if you've used C++ before.
Also, the GC doesn't necessarily collect the heap data at that point. It might never collect the memory if it doesn't run before the process exists. In rust the memory is guaranteed to be freed as soon as the pointer goes out of scope.
Ah, yeh. I had forgotten about freezing. I think wrapping it in a RefCell&lt;&gt; would work.
Since talks related to Servo &amp; Rust are rare I decided to watch it anyway, despite its age. I'm glad I did. It was a good talk. Josh Matthews gave a more recent talk at FOSDEM 2014 that contains much of the same material, but he talks at such a brisk pace that my brain couldn't keep up. This older talk by Jack Moffitt isn't so rushed, which I found helpful. YMMV.
Yup. I just meant it's in GC's domain after that, we no longer have any control..
So what's the boost's shared_ptr equiv in rust?
Ok. Lil confused about the m:n statement here.. spawn is creating 1 thread or 10k threads? 
10k threads? Why would it spawn 10k?
It used to be `@` which was removed but have a look at std::rc::Rc for refcount.
&gt; but he talks at such a brisk pace that my brain couldn't keep up. Noted. Thanks for the feedback!
What would you do if you wanted the Person struct to last longer than the &amp;str it was taking in? I have a similar problem and I can't figure out how to copy the string to store it there.
In that case you need to use an owned pointer or similar, a reference can't do that kinda of thing because it doesn't control the lifetime of what it points to.
If I understand correctly, it is because trim() returns a reference pointing to a slice of the same string (i.e. no extra heap memory is allocated). The &amp;str type is a so-called fat pointer which points to the start and end of the substring. When you call to_str() on the reference, a new copy of the substring is created and an owning pointer returned.
hmm. so its going to mean an extra runtime check, to save those bytes *safely*. But only where a reference is added (you can still borrow Rc&lt;T&gt;'s, right) if it does mean changing performance characteristics .. doesn't sound like behaviour you'd want in the default Rc&lt;T&gt;. having said that, a lot of the time size *is* performance (caches).. but others it's reducing branches and checks..
There is an [RFC](https://github.com/rust-lang/rfcs/pull/23) about the smaller reference-counted smart pointers (see also the linked original PR, which also has a conversation about this matter).
it's cheaper to take a ten way branch than to access memory (hundreds of cycles usually)
Nice job. You don't really need to manually dereference a pointer before calling a method on it, or accessing a field (e.g. `(*sP).name`). The language automatically takes care of that (just write `sp.name`). 
And this is advantageous because it gives you control over the memory behavior you want to use. See what Oracle did in a recent JVM release. They changed the behavior of `String.substring()` to return a copy of the string, instead of a reference to the substring inside the parent string. They were claiming this was due to "memory leaks", as the GC would not be able to collect the parent string, since someone was already referencing this. Of course, a solution was to manually copy the string after calling substring(); however, due to one reason or another, most people did not do this. In Rust, the type system is able to convey this behavior explicitly, unlike Java where everything is a reference. In Java, `String.trim()` already returns a new copy of the string: http://docs.oracle.com/javase/7/docs/api/java/lang/String.html#trim(). Therefore, you don't have as much control if you want to avoid an allocation.
Offtopic, but just curious about "Atomics are expensive especially on ARM architectures". Do you run "Functional and Cycle Accurate simulators for Deeply pipelined Hardware Devices" on ARM? I find that surprising.
I posted [a comment inspired by this](https://github.com/rust-lang/rfcs/pull/23#issuecomment-40298511) on the RFC.
Ahh I see, I get it now. Thank you both!
&gt; [a] Is there an easy way to dump out a particular Tasks' heap? Since each Task appears to have its own Heap? There aren't really task-local heaps. The language just has a concept of sendable types (`Send`). Values are sent to other tasks via a shallow copy either by capturing a value in a `proc` or passing it to a function. Rust certainly has shared memory between tasks, just no data races outside of `unsafe`.
What numbers are you seeing? I'm getting: Python $ time python unlambda.py quine00.unl &gt; /dev/null real 0m1.073s user 0m1.065s sys 0m0.008s Rust (no optimziations): $ time ./unlambda quine00.unl &gt; /dev/null real 0m0.916s user 0m0.907s sys 0m0.008s Rust (-O and --opt-level=3 yield effectively the same time): $ time ./unlambda quine00.unl &gt; /dev/null real 0m0.130s user 0m0.126s sys 0m0.004s I couldn't get the Haskell version to compile. What should I run beside `ghc unlambda.hs`?
that looks great, I guess the compiler could be trusted to eliminate unnecessary checks when its uint==uint , . especially nice if ()-ing out the second refcount work aswell.
Docs: http://burntsushi.net/rustdoc/regexp/index.html
That would stop one being able to write `static RE: Regexp = re!(".');` currently. (It's slipped in toward the end of the RFC, but the fact the static is needed is [just a bug](https://github.com/mozilla/rust/issues/11640).)
Rust is getting better every day.
It is not a successor. 0MQ goes on, and nanomsg is a separate reimplementation because of 'artistic differences' between developers.
Source code is [here](https://github.com/dwrensha/acronymy). Note that Acronymy speaks Cap'n Proto, not HTTP. It connects to the outside world through the [Sandstorm](https://sandstorm.io) platform. 
This was harder than I thought. Brilliant.
This is really fun!
I would also add that despite SML being the 'truest' descendant of ML in use today, the original ML [seems to have been a very simple language](https://github.com/bjz/glfw-rs/pull/108). So things that we think of as very 'ML-ly', like the module system were added in the dialects. So despite Rust's use of typeclasses for stucturing programs, systems language features, and the additions of linear types and regions; the real differences between Rust and ML seem to boil down to: - vastly different syntax - no support for exceptions - lack of whole-program-inference - lack of tail recursion - lack of a well defined, formal foundation The rest is left intact. Granted, those differences really change the feel of the language, and those alone are probably enough to bump Rust out into the 'extended family'. Bear in mind I'm not the best person to ask about ML history, so please call me out if I have something wrong.
This is awesome, thanks for sharing!
You may however find something interesting here: /r/AbandonedPorn/ ... or perhaps here: /r/playrust
There's even some [pictures of Rust](http://www.reddit.com/r/rust/comments/1tcat1/i_etched_myself_a_rust_logo/) here as well!
Python: coelacanth ~/src/projects/unlambda 09:47:16 $ time python unlambda.py ~/bin/quine00.unl &gt;/dev/null real 0m2.423s user 0m2.400s sys 0m0.008s pypy-compiled python: coelacanth ~/src/projects/unlambda 09:47:23 $ time ./runlambda-c ~/bin/quine00.unl &gt;/dev/null real 0m0.102s user 0m0.060s sys 0m0.040s Haskell (compiled with `ghc --make unlambda.hs`, but it might complain about missing dependencies): coelacanth ~/src/projects/unlambda 09:47:39 $ time ./unlambda ~/bin/quine00.unl &gt;/dev/null real 0m0.255s user 0m0.240s sys 0m0.012s Rust: coelacanth ~/src/projects/unlambda 09:47:43 $ time ./rustunlambda ~/bin/quine00.unl &gt;/dev/null real 0m0.280s user 0m0.276s sys 0m0.004s 
Thanks for writing!
How does this differ from [collections::bitv](http://static.rust-lang.org/doc/master/collections/bitv/struct.Bitv.html)?
bitv only does one-bit slices, whereas this will do any number of bits. that is, bitv offers access to it as if it were an array of bits, rather than an array of bit slices.
Thanks! This is great!
This weekend I grabbed this Makefile and used it for my Rust playground where I'm working through a few problems from Project Euler. I hadn't bothered to create my own Makefile, so this was a considerable improvement over what I had before. I couldn't figure out how to have a conversation with rusti, however. No matter what I typed I'd always see this: error: Argument to option 'o' missing. Are there any example rusti sessions out there on the web? I couldn't find any. 
Fixed the bug. Thanks for noticing!
This is... genius. Truly.
I've never been a fan of the whole 'tao' format, but this one is decent anyway.
If you need only borrowed vectors, you should use slices instead (of course if you don't need growing). I would use a helper function like that: fn as_u8s&lt;'a, T&gt;(vec: &amp;'a[T]) -&gt; &amp;'a[u8] { unsafe { std::cast::transmute( std::raw::Slice::&lt;u8&gt;{ data: vec.as_ptr() as *() as *u8, len: vec.len() * std::mem::size_of::&lt;T&gt;() } ) } } And then you can rewrite `write_u64` as fn write_u64(to: &amp;mut std::io::File, items: &amp;[u64]) { let written = as_u8s(items); match to.write(written) { Ok(_) =&gt; (), Err(e) =&gt; fail!("writing failed: {}", e), }; } edit: `transmute_copy` may work with borrowed pointers, but in your case it may lead to lifetime unsafety issues.
You shouldn't do what you're doing, Vec stores its length and capacity in number of elements, and when you cast it like that... it will break. Try accessing the last element of the buffer you get out of read_u64, I would expect that to segfault... If you really must do it unsafely, I suggest buffer.shrink_to_fit(); let len = buffer.len() / 8; let read; unsafe { read = Vec::from_raw_parts(len, len, buffer.as_mut_ptr() as *mut u64); std::cast::forget(buffer); } And you can do that to convert back to Vec&lt;u8&gt; later... Edit: minor corrections to code.
"Lang items" are the primary mechanism of integrating the language with its libraries. There are a fixed number of library items with special meaning to the compiler, including traits like `Copy` and `Send` that have special static semantics, and functions that implement runtime semantics required by the language, like task failure. Lang items may only be defined once globally. The RFC is addressing a problem with the `fail` lang item that implements task failure. The language semantics demand that `fail` exist because there are core language features (like array indexing) that might fail, but failure as task unwinding can only be defined if the runtime `std::rt` is defined. There are scenarios for using Rust that do not want any runtime at all or want their own behavior, and we want as much of std to work in those scenarios as possible. "Weak" lang items will let a large portion of the standard library to be defined independently of the definition of `fail!`, so that e.g. it can be used in kernelspace.
"Lang item" is not a standard technical term, though I gather a similar technique is used in Haskell.
No, it doesn't segfault. I've tried to write and it writes it out cleanly. But you're right, /u/cmrx64 told me on the IRC to use slices instead of vectors. So I did. I will post the revised code soon, it is currently going through unit-testing.
Thanks, your helper function looks Interesting. Is there a good intorduction to these 'a lifetime specifiers? I've ended up doing something a bit more hacky, which seems to work as well. But I really like your helper. Is it possible to take another type parameter and parametrize the u8 out, so I could use it also as a helper for u8 to u64? Then I could just throw away all manual casting code.
I guess this passes for a more general solution: fn castVec&lt;T:Copy,U:Copy&gt;(v:Vec&lt;T&gt;)-&gt;Vec&lt;U&gt;{ use std::mem::size_of; use std::cast::forget; let (sT, sU) = (size_of::&lt;T&gt;(), size_of::&lt;U&gt;()); let bytes_len = v.len() * sT; assert!( bytes_len % sU == 0); let mut v = v; v.shrink_to_fit(); let len = bytes_len / sU; let result; unsafe{ result = Vec::from_raw_parts(len, len, v.as_mut_ptr() as *mut U); forget(v); } result } Still, it's not a good idea :p
Yes, that's because I don't have a proper Setup.hs for the haskell file, because ... well, for no good reason, really, so you have to manually install the dependencies. Sorry, it's a hassle, I know. You can generate the pypy-compiled interpreter with the translate.py script in the pypy distribution on the file runlambda.py; it creates a binary called runlambda-c. See the section "translating" [here](http://morepypy.blogspot.com/2011/04/tutorial-writing-interpreter-with-pypy.html).