&gt; Well, that's the catch, isn't it? I don't know if that's semantically what I'm trying to achieve or not--I just know that I couldn't get it to compile otherwise. I meant at a high level: are you trying to generate random dice rolls where the registered numbers are shared across all requests? If so, you'll need shared memory (e.g. `Arc`) and it needs to be mutable (e.g. `Mutex`).
`You'll just have to get used to speaking only in monospace, I fear.`
Ok, some context: I'm maintaining the deque crate, and on a recent nightly one of the tests began reporting panics because of overflow in lib.rs:392. I'm guessing that WrappingOps provides the old pre-panic behavior?
You can really just wait until tomorrow, I'll surely be bored of this by then. :)
Humans are really quite *fascinating* creatures...
The use of `Arc` is redundant/unnecessary here, since `Handler`s just have to be `Send` and `Sync` (which `Mutex` already is), and iron will wrap your main `Handler` in an `Arc` within the implementation anyway. You *can* use a `Mutex` like this, but it's a bad idea since it kills your ability to handle more than one request at a time. If at all possible you want to avoid having a high-contention (every request needs the lock to be handled) mutex like this as a bottleneck in your server. Also, as mentioned elsewhere in the thread, *please, please* use meaningful status codes. I'm not sure what else you've heard, but it's almost universally considered bad practice to always send 200. For reference, the `Status` type is documented [here](http://ironframework.io/doc/iron/status/enum.Status.html). You mentioned a bit of trouble finding code and docs: all the relevant source code for iron and the underlying http implementation (hyper) is located [here](https://github.com/iron/iron) for iron and [here](https://github.com/hyperium/hyper) for hyper. Documentation for iron in general can be found [here](http://ironframework.io/doc/iron/) and documentation for hyper can be found [here](http://hyperium.github.io/hyper/hyper/index.html). For the most part you don't need to know about hyper when you use iron, since everything relevant is re-exported, but the source code for those things can only be found in hyper, not in iron.
I have to say, whoever is in charge of the "readers"/"users here now" section of this sub is awesome.
The CSS maintainer has made me have a new appreciation for the importance of typography for the usability of a site. Can we now remove the caps from posts?
/r/rust+null
small nitpit (from the threading section): &gt;`fn scoped&lt;'a, T, F&gt;` a function that will use generic parameters T and F, and lifetime parameter 'a. We are making up names here; they do not mean anything yet. I think the correct terminology would be "... that will use two generic type parameters T and F, and one generic lifetime parameter 'a." Lifetimes are generic too! 
I don't really like must-use returns. Maybe a warning of ignored returns is appropriate, but like he mentions in his video, other "must-check" behaviors of other languages is a real annoyance (Cough Java exceptions), and will probably lead to a lot of assigning return values to variables that will never be used.
I think 'must'-use returns are pretty great (as implemented in Rust, anyway), as long as they're used selectively and with restraint, and the restrictions imposed aren't too onerous. For example, Rust's `std` marks the return value of functions like `some_iterator.map`, `std::thread::scoped` and `Mutex::lock` as must-use, since ignoring them is *never* useful (literally never, I can't think of a situation in which one would want to call those functions and not use the result). Similarly, we mark `Result` as must-use as ignoring errors is usually not desirable, and can lead to subtle bugs (e.g. the alpha-1 release initially didn't have any CSS for the documentation because of some code with `// TODO: handle errors`). Ignoring those will cause the compiler to emit a lint warning (disable-able with `#[allow(unused_must_use)]`, error-ifiable with `#[deny(unused_must_use)]`). Of course, misuse is definitely annoying. We certainly are careful about what we mark `#[must_use]` in `std` to avoid too many false positives/unhelpful warnings.
You should look into it, in case it’s a sign of a real bug. If you’ve determined that you do want the wrapping behavior, then yes you can use methods from the `WrappingOps` trait or the `Wrapping`… er, wrapper type.
Hi, nice to meet you here! I'm wondering whether it was hard to follow the changing landscape of the Rust compiler in terms of maintaining the package. The macro system is feature gated because it can change, but how much it's really changed over time and how rocky do you thing it will be after 1.0? Your comment, "*I think it's fine to work on syntax extensions now*", probably answers all of these. Thanks. )
Fine, subreddit styles enabled, for today only.
Both have the percentage calculation, I forgot to omit the pasted code here. Calculating the percentage is negligible compared to the calculation done while traversing the octree for each pixel, so it does not really affect the performance, besides I want to see progress percentage.
Looks good!
In essence, you're handing `test` a mutable reference to `a`, so there is no safe way to *run* `test` (because it mutates `a`) unless `test` itself is mutable as well. What's happening in the background: * By mutating `a`, you indicate to the compiler your closure implements FnMut. (It could *not* implement Fn, an immutable closure, when mutating its environment, or it would get an error roughly like [this](http://is.gd/PMDCVp).) * By implementing FnMut, when you call your closure you are calling call_mut() on it. You can read about it and more [here](http://doc.rust-lang.org/std/ops/trait.FnMut.html). * call_mut() uses `&amp;mut self` as the receiver. Therefore `test` must be mut.
Thanks!
Nice Friday morning read :) &gt; Instead, there are three "reference wrappers" that turn a value type into a reference type. Please mention that they are library features and not built in (like &amp;, &amp;mut, *const and *mut). And i think that the word "three" should be avoided when talking about Rust's pointers :p &gt; Rust has a final reference wrapper type, called Arc. It is better to assume that this is nothing like the Swift technology ARC. Can you elaborate on this please? How does Swift's ARC *technology* work? how is atomicity provided (mutexes or atomic ops)? and is the data guarded by a mutex? &gt; In C for example, when we say "pass-by-value" what we really mean is "pass-with-copy". Whereas pass-by-reference is "pass-without-copy". And so people pass references around in C, not necessarily because they have to for some semantic reason, but because it is fast. &gt; **However, this view does not hold true for Swift and Rust. "pass-by-value" can be zero-copy (if it's immutable), or it can be copy-on-write (if it's mutable), or something like that** Is this true? If compiled with LLVM, what would prevent C code from being optimized like that? &gt; Closures Why not provide a working solution? Here is [a single threaded one](http://is.gd/5BcauT) which preserves the semantic of the Swift code. 
Didn't think I would notice the easter egg, did you? &gt; Rules: &gt; ... &gt;6 . There is no justice. There is just me. &gt; ...
Kudos to the author for writing this blend of an experience report and a tutorial. I have a couple of comments, though. I wrote them because I wanted to send them in private to the author. But I guess, this is also not a bad place for sharing them. I've sent him an email anyways to make sure the author is aware this thread. How is it? ---------- You say you spend 25% time wrestling with the compiler to achieve safety. That's probably because you're still learning the language. The restrictions are still alien to you. I recommend [Niko's talk](https://air.mozilla.org/guaranteeing-memory-safety-in-rust/) about memory safety which also covers the principles behind it (mutation versus aliasing). For me, it quickly started to make a lot of sense and I'm not close to spending 25% of the time wrestling the compiler on that anymore. It feels like almost 0% by now. Methodology ----------- You have a point. People who programmed in Rust for a long time sort of forget what the struggles were. On the other hand, these people have a broad perspective on things. They already have a working mental model of things and can try to share its essence. So, there is value in every perspective. Rc &amp; Arc -------- You say an Arc's clone method makes use of a mutex. This would be too much of an overhead. Mutexes are something that requires cooperation with the OS and its scheduler. For reference-counting simple atomic increments and decrements are used. This is handled at the machine level. There are typically CPU instructions for this kind of synchronization. Also, you should probably say a word or two on the fact that `Rc&lt;T&gt;`s and `Arc&lt;T&gt;`s only give you access via a `&amp;T` whereas `Box&lt;T&gt;` can also give you mutable access via `&amp;mut T`. This has something to do with the core principles of Rust: avoiding sharing and mutation at the same time. The sooner these principles sink in, the less you'll struggle with the Rust compiler. Closures -------- Word-by-word translations from language X to language Y are generally an issue. Prefer to ask yourself "How do I solve this specific problem in Rust?" instead of "How do I translate this Swift code to Rust?". You started from a Swift design and tried to translate this design to Rust. There is a chance that there is a different design that makes more sense in Rust. The list of error messages is so long, because you made a lot of errors in Rust. IMHO, situations like these demonstrate the need for an experienced user to explain the principles behind these kinds of language constraints. In this case 1. You want both closures to _share_ a string variable in a way so that one closure could _mutate_ it. This goes against Rust core principles: Avoding the combination of sharing and mutation. It would not be safe as is. But you could use a Cell or a RefCell wrapper for your string variable to make it safe. A RefCell uses dynamic borrow checking and a Cell prevents references into the Cell's interior entirely so nothing could go wrong. 2. You left the str variable on the stack frame of the test function (lifetime issue). Your Swift program probably allocates it on the heap implicitly. Since you want sharing, you need an Rc or Arc for this. An Rc would suffice. An equivalent Rust program might look like this: use std::rc::Rc; use std::cell::Cell; fn main() { let test = || { // allocate the "environment" on the heap // in a Cell for safe sharing+mutation. let x = Rc::new(Cell::new("Test")); let y = x.clone(); let w = move || { // move x x.set("Whatever"); }; let r = move || { // move y println!("{}", y.get()); }; (w, r) }; let (write, read) = test(); read(); write(); read(); } To be honest, I don't like the idea of a compiler implicitly generating code for allocating a closure's environment on the heap. It's sort of a hidden cost that should probably not be there in languages you want to use for performance reasons. 
BEST Rust tutorial ever!!
 ------ 0 row(s) selected.
He means that Blow's language summons Great Unclean Ones, from the Warp.
Named args make the argument names part of the public API, so that could be seen as bad, and as a subtle way to break an API. Default arguments can increase complexity. I've been bitten by them before in an app I developed, and the problem was coupled with poor documentation. They could also lead to subtlety broken APIs. I'm against them because I'm fine with the verbosity of Rudy, and do not think the complexity is worth it.
I understand the homage. But i really had to uncheck use subreddit style, my eyes were hurting :)
To clarify this comment for those who are not in the know, Terry Pratchet, a famous fantasy/comedy writer, just passed away ([link](http://www.bbc.com/news/entertainment-arts-31858156)). Death is a character that is fairly well known, shows up in many of the novels, and always "speaks" with this sort of font.
I'm not sure from your post how much you know about this stuff already, so my apologies if I teach you to suck eggs in this reply :-). It's not a garbage collector in the the sense that most people mean when they say 'garbage collector', which is to say a *tracing garbage collector*. A tracing garbage collector scans through the live objects in the system, finds all the ones that are alive, and drops every object that doesn't have a reference to it. Rc works by incrementing a counter every time you clone it, and decrementing when that clone drops out of scope. When the counter hits zero, there's no references to the object, and it gets dropped. This is much simpler than tracing garbage collection, but is also vulnerable to memory leaks: if two object refer to each other, their counters will never drop to zero, and they'll never get dropped. To get around this you need to use measures like 'weak' pointers. So, to answer your question, yes it has a runtime cost: [edit]you need to perform an initial heap allocation to hold the item, along with the space overhead of the counter, and[/edit] you need to increment the counter every time you share it.
See this thread: [Ok, what happened to the font on this sub?](http://www.reddit.com/r/rust/comments/2yuumb/ok_what_happened_to_the_font_on_this_sub/)
Oh, that's actually awesome. Thanks!
Yes. In general, a reference-counted pointer has a runtime cost every time the pointer if duplicated or when it goes out of scope, when the counter has to be incremented/decremented. Also, in general these increments/decrements are (1) frequent, because they happen for every function call when the arguments are "duplicated" (from the caller's to the callee's stack frame), and (2) expensive, because they have to be atomic in a multithreaded environment. There are some pretty advanced techniques that remove a lot of this overhead; for example, of the caller knows that the reference will survive a function call, there's no need to increment the reference count when calling the function only to decrement it when it returns. Also, in multithreaded environments it is beneficial to cache decrements and perform a large batch one every so often. You can read about the latest techniques in [1]. Given that RC are implemented as a library in Rust, I'm assuming it doesn't do any of the above optimizations. On the other hand, it might not need them; in Rust, you don't need to copy the pointer, you can move or reference it as well, in which case no in-/decrements are needed. I don't know if RCs can be shared between threads, but if not, you can also significantly improve performance by not using atomic instructions. [1] *Shahriyar et al*. [Down for the count? Getting reference counting back in the ring](http://users.cecs.anu.edu.au/~steveb/downloads/pdf/rc-ismm-2012.pdf)
Rc&lt;T&gt; is not thread safe. If you need thread safety, you use Arc&lt;T&gt; (Atomically Reference Counted pointer to T). The only times the counter is incremented is when you call .clone() and the only time the counter is decremented is when an Rc&lt;T&gt; is dropped. Moving an Rc&lt;T&gt; does not mess with the counter.
Nah, I meant stuff like `!!kibwen` or `[larsberg]` or `/// steveklabnik1` etc etc.
What happens on `&amp;*obj` where `obj: Rc&lt;T&gt;`? I suppose that drops `obj`? Im just wondering whether you should avoid putting bare references into a struct but instead always use `Deref&lt;T&gt;`? Then you would abstract over all reference types.
Thanks!
*Atomically
Unsubscribed. Causes physical discomfort. 
FYI, the official Rust builds are produced on CentOS 5 and the only glibc-based distro I'm aware of that it has compatibility problems with is NixOS.
I'd prefer a `g`, `g_`, or `google_` prefix. It also makes them easier to identify at a glance, and ones like `books`, `dns`, `drive`, etc won't create conflicts in the future.
Death is known to do that.
As there is a version suffix for all of these, it would be something like `dns1`, `drive2`, and so forth. However, I agree, there should be some sort of indicating what kind of package it is.
I don't know how to express this right without resorting to writing a bunch of typing rules, but I feel like Rust could infer whether a function *possibly consumes* a parameter, or if it simply *borrows* it.
Just wondering, what's the reason to have version suffixes rather than just use crate versions (1.x.x, 2.x.x, etc.)?
&gt; general these increments/decrements are (1) frequent, because they happen for every function call when the arguments are "duplicated" (from the caller's to the callee's stack frame), and We have it better in Rust. You can pass &amp;T or &amp;Rc&lt;T&gt;, or you can transfer ownership of the `Rc&lt;T&gt;`, in neither of those cases is the ref count touched. Your .clone() calls will tell you exactly when the ref count is increased. Edit: This hasn't been merged fully, but we can do some amazing optimizations of Rc in the library still: https://github.com/rust-lang/rust/pull/21418 Edit2: Please don't downvote the parent
Ok, but aren't there some non dev oriented distros that do not install gcc by default?
I think adding a `google-` prefix would probably be the best option. Using `-` in crates will add a nice-looking way to separate google API crates, and will differentiate from just words, like `books`. Having `-` instead of `_` will also encourage people to use them as `books` or `dns` instead of `g_books` or `g_dns`, because you have to specify a name which doesn't contain `-` to use anyways. I'd also be against using version suffixes, because you can just use major versions in cargo instead, and as far as I know there isn't much usefulness in maintaining two incompatible versions of an API client.
I would prefer `-` as well, but I have seen an RFC that wants to prohibit dashes in crate names. So I backed off from the idea ... maybe the RFC won't go through though (?). Also I believe google supports various versions of an API, at least for a certain period, which is why I believe it's necessary to maintain the protocol version as part of the name. However, i'd be happy to simplify this and just use the crate version, which would then have to maintain major and minor version of the protocol, and the version of the client code itself.
&gt; (2) expensive, because they have to be atomic in a multithreaded environment. Note that the compiler enforces that Rc is never shared, thus obviating the need for atomic operations (this is also therefore an improvement over C++'s shared_ptr type).
Ah, ok - I wasn't sure if you were going to maintain multiple versions or just the latest.
This example is one of the best arguments I've seen in favor of crates.io namespacing.
Good - in case there is a prefix, I will gladly separate it with dashes. Also good to hear that protocol version suffixes do make sense for you as well. Maintaining multiple versions doesn't represent any effort, as all code is generated.
At the very least, there might be clear guidelines on how to use prefixes to simulate a real namespace.
Note that the reason that wasn't fully merged was due to the significant (negative) impact on compile times certain parts of the patch caused. Rather than it being anything wrong with the patch itself. 
&gt;Yes, but the body of the function can never affect its signature. How so? Doesn't "return true" affect the signature of the function? A type system is just a syntactic restriction on what programs are valid.
I've already had to do this for Night Mode, I wish the CSS was compatible with it
I got an email a few days ago from a Google recruiter citing interest in my Rust experience. I asked on IRC and /u/steveklabnik1 said he got something very similar. This might be a strong sign that Google has taken an interest in us already. Whether that's good or bad has yet to be seen. Google is technically a competitor, they might be trying to poach talent. Or they might just be looking for help getting started in Rust. I couldn't say.
Why would moving be expensive? Isn't it basically free/compile-time?
In what way would `return true` affect the signature? Either the signature contains `-&gt; bool` or it doesn't, and that will determine whether that return is valid. There do exist languages that allow function signatures to be inferred via whole-program type inference (e.g. Haskell), but that inference is explicitly a non-goal of Rust's. But Rust does allow you to infer the signature of a closure, which is perhaps the root of the confusion here.
You can use different names for packages and library name. In the Piston project, we use prefixes such as `piston-` or `piston3d-` for library that are specific to the Piston ecosystem.
&gt; This might be a strong sign that Google has taken an interest in us already As I also said on IRC, I'm not sure that this is the case. Recruiters pick projects on github and then just spam people, and Rust is a very popular github project.
There's one additional cost between a simple T versus Rc&lt;T&gt;, which is the need to heap allocate a box to place a T and the counter in, right?
Does there exist some information on how you guys build Rust for distribution? Like what platform/versions you use and any other configuration details? I ask because I'd like to do something similar for my own Rust projects (compile &amp; distribute binaries), but I think it would go much quicker if I could leverage the knowledge you all have already gathered. :-)
Thanks for linking to this. Simple projects like this are what allow me to easily pick up languages because they are easily digestible ported.
&gt; It might be an interesting thing to investigate, but I suspect there will be ergonomic difficulties that outweigh the perf benefit (if there is one). It depends on how much you share the ref counted object within the thread I suppose. A one-off allocation of a thread local ref count is nothing compared to a thousand atomic increment/decrement pairs (each one is what, a couple hundred cycles?) 
Sounds good!
That is a very good point, which completely slipped my mind!
`nth` returns an `Option` so you need to get access to the contained string (if it exists: it could be `None`), e.g. with `match`, `if let`, `.map` or `.and_then` or one of the other methods.
Another, with pattern matching let n: u32 = match std::env::args().nth(1).map(|a| a.parse()) { Some(Ok(n)) =&gt; n, _ =&gt; panic!("I need a number"), }; 
I'm pretty sure this is false, but I'd like to give you the benefit of the doubt... Any references, or an explanation of how this would work?
Firefox's DOM implementation uses reference counting and a cycle detector. https://air.mozilla.org/gecko-cycle-collector-intro/
The only problem on NixOS is the full path to ld-linux and the rpath - but this is a general problem with impure binaries and is easily solved with the `patchelf` utility.
If you put dashes in crate names, you force all users of your crate to use your code via the following gross method: extern crate "google-foo" as google_foo; You might as well just call your crate `google_foo` with an underscore in the first place. Your users will thank you.
I think that until somebody actually hits that point, there's no reason to try writing that optimization, and the person who's job it is to optimize that is the one who is doing a thousand local clones. Also, cannot one just do Rc&lt;Arc&lt;T&gt;&gt;?
I believe the goal with using a dash is, here at least, to force: extern crate "google-foo" as foo;
&gt; To be honest, I don't like the idea of a compiler implicitly generating code for allocating a closure's environment on the heap Swift has two kinds of closure: escaping closures and non escaping closures (based on whether or not they are copied outside their environment's scope but you can also explicitly adorn closure variables with @noescape). Once you use an escaping closure, you are opting in to (potential) heap allocation because escaping closures are reference types in Swift. It's part of Swift's memory management philosophy and it's how Swift ensures pretty good memory safety without Rust's borrow rules. Yes, it chooses ease-of-use over performance. But if you want to avoid heap allocations in Swift, you can; you just need to know what is and isn't a reference type.
The assumption tracking in LLVM is fairly expensive. `assume(ptr != null)` is lowered to not-null metadata, which seems to be less expensive.
Yeah that would probably work. It's an extra indirection but maybe that's fine. 
This is the followup paper and is even cooler: http://users.cecs.anu.edu.au/~rifat/files/rcix-oopsla-2013.pdf Basically it beats tracing GC now. 
But how do you get a pointer to an object in the cycle in order to test it? You must have a reference from outside the cycle somehow!
FYI, it looks like the GitHub pages site is down.
Yeah, I think this article is one of the first "real world Rust" articles out there (that isn't devoted to a specific topic, like macros) and should be extremely helpful to people who are struggling with the borrow checker.
Thanks! I've no idea what I'm doing so the `TypeId` stuff went over my head. Have you complained about this elsewhere :) I'd like to read it incase it'll help in my further adventures with `DynamicLibrary`!
Well, here's [a repository that shows the `TypeId` problem](https://github.com/DanielKeep/proto-glium-hot-reload). Everything else works (more or less). To clarify, the goal with this is to support a workflow where a driver program specifies a set of components that should be loaded. Each component is represented by a trait that describes its interface. `sandbox_abi` is the bridge between the driver and the component libraries. The reason for wanting `TypeId` was to make reloading more flexible. The idea is that every component trait has a `Reload::freeze` method that turns a component into a blob. The library can then be unloaded, the new one loaded up, and the blob passed into the component's constructor, allowing it to reload the current state. To do this, I wanted to use a `TypeMap`, that allows the blob to store basically *any* values it wants, without having to serialise them to some intermediate format. If you want to read through the execution of it, the `main` function is in `src/main.rs`; `sandbox_abi` and `sandbox_render` are separate Cargo packages, with `sandbox_render` containing the rendering component implementation. Like I said, if you hack around the `TypeId` problem, it all seems to work just fine. ... provided you don't compile `sandbox_abi` as a dynamic library, which causes the linker to explode *for some reason*.
I have just fixed that ! Travis kept overwriting pages with the wrong project :) ! This issue has confused me from the start, but this morning it became obvious to me.
How can a crate name be different from the package name ? Can I say `extern google_youtube3`, but `youtube3 = "*"` in my dependencies ?
Of course you can still "make the program". You just can't build it in the same way.
`Gc&lt;T&gt;` no longer exists; so yes, there's a proof by example that you can do it, as no current Rust programs use it. While `Rc&lt;T&gt;` is still in heavy use, the language without `Rc` is still Turing complete; in fact, `Rc` is just defined in a library, so if it didn't exist, you could define something similar yourself if you needed it. It just makes a lot of things a lot more convenient and efficient, that's all. In idiomatic Rust code, it is generally considered best to try to avoid using types like `Rc`/`Arc` unless absolutely necessary. It's frequently possible to just arrange things with the proper ownership relations, or moving ownership between different object, such that you can just use the built-in reference types along with `Box` (an owned heap allocated pointer) everywhere. For some problems `Rc` is a big help, but there is an awful lot of code for which it's not necessary.
`Rc&lt;T&gt;` is a library type, so you can just define your own called `ReferenceCounted&lt;T&gt;` and implement it exactly the same. Or slightly differently if you want to collect cycles and stuff... Now `Box&lt;T&gt;`...
Travis can build your site? I haven't used that feature before! 
&gt; Google is technically a competitor, they might be trying to poach talent. It's not really a competitor for anything other than Servo :) (Go is not on the same field as Rust, mostly) I like the idea of Google using Rust, that means that the language will get a bunch more contributors, and it gets a major stamp of legitimacy. I'm not so fond of it if they're using it do develop Chrome before Servo reaches a reasonably feature complete state, but then again, [we have pcwalton](https://twitter.com/horse_rust/status/526234668526358528) :P 
[**@horse_rust**](https://twitter.com/horse_rust/) &gt; [2014-10-26 04:51 UTC](https://twitter.com/horse_rust/status/526234668526358528) &gt; 00:29 &lt;@pcwalton&gt; ok, I'm beating Chrome by 20%… good enough for today, I'll do more tomorrow [[Attached pic]](http://pbs.twimg.com/media/B02PpF0CQAQhdGs.jpg) [[Imgur rehost]](http://i.imgur.com/GHAQn79.jpg) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I believe there are separate package name and lib keys in Cargo toml files.
Travis has publish options. Many people run `cargo doc` and then push to gh-pages. Rather [simple script](https://github.com/servo/rust-url/blob/master/Makefile#L10) to do it.
I think you could use the `conditional features` feature of crates to keep all the product libraries in one crate and make the protocol version switchable via Cargo.toml `features=...`
I will wait as long as I can. According to my schedule, I should have cooked something useful in about a month. It would certainly not be a problem to wait until Rust 1.0.0 with that, and synchronize my release :).
You can even avoid wrapping the Ok in the Some: std::env::args().nth(1).and_then(|a| a.parse().ok()): Option&lt;u32&gt;
Even though it's certainly possible, I would be afraid to have a huge crate that takes forever to compile, even though you just want to use a small part of it. Putting each API into it's own crate seems a necessity. I am talking about 415k lines of code, which will probably end up being more like a million lines when the APIs are feature-complete. `youtube3` currently clocks in at ~40k lines.
True, I will investigate that. Thanks
I got an email refering to my Rust projects more then a year ago. I'm on their database already for a reason, they just dig up a new fact about me every other year and mention that. I wouldn't put any weight on that.
&gt; Also, I don't understand why I need to import std::io::prelude to make read and write methods available when they are apparently included in std::net::TcpStream They are not included in `TcpSteam`the come from the `Read` or `ReadExt` trait which is implemented by `TcpStream`. Methods provided by trait implementations are only available if the corresponding trait is available in the current scope. The need to manually import the prelude will go away as soon as `std::old_io` is deprecated. Please note that it is not recommended to use `std::io` yet.
Oh, ok. So, out of curiosity, is there anything that can be done with `stream` if it is not defined with `mut`?
&gt; How we can specify both greet and hello if we want to use cargo to compile main.rs You can't. All names in an application exist in a single flat [symbol table](http://en.wikipedia.org/wiki/Symbol_table), and duplicates are not permitted; because it would make the execution of the application ambiguous (when calling foo(), which foo() is invoked?) The common solution to this (that both c++ and rust use) is [name mangling](http://en.wikipedia.org/wiki/Name_mangling) where a symbol is mapped to a hash (foo -&gt; sahfkasdfhkadklsfh) which is unique for each instance; and a higher level system resolves local variables and calls to the hash before it hits the linker. However, rust uses the c calling convention for ffi (you can read more about that here http://www.codeproject.com/Articles/1388/Calling-Conventions-Demystified), which basically means that it can only call external symbols in the standard flat c namespace. I don't believe it's possible under any circumstances to have duplicate symbols in external libraries which are linked via ffi. Your best bet is to manually alter the names of the symbols in the libraries to be `greet_hi` and `hello_hi`; or compile the the libraries using C++ with extern C blocks that contain unique namespaced names. (nb. When you open a dynamic library you import the symbols into the same global namespace; but plausibly as long as you unloaded greet before invoking hello, and vice versa, you could get the behaviour you want using DynamicLibrary... but I doubt it's worth the bother) &gt; Moreover why is it not possible to use links without build script? ? Don't know what that means. The 'links' directive in cargo.toml basically does nothing. It just ensures no duplicates occur as a top level for crates that bind the same system library.
I know, but with everything in flux changing that warning is probably not that high on their priority list. I suppose they'll change it when it's fully stabilised, but it could be recommended well before that to switch.
Same. They come around with some frequency.
Weird. I went through some of the tests, using Firefox on Ubuntu Linux, and not even Firefox passes some of those tests. e.g: http://test.csswg.org/suites/css-writing-modes-3_dev/nightly-unstable/html/bidi-isolate-override-001.htm (To be fair, Chrome doesn't pass this particular test as well. Curious.)
Indeed. Adam Chlipala wrote [this](http://adam.chlipala.net/mlcomp/) helpful comparison of them.
I'm trying to grok what you've done, but the last **:** is throwing me off... It doesn't compile either.
The last `:` is just to show the type of the expression, it's not valid rust.
It's rather unlikely that Google would hire Rust people specifically for the purpose of making a Servo competitor. At least now. (There are plenty of other things that they could Rustify that aren't a large project)
You can fork [the repository](https://github.com/rust-lang/rust), clone your fork, fix the typo, and [submit a pull request](https://help.github.com/articles/using-pull-requests/). The book is in `src/doc` within the repository.
Both of these answers are great. A pre-emptive 'thanks!' is in order: I appreciate the help.
&gt; All names in an application exist in a single flat symbol table, and duplicates are not permitted; because it would make the execution of the application ambiguous (when calling foo(), which foo() is invoked?) This is true on GNU/Linux and other systems that use glibc, but it's not true on Mac OS X ("two-level namespaces"), Solaris (`-Bdirect`) etc. The way it works is that the symbol names, internally, also carry the name of the library where the symbol is to be found. You still can't link everything together at once like in this example, but you can make one object file that dynamically links one library, another object file that dynamically links another library, and an executable combining the two object files, and each object file will get the library it wanted. It's extremely unfortunate that glibc doesn't support it. The OpenOffice developers [wrote a patch for this](https://sourceware.org/ml/binutils/2005-10/msg00436.html) a long time ago, but the glibc maintainer at the time didn't understand the use case and rejected it. There was a leadership shakeup a couple years back, but nobody's updated and re-posted the patch. &gt; When you open a dynamic library you import the symbols into the same global namespace If you're dynamically loading the library, you can use the `RTLD_LOCAL` flag to `dlopen` (or whatever the equivalent is on your platform), which keeps it out of the global namespace. Then the symbols are only accessible via the handle `dlopen` returns to you.
 &gt;in fact, `Rc` is just defined in a library, so if it didn't exist, you could define something similar yourself if you needed it. To put a different spin on it: Wouldn't it exist, people would have worse ad-hoc replacements of it everywhere
You can also make small edits in Github. Just click the edit icon (pencil) on the top right of a file. (Bonus: It lets you preview the rendered Markdown!)
Yes, more or less. Think of it this way: when you're linking a shared library, the library owns the symbols. When you link a static library, since the library becomes part of your code, you own the symbols. In particular, you might be linking multiple static libraries to _create_ a shared library. If so, how would people access both versions of `hi` from your shared library? So when you're statically linking, it matters that your executable (a single unit) doesn't have multiple definitions. But if you're dynamically linking, it doesn't matter if there are multiple definitions across multiple libraries. When you're linking a shared library, all that matters is that any undefined symbol in your own code is defined in at least one of your shared libraries. It doesn't matter if it's defined in more than one, as long every symbol in _your_ code has exactly one value. Shared libraries can also add symbols on their own, as long as they don't change the ABI for existing symbols. So you might be linking with a version of `libgreet` that doesn't include a function named `hi`, but tomorrow, a new version of `libgreet` added a function named `hi`. That shouldn't cause your code to fail to run. (That said, on glibc, you might accidentally start calling `libgreet`'s version of hi. On OS X and Solaris, this sort of bug can't happen.)
It took me way too long to figure this out when I went looking: the book is specifically in `src/doc/trpl`, "trpl" standing for "The Rust Programming Language". (I think my brain somehow glossed over it as looking like `tmpl`, "template.")
Yes, there are no PRs to small. Please send them even for the smallest of typos. &gt; It seems like Travis or whatever has to build the project to check if it is still correct, even when you just modify a markdown file. Travis does not run any tests, just does some basic sanity checking. It's not a big deal. We also have a rollup process to combine pull requests that are likely to not cause problems, but that's for me to worry about, not you :) &gt; Or a file protocol for pointing to line and column in a file in a specific commit. Click on any line number on Github, and then press `y`. The URL in your browser bar will look something like https://github.com/rust-lang/rust/blob/766a4e1acc06061a30cf456840a9915526fb681e/src/libarena/lib.rs#L22 
.. though it may be if and when type ascription lands.
Oh, thanks for the clarification. I hadn't actually looked at the layout change when it switched from being the guide(s) to the book, I didn't notice that it was now nested one more level deep.
Thanks! It is very informative :)
Would you mind posting some example code for what you meant by &gt; avoiding `Rc` often requires initializers to accept a rooted entry lower in the stack as a parameter, which may contain something like a `TypedArena&lt;T&gt;` Thanks!
I'm not well versed in GitHub, but I'll figure it out and submit the changes. Thanks.
Got the same about a year ago.
Yeah, but sadly there have been deadlocks at the intersection.
It might be nice to have a decorator that provided this feature. Certainly sounds doable.
You could also argue that using a library via an FFI is not a typical use case for most libraries in most languages though.
Is it possible that the LLVM backend is optimizing away the repeated runs?
Rust's build automation can batch commits that are unlikely to break the build. The only burden on the project owners is to approve the PR with "r+ rollup" instead of just "r+".
This thread has been linked to from another place on reddit. - [/r/rust_gamedev] [A `#![no_std]` implementation of Handmade Hero (x-post r/rust)](http://np.reddit.com/r/rust_gamedev/comments/2z3k9m/a_no_std_implementation_of_handmade_hero_xpost/) [](#footer)*^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote. ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot)
That's a cool idea! I'll look into it.
It's a method of controlling conditional compilation so I don't think so. But haven't tried it myself either so can't say for sure. 
I see. I thought the pull request is the "right" way, so that's why I wanted to do it. Thanks for the heads up.
I don't dare dream the dream.
You should try to make yourself more comfortable with Rust's type system. The types in Rust's standard library are nothing to be afraid of (they won't bite!) ;) - When working with characters, use `char`, not `u8`. (This also supports Unicode without any thought on your part!). - Use a `String`, not a `Vec&lt;u8&gt;` or `Vec&lt;char&gt;`. Using this also allows you to take advantage of some wonderful string functions. For example, you might be interested in [`split_str()`](https://doc.rust-lang.org/std/str/trait.StrExt.html#tymethod.split_str) or [`words()`](https://doc.rust-lang.org/std/str/trait.StrExt.html#tymethod.words) (the latter of which splits a string into words, and hey! Now you'll support tabs and other miscellaneous whitespace)
1. You should listen to the compiler, it will tell you that you would better use loop instead of recursive call. Rust does not guarant tail call elimination so you may end up with stack overflow. 2. `for byte in bytes.clone()` -&gt; `for &amp;byte in &amp;bytes`. No need to clone vector here, you could instead iterate over it inplace. 3. Replace `else if`s with `match`: `if words.len() == 1 { ... } else if words.len() == 2 {...} else { ... }` -&gt; `match words.len() { 1 =&gt; ..., 2 =&gt; ..., _ =&gt; ...`.
When saving the changes after editing a file on Github, it actually offers you to create a pull request based on those changes. So you end up making a pull request either way :)
Does the rust parser uses backtracking? Looks like it doesn't, but I don't know rust to be sure. The haskell version uses backtracking with no real reason. (Actually if you don't need backtracking, then you don't need attoparsec. The mp4 is a good use case for binary package.)
What if we had a `everything` keyword that added constraints to every generic parameter? where everything: 'a 
Perhaps if you write `where &lt;_ as Foo&gt;: 'a` it could refine it for types that implements the trait `Foo`.
Not exactly. Look at [an internals Discussion](http://internals.rust-lang.org/t/issues-in-new-i-o/1658/14) - what keeps Rust from having linear types is the lack of type state mutability.
I guess you want to use [`thread::scoped()`](http://doc.rust-lang.org/std/thread/fn.scoped.html) here?
At this point, it's better to use the new io. 
Thanks!
Thanks!
The handmade libstd wasn't "necessary", but it is a cool and fun challenge :)
Why would I use Direct3D or OpenGL for a 2d game? He's not using either of them. He's using the windows platform API. If you said, "why don't I write directly on X", then I could maybe understand you, but I didn't do that because, well, X is ridiculous (and there are no good manuals/tutorials for how to write a game on top of it), and Wayland is coming, but I can't actually write in Wayland yet because I don't have Wayland running. Also, SDL has a really great manual.
Checking for words can be done with `string.words().collect::&lt;Vec&lt;_&gt;&gt;()`, you don't need some weird solution with bytes.
I don't use Cap'n Proto right now but these wrappers for it might come in handy later on. Cool stuff!
Awesome! I'm impressed with how much can be accomplished just through `macro_rules!()`.
One solution might be to `touch` your `build.rs` whenever you make a change to a ispc file. It's bit clunky, but is better than using a `cargo clean`, which would rebuild your dependencies as well.
what this code does is collecting the `JoinGuard`s returned by `thread::scoped` in a `Vec`. This `Vec` is dropped immediately causing the destructors of the `JoinGuard`s to run. These destructors [will implicitly join the child thread](http://doc.rust-lang.org/std/thread/fn.scoped.html).
You need to store the `JoinGuard`s somewhere to be able to wait on them (they're basically thread handles)--if you waited on them before creating all the threads, the code wouldn't actually be run in parallel. It would be possible to rewrite the code to store the `JoinGuard`s in a static array instead of allocating a new `Vec`, but usually when you're dealing with threads a single allocation like this is dwarfed by the cost of creating the threads in the first place.
There was, and it was added to the itertools crate.
https://gist.github.com/pythonesque/fc43fe529adf8ddc26cb
It's actually from the `syntax-rules` of Scheme. See http://docs.racket-lang.org/guide/pattern-macros.html for example.
Cool -- will give it a try.
Afaicr, just referring to structs without `struct T` like in C, and automatic initialization of struct fields.
I don't see what the issue is with Foo::new() really.
Just that if "use Foo::*" imported Foo::new, and then you did that with a couple other types, things could get messy quicker than they might otherwise. Or something like that. I'm just trying to understand why that has to be illegal.
You may want to check out the getopts or docopts crates.
I don't think the down voting was due to bad advice, but lack of helpful explanations (being that its for a new user).
[Cyclic redundancy check](http://en.wikipedia.org/wiki/Cyclic_redundancy_check), a common form of error-detecting code used to detect accidental changes to the data. For example, every ZIP file has CRC-32 code.
It seems that all GitHub projects have an implicit dependency on www.google.com nowadays.
I'm impressed. I'm working for a longer while on roguelike (https://github.com/dpc/rhex) and I can appreciate what you achieved. The game has a nice, minimalistic but complete feel and an interesting idea. Also it uses hex-tiles which I enjoy a lot. I really like the UI. Would you mind if in the future I stole couple of ideas from you? :D
&gt; // The unwrap here should say why the file couldn't be opened. That should be sufficient. That's not really sufficient in general; in this particular case, you can maybe get away with it, but especially for file not found errors, you want to say *what* file wasn't found. &gt; // .lines() eats EOF error, so this is will only panic if something went horribly wrong. &gt; let line = line.unwrap(); This panics if the input isn't valid UTF-8, which is a relatively common error condition. &gt; At that point you just have to replace everything else with `.expect()` and the reason why it wouldn't meet your expectations `.expect()` and `.unwrap()` don't really provide very good errors for things that are user errors or input validation errors, rather than program bugs. Here's what yours prints out if the input file is missing: &gt; thread '&lt;main&gt;' panicked at 'called `Result::unwrap()` on an `Err` value: Error { repr: Os(2) }', /Users/rustbuild/src/rust-buildbot/slave/nightly-dist-rustc-mac/build/src/libcore/result.rs:744 And here's mine, which was more cumbersome to write, but it provides the user with much better error messages. &gt; Couldn't open nofile: No such file or directory (os error 2) I realize that for quick and dirty tools where you don't care about it that much, you can use `.unwrap()` or `.expect()`, and you'll be able to figure it out if something goes wrong. But a lot of those "quick and dirty tools" are going to need to be used by someone else at some point, and given that Rust is compiled they may not even have the source to try and figure it out. So you really do want to try to be good about your error handling; `.unwrap()` and `.expect()` should only be used for cases where if you get into that state, it means your program is buggy, not that it got passed in some bad input. `FromError` and `try!` do a decent job of allowing you to directly wrap one error type up in a more general one and propagate it upwards, but they don't give you a chance to attach a little extra information, or log things that are owned at this scope without having to copy them into new buffers so that you can pass them up the error handling chain. Anyhow, it is possible to do error handling well; and the error handling tools have gotten better recently. I would prefer fewer people to resort to `.unwrap()` and `.expect()` in example code, because I think it's likely to lead to more people thinking that that's the way it should be done even in real code. I feel like instead, if we actually write out good error handling every time, it may become more obvious where the pain points are and what corners need to be smoothed out. In this particular case, I think that the case of "log a good error message using local contextual information, and then return early from the function" is a bit awkward to do, since you can't return early from within a closure (as the `return` would just exit the closure, rather than the larger function), so if you want to both log something and return early, you need to use both the closure and a `try!()`, or just a `match` statement with an explicit log and `return`. Which isn't so bad, but does interrupt the flow a little bit. Anyhow, part of this is just me trying to figure out what the best habits to get into are, and explore what the existing tools provide.
&gt; If your file is all just that there, one integer and one float That's just an example. I was looking for a general solution built into Rust.
Rust doesn't really have anything like that built-in. I suggest checking out the [scan crate][1], though it hasn't been updated in a couple weeks so it's probably not building against current Rust. Also, it only looks to work on stdin, so you'd have to have files piped into the program. [1]: https://github.com/mahkoh/scan
Cool. It looks like the C code you have could be replaced by a syntax extension in Rust?
I couldn't get it to work the way I wanted with a manually compiled libcore. Main problem is specifying library search path, the smaller one custom output directory. That's something to work on.
Are you sure? Python, Ruby, JS, Lua...
table is generated from supplied polynomial at run-time but only need to be generated once table for most commonly used CRC-32-IEEE can be generated ahead of time or just hardcoded and 'static' checksum function can be provided i'm new to rust and it seems rust does not allow constants to be function result, only from constructor, any alternatives?
I'm curious as to why the `_` is needed there in `add`'s `match` statement.
I was referring to the inverse. Those languages commonly call *others*.
Rust is insufficiently smart to prove that the three preceding arms cover the entire set of possible `i8` values, and `match`es have to be exhaustive in Rust.
Not really. They commonly call C to make up for their own shortcomings in performance but I would not say that they commonly call many different languages. It makes sense to split code between a language like C which is hard to use due to low level concerns and one which is easier to use but slow. This split doesn't really make sense for a relatively fast high level language like Haskell though, particularly in trade offs where the code using Haskell exclusively would be worse off just to improve the FFI use case. 
I suspect you may be able to handle this by making a standalone `core` cargo library, and then use a `path` dependency on that in your main library, e.g. if you copy it into a `core` subdirectory (and add a `Cargo.toml`) of your main project: [dependencies.core] path = "core" This should allow `cargo build --target=...` to do the right thing, if you have a `[target....]` `ar` and `linker` [configured](http://doc.crates.io/config.html#configuration-keys). (There may be speed-bumps along the way, though.)
This entirely misses the point. There are three values in the enum *because it's a toy example*. Sure, it's easy to write matches when you've got three values. What if you had a hundred?
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: http://www.reddit.com/r/Serendipity/comments/2z7o07/tutorial_how_to_collect_test_coverages_for_rust/
How is the math supposed to work if you have a hundred?
Those seem to be doable as a macro? (also, [there is a crate that does this already](https://crates.io/crates/bitflags)) Most C preprocessor macros can be done as Rust macros (modulo hygiene). Syntax extensions are more powerful, but only needed when you want to do really complicated things.
Awesome!
I ported crc64 from Redis, first a naive one, then using Matt's crcspeed implementation. It's at https://github.com/mattsta/crcspeed I use hard coded constants though, namely the "Jones" coefficients, so results will differ. Code is at: https://github.com/badboy/crc64-rs
Could you send a patch for this?
Ah, I see. It seems like I was mistaken. I was on mobile and about to sleep, I saw the `map_err` calls and had a knee-jerk reaction. Sorry about that! But yeah, if you want to fill your errors with useful information (how dare you!) that isn't contained inside the original error, then I think you'll wind up having to write extra code somewhere.
Thanks for suggestions. Though it is v0.2.5 in .lock file and in the output of cargo build (--verbose). So version is correct.
That's why I said I was a little unclear on the requirements. Based on the description, the reason he needed to do arithmetic was due to this three-value saturating arithmetic; but given that this is only three values, the above is a pretty short, succinct way to do it, so I don't see the problem. That's why I was wondering what the real problem is. It's a lot easier to address real problems than extrapolated hypotheticals. What is the use case in which you have 100 values, and need to convert often between them and integers to do arithmetic on them? For most cases with 100 values, I would wonder why you don't just keep them as an enum, or just use it as a custom integer type. But it's pretty hard to say without seeing the actual problem involved.
Ah, that's a good point, yes that looks a little nicer.
This seems like it would be fairly easy to patch for bounded types.
The toy example I provided is my actual use case, I was just wondering if this is really something that must be manually implemented each time and by using a match statement for each enum value (although I guess that is the suggestion in the last line of the github issue). The code snippets provided both work for what I'm trying to do, but I was looking to see if there was a way to generally convert literals -&gt; enum that would ideally be short and work for an arbitrary number of enum values. I guess you could create an array containing every enum value and iterate over it checking for equality. Sorry about the lack of clarity, thanks for all the responses.
&gt; So does that mean you can only use try! in a function that returns a core::result::Result&lt;&gt; ? Yes - the enclosing function must return a Result with a compatible error type to the error type inside the `try!` expression. 
Ok that makes sense, thanks
There's a constant tension in these examples that use `try!`: do we leave off the function declaration, because once you know `try!` needs this, it's not an issue, or do we add it in for each one, helping those who don't know `try!` but increasing verbosity and incidental complexity otherwise. I'm not sure. :/
It's worth noting that when we implemented the lifetime elision RFC, structs were intentionally left out: https://github.com/rust-lang/rfcs/blob/master/text/0141-lifetime-elision.md#lifetime-elision-in-structs It's still not totally clear to me if it would be better to allow them to be elided or not. It's true that it's obvious, but it's nice to have it spelled out explicitly, too...
As a complete newbie to rust, my two cents is that the issue isn't with the examples at all (to some extent). Any macro has the potential for this to be an issue if you are not familiar with the expansion and if it has an explicit return inside of it. I feel like the bigger issue is the compiler error. As a rust newbie it was difficult for me to even realize that the mismatched type it was talking about was the return type, rather than something wrong in the body of the try. I don't know if it's possible, but re-reading the compile error it does seem to recognize that it's being caused by the expansion of `try!`, and maybe that it's a return mismatch (I'm not sure if that's what the line that starts with `return` is saying or not), so if there was some way to give a help line for this it would be quite helpful.
GHC Haskell supports both "associated type synonyms" and "associated data types". The latter are injective by construction. If Rust supported these, the code in your example would look something like this: trait Foo { struct T; } impl&lt;K, V&gt; Foo for HashMap&lt;K, V&gt; { struct T { pub inner: HashMap&lt;K, V&gt;, } } `&lt;HashMap&lt;K, V&gt; as Foo&gt;::T` and `&lt;VecMap&lt;V&gt; as Foo&gt;::T` are different types by construction, as if these `struct T` were defined in different modules. (You'd probably use the tuple syntax for that "newtype" struct, but I ignored it here for clarity. Also, most likely you would be allowed to `impl` a `struct` as an `enum`.)
I'm probably missing the obvious, but could you give an example of associated type synonyms not being injective ? I looks like "if Foo&lt;T1, T2, T3,...&gt; == Foo&lt;U1, U2, U3,...&gt; then Foo&lt;T1, T2, T3,...&gt;::T == Foo&lt;U1, U2, U3...&gt;::T" edit : i mean the Foo implementation for a specific type.
One side note: Rust doesn't have `void` return type. So, even though you haven't specified a return type on the function, it actually has one: `()` (i.e. unit). No return or last expression implies a `return ();`.
Cool! I designed [dynamodule](https://github.com/kmcallister/dynamodule) for this use case; I want to do livecoding of fractal video feedback demos. The project is on hold but maybe you can run with it :) For now I'm only livecoding the shaders, which is simple.
If you would write down even a brief doc pointing out what did you enjoy about your design (+ link to source) I would be happy to read it! :)
&gt; Taken all together, we currently have 6/12 of the top crates running on 100% stable Rust, and I'm sure there are many more candidates for earning this so soon as well! :confetti_ball: Still more work to do, of course.
Wow, Rust 74's out already? I mean, I've heard of semver, but *this* is *ridiculous*!
[Here's a better link](http://users.rust-lang.org/t/using-unstable-apis-tell-us-about-it/157/46)
oh weird, thanks.
&gt; newcomers are less likely to learn lifetime annotations if these would be elided for structs in future. You have to understand the historical context of a document. When elision was proposed, many people were very skeptical of the idea in general. After a long time using elision, the vast, vast majority of people have enjoyed it, but at the time, that was not clear. &gt; If that's the case, why not teach everyone about the exact types involved everywhere, and turn off type-inference ? Well, for one thing, it's important to note that lifetimes do elision, not inference. So these two things are fundmantally different. And types are something that is well understood by most programmers, but lifetimes (as part of the language) are unique to Rust. &gt; And unhappy people turn to away more easily, which probably should be avoided. This is absolutely true, but the existing paragraph basically says "let's see how it goes and we can maybe talk about structs at a later date" not "we will never implement elision for structs." One of the reasons why they were left off originally is that it's actually less clear what the correct default is. &gt; there still is a chance it comes with Rust 1.1 ... or 1.2 ... or isn't it ? Yes, adding more elision means accepting more programs, which means that it's additive, and therefore, could be added in a point release.
&gt; 1) Operator overloading seems overly verbose, This allows you the flexibility of having the abilty to allow one side to be one type and the other the other, though. &gt; 2) I've had name clashes between variables and functions due to snake case. Is there some Rust naming convention that helps reduce these clashes? I have never had an issue with this, as my previous language, Ruby, had the same name reccomendation. So I can't really comment. &gt; 3) How can I declare the equivalent of a C++ static const var inside a struct? I was writing a RNG and needed a few constants local to the RNG, but the compiler wouldn't accept 'const A : u32 = 37u;' inside the scope of the struct, but it works in both global and method scope. IIRC, there's no way to make a member of a struct constant. Generally speaking, you can't even make part of a struct mutable or not, as it's inherited, rather than field-level. &gt; 4) How would I go about writing to the same line on stdio over and over again, fx to create a progress counter that loops from 0 to 100% in place? No comment for now, I'm sure someone else can lend you a hand though :)
Gonna move over all my code this week then. \o/
Let Oᴠᴇʀʟᴀᴘ be the set of Rust programs which contain overlapping impls, even after constraints on associated types have been taken into account. **Theorem**: Oᴠᴇʀʟᴀᴘ is [**NP**-hard](https://en.wikipedia.org/wiki/NP-hard). *Proof*. By reduction from [3-Sᴀᴛ](https://en.wikipedia.org/wiki/Boolean_satisfiability_problem#3-satisfiability). Given a 3-[CNF](https://en.wikipedia.org/wiki/Conjunctive_normal_form) formula *φ*, we construct a Rust library crate as follows. First, define: pub enum T {} pub enum F {} For each variable *x*[*i*] in the formula, define a trait: pub trait X_i: MarkerTrait { type V; } For each clause *c*[*ℓ*] which mentions variables *x*[*i*], *x*[*j*], *x*[*k*] (where any of *i*, *j*, *k* may be equal), consider the following trait and impls: pub trait Sat_ℓ: MarkerTrait + X_i + X_j + X_k {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = F&gt; + X_j&lt;V = F&gt; + X_k&lt;V = F&gt; {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = F&gt; + X_j&lt;V = F&gt; + X_k&lt;V = T&gt; {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = F&gt; + X_j&lt;V = T&gt; + X_k&lt;V = F&gt; {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = F&gt; + X_j&lt;V = T&gt; + X_k&lt;V = T&gt; {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = T&gt; + X_j&lt;V = F&gt; + X_k&lt;V = F&gt; {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = T&gt; + X_j&lt;V = F&gt; + X_k&lt;V = T&gt; {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = T&gt; + X_j&lt;V = T&gt; + X_k&lt;V = F&gt; {} impl&lt;R&gt; Sat_ℓ for R where R: X_i&lt;V = T&gt; + X_j&lt;V = T&gt; + X_k&lt;V = T&gt; {} There are eight impls, one for each truth value assignment to the three variables of the clause. If the same variable appears more than once, some of these assignments will be self-inconsistent. We discard these, as well as the assignments which contradict the clause. As an example, when translating the clause *c*[5] = *x*[1] ∨ *x*[1] ∨ ¬*x*[4], we are left with pub trait Sat_5: MarkerTrait {} impl&lt;R&gt; Sat_5 for R where R: X_1&lt;V = F&gt; + X_1&lt;V = F&gt; + X_4&lt;V = F&gt; {} impl&lt;R&gt; Sat_5 for R where R: X_1&lt;V = T&gt; + X_1&lt;V = T&gt; + X_4&lt;V = F&gt; {} impl&lt;R&gt; Sat_5 for R where R: X_1&lt;V = T&gt; + X_1&lt;V = T&gt; + X_4&lt;V = T&gt; {} The various `impl&lt;R&gt; Sat_ℓ` for each *ℓ* will never overlap with each other, because they have mutually exclusive constraints on associated types. And they will never overlap with other impls in this construction, because there are no other impls of `Sat_ℓ`. Therefore the crate so far does not contain any overlapping impls. Finally, we implement a trait for all satisfying assignments, and separately for all well-formed assignments. pub trait Sat {} impl&lt;R&gt; Sat for R where R: Sat_1 + Sat_2 + ... + Sat_n {} impl&lt;R&gt; Sat for R where R: X_1 + ... + X_m {} These two impls overlap if and only if *φ* is satisfiable. □ *Remarks*. The polynomial time and output size complexity of the construction are clear. The types `T` and `F` are public, as are the traits `X_i`. This means, if *φ* is satisfiable, then an external user of this library can define a type which lies in the overlap of these two impls. `rustc` could defer overlap checking until the specific `X_i` impls are available. This removes the existential search in the satisfiability question. It's not clear how much this would affect static reasoning about generic libraries. Restricting Oᴠᴇʀʟᴀᴘ to monomorphic types, as used in the above construction, makes a straightforward brute-force search possible. This restricted problem is therefore **NP**-complete. However the general overlap problem for polymorphic Rust may be harder than **NP**. It seems to me like [higher-order unification](https://en.wikipedia.org/wiki/Unification_%28computer_science%29#Higher-order_unification).
&gt; 2) I've had name clashes between variables and functions due to snake case. Is there some Rust naming convention that helps reduce these clashes? One way around this problem is to not use bare functions. Use methods instead. For example, in your code, I could see `luma` being a method on `Vec` (by the way, I would recommend using a different name for that, as there's a `Vec&lt;T&gt;` in Rust already which means something very different). Actually, I might use a separate type `Color`, rather than using `Vec` to mean either vector or color. &gt; 4) How would I go about writing to the same line on stdio over and over again, fx to create a progress counter that loops from 0 to 100% in place? The usual way to do this (at least on Unix-style terminals, not sure about Windows) is to print out a `\r` (carriage return) to return to the beginning of the line. You'll generally need to flush the output afterwards, as output is usually line-buffered. In order to also clear away anything else on the line it's usually best to output the `el` terminal capability string (clear to end of line); I tried to see if I could do that via the [`term` crate](https://crates.io/crates/term), but it doesn't allow outputting arbitrary capabilities, only a few that it supports like colors. You could probably just hard code it, however, `b"\x1b[K"`, as that's the VT100 sequence for. #![feature(libc, io)] extern crate libc; use std::io::prelude::*; use libc::funcs::posix88::unistd::sleep; fn main() { let mut stdout = std::io::stdout(); (write!(&amp;mut stdout, "hello, cruel")).unwrap(); stdout.flush().unwrap(); unsafe { sleep(1); } stdout.write(b"\r\x1b[K").unwrap(); (writeln!(&amp;mut stdout, "world")).unwrap(); } 
I don't know the answer to your question, but I just [copied the implementation](https://github.com/BurntSushi/xsv/blob/master/src/util.rs#L14-L22) into my project.
I recommend that you leave a comment in the OP thread to request a replacement, in order to inform the team which remaining APIs should be prioritized.
Apparently writing this out served as a sort of rubber-duck debugging. I was going to ask a couple questions about the ugliness of this method: impl Encodable for Box&lt;Delta&gt; { fn encode&lt;S: Encoder&gt;(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), S::Error&gt; { if let Some(delta) = self.downcast_ref::&lt;DeltaSetName&gt;() { (DeltaKindTag::SetName, delta).encode(s) } else if let Some(delta) = self.downcast_ref::&lt;DeltaAppendContents&gt;() { (DeltaKindTag::AppendContents, delta).encode(s) } else { panic!("Unknown concrete delta type") } } } like: * Is there a better way to write this? * Is there a different tool or technique I should be using to not even back myself into writing this? when I realized the answer to that second question is that I should be using an enum for Delta since the cases are all known ahead of time, which would negate the need for trait objects completely. Instead, I'll ask a different question. If I wanted to allow users of the library to define their own Delta implementations, is there a way to get rustc-serialize to encode them?
Can't wait to try the new debug formatting.
Unfortunately the major version number should be monotonically increasing. Hereby I suggest a variant which would conform to the semver standard: 1. Start with the major version 0. 2. Whenever the new version is released, pull first *N* bits of the object ID of the head git commit on `master` as a new major version, for the smallest value of *N* such that its numerical value strictly exceeds the previous major version. 3. When *N* exceeds 160, a suitable [key derivation function](http://en.wikipedia.org/wiki/Key_derivation_function) with fixed arguments is used to extend the bit sequence. The exact choice of KDF and arguments is up to the core developers.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Key derivation function**](https://en.wikipedia.org/wiki/Key%20derivation%20function): [](#sfw) --- &gt; &gt;In [cryptography](https://en.wikipedia.org/wiki/Cryptography), a __key derivation function__ (or __KDF__) derives one or more [secret keys](https://en.wikipedia.org/wiki/Key_(cryptography\)) from a secret value such as a master key or other known information such as a [password](https://en.wikipedia.org/wiki/Password) or [passphrase](https://en.wikipedia.org/wiki/Passphrase) using a [pseudo-random function](https://en.wikipedia.org/wiki/Pseudo-random_function). Keyed [cryptographic hash functions](https://en.wikipedia.org/wiki/Cryptographic_hash_function) are popular examples of pseudo-random functions used for key derivation. &gt; --- ^Interesting: [^Scrypt](https://en.wikipedia.org/wiki/Scrypt) ^| [^Passwd](https://en.wikipedia.org/wiki/Passwd) ^| [^PBKDF2](https://en.wikipedia.org/wiki/PBKDF2) ^| [^Shared ^secret](https://en.wikipedia.org/wiki/Shared_secret) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cphdm4p) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cphdm4p)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; When N exceeds 160 What's the expected number of commits before this is necessary?
This is why Git is evil and everyone should switch back to SVN. 
Every trial produces a hash with the nth bit set to 1. In the next trial, one of 3 things can happen: 1. The nth bit is set to 0. We add bits until we hit a 1 (this is just flip a coin until we hit heads; expected increase of n by 2) 2. The nth bit is set to 1, and the tail is smaller than the tail of the old hash. Perform the same procedure as above (expected increase of n by 2) 3. The nth bit is set to 1, and the tail is larger than the tail of the old hash. No bits added, but the probability of case 2 increases towards 1/2 while the probability of 3 decreases towards 0. Since 1 occurs with probability 1/2, and it adds 2 bits on average, we can conclude that the expected number of bits added is *at least* 1 per trial. Since the probability of 2 is at most 1/2, and it adds 2 bits on average, we conclude that the expected number of bits added is *at most* 2 per trial. So 1 &lt;= expected_bits_per_trial &lt;= 2. Now here's where I start fudging. My vague intuition is that every time 3 occurs you expect to roughly halve the range of values for which 3 will occur again. That is: [ ] = number line x = expected location of value in valid range - = invalid range (&lt;= old biggest) trial 0: [ x ] trial 1: [ x -----------------] PR 1/2 trial 2: [ x -------------------------] PR 1/4 (PR 1/8 overall) trial 3: [ x -----------------------------] PR 1/8 (PR 1/64 overall) ... trial n: [ x-------------------------------] PR 1/2^n (PR 1/(product i=1 to n of 2^i) overall) ... expectation ~= 1.8 = 1 + sum i*(product 1/2^n, n=1 to i), i=1 to infinity 1.8 is pretty close to 2, which is our expected coin-flips number, so I'll approximate that as "2 and 3 are equally likely when 1 doesn't occur", so we roughly have: 1. Occurs with PR 1/2 with expected increase of 2 bits 2. Occurs with PR 1/4 with expected increase of 2 bits 3. Occurs with PR 1/4 with expected increase of 0 bits Which gives expectation ~= 1.5 bits/commit 
[Chrono](https://github.com/lifthrasiir/rust-chrono) looks promising.
used lazy_static module, works great!
for a language with concurrency focus
I am impressed you managed to decipher the meaning of my comment when it was missing half of it and the only key word included was misspelled. Have an upvote.
What about memory requirements? Would be nice to see something about them in the README.
It was deprecated because the whole of `std::os` was deprecated, not because there is a particular vendetta against `num_cpus`. However, the intention has been to keep it `#[unstable]` at 1.0 because several people were unhappy with it: https://github.com/rust-lang/rfcs/pull/578#discussion-diff-22839432 ([the previous discussion referenced in that comment](https://github.com/rust-lang/rfcs/pull/517#discussion-diff-21758982)). C++ provides [`std::thread::hardware_concurrency`](http://en.cppreference.com/w/cpp/thread/thread/hardware_concurrency) and explicitly states that it is just a hint. I think that approach is good, and the renaming solves the problems people had with including "CPU" in the name.
Two things I've noticed: - Rust is not pure by default. You can do I/O, etc. inside any function without having to mark that function impure or do anything special. While working with mutable static data directly requires an unsafe block, it can be abstracted behind a safe interface, and using that interface it can be mutated anywhere. - Why is stack unwinding crossed out? The stack of a thread is unwound when it panics.
Thanks for your feedback. Highly appreciated. &gt; * Rust is not pure by default Point taken &gt; * Why is stack unwinding crossed out? The stack of a thread is unwound when it panics. Maybe I expressed myself improperly. What I'm trying to tell - there is no runtime time C++ style exception mechanism, with huge runtime time costs, when unwinding. I'm familiar with the fact that Rust does stack unwinding on thread panic, however I'm not that familiar to reason about it very confidently. My understanding is - it's somewhere similar to C/C++ http://en.cppreference.com/w/cpp/error/terminate and http://en.cppreference.com/w/cpp/error/unexpected. Does Rust allows to intercept/catch and process (ignore) thread panics? 
Very interesting, thanks !
I wasn't sure how to measure it. Any suggestions? 
It already implements most features, the only hard restriction is the string keys. 
Since macros and syntax extensions can gulp arbitrary token trees, one could easily write one that detects the version and just spits out the tokentree verbatim. I guess.
Measuring is better, for sure, but [this presentation (Burst Tries: A Fast Burst Tries: A Fast, Efficient Data Structure for String Keys; Steen Heinz, Justin Zobel, Hugh E. Williams)](http://www.cs.uvm.edu/~xwu/wie/CourseSlides/Schips-BurstTries.pdf) claims "They use about the same memory as a binary search tree".
Others have tackled the questions, so I guess I'll do the code review. Looks great! A couple minor things. 1. You probably shouldn't name Vec Vec as that conflicts with the stdlib Vec that does something slightly different, Also your Vec isn't really a Vector, it's a Point (though I'm assuming it's meant to represent a Vector from Origin) 2. For larger objects, such as Ray and Vec, you probably want to pass by reference instead of value, so you're not copying the large object onto the stack for every function call. 3. In radiance estimation, 98% of the function is two blocks deep. You could probably use if-let here, and just early-out with Vec::zero. 4. scoped returns a JoinGuard which joins automatically when it goes out of scope, which in this case would be when the threads vector does. No need to call join explicitly. 
No, you'd need to have the string live inside `self` to begin with so it naturally has the same lifetime.
Just a quick note- Rust hasn't had typestate in a long time.
Just return s, as a String, not an &amp;str.
Hmm. Is it possible to create something with a specific lifetime? let 'a s = format!("abc"); // is there a syntax for this? I seem to be missing something. It seems like a common task to: 1. call fn 2. fn creates new str and returns it ?
Yeah, I thought I scrolled down on purpose.
This is impossible, since lifetimes cannot be assigned. The lifetime of the `String` you created cannot be extended by an annotation, only by transferring ownership to the caller of the function (EDIT: or storing it in your struct).
https://github.com/rust-lang/rfcs/pull/529 IIRC
We don't intend for there to be a single rust compiler, these days people are getting clearer about the distinction between Rust and `rustc` whilst making language decisions.
That depends on the implementation details. I'm using a sorted array (limited to 64 items) as the container. So it's probably more compact than a BST if you account the malloc overheads.
This question is not about backward-compatibility but about forward-compatibility.
Thanks for the example. I'm going to change my error handling to work like this. I do implement std::fmt::Display. I just didn't understand the reasoning behind what looked like two ways of doing the same thing. Armed with this new understanding I can see why the description should just be a static string, and I see why one should use std::fmt::Display. I think the rust guide should use your example. It's really nice to have error handling structured correctly from the start.
This works: use std::collections::HashMap; pub fn get_fingerprint(value: &amp;str) -&gt; String { let mut current_char = 'a'; let mut new_value = String::new(); let mut used_chars: HashMap&lt;char, char&gt; = HashMap::new(); for character in value.chars() { let lowercase_character = character.to_lowercase(); let c = used_chars.get(&amp;character).map(|&amp;c| c); match c { Some(x) =&gt; new_value.push(x.clone()), None =&gt; { new_value.push(current_char.clone()); used_chars.insert(character.clone(), current_char.clone()); current_char = ((current_char as u8) + 1) as char; } } } return new_value; } fn main(){} The trick is to realise that `get` returns an `Option&lt;&amp;char&gt;`; thus, it (potentially) contains a borrowed reference into the `HashMap`. You can get around this by copying the thing it is referencing, thus releasing the borrow. **Edit**: You don't need `clone` because `char` implements `Copy` and thus has copy semantics.
Thanks, good to know. Still, it's not the same as C++ exception mechanism.
`macro_rules!` indeed.
We do have http://doc.rust-lang.org/nightly/book/error-handling.html , but it doesn't have stuff about this, because this stuff is fairly new. I would totally welcome adding the above to the book.
btw, this is what `Arc` and `Mutex` do (they are also interiorly mutable)
take a look at [cubby](https://github.com/viperscape/cubby) if you have a chance, something I am toying with; I make the collection totally immutable via a pre-allocated array and use &amp;self on the method calls, but each inner element is mutable via a sync primitive such as mutex. the problem with this I think is if the sync is poisoned, which can happen and your sync primitive should gracefully notify this in some way, the now immutable collection has one dead element. I don't know how far off cubby is from what you're working on, but it's something I came to realize and I will have to have an optional trait to take care of the collection mutably.
The `.map(|&amp;c| c)` call can be replaced with `.cloned()`.
Beat my to the punch by two minutes! Though my version has comments and uses the lowercase call which OP probably intended :P
This... is [an excellent idea](https://github.com/rust-lang/rfcs/pull/985)!
Not yet, and we don't currently plan on having one for 1.0. One option is to use the [time](https://crates.io/crates/time) crate and the other is the chrono crate /u/ChaosPony pointed out.
The only real sticky point about intrinsics is developing an API for them (e.g. name, location, signature). The `assume` intrinsic is pretty new to LLVM to we may wait to see how it plays out over there (we've already had some bad experiences with it), but I suspect we will stabilize it in time!
Thanks, most of my uses have been with `#[derive(FromPrimitive)]` / `from_*` with c-style enums.
Ok, I think I get it. That makes much more sense. After reading your description of `Copy` I came across `http://stackoverflow.com/questions/24253344/move-vs-copy-in-rust` which also explained the difference nicely. The differing semantics between movable and copyable values is kind of disconcerting (in the fact that it sounds like I need to keep careful track of object types and which ones are copyable and which aren't to keep the compiler happy) but other than that I think I understand the situation with that now. And thanks for the Entry api, that looks perfect for what I need. 
Heartbleed was an combination of unfortunate design decisions and some simple mistakes. Sure the same would be possible in Rust too, but you would have to do one more uncommon thing (by explicitly not using slices) to achieve the same outcome. I think Rust would have lowered the odds of heartbleed happening a lot.
&gt; Improper pointer arithmetic resulting in out-of-bounds memory reads [That just isn't true](https://www.reddit.com/r/programming/comments/2uinge/heartbleed_in_rust/co93lvo). The minimal example of tedblead only showed the case where previous information of the same kind was leaked, but I don't think it would be impossible (hard? Yes, but so is fucking up in such epic proportions in C) in rust to use another portion of the same array for the private keys, in which case we are back to zero.
If you'd like to take a look, the whole code is [here](https://gist.github.com/vincom2/4a9329a70da76e04957d) (the relevant section being the part in `mod tests`). I tried your suggested change (dropping the `Arc` and the `move`), but [it errors](https://gist.github.com/vincom2/b624d4c748b2cd265fcb).
Well... maybe better luck with this one: http://www.theregister.co.uk/2015/03/17/openssl_preps_fix_for_mystery_high_severity_hole/?mt=1426571709741 
The arguments shouldn't stop, and not just because I like a good flame war. The whole raison d'etre of Rust and its somewhat unique lifetime and ownership system is that it promises to eliminate a whole class of bugs, or at least mitigate them. What we need is a concrete example of how it goes about doing achieving that, and also the limits of what it can do. It's less about perpetuating a Rust vs C war, and more about highlighting the differences in implementations, and demonstrating the benefits and costs of adopting one vs the other.
I like to say that of course Rust prevented Heartbleed, because we don't currently support wrapping malloc ;). That's of course a bit glib. While it's true that Rust gives you a lot of assistance with writing safe code, given that unsafe exists, and that we, as a systems language, need to give you underlying access to the machine, Rust will allow you to do all kinds of stupid things if you vouch to the compiler that you're going to handle the safety aspect. This means that ultimately, you can do any kind of stupid thing in Rust that you can do in any other language. An example: a common problem in C code is buffer overflows, which happens when some code thinks that an array has more elements in it than it does. Because C's arrays don't have a length as part of the type, it can be error prone to keep track of the length of the array. Rust's arrays, vectors, and slices all have a length as a part of the type, and so it's more difficult to have this data be out of sync. But Rust will let you be stupid if you want to: fn bad_stuff(v: Vec&lt;i32&gt;) -&gt; i32 { unsafe { *(v.get_unchecked(v.len() + 1)) } } fn main() { let vec = vec![1, 2, 3]; let val = bad_stuff(vec); println!("{}", val); } The `bad_stuff` function here uses `get_unchecked()`, which skips the array bounds check, to grab some memory past the length of the end of the vector. Rust will happily compile this code, your `unsafe` block tells the compiler to trust that you're using `get_unchecked()` in a safe way, even though it can't verify it. Now, there's a decent discussion that can be had on this topic: 'usual' Rust code will of course be significantly safer than 'usual' C code, since you have the compiler's assistance. Unfortunately, security is often about the edge cases. While it's true that not wrapping malloc would have prevented Heartbleed from happening, and most C projects don't wrap malloc... it did happen. Once Rust starts getting used in the wild, we'll see Rust with security problems, I'm sure of it. That doesn't mean that Rust has no value, just that we need to be realistic about the tradeoffs that any of our technologies make. Rust is not a panacea, even if it does cure some of your aches and pains. 
Thanks for the feedback and the pull request. Regarding #1. In C++ we can overload the operators simply by declaring fx operator+, which gives me the possibility to overload as many times as I like, fx 'float + float3' and 'float3 + float'. (Which you already know I'm sure :) ), so I'm wondering what extra flexibility I gain by having to type those extra two lines. And regarding #3, is there then an idiomatic rust way of 'hiding' const values only used inside a struct?
&gt; The whole raison d'etre of Rust and its somewhat unique lifetime and ownership system is that it promises to eliminate a whole class of bugs, or at least mitigate them. I get it. I really do. Rust makes it easier to write safe code. The thing is, when these arguments happen in the context of Heartbleed, the argument isn't about idiomatic C code or idiomatic Rust code. Its about a _poor implementation in C_, and how a _good implementation in Rust_ would solve the problem. Its insincere, and its not only unfair from the C perspective, but it also opens up a justified reason to bitch slap poor Rust arguments; Rust can't implement Heartbleed you say? In fact, you can implement Heartbleed using non-idiomatic, shitty, and idiotic Rust just like the non-idiomatic, shitty, and idiotic C code did. The typical response is a No True Scotsman retort (you wouldn't use "unsafe" blocks in security critical code!), and it just goes straight to shit from there. EDIT: I want to add, I do very much like this point: &gt; What we need is a concrete example of how it goes about doing achieving that, and also the limits of what it can do. I would very much like to see this.
When Ember gains FastBoot, this should be fixed.
I believe print! is still using old_io. Either write to io::stdout yourself, or use std::old_io::stdio::flush().
Thank you! It's working as expected now.
My point is that there are subtle and not so subtle security bugs. This one was the latter. They made a horrible design decision favoring performance over integrity of the program, which is totally against the standards of defensive coding in a security critical project. The problem is this &gt; [OpenSSL uses an internal allocator for this and other tasks, and that allocator doesn't zero memory when it's allocated or freed.](https://mikeash.com/pyblog/friday-qa-2014-05-23-a-heartbleed-inspired-paranoid-memory-allocator.html) They explicitly chose to do this. They implemented this. It's graded facepalm by people who care about engineering for security.
For the first one it's basically that in C++ I overload + by float3 operator+(const float3&amp; rhs) const { return float3(x + rhs.x, y + rhs.y, z + rhs.z); } whereas in Rust I have to write impl Add for Vec { type Output = Vec; fn add(self, rhs: Vec) -&gt; Vec { Vec { x: self.x + rhs.x, y: self.y + rhs.y, z: self.z + rhs.z } } } I prefer writing as little code as possible, so two extra lines pr. operator overload, not counting the closing backet, is something that I notice. So what I'm wondering is what the benefit is of those two extra lines. I'm sure that the Rust core team is well aware of how operator overloading is handled in other languages and wouldn't require extra lines of code if they weren't needed. Adding a separate Add trait that you have to implement makes sense in the context of Rust, but I don't understand the need for 'type Output = Vec;' as that will always match the return type of the implementation or? As for the second one it should probably have read 'used inside a structs's member functions. Fx, in C++ again unfortunately, class LeafNode { static const int MAX_ELEMENTS = 42; // Magic constant used everywhere in LeafNode. .... }; The constant doesn't necessarily have to be private, but it would be nice to be able to declare a static const member inside a struct to signal that that constant 'belongs' to the struct, in this case the maximum number of elements stored in a leaf node. What I'm wondering is what is the equivalent way of writing that in Rust? Do I just have to declare a const value outside the struct? Or can I do something else?
IIRC, it respects `--prefix`.
How about something along these lines... fn watchdog&lt;F&gt;(f: F) where F: Fn(), F: Send + Copy + 'static { for _ in 0..3 { if std::thread::spawn(f).join().is_ok() { break; } println!("Thread panicked, restarting."); } } fn main() { fn print_hello() { println!("Hello"); panic!("Oh no!"); } watchdog(print_hello); }
That was really helpful. I updated the README with my findings.
No, that does nothing. You could put the "interesting" stuff in an UnsafeCell, like you did with your wrapper. Then just dereference the pointer, don't do that .get() stuff. Also note that if your lock-free datastructure relies on atomics, atomics can be mutated through &amp;, not &amp;mut.
The problem is that I wrote a `clone_each` back when `cloned` didn't exist, so in my brain, `cloned` isn't a standard extension method. :P
I would like to remind all participants in this conversation to keep a cool head here.
Sorry. I'll stop. I hope no one took my points as personal, that wasn't my intent.
Gates do it. It hasn't been "solved" because the problem doesn't "exist" yet (once we go into beta we'll change). Maybe it could be done with features `#[cfg(some_feature)` or by version `#cfg(rustl_version &gt; 1.2)`, but solving it now would be rushing a solution to a problem that we don't even know how it'll be yet.
&gt; I just think these "Heartbleed and Rust" arguments need to stop. No, they really don't. It's an interesting argument to be had, and it isn't doomed from the start. What the problem is that *people need to agree on a common set of fundamental definitions*. In other words, they need to have their underlying axioms in common ("axiom" feels pompous in this context, but it's the best word I could find). Here's one set of axioms that someone might have: - A language is defined by its specification. - When one is talking about what is possible and not in a language, one is talking about the specification of that language unless specifically stated otherwise (i.e., modulo bugs in the implementation). - A language might have escape hatches that allows the programmer to break the semantics of the language. This might be true for any language with an FFI. Rust has `unsafe`, Haskell has things like `unsafePerformIO`. When one is talking about what is "possible" to do in that language, you might reasonably talk about what is possible when using those escape hatches. But you have to draw the line *somewhere*: is it reasonable to say that Rust code that doesn't use `unsafe` anywhere is still potentially unsafe because one of the libraries that it uses uses an `unsafe` block? So you have to have some common ground here: you might argue that Rust code that doesn't use `unsafe` and only uses widely used and tested libraries can confidently called **safe Rust**. That's one set of common axioms. Here is another: - All languages that have equal computational power are just as *dangerous* as each other. Hah! There is nothing that is stopping me from reinventing manual memory management in a java app: maybe I create a class which stores a large byte array and lets other objects claim and free memory from this array. Then I let all critical, potentially secret data be stored in this array. oops. There is nothing stopping me from creating a Haskell library in which all types are like this: `String`. Also all functions take a list of string as an argument and consume any number of those elements (Oh yeah, and you also get a list type: that's it though. If you want a list of multiple types, you can also encode a list as a string, so now you can have a list of numbers, strings and lists, for instance). Also, I reinvent things like addition to handle addition of strings in a "weakly typed way": if the two strings only contain numbers, convert them to numbers, add them, then convert to a string and then output that string. If one of the strings looks like a non-number, concatenate the two values (lol). Now I have, effectively, a weakly typed Haskell library! Similarly, if the participants in this debate don't have some kind of common set of axioms, nothing is stopping either party from continually moving the goal post, all the while being in some way credible... in the end you have arguments to the effect of "well the Rust compiler is not proven to be a correct implementation, not to mention that most CPUs that Rust code is running on top of is not proven correct". Just agree to some damn common ground beforehand.
Thanks! I didn't know they propagate, that's great to know!
No Turing-complete language can "prevent" heartbleed; so the basic premise that keeps getting discussed is a bit silly. A more sensible discussion, given that, would be "just how high can/should the bar be set for making heartbleed more difficult to do accidentally?" Or, picking an arbitrary point as an answer to that question, "does Rust meet that goal?" Optionally, "does Rust do so easier and more thoroughly than defensive C programming" - but does anyone seriously argue that it doesn't?
&gt; Is it possible to assign a specific lifetime to a new String A lifetime basically denotes a (possibly temporary) area of memory. For example, the scope of your function is a lifetime, and the area of memory represented is the stack space allocated by that function. So to spawn an object with a particular lifetime, you'd need to be able to allocate memory in that lifetime's area. What if the lifetime was the stack frame of the caller? You'd need to somehow add memory dynamically to the caller's stack frame, which is pretty tricky since your code is already using up the stack right above it. And in general, code has no idea how to allocate in some arbitrary other region of memory. Now, in this case, `description` wants a `str` returned with lifetime no-shorter-than the passed in MyError's lifetime. One way to guarantee that is if MyError had a `String` field, then `description` could return a reference to a slice of that field. Another way is to just return a static string which has lifetime equal to the entirety of your program (and `'static` represents the area of memory your program allocated for global variables). TL;DR: No.
I followed the link through on `try!` syntax. I have an idea, but I haven't heard it yet, so it must be crazy. Is there any chance of some kind of namespaced macros at some point post-1.0? Eg. I have a class `Foo`. In the `impl` for `Foo` I have a macro, which can apply to a `Foo` in chaining positon. Thus the following code: let x = Foo::new(); let y = x.my_macro![wild macro syntax]; might desugar to something like let x = Foo::new(); let y = Foo::my_macro![x, wild macro syntax]; Where `Foo::my_macro` is a macro namespaced under `Foo`. This would allow `try!` to be a a macro acting on `Result` and `Option` and other error types I might invent. The resulting code would look like. let x = result_fn().try!().can_error().try!().to_something().try!(); Which seems highly ergonomic, compared to let x = try!(try!(try!(result_fn()).can_error()).to_something()); or let tmp1 = try!(result_fn()); let tmp2 = try!(tmp1.can_error()); let x = try!(tmp2.to_something()); Which is what we currently have. Crazy?
The macro system will almost certainly be seeing a revamp in the future, but I don't know if anyone has proposed the exact mechanism you describe. With regard to `try!` specifically, lots of people (myself included) have our fingers crossed for the proposed `?` sugar which would turn your example: let x = try!(try!(try!(result_fn()).can_error()).to_something()); ...into: let x = result_fn()?.can_error()?.to_something()?;
Is it too late to get some of this sweet Rust flair? :) "clap" would be awesome! https://github.com/kbknapp/clap-rs 
I think the fundamental problem with these "Heartbleed and Rust" conversations is that they start with a false assertion by a Rusticle: That Rust is always memory safe and Heartbleed *can not* happen in Rust. This opens wide the possibility of showing that a _very_ poor, almost comically poor, implementation can implement Heartbleed in Rust. Case and point: The entire reason the Tedbleed article was created. If, instead, Rust people would make assertions like: * Its harder to have memory errors in Rust * Its easier to write safe code in Rust * Rust can help prevent such errors Things would go much smoother. None of the assertions above are absolutes. They can easily and clearly be shown to be true, without aggressively pushing Rust up peoples noses. The moment an absolute is asserted, Rust can be accused of being overzealous, citing the unsafe block as its "fatal flaw". I love Rust. I really do. I just want it's community to communicate it's awesomeness in a realistic and accurate manner. To do otherwise will only reflect negatively on the language and its ambitious and important goals. 
&gt; You're asking someone to use a function meant for reporting things like spam, abuse and other moderator-level things, for not towing the party line? On Reddit that's for reporting breaches of any rule. We have a rule about zealotry: &gt; When mentioning other languages, keep the discussion civil. No zealotry! And taking them down a notch just means a comment; not any moderator action. 
As soon as "stable std::io" and "I have time" intersect, I plan to rewrite my gif.py and fastdupes.py utilities in Rust. (gif.py is an optimized tool for determining whether GIFs are animated, counting frames, or doing basic corruption checks. fastdupes.py is a faster alternative to fdupes.)
Oh no, I'm an anti-zealotry zealot! Sadly, I must report that I continue to feel no qualms about correcting people who misrepresent the language.
I remember when integrating patches was Graydon's full-time job! And the tree was *still* broken all the time. I'm honestly curious how big projects without automatic continuous integration manage to cope.
This ICE has been totally blocking me and several other people for almost one week now. Paradoxically I'm happy that Servo is getting it too, because that means that it's going to get some attention! 
Thanks for your feedback. I see I might misinterpret the way Rust handles `panic!`s. So, on the performance side of things it's pretty much apples and apples, right? The second part is the safety concern. What do you mean by that? Are you talking about `C`/`FFI` compatible interfaces? 
I usually either write a simple path tracer or an image processing command line util. But since the functionality of my latest image processor has grown to a size where it is actually useful, I'm back to writing renderers.
Thanks for your comment. I see. It's actually coming from steveklabnik presentation, as he pointed out https://www.reddit.com/r/rust/comments/2zc7kc/the_sweet_flavour_of_rust_introduction_ppt/cphqy67 But yeah, it's something I got wrong and confused about. Point taken. 
Thanks once again. After taking a closer look and reading through other comments here I see what are you pointing out here - basically it's apples to apples in in Rust vs C++ comparison. Thanks for pointing that out.
Thanks! This does help a lot. I deliberately used the following structure: struct Node { data: i32, next: Option&lt;Box&lt;Node&gt;&gt; } struct List { root: Option&lt;Box&lt;Node&gt;&gt; } And here is my append() code which I trying to figure this out (i.e. a very C++ way of doing things): fn append(&amp;mut self, data: i32) { let mut temp = &amp;self.root; loop { match *temp { Some(ref n) =&gt; { temp = &amp;n.next; }, None =&gt; { let mut temp2 = temp; *temp2 = Some( Box::new(Node{data: data, next: None}) ); break; } } } } On the documentation part as an example, checkout the slides by one of the core contributors of Python, Raymond Hettinger: [here](https://speakerdeck.com/pyconslides/transforming-code-into-beautiful-idiomatic-python-by-raymond-hettinger-1) Some thing like this will be awesome. 
AFAIK this is a [bug](https://github.com/rust-lang/rust/issues/20400). I have also run into it and into a [related one](https://github.com/rust-lang/rust/issues/23341).
I guess it should be possible. You can find `mmap` and friends in [libc](http://doc.rust-lang.org/nightly/libc/), but they are considered unsafe, so you will have to be as careful as with C when working with them directly. Rust can cross compile to the Raspberry Pi without any major problems. You may have to fiddle around a bit to make ld understand where some libraries are, but the rest should work like a charm. I'm actually maintaining some [Rust bindings for WiringPi](https://github.com/Ogeon/rust-wiringpi) for my lamp automation project. They may or may not work today (they did a couple of days ago), but give me a shout or send me a pull request if they are broken. Edit: I looked at the build instructions again and I can't promise that they will work. I should update them when I get home. Edit again: I have written some new instructions in a separate repository. They can be found here: https://github.com/Ogeon/rust-on-raspberry-pi
I guess I overreacted. I still get some cult-ish vibes here sometimes though.
I guess one would need a dedicated environment (or multiple for different targets) for the performance tests, and then one also need to account for this environment being upgraded over time. It is not an easy task.
How would a non-recursive version of append() look like? I struggled a lot while iterating over the list to go to the last node and place the new node there.
Since you mention one of the GNU utilities, there's actually a rewrite of them all in Rust and I'm sure some of those guys/gals wouldn't say no to some help! https://github.com/uutils/coreutils There's also exa which is a drop in replacement for ls with some added features https://github.com/ogham/exa
Yup, hoping it is fixed soon so that source links work in my own rustdocs too.
Yeah, definitely; the difficulty of doing it properly is presumably part of the reason it doesn't exist yet. :)
I might be poking at this later, where later requires: - Shouldn't be a time of lots of Rust churn, so not until after the beta at least - I should have time (possibly never?) - I will have to have figured out how to do this I'll probably work on Servo's perf reporting infra first, since I understand the Servo build system better and already have access to everything for experimentation. So, still a while off unless someone else does it :P
&gt; Is there any existing doc/site/blog which talks about "rust patterns"? There isn't, yet. I've mostly been trying to document the language itself. Gotta get that done before we can do higher level stuff like that. &gt; Is there any plan to write a doc which explains idiomatic way to do things in rust when 1.0.0 ships? Yes. it's not actually linked from anywhere because it's not really 'ready', but http://doc.rust-lang.org/nightly/style/ is the current copy. It does have some pattern / idiom stuff in it. Really, this is the kind of thing we'll see more of _after_ 1.0, as we get people actually trying the language out. Patterns are something that gets discovered over time, and most people don't want to write up how to do things in a language that isn't even done yet.
Rusti exists? I thought it was no longer developed since like a year ago. What version are you running? 
On mobile but look for murath/rusti
1. It is actually both. It is a point, because it is literally just x, y and z in 3 dimensional space. However, you can call it a vector when you're using it to represent a line from 0,0,0 to your point. Your dot, cross and length functions all make this assumption. 2. yea, &amp; on the type makes it an immutable reference. But when you pass a reference to an object, to a function, that function has now borrowed that object. If you pass a mutable reference (&amp;mut) to an object to a function, that function is now the only one that can borrow a reference to that object. 
The idiomatic way of doing low-level memory management is to write a crate that wraps those [unsafe](https://huonw.github.io/blog/2014/07/what-does-rusts-unsafe-mean/) bits ("open /dev/mem, mmap it, write to location pointers") in a safe interface. Then, write the rest of your code using only the safe interface. The unsafe code will be dangerous like C (if you make a mistake you may segfault, or worse). But if the unsafe core is *correct*, no bad things will happen in practice. The safe interface should be free of data races, and memory safe: no matter how you use it, it will never cause a segfault, provided that the unsafe code is actually correct. That's a guarantee you should provide, the compiler won't enforce it.
&lt;plug&gt; And if you like working on things like these, please apply for the [Operations Engineer](https://careers.mozilla.org/en-US/position/oymA0fwe) position! ^(sorry about posting this everywhere) ^^we ^^really ^^need ^^someone ^^working ^^on ^^the ^^infra ^^^it's ^^^all ^^^chewing ^^^gum ^^^and ^^^duct ^^^tape ^^^^help
Nice!
I built a grep clone that used channels between threads: you had a thread reading stdin, a thread filtering things and a thread writing to stdout. Totally overkill, but really fun to do.
We're looking into a replacement via [generic conversion traits](https://github.com/rust-lang/rfcs/pull/529) instead. Should have more news soon.
You always have at least two versions: stable and development. Without a mechanism like described it makes it hard to gradually add support for development features in a library while still having it compile with the stable version.
Initial thread (for the lazy): http://redd.it/2xwe4r
Unfortunately, there's really not much support for autocomplete yet, though there's been some talk of having rustc itself provide some hooks in the future. Right now, the best (only?) option is Racer: https://github.com/phildawes/racer Racer only has plugins for Vim and Emacs though, and if you're not comfortable with either of those editors you'll have to find (or create) a plugin to call racer on your own.
Sublime Text has plugins for rust auto compete. It isn't really an IDE however.
There's also unofficial plugins for Sublime Text 3 and Github's Atom.
Rust will have three channels: nightly, beta, and stable. The mechanism that allows you to add new features is feature gates. http://blog.rust-lang.org/2014/10/30/Stability.html has more.
&gt; Hopefully you are OK with that :) I am super okay with that! I was just trying to convey that sometimes, slides without context give the wrong impression, but since I didn't actually see your presentation, I couldn't tell if you actually pointed out that it was wrong or not :)
Everyone has CI, but it depends on how they use it. I don't know about Linux, but with Firefox there are a bunch of people who sheriff the tree full time. There are a bunch of integration repos onto which stuff gets pushed and CId in a batch, and then stuff gets merges onto the main repo -- but things to break often, and then they have to backout patches and close the tree. With the current system of Rust, I sheriff the tree mostly (not my job, just a volunteer thing I've been doing these days), but it's not much work -- I just create rollups, prioritize PRs, and make sure stuff is smooth in general. The only backout I've had to do is for accidentally merging an unapproved PR, but there never has been breakage. Without homu we wouldn't need to do rollups, but we would need to have people backing out bustages and keeping a close eye on things. It's a tradeoff. Having CI and homu is still a time saver. But someone still has to put time into tending the tree. Just less.
Cool -- does it improve throughput?
Emacs is easily the most powerful editor/IDE out there and there is good [Racer](https://github.com/phildawes/racer) support for it.
By default it's probably not the most productive editor, but Evil mode fixes that. It's real power lies in the massive ecosystem of plugins and ubiquitous language support.
What about [this](https://www.youtube.com/watch?v=px7tVrY3QLg&amp;t=1m42s)? Is it still supported?
The Rust plugin has sublime-build files for both Rust and Cargo, so it becomes really smooth to build with it (especially since clicking on a file/line in the build panel brings you to that file/line). With RustAutoComplete, it's really good, though RustAutoComplete was slow last I tried it.
(This doesn't answer your question) In my opinion, if you find yourself in the habit of using cutting edge (aka, new) programming languages, you might as well get comfortable with using some power editor. Even if you feel that IDEs are ultimately better, a power editor (+ external programs?) is often the best compromise you'll be able to find when there really is no IDE (plugin) for the language. Plus a power editor is more of a jack-of-all-trades with regards to other programming languages.
I agree, I've been using ST 3 with those two plugins and it's met all my needs! I had been procrastinating using SublimeLinter-rustc out of laziness, and now I'm kicking myself because its awesome and saves me tons of time. 
Learn vim or emacs. Edit: way to downvote me for giving good advice. Both vim and emacs will do what OP is asking and are free. And will increase productivity after a slight learning curve. /shake my head. 
I can use Vim and I use it a lot (every second day?) when I write scripts/configs on VPSs, but when working on my desktop I prefer IDEs. I mostly write desktop/mobile applications, in Qt and on Android, so IDEA and QtC are much better solutions for me.
I feel you... being block as well.
Of course. Sounds like a cool project!
They currently do not.
Are you sure this is the place/file where the error occurs? I ask because * The error says it occurs on line 44, the snippet you posted is &lt; 44 lines long, and in your snippet, the ``use`` statement is in the very beginning * I can't reproduce i a [simple testcase](http://is.gd/kGbNXs) Of course this doesn't mean you're wrong, I just wanted to make sure :)
&gt; open /dev/mem, mmap it, write to location pointers You can do this in any language with a foreign function interface. I've done similar things in Haskell, and in Python with ctypes. Rust has a good FFI and a performance profile similar to C. So it's a great choice.
Well crap :(. It was complaining about the import in the test mod inside that file, I didn't notice the line number. My bad.
The zero-copy performance boost is really neat. Congrats!
That sounds like a nightmare.
It would be great to replace our usage of [hoedown](https://github.com/hoedown/hoedown) with a version written in Rust. :)
I kind of vetted the project by first looking on [crates.io](http://crates.io) to see if there were that many implementations out there. It looks like there's a couple, but none in widespread use. I figured, why not start a project that will: 1. Challenge me and help me learn the language 2. Create a library the community might find lots of use in I don't know how soon I'll get it done, but I have every intention to finish this one and since CommonMark, it seems like writing a Markdown parser has never been more accessible :) The first version will likely be extremely rough around the edges, code-wise (but as far as implementing the standard, I will make it pass the benchmark). After I've finished the first version, I'll definitely ask /r/rust for a code review and if anyone's up for it, I'll be more than happy to do a cooperative refactoring project on it.
*Subroutine* #Basic4Life
If you're using it only in tests, that means you will get a warning when building normally and no warning when you're compiling for testing. This is expected, you might want to prepend `#[cfg(test)]` to `mod test`.
To piggyback onto this, I checked out this code segment out of curiosity. When I have `rustc` expand the code generated by the macros, I find that print! and println! do point to `std::io` but they're still not working as the OP would expect. Is that by design? If so, do you have to write directly into stdout and flush the buffer from here on out?
TIL rendering engines are a series of clever hacks to work around past mistakes.
I'm so sorry. 
I don't think this is SEME issue. This seems to be temporary lifetime issue. My understanding is that the compiler can figure this out, but refuses to do so to have a consistent rule. Unfortunately, this consistent rule is not documented yet. Documentation (and justification) of the temporary lifetime rule is an issue [#12032](https://github.com/rust-lang/rust/issues/12032).
The custom allocator at least acted to hide the vulnerability from external tooling. The assault on the allocator mostly stems from a [conversation](http://comments.gmane.org/gmane.os.openbsd.misc/211952) on the OpenBSD mailing list. Read down to Ted and Theo's posts.
&gt; The zero-copy performance boost is really neat. Congrats! And after that speculative parsing. We need to benchmark html5ever (another one of my neglected tasks :( ). 
Great project name! Made me click and read.
I'm already doing the `#[cfg(test)]` thing, the issue was I apparently added the `use` statement specifically for the test and ended up not needing it, and then forgot it was down there.
Why `vec.len() - 1` is not calculated before call `vec.remove`? Does rust use something like *call-by-need* evaluation strategy? I don't expect it. It seems that borrow checker consider whole expression before argument are calculated. And this might be the real issue.
That's not the issue, it's that the evaluation order, logically (what is seen by borrowchk) is `&amp;mut self`, then `vec.len() - 1`, then `vec.remove()` -- this is in the order of parameters. Until someone special-cases `self` so that it's not a regular argument in this regard; we're stuck here. &gt; It seems that borrow checker consider whole expression before argument are calculated. I'm not sure what you mean here.
I was trying to explain that it is wrong that method captures `&amp;mut self` before calculating arguments. We treat `self` as the first argument and this breaks the expected order of borrow check in this case.
The simplest (semantics-wise) solution is to "evaluate" the auto-borrows after evaluating all the arguments. The compiler doesn't **need** to "evaluate" `&amp;mut self`, just `self` as an lvalue, the borrow is a type-level construct, it can be applied after evaluating all the other arguments as well. Consider `foo().bar(baz())`: changing the *actual* evaluation order (calling `foo` after `baz`) would be a breaking change, and a quite ridiculous one (only the smallest programs would not be affected).
``` char* main = "usually a programming blog"; ``` It should be `const char*`.
&gt; Consider `foo().bar(baz())`: changing the actual evaluation order (calling `foo` after `baz`) would be a breaking change, and a quite ridiculous one (only the smallest programs would not be affected). It's not obvious to me why this order must be defined and why the majority of programs would rely on it.
I see the point about footguns, not sure where to look for this in the reference though. But actually relying on this behavior seems like a no-no at least for the sake of readability… Is it really used just about everywhere?
I meant that the borrow of `&amp;mut self` (not `vec.remove`) starts before `vec.len() - 1` is evaluated. It doesn’t have to (which is why this is fixable), but this explains the current behavior.
In general, in languages in which side effects are not controlled, the order of execution must be defined.
 let mut pipe_fds: [c_int; 2] = [0; 2]; You can only use suffixes for native Rust types. Also, you may not need the type annotation if you're passing this to something that can constrain the type.
I'm contributing to https://github.com/ujh/iomrascalai which is a Go playing engine. I wrote some random stuff for it for the command parser, haven't even touched the engine code. There's some easy tasks on the Trello board: https://trello.com/b/3lIYxva7/development like board setup and other things like testing and code refactoring that I'm also doing
&gt; Then we can write bindings for every language under the sun and bring fast, correct, memory-safe HTML parsing to the masses :) An awesome feature of Rust: - write provingly safe code - expose wrapper API in C for easy integration in about every language Security + Interoperability make for a very compelling usecase!
A surrogate pair can be split between `document.write` calls, with any JS code in between. &lt;script&gt; document.write("&lt;foo&gt;\uD83D"); var foo = document.getElementsByTagName("foo")[0]; document.write("\uDCA9&lt;/foo&gt;"); &lt;/script&gt; UTF-8 can't represent bare surrogates in the input stream, but WTF-8 can.
Oh I see, thanks. That's pretty looks pretty annoying to deal with!
Have you tried logarithms?
Linked lists aren't intrinsically unsafe. This looks like a weakness of the borrow checker to me.
Rust excels at reasoning about hierarchical ownership, which a doubly-linked-list specifically *isn't*.
Question: what do you mean when you ask for a "mutable linked list". Are you trying to mutate the data, or are you trying to change the structure (eg to delete an entry)? 1. Mutating the data: If you wrap your `T` in a `Cell&lt;T&gt;`, then you'll have inner mutability, and you can now modify the `T`. 2. Mutating the structure (insertions and deletions) should be possible to do safely, as long as you aren't holding any references to items further down the list. I could try and write one, but I get the feeling you've already made an attempt. Care to share it? Of particular note is that the "current" node should be mutable, while the nodes you're moving around should be immutable.
And this is a perfect demonstration of our need to remove suffixes. However, [type ascription](https://github.com/rust-lang/rfcs/pull/803) (the feature allowing this feat) and the [implementation of infrastructure](https://github.com/rust-lang/rust/pull/23017) to evaluate the results of inference (after suffix removal) came a bit too late for the 1.0 beta. That code would then look like: let mut pipe_fds = [0: c_int; 2];
Where did /u/BenFoppa mention a doubly linked list? His example is singly linked.
Oh you're totally right; I'm used to having tons of trouble with doubly-linked lists. Rust can handle singly-linked no problem.
&gt; And "BTree" never refers to a binary search tree. You're pretty sure of yourself. Here is "binary search tree" called a "BTree": http://www.youtube.com/watch?v=x6At0nzX92o As for B+Tree and B-Tree, you're free to keep assigning precise meanings to these loosely formulated terms. :-p But you might find that in the literature these terms often overlap.
This is singly-linked, though. The ownership is obvious, and again, it works for other nested data structures like trees. It doesn't need to inherently be unsafe, and specifically, if I can denote explicitly to move a reference instead of borrow it, then this becomes easier to express. Edit: Oh sorry somebody already pointed out this is singly-linked.
Can you actually do this? I couldn't find a way to make `mem::swap` or `mem::replace` work. The problem is that you can't say `list = next;` because `list` is borrowed to construct `next`. For the same reason, I can't use `mem::swap` or `mem::replace` because I can't `&amp;mut list` because `list` is borrowed.
This problem isn't specifically for linked lists: it's for any recursive data structure with ownership this obvious. It's not a problem that needs to be solved using data structures that explicitly *opt out* of compile-time ownership checks. This is exactly the kind of thing that should be in Rust's wheelhouse, but doing this safely and efficiently turns out to be very ugly.
You're wrong; and you're falsely optimizing. `Cell` is really efficient. It might even compile into no-op, I'm not sure. extern crate test; use test::Bencher; use std::cell::Cell; const max: u32 = 100_000; #[bench] fn bench_cell(b: &amp;mut Bencher) { b.iter( || { let x = test::black_box(Cell::new(0u32)); for i in 0..max { x.set(x.get()+1); } x }); } #[bench] fn bench_raw(b: &amp;mut Bencher) { b.iter( || { let mut x = test::black_box(0u32); for i in 0..max { x = x + 1; } x }) } Which gives the results: test bench_cell ... bench: 254140 ns/iter (+/- 6189) test bench_raw ... bench: 261487 ns/iter (+/- 9806) I guess `Cell` does compile to no-op.
Here's a safe version of a mutable singly linked list with the functionality you demonstrate: https://gist.github.com/Gankro/155234908dae0d984fe5 Right now I'm doing some redundant work because you need to "forget" the next node after you check it.
Please add suggestions to this list if you have interesting projects/issues that are suitable for beginners. Thanks!
Did what? Explicitly forget the next node? (useful to know what people have trouble with). In theory non-lexical borrows could permit a "nicer" pattern for this, although you'll still need to forget the node for a bit.
So you want something like the following? fn find_mut(&amp;mut self, t: T) -&gt; Option&lt;&amp;mut T&gt; { let mut current = self; loop { match *{current} { List::Link(ref mut link) =&gt; { if link.val == t { return Some(&amp;mut link.val) } current = &amp;mut link.next; }, _ =&gt; return None } } }
Also, crates.io should indicate other crates hosted on the same Github repository. Or have an optional field in Cargo.toml for "related" crates. (or both - crates on the same repo would be the default value for this field) Another thing is that crates.io doesn't show reverse dependencies.
If I wanted to propose a redesign of the crate landing page, to rearrange things and have more of the relevant information show up earlier, is there an RFC process or anything like for Rust, or should I just file a ticket, or just send a pull request with the proposed change?
A ticket with the proposed changes would be most welcome. Please CC me.
I've heard that emacs is a good o's, too bad it lacks a decent text editor. Sorry couldn't resist
This is a presentation done by a member of [Korean Rust Users Group](http://rust-kr.org/).
Note that it's the same with matching on `NULL`, but I'm not dealing with that currently.
There's ptr.is_null() for that case. 
Yes, but you have to do let ptr = nullable_fn(); if ptr.is_null() { panic!(); } instead of let ptr = match nullable_fn() { 0 as *mut c_void =&gt; panic!(), p =&gt; p, } which I believe is much nicer, and follows the rest of the language better.
That was before Evil mode came along! Now it's got an awesome text editor, and it's still a great OS.
You could also do let ptr = match nullable_fn() { ptr if ptr.is_null() =&gt; panic!(), ptr =&gt; ptr, }
Huh, that is kinda weird, I did not realize you could if inside a match... Well, now I know. Thanks!
In the past, it was [half-jokingly suggested](http://www.reddit.com/r/rust/comments/2bhwgc/what_does_rusts_unsafe_mean/cj5lavk) that we use "trustme" instead of "unsafe". Although I don't think it is a good idea, I do think it can be used as a good explanation of what "unsafe" does.
Thanks for your advice!
So then how about: match mmap(...) { ptr if ptr as isize == -1 =&gt; ..., ptr =&gt; ... } Though, frankly, you should probably just wrap `mmap` up into something nicer that returns an `Option` or a `Result` type.
`unsafe` is declarative, not imperative — it declares a function or block that won't be vetted by the compiler/borrow checker, thus does not benefit from the usual safety net.
We will have language issues around "unsafe" for a long time to come. "Unsafe code" -- is that *code in an `unsafe` block* or is it *code that exhibits undefined behavior* or is it *code that may not be safe for all inputs*? I'd try to use the term "unsafe block" to talk about those blocks of code inside `unsafe`.
Here we find the birth of a new Rust idiom, obscure and ugly but useful!
I'd love to have better category browsing. For example listing all known tags.
Well I guess I just don't get why a builder whose method `foo` is somehow entangled with any `bar` that supplies an argument to `baz` is a good idea or can happen often. Sorry for dragging this out.
 *you* have to make it safe, the compiler can't. your directive to the compiler can only make it less safe. unsafe is already used in other languages for that purpose ..I think they benefit more by using a familiar term (principle of least surprise, less inconvenience for people forced to use multiple languages) than by coming up with a different term (even if it did make slightly more sense) "C++ is an unsafe language" .. but you make c++ programs safe empirically; (they work enough for it to be ubiquitous)? we don't say c++ is a 'trusted' language or 'make safe' language, rather we think of a c++ program as a giant unsafe block. "here's a part which is not guaranteed to be more safe than anything you can write in c++". "by contrast, the rest of the program is proved safe" 
The point of this is to make life just a little easier, without actually removing anything. If you're doing a lot of ffi, it gets bothersome creating multiple wrappers for every function that might return a NULL pointer or a (void *)-1, that you might use only once. Although this works, it is extra work for, IMHO, no reason.
There are unsafe blocks written by people who actually know what they are doing, which are safe. Then there are unsafe blocks written by newbs or optimists which actually really are unsafe and make everything around them unsafe as well. So those are safe-unsafe blocks versus unsafe-unsafe blocks.
Do you mean keywords? There is https://crates.io/keywords but its not as usable as it could be.
Last time I checked that link is not present anywhere. I found it by "url hacking" at least. So that's one thing that could improve.
In same vein, I also half-jokingly suggested they be called `dragon`. As in here be dragons.
The language assumes that they are safe though.
If it was called `safe` to begin with, nobody would complain. "Oh, you have to mark that code `safe` for the compiler so it knows it's safe even though it can't prove it" and nobody would question it. Then people would rather talk about "unchecked" safe code that is marked `safe` rather than "unsafe" safe code.
My understanding is that it isn't nestable because it uses thread-local variables to save state, and doesn't try to preserve previous state (I could be wrong here though, that's just the impression I got from reading the code some time ago). I'm sure it *could* be nestable, though - after all, unwinds in Rust use the same underlying mechanism as C++ exceptions. I just don't think that the core team wants to support this use. Until not too long ago it was possible to use `Thread::scoped` to essentially catch panics - a panic inside a scoped thread would stop at that thread's boundary instead of propagating to the parent thread. That was changed because it exposed code to the problems of exception safety outside of destructors. I believe a request to change `try` wouldn't be fulfilled, because of the same argument.
I created a batch-file. Not very neat, but it works: if "%~1"=="" ( echo NO PARAM ) else ( echo FILE SUPPLIED: %1 rustc %1 if "%~2"=="" ( echo NO FILE TO RUN ) else ( echo RUN: %2.exe %2.exe ) ) and my custom build is the standard rust-build, but the first two lines are: "cmd": ["run.bat", "$file", "$file_base_name"], "selector": "source.rust",
So should haskells `unsafePerformIO` be called `safePerformIO`? I prefer `unsafe` because the code ***is*** unsafe, it is ***not*** safe. You can say, "this block of unsafe code satisfies all the invariants of safety" but that does not make it safe, it still needs to be maintained carefully.
... and not where you'd actually like to use it, IME.
Haskell doesn't track which code uses unsafe features, though. GHC's [Safe Haskell](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/safe-haskell.html#safe-trust) extension uses the terminology "Safe", "Trustworthy", and "Unsafe". It's pretty complicated actually.
No.
If I were painting the bikeshed, it'd be `careful`. Because it's both a warning that the author *should* be careful, and a promise to the compiler that you *are* being careful, which seems to satisfy both sides of the should-we-call-it-safe-or-unsafe debate.
The cache in intel CPUs is basically a simple hashing scheme based on the lower-order bits of the memory. So something like: value_at_address(aaaa_bbbb_cccc_dddd) =&gt; cache[dddd]. In principle this makes sense: if you access a bunch of contiguous memory, all the variation will be in the lower-order bits, so each contiguous chunk of memory will go into a different bucket, minimizing cache faults (which are slow). However if you allocate memory that is *aligned* to some multiple, you're essentially fixing the lower order bits to a much smaller set of values. For instance if you align all the data you access to multiples of 2^12 = 4k, then everything you allocate will end with x000, which means that while your cache might have 2^16 = 65k slots for data, you've effectively truncated it to 2^4 = 32 slots. If you're allocating things of size 4k that you use totally, then that might work out; but if you're not using all 4k, then you're basically shrinking your cache.
hold_my_beer_and_watch_this
See http://doc.crates.io/config.html#configuration-keys.
Thanks. I already have a .cargo/config in the root repository folder with contents: [target] linker = "mpicc" but to no avail. On a different note, I manually added "-C linker=mpicc" to the rustc command string but to no avail.
Looking only at this [unwind.rs:123](https://github.com/rust-lang/rust/blob/master/src/libstd/rt/unwind.rs#L123) and kinda looking at [rust_try.ll](https://github.com/rust-lang/rust/blob/master/src/rt/rust_try.ll) (without any knowledge of personalities), I think `try` is preserving the previous state. But I'm probably wrong, since I'm looking locally and have no in-depth knowledge of the exception-handling system of LLVM or the underlying platforms. From the third paragraph from your reply and [this reddit answer](http://www.reddit.com/r/rust/comments/2ze539/managing_panicking_threads/cpi392h), I understand the following: the variables shared with the scoped thread/try parameter can be left in an inconsistent state. Can't this be bypassed if the closure passed to`std::rt::unwind::try` function is 'static, so inconsistent variables can't be reached? Of course leaving the unsafe variant in case you need to use borrows. And if this is the only reason to make the function unsafe, why the docs are warning us about nesting? If the nesting is no longer a problem, shouldn't someone fix them?
Another option would be to [make a Lisp](https://github.com/kanaka/mal/blob/master/process/guide.md).
QT?
Yeah, a nontrivial amount of functionality only exists via manual URL hacking.
Try to find bindings for your favorite platforms’ native toolkit.
https://github.com/kenz-gelsoft/wxRust, if you feel like upgrading it to a recent rustc.
Also, I don't know if it's helpful in this particular case or not, but sometimes you can get the real types of a symbol from the GDB. Here's something I've used while making an ICU transliteration wrapper: $ apt-get install -y libicu52-dbg $ cd /usr/lib/x86_64-linux-gnu &amp;&amp; gdb libicui18n.so ) print utrans_open_52 ) ptype UChar ) ptype UParseError 
Why don't you use cargo, instead of calling rustc directly? Then it's just `cargo run`..
I don't think there's very many stable/updated GUI bindings at the moment. /u/joshmatthews already posted rgtk, which I hadn't known about, but seems to be building correctly. If you just want an interactive user interface and don't mind text, there's a few decent console wrappers: [rustbox](https://github.com/gchp/rustbox/) and [ncurses-rs](https://github.com/jeaye/ncurses-rs)
Ah good point actually! That's probably true for a fair few MPI constants (MPI_INT and other char types for instance).
Then it looks like wxRust is dead. 
Yeah. It uses more of a web-focused tech stack (Ruby on Rails, Ember.js, ES6) compared to Stack Exchange's Microsoft-heavy stack.
Maybe we should fork it and update it?
I've checked out rgtk as mentioned by /u/joshmatthews, and I've tried it in linux, which was a great success. Check out their example programs and you can get a good taste of what it can do. The only problem I've found is that it doesn't support Windows yet, so you'd have to develop for Linux and/or os x.
Wow that's sneaky/clever/cool. +1.
*Didn't try moving into a temp at the beginning so I could reassign the original. I *did* try something like that once the original was already borrowed, but that didn't work.
I have been wondering about the possibility of a simple QML wrapper written in C++. Just load a QML file and provide some generic API to modify values and provide callbacks/events.
I'm not aware of GUI toolkit written in Rust, and it's really too bad that there isn't one. We may be doomed to writing glue code. FWIW there are bindings to a few native apis: * [Low-level](https://github.com/retep998/winapi-rs) and [incomplete high-level](https://github.com/klutzy/rust-windows) Windows * [Low-level](https://github.com/servo/rust-cocoa/blob/master/examples/hello_world.rs) OS X * [Incomplete low-level](https://github.com/Daggerbot/x11-rs) [X11](https://github.com/servo/rust-xlib) * [Incomplete](https://github.com/eyolfson/rust-wayland) Wayland.
https://techbase.kde.org/Development/Languages/Smoke lets you do it, but it's dog slow.
I ran into the same issue on my current project. I ended up writing the core of my program in Rust and the GUI in PyQT, using IPC to communicate data. Kind of round about, but it actually works quite well!
As someone who owns a lot of crates, I would just never in a million years use a feature like this. If I have time at all to write a proper change log, it's going in my repository root in a file called `CHANGELOG`. A link directly to that change log from crates.io is something I could get on board with.
`[0: c_int; 2]` *Yuck.* Rust expressions have historically tended not to be *too* incomprehensible to one familiar with C-class languages but not Rust, but this is a bit much. Alas.
There have been a few mentions of using servo to render you own custom HTML documents for the GUI, but it's probably not quite at the stage where that's a viable option.
See [qmlrs](https://www.reddit.com/r/rust/comments/2pl9m7/qmlrs_bindings_for_the_qtquick_gui_system/)
Cool, but I think you spawn a new thread for every connection? Not acceptable for servers with a lot of connections... Check out mio and see if you can make an event-based version of this; I'd use it.
That's a lot of money made in 2 days.
I have made good experiences with embedding a Webserver+REST interface in your own application. That way, you can host a website and show it in any browser. AngularJS was used to keep a single-page application in sync with the server state. Previously I have used Go for that, but Rust should do equally well. Have a look [at this example](https://github.com/Byron/godi/releases) to see how it looks if the website is done by someone who isn't a frontend developer (me ;)). The good thing is that this *should* work natively with [Thrust](https://github.com/breach/thrust) in case you want to ship the browser as well.
Basically any time you need to deal with events or the DOM. So an event occurs, you make a GET/POST/etc request to your Rust code, that does the heavy lifting, and then when that's done reflect the changes (if needed) back in your UI with JavaScript. If the library author wants to be fancy, I imagine they could set up a way to do binding so that you could just include a JS file and it would handle most of this for you, but I haven't given that much thought. It would definitely be a lot more work than my original suggestion though. But it seems like it could work nicely... a lot of UI libraries already work similarly to this (XAML for WPF, FXML for JavaFX, QML for Qt, etc). The difference being instead of using a custom markup language, you use HTML.
Actually, somebody else made a good point [in this same thread](http://www.reddit.com/r/rust/comments/2zqjix/what_would_you_use_for_a_rust_gui/cplv2x3). So you can leverage already existing libraries as well (AngularJS was used for this one).
The main winner claims it's a first: https://youtu.be/V99skqmTyiY?t=2m14s (I'm not sure if the person taking the interview belives him, though).
It was mentioned in the question 
It is interesting to notice that most of these exploit should be prevented by Rust. It seem to me (correct me if I'm wrong) that only the IE exploit could happen in Rust.
Strangely, it didn't list Conrod (submitted a pull request).
Unfortunately, I do want Windows support.
Yes, that's an option (also not at all a frontend developer :))
What's wrong with a few thousand threads?
The point of cross platform isn't to look better or worse than native, its to look exactly like it were native. And In all honesty, I dont think it does (unless something has changed quite recently).
Well that does seem to be the whole reason Rust has been developed. Is to try and eliminate these sort of bugs that are caused by C/C++, make programs more thread safe, and eliminate race conditions.
Cross platform GUI doesn't work. QT for instance claims to be cross-platform while providing a bad experience under OS X as lots of small things are different.
Pretty much.
Even then, it's a bit scary when you consider that those exploits might have been doing the rounds on the black market for some time already. And of course, there is also the "tip of the iceberg" effect, for those few bugs that were announced how many are still undiscovered or used for nefarious purposes? It's the kind of event that could make one paranoid.
In the first example, you don't actually use the value in any of the `match` arms. In the second one, `n` captures the value for use in the `println!`. It gets more interesting with destructuring, e.g. match someOptionalValue { Some(v) =&gt; { println!("Got a value: {:?}", v) }, None =&gt; { println!("There's nothing here!") } }
Do you mean that if the server test fails, then the client test should not be run? I don't think this is possible to express currently - all tests are usually launched in parallel.
The second example might look a bit silly, since you could just use `age` directly: match age { 1...12 =&gt; println!("I'm a child of age {:?}", age), ...and so on, but it could be more handy if the thing you're matching on is more complex, e g like this: match try!(people.getByID(someID)).currentAge() { n @ 1...12 =&gt; println!("I'm a child of age {:?}", n), 
This is a much better example - thank you! Am I correct in assuming that "n" (or "v" in your example) is only used / valid inside the match block? It's initialized when called by Some(v) and freed once the match is made?
I'd probably write a custom 'rusty' MVC GUI framework with libcairo (or Quartz on OS X) as the rendering backend. The existing GUI frameworks weren't developed with Rust in mind and mostly have some kind of "huge inheritance tree" architecture where you subclass widgets, etc. 
Supposedly speed. CPUs handle certain transactions to certain sizes. The most famous one is word aligment, a CPU has a word size (64 bit is becoming the santard) which fits into register. Many CPUs optimized their operations by also having everything be word aligned. A page is how you normally handle a "chunk" of memory, this is defined by the system. By aligning everything with a page you guarantee which "chunk/page" everything is going to get loaded with, and can the focus on only page chunks and offsets within them. The problem, as the article describes, is that benefits are probably not worth a limited cache.
I'm not sure that example would work on rustbyexample though. It'd be too much machinery to setup and require lots of extra concepts. I modified the original example to be more indirect. I think it should be clearer. It could easily be changed to something like this: // A function `age` which returns an `u32`. fn age() -&gt; u32 { 15 } fn main() { println!("Tell me type of person you are"); // Sometimes the variable isn't directly accessible like here // where a function is required to get the age. By binding the // variable to certain ranges, it can be categorized in spite // of the indirection. match age() { 0 =&gt; println!("I'm not born yet I guess"), // Bind to `n` for the sequence of 1 through 12. n @ 1 ... 12 =&gt; println!("I'm a child of age {:?}", n), n @ 13 ... 19 =&gt; println!("I'm a teen of age {:?}", n), // Nothing bound. Return the result. n =&gt; println!("I'm an old person of age {:?}", n), } } [playpen](https://play.rust-lang.org/?code=%2F%2F%20A%20function%20%60age%60%20which%20returns%20an%20%60u32%60.%0Afn%20age%28%29%20-%3E%20u32%20{%0A%20%20%20%2015%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20println!%28%22Tell%20me%20type%20of%20person%20you%20are%22%29%3B%0A%0A%20%20%20%20%2F%2F%20Sometimes%20the%20variable%20isn%27t%20directly%20accessible%20like%20here%0A%20%20%20%20%2F%2F%20where%20a%20function%20is%20required%20to%20get%20the%20age.%20By%20binding%20the%0A%20%20%20%20%2F%2F%20variable%20to%20certain%20ranges%2C%20it%20can%20be%20categorized%20in%20spite%0A%20%20%20%20%2F%2F%20of%20the%20indirection.%0A%20%20%20%20match%20age%28%29%20{%0A%20%20%20%20%20%20%20%200%20%20%20%20%20%20%20%20%20%20%20%20%20%3D%3E%20println!%28%22I%27m%20not%20born%20yet%20I%20guess%22%29%2C%0A%20%20%20%20%20%20%20%20%2F%2F%20Bind%20to%20%60n%60%20for%20the%20sequence%20of%201%20through%2012.%0A%20%20%20%20%20%20%20%20n%20%40%201%20%20...%2012%20%3D%3E%20println!%28%22I%27m%20a%20child%20of%20age%20{%3A%3F}%22%2C%20n%29%2C%0A%20%20%20%20%20%20%20%20n%20%40%2013%20...%2019%20%3D%3E%20println!%28%22I%27m%20a%20teen%20of%20age%20{%3A%3F}%22%2C%20n%29%2C%0A%20%20%20%20%20%20%20%20%2F%2F%20Nothing%20bound.%20Return%20the%20result.%0A%20%20%20%20%20%20%20%20n%20%20%20%20%20%20%20%20%20%20%20%20%20%3D%3E%20println!%28%22I%27m%20an%20old%20person%20of%20age%20{%3A%3F}%22%2C%20n%29%2C%0A%20%20%20%20}%0A})
The examples I've seen look nice. Please post a link for more details. Searching for '?' and try! doesn't work that well :-)
It's pretty good at giving people a bad first impression. The borrow checker is a real tsundere. She's cold and harsh about the mistakes you initially commit in your code, but she's a real sweetheart for pointing at all the wrong things in your code. It's just too bad that some people don't have the patience to warm up to her for a bit. There's quite a bit of dere in the borrow checker. 
I see you're misspelling borrow checker multiple times as burrow checker. Overlords and Probes check for burrowed units. This checks for borrowing.
I don't understand why you can't borrow in the guard without wrapping it in a closure. However, why don't you just return the expression without using a guard? ie: fn main() { let v = vec!["1".to_string(), "2".to_string(), "3".to_string()]; let q = match Some(&amp;v) { Some(iv) =&gt; iv.iter().any(|x| &amp;x[..]=="2"), _ =&gt; false }; } 
I remember reading about that `?` sugar somewhere, but I'm unable to find a link at the moment. Part of the beauty of `try!()` is that it is not a special language construct -- it's just a macro that uses a trait and a return statement. My understanding is that `?` would not have that property. It would be a new language construct used just for this one special case. That seems ugly. Or maybe the proposal is more general? I think what we really want is syntax for a monadic bind operation. If that's what `?` is, then I could get behind it.
It is basically impossible to implement non-hierarchical data structures without dipping into unsafe code. Doubly-linked lists come to mind. I also think that the type system will make it basically impossible to write asynchronous code. 
Could you expand a bit more why you think asynchronous code will be impossible to write?
Fortunately, the collections crate takes care of most of that work for you by wrapping the unsafe code around a safe API, so this is really only a strike if you simultaneously need a very custom data structure and don't want to use unsafe code to build it.
It's not necessarily hard. You just have to have large `unsafe` blocks. Writing safe abstractions with minimal unsafe code is anyway a problem that has no parallel in other languages; at least not an "easy" one.
And I'll make sure anyone who tries to argue otherwise (or more specifically, tries to block generic integers on dependent types) has an... *accident*.
Wow, there is a lot of interesting discussion over on the ["trait-based exception handling" RFC](https://github.com/rust-lang/rfcs/pull/243), including refutations of my claim that we should "just have monadic bind". Highly recommended reading!
The usual definition is "thing that break memory safety" . Lots of things can cause logic bugs, preventing commonly useful idioms because sometimes they cause bugs (even when they're memory safe) is overly draconian IMO. Tell most people that in Rust you can't have a nested loop over an array and pass two mutable references to the array elements to a "swap" function and they will shake their head in disbelief. Yes, very rarely these things cause problems, and you should avoid them (just like any mutable state really), but outlawing it at the language level makes the language clunky and painful. Rust has plenty of things that are error prone in the language, so it seems odd that people insist this isn't an issue when things like early returns (which are far less fundamental) are ok. 
The difference is that compile-time constants could, without loss of generality, be represented as types: struct Z; template &lt;typename&gt; struct S; typedef Z _0; typedef S&lt;_0&gt; _1; typedef S&lt;_1&gt; _2; typedef S&lt;_2&gt; _3; typedef S&lt;_3&gt; _4; typedef S&lt;_4&gt; _5; template &lt;typename T, size_t N&gt; struct array; template &lt;typename T, typename N&gt; struct array_alt; array&lt;int, 5&gt; xs; array_alt&lt;int, _5&gt; ys; // implementing normal integer ops for Z/S left as an exercise for the reader Actual dependent types would have types depending on *runtime-varying* values. 
&gt; Overseers, Observers, Ravens, Spore Crawlers, Photon Cannons, Missile Turrents, and Scans. FTFY.
Thanks for clearing that up! Dependent types really do look much more complicated than I thought. Just being able to pass values as generic parameters would already help Rust a lot, though (for example the horrible trait implementations for fixed-size arrays could be made much simpler).
starcraft references. I'm in the right programming subreddit.
Actually both Peano- and binary numerals have been implemented within Rust's type system. The former is part of dimensioned, the latter of shoggoth_rs (if memory serves).
I think being conservative about what it guarantees is the main point of Rust and its type system. The fact that a Rust program compiles rules out a whole range of bugs, and that's what makes it attractive. The usability cost of this does make it less suited to many tasks than, well, more suited languages, but Rust's type safety should benefit to more sensitive systems applications. Some difficulties are inherent to this level of safety, many others are open problems waiting to be solved. But that allows the home page of the language to say this: &gt; Rust is a systems programming language that runs blazingly fast, prevents almost all crashes*, and eliminates data races. --- I strongly disagree with this: &gt; Plenty of languages manage to do this just fine without using unsafe code (they just use a GC). These language also don't advertise zero-cost abstractions and minimal runtime.
It's a small thing, which I suspect will be fixed eventually, but if you write code intended to be generic over multiple numeric types (be it all ints, all floats, or all numeric types in general), it handles constants very badly. You can't just write: fn add5&lt;T: Float&gt;(a: T) -&gt; T { a + 5.0 } And the error message you get back when you try doesn't do anything to help you find the right way to write this. It turns out what you need to do is: fn add5&lt;T: Float&gt;(a: T) -&gt; T { a + num::cast(5.0).unwrap() } Which is both obnoxiously verbose, and not very discoverable. It's one of those things you just have to know. And Rust has a number of weird little gotchas like this. That said, the community is awesome about helping with this sort, provided you go to the effort of reaching out. Aside from this subreddit, the #rust IRC channel is amazing. But you have to make that extra effort to reach out to solve a lot of these sorts of problems.
Rust should either, eventually and at some point: 1. Get a sufficiently more expressive language to express more tricky lifetimes and looks-unsafe-but-is-safe stuff, enough to implement the simple data structures, or 2. Ally itself with another language that actually *can* implement provably correct low-level abstractions. I'm guessing that will be some kind of full-on theorem proving with linear and dependent types, if the current "trends" are any indication (though I don't have experience with these things so don't know the limits). There could be an interface between these languages, or maybe the other language outputs C code when it has been proven correct, and Rust uses that C code through the FFI. And of course has some mechanism to ensure that it actually *is* the C code emitted from the compiler and not some code that has been tampered with after having been outputted by the compiler. Or just continue with `unsafe` and human/computer-assisted auditing.
I think it's like this: you want to listen to an event generated by some object that you own, and run a function when that event occurs. In C# you would just register a callback with `+=` (so easy!) or `.Observe()` (if you're using Rx). You just have to remember to unregister the callback later. In Rust... I'm not sure. I would love it if someone could write some example code for this kind of scenario.
[1] bindings to existing C++ libraries, due to the namespacing/overloading working differently. Of course its' subjective, wether you consider this a language weakness (lack of practicality, inability to leverage existing assets &amp; knowledge) or strength (escaping C++ misfeatures). [2] personally I like C++'s overloading and a few extra features in the template system for representing data structures &amp; maths operations on them, useful for graphics programming ... I wouldn't go as far to say its' a Rust weakness, just something C++ is (for me) better at. [3] C/C++ express unsafe code more elegantly* (*I haven't checked in a while, its' things like pointer arithmetic operators and how casting works last time I looked) .. where you do need unsafe , these languages are more designed for it. Its' like rust goes out of its way to discourage you from writing it, beyond wrapping it in `unsafe{}`.
I've been becoming fond of Clojure lately and it's philosophy with regards to core.async, which seems to be what Rust does by default using the channels. Note, I'm not experienced with Rust, but the channels look great for asynchronous programming.
It can be made to work, but it does require an extra effort. And IMO you need to start with the goal of making your UI fully native from the start -- as wxWidgets does -- instead of trying to bolt on nativeness later.
We have to decide whether we're referencing Brood War or SC2.
What's unsafe about having a mutable int on the stack and two references to it? As long as you ensure the references don't outlive the owner nothing is unsafe. So clearly at the very least it's not "necessary" to disallow that in order to have memory safety without a GC. There's tons of obviously safe things that the borrow checker disallows even though it wouldn't do anything to protect memory safety. If what you're trying to say is "we haven't figured out a way for the borrow checker to solve what it needs to do without a ton of innocent casualties" then fine, but I don't think couching it in "it's necessary for safety" is very honest. I've demonstrated two easy scenarios that you could obviously allow and still be safe, so there's an existence proof that it's not a safety issue. So either it's because you can't figure out how to let the borrow checker allow safe uses of mutable references (even easy ones), or it's because you think that mutable references *should* be disallowed because they can cause subtle bugs (which IMO is overly draconian and makes your language less likely to get adopted - see e.g. Jon Blow making *his own language* largely due to that kind of thing). 
If it's a single thread and you have two references to a variable they will access them one at a time in a deterministic way. That's not a data race. D allows this without causing memory corruption, why can't Rust?
If this is single threaded? Yes. But Rust, at the type system level, doesn't know anything about threads.
* Probably not great to prototype in, at least compared to high-level managed languages like C#, F#... * Requires a lot of CS chops, so hiring could prove difficult. You'll want the functional programming crowd, they have experience with avoiding circular data structures and such. * Immature, so full of little gotchas. * Not enough tutorials, documentation, tooling and libraries, as you would expect from a young language.
That doesn't mean it couldn't. All I'm saying is that you shouldn't pretend that this kind of clunky and painful stuff in Rust is inevitable or else you get memory corruptions. It's not. You chose to do it in a way which causes this kind of pain, but you could've done it some other way instead (as other languages have done). I would recommend taking the ergonomics issue of the borrow checker more seriously than saying "eh, it's the price you pay for safety". It's the price you pay for choosing a blunt and draconian tool to ensure safety. Maybe you think it's still worth it, fine, but at least acknowledge the price you pay. 
No higher kinded types. I was very pleased with Rust but quickly hit the limits of what that particular type system is capable of.
&gt; You chose to do it in a way which causes this kind of pain, but you could've done it some other way instead (as other languages have done). I don't think this is true, but I also don't feel like arguing about it, to be honest.
Um, yes it does. `&amp;mut T` isn't `Send`, for example. Edit: Nvm, completely wrong and addressing the wrong point anyway.
Writing code quickly. It's harder to get things to compile in Rust due to lifetimes. This is partially my lack of experience but it's also I think the nature of the language. Once the code does compile it is usually in a more reliable place than it would be in a less strict language. This is a property of static versus dynamic languages in general, just Rust is stricter than most. It's faster to bang out some code in a language like Python or Ruby, but you often pay a cost later in performance and errors which are only caught at runtime. 
Have you ever heard of the saying of "share data by communicating, don't communicate by sharing data"? It's much easier to construct scalable code when the primitives you use correspond more closely to a precise causal dependency.
`Send` is one of those borderline cases of something that's sort of part of Rust, and sort of not. It's a library type, it's not an inherent part of the language. And even if you have a value that is `Send`, you don't know if the current bit of code is running in a single or multi-threaded context.
|✓| privilege.
The big difference is that you can't write functions like these in C++: template &lt;typename T, int N&gt; class Array { ... }; // no problem here template &lt;int N&gt; int sum(Array&lt;int, N&gt; arr) { int total = 0; for(int i = 0; i &lt; N; i++) total += arr[i]; return total; } // runtime parameter, this doesn't work void triangular(int n) { Array&lt;int, n&gt; seq; // error: n not determined at compile-time. for(int i = 0; i &lt; n; ++i) seq[i] = i+1; return sum(seq); } Whereas in a dependently typed functional language that works just fine: data Vec (n : Nat) (a : Type) where nil : -- nil takes an implicit argument for the type {a : Type} -&gt; -- and returns an empty vector of that type Vec 0 a _::_ : -- cons takes an implicit argument for the type {a : Type} -&gt; -- an implicit argument for the length of the previous vector {n : Nat} -&gt; -- an element of the type (x : a) -&gt; -- the rest of the list (xs : Vec n a) -&gt; -- and returns a list that's one bigger Vec (succ n) a -- [n, n-1, ..., 1] triangular_seq : (n : Nat) -&gt; Vec n Nat triangular_seq 0 = nil triangular_seq (succ n) = succ n :: triangular_seq n -- implicit n here since it's determined by -- the 2nd argument's type sum : {n : Nat} -&gt; Vec n Nat -&gt; Nat sum nil = 0 sum (x :: xs) = x + sum xs triangular : Nat -&gt; Nat triangular n = sum (triangular_seq n) 
&gt; Plenty of languages manage to do this just fine without using unsafe code (they just use a GC). The parenthetical is the key point: doing it easily without a GC is *hard*. It's worth noting you can get GC-like "ease" (where 'ease' == 'no unsafe') in Rust using a shared pointer type like `Rc` (unsurprising, as its a form of GC). In any case, on the GC point: a doubly linked list has a non-trivial semantic invariant between the pointers in the two directions. GC only solves one part of the invariant: ensuring that you'll never get a crash if the invariant is broken. Garbage collection doesn't fundamentally solve the "hard" part, of making sure the pointers have the right forward/backward relationship, e.g. there's nothing stopping you from forgetting to update a backwards pointer. Rust "recognises" that breaking this invariant without some form of GC (including reference counting) will lead to memory unsafety, and, isn't (currently?) powerful enough to prove the invariant automatically, i.e. it is up to the programmer to do it with `unsafe`. The same concerns apply in other languages without GCs, like C and C++, but they don't make the danger in the invariant so obvious. Those languages are really the ones of 'interest' for this sort of comparison, as Rust's major drawcard is the lack of garbage collection. Of course, all this doesn't mean that Rust *isn't* bad at these in an absolute sense, but conversely, being bad in the space of all languages also doesn't mean that Rust is comparatively bad in its niche of not needing a GC. In some sense implementing these data structures is easier in Rust, because the compiler is telling you where you need to be careful about invariants. Unfortunately, at the moment, there are some failings of the borrow checker that mean there are 'spurious' errors, particularly [the non-lexical borrows](https://github.com/rust-lang/rust/issues/6393), which can be rather annoying when you hit them (and writing data-structure code seems to do so proportionally more than other code, IME).
You claimed that aliased mutable references were not memory-unsafe; /u/Manishearth said that that "It depends on your definition of unsafe" and pointed out iterator invalidation; you replied with "The usual definition is 'things that break memory safety'". I took that to be you claiming that iterator invalidation is still memory-safe. If you don't actually believe that, then you need to explain how iterator invalidation could be avoided statically, even in the case of aliasing mutable references.
For a memory safe language, you need either a GC or something like the borrow checker. It's true that some things like non-lexical borrows would be nice to have, but I still find Rust pleasant to use without them. YMMV.
Edit: The stuff described here isn't _memory unsafety_, per se, it's more of a common footgun that happens because we can't reason about pointers that well. [huon's comment](http://www.reddit.com/r/rust/comments/2zu3eo/what_is_rust_bad_at/cpmkdi7) shows how Rust enums can break actual memory safety when nonuniquely aliased mutably. &gt; What's unsafe about having a mutable int on the stack and two references to it? x = get_an_int(); // Lots of lines of code later y = mut_alias(x); // more code if (some_condition(x)) { do_some_complicated_things(y); // May modify y/x, hard to tell, might involve multiple nested calls to large functions // some_condition may no longer be valid here do_other_things(x); // Might depend on some_condition } One could argue that this isn't memory safety, but it is a common footgun in code, and it's part of the definition of safety Rust assumes. Actually, a better example would be when you add a function to this: fn foo(&amp;mut x, &amp;mut y) { if (some_condition(x)) { do_some_complicated_things(y); // May modify y/x, hard to tell, might involve multiple nested calls to large functions // some_condition may no longer be valid here do_other_things(x); // Might depend on some_condition } } and somewhere else `foo` is called with a pair of aliases that _might_ point to the same memory. Here's a more concrete (and simple) example: fn rockstar_interview_swap(x: &amp;mut u8, y: &amp;mut u8) { // Look, ma, no temporary variables! *x = *x + *y; *y = *x - *y; *x = *x - *y; } This works great when x and y point to different memory locations. `(x, y) =&gt; (x + y, y) =&gt; (x + y, x) =&gt; (y, x)`. Perfect. What happens when the two alias the same memory location? `(x, x) =&gt; (2x, 2x) =&gt; (0, 0) =&gt; (0, 0)`. That certainly wasn't a swap. But this is a corner case that's easy to forget. When reasoning about the swap function we look at `x` and `y` as separate little algebra-ish things and bop them around, but we usually make the assumption that they _are_ separate and never the same. This is an exceedingly simple bit of code, yet easy to get wrong -- and the assumption or the condition (`some_condition` in the previous pseudocode) isn't even obvious! Someone calling swap won't know the internals. Temporary variable swap works fine (though XOR swap is similarly broken). When I, the developer, see `swap()`, I assume that it can be used to swap things. Logically, swapping something with itself should be a no op, perf aside. Of course, it would be pretty silly to outright ask it to swap two things which I know are the same, but I could place it in a situation where the arguments _might_ point to the same memory location depending on the conditions. For example, a slightly extravagant implementation of selection sort with the indices bounded (say I used `&lt;=` instead of `&lt;`) so that I am comparing elements with themselves as well. Suddenly, sorting an array would zerofill it. There you have it. We have a mutable int on the stack and two mutable references to it, causing footgunny behavior. Now remember that the example above was swap+selection sort, which are both taught in introductory CS courses and are tiny/simple, and using `&lt;=` instead of `&lt;` -- a mistake made often by all kinds of programmers. Imagine what can happen when the functions are complicated and intertwined in a huge codebase.
&gt; when the primitives you use correspond more closely to a precise causal dependency. Maybe I'm just dense, but I can't make heads nor tails of what you mean by that. A more concrete explanation would work wonders here. It's easy to repeat design advice like "share data by communicating, don't communicate by sharing data", but with facilities like Rx in C#, you are explicitly subscribing to a stream of data (instead of notifications to changes in shared data). But that leaves me back where we started. In C#, I can use a callback for when an event occurs. But in Rust, that's clumsy. For a more concrete example, let's say I need to load an image, and I want to keep it up to date. Maybe it's on the disk, maybe it's on the network, who knows. When the data is read, I can pass it to the image decoder, and when that is finished, I can notify my owner that the task is done (or that it failed). This is not too hard in C#, even if you want to handle async callbacks manually instead of relying on sugar. How would you do something like that in Rust?
That's correct.
It's a lang-item, but yes, it could be a pure library trait (just without the autoimpl magic). Yeah, I see what you were trying to say. Agreed, we can't be sure if an alias is being passed to just a new stack frame or also to another thread when there's a function call.
I'm not a real big C# user, and I'm not familiar with Rust itself. I'm in the Rust channel because I'm always trying to find new ways to author concise and correct code. I'm not going to try and sell you on any one particular language, but from what I understand Rust has communication as a primitive, and that's nice. Let me explain two things. First the answer to your question as I understand how to write good concurrent code, and then why communication primitives are a good approach to concurrency. Generally, with communication primitives, you would can off the operation in some concurrent actor primtive (goroutine, thread, greenlet, whatever), and then it would send some signal to some mechanism by which it will be buffered (not lost!) when it reaches the endpoint, and read by the recipient when the recipient calls a blocking receive operation, get. If the actor hasn't sent the signal yet, get blocks which means that the recipient is correct in either case. This is a textbook causal relationship. A -&gt; B. Channels are a compiler level language facility that allows you to manage the typing facility of data exchange between concurrent actors (as I understand it anyway). Think of communication primitives as being akin to a decoupling of some of the most basic of facilities that you know: function calling and returning. F(arguments...); in imperative languages means execute function F passing it "arguments", and when it's finished return some result, whatever that is. With channels, you get the same facilities, but the timeline of decoupling of the awaiting (causally) the result of F with the procession of the current sequence of operations is really the only difference. This is why go routines are so simple-they facilitate exactly this. As a result, its far more clear to author scalable, correct, concurrent code. Whether or not whoever actually executes F itself is on the same machine can also be abstracted too, since now results can just be sent over the network. Consider alternatively, using the classic difficult locking primitives. What locking primitives connote isn't exactly a very precise causal relationship; it's something else entirely. And scaling (in many different senses of the word) is hard for a number of reasons. Here's two good examples of scalability in one: You have a linked list, and you want it to operate correctly in a concurrent context, but hide the implementation details from actors. Obviously, when you want to remove or add an item, you have your list internals hidden by some object system, and you hold a lock while you edit the linked list. But this nieve solution fails for several reasons: First, consider API design to be the ultimate of worst case scenario consideration. So you want the linked list to work correctly even if there are billions of threads using it. The semantics of using a lock primitive is that *each and every thread* that was waiting, wakes up, competes for the resource and then must go back to sleep as whoever acquires the lock does whatever until the lock is released. That's a lot of trap servicing that the OS will be doing, all of which is unnecessary. Second, it fails because as an API, if you want to have one thread *replace* just a single element, then it must call remove and then add on the list. In the worst case, if another thread is competing, there is no guarantee that the other actor may acquire the lock between when it was release by the first's remove and subsequent add. So then how do you *compose* software in an asynchronously scalable fashion? In an efficiency scalable fashion? In a machine scalable fashion? Communication.
Rust doesn't have function overloading, in the sense that you can't create two different functions with the same name but differing actions. This is considered to be a major misfeature. Thus, you need to know the exact type of every object in order to know roughly what a piece of code does. Eg: auto x = some_func(); auto y = some_other_func(); std::vector v(x, y); Tell me what this code does? If x can be coerced to a size, you'll get a vector with x copies of y. But if x and y can be coerced to an iterator, your code will collect the iterator into the vector. You would have to do a search of your entire codebase for the objects x and y to ensure you know every possible coercion (as they can be implicit). Rust doesn't let you do this kind of hodgepodge mess that C++ programmers consider normal. Good riddance. Rust, does provide a way to relegate tasks to a specific implementation, through traits. For example, the collections all implement a trait, called `FromIterator`, which implements a method, `collect()`, which converts the relevant iterator into the relevant object. It works very well in practice. Even though the types change, the behaviour is well defined and predictable.
Plugins might be the easier option here in many cases. There's a research team working on adding extra safety guarantees to the usage of channels in Servo which only needs the addition of sort-of-linear types to Rust (we're doing that via a plugin), and the rest is all via the type system. Servo also already uses plugins to provide some level of extra safety for our Spidermonkey-GC-managed DOM pointers, though most of it [is done by the type system](https://blog.mozilla.org/research/2014/08/26/javascript-servos-only-garbage-collector/)
....which you'd need anyway in a cycle collecting GCd language. Well, the checks introduced by the `RefCell` wouldn't be there but a cycle collecting GC is anyway heavier than an `Rc` And if you don't want those runtime checks, you can just use raw pointers -- it's just like C with a sprinkle of `unsafe {}`. Rust isn't worse than other languages at writing a DList. It's just that as Rustaceans we have different expectations of the code, expectations that sometimes can't be expressed in the context of other languages. 
| By comparison, creating a lightweight thread for every single... Ergh, no, generally, if you implement the algorithm correctly, then the number of threads created throughout the system is constant and dependent on the hardware of the machine. And it will work correctly whether you use 1 or N number of threads. Languages like Go (a competitor to Rust, I've not actually learned Rust but plan to learn it) introduce a runtime that manages the number of threads, and the programmer doesn't ever actually pay attention to that. Programmers just write the algorithms to consume as much of the hardware support as it can, and move on. Although, with any primitive you know it's limitations and overhead and use it judiciously-I don't know what scenario you would never a communication primitive in "every object"... I guess what I think about what you're saying is that the language facilities should make receiving and sending over channels concise and straightforward. ^ for threads read concurrent actor. Just anything that operates concurrently. As to your point about increasing complexity meaninglessly, whether or not there is actual asynchrony on a system or target really boils down to your infrastructure and target. Maybe there's threads in the program, but the OS just does switching, and so it just appears parallel. The point in doing the peppering is so you get the performance if it's supported. But if you don't plan on having a platform that actually has both hardware and software support for executing in parallel, then why program in parallel at all? I don't do much GUI programming, sorry. So I guess I'm lost as to why you would introduce that into the point. Lastly, do you ever really have asynchrony without concurrency?
[`&amp;mut T` is `Send` if `T` is `Send`](https://github.com/rust-lang/rust/blob/ecf8c64e1b1b60f228f0c472c0b0dab4a5b5aa61/src/libcore/marker.rs#L409).
[Opt-in built-in traits](https://github.com/rust-lang/rfcs/blob/master/text/0019-opt-in-builtin-traits.md) aims to make it a completely pure library trait, including the auto-impl magic. I believe we may even be at that point now.
Java's standard library collections do not perfectly detect iterator invalidation in a multithreaded JVM (they sorta do if you explicitly use a threadsafe one everywhere, but at that point you are paying substantially more overhead over Rust than just the GC). I suspect the same is true of C#. In a single thread, you can get fairly similar semantics to what you expect from those languages with `RefCell&lt;Container&lt;Cell&lt;T&gt;&gt;&gt;` (or you can use `RefCell` for the `Cell`s as well, if `T` isn't `Copy`).
&gt; tech culture and *society at large* Your ethnocentrism is cute.
You could make it look better with a macro. E.g. `gen_float!(5.0)` -&gt; `num::cast(5.0).unwrap()`.
 &gt; but with facilities like Rx in C#, you are explicitly subscribing to a stream of data (instead of notifications to changes in shared data). Hmm interesting. What do you mean by that? Subscribing to a stream of data. So you listen on a queue or channel and when it gets an item you as a consumer get the item and continue executing? Sorry, I guess I must be the only one who doesn't know what Rx in C# is. If you just have a callback, in what thread is that callback executing. Does the other thread (your image loader from your example) call a function? But isn't that function now running in a image loader thread? That seems like a recipe for disaster. You'd need a locks and mutexes everywhere. Here it seems it is clumsier because it is dangerous. Sure, saying "just call this callback" is very easy and seems simple but you have to ask what context of execution (like "what thread") is that callback running in. Spawn a thread, let it decode the image and then you wait for you to send the result back to you. Isn't that more reasonable or how RealWorld(tm) concurrency and parallelism work. You asign a task to a helper/worker. You continue working, and go off on their own, do the work in parallel with you and at some point later you "synchronize" with them by wanting to use their result. 
I think a lot of the answers you're getting are "duh, the borrow checker" or "it's missing {my pet feature}." I'll try to avoid those, but I make no promises. Also, I'm not going to limit myself strictly to the language because I care very much about the quality of tools that I use. * Our current API documentation is wonderful for browsing *known unknowns*, but I've observed that newcomers have a lot of difficulty finding things. My hypothesis is that it is bad at finding *unknown unknowns*. That is, you need to gain a certain amount of experience before the API docs start holding their weight. For example, if you wanted to find a method on `String` that replaces one substring with another, you essentially have to know about deref coercions, that `String` derefs to `&amp;str` automatically and that `replace` is defined on `str` (was `StrExt`). It's tricky navigate without a lot of context. (Alternatively, one could guess and just search `replace`, but you still have to know that methods on `str` are applicable to `String`. And searching isn't always going to lead you to promised land if you don't know what to search for. Sometimes browsing is the best way to get a high level overview of the landscape.) * The compiler is slow. This is a minor annoyance and the future looks promising. * Automatic type based serialization needs a lot of work both in terms of functionality and performance. [It's being worked on, but it looks like a hard problem to solve.](https://github.com/erickt/rust-serde) I say this as someone who heavily uses this functionality with success. * I don't think there's a good story for distributing *applications* written in Rust yet to Linux/Mac/Windows. It looks like Cargo will grow an `install` command soon, but a lot of people think it's bad juju to require a language specific package manager to download and compile an application. (I personally don't have a strong opinion.) * My personal pet feature is abstract return types. This would enable returning iterators and unboxed closures without paying the cost of a heap allocation. QuickCheck could certainly benefit from this (currently uses `Box&lt;Iterator&gt;`). * The `Iterator` trait appears to be fundamentally incompatible with certain types of streaming abstractions. See: https://github.com/emk/rust-streaming --- You can of course work around this to get the performance of a streaming iterator, but you lose the conveniences afforded by `Iterator`. * The `num::cast` issue pointed out by /u/Cifram is another one, but I've only very rarely written numeric code that required generic constants, so it hasn't been a major pain point of mine personally. I normally *hate* complaining about stuff, but I don't like to think of these as complaints per se. They are pain points I've experienced in the trenches, but I have a lot of confidence that all (most?) will be improved upon in time. :-) (The list of things I *like* about Rust is a lot longer, but also less interesting. I like the same things that everyone else does.)
If you don't have to worry about deletion or updates, it's not particularly hard to create a doubly linked list in current Rust, since you're free to alias `&amp;T` as many times as you want :) In practice, as in Haskell, the easiest way to create a semantic doubly-linked-list in safe code is to use a zipper, though in Rust you usually perform destructive updates during the walk since you have the additional uniqueness guarantee.
&gt; Often, we are solving problems that are asynchronous but not concurrent, so peppering our asynchronous problem with concurrent primitives seems like a good way to increase our application's complexity without any benefit. What do you mean by "asynchronous" but not "concurrent"? You either have concurrent, or sequential programming logic -- things that have to execute in a sequence, vs things that don't. &gt; If we are using asynchronous callbacks, then a "model changed" event fires, causing all windows pointing at that model to redraw, updating the model inspector, triggering an autosave timer, or whatever. But what fires the event? Is it fired from a different thread. And in general how does "firing" work. Is it putting a message in a queue? If you are thinking of a GUI application there is usually the main execution thread and it runs outside your control and you only get callbacks from it. Like say "Window was resized", "User clicked button 'X'". They will often take custom events as well such as "Model A was updated". But often they work that way because there is a some kind of a message queue mechanism underneath in the GUI framework, which is exactly how languages with channels/threads/processes work. &gt; By comparison, creating a lightweight thread for every single object that needs to listen to an asynchronous event... well, let's say I'm not sold, but I'd love to see a demo. (My problem is not with performance... I just feel like this is introducing concurrency into places where we only wanted asynchronous events.) The main question is, after these objects receive the event can they update or do anything concurrently (are they independent objects) or do they depend on each other? If they are truly concurrent and don't share data with others, a green thread/process per object might not be bad. As it models exactly how thing would work in the real world. Then each one is a class instance running in a separate lightweight threads and a threadsafe mailbox/event queue on which it receives external events and acts on them. 
I feel like these types get less attention than they should, and that's partially my fault...
I was actually thinking about this the other day... I think a `'static` version of try should basically do the trick (or at least, it is no more exception unsafe than `thread::spawn`, since it allows the exact same types through*) except that `RefCell` doesn't currently have a poison bit. We could probably add one without changing its size... or, now that we have the amazingness of OIBIT, we could create a `Send`-like trait designed purely to track exception safety, and bound try by that instead :) (Personally, I am not a huge fan of `Cell` / atomics not being subject to exception safety restrictions though... but if we went the OIBIT route we could disallow `Cell` as well by default, if we wanted). \* except for the types that aren't `Send`, aka `Rc`, which is not inherently more problematic than `Arc`.
Mutations aren't always what's needed though, and any thread-safe mutations are going to have to have some sort of cell. Just having a `Vec&lt;Arc&lt;Fn(..)-&gt;..&gt;&gt;` works, then any listeners which need inner mutability can use RefCells when they need them.
My point is that you could solve iterator invalidation without outlawing *all* mutable references. The fact that something is dangerous in *some* circumstances doesn't mean it should be outlawed in *all* circumstances. There are a number of ways of fixing that on the philosophical level (without going into too much detail). For example rather than always outlaw mutable references, you would outlaw destroying data while there are mutable references - so any destructive operation would "claim unique ownership" first and you'd track that with linear types like you do ownership now. v2.clear would fail to claim ownership because the other reference exists. The other approach is to go the other way and do more analysis to prove safety as the exception. So in this case you can't prove that clear() won't mess with the internals of the iterator and disallow it (but perhaps the user could add unsafe annotations to assert safety to help the compiler along), but if I'm doing a double-nested loop over an array you can see that nothing is going to cause memory problems.
From a quick look: https://github.com/maurizi/retag.rs/blob/master/src/main.rs#L47 This is Option.expect. https://github.com/maurizi/retag.rs/blob/master/src/main.rs#L77 You keep doing this "! condition" thing, style is "!condition" https://github.com/maurizi/retag.rs/blob/master/src/main.rs#L177 It seems that you want HashSet.contains (which is also substantially faster.) https://github.com/maurizi/retag.rs/blob/master/src/main.rs#L178 The write! macro should work here. https://github.com/maurizi/retag.rs/blob/master/src/main.rs#L114 You could use .any() here. https://github.com/maurizi/retag.rs/blob/master/src/main.rs#L109 The vec![] is going to allocate here, on every call! Use a normal array instead. https://github.com/maurizi/retag.rs/blob/master/src/main.rs#L70 This is .ok().expect("Could not start file watcher") All in all, you seem to be doing fine with the language itself, just getting comfortable with the libraries. 
There are exactly two cases that I know of where the single threaded semantics of pure Rust were changed to make it better for multithreading: one is modifying a variable directly through its original `let` binding when it was `&amp;mut` aliased (something that is not all that useful anyway, to be honest), and the other is requiring statics to be `Sync`. That's it. Everything else already had to be there for single-threaded memory safety, or was able to be modeled using special traits (now no longer so special), plus Rust's existing lifetime support. If Rust did have the hypothetical reference type I mentioned earlier, that could perform safe mutation only in a single-threaded environment, it would be trivial to make it non-`Send` in a library--it would literally be a one liner. So it's not clear to me that Rust knowing about threads at the type system level (as opposed to Rust providing facilities for writing types with the same semantics in a library) would be a win at all.
The skydiving company makes no *guarantees* about my survival when I choose to use my own parachute (which might be molting). It would be perfectly reasonable for them to say I'm being *unsafe*. I wouldn't make a fuss if they reminded me every time. ;)
Of the top of my head I don't know any non-trivial open source examples, but we've written async Go code at work that's pretty clean. Imagine something like the second of these trivial examples: http://www.golangpatterns.info/concurrency/futures and http://matt.aimonetti.net/posts/2012/11/27/real-life-concurrency-in-go/, but with more cases in the select statement.
&gt; If you just have a callback, in what thread is that callback executing. You're thinking about callbacks, when it's really about observable values. The observable sequence sends values to its observers. Notice that I said *nothing* about threads. Unless you specifically ask for more threads, everything happens on one thread, without any need for locking. Rx is an open-source library for reactive programming from Microsoft. It basically gives you the ability to work with observable (time-varying) sequences the same way you work with iterable sequences. For example, think about how you work with iterable sequences: let min_max = array.iter() .filter_map(something) .min_max(); Now imagine that instead of `array` being iterable, it is instead an *observable,* which varies over time. In other words, it is a "push" instead of a "pull". Yet the syntax is mostly the same. Everything is working fine and we haven't yet spawned a second thread. I remain unconvinced that spawning a bunch of green threads makes this better.
&gt; What do you mean by "asynchronous" but not "concurrent"? Asynchronous: events occur independently of program flow. For example, you can write an asynchronous web server with `select()`. Concurrent: multiple operations occur without waiting for each operation to complete before the next one starts. &gt; But what fires the event? Is it fired from a different thread. Yes, if you drag the OS into things, everything is multi-threaded, because the OS will always be executing other threads on your behalf or on the behalf of other programs. But that doesn't make your program itself multi-threaded. Here's a more explicit sequence of events: * Main loop registers a mouse click event. * Event gets routed to a specific view in a window, which changes the model. * The model sends out a "model changed" event, which triggers callbacks in other windows, which respond by requesting to be redrawn. What I like about this is that nothing is happening concurrently, so it is much easier to reason about the behavior of the program than if it were concurrent. No need for locks. You just have to be careful that e.g. you don't send a message to a dead object, which is the kind of problem that Rust is designed to tackle.
Oops. :D
A few things that I don't think have been mentioned yet (not an exhaustive list): * Rust really, really needs `nothrow` or an equivalent effect. It's going to be a serious hole in a lot of unsafe code until it happens (IMO it should default to on in unsafe blocks, so you at least have to deliberately engage the footgun). Rust also needs the option to disable panics (it has been talked about and will happen eventually, but I don't think it's been implemented yet). * Rust would benefit heavily from extension methods. Currently, there are quite a lot of traits floating around that exist solely to provide method syntax for what would be just fine as free functions, rather than being a carefully thought-through interface that should have multiple implementors. This is a product of Rust not making it super ergonomic to use free functions. Besides it being fairly verbose to create a new trait just to add some methods, this introduces significant backwards compatibility hazards (because traits can have many implementors). Extension methods would solve these problems neatly. * The `Deref` family of traits, while useful, have a lot of gotchas compared to most other features in the language. I would have to search for the relevant threads, but it is responsible for some of the more surprising behavior in a language that expressly set out to avoid surprises. * The standard library does not provide any way of dealing with allocation failure, meaning that for robust systems you will have to rely on an external library. I do not think this in itself is the worst thing in the world, but currently it's *possible* but unpleasant to use Rust without the stdlib (for example, many common macros have hardcoded stdlib locations for things they expect to find--though maybe this is really a macro hygiene issue). Better support for alternate preludes and standard libraries, both in the language itself and in the crates ecosystem, would make this a much less scary proposition. * Lingering undefined behavior. In particular, unless this has been fixed very recently, too-long bitshifts are currently UB. * Dynamic bounds checking for arrays. While performance is definitely one aspect of this that is problematic, bounds checking doesn't solve the fundamental problem, which is the inability to determine safety at compile time (to the point where I suspect the majority of unwind calls in Rust are related to array indexing, so this ties in with nothrow too). My hope is that one of the areas Rust tackles going forward is static determination of safety for many bounds checks; it is not possible to verify them all, but verifying a large, "easy" subset statically and providing explicit dynamic checks only for the "hard" cases would be much more consistent with how the rest of the language works than the current behavior, not to mention removing a pernicious source of bugs :)
Constructive comments only, please.
I should have said that I did not think GCd languages were relevant in this discussion. But now I see your point. I apologize for answering without paying more attention to the topic.
Rapid development for game prototyping is easy if you use https://github.com/pistondevelopers/current to code up something quickly and refactor it to safe code afterwards.
I've sometimes done something like: fn cast&lt;A: Float, B: Float&gt;(val: A) -&gt; B { num::cast(val).unwrap(); } So calling `cast(5.0)` is much better than `num::cast(5.0).unwrap()`. I've also done: let five: T = num::cast(5.0).unwrap(); Which lets me write things like `a + five`. Though this is only really valuable if the same literal is used more than once in the function. And when dealing with constants in this kind of code, I've taken to writing them as: fn my_constant&lt;T: Float&gt;() -&gt; T { num::cast(5.0).unwrap() } All of these shortcuts make the verbosity problem slightly less bad, though they don't really solve it. However, they do nothing for the discoverability problem.
If you want an example of what dependent types can add I wrote an [introduction for the ATS language](http://bluishcoder.co.nz/2010/09/01/dependent-types-in-ats.html) a while back. Although it's ATS the subset of the language it uses is similar to any functional programming language.
No burrowed unit can resist a lone probe(s) ;) You're right of course.
&gt;you might want to avoid personifying ... Or general manga/anime related language. It's really alienating to anyone who isn't 14.
The problem is that without a runtime it's very hard to batch (and thus elide) operations. These kinds of optimizations are discussed [here](http://users.cecs.anu.edu.au/~steveb/downloads/pdf/rc-ismm-2012.pdf) (which is frequently linked in /r/rust), and it's pretty clear that Rust's reference counter is pretty naïve by these standards. That said, if C++ programmers use it (theirs is atomic!) it can't be that bad!
&gt; I don't think there's a good story for distributing applications written in Rust yet to Linux/Mac/Windows. It looks like Cargo will grow an install command soon, but a lot of people think it's bad juju to require a language specific package manager to download and compile an application. (I personally don't have a strong opinion.) Reasonable distribution package managers (for Debian, Arch, etc) will just call Cargo during the package build, and use Cargo's metadata to create the package (collect dependencies, etc). That way end users won't need to have Cargo: a package and all its dependencies can be installed by the distro. That's how it's done with Haskell's Cabal, at least (see [this](https://wiki.debian.org/Haskell), [this](https://wiki.archlinux.org/index.php/haskell)).
The too-long bitshift issue is fixed along with arithmetic overflows. 
Unfortunately, the C++ collections expose an unsafe interface. The typical example: for (auto it = vec.begin(), end = vec.end(); it != end; ++it) { if (some_condition(*it)) { vec.erase(it); } } which some many beginners trip over, and some not-so-beginners still hit in more convoluted contexts...
There are a couple of different variants, but essentially the feature I have in mind would allow the programmer to guarantee that a particular function (or all functions in a particular block) could not panic--as in, cannot possibly panic because they never call any functions that unwind. Under the proposed abort on panic option, all functions satisfy this constraint (because they never unwind).* Without this feature, the only way to know for sure that a segment of code can't panic is to manually audit it. While this can cause problems even in safe code, the biggest issue is with unsafe code, since panics during unsafe code can cause memory unsafety. This means that: 1. Generic unsafe code frequently must do suboptimal things in order to avoid the potential UB that would come with an unexpected unwind. For example, you can't use `std::mem::uninitialized()` on something with drop glue if there's a possibility that the drop glue is called before you finish initializing it, so if you are calling any functions you haven't personally audited you have to be careful to make sure this doesn't happen. 2. Because exception safety is very difficult to reason about, unsafe code often has such issues without the author realizing it. A nothrow effect (ideally required by default in an unsafe block, and usable elsewhere if desired) would make it much harder to screw this up. So long as the author was using the unsafe block "correctly" (that is, using it to cover the entire unsafe period where Rust's invariants are not being upheld, rather than just for the one function call that is marked unsafe), any decision to call a function that might panic in unsafe code would have to be deliberate. It would also allow for, e.g., a faster version of a function if you can satisfy the nothrow requirement, and a slower one for potentially panicking functions. \* Note that the effect would not cover aborts, because: (1) there are ways to abort (e.g. getting SIGKILLed, power loss [depending how you look at it]) that you can't control, (2) on devices where you can control it, you can still abort due to things like running out of stack space; proving that you couldn't do these things would be substantially beyond any static analysis Rust currently does, for a whole host of reasons, and being able to do it at all would probably be very difficult outside of a very narrow range of programs.
I’m surprised, wxWidgets seems to be the only thing that supports custom key bindings under OS X. Neither Java, GTK+ nor Qt do that (I'm always remapping home and end because the OSX default simply doesn’t make much sense). Besides that I think the only viable way is having a separate UI for every platform like Transmission.
I personally don't consider "create a package for every Linux distribution" to be a good story. It's probably the *right* way to do it, but there's no way I'll ever have the time to achieve something like that. A nicer story would be to just distribute binaries. But there are caveats that come with this, and they're not necessarily the fault of `rustc` either.
Remind me to dig up a certain design which allows reading from a mutably borrowed stack local and prevents potential issues with multiple threads and temporarily invalid/undef data. cc /u/nikomatsakis - he might remember the one I'm talking about ("no destructors of this scope can reach the data")
If you want unsafe Rust to count you have to stop claiming that it's memory safe. You can't say that Rust supports feature X, Y and Z and then omit that you only can get one of those at a time. That would just be false advertising and trying to cheat people. &gt; but now you're expecting something out of the Rust implementation which you weren't expecting from other languages If I can't expect anything out of Rust that I don't already get from a different language, what reason would there to use Rust?
Why would one need a package for every distribution in the first way? The executable file is the same for Linux or not? So what's the point of doing it 50x for each distribution?
You're jumping between the two extremes here, and conflating what I was trying to say. &gt; If I can't expect anything out of Rust that I don't already get from a different language, what reason would there to use Rust? There is _nothing_ bad about using `unsafe` blocks to create zero-cost implementations of basic data structures like dlists as long as you expose a safe interface. You don't get to say that "Rust is bad at dlists because `unsafe`" since a C++-like dlist implementation with `unsafe` blocks easily can be designed to expose a safe interface. Rust still has its memory safety there, and Rust was just as good as C++ at implementing a dlist. In this thread others seem to be mentioning that the borrow checker makes it hard to implement a dlist. No. The borrow checker makes it hard to implement a dlist in safe Rust code. One can implement a dlist with some unsafe code (unsafe code that is _easy to reason about_, and isolated), and expose a safe interface. Rust is still living up to its guarantees, and it wasn't _bad_ at doing the dlist. Worst-case Rust is best-case C++, here. Now, if someone was to claim "Pure safe Rust code is bad at implementing things like dlists", I would agree. It takes a combination of `Rc` and `Weak` to do that, and that requires some thought and gymnastics. _However_, no other language has the concept of "pure safe X code", so this point should not come up whilst _comparing_ languages (see: "Writing a double linked list is really hard for instance, while it's pretty trivial in Java/C++/etc.") &gt; If you want unsafe Rust to count you have to stop claiming that it's memory safe. Safe Rust is memory safe. You can use unsafe Rust to create the memory safe abstractions that safe Rust doesn't allow directly, and expose a safe interface. There's no false advertising here.
&gt;&gt; [1] is unfair. I don't think so, because Rust starts with C abi, and with slightly different choices they could make a 1:1 mapping between trait method implementations and member functions. Its the ability to have 2 traits with the same name function on the same type that prevents it. My view is, it would be preferable to remove that part of the namespacing, and require that library authors come up with complimentary method names when 2 traits are used in the same domain .. for the benefit of opening up the ability to , say, interface with Qt libraries or Unreal Engine or whatever more easily.. or gradually insert it into existing C++ projects just like you get mixed language ability on the JVM or CLI. I think this is a missed opportunity, so I think its' fair.
There can only be Brood War.
It's trivial in Go too. But not generically.
Doesn't n =&gt; println!("I'm an old person of age {:?}", n), cover that case?
It's pretty easy to reason about tiny bits of unsafe code. Compare it to your usual C++ program where _everything_ is unsafe code, having a couple of bits of unsafe code is a big improvement. Sure, we can all hope for a language that does complicated proof-checking and whatnot, but such a language probably wouldn't be that usable, or even feasible. Rust has a small human element in its memory safety -- that's a small price to pay for overall memory safety. &gt; Creating applications by combining some library functions is a secondary target. So creating the interface safely would be more important than being able to use the interface safely. Implementing the libraries is the most important use case for a systems language, not using them. Secondary target or no, that's where the bulk of systems programming is involved in anyway. Do you think the C++ portions of applications like Firefox are just _libraries_? No, there's tons and tons of application code there. We want as much memory safety we can get, and if we have to manually verify a few unsafe blocks, that's fine. Besides, when I said libraries I was mostly talking about small abstractions like `DList`. These are very easy to verify, and once you have these down your larger, more meaningful libraries can use these without needing `unsafe`.
It sounds like the current rust mangling prevents linkage hell where you want to use two crates and each one uses a different version of a third crate. To me this is a really nice thing to have, after experiencing similar problems in haskell. C++ classes and rust traits don't really line up 1 to 1 so there would be problems there. You wouldn't be able to support the more out-there C++ interfaces with templates, multiple inheritance, stdlib, etc. I for one am glad they didn't decide to support C++ abi, since that would bring in all the complexity of C++ into the rust language - to fully understand rust linking one would need to understand C++ linkage and not everyone comes from a C++ background. In my experience its always better to make a C interface for your C++ library anyway. Less convenience its true but you pretty much know what such an interface will look like, and then you can use your stuff from python or wherever. For existing libs that are C++ interface only someone will have to make a shim in C++ that will present a C interface, and its guaranteed to work since that would be done in C++. 
is anything personified as male? from dinosaurs, to the ocean to airplanes, i can only think of examples where "she" or "her" is used.
Thanks! I hadn't found the answer yet, I find it quite disheartening to be defeated by such a seemingly simple problem. Thanks a lot, you're a legend &lt;3
&gt; Rust type system is Turing complete Is there an example program with non-terminating type checking?
I haven't seen evidence that 'all the libraries will do lots of unsafe things'. The class of things that require unsafe seems to be quite small. It seems that a few libraries containing various types of collections will cover a lot of them. For the work I'm doing currently it should cover all of them. I'm ignoring FFI (obviously). 
I mean, the best way to distribute software on Linux is to include it on the various, uh, Linux distributions. Binary packages on Linux aren't very ergonomic. Cargo has the same limitation of other language package systems like Cabal or Rubygems: it can't refer to native dependencies metadata in Cargo.toml. One can have ad-hoc dependency resolution by calling curl and then make on build.rs but this is.. hacky, and comes with no metadata. (also hacky is calling git clone instead of having git submodules.. but I've seen some packages do this). Distribution packages can refer to native dependencies and integrate well with the system. Now, on Windows, developers are on their own to deliver their end-user binary package (at least for traditional desktop apps not tied to the Windows 8 store), so perhaps Cargo should build proper Windows packages (I would prefer .msi packages and not just .exe installers).
Here are a couple of messy plugins I've made to solve this problem for 1.0: [string_to_expr](https://github.com/SkylerLipthay/string_to_expr/blob/master/tests/tests.rs#L19) [interpolate_idents](https://github.com/SkylerLipthay/interpolate_idents)
Rx is a library for event streams, which covers composing of different streams and scheduling the streams across schedulers(threads). It can be used for approximatly the same use cases as one would use for examle channels in Go. With some differences like push vs. pull approach, synchronous vs. asynchronous delivery and threading behavior. So it is exactly for "sharing data by communicating". 
Or "simply" have a combination of: - numeral inference =&gt; a `double` is required, so `5` is a `double` - implicit cast =&gt; `5` is representable as a `double` without loss of precision, so it's allowed
Sounds slightly dangerous since it hides the `unwrap()` and the potential associated panic.
&gt; It wasn't too hard to provide operators where the vec parameter came first, but I also wanted to support having it second. Wasn't this solved by the implementation of associated types to distinguish in types and out types? &gt; but I'll probably give it another go when the 2.0 release comes out I would not wait for 2.0, because it might be a while. Rust aims at respecting SemVer, and 2.0 will mean a new version of Rust that is backward incompatible with 1.0. On the other hand, continuous improvements and even big features (as long as they are backward compatible) are planned to be delivered every 6 weeks.
Yes I wasn't looking for hacks. That is fascinating though.
Yup. And I find your 'unknown unknown' categorization to be a really good way of putting it. I wish I was a UX person...
This is also a problem for me. Please consider calling this feature something like 'willnotpanic' or 'nopanic'. Since Rust does not have exceptions it is confusing to have 'throw' in the name. 
First of all: stop downvoting these guy's posts simply because he has a critical position towards rust! His posts are well-written, he is listening to replies, and simply has an opinion he would like to talk about. He also seems to be familiar with rust, and is not basing his arguments on "hear-say". Remember: *disagreeing* is *not* a reason to downvote someone. Disagreeing *is* a reason to put forward your own view, however. &gt; So I see it as a problem that people seem to go to unsafe when it comes down to the bits and bytes and high performance. Yes. And people don't like it either. As much as possible is done with safe code, and when unsafe code is necessary to accomplish things with the same speed, people ask "why?". An improvement of the borrow-checker/rust might very well come out as a result.
I've heard complaints from C people, working closely with hardware about Rust not being "transparent" enough, not being easy to "reason about". They usually have the same claims against C++. This especially applies to functional-style constructs, abstractions resulting in layers of inline functions, not mapping obviously into hardware instructions. While it's usually clear what code will be produced from traditional C for loops, the same can't be said about things like `v.iter().skip().zip().enumerate()` and we can only cross our fingers and hope that LLVM will break through all these layers and maybe even vectorize something. Some of those people actively despise Rust for "pretending being a low level language" and not providing such transparency and predictability. It may be a cultural thing (or may be not), but this culture really affect the minds and Rust will have hard times being adopted by people valuing the mentioned qualities in C.
&gt; It depends on your definition of unsafe. And Rust includes things like iterator invalidation in its definition. The reason behind this rule is basically that in large codebases, mutating the same object from different functions is almost the same thing as a data race in threaded code. Iterator invalidation is just a form of aliasing violations, and preventing those is absolutely essential for preserving type safety because Rust allows to change the type of some objects in place: [A Type Safety Hole in Unsafe Rust](http://www.enyo.de/fw/notes/unsafe-rust-type-safety.html)
Going by this error handling page: https://doc.rust-lang.org/book/error-handling.html - It uses an enum. I don't have any error handling cases that work with enum. It's important (to me at least) that I always be able to include some context from the scope where the error occurred. So I always wind up using a struct. As a newb I just want to start with a flexible error handling mechanism. I can optimize later if I'm concerned about the Error object taking too much RAM. An idiomatic error handling pattern should also include: - impl FromError&lt;io:Error&gt; for YourError ... - impl Display for YourError ... - impl error::Error for YourError ... - impl YourError { fn new(...) -&gt; YourError {...} } and have the example use 'new' something like: if some_result == -1 { return Err(YourError::new(some_result)); } Ok(some_result) I know it's a bit more code but I think helping people structure their Error handling this way from the start is going to be helpful. 
Awesome, thank you.
The [print example](http://rustbyexample.com/print.html) was just rewritten and expanded because of this thread. Check it out. If there are still issues with it, please note them here or on this rustbyexample [issue](https://github.com/rust-lang/rust-by-example/issues/495) or just create a new issue.
I'm not saying it's impossible, it's just more cumbersome to use because of the ownership of the captured variables in Rust.
&gt; Wasn't this solved by the implementation of associated types to distinguish in types and out types? If it has, that'd be great! Do you have any more info about it, or a link to some docs? &gt; I would not wait for 2.0, because it might be a while. Sure, but it's not like I've got nothing else to do in the meantime. :-)
Not related to the code, but it is interesting how my brain gets bit confused by the screenshots and recognizes both hex and rectangular grids simultaneously: http://i.imgur.com/BdiA5lo.png 
I've always thought of Rust as an alternative to C++, so the criticism that Rust has the same caveats as C++ does is actually pretty satisfying to me! Frankly speaking, given the abstractions we're given, I doubt we could achieve the same low level transparency as C has. That said, I want to say that even C is dependent on optimization nowadays. It is not an assembly language, after all. The speed difference between the unoptimized build and the optimized one in C may not be huge compared to Rust, but it's still there, and sometimes [it can make significant differences](http://www.roguelazer.com/2015/02/beating-the-compiler/). Therefore, being low level is really a relative thing. We should always care if our code optimizes well, regardless of the language. Don't get me wrong, I don't want my comment to be seen as a shameless defense on Rust! I do like C's more-bare-metal feeling. I just think that given the abstraction Rust gives to us, the language (and its implementation) is doing its job quite well.
You can't move horizontal right? So there is actually a hexagonal grid. Or... is the rectangular grid actually an octagonal grid reduced to 6 sides? I think they're called hexagonal... :P
I've also heard from /u/jonreem that HKT would help specifying the lifetime of streaming iterators. Is this still considered viable?
Just FYI: you would have actually had a considerably easier time if you *didn't* make the world thread-local, and just passed it around as a context parameter. This is because Rust can know for sure that the world is not aliased when you pass a `&amp;mut World`, while when it's thread local it... doesn't (and as you found out, it can get quite annoying to keep track of). If the boilerplate gets to be too much, you can alleviate it by creating a temporary structure to hold the context (plus some other stuff) and implementing your program as methods on that context. (I'm not sure if this was what you were referring to when you called it an "ergonomical nightmare" but it shouldn't be more than a single extra parameter per function, as long as you're passing the same context around consistently and are keeping to a single thread). Another useful trick, if you for some reason need a few mutable references into the world at once, is to write some free functions that only take the parameters you need, and then destructure like so (it's not permanent if you use references): { let World { ref mut my_first_thing, ref mut my_second_thing, .. } = world; some_code_that_takes_my_first_thing_mutably_this_block(my_first_thing); some_code_that_takes_my_second_thing_mutably_this_block(my_second_thing); } // Back to normal.
This was my original prognosis, although I know /u/eddyb has some other good ideas I think. From [`rust-streaming`](https://github.com/emk/rust-streaming/blob/master/src/csv.rs#L8) (warning, very old Rust), here's a candidate for a streaming iterator trait: trait StreamIterator { type Item; fn next&lt;'a&gt;(&amp;'a mut self) -&gt; Option&lt;&amp;'a Self::Item&gt;; } Compare this with the standard `Iterator` trait: trait Iterator { type Item; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;; } The key difference here is that `StreamingIterator` *ties the lifetime of the item to the iterator itself*. In the standard `Iterator` trait, you can only tie the lifetime of the item to the lifetime of the original "collection" (or buffer, streaming parsers). This is a fundamental aspect of streaming iterators because it means you *cannot* call `next` until the previous result of `next` has gone out of scope. This means that iterator extension methods like `collect` won't work! (As they are currently written.) The reason why `StreamingIterator` as defined above doesn't work is because it fixes the type of the item to be `&amp;Item`. This does not make for a nice abstraction. (Look at the return types on the methods defined for `IteratorExt`.) If we had higher-kinded polymorphism, then perhaps we could return `Option&lt;P&lt;Item&gt;&gt;`, where `P` is a polymorphic type constructor. It might be a reference, it might be the identity or it might be something else entirely. The key is to see `&amp;` as a type constructor itself, so in theory, it could be abstracted over with HKP. This is about as far as I've thought about this, so take what I say with a grain of salt. :-)
I would prefer the name *higher-order type constructors* by the analogy with functions: `u32 -&gt; u32` - function `(u32 -&gt; u32) -&gt; u32` - higher-order function `Type -&gt; Type` - type constructor `(Type -&gt; Type) -&gt; Type` - higher-order type constructor This is only for higher-order type constructors, for traits I think different name is necessary.
Without being able to use the local libraries to do an offline package build this isn't very friendly for distros. Needing to setup a local git mirror for all the rust packages isn't super fun. I'm looking forward to a good cargo story for handling things like I do in C land for dependency management.
Rust already has inference for arbitrary `float` → `fXX` and arbitrary `integer` → `iXX` at compile-time; it seems possible to have hooks for this. One problem is that Rust's traits won't let you describe arbitrary limits like `CanParse&lt;'32'&gt;` which would be needed for getting static guarantees, hence the want for some static assertion mechanism. Your version would still need `unwrap` or some static assertions since you need to deal with the failure cases, no?
A function argument has syntax `&lt;pattern&gt;: &lt;type&gt;`, and it is only the type that matters externally, anything in the pattern only matters for the body of the function. One can regard a pattern in a function argument as sugar for pattern bindings in the body, e.g.: fn func4(tmp: &amp;u32) { let &amp;a = tmp; }
I think the HKT solution makes it harder to maintain backwards compat because an unbound `&lt;T as Iterator&gt;::Item` *might* borrow the iterator.
Can you pattern-match a `&amp;mut _`?
The pattern for a `&amp;mut` reference is `&amp;mut &lt;pattern&gt;`, e.g. if you have `tuple: Option&lt;&amp;mut (X, Y)&gt;`, then match tuple { Some(&amp;mut (ref mut x, ref mut y)) =&gt; { ... } None =&gt; { ... } } gives mutable references to the `X` and `Y` elements of the tuple. (Works the same in `let` and function arguments.)
Well you could do: fn func2&lt;T&gt;(&amp;mut (ref mut x,): &amp;mut (T,) {}
I'll probably try a different approach at some point, the context parameter approach might be doable, though it does produce a bit more cluttered API. I am using the context parameter approach to deal with the rendering system already, and it's reasonably workable. The ergonomical nightmare was more about the naive world implementation where you want game objects that are physically contained in the world have complex methods that want to mutate both the game objects and the rest of the game world. This part gets solved by an entity component system. The partially mutable destructuring trick does look interesting, didn't know that one.
I am planning on making the keys user-configurable too, but since I use a non-qwerty layout myself, I want the game working with sensible defaults out of the box instead of forcing me to start by writing my own key remapping config. Though now that you ask, I realize I do need a configuration option to disable the scancode remapping. People might want to run on platforms where the magical scancode maps don't work, and there the simplest way to get going is to just use the keyboard layout instead and let the user rebind the keys if needed.
The presence of the `while` keyword in the absence of a C-style `for` loop for anything other than a simple iterator may lead to increased accidental/unintentional [infinite loops](http://en.wikipedia.org/wiki/Infinite_loop).
&gt; It makes me fear that all the libraries will do lots of unsafe things, much weakening the "memory safe" promise. There are already a lot of libraries, so you can test this for yourself *right now*. At least of the things I've written, `unsafe` does not tend to crop up that much. This includes a fast CSV parser, regular expressions (reasonable performance, but not blazing fast) and fast suffix array construction. 
I don't see any reason to be a stickler about this. [andrew@Liger regex] grep unsafe src/*.rs src/re.rs:unsafe impl&lt;'r, 't&gt; Searcher&lt;'t&gt; for RegexSearcher&lt;'r, 't&gt; { # nothing to see here... Plenty of other things I've written either don't use `unsafe` at all (e.g., `quickcheck`) or have only a few select uses (e.g., `docopt`). The claim here isn't, "unsafe is never ever used so don't worry about." The claim being combatted is whether unsafe is sprinkled everywhere throughout libraries. At least of the code I've written, there is very little `unsafe`.
And this comment is alienating to people who read manga / watch anime ;)
There is `std::net::SocketAddr` now, which is an enum over a `SocketAddrV4` or `SocketAddrV6`, which are `IpAddrV4`/`IpAddrV6` plus a port number. It looks like nickel.rs could be modified to take a `SocketAddr` instead of an `IpAddr` plus port number easily. I suspect the short reason for why `IpAddr` was removed is that `SocketAddr` is better to pass around for connecting to things, there's nothing in the standard library which needs an `IpAddr`, and it can be added in the future backwards-compatibly if it's needed.
&gt;Basic mutability: let a: i32 = 0; a = a + 1; This is wrong. Also: &gt; Performance enhencement
I also changed the default run command from rustc to 'cargo run' It's really convenient because it will compile and run the whole application, regardless of which file you have open when you hit ctrl-b (it can be multiple folders deep under the app root). 
Exactly. I used to have plenty of time, but `std::io` wasn't stable. Now, I haven't had time for a little over three weeks, and it's stable.
Might've been a mistake. https://github.com/rust-lang/rfcs/issues/988
nice work! i will have to give it a spin sometime
try to solve C10K with threads in Rust.
Did you compile with optimizations on? 
You may be interested in https://crates.io/crates/cfor.
For now, I'd recommend using the mmap bits in [nix-rust](https://github.com/carllerche/nix-rust). I used it to build an [mmap_allocator](https://github.com/12sidedtech/RxRust/blob/master/src/mmap_allocator.rs) a while back. Although now it is horribly outdated. 
I ran a test binary with ```cargo build --release``` my understanding is that enables optimizations.
I would tend to avoid macros like this as it isn't really part of the base language. Whilst it is a reasonable solution I don't want to redefine the language and would prefer to use a `while` loop as it is a recognised part of the base language.
&gt; Roguelikes are basically some simple display code you throw up in a weekend and then years and years of accrued rat's nest of world logic code that needs to know obscure things about dark corners of the game world deep in some logic path or another. I think many games are like this :)
Bigint is several orders of magnitude slower than GMP. I recommend to use `rust-gmp`.
Wow, already at 0.1.1! It's crazy how productive you are – even though you're recording and explaining everything!
`String` is just a short name for `collections::string::String`. The difference is in the `&amp;` - `&amp;String` is a reference to a string and `String` is just a type name. Then there is a `&amp;str` which is a string slice - all of this and more is described in [the Rust book](http://doc.rust-lang.org/book/more-strings.html).
For future reference, I think you want `cargo run --release` to run the optimized binary. 
The thing is, I assume that most of those libraries haven't gotten to the point yet where they really try to optimize performance. As long as you just implement a nice library "for fun" and don't really care about the ugly corner cases all that much, of course it will look pretty. But the time will come where people start to benchmark and compare speed and features with established libraries. Only then it will really get clear if or what sacrifices will have to be made and to how much `unsafe` code or other tricks this will lead.
I would have expected the compiler to refuse to compile if the cast cannot be done without loss of precision.
You are correct, thank you.
Why don't you want to call mmap manually? Just curious.
Hmm, perhaps the keyword should be called `euclid`. :o)
&gt; It is not fair to characterize libraries as "for fun" unless you've actually used them or read their code. That wasn't specifically aimed at your libraries, just that in general probably many libraries currently aren't very mature. &gt; It's not surprising IMO when extreme performance requires one to do unnatural or complex things. It's not surprising at all, that's why I expect it will happen. But since Rusts target is to replace C or C++ in these areas, it would be important that it's not necessary often. Because the guaranteed memory safety would be a main point why one would use Rust.
You should put a screenshot of your game in each blog post, you'll get much more readers that way, hacker news won't be able to withstand it.
I think it relates somehow to the fact that you can compare `String` and `&amp;str` but not `Option&lt;String&gt;` and `Option&lt;&amp;str&gt;`: let c = "".to_string() == ""; // this works let d = Some("".to_string()) == Some(""); // this does not Maybe it is some special rule in the Rust compiler. 
Couldn't you just implement a MemoryMap yourself, with Drop?
The prototype inheritance idea was interesting with copy-on-write from the parent.
&gt; You also want to *avoid leaking any secrets via side channels*. In the context of RSA, you want the runtime, power consumption, electromagnetic/sonic radiation, etc of modular exponentiation to *not depend on the exponent*. This is not exactly trivial. Exactly. Avoiding relatively trivial things as timing attacks should be a concern on day one. *Normally initialized Big Integer is inheretly* ***bad*** *for this*, as the timing of the operations highly depends on the size of the integer (and reallocations are only done when needed). A (probably better) approach would be to estimate the maximum number of bits needed for a certain operation beforehand, and then use exactly this number (and use u64 or something if possible). From a short look, it seems to be that *mpz_init2* from [the mpg library](https://gmplib.org/manual/Initializing-Integers.html#Initializing-Integers) (bindings seem available) could be used for this, and would also not need any reallocations. 
ECS is a popular game programming pattern but I'm wondering if it really fits Rust's type system. We should be looking for other patterns out there that might work better in Rust.
Your last example is absolutely true but I'd like to point out that in C99 and C11 you can help the optimizer by using the `restrict` keyword, like so: void add_to_array(int *restrict out, int *restrict rhs, int length) { for (int i = 0; i &lt; length; i++) { out[i] += rhs[i]; } } http://ideone.com/OaQ2xy However, you the programmer are responsible for verifying that there is no aliasing, and if you get it wrong I imagine you'll get some *very* strange behavior.
aturon says it will probably be some time before std has a stable replacement, so if someone hasn’t already you could copy this code from std into a standalone library on crates.io. The advantage of non-std libraries is that they can evolve independently of the language. If there is a new backward-incompatible version of a library and I don’t want to update for whatever reason, I can keep using the old one and still upgrade my compiler and use new language features.
Yeah, `restrict` is great, but, as you say, there's no compiler assistance.
Even if you're writing your own toy compiler in rust?
The bit about the items being classified as a room seems really interesting, would you mind expanding on that? I'm having trouble visualizing what you mean.
Yes.You'd make a better, external crate and use that instead. Or use someone else's. :)
Thanks, that helps clear it up for me. Do you know whether the rust development team is looking to add in support for streaming iterators, or is this something that has other technical limitations?
So that cop-out answer is of course "yes you can implement anything you can dream of, including just implementing Haskell to get lazy-everything". The responsible answer is "iterators are our solution to lazy computations". use std::ops::Add; let x = vec!["hello", "there", "you"]; let y = x.iter().map(|s| s.len()).fold(0, Add::add); let z = x.iter().map(|s| s.len() % 2).fold(0, Add::add); println!("{} {}", y, z); let legit_lazy = 0..100; let still_lazy = legit_lazy.skip(10).take(40).map(|x| x*x); // force evaluation println!("{}", still_lazy.fold(0, Add::add)); [works today (playpen)](https://play.rust-lang.org/?code=fn%20main%28%29%20{%0A%20%20%20%20use%20std%3A%3Aops%3A%3AAdd%3B%0A%20%20%20%20%0A%20%20%20%20let%20x%20%3D%20vec!%5B%22hello%22%2C%20%22there%22%2C%20%22you%22%5D%3B%0A%20%20%20%20let%20y%20%3D%20x.iter%28%29.map%28|s|%20s.len%28%29%29.fold%280%2C%20Add%3A%3Aadd%29%3B%0A%20%20%20%20let%20z%20%3D%20x.iter%28%29.map%28|s|%20s.len%28%29%20%25%202%29.fold%280%2C%20Add%3A%3Aadd%29%3B%0A%20%20%20%20println!%28%22{}%20{}%22%2C%20y%2C%20z%29%3B%0A%20%20%20%20%0A%20%20%20%20let%20legit_lazy%20%3D%200..100%3B%0A%20%20%20%20let%20still_lazy%20%3D%20legit_lazy.skip%2810%29.take%2840%29.map%28|x|%20x*x%29%3B%0A%20%20%20%20%0A%20%20%20%20%2F%2F%20force%20evaluation%0A%20%20%20%20println!%28%22{}%22%2C%20still_lazy.fold%280%2C%20Add%3A%3Aadd%29%29%3B%0A}), with most adaptors lazily evaluated (fold forces evaluation). See: * http://doc.rust-lang.org/std/iter/trait.Iterator.html * http://doc.rust-lang.org/std/iter/trait.IteratorExt.html The hard answer is: how much do you want to support? Are your lists persistent? shared? Do you need to be able to move out of them? How much is lazy (is the result of fold only evaluated if it is *used*)? If you evaluate a lazy list once does it ever need to be evalutated again? In general you can't move out of an immutable list without consuming it (and consequently having the only remaining references to any version of it). On a lark, I wrote a [functional-style persistent cons-list in Rust using reference-counting](https://github.com/Gankro/collect-rs/blob/2c724ca7631a792a1bc6fd6fa4f89f6f80b6bb3e/src/immut_slist.rs). In the destructor you can see under what conditions values can actually be moved out. True Haskell-style evaluate-at-most-once lazyness in Rust is a pretty nasty proposition. It requires some kind of shared mutable(!) state so that the first true evaluator of a value can make it available to all other potential evaluators. It also consequently requires some kind of reference counting or garbage collection scheme to properly discard these potential and actualized evaluations. In Haskell this is "easy" because of the language's semantics, but in Rust this would be quite nasty. In fact nastier than in most languages because of all the shared mutable state and the potential need for a legit GC implementation for performance. It's doable, but you're going pretty hard against the grain. What on earth do you want this functionality in Rust for?
I wish I knew! But I haven't been following this issue very closely. I'm interested in the answer, as well.
I'm just guessing but I think rooms are simply the current object /entity that you can interact with. Some properties are probably exits to other rooms. Then you could say another 'exit' is an item. Entering an item would be an interaction with that item just like the room but would only have one exit back to the previous room. Not sure if that makes sense outside my head.
Looks like the build is failing. I am unable to compile this with the current nightly. What rustc version are you using?. The travis also seems to be broken.
His github contribution activity is crazy! 443 days? that's the longest i ever seen. 
&gt; there's nothing in the standard library which needs an IpAddr How about UdpSocket join_mulitcast() and leave_multicast()? They now take SocketAddr, but used to take IpAddr, so the port must be unused. And there are plenty of situations where one may want to talk about an IP Address abstractly (across v4 and v6) without talking about a port. 
It's all due to this one: https://github.com/erickt/rust-serde/pull/42#issuecomment-85291000 There should be a new serde release in two days, until then you would have to use local cargo overrides.
Man, I totally misunderstood his username ... I always read it as Man|is|hearth, not Manish|earth ...
That makes sense. I guess our current Iterators let us write code like let l = ["hello", "world"]; let mut r = l.iter(); let x = r.next(); let y = r.next(); println!("{} {}", x, y); Which is mutually exclusive with the notion of a streaming iterator (where the next value is only valid until the current value is returned). I guess you can't have your cake and eat it too. This behaviour is something that anything implementing `Iterator` *has* to include, unless we were to make the above code illegal.
I suspect that game world logic programming is just intrinsically tricky with a strict type system. It could work if you started out with pretty much a full idea of everything your game will have (chess, Tetris clone), or if your game will have mostly data-driven variations of objects with very few standard interactions (side-scrolling shooter). With roguelikes, interactive fiction, and whatever all the Zeldas and Assassin's Creeds are called, you start out having very little idea of all the nuts and bolts you'll end up needing with the complete project. A lot of them end up doing something like dynamic typing, either by adding a scripting language layer and using it to implement the world logic, or implementing an ad hoc dynamic typed system like an ECS or a [properties pattern](http://steve-yegge.blogspot.com/2008/10/universal-design-pattern.html) on top of the statically typed base engine. Finally, there are some really out there systems like [Inform 7](http://inform7.com/), which mixes a declarative logic programming style and the always iffy attempt at using a natural language syntax, which sorta makes sense because it's for programming interactive fiction. Still, I7 gets you [actual source code](http://inform7.com/learn/eg/rota/source.txt) that reads like a pen &amp; paper RPG manual at times, and that's pretty close to how I see an ideal world logic description language working. Don't have that many ideas on how you'd get from Rust to there though. I guess you'd need a pretty big compiler plugin.
Yeah, almost everyone reads it as ManisHearth or ManIsHearth the first time. No biggie.
Yes, it is implemented only when coresponding trait is implemented for inner type, but it was mental shortcut.
Thanks for your detailed answer. &gt; What on earth do you want this functionality in Rust for? I think the benefits of persistent data structures are quite obvious, e.g. to avoid excessive copy-on-write. Edit: When reading this and the "What is Rust bad at?" thread I cannot help but being disappointed with Rust.
I don't think those gaps will be filled. In any case, it seems GitHub will cap the longest streak calculation to only start 365 days ago unless the current streak is the longest one.
I do not think `rust-lazy` is good enough. It requires its thunks to be of `'static` lifetime which would make it impossible to map over a list lazily.
I don't know really; I was invited for an AMA by an acquaintance. It seems to be a site where you can collect your skills in one place, and participate in competitions and whatnot.
Ah, makes sense.
TIL about `git commit --fixup` and `git rebase --autosquash`. :) I should definitely try to tackle some E-easy ones as well…
If you skip the "Building Rust" section and the `.servobuild` file, the default for mach is to download binary snapshots of the compiler automatically. Rust takes a long time to build, and you probably don’t need to do it yourself!
&gt; Or are you thinking of a "support library", i.e. adaptors etc? I think these can develop at least as well ouside libstd as inside. If I knew a reasonable way to do this, I would have started that project already. I want them bad enough that I do [wildly unsafe things](https://github.com/BurntSushi/rust-csv/blob/master/src/reader.rs#L986-L1000). It makes for a rather dramatic performance different in tools like [`xsv`](https://github.com/BurntSushi/xsv). For example, consider a large CSV file with 100 columns. If the user wants to filter that down to only two columns (or perhaps, perform an operation like `join` with only two columns), then a streaming iterator makes for an excellent and performant abstraction (because you only need to allocate a 2-element `Vec` instead of a 100-element `Vec`). But in order to use it in a way that isn't painful, you need all of the iterator adapter methods. But how do you provide those in the context of a streaming iterator without HKP? I dunno. Streaming iterators are hard to talk about because we have to be careful to delineate two distinct issues: 1. Compatibility with `Iterator` and/or the `for` loop. 2. Good abstraction tools for working with streaming iterators. (e.g., `StreamingIteratorExt` --- how do you write it?) I care much less about `1` than I do `2`. `1` would be great, but I would be happy with the ability to do `2`.
I still can't imagine how is it possible :) There must be days when you physically can't access internet, for example. Does github count commits made offline and pushed later?
Cool, that clarifies. First question, why isn't the item an associated type? Does that work? See here: http://is.gd/KX7L5m
yup its rust-gnuplot, here is what my code basically amounts to (trying to do semi log plot) This line in particular seems to also disable manual zoom in addition to not setting the plot to y-log fig.axes2d().set_y_log(Some(10.0));
write! uses format_args (format!) which is meant to produce things for human readable consumption. So you are producing exactly what you suggest, that is the data converted to ascii so that it can be printed to a terminal. if you want the vec to contain 255, 2, 1, then instead of write! use ```rust v.push(ch); ``` 
ah yes... v.write works. Thanks a lot! I was thrown off by my assumption that println! and write! should behave in the same way here (I still expect they should, actually...)
The first println! is using format_args in exactly the same way, it is producing the ascii** representation of each digit of each number in order. :) The rule of thumb is simply don't use format! or write! if your intention is to fill data structures and push bytes around, use read/write/push etc. **actually utf-8
I tried to implement map, too, that's why I was silent! It's hard :-) Your example compiles with this simple change though: match self.iter.next() { Some(elt) =&gt; Some((self.f)(elt)), None =&gt; None, } However this is another bad problem: After giving a lifetime'd reference to the map closure, we never get the borrow back (because the closure could theoretically store it away). http://is.gd/VABMhr
HKT seems like a technically inaccurate term. See: http://www.reddit.com/r/rust/comments/2zxk6n/difference_between_associated_types_and_type/cpn93hw
Immutable data structures are neat, but have overhead on some levels. For instance, in Clojure this was done with RB trees and append-like functions, which I can't help but think costs more than mutation in certain scenarios-- then again it has its upsides for concurrency. [interesting discussion/reference](http://stackoverflow.com/questions/16270598/what-is-the-data-structure-behind-clojures-sets)
So let's assume a list implementation like this: pub struct List&lt;'r, A&gt; { head: Option&lt;Rc&lt;Node&lt;'r, A&gt;&gt;&gt; } pub enum Node&lt;'r, A&gt; { Nil, Cons(A, Thunk&lt;'r, List&lt;'r, A&gt;&gt;) } (The `Option` is there to support a custom `Drop` implementation). Now when defining some basic lazy folding (assuming `rust-lazy` uses explicit lifetime variables instead of `'static`) and trying to define `map` in terms of `foldr` there are immediate lifetime problems, which presumably mean that the closure defined inside of `map` does not life long enough: impl&lt;'r, A&gt; List&lt;'r, A&gt; { pub fn nil() -&gt; List&lt;'r, A&gt; { List { head: Some(Rc::new(Nil)) } } pub fn foldr&lt;B, F&gt;(&amp;self, z: B, f: &amp;F) -&gt; B where F: Fn(&amp;A, Lazy&lt;B&gt;) -&gt; B { match **self.head.as_ref().unwrap() { Nil =&gt; z, Cons(ref h, ref t) =&gt; f(h, lazy!(t.foldr(z, f))) } } pub fn map&lt;B, F&gt;(&amp;self, f: F) -&gt; List&lt;B&gt; where F: Fn(&amp;A) -&gt; B { self.foldr(List::nil(), &amp;|x, acc| List { head: Some(Rc::new(Cons(f(x), acc))) }) } } error: cannot infer an appropriate lifetime due to conflicting requirements self.foldr(List::nil(), &amp;|x, acc| List { head: Some(Rc::new(Cons(f(x), acc))) }) note: first, the lifetime cannot outlive the anonymous lifetime #2 defined on the block at 47:12... List { head: Some(Rc::new(Cons(f(x), acc))) }) ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ note: ...so that expression is assignable (expected `lazy::Thunk&lt;'_, List&lt;'_, B&gt;&gt;`, found `lazy::Thunk&lt;'_, List&lt;'_, B&gt;&gt;`) List { head: Some(Rc::new(Cons(f(x), acc))) }) ^~~ note: but, the lifetime must be valid for the anonymous lifetime #1 defined on the block at 45:73... pub fn map&lt;B: Clone, F&gt;(&amp;self, f: F) -&gt; List&lt;B&gt; where F: Fn(&amp;A) -&gt; B { self.foldr(List::nil(), &amp;|x, acc| List { head: Some(Rc::new(Cons(f(x), acc))) }) } note: ...so that types are compatible (expected `List&lt;'_, B&gt;`, found `List&lt;'_, B&gt;`) self.foldr(List::nil(), &amp;|x, acc| List { head: Some(Rc::new(Cons(f(x), acc))) }) At this point I am stuck :( 
That can be accomplished using Any AIUI.... Or probably using generics and a better design. I'd look into using rusts type class system, traits are great.
No, it will work on anything where _all contained references_ are `'static`. So any struct/enum that does not contain references is fine. So, `let x = vec![1,2,3,4];` is `'static`, but `let x = vec![&amp;1,&amp;2,&amp;3,&amp;4]` isn't, since the latter is a `Vec&lt;&amp;'a int&gt;`.
The module documentation has examples: http://doc.rust-lang.org/nightly/std/any/index.html And oh, I see what you're referring to: http://doc.rust-lang.org/nightly/std/any/trait.Any.html says &gt; The Any trait is implemented by all 'static types, and can be used for dynamic typing This is a really poor line, that's what's being confusing. The following line is way better: &gt; Every type with no non-'static references implements Any
The OP wasn't asking about inference...
I don't think he's actually talking about type inference. I think he's talking about the Go-esque style of `x, y: i32` being shorthand for `x: i32, y: i32`.
Does any other language have this (kind of strange) sugar? I can't remember any.
Ah, I see, it's Go. But it still looks strange.
As far as I know, Go is the only language that does this. I really like that feature in Go, but given how unique it is, it's not surprising that Rust doesn't have it.
IIRC, Nim uses this syntax too.
Note that function declarations are actually patterns, so the syntax you propose would be inconsistent with that.
Pascal/Delphi uses it.
https://github.com/rust-lang/rust/issues/20063
There's some other discussion of persistant data structures going on right now: http://www.reddit.com/r/rust/comments/302gm2/immutable_lazy_lists/cpoyoov
This encourages weak typing, and I would consider it an antipattern to have lots of parameters of the same type.
C does similar for variable declarations: Eg int x,y,z;
Feels like syntactic diabetes to me.
While you're right that this isn't about type inference... I occasionally hear people mentioning that Rust's hard requirement of type annotations in function signatures is a stylistic choice. Maybe that was the case when the project began, but I don't think it is currently. I doubt that whole-program lifetime inference is tractable if people want to keep compilation speeds sane (it already has some trouble with complex closures, and MLton compilation is rather infamous in that regard). It would have to be done at link time unless you had to describe types at crate boundaries. And since Rust does not have a decidable type system, we know that some of its features (like associated types, maybe?) are for sure not inferable. I think it would be interesting to try to write a version of Rust that did, though... if only to get decidability back :|
As others have pointed out. Your write! will convert your data into a string. So your 255 u8 value gets converted into an array of individual characters ['2','5','5']. Those characters are mapped into ascii u8 values, so ['2','5','5'] is the same as [50, 53, 53]. Here's an example that might help: playpen: http://is.gd/vegzic
I laughed really hard at the quote this week.
What msys2 shell are you using?
I went through something similar not too long ago. I commented the steps I went through [here](http://www.reddit.com/r/rust/comments/2wz20m/trying_to_build_hematite_on_windows_7/covh1nh). &gt;Run through this guide on the Rust website on [Using Rust with MSYS2](https://github.com/rust-lang/rust/wiki/Using-Rust-on-Windows#using-rust-with-msys2). &gt;Chances are you'll need to have OpenSSL installed too (At least you will at some point). I suggest going through the Windows guide [here](https://github.com/sfackler/rust-openssl?files=1). I went with the &gt;&gt; copy the include/openssl directory, libssl32.dll, and libeay32.dll &gt;option and put everything in Cargo's bin directory. (Not sure if that's the best place but it worked) &gt;Once you've done this, you should use MSYS2 to run your cargo build commands. These are both things I had to go through last night so if you have questions, let me know. 
Incorrect link for New RFC: Add std::env::concurrency_hint. It's pointing to https://github.com/rust-lang/rfcs/blob/master/text/0968-closure-return-type-syntax.md
Strong -&gt; string
Then you might want to delete dnhgff's comment, because it has nothing to do with what Rust is bad, and is thus off-topic.
Already fixed by https://github.com/cmr/this-week-in-rust/pull/48.
Thanks! Why didn' I think of that...?
In many cases you lose efficiency when you use persistent data structures. For example, a flat array (`Vec&lt;T&gt;`) has by far the best performance for iterating through in order, thanks to cache effects. But the only way to implement it as a persistent data structure would be to clone the entire thing every time you make any change aside from appending.
But if you do as clojure does with it's vector and use a very flat tree (so still pretty decent locality), most operations are still O(1). http://hypirion.com/musings/understanding-persistent-vector-pt-1
You still lose cache efficiency when you go from a single contiguous block to a set of contiguous blocks. Plus you lose the ability to pass around a slice as a ptr+len pair like C and Rust do so often.
You could also use the standard `split` method in some nifty way to use both delimeters. The argument should implement the [`core::str::Pattern`](http://doc.rust-lang.org/nightly/core/str/trait.Pattern.html) trait, so you could even build your own pattern if you want. Edit: Don't recommend `split_str`.
I believe `,` is a pattern separator, so we could theoretically allow `$($arg: pattern),+: $t: ty`, e.g. `fn foo((x, y), tuple: (i32, u32))`. would take two arguments... seems confusing, though!
Can you please show a specific example?
&gt; Maybe but it is strictly less expressive I'm not so sure about *strictly*: one can always lazily collect an iterator into a lazy list (i.e. have the "force" operation of the tail call `next`, see below). And, iterators can yield types by-value or by-mutable-reference, e.g. an `Iterator&lt;Item = String&gt;` or `Iterator&lt;Item = &amp;mut String&gt;`, but it is impossible to get such things directly out of a persistent list. --- E.g. with your `List` definition from [above](http://www.reddit.com/r/rust/comments/302gm2/immutable_lazy_lists/cpp2wxo) fn listify&lt;'a, I: Iterator + 'a&gt;(mut iter: I) -&gt; List&lt;'a, I::Item&gt; { match iter.next() { None =&gt; List::nil(), Some(item) =&gt; { List { head: Some(Rc::new(Node::Cons(item, lazy!(listify(iter))))) } } } } Changing `go(0)` to `listify(0..)` in [my example](http://www.reddit.com/r/rust/comments/302gm2/immutable_lazy_lists/cppl6lo) gives the same output (without the `forcing` lines). 
Thanks, that works. :)
It's really unproductive to complain about something for which you have no evidence. If this is something you have actually observed, please provide a link yourself. It's impossible to give a good answer for such a vague question without the appropriate context which you have not provided.
I had to: - add some lifetimes to `foldr` and `map` (which are useful for full flexibility anyway, allowing the user to do what they want with the `&amp;A` reference, e.g. return it, or a reference into it, from the closure) - switch the closure passed into `foldr` to an `Rc` (this reflects how it works in Haskell more closely, where closures are garbage collected). I guess it might be possible to use a reference too, didn't really try. (It definitely took me several attempts at tweaking to get it right.) #[macro_use] extern crate lazy; use lazy::single::Lazy; use std::rc::Rc; pub struct List&lt;'r, A&gt; { head: Option&lt;Rc&lt;Node&lt;'r, A&gt;&gt;&gt; } pub enum Node&lt;'r, A&gt; { Nil, Cons(A, Lazy&lt;'r, List&lt;'r, A&gt;&gt;) } impl&lt;'r, A&gt; List&lt;'r, A&gt; { pub fn nil() -&gt; List&lt;'r, A&gt; { List { head: Some(Rc::new(Node::Nil)) } } pub fn cons(self, x: A) -&gt; List&lt;'r, A&gt; { List { head: Some(Rc::new(Node::Cons(x, Lazy::evaluated(self)))) } } pub fn foldr&lt;'a, B: 'a, F: 'a&gt;(&amp;'a self, z: B, f: Rc&lt;F&gt;) -&gt; B where F: Fn(&amp;'a A, Lazy&lt;'a, B&gt;) -&gt; B { match **self.head.as_ref().unwrap() { Node::Nil =&gt; z, Node::Cons(ref h, ref t) =&gt; { let ff = f.clone(); f(h, lazy!(t.foldr(z, ff))) } } } pub fn map&lt;'a, B: 'a, F: 'a&gt;(&amp;'a self, f: F) -&gt; List&lt;'a,B&gt; where F: Fn(&amp;'a A) -&gt; B { self.foldr(List::nil(), Rc::new(move |x, acc| List { head: Some(Rc::new(Node::Cons(f(x), acc)))}) ) } } fn go(x: i32) -&gt; List&lt;'static, i32&gt; { println!("forcing: {}", x); List { head: Some(Rc::new(Node::Cons(x, lazy!(go(x + 1))))) } } fn main(){ let list = go(0); let n = list.foldr(0, Rc::new(|&amp;a, b: Lazy&lt;i32&gt;| if a &lt; 10 { a + *b } else { a })); println!("first: {}", n); let n2 = list.foldr(0, Rc::new(|&amp;a, b: Lazy&lt;i32&gt;| if a &lt; 20 { a + *b } else { a })); println!("second: {}", n2); } Output: forcing: 0 forcing: 1 forcing: 2 forcing: 3 forcing: 4 forcing: 5 forcing: 6 forcing: 7 forcing: 8 forcing: 9 forcing: 10 first: 55 forcing: 11 forcing: 12 forcing: 13 forcing: 14 forcing: 15 forcing: 16 forcing: 17 forcing: 18 forcing: 19 forcing: 20 second: 210 
If you consider it an antipattern, what would you do instead?
I've actually considered writing up an RFC for this a few times. I'm not the biggest fan of the syntax, but our type signatures are so verbose today that any little thing might be an improvement.
Maybe they didn't expect that they would have to provide "evidence". The *complaint* seems clear enough if you have had or seen similar attitudes yourself. If you haven't then, well, maybe this thread isn't relevant to people who haven't experienced anything like it. Not everyone starts a discussion with the intent of battling it out as if they're in a court room. 
Nim
One of my favorite features.
True, but it's O(log32 n), not log2 as usually referred to. Unless there are more than a billion elements in the vector, it's no more than 6 levels deep. I consider that effectively constant bounded.
Could you please elaborate a little? I did already find this function, but I was unable to understand how to use it the way I want to. Sadly, the documentation of some parts of rust still lacks examples.
Removing the risk of accidentally calling your function with out of order parameters being the most obvious benefit, but there are others. 
&gt;Not everyone starts a discussion with the intent of battling it out as if they're in a court room. Fair enough. As it stands, however, this post is just a statement of the authors personal perceptions of the project, and ones I don't really understand. I keep moderately in touch with Rust's development on GitHub and in this subreddit, but I cannot recall any recent and glaringly wrong problems that have been ignored in the name of pushing out a 1.0 release. So if the author doesn't want to discuss a specific instance, then there isn't as much to discuss. The best I can attempt is this sentiment: If they say no to a breaking change, that's because they do not deem it a serious problem. It's important to realize the core team have been making breaking changes for *years*, and I'm sure they're eager to finalize a stable base upon which to improve instead of reinventing things every few months. Rust will not end up the perfect language, but I believe the core team have done an amazing job of working towards providing the best possible foundation for Rust.
If a type has an inherent method, it will be selected before any trait method. In this case, [`io::Error` has just such a deprecated method](http://doc.rust-lang.org/std/io/struct.Error.html#method.description). One way around the warning is to call the method on the trait, `std::error::Error::description(err)` instead of `err.description()`. I am inclined to think that the inherent `description` method should be removed now.
https://github.com/rust-lang/rfcs/pull/866 mostly people complaining that it would break a lot of current code and they can't afford to change it at this point
It doesn't, e.g. // iterator map let list: List&lt;String&gt; = listify((0..).map(|i| i.to_string())); let n = list.map(|s| s.len()) .foldr(0, Rc::new(|&amp;a, b: Lazy&lt;usize&gt;| if a &lt; 3 { a + *b } else { a })); println!("{}", n); let n2 = list.foldr(0, Rc::new(|a: &amp;String, b: Lazy&lt;usize&gt;| { println!("string: {}", a); if a.len() &lt; 2 { a.len() + *b } else { a.len() } })); println!("{}", n2); 
but that has always been that way, has it not?
Yeah, the most recent change was switching the second form to require a block instead of just an expression.
This was a situation in which there was no clear consensus about what to change the syntax to, even. This was a change that was significantly bikeshedd-y with a small positive, and breaking almost all Rust code in existance, which is a huge negative. It's about balance. :)
&gt; Hmmm... does the purple bar to the left of the fn mean it's deprecated Yes, and you can hover for a reason, which is the same as your warning.
 source.split(&amp;[' ', '|'][..]) Is nicer, IMO.
This was [cross-posted to Stack Overflow](http://stackoverflow.com/q/29240157/155423)
I like it!
Won't this have some potentially bad performance characteristics? For example, if the string were "...1 mb of data...||++"? 
The conversion traits rfc was pretty interesting to read!
Servo is already using persistent datastructures (just a linked list, I think) in layout. /u/pcwalton can tell you more. I'm working on a proposal for a partially persistent slice / string / rope type as well, maybe going all the way to 2-3 finger trees. If you Google a bit, you can find a bunch more people using or proposing persistent data structures in Rust. It's not only a possibility but a reality. You don't need to be disappointed in Rust because one person gave (rather strong) pushback to your idea.
Btw, thank you for basing key mappings on scancodes and not on whatever is used when typing (key map? current keyboard layout? not sure about the terminology). I'm using dvorak myself and am always annoyed when the first thing I have to do when trying a game is rewrite all the shortcuts. There are games out there that do it right: use the physical layout of the keyboard, but still make it configurable, and showing the correct character of the currently selected keyboard layout when doing so. Example: Dragon Age: Inquisition.
If you're going to query Google products, perhaps. For example, [this](https://byron.github.io/google-apis-rs/google-youtube3/index.html) enables your to interact with Youtube (for example, to [upload a video](https://byron.github.io/google-apis-rs/google-youtube3/struct.VideoInsertCall.html) - take a look at the example code). I never had a need to do that though. edit: about that example code. One nice thing about Rust is that those `.unwrap()` calls mark places where your program could exit due to an error (in Rust this is called panicking). For example, `fs::File::open("file.ext").unwrap()` will panic if the video file "file.ext" doesn't exist. A real program would likely handle the error instead of unwrapping, like telling the user "file not found" and prompting to select another file.
I actually have a psychological theory: While Rust is pre-1.0 and in "hype mode" (from external sources), hackers are free to project their dreams on the language. Either they are already seeing the signs, or trust that they will be implemented in future Rust. With 1.0 approaching, some dreams will be gently crushed and we will see an increase in emotional stress. Rust dreamers will meet the cold hard reality full of compromises. ;-)
I think docs could use better layout/design, but AFAIK it's not steve's job. He is working on content. Not to mention steveklabnik is one man, you could contribute to Rustdoc.
Can you show some code please?
The only simple explanation I can think of is "multiple versions of the same crate" - in which case I think `log` (presumably the crate name) should be disambiguated.
&gt; Not OP, but a while ago I suggested that cmp::min/max() should return the first argument when the two are equal (current behaviour is it returns the second argument, which is a little bit unintuitive), and was met with the response that it would break backwards compatibility of a #[stable] API and so wasn't worth doing. &gt; &gt; [...] &gt; &gt; For the most part, the core team have done very well. There's just a few isolated incidents that should be reconsidered. To be clear, there was exactly one voice "against" changing the order of `min`/`max` due to stability, and it wasn't a member of the core team. I don't think it is an example of the core team rejecting something for back-compat reasons. :) (And, even then, it wasn't an outright rejection or even a direct comment on the best argument order, just a comment on the mismatch between the functions... although it certainly had the implication that the `#[stable]` ones couldn't be changed.) (I personally think swapping the order is fine.)
http://is.gd/RJq4Uy Here are all of the extern crates and use statements before error message lines.
You're probably passing a `LogLevelFilter` into a function that is defined in a crate that uses the `log` package from crates.io, whilst your crate is using the in-tree `log` package Keep a `log = ""` in your Cargo.toml. Or if one exists already, remove it.
I would argue that `min` should return the left and `max` should return the right when they are both equal.
This is really helpful. Thank you. I also prefer static approach, in general. I am going to investigate #1 for this use as I really do need to keep the requesting client (actually a micro-service endpoint) decoupled from the service (also actually a micro-service endpoint). I also need to keep things off the stack so boxing is what I understand I need to do. The current version of this system (written in Java) has an effective "lowest common type" of a BinaryStream so I can see that designing a trait to model an equivalent in Rust is what I need to do.
Very interesting example and quite on point. The system I am contemplating moving to Rust from Java is actually the 180 degree opposite of Spring in that there is absolutely no context injection - instead context is determined for each request and obtain via pull requests. I'll investigate whether or not reflection is key for this Rust implementation as part of my investigation.
I had not realized that before. I'll dig into this in Rust. Thank you.
So the crate version is understood by Cargo, but not really by rustc -- it's not actually a part of the _code_ or the invocation. Cargo could probably warn on such things though.
Whoa, what's going on there? How does that work?
It will traverse the string twice. That's why it's not a great idea :P Edit: You could say that every pattern including two delimiters would essentially traverse the string twice, but I guess this is one of the least optimized ways, while still being sane.
Its bindings to NaCl are complete, and some extra functionality from libsodium will be added. Most of the changes lately have been to keep up with the rust releases.
Always try to keep your computing devices far from water :)
I didn't meant to offend or to imply anything. In this instance, I'm merely agreeing it's a problem and noting that you could probably raise an issue on Rust's issue tracker and/or implement a fix. 
I wouldn't go as far as calling it an anti pattern, but I agree its good to leverage types to create meaning... its' why I like adhoc overloading in C++
I'm submitting this here for a few reasons: 1. It's interesting, because it's a blog post about ownership, lifetimes, and borrowing, but not in Rust. 2. We, though the history of Rust, basically same to the same conclusions. It's cool to watch the same thought process in a totally different language. 3. This post is useful if you apply it to the Rust context. It's also important that there are some differences between C++ and Rust that make some of this very different, mostly around the performance stuff. Rust's cleaner semantics here help a lot, and you can see how with the additional complexity and gotchas described here. Another is atomics: &gt; If the object being shared does not need to be shared between threads, the overhead of an atomic is unnecessary. This is why we have both `Rc&lt;T&gt;` and `Arc&lt;T&gt;`, but thanks to `Send` and `Sync`, we can let you not have to pay for atomics when you don't need them, and still be safe. That said, I'd also like a pre-emptive shout-out to rule 4.
Cool. :)
I never realized we can do "shared_from_this" too, with `Rc&lt;T&gt;`. There is really no magic to it, just wild unsafety, just like the C++ version. [implementation of .rc_from_self()`](http://is.gd/5NJsds)
I'll update fern to use log 0.3.0, so this particular instance shouldn't be a problem in the future. However, I agree this could have a better error message!
Apparently 'next' means today's: http://users.rust-lang.org/t/psa-feature-staging-will-be-fully-activated-in-the-next-nightly/731/4?u=steveklabnik
I see, well it should probably be changed then
&gt; His proposed solution to use ID's and map lookups has it own set of problems like bad performance, how to reuse ID's etc. In fact, I'd argue that "IDs and map lookups" is just a GC, once you have automatic storage reclamation. It replaces pointer addresses with IDs, but a pointer address *is* nothing more than ID. `shared_ptr` is also a GC, incidentally.
No, probably not because c-style for loops are not really necessary in the language.
I'd prefer to remove the line completely. It unnecessarily runs a hash.
I used to teach Ruby to beginners for a job. I echo /u/DroidLogician here. What you really need as a beginner are _lots_ of resources for learning. Rust does not have many. I personally think that Ruby, Python, and JavaScript are all great first programming languages. It also depends on the beginner, though. Different people learn differently. I would encourage you to let your mother know that differnet languages are good for different people, and be willing to switch languages if it isn't working out at first.
For what purpose are you teaching your mother programming? If all this is you telling her that "everyone should learn programming" while she barely knows what the difference between a screen and computer then.. Oh I don't know. Anyway, if you want her to learn the minimum basics to get a understanding for what a program really is (bits and bytes) and how programming works (ifs and types)? Then Rust is fine, it's not like you'll run into lifetimes and borrower issues. Plus the playpen is well suited for small teachable moments. Since you can infer most types (perhaps all you will run into) the syntax is not very hard, it's just verbose enough to get a clear picture of what's going on, in my opinion. Even if you use functions you can force her to use only i32, f32 and Strings. If you want her to get more tangible experience of how she can interact with a GUI then go javascript. Rust will teach her more "about programming" rather than "how to make a program" so choose whichever one fits your purpose!
interesting article. The C++ standard libraries have often irritated me. Never really been interested in shared_ptr for the reason that it *can* place the reference count in a separate allocation - that adds another pointer.. if I've wanted reference counting, I've want to know for sure the reference count is embedded, and that the reference is one pointer.
I just realized another approach that might be a little cleaner: macro_rules! add_constraints { { $map:ident; $( $x:expr; [ $( $y:expr ),* ]; )* } =&gt; {{$({ $map.insert($x, vec![$($y),*]); })*}} } The `vec![]` macro will create an empty vector if no items are passed.
Many people think that garbage collectors with cycle detector and finalizers are the ultimate solution to all ressource related problems and clearly superior though a bit slower than the old C++ way but actually GC is a general solution just for memory management and RAII is a general solution for all ressources except in cases with cyclic references. So I think it is important to make clear that garbage collectors sometimes are part of a problem and that people don't have to feel too secure when using them.
This seems like it should work, though I'm having a little trouble getting it to compile. The empty expansion `()` causes an unexpected EOF error.
Nice! Much cleaner.
Some helpful notes: 1. If you want to reference objects from multiple locations, you'll need to use `Rc`, the reference-counting pointer. I also like your idea of storing indices into a master list! 2. You want to mutate those objects through their references, so they need to be wrapped in a `RefCell`, because `Rc` doesn't allow mutation unless it's internal mutation such as in the case of `RefCell`. 3. You can take a similar approach to have cars pointing to cars. Rust doesn't have a cycle-collecting garbage collector, so you need to be careful to not accidentally make a cycle when you start having Cars containing references to other Cars. [Here's a gist](https://gist.github.com/tikue/9621f52b086ac5f5a620) to get you off to a reasonable start. 
&gt; In fact, I'd argue that "IDs and map lookups" is just a GC, once you have automatic storage reclamation. Yes, and a really inefficient and cumbersome one.
If you use unsafe code then of course you can do anything.
But isn't that applicable to all structural types? If so, you may as well say that every structural type system is really just an anonymously nominal type system. :)
It wasn't a 100% given that the rcbox address can be derived from the object pointer, but in our implementation of rc it can.
Another solution (other than using `Rc&lt;RefCell&lt;Car&gt;&gt;` or storing indices to some list/map) is to use an [Arena](http://static.rust-lang.org/doc/master/arena/struct.TypedArena.html) (arena can be described as insert-only list). Then you can remember your cars as `&amp;RefCell&lt;Car&gt;`. You can even store the "based on" field as `Option&lt;&amp;'a RefCell&lt;Car&lt;'a&gt;&gt;&gt;`. And you don't have to put a whole `Car` in a `RefCell`. For example, you can only change type of the `value` field to `Cell&lt;f64&gt;`, change signature of `drive` to take `&amp;self` instead of `&amp;mut self` and get rid of `RefCell` indirection entirely. And remember: while `&amp;` and `&amp;mut` are most commonly used as immutable and mutable references, they truly mean **shared** and **unique** references. So `RefCell` is just a shareable wrapper, which can temporarily provide unique access (by checking uniqueness of it at runtime), so you can call `&amp;mut self` methods. Objects that can be mutated through shared reference are often described to have *interior mutability*.
If the Ngram is only going to last as long as the document String itself, it would be fine to just use `&amp;str` without Rc&lt;&gt; or any other assistant. The first time you find the reference in the String, store the slice in an Ngram, and then you can just compare it from there. Taking a slice of a string is essentially free, storing an &amp;str from a document String won't cost much at all.
Why would you do this? Is there a case where Rust's type system is failing you? The way I see it, you should ask for an `Rc&lt;T&gt;` if you need to keep references, otherwise you should ask for `&amp;T`. `Rc&lt;T&gt;` can be easily coerced to `&amp;T`, but there is *no reason* to convert it back.
Often when I have a constructor that wants 2+ args of the same type, I will consider using the builder pattern to more easily disambiguate the arguments.
Thanks, this is helpful. I'll give it a go and see how far I get.
Thanks, I'll be sure to avoid cycles :)
It feels as though a lot of this article could be similarly refuted, at least in terms of what's possible with C++ semantics.
[**@jruderman**](https://twitter.com/jruderman): &gt;[2015-03-26 00:25:29 UTC](https://twitter.com/jruderman/status/580888265642770433) &gt;The @steam\_games store looks great in [@ServoNightly](https://twitter.com/ServoNightly)! [*pic.twitter.com*](http://pbs.twimg.com/media/CA-6yboVIAAdZr7.png) [^[Imgur]](http://i.imgur.com/8ugDkUb.png) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/30blul%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
I suppose you don't refer to [this](https://github.com/vzvenyach/servo-shell) servo-shell, right?
One question (for whoever would know), could XUL be rendered using something like this? Something like a "Shumway", but for XUL.
You would need a XUL layout engine for that. I don't really want to add such a thing to Servo proper to keep the code lean and maintainable, but maybe CSS Houdini could be used to implement a XUL layout engine in JavaScript.
Is there a chance of having compatibility with (a limited subset of) Chrome extensions? Or at least Userscripts.
Both certainly ideas worth investigating.
We have userscript support (ish). None of the greasemonkey APIs or the comment based metadata; just support for dropping scripts in a directory that will be loaded everywhere. Chrome extensions are basically scripts with: - Access to better storage apis (including sync storage) - Access to tab manipulation APIs - Ability to hook into the toolbar or context menu They're otherwise pretty much javascript. Probably wouldn't be hard exposing these APIs from mozbrowser, though we'd need to wrap them to look like the Chrome APIs then.
Individual character access is an O(n) operation because of UTF-8, so I imagine that the decision to remove indexing was intended to discourage doing this operation repeatedly on a string. Instead, you should collect the `.chars()` iterator into a `Vec&lt;char&gt;`.
I thought `char_at` indexed by code unit, which is O(1) and perfectly well-defined... provided you do actually index at a valid position.
If we want to be pedantic, `char` isn't even named correctly in the first place! Damn, but text sucks. :D
If having the file contents isn't needed, I guess this is going to be better - then it would be possible to just iterate over the file as a whole, and not even store it all in memory.
Does Servo support "Zoom text only"? It's a big feature missing from Android Firefox for me.
This will only hold for another week or two, until DST support is implemented for `Rc` (there's an RFC open, I believe).
I'll take the deal
Let's just hope that before then they manage to upgrade to a version of SpiderMonkey that's not from 2012. :)
And pray there aren't any non-ascii characters in there!
Sean Parent often mentioned in his presentations that a shared pointer is as good as a global variable. That said, I find `std::shared_ptr`particularly useful when combined with copy-on-write techniques for concurrency applications.
It sounds like you could use a HashMap for this. For the slice -&gt; count mapping, I mean.
Oftentimes that's a safe assumption, especially if you're only intending your application for English speaking markets. However, user-entered text is an entirely different story and it shouldn't make your program explode if they put in ( ͡° ͜ʖ ͡°) while screwing around.
just one more thing to remember or get wrong. if I rolled my own reference counting scheme any time over the past 20+ years, I know the reference count is embedded and there's no complexity associated with an option I don't want, the desired behaviour is not buried behind abstraction layers.
Yeah, the multiple definitions of "character" makes it hard to talk about. See: http://unicode.org/faq/char_combmark.html#2 
And of course, `String.charAt` will fail gloriously if you have characters in your string outside the BMP. (MDN graciously provides a work-around, though.)
If she wants to learn, go with Python 3. Clearer, forgiving syntax, doesn't hide or elide too much, strict in some useful ways but not all, and the error tracebacks really assist learning from mistakes. JS doesn't provide useful errors and this, in my view, propagates either frustration or bad habits. Rust is the kind of language that would kill any interest in programming for a beginner, I feel; it's too strict, and too young, and many syntax elements and programming concepts are confusing to experienced programmers and utterly opaque to a beginner.
Being able to access characters in a string, *by byte offset*, in constant time, is important. `regex` relies on it and my library for fast suffix array construction relies on it *even more*. You can see that, indeed, using the iterator in combination with byte slicing is how we're supposed to do it: https://github.com/rust-lang/regex/blob/master/src/vm.rs#L396 I have my suspicions that this is slower than `char_at`, but I have yet to produce a good microbenchmark.
Thanks for the answer! I, too, considered Python, because it's used as first language at many universities and, as far as I know, it's not seen as a bad language, like Java and PHP for example (which are "popular" beginner languages, too). But I also often read and hear the statement, that the choice of language isn't really important. Firstly, of course, because one often want to teach foundations of programming and not the specific language. But also because motivated beginners will always learn something from the language they use. I know programmers who started with Assembler and others who started with python. When starting with a low level language, the student will learn more about the low level computer details, and when learning a high level language, the student will learn more about programming techniques. What I'm trying to say: If one is motivated, the language doesn't matter too much. But I see the "lack of GUI"-problem coming with languages like Rust and C++ (where "lack of GUI" of course means: GUI is more difficult). I guess everyone already heard this sentence from a beginner once: "And when can I make 'real' programs?". Seeing colorful stuff on the screen is more fun, of course. Regarding your last point: My plan included not just pure programming. I would start with some computer basics like the binary system, teaching her math here too (however, I won't teach her how to use Word or something like that). And after that my plan really was to sneak a bit math into programming. I once heard a TED speaker talking about the future of maths education. He argued that computers will play a major role in math education, because solving complex stuff by hand is rarely useful today and not motivating, too. So computers will be used to compute and visualize the "maths". And that makes sense in my head, so I will try to sneak math into programming and see, how it goes ;) Sorry for the wall of text... 
Text was never simple, a number of people in the west (especially anglo-saxon countries) simply assumed it was a trivial mapping to bytes and didn't bother going any further.
I hadn't thought of that. So then the lookups would be slightly slower, I assume due to bounds checking? But like you said, that would decouple the deallocation.
SimonSapin is correct, but if you really need an indexing-like `char_at()` operation (speaking in ergonomics, not performance) I prefer `some_str.chars().nth(0)` as it still appears like indexing but also correctly dictates that you're using an iterator. Plus, if you need to change to some other arbitrary index is an easier transition. 
You should always use the paths given by the documentation, as the docs reflect the public interface of a module. In this case, the error type is located at [`regex::Error`](http://doc.rust-lang.org/regex/regex/struct.Error.html). This is a common pattern among Rust libraries. Often, they are split up into multiple modules. But the library only exposes one single cohesive interface in the top-level module, which means you don't need to care about how the library's modules are structured. (One of my favorite parts of Rust is that this is so easy and natural to do!) [This is where `regex::parse::Error` is re-exported as `regex::Error`.](http://doc.rust-lang.org/regex/src/regex/lib.rs.html#376)
Goal: have a SpiderMonkey version that's newer than our Rust compiler version :-)
I'm hoping for a strong focus on backward compatibility post-1.0. It would suck to have breaking changes combined with strong incentives to move to a 'newer and better' version of rustc, as seems to happen too often (outside of Rust, I mean).
English text using ASCII is simple. European languages using non-English letters or accents (é, ō, ü, ç) is complicated. Languages using non-Latin letters are even harder.
It’s funny/sad how emoji is what’s driving good Unicode support in many systems, and better support for non-English languages is just a side effect.
I honestly don't see how it is with utf-8 these days.. You can simply treat all characters equally and don't have to worry about encoding. It's pretty hard to mess up, especially in rust. Unless of course you want to capitalize characters like é. That may be tricky.
I'm not sure what you're talking about, exactly. This has been the plan since http://blog.rust-lang.org/2014/10/30/Stability.html, posted last October. Beta is next week, so it's time for this to land, since we're finally gaining a non-nightly channel. &gt; Now I hear box is coming back during the beta, because were not actually done with language features after all? `box`, like other language features, is still behind a gate, and the gate will come off when it's ready. This isn't anything special. We were never 'done' with language features, new ones will be continued to be added.
This sounds a bit like some of the problems we've encountered doing zero-copy parsing in html5ever. I have [a new proposal](https://github.com/kmcallister/tendril) for a zero-copy string type that might be suitable in your use case.
I have the feeling people here are not actually trying to write efficient parsers.
Oh... then I don't care. I thought it had changed based on this comment.
We do not plan on a 2.0 anytime soon, for sure.
&gt; Oh if you think there would be no complexity in having to always embed the reference count directly into the object you would be absolutely wrong. Surely a system which can put the allocation somewhere else has more complexity. Its not something I want to do. Any more complex ownership scenario I'd probably want to build some sort of manager object which can track the thing. the article the OP linked to mentions debugging. anything I've done in anger, tracking where memory is going has been tremendously important, similarly managing locality. 
Tuples are perfectly nominal - they are just internally-implemented n-ary generic types, in the manner of struct Tuple0; struct Tuple1&lt;T0&gt;(T); struct Tuple2&lt;T0,T1&gt;(T0,T1); struct Tuple3&lt;T0,T1,T2&gt;(T0,T1,T2); 
I think we're just approaching the singularity. I'm guessing a lot of people (me included) were hoping some things would stabilize---but if they aren't by now you're probably having that "oh shit" moment where you realize you have to go update your crates. :P
`Box&lt;Trait&gt;` now defaults to requiring a static lifetime. It didn't used to, you used to always have to write one explicitly, but this is the first time I've seen anyone bitten by it. Change `Box&lt;Celestial&gt;` to `Box&lt;Celestial + 'a&gt;`.
You're right! I had totally forgotten that syntax and what it was good for. Thanks!
[I had this issue a week ago](https://github.com/SBSTP/cylonn/commit/5cc1999fd7c664310a91cf24ac998adb9515e293). From what I understand, it's that a trait could be a reference, so it needs a lifetime. The compiler used to add `'static`, but it doesn't anymore. So you need to add `'static` manually or assign your own lifetime.
Ah yes, I re-read my answer and see what it looks like I was saying. I meant ergonomically similar to indexing, not similar in performance. Thanks for pointing the difference :) Although, when it truly is the first character you're trying to obtain what is the real difference between O(1) and O(n) as the size seems irrelevant, no? (I'm asking for real, not being sarcastic :) ) 
Nice job on the macro! However, Rust already has the ability to do this with its [functional update syntax](https://doc.rust-lang.org/reference.html#structure-expressions), so you don't even need the macro: #[derive(Debug, Clone)] struct Point { x: f32, y: f32 } fn main() { let a = Point { x: 1.0, y: 1.0 }; let b = Point { y: 2.0, ..a }; let c = Point { x: 3.0, y: -4.0, ..b }; println!("{:?}", a); println!("{:?}", b); println!("{:?}", c); } 
Yeah, it doesn't look like it's mentioned in [the book](http://doc.rust-lang.org/book/), but it is mentioned in [the reference](https://doc.rust-lang.org/reference.html) at least.
Cool, I was sort of wondering how that `..Default::default()` trick worked.
&gt; Although, when it truly is the first character you're trying to obtain what is the real difference between O(1) and O(n) as the size seems irrelevant, no? (I'm asking for real, not being sarcastic :) ) Probably negligible. There could be some constant factor overhead from invoking an iterator for the `O(n)` way, but I have no idea whether that exists or not. I'd benchmark it first and/or look at assembly output to confirm. :-) But yeah... Sorry about the confusion! I re-read the OP. OP starts with `char_at` but does indeed go on to say that they want only the first char!
I don't think that makes them nominal, it just shows that there is an encoding from structural types to nominal ones. Tuple structs are nominal because you can give them names and this affects subtyping. E.g., if you have `struct Foo(i32)` and `struct Bar(i32)`, they are not subtypes. But tuples are purely structural because if you have `(i32)` and `(i32)` they are always subtypes - there are no names here so it is impossible to have nominal subtyping. Alternatively, to check if two tuples are subtypes, you look only at the structure of the types, you don't look at names of the types. As an interesting aside, generics are a kind of structural subtyping on top of a nominal system (which is why your encoding works). The variance rules specify exactly which structural subtyping rules apply. See http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.9795&amp;rep=rep1&amp;type=pdf for some discussion of the idea
The beta is increasingly looking like it will continue to see *significant breaking changes*. That's not a beta, that's an alpha. Seriously, are we supposed to use From (unstable, experimental) now? Where the heck did PathBuf::new go, and what are we supposed to use instead? What happened to 'all the apis we expect in 1.0 should be stable now'? Is old_io ever going to go away? Are timers in new io ever going to land? Im just saying, for a 'stability abd bugfix only' beta, things seem completely all over the place at the moment. Maybe you guys need a bit more time, instead of rushing all this stuff out, untested, into the beta.
Sweet!
But... I can't wait for another three days for beta. D:
This code work for me: let x4 = &amp;[1f32, 3f32, 9f32]; let y4 = &amp;[1f32, 10.0, 100.0]; let mut fg = Figure::new(); fg.axes2d() .set_title("Semi-log", &amp;[]) .lines(x4.iter(), y4.iter(), &amp;[]) .set_x_ticks(Some((Auto, 1)), &amp;[], &amp;[]) .set_y_ticks(Some((Auto, 1)), &amp;[], &amp;[]) .set_y_log(Some(10.0)); fg.show(); Make sure your actual data is positive along the axis that you want to be logarithmic.
You mean the cruel and unusual date system where we don't just say "42 days after day 51"
I heard the contrary. That is, by 1.0 it's going to prove P != NP
Nice work!
My guess you're doing something like assert_eq!(something.unwrap(), some_value) and it's the unwrap that fails, not the assertion. As nwin_ said running your program like RUST_BACKTRACE=1 ./my_program will give you a nice stack trace of any panic. When I'm debugging this is what I run cargo build &amp;&amp; RUST_LOG=debug RUST_BACKTRACE=1 target/debug/my_program
Sounds like a demo waiting to happen.
In that particular case, you probably want to do `assert_eq!(something, Some(some_value));` instead to make sure that this is really the issue.
&gt; Sean Parent often mentioned in his presentations that a shared pointer is as good as a global variable. Yes. Because the problematic aspect of a "global variable" isn't the globally-visible scope (our `const`s are harmless, for example), but the *globally observable mutation*. Anywhere in the program can mutate it, and anywhere else in the program can observe it. `shared_ptr`s, if they allow mutation of the contents, are indeed very similar in this respect. &gt; That said, I find `std::shared_ptr` particularly useful when combined with copy-on-write techniques for concurrency applications. ...which is much more benign precisely because it gets rid of the globally observable mutation. :) If anyone wants to modify it, they get their own copy.
There have been a lot of misconceptions these last days on IRC. For anyone bringing this up, [package names can still have hyphens in their name](https://github.com/rust-lang/rfcs/blob/master/text/0940-hyphens-considered-harmful.md#disallow-hyphens-in-crates-only) and the `-sys` convention hasn't changed.
Wolfram|Alpha lets you convert from the C&amp;U date system and back with ease. ["42 days after day 51"](http://www.wolframalpha.com/input/?i=42+days+after+day+51&amp;dataset=)
stats_print is available in the docs -- [see here](http://doc.rust-lang.org/std/rt/heap/index.html). I think rust can also be compiled with replaceable (dylinked) malloc, but that's not the default.
Agreed. This has bit me more than once, too. It's a confusing issue for new library authors.
You may be interested in the heap profiling code that Servo has recently gained: - http://mxr.mozilla.org/servo/source/components/util/mem.rs - low-level definitions of jemalloc and heap-size reporting traits for types. - http://mxr.mozilla.org/servo/source/components/profile/mem.rs - a profiler that reports various jemalloc measurements and the results from other threads - http://mxr.mozilla.org/servo/source/components/gfx/display_list/mod.rs#200 - an example of a type reporting its heap memory
&gt; There is no reason to capitalize heap :) I bet they are because of http://doc.rust-lang.org/nightly/std/boxed/static.HEAP.html
Also, it’s unclear whether unpaired surrogates need to be preserved in the DOM.
Unfortunately yes, this functionality has been removed with no replacement. The rationale behind this decision is motivated by the old implementation not being up to par with the current style of the standard library. For example, to implement `close_accept`, every single `TcpAcceptor` created would actually create *two* file descriptors (the socket plus an anonymous pipe to select on). This functionality is inherently tied to functions like `select`, which we will not have bound in the 1.0 time frame. For now I'd recommend using [mio](https://github.com/carllerche/mio) for serious applications, and for some smaller things like tests I've been manually sending a connection to an acceptor and then the acceptor detects that it's supposed to shut down.
To be clear, we definitely want to bind functions like `select` in the standard library! For now we're not willing to commit to a stable interface one way or another, but this should come soon after 1.0. Right now we've been recommending `mio` because it offers much of the functionality one might want on unix platforms, but you're definitely right in that it doesn't have full Windows support just yet.
I would like to see an emphasis on cross-platform debugging tools. Don't forget about us Windows devs :)
I feel like I need to do a bunch of studying to get a grasp here... Can you give an example of how this might be used?
In theory, I could add windows support to mio, the problem is that it would be second class in windows. To write high performance IO on windows, CPIO must be used (which is completion based). Aka, a completely different API is needed. Aka, to get the last drop of performance, you have to write code twice anyway. Once for readiness systems and once for windows (and other completion based systems). However, if you are willing to accept a little bit of overhead, it is possible to write libraries that work on both windows and *nix. For example, libuv does this. I am also working on a higher level rust library that should be able to support all platforms equally, but it is still pretty early on.
That's what it's for? I assumed something related to bookmarks. Thanks
If I'm reading your code right, you can't (ab)use generics like this. Essentially you're creating a function like this: fn test&lt;T&gt;() -&gt; T { "hello!" } "hello!" can only be a `&amp;'static str`, you aren't actually generic over anything, even if you add bounds to T. ([Play with the playpen](http://is.gd/Z7tHBp) and see why.) There's actually no way in rust to express what you want to do (return a closure). Someday you will be able to, using "anonymous" return types like this: fn test() -&gt; impl Fn(usize) -&gt; usize { |x| x } **In the mean time**, you will have to use trait objects, which you can return if they're boxed. fn test() -&gt; Box&lt;Fn(usize) -&gt; usize&gt; { Box::new(|x| x) } **edit**: By the way, you can impl `Fn(NaiveDate) -&gt; NaiveDate` on your own custom struct, and not use a closure at all, which actually seems acceptable with a cursory glance at your code. Then you can return the struct instead of a boxed trait object.
I actually feel that debuggability is not a priority. Many classes of bugs which require a debugger to squash are already covered by the compiler. Of course, debuggers have uses beside chasing segfaults. It's just that I don't think they're as vital for Rust as they are for C.
Do you have any objections to people modifying your code and not making those modifications available as open source? If you don't care about that sort of thing, then MIT is a fine license.
Ah I suppose gdb might not be standard on mac installations any more, though I do use a version from homebrew.
The whole "to underscore or not to underscore" thing is such a fiasco right now. I'm in the "use underscores everywhere" camp because I'm a fan of consistency; requiring "llvm-sys" and importing "llvm_sys" is silly.
GBGamer117 is right, that's how the installer will set things up. That being said, `rust-gdb` should work just fine if the GDB you are using is new enough and was compiled with Python-support. If both `rustc` and `gdb` are in your PATH, you can try running the `rust-gdb` file located in `src/etc` (or copy it off of Github if you don't have the Rust source code checkout out).
I wouldn't say that. I'm doing quite a bit Java programming where segfaults can't occur either and having an IDE with proper debugger support is an incredible boon to my productivity. Many programming errors are simply logic errors and no compiler can catch those for you.
I'd be happy to hear constructive criticism on the pretty printers. They have been used for quite some time in the debuginfo auto tests now, but they certainly haven't seen a lot of polish.
You can use the if let syntax if let Foo::Bar(ref mut wrapped_value) = foo { *wrapped_value = 15; }
Yeah, so in this case it fails because the caller is not granted the opportunity to actually provide the type. At least that was the understanding I developed.
Nice! I have just started looking at the rust bindings to libjit, they also look good. If anybody has advice on libjit vs. llvm please comment. The usage would be for jit in my expermint with a simple scripting language, intetion is to be embedded in user app.
Another thing that was surprising is that std::mem::size_of::&lt;*mut Error&gt; is double std::mem::size_of::&lt;*mut Foo&gt; where Foo is not a trait. I guess this makes sense in that "trait objects are fat pointers", but it's very weird coming from C.
This particular error type *will* be fixed for things like this in rust 1.0.0, as an in-rustc version of log and other crates like rand won't exist, and will instead give an error about not finding the log crate. This would still happen for crates depending indirectly in different versions of crates, like depending on the hit version of a crate while another dependency depends on the released version.
Is a macro necessary for this? I think it would be better if you create a function like `arc_mutex_new`, or if it's ever supported, `ArcMutex::new`. I don't know how others feel about this, but I only use macros when I cannot accomplish what I want to do with a function.
Currently librustc/libsyntax does not provide a simple api to get things like types easily. However workarounds exist. If you have any ideas about that you can contribute them. There is also [RACER](https://github.com/phildawes/racer)
I was trying to get `ArcMutex::new`, but couldn't figure out how to write a generic impl for the type alias. Couldn't find docs on how to do it either.
you don't even have to use foreign words, if you start using typographically correct quotes etc. the problem starts already. These are not even included in latin1 because somebody found it wise to include &lt;&lt; and &gt;&gt; instead which are so commonly used that I can't even enter them on my iPad.
I don't think that it is possible to add generic impls for a type alias. I agree with SBSTP though, a macro for this is pretty overkill. Function would be better.
Oh, well sure. I didn't think adding a trait and then implementing some methods for a concrete type was what kurotetsuka had in mind.
Compiler cannot help you with autocompletion because it requires to parse incomplete ill-formed source and Rust compiler will abort compilation early with error. Maybe this will change in the future but it is not a priority now. Compiler consists of several crates, https://doc.rust-lang.org/std/ (see `rustc*` and `libsyntax` crates). Their interface isn't the greatest, is unstable and probably will change in the future. Documentation for internals is mostly missing so you will need to look at the source to undesrstand how it works. There are some articles/tutorials in the internet about how to create your own tools using rustc.
Actually, that's the easy part - that is, running all passes up to and including type-checking and then visiting all expression nodes in the AST in order to record the span and type for each of them. But that's not enough for an IDE. You need the ability to parse incomplete code (does "being able to handle modifications if you started with syntactically valid code" count? it's a bit easier). You also need to control type-checking and handle partial cases gracefully, even extract information for autocompletion. `librustc` (and associated crates) could eventually do all of that without fundamental changes *across* passes. However, what I fear most is the reusability of compilation state. I believe such an IDE could be considered a success only if it works on the compiler itself - the crates are so big, type-checking takes way too long for real-time editing. You would naturally want to preserve most of the AST and reparse a small subset, each time. Even with the current immutable AST model, this is quite feasible, if you **own** it (in-place "linear" `fmap`). But to create a "type context", which you require in order to run type-checking, you must give up ownership of the AST by borrowing it immutably. And if you give up that "type context" to regain control over the AST, you lose all the type information you spent time generating. There is another way, though: keep adding AST nodes to the "AST forest" in the "type context". The functionality is there because we use it for inlining constants and generic functions from other crates. You can probably get decent timings for redoing all passes from parsing to type checking (on a single function) that way. The catch? Continuous memory growth, as you can't evict the old AST. I wonder if there could be an "indexing" pass turning immutable ASTs into shared mutable "flat data" that type-checking can add types to, instead of the current "assign unique IDs to AST nodes" pass. cc /u/nikomatsakis /u/Aatch
It is possible to use [cv2pdb](https://github.com/rainers/cv2pdb) to convert the existing DWARF debuginfo of Rust programs to PDB. It's not ideal but it works.
Or you can simply do `foo = Foo::Bar(15)`.
Sounds like an ideal case for tracing GC, no? Or would the old data still need to be referenced somewhere?
Except that *any* form of GC will slow compilation down, whereas the current linear-like `fmap` can be potentially optimized to mutating only the logically modified fields/substructures. And I'm not sure we should use a single blessed structure everywhere - or even a recursive structure at all.
This is very cool to have as a ready-to-use Rust library! :D Might finally tempt me to experiment with this stuff.
Alternatively, you could define a newtype `struct ArcMutex&lt;T&gt;(Arc&lt;Mutex&lt;T&gt;&gt;);` and then have it auto-deref to the inner value.
While this might be a nice convenience for some cases, I actually disagree with the premise. Ever since `std::thread::scoped` was introduced a `Mutex` can often be used without an `Arc`. Notably any fork-join style parallelism should be expressible without `Arc`. The book might need to be updated to better convey this. I'm under the impression that people are currently overusing `Arc`/underusing `std::thread::scoped`.
Is there any way to access the innards of a tuple-variant or a tuple-struct? `.0` doesn't work. Aside from that, this is really awesome! :)
The only reason that doesn't do what I want is in cases where multiple values are wrapped: enum Foo { Bar((u32, i32, bool)), Baz { some_value: bool, other_value: f32 } } Especially in the case of the struct-variant, it would be nice to be able to create an enum of type `Foo::Baz` and then directly change `other_value` without changing `some_value as if it were a normal struct.
Oh I know (although, use `as_ptr`), I was just saying it weirded me out when I first found out.
&gt; Every time you change a feature gate, change and api or depreciate an item, it's a breaking change. Well, changing a gate is, but depreciating something isn't breaking. You can still use it. Adding new things is not breaking. &gt; The point I'm making is that you're dropping these significant api changes extremely close to the beta release date. Okay. This is very different than "there will be significant breaking changes after beta." We have been doing a lot of breaking changes lately, specifically so that after the beta, we don't have to be doing them anymore. &gt; Feature gates are a useful thing, but adding or removing them is a breaking change. Adding a new one is very much not, and removing one is not, as it's additive. Gating an existing feature is. 
I assumed all the same binary are installed across all cluster nodes. I thought that Rust uses LLVM internally and probably can makes use of LLVM IR to serialize and deserialize lambda expression. But, it may be a silly idea because I don't know Rust internal.
Thanks for the pointer to TraitObject. I hadn't seen that before. Yeah, the double-boxing idea had crossed my mind. That would let me have the more natural C API where errors are just another opaque struct. I'll go with it. Thanks!
You can understand a single artificial neuron (a [perceptron](https://en.wikipedia.org/wiki/Perceptron)) as a [linear classifier](https://en.wikipedia.org/wiki/Linear_classifier). The configuration parameters of the neuron are the synaptic weights `w`, and the bias `b`. When the neuron receives an input array `x`, it calculates the function `f(x) = x · w + b`, where `·` is the dot product. In the 2D case (where x and w are vectors with 2 values), f defines a line in the plane, classifying it in two semiplanes: one where f is positive, and another where f is negative. This is equivalent to passing f to the *activation function* `φ(x) = 1 if x &gt; 0, 0 otherwise`, and checking whether it is 1 or 0. You can have many perceptrons running in parallel (a *layer* of perceptrons) receiving the same input but each having different synaptic weights and bias. That way, you can classify more complex regions of the plane, [like this](http://www.byclb.com/TR/Tutorials/neural_networks/ch8_1_dosyalar/image069.gif) (taken from [here](http://www.byclb.com/TR/Tutorials/neural_networks/ch8_1.htm)). But those regions would still need to be bounded by linear functions. This crate uses a network called MLP or [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron). It consists in many layers of perceptrons: one that receives the user input (the `x` above) which feeds its output to the next layer until reaching the last layer, that gives the output of the MLP. It looks [like this](http://www.jpathinformatics.org/articles/2014/5/1/images/JPatholInform_2014_5_1_9_129442_f7.jpg) where each circle is a perceptron. The MLP can classify regions that aren't linearly separable. See [this image](http://www.vias.org/tmdatanaleng/img/hl_classif_separation.png): a single perceptron can separate the regions in the first case (with just a line) but in the second case there is no line dividing the two regions. The perceptron learns by iterating examples, where you give them a *training set* of vectors `x` and how they should be classified, and you calculate the error (their output minus the expected output), and update the synaptic weights following the algorithm defined [here](https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm) (this is an example of *supervised learning*). The MLP uses a variant called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation), but it only has (directly) the error for the output layer - it propagates the error to preceding layers through the derivative of the error. PS: I can recommend Simon Haykin's *Neural Networks and Learning Machines*.
Good idea on the junk directory. It's so simple, yet eventually so necessary. 
I have a `~/src` and `~/tmp` directories, and I keep everything in either one of those.
Unless your parser is ridiculously slow, you do not actually need an incremental parser on semi-modern hardware. Re-parsing 5k lines every second is just fine when your parser does 200 kLOC/s. (source: I implemented the C# code completion in SharpDevelop). What you need in an IDE is: 1. The ability to re-parse one file without having to re-parse a ton of header files -- this is trivial if the parser doesn't look at any symbol tables. 2. The ability to update the symbol tables, replacing one file with an updated copy of the file. Using immutable persistent data structures is optional here -- it's nice because it allows the IDE to consume the symbols without having to worry about another thread updating them, but by no means necessary. SharpDevelop used in-place mutations + locking of the type system up to version 4.x; we only had to switch to immutable persistent data structures when adding background semantic analysis (running lints on the current function while the user is editing it). Note: the 'symbol tables' here are really just the AST of each file, minus any function bodies. 3. The ability to lazily resolve just the code constructs necessary for semantic analysis of the current line/function/screen/file. So for example, when doing code completion, this means doing type inference for the current function only, and resolving the type references appearing in the signatures of the functions that are used by the current function. This is usually just a small portion of all function signatures, so it runs fast enough that you can afford to throw all this information away on every edit, which means you don't need any complex update logic ("the first parameter of module1::function() has type module2::X but only if the re-export in module3 doesn't change"). Also, ASTs can take quite a bit of memory. A good AST for an IDE contains enough information to reconstruct the exact original source code, including the location of every single token. For large projects (remember: an IDE will be used to edit multiple crates at a time), the ASTs can take hundreds of megabytes, so it's a bad idea to hold that all in memory. I think it's a good idea to only keep the AST of the current file in memory, and only hold the 'unresolved type system' (=description of available types/functions = AST without function bodies, comments, exact locations, ...) for all other files. (or did this change? I only had 512MB of RAM when I started working on SharpDevelop...) You should be able to reconstruct the full symbol tables from the 'unresolved type system' alone. By doing this lazily (only for types/functions that are actually accessed), you can be fast enough to do this on every edit. The downside of this approach is that you can't rely on compiler phases running one after the other -- the work is being done lazily, and it's possible that a later stage such as type inference within a function triggers lazy actions that find compiler errors that normally would have aborted compilation in an earlier stage (e.g. unresolved types, cyclic inheritance, ...).
I don't suppose you'd mind giving an example? How do you pass the mutex to multiple scoped theads without an `Arc`?
I like this idea - I didn't know you could make structs like that (like the algebraic data types). I was thinking of doing something like `struct ArcMutex&lt;T&gt; { inner: Arc&lt;Mutex&lt;T&gt;&gt; }` then auto-dereff-ing, but figured the type alias would be simpler. I might switch to your way though.
You could even archive old projects by year if you were so inclined.
At the moment I don't have the time for an in-depth answer, but I have to say: thanks for this, seems really useful real-world information! Reparsing just one file won't be easier than parsing just one function, AFAIK, with the current `libsyntax`. I am not sure I have considered simply not running intra-function type inference until it is actually necessary, this seems like it could cut away most of the time spent type-checking. Here are the timings (in seconds) for `librustc`: time: 1.776 parsing time: 0.000 recursion limit time: 0.010 gated macro checking time: 0.127 configuration 1 time: 0.000 crate injection time: 0.461 macro loading time: 0.000 plugin loading time: 0.000 plugin registration time: 0.950 expansion time: 0.029 complete gated feature checking 1 time: 0.213 configuration 2 time: 0.112 maybe building test harness time: 0.097 prelude injection time: 0.012 checking that all macro invocations are gone time: 0.029 complete gated feature checking 2 time: 0.114 assigning node ids and indexing ast time: 1.000 external crate/lib resolution time: 0.089 language item collection time: 1.316 resolution time: 0.021 lifetime resolution time: 0.000 looking for entry point time: 0.012 looking for plugin registrar time: 0.158 region resolution time: 0.012 loop checking time: 0.013 static item recursion checking time: 0.157 type collecting time: 0.028 variance inference time: 7.297 coherence checking time: 24.877 type checking The few `resolution` passes combined and `type collecting` are enough to run type-checking (i.e. inference) on any function at will.
Try: let mut x = Foo::Bar((1, 2, true)); if let Foo::Bar((_, ref mut y, _)) = x { *y = 42; } [Play!](http://is.gd/L0YUTk)
I suppose "You just do" is a bit unsatisfactory as an answer, so here is the `std::thread::scoped` version of the [example from the book](http://doc.rust-lang.org/nightly/book/concurrency.html#safe-shared-mutable-state): use std::sync::Mutex; use std::thread; fn main() { let data = Mutex::new(vec![1u32, 2, 3]); let data_ref = &amp;data; let _guards: Vec&lt;_&gt; = (0..2).map(|i| { thread::scoped(move || { let mut data = data_ref.lock().unwrap(); data[i] += 1; }) }).collect(); } The key point here is that you don't move the `Mutex` itself into the closure, but only a reference to it. This is safe as the referenced data lives at least as long as the thread executes. The guards ensure the threads finish execution, once they go out of scope. Actually this example is still bad, not only because the 3rd element is never actually accessed, but also because you can do this without a mutex: use std::thread; fn main() { let mut data = [1u32, 2, 3]; let _: Vec&lt;_&gt; = data.iter_mut().take(2).map(|x| { thread::scoped(move || { *x += 1; }) }).collect(); }
As for your other question: is it possible to do this without a branch? So far as I can tell, there isn't, without serious likelihood of breakage. You'd be hard pressed to do such a thing, even in C. I suggest that perhaps if you are absolutely certain that you're going to be passed a `Bar`, you should modify your function signature to accept `(u32, i32, bool)` instead of `Foo`. That'll push the branch further up the stack, possibly collapsing the many branches you're concerned about into one. And if you can't find a way to structure your program such that you can prove that a `Baz` can *never* appear, perhaps expending a branch to `panic` instead of silently breaking is a good thing. 
&gt; they're both file descriptors in the end. Not on Windows...
Fair enough.
According to Murphy's law, best way to realize you forgot to add something to a language is to ship it.
I agree with the OP. One point I'm slightly concerned with is unstable features after 1.0. Correct me if I'm wrong, but crates utilizing unstable features (even with `#![feature()]` will not compile on the stable branch of Rust? I'm wondering if there is going to be some kind of notice on crates.io whether a crate is using unstable features or not. As it seems like it could fracture the language between those that use nightlies (or a dev branch) and those that use stable. Just curious.
wow, really informative. nice!
&gt; I didn't realise they were still trying to shove features in before 1.0... I don't think they are. But as a community there is less direction to it; I'm trying to direct the community here. The core team members pretty much know what they're doing :) &gt; Mozilla's 6 week turnover is fine for a browser but this is a programming language and a slower release schedule will probably be better. With feature staging I don't think it's a major problem. Features can be landed into the compiler as unstable and can skip a few cycles to stabilize if need be.
This is why I think Rust ought to, at some point, incorporate a concept of "purity". When higher kinded types come to Rust (and they should, at some point) then the set of easily written higher order functions will explode. At that point, side effects become vastly more important to understand in sequencing for two reasons: 1. Some functions can be optimized using rules that could be provided as an optimization pass, in the same manner as GHC's `rules` pragmas. (These can yield enormous speedups!) 2. These rules can only be applied when pure functions are involved. 3. At some point it will probably be important for large Rust programs to support modular programming, and in very large projects, allowing submodules to access the network, do IO, and so on could be considered unsafe. 4. Certain types of systems can only be safely interacted with in pure code. Many systems in Haskell are just not safe at all unless you can guarantee purity. Of these, the most important I can think of are `Cont` and `STM`. I'm not sure how this *ought* to be implemented in Rust, because it seems there are several ways forward, with more or less involvement from the compiler. Some have suggested that this feature should be implemented as a lint pass, but I think that requires end-users to adhere to an honor policy, and it doesn't seem obvious that it would be transitive. (e.g.: If I download crate `x` which depends on crate `y`, would a lint pass for purity apply recursively?) It also seems prone to breaking, as `std` changes or the surface area of the Rust compiler would need to be tracked very closely. For an example of one area where Haskell's type system, higher kinded types, and the `rules` optimization pass producing huge gains in performance, see these papers: [Exploiting vector instructions with generalized stream fusion](http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/haskell-beats-C.pdf) [Playing by the rules: rewriting as a practical optimisation technique in GHC](https://www.haskell.org/haskell-symposium/2001/2001-62.pdf#page=209) For a case where higher kinded types and purity allow novel methods of parallelism with high performance and strong guarantees, see: [Lattice-based Data Structures for Deterministic Parallelism](http://www.cs.indiana.edu/~lkuper/papers/lvars-fhpc13.pdf) In general, there are many models of computation that will remain out of reach to Rust users without a notion of purity. Memory safety is not sufficient, so as the compiler team looks to post-1.0, I believe purity and higher kinded types should go hand in hand.
I think that Rust is not, and should not be, a functional programming language - it does not need every single feature from Haskell to be useful in its domain. If a developer wants every single feature from Haskell, why not use Haskell? I am already worried that the introduction of HKTs will result in commonly used libraries that implement half a dozen layers of abstraction, none of which actually deal with the problem I'm having, as is the case with some of Haskell - although I can see that HKTs are useful enough despite that. EDIT: Yes, possibly, a new language is needed that is Haskell-ish without a GC. Does Rust need to be that language? "Haskell without a GC" has never been a design goal for Rust, as far as I can tell.
I wouldn't call it new features. The main big changes happening at the moment all seem to be filling in missing functionality for already-existing features (conversion traits, operator changes, associated constants, etc.)
&gt; If a developer wants every single feature from Haskell, why not use Haskell? Because that developer can't afford a garbage collector but still needs to deal with efficient in-place mutable data structures without segfaulting?
One issue is that a closure can contain references to pretty much any object that's in scope when it's created. This includes stuff like open files, large buffers, sockets, and other stuff that's tricky to serialize. Come to think of it, I don't know how Python handles that sort of stuff, but it's probably stuff you can only get away with in a dynamically typed, interpreted language.
I'm already quite afraid of what will happen post 1.0 I'm a library developer, so what happens if I want to have a library that is usable from 1.0, but also has a branch for nightly features? "if you are using nightly, use cargo version 2.0, if you are using 1.0, use cargo version 1.0" will **not** scale, and it'll make it super hard for anyone trying to learn rust.
Personally, I believe in Rust as "the new C/C++". I like the functional language stuff, but only when it is *secondary* to Rust as a safe systems language.
It's not quite that binary: a crate may optionally use unstable features for optimisations (e.g. SIMD or inline assembly) or for extra features/modes (e.g. running with `no_std` to be usable for low-level development).
&gt; (Take C for instance which has had what - 6 major versions in 43 years?) It's worth noting that each Rust release will be much much smaller (individually, at least) than any of the revisions C has undergone.
I personally expect the six-week schedule to lengthen over time as the language becomes more complete. As it is Rust will continue to evolve at a rapid pace post-1.0 (just without the luxury of breaking things), but that level of rapidity won't last forever. I give it a year before it's on an eight-week schedule, then another year until releases are tri-annual, and then another year until it settles into a biannual release schedule.
I can see how someone might look at a library like `lens` and be a bit, well, overwhelmed. That's natural, because the types involved in libraries like that are complicated and a lot of the work is done by the library writer to ensure things work out to provide useful abstractions. But `STM` is remarkably simple library in Haskell. It's so simple, you wonder, "How the heck could this *work*?" Well, unless you `unsafe`ly do something, it works because Haskell keeps you honest and doesn't let you break its assumptions. Here's STM: http://git.haskell.org/packages/stm.git/blob_plain/HEAD:/Control/Sequential/STM.hs And with that and a handful of (small) libraries, one can compose all sorts of wonderful abstractions over STM that are entirely safe and and atomic. Consider the canonical example of a concurrency problem, a bank with many accounts and the need to transfer money from one to another. You could write it using locks, but in a system with *many* accounts, locks impose substantial constraints on throughput and require the designer to be cognizant of readers and writers. Rust will keep two threads from claiming ownership to the same value. This is good, because it at least means you won't have memory races. However, it isn't, in general, possible to compose primitives like a `transfer` function. That is, if you write a function `transfer` in Rust, you can't then use that `transfer` function in another function that might attempt to acquire locks on the same variables. That is, Rust can prevent you from making simple errors with a memory race, but it can't prevent you from making a data race from *two separate functions* that both attempt to acquire locks. All lock handling has to be explicit, and you can't automate as one would with memory and the borrow checker. Let's say we wanted to write a function `pay_bill_for` that would take three `Account`s and a dollar amount. For Enterprise Business Reasons, we would like the ledger to register a transfer of funds from account A to B, then B to C, and guarantee that we never credit an account to below $0. In Haskell, `payBillFor :: Account -&gt; Account -&gt; Account -&gt; Int -&gt; STM ()`, and using a simple definition of `transfer`, we could write: payBillFor :: Account -&gt; Account -&gt; Account -&gt; Int -&gt; STM () payBillFor payor intermediary payee amount = do transfer payor intermediary amount transfer intermediary payee amount Here's the thing. ***This is safe***. No locks have to be explicitly taken or re-ordered in order for this to work. However, let's consider some Rust from [a bank example](https://gist.github.com/AaronFriel/56ed5ff3ef235d2d286a): fn transfer(payor: &amp;Account, payee: &amp;Account, amount: Money) -&gt; BankResult&lt;Money&gt; { let mut payor_inner = payor.lock().unwrap(); if payor_inner.balance - amount &lt; 0 { Err(BankError::NegativeBalance) } else { let mut payee_inner = payee.lock().unwrap(); payor_inner.balance -= amount; payee_inner.balance += amount; Ok(payor_inner.balance) } } fn pay_bill_for(payor: &amp;Account, intermediary: &amp;Account, payee: &amp;Account, amount: Money) -&gt; BankResult&lt;Money&gt; { let payor_balance = try!(transfer(payor, intermediary, amount)); match transfer(intermediary, payee, amount) { Ok(_) =&gt; Ok(payor_balance), Err(err) =&gt; { // It's not actually possible to get here in this example, but for // the sake of completeness, the rollback should restore the payor's // balance. println!("Error: {:?}", err); try!(transfer(intermediary, payor, amount)); Err(err) } } } This code has a data race, because while transfer is atomic, composing two transfers *is not*. And there is no way to make it so. And some Haskell from [an identically written example](https://gist.github.com/AaronFriel/1051fdc640cde826c457): transfer :: Account -&gt; Account -&gt; Int -&gt; STM Money transfer payor payee amount = do payorInner &lt;- readTVar payor if (accountBalance payorInner &lt; 0) then retry else do payeeInner &lt;- readTVar payee writeTVar payor (payorInner { accountBalance = (accountBalance payorInner) - amount } ) writeTVar payee (payeeInner { accountBalance = (accountBalance payeeInner) + amount } ) return $ (accountBalance payorInner) - amount payBillFor :: Account -&gt; Account -&gt; Account -&gt; Int -&gt; STM Money payBillFor payor intermediary payee amount = do transfer payor intermediary amount transfer intermediary payee amount -- Note that no error handling or rollback code is necessary. This has no such race. Transfer can be *composed* as part of other transactions, and chained together with the `STM` type. I think in the language of Haskell users, it would be *in the STM monad*. For reference, here is the result of running the Rust example: &gt; cargo run Running `target\debug\bank-rs.exe` Result from thread_a: Ok(1000) Result from thread_b: Err(NegativeBalance) And here is the result of running the Haskell example: &gt; cabal run Preprocessing executable 'bank-hs' for bank-hs-0.1.0.0... Running bank-hs... Result from threadA 1000 Result from threadB 1000 Rust lacks composable concurrency. It isn't possible to give it that power with higher kinded types alone, as STM is wholly unsafe without purity.
I'm starting to wonder if the better choice would've been not to aim for Rust 1.0, but only Rust-lang 1.0 at first. A stronger separation between language and ecosystem. That would've allowed for the language to be completely frozen at a certain date, to be followed by cleanup and polish of rustc, the docs, etc. during the period after Rust-lang 1.0, with the software itself still being 0.x. The difference is that that would've allowed for the Rust ecosystem to have some additional time before users would look at the version number and assume everything to be in order - like the assumption that users made when KDE 4.0 was released, and reality was... Well, there was a bit of a disconnect there. Contrast this with the status quo, where the language is still unstable. But this may just be hindsight at this point. I only just now realized the usefulness of this approach. =/
As a systems language, I think its important to have a clear mapping from source to assembly generated. As soon as you introduce purity and have these types of optimizations, you lose that. Edit: upon further thought. I think purity would be OK as an opt in feature. Maybe as a function annotation or something. Thanks to the comments below
Perhaps there should be a field in Cargo.toml that states the package depend on certain language features..
Well, I'm sure that the stabilization process has had at least some impact on the language. So if you only stabilize the language without the ecosystem, you run the very real risk of neglecting an important use case.
I'm expecting that'll we develop useful tooling for this, e.g. maybe a minimum-language-version field in a Cargo.toml (or in the code itself), and which might be filled in with things like `nightly-2015-08-01` or `1.2` (all completely hypothetical), or maybe some other ways to ensure that users get the most useful error message.
Wouldn’t it be easier to use Cargo’s `--feature` for this kind of thing?
GHC is no longer a pure research compiler, much to the chagrin of its developers. They have been too successful.
3 points: - 1.0 will be backward compatible, unless a soundness issue/horrid bug is discovered in which case the team/community may decide to break backward compatibility to allow the fix - A lot of effort went in pre-1.0 to ensure that most envisioned features could be developed in a backward compatible manner, this includes HKT for example, to the point that I am not aware of a single feature that would require breaking backward compatibility today - If a new feature comes in that requires breaking backward compatibility, then it will have to be developed on a separate branch, as specified by SemVer
But do you really have a clear mapping? If you look at the sheer number of optimizations implemented in LLVM, you are going to fawn over. And then there is the fact that even assembly nowadays is lying through its teeth ([x86 is a high-level languages](http://blog.erratasec.com/2015/03/x86-is-high-level-language.html)). I think that what matters is that you need a clear *upper-bound of costs*, but unless you write in assembly and know that the CPU is NOT going to rewrite the instructions for you, you are unlikely to have a lower-bound...
I would expect it to; its design was reviewed with backward-compatibility in mind certainly.
To be fair, it will be a binary yes/no for many features; alloc, core, std_misc, path_ext, unsafe_destructor, fs_walk, unicode... Those aren't really optimizations; they're core apis; either you use the standard ones, roll your own low level (ie. libc binding) implementation, or you don't offer the functionality at all. I agree the parent comment; there are probably going to be a large number of crates that just never get ported (heck, they're already abandoned). We should probably `cargo test` each crate periodically, and remove it from the public listing if it fails for the latest stable. 
To be fair, this is not a Java bug. The described problem is, that "child" depends on some pointer being valid which is invalidated during the destruction of "this". The undefined order of execution of Java finalizers now triggers a bug in the JNI code. Instead of inserting some bogus function call which again exploits undefined behavior, the author could have used some reference counted structure within his native code to keep the pointer valid. std::shared_ptr would have saved the day.
it just worked for me a few minutes ago (Ubuntu 14). Never seen that error before.
Whenever I encountered this, I tried again a few hours later. 
In this case you either need to add a `Debug` bound on `&lt;T as core::str::FromStr&gt;::Err`, or use `.ok().expect("Error message")` or `.ok().unwrap()` instead.
While I can understand doing this only late in the process, the alternative, which seems to have been chosen, fails to address the issue I've identified and runs the risk of some strongly negative PR at the time of 1.0, something which may haunt Rust for a very long time. People seem to lock-in to their opinions; first impressions matter disproportionately. Doing it late in the process ensures that the language has reached a truly stable point, but also that there's a little bit extra time left for polish. I do appreciate how it's a difficult matter to handle of course.
Java without JNI is indeed memory-safe.
It's like writing segfault-causing code in an unsafe block and then calling Rust an unsafe language.
Theoretically yes that would work. Multilayer perceptron networks like this one have been used for image recognition tasks before, often with pretty good results, but you would probably want to do some preprocessing on the images before feeding them to the network. Theres another class of neural networks called [convolution neural networks](http://en.wikipedia.org/wiki/Convolutional_neural_network) that are designed specifically for image related tasks and require less preprocessing. You might find more success with those in this case.
That's way, way above my paygrade - but thanks for the comment anyways!
I have a `~/code` directory where all my programming-related things go. Within `~/code` I sort projects by language with special folders for projects which will use many folders. Right now I have `rust`, `iron`, `go`, `haskell`, `js`, `c`, `python`, and `misc` folders. I keep all my `zsh` scripts elsewhere for PATH reasons. I segregate by language since some languages are opinionated about the way you organize projects (e.g. go and `GOPATH`). Rust isn't picky about this, so I just have a giant list of rust projects in `~/code/rust`. I keep the rustc source at `/usr/local/src/rust` for tags support. I name all actual project directories `lang-x` where `lang` is the language and `x` is the project name, so I can use languages as namespacing on github. I do the same thing /u/michaelsproul does for multi-folder projects, so my `iron` folder has `~/code/iron/iron`, `~/code/iron/mount`, etc. EDIT: I also highly recommend `fasd` for traversing your file system. I just do `j rust-lazy` or `j stainless` to immediately go to those directories from anywhere.
You can override the caching by specifying a nightly for a specific date, like `curl -L https://static.rust-lang.org/rustup.sh | sudo sh -s - --date=2015-03-29`. This will download a file that is unique for each date, and isn't overwritten for each nightly, so there's no caching issues.
Whether they are optimisations or optional extras or whatever depends entirely on what the library author is doing with them. While I agree that *usually* those features will often be completely required, I still don't think it's binary. E.g. I could imagine someone being able to use `Vec` for their application, but, if available, a more specialised data structure written with `alloc` is slightly faster; similarly, someone may have some file-system interaction library with a utility function that uses `fs_walk` that isn't required for the core functionality offered. In any case, returning to the original point, I guess that most examples of that sort of thing are adequately handled by cargo features rather than an entire separate branch. &gt; We should probably cargo test each crate periodically, and remove it from the public listing if it fails for the latest stable. I think this would be pretty annoying and have a *lot* of false removals, e.g. some libraries may need a database connection, or a webserver, or certain shell commands, or only run on certain platforms... lots of edge cases.
I think such releases would eventually become as-needed rather than on a schedule. There's only so much you can add to a language and the standard library.
You'd think that, but take Ruby: it's been 20 years, still adding stuff. Don't forget, it's not just about adding: (x+1).0 is usually about what you remove, not what you add.
I think we left dangerous back at `curl -L https://static.rust-lang.org/rustup.sh | sudo sh -s`.
Well... It *is* HTTPS, or does curl not use a trust store? It looks like it does in fact use your OS's 'CA cert bundle'. Then again, that does raise the question of why the hash should be checked. Isn't the source for the hash no more reliable than the download at that point, I wonder?
&gt; but it's probably stuff you can only get away with in a dynamically typed, interpreted language [Alice ML](http://www.ps.uni-saarland.de/alice/) is a statically typed language that can serialize functions and modules. For things like open files it prevents them from being serializable. Instead you serialize a proxy that calls back to the original process/machine to access the resource.
Wow, OK. I guess what I should have said that it requires a higher level of abstraction than a language like Rust is comfortable with. :) 
release
So; I'm not very familiar with the Unity Scopes API either, but looking at their Go -&gt; C++ bindings, it looks like you should start with [shim.h](http://bazaar.launchpad.net/~unity-team/go-unityscopes/v2/view/head:/shim.h). I would probably copy shim.h and all cpp files it depends on, over to some separate directory and use a C++ compiler to turn this into a .so file. Then you can link/import this into your rust code like this: type ScopeBase = ::libc::c_void; #[link(name = "unityscopesshim")] extern "C" { pub fn scope_base_scope_directory(scope: *mut ScopeBase) -&gt; *mut ::libc::c_char; /* etc */ } (It's probably possible to do static linking into a single binary too, but I'm not sure how to do that)
I don't think Rust will ever be as powerful as Haskell (or Idris) when it comes to functional programming, but at the same time I don't see a conflict between adding more powerful type (or effect) system features and still keeping the low level predictable performance. In fact it will make the language even safer to use and can even facilitate more advanced compiler optimizations like bounds check elimination, stream fusion etc. However, how to integrate these features in an impure imperative language is still very much an active research field and any additions should only be done after careful consideration. Some inspiration can be taken from Scala, Spec#, ATS, DDC, D etc.
I believe this is the same problem as mentioned in [this issue](https://github.com/rust-lang/rust/issues/21239). Hopefully someone will find some time to fix this.
...it's not like I like your code or anything, baka!
I think you have the wrong subreddit mate. This sub is about a programming language, not the game of the same name.
[Ur](http://www.impredicative.com/ur) is pure, strict and GC-free (region based allocation). Not sure about the effect system though, but the type system looks powerful enough to encode something useful.
I didn't know that, thanks.
How is pass by-value more flexible in this case?
OMG guys sorry, I was in a rush this morning, SOrry!
If your iterator is on an eavy type (for example String) and you want to take ownership on it, or use a &amp;mut method (s.push_char for exemple) you can avoid a clone.
You could consider using `serde` instead of the compiler-intern `rustc-serialize`.
The HashMap, as well as the nodes of the map as well as what the nodes point to, would all need to be in this shared memory space, so you'd need a custom allocator that understood how to do this. You also have the fun task of mapping this memory to likely different addresses in each processes address space. This means you're not storing pointers in your hash map, you're storing offsets which you then add to the base address of your mapped region to get a usable pointer. In short, this is entirely custom work, stuff like https://github.com/aidancully/containerof will help you build intrusive data structures which will simplify this effort. Good luck and have fun. 
That is some ugly code for a simple comparison... It seems like UUIDs will be the way to go then. EDIT: I didn't mean to be offensive with the ugly part, sorry if it came off like that. The main point still stands though: I thought this was some pretty surprising behaviour, since pointer comparisons are not that uncommon in lower level languages.
Oh. Also you're going to need to marshal access that is multi-process friendly, which means you can't use mutexes, I'd recommend a lock-free hashmap implementation, like ~~http://web.stanford.edu/class/ee380/Abstracts/070221_LockFreeHash.pdf~~ http://preshing.com/20130605/the-worlds-simplest-lock-free-hash-table/ 
I mean more like... conceptually simple, coming from a language like C++. I can see why it's not working, and I'm actually fine with using UUIDs, so it's not that big of a problem. This is just the first thing that came to mind, and I was wondering if I could implement it. Here's the other thing I tried, but it fails with an ICE (and I'm not even sure if it would do the right thing anyway): [playpen](http://is.gd/CAElwd).
I think this addresses something that's different in Rust that I've seen from C++ programmers: sometimes, they rely heavily on adress equality. Rust almost never uses address equality.
This is an incorrect statement. There's nothing preventing you from using mutexes that lie in shared memory at least on Unix systems. In fact this is what LMDB uses a mutex for inner process communication (to protect the lock file). Here's the comment from the code "Mutex for the reader table (rw = r) or write transaction (rw = w)" https://gitorious.org/mdb/mdb/source/3368d1f5e243225cba4d730fba19ff600798ebe3:libraries/liblmdb/mdb.c#L321 Finally, if you look at how the linux futex() is implemented / documentation you will see there's a FUTEX_PRIVATE_FLAG that is off by default. It's an optimization that lets you limit who has access to the futex (building block of the mutex) to only your process. By default the notifiers / waiters can cross process boundaries. 
The train release model does not preclude bugfix releases happening at any time. What the train release model excels at--the reason why both Chrome and Firefox adopted it--is delivering new features on a consistent schedule. I do not expect Rust as a language to grow new features without bound, and so I believe that as time goes on and as the language becomes more complete users will perceive less of a benefit to such a rapid release schedule (developers won't abide Firefox-style automatic updates, and so updating Rust will always be a manual affair and at least a minor annoyance) and will apply pressure to reduce its frequency.
I stand corrected. I haven't even considered that this might be the case, but it is pretty clear in the man page for mutex_init. I do quite a bit of shm data structures for my day job, but we also tend toward lock-free data structures. http://www.lehman.cuny.edu/cgi-bin/man-cgi?mutex_init+3 &gt; Mutexes can synchronize threads within the same process or in other processes. Mutexes can be used to synchronize threads between processes if the mutexes are allocated in writable memory and shared among the cooperating processes (see mmap(2)), and have been initialized for this task. 
Thanks for the notes, and the new preshing.com link. The constraints are mostly fine; I just need to add remove(). Neat. 
Rust discourages aliasing. The biggest issue I've seen in C++ where you need to check for address equality is in things like `operator=(const Thing&amp; other)` where you break things horribly if `&amp;other == this`. You can't have that situation in Rust.
Use this sub: [/r/playrust](http://www.reddit.com/r/playrust)
Thanks!
I think the idea is that piping unknown code directly into `sudo sh` is pretty dangerous to begin with, regardless of that code's origin. What happens if some line accidentally evaluates to `rm -rf /`, as [happened with steam recently](https://github.com/valvesoftware/steam-for-linux/issues/3671)?
Worth noting that std::collections::HashMap doesn't have any kinds of nodes. It's a single contiguous allocation of bytes (treated as 3 seperate arrays).
Keep in mind that many bounds checks are *already removed* in practice thanks to LLVM's great analysis. Not all, mind you. But e.g. a naive `for i in foo.len() { foo[i] }` will probably have it's bounds-checks stripped modulo compiler regressions.
I can only suppose this is a check against transmission errors (like a CRC would do), and not a security feature - if it were, they would use a digital signature. (if the hash is transmitted in the same medium as the file, checking it doesn't add any *security*!)
Hey this is fantastic! (weird reading a paper with "I" instead of "we"...)
That would also require (cross-process) memory barriers, right? Does Rust have a way to do that?
Cheers! I'm also a big fan of the academic "we". I deliberated over whether it was stranger writing "we" when it was just me or using "I" and figured it was better to be honest about it :P
What are you calling comparative programming? It sounds interesting.
The reference you have for TCO ([2]) is outdated. See https://github.com/rust-lang/rfcs/pull/81 for a more recent discussion. It's closed and labeled as postponed until after 1.0. Updates to LLVM (musttail) now appear to make this possible.
I think he just means a traditional paradigms class in which you discuss multiple languages/approaches to solving programming problems.
Ah, good to know. Thanks!
One you start storing references in objects, lifetimes start mattering. :) Try this out: http://is.gd/jz5kZF To fix your code, we can just listen to the compiler errors. Here's what we started with: &lt;anon&gt;:48:25: 48:34 error: wrong number of lifetime parameters: expected 1, found 0 [E0107] &lt;anon&gt;:48 impl TraitRectangle for Rectangle { ^~~~~~~~~ &lt;anon&gt;:51:53: 51:62 error: wrong number of lifetime parameters: expected 1, found 0 [E0107] &lt;anon&gt;:51 fn new(topleft: &amp;Point, bottomright: &amp;Point) -&gt; Rectangle { ^~~~~~~~~ I presume that just before you pasted this code you altered `Rectangle`'s definition to `struct Rectangle&lt;'a&gt;`. So here we just need to likewise update `Rectangle` in the offending lines of code: impl TraitRectangle for Rectangle { // before impl TraitRectangle for Rectangle&lt;'a&gt; { // after fn new(topleft: &amp;Point, bottomright: &amp;Point) -&gt; Rectangle { // before fn new(topleft: &amp;Point, bottomright: &amp;Point) -&gt; Rectangle&lt;'a&gt; { // after New compiler error: &lt;anon&gt;:48:35: 48:37 error: use of undeclared lifetime name `'a` [E0261] &lt;anon&gt;:48 impl TraitRectangle for Rectangle&lt;'a&gt; { ^~ &lt;anon&gt;:51:63: 51:65 error: use of undeclared lifetime name `'a` [E0261] &lt;anon&gt;:51 fn new(topleft: &amp;Point, bottomright: &amp;Point) -&gt; Rectangle&lt;'a&gt; { ^~ Lifetime parameters need to be declared up-front just like type parameters (a.k.a. like the `T` in `fn foo&lt;T&gt;(x: T)`). All we need to do now is declare the lifetime parameter for this impl block: impl TraitRectangle for Rectangle&lt;'a&gt; { // before impl&lt;'a&gt; TraitRectangle for Rectangle&lt;'a&gt; { // after If you don't know where to put type parameter declarations on impls, this is definitely tricky. :) Next: &lt;anon&gt;:52:25: 52:32 error: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements &lt;anon&gt;:52 Rectangle { tl: topleft, br: bottomright } ^~~~~~~ &lt;anon&gt;:51:5: 53:6 help: consider using an explicit lifetime parameter as shown: fn new(topleft: &amp;'a Point, bottomright: &amp;Point) -&gt; Rectangle&lt;'a&gt; &lt;anon&gt;:51 fn new(topleft: &amp;Point, bottomright: &amp;Point) -&gt; Rectangle&lt;'a&gt; { &lt;anon&gt;:52 Rectangle { tl: topleft, br: bottomright } &lt;anon&gt;:53 } The compiler is offering us a suggestion there, so let's try that and see what results: &lt;anon&gt;:51:5: 53:6 error: method `new` has an incompatible type for trait: expected bound lifetime parameter , found concrete lifetime [E0053] &lt;anon&gt;:51 fn new(topleft: &amp;'a Point, bottomright: &amp;Point) -&gt; Rectangle&lt;'a&gt; { &lt;anon&gt;:52 Rectangle { tl: topleft, br: bottomright } &lt;anon&gt;:53 } &lt;anon&gt;:52:38: 52:49 error: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements &lt;anon&gt;:52 Rectangle { tl: topleft, br: bottomright } ^~~~~~~~~~~ &lt;anon&gt;:51:5: 53:6 help: consider using an explicit lifetime parameter as shown: fn new(topleft: &amp;'a Point, bottomright: &amp;'a Point) -&gt; Rectangle&lt;'a&gt; &lt;anon&gt;:51 fn new(topleft: &amp;'a Point, bottomright: &amp;Point) -&gt; Rectangle&lt;'a&gt; { &lt;anon&gt;:52 Rectangle { tl: topleft, br: bottomright } &lt;anon&gt;:53 } Look at that second error first: it's basically the same as the prior suggestion, just on the other parameter to `new`. So let's fix that. To recap, the signature has changed thusly: fn new(topleft: &amp;Point, bottomright: &amp;Point) -&gt; Rectangle&lt;'a&gt; { fn new(topleft: &amp;'a Point, bottomright: &amp;'a Point) -&gt; Rectangle&lt;'a&gt; { // after Looking at that first error, it's telling us that the signature for `new` in our impl block doesn't match the signature for `new` as it's defined way up in the `TraitRectangle` trait. Let's fix that: fn new(topleft: &amp;Self::Point, bottomright: &amp;Self::Point) -&gt; Self; // before fn new(topleft: &amp;'a Self::Point, bottomright: &amp;'a Self::Point) -&gt; Self; // after Just like in step 2, we'll need to declare this parameter on the `TraitRectangle` definition: trait TraitRectangle { // before trait TraitRectangle&lt;'a&gt; { // after And just like in step 1, now that we've changed the definition of `TraitRectangle` we'll need to change its usage elsewhere: impl&lt;'a&gt; TraitRectangle for Rectangle&lt;'a&gt; { // before impl&lt;'a&gt; TraitRectangle&lt;'a&gt; for Rectangle&lt;'a&gt; { // after (We could have also determined these last two steps from compiler output, but this comment is already long enough.) Try compiling, and... Width: 2.2 Height: 2.2 AREA: 4.84 Success!
If it’s provable that if-let will always succeed, then the optimizer should be able to remove the branch (although it might need to be taught to do that).
You can `impl Add&lt;&amp;BigInt&gt;`, and then you can do `(&amp;big_int_1).add(&amp;big_int_2);`
I'm assuming your question is "why aren't they like this", since you forgot to actually *ask* anything. :P The justification is in [RFC #439](https://github.com/rust-lang/rfcs/blob/master/text/0439-cmp-ops-reform.md#proposed-design).
actually, the Add trait used to take the right hand side by reference. But, this way is strictly more flexible, because you can define RHS as &amp;Self. For example, BigInt impls both Add&lt;BigInt&gt;, and Add&lt;&amp;BigInt&gt;, and &amp;BigInt impls Add&lt;&amp;BigInt&gt; as well.
But in Rust generally address equality is known statically beforehand due to ownership and all. Unless you're using an `Rc` or a double-pointer, and in both cases the way of getting address equality isn't too bad.
But first, have you tried compiling with optimizations? :P