It looks great! It is weird how rust is a low-level language and so far there wasn't really a convenient way to work with binary data. 
How is the compiler supposed to work out what `InfoT` and `ErrorT` are? They're not related to any of the arguments, the implementing type, or the return. There's nothing for the compiler to infer *with*. I can't see any way to make this work because there's just nothing to connect the data you're working with and the type it's supposed to be. Is there any reason you can't use `Box&lt;Any&gt;`? As for the transmutes, it's *probably not* safe. I mean, if the source/destination types have `Drop` implementations, that's likely going to cause things to explode. I'd recommend reading and internalising [The Nomicon](https://doc.rust-lang.org/nomicon/) before proceeding.
Make it an associated type. trait VecOps { type Component; // ... }
What does this not compile? I am trying to match a tuple of enums and I don't want to match borrows of the enums to avoid typing `&amp;` everywhere; so I made sure that there's nothing that's moved so I don't know whats being moved? https://is.gd/PqFBGo
Well, there is the bincode family of crates for serde. Also, in many cases binary formats aren't simply fixed and it's safer and more convenient to write a full parser that can validate the data on the fly, e.g. with `nom`. That said, I love that this crate is available (and already got my first PR merged :)).
&gt; Is there any reason you can't use Box&lt;Any&gt;? I guess I was hoping there would be a cheaper way to do this. I guess Box&lt;Any&gt; is as good as I can get for now, and just downcast to the right type. It just feels like unnecessary overhead. &gt; I mean, if the source/destination types have Drop implementations, that's likely going to cause things to explode. Why's that? Does transmuting drop? &gt; I'd recommend reading and internalising The Nomicon before proceeding. It sounds like this whole thing won't work anyways, so I'm stuck with Any again :\
bincode is its own format, it won't really help you with an established one. And nom and other parsers are great, but the problem is that they are only one way, so if you want to emit data you have to write the code again in normal rust anyway.
Great!
 let buf: Vec&lt;u8&gt; = s.pack(1, 2, 3)?; ^ | What can fail here?
But the enum variants **are** moved when you dereference them. I'm not sure you can do this without sprinkling _&amp;_, but maybe you can make this more compact using: use Val::*; https://is.gd/mqJDia Doesn't solve your problem, but at least it's pretty ™
With "s" or "S" format buffers, given as `&amp;[u8]`, their length can exceed the space in the format.
I have similar experience.
As much as I prefer distribution package managers, I must admit that I find rustup more useful because of it's ability to manage different toolchains.
Never mind then, I was under the impression that bincode is plain old binary representation of integers. Hm, in that case a serde backend for "struct" like data should still be useful...
&gt; last time I checked, it doesn't support multiple users properly. I found this a bit painful too.
It depends on what your needs are. Rustup lets you manage multiple rust installations, additional components (rls, for example). The packaged one will likely be just the basic one, but it'll get updated with the rest of your system without you having to run additional tool. So, if you use rustc every day, want to stay on the bleeding edge with nightly, etc, rustup will be better. If you just need rustc and don't care about which version, then the packaged one will be more convenient.
&gt; makes me feel like StackOverflow may not be the primary channel for asking for help. Because it's a terrible metric. If you design a language to encourage fewer errors, you get fewer questions. There's a reason JavaScript is at the top of the list. 
I think it means that if your structure has a `2S` (a 2-element fixed size buffer) in it, and you pack it with `[0, 0, 0]`, that can fail at runtime.
Good point. Maybe there are other parts of the type system that rely on this. (The same applies to the other guards, the ones from RefCell and RwLock -- however, there the bounds on `Send` and `Sync` end up being correct, but overly restrictive.) Notice however that just adding such a field will result in `MutexGuard&lt;T&gt;` to be `Sync` only if `T` is both `Send` and `Sync`, which is unnecessarily restrictive. Though maybe, from a practical standpoint, this does not matter -- after all, using a `Mutex` with a non-`Send` `T` is completely pointless.
Well, to be fair, I see a fair amount of questions about borrow-checker fighting ;)
&gt; the worst that happens is some unexpected behavior. What would that be? Panics? Aborts?
It's maybe not the main design goal, but bincode works for many C-like structs.
Ah, yeah. mem::forget seems reasonable. Unfortunately I'm hitting some roadbloacks that just make this whole thing feel non-viable.
Oh, I see. Couldn't it be designed in such way that only your example would return `Result&lt;T, E&gt;` but other examples (provably correct) would either not fail or would return `Result&lt;T, !&gt;`?
I think the `Rc&lt;RefCell&lt;Bus&gt;&gt;` approach is pretty good. The runtime borrow-checking should never fail because the functions that use `Bus` don't appear to be recursive, so you're not really losing anything (except for a tiny runtime penalty that you probably won't notice next to doing IO).
The rustup package provides rust, use it.
I think you meant to post this to /r/playrust?
From /u/ksion's not-yet-obsolete (;-]) [post](http://xion.io/post/code/rust-patterns-ref.html): &gt; Pattern matching, however, due to its ability to unpack values into their constituent parts, is a _destructive_ operation. Anything we apply `match` (or similar construct) to will be _moved_ into the block by default 
Ok, thanks everyone. I will uninstall rust and start over with rustup.
I use a distro package, mostly (for Servo stuff I use the one that their Mach chain downloads). I don't want to use nightly.
Oooh, that's a great idea. I'll implement soon.
&gt; `Result&lt;T, !&gt;` Is that a real type that can be returned? I mean, I know you can say `-&gt; !` as the return signature for functions (e.g., `panic!()`) to indicate that the function will not return, but I figured it was just a special case, and not a real type? What does it do if you try to access the error portion of the result?
Ah yeah sorry, if you want to use rustup you need to remove rust. If you want to use Rust to write some code, rustup is very much preferred over packaged rust.
They should rewrite it in rust.
i have been trying for months to learn Hyper, now some will say i am Dumb, but fact is fact Hyper is not useful, most examples i find are about a useless thing called Hyper::Client, So after months of reading i find Rust useless. There are no Guides, no proper examples, Not a single complete example with HTML Page which calls back to Rust server and gets response, Only hello wold programs, even worst is , i get banned from Reddit for speaking out.
The simplest thing is to just use `rustup` from the Arch repos. Alternatively, you can use `rustup-git` from the AUR if you want bleeding edge.
Ok, I got it done. Thank you.
It will be, but the implementation has not stabilized fully yet. https://github.com/rust-lang/rust/issues/35121 &gt; What does it do if you try to access the error portion of the result? Since `!` is unpopulated, it is guaranteed that a `Result&lt;T,!&gt;` only ever is `Ok(T)`. So you cannot actually access the error portion of the result, there never is one. A `Result&lt;!,E&gt;` only ever is `Err(E)`, and anything saying they'll return `Result&lt;!,!&gt;` won't ever return at all.
You want /r/playrust. This sub is for the Rust programming language.
I think you want /r/playrust ... /r/rust is for the rust programming language
Yes, eventually.
I haven't checked this in detail, but I think it is similar to a poisened mutex: If you have your own invariants on things (like "this RefCell&lt;Vec&lt;u32&gt;&gt; is always sorted"), they may be violated because things paniced in the middle of an operation.
Thanks! Glad to hear it looks reasonable.
Good point, though I wanted to avoid `unsafe` as it seems a bit overkill in this situation. Definitely good to keep in mind, though!
Any particularly notable exclusions from your model?
[removed]
 -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 1) Are you on the correct subreddit? /r/playrust is over there... 2) Is it the mods or the admins that you are accusing of doing this? 3) Have you tried signing all of your messages with OpenPGP? -----BEGIN PGP SIGNATURE----- iHUEARYIAB0WIQS9s/ioQqFlbMELXNnuO4uMiPxydQUCWTyuWwAKCRDuO4uMiPxy dR1bAQD1kfL/nUO+C57f4H43JVH45hnQIGrqZmpuktCZc9LC5wEA+UEXer4aGHMI h3pg0e0kRQSobEHCM8IqeVYnyAoxCgI= =Pskg -----END PGP SIGNATURE-----
Everything except for borrows and lifetimes. ;) In particular: We have no panics, no drop. I think these are the biggest. Panics don't interact with borrow checking *too* much I think, though they do make take_mut a hell of a lot simpler, which is cheating. Drop, well, I think there are some interesting and subtle issues lurking here. We also have no traits, bit I feel the issues raised by traits are mostly orthogonal to borrow checking. And anyway there is little point in formalizing the pre-Chalk trait system. :D Oh, no unsized types either. On the plus side, we do have Cell, RefCell, RwLock, Mutex (all with their respective guards), Rc, Arc (both including weak references), thread::spawn, take_mut::take, alias::one and "cast &amp;&amp;T to &amp;Box&lt;T&gt;". So I think we got many interesting patterns covered. We *did* have a brief look at your amazing "safe indexing via lifetime polymorphism", and I am somewhat doubtful it fits our framework, but my colleague thinks we could do it. The reason I am doubtful is the fact that every type in Rust has to respect "lifetime equality" (mutual outlives), and that seems hard to achieve when lifetimes are mis-used the way you do. However, we noticed that some of the things you rely on -- essentially a form of lifetime parametricity, in the sense that the compiler must not get too "smart" when type checking a lifetime-polymorphic function -- also play a crucial role in Ref{,Mut}::map. I plan to write a blog post about this one day.
I would like to add a side note. For those crates like ripgrep and exa that are now available in many distro's package managers. Go with the cargo version every time. It will be compiled for your system specifically and will be better optimized for you and your system. Take advantage of all those cores for better thread performance. 
Close. Arch. 
I'd recommend using the `reqwest` crate. It will handle the TLS, uploading a file, parsing the response JSON, and perhaps other things I didn't notice.
I think the reason JavaScript is at the top of the list is that it is one of the most commonly used languages in the world.
If unsafe code relies on those invariants, it could segfault though...
I personally recommend [derive_error](https://docs.rs/crate/derive-error/0.0.3), since all it really requires you to do in most is just use the most natural `#[derive(Error)]`.
Is that a pun on the fact that Pants (alongside few other build systems) is directly inspired by Google's Blaze? :)
&gt; I also find interesting to note that Rust is much more ahead in the github ranking than in the stackoverflow one I'd think this is because of the abundance of Rust-specific spaces that the community has created for newbies, including this subreddit (esp. the "got an easy question?" thread) and IRC (esp. `#rust-beginners`). They are usually much faster than SO at getting your answer; they're more interactive, and it's much less likely for your question to be shot down / closed.
You can take a look here https://github.com/lise-henry/books/ but it's far from perfect.
mobi/epub would be also awesome - i saw that there was such version available somewhere, but a bit outdated :/
On nightly you can do it [like this](https://play.rust-lang.org/?gist=0263a614fce03de5394807a5ea2a46d1&amp;version=nightly&amp;backtrace=0). On stable you may [use this](https://play.rust-lang.org/?gist=594765982af31da7ff2d1a8bc95fc727&amp;version=stable&amp;backtrace=0) Edit: you probably want to follow [this issue](https://github.com/rust-lang/rust/issues/39817)
&gt; try using it to invert a sha256 hash Haha!
Is there is a single page version of the book I missing?
Yes – when you press the print button in the top right corner it navigates you there before showing the print dialog. :-]
Yes. There's also a factor of the crowd. I used to follow the C++ tag, and the Eternal September is very much an issue, with many students asking basic programming questions about their homework which just happened to be tagged C++ because that's the language elected for the homework, but would often be similar in other languages. Rust being for now used more by "already" programmer does not attract this kind of "simple" questions.
This seems weird. I posted a comment on the article asking for clarification about the SO ranking as I could not find it at a glance. It may be they are measuring something different than the raw number of questions.
Others love it because you don't necessarily need root. Similar to nix.
The thing is, it understands that dereference is not necessarily a move if you only match on `*self`: impl Val { fn add(&amp;self, rhs: &amp;Val) -&gt; Val { match *self { Val::Int(l) =&gt; Val::Int(l+1), _ =&gt; unimplemented!() } } } This works fine. Not sure why it breaks when you try to match `(*self, *something_else)`. Probably because `(*self, *something_else)` is a tuple construct where the compiler can do nothing else than create a temporary tuple which `self` and `something_else` should be moved to. Even `match (*self, 1)` complains about moving. 
I am new to rust, and kinda stupid... so excuse me if I say something stupid. - Maybe run RustFmt? - Also, I feel like you're panicking too much in your code. Stuff like running `xdg-open` can fail without a completely messed up system - you just have to uninstall it. So printing the error yourself instead of `.expect("Failed to open");` feels like a cleaner approach
Here are some examples that I found really useful when I started out building web apps with rust https://github.com/brson/httptest
I love it too. Being unable to choose to install it for everyone is what I dislike.
are you thinking of [byteorder](https://crates.io/crates/byteorder)?
Use rustup package, unless you need Rust compiler to be available for multiple users (a rare usecase) and don't care about nightly/rls/clippy.
Yeah this is something that might eventually expand into tens, maybe even hundreds of functions. Not really a problem if I namespace them away into a module, but would get quite boilerplatey. I did think of your way, and it is my fallback solution if all else fails. I made this reddit post to see if there is a more elegant way.
Well, I can see your point - however, that's only if the --open flag is set. I could match the error and provide a nicer error message fairly easily, though. Also, this happens after uploading, so it will still print the new link to stdout. I suppose if you were having issues you could pipe it into xargs and use anything similar to xdg-open. 
Umm, I think you are misunderstanding me. The `expect` doesn't matter on program error, it triggers if there was a problem starting the program all along. So, say, if xdg-open fails, nothing happens. But if it's uninstalled or some other unknown system error, then it does a panic because `expect`.
[This](https://play.rust-lang.org/?gist=422f3b643e9e886949d7a6c704d1acac&amp;version=nightly&amp;backtrace=0) silences the warning; seems benign.
Of course, the crate already generates different return types for `unpack` - so would it make sense to generate different return types for `pack` too, depending on whether packing can fail?
Some of the versions which aren't marked as git also compile from source. Yeah, not all AUR users follow the drill
Cool, thanks for the details! I really hope you got that cast backwards. 0.o
I think you're right. I've changed the code to not panic but instead return a slightly friendlier error message explaining what went wrong. It might be beneficial to match the io::Error to provide even more in-depth error explanations (i.e. xdg-open was not found on your system as opposed to "file not found", etc.)
Thanks. Transmute is the answer I looked for, despite of its magic look.
On the topic of cross-compiling from x86_64 to ARM, it doesn't really require much and Docker may be overkill, depending on your goal. Have you checked the RasPi community for a distro-agnostic cross-compiler tarball? While my [OpenPandora](http://openpandora.org/) isn't a RasPi, I was able to cross-compile pure Rust code simply by unpacking such a tarball (no Docker needed), identifying the LLVM equivalent to that GCC's target triple (`arm-unknown-linux-gnueabi`), and then running this command... rustup target add arm-unknown-linux-gnueabi ...and adding this to `~/.cargo/config`: [target.arm-unknown-linux-gnueabi] linker = "/home/ssokolow/opt/pandora-dev/arm-2011.09/bin/pandora-gcc"
CHECKERS is now a reference, instead of an array.
You can get the type parameter using `Self::Elem`. If you want to place a bound on `Self::Elem` (or `T::Elem` if `T: VecOps`), then do it with a `where` clause: fn do_stuff&lt;T: VecOps&gt;(t: T) where T::Elem: Clone { } Not sure if exactly that would work, maybe you need to explicitly cast `T`: fn do_stuff&lt;T: VecOps&gt;(t: T) where &lt;T as VecOps&gt;::Elem: Clone { }
Regarding the AUR, assuming you have a raspberry pi 2 or 3, you should only need `arm-linux-gnueabihf-gcc` (which depends on `arm-linux-gnueabihf-binutils` and `arm-linux-gnueabihf-glibc`). Building this compiler goes through a 2-stages process, where you build intermediate compilers, but it is a common process when building cross-compilers. I agree it's a shame this hasn't made it to `community`, while `arm-none-eabi-gcc` and `aarch64-linux-gnu-gcc` have.
The ABS lets you build source for official repo packages too.
I just assumed it was the total (all-time) count of Stack Overflow questions, i.e. the area under the curves on that chart I linked, which leads me to believe that rust will make its next few jumps in Stack Overflow rank over the next year just by sustaining the current Stack Overflow participation levels. I wonder if the same is true of Github PRs rank, that languages like perl and coffeescript might have more history but be less active currently... 
Not yet, but that is definitely the plan. Long term, reqwest would offer both a sync and async client. First, hyper 0.11 needs to be released, and then reqwest can start to build on it.
That makes sense. Thanks!
See https://reddit.com/r/rust/comments/6gkmt2/whats_coming_up_in_imag_26_published_on_the_new/dirfjsg?context=3 Thank you! :-)
I recommend just using the official rustup script so it keeps itself updated independently as you are developing so nothing breaks.
If these truly aren't closures, you could do pub type Checker = fn(&amp;Value) -&gt; bool; (with a lower case f). Then values of type Checker are simple pointers to static functions. They will be Sync and can be put directly in the array.
part of the problem was: it seemed to require a qualified path (e.g. 'Self::Elem' or whatever) all the time, the existing code was just more pleasant with that labelled as 'T', and it was too much to go back and change The 'VecOps' trait itself also depended on others - the functions were split into layers (basic accessors, permutes for anything with accessors, etc..) That further complicated 'Elem' i.e. 'which of the component traits is it defined in?' .. the path of minimum resistance seemed to be to just make the HasElem, which gave one clear place to define it I didn't know which of my traits I wanted to be the most fundamental: because I might have wanted some vector operations to be doable on types where the elements *aren't* necessarily accessible.. but having component accessors allowed default implementations. Anyway the real problem was having gone one several ways . I also used these traits to abstractly allow (T,T,T), [T;3], Vec3(T,T,T) and struct Vec3{x,y,z} to slot into the same code :) the idea was that such traits would allow interfacing with other vector maths approaches, because I can see a draw to all of them e.g. I actually like how rust makes just grouping a couple of few (T,T,T) without needing to name it (introducing dependencies..). [T;3] enforces they're the same type , but the tuple also allows *different* types per component (real world applications - slotting in special case 'zeros' to make 'a vector that is axis aligned' , or 'Point' = X,Y,Z,One.. or having 'longitude,latitude' in high precision, height in low precision. whatever. ... I've subsequently settled on **Vec3(T,T,T)** as my favourite since Rust got the '.0 .1 .2' tuple &amp; tuple struct accessors. This code would all be a lot simpler if I just binned it and re-wrote it.. nonetheless I wanted to get used to what you could and couldn't do, and see how to adapt my earlier approaches. Not trying to allow every possibility would have made the whole thing a lot shorter of course :)
I'm working on my first Rust project and I'm trying to create a fixed-point decimal math library (I know a couple already exist, but I wanted to do it as a learning exercise). I'm currently at the stage of overloading all the math operators on my Decimal type. These operations can fail if the result overflows the maximum number of digits or if the left-hand side of an assignment has lower precision than the right-hand side. For Add/Sub/etc. I'm returning `Result&lt;Decimal, DecimalError&gt;`, but AddAssign/SubAssign/etc. have no return value, so it looks to me like there's no option but to panic if something goes wrong. Is there a better way to handle this? The code is below. Feel free to point out anything I'm doing wrong; I'm still very new to Rust. impl ops::Add for Decimal { type Output = DecimalResult; fn add(self, rhs: Decimal) -&gt; DecimalResult { let (precision, lhs, rhs) = self.convert_decimal(&amp;rhs)?; let (value, overflowed) = lhs.overflowing_add(rhs); if overflowed || value &gt; MAX || value &lt; MIN { Err(DecimalError::ArithmeticOverflow) } else { Ok(Decimal { value: value, precision: precision }) } } } impl ops::AddAssign for Decimal { fn add_assign(&amp;mut self, rhs: Self) { if self.precision &lt; rhs.precision { panic!("Insufficient precision on left-hand side of += operator"); } let (_, _, rhs) = self.convert_decimal(&amp;rhs).unwrap(); self.value += rhs; } }
This is just what I needed! Thanks! The book looks great and the new API is super clean! I'm excited to try this out!
&gt;&gt; if the bounds are that overwhelming, I'm glad I asked because the answers did reveal how to mange them better. I didn't know the full syntax for bounds on associated types. And when I can label the intermediates as was shown in this thred (instead of each one being a successively bigger expression) it's a lot easier to deal with. I do like the fact I can constrain output=input unlike 'auto' return in C++.. I realise that'll have benefit with more inference elsewhere. I would prefer to avoid using a macro. I do like the idea of using macros for things like setting up bindings to shader parameters.. (where I'd probably be using messy x-macros in C++ anyway) but I seem to find it messy to rely on them elsewhere. Maybe my fear is: IDE's wont work with them . I also find they disrupt the nesting level.. **I wish I didn't care, but I do**. I would probably like macros a lot more if there was a 'receiver' option, $self ... "MyTrait.my_enhanced_declaration!{... body..}" .. I think it really stabs me in the eye when major information ('title', loop setup in 'for!', etc) is swapped inside the main brace
I really really really want Amethyst to mature quickly and enable people do make awesome games in it. This update of `specs` makes that dream even closer to being true! :D Oh, question: Will `specs` be visited by the Rust Lib Blitz team eventually?
It's in a weird spot because it's the *only* language for web, but then it's not well-designed so it's perfectly poised to generate lots of questions. 
Made in Hyper ? is Hyper useful or just hype ?
&gt; Made in Hyper ? tiny is written using plain old and boring non-blocking sockets + `poll()`. (Also, I think hyper is a http client library so I'm not sure how could it be used here)
[removed]
Yup
Look ma, I'm in the screenshot! ^^^^ignore ^^^^me
Sorry , a sane man cant show respect to those who dont respect freedom of expression.
&gt; "tiny irc client" You don't see how that would be confusing?
To paraphrase this: Essentially, the Rust team decided to put the blame for the kind of segfaults /u/kixunil brought up on the unsafe code that relied on these invariants. That decision could just as well have been made the other way around, but we have to decide either way. That said, if you control everything that accesses e.g. some particular mutex, and if you are disciplined with what you do when the mutex gets poisoned, you *can* write unsafe code that relies on such invariants -- and the poisoning mechanism helps you do so. It inserts a "speed bump" into your code which forces you to consider the case of a poisoned mutex every time you acquire the lock, so it's harder for you to screw this up. That is, in fact, one of the motivations for having the poisoning mechanism. The same applies to `UnwindSafe`: It makes it less likely for you to accidentally violate such invariants, by having you write `AssertUnwindSafe` [1] in the type when the possibility of a violated invariant occurs. [1] &lt;https://doc.rust-lang.org/beta/std/panic/struct.AssertUnwindSafe.html&gt;
That you're allowed to express yourself doesn't mean you have to be listened to :)
I thought we were just putting the blame for these segfaults on C
you are free to ignore me, but if you ban me you must feel the wrath of Titans because we inherit your planet and we take freedoms very seriously.
yes, i have seen that but thats not enough , and i am sure its not enough specially for creating a framework that thousands of programmers will be using. we need use cases and hyper certainly needs that due to its intended application
But it is just called tiny and nothing else.
So what , so was Newton considerer at his time , so was Copernicus and every great scientist at his time. is the world progressing or regressing, why cant we speak our mind. And why will we stop any time since its not our fault that most people are STUPID SHEEP , BLIND FOLLOWERS DUE to the world wide memory based education system
certainly, i agree but only if reddit is not using those votes to further suppress our new post and comments, in that case we will be better off making a new profile EVERYDAY with new name and get every profile deleted by abusing on the first hint of dumbness from these so called downvoters/ dumbos/ humans. We Titans will keep fighting until we get our freedoms, we are not like you, we never give up.
thanks , thats a good point, i will do that, thanks for the effort to reply.
I also found this recently: https://github.com/chucklefish/rlua 
relevant link: https://www.reddit.com/r/rust/comments/6ckolo/announcing_a_new_high_level_lua_bindings_library/ Note that chucklefish is actually a game dev studio, which makes it particularly interesting here.
i am not human
The best you(or at least I) could ask for an in a HTTP library tutorial is ability to send valid JSON, HTML and other content types. You won't find JavaScript examples in Rust tutorials. You can browse the crate.io repository for howto. It's an ember app, with postgres backend....also are you a troll?
So... any actual games using it? Rust has a new ECS library every month or so but I think I've only heard of one game that is anywhere near finished and that didn't make use of ECS.
there was a very similar post a few months ago, but i can't remember what it was titled either...
At the end of June I'm going to be beginning a project using Amethyst with a small team (which is using specs for an ECS).
[removed]
Civility on Reddit ? Does Reddit respect our freedoms ?
I have read your article, and I did enjoy it! As you say, it did not address this particular issue. Thanks for the link to the PR, I will have a look through the comments. Though I'm not an expert, I think you're correct that what I'm looking for isn't possible with today's rust. I wonder if there are any RFCs which could change this.
Quite likely the article linked by Cobrand?
https://en.wikipedia.org/wiki/Galileo_affair
Would love to see support for LuaJIT as performance gap between LuaJIT vs default Lua compiler is huge. http://luajit.org/performance_x86.html 
I might be missing something, but I think the macro hygiene rules get in the way. Whilst it does error in the bad cases, I can't get your macro to work in the good case. https://play.rust-lang.org/?gist=601c152332b0122aacc1fc1923599d5a&amp;version=stable&amp;backtrace=0 "field `phantom_id` of struct `render::Canvas` is private"
Yeah, if it's pub then you can build one without the macro, sort of defeating the purpose of having the macro.
I was using Ring for a while and looking up support for it was a nightmare because of it's name. People trying to use your program won't really care what language it is in unless they are only using it because it is written in Rust, so adding that to the query isn't really viable. 'Tiny IRC Client' as a search might work, but you may get a lot of results of people looking for lightweight IRC clients and not necessarily your own. The more unique the name, the better. Even just having TinyIRC as one word would help searching immensely.
You could also use rust-gdb to get a full-fledged debugger. 
Regardless of whether there is nothing that helps you use Hyper, everybody including you needs to heed the rules of this subreddit to participate the discussion. Note the rules 1# "Respect our CoC", #2 "Constructive comments only", #4 "No zealotry" and #6 "Chill out".
Lol, there are some good posts there. Thank you for sharing.
I've not used `impl Trait` myself, but I believe functions using it have to always return the same type, you just don't have to publish what that type is in the public signature. What's required here is a function which returns a different type every time it's called. Something like `fn&lt;T&gt; magic() -&gt; T`. I feel like it could be a long time until we see 'full-blown dependent types' in rust!
Well, this isn't a new ECS library, it was one of the first ones. And there is actually a small game, however it wasn't updated to 0.9 so far: [yasteroids](https://github.com/kvark/yasteroids). But you're right, we shouldn't forget to build games when building tools for them. Fortunately, Specs is now ready for that. Happy Coding!
I'm hitting a problem with trying to add serde integration to [optional](https://github.com/llogiq/optional), so I'll reach out here or on IRC. Also TWiR and possibly some clippy stuff.
Modules question. I'm moving my python library to rust (to force me to nail down the spec I've been writing) and copied in a few code samples for kicks and giggles. They hate to build: github.com/congredi/congredi.rs Travis-ci.org/congredi/congredi.rs Am i using "extern crate" correctly? (Fairly certain that's a no) how do i do so?
Great, looking forward to it!
Untested, but something like let finished = game.waypoints .iter_mut() .find(|wp| !wp.hit) .map_or(true, |wp| { wp.hit = true; false }); if finished { for wp in &amp;mut game.waypoints { wp.hit = false; } }
https://www.reddit.com/r/rust/comments/3oo0oe/sound_unchecked_indexing_with_lifetimebased_value/ does something similar with lifetimes for the purposes of indexing slices without bounds checks in a safe way, and was expanded into [the `indexing` library](https://github.com/bluss/indexing).
In Python, `bin(x).count('1')` really is the fastest way to count bits in an int. A manual loop is slower due to the interpreter overhead.
I once developed Haskell infrastructure for a Linux distro, with the overall plan of providing all Haskell libraries as distro packages. My takeaway from that experience is this: not a good idea. It's nowhere near worth the trouble, and is hard to maintain. Use the language's own tools for this. In short: use rustup. 
I wouldn't have noticed that small, low-contrast button if you hadn't pointed it out. Thank you! For anyone interested in a direct link: https://doc.rust-lang.org/book/second-edition/print.html
I believe this was the correct answer - the segfault goes away when I make some constants (that determine the sizes of certain arrays) fixed-size arrays smaller.
I have recently released [`i3nator`][i3nator-gh], an application that tries to be [Tmuxinator][gh-tmuxinator] for the [i3 window manager][i3wm]. I have written it for my own usage, but I hope I made it general enough such that others can benefit from it! Sticking with i3, I am also currently contributing to [`i3status-rs`][i3status-gh] to try to make it easier to use, both from an end-users perspective and from a developers perspective. Specifically I am looking into [streamlining the configuration][i3status-gh-41]. [gh-tmuxinator]: https://github.com/tmuxinator/tmuxinator [i3nator-gh]: https://github.com/pitkley/i3nator [i3wm]: https://i3wm.org/ [i3status-gh]: https://github.com/greshake/i3status-rust [i3status-gh-41]: https://github.com/greshake/i3status-rust/pull/41
It's not just a strategy, we're adopting "lifetimes cannot affect execution" as a semantic rule.
Do you mean [slices](https://doc.rust-lang.org/std/primitive.slice.html)?
That's a great explanation! I think you could improve the "Introduction" chapter, if you want to.
Thank You :) Here's the [commit](https://github.com/congredi/congredi.rs/commit/952af71e0e795506b6e69118a42e1415d1a554a9) and [build](https://travis-ci.org/congredi/congredi.rs/builds/241941670). Stellar response time :)
Correct, AddAssign can only ever be used to "update" a value, and thus can't return a Result. If you do want to return a result, I suggest have an 'add_assign' function, and not implementing the trait. Working with numbers overflowing is one of the few areas Rust decides to favor ergonomics strictly over complete error handling - when this doesn't fit what you need, I'd suggest just going with methods for these operations instead. One alternative though: maybe implement AddAssign on `Result&lt;Decimal, OverflowError&gt;`? This way you could still use `+=`, and the result would be stored in the result. You can also implement Add, etc. for Result with your own types inside too if you want to, for more ergonomic operator chaining (`(x + y + z)?` looks nicer than `((x + y)? + z)?`).
no, it was a different one, but it was also about making runtime-uniqe types by using lifetimes, im not sure if they used closures for it though.
I'm curious about the same thing. I think what makes games such a nice fit for ECS is that ECS is basically about updating lots of state that interacts in different ways. This is basically what the logic in a game needs to do. I can't name any use cases outside of games that have the same use case of many different kinds of state that interact and need to be updated all the time, but I would love to hear about some :)
I'm planning on cracking on with the development of a library I've been working on that generates a colour palette from an image: https://github.com/elliotekj/distil. Most aspects of if still need work but the actual palettes it's currently yielding are pretty good I think. The above depends on my [delta_e](https://crates.io/crates/delta_e) crate which needs some work too. I'm planning on removing the `Lab` dependency and having it take a `&amp;[f32; 3]` instead. Its test suite also needs improving. As I'm here, I've a quick question in relation to the `delta_e` crate. I hope this makes some sense: Rust favours snake_case in variable names, but what about when you're calculating the value of some number `T`? The compiler doesn't like `let T = …` so I went with `let upcase_t = …`. Is there a naming convention I should be made aware of for this type of mathematical declaration?
thanks for more ideas. see my update, since I learned more about what it can actually do (the other syntax for bounding associated types, and how to label the subexpressions), I've cleaned it up a lot, it's nowhere near as bad as I thought. &gt; trait VectorField&lt;Scalar&gt; : If i've understood right this definitely helps in the case where the intermediates are the same type (which to be fair would be for 99% of users), but I was keeping it open for fluid intermediate types (dimension checking, different precision of fixed points , whatever). &gt; something that could get used in an inner loop (you could consider marking it #[inline] as well) indeed. I did kind of like keeping the symmetry open, that 'lerp' could be applied to heavier objects ('interpolate 2 animation states'?) but that has some downsides (T,T are always compatible, but requesting a blend of 2 AnimState's might fail if they are topologically different objects) I see I should stick with '.clones' rather than ':Copy' to keep these kind of options open perhaps. (redundant clone on POD will just compile out,right) I might like to group 'lerp' and 'invlerp' in an 'Interpolate trait', maybe that could make a shared type for the 'blend-factor F' fn lerp(T,T,F)-&gt;T fn inv_lerp(T,T,T)-&gt;F and even share the 'Diff,Prod' types in inner helpers.. add_scaled(T,Diff,F)-&gt;T some people write lerp(x,x0,y0,x1,y1)-&gt;'blended y value' .. one could go further distinguishing the X and Y's there &gt;&gt;"In short, this kind of generic programming with Rust is possible, but awkward," What I can see is that actually bounding the output -&gt;T rather than writing auto lerp(..){..} will help when it comes to 2way type inference which I do like a lot. going back to C++ I do find subtle situations where I start missing that.
I think ECS concepts are important in designing DBs, but for business applications you'll generally have your ECS be SQL An ECS in SQL is represented by preferring to have something like a worksheet be mostly an ID &amp; then having various panels refer to it, rather than shoving all the panel data into the worksheet row, or having the worksheet row reference this. Example: applicant. I'm currently working with a system that has Debtor &amp; Co-Debtor fields. We're going to want to be expanding to having arbitrary numbers of applicants. This would be easy if we had instead made Debtors be a component which reference the worksheet, but we have the worksheet referencing the debtors, so we either need to rework the data into as I described, or hack in fields CoDebtor2/CoDebtor3/CoDebtor4/CoDebtor5
If you think this should be handled by reqwest, you would want to file an issue and see what the maintainers say about that. Definitely better to sound the waters before committing to a big patch and have it rejected :)
It's an interesting crate. Not sure if it helps but will take a careful look.
You are right. reqwest already handles some content-encodings transparently, such as gzip. From looking at their issues I get the idea that they simply have not managed to handle all possible content-encodings yet. I also looked at hyper's issues. Actually [looking at it again](https://github.com/hyperium/hyper/issues/2), I realize that it is not so clear (at least from this issue), whether hyper's maintainers feel that decoding of the content should or should not be done directly within hyper. I guess I will need to get in touch with them before I can decide whether work on this issue should be directed at hyper or reqwest. Thanks for pointing that out.
&gt;high performance for real-world applications Any benchmarks ? Is threading really win in a ECS processing ? Would just having something with good memory locality/cache coherency and single threaded beat the overhead of synchronization and stuff ? Somehow I don't see a lot of computation going on inside of ECS - stuff that spits things to ECS and then reads it back - like physics engine, rendering engine - sure - but actual ECS processing ? Also would this work in a single threaded environment (WASM) ?
A hlist for systems and/or components would be great, rather than a typeid-based hashmap. Though, I can imagine that that may cause problems without some kind of `Contains&lt;T&gt;` trait for the systems to be polymorphic over. Honestly as much as I'd like to stop having "unregistered component" panics now that I think about it I have to admit that it would be quite difficult with the type system that we have now.
The idea with the `hlist` for systems is great and already came to my mind, but I didn't have the time to implement it yet. You can, however do the following: scope(|s| { s.spawn(|_| sys_a.run_now()); s.spawn(|_| sys_b.run_now()); }); Yeah, it's not that nice, but you can do it if you want. As for components, yeah, that would be rather impossible at the moment. **EDIT:** I'm not sure if it isn't possible. Actually, I think it is. I'm just not sure how much it affects flexibility and which approach we should take here.
[I am stuck](https://www.reddit.com/r/rust/comments/6e6nwu/accessing_event_loop_via_mutex/diba5c4/), so pretty much nothing :(
One way to solve this would be to have create_texture take &amp;mut Canvas and return (&amp;'a mut Canvas, Texture&lt;'a&gt;), where Texture contains shared reference. Then in draw method compare the addresses and panic if they differ.
[Obligatory](https://www.reddit.com/r/IAmA/comments/1u75hh/i_am_the_guy_with_two_penises_ama/).
The renderer, the scene graph, the simulation and the AI should all be system, otherwise ECS wouldn't make much sense. Or at least "S" (for System) of it. In general, you should make as much as possible a system because you get all the nice stuff like running them in parallel, you get some nice concept replacing inheritance (very flexible composition) and you can even iterate over that in parallel. Another example, if the renderer would do its own thing, then it would have to know about all the transform values. But there are many other parts in a game engine which want to know about that. So it would become pretty hard to manage that. ECS is not a glue; it's the all the data, grouped with entites, and the behavior nicely separated. In fact, I would say a game engine should consist of two parts: a state manager and an ECS. And that's where we're getting to Amethyst 0.5 ;)
Sorry, I haven't encountered this "worksheet" and "panel" terminology before (nor "applicant"), could you elucidate (or give a link)?
In contrast, I might try updating to 0.9 of specs, see how it goes :)
`.collect::&lt;Vec&lt;String&gt;&gt;().join("&amp;")`?
Well, other simulations – of physical systems, of robots and other automata, etc. – sound to me like a good fit, but, well, what are games if not simulations? So I’d suppose ECS are just good for doing (especially real-time) simulations with visualization.
If you take a step back and think about what you're trying to achieve it may help you here more than someone giving you an answer that happens to work in this case. You're asking two sort of conflicting things: firstly, you want a `Subscriber` to borrow a reference to a `Select`; then you want to create a `Subscriber` without first creating a `Select`. Where's the `Subscriber` borrowing `Select` from then? It sounds like what you actually want is to store the real value, and that's the problem you should focus on instead. But I think it's worth trying to understand the problem rather than being annoyed by the compiler, at least, it's helped me much more in the mid-long run. :)
I am not familiar with the use of ECS in game development. But it sounded to me like this might be helpful in developing a GUI library (like Gtk?). "Helpful" as in to use as a core for managing data structures and events. Can anyone comment on this?
Because he is considering to write it in Rust?
Released 0.0.6 version of [rust-rocks](https://github.com/bh1xuw/rust-rocks). Another RocksDB binding. basically works now.
It certainly sounds like a good idea to me.
Thanks for the feedback! I am planning on bringing the final API as close as possible the the File struct in stdlib. Including an OpenOptions-style API for defining TermIOS with documentation for each field (currently hackily pulled from manpages and GNU documentation). The Terminal struct actually wraps the std File class in an attempt to leverage all the work already done there (cross-platform CLOEXEC etc). 
From the site: &gt; you would get [Rust stickers] for free by DMing @qedunham on Twitter with a mailing address. If you want to cancel your pledge, you can do it via your Kickstarter account.
Absolutely don't want to cancel. I tried DMing that fellow, but I guess I'm too impatient :P
`.collect::&lt;Vec&lt;_&gt;&gt;()` it can probably infer the `String` part
I want to learn to work with futures, so I've started work on an MPD library that uses them. Can't say tokio's documentation on writing clients is too great. I'm currently stuck on reading the "OK MPD 0.x.y" string that's sent on connect. ...or rather, I've written an ugly hack that reads two lines the very first time. I'll get it right eventually, though.
The `::&lt;&gt;` is sometimes called a turbofish!
LOL! :-D
I would like to point out that freedom of speech is a right that protects your speech from being censored by the *government*. The 1st amendment has no power to stop a moderator from banning you or deleting your posts on a subreddit, nor does it have any power to stop people from downvoting your posts. If you care so much about freedom of speech, you should consider learning what it actually means.
Can't figure out how to interact with this page on my phone
If I remember the terminology correctly, what you are looking at is *branding*.
So my nix-rust PR should be merged hopefully by the end of today or tomorrow. Pretty sure everything is covered satisfactorily. And my main project cargo-tarpaulin now works a lot better. One last tricksy issue with thread/process IDs in unix but my results are pretty close to what kcov generates for projects of moderate complexity. Hoping to fix the last issue then roll out a more dramatic release by the end of next week
It might be more on-topic on the new r/rustjerk subreddit, but it did made me chuckle :D
it's a slideshow that shows the google maps directions from Perl, Germany to Rust, Germany
me too, works just fine. weird.
wtf did I just read
I added [an example to Claxon](https://github.com/ruuda/claxon/blob/0c53140262825dea1a781c7ff85f6338c17bf9a3/examples/decode_mp4.rs) that shows how to decode FLAC audio embedded in an mp4 container using [`mp4parse`](https://github.com/mozilla/mp4parse-rust).
I especially love the warning "This route crosses through France". Here be baguettes!
That's because as if by magic, it feels like every single slideshow posted to this subreddit does not respond to the down arrow, scrolling down, and sometimes not even to sliding up on a touch device.
This one, along with most slideshows I've encountered (here and elsewhere) uses space bar or right arrow to advance. So maybe swiping left/right would work on mobile?
He's looking for a compile time check. Runtime is easy.
Ah, that clears doubts I had about `SendError`. Thanks!
Well Texture instances could have RefCell reference to Canvas, and draw() on Texture then does a borrow_mut. Then it would not be an issue unless somehow Textures were transferred between threads. But I doubt SDL is that thread safe anyways. :) 
My bragging about how I effortlessly understand Spanish is now immortalized.
I've attempted to separate the layout and the rendering as much as I could. Its currently two crates: `stylish` and `stylish_webrender`. `stylish` handles computing the position of elements using a layout engine (defaulting to absolute but this can be changed per an element) and collecting the style attributes. The attributes themselves have no meaning (excluding the `layout` one that selects the layout engine) the rest are passed to the renderer (`stylish_webrender`). This is all runtime instead of compile time mind you, which is something to look into. Currently the code is just thrown together to get an idea what would be required for this. 
7pm PDT happens when this comment is 7 hours and 18 minutes old. You can find the live countdown here: https://countle.com/2566RrRv --- I'm a bot, if you found my service helpful please consider upvoting. If you want to send feedback, please send a PM or reply below.
I must say errno is one of those crazy corners in C that I could never understand. Kernel syscalls generally gives the error code nicely in the return value and then libc takes that and stuffs it an a wacky weird errno global and returns -1 (throwing the valuable information essentially away), just so that the application can do the reverse dance to get the actual error value. That makes no sense! Why not just return the error code directly like kernel does?
Have you tried [cross](https://github.com/japaric/cross)?
At work, I just ordered a set of development boards for an ST [STM32F767](http://www.st.com/content/st_com/en/products/microcontrollers/stm32-32-bit-arm-cortex-mcus/stm32f7-series/stm32f7x7.html?querycriteria=productId=LN1998) MCU for the development of a completely new product. Stay tuned in a couple of months! On a private, Rust, level: I placed an order for an ST [STM32F3DISCOVERY](http://www.st.com/content/st_com/en/products/evaluation-tools/product-evaluation-tools/mcu-eval-tools/stm32-mcu-eval-tools/stm32-mcu-discovery-kits/stm32f3discovery.html) to match the embedded quickstart @japaric has written at http://blog.japaric.io/quickstart/ . And I just received a box of some [LoRaWAN](http://www.st.com/en/evaluation-tools/i-nucleo-lrwan1.html) and [BLE](http://www.st.com/content/st_com/en/products/ecosystems/stm32-open-development-environment/stm32-nucleo-expansion-boards/stm32-ode-connect-hw/x-nucleo-idb05a1.html) boards for STM [Nucleo-L073RZ](http://www.st.com/content/st_com/en/products/evaluation-tools/product-evaluation-tools/mcu-eval-tools/stm32-mcu-eval-tools/stm32-mcu-nucleo/nucleo-l073rz.html)... So much toys, so little time ...
I don't have the time to take a look at this now, but this sounds great. I know there are a couple qml libraries for rust already and iirc the main problem I saw was that they are incomplete or not maintained. Qt is a very generalist framework now, so there may not be a perfect way of integrating it.
Perfect time to release [my new crate](https://docs.rs/rofl/0.0.1/rofl/) it seems!
Just wanted to point out the [`nix` crate](https://github.com/nix-rust/nix) which aims to provide a safe but thin API on top of `libc` to remove some of these trickier corner cases. I encourage you to check for functionality there if it doesn't exist in the stdlib and before you look into the `libc` crate.
crossing France, without fear!
Doesn't some dependency use C for crypto? Last I heard rust still had problems for building constant-time crypto.
Are there any plans to make lifetimes and borrows more smarter/friendly? My code is ugly because of all the hacks I have to do in order to satisfy the checker... ex: method that returns a slice is restricting all the other fields until the slice is out of scope, so I need to return a Range and its ugly...
Existential types would solve this problem nicely in Haskell, so `impl Trait` was my go-to in Rust. I was surprised to learn that, in Rust, a function that returns `impl Trait` is assumed to always return the same type. I can't really imagine a situation where that assumption is useful, and it prevents using `impl Trait` to solve this problem. I hope they change that eventually.
##Database normalization Database Normalization, or simply normalization, is the process of organizing the columns (attributes) and tables (relations) of a relational database to reduce data redundancy and improve data integrity. Normalization is also the process of simplifying the design of a database so that it achieves the optimum structure. It reduces and eliminates redundant data. In normalization, data integrity is assured. It was first proposed by Dr. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.2
Yeah, thrussh uses [*ring*](https://github.com/briansmith/ring) for crypto, which is a mix of Rust, C, and assembly.
Could you share some of your Rust code? the [Playground](https://play.rust-lang.org/) is a good way to collaborate on a small sample of Rust code. This is a literal Rust implementation of what you wrote in Python: https://is.gd/lk9Cp2 More idiomatically, you could have a "new" method on the Object: https://is.gd/N779V1 If you have any questions, let me know!
&gt; I have a good sized Vector (about 50,000 entries), so copying that everytime would be...bad. Note that the only time the data will be copied is if you call .clone.
also, I highly recommend the ownership section of the Second Edition of the Rust Book. [\(part 1\)](https://doc.rust-lang.org/beta/book/second-edition/ch04-01-what-is-ownership.html) [\(part 2\)](https://doc.rust-lang.org/beta/book/second-edition/ch04-02-references-and-borrowing.html) [\(part 3\)](https://doc.rust-lang.org/beta/book/second-edition/ch04-03-slices.html)
The article highlights the benefits but fails to mention the costs because of those abstractions. It would be good to see a comparison of performance of using rust comparing to raw libc calls.
&gt; I think You thought spot on
https://is.gd/gMGt42 my_obj just needed the `mut` keyword to mark it as mutable!
It does not (yet) have a password generation function at all. It also isn't implementing any crypto, relying on Keybase for all encryption / decryption (this is the purpose of using Keybase FS).
I was having a problem building the project. I've filed an issue here https://github.com/osa1/tiny/issues/6
I think it might be interesting to watch and draw conclusions from. crates.io ecosystem might have to deal with issues like that at some point.
I have the impression that keybase is a whole big application on itself. I wouldn't want to use that as a dependency of password app. I generate my passwords using `pwgen` in linux for each website and store it in a textfile inside my `.ssh` folder. If they get access to my files in `.ssh` I assumed they have already acccess to my private keys, so my social apps passwords is the least of my worries.
If you are looking for an X11 window manager that is similar to XMonad but written in Rust, there is [wtftw](https://github.com/Kintaro/wtftw)
It depends on how easy it is to resolve "latest version is broken" problems and whether crates.io does silly things like setting the modified time on files to the past.
Just a comment: there's no guarantee that this will be stabilized with the same API as the unstable feature (that's the point of having an unstable feature, after all). Future proofing your unstable code so it works when the feature gets stabilized could introduce subtle bugs.
Yeah, it relies on `for&lt;'a&gt;` bounds which only, AFAIK, work in the required way with a closures. (Also, another way to violate the always-different-type invariant with the empty-closure trick is using top-level functions, e.g. `Canvas::new(main)` will always have the same time.)
NO MEMES YOU BLEW IT
As an aside, you could also use the [join()](https://docs.rs/itertools/0.6.0/itertools/trait.Itertools.html#method.join) method from itertools, which takes you directly from an iterator to a string without the collect.
This is a false dichotomy. It's not "fake". &gt; focus on what is said rather than how This assumes that when you don't care about the "how", the only thing left is the "what". That's not how it works, though. There's that old saying &gt; "People who are brutally honest generally enjoy the brutality more than the honesty." I mean, do whatever over there. It's a different subreddit. Feel free to make "/r/rustforjerks" or whatever too; there can be a plurality of communities.
slices have `.split_at(_)` and `.split_at_must(_)` – so sometimes one can structure the code to return a borrow to both the desired slice and the rest. Similarly, one can sometimes structure borrows to return the desired field and a ref to a substructure with the rest of the fields.
A byte has two hex digits. We old hands call an entity with one hex digit a 'nibble'.
I'm so tired of socialjerking. [The last thread has barely even had time to cool on the sill.](https://www.reddit.com/r/rust/comments/6ewjt5/question_about_rusts_odd_code_of_conduct/) No more.
I won't go into the 'fake politeness' discussion – Steve has already brought up what's wrong with it. So if this new subreddit siphons off all the trolls and jerks from here, I as a mod am in favor. That said, I'm doubtful this will work.
&gt; 0xFFFFFFFF (8 bytes == 8 hex digits) A byte is two hex digits. What you showed there is 4 bytes. u64 can handle 2 pow 64 different numbers or from the number 0 up to the number (2 pow 64) - 1, hence the name. you also have https://doc.rust-lang.org/std/primitive.u128.html on nightly for bigger numbers please take a look at this create https://crates.io/crates/bigint
I just got called a "shithead" on IRC for paraphrasing (without agreeing with) reasons why other people have objected to the CoC in the past
https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines
Thanks, responded.
~~2^8 / 16 = 2~~ log(2^8 ) / log(16) = 2
No. A byte has 8 bit. With 8 bit you can represent 256 different values. 0-255 (0 to 2 pow 8 - 1). With 4 bit "a nibble" you can represent 16 different numbers. 0 to 15 or x0 to xF. With two Hex digits you can count 16 times to 16. That equals 256 that incidentally is the number of different values representable with 8 bit. 
No. A hex digit is made up of four bits. `f == 2^4 - 1 == 2 * 2 * 2 * 2 - 1 == 15`. Also, 0 == 0000b 1 == 0001b 2 == 0010b 4 == 0100b 8 == 1000b F == 15== 8 + 4 + 2 + 1 ==1111b 16 == 10000b A byte has 256 possible values, which is 8 bits (1111_1111b), so it requires two groups of four bits to express.
For quick reference, [here's the `rayon::iter::ParallelIter::fold` documentation](https://docs.rs/rayon/0.7.1/rayon/iter/trait.ParallelIterator.html#method.fold). The short answer is that Rayon's `fold` produces another `ParallelIterator`, not a single value. If you want to turn a `ParallelIterator` into a single element, use [`reduce`](https://docs.rs/rayon/0.7.1/rayon/iter/trait.ParallelIterator.html#method.reduce). You can also [use both `fold` and `reduce`](https://docs.rs/rayon/0.7.1/rayon/iter/trait.ParallelIterator.html#combining-fold-with-other-operations).
On my machine, the private keys in `.ssh` are encrypted. So *if* an attacker did manage access, all they'd get is the encrypted blob, not the private key. (Same with my password database: they'd get the encrypted blob.) (And like u/malachias hints at, I store my password database not in `.ssh`, since they're not related to the operation of SSH.)
If you happen to be German, watch this video: https://www.youtube.com/watch?v=1ND79YMDV54 This is how I learned lifetimes, it explains stuff better than the documentation, imho. Otherwise, this chapter explains it well, especially what lifetimes are elided in a function declaration (this can sometimes bite you), etc.: https://rustbyexample.com/scope/lifetime.html
wow, I actually kinda' speak German, so thanks! :D
The best way to put it is that Rust follows a multiple-readers XOR single-writer model for shared data, which is implemented via ownership and borrowing. You can "pass ownership" to a function, by having that function take your complex data structure "by value", as in func(data). However, this will normally result in the destructor of *data* being run at the end of func - in any case, the caller has "lost ownership" of *data* entirely, and thereafter can't do anything with it. (I suppose you *could* return *data* explicitly to keep it going, like in a threaded-state pattern, but see below for a better way of doing this!). You can have a function "immutably borrow" a value, by passing it "by reference", i.e. func(&amp;data). In that case, neither the original owner nor borrower can modify *data* while the borrowing is active, but reading is allowed from either. Finally, a function can "mutably borrow" via the syntax func(&amp;mut data). This means that func is allowed to read or scribble over *data*, but the original caller can't do anything that involves *data* while the borrowing is active, not even read it. As the borrowing ends, *data* is kept alive and the caller can once again read or write it. Obviously these rules are consistent, e.g. a mutable borrower can pass on the borrowing or create immutable borrows, and so on. Hope that helps!
Isn't it a "nybble"?
I think the abstractions are compiled away in opt &gt; 0 . Rust has this idea of "zero-cost abstractions" and nothing that I can see in these source examples seem unabstractable. But maybe I'm wrong. 
&gt; I understand why the borrow checker is complaining. I'm borrowing a mutable reference to give to mutator_func() so it makes sense that it wouldn't want me to give another one to instantiate_object(). Actually, the borrow ends at the end the `mutator_func`, so you're free to give the ownership of `mydata` to the `instantiate_object` function (which itself gives it to the `SomeObject` instance it creates). The problem you face here is just that the `my_obj` variable in your `main` function isn't mutable, but the `store` function wants to mutably borrow the object. I find the compiler error quite helpful in this case : &gt; rustc 1.18.0 (03fc9d622 2017-06-06) error: cannot borrow immutable local variable `my_obj` as mutable --&gt; &lt;anon&gt;:22:5 | 18 | let my_obj = instantiate_object(mydata); | ------ consider changing this to `mut my_obj` ... 22 | my_obj.store(1, "test") | ^^^^^^ cannot borrow mutably Maybe you were overthinking this a bit :)
Thanks!
Bit of a typo there: 2^32 = 4294967296.
No, the whole point of rust-cpython is access in both ways. Work with python from rust, and work with rust from python. For example, it has bunch of macroses to simplify definitions of functions and classes on rust side that you can use from python in python way, like `from x import y` to get access to functions.
Whoops, you are of course right. The correct math to calculate the number of digits should be log(2^8 ) / log(16).
Now, if you pass it by value, it will be moved. Passing by value in Rust does not work like in C++.
Ah, I don't use spreadsheets very often (as you can guess), thanks!
&gt; This also can't be solved by something `decltype([]{})` since you can't have lambdas in unevaluated contexts. Not [yet](https://github.com/ldionne/wg21/blob/4bdcc20a8887438cc2d5f7eb38dccfea0e517342/generated/D0315R2.pdf), hopefully...
A meme about a non-existent meme?.. Just how much meta is *too* meta on reddit?
I wonder if people know about cargo check, which has mostly solved the slow build time issue for me. Most of the time I don't actually need to build my project. Cargo check, which runs very quickly, is all I need.
While I understand the point of view of the Author I think that porting libs to Rust would benefit the ecosystem in the long run. To be clear, I am not advocating to tell people to rewrite their C/C++ library into Rust because unless they live under a rock they heard about Rust. And if they are not rewriting their libs in Rust that is probably because they don't want too so there is no need to bother them with that. But still, I think having pure idiomatic Rust libs would benefit the developers in the long run. Otherwise in cases where I want to experiments with the internals of a C/C++ with a Rust wrapper I might need to learn C/C++ and maybe get familiar with another build system. Staying in the boundary of a single language seems better.
well, thanks! awesome!
&gt; This isn't so much of a hit against Rust compared to C++ (which also has slow build times), but hearing about e.g. Go and D's build times makes me really jealous. And the language that Jonathan Blow is developing seems to just blow things out of the water. Sure, but most of those things seem to be on LLVM side, so I don't think Rust devs, can change much. AFAIK, JAI basically just compiles to C and doesn't attempt to do as much type checking as Rust, so it's going to always be faster, by design.
Meta memes? Why not rewrite it in rust? I hear it's great for bare-meta programming! (I'll show myself out…)
I don't know much about Windows-specific stuff, sorry. I'd recommend asking this question in the [gtk-rs gitter channel](https://gitter.im/gtk-rs/gtk).
I don't have the link, but if my memory serves me, it's around 75% of compilation time.
There isn't really one. You might then ask: &gt; Why use it then? It's a great way to implement a trait (like TryFrom or Serde's serializers) which require you to return a `Result` object, yet tell the compiler and the consumer that this particular implementation will never error. The advantage over `Result&lt;T,()&gt;` is that the compiler can completely elide the impossible branches at monomorphization time.
EDIT: Best I could find it this: https://www.reddit.com/r/rust/comments/4vbmv4/can_we_talk_about_build_times/ It spends around 50-60% in LLVM pass. I think MIR was speed up afterwards, so now it's more time percentually in LLVM.
Ah, okay. That makes sense. The borrow ended so it went back to main(). Is there's way to do something like this, in which you have multiple references with the same lifetime to the same Vector? https://is.gd/f2OR5t In the event that you have a large dataset that multiple threads need access to, this would be how I would approach it in other languages (with different struct types, but this is a simple example). I'm curious what the Rust way is to do that. 
[removed]
I understand that, I guess what I'm struggling with is what to do when you *need* to have multiple writers to the same data. Is that just something that *must* go inside unsafe{}? A multithreaded bit of code loses ordering guarantees, so mutable references can't have very reliable lifetimes at compile time (I think). 
[removed]
One person doesn't get to decide what is or is not a meme. It's an idea that has permeated through a community, and the article's author is correct. You can't be a meme-denier when everyone knows what the author is talking about.
I believe this is because, on some Linux systems, the Python binary is statically linked to libpython and so `python-config` will give you ldflags to skip resolution of Python symbols (for Python extensions at least) since they will be present at runtime from the Python binary.
That makes sense. It would also help my primary use (through vim), because this way, the buffer doesn't have to be saved to upload it.
Yes, 2\*\*8 = 256. Think about it this way: If you had one bit, it can only be 0 or 1. So it has only 2 possible values. What if you had two bits? Then each of them can be either 0 or 1, so each bit has two possible values. Therefore, both bits together have 2\*2=2\*\*2=4 possible values (i.e., 00, 01, 10, 11). What about three bits? Then, same as before, each bit can be either 0 or 1, so we have 2\*2\*2=2\*\*3=8 possible values total (i.e., 000, 001, 010, 011, 100, 101, 110, 111). Now, what about `n` bits? Each bit can be either 0 or 1 again, so we have 2\*\*n possible values total. This is why an 8-bit number can represent 2\*\*8 different values, and a 32 bit number can represent 2\*\*32 different values, etc. You also asked what the values are. We just saw that the different values are just the different possible bit patterns - like 00, 01, 10, 11, for example (with 2 bits). Now, what does each pattern mean? What do they actually represent? That depends on the context they're being used in. For example, an integer can be signed or unsigned. In a 2 bit unsigned integer, 11 means 3, while for a signed integer (with the usual interpretation), 11 means -1. 
Well, if the compiler can't find a method, it can't tell you where to look, since it *could* be coming from almost anywhere. As for fields specifically, they can *only* come from the `struct` itself, so in that case you *can* just grep for it. If it's in another crate, then that source is buried in a cache directory somewhere, so it's not as though you're going to have that to hand. What makes more sense to me would be to integrate the compiler error messages with Cargo. A simple way would be to make `cargo doc` accept a fully-qualified type name. That way, the compiler could suggest something like: error: no field `b` on type `m::X` note: use `cargo doc --open m::X` to see this type's documentation. I can't imagine it would take *too* much more effort to have `cargo doc` generate an index (as JSON) to make direct lookups work. IDEs and editors could possibly parse the message and convert it into a link. Maybe add something like `cargo doc --locate` or something to help them find where the HTML files are.
This would be more in line with "existential types", as in: the type is already chosen for you (it exists), but you don't know what it is, so you create a new type for it each time so you don't accidentally make a bad assumption. As opposed to "universal types" where you know anything can work, and are free to choose it to be what you want. "Dependent types" would be more like fn new_vector(n: Int) -&gt; Vec&lt;n&gt; if such a thing were possible in rust.
I have a collection of structs in a `Vec`, wrapped in a struct. They're all the same type, and that type has a trait to turn the members of that struct into a `Vec`. The values in that `Vec` are of type `DataValue`, which is basically an enum that wraps `String`, `f64`, and `i64`. i.e., enum DataValue { Int(i64), Float(f64), Text(String) } trait Instance { fn target(&amp;self) -&gt; DataValue; fn attributes(&amp;self) -&gt; Vec&lt;DataValue&gt;; } struct Dataset&lt;T:Instance&gt; { items: Vec&lt;T&gt; } Now, I can map over the `items` in the `Dataset` to get the attribute vectors. But I also want to get *column* vectors. It's relatively easy to get those as a `Vec&lt;DataValue&gt;`, but I would like to be able to guarantee that they're *actually* a `Vec&lt;DataValue::&lt;Float&gt;&gt;` (for example) -- like, I can promise that they'll all be the same type from the enum. Can I do that?
Correct.
That depends on what you're trying to accomplish with your multiple references. For example, if you intend to mutate completely disjoint sections of the `Vec` in parallel, there are methods like `Vec::split_at_mut` and `Vec::chunks_mut` which return guaranteed-to-not-overlap mutable slices that you can then send off to your threads and operate on without any kind of locking primitives.
Yep. Cargo check for dev.
&gt; Is there any comparison of how much time is spend in rustc and llvm (plus linker)? run the compiler with `-Z time-passes` and you can see exactly how much goes into what step.
&gt; I can't imagine it would take too much more effort to have cargo doc generate an index (as JSON) to make direct lookups work. I'd imagine you could re-use the search index for this.
&gt; It's going to need all this available for IDE 'jump-to-def' to work. This already does work with the RLS.
&gt; Has anyone else build diesel on Windows? With VS2017? I've only done it with VS 2015 since the 2017 support for Rust is so flaky. If you post your repository, I can give it a try on my machine. &gt; influence of Windows but I would certainly hope that rust wants to attract Windows developers too we absolutely do; I'm on the core team and use Windows for my daily work.
Author, here. Yes, I absolutely recommend cargo check for getting things to build, seeing warnings, etc. But it doesn't do much good when you're tweaking code to see how it affects program behavior, which is also a really common thing that people do. I certainly do it all the time working on Psychopath. And you really start to feel those build times then.
The code you just linked to is not having an error because of lifetimes. The function `instantiate_object` does not take a reference to a `Vec`. It takes ownership of a `Vec` parameter. As soon as you call `instantiate_object` on a `Vec`, the variable is marked as invalid in the function that called `instantiate_object`, which means `mydata` is invalid by the time you try to call `instantiate_object` on the second line. Ownership is what determines who is responsible for destroying an object. The function `instantiate_object` *takes ownership* of the `Vec`. It says "hey, I'll destroy this." So, main hands it over and is done with it. Now, `instantiate_object` takes the `Vec` and stores it inside of a `SomeObject` and hands it back to main, transferring ownership of the `SomeObject` to main. It just so happens that the `Vec` inside `SomeObject` is the same value that used to be known as `mydata`. At the end of main, main now has responsibility for destroying `SomeObject`, and inside `SomeObject`, it will find a `Vec` that it needs to destroy -- it doesn't care that it used to be called `mydata`. If you want to let two `SomeObject` variables have a reference to the same `Vec` in memory, you will need to store *immutable references* to a Vec, which is quite possible. However, you cannot have more than one mutable reference to any piece of memory! This is one of the fundamental safety guarantees of Rust. You could store a mutable reference to a different, non-overlapping slice of the `Vec` in each of your `SomeObject` variables, which would let you modify the existing elements in that slice, but there's no way to _insert_ additional values into a slice. A slice is a chunk of memory with a specific size. If you wanted to add an additional value, you would have to allocate a bigger chunk of memory, copy all of the existing values into that one, and then stick the new value at the end. This is just how memory works on a computer, and this is one of the hard lessons programmers learn in C. A `Vec` amortizes this reallocation cost. Whenever you insert an element into a `Vec`, it checks if it has extra room for it. If not, it will double the size of the memory slice it uses to store data, then stick the new element in there. This makes it so that _most_ of the time, there is no reallocation needed, but a simple slice can't be reallocated, and defintely can't amortize reallocation. If you need to add additional elements in parallel, you're going to need separate `Vec` objects in each `SomeObject`, which means completely isolated.. you can't just use `mydata` twice in a row, unless you want to `.clone()` it the first time. These will not be referencing the same chunk of memory. You could use synchronization primitives to manually synchronize access to a shared Vec object, but this is super advanced stuff, in any language. In unsafe languages like C or C++, it is easily possible to hand out mutable references to overlapping chunks of memory, but there's absolutely no way to add additional elements to those references. In safe languages like C# or Java, it's entirely possible to hand the same dynamically sized array to multiple threads and have each of them pushing new elements onto the end, but this is a race condition by definition, and it may even cause your program to crash or lose data in the array if the array type is not thread safe. Rust is not making things harder than they actually are, it's just forcing you to choose which tradeoffs you want. You're going to need to be more specific about what you _actually_ want to do, at this point. I recommend [Rayon](https://github.com/nikomatsakis/rayon) for easy data parallelism in cases where you can refrain from adding arbitrary new elements to your data set, only mutating existing ones.
There are things that could be done in the compiler to give llvm less work to do. Sure, rustc is not going to compile faster than JAI or D that were designed with fast compilation in mind, but it would be great to not be two orders of magnitude behind.
&gt; promise that they'll all be the same type from the enum That's the kicker: they're not actually different types. They're different variants of the single `enum` type. So, no, you cannot do that directly until something like [this RFC](https://github.com/rust-lang/rfcs/pull/1450) gets revisited. You could introduce a new type like `struct FloatValue(f64)` and return `Vec&lt;FloatValue&gt;`. Or you could do it dynamically with something like `trait IsFloat`, return `Vec&lt;&amp;IsFloat&gt;` and ensure that its implementation panics on the wrong `enum` variants.
I think porting libraries to (or better, writing rustic alternatives in) Rust can absolutely make sense in many cases. I'm considering taking that approach for e.g. Open Image IO, for example. But there are also cases where the libraries themselves act as a kind of standard, and trying to reliably replicate their full feature set and behavior could be... tricky. I think many of the C++ libraries in vfx--especially those that involve niche file formats--are like that. Though I may be overestimating how much that's actually the case. But, for example, OpenEXR has a bunch of compression modes, some of which are undocumented and essentially defined by the code. It's certainly not impossible to reimplement those in Rust, but I think it's a qualitatively different choice than choosing to implement a library for a formally standardized and documented file format, because e.g. it's hard to be as sure that you've handled all the corner cases in a compatible way.
Sure, so what I'm working on is basically a simple little emulator (like LC-3, if you ever did that in college), and this specific issue relates to how memory is represented to the code running in the CPU . What I would normally do in a language like C is represent memory as a heap allocated array. I could then pass around pointers to the array to the various parts of the emulated hardware (CPU, input controllers, etc) so they can set/get values from the "memory". I used a struct-wrapped Vector to stand in for the heap allocated array, but I'm stuck trying to come up with a way to do this without having multiple mutable references to the memory struct. This is just not something Rust will do without using unsafe{}, but I'm always open to other approaches to do this. Maybe there's some purely Rust way to do this that I'm just not considering.
This is probably more appropriate to r/rustjerk.
Or if the `{slice}` trick seems too subtle, then you can replace this: let (mut now, mut later) = {slice}.split_at_mut(10); with a more explicit move: let old_slice = slice; let (mut now, mut later) = old_slice.split_at_mut(10);
Actually in many cases C++ can even faster than Rust currently, provided the code doesn't suffer from template meta-programing abuse. Because with C++ you can take advantage of binary libraries (which cargo does not support yet), incremental compilation and incremental linking. Also from WWDC 2017 talks, it appears that Swift's incremental compilation keeps on improving.
Good point!
It's a bit of a hack, but you could do something like the imageproc map_colors function does (docs here, you should be able to click through to the source: http://docs.piston.rs/imageproc/imageproc/map/fn.map_colors.html) to flip the R and B channels. Or just use that function directly: let fixed = map_colors(&amp; dodgy, |p| Rgb([p[2], p[1], p[0]])).
This is because the hasher has internal state which it **does not** reset (and can not reset, as `finish()` takes an immutable borrow (`&amp;self`)). So in your example the second hash is of string "abcabc". However, the state itself is just a few numbers, which should be lightweight enough for you not to worry about the overhead of re-creating the hasher. Compiler should optimize that pretty well. https://github.com/rust-lang/rust/blob/1.18.0/src/libcore/hash/sip.rs#L64
are you wanting these hardware subsystems to run in parallel, then, I take it? If they were running sequentially, this would be relatively easy, since you wouldn't have to worry about two subsystems borrowing overlapping regions of memory at the same time. One "safe" solution to this that I would suggest is that you could make the Memory subsystem thread operate on a channel/message queue, coordinating access with other threads. So, if the user pushes button "A", then the controller thread could send a message to Memory letting it know that byte X needs a value of Y now. If the Display is memory mapped, representing a region of memory, you could have the Memory subsystem send messages to the Display whenever bytes in that section of memory are changed, and Display could use those new values to update the screen. Ignoring the part about `fsevent`, the code in the question on [this thread](https://www.reddit.com/r/rust/comments/649b0a/question_about_mpsc_channels_and_enums/) is interesting because it combines MPSC with an Enum for messages that are sent. This pattern is used in a lot of Rust code, but this is the first thing that popped up on Google. If you need a better example, I can probably help out. Similarly, you could have a set of different messages, for updating or retrieving individual bytes of memory, and for updating and retrieving chunks of memory. You might even have a message for subsystems that want to subscribe to changes on a block of memory. Now, this is likely to be slower than just sharing the memory in an unsynchronized fashion, but it could allow you to build a fun, modular architecture. If you want to do unsynchronized memory access, I would suggest just statically allocating a fixed-size array of memory [like this](https://is.gd/HwbjWc). It might be easier to just pass around indices to various functions and subsystems rather than proper slices into this memory, but you can try several things. Since it's an emulator, fixed-size memory makes sense, and makes it pretty easy.
Thank you! I suspected that it doesn't reset, I just wasn't sure why and how much does it cost to create hashers over and over again. You gave quite a comprehensive answer in that regard :)
Maybe as a halfway house it would be possible to convince some libraries to have C++ and Rust bindings out of the box. (given the overlapping domain of these languages), but a possible sticking point is that naming in C++ and Rust gets to be quite different (r.e. overloading, etc) I wondered if there'd be any collective interest in a cut-down name-mangle following the C-like fake OOP convention 'TypeName_methodName(TypeName*, ...)' - Imagine if rust supported that out of the box, e.g. 'impl Foo { extern fn bar(&amp;self) }' maps to a C-callable 'Foo_bar()'
&gt; but it now has its own type checker and both a non-optimizing x86_64 backend and an LLVM backend Really? I don't recall there being a compiler. &gt; As far as I can tell that's because Jonathan Blow has put a lot of effort into making them that way, not because of any real language differences. Right, but you can can make language more friendly for compile times, which is a path Rust, didn't take.
It's the example in the diesel repo https://github.com/diesel-rs/diesel.git I went back to their example code to be sure my code wasn't the issue. The getting started example from diesels own home-page is in their repo as `examples/postgres/getting_started_step_1`
Rust is pretty friendly for compile times already. It has none of C++'s problems with headers or unchecked templates, the grammar goes out of its way not to be undecidable like C or C++, etc. I suspect the least compile-time friendly part is that it requires a lot of inlining to optimize things like iterators and closures, but even that's pretty standard for the languages we're talking about.
Doesn't JAI get a compile-speed boost by having more types 'inbuilt' ... C++ and Rust rely on generic programming to implement dynamic arrays etc, but JAI seems to have more features like that hard-coded in the compiler? he seemed to be hard coding quite a few features into his 'for' loops like a dedicated keyword for removing objects during traversal, which I would have expected to be done with templated high-order-functions/iterator magic elsewhere. (silly thought, I wonder if you could meta-compile .. compile a compiler that knows about your generics.. then compile your program with that..)
what I personally find at the minute is working without an IDE (dot-autocomplete) slows one down, and there are cases where I find the templates and overloading of C++ easier to use.. maybe it's just my approach. rusts type inference is really nice, but I wish they would make a broader compromise with the type inference to leverage it further.. e.g. "public functions require signatures", allowing un-signatured helper code &amp; debug/test code within modules. There are times when you want to extract code from a working function, and at that point figuring out the types/constraints can be hard. I used the example of 'lerp' where you can write a 1 line function that needs 10x as much work figuring out the constraints to make it generic, and the possibilities of computed types coming out of each operator are actually useful
Oops, fixed.
&gt; If you use rust-cpython for call rust code from python your .so|.dll library also not require libpython, at least ldd tell so. The problem is not that anyways. Snaek mostly is an elaborate hack around setuptools to produce wheels that do not link against libpython. rust-cpython does do setuptools. The project to use for that is `setuptools-rust` which does link against libpython.
oh shit that closing keynote, excited to see Joe Duffy speak edit: actually, that is *quite* a lineup, exciting
Regarding parallelizing iterators directly. I will quote a [previous answer](https://www.reddit.com/r/rust/comments/63y146/rayon_07_released_beginning_the_path_towards_10/dfyfnet/) to a similar question: &gt; Check out the `pipeliner` crate. You can use it with iterators directly. Just &gt; beware that order is lost\*. And real OS threads are used behind the scenes. &gt; &gt; \* you can use `enumerate()`/`sort()` to restore order at the end.
help us out: https://github.com/BurntSushi/stdsimd 
Where is it being held? No city anywhere on the page... Oh, I had to click on the big splash icon on the top of the page to be taken to a page that tells me where the con is. Terrible affordance... The twitter link dumps you on the non-index page, and its not obvious from the menu where to click to get to it.
hmmmm yeah. https://github.com/rust-lang/rust/pull/42225 is the PR to add support; it landed a few days ago. You said you had the same result with nightly; is nightly newer than this?
http://rustconf.com/index.html Portland
Sorry, since this was an announcement, it linked to the announcement page. The home page has that info.
Looking forward to Joe Duffy's keynote, *Safe Systems Software and the Future of Computing*. Are they going to be available online afterwards?
In the short term, incremental compilation will help a lot. In the long term, efforts like MIR and Chalk will clean up the compiler's architecture to the point that it can generate better code in less time. Debug builds could potentially use a faster backend like Cretonne. I don't know that `rustc` itself will necessarily ever catch up to D or Go or Jai, but I can definitely see a new Rust compiler pulling that off at some point.
It seems like this should be possible with [frunk's `Generic`](https://github.com/lloydmeta/frunk#generic) (or possibly `LabelledGeneric` so it's not dependent on field ordering), converting to `HList` and prepending the extra fields then converting back, but it's going to be about as much of a pain as implementing the conversion yourself. It's weird that `frunk` doesn't have a macro for calling `h_cons` repeatedly, that would be simple yet extremely useful.
Two days before solar eclipse, which is also in Oregon. Good opportunity to participate in two the most interesting events in 2017!
Wow, congrats!
seems good, will do
To be fair, I only noticed this becuase *I* was about to submit it. :P
Thank you seanmonstar and all of the other contributors for working so hard on this!
It's difficult to imagine any of these (trivial, inlining-friendly) operations comparing to the cost of the IO the libc call ultimately performs.
You're looking for /r/playrust
Yes, my nightly was just updated so it's definitely newer and doesn't resolve these issues. I'll try VS2015 and verify that the issue is VS2017 support and not some other local-configuration quirk.
Create two crates that impl iterators for Serial and Par, and then import each as needed. Define a core crate with the structs import core import par; 
This is a cross post from users.rust-lang.org, since I am new to the community and not sure where most of the conversation happens yet! I'm a game developer who has worked mostly in C++ for the past 15 years, and with some upcoming vacation time I'm starting to look into what rust has to offer. As an initial experimental project I want to try to port a small utility for working with what we simply call 'symbols'. They are 64 bit hashes used in place of strings, whose fixed size makes them easy to embed in structs and fast to compare. Frequently the string is known at compile time, and with C++ constexpr it was possible to compute the hash as a compile time constant. Because the symbols are implemented as 8x8 binary matrices, they support easy concatenation using matrix multiplication - Concat(Symbol("abc"), Symbol("def")) produces the same result as Symbol("abcdef"). They can also strip off prefixes or suffixes by concatenating with the inverse of the hash of the leading or trailing substring: Concat(Symbol("abcdef"), Inverse(Symbol("def"))) gives you back Symbol("abc"). In C++, I was able to achieve a 2x speedup for concatenation by implementing this matrix multiplication with simd intrinsics (specifically mmcmpeq_epi8, mmand_si128, mmxor_si128, and mmshuffle_epi8). I'm hoping that in rust it will be possible to achieve similar performance. From my initial research, it looks like the simd crate cannot be used with stable rust. Are there alternative options for getting access to these lower level intrinsics?
Just so I understand. The `Response` is a `Stream` which provides the response data in chunks as they are ready. Would writing this `Stream` to a file, for example when downloading an image to disk, write the potentially out-of-order chunks in that "order" to disk or does there exist some kind of mechanism to put the chunks in order?
The chunks should always come in order, as they would in synchronous IO. That's TCP's job, and all of the IO here is on top of TCP sockets.
The `Stream` from a `Response` does not yield chunks out of order. As they are received by the TCP stack, they are then yielded in the `Stream` in the same order.
Quick questions 1. If deployed behind nginx, do we benefit from async features? Is there any setting to let nginx know this server can handle more than one request at a time? 2. How to deploy with ssl (without nginx) 
Ah, okay. I took &gt; A Stream will yield its items as they become available as a sign the chunks *could* be yielded "out-of-order". Guess that worry is not there anymore, thanks.
Ah, it sounds like using rayon was a mistake in the first place then, this sounds much more like what I need. (Since I'm doing a tree search in parallel) Thanks for the help.
1. Yes, depending on how your server is written. If requests/response had to block on some other IO, like talking to a database, then that thread would wait for that IO to finish before being to finish the response. If your server uses async IO, then while talking to the database for one request, the server can start to deal with another. I don't believe nginx wil need a change either way. 2. See https://github.com/tokio-rs/tokio-tls for a protocol agnostic TLS piece that can wrap `hyper::server::Http`.
Alrighty, I'll go with nightly for now. Possibly after I get more familiar with the language I can get involved with the effort to stabilize the simd features. My passion is R&amp;D for real time physics simulation, and my experience has been that building on the right numeric and multicore primitives is vital.
npm actually has TWO lockfiles now; shrinkwrap, and a package.lock. The latter is close to what Cargo does.
Rule 4 please.
Looks nice, glad this is stabilized! Will `hyper-tls` be getting a `0.1.0` release soon then, and/or will `hyper-native-tls` be updating to hyper v0.11?
For `hyper-tls`, there isn't an exact plan yet. I've updated it to use the released hyper, for now. For `hyper-native-tls`, I don't know. I suspect the ecosystem will move forward as they have time.
Thanks for the update! Looking forward to having a fully-crates.io stack with hyper and TLS.
I am suggesting that the problem won't get solved with dependent types. Basically the problem is inherent to the difference between static and dynamic land. Basically the type system can make each declaration of a Canvas type different, but it cannot prevent you from making multiple instances of that type. There are a few ways around this, but they all occur in runtime, and they all have some runtime cost. I honestly think this is a problem with the SDL design, which is built on C where it's expected that developers are very careful (and things crash otherwise). Thinking in a Rusty world requires a very heavy rethinking of the whole issue, SDL is a non-trivial library though. I am thinking of a crazy idea, using empty variables whose whole purpose is to allow passing lifetimes without explicit borrowing the whole thing. And make this as pretty as possible.
&gt; Really? I don't recall there being a compiler. Just because it's not public doesn't mean that it doesn't exist.
&gt; But if, on the other hand, you regard FP mostly as a curiosity that seems to be seeping into your favorite imperative language at an increasing rate, Rust can be a good way to gain familiarity with this peculiar beast. I'd say that Rust is about as far into FP as I want to go, because I've always defined my FP limit as "at what point does it become difficult to reason about performance or easy to misunderstand how much work will actually be done?" ...so I generally take as many of the bits which don't make it difficult to reason about performance as possible (eg. First-class functions, map/reduce, sum types, tail-call optimization, etc.) and avoid the rest as much as feasible (eg. lazy evaluation by default, garbage collection, etc.).
[I liked these which showed up recently.](https://www.reddit.com/r/rust/comments/6ezlwa/introduced_rust_to_team_members_share_my_slides/)
Pipeliner requires that any callable you pass to map is `'static`, which mine is not. Do you know if there's any workaround for that?
So, there's nothing that exists to do it with a simple config option or anything. Upgrading protocols is not specific to HTTP, and so we've filed https://github.com/tokio-rs/tokio-proto/issues/138 As it mentions, it'd be possible to make an upgrade wrapper that could take the socket out from the original protocol and spawn a new protocol.
&gt; the type system can make each declaration of a Canvas type different, but it cannot prevent you from making multiple instances of that type But you don't need to prevent multiple instances from being created. In fact you actively do want to create multiple instances. The problem statement is that you should be able to create multiple canvases (of whatever types), and multiple textures from each canvas (of whatever types), but you shouldn't be allowed to send the textures from one canvas to render on another. So you don't want unique canvas types to solve the issue; you want unique texture types where the fully qualified type of the texture value you create from a canvas depends on the instance of the canvas from which it was created. That's precisely what a dependent type is: a type which depends not on another type, but on a value. For instance, Scala has a way you can encode this kind of constraint. trait Canvas { trait Texture type Tex &lt;: Texture def createTexture: Tex } class RealCanvas extends Canvas { class RealTexture extends Texture override type Tex = RealTexture override def createTexture: Tex = new RealTexture } // note the path-dependent type c.Tex def render(c: Canvas)(tex: c.Tex): Unit = println("Rendering " + tex) // same types of canvases val c1 = new RealCanvas val c2 = new RealCanvas // same types of textures val t1 = c1.createTexture val t2 = c2.createTexture // can render textures to their original canvases render(c1)(t1) // works render(c2)(t2) // works // but not to other canvases // render(c1)(t2) // compile error // render(c2)(t1) // compile error So I'd argue dependent types can solve the problem.
No, it is not. The "latest" there, as it says, is from January 2016. I've never verified those claimed benchmarks myself, but I can say that the comparisons I did a few months ago showed them to be perform the same, with hyper of course handling tons of extra connections better (that's what async IO is good for), and in resource-limited machines. In the past month, there's been some significant gains in hyper, but I haven't compared again.
These are nice. I'll probably chop these up with some other slides. I don't actually present for a few more months (though I'm pretty sure dates can move around if I'm ready early) so I'm just at the looking-around phase for this. I need to do some more learning of Rust myself before I present anyway.
Wow, thanks for the comprehensive answer! Those are really helpful to know! :)
I've been doing an implementation of Conway's game of life using the pancurses library. I wanted to have a separate config file to be able to feed to a screen class. I'm trying to use serde to deserialize a toml file. 
Woo! Website needs moar HTTPS, though.
Going to add table of contents to [gutenberg](https://github.com/Keats/gutenberg) and try this fix [this issue](https://github.com/Keats/tera/issues/185) on Tera, which seems to be some kind of recursive `Drop` happening in the parser
There's always something going on in big cities. One time, I went to a Ruby conference in the midwest; turns out it was the same weekend at a ComiCon. The date wasn't made with the eclipse in mind.
Clearly they should reschedule the Eclipse
I would say that it does "close the gap" between the mainstream imperative langs and FP langs. Even if you prefer a strict FP lang instead of Haskell.
Really? No one else? *Fine*, I'll do it... \**ahem*\* But I thought Rust [supported eclipse](https://github.com/RustDT/RustDT)s.
great stuff!
In fact, it turns out you *can* concat `HList`, so you could do something like this: extern crate frunk; #[macro_use] extern crate frunk_core; use frunk::*; use frunk::generic::Generic; #[derive(Deserialize, Generic)] struct ServerConfigToml { /* ... */ } #[derive(Deserialize, Generic)] struct ServerConfig { /* ... */ } let fields = config_toml.into(); let fields = fields + hlist!( salt, password, aws_access_key_id, aws_secret_access_key ); let config = ServerConfig::from(fields); 
To be fair, the last total (not partial or annular) solar eclipse where the path of totality crossed mainland USA was in 1979 (there was one that touched Hawaii in 1991); after the 2017 one, there's one in 2024 and then one in 2045. So it's a bit different than being the same weekend as a ComiCon. :) 
&gt; How would someone use Qt's moc under Rust as well as connect signals to Rust closures? I don't know much about Qt or even what moc is :) But I recall someone on Phoronix forums commenting on moc being a pain point and that it could be replaced with [Verdigris](https://woboq.com/blog/verdigris-qt-without-moc.html), which does something with macros and header compatible files or something? The link explains it better why it's diferent to alternatives.
Really looking forward to start experimenting with this release! (and futures) Typo: "There a lot of great things to say about it,"
Yeah, I actually tried that as well and was disappointed when it didn't compile :( I suspected as much, though, so I might have to give up on doing this abstraction and just copy-paste the code and split it between `RefCell` and `RwLock`.
Sounds cool :) Can you elaborate on what makes Judy arrays different than other data structures? The website says Judy arrays can replace "common data structures, such as arrays, sparse arrays, hash tables, B-trees, binary trees, linear lists, skiplists" which makes it a bit hard to grasp for me; when would it make sense to use Judy/Rudy over other data structures?
Hello everyone, I am still getting used to Rust and I have 3 beginner level questions. Using higher-order functions, how would you create: * Ranges/vectors/arrays? For example [iota](https://dlang.org/library/std/range/iota.html)(0, 10, 2) should return [0, 2, 4, 6, ... 10] * Get each item in the given range? For example iota(5).[each!](https://dlang.org/library/std/algorithm/iteration/each.html)writeln() should print 0 ... 4 in each line * Create an infinite loop? iota(0, INFINITY, 1).take(5).each!writeln() should print 1 ... 5 in each line
Range: let x = 0..2; let y = ..16; let z = 30..; Vectors: let a = vec![1,2,3]; let mut b = Vec::new() // or Vec::with_capacity(...) b.push(37); Arrays: let arr = [0u32; 56]; let arr2 = [1, 2, 3]; Iteration: for i in 0..32 { println!("{}", i); } Infinite: for i in 0.. { } 
The [Wikipedia entry](https://en.wikipedia.org/wiki/Judy_array) has some additional information w/ regard to their usefulness: &gt; Unlike most other key-value stores, Judy arrays use no hashing, leverage compression on their keys (which may be integers or strings), and can efficiently represent sparse data, that is, they may have large ranges of unassigned indices without greatly increasing memory usage or processing time. They are designed to remain efficient even on structures with sizes in the peta-element range, with performance scaling on the order of O(log_256_n). Roughly speaking, Judy arrays are highly optimized 256-ary radix trees. &gt; Judy trees are usually faster than AVL trees, B-trees, hash tables and skip lists because they are highly optimized to maximize usage of the CPU cache. In addition, they require no tree balancing and no hashing algorithm is used. &gt; […] &gt; Judy arrays are designed to minimize the number of expensive cache-line fills from RAM, and so the algorithm contains much complex logic to avoid cache misses as often as possible. Due to these cache optimizations, Judy arrays are fast, especially for very large datasets. On data sets that are sequential or nearly sequential, Judy arrays can even outperform hash tables, since, unlike hash tables, the internal tree structure of Judy arrays maintains the ordering of the keys.
Oh, I see, I missed that. Look at the documentation of the Iterator module and see the available adapters
Yay! Finally a blog with HTTPS!
why not https://aatch.github.io/ramp/ramp/int/struct.Int.html ?
Great to finally see it :) One question though. As far as I understand, starting from this version hyper will no longer offer backwards compatibility with the previous synchronous API. If you prefer to stay in the sync world, it means you have to stay in on 0.10 forever? ;/ This isn't very clear especially since the front page example on hyper.rs is definitely synchronous, although it seems to be using some `service_fn` shim crate which I cannot find anywhere...
Just make a crate and publish it on crates.io. You will see how popular it gets.
Right. I was genuinely curios when/where the LLVM backend was introduced.
There is the [`sid_vec` crate](https://docs.rs/sid_vec/0.1.0/sid_vec/struct.IdVec.html). (I don't know why its `with_capacity` and `resize` take `u16` though.)
Dates, CFP and Location: soon, probably next week.
Ah, that's not an aside, that's the best answer of all! I was chaining `.collect().join()`, and only moved `join` to the bottom because I didn't want to use type ascription and neglected to think of `::&lt;...&gt;` for `collect`. I didn't realise I didn't need `collect` at all! Thanks :) Edit: Spoke too soon. I do need the `collect`, because otherwise `join` sees the wrong type (`(String, String)`) for the contained items.
&gt; Where have you seen anyone embarrassed by IO in Haskell? I once tried to do something with random numbers in Haskell. I didn't believe I was dumb until after a day of trying, I still couldn't do it.
Yeah, it is not backwards compatible. I believe the `reqwest` crate is the spiritual successor of sync `hyper`.
Yeah the gif IIRC I've uploaded to github's comment image thing and they deleted it eventually because it wasn't used in any comments, or something. I've been too lazy to fix it :D I actually don't think it's all that incredible. It just has the advantage of being able to recover your accounts if you lose *all* your stuff (devices, paper backups of crypto keys) at once or something. And the disadvantage of "the attacker only needs to steal your password and "full name" (salt type thing), not a database file like in 1password/keepass/etc.", however — if it's malware it can steal the file as well, if it's a person looking over your shoulder they don't see the string you've used as "full name".
Looking forward to this; one thing I would suggest is some "getting started" examples with common use cases, JSON decoding using streams for instance.
Yeah, this is looking like an inference bug WRT specialization. I can't look into it right now but maybe search the issue tracker for "inference specialization" to see if something like this has already been reported.
Well, the one I update every couple of months also has HTTPS for over one year now, although I do conceded most content is not that relevant for anyone else.
Ah, as usual, he mentioned it in one of the videos IIRC, I forget which one.
this is the big one: you also don't have to trust a third party to securely store and sync your passwords. if your local machine is compromised and you don't know it, there's basically no technology that can protect your passwords, which is where two factor auth comes in, so I don't see that as a disadvantage for master password. But, you never have to worry about losing your last copy of a password database, and you don't have to try to hack some sync mechanism together using an encrypted database file and Dropbox.
`Into&lt;T&gt;` consumes the value given while `ToString` operates on a reference, so you get to keep the value. This is a common naming pattern: *into* is a used for conversions that consume values, *to* for those that keep the original value but may be expensive, and *as* for cheap ones.
I find it quite convenient that I get an excuse to go to portland and stay for the eclipse.
Ah, yep: https://github.com/rust-lang/rust/issues/36262 and https://github.com/rust-lang/rust/issues/40718 :-]
that looks the most similar so far, let me check exactly what it means by saying 'id' instead of index; the fact that they parameterize it as ID,Data rather than &lt;T, Index=usize&gt; suggests slightly more distance in the concept (maybe)
Ooooh that was me a long while ago. It started out as u16 because the Id type was always u16 and after adding the Index trait which abstract away the Id type completely I guess I forgot to do something about the capacity/resize stuff. Also back then there was no associated items so I am not sure how I would have done that properly. I can update the crate if anybody is interested in using it. Also, it's fun that this comes up in the context of a mesh data structure since this was exactly what I made this IdVec container for.
This is sure to see a lot of usage if completed.
You don't have to trust a third party when you use anything that does local encryption (Keepass, 1Password etc.) Freepass actually supports both derived and stored passwords. There is an encrypted database file, but you can recreate the same file and get the same derived passwords again if you lost the last copy.
But in my case (it was a few years ago, so my memories are kind of blurry), I wanted to “extract” the random number from IO to put it somewhere in a computation, and I never succeeded in doing so. And IIRC, I ha to carry over the random generator from one call to the other, and that's where I cowardly dropped the ball and went back... to Rust.
This looks really cool, but it seems they only support using it on the RPI right now (unless I'm missing something)? That's a bummer.
Well, that's just the beginning. We have internal builds for bigger platforms, we are trying to find out what makes sense. Smaller gets tricky: below roughly the Raspberry Pi 3 size, running on-device speech-recognition might be impossible.
No zealotry intended, having used both languages I am of the opinion that they are a poor fit for the roles they have been forced into. It is well known Eich designed JS rather quickly, and it's lack of safety and weird behavior are not up for debate. With rust we tend to think about how bugs in c/c++ have led to many horrifyingly real issues in the real world and we need a better language, I feel the same about JS. Heck I'd rather deal with C's type system over JS's every day.
This comment has significantly less zealotry; I can elaborate if that's helpful.
Thanks a lot! I'm excited to use this data structure, anything I can do to help get this across the finish line?
[That `you_eight()` function](https://github.com/rust-lang/rust/pull/42247/files#diff-bc292d18255fb09159b3bde83eb33a58R86) is messing with my mind. Congrats to Rust for keeping the type, variable, module and macro namespaces completely separate? I guess?
It responds to arrow keys, but there's no mouse-interaction or obvious indication that you should start hitting the keyboard.
Yeah implying JS is not a "good" scripting language was probably zealoty, I just feel it is way outside it's niche, and with the web platform being THE platform; well a js only future just feels bad. And then we look at this, and the left-pad incident and so on. Programing as I've found seems like we are holding the world together with patches of bailing wire and bubblegum; and that is using c++ and c#/vb.net, js just makes this worse. I've been looking into dart, it looks like it has a semi-decent type system; and will compile to js which is great; and web assembly may be a good choice as well.
&gt; Yeah implying JS is not a "good" scripting language was probably zealoty It's not just this, but the *way* in which you did. That is, pointing out something's flaws isn't zealotry; this isn't a "no negative comments allowed zone" or something. But there's a difference between &gt; I find that JavaScript's weak typing leads to a lot of errors. and &gt; Fuck JavaScript, weak typing is garbage. even though the factual content is close. &gt; And then we look at this, and the left-pad incident and so on Did you know that that incident caused npm to change their policies so it can't ever happen again? And that it also caused roughly two hours of build failures, and no downtime for any known production sites? &gt; I've been looking into dart Dart seems really neat! I respect some of the team members *immensely*.
The colour theme is poor: `::` in the code blocks is near-invisible, and contrast of code is generally poor.
Performance-wise, Rust and C++ are similar (both fairly fast; LLVM does a lot of the optimisation in the case of Rust and Clang). Safety-wise, Rust is head-and-shoulders above C++ in terms of how many bugs the compiler will catch. Easy-of-use, Rust is still better, except that C++ has more existing tooling. FFI to C is fairly similar (functions must be declared "extern C" in both cases). But I have no clue about WebASM.
&gt;&gt; I can update the crate if anybody is interested in using it. reading what I have wrote, do you think your goal is the same as mine? you've called it an 'IdVec' .. is the intent really just 'a vec with semantically meaningful index'. Would it be possible to flip the order the same just to emphasise how it is just a simple extention of a Vec.. Vec&lt;Data,Id=usize&gt; perhaps.. then it really can just look the same. IdVec&lt;T&gt; just gives the default Id=usize I would use this sort of thing by default, because I know 99% of the time a 32bit index is the best compromise. a single byte array that will fill over 4gb would be a rare example.
Methods borrow the entire struct. The borrow checker doesn't peek into methods, so it sees that you borrowed part of self (self.children) mutable and now are trying to borrow all of self (whether mutably or immutably doesn't matter, since a mutable reference must be the only reference). This is annoying when the method doesn't use the part of the struct that's already borrowed, and the solution to that is "splitting the borrow"[0]. But I suspect that the add_child method probably does touch self.children, so that won't help you here. What you probably want here is something like: if let Some(child) = self.children.iter_mut().find(|child| suffix.as_str().starts_with(child.value.as_str())) { child.build(suffix.clone()); } else { self.add_child(i as 32, suffix.clone()); } Note that this isn't actually the same logic as the original, but not searching all of self.children before adding is wrong, of course. [0] https://doc.rust-lang.org/nomicon/borrow-splitting.html
This wouldn't work in almost any language, because if "self.add_child" modifies "self.children" then eventually you're going to invalidate the iterator that you're using to loop though self.children. The traditional way to get around this in other languages is to have a Vec of the changes you want to make. any time you would make a change to the thing you're iterating over, push it to a Vec instead. Then, after the loop, add everything from the Vec to the thing you were iterating over.
My concern was less about TLS and more about the idiomatic way to have a client and server running on the same event loop. When I tried this recently, I couldn't quite figure out how to get a handle from the server early enough to construct a client with it and pass the client down into the server's `Service` implementation.
Awesome!
Thanks. After all, I had taken out the add_child method outside of the loop because as you said, that's the correct way. I see, however how your approach will help me in similar situations in the future.
This is going to be extra fun for me: I'm driving SLC to PDX on Thursday, then driving PDX to Driggs, ID which is [directly under totality](http://xjubier.free.fr/en/site_pages/solar_eclipses/TSE_2017_GoogleMapFull.html), then back to SLC afterwards. \#rusticroadtrip ---- FWIW though, the Marriot City Center a few blocks from The Nines has rooms for ~$200/night, which is steep but pretty okay for being downtown a few days before the eclipse. Book quickly.
I'll try to cook up a Contributing section to the README soon. Most of the development so far has been rapid development on the core data structure. That will shift when it comes to tuning Rudy's transitions between node types. I have limited experience with tuning data structures, so help there would be good.
Sync hyper is done. 0.10.x could see security or bug fixes, but no more focus. It's possible to make a synchronous API on top of asynchronous, and cheaply. The other way around is not cheap. I imagine some frameworks could pop up offering that. The home page example looks sync, but only because the values are ready immediately. It's still being put into an asynchronous queue to write to the socket when it signals it wouldn't block.
I like how the current key-then-value order is consistent with things like the standard HashMap, but I don't care strongly enough to bikeshed about it if that's really important for you and you want to use it. I would make the key type an Id by default rather than a usize though, since the point of this crate is to have (as you said) semantically meaningful index types (you can always make your own type alias with u32 if that's what you'll use 99% of the time).
That would be awesome, thank you so much!
Except that he immediately leaves the loop afterwards via the break. A sufficiently smart borrow checker could realize that these two borrows don't actually conflict and allow the code as written.
You can make one! The directions could use more photos but the pattern is at http://edunham.net/2016/04/11/plushie_rustacean_pattern.html
I have no degree (or knowledge of category theory) at all and I wrote [Haskell Book](http://haskellbook.com) so I agree with you here.
I want to wish you lots of success. The way we do IOT right now is not safe or resilient in any way. Most contemporary IOT hardware is engineered to be dependent on cloud services, and its also insecure because its software is written in memory unsafe languages, and because it doesn't play in patches regularly. We need a shift in best practices towards running the application on the device without the data being sent to servers without the user's consent, and towards security updates. You shouldn't be locked out from your home or unable to flush the toilet because the internet went down. Sadly though there is an economic advantage in grabbing the user's data without their consent and profiting from it by selling it or by targeting ads. Most users are not aware of the issues of their data being available for sale and compare products based on the monetary price tag, not the price they pay via their data. To them, other things matter like convenience. But in any case, good luck!
You still can worry about destructor leaking insofar as making your API completely foolproof, but it's probably not worth the effort. Our concerns for destructor leaking were *mostly* a matter of rules lawyering.
Another concern when compiling for the web is code size, and the size of the parts of the standard library that you'll end up using. I can't say whether Rust or C++ will give smaller executables, but [here's](https://lifthrasiir.github.io/rustlog/why-is-a-rust-executable-large.html) a great blog post about the possible compiler and linker settings that have a huge effect on the size of the executable. If you end up using Rust, be sure to keep us updated on what is working well and what isn't. I would definitely read it!
&gt;So there's our first bug that You Should Only See In Rust: writing unsafe code, and failing to audit the stdlib's two unsafe autotraits for your types: Send and Sync. Maybe it would be a good idea to *not* autoderive `unsafe` traits when a datatype implements `unsafe` operations.
On that note - what's the best writeup on non-`unsafe` destructor leaks?
It would have been very easy to write a version of MutexGuard that was "safe" because Mutex did all the unsafe work for it.
This RFC makes the language a million times more easy to use! Or at least it probably will, but the `Rendered` link 404's right now.
I agree Haskell has its corners of »types are documentation enough«, which is very unfortunate. I’m in the interested-but-doesnt-really-use-Rust camp, so I don’t know the Rust equivalent – hopefully there is none, but I’ve met too many »self-documenting code is enough« people to believe that. I’d say Haskell is mostly hard because it disallows things that you shouldn’t do anyway, much like Rust, albeit in a completely different domain. Having learned both to a reasonable degree I think they’re both similarly awkward and annoying in the beginning, but if I had wanted to learn new syntax for something I already know I’d have tried Go instead ;-)
&gt; Since the the developers who will work on this don't no neither C++ or Rust, the fact that the Rust compiler helps this much is important. As someone who does web development using Python+JS (mostly via Django), I would also recommend Rust for your WASM work. In fact, once you're more comfortable with it, I predict you'll want to use it for your server-side stuff too, simply because of how much the type system allows you to verify at compile time. That's why I'm using [rust-cpython](https://github.com/dgrunwald/rust-cpython) to migrate server-side bits which can be neatly encapsulated (so I don't have to write the boilerplate to keep bouncing control flow back and forth between languages). For example: 1. Rust's use of monadic error handling and optional returns (`Result&lt;T, E&gt;` and `Option&lt;T&gt;`) rather than exceptions and `null` makes it possible for me to confidently say I'm handling all possible results of calling a function. (Except `panic!`s, which are used to report things which can't just be "handled" because they indicate that the program is in an uncertain state, like attempting to index off the end of an array or receiving a report of detected memory corruption from the kernel. By default, panics unwind the thread they occur in.) 2. While it's not idiomatic Rust, you can use Rust's `enums` to get compile-time verification of patterns I commonly see in JavaScript, like "the first argument may be a URL or a callback which returns a URL". (Rust's enums are tagged unions and `Result&lt;T, E&gt;` and `Option&lt;T&gt;` are examples of them.) In a more idiomatic use case, it allows you to do things like writing a JSON serializer which will verify, at compile time, that all input data is in types JSON can represent. 3. Because Rust allows methods to consume ownership of their parent structs and it's possible to implement a method on a struct, limited to a specific type parameter, it's possible to implement "session types", which is how the [Hyper](https://hyper.rs/) library gets the compiler to say (paraphrased) "ERROR: You tried to call a method on `HTTPRequest&lt;WritingHeaders&gt;` but `.start_body()` invalidated your reference to it." or "ERROR: There's no such method as `HTTPRequest&lt;StreamingBody&gt;::add_header()`. Anything which can be modelled as a finite state machine (visualized as a graph of states, with arrows indicating valid transitions between them) can benefit from this capability. If your use of NPM indicates that you're running Node.js on the server side, the equivalent to rust-cpython would be [Neon](https://www.neon-bindings.com/). (...and it makes it easy to run CPU-bound promises without blocking your Node thread. If you're already receiving enough heavy requests in parallel to match or exceed your number of cores, just fire off a thread, wrap the handle in a promise-style API, and return it to Node. If not, and you want to accelerate things, a high-level threading abstraction like [rayon](https://github.com/nikomatsakis/rayon) makes it trivial to perform multi-core map/reduce.) As for the client side, I haven't needed WASM yet, because I mainly use JS for progressive enhancement which spends 99%+ of its time waiting on the DOM, but, if you try Rust and get hooked on how much it can catch for you, I'd suggest evaluating [TypeScript](http://www.typescriptlang.org/) for future efforts, since it allows you to incrementally augment your JavaScript with a type system that shares some of Rust's features (eg. tagged unions).
The "scope of unsafe" is a bit nebulous. Generally you have to just let it be within the module, but having things depend on how your code is organized is icky. In this case, `MutexGuard` could do zero unsafe stuff and instead let `Mutex` do all of the unsafe heavy lifting when constructing it. I prefer the current situation, which is less magical in this way. The problem with magic is that you end up assuming it works where it doesn't, and here that can be disastrous. This kind of thing is great when "it doesn't work" is a compile error (see: elision, autoderef), so you can assume it works and if you get an error then go the long way around, but in this case that would mean a silent bug. (A lint check makes more sense for stuff like this)
You should really do some tests whether to use WASM at all. This could be the right decision for future proof applications but WASM is not automatically faster! We had a problem at our workplace that needs some more computation than your average Angular.js app. We played around with asm.js and WASM and also implemented a small testcase in javascript. We ended up using javascript as it turns out javascript and typed arrays are pretty fast and we didn't really gained much by using WASM. This is however very dependent on your use case and how this situation develops in the future. But bringing WASM into you build process etc. is an investment by itself. After all we decided to use javascript and typed arrays which was really fast. 
Type safety and compilation guarantees are separate concerns, though as with many things in CS you can use one to implement a lot of the other. The above example comes with various decisions that scala made and the problems that it leads. It also has issues when we add lifetimes. First of all it's easy to abstract a `c.RealTexture` into a `RealCanvas#RealTexture` or just `Texture`. In all these cases we loose information and have to somehow deduce we can downcast back to the instance type. The whole idea is that we want to avoid having to do this homework because it can get very limiting very quickly on such a common resource. The second problem is that this also implies that our `c.Texture` borrows from `c` which is a problem, because again we don't want that (either would prevent others from mutating). It's not a problem in a GC world were we can assume everything exists forever, it is a problem in Rust were this puts constraints not just in what you can do with the texture but also `c`. Moreover it needs exposing the type of `c` is which might be an implementation detail of the bigger renderer (and we might not want to expose that). I am thinking of some workarounds but I haven't got the time to sit down and code it out.
&gt; For instance, if you do the reference-counted cycle leak trick, Rust will correctly determine that **the borrow** can safely expire -- after all no one can ever access the reference that was leaked. What does this sentence mean. What borrow is it talking about?
Aeson, really? I think it’s the most pleasant to use library across all ecosystems I’ve encountered so far, by a huge margin. If you’d like to give it another shot, the folks over at #haskell at Freenode are very welcoming. You can also find me there. &gt; It's a shame IMHO, because Haskell is really a language I'd love to dig, but I currently don't have enough time for that. It's a shame IMHO, because Rust is really a language I'd love to dig, but I currently don't have enough time for that. :-) I’ve got this project that I recommend to beginners whenever they ask me what to write. Actually, I thought about writing about this many times, and you finally made me do it! [Here you go.](https://github.com/quchen/articles/blob/master/fbut.md#what-program-should-i-write)
Ah, so if Drain has unsafe code that is assumed to be safe because the expectation is that the destructor will run, things can get funny?
Nim is compiling great to WASM
Yeah exactly. In this case Drain's destructor was responsible for setting `Vec.len = 0`, so it would know it is now empty. (The more complex version in std today is responsible for backshifting "remaining" elements).
FYI, I've just published `hyper-tls` as v0.1.0. I still don't know if this will be the eventual standard way of doing it, as the tokio ecosystem is evolving, but at least there's something you can use right now.
Despite the promise of reasonably being able to avoid `unsafe` supposedly I find that really you often can't unless there happens to be a crate which did exactly that in which case the crate itself contains the code.
I'm very worried about this thing that just happened to me. I've been working on a little handler to handle human-readable datasets and massage them into machine learning friendly formats, specifically the matrices and vectors required by [rusty-machine](https://github.com/AtheMathmo/rusty-machine). Note that &gt;Rusty-machine uses [rulinalg](https://github.com/AtheMathmo/rulinalg) for its linear algebra back end. so I went through the work to convert collections of handy/easy-to-use structs into the less-wieldy matrices etc. I got through enough to try a little example and I'm seeing this: 271 | model.train(&amp;(dataset.to_attr_matrix()), &amp;dataset.target_vector()); | ^^^^^^^^^^^^^^^^^^^^^^^^ expected struct `rusty_machine::prelude::Vector`, found struct `rulinalg::vector::Vector` Was my idea stupid? Is this not even a thing I can do?? I am very discouraged. It would be unfortunate if this can't generalize to anything that uses `rulinalg` (for example)
I'm not sure if this qualifies as a bug you'd *only* run into in Rust, but it certainly seems more likely in Rust than other memory managed languages, and has bit me several times: ``` some_ffi_function(CString::new(s)?.as_ptr()) ```
`rusty_machine::prelude::Vector` is indeed a re-export of `rulinalg::vector::Vector` so this should work. I suspect that your crate and `rusty_machine` depend on different versions of `rulinalg`.
And the lifetime namespace!
&gt; I don't see why it should imply that at all. We don't want you to keep textures once its canvas is gone, it's useless. Limiting by memory assures that people can guarantee lifetime. The type contains references to an instance, so the type itself cannot exist outside of that instance's life. Intuitively I think it's a very dangerous thing. Basically if types can be members of instances, then they should have the same rules of instances apply, including lifetime. &gt; I see no reason to deduce whether a downcast is safe. If we have a true existential type then we can't easily recognize the difference between two `Texture` one which was `c1.RealTexture` and the other `c2.RealTexture`, the definition of existential type is that the knowledge has been deleted. The reason I used Voldermort type is that the type information isn't deleted, it just can't be "named". But this means that even though you have two `Texture` types they are not compatible because they are actually completely different types (and nothing has been erased, or hidden, only made really hard to name). &gt; I don't see why a dependent type system should necessarily expose that either. It does if the exposed type exposes the instance that created itself. Since we need type info and such we can't just hide this info away so easily.
&gt; Usually when people talk about this they're talking about a garbage-collected language No, even in C++ there are multiple circumstances where the stack is not guaranteed to be unwound (or, indeed, is guaranteed _not_ to be unwound).
&gt; Vecs aren't HashMaps; the indices aren't stored in memory. They very often are. For example petgraph nodes all refer to each other by index. You are being extremely normative about how Rust "should" be used when in fact, it is a language with very wide applicability. &gt; That said though this is a premature optimization. Didn't the OP already say they know they want this from experience? Way to be dismissive.
Yes, and those circumstances are rare and much easier to reason about than the whims of a GC.
But it was evidently even easier to write the unsound version.
Well, speaking for myself: when talking about this I'm absolutely _not_ talking about GC'd languages, where you don't have "destructors" and thus have no expectation of deterministic finalization (at least in any of the GC'd languages I use); I'm indeed talking about languages like C++ where many people have the illusion that RAII is foolproof. As to bypassing stack unwinding usually being intentional, agreed only in the case of asserts – `noexcept` violations and missing catch handlers are anything but intentional IME.
Something I'm a bit torn about. It took me a while to understand it, and now that I do, I feel proud. And now it's for free? :/
Can you share your code?
The C++ standard says it's implementation-defined whether the stack is unwound when `std::terminate` is called due to a missing catch handler or a `noexcept` violation. You cannot "contrive" to alter `std::terminate` to avoid this behavior – it's _how it works_. "Finalizers" do not imply _deterministic_ finalization in any language I've ever seen; "RAII", and "destructors", do. You really think people are confused about the former rather than the latter? Given that you yourself seem confused about `std::terminate`'s guarantees, I wouldn't take that bet. ;-]
[Very early stages, just hacking on it, etc etc, please don't judge too harshly](https://gist.github.com/csreid/a6f632f2503fb5a4051876064ae59920)
I'm pretty sure this is an incredibly classic C++ bug. And in Swift people are always trying to return the pointer from the closure used by this method, even though it's the same thing: https://developer.apple.com/documentation/swift/array/1540400-withunsafebufferpointer
Sounds great :) thanks for the explanation 
Is this a lifetime issue?
What `std::terminate` says on the tin is that it *terminates your process*. There are very few resources in common use that this does not clean up. You can override what actually happens when you call `std::terminate` with [`set_terminate`](http://en.cppreference.com/w/cpp/error/set_terminate). I think acting like destructors and drop are wacky unpredictable things is apt to cause confusion, yes.
*shrug* you know my philosophy on the scope of unsafe well, and I have no interest in going around in circles about it with you today. :)
Note that other RAII languages like C++ and D have the same limitation on destructors. Without a linear type system its near impossible to guarantee resources are cleaned up, especially in the presence of exceptions.
&gt;What `std::terminate` says on the tin is that it terminates your process. There are very few resources in common use that this does not clean up. That's fine, but that's not what we're discussing, which is whether "destructors aren't guaranteed to actually run"; whether the process terminates afterwards is orthogonal. &gt; You can override what actually happens when you call `std::terminate`. You can override the termination behavior, but not the unwinding behavior; here's what's actually relevant (C++14 [except.terminate]/2): &gt; In the situation where no matching handler is found, it is implementation-defined whether or not the stack is unwound before `std::terminate()` is called. In the situation where the search for a handler encounters the outermost block of a function with a _noexcept-specification_ that does not allow the exception, it is implementation-defined whether the stack is unwound, unwound partially, or not unwound at all before `std::terminate()` is called. In all other situations, the stack shall not be unwound before `std::terminate()` is called. An implementation is not permitted to finish stack unwinding prematurely based on a determination that the unwind process will eventually cause a call to `std::terminate()`. Bottom line: destructors aren't guaranteed to actually run. &gt; I think acting like destructors and drop are wacky unpredictable things is apt to cause confusion, yes. I feel like we're having two different conversations... Nobody said wacky or unpredictable, merely "not guaranteed" which is probably more surprising than regards a finalizer.
Which used to be separate from the labels namespace, but at least those are now unified IIRC.
I'm very excited for this! It's going to reduce boilerplate type code a lot. 
The version of rusty-machine that depends on rulinalg 0.4.2 hasn't been released. The latest version on crates.io depends on rulinalg 0.3.7. That fixes this error.
Yay! I'm looking forward to this change. I was originally hesitant, but, similar to lxrec, tried to work out why, and ended up deciding that it wasn't so bad. You might have to adapt your mental model of pattern matching, but it's worth it, I think. One way to think about it is as follows: 1. First, make sure you're thinking of the type of a variable as being data plus ownership. For instance, &amp;i32 and i32 refer to the same kind of data, but own it in different ways. If you forget about the ownership information, there's still meaningful data; it's just not clear how it's stored in memory. 2. Patterns (in the new style, without &amp; and &amp;mut) are only concerned with the data part of a type and have no concern about the ownership part. 3. When matching against a new-style pattern, first the data part of the match is formed, which indicates which binding sites refer to which parts of the data. Then, afterward, ownership is ascribed at the binding points in a maximal sort of way (granting full ownership if possible, a mutable borrow if possible, or finally a shared borrow). That's the general idea. I think the main missing nuance can be clarified in this example: let Some(x) = &amp;Some( &amp;6 ); // x has type &amp;&amp;i32 where the binding point x is bound to the "&amp;6" while in a "shared reference" binding mode, and so x becomes a reference to the &amp;6, hence a &amp;&amp;i32. (You might expect it to be an &amp;i32 at first, but then consider what would happen in the following simple example: let a = 7; let r1 = &amp;a; let r2 = &amp;r1; // should r2 have type &amp;&amp;i32 or &amp;i32? .)
Which is why I'm confused to see people using the kind of language used to warn against improper use of finalizers when discussing deterministic destructors.
You're my hero. Thanks so much for your help!
&gt; The server and client communicate over TCP, is there a nice drop-in solution I can use to encrypt this connection? That's what SSL is for. &gt; The server application uses postgres for its database, is there a preferred way to go about input sanitation in Rust? Database driver will provide an API that will take care of that. &gt; What impact on performance will encrypting my TCP connection have? What exactly are your requirements here?
That last example is exactly why I'm not a fan of this change. Rust is a systems language- it usually *is* important to know precisely whether something is a value or a pointer to a value. IMO, there are already too many places where that syntactic distinction is lost (e.g. the way `Box` and `Vec` interact with `&amp;`). The way to deal with this complexity is not to hide it, which introduces more weird corner cases, but to teach pointers better. :/
Great post! As unsafe code needs to explicitly uphold the system's invariants, failing to do so leads to subtle bugs that manifest elsewhere. Rust is unique in that it's the only language where the type system + various checkers already do most of the work upholding the guarantees. Other languages don't have those guarantees at all or need a runtime to mediate access to resources. Perhaps this should be classified along with e.g. Java sandbox bugs? How great that review, static analysis, validation, fuzzing and other techniques help us find and eliminate those bugs so we can continue enjoying safety &amp; speed. I think the article missed the `slice::Iter:as_slice()` bug that led to the rollout of Rust 1.15.0.1 (IIRC), where due to a copy&amp;paste error a mutable slice was created from an iterator over an immutable one. [Clippy](https://github.com/Manishearth/rust-clippy) has long since gained a lint against such cases. I believe we could add lints for other errors you describe, too. 
If I power off a machine running code that uses destructors or finalizers (or both) those won't run, no language can guarantee that they do in this, and many other cases (this case is just the most absurd case of them all).
The problem I have with this is that it limits my source. I may want to stream my input for example, I might want to read snippets, or I might want to read a *huge* file. First loading the entire thing into memory seams like a bad idea if I don't have to, especially when my only requirement is 'read one character at a time'.
Which documentation?
Is it though? How often do you *really* run into things like `v.push(v.len())`?
Yeah I wrote that documentation, and I'm pretty sure it's outdated. There's been changes to how dropck works since: https://github.com/rust-lang/rfcs/blob/master/text/1327-dropck-param-eyepatch.md I need to take some time to properly review that RFC and update the dropck discussion. But mostly my understanding is that all the defaults are now safe?
Isn't this a special case of nonlexical lifetimes? Wouldn't it then make more sense to get nonlexical lifetimes in general? Would this proposal make this compile? fn process&lt;T: Float + AsPrim&gt;(&amp;mut self, events: &amp;api::Events, buffer: &amp;mut AudioBuffer&lt;T&gt;) { for (i, (input_buffer, output_buffer)) in buffer.zip().enumerate() { self.process_one_channel(input_buffer, output_buffer, i); } if !self.state.user_state.keytrack { return; } let state = &amp;mut self.state; let events = events.events().filter_map(|e| { match e { Event::Midi(mut ev) =&gt; { if status(ev.data[0]) == Status::NoteOn { let pitch = ev.data[1]; let freq = midi_pitch_to_freq(pitch); let log_freq = freq_to_log_freq(freq); state.set_param(ParamId::BandpassFreq, log_freq); } Some(ev) } _ =&gt; None } }); let send_buffer = &amp;mut state.user_state.send_buffer; send_buffer.store_midi(events); self.state.host.process_events(send_buffer.events()); } &gt; error[E0501]: cannot borrow `state.user_state.send_buffer` as mutable because previous closure requires unique access The closure only needs to borrow `state` until the iteration finished. Would nonlexical lifetimes solve this or are there more changes necessary to make the compiler smart enough to accept this? Will it ever be possible to express that `set_param()` should only borrow a subset of struct members, **NOT** `send_buffer`?
Oh god. This explains an issue I was having. That's subtle as hell
Often enough 
There's also the [Read](https://doc.rust-lang.org/std/io/trait.Read.html) trait, which might be better if you're actually parsing bytes. But `&lt;It: Iterator&lt;Item=char&gt;&gt;` is by far the most general trait you could ask for, anyone should be able to support it.
You can put your server behind Nginx and use the Letsencrypt Certbot, or if you're using Rocket: https://github.com/SergioBenitez/Rocket/tree/master/examples/tls Do the input sanitation in your controllers before you touch the db. I recommend looking at how [this project](https://github.com/TheNeikos/furry.cafe/tree/master/src/controllers) does it. Also relevant: https://github.com/alex/ct-tools https://github.com/onur/letsencrypt-rs
You pretty much have to allocate a copy of each thing in any case. Otherwise you can't reuse your buffer chunks. That would be a serious problem if you were searching and parsing matches from a 50 GB file and only had 16 GB of RAM. Slicing seems amazing at first, but in Java, Go or Rust you soon realize it mostly causes headaches.
&gt; The server and client communicate over TCP, is there a nice drop-in solution I can use to encrypt this connection? I really don't fancy rolling my own crypto as it's not my area of expertise and it sounds like a recipe for disaster. Use TLS (aka SSL) and it'll do the job. Let's Encrypt can get you free certs for your domain, since you''re on a shoestring budget. There are many TLS libraries in either pure Rust, or as bindings around existing libraries. &gt; The server application uses postgres for its database, is there a preferred way to go about input sanitation in Rust? At the current rate of progress I'm going to be left with egg on my face if I keep sticking user-input strings straight into the database. Read about prepared statements in SQL. Consider using [Diesel](http://diesel.rs/). It does some neat things to make things both fast and safe. &gt; What impact on performance will encrypting my TCP connection have? One reason I chose Rust is that this is on a shoe-string budget and I need every ounce of performance I can get my hands on. Unless you're running a pure echo server (or equivalent level of input processing), processing queries will be much, much larger than any TLS overhead. Other advice: - [OWASP's top 10 guide](https://www.owasp.org/index.php/Main_Page) is your friend. - If you care about performance, profile code first, then optimize. Otherwise you will spend lots of time attacking the wrong problems.
Also, https://doc.rust-lang.org/std/marker/struct.PhantomData.html would need to be updated too, if PhantomData isn't used for determining safety properties any more.
Is there a way to construct a [CStr](https://doc.rust-lang.org/std/ffi/struct.CStr.html) where I know the length of my `*const c_char` (because it is from calling c_str() on a C++ std::string). Is the proper procedure to call [`slice::from_raw_parts`](https://doc.rust-lang.org/1.5.0/std/slice/fn.from_raw_parts.html) on my pointer, then feed that into [`CStr::from_bytes_with_nul`](https://doc.rust-lang.org/std/ffi/struct.CStr.html#method.from_bytes_with_nul)? If I just want a &amp;str, should I go straight to [`str::from_utf8`](https://doc.rust-lang.org/core/str/fn.from_utf8.html)?
Holy hell. How have I never heard of this before. This is nuts
The downvotes are not for "insisting that your experiences are valid." They're for being so combative about it when someone is just trying to help. The idea is to assume good faith (e.g. maybe someone just misunderstood you, or you misread them), so we can focus on the actual issues, rather than turning this into yet another internet argument.
I published a preliminary version, mostly to reserve the name on crates.io.
Yay! My first merged RFC :)
Missed a digit. Corrected the post
SICP is what got me interested in programming in the first place
&gt; I'm pretty sure this is an incredibly classic C++ bug. It's certainly the kind of bug that could trip up C++ programmers coming to Rust; since in C++ the temporary would not be freed until `some_ffi_function` returned: "The lifetime of a parameter ends when the function in which it is defined returns" (5.2.2 clause 4), and "Temporary objects are destroyed as the last step in evaluating the full-expression (1.9) that (lexically) contains the point where they were created." (12.2 clause 3) See also [this StackOverflow answer](https://stackoverflow.com/questions/3041249/when-are-temporaries-created-as-part-of-a-function-call-destroyed).
I've run into it before. So at least once.
I'm convinced that it was a mistake when designing Rust to use &amp; * along side vocabulary like Box and Rc. The problem is that you just have arbitrary glyphs representing what are really primitive types with specialization, along side object types. Yes, writing ref&lt;i32&gt; instead of &amp;i32 is slightly more work, but it is completely unambiguous to *everyone*, not just the experienced programmer. I'd say "C++ gets away with it" but it really doesn't, all the special glyph interpretation in C++ is horrifyingly bad. I don't think you necessarily have to depreciate the current syntax, but nobody would think twice about having a ref&lt;ref&lt;i32&gt;&gt; vs worrying about &amp;&amp; looking wrong, and I don't think letting it be written both glyphically and gramatically hurts anyone. Just make * deref&lt;T&gt; and &amp; ref&lt;T&gt; (and let the type deduction happen transparently so you don't need all the &lt;&gt; all the time). And I think that even more accurately demonstrates whats wrong with just calling a RefCell reference ref. It sounds way too generic for what it is, and is what threw everyone off writing match statements - "Why do I have to write ref? Where did this come? Is ref == &amp;? What day is today? What is my life?".
Could the destructor call `destroy()` if it hadn't been called earlier?
This is also true of Rust, afaict: https://play.rust-lang.org/?gist=75c281e318e8967b51c274f9b0b32f98&amp;version=stable&amp;backtrace=0 Rust has generally always been very conservative in temporary/expression evaluation.
Well the point here is that destructors aren't always called.
Unless the proposed changes mean people will have a harder time writing rust than before, you should just be glad that it will take new folks less time to understand it.
you can tell that about any crates that are aimed to solve the same problem.
I believe so.
But what if I want better job security?
A simple(?) lifetimes problem. I think this should be possible, but I'm not sure exactly how to express the lifetimes to make this work. I have a `Vec&lt;(T, _)&gt; where T: Ord + Clone`. I want to `sort_by_key` it with the first item as the key. However, the first type is not `Copy`. (My actual full type is `Vec&lt;(BTreeSet&lt;usize&gt;, (Vec&lt;BTreeSet&lt;usize&gt;&gt;, String))&gt;`, from `into_iter`ing over key-value pairs from a `BTreeMap&lt;BTreeSet&lt;usize&gt;, (Vec&lt;BTreeSet&lt;usize&gt;&gt;, String)&gt;`.) I've tried `entries.sort_by_key(|x| &amp;x.0)`, which fails as the lifetime of the borrow does not live outside the closure. My current workaround is `entries.sort_by_key(|x| x.0.clone())`, which works but is likely horribly inefficient because of the unnecessary cloning. I feel like I should be able to sort this Vec by the first element of the tuple, but I just don't know how to tell Rust the correct lifetimes. (This feels like a valid SO question but googling turned up nothing and it's specialized enough that due to my low rep I'd just get ignored, likely)
I'm curious to see a good way to do this using `sort_by_key`, but in the meantime a simple workaround to avoid `Clone` overhead: entries.sort_by(|l, r| l.0.cmp(&amp;r.0));
Fortunately, Rust has made the implementation much easier in general. The reference implementation uses a delicate mess of macros and gotos. Rust allowed me to make clean abstractions. The arrival of integer generics and associated constants will make things even cleaner. I haven't had much trouble with debugging so far, but I haven't gotten to the toughest problems. I'll probably build in some debugging tooling as needed. Instruction cache and branch mispredictions are more of a worry. There's a lot of unnecessary branching right now, and I'm not sure if it gets removed in during optimization passes.
You're looking for /r/playrust 
[removed]
I took a look at the Specs crate. I think Rudy has the operations that it needs mostly already in place. The one missing piece is remove(), which currently only works up to 31 entries. Implementing remove for arbitrary sized trees should be just a few more days work at most. Tuning shrinkage will unfortunately be a very different story.
Thanks for taking a look! That's great news!
Pedantically: `&amp;String` points to a `String` somewhere (not necessarily on the heap), which points to the raw bytes on the heap.
&gt; So if I pass &amp;String to a function that accepts &amp;str, how would it make use of the metadata that &amp;str typically has? Through this definition in the stdlib: impl Deref for String { type Target = str; fn deref(&amp;self) -&gt; &amp;str { ... } } Basically the `deref` method takes the pointer and length fields of the `String`, ignoring the capacity field, and constructs a `&amp;str` from them.
&gt; many TLS libraries in either pure Rust, or as bindings around existing libraries. Don't use any pure Rust crypto yet. Firstly because I've yet to see anything pure Rust to have been audited, and secondly because Rust still has problems with doing constant-time crypto. Ome day we'll be there, but for now just use some crate that binds to known C libraries.
I'm not sure if this is just one big pun or if it's serious :) But if it's the latter, the problem Tokio faces is lack of necessary support on the language side. Without some async-related ergonomics, it will remain quite painful to develop bigger application in a `Future` style. The worst thing that could happen here is everyone betting on Tokio _anyway_ (like Hyper did), and then finding out it has hamstrung the adoption of Rust as a language (or at least in the niche of fast concurrent servers). We're not JavaScript where people dealt with `Promise.then` for years because they had to. The space is crowded and there are plenty of alternatives.
Check out http://stevedonovan.github.io/rust-gentle-intro and see what you think...
I know very little about Tokio, or async in general, but I'd be interested to learn what the problems with Tokio are, and what language ergonomics is seen necessary to ease the pain. With the increasing number of libraries using Tokio, I agree that discussion is much warranted here.
&gt; it usually is important to know precisely whether something is a value or a pointer to a value. I don't agree at all! *Usually* what's important is what the compiler already tracks for you - can I move this thing I want to move? can I mutate it? You're not going to be getting any unforced runtime errors because you have an `&amp;T` and you thought you had a `T`. If you're worried about memcpying large stack objects and so on that's a different thing, but that is not *usually* the case. (That said, I remain unconvinced that anyone who is paying enough attention to be worrying about the exact compiled repr of this code can't also apply the rules this RFC establishes). As a larger point, the "appeal to systems programming" is almost a bad argument. Its inoperably vague and open to interpretation and *moreover* the whole point of Rust is to do systems programming in a different way from how it has been done.
It's mostly the lack of some form of async/await syntax. There have been some community attempts at that but it's not very clear how it's going to look like in the long run. RFCs on this topic seem to be mostly in the exploratory/discussion phase at the moment.
It's serious. And a pun.
/r/rustjerk would appreciate this
This looks much better. It also convinced me to finally write a cargo workspace sub-command https://github.com/MaikKlein/cargo-workspace. (experimental) `cargo workspace fmt` Edit: It is currently pretty slow, but probably still useful for `fmt`.
The "good" book: https://doc.rust-lang.org/nightly/book/second-edition/
&gt; The things that are fat pointers are hard-coded: You can create your own DSTs, FWIW. Stick an unsized type at the end of a struct and you're done. Currently the only way they can be safely constructed is by doing `struct Custom&lt;T: ?Sized&gt; {.. fields .., last: T}`. Then, you construct a `Custom&lt;[T; N]&gt;` which you can box or take a reference of, which can be coerced to a box/reference to `Custom&lt;[T]&gt;`, a DST. You can do a similar thing with traits IIRC. This can't be done without generics, however. But in that case you can [use unsafe as done here](https://github.com/servo/servo/pull/17179/commits/21448cf2668e1a0ed2ee8b5cfeace7afd682e088#diff-24b9c6330993e75071881dcef8541767R442) Custom DSTs are pretty rare right now. This may change in the future if better abstractions spring up around these.
&gt; We're not JavaScript where people dealt with Promise.then for years because they had to. The space is crowded and there are plenty of alternatives. I'm perfectly fine with Futures not having special language support. Java is in a similar situation since ages and doing just fine on that front. That said, I would be fine with the language adding related features, but they should not come in a rush. Also, I think impl trait will bring many gains there.
&gt; Since 2000, you're visitor . They wouldn't have had this problem if they wrote it in Rust
&gt; We don't want you to keep textures once its canvas is gone, it's useless. That strikes me as a useful "nice to have", not something mandatory, and certainly not something that dependent types inherently requires. Similar to how a memory leak is not the same as memory unsafety, keeping textures around needlessly is not necessarily optimal, but not really something that _must_ be enforced. I can also imagine a separate feature in the lifetime system which indicates "`b` lives no longer than `a` but does not truly borrow from it", such that if you dropped `a` while `b` existed it would error _as if_ there were a borrow, but would not prevent mutating `a`. &gt; The type contains references to an instance, so the type itself cannot exist outside of that instance's life I think at best that's an imprecise assessment. A type may depend on a value in that it's logically tied to the value from which it came, but an instance of that dependent type does not necessarily have to refer to anything within its parent type, or the value on which it depends, in a borrowing manner. Hence the instance does not _have_ to depend on the parent value's lifetime. Such a thing might be a desirable API property, but that is an orthogonal concern to a dependent type system which can solve the problem. &gt; Intuitively I think it's a very dangerous thing. . .if types can be members of instances, then they should have the same rules of instances apply, including lifetime. I don't see why. I am envisioning a system that is more like a "tagged associated type". Such a type can own its data and have no relationship to its parent other than being "tagged" with the instance from which it came. &gt; If we have a true existential type then we can't easily recognize the difference between two `Texture` True! But as long as those textures are only usable in ways that don't depend on the differences, you're okay. If you can statically maintain such information, then you should be free to use it. I think my Scala example demonstrates this is possible. The specifics in Rust might be different, and will certainly have different trade-offs than Scala's should it ever exist, but it's not inherently impossible. &gt; even though you have two `Texture` types they are not compatible because they are actually completely different types I have a suspicion this is isomorphic to the dependent type system I'm suggesting. You can imagine that value-dependent types could be tracked as unique types in the type system. &gt; It does if the exposed type exposes the instance that created itself. I believe it only exposes that there is _some instance_ that created it and requires that you send it back to the _same instance_, but it seems possible that you know nothing about that instance other than that you have it and can pass it a texture. In other words, you only know that you have something which conforms to a particular trait even if you don't know everything else about it.
Thank you for this article! Especially for links to proper explanation of `PhantomData`. I suspect that it will solve my recent problem.
Well, RPi zero is tiny. Running a generic speech recognition on it will be a challenge. It could run using Google Speech API instead, or with domain-specific speech recognition. We are studying these alternatives, but the current release platform of choice is a Raspi3.
But you wouldn't have the visitor number be undefined, at least 
Wow, you worked a lot on this answer! Thank you for taking the time to write it out, it's super useful for me!
In Rust, [HashMap](https://doc.rust-lang.org/std/collections/struct.HashMap.html) makes use of some randomness when instances are created, to protect against DoS attacks. If you wanted to do that in Haskell, would you only be able to create instances inside a monad? Would you then run into monad composition problems as soon as you also needed another monad, say for IO?
Please take a look at my [enigo crate](https://crates.io/crates/enigo). It is not finished though but i hope it is going in the right direction. Currently it is only for dispatching events system wide cross platform (Linux-X11, macOS, Windows currently) Feel free to participate if you like, by opening issues for error reports or ideas of enhancements or even code :)
May i ask how do you searched for it? I would like to improve the search ability of enigo. Currently i am very high on crates.io if you search for the keywords "[input](https://crates.io/keywords/input)"(6th) or "[keyboard](https://crates.io/keywords/keyboard)"(1st) 
&gt; as you probably are aware, you can do this with a relatively easy wrapper type. Yes that might be an option , I think I remember doing that before. &gt; Would the implementation of Vec allow for an Index type parameter that must satisfy Into&lt;usize&gt;? And the the indexing and getter methods would do an implicit case to usize? Seems like that might be enough ... I don't know enough here yet to say for sure. I've just started trying this out and started experimenting beyond that (feature creep..), e.g making an 'ArrayIndex trait' trying to allow slotting in Index=(int,int) to make a 2d array. Not sure if that will work out well (e.g. actually i'd prefer a multidimensional array to allow returning slices with one dimension indexed, etc..) I'll see how it goes. i know Vec&lt;Vec&lt;&gt;&gt; is probably a great solution most of the time for that.
&gt; Yes, I've been pointed towards Lazy-static but it doesn't do what I want it to do. OK, that's fair, but I pointed you to it because you said "global state." What you're doing seems a bit different than that. :-) With respect to mutating a `&amp;T`, you'll want to read this: https://doc.rust-lang.org/std/cell/struct.UnsafeCell.html &gt; For instance a while back I had to forget about a argv contents which I passed to the C execve to pass it but if the execve failed some-how I needed to reconstructed the original to provide error messages in a log of what argv failed to exec. I think this is in the weeds. Having to use unsafe when dealing with `libc` directly seems pretty reasonable. Perhaps the better question is why you had to use `execve` in the first place. Is there something missing in the `process::Command` abstraction that you needed?
Reminds me a bit of [burst trees](http://goanna.cs.rmit.edu.au/~jz/fulltext/acmtois02.pdf). [This paper on ARTs](https://db.in.tum.de/~leis/papers/ART.pdf) mentions burst trees in the introduction, but gives them short shrift.
&gt; With respect to mutating a &amp;T, you'll want to read this: https://doc.rust-lang.org/std/cell/struct.UnsafeCell.html Yes I know but I didn't do any of the things in regarding mutable references. There in fat are no mutable references existing, only immutable ones. Internally it already uses a RefCell to handle that. &gt; I think this is in the weeds. Having to use unsafe when dealing with libc directly seems pretty reasonable. Perhaps the better question is why you had to use execve in the first place. Is there something missing in the process::Command abstraction that you needed? Yeah, couple of reasons: 1. I have no idea whether `process::Command` is async-safe; the documentation does not mention it 2. I needed to fork, then do a bunch of stuff and then execve; it wasn't a simple fork/exec
It's just an example and not a concrete list of difference, but here's a comparison between before and after running it on one of my crates: [Before](https://github.com/brdgme/acquire/blob/18e2b2339fbed79b495e8458511ae5bc2c829383/src/command.rs#L67) -&gt; [after](https://github.com/brdgme/acquire/blob/8be1d8a144589fb9b0b66df7f6039d057cc8c1a6/src/command.rs#L68).
You could format all crates in the workspace using cargo fmt --all Also, you could list all members of the current workspace with `cargo metadata --no-deps`, there's no need to parse the Cargo.toml yourself. (And then there is also [cargo-multi](https://gitlab.com/imp/cargo-multi) which seems to have the same purpose of your crate)
Personally I'm not sure QoL will be better when `t.f(a, b)` has a different order of evaluation than `T::f(t, a, b)`, and think newbies will be ultimately better served by rules that are a little less ergonomic but simpler. But I seem to be in the minority so I guess we'll see.
Do you have any how close we are to having general non-lexical lifetimes? This RFC seems to indicate that we're still pretty far away, because otherwise the effort that would go towards implementing 2-phase borrowing would be mostly obsolete after it gets superseded by a full NLL implementation. Or would that not be the case either, and is the effort towards 2-phase borrowing actually useful for implementing full NLL also? Sorry if any of these questions are really stupid. I don't know any of the details about non-lexical lifetimes.
As a huge Neil Young fan and programmer interested in Rust, this was a nice surprise to see here. HyperRust used to be *the* Neil Young website to visit, back in the day. As you can see, it is no longer being maintained. Fun facts: Neil Young fans are often called "Rusties", after his albums [Rust Never Sleeps](https://en.wikipedia.org/wiki/Rust_Never_Sleeps) and [Live Rust](https://en.wikipedia.org/wiki/Live_Rust). Gatherings of Rusties are called [Rustfests](http://rustfest.org/), sounds familiar? And finally, there is an Internet audio stream that features only recordings of Neil Young concerts, hosted by yours truly, called [Rust Radio](http://rustradio.org/).
Clearly, Neil Young needs to play at a Rust conference.
I think it's headed in the right direction. What I mean, currently it works, but is often a bit hard to work with. On the other hand, it is 0.1 release and I expect it'll change, hopefully for the better, in 0.2. I'm optimistic, it'll get to an ergonomic state.
That is great. I love the way it intelligently starts at an indentation level. It looks much nicer and more readable.
For listening to Keyboard events, there's my [livesplit-hotkey](https://crates.io/crates/livesplit-hotkey) crate (It is "namespaced" under the livesplit name, as it's not quite mature yet for me to take a non-namespaced crate name). It works pretty well on both Windows and Linux. I'm looking into adding Mac and Browser support too. Eventually it might support mouse and gamepad hooks as well. Feel free to contribute a few things that are missing, if you are going to use this.
The new style looks better. For if-else blocks I like to write code like this, which might get stretched when formatted, but I would like to keep the shorter version: if foo() { .... Some(foobar) } else { None } Having the else-clause stretched just increases the line-count, but IMHO doesn't improve the readability.
Some issues I'm having: First, it looks like there's stil some visual(ish) alignment? impl&lt;M, S, F, X&gt; SubSelectDirect&lt;M, S, F, X&gt; where M: select::Selector, S: event::Stream, F: for&lt;'t&gt; FnMut(transform::Api&lt; 't, Stream&lt;ContentStream&lt;S&gt;&gt;, &gt;) -&gt; transform::Api&lt;'t, X&gt;, X: event::Stream, { ... } and I'm not sure why that `F` bound is broken up at all, since it's below 100 chars. Also in patterns: 'search: while !rest.is_empty() { match try_parse_attribute(tag, rest)? { Some(AttributeData { attribute, rest: new_rest, }) =&gt; { attributes.push(attribute); rest = new_rest.skip_whitespace(); } None =&gt; break 'search, } } I'm also wondering if binary operators like these can be swapped to being prefixed: fn main() { this_is_a_very_long_expression_with_a_result + another_very_long_expression_and_they_exceed_the_line_limit } so the operator doesn't disappear.
Yeah, this is interesting. I looked more closely to the search results and my Ranking did not correspond to anything i could really influence or would filter good from bad hits. The links i posted above are based on the alphabetical order, so being on the first place on the keyboard keyword only lasts as long as there is no create with the same keyword that starts with a latter a,b,c or d. Another thing that i find worth mentioning is, that you cant really search for keywords in the interface. If i type a search term in the input field it does something else (i think keywords are also weighed in here but it is primarily searching for crate names) So in this case (and that is the case for almost every user) searching by the input field enigo lands on place 3 for "keyboard" and place 27 for "input" still very good. I wish there was a way to explicitly search for keywords as that is only available through the huge list accessible through the front page. But i don't know how helpful this would be in this concrete case as the current crates list filtered by keyword is only sortable by name (alphabetical) and download numbers. i feel like crates.io is not to bad at finding stuff? That's the reason i was asking in the first place. 
[This libinput binding](https://crates.io/crates/input) works on Linux and doesn't depend on a display server.
Is the split between rustfmt and rustfmt-nightly permanent? I can see that getting really confusing quickly: &gt; You: You should use rustfmt to keep your code formatted nicely &gt; &gt; They: OK I installed rustfmt, but it's throwing weird syntax errors and what not? &gt; &gt; You: Oh, you have to install rustfmt_-nightly_, not rustfmt &gt; &gt; They: OK....
I've got good news for you! Async/await syntax has been [validated as the #1 desired feature for async I/O](https://users.rust-lang.org/t/what-does-rust-need-today-for-server-workloads/11114), it's [been implemented](https://github.com/alexcrichton/futures-await), had some good [follow-up discussion](https://internals.rust-lang.org/t/post-rfc-stackless-coroutines/5021/11?u=alexcrichton), and will be [talked about today by the compiler team](https://internals.rust-lang.org/t/compiler-team-proactive-agenda/5230/6?u=alexcrichton). Note that we're also already looking ahead and fixing bugs coming down the pike. A major pain point with the current implementation is bad error messages in procedural macros, but this has [long been planned for](https://github.com/rust-lang/rust/pull/40939) and `syn` has been [completely rewritten](https://github.com/dtolnay/syn/compare/0.11.11...master) to take advantage of this. We're quite serious about async/await, things just take time sometimes!
Does it with with Wayland?
Currently it does not and i have not investigated if it is possible at all with how wayland works. My current understanding is, that there needs to be a wayland protocol extension to be implemented into every compositor ... if that happens i would start to implement that but i have not really ditched my nose into wayland unfortunately. The other way would be libinput but this is also a topic i have not much knowledge currently and i don't know if that would help with wayland, i think it would not. I feel like wayland is really a showstopper regarding this kind of applications/librarys.
I do agree. For me the most important thing is that I can see losing the explicitness of `ref` and `ref mut` on the horizon, like with deprecation or outright removal. I'm a bit sad that the ability to be explicit, and use code to document expectations seems to not be considered much.
https://www.reddit.com/r/rust/comments/5l7ccs/just_getting_started_with_rust/
Even with impl trait, building a semi-complex pipeline with futures/streams is... unpleasant, and occasionally downright head-scratching. Also, these error messages... I don't know if impl trait would do anything about it. Not to mention that futures and lifetimes don't really mix, an issue that Java does not suffer from.
I, for one, would like to welcome our shortly-awaited async overlord.
Glad I could help... but it wasn't really that much effort. I'm a chatterbox, my brain is optimized for being a trivia fiend, I've covered this stuff before, and I want to share my enthusiasm about Rust, so it was quick, fun, and easy to write.
https://www.ralfj.de/projects/rust-101/main.html I started with this ;)
Await doesn't do the same thing as ? or try! Both of those return early whereas await resumes after the future has been completed. With the way macros work now, you would have to wrap the entire rest of the body of the function you're in when you invoke the macro. This is essentially the same as what we have today so it wouldn't really solve the problem. I'm not necessarily in favor of a special syntax just for one thing, but I don't think these alternatives really work here. 
Your example isn't nonlexical lifetimes. We'll need a way to tell the compiler that certain method calls borrow a subset of the struct. I've considered something like `fn foo&lt;'a&gt;(x: &amp;'a.fieldname StructName)` for this in the past, but there's no RFC as of yet. The way to solve this currently is to have a method on state that returns two mutable references -- one to the send buffer, one to the param storage, which you split out and deal with. In this case another solution (which IMO leads to a cleaner API) would be if instead of an iterator you accepted a closure that takes an iterator to events, and `&amp;mut state`, and manually advances it. Then, `store_midi` can be a function on `state` that passes the state down whilst iterating. It leads to clearer responsibility in the API design, as opposed to field lifetimes, which IMO are very useful but will likely often make for a bad API.
I suspect most of NLL will be in by this year. All the necessary stuff exists, but a lot of folks are also working on the typesystem overhaul. Also I think they want to be careful with the rollout, as with any features.
But it does make for more predictable reading as your eye will always look to the same place for an else block.
It is, sort of depending on what's chosen. &gt;Two-phase borrows in this RFC are only used when desugaring method calls; this is intended as a conservative step. In the future, if desired, the scheme could be extended to other syntactic forms, or else subsumed as part of non-lexical lifetimes or some other generalization of the lifetime system.
IMHO Rust is already quite noisy, when writing code I try my best to reduce code-noise, and the else-in-one-line thing is something I do when the content of the else block is short and predictable `else → do the error-case-thing`. Sometimes I wish I could specify that the return-type is `Optional` and have the code return `None` automatically, if I don't return an error-value – like in the example below – or just write `return` without a value at all. fn foo(n: u32) -&gt; Option&lt;u32&gt; { if n &gt; 42 { n } // n would be wrapped into Some() // if n &gt; 42 { n } else { return } // verbose, but should work too }
&gt; Yes I know but I didn't do any of the things in regarding mutable references. It's about mutation in general, not just mutable references. RefCell should fix this but I'm not sure -- there's still a good chance your code is undefined behavior. Mind if I have a look?
I'm talking about the "Will it ever be possible to express that set_param() should only borrow a subset of struct members, NOT send_buffer?" here.
For me it's the same thing, since both `Result` and `Future` are monads :P Consider this: let some = Some(5); // code here let number = some?; And now consider this: let req = client.get("https://google.com"); // some code here let answer = req?; There's literally no difference. The only thing is that when calling the `?` operator on a `Future`, it will wait until the future has been completed. If it completes successfully, it will give us the answer. If not, then the function returns with error handling. Of course that that's not how it works now, but the point is that it *could* be made to work like this.
Ahhh okay, you replied to my comment that's why I was confused since I didn't talk about that
It's not different order of evaluation though. In `t.f(a, b)` the `t` is still evaluated first, this RFC just turns it into a two-phase borrow which, AIUI, basically acts as though the `t` is a `&amp;`-borrow while the arguments are evaluated, and then is promoted into a `&amp;mut`-borrow for the actual call.
Yeah except I've been messing with this a lot so it isn't an MCS queue lock any-more really. It uses basically similar ideas but this particular algorithm uses a stack which can be slightly faster in many cases. As well, the particular ADT can be replaced with other ADTs such as a priority queue very easily so alternative algorithms can be tried very easily.
Not safely, no.
Too bad. I was hoping there was some magic allowing me to make my own `Deref` compatible string slice wrappers :)
You can, you just need unsafe. Check out how `OsStr` and `Path` work.
Just as a side note, you should avoid having a `Vec` of other heap-allocated collections. You may be able to replace the inner collection with `smallvec` or `arrayvec`.
It is never possible to guarantee destructors are run. Consider what happens if the user pulls out the power cord for example. This is IMPORTANT and not just a corner case. Users get really frustrated if their computer crashes and their term paper is erased.
However in terms of memory safety aborts skipping destructors is safe. Linear types plus a `leak` auto trait that prevents `Rc` cycles, etc would likely be sufficient for Rusts purposes. 
IMO the best sort of savings for this type of optimization isn't with flat vectors but with tree-like datastructures. What you do is have a `Vec&lt;u8&gt;` of indices and a `Vec&lt;T&gt;` of data and a pointer to a node is a `u8`. This is useful for compressing data and cache locality. The savings for a flat vector are much more puny.
I'm waiting for https://github.com/rust-lang/rfcs/pull/2000 to remove some of the `Vec`-ness of this code. Ultimately I'm hacking around these structs: struct DFA&lt;const Ts&gt; { transitions: [Transition; Ts], table: Vec&lt;([usize; Ts], String)&gt;, } struct NFA&lt;const Ts&gt; { transitions: [Transition; Ts], table: Vec&lt;([Set&lt;usize&gt;; Ts], String)&gt;, } I'll look at `smallvec`/`arrayvec` though! EDIT: it looks like I just need to wait for const generics. `Ts` can be arbitrarily large (up to a theoretical `0x110000` though I'm definitely doing optimizations to avoid that pathological case).
yeah. It could use `str` too though. There was once a PR on Rust for replacing `str` with `struct str([u8])` and removing the builtin. IDK why it didn't happen though.
So it should take the closure that takes an iterator over events and &amp;mut state but it would also have to take the &amp;mut state and iterator as args so that it could call the closure with the events to be sent. But then it would be the same lifetime conflict because the closure and &amp;mut state would both borrow `state`. Or how do you mean? And [`store_midi()`](https://github.com/Boscop/rust-vst2/blob/optimized/src/buffer.rs#L319) is a method on `SendEventBuffer`. `user_state` is a parametric type. [`PluginState` doesn't know which type it will be](https://github.com/Boscop/easyvst/blob/optimized/src/state.rs#L20). Not all plugins have a `SendEventBuffer`, only those that send midi out. The point of `user_state` is to allow the plugin developer to have a custom state for the plugin. So how should the signature of `SendEventBuffer::store_midi()` look like in your opinion? :) 
fair enough, but polygon meshes aren't themselves always tree structures. the most basic representation of a renderable surface is 'an array of vertices, then an array of primitives indexing into the object'. This is why I keep talking about 'a 16megabyte machine'. If the vertex is at least 4 bytes (typical size is more like 16+..) you know for a fact you aren't going to need to index more than 4billion vertices, because it wont fit. beyond that, you might get data from 3d packages with various indexed channels.. 'a vertex' is 'a position index, a UV index, a normal index...' Sometimes you get 'bone influence' stored as (per bone) 'an array of position indices and weight'. then you want to write some tool to sort that per vertex .. 'find the 3 strongest bones per vertex'. &gt; A mesh could be represented as a bunch of nodes that have pointers to each other Right; at this level i'd be talking about 'scenes', 'skeletons' etc, but we're just arguing about names at this point.. we're both describing the same thing. I guess you're alluding to the case of more complex objects with scene-like organisation internally, fair enough. 
Eh a guess one for only tagging a single bit would work but plenty of people need more bits. Unfortunately, stealing more bits is highly platform dependent and usually relies upon giving the pointed to data excessive alignment.
Why is it that let stdin = stdin(); for line in stdin.lock().lines() { // do stuff } works, but fn main() { for line in stdin().lock().lines() { // do stuff } } doesn't?
I think the first example is a bug, I'll file an issue to fix. The second issue, there is a PR to fix already and I need to review it this morning. The third, we have been discussing in the formatting RFC process and we agree that the binary operator should go on the new line. This hasn't been implemented in Rustfmt yet, but should be soon.
&gt; So if I pass &amp;String to a function that accepts &amp;str, how would it make use of the metadata that &amp;str typically has? A `String` itself in a weird way is an ultra-fat pointer. What a `String` is three words: 1. A pointer to the heap 2. A word to indicate the capacity of the string 3. A word to indicate the length of the string The string itself "manages" this memory on the heap via its own unsafe code. The string ensures that this memory is properly allocated and deallocated and that it is always valid utf8 and that if you push to the end and the capacity is exceeded it is correctly re-allocated. A `&amp;str` is in fact closer to a `String` than to a `&amp;String`, a `&amp;str` is _two_ words: 1. A pointer to the heal 2. A word to indicate the length of the str a `&amp;str` has no capacity it is always completely filled up. So what is a `str` without the reference? That's basically just something that exists at the type level like all dynamically sized types you cannot directly use it. So when you use a `String` or `&amp;str` what lies on the the _stack_ are those three or two words respectively I described. So what's a `&amp;String` that's a single word: 1. A pointer to the three words in a `String` So obviously converting a `&amp;String` to a `&amp;str` is easy: 1. Look up the address of the three words of the `String` via the pointer 2. Get the pointer to the heap and the length 3. Package them into a fat pointer and call it a `&amp;str` And that's exactly what the code does [here](https://doc.rust-lang.org/src/collections/vec.rs.html#1601-1611) to get the pointer and the length and [here](https://doc.rust-lang.org/src/core/slice/mod.rs.html#2381-2383) to package it into a fat pointer. The code there is for dereffing `&amp;Vec&lt;T&gt;` into `&amp;[T]` which the `&amp;String` to `&amp;str` code uses internally because in the end all a `str` is is a `[u8]` slice with the guarantee that it's utf-8, same for `String` and `Vec&lt;u8&gt;`; a String is really just a struct containing only a `Vec&lt;u8&gt;` member. So yeah a `String` is closer to a `&amp;str` in memory representation than a `&amp;String` is which requires an extra indirection to get to the buffer. From the type level however a `String` _owns_ the memory on the heap and is responsible for dealocating it while a `&amp;'a str` does not own the memory on the heap, it is owned by _something_ which has lifetime ``a`, typically indeed a `String : 'a`. but typically not n the case of a `&amp;'static str`. where it's owned by the program itself. Edit: Is should also add that the `String` "owning" the heap buffer is completely agnostic of the type system. That it "owns" it is purely something implemented in the library which itself needs to define `Drop` for it all the language and its type system itself care about that `Drop::drop` wil be called on it when it or something it owns goes out of scope and after that the language itself will drop all the three words in the `String` I spoke off because it does own those on the type/language level itself; you cannot not-drop them.
Thank you! The style team have done a great job in defining the new style.
&gt; Unfortunately, stealing more bits is highly platform dependent and usually relies upon giving the pointed to data excessive alignment. Yeah, Judy gets away with it because all nodes are aligned to word boundaries.
So, even with Box&lt;Any&gt;, I can't really do what I want. I need the compiler to understand that the generic type for an actor method is the same type for the underlying structure's method. I don't think there is any way to express this currently in rust, but I may be wrong - do you have an idea? If this isn't possible it'll mean some ergonomic grossness with my library. Not the worst thing in the world I guess, but it won't be particularly usable.
Excellent, thanks.
This doesn't seem *that* in depth. It's basically sending measuring packets sent between nodes and detecting bottlenecks. It's definitely not trivial, but I don't think it requires any special knowledge, except maybe how system networking APIs work (may require some syscalls and whatnot). I could be completely underestimating the work here, but it seems like it should be doable for anyone with reasonably good programming experience, time and tenacity. It doesn't seem like you need any domain specific knowledge and it's pretty well contained. So yeah, don't let the Tor name frighten you, it's probably not out of reach. I don't know your experience level, but if you've done a few years of programming with a little bit of network programming experience, you should be fine :)
&gt; it's been implemented Wow, that's amazing! I might have to play with this :) Do you know if the general plan is to stick with macros or will there potentially be some syntax added to the language? If the latter, does this mean that at least part of `tokio` will be in the standard library? If the former, what additional work, if any, does the language/compiler need (apart from better error messages)?
Wow, thanks for the thorough guide! The thought of scripting honestly hadn't even crossed my mind. I was just gonna do a config file with JSON or TOML or something, but I guess you can be a lot more expressive with a proper Turing-complete language... I've used hlua a few times and found it pretty easy to use, so that's what I'm leaning towards atm. I personally like Python better than Lua, but Lua is simpler, and simple is probably the way to go here.
Yeah, I also used Google, because crates.io's search is... a bit lacking. I'm not really sure how to fix this tbh. Maybe add tags for things like "system[-wide] input" and "global key press" (which were a few of my Google search terms that didn't work). Maybe add them to the GitHub page as well? I think they have a "project tags" feature now.
I was under the impression that NLL first required the borrow checker to be moved to post-MIR in the pipeline, which AIUI isn't done yet (because there's plenty of unrelated bugs that could seemingly be closed if it had been). Do you think this won't prove to be a large challenge?
Oh, I thought we'd finished that already. My bad.
The `where` syntax is a bit unfortunate, but I think I like the decision that the style team has come to. If you make it the one place where you use visual indent, it's going to stick out. If you put the first constraint on the same line as `where` and the rest block indented, it's going to stick out. If you special-case a single constraint to be on the same line as `where`, but to use block indent for two or more constraints, it's going to introduce unnecssary diff churn when you add a second constraint. There's just no better option.
You definitely want proper scripting. Otherwise, it's not much better than lashing `xbindkeys` to `xdotool` and `dbus-send` and putting up with the fact that you'll get weird interactions if the user types their hotkeys too quickly because of the variable delays in firing off the subprocesses. (It'd also be a good idea to support requesting that calls to a given command be serialized, so you can't get race conditions within, such as the ones in some of my Audacious Media Player toggle commands, where they have to build a toggle by checking the state before explicitly setting a value.) If you want any significant chance of taking market share from AutoHotkey, you'll probably also want a way for people to write plugins, which Python would help with more, since you could just let them `import` them.
But what would the question mark desugar to? How do you encode pausing execution (without threads) and resuming whenever something polls and a condition changes?
The problem is that you seem to be trying to erase and then restore type information at runtime, whilst still being able to use generics. Rust isn't capable of that, no matter what you try. Maybe replace generics on the receiving end with trait objects? Convert from generic type to boxed trait object in the actor method, store that in the message, and send that over the boundary.
No, you just need to look up those definitions in the standard lib. They're very simple and often don't match the CT definitions anyway. `Functor` is just anything with `map`. Do `Applicative`, `Foldable`, and `Traversable` even exist in CT? Maybe, but I've never heard of anyone caring. Source: I have read probably 100 Haskell papers and I know roughly nothing about category theory.
From [Stack Overflow](https://stackoverflow.com/questions/35013293/what-is-applicative-functor-definition-from-the-category-theory-pov): &gt; In category theory, preservation of monoidal structure is related to *tensorial strength*, so an applicative functor is also known as a *strong lax monoidal functor*. However, in **Hask**, every functor has canonical strength with respect to the product, so this property doesn't add anything to the definition. i.e. A bunch of gibberish that doesn't apply to Haskell in any way. Seriously, just because something's name comes from a field doesn't mean you need to understand that field to use it. You can cook on a propane grill without understanding organic chemistry. I don't understand why people are so consistently confused about this idea.
&gt; […] it seems like it should be doable for anyone with reasonably good programming experience, time and tenacity. It doesn't seem like you need any domain specific knowledge and it's pretty well contained. Yep! That's correct. There's very little Tor-specific knowledge required to successfully complete this project, which is the primary reason I saved it for someone else, since it's exactly the type of thing that can get a new person involved.
I guess I will try to run this on my codebase (~20kloc) before the next release. Although I never liked automatic code formatting as it makes collaboration a bit hard(er)...
The same way the current `await!` macro does it. But instead of being a macro, the implementation will be inside the implementation of `Try` for `Future`.
If I understand correctly, the macro [uses](https://github.com/alexcrichton/futures-await/blob/master/futures-await-macro/src/lib.rs#L25) a `yield` keyword, which needs to be supported by the compiler (or a [procedural macro](https://github.com/alexcrichton/futures-await/blob/master/futures-async-macro/src/lib.rs#L26)). So you still need that. Is your argument that, given `yield`, you don't also need an `await` macro or keyword, and we can do with with question mark? That might be possible, I haven't thought of that. Not sure which would be nore ergonomic, either. Edit: Actually, in the [RFC](https://github.com/rust-lang/rfcs/blob/master/text/1859-try-trait.md#desugaring-and-the-try-trait) the `Try` trait is explicitly defined as *returning*, not yielding, so I don't think it can be reused in this case.
:\ Yeah, the thing is that I want the receiving end to be "normal", and then generate all the boilerplate Actor stuff from it. It isn't working though. I don't think it can work, either.
Links: * https://github.com/hsivonen/encoding_rs * https://docs.rs/encoding_rs/
Seems a little late for summer internships, seeing as most have already started. Also, the pay is low. Not sure what they will get.
Hm... Can we amend the `Try` RFC to account for yielding too? Otherwise all generators will be left out of the `?` operator.
&gt; Although I never liked automatic code formatting as it makes collaboration a bit hard(er)... How so? Once the format stabilizes, I love automatic code formatting since it removes whatever artifacts contributors' editors throw in. We have a pre-commit rule in our Go project that refuses to push if the formatting isn't correct (and then formats it for you with a warning saying it did so). It's a bit tough to get it going on an existing codebase though. I did this with a fairly large project and broke it up into pieces (one module at a time) to limit conflicts.
Definitely agreed! We just have to have any designs at all before we can start to formulate patterns around them :) &gt; If Async/Await is there would you say It should be used everywhere where It's possible? Because that's a big question mark for me. Eventually, yes! Right now it's not even on nightly, but we hope to get it there in the next months.
Thanks for all the work you all have been doing on lib polishing! In looking at the tracking issue, I'm not seeing any discussion about WalkDir implementing Clone or Eq. Is there a reason for that?
&gt;&gt;The crate wasn't initially meant to be used with raw integer as ids but if you need it, it's now possible. great, would i32 work aswell? I heard talk of trying to formalise the idea of indices with -1 being like Option::None various scenarios for index arithmetic.. extracting tri-strips, concatenating vertices into an index array and adjusting the indices of the primitives etc. (i didn't get the impression that your IdVec would enforce that anything entered was unique.. i would imagine going back to something more like a hash map for that)
Checking the history, it seems like https://github.com/barosl is responsible for those. I don't think there is much to it, other than somebody really liking Puella Magi Madoka Magica and putting references to it as placeholder names in tests. It's not that those names matter, those are tests.
When adding a node the index is returned as `NodeIndex`, which is from then on the only way to access the stored data. The usize is returned to the user.
*clicks link* &gt; Kurisu avatar Makes sense, I guess. BTW, there's also [a reference in the documentation](https://doc.rust-lang.org/error-index.html#E0432).
I even can use in on my raspberry Pi completely painlessly thanks to Rust's cross compilation :)
Look at your username before saying that. Edit: Not sure why I am getting downvoted. "qux" is another placeholder name like "foo" and "bar".
Weird, wasn't aware that could happen with links to tweets. The links to the relevant Github repos are in Bromskloss' sibling comment.
I get that on mobile sometimes, and just have to try again a few times and it loads just fine.
&gt;Because coming up with names for tests and examples is hard? I use `x`/`y`/`z`, `a`/`b`/`c`, `u`/`v`/`w`, `f`/`g`/`h` and `i`/`j`/`k`. Too bad greek letters are PITA to type.
Also see: https://groups.google.com/forum/#!topic/tlaplus/mbQvjs72bFs
So if I wanted to port the [`Str`](https://github.com/myrrlyn/wyzyrdry-c/blob/master/include/str.h) I'm using in C code to Rust, I'd declare it as `pub struct PrefStr&lt;T: ?Sized&gt; { len: u32, data: T, }` and instantiate one with something like `let foo = PrefStr&lt;[u8; 2]&gt; { len: 2, data: [b'H', b'i'], };`? I feel like this is going to need type-level integers...
Can you give us a link to the original C-based version of this? Is it something that is widely-used, or is Firefox-internal?
I love references so it's nice to see people print them in. The names don't matter like you said. It's just a bit if fun for those that look.
&gt; Too bad greek letters are PITA to type. Set up a Compose key! There's even [one for Windows](https://github.com/samhocevar/wincompose). :D
It is (well, was) a Gecko-specific thing, dating way back to old Netscape times. https://hg.mozilla.org/integration/mozilla-inbound/rev/4c8a154bf143
During my work on float formatting I have even spotted [the references to iDOLM@STER and Love Live!](https://github.com/rust-lang/rust/pull/24612/files#diff-52339159b8f44e3293d47535b26463d3). Unfortunately (for barosl?) the PR broke the test, so the latter had to be removed. Tangentially related: When I've named [Kailua](https://github.com/devcat-studio/kailua) I really meant [this](https://remywiki.com/Kailua).
Booooooooring.
I'm glad there are interested people, Tor is a great project. I'm sure it will look kick-ass on a CV too.
Huh, that's interesting. Thanks for the explanation.
You should probably open a new thread about using `?` for awaiting futures, since this one will get buried fast. http://internals.rust-lang.org seems to be a better place for this too (take a look at the pre-RFCs threads there). I kind of like it aesthetically, because `?` can subsume both the old `try!` and the new `await!`. But it might be confusing for people that expect `?` to be just for early return. Also confusing is the appearance of a `??` - it does different things if there's a `Result` inside a `Future` or a `Future` inside a `Result`. Another trouble is that, when you would implement `Try` for a custom type, you would need to choose whether to `return` or to `yield`. This choice is not obvious and doesn't appear in any signature, unless you inspect the `Try` *implementation* (unless the programmer bothers documenting this choice). I get that Haskell monads could have similar confusions, but in Haskell code doesn't live in an imperative "ambient monad" so people have different expectations. So I think that unclear control flow is a bigger trouble in Rust than in Haskell. Anyway, I think it's likely that `await!` will be just a temporary syntax too, but I'm not sure what will replace it.
Worked on optimizing an internal search-based analytics engine... Ended up publishing a crate for fast division. https://github.com/fulmicoton/fastdivide https://crates.io/crates/fastdivide
This doesn't look like the right place to add it (it deserves its own thread), but better than being buried in a Reddit thread for sure. edit: the thread was posted [here](https://internals.rust-lang.org/t/pre-rfc-unnamed-struct-types/3872).
Maybe a new trait that is specific to the use of `?` for generators. Sort of like how we already have multiple traits for things like `Index` and `Deref`.
Great find. This is pretty funny. Thanks barosl.
Congrats, Henri. There's so much exciting stuff happening in production Rust lately. Feels good.
Iota is a recent distributed ledger project who's token launched this week with a large market cap. Their roadmap includes a Rust client. The claim to fame of this project is that it is not based on a linear blockchain but a dag (they call it a 'tangle'). That makes 6 cryptocurrency projects using Rust in some capacity that I'm aware of: MaidSafe, Parity, EtcDev, Cryptape, Iota, Zcash. 
Well, it's a great series. Rust developers having classy taste is not a surprise to me :)
The work sounds 10x more interesting than what I'm doing at my internship this summer. I really wish I could have known about this opportunity sooner, even though the pay is significantly less, I would have applied in a second.
I haven't heard anything about Tokio entering `std`. Am I out of the loop?
My Caps Lock switches between English and Greek for λx.e and other shenanigans.
For people who want to investigate further, it's called [goroawase](https://en.wikipedia.org/wiki/Japanese_wordplay).
Slightly OT, from the doc: &gt; In particular, simply ignoring errors is a security problem, so it would be a bad idea for encoding_rs to provide a mode that encouraged callers to ignore errors. Does anyone have a concrete example of how this can be exploited? Ideally real-world vulnerability.
[Seems not](https://play.rust-lang.org/?gist=d9fa84784f14538aecc3bdd0764b8bf0&amp;version=stable&amp;backtrace=1).
&gt; I'm perfectly fine with Futures not having special language support. No special syntax = pyramid of doom: let future = operation1(42).and_then(|value1| { operation2(value1).and_then(|value2| { // We must nest here because we want `value1` to be in scope // for the later operations. operation3(value1, value2).and_then(|value3| { operation4(value1, value3) .and_then(|value4| operation5(value1, value2, value4)) }) }) }); Compare with the Haskell do-notation equivalent: future = do value1 &lt;- operation1 42 value2 &lt;- operation2 value1 value3 &lt;- operation3 value1 value2 value4 &lt;- operation4 value1 value3 operation5 value1 value2 value4 
There was talk about integrating an experimental version of generators / aync/await into nightly to enable experimentation and iteration without a full-fledged concept.
In what product are they using it? Edge?
&gt; The easiest solution is to just make up a random number using an unsafe primitive, which isn't too un-haskell since it is not *observationally* impure. That seems very sensible. You're shattering my image of Haskellers as implacable purity fanatics here. &gt; In this case though, you lose determinism, much like in Rust, with the same drawbacks, such as that two hashmaps with the same contents need not compare to be equal. They won't be structurally equal, but presumably you can define equality in a way which is independent of order, [as Rust does](https://doc.rust-lang.org/src/std/collections/hash/map.rs.html#1283). I think you need to do this even without randomisation, as the exact structure of a hashmap depends on its history. I think the only other way the randomisation is visible is through iteration order, and you can hide that by keeping a linked list through the entries in insertion order, and using that for iteration. It's not clear whether that would be worth it in the general case, though.
I didn't implement the proper trait for signed integers, but it's trivial enough to do, I'll add that tonight. The IdVec does not enforce much more than a regular Vec (outside of enabling strongly typed indices). In some of the things I do I store a lot of things in arrays and end up manipulating lots of indices, so I like how fool-proof it is manipulate typed TransformIds, PrimitiveIds, VertexId, etc., rather than having integers all over the place that I could use improperly by indexing the wrong array or messing up the order of arguments in a function call. The integer handle in the Id is public, so you can do arithmetic with it if you need to apply an offset or whatnot (it hasn't come up often enough for me to have special code for it).
Well afaik there hasn't been any ideas of baking Tokio into language. However there has been talks baking futures into it.
They could just bake do-notation into the language. No need to hardcode it for one particular m... err, abstraction!
How hard was it to get started with TLA+? Have you had any experience prior?
How is that a response to me being perfectly fine with the current state of things? :) I know about both of these things, I just see the danger of a half-assed solution making things even worse and find the badness of the current situation overstated. At least we have type-system support for all that. The first isn't pretty, but programmers are surprisingly good at building working systems with less-then-prefect systems. I don't subscribe to the "everything is broken and needs to be fixed before it is usable" style of opinions here and wanted to make heard that I'm good with how things currently are.
Hate to be that guy, but IOTA in my native tongue is: "Let's have sex".
Thank you for this suggestion. Does this work on stable, or is it nightly only?
The body is actually a `Stream`, the futures equivalent of an iterator. That is, it keeps returning `Chunks`. You can convert it into a regular future that returns all chunks by using the `Stream::concat()` method. If you use `.and_then()` on the resulting future, you get access to the whole body as a `Bytes` struct which you can then convert into a string. request.body().concat() .and_then(|whole_body| { // do things }) 
[removed]
He is really good, and I think the lightbulb went on after reading his introductions...
You can't win. No language support =&gt; you'll get trampled by others' offerings. Ultra-experimental pre-pre-rfc out-of-tree implementation =&gt; you're ramming things down people's throats. Ugh.
The latest proposal is [here](https://github.com/rust-lang/rfcs/pull/2000). There were some before this one, but they were reworked for varying reasons.
To be fair, "has been discussed for years" is not always the same thing as "ready".
Thanks for the lnk! So is this looking likely to be approved? What's the general consensus, is it something only a couple people want and it's probably not gonna get in, or are people just trying to iron out all the kinks still?
There's a lot of demand for it, but there are also many questions that still need working out. This is the fourth or fifth attempt to write an RFC for const generics, and possibly not the last one. That said, the lang team's stated goal is to have const generics past the RFC stage by the end of the year.
a'ight thanks man, will have a look through the current one!
Synergy is tackling input with their package to support Wayland somehow(still WIP), not sure if there is any code you can look at atm, but when the feature/support is released it could be a good reference, they have a github repo with source.
Looks like that RFC now exists, at least to a certain degree: https://github.com/rust-lang/rfcs/pull/2033
Right, that's what I meant by I don't have enough knowledge to properly set the context. Someone else with more browser implementation experience needs to answer. :-)
That does seem like a sensible alternative. It seems really nicely documented and convenient to use. Wouldn't hurt to give it a try.
One of these days we will have an FAQ entry for "why Rust's affine&amp;borrowed types and imperative controlflow makes existing monadic abstractions unusable". Or someone will prove me wrong. But if `do` notation comes to Rust, it will most likely have complex rules for the sake of ergonomics and not interact with traits through closures.
The first time I used Rust, I saw a reference while debugging. It's probably the only reason I kept using Rust, and realized how useful it is.
My company did go without tokio, and there were multiple reasons for that choice: - Ergonomics without `-&gt; impl` are not ideal - Tokio is heavily skewed towards networking i/o, and you can't easily use the internal thread pool for your own async stuff - Composing conditional async execution flows feels strange, especially with nested composition (But `Future`s aren't the right abstraction for that anyway) Tokio isn't mature yet, and can certainly improve a lot, but there is no need to be that harsh. We need to carefully explain what doesn't work for now, and nightly seems like the right channel to experiment on (Though I can understand the worry that once it's integrated, it may be pushed through, we can't exactly predict how things would go).
Cool,Redox selfhosting coming soon. Kudos to Redox team.
My mistake, it looks like this idea is neither undiscussed nor ready.
I feel like the condition is inverted on your event loop... it looks like it should break out immediately since done is false, and will not be true for another three seconds. Otherwise, it looks like a pretty good solution. My main issue is not understanding why the `TimeoutData` struct members need to be raw pointers. Allocating a `TimeoutData` object on the heap with Box, then converting *it* into_raw makes sense, and it would be a void* blackbox at that point to the C code. There's no benefit that I can see to making the members themselves raw pointers, and it adds complexity and more room for error. I also love how Rust's generic facilities allow you to express that SetTimeout can work on any type of data, with a callback function that accepts specifically only a reference to data of that same type. After working in Go professionally for months now, I miss the elegance of Rust's generics.
In what language? I don't think `rustc` runs on Redox, haven't checked though.
With [Rust 1.15.0](https://blog.rust-lang.org/2017/02/02/Rust-1.15.html), it is now possible to have Custom Derive, and [here's a small tutorial](https://doc.rust-lang.org/beta/book/first-edition/procedural-macros.html). ~~It is not possible to just have general custom attributes (attributes which are not Derive), at least not in stable Rust. If it is possible in nightly Rust, I believe it is just from implementing a compiler plugin that parses the whole AST looking for attributes, acting on them, and then removing them before handing them to the normal compiler.~~ **EDIT:** it turns out I might've been wrong. The tutorial I linked to above does [demonstrate how](https://doc.rust-lang.org/beta/book/first-edition/procedural-macros.html#custom-attributes) to create a custom attribute on stable Rust. I didn't realize this was possible, but now that I think about it, Serde does use that feature. Good to know!
&gt; One of these days we will have an FAQ entry for "why Rust's affine&amp;borrowed types and imperative controlflow makes existing monadic abstractions unusable". Who is writing it?
i mean rust language.
Mostly style comments, overall it looks very good. * I would add some comments in certain places to make the pluscal code stand on its own without the accompanying implementation. E.g. in the wait procedure say that in reality this is a busy wait, or say that the stack is implemented as a lock free stack which is out of scope for this specific. * NUM_PROCESSES, etc should be constants so you can vary them in the model checker without having to recompile the spec each time. Usually you don't explicitly set the number either, but just have Processes be a constant, which you can check with either 1..3 or {"P1","P2","P3"} etc. * NULL shouldn't be a constant, just a definition like: CHOOSE x : x \notin 1..NUM_PROCESSES * Multiple locks aren't really used in the spec, better to specify it as just a single lock.
&gt; fellow FYI: not everyone on Twitter is a fellow.
s/months/days/ ? Pleaaase ? :D
&gt; Tokio is heavily skewed towards networking i/o FWIW I personally don't consider this to be entirely true. I do think, though, that Tokio only really shines if you've got *some* I/O somewhere. If you're a purely CPU-bound application then libraries like rayon/crossbeam are likely more useful. Once you've got I/O though Tokio should do great for anything related to `epoll` or the various abstractions on each system. For example [sccache](https://github.com/mozilla/sccache) has only a tiny bit of network I/O between client/server and fetching from S3, otherwise it's entirely management of filesystem reads/writes and process/pipe management. &gt; Composing conditional async execution flows feels strange I agree! While always possible it's difficult to do today. This is the precise problem that async/await is targeted at fixing though :)
sorry - it's hard to remember how immersed you are in your own problem. here is a working example: https://play.rust-lang.org/?gist=9768184cd10af387f17d890fd9acf346&amp;version=stable&amp;backtrace=0 Hopefully it's simple enough to understand quickly. Thanks!
One of the most important tools when writing unsafe rust is compiletest [1]. It's a tool extracted from the compiler project that lets you write tests that are supposed to fail compilation. Since safe abstractions rely on the type system to make unsafe code safe, it's critical to make sure the compiler is properly rejecting code. I wrote a post about this years ago when I got hit by one of the bugs Gankro wrote about [2]. [1]: https://github.com/laumann/compiletest-rs [2]: http://erickt.github.io/blog/2015/09/22/if-you-use-unsafe/
Sorry.
Rust is anime
A minor nit: it should be `fn get_sound&lt;'a&gt;(&amp;self)`, rather than `fn&lt;'a&gt; get_sound(&amp;self)`. I only know this because i couldn't remember the syntax, so copy-and-pasted yours, only for the compiler to tell me off!
You can't specify fields in traits, but you can specify accessor methods. Implementers of the trait have to declare the fields, and implement the trivial accessors, but that's all. It's probably the closest in spirit you can get to the kind of inheritance you want. An example: pub struct VoiceSound; impl VoiceSound { fn render(&amp;self) { println!("Rendering sound ..."); } } trait Voice { fn sound&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a VoiceSound; fn render_next_block(&amp;self) { self.sound().render(); } } pub struct MyVoice { sound: ::VoiceSound, } impl Voice for MyVoice { fn sound&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a VoiceSound { &amp;self.sound } } pub fn main() { let voice = MyVoice { sound: VoiceSound {} }; voice.render_next_block(); } Here, it's pretty verbose. However, with bigger traits and structs, the fields and accessors become a fairly small, and simple, part of the code.
I mean, you only get it in the context of a custom derive, not generally.
If your CI enforces that there's no diff after running fmt, you're good.
Shouldn't it be `alias` instead of `root` in the nginx conf? And the 301 redirect should be in a `location / {}` block, otherwise the `location /.well-known/acme-challenge` will never match, right? In the service conf, shouldn't it also include nginx as dependency, like `Requires=After=postgresql.service nginx.service`? 
That is the question, isn't it? (To be clear, by "one of these days" I mean that I hope it will eventually happen)
&gt; You're shattering my image of Haskellers as implacable purity fanatics here. Hah, you should talk more to users who have used it for a while. The initial enthusiasm has to make way for the fact that it's just a programming language after all in eventually. :-) In the end, it comes down to the specific use case. There's no single hashmap that covers every use case, and sometimes the solution may not even be a hashmap. That additional pointer for finding the next element for example has the huge drawback of a cache miss on every operation, and it also costs another word to store the pointer.
I believe codewars.com recently added Rust katas.
Unfortunately I'd have to script this on my own... Not that complicated with Travis I assume, but still... I'm thinking about it...
What would it even mean to ignore errors? Returning an empty string?
&gt; enforcing style drives away some contributors (IMO) Having it be automated is a pretty low barrier to entry. They can write it however they want then run one tool to make it compatible with your codebase. I think it's worth the minor inconvenience as it keeps everything in a consistent style, which sometimes helps catch errors.
Oh, true, I had a brain fart. :)
It's a martial arts term for a practiced form. It was co-opted by Uncle Bob to refer to guided programming exercises designed to emphasize a particular language feature or development process (in particular, test driven development.)
It's a salve for the consequences of DRY. There is a perverse incentive where we spend all of our time writing relatively unimportant bits of code because the really important bits were already written by someone else and why would you make potentially bad copies? This can lead to a situation where the team is incapable of recreating the work from first principles. Which is for instance quite awkward in interviews. I once flubbed one because we (mostly me) had devised a solution to a hard problem several years prior and I could not describe how it worked. Once it worked and was tested I didn't have to touch it again except to fix a couple of bugs. I hadn't thought about it since. It worked great, but possibly cost me a job. 
The only borrower of the state would be `send_midi`. Send_midi would thread its borrowed copy of state trough the closure.
https://github.com/rust-lang-nursery/rustfmt#checking-style-on-a-ci-server
My 2gp. MIO would be better in the standard library than Tokio but a generalized Futures trait is a good thing especially for abstracting locking (RAII mutex guards), and lazy evaluation. --- While `mio` just offers an event abstraction which is also useful for `signal_fd`, `perf_fd`, kernel ring buffers, unix sockets, etc. This is a lower level less useful function to the average programmer but it provides a richer interface with less opinions. Tokio generally assumes you will do your IO on the same thread as your events and this starts to become an anti-pattern at and around 1GiB/s. When you want events+interrupts to be handled on a few cores, and the reading/writing/packet parsing/event handling to be offloaded to others. While yes Tokio is *miles* better than synchronous IO it won't age well. Seeing a the new X299 chipsets are making 10G Ethernet a consumer feature this isn't *super* far off. It'll age fine for *front end* or *db wrapper API server* but those are written in Ruby, Python, and Go. So we can't really argue performance is an *extreme* concern here. 
&gt; The key is, syntactic sugar for what For `flat_map`/`and_then`. Its usefulness should be the same as theirs. Now if those functions are rarely used anyway, then of course it makes little sense to give them special syntactic privilege, but to my credit, I'm replying in a thread specifically discussing whether futures deserve their own syntax.
&gt; For flat_map/and_then. These are not polymorphic. If you try to make them so, you'll see some of the problem. As I said &gt; I mean, I know how do notation desugars,
exercism.io is similar, and people enjoy them.
Tokio isn't entering the language. Async/await may, backed by futures in the stdlib. None of this will happen without multiple RFCs and significant baking/iteration. It's basically impossible to shove a feature down the rust community's throat, the decision making process is community driven.
I think that's more like a fault of the interview than a fault of yours. Not disagreeing per se – we've got to adapt to what we've got. But I find it perverse that the things that are supposed to assess us for something real don't manage to measure the thing they are supposed to measure, at all.
&gt; How is that a response to me being perfectly fine with the current state of things? :) It points out the current state of things is not good. &gt; The first isn't pretty, but programmers are surprisingly good at building working systems with less-then-prefect systems. Actually, the most likely outcome is that they will avoid doing anything like it at all, even in situations where it would be the best solution, because everybody tends to follow the path of least resistance. The way I see it this is why it was so urgent to have the `try!` macro and to add the `?` syntax to the language—because otherwise the path of least resistance would have been "panic everywhere." (I'm reminded of all the countless folks who just catch and ignore checked exceptions in Java...) 
Oh, you mean there's no higher abstraction `&gt;&gt;=` of which `and_then` (Future), `flat_map` (Iter), etc. are all members of? That's not a problem! For a purely syntactic extension, you don't need to unify them all into a parent typeclass (although you can if/when language supports that). This is what I was trying to point out when I said with `RebindableSyntax`, you can use do-notation in Haskell without any relation to the `Monad` typeclass at all. You can see in [this article](https://ocharles.org.uk/blog/guest-posts/2014-12-06-rebindable-syntax.html) you can even just use it on `String` if you want to. And `String` definitely does not conform to `Monad m` in any imaginable way!
Yup, to be clear: I'm not arguing that do-notation should be in Rust; I'm contending that if special syntax for Future is on the menu, there is almost certainly a valid basis for generalising that same syntax to other stuff.
/r/playrust
Someone recently shared with me a satirical [article](https://medium.freecodecamp.com/welcome-to-the-software-interview-ee673bc5ef6) you might enjoy.
Looks like there's still a buffer in that case. I don't want any buffering at all, I just want to give my string directly to the OS.
Ah right, of course -.-
The buffering you mention there is provided by the C library, not the OS. If this were C I'd use write instead of puts.
wouldn't that just add another layer of buffering? I want to remove all the layers
The trouble I had specifically had to do with how to get data into the output stream that wasn't directly based upon anything in the input stream (eg: a timer runs out).
That's actually why I decided to search for it in the first place.
I expect something similar works, but one would chain the generation of the response (which will be a future) with `and_then`. However, as others have said, it is likely a better idea to use a higher level library like rocket, iron or nickel.
when `async/await` syntax arrived, expect some example with microwave and banana references for `Future`
Well, I dislike whole HTML+CSS+JS ecosystem. :D
Fetch redox and do 'pkg install rust' and try it. It has not been tested with complex programs but a hello world should work. 
&gt; If filtering happens before decoding I'd consider this being bad design. However I agree that it's good to mitigate it. And yes, whitelisting is much, much better.
 You can get the former through the num crate, and the latter is controversial. Many people like it the way it is. It's not a slam-dunk change! IMHO, this one should be inside of rust, or atleast inside of std.I can't use crates in programming contests. and the fact that it makes code verbose makes me 2: Removed 3- So I can have hopes right? 4- youre right. I can use aliases. my mistake :) 5- code example : https://pastebin.com/36iSvdH7 6- I want to be able to change some variable inside a struct without ReAssigning struct. what does it have to do with types? 7- code : https://pastebin.com/AaB3q1rD 8- I understand. but you can change syntax and be backward comptiable. just let rust accept both. in this especiall case it is possible. but I understand if it won't happen :) 9- I was looking for a project to do in rust. I think I should look for another one :)
&gt; C++ is a huge strange beast and learning is is a heck of an investment. Typo: is -&gt; it
The blog is on `github.io`. That means [pull requests work](https://github.com/llogiq/llogiq.github.io/pull/18).
I believe these are mostly crowdsourced, so my hope is that they get better with time.
You can mix `verify!` and `anychar`: verify!(nom::anychar, |c: char| c.is_alphabetic())
Instead of implementing for function pointers only (`fn`), you can use generics and implement for any `Fn`, which includes `fn` and many closures. https://is.gd/PewJDa
For 2. Probably the same kind of bugs described here &lt;https://blog.golang.org/constants&gt; (see the Background C part). Though we probably could learn a thing or two with how Go handles constants.
&gt; Mutability is the property of the binding in Rust, not the type itself. Ah, thank you. I think this is an undersold aspect of Rust and it's one of the first things I reach for when evangelizing. I don't know why but it's been hard for me to come up with as nice and concise a way to state it as you just did. 
This is my dream Rust job. Unfortunately, I just took a (good) new job a few months ago, so I will not be applying :( Good luck to whoever applies.
For image p-hashing, there may have been something in c++/c as well, but rust does seem like a good use case for 25k or so images.
I would love to read that programs source code :) thousnads?! I get the point but thousands?
Sounds like it boils down to &gt; Rust is still quite young.
which honestly makes sense to do. Literal integers should automatically coerce as needed into any primitive, numeric type they will fit into including floats, and literal floats should automatically coerce into whatever float type is required. If automatic coercion is undesirable, the type literal can always be appended like 3.4f32 or whatever.
Very nice! I'd be careful about some of the online services for that though, they can do some funky things with thier instances I hear. Still need to get to using exersism.io for that rust training, hopefully will be able to better comment on that sort by Monday
I heard the same, plus we wanted GUI for demos so we ended up going with digits and caffe on our own servers. Trained quickly and without too many issues. 
To be clear, integers and floats already do let you infer their type.
Got to be careful about digits too. Clumsy co-workers tend to crash that a fair amount. Can be really annoying when they do. Then sometimes you are just left with crying sometimes when you are coding in bed just to get it all out, what's in your head. Really, digits is just a little peculiar.
`let x: f32 = 4;` is what I'm talking about
thanks! well that should be what i needed, but of course I can't combine it with 'now read any alphanumeric'. This is why I freaking hate nom with a vengeance. The idea is awesome, it looks like the right thing to do. But the documentation is shit, the example (if they exist at all) are 90% 'tag' which is super simple and leaves out any complexity, 'just read the doc' is awesome if you understand the entire thing all ready but the learning curve is off the freaking cliff. or I can just hand roll a parser myself and understand the whole thing myself. slower? probably, even likely. less extendable? sure. annoying that I don't get the new coolness? yup. But I've got a problem to solve and I don't want to spend a month just figuring out nom before I can solve my actual problem! ARRRGH! &lt;deep breath&gt; sorry about the rant, but this is like my fifth time trying to use nom for a problem and each time I get only a tiny way into it before I run out of examples to learn from (I learn way better from examples than anything else). Doesn't help that half the 'tutorials' are from older versions. &lt;sigh&gt;
I'm sorry if the phrase was offensive, that is not the intention. I have nothing but respect for everyone involved, and gratitude for the hard work they do. And I am optimistic about the eventual outcome of the projects. I simply wanted to express my opinion, which is to be cautious about making adoption of these libraries such a priority that they unduly influence the underlying language. I take the assurances that this is not happening at their word. Once again, my goal was not to troll or otherwise be noxious, but to open up a frank discussion (which has happened.)
I think you.re wrong on the first count. Rust has gained a lot of adoption already without these features having language support. On the second count you are misquoting me. I made a plea not to push futures support into the language before it's ready. I haven't claimed anyone IS forcing features down users throats. I have simply asked to not do so in future with respect to these features in particular.
Go very similar to Rust in terms of concurrency? They could hardly be any more different!
I'm sorry, but I can't publish it while my lab didn't finish the papers on it :/ However, we're working on genomic processing, so *a lot* of data and our main way of interfacing with other programs is text files or pipes, so we simply `println!` the results.
Writing to STDOUT is literally doing a println! If you're writing unix util that works with, or is expected to work within a pipeline this is a common use case. Also a majorly underrated approach to problem solving in present days. 
LLVM has a PIC backend, by the looks of it, so yes, inasmuch as Rust supports any deeply embedded target at the moment, which means, you need to use Rust Nightly, and you need to do a lot of legwork on your own, but it should technically be possible. For ARM Cortex-M microcontrollers, a lot of the basic legwork has already been done by /u/japaric, but you still need nightly, for now. EDIT: actually, turns out that LLVM dropped the PIC backend awhile ago... so that's fun. Right now, it is not possible to target PIC. But there is an AVR backend coming around that everyone is talking about.
Mine as well. Unfortunately, I don't think I qualify as "Senior" anything.
Both. 
If you're working with text files, you can mark your parsers to consume `&amp;str`, not `&amp;[u8]`: named!(foo(&amp;str) -&gt; Something, ...) That way, all your default types would be `&amp;str` not `&amp;[u8]` and it should work. (It seems that `alphanumeric` returns `&amp;[u8]` if the input type of your parser is `&amp;[u8]`. Anyway, you can always `map` or `map_res` with `str::from_utf8`).
This misses the biggest one in my mind, which is "meh, GC is fine for all my apps". Something like Java is more or less memory safe by default (except for concurrency). If you don't need the performance and aren't already a fan of the type system, Rust might not have enough ROI.
[gist](https://gist.github.com/8274fd22863a1f4d84d9c43378f40897) That's what I thought i did!
I need to learn rust! ;)
It's not a realistic option (or an option at all) on compute clusters and supercomputers. C, C++ and Fortran remain the only viable alternatives among compiled languages. If you're writing something that you think you will want to run on something like that in the future, rust is not the way to go.
Feedback would be very much appreciated! Also, if somebody has tips on how I could test my unsafe code, that would be helpful. I've been thinking of creating a build profile with jemalloc disabled (which I'm sure Google will be able to tell me how to do) to then be able to run a small example program through valgrind; but I don't know if / how I could integrate that with `cargo test`, and it also requires valgrind to be installed. So maybe there's a better way?
Are you sure that that compiler is (only) compatible with the Pi 2 and 3? I'm pretty sure that's the one I have used before when cross-compiling for a Pi 1B.
You need MPI; you need mature mathematical libraries (or rather, bindings to the same); you do need SIMD (even if only with inline assembler) or you may throw away half your performance; and if at all possible you want to be able to link to and use vendor-specific libraries (MKL for Intel for instance) since those confer a substantial performance benefit. Yes, you could write those bits in C and link with your Rust program. But if you do that with your typical numerical codes you've already written 90% of your entire app in the C layer. Might as well write the final bits there as well instead of splitting into two languages. And for many supercomputers it's not an alternative. It's not just Rust — you need to use the vendor compilers to get your code running there at all. The machines have hardware support for their high-speed interconnects or cross-node barrier synchronization and their compilers and libraries are the only ones that can make use of those features. Edit: GPUs are a whole other can of worms of course. But I do expect that targeting OpenCL (or CUDA) would be a relatively minor undertaking. You effectively write a separate program for the accelerator anyhow, so you'd might as well just write those bits in C and leave it to the usual compilers. I don't think Rust would confer a lot of benefits to what is usually, after all, quite small and self-contained bits of streaming code. 
one word....WOOT! Thanks. I was getting so frustrated. I was a day or so from just rage quitting the project till later. I have sooooooo much to work on for this, but this moves it one step closer. thanks again.
rustls hasn't been audited either. The native-tls crate is probably the safest choice for OP right now.
what lib did you use?
I ended up using piston's image library to unify the formats (we were working with old school tiffs) and resizing, img_hash to generate hashs, and Rayon parallelism library for well parallelism lol. 
Because they seemed mostly observational, I felt like I could support you in this response all the way up until your last paragraph, at which point you seem to resort to something that matches same tone you've criticized. I still think your first two paragraphs are valid, though! :) The OP may still be operating under stereotypes in other PL communities we're trying to avoid. Let's help him out!
Macros are processed before types *exist*. You'd have to fundamentally restructure the language and the compiler for it to work, likely losing things like order-independence in the process. It's very probably not worth it.
Basically developing for PIC sucks in general (even though the hardware itself is fine) because Microchip persists with their ridiculous practice of charging for their fork of GCC 4.1 (or so). Even though it is technically open source, it has diverged so much from mainline GCC that nobody could merge it back in if they wanted, and Microchip can't reap the benefits of contributing upstream. So the PIC toolchain is stuck in the dark ages with no hope of modern language support.
I'm actually trying to parse c code. but yeah I'll look into it. Thanks again.
Senior practicioner of type-fu? (from your flairs)
Fully support for removal is now in master, if you want to play around with it.
&gt; Complaints include overabstraction Which abstractions should be removed?
&gt; Mutability is the property of the variable in Rust, not the type itself. Except for mutable vs immutable references, in which case it is a property of the type... right? I agree, words are hard. 
I was turned down 2y ago after 3 interviews, you never know, give it a shot! Might not be so much about your skill set, not as much as you might think anyway. I'd apply again if I could!
The article's last point conflates two very different types of existing investment: A) skill (how hard is it to become as proficient at Rust as C++) and B) codebase (how expensive is it to convert your existing tech to start using Rust.)
Check the qualifications, they only require "BS in Computer Science or equivalent experience", which is something most Rust programmers probably have. The interview process might be very tough, but if it's your dream job you might as well give it a shot.
Yes, Rust *is* very portable. I don't deny that. However there are still platforms where it isn't and asking people developing for those platforms to use Rust isn't going anywhere.
That and Rust may be one of a number of languages to use. Not everyone needs it RIGHT NOW.
But imagine async/await was baked into a nightly version of Python years ago so they had some time to iterate, and then released async/await to the public. Kind of what happened a few months ago, isn't it?
Depends on what tools one is willing to use. If I had to do PIC development I would be an happy Pascal or Basic developer, assuming I had anything to say regarding which tools to use. https://www.mikroe.com/pic/
There are companies working with Rust in production and either they stick to one version (stable or nightly) or they haven't had as much breakage as you claim; otherwise we'd hear from them. I'm genuinely surprised you have so severe problems with it. I'm also curious what breakage you encountered?
Hi, This is /r/rust, the community for the Rust programming language. I think you most likely want /r/playrust, the community for the Rust game.
they are on nightly because they actually need features that are not yet in stable rust so they can't just release a version for stable.
Rust is still in heavy development and there's many features not stabilized yet. Some libraries make heavy use of unstable features so programs using them will need a nightly compiler. But for many programs it's expected you can use a stable compiler just fine. But note that you can run dev tools on nightly (such as rustfmt and clippy) but still compile your code with the stable compiler. That's because rustup can install stable and nightly compilers alongside each other, and cargo can take a parameter to run a command on nightly. Because of this, the fact that rustfmt needs nightly to run is just a funny implementation detail.
I kinda agree with you: if you don't care about performance at all, using a language with garbage collection can make you way more productive. *However*, note that GC is not actually a solution to all our problems with the only disadvantages of runtime overhead. Compared to C-like `malloc`/`free` workflow, a GC avoids most of all memory leaks. But memory leaks still happen, it's just more hidden. [This talk (which explains everything in this comment far better than I do)](https://www.youtube.com/watch?v=4L5T96J10dQ) mentions so called "lingering objects". A more complete solution is RAII: here we can deal with all kind of resource cleanups. And I don't think there is a way to marry GC with RAII: GC fundamentally makes destruction indeterministic which makes RAII impossible. So by making it easier to deal with the most common resource, GC makes it impossible to properly deal with other kinds of resources. --- I guess my point is: GC has more disadvantages than merely "not that fast". And while many programs probably don't care about this runtime-overhead, they probably^IMO should care about properly handling their resources. 
1a) Rustfmt and Clippy are special cases because they rely heavily on compiler internals. 1b) Many other libraries can work on stable Rust, though may benefit from unstable features; in this case it may be a trade-off depending on how much need there is to make the library work on stable. 2) I don't think there's a good answer to that one, though I think a lot of developers do try to use stable now. 3) There are rarely forward compatibility issues using nightly except for use of [unstable features](https://doc.rust-lang.org/unstable-book/).
Also, supercomputers only support data transfers with verified users via ssh (or hpnssh or similar) which means no cargo, so users have to either tunnel it via their own ssh connection or loose a very important part of rust workflow. Most "modern" supercomputing software is written in C++ nowadays and those writing that software are the ones that might want to switch to rust (those that cannot switch from C or fortran to C++ cannot switch to rust either) but these devs are just used first to integer genetics, and second most of them write EDSLs in C++. These two things are almost impossible to do in Rust. So they will find it very limiting, at least compared with C++.
I'm trying to use `error-chain` to do my error handling for me. I have an `errors.rs` defined as such: extern crate error_chain; error_chain! { errors{ JotNotFound {} } } and all my other files use `use errors::*;`; `main.rs` also includes `mod errors;`. However, I'm always getting a `no associated item named 'JotNotFound' for type 'errors::ErrorKind' in the current scope`. I've cross-checked with numerous repos using `error-chain` and I can't figure out what I'm doing wrong, but since it's a scope error, it's probably something stupidly obvious.
Senior: having actual experience and some clue how to distinguish bad code from good one and knowing why. Professional: just the experience but no code smell. Junior: I have read the documentation Enterprise: I like to wear dress shirts, write more documentation then code and create very large visio diagrams.
Writing your own nom functions is really easy. You don't have to use macros for everything. I resort to functions quite often, simply because they also make the code much easier to read compared to all those macros and closures.
Lots of features require nightly. And for me - my applications aren't that big that nightly breakage affects them much. For example, rust went from `pub(module)` to `pub(in module)`. It's like two or three areas in the code that break, it's trivial to fix. I don't have a codebase of 100k lines to maintain. Plus some libraries need nightly so I am more or less forced to use it, too. Features like matching floating point number ranges, type IDs, etc. are not on stable yet.
I would even go so far as saying that GCs just enable spaghetti data flow. One of the neat things of Rust is that it is very hard to create cycles of objects. Some people will complain about it, of course, however for me this has the immediate benefit of making the data flow cleaner. On the contrary, in Java, I've seen objects pass `this` to others, and using callbacks and observer patterns nested into visitors and... the whole shebang doesn't leak (GC) however the data-flow becomes so convoluted that it's hopeless to try and understand (statically) how the data arrives there, and where it goes from here on. You **have** to break out the debugger and find a way to create a test-case... and hope it matches what you really are trying to do. After this mess, clear ownership is such a relief!
Not when coming from C++ :(
Please note that the `rustfmt` crate (`cargo install rustfmt`) works on stable. The next version, which uses the compiler's libsyntax (instead of a fork) is available as `rustfmt-nightly` and is what the repo's master branch contains.
&gt; Has anyone had any issues developing on nightly, such as having to remove a bunch of code right after rust developers deprecate a bleeding edge feature? Or, alternatively, is this not an issue? I'm lame like you. :-) While I tend to use a nightly compiler for development, just about all the code I write is for stable Rust, so I don't experience any breakage. Using a nightly compiler is useful for running benchmarks or one of a few useful tools that are coupled with the compiler itself (like rustfmt or clippy). I'm curious, though, why you said "so many libraries?" rustfmt is a developer tool...
I'll give that a shot. Thank you.
If its the case that dropping Session early will ensure the timeout will never be run then it should be safe.
The session borrows the data (and the data lifetime must live as long as the session), so you can't drop the session when you set a time out. If it can happen though, it is unsafe because the timeout will still run if the session is dropped. 
This is one of the few times where I don't think anyone would fault you for reaching for `unsafe` because it makes things so succinct (and it's kinda hard to screw up, honestly), though you do have to do some funky casting to break the intermediate borrows (example using generics): pub fn get_two_mut&lt;T&gt;(slice: &amp;mut [T], a: usize, b: usize) -&gt; (&amp;mut T, &amp;mut T) { assert_ne!(a, b, "indices must be disjoint"); unsafe { ( &amp;mut *(&amp;mut slice[a] as *mut T), &amp;mut *(&amp;mut slice[b] as *mut T), ) } } 
Unfortunately, `RUSTFLAGS="-Z sanitizer=address"` and `RUSTFLAGS="-Z sanitizer=leak"` error with ==4202==LeakSanitizer has encountered a fatal error. ==4202==HINT: For debugging, try setting environment variable LSAN_OPTIONS=verbosity=1:log_threads=1 ==4202==HINT: LeakSanitizer does not work under ptrace (strace, gdb, etc) and msan is very annoying to deal with: it requires all code including libc (and libproxy in my case) to be compiled by LLVM, with msan enabled, otherwise it will just report false-positives everywhere: A fact that is hidden [near the bottom of its documentation page](http://clang.llvm.org/docs/MemorySanitizer.html#handling-external-code).
Can't you ?
In that case, its unsafe because men::forget exists. But if that's the case then that code shouldn't compile, since the compiler has no way of knowing whether the timeout was run when it drops session at the end of its scope. How does the event loop work? Does it run in a separate thread independent of the session it was created by, or does it require session.dispatch_event_loop() to progress? I think your borrowing reasoning is backwards, if a borrows b then a CAN be dropped, but b can't as its being borrowed by something else. So in your example it would be data that couldn't be drooped, not session
Java's type system, with all its warts, is more powerful than people often acknowledge. That said, I have a similar problem with one of my projects that leads to massive code duplication (where in Rust I could count on monomorphization).
I'm trying to wrap my head around tokio stuff and I'm finding it super hard to do. I've got some code here - http://sprunge.us/ELhF It fails spectacularly during compilation - http://sprunge.us/QUZG. Here's my cargo file so you can easily replicate this - http://sprunge.us/WFTY. I can't seem to be able to return a future that sends something off after I've read in data. Basically, I want to map an incoming future to an output future. SEND HELP!
I technically could but it would negatively impact my current quality of life. Maybe sometime in the future I will :)
I am using C++ since 19 years now (and despite my love for Rust won't stop any time soon). I feel a lot of knowledge translates well. The knowledge that does not translate well deserves to be deprecated, mostly (pimpl anyone?). Still missing Pi types though, ...
I wish the same keen sense could apply to data-races, unfortunately experience so far seems to imply that those are much more intricate to spot :(
Is there a rust equivalent to ruby's highline (https://github.com/JEG2/highline) - I've rewritten the pieces that I need for my app as a learning experience but if there is a crate out there I'm at a point now where I'd rather contribute to an existing solution. If not, I can post mine up.
I'm a pretty comfortable C programmer, and I like writing C code. But I'll always write new projects in Rust, simply on the basis that if Rust can't do as good a job, I want to have first hand experience about *why* and maybe find some way to make Rust better. So even though Rust may not be the best for everything, it's still worth working in if you're invested in making it better at the things you want it to do. Of course, this is easy to say when it's all hobby programming, and not constrained by budget, deadlines, and politics. Once those come into the picture, "Rust evangelism" should be a total non-issue.
It's my first time writing a blog post like this, I appreciate any and all feedback.
&gt; 2. Which is the real latest version of rust? Stable or Nightly? Taking "latest" in terms of time, nightly is the "real" latest version of Rust. The code in the stable release is between 6 and 12 weeks old (depending on where we are in the release cycle), and nightly's code is usually at the most a few days old. There's also beta in between there :) Regarding 3, I usually develop using nightly to get the latest and greatest improvements in things like compile times. I rarely use unstable features though, so the worst issue I run into is that there's a bug in a particular nightly, so I just use a nightly from before the one with the bug. I have CI testing my code on stable, beta, and nightly.
Yes, /u/neuronsguy, I'd be interested in your thoughts on how the Rust teams could have communicated the status of these experiments more clearly to have addressed your concerns before this post. We're always trying to get better at communicating out to the community.
~~Yes, it is supported but only on nightly for now.~~ **EDIT:** I missed the part "stable Rust" of your question, so no. It requires Nightly. I gave this a try a couple of weeks ago, code here: https://github.com/anderejd/extern_attrib/blob/master/src/lib.rs crate here: https://crates.io/crates/extern_attrib I don't remember where I found all the different pieces of documentation, but I think these links should be a good place to start: https://github.com/dtolnay/syn https://github.com/dtolnay/quote I would also recommend reading the source code for syn and quote. I hope this helps, good luck!
Yeah, so that's a thing. `&amp;mut T` is basically an exception. We actually even argued if it should be called `&amp;mut T` or `&amp;uniq T` during Rust's development.
&gt; "why Rust's affine&amp;borrowed types and imperative control flow makes existing monadic abstractions unusable" I think lifetimes + futures are already problematic right now, don't even need monads for that! Or perhaps that is because futures are based on a monadic abstraction? 
WoW. --- &gt; Without segmented stacks we are stuck allocating reasonable sized stacks and hoping we don't overflow them. This increases memory requirements and creates a chance of crashing in mysterious ways due to stack overflows. Unfortunately I'm pretty sure it's not possible to fix this without compiler support. I don't think it is necessarily unsafe. The same mechanism that Rust use currently (using a page guard at the top of the stack) should be accessible without compiler/runtime support. Of course, since the Rust run-time does it anyway, it might be worth simply asking that the API which allocates thread stacks be exposed. --- Regarding safety, I seem to remember there were issues mixing scoped threads and coroutines. It seems that you took the appropriate precautions here (`'static`, `!Send`, `!Sync`) to avoid this issue, but as you mentioned it's a tricky subject *shrug*. 
Yes, this. I write all my code on stable, but use nightly for these tools.
Yeah, I should be able to implement a page guard (I think I mentioned that in a footnote too). Like with the normal stack it's not ideal because a large array can cause you to skip right over it, but it's an improvement. Shelling out to Rust's current stack allocation is an interesting idea, especially since that would hopefully mean that if rust ever gets segmented stacks my code would as well.
'cargo check' for most of your code validation, saves a lot of time during development.
Java Streams have a severe problem related to exception handling. And then there is exception handling itself. I was realizing the other day that Java's Closeable, is a horrible wart related to the GC. It struck me that close() is really not all that different from free() in C. It can be very destructive to applications when close() isn't called properly. The new try with auto close syntax is nice, but it's nowhere near as powerful as the guarantees of drop in Rust. Result in Rust was such a great decision. So while I agree that Java is mostly good, there are some many benefits from Rust, that I'm doing the crazy thing and trying to take my Java oriented team at work and thrusting them into Rust. I'm writing the code right now, soon they will ;)
&gt; Shouldn't libraries that would benefit newcomers and companies using the language release a copy on stable at least? These are tools, not libraries. It's fine to make tools work on nightly only; this won't lead to your code breaking, but the tool might not work for a few nightly cycles. No big deal. As long as you don't use any feature flags on the nightly compiler, stuff shouldn't break; it's almost the same as using a stable compiler when you do that. This also means that you code will continue to compile on stable even if you're locally using nightly to develop. Wrt clippy some folks do `cargo install +nightly clippy` and `cargo +nightly clippy` with stable as the default so that they can keep running stable locally but use clippy with nightly for linting. Rustfmt and clippy need nightly because they hook directly into the compiler. Stable rust doesn't let you do that, because compiler internals are not stable. We plan to get both of these on stable via rustup, but there's still time for that to happen. FWIW, `cargo install rustfmt` works on stable, it's `cargo install rustfmt-nightly` that doesn't. This is something that's in active flux right now, which is why it's confusing. --------------------- What other libraries are nightly only? I feel like you're exaggerating a _lot_ here, rustfmt, clippy, and some _experiments_ (the async stuff for example) are nightly only at this point. Not "so many libraries".
One nit: &gt; The trait is unsafe because a bad implementation that returns non aligned memory, too small a buffer, or just null/an invalid pointer, can result in a segfault. This isn't really true. The existence of that pointer cannot cause a segfault, only attempting to deference it can. There are very few (if any) cases in the standard library where functions/traits that return a pointer are considered `unsafe`.
I mean, I have two choices here. An unsafe trait and a safe `start` function that takes a stack. Or a safe trait and an unsafe `start` function. I'd rather call the trait unsafe because that way users can use my implementation of allocating a stack, and the start function, without having to use `unsafe` themselves.
&gt; you can't easily use the internal thread pool for your own async stuff What internal thread pool?
Oh, well that makes plenty of sense.
Actually it is, because cargo doesn't handle binary crates and incremental compilation and linking aren't yet available. Not every C++ project is a pile of template meta-programming headers. And I hate with passion headers-only libraries. So, thus far most of my C++ libraries, for integration within managed languages, happen to compile faster than any Rust project I have tried. Of course, I expect the overall experience to improve and kudos to everyone involved in making it happen.
Not when you need to see the changes.
Great post! Though, I would argue that if we don't encourage Rust usage, then it won't be adopted. I don't think that was your point, it was more listen to why people are saying they can't use it, and trust them to know better. This is good advice in general. &gt; But it’s there for a goal and that is not to take over the world: I know I sound crazy, but: Rust is in a great position to be the "One True Language". It is scalable in terms of usage, up and down the stack. It can be used to target the most low level systems, native applications, and with WebAssembly it can target the web. Yes C/C++ can do all of that, but they don't offer the safety and ease of use of Rust. If your loose with Cell, Rc, Arc, Box and Clone; Rust is just as easy to write as Java and JavaScript... &gt; The goal is to raise the safety level of all software – written in Rust or not. The end game for Rust is to be replaced – hopefully – with something even better. I sure hope I live to see that thing. Given the age of C and the length of time it has been used, I actually don't want to see Rust get replaced! Maybe a v.2 that becomes easier to use, etc., but I hope it's not replaced anytime soon! We still need to rewrite everything in Rust, right? ;) People can't handle another 'rewrite everything in &lt;new-language-here&gt;' campaign...
When he says "stick to one version" he means "stick to a release train". Firefox uses a lot of Rust and we only use Rust stable. Rust stable does not break. Use Rust stable, and you should be fine. Or, use Rust nightly (it tends to be faster with better tooling), but don't use `#[feature()]` (which enables individual unstable features). If you use unstable features, they _will_ break, you can't blame the stdlib for that.
`setuptools-rust` has a `no-binding` option now so it can be used with CFFI for example. Snaek looks like a real step forward though and I'm excited to use it on a project!
The compiler already generates stack probes on platforms where LLVM supports it. This avoids the problem of accidentally skipping over the guard page(s).
&gt; there might be reasons for using C that we hadn't thought of. I don't think you'll find a reason for using C that isn't widely known and has't been discussed many times before.
Is this fully remote work? I have a contract going right now but remote + Rust is pretty much all I want.
Stack probes are just making sure to touch every page when growing the stack? So this means that implementing a guard page would actually make me "safe" (i.e. reliably segfault) against stack overflow?
I don't think it will be replaced fast – look how long it's taking us to replace C! What I mean is we should be happy if Rust leads C++ devs to write safer code in C++. As long as the code is safe, we win. And if /u/desiringmachines is successful in making full-program verification usable in production, we won't miss Rust at all...
Yes, it was a sarcastic jab. That being said, the projects I work for tend to: - be statically linked binaries, - with many small gtest binaries. And the sheer amount of time spent relink all those binaries for even the most trivial change completely dwarf the production of the object. You really have to make sure that you only rebuild the binary of interest :x
I think kernel stacks need to be small because they are always resident, but it doesn't apply to userspace. Edit: this was actually fixed recently, [there's a great LWN article about it](https://lwn.net/Articles/692208/).
You could test it out by using the system allocator instead, and then comparing.
You're looking for /r/playrust.
If you always jump between the same two points this shouldn't be an issue: the TLB has more than 1 entry. If you jump between thousands/millions of positions scattered all around the virtual address space, now that's another story.
Shouldn't affect TLB misses, since even "small" stacks are bigger than typical pages, and if you want guard pages you have to allocate pages anyways. It would have a bit of an effect of allocating more page tables though.
I saw it in a stack overflow answer while writing this blog post, I was surprised and somewhat skeptical. Haven't verified either way. Edit: I can say that in my tests using stack allocated stacks I never experienced the kernel doing that to me.
It's more that there's generally relatively few benefits from performing huge allocations on the stack. For small allocations, the overhead of making the allocation compared to the cost of using said allocations is really high, which is why you'll get a really nice boost if you can avoid dynamic allocation. On the other hand, it takes jemalloc ~20 instructions to perform a memory allocation in the happy case whereas it'll take you 100s/1000s of instructions to initialize that multi-KB array. I mean, even a vectorized zero-initialization of the array can only write at most 512 bits at a time: for 4KB that's only 64 instructions calls so it still makes sense, for 4MB that's 65536 instruction calls so the cost of the allocation is dwarfed. And that's the simplest form of allocation, if you have any computation to perform it'll cost more. Of course, you won't get the happy path of jemalloc every time; more specifically the first time is likely to be slightly more expensive (TLB miss, OS involvement, ...). However if your program only need it once, I have trouble understanding how that affects its performance, and if it needs often, then it's simple enough to pre-allocate once and then keep reusing the buffer. I really don't understand why some are all googly-eyed at the idea of being able to allocate large objects on the stack; I must be missing something :x
I can't answer you question directly, but I think you might find this blog post interesting. &lt;https://lifthrasiir.github.io/rustlog/why-is-a-rust-executable-large.html&gt; Edit:. I realized you mentioned memory usage. I don't know why I was thinking file size.
That was the case for everyone, I'm not quite sure why the theme has them so big. Shrunk :)
Gonna do more (different) tests, but it was a release build if my memory isn't wrong. How much do you get when you statically link with musl instead of using glibc? It should be even less than your cited 1MiB.
Are you sure the object code overhead of statically linking everything but system libc can explain the initial/base memory difference?
Indeed! That's a very interesting implementation that I hadn't seen.
Howdy, happened across this thread, the OP didn't write the blog post, but I'm here! :) I don't have plans to implement MQTT currently. That said, I may be able to make the layers of this project into crates that would be usable to someone who is solving that problem. I'll keep that in the back of my mind as I continue through this project. The goal of this is to learn a lot about message brokers in general, finding places where they made trade offs and deciding what tradeoff I'd like myself. Sometimes, I may align with a broker (my channel naming is very similar to RabbitMQ's exchange names) other times I may choose to deviate to force myself to think of more of the trade off, the protocol and verbs allowed is another place for that. Also, I recently became unemployed, being more self directed than directed by specs or managers is very nice right now.
FWIW, I think a lot of stuff depend on serde, which in turn depended on features in nightly. The necessary features have now been merged into stable, serde now works on stable, and so hopefully many other libaries will migrate to stable too. There's always going to be a thing where the LATEST versions of a crate depend on the latest compiler though, I suppose. Maybe try an older version?
A trait is not the same as a type though. At the end of the day you're returning a raw pointer, which no longer encodes any of the assumptions you've made, and requires code which interacts with it to be unsafe. I think `Unique` and `NonZero` are better examples of the kind of types I'm referring to. Even slice or `str` is a good example. `str::from_utf8` and `str::from_utf8_unchecked` is exactly the sort of thing that I'm getting at. `str` encodes the assumption that the bytes its pointing to are valid UTF-8. Once you've constructed a string, you've either verified that is the case, or used `unsafe` to construct it. The assumption is now encoded, and anything which is unsafe were that assumption not met can be confined to a single place.
You don't need to. Call stdout.flush() when you want to write something without a newline. https://doc.rust-lang.org/std/io/trait.Write.html#tymethod.flush
At it's most basic level, "not subject to an RFC process and committed to stability guarantees outlined by the Rust project". Compiler internals can change at any time and even inbetween point releases, which is why they aren't advertised as being stable. An additional point to the above: Rustfmt and Clippy rely heavily on compiler internals, but will eventually be distributed with the compiler itself (really rustup) so you will just have a corresponding version for each Rust version you install. This sidesteps the issue of achieving stability for these interfaces.
How about `Some(80...90)`?
Interesting. We'll be using MQTT for a broker for realtime "find me" style messaging in a mobile application. At some point we will definitely exceed the capabilities provided by the prevailing MQTT brokers and will be interested in building our own (in Rust, of course). Perhaps when that time comes there will be some things on which we can collaborate. 
&gt; I would even go so far as saying that GCs just enable spaghetti data flow. The whole point of GC is to cope with spaghetti dataflow. Once you have a powerful ownership system available though, as in Rust, there's no reason to make either GC or reference counting ubiquitous; they should only be used where they are strictly needed.
Awesome. That would suit my traveling intentions nicely. Wish I'd seen the posting before my contract was renewed this week. Think I'll have to check back in after the contract is up, though maybe I should apply anyway..
Things that don't work on stable that I use in my hobby projects: * The `eprintln!()` macro * [Atomic integers](https://doc.rust-lang.org/core/sync/atomic/struct.AtomicU64.html). * Benchmarks and tests &gt;Has anyone had any issues developing on nightly, such as having to remove a bunch of code right after rust developers deprecate a bleeding edge feature? Or, alternatively, is this not an issue? Not yet, but it's unstable for a reason. I have run into an API change once. &gt;Which is the real latest version of rust? Stable or Nightly? Pretty silly question if you ask me. &gt;Shouldn't libraries that would benefit newcomers and companies using the language release a copy on stable at least? It's more important to keep the API stable and working. If Rust had to be stable *this moment*, it would be a big problem in terms of stifling potential innovation. Rust is a young language and it needs to experiment! Nothing else really fills the same niche as Rust at the moment, and if Rust can take advantage of its place it can be immensely successful.
Ah, I wasn't thinking about the `FnOnce`, `FnMut` and `Fn` problematic. Thanks for clarifying.
Memory.
I'm not fully informed on the topic, but I think you're correct: the divergence of syntex and libsyntax suggests even after extracting the library from the Rust compiler internals, minor discrepancies accumulate quickly and cause a maintenance burden, not once but twice, since the Rust compiler is also written in Rust and often uses bleeding edge features. Since syn displaces most of the users of syntex (macro authors) it's largely become a developmental dead end vs ensuring the handful of tools that need it, rustfmt and clippy, are updated in lockstep with compiler internal changes.
&gt; What? Which datatypes use unstable things? Please give some details. You just gave your own examples; essentially the entire dynamic alloc interface is unstable making it obviously very hard to define a lot of interesting data structures yourself without relying on that. &gt; Box and Vec do, because you're supposed to use Box and Vec for allocations outside of the stdlib, not the APIs they rely on. (Allocation APIs are coming, and for now you can build your own vec and box by directly calling malloc if you wish. It's not the same thing but it works.) Yes, but my point is you could not have made those yourself without relying on unstable intrinsics. The documentation boasts how things that are primitive in a lot of languages are in fact just standard library as a testament to the expressiveness with the implication that you could've made them yourself were they not provided. That's a half truth: you can but not relying on only a stable interface. &gt; Many of these things are wrappers around intrinsics, basically, because we don't want to expose intrinsics, but these wrappers expose all the relevant operations, which you are allowed to use. That says nothing about whether the boast that the Vec and the lot are "implemented purely in the standard library" is a half truth giving a false impression. When people read that they assume "oh, I could've made this myself if it wasn't provided" and assume you can use a stable API to do so. If Rust didn't provide `Vec` you couldn't have made it yourself without relying on unstable features.
Good call. This might be me not understanding Tokio enough, but: * `Core`, `Handle` and `Remote` all seem to be three different smart pointers to the same thing. Do we really need all three, and if so could they be named so it's obvious that they represent the same kind of object? * I'm thinking that maybe the entire `task` thing could be hidden from the user. Basically, you would instead wakeup the `Future` or something you get from it, like a FutureHandle or something...? (You might still use tasks internally, just not expose that abstraction to the user.) 
&gt; essentially the entire dynamic alloc interface is unstable making it obviously very hard to define a lot of interesting data structures yourself without relying on that. Right, because Box and Vec are how you get dynamic alloc outside of the stdlib. Yes, they are implemented using unstable APIs, but they expose stable equivalents to those unstable APIs, which regular code can use. &gt; Yes, but my point is you could not have made those yourself without relying on unstable intrinsics. Sure you could, just use `malloc`, the unstable allocator API is for using the allocator chosen by the user. Choosing an allocator _itself_ is an unstable feature, so I don't see why this API shouldn't be. If you want pure stable code, just link to malloc/jemalloc. Or, do the thing I mention below. &gt; If Rust didn't provide Vec you couldn't have made it yourself without relying on unstable features. But Rust _does_ provide Vec, which exposes all the functionality you need to build your own Vec (which can have its own semantics and stuff.) Vec gets you dynamically-sized allocation, Box gets you statically sized allocation, and that's it. `Box::into_raw`/`Box::from_raw`, `Vec::as_ptr()`/`Vec::from_raw_parts` give you the equivalent of an allocation API. Using just those as an allocation API, you could build your own Box and Vec. This is not the vacuous statement of "oh well you can build Box using Box", you can build your own Box/Vec with _whatever semantics you want_ using these as allocation APIs. In fact, there already are a lot of crates out there with alternate datastructures that do stuff like this. (Also, I'm not sure what "boasts" you're talking about; link?) ------------------------- Basically, the stdlib has a right to rely on private code, like any other library. It still exposes the necessary functionality. But it has a bunch of internals which are "private", like the allocator API. The main "language magic" there is that it hooks into the functionality for choosing an allocator (which itself is unstable); and both the functionality and the API will likely stabilize simultaneously.
I haven't looked deeply into the others, only skimmed them, but my impression was all the others wanted to be stand-alone ‒ have their own scheduler and provide coroutines as the only primitive to be used. Therefore, they'd compete with the current de-facto standard of futures and tokio. I tried to provide coroutines only as an extention on top of tokio, to be able to combine the approaches and to be able to reuse all the already existing asynchronous implementations of various protocols.
Oh ok :) I didn't exactly mean for it to be rude, so I apologize if it was.
`Some(&amp;80...90)` works though. https://is.gd/bfXbNq
Author of coop here. Coop was really just me making/learning about the primitives needed to make coroutines and the like. I'm pretty happy with how the `Generator` api came out, but the coroutine api was very much just a quick proof of concept to show that the lower level primitives do what you need. If I was implementing it for actual use I'd likely be trying to integrate with tokio/futures like Corona. Unlike libfringe I made no particular effort to be efficient (indeed I didn't even make use of the redzone, which would let me remove add/sub instructions whenever switching coroutines for free, because I decided it would be better for the blog post to be explicit about it). While I don't want to put words in edef1c's mouth, it looks like libfringe largely has the same goal of experimenting with primitives. It tries to preserve stack traces (for generators), and be as efficient as possible. I don't think coroutines are a goal. Corona just uses Boost's primitives (which are very close to the ones I implemented), wrapped up in a rust library (context). It looks to me like it's focused more on experimenting with how to expose a nice API to end users that plays nicely with futures. I haven't looked closely at any other implementations.
Haha no prob! :D
Glad you like it, I'm pretty happy with that API too. If you're just looking to return an iterator from a function, you can return `Box&lt;Iterator&lt;Item = Blah&gt;&gt;`, or if you're willing to use a unstable feature `impl Iterator&lt;Item = Blah&gt;`. Both will be far more efficient than using one of my generators.
How? Their point is that you can't build box and vec outside of the stdlib. I show that you can, in multiple different ways.
&gt; you could not have implemented them yourself _and be confident they continue to work the next version_. (Emphasis mine.) No one said you can't build these outside of the stdlib; they said you can't build these outside of the stdlib "without relying on unstable intrinsics," which is what you're also saying, AFAICT (though you mention unsafe C calls as an alternative).
But I didn't!. You can build these outside of the stdlib on stable by just using `malloc` or the `jemalloc` C APIs directly. Or, you can build these outside of the stdlib on stable by using `Box::into_raw`/`Box::from_raw`, `Vec::as_ptr()`/`Vec::from_raw_parts` as an allocator API. That's the _intended_ way for folks to allocate/deallocate things. The internal allocator API provides some more powers, but they're mostly unnecessary here. Also, the allocator APIs are not "unstable intrinsics", they're just unstable functions, which indirectly call malloc or jemalloc based on your configuration. The ability to configure the default allocator being used is itself unstable, so I'm not sure why this is a problem. You can build Box and Vec yourself, either by using malloc or by leveraging stdlib Box/Vec as an allocator API, as they're supposed to be. There is an unstable feature that exists, and to support that feature, box and vec call unstable allocator APIs. So you cannot build a Box and Vec that have support for that unstable feature on stable, but ... that's a nonsensical thing to try anyway?
It doesn't run in a separate thread, it requires dispatch_event_loop to continue. In this case, `Session` just owns the means of progressing the state to run the callback. If it is dropped, then the data will never be accessed by the timeout. That should be safe then, shouldn't it? The only thing that matters is data must live at least as long as the session, if the session does before hand that's ok because it just has a reference to the data. 
It doesn't run in a separate thread, it requires dispatch_event_loop to continue. In this case, `Session` just owns the means of progressing the state to run the callback. If it is dropped, then the data will never be accessed by the timeout. That should be safe then, shouldn't it? The only thing that matters is data must live at least as long as the session, if the session does before hand that's ok because it just has a reference to the data. 
A large executable size will lead to large memory, since your OS generally loads the whole file into memory in the process of executing it.
It doesn't run in a separate thread, it requires dispatch_event_loop to continue. In this case, `Session` just owns the means of progressing the state to run the callback. If it is dropped, then the data will never be accessed by the timeout. That should be safe then, shouldn't it? The only thing that matters is data must live at least as long as the session, if the session does before hand that's ok because it just has a reference to the data. 
It doesn't run in a separate thread, it requires dispatch_event_loop to continue. In this case, `Session` just owns the means of progressing the state to run the callback. If it is dropped, then the data will never be accessed by the timeout. That should be safe then, shouldn't it? The only thing that matters is data must live at least as long as the session, if the session does before hand that's ok because it just has a reference to the data. 
Compile time checks are the point. Oftentimes, memory issues will not be immediately apparent in C++. I have lots of experience with this. Things will work fine, most of the time, then a random part of the code will crash unpredictably, and not the part of the code that you expect. A sufficiently skilled programmer does not exist, as evidenced by even Curl having a number of preventable vulnerabilities every year, that any memory safe language would have prevented, and Curl is developed by brilliant people.
It doesn't run in a separate thread, it requires dispatch_event_loop to continue. In this case, `Session` just owns the means of progressing the state to run the callback. If it is dropped, then the data will never be accessed by the timeout. That should be safe then, shouldn't it? The only thing that matters is data must live at least as long as the session, if the session does before hand that's ok because it just has a reference to the data. 
&gt;&gt; I have lots of experience with this. so do I. generally to get something interesting done, 'debug code' is required anyway, not related to memory problems, but checking (eg.) that after encoding a datatsructure in some manner, it can be decoded and it still gets the right result, etc. I've come to Rust because I it does several things more elegantly; however, when it comes to actually getting things working, I can still do this faster in C++ .. because it's faster to write the tests that I need for other reasons (and of course the mature tools help). &gt;&gt; " then a random part of the code will crash unpredictably, and not the part of the code that you expect." if there'a a memory problem there would have been some behavioural problem aswell - indices out of range = faulty logic calculating the indices; dangling pointers = things which have been destroyed at a time when dependancies still exist. So in my experience, simply getting a program *working* has synergy with getting it 'memory safe'. I think you could retrofit a borrow-checker as a tool over C++. All it would take is to make an annoted smart pointer for a reference type e.g. ref&lt;T,N&gt;. the 'N' would be ignored for compilation, but used by the analyser. ban the use of raw pointers outside of a few key source files, etc etc. Of course it wont be a 'compile time error' - it would be a static analyser you must **choose** to run. But is that any different from **choosing** to use rust and not choosing to write 'unsafe' ,'unwrap' etc everywhere :) It isn't a magic bullet. People are going to need reasons beyond safety to switch language. (my reasons are: being fed up with various syntactic frustrations, chasing shiny new toys.. the desire to try something different after being stuck with one set of problems for so long.. but as yet I haven't found it unambiguously beneficial, unfortunately. There's one set of things it improves but you still pay for it in other ways. It is very promising though and has opened my eyes to some possibilities)
it was specified that residential memory was measured
yeah, also long running applications!
OK in that case it's safe. As long as session is not Send.
Exactly. On the other hand, it's okay for the stdlib to use nightly stuff, since it's developed in synchrony with the compiler.
&gt; ...Haskell is an idealogical can of worms in itself... It really isn't. You can even opt out of laziness-by-default entirely these days, and end up with something not unlike Ocaml except with a far more developed software ecosystem.
For comparison a boring Rust program on `x86_64-pc-windows-msvc` (which uses the system allocator by default) has a private working set of 276KiB and a total working set of 2,148KiB.
That's pretty cool. Wonder how it handles strange OSes like Nix.
Doing a little dance like this fixes the problem trait Get&lt;'a&gt; { fn get(&amp;self) -&gt; Result&lt;&amp;'a u32, i32&gt;; } fn check&lt;'a, T: Get&lt;'a&gt;&gt;(foo: T, data: u32) -&gt; bool { foo.get().map(|data| &amp;*data) == Ok(&amp;data) } Wtf!?
Say I have a struct/impl representing a Board in a board game. This Board has a 2d array of u8s, which stores all the pieces at all positions on the board. (Say 0 = empty, 1 = rook, 2 = queen, etc). This board will be moved around a lot, sent all over the place. But because the underlying data structure is an array, which is stored on stack, that means every move is going to copy this array, right? So if I'm moving a lot of these Boards around, it makes more sense to store a Boxed array instead of a "stack" array, because then each move won't copy the board array, instead it will just update a pointer, right? Bonus question: I just started writing Rust yesterday, and I've found myself doing this a lot so far: `fn Something&lt;T: Clone + Sync + Order + Cat + Dog + Giraffe&gt;(t: T) { ... }` there must be a better way, right?
~~The problem is that `Get&lt;'a&gt;` can only compare to another `Get&lt;'a&gt;`, i.e. the lifetime for `'a` must match. Consequently~~ `Ok(&amp;data)` is deduced to be a `Result&lt;&amp;'a u32, i32&gt;`, and forming a reference to `data` here obviously will not have the same lifetime. One fix is to deref the value in `Result::Ok` rather than form a reference to `data`: fn check&lt;'a, T: Get&lt;'a&gt;&gt;(foo: T, data: u32) -&gt; bool { foo.get().map(|&amp;d| d == data).unwrap_or(false) // or foo.get().ok().map_or(false, |&amp;d| d == data) } [Playground](https://play.rust-lang.org/?gist=30648eecd0726401c9bbb35f9e4ecc98&amp;version=nightly&amp;backtrace=0)
There is a reason: "Implementing a complete rust typechecker and resolver is extremely hard". We have lints that rely on all kinds of things all over the place. Clippy does have an abstraction layer in it, it's the `utils` module. Still breaks often. We could maintain it for a set of rust releases but keeping up with nightlies itself takes up time and we don't have more time to deal with backporting and stuff. We could stick an abstraction layer in the distribution, but then that would need to be standardized and stabilized forever for Clippy to be allowed to work on stable, and it could end up crippling compiler evolution by pinning down certain APIs. Never going to happen, and even if it did it's a lot of effort. It took us years to get to even a small proc macro API, and even then that API only exposes strings and you rely on a completely different external crate implementation of the rust parser to do anything. This is why we plan on just shipping with the compiler. It's less effort, and doesn't change the stability guarantees of the compiler. The only thing blocking this is that the team wants to test stuff out with RLS before doing it with rustfmt and clippy.
&gt; That `&amp;*` should be a no-op, but somehow it gets it right? No, `&amp;*` is a [reborrow](https://stackoverflow.com/a/32085042/636019), which I believe has some effect on lifetimes. Sorry, I don't know enough about this yet to feel confident trying to give details, but [this](https://stackoverflow.com/a/34977048/636019) seems relevant... :-]
A good argument against that "GC is okay for me, I don't need programs to be speedy" argument is that in garbage collected languages, you usually don't have good ways to deal with resources. Memory is just one resource out of many, and in java you still have to remember to close a port/database connection/etc. Rust on the other hand already can re-use the existing drop semantics for this. Yes, Java since got Closeable, but it has been a problem for a long time.
Borrowing. And laziness. pub struct SuffixIter&lt;'a&gt; { slice: &amp;'a str, } impl&lt;'a&gt; Iterator for SuffixIter&lt;'a&gt; { type Item = &amp;'a str; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { match self.slice.chars().next() { None =&gt; None, Some(c) =&gt; { let head_len = c.len_utf8(); if head_len == self.slice.len() { self.slice = ""; None } else { self.slice = &amp;self.slice[head_len..]; Some(self.slice) } }, } } } pub fn suffix(s: &amp;str) -&gt; SuffixIter { SuffixIter { slice: s, } } fn main() { assert_eq!( suffix("tails").collect::&lt;Vec&lt;_&gt;&gt;(), vec!["ails", "ils", "ls", "s"] ); assert_eq!( suffix("しっぽ").collect::&lt;Vec&lt;_&gt;&gt;(), vec!["っぽ", "ぽ"] ); } 
&gt; look how long it's taking us to replace C! C++ is hardly better than C. But this thing has been done before. Ada among many other languages is already a better language than C++ before C++ came about and like most of these languages has completely failed to catch on.
Did you do a release build? My program shrinks massively in release mode. From 15 to 5
&gt; Copying into and out of queues might be prohibitive, so you might want to just copy a ref-counted handle. Why not just use owned data like a Box?
- `alias` vs `root` there would just depend on where you tell certbot to put the `acme_challenge` files - Yes, I think I have it listed again after the 301 redirect and I just keep the redirect disabled until the certs are created, but that could get screwy if your certs are dead - Yes, thanks!
&gt; self.slice = &amp;self.slice[1..]; That will likely panic on a non-ASCII character.
Supercomputers may have some rather exotic architectures/environemnts (and even if it's your cookie cutter Linux system, it's probably ancient), so the chances of your binary working on a supercomputer is very slim. Besides, you do want to take advantage of the arch-specific optimizations too. I have not used a supercomputer where they airgap the thing though. The ones I use, you have full access to the Internet.
Also, it might help to allocate the vector `with_capacity` since it seems to be known.
Unfortunately, I *do* need it to work on non-ASCII values. 
I *just* fixed it.
If the board is big enough and you move it a lot, then yes, maybe boxing it is a good approach. But why are you moving it so much?
Great, thanks! This is a lot faster than what I'd done originally :)
Instead of moving the board around, why not pass references around?
It's not known precisely, but the length in bytes of `s` gives an upper bound that will probably be pretty close.
&gt; 25 years old, less than 3 years after graduating from a master’s degree. Wat? How did you achieve a master's degree at 22/23? 
The parts of C++ that do not transfer to Rust could be better characterized as "warts" or "design mistakes" rather than "idiosyncrasies". Rust really feels like what you would get if you tried to redesign C++ to fix all the problems while still maintaining zero cost abstractions and bare metal performance and C compatibility. Of course, that's because to a large extent, that's what it is.
"less than", also masters degrees can be just one and a half year. I know plenty of folks with a master's degree at 23.
Modern C++ is *much* better than C, at least concerning memory management.
I don’t know how long your contract is, but Mozilla is often slow-ish (~months) to fill a position and then do all the paperwork. If you want to apply you should do it now, after a while we’ll stop looking at new applications for this position and I don’t know when another one will open :)
So an interesting idea for light weight stacks on a 64 bit system would be to allocate some "large enough" stack but instruct the OS to lazy-commit. So maybe allocate 1GB per stack of VA, but let the OS demand-page physical memory. If 1GB per stack eats up VA too quickly with millions of threads, A hybrid would be to have some "large enough" stack size at something like a couple of hundred MB. Just enough to avoid hitting the "out of stack" condition for the vast majority of use cases, still using lazy commit of pages. Then use a guard page to trap anything exceptional that's still running out of stack and in that case do something more violent like a realloc with copy and pointer fixups. This is obviously expensive, but you'd assume the VAST majority of threads never ever need to go down this route, and if they do they only do one expensive copy until they get special treatment with an exceptionally large stack VA (e.g. maybe bump the stack VA up to multiple GB). This would also need compiler support (for the stack copy + pointer relocation), I assume. 
 Probably the default settings of jemalloc which iirc has multiple thread local or cpu local pools for dealing with small allocations and such. So it probably preallocates a number of memory pools or at least the supporting bookkeeping from the get go by default.
Isn't it exactly `s.chars().len()-1`? edit: word order
Yes, most of the time they do. They typically come with their own versions of most of the stack, so if you want to statically link something you have no option besides building there. Most of them also come with their own compilers or forks of LLVM. In some of them you need to cross compile, but from some "compile" nodes to other nodes. I actually don't know of any supercomputer in which you can cross compile e.g. From your laptop or workstation to it.
I think in this case there's some type inference failure here. The following two syntaxes work correctly: Any::is::&lt;TypeThatIsFoo&gt;(&amp;x) and (&amp;x as &amp;Any).is::&lt;TypeThatIsFoo&gt;() I think the reason this doesn't work is pretty unique to `Any`: `Any` methods aren't methods on the trait, but rather methods implemented on the trait object `Any + 'static`. It's a small difference, but it means you need an `&amp;Any` or `Box&lt;Any&gt;` exactly in order to run the methods - and apparently rust doesn't yet know how that `x.is()` can be converted to `(&amp;x as &amp;Any).is()` to succeed.
Correcting my previous snippet to use .count() instead of .len() so it actually compiles. The point of the `Chars` iterator is to take into account multibyte chars or more precisely provide an iterator over unicode codepoints, be they single- or multibyte. The `Bytes` iterator on the other hand doesn't take that into account. [Example on Playground](https://is.gd/d2GqLH)
Stdout is [wrapped by a LineWriter](https://github.com/rust-lang/rust/blob/master/src/libstd/io/stdio.rs#L342-L347). The only way I can think of for getting around this is handling Stdout yourself. retrieve stdout Unix: use std::os::unix::io::FromRawFd; let stdout = File::from_raw_fd(1); retrieve stdout Windows: extern crate kernel32; extern crate winapi; use std::os::windows::io::FromRawHandle; let h = kernel32::GetStdHandle(winapi::winbase::STD_OUTPUT_HANDLE); let stdout = File::from_raw_handle(h); [Set Rust's stdout](https://github.com/rust-lang/rust/blob/master/src/libstd/io/stdio.rs#L639-L660)(unstable): std::io::set_print(Some(Box::new(stdout)));
I stand corrected :) Perhaps I was thinking I saw something like `s.len()`.
It sounds like you don’t want a trait method, but rather a standalone function. It should only be a trait method if it will vary by type implementing the trait, which this won’t (as you’ve described it).
There is some work going on like [compiling HLSL to SPIR-V](https://github.com/KhronosGroup/glslang/issues/362)…
Then you'll be writing a +16KB yourself (with your new stack frame), so the probe would not help :)
Yes, but then you'll be iterating over `s.chars()` once for the length and another time to build the vector...
I think the type parameter belongs on the method, not the trait itself, since the caller (and not the implementor of your trait) should decide on the actual type of `W`. Only allowing valid code to be serialized by hiding partial serialization behind an internal trait sounds like a good idea.
&gt; I think the type parameter belongs on the method, not the trait itself That's a good point, thanks! &gt; hiding partial serialization behind an internal trait Ah, it didn't even occur to me that I can keep that trait internal-only. Good idea.
`cell.open_mut(sync_key)`
Imagine you have multiple types that `impl FooBar`. Without specifying a concrete type, how is the compiler to know which implementation of `FooBar` to call? As /u/chris-morgan suggested, it sounds like a trait method is the wrong tool for this use case.
It's probably the limitation of my current implementation. Or possibly my mental model is broken. But I need one stack the core runs on and blocks, and I'm afraid of what'd happen if I await inside that one (eg. from inside of the core.run). Besides, the others have stacks that need to be either cached, or freed, once the coroutine terminates, so I carry them around. I guess I have some thinking the design over to do O:-).
&gt; but people ship, and use, working C++ projects all the time , so the underlying functionality must be the same - 'manual borrow checking' has been performed. Console games are written in C++ and they have to survive 'soak tests' etc. No, they just use abstractions that don't require borrow checking, or they make mistakes, and you get games that crash. Actually, almost all new games crash, a lot, these days, and then they start patching what bugs their million play-testers found, and things get a little better. &gt; &gt; &gt; But is that any different from choosing to use rust and not choosing to write 'unsafe' ,'unwrap' etc everywhere :) Yes, completely. You can `grep` for `unwrap` and `unsafe` and remove or audit them very quickly. Not only that, but those aren't even related to the borrow checker. The borrow checker is always active, even in `unsafe`, just not for raw pointers, and if an API is returning raw pointers in Rust, it *will* raise eyebrows, because it is very obviously doing something wrong, or requires extreme justification. You can't just quickly `grep` for all of bad/dangerous code in a C++ code base, unless you consider all of it to be dangerous, at which point it is no longer quick. &gt; you could equally say "all you've done with Rust is re-syntaxed your working C++ code.." by that definition, all programming languages are just re-syntaxed C++. Rust actually isn't related to C++ very much at all. Rust is much more of an ML-derived language, with a few conveniences from C added on to make C/C++ users comfortable enough that they don't realize they've been tricked into using a more research-influenced language like the ML languages. C++ does not have algebraic sum types, nor does it have pattern matching, nor does it have first-class tuples, or traits/composition, or move-semantics-by-default, or the stronger generic constraint system Rust has, or the proper module system (instead of just text-including other files) without headers, or "everything is an expression, not a statement", or... the list goes on for a long time. Rust and C++ are extremely different languages. The similarities are mostly surface level, to make developers from C++ feel more at home. They're very different languages. C++ would be smart to steal the borrow checker, but that's really complicated, and it would be opt-in, which means only isolated chunks of code benefit. The standard library, and many other libraries, would be unlikely to use the borrow checker, since it would require all code that touches them to use the borrow checker for those objects, and it would break old, pre-borrowchecker code. Opt-in borrow checkers can be useful, but they wouldn't have the wide-ranging benefits of Rust's all-inclusive borrow checker. If it were run as separate tool as you suggest, no one would use it, because by the time they decided to run it, all of their code would be rejected, and they would give up. &gt; people here complain about overloading , yet in C++ we take for granted the ability to hit 'jump to definition' , fully resolved, and a drop-box of suggestions that updates in realtime as you type. Come to rust and we lose this mature tooling. If you use [IntelliJ Rust](https://intellij-rust.github.io/), you can get that **today**. If you wait until mid-august, you can get that with VS Code and almost any other editor through the Rust Language Server. This is so close to becoming a non-issue that it's strange to focus on it now, rather than a year ago when things really sucked in that regard. Things don't suck now in Intellij, and they're about to not suck anywhere. You can beta test the RLS on VS Code today, and it works reasonably well, but it is hard to set it up right now. It will be distributed through [rustup.rs](https://rustup.rs) like everything else in the core Rust language once it is ready for release, requiring no effort. &gt; It's like I've been seduced by the promise of new toys into wasting a lot of time. So you're saying that using algebraic sum types, the borrow checker, and compiler-enforced safe parallelism haven't opened your eyes to what is possible? Not to mention the super easy to use package manager, which lets you easily add dependencies? I love in C++ when I download a dependency, spend 10 minutes finding the right place to extract the tarball so it'll be visible to my Makefile, then I have to read the README, find out it has 5 dependencies, go download those, and then rinse and repeat... okay, I don't really enjoy that. If you like C++, that's your thing. No one is going to stop you from using and enjoying C++. But once I started understanding how much better Rust allowed me to express my ideas to the compiler, and how much that allowed the compiler to double-check my work better, it's hard to go back. Did you know that it's perfectly valid to have a C++ function that says it will return a non-void thing, and then you don't return anything? It will just return garbage. It's even possible to make this mistake by accident, having a function that checks "if (this) { return something; } if (that) { return otherstuff; }" and then you forget to handle the case where neither condition is true. I actually have seen one of the best C++ programmers I know make that mistake before, although there was other code going on in the function to obfuscate the fact that one of the code paths was not returning a value. Fortunately, this was about two weeks after they started using Rust, and the Rust compiler stopped them. It said no, and it would not let him compile it until all possible code paths returned a value. Equivalently flawed code tested in C++ compiles just fine, and you're none the wiser, until things go horribly wrong in production. If you use `-Wall` and read all of the warnings, the static checks built into g++ and clang++ will *usually* catch this and warn you, but I've worked on a real, multi-million line C++ code base, and no one read the warnings. There were too many to read. That's just one of the many horror stories caused by relying on a "sufficiently smart and attentive programmer". Sure, you *might* prevent that with better coding practices, or you could just guarantee it with the Rust compiler and sleep well at night. Rust's type system isn't perfect, and there are features that need to be added to make it even more expressive, but it goes a long way. &gt; C++ isn't standing still; now that it's got the newer tools of rvalue references, 'emplace back', and lambda functions, many of the use-cases of raw pointers go away. Certainly, it is getting better, but there is a lot of old code that will take a decade or two to be updated to use these new features, at best, and even then, C++ is still missing most of what makes Rust actually be Rust. If C++ would even get Algebraic Sum Types, that would be a huge step in the right direction, rather than manually tagging a union and using the honor system to say that you will check the tag before accessing the union. I'm excited about C++ eventually getting a module system.
Oh, maybe this is a regional difference. American universities typically require a 4-year bachelor's degree before admittance into a master's program, which usually takes 3–5 years, from what I understand.
&gt; benefit there is over plain Cell Dropping the Copy constraint
Great to know, thanks! I'll at least apply. Worst case the timing just doesn't quite work out.
Hm, so `&amp;mut` and `&amp;` themselves are sort of a compile time `RefCell` themselves; I'm curious how this differs to them?
I just released a tiny "adapter" library for connecting mpsc unbounded channels into futures pipelines https://crates.io/crates/futures-shim
(I updated my comment above yours a few times, so I would check it again, just in case you missed anything.) It looks like the main Rust code you've worked on is "rustfind", which pre-dates Rust 1.0, and pre-dates Cargo, and many of the other "nice" things in Rust that we take for granted these days, like a test framework and documentation generators being part of the language itself. This makes me kinda sad. If your opinions are shaped by pre-1.0 Rust, if that was your last real experience writing Rust, I almost feel like you need to try the language from scratch again, [treating it like a new language](https://doc.rust-lang.org/book/second-edition/). It's not the same as what you experienced years ago. The tooling is so much more mature, and the the language is much more expressive than it used to be, and the "best practices" are more well defined. I have followed Rust for years and years, ever since it was first announced. Rust in the 2013-2014 time frame is nothing like what we have today. Rust 1.0 initiated a purging of all immaturity from the standard library, and a focus on making decisions that people could live with for decades, to maintain compatibility from then on.
&gt; the kernel doesn’t know that the data in the stack below rsp is no longer needed Are you sure about this? I saw a [SO answer](https://stackoverflow.com/questions/36489049/is-the-stack-only-preserved-above-the-stack-pointer) that suggested otherwise, and I'm kinda looking for confirmation either way. The choices here seem to be "allocate some memory that won't be freed to the OS" or "segfault", I feel like there's a strong argument for the first option.
Spin up a different thread and use a channel to transfer logging messages from the main thread to the logging thread?
Isn't it just like having a generic param on the function, just like with types? I mean, if there's a type that's Type&lt;A: Trait&gt;, how do we know that any Type&lt;A&gt; are the same? We don't, so we include the generic param in the function declaration where we have it as a parameter right, so fn asd&lt;N: usize&gt;(foo: Foo&lt;N&gt;) would be fine right?
&gt; I don't think fixing up pointers is possible in rust even with compiler support, how do you know if a value stored in a usize is a pointer or just a value? Well, the compiler knows the difference between and int and a pointer (barring unsafe code.. which is.. you know, unsafe.. so maybe disable stack growth while in an unsafe block and let things crash). In fact, I think Go used to use copying stacks for a while (although they did so to go from small to big.. I'm proposing only using it as a last resort strategy for stacks that go from big to ludicrous). In Rust I suppose it may require a (thread-local) heap scan, unless you store some (usually cold) meta data in the function that identifies possible heap references that may refer to stack variables (could be inferred from the lifetime system). You'd need to store this for each stack growth point.
The compiler knows the difference, but casting to an int and back to a pointer is valid. E.g. the following is valid. let mut x = 4; let y: usize = &amp;mut x as *mut i32 as usize; let z = y as *mut i32; unsafe{ *z = 5 }; println!("{}", x); You also have to consider that with FFI code the compiler doesn't necessarily know what data is pointers and what data is integers.
Can't tell from your comment whether you are confused about the role of `Cell`s in general, or the role of `SyncCell` in particular?
That'll still be way faster than resizing (and thus copying) the `Vec`, and I believe `chars().count()` has a specialization for speed as well.
I'm not sure if it's the fastest, but I know slog [cares about performance](https://github.com/slog-rs/slog/wiki/What-makes-slog-fast). It's also a full featured modular logging framework, and supports multiple outputs, including (I think) support for logging from a separate thread.
I'm guilty of putting 'i--' in my for-loop bodies after removing an element while iterating in ascending order
Masters programs rarely take more than 2 years if you go full time. 5 year combined bachelor/masters programs are also not too uncommon in the US (although hard to get into)
&gt; I log to thread local buffers which are randomly flushed to disk. I just said I did that.
I believe this isn't currently possible. See: https://github.com/rusoto/rusoto/issues/199 and https://github.com/rusoto/rusoto/issues/565
Instead of restricting `F: 'static`, why not add a phantom invariant lifetime to `SyncCellKey` and require it to be equal to `'cc`? That way, you can nest multiple environments without fear of contamination from outer scopes.
Sure, but if this really matters, I'd just over-reserve by using `s.len()` which is even faster :)
Yep, it's specialized [here](https://github.com/rust-lang/rust/blob/ff9f2d2ae94fd951229c33ae55076fce4f68fab0/src/libcore/str/mod.rs#L526-L535). It counts the number of bytes with the high bit set (indicating that they're before the end of a multi-byte code point) and subtracts that from the byte length.
Tokio is single threaded. `Rc` and `RefCell` are fine.
But `while` isn't used just for iterating, it can implement an arbitrary state machine. And when iterating, I usually don't want to deal with indexing because this implies bounds checking. If you need the index for something else, you can do `for (i, element) in iter.enumerate()`.
https://github.com/slog-rs/slog/wiki/Bench-log `slog` can do async logging in around `130ns` per record to be logged. Since slog is modular: it might be doable go lower that down if you could find `mpsc` that has a better latency than that. Right now it's just `std::sync::mpsc`. Also, since you're fine with reordering, you could take `slog-async` and write your own version that eg. gathers logging records in thread_local batches and sends that through `mpsc`. I can't do all of that for you, but feel free to ping me on gitter if you need help. :)
Is that safe? If you do it like that, then cells, keys and creators from surrounding environments are actually visible inside a nested environment. Opening a cell with the wrong key would definitely be unsafe. I tried implementing it ([playground link](https://play.rust-lang.org/?gist=22d0e824850c30bef614087325146b77&amp;version=stable&amp;backtrace=0)) and it does seem like it's still safe, because opening a cell with a key from a different environment (line 108) spits out a lifetime error. However, I'm not 100% sure if there's no way to "bend" these lifetimes so that it will wrongly accept it. Also, there's an issue with lifetime elision. I had to change line 129 from fn increment(cell: &amp;SyncCell&lt;i32&gt;, sync_key: &amp;mut SyncCellKey) { to fn increment&lt;'cc&gt;(cell: &amp;SyncCell&lt;'cc,i32&gt;, sync_key: &amp;mut SyncCellKey&lt;'cc&gt;) { Otherwise it spits out another, pretty confusing lifetime error. I know there's an [ergonomics 2.0 initiative](https://internals.rust-lang.org/t/lang-team-minutes-elision-2-0/5182) that aims to improve lifetime elision but I don't know if it would help with this case (they're actually going to deprecate some cases of lifetime elision). On the plus side, with this change the whole `SyncCellCreator` is not needed anymore. Since the key knows `'cc`, it can create `SyncCells` directly, or (if getting access to the key is problematic) it can create instances of things that can know `'cc` and those instances can create `SyncCells`.
I meant latency at the producer side (since that's what the OP is worrying about). And uhm, interesting.
I see. Thanks for filling me in!
&gt; However, I'm not 100% sure if there's no way to "bend" these lifetimes so that it will wrongly accept it. You have to make sure all parameters that host `'cc` are [invariant](https://doc.rust-lang.org/nomicon/subtyping.html). This means `'cc` should only appear like `PhantomData&lt;Cell&lt;&amp;'cc ()&gt;&gt;`. The `Cell` ensures invariance. Use a bare `&amp;'cc` by itself leads to a *covariant* `'cc`, which is bad. --- &gt; fn increment(cell: &amp;SyncCell&lt;i32&gt;, sync_key: &amp;mut SyncCellKey) { Elision means that Rust will interpret this as: &gt; fn increment&lt;'__a, '__cc1, '__b, '__cc2&gt;( &gt; cell: &amp;'__a SyncCell&lt;'__cc1, i32&gt;, &gt; sync_key: &amp;'__b mut SyncCellKey&lt;'__cc2&gt;, &gt; ) where '__cc1: '__a, '__cc2: '__b { --- AFAIK, the ergonomics 2.0 initiative doesn't propose changing the semantics of elision. It's just proposing a different syntax for certain cases.
The infer_api macro should target a local file and not one on the web for a number of reasons. 1. The file is not in source control. 2. The build now requires internet.
Thanks for the feedback :) I thought about this quite a bit actually, and I ended up thinking not being in source control was actually a bonus: the idea is to be coupled to the server's schema, not to something in source control nothing to do with (and potentially out of sync with) the server - if the live schema changes, you *want* your build to break, because deployed binaries will. Internet's only required initially, the file is cached locally. (And actually never invalidated at the moment, which sort of invalidates my point above...)
I suppose that you're talking about the wasm32-unknown-emscripten or asmjs-unknown-emscripten targets of the rust compiler, but in both cases there are no way of interacting with the dom directly. Because both asmjs and wasm have no dom/gc API, your question doesn't really make sense. Actually because both targets are actually so low level, they both have no idea about the concept of classes or objects.. Emscripten have some API's to interact with javascript, either by inlining javascript (for asmjs) or with the import table (for wasm), or even other techniques.. But the emscripten docs are quite good to explain how to use those possibilities. As far as I know, in all cases you'll need to write some javascript glue code.
i'd agree with /u/sstewartgallus&amp;mdash;this is a very opinionated design decision, and there are a whole lot of potential users who won't agree with it. what if the server's down? you just can't build? what if you're testing a new schema? do you really need to spin up a local http server to test it? what about running automated tests? imo the best solution is to have `infer_api` accept both local paths and URLS&amp;mdash;that way, the decision about which way to do things is up to the individual user.
Regardless of language used, that will depend on which library you're using to create the X11 window. The raw X11 API, GTK+, SDL, etc. all have different ways of exposing the X11 window ID.
How does this look to you? https://is.gd/1IXtGk If you wanted to do this pretty often, it might make sense to reduce the repetition into a macro. Most of the time, people define exactly only the constraints that are needed for each function, rather than having a generic set of constraints that may be more restrictive than necessary shared across multiple functions, from what I've seen. This would explain why this pattern isn't more common, but it can be done if desired. 
...though, if you're trying to do it in Rust for the compile-time checks, I'd suggest using [TypeScript](http://www.typescriptlang.org/) instead of JavaScript. (It's a superset of JavaScript which compiles to plain JavaScript and supports optional type annotations which will be verified at compile time.)
Can you give an actual example of code which is today written using `Rc&lt;RefCell&lt;T&gt;&gt;`, is impossible to write without it and at the same time it's possible to use `SyncCell`?
Redox does not have symlinks yet. Sorry
It seems you do a kind of manual parse of the openapi files. You could use the [openapi](https://github.com/softprops/openapi) crate instead. It would look good in your code to just use the structs. The crate is not perfect and does not support v3 but I think it is worth improving it instead of having your own implementation so everyone in the community can benefit. 
You declare structures that have equivalent memory layouts as what the C functions expect to deal with, and you declare function prototypes that also mirror the C function signatures. All of it is just writing Rust code. Then you tell Rust what C library to link to. There is no magic, and I don't think there's much to explain? There are tools to automatically generate that boilerplate, of course, like bindgen, and Strings are always hard in every language, even if they're easier to misuse in some languages than others. I mean, you can look at the Rust documentation for more detail: https://doc.rust-lang.org/book/first-edition/ffi.html Do you have any specific questions? I'm not sure what "internally" means. "cgo" is highly "magical", which means it either works, or you're unlikely to get it to work. Rust isn't "doing" anything for you in terms of FFI, besides allowing you to define function prototypes and force a struct to use a C compatible representation with a `repr` clause.
Thanks for the input; that certainly looks in the spirit of what I was trying to do &gt;but doing that would require more where clauses yes thats what I'm seeming to find; Unfortunately the separate **'difference'** (rather than sub&lt;output=Self&gt;) internally is important to me: to be able to apply the same 'lerp' function with scalars or vectors, which in turn can statically distinguish between 'points' and 'offsets' (e.g. in turn you can have overloads for matrix multiply that take a 3-element 'point' but it behaves as if there's an implicit 'w=1', but whilst taking a 3 element 'offset vector' with implied 'w=0' it will just apply rotation/scaling. Of course 'Sub&lt;Output=Self&gt; is enough for dimensional analysis, I might sound like I'm shifting the goalposts. Maybe this sort of thing will be able to clean up 'signed' vs 'unsigned' issues , e.g. if you have an Unsigned Int, and take a difference, the result should be a Signed type. I'm usually quite sloppy with that sort of thing *, maybe going through this exercise will leave me with traits that clean all that up once and for all.. ( * .. why I end up array indexing with signed types..) Having continued a bit with this a bit more, I think what I should do is (a) continue to break functions down into components and (b) actually write traits with member-functions , since in Rust there is no need to fear the 'member-function syntax' (in C++ land.. free functions make life easier, but here we can bolt impl's on..). lerp(a,b,c) becomes a.lerp(b,c) = a.madd(b-a,c) (aka multipy-accumulate) .. with traits Lerp, and Madd of course. It's a paradoxical finding, mirroring my current use of headers in C++. "I hate managing headers" -&gt; "writing 1 header-per-class actually makes it easier for them to manage themselves " "I hate writing traits" -&gt; "writing one trait per function makes managing their inter-dependancies easier.." maybe when there's things like 'negative trait bounds' and more I'll see more utility in them (e.g. making more complex compile time decisions in a clearer way than C++) 
(this will all make more sense with vector types, e.g with a special 'normal' type, most arithmetic on those no longer yields a 'normal'; similarly some functions will demand a normalised vector as input, e.g. 'reflect(offset_vector, axis_normal)' . adding a 'normal' to a 'point' makes no sense, but adding a 'scaled normal' to a 'point' does)
Thanks, I think I agree with having a fallback option. Initially my thinking was that if you only need internet access for first use it's no different to a `git clone`, but of course there are ways around that, and as for my earlier point about coupling to live changes - of course the onus is on the developer to be keeping up to date anyway in all sorts of ways. I still don't think checking it into source control necessarily makes sense, but I do agree that making it easier to supply a local file would be good. (Presently it is possible, since the remote copy is cached, one would just have to replace the cached copy for an updated one, or place it there in advance.)
Huh, I'm working a lot with Polymer and never needed static typing… asm.js/wasm is too low level for any DOM-related stuff. You can expose JS functions to them for a workaround but that's really awkward and just… don't. Look into TypeScript/Flow/Scala.js/PureScript/BuckleScript. Re: ES6 class requirement — there's a patch thing that makes "fake" classes work, polymer-cli build can include it.
Not yet, but I'd like to :)
Wow, I can't believe I missed that: it's basically the refactoring I've wanted to do, albeit not v3. I agree completely; I'll see if the author would be open to a PR for v3, and I'd happily depend on it rather than duplicate the effort.
I know a guy who did his in the same time I did my bachelor's (4 years). He was quite smart.
I think so. There is [an issue](https://github.com/softprops/openapi/issues/7) about it.
As others have said, a trait is not the right solution for this. Think about it: in another trait, I could do something like: struct NewStruct; impl FooBar for NewStruct { fn new() -&gt; Foo { return Foo::new().some_modifications (); } } How does your code know whether to call my new `new` method, or the one for Foo? I think the best way to solve your stated problem would simply to be methods on Foo itself that have `#[cfg(...)]` on the methods themselves. If you do want a trait however, simply change `new() -&gt; Foo` to `new() -&gt; Self`. This way `new` returns whatever the implementing struct is, and there will only be ever one method to call for `Foo::new()`.
Hi, everyone. I noticed there weren't any good Rust bindings for Wren (the only other crate is incomplete and abandoned), so I made this. It's my first publicly released crate, so feedback is welcome. Also, I've been trying to get this to work on Windows, but for some reason I can't get Wren to compile with MinGW using `mingw32-make`. If anyone has some insight regarding that, I'd appreciate the help. 