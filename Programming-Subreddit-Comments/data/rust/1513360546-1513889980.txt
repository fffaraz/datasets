In the static-buffer branch you're making two full passes over the vector. I don't know if that's the bottleneck here, but have you tried a static buffer without the changes that write the 0xff values in a separate pass?
If there's no threads why bother with the mutex? Just have a bare global static and mark your updates to it with an unsafe block.
The first pass is only executed if the vector changes length, which happens only on canvas resizing (i.e. not often).
It allows for portability between WASM and all the other targets that have multithreading. It still checks that values aren't borrowed mutably more than once, making it basically a specialized `RefCell` (`RwLock` is pretty much just exactly `RefCell`). The facade isn't flawless or complete. A lot of stdlib APIs currently panicked when invoked, like trying to spawn a thread or open a socket. But I'm guessing the ultimate goal is to support almost everything that the other targets do. TCP and UDP are out of the question due to sandboxing, but a similar interface over Websockets would be just as useful. [Even threads are a proposed feature in the spec](https://github.com/WebAssembly/design/blob/master/FutureFeatures.md#tracking-issues).
There is also : https://github.com/TheWaWaR/simple-http-server Hope you guys like it :)
Define slower/faster? I assume you're running release. Also, allocation is probably just a mem cursor movement, so there's not a big overhead I wouldn't expect. The only answer is to inspect the WASM, which you can do by dumping it out as WAST, checking the export for your function names (at the bottom most likely) and following them around. Also, there is another option. Expose a buffer creation function that returns a raw heap pointer to it, call that from JS and store the resulting int pointer, and pass that in to the draw call each time. Or make a "Context" object and create all sorts of stuff in that struct including your buffer...almost always better than static, but that's a code design thing not a performance one.
&gt; Hmm, could you, at least until wasm supports threads, facade the value in a struct that holds a mutexed version for normal rust and a no-lock RefCell deal version when in wasm mode? Then use a feature flag. That's literally what libstd is doing. It's just done it in a way that requires basically no changes to code that uses `Mutex`. The file I pointed to is the WASM equivalent for the OS abstractions for mutexes, where it's just supposed to provide "lock" and "unlock" methods, expecting the former to block the current thread until the lock is available. Then `std::sync::Mutex` calls those when it creates a guard or when the guard is dropped, respectively. On WASM it's just updating a flag and panics if the client code tries to recursively lock, which is already done on the other targets (or at least some of them, as implied [here](https://doc.rust-lang.org/nightly/std/sync/struct.Mutex.html#panics)). 
I would currently recommend the kalitaalexey Rust extension over Rust (rls). Per my tests yesterday neither seems to actually work when it comes to refactoring or things like find all references, but the Rust extension has a killer app that I can't live without anymore: "rust.actionOnSave": "check",
Ah, sorry then.
&gt; I assume you're running release. No need to assume; debug mode is currently totally broken on the `wasm32-unknown-unknown` target so you can't have it any other way. (:
Oh gross, I love it.
&gt; migration Self-plug but there is my tool for migrations as well: https://github.com/Keats/dbmigrate TQL looks pretty damn nice!
why not just do it properly :) 
Yup, was for me too, ref: https://github.com/rust-lang/rust/issues/46339. 
I don't use lazy_static for wasm, I use thread_local!
Well, it's rust, so you can hand someone a binary, and then they don't need rust or python on their computer to use it. 
`RLS` was missing in a nightly. I installed nightly without checking if `rls` was included.
Same here---does anyone know if miri provides a path to a native `irust`?
&gt; The TL;DR is that determinism is mandatory for the soundness of the type system, which precludes I/O. I think the last part ("which precludes I/O") is a bit off. Precluding I/O is a sufficient, if unnecessary, way of obtaining determinism. Some forms of I/O could perfectly be allowed: - writes to "/dev/null" do not affect determinism, neither do writes to a compiler-controlled log file, - reads of cached files would also always return the same result, - time queries giving an arbitrary time (last modified time of the file containing the query?) would also be deterministic. I myself would appreciate special-casing logging *and* allowing reading files (from a specific "resources" folder). The latter could eliminate part of build scripts for example, or allow speeding up complex computations by reading pre-computed results (or parts of).
That's what I thought, but precisely today I had a need for something like this, and I still had a test I did with iron: it serves files from two different directories, based on the path, plus a data file that is an extra argument. I start it up, and it says: dyld: Library not loaded: /opt/local/lib/libssl.1.0.0.dylib 
I expect that heap allocation in the absence of `UnsafeCell` would also be fine. static FOO: Vec&lt;i32&gt; = vec![1, 2, 3]; For example, is nothing more (in memory) than: +---+---+---+ | 1 | 2 | 3 | +---+---+---+ +-------+-------+------+ | 3 | 3 | * | +-------+-------+------+ What cannot be allowed is situations in which the memory could be modified, including by `Drop`. Disallowing `UnsafeCell` in the result of the constant evaluation seems sufficient for that purpose.
&gt; Something like that is always going to need some sort of lazy initialization. Only if you initialize mutable data. As long as: - you are initializing a `static` (not a `static mut`), - the type of the variable does not recursively include `UnsafeCell`. Then I don't see an issue. For example: static FOO: Vec&lt;i32&gt; = vec![1, 2, 3]; Could lead to: +---+---+---+ | 1 | 2 | 3 | +---+---+---+ +-------+-------+------+ | 3 | 3 | * | +-------+-------+------+ Of course the destructor would not be invoked, but since there's no memory to free (it's all `.rodata`), then who cares?
Sure, I can see useful exceptions being carved out if they don't violate determinism, but that can be added later if enough people demand it. At the same time I'm not sure what the use case for const logging is (debugging?), and the latter sounds like one would be better off just using `include!`/`include_str!`/`include_bytes!`.
&gt; If you did this, it would be by some special type of hash map that creates the data to be pointed to in a rodata section instead; not just a trivial thing you can do on arbitrary types. I disagree. It should be possible to initialize a `static H: HashMap&lt;String, String&gt; = ...;` with a `const fn`. The key bit is that the whole reachable memory is immutable, and then can be entirely placed in `.rodata`. `Drop` will never be called, but I don't see this as an issue: there's no memory to free anyway.
I'm skeptical but would be happy to be proven wrong.
I think there are two potential issues with `Regex` for now: 1. It's not clear that MIRI is fully capable of handling all the code required for regex compilation, 2. For simplicity reasons, the types generated by MIRI are unlikely to ever be modifiable (`UnsafeCell` should be prohibited in the output); it should be possible to place the "const" variable, and anything it can reach transitively, into `.rodata`/`.text`. I think that (1) can be solved in MIRI, and (2) could possibly be made to work at least in parts by improving the compiler and possibly tweaking the Regex crate. Please correct me if I am wrong: I believe that today a Regex may be compiled either "entirely" (in which case I assume the VM is immutable?) or "partially" (the lazy DFA bit?). I can imagine that, in the future, `Regex::new().unwrap()` would succeed if the Regex is compiled entirely (as the result is transitively immutable) but fail if it is compiled partially (as the result is then not transitively immutable).
Sure, you could probably do something special for types which implement `Freeze` to allow them to leak through.
Let's put it the other way around: *it's a feature of MIRI that the compile-time result of a `const fn` equal the run-time result on the target architecture*. Therefore, whenever MIRI is *not* capable of achieving this result, it's either a bug (woops) or an unimplemented tidbit (which should abort evaluation). And therefore, I expect MIRI capabilities to be activated *little by little*, each time ensuring that the output of MIRI matches that of the target architecture. --- To answer your exact question: there are *many* differences between targets. A target *platform* is not only the CPU, but also the OS, and possibly its flavor (vendor). For each target, the OS is in charge of defining the *C ABI* (Application Binary Interface) which notably defines how C structs should be laid out in memory (size and alignment of various types, how to pad, ...) as well as the various calling conventions, etc... (as another example, `libc::c_char` is either `i8` or `u8` depending on the target). Similarly, the *Rust ABI* may also differ from target to target, notably based on the size and alignment of various types. Beyond the ABI, as was noted in comments, some targets may have "special" behaviors which are specific to the target: trap representation of certain values, rounding modes of floating points, etc... To faithfully emulate targets, MIRI needs to encode all this domain-specific behavior.
I personally would love to be able to run a single test using MIRI when I'm hacking away, rather than having to recompile the world before my small test can run^^
Partially. At the moment, the current `const fn` framework does not allow returning a type with dynamically allocated memory. So... it depends whether what you had in mind is blocked by this. *Note: dynamic allocations during the evaluation of the function are fine, unlike I/O.*
I think that at the moment any `const` evaluation runs twice, once with the old framework and once with MIRI, in order to test that both have the same behavior. I surmise this will change soon, but in the mean time it could lead to some slow down.
I doubt that anyone here will have more concrete information than the linked issue contains. If the problem is getting this to work with incremental compilation, then I would expect that a good time to ask in that issue for a status update would be after incremental compilation gets officially enabled and supported. In the meantime, it sounds feasible to write an external tool that would do what this once did. Perhaps Clippy could be convinced to maintain such a lint.
Actually, the current `const` rules allow *value*-level immutability. For example: static FOO: Option&lt;Vec&lt;i32&gt;&gt; = None; works because despite `FOO` possibly including a memory allocation, `None` doesn't. This could easily be extended to allow values which transitively are immutable, even if some other variants of the same type would not comply.
:) I think the most difficult part would be identifying the exact set of rules which makes it safe. The "easy" rule is forbidding `UnsafeCell`, but I am not sufficiently well versed in Rust rules to know if this is sufficient to guarantee transitive immutability.
const logging is indeed for debugging your `const fn`. It's a bit complicated to break out the debugger on the evaluation, so logging is the only way to gain insight into the execution to find out *why* the output is not to your liking^1 . As for the latter, using `include!` means including the file into your binary (at least in Debug) which is not necessarily desirable. For example, think XML/Json schemas, etc... if you only need them during compilation it seems wasteful to include them. Of course you'd hope they would be stripped in Release at least, but still. ^1 *Well, you could run in at run-time in a `#[test]` under the debugger, too :p*
Hum does FFI calls include `mmap` calls ? so no allocations possible ? so no `Vec` or things like that ?
I don't understand how `Filter` would be able to make any guarantees about its length via either API. `foo.filter(|_| false)` is bounded for any `foo`, but `Filter` can't know that in the general case without solving the halting problem.
That's different from leaking compile-time allocations into runtime rodata-- in your example, there aren't any allocations at all, so there's no problem.
This aligns with my expectations and understanding, yeah.
&gt; Precluding I/O is a sufficient, if unnecessary, way of obtaining determinism. &gt; Some forms of I/O could perfectly be allowed: [...] Thinking about this a bit more, it seems you are right! In fact, we could allow all writes/seeks/file creations of any file (without reading support), as long as all I/O errors yield in const eval compilation errors. Anything that involves reads (including directory listings or checking whether a file is present) is harder to get deterministic: you'd have to make sure that any read from the same file gives you the same output. This can either be done by having to pass the sha-256 hash of a file together with the filename in order to read it, or alternatively by computing these hashes when reading the file and then somehow coordinating the computed hashes across invocations of the compiler.
Is it possible to make `ops::Index` return a slice? ``` impl &lt;'a, T&gt; Index&lt;usize&gt; for MyStruct&lt;T&gt; where T: Copy + Sized, { type Output = &amp;'a [T]; fn index(&amp;self, index: usize) -&gt; &amp;&amp;[T] { ... } } ``` If I set `Output = [T]`, it won't actually return a slice at the call site (e.g., `x = mystruct[0];`then `x` would be `[T]`). However, when I do like in the example above, I get `unconstrained lifetime parameter` for `'a`. 
&gt; As for the latter, using include! means including the file into your binary (at least in Debug) which is not necessarily desirable. Looking at the LLVM IR, [it seems you are right](https://play.rust-lang.org/?gist=e83b317407a2b38304dd308f6e4c727a&amp;version=nightly), but I'm pretty sure that fixing this so that the file doesn't get included is easier than getting I/O deterministic. Also, proc macros will be available at some point and there you'll have no purity restrictions *at all*.
This sounds like the kind of job the RLS would be useful for, but Ao don‚Äôt know if it has that particular capability yet.
Note that I have looked at the issue from a determinism angle only. From a stability angle, you will probably want to make all I/O unstable, as the *order* in which const eval happens might change and stuff that worked previously would now be an error because e.g. the compiler is doing multithreading now. I mean right now it is already impossible to get parsing multi threaded due to impure custom derive macros, would be sad to have the same situation for other places in the compiler.
The indexing operator is a bit funky, it basically throws in an implicit dereference. So you have to add a reference operator to balance things out if you can't take a copy of the value (like if it's unsized, e.g. a slice). `type Output = [T]` will work if you do `x = &amp; output[0]`.
Not sure how Rust's Path type handles this, but this looks vulnerable to directory traversal ie: I can put a path like "../../yourfiles". 
Thanks!
`foo.filter(|_| false)` is indeed a case which I also thought about. If `foo` is an `UnboundedIterator`, this will always result in an infinite loop. Thus, any (and no) guarantee is held, as false ‚Üí true. Therefore, we can safely implement `UnboundedIterator` for `Filter`. Either results are returned (which will be infinitely many), or it'll result in an infinite loop, in which case everything after is dead code anyway.
For personal projects I'm a lot more willing to 'tweak' design - I strive for perfection because I have no real constraints. For work I don't use Rust, but I do use Python with Mypy and the workflow is the same for me; I write out the classes and types that I suspect I'll need, move them to their appropriate modules/ files, and start hooking them together. Every function/ method has a `raise NotImplementedError` in it, and then I start filling things in. Often there is friction - some types change, requirements change. But I haven't had to throw much out, just refactor a bit. It's almost always due to a faulty assumption about some external service, forcing the client to change.
Until recently I didn't know that, I just knew that the Python simple HTTP server is obviously really slow‚Äîit's painful even when doing rather simple local development.
Depends on if diverging (infinite loop) counts as satisfying the contract of returning another element. Basically, if you say "I have a trusted length of at least 1", am I saying "I will return another element in finite time" (a strong claim) or am I merely saying "I will not return a None" (a weaker claim). I would claim that the latter is still useful for memory safety... you can guarantee that unsafe unwrap of the next will succeed if it ever is called, it just might never be called again if the program goes into an infinite loop. To put it more simply, a filter of an unbounded iterator can guarentee that it will never have .next() return a None, which is enough of a guarantee... sure, it might not return at all but that's "fine".
&gt; The key bit is that the whole reachable memory is immutable, and thus can be entirely placed in .rodata by the compiler. How would you deal with pointers/references to the compile-time heap? I suppose you could map .rodata at the same absolute address where heap was at compilation time, however this means that your crate would be non-relocatable and might conflict with other crates at link time (or other shared libraries at run time). It would also likely contain gaps due to compile-time deallocations. I suspect that in order to do this properly, you'd need something very close to garbage collector infrastructure that would allow you to discover all pointers to the heap.
I like your wording of "I will not return a None". It better expresses what `UnboundedIterator` actually uses as a guarantee for optimization. And it still allows all kinds of optimizations I thought about when designing the type (which is better `extend`, `collect` and to remove `None` branches as dead code).
In addition to having a read-only list of VM instructions, each Regex has a chunk of mutable scratch space which lives on the heap. Would that present a problem?
miri handles allocations internally, but you can't call `mmap` on your own
Everything that I do looks most similar to [exploration-based design, or compression-oriented programming](https://www.youtube.com/watch?v=jlcmxvQfzKQ). I'm not sure there is a better way to go about things. Unless you're already very experienced in solving the particular problem at hand, I don't think it's sane to create a design before writing any code then stick to it. Refactoring is normal, and instead of trying to avoid it I think we should look to make it less painful so that we can do it more often.
RLS is gone after today's nightly toolchain update. rustup component add rls --toolchain nightly says that such a component is not present in this toolchain. So, wtf is going on?
ok so it sort of declare it's own custom allocator ?
In that program of mine the only "const" things that I have are 20-30 constant values. I don't think evaluating those constants twice could justify a "check" time increase of more than one second.
A few things I would be thinking of if I was writing this: * How are you handling out of ordered events? UDP is not guaranteed to be in order or reliable. I'd take a look at this series of articles: https://gafferongames.com/post/udp_vs_tcp/ * What if you try to rewrite it without the use of mpsc and threads? The [tokio examples](https://github.com/tokio-rs/tokio-core/tree/master/examples) I've had a look at seem to not rely on this. * Client connecting and disconnecting could be implemented as an enum, so if a client sends a "disconnect" message, then the client can be dropped. Relying solely on timeout for this may not be a great idea. You could expand the enum to include a counter also, so you can see whether you've missed any client packets.
Perhaps I should be more disciplined, but my hobby projects tend to progress in two steps: 1. Spend some time thinking about the design (eg. architecture, UI wireframes, workflows, etc.) without writing a single line of code (with the time spent proportional to how much work I anticipate I'll have to throw out if I get something wrong and how easy it feels to model the entire thing in my head.) 2. Fire up a code-test-refine cycle and hammer away at it. Where automated tests fit into it depends on their purpose and the difficulty of writing them. For example, in one project I currently have on hold, I... 1. ...wrote up a very dumb, simple implementation of my proposed filename‚Üítitle heuristic 2. ...used it to generate the first draft of the fixture for the test suite 3. ...hand-edited the fixture to fix everything it got wrong 4. ...wrote the test harness 5. ...started iterating on the code to drive the accuracy percentage as high as possible. (It's currently up around 93% accuracy and most of what remains is due to some precedence bugs that require deep, conceptual refactoring of the heuristic which I could *never* have anticipated when I started out.) Conversely, the "find the icon for a Linux game given only the root path to its install location" code is basically untested because I have a history of procrastinating the creation of filesystem mocks. (Once I run out of more appealing tasks in this hobby project, I think I'll start brainstorming and designing a reusable solution for my problem with filesystem mocks once and for all.)
/r/rustjerk
That's legit. Shared libraries are still a thing. 
This would be more appropriate in /r/rustjerk/
Ooh, thanks for sharing. I need a simple http server all the time.
This is my current side project, and first major foray into graphics programming. I don't think I'd have ever gotten this far without all the safety rails Rust and gfx provides for me. Long-term, this is gonna become a better version of the game [Europa Universalis IV](https://www.reddit.com/r/eu4/).
Hello folks, how can I make http calls when compiling to `wasm32-unknown-unknown`? I tried to use the `http`, `hyper` and `reqwest` crates but none of them compiles for this target. I saw someone talking about some emscripten's functions github but without much details.
i recommend to switch to the new firefox and install tree-style-tab plugin. you will not look back. for chrome there is tabs-outliner, but still the new firefox is just sexy and you should try it :D
I'm slowly learning rust and would like to have some suggestions from you. This is somewhat vague and opinion-based, but what are some good ways to find the method you want and the trait that implements it, other than the eyeball search? Although I've been enjoying rust, it sometimes bugs me to find the appropriate method that I want. (e.g. recently I had hard time finding [`Vec::retain`](https://doc.rust-lang.org/std/vec/struct.Vec.html?search=#method.retain) and [`Iterator::partition`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.partition)). Docs can sometimes be [lengthy](https://doc.rust-lang.org/std/vec/struct.Vec.html), and reading through it can be too consuming. I'm more familiar with Haskell, with which I'd [search by type](https://www.haskell.org/hoogle/?hoogle=%28a+-%3E+Bool%29+-%3E+%5Ba%5D+-%3E+%28%5Ba%5D%2C+%5Ba%5D%29). The functionality itself seems to be [implemented for rustdoc](https://github.com/rust-lang/rfcs/issues/658) but it shows no result for me. (It might be less effective in rust anyway, where [`take`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.take) returns a struct `Take`, etc. ) Is there a easier way to look for a method? What would you advise to a newbie?
thanks for the sharings„ÄÇI am a fans of python SimpleHTTPServerÔºåand now I have more choicesüëç
AFAIK, there's currently no native way to issue an HTTP request in WebAssembly. You'd have to bind that on the Javascript side using XHR or jQuery and expose functions to invoke it to the WebAssembly module.
Enjoy! I have been in that same boat so many times :)
Enjoy!
That seems extremely difficult to deal with in a general way. If the total possible space is static based only on the regex or maybe if it allocated brand new structures instead of using the scratch space it could be possible, but I am not aware of any proposals (or even ideas) for const items that can transition into dynamically sized items when they're interacted with. Everything I can imagine would require data structures that are specifically designed for this use case, and even with that it seems like it would be *rough*.
[Repository](https://github.com/CAD97/rust-single) | [Documentation](https://docs.rs/single/0.3.0/single/) | [Old CodeReview.StackExchange Post](https://codereview.stackexchange.com/q/165393/100439) I made this when originally learning Rust, because I was often missing Kotlin's [`&lt;T&gt; Sequence&lt;T&gt;.single(): T`](https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.sequences/single.html). The point of `sequence.single()` is that you've taken some stream of data and filtered it down so a single element that you want out. The use case that caused me to come back and release 0.3.0 was running into turning a single-character `&amp;str` into `char`. This is trivially done by `string.chars().next().unwrap()`, but it's always nice to check your invariants, so now I have the more descriptive `string.chars().single().unwrap()`. So the point is that `.single()` is a proper _terminal operation_. It's to be used to consume a stream you've (supposedly) filtered down to one element. It'd be nice to go 1.0.0 with the crate. It's a small, focused crate, and unlikely to need touching up (except maybe to bump the `failure` dependency). Is there any other functionality that'd be useful to have for 1.0?
I'd like to note that I've at least considered returning the iterator in `Error::MultipleElements`. The problem is that this would necessarily increase the size of the result a very large amount -- the failure case would have to carry two `Self::Item`s (the first and second element from the iterator) and the iterator itself. If you've got a way that I can return the iterator without increasing the result's size beyond `Self::Item` + tag, that has a equivalent version workable on `no_std` (because I'd like to retain `no_std` support), I'm all ears.
darkhttpd.c is easy to distribute too. And it's faster to compile.
That's why I switched from Python to webfsd, then darkhttpd. darkhttpd is a single C file, has support for directory listing and HTTP ranges.
Legit, but it means you can't "just hand" someone a binary.
Non-web software isn't that much different when integrating with a huge number of targets. libc is - in huge parts - just mapping names to different integers for all OS platforms. The web at least has the notion that at some point the stuff will be accepted as standard and we will all end up with `user-select`. Just as we have libc, there's libraries that do that for you for CSS, constantly evolving to the current state of things. It's a huge thing to track and follow, but there's a lot of people working on making that convenient and usable.
very cool! i really like the WasmBlock project shameless self plug - i have something similar with the ability to move around here - http://www.tomcraven.io/project/wasm-mandelbrot/
Well. for what it's worth the Operator used to work like that, but got changed to the current by-value semantic for among the reason that C++ allows moving through an operator, and uses that feature in many important (scientific, numeric?) libraries.
Correct, that's also necessary for the undefined behaviour detection miri does.
Consider this scenario: int main() { char *a = malloc(4); strcpy(a, "Foo"); do_something_with_a(a); use_a(a); free(a); char password = malloc(MAX_PASSWORD_LENGTH); read_password(password); use_password(password); printf("%s\n", a); free(password); } If the allocator was optimizing and didn't release freed memory to the OS, but kept it for further allocations, then the next `malloc ()` could get the same piece of memory and the last `printf()` would print out (leak) the password. Also another scenario would be that instead of printing, you read data from untrusted source, overwriting the password with something else (e.g. attacker's password). Or if instead of password, an object was created, the attacker could overwrite the pointer to vtable and run arbitrary code.
Thanks for your answer! - I‚Äôm not handling out of order packages or other stuff yet like reliability etc, but I am trying to leave space in my code to add that later. That link is great and I‚Äôll certainly use it - That‚Äôs my biggest issue to be honest, and the reason I wrote this post. I‚Äôm not sure my design is any good but tokio examples are too simple, is there a specific one you think I could base my code on? - You are right! There‚Äôs a lot of room for improving the protocol but I‚Äôm trying to make sure my foundations are correct before going for that.
I actually kind of like super simple trivial things like this wich are dead simple to implement but show a nice idea. That this is a trait though also highlights the problem with Rusts syntax; this is surely a trait rather than a simple function definition because Rust for some reason I don't understand lacks a true universal function call syntax so making it a trait is superior because now you can choose between "function" and "method" syntax which you can't in a normal function. Really there is almost no disadvantage to implement everything as a trait because of this.
&gt; I suppose you could map .rodata at the same absolute address where heap was at compilation time I was naively thinking of: 1. Defining an internal symbol for each separate reachable compile-time heap allocation, 2. Replace any pointer via the corresponding symbol. &gt; I suspect that in order to do this properly, you'd need something very close to garbage collector infrastructure that would allow you to discover all pointers to the heap. Indeed, the trick though is that MIRI and the compiler both know this. Doing so at run-time requires having some form of introspection encoded in the binary, and who encodes this knowledge? The compiler. In order to create the constants, MIRI and the compiler both know the exact memory layout of structs and the meaning of each byte. As a result, it is also perfectly possible for the compiler to use this knowledge *backward* and reinterpret the bytes it get passed by MIRI at the end of the compile-time evaluation to perform this analysis. The one point where this could go wrong would be a user storing pointers in integers. This is fine with Rust's safety story though: transforming those back to pointers and dereferencing them is `unsafe` (it would only be a problem if one were to store a delta between two pointers not belonging to the same heap allocation).
Sure. But proc macros result may not be usable in `const` contexts (such as for the size of arrays). Do you really think getting I/O deterministic would be so hard? I must admit to generally being an optimist, but I had thought that as long only allowed file I/O within a separate read-only folder (whose contents the compiler cache on first read, to guarantee consistency) and a write-only folder, then it would be deterministic.
To avoid the evaluation order issue, I was thinking of restricting reads and writes to guarantee they cannot overlap. Reads and writes should go to different directories: - reads would go to a "etc" directory, sibling of "src" maybe? - writes would go somewhere in the "target" directory, alongside the other build artifacts. To start with, writes could even be limited to *logging*, and not support arbitrary writes. It would already be incredibly useful to diagnose issues in `const fn` evaluations.
Sure. This is just an example that the compiler already accepts/refuses constants on a *per-value* basis, and not on a *per-type* basis.
I've watched the video. It's an interesting project. I do find the title of this post a bit too clickbaity though.
&gt; and I've been getting stuck where often times I have to refactor Completely normal. &gt; or delete a lot of code Not so much. What I attempt is bottom up programming. Go from the lowest level up, with an eye for what will be needed later. Refactoring is common and necessary. When you start seeing huge functions/structs it is always time to refactor.
Correct, the pipes "create" an anonymous function, or [closure](https://rustbyexample.com/fn/closures.html) as they're known in Rust.
I always felt it was wrong that size_hint could not communicate "this iterator will never return `None`". I ran into this a while back with the an unbounded iteraror adapted into a filter which sets the lower bound to 0 always given that the filter can obviously not match anything but if the iterator itself is unbounded the filter should also be unbounded. Then it went to a take(10) which should in the case of an unbounded iterator have known size of 10 it seems to me. In theory the computation could diverge at the filter stage yes but in that case it also never returns None. I don't like what size_hint retulrs, in my opinion it should be returning: enum SizeHint { Exact(usize), Bounded(usize, usize), BoundedLower(usize), Unbounded, } Rather than the simple `(usize, Option&lt;usize&gt;)`
I get your point, but I feel that while this issue is very commonplace in web development, it's limited to niche applications for most other technologies.
The problem is that `size_hint` is stable and thus can not be changed without breaking existing code. While I also don't like the current return type of `size_hint`, changing that is unfortunately not an option. But that is where my idea comes in, as it can be implemented without breaking any existing code, but can express the exact same idea.
EUIV clone, I like your ambition. Looks great so far :)
`size_hint2` can just be added though with a default implementation of `BoundedLower(0)` and `size_hint` can in fact express its default implementation in terms of `size_hint2` with `unbounded` becoming `(usize::MAX, None)`
Do you have Racer installed, or just RLS?
Emacs and racer work fine for me. Sometimes it will complete methods from traits that aren't in scope, which is a bit confusing, but it works well otherwise. I've tried the IntelliJ plugin, too. Its completion is pretty good but I found that the plugin loses its state sometimes and just stops completing (especially if a macro had been used in the file). No experience with the language server here. 
Could you include links into the main post to make it easier to see how your crate works please? [crates.io](https://crates.io/crates/rpm-timer) [docs.rs](https://docs.rs/rpm-timer/0.0.1/rpm_timer/) [github](https://github.com/synek317/rpm-timer)
This here is the spec that basically everyone dealing with sourcemaps follows (yes, it is only hosted on docs.google.com, no joke!): https://docs.google.com/document/d/1U1RGAehQwRypUTovF1KRlpiOFze0b-_2gc6fAH0KY0k/edit#heading=h.djovrt4kdvga
It looks like a strong implementation of this approach (multiple threads polling a shared task queue/list/slice and doing tasks), the way default and configuring methods work is a nice API. It is hard to compare this approach vs others without knowing what the use case you are working towards is. If you are after test cases, maybe find a public API which is rate limited and either try to write a client against it or create an api server which implements the same rate limiting strategy. Some examples of rate limited APIs: [Twitter](https://developer.twitter.com/en/docs/basics/rate-limiting) [Facebook](https://developers.facebook.com/docs/graph-api/advanced/rate-limiting) [Gmail](https://developers.google.com/gmail/api/v1/reference/quota)
Thanks, I have no idea how could I forget about it
If you're on nightly you could use [splice](https://doc.rust-lang.org/std/string/struct.String.html#method.splice).
`String::insert` actually exists: https://doc.rust-lang.org/std/string/struct.String.html#method.insert Having said that editors typically use the strategy of purposefully breaking down the entire buffer into chunks whose optimal size is of course a bit of wet fingerwork to ensure that entering a character at the middle of a super long document does not involve shifting the entire document in linear time. Profiling is obviously needed but you might find that breaking up the readline buffer into max 16 character chunks would be superior. You might in fact even find that storing the buffer as a linked list of characters rather than a string is superior or maybe a hybrid approach where each node in the linked list does not contain one character but a string of a maximum amount of length since for readline you need to maintain a pointer to the current cursor anyway a doubly-linked list of segments lends itself very well for this perhaps. It will at the very least certainly improve the time needed for insertion at the cost of more memory.
Code completion works great in VIM with YouCompleteMe.
I could detect spaces and store according to that too! Thank you for the ideas!
Well yeah, since it is common to edit a single word while keeping the rest of the command line the same a strategic point to break into chunks might be at word boundaries yeah.
Given the interest with Wasm lately, won't be a great idea to make this work on Wasm? For normal users your server would contain the data (devs will need to download, etc also). That would be something EUIV can't do now.
Though, this readline thing is just a component of a calculator I'm making... So it would have to handle cases where I may not enter any spaces at all, in which case I would want to break things up according to some more complex rules. I already have a mock up of this in C++, so I guess I'll try a similar approach here.
I am on nightly, but `String::insert` looks like a better bet for me.
As far as I know IntelliJ has the best code completion (and got some really useful updates recently).
I haven't used `RefCell` in a very long time, but you could do something like this https://play.rust-lang.org/?gist=dcb89d6537c08f5994e6477161f61aff&amp;version=stable. I don't know why there isn't a `get` method. Also I am not happy with the additonal allocation. There probably is a better way but I don't see it.
VSCode with RLS works pretty well for me. Code completion works (usually), so there might be a problem with your setup.
I find it amazing how little credit intellij-rust gets: code-completion using this plugin is nothing short of amazing.
Ah nice, that would work, too. However, I did simplify my use case a bit, so it doesn't work in my particular case. Still a good suggestion though!
It says Racer on. If I type `Vec::` it will give me some completions. But if I do this: for x in arr.split_whitespace() { let int_x = x.| } I don't get any autocompletions (`|` to indicate cursor position). `arr` is a string. let mut iter = arr.split_whitespace(); while let Some(x) = iter.next() { let int_x = x.| } I thought maybe it's just the four loop without explicit assignment. But yet even in the iterator version still no completions. Ok maybe these two cases expect too much (although e.g. the F# ionide plugin for VSCode does autocomplete on things similar to this). But especially for these cases I find autocompletion helpful.
I don't have IntelliJ, I'm not sure if I want to buy a Java IDE just to get a plugin for Rust, which I don't use professionally.
The plugin works with the free "community" edition of IntelliJ
Yeah what the optimal part to break is is very heuristic. Insertion will pretty much always be faster the smaller you make the chunks but space will increase. Personally I would be neurotic enough to create an "unstable" namespaces of both compile flags and config syntax that defines this. So you can set it at compile time as well as in the config file there is eomthing like `_unstable.chunksize = ...` where you can override the compile flag to set this with the understanding that this setting may do nothing any more in the future and be silently ignored without warning since you are hacking into an implementation detail.
Pretty! I like how yours is interactive. I'm working on extending WasmBlock's canvas api plugin to allow directly setting the pixel data of a canvas. Hopefully to support more "realtime" canvas experiments. :)
Ah ok, I browsed the IntelliJ website quickly and didn't see a community edition. I only found student licenses and 30 day trials. Thank you!
IntelliJ has the best but for large crates nothing seems to work well
Do you absolutely need `Vec&lt;&amp;T&gt;` or you can just use `Vec&lt;Y&gt; where Y: Deref&lt;Target=T&gt;`?
You can establish this pretty easily, actually. If it's got zero external dependencies or all Rust-only dependencies, then it probably works. The WASM target is pretty neat in that the entire stdlib is still accessible, though things like threading and networking and reading environment variables have stubbed-out (panicking) implementations. So the library can still be aware of types like `Thread` as long as it doesn't try to use them unless you tell it to. If it links to any C or system dependencies, then it's probably not going to work. Some C libraries could potentially compile to WASM but that probably requires some manual setup.
Well it's actually a bit more complicated than I wrote above. I basically have a `Vec&lt;A&gt;` where `A` is an enum that has one variant with such a cell of `T` and I need to convert it to `Vec&lt;B&gt;` where `B` is an enum where one variant accepts `&amp;T`.
https://intellij-rust.github.io/ FYI
My intuition says that a normal string with O(n) insertion performance will be better than any linked list up to very large string. But if you encounter performance problems, maybe think about a gao buffer. Im on my phone and ob the run, so i cant write a explanation, but googleing the gap buffer and text editor will yield many explanations.
When you say "most advanced math library", what exactly are you looking for? Numeric tower (rationals, complex, etc)? Vector/matrix/tensor? Symbolic algebra?
I would refer you to https://areweideyet.com/
You could cross-post this to /r/gis. A question, what do you use the height data for? I can't see any (visible) elevation in the video. What library do you use for triangulating the polygons - or does gfx do this for you? There is also https://github.com/simonepri/geo-maps which is already in GeoJSON format + varying themes (coastlines, lakes, land, etc. as seperate files) + better generalization (10m, 50m, 100m).
It is in the Atom IDE ide-rust package https://atom.io/packages/ide-rust 
That doesn't really help, it doesn't say anything about the quality of code completion? VSCode according to this would be the best IDE. But code completion works so badly that it's almost no use at all IMHO.
One of the bindgen authors is co-author of that spec, very nice :).
To solve the problem as stated you likely need to allocate an intermediate Vec&lt;std::cell:Ref&lt;'a, T&gt;&gt; where 'a borrows the original Vec of Refcells. The references in the final Vec will borrow that intermediate Vec.
Your [`run_slice`] and [`run_iter`] methods should not take references to closures. Instead, just have them take `F` directly. `Fn` is implemented for `&amp;T` where `T: Fn` anyway. Also, you can probably drop the `Send` requirement on `F`. [`run_slice`]: https://docs.rs/rpm-timer/0.0.1/rpm_timer/struct.RpmTimer.html#method.run_slice [`run_iter`]: https://docs.rs/rpm-timer/0.0.1/rpm_timer/struct.RpmTimer.html#method.run_iter
&gt; But proc macros result may not be usable in const contexts (such as for the size of arrays). The output of a proc macro is just like what you would be writing normally. Proc macros can emit whatever they want, and run any computation or any I/O in order to obtain the result.
Which Rust extension are you using? There are several at this point, some abandoned.
it would be incoherent to add `PartialEq&lt;&amp;T&gt; for T where T: PartialEq&lt;T&gt;`, because there's an impl of `PartialEq&lt;&amp;T&gt; for &amp;T where T: PartialEq&lt;T&gt;` and this would provide multiple paths to `PartialEq&lt;&amp;&amp;T&gt; for &amp;T`.
So it seems what you want is a [Rope](https://en.m.wikipedia.org/wiki/Rope_(data_structure)). A few creates exist that give a Rope type already, go and look at them and see if any are usable in your use case.
A problem with your `SizeHint` is that it is still just a hint. You can't be sure that the iterator will behave correctly. Thus, even if you get a `SizeHint::Unbounded` from which you `take(n)`, you can't implement`TrustedLen` for that `Take`, as it is not ensured that the underlying iterator is actually unbounded. Thus, you can still only interpret it as hint for some memory-safe optimizations. But for example you can't eliminate the `None`-branch from code using an unbounded iterator. I also guess that a static analysis to find iterators whose `size_hint2` returns `SizeHint::Unbounded` would be pretty hard.
TrustedLen2 as a trait can just as well exist to vouch for the Unbounded case though.
How can I get autocompletion of inferred types? There's a lot of situations where the autocompleter just gives up and doesn't display anything. Especially when chaining things like `.map(...).filter(...).map(...)...` and it would be really convenient to have better suggestions.
I don't know that :/ I'm not really using code completion, but I see them when I'm coding. 
I definitely get usable (albeit not perfect) auto-complete in VSCode. Do you have `rls-preview` installed with rustup and the `Rust (rls)` VSCode plugin (instead one of the two others)?
Got it, I‚Äôll try to pass the fetch function to my module and see if I manage to use it. Thanks!
I've recently started using atom because I didn't find a mature Julia plugin for IntelliJ. I hate it so much I'm thinking about just using a dumb text editor when writing Julia instead. 
&gt; Some C libraries could potentially compile to WASM but I would expect that to require some manual setup. This would probably be much easier with the emscripten way of going from Rust to wasm.
It's a pity that tooling is not mature enough. 
&gt; Yes, because you'd have to design far more stuff. For example, you wouldn't be able to use std's file api to do I/O [...] I don't see why not, to be honest. &gt; Also, as I've said, you either have to pass the sha-256 hash when opening files What for? As far as the compiler is concerned, any file that is read becomes a source file, and the same rules apply than about "code" source files. The file would only be opened and read once. Any subsequent call to `open` would simply return the same file over and over again. As a result, the cached content is *necessarily* consistent with itself and there is no need for any hash shenanigan, and therefore no need for any API change. You do not pass hashes of the `.rs` files which are compiled, and instead trust the compiler to handle them correctly *automatically*. The compiler can perform the same verification work by itself for other files than `.rs`. &gt; and every potential sha-256 collision would cause unsoundness bugs lol As much as any partial compilation today can, indeed. There's nothing worse than a binary compiled from half-old and half-new sources which happen to line up just enough for compilation to end "successfully". This is nothing specific to non-code files, though.
Yes :) but seriously, I really want to tackle some tough problems in WASM, so having a good math library would help me use WASM's strength
Yeah it seems like a really hard problem. If there was a way to embed a `lazy_static!` thunk in a const object it seems doable, but that would require some major language level machinery. It would also require either allowing const objects to be mutable or introducing a notion of purity, neither of which seems like a good idea to me. Basically it seems doable, but the tradeoffs are just not worth it.
Of the 3 ways we can "construct on heap" at the moment, which is the most likely to stay/become stable at some point? #![feature(placement_in, placement_in_syntax, box_syntax, placement_new_protocol, box_heap)] use std::boxed::HEAP; let foo1 = HEAP &lt;- [0; 1024*1024*30]; let foo2 = in HEAP { [0; 1024*1024*30] }; let foo3 = box [0; 1024*1024*30];
&gt; As far as the compiler is concerned, any file that is read becomes a source file, and the same rules apply than about "code" source files. Source files are opened during lexing/parsing. That's also when include! operates. However, we are talking about const eval here, which is a much later phase. The "source file" analogy applies to all input read by proc macros and include! macros, but it doesn't apply to const eval. Most importantly, there are no soundness issues if you include! a file in multiple places throughout the codebase and you get different contents (unless you violate the preconditions of user written unsafe code). There are unsoundness issues however if two invocations of a const fn read the same file and come up with different results. This is a big difference! The unsoundness even occurs if the const fn computation happens in separate compiler invocations, for separate crates. &gt; As much as any partial compilation today can, indeed. The difference is that you can change the hashing function quickly and easily if you run into any issues. Changing the hash function is harder. Either way that was only a side remark. &gt; The file would only be opened and read once. Any subsequent call to open would simply return the same file over and over again. As a result, the cached content is necessarily consistent with itself and there is no need for any hash shenanigan, and therefore no need for any API change. Yes, this proposal is nice, but as the irlo link in the [parent comment](https://www.reddit.com/r/rust/comments/7ju75d/miri_was_merged_into_rustc/drasyrl/) points out, the computation must not just always come up with the same result within any calls inside the compiler. That's the easy part. It must also always come up with the same result within any calls inside the compilation of *any other crate* inside your cargo tree. If you do the caching approach, there needs to be coordination between compiler invocations of different crates about what the content of the files was. I've been thinking a little bit, apparently you don't need any IPC like I thought when I wrote GP post. You just can store the hashes of all files read during CTFE in the metadata of your library and then error if you try to use libraries where filenames match but file content hashes don't match. The only way of avoiding an error and just using the first version of the file would be to do IPC though. &gt; &gt; For example, you wouldn't be able to use std's file api to do I/O [...] &gt; &gt; I don't see why not, to be honest. Yes I wrote "you wouldn't be able" but I said afterwards that its not impossible yet very hard. In the comment above I've pointed out why this would be a hard thing to do but maybe you missed it. Making the point again, more verbosely this time. Miri uses the *very same* MIR that Rust uses later on. If you call `std::fs::File::open` in your const fn, or in your non const fn, or anywhere else, the compiler emits a MIR call to C APIs of the specifc OS, because that's the implementation of the `std::fs::File::open` function. `File::open` looks like this on windows: pub fn open(path: &amp;Path, opts: &amp;OpenOptions) -&gt; io::Result&lt;File&gt; { let path = to_u16s(path)?; let handle = unsafe { c::CreateFileW(path.as_ptr(), opts.get_access_mode()?, opts.share_mode, opts.security_attributes as *mut _, opts.get_creation_mode()?, opts.get_flags_and_attributes(), ptr::null_mut()) }; if handle == c::INVALID_HANDLE_VALUE { Err(Error::last_os_error()) } else { Ok(File { handle: Handle::new(handle) }) } } And like this on unix: pub fn open(path: &amp;Path, opts: &amp;OpenOptions) -&gt; io::Result&lt;File&gt; { let path = cstr(path)?; File::open_c(&amp;path, opts) } pub fn open_c(path: &amp;CStr, opts: &amp;OpenOptions) -&gt; io::Result&lt;File&gt; { let flags = libc::O_CLOEXEC | opts.get_access_mode()? | opts.get_creation_mode()? | (opts.custom_flags as c_int &amp; !libc::O_ACCMODE); let fd = cvt_r(|| unsafe { open64(path.as_ptr(), flags, opts.mode as c_int) })?; let fd = FileDesc::new(fd); // Currently the standard library supports Linux 2.6.18 which did not // have the O_CLOEXEC flag (passed above). If we're running on an older // Linux kernel then the flag is just ignored by the OS, so we continue // to explicitly ask for a CLOEXEC fd here. // // The CLOEXEC flag, however, is supported on versions of macOS/BSD/etc // that we support, so we only do this on Linux currently. if cfg!(target_os = "linux") { fd.set_cloexec()?; } Ok(File(fd)) } So calling `File::open` in a const fn leads to MIRI evaluating this code. Only at the C API is where MIRI would interface. MIRI would have to implement both the `open64` function for unix targets, and the `CreateFileW` function for windows targets. Everything in a cross platform way. And any changes in the implementation in std would need changes in MIRI as well. I'm not saying that this is impossible, but is it hard? Definitely. 
In the last few days I started trying to set up VSCode for rust development (in preparation for having some free days to hack over the holidays.) I've been unable to get RLS working, neither on the nightly nor beta channels. 
It's called 'rls-preview'.
I just figured that out mere seconds ago. Thank you. 
Tokio has an echo example. Use that as a reference. https://github.com/tokio-rs/tokio-core/blob/master/examples/echo.rs
Hopefully this is useful to someone =)
My first thought after watching this video was ".. but what it would be good for", but I've read last line of the comment, and the only thing I can say is keep doing this, great job :D
I agree it's clickbait-y, sorry about that. But I'm having trouble finding a pithy way to describe what this project is (would love suggestions). "2.5D Planet Visualizer" was the next-best thing I thought of, but it's also inaccurate, since it's not actually 2.5D.
I've been using LanguageClient-neovim with RLS. Pretty solid. RLS is much more solid than other language servers (eg javascript-typescript-langserver), and generally pretty quick and accurate. I have Denite.nvim as well, for Find References.
It's something I'm considering. A server-based setup would also let me use much higher-res images, larger than what most people would care to download at once.
RLS falls back to Racer when the compiler is slow, not the other way round.
It's hard to see, but there is elevation there. When you play with it yourself, it has a subtle but neat effect. It's hard to find the right balance that makes the Alps visible, but doesn't make the Himalayas look painfully spiky. If you look carefully at the screenshots, you should notice that the terrain doesn't look like a plain flat image around the Balkans -- that's the effect of elevation. The polygons aren't triangulated -- under the hood, I'm using a crate I made called [`gfx_draping`](https://github.com/ucarion/gfx-draping) that does shadow-volume stuff to overlay the polygons. Triangulating the polygons would produce _way_ too many vertices, though it did produce neat images when I tried it out: https://drive.google.com/file/d/0B209zZpqpNEFbXVOb2IxenB4UV96OWZFUGYwMzhMaVRmWnNV/view?usp=sharing Thanks for suggesting /r/gis and geo-maps! I wasn't aware of either.
Since Raspberry Pis are often (but not always) running Linux, what does the Raspberry Pi support add on top of the Linux support?
`rate limited API` - I was really missing this term. Thanks a lot, I will update description, docs and keywords next week! :) 
very nice, some questions:- [1] would it be within the scope of this repo to add native-desktop emulation of the canvas functionality (e.g. canvas style rendering implemented in OpenGL, whatever) [2] How much of this is specific to *wasm*, could it be used in conjunction with asmjs aswell (... does wasm come with deeper access to the browser or something?)
[removed]
The main difference is that raspberry pi are running on arm architectures.
Thanks for this suggestion. Until now I thought it is rather impossible. Your comment motivated me to recheck it and I see I can remove reference in fn argument by creating another variable just before spawning new thread and store the reference there. Then I move this variable to the thread, so, just as you said, I don't need `Send` on `F`. Perfect! Btw. I disliked `&amp;Fn` too, nice to have it fixed now! Thanks once again!
How backwards-incompatible would it be if we just let operators autoref like method calls?
I use intellij and it works very well.
[1] that's a fascinating thought! to be honost :P I wonder if it would just be easier to write an Electron app ( cross native desktop chrome in a window frame ). https://electronjs.org/ [3] wasm comes with zero integrations with the browser api, all my integrations are through super super minimal dom api wrappers with just enough code so wasm can talk with them efficiently.
Oh wow, this really incredible. I'm just blown away that I just ran a web application on my computer that was written in Rust. Wow.
Yes and no. intellij-rust is okay, have some issues, but pretty usable and they move fast. RLS + Racer is better in some cases, but worse in others. In some cases, it's easy to make in constantly crash until you remove faulty code. I'm using both. Feels weird because in some cases one can help you, but others cannot. 
Wow, this looks like it keeps going easier and easier to use code written in Rust from Javascript
I don't know if I have the patience to learn vim. :O
I have Rust (rls), but I'm not sure which version of rls I have.
I have tried both Rust and Rust (rls), both seem equally bad.
Wonder what the runtime cost of this is. iirc, with pypyjs there was quite a lot of overhead to crossing the boundary, even compared to its existing overhead
This is really great. I've made a [proposal](https://internals.rust-lang.org/t/wasm-unknown-target-implementing-stubbed-out-functionality-inside-libstd/6388) to get this integrated into libstd so that we get stuff like println! working.
Probably the cost of calling from JS -&gt; Rust and Rust -&gt; JS is measurable. But if we're interacting with the DOM directly then we can bypass JS completely. I do know that most of the Rust -&gt; Language-X projects (e.g. Neon, Rustler, Helix, etc.) incur that performance penalty, so it's definitely worth worrying about.
I beg to differ. If you want to develop any sophisticated (maybe graphical) application for more then one platform (or even hardware), issues like that are commonplace.
Actually the rls _always_ use racer for completions. It never use the compiler for this task 
Yeah this setup is very pleasant to work with 
I'm not a big fan of the stdlib panicking if you use a function that isn't supported on that particular platform. That hurts confidence in the compile-time safety of programs. I'd much rather see unimplemented stuff fail at compiletime. Is there an RFC to this effect?
It sounds like your basically describing futures? 
&gt; Is there an RFC to this effect? I don't think there is, the unknown wasm target is very young. The general issue is that if you actually gut libstd and remove everything unsupported from std, many working programs will fail to compile because some unused (but compiled) part of them uses file I/O or something. The choice by acrichto to make these impls panic definitely made sense IMO.
Hi! I'm trying to define a structure (a graph) with self references. I'm thinking that I could get by with just a RefCell. However I run into a problem while trying to initialize my structure. A minimal example looks like: &lt;code&gt; #![allow(dead_code)] #![allow(unused_variables)] use std::cell::RefCell; struct A&lt;'a&gt; { b: Option&lt;&amp;'a RefCell&lt;B&lt;'a&gt;&gt;&gt;, } struct B&lt;'b&gt; { a: Option&lt;&amp;'b RefCell&lt;A&lt;'b&gt;&gt;&gt;, } struct C&lt;'c&gt; { a: RefCell&lt;A&lt;'c&gt;&gt;, b: RefCell&lt;B&lt;'c&gt;&gt;, } fn main() { let a1 = RefCell::new(A { b: None }); let b1 = RefCell::new(B { a: None }); // doesn't work: //let c: C = C { a: a1, b: RefCell::new(B { a: None }), }; //let c: C = C { a: RefCell::new(A { b: None }), b: b1, }; //let c: C = C { a: a1, b: b1, }; // works: let c: C = C { a: RefCell::new(A { b: None }), b: RefCell::new(B { a: None }), }; c.a.borrow_mut().b = Some(&amp;c.b); } &lt;/code&gt; The error in question is &lt;code&gt; error[E0597]: `c.b` does not live long enough --&gt; src/main.rs:32:1 | 31 | c.a.borrow_mut().b = Some(&amp;c.b); | --- borrow occurs here 32 | } | ^ `c.b` dropped here while still borrowed | = note: values in a scope are dropped in the opposite order they are created &lt;/code&gt; I don't understand by what logics it matters if I move things in during initialization. Also I found that the problem arises only if those things are parametrized with lifetime 'c. I'd love to get some idea about rust compiler's reasoning here.
What's the overlap/differences between wasmblock and stdweb, now that stdweb has wasm32-unknown-unknown supported as well?
I support far less of the API, but I have little magic about how my code works.
It's interesting that the most advanced IDE implementation doesn't use the language server. Wasn't that the purpose of the language server?
If I recall correctly, wasm can't interact directly with the dom, and has to use JavaScript to do so. 
Panicking is safe.
I don't understand the part "it won't scale up when I have lots of classes that I want to keep thread-safe". First of all, Rust doesn't have classes, are you talking about structs or types in general? What's the problem with keeping the threads thread-safe? Are you afraid of lock contention or what?
If you want high performance, you could have thread local loggers that have internal buffers, and the on logger thread that aggregates those buffers and produces output.
I don't think the argument is about whether it's safe or not ‚Äì it's about if it's desirable or not.
The Rust Language Server is designed to be used by IDEs, yes. But its auto completion just isn't that good yet. If you're asking why they created their own completion engine instead of contributing to the RLS, I don't know.
What is this `@` syntax used in the examples? Is that part of the rust language? var print_hello = @{print_hello};
I don‚Äôt think that wasm has a DOM API, which I see as an unfortunate oversight. Also, there‚Äôs things like WebGL and all the other browser interaction things to care about, even when it‚Äôs just simple things like opening a link in a new tab.
I did a little bit more reading on it and it seems they are ignoring it for their mvp release. My guess and hope is that it will be revisited in the future when they can.
Perhaps a custom part of the macro.
You can do it! Really helps if you ever have to admin a linux server, because vim is everywhere. Even if you just do vimtutor, an hour of your life learning vim is worth countless times not having to use `nano`.
I stand corrected. Interesting. And that is some delightful rust code, reminds me of Event Sourcing.
Use `String::insert` as others have suggested :) If you want to go more advanced, consider a [gap buffer](https://en.wikipedia.org/wiki/Gap_buffer). A cursory search on crates.io reveals some relevant crates: [gap-buffer](https://crates.io/crates/gap-buffer), [gapbuffer](https://crates.io/crates/gapbuffer) and [scribe](https://crates.io/crates/scribe) are at least examples of this :)
**Gap buffer** A gap buffer in computer science is a dynamic array that allows efficient insertion and deletion operations clustered near the same location. Gap buffers are especially common in text editors, where most changes to the text occur at or near the current location of the cursor. The text is stored in a large buffer in two contiguous segments, with a gap between them for inserting new text. Moving the cursor involves copying text from one side of the gap to the other (sometimes copying is delayed until the next operation that changes the text). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I like to keep projects extremely modular. Modular as in distinct separation of responsibility. Which allows me to do sloppy implementation in some places and very refined on others. Project I'm working on right now - I know the end goal and I've spent zero time on architecture overall project. However I know that to achieve that goal I must make at least two modules: zfs and jails. Both modules don't have much freedom design wise, so I've spent very little time on architecture there. Now, my other project is 80% design when I'm working on it. This one is different because I don't know what I want in the end. That's because it's a protocol and rust here is just reference implementation.
Nope; it's part of the `js!` macro!
Are the `js!` parts typechecked? If not has there been any thought put into Flow or Typescript integration?
What problem? :)
For short term adoption, but for production if a program requires a feature that isn't present, it's better for it to raise a compile-time error so that the developer knows as soon as possible. This is basically the Rust ethos of safety first. A panic is in some ways a step backwards from the existing practice in C etc of having stub functions and/or an error code to indicate an unimplemented function. You can't recover from a panic, so even if there's an alternative code path that the developer put in place if opening a file failed, the user is stuck with a broken program. Panicking may be "safe", but to the average user it's a "crash" and they'll still perceive the program as broken and unreliable.
&gt; Are the `js!` parts typechecked? If not has there been any thought put into Flow or Typescript integration? Currently they're not. They're not even syntax-checked. Right now `js!` is implemented as a normal `macro_rules!` macro, so what I can do with it is quite limited (And I'm already streching the limits of `macro_rules!`; I suggest you don't look at how `js!` is implemented if you want to stay sane). In the future I plan to at least replace it will a procedural macro (once that stabilizes) which will make syntax checking possible and cut down compile times drastically (right now `stdweb` doesn't have the recursion limit set to `1500` just for show). As far as integrating type checking - that's a good idea! Now that I think about it I guess we technically *could* typecheck it *after* compilation by feeding the generated `.js` file to a normal JS typechecker. Thanks for the idea!
In my ideal world, in the future one could have an easy bridge between typed JS and Rust. [Here is an example of what has been done for inline Java in Haskell](http://www.tweag.io/posts/2017-09-22-inline-java-ghc-plugin.html). Would be super nice! Not sure how much is possible currently though, especially given how limited the Webasm bridge is.
I'm not a developer or have expertise in any of these matters, but a thought I had is to stretch the height of all your elevation data (or maybe not linearly for flatter stuff) in order to make things more visually interesting.
Get something that works, experiment after with benchmarks. Rust's safety guarantees allow for safe refactoring
One critique I know about LS protocol is that it's not perfect for IDEs. Also that JetBrains has its own analysis framework.
You can press '+' to collapse/expand everything. Scanning through the methods for likely candidates may be easier when it's collapsed.
the comparison operators already do autoref, unlike the arithmetic operators. What I mean is that `x == y` calls the `PartialEq::eq` function with arguments of type `&amp;X` even if `x` and `y` are of type `X`.
How useable is it currently? I have almost no experience with javascript of webdev at all, but I wrote a js snippet a few days ago that reads a `.rs` file from my github repo, and executes it in the playground. This allows me to run Rust snippets on my blog. Can I do this now completely in Rust? [Reqwest](https://github.com/seanmonstar/reqwest) doesn't seem to compile and I am also not sure how I would do async things as I can not use threads. It seems that the easiest solution for now would be to expose a Rust api that just calls js behind the scenes.
&gt; Miri uses the very same MIR that Rust uses later on. If you call `std::fs::File::open` in your const fn, or in your non const fn, or anywhere else, the compiler emits a MIR call to C APIs of the specifc OS, because that's the implementation of the `std::fs::File::open` function. Today yes. However since using I/O would require special handling anyway, as part of making the function `const` I can imagine intercepting the call in MIRI. &gt; Source files are opened during lexing/parsing. That's also when include! operates. However, we are talking about const eval here, which is a much later phase. I feel dumb now as I really fail to see why the phase matters. I assume I may be missing an implementation detail :( &gt; You just can store the hashes of all files read during CTFE in the metadata of your library and then error if you try to use libraries where filenames match but file content hashes don't match. How are source files handled? The same problem which happens for the content of a file also happens for an inline function: if two crates use a different "implementation" of an inline function, then this can quickly lead to havoc, and I'm pretty sure soundness issues. As such, it seems that the file should be bundled within the one crate it was defined in, and all other crates should refer to *that*. This calls for a special handling of paths here: it would be impossible to use absolute paths (as the user may install the crate in any location they wish) and it should be possible to refer to a file in different crates, therefore the path would have to be something like `"/&lt;crate&gt;/&lt;local&gt;/&lt;path&gt;/&lt;in&gt;/&lt;said-crate&gt;"`, with MIRI translating this to the appropriate path based on where the crate is installed.
I'm curious when we'll see frameworks similar to [Blazor](https://youtu.be/MiLAE6HMr10?t=33m20s) but in Rust.
I always saw this as a short-term work-around until they can be properly implemented. I myself use `unimplemented!()` pretty regularly in my own programs: I see it as a TODO list which cannot get out-of-sync.
I probably need a tree to hold the buffer anyway, since I probably need an AST as well.
is nano really that bad? I just can't get used to vim so easily, so nano works ok for small amounts of editing work...
You could try [slog](https://github.com/slog-rs/slog) with an [async](https://github.com/slog-rs/async) drain.
&gt; and doesn't have that level of optimization for each of them Oh really? Which ones exactly? :) The only one I can think of is maybe `-f`.
Thank you for your comments. I like the idea of the matching the API with the `HashMap`. Can you please explain what does &gt; "normal" trie mean?
Currently lacking in examples, but I hope to have a few up for things like strings and event handlers/callbacks in the next few hours. The main selling point of embed_js is it's lightweight. It doesn't make any choices for you about how to structure your web application. It's also lightweight in that effort has gone into making sure the resulting wasm and JavaScript is small. I hope that these crates (embed_js and embed_js_build) can be used as a building block for modular crates for web functionality.
Hello Rustaceans!, going through the Rust book. Does anyone know why I cannot use s instead of &amp;s in this case? https://i.imgur.com/OkKGzOr.png https://imgur.com/HczQqBP Thanks in advance!
As I've matured as a programmer I've given considerably more attention to software architecture, whereas originally I'd entirely neglected this area and focused primarily on learning programming languages and implementing small to somewhat medium sized projects with a heavy reliance on specifications in the form of detailed whitepapers. I picked up quite a few data structures and similar things on the way, but only recently did I really spend considerable time learning software architecture in a more rigorous and focused manner. Software architecture isn't really focused on data structures but on higher level code organization, I believe data structures are largely considered something like 'design choices' whereas architecture is a level of abstraction above that, though including patterns. Anyway, there isn't really a whole lot to software architecture and after reading two or three good books on the subject you will already have a decent foundation of knowledge and probably learn some useful modern design patterns, I think it is time well spent to get a basic introduction to software architecture, but it does have for me anyway quite a low information density because a lot of it covers things beyond the technical designs of software but gets into social and managerial things and organizing social projects, working with stakeholders, and so on, whereas I was primarily interested in learning the more technical aspects, and some of the books also are overly formal I mean you need to get a good selection. Sorry if this was kind of a rant like post, I didn't intend to go on and on like that but for medium sized projects I find that my ability to design them and structure them ahead of time substantially increased after having read about three books on software architecture, but overall my impression is that short of learning some additional architectures like flux or whatever in depth that I've basic familiarity with the area and it will help me with solving your issue. Rust is also particular bad for some architectures I believe and more suited to others, perhaps someone can extend on this? 
&gt; Edit: The implementation approach is also quite different to stdweb/cargo-web - each has their advantages, but embed_js manages to keep the inline JS from being duplicated in the wasm binary and the accompanying JS. The duplication of embeded JS is actually a bug which I do plan on fixing. (:
 I tried intellij-rust for ~2 hours. The main problems I had were: * The more complex types pushed my code all the way to the right. ( granted I barely skimmed the settings page ) * I couldn't find a simple error list to click through ( as with vscode + rls ). Especially the last one was to much to bare and a quick google didn't get me any answers so i switched back. But there must be something like this right? 
Well, let's say it this way. Trie is a strange data structure. The first tool you use for a problem usually isn't a trie, but an array (or vec), a hash map, maybe a tree-based map of some kind. So, if you already *know* you want a trie, you usually have some very special needs. If I want to represent a set of IP ranges, I might want to have a trie that has branches of arbitrary bit lengths (eg. 17 bits long, for example). Indeed, I don't want a node after each bit, that would take a lot of memory and would be slow. If I want to store a bunch of strings, I want to compress the trie, I don't want a node after each single letter. But then, if I want to add a new string there, I might need to split a branch in half and insert a node there. Which makes deletion complex. These are all "specialized" tries. What I mean by a "normal" trie is the most ordinary, text-book trie you can think of. Nothing fancy, nothing special.
Some nanos will hard-wrap lines to your window width by default, ruining config files. And you don't get undo and redo. You can't do the following in nano: 1. Find and replace but stop to check each instance * vim: `/`, match text, `enter`, `ciw`, replacement, `n`, `.` 2. Make a bunch of lines into comments * vim (barebones no plugins): `ctrl+v`, `10j` (10 lines), `I`, `#&lt;space&gt;`, enter.
&gt; When a function on the logger is called, it should return immediately but also begin doing some work in the background. and &gt; but it won't scale up when I have lots of classes are telling me you're trying to do OOP. You can't do this in rust. The basic pattern in rust using threads would be: A (logger) thread is looping on a MPSC Receiver and writing incoming messages to disk,. Your other threads would have a corresponding MPSC Sender so they can fire and forget. If you tell us more about your system we could be more specific in what are some good options. P.S. &gt; I'd also like to be able to schedule low and high priority tasks, and have it figure out which ones to run when. That's a whole different can of worms. If you want a huge amount of parallel threads , languages like Go or Elixir might be a better option. Otherwise you will likely have to create a new tokio reactor. 
s[..] is a string slice, a special type that can only exist as part of an owned type. You can freely borrow it, but neither copy nor move it, because the compiler cannot be sure it will fit on the stack.
I have a hyper / tokio for stdweb based on the futures + http crates so it‚Äòs compatible with the entire ecosystem. So far koute doesn‚Äòt seem to want it to be merged into stdweb though and I don‚Äòt know if it‚Äòs worth releasing as a separate crate.
Can't answer the exact question, but what might trip you up are linker errors you'll experience in the browser (eg. "import object field 'fmod' is not a Function (unknown)"). You might need to make available JavaScript's math functions then to get things running (see [Rust's shim.js for that](https://github.com/rust-lang/rust/blob/71340ca4e181b824bcefa887f1be60dd0b7352ce/src/etc/wasm32-shim.js)) before we start thinking about how to get those functions to work fast.
It's not an oversight. Adoption of wasm only happened because it was small; the idea is to start small, and go from there. DOM bindings are absolutely on the wasm team's radar; they're actively working on it.
thanks for the answer!! so the slice type is a struct with: a pointer to other string, and a length? What if I use something like "let hello = &amp;s[0..5].Clone() " (probably the syntax is wrong but my question is if clone will create a String and not a slice pointint to another string) Thanks in advance!
As far as I know it wasn't the kernel, but the Live ISO image. If you would install Redox, it would take less memory.
The strings example is now up, at https://github.com/dylanede/embed_js/tree/master/examples/strings
How are you thinking of going about it?
This is all still up in the air. See [this issue](https://github.com/rust-lang/rust/issues/27779) for discussion. Unfortunately, addressing placement-new doesn't seem to be high priority right now but the impl period is coming to a close before long (**today** if [the original announcement is still correct](https://internals.rust-lang.org/t/announcing-the-impl-period-sep-18-dec-17/5676)). Then everyone's probably going to take time off for the holidays, and in the first couple weeks of January there will be a lot of discussion about the 2018 roadmap, so we'll see then if this is on the board. 
Another general-purpose option would be a [rope](https://en.wikipedia.org/wiki/Rope_(data_structure)).
Apparently it doesn't apply to comparison operators, but: &amp;(&amp;(a + &amp;(&amp;b * &amp;c)) - &amp;d)
The page use big-theta notation, not big-O notation. Not that it changes the veracity of the claims and whatnot, but my "OCD" won't let me not point this out :P
I'd rather write either `s[..5].to_string()` or `s[..5].to_owned()`. Both will produce a string (as long as 5 is a valid index into the UTF-8 byte sequence).
Thanks for your answer!
Yes, there is, and it's already been merged: https://github.com/rust-lang/rfcs/pull/1868
But we can't have thread-local everything. Any sort of per-thread cost for each struct is probably not going to scale.
I know my platform though, and it is not built for hundreds of (usually inactive) threads.
Thanks for the help, I'll check out tokio reactors. By classes I mean structs. Making each struct be thread-safe by having its own thread is just not going to scale.
Does it use green threads?
By classes, I meant structs. I'm afraid of the cost of having hundreds of threads, something that the platform is not meant to support. It's an iPhone, and using its own well-tuned C multi-threading API, it won't go any higher than about 50 threads. Although we could use this API with a few hacks, we also want to make this work for Android, so I want to go with a Rust multi-threading library if possible.
Thanks for the clarification. I still don't understand why would you need that many theads ‚Äì one for the logger and some other amount ‚Äì maybe one worker thread? No need to scale that to tens or hundreds.
Yeah. Whether Panicking is memory safe or not, it will still come across to users unfamiliar with Rust as a "crash". There's also no way to handle it with flow control - even if you were just trying to open a file for caching purposes or check for an update and the program would just need to warn the user if failed, the program will instead terminate fatally with no recourse.
How does the performance compare to JavaScript?
&gt; How are source files handled? As I've said, source files are being read in an earlier phase. The soundness issue that we want to avoid is to have two calls to a const fn foo *with the same input* evaluate to different output. If you change a source file, you either change the *implementation* of foo (or the implementation of something that foo calls), or you change the *input* for foo. In neither case, the behaviour is violated.
Hey, thanks for the explanations. In effect, what I was concerned was that my main program/crate (let's call it A), that uses functionality of another crate (B, containing my library) that delivers some functionality that is derived from a third crate (C), and in that way becomes directly dependent on it. I completely understand that this is expected to be an API dependency, as I'm touching things that are not implemented by B, only derived. What /u/newpavlov said about re-exporting is how I feel more comfortable with this - the fact that my program crate is now only touching things via the library crate, and not directly going around to another dependent crate to sort of try and help itself. Thank you for your replies, guys. I feel more at ease now that I've had someone confirm that this is expected. :)
Can you recommend some good books about software architecture ?
HI! This is sweetaurora with the CAMNB clan. I'm looking to populate this battlefield server! M249s in drops/loot boxes, x5000, TP, no twig raiding, server owner with no abuse! I look forward to seeing you! Come challenge me! xoxo
If you need the equivalent of green threads you should to look at Tokio/Futures and perhaps the experimental coroutines in nightly. I highly doubt you actually *need* those just for logging however. You shouldn't need a thread per struct. You can use an [mpsc](https://doc.rust-lang.org/std/sync/mpsc/) (or some equivalent such as [crossbeam-channel](https://github.com/crossbeam-rs/crossbeam-channel)) to pass messages between threads safely.
https://www.reddit.com/r/playrust/
I can not say for sure but just by looking at this example i think its very significant - i have once made a game of life in javascript with webworkers and this here performs way better. Although my version performed very well, too, depending on canvas sice.
Someday somebody is going to write something rust-game related in rust and then nobody will know where to post it.
The callbacks example is now up, at https://github.com/dylanede/embed_js/tree/master/examples/callbacks
so its possible to make http requests from rust without handing it over to browser APIs over you do this bindings behinds the scenes? I‚Äôve been playing around wasm these days but got stuck once I had to make xhr calls from my rust code(I also wasn‚Äôt using webstd so might try it later as well)
So, if the compiler reads a file *once* and then serves the same content for any subsequent request, then the `const fn` call is "pure" and there is no soundness issue.
I just used stdweb's js macros to invoke a bit of JS here and there to implement the different futures.
Sure, just with the caveat that if the same file is being read by another invocation of the compiler for another crate, the two crates maybe not even being dependencies to each other, **the file must still be the same**. That's what makes this problem *hard*.
Yes, the xi-editor uses this: https://github.com/google/xi-editor/tree/master/rust/rope
I'm pretty new to Rust, but so far I've tried VSCode, SublimeText, Vim, and IntelliJ. Out of all of them, I find IntelliJ to be the best overall in terms of code completion and user-experience. However, I am primarily a Scala developer, so I might be a bit biased towards IntelliJ.
Why does making a struct thread-safe entail each struct having its own thread?
The current program uses something similar to `f(z) = 1/z`, which gives it a horizontal asymptote and is very responsive near z = 0. But it's hard to make sudden changes, like the [Sierra Nevada de Santa Maria](https://en.wikipedia.org/wiki/Sierra_Nevada_de_Santa_Marta), an isolated, small, but very tall mountain range. Long story short, I'm still working on it -- right now, I'm considering doing some kind of gaussian blur as a pre-processing step. But I'm not sure how that will behave with the e.g. the Andes, which are basically a wall running along the Pacific coast.
Not sure what kind of scaling you mean here? If you save the work per-thread with rw locks and notify the logger, you likely have better scaling due to lower contention (threads don't block each other while adding lines). You could probably do even better with per-thread circular buffers.
If the context is the logger, why do you mention 100s of threads? Logger needs only one
For now I'll just simply remove that entry in the data section of the wasm file. (That will still waste a few bytes of memory at runtime, but it should be gone from the wasm file itself.) Once procedural macros land I'll probably make it emit the JS out-of-band without making it a constant at all. If you see any other issues with `stdweb` I'd love to hear about them in more detail so I can fix them. (: 
Code completion and error highlighting would be my two main things I like an IDE for. Code completion works very little in VSCode and errors are only highlighted after compiling.
That's true. But it hard to write big-theta on reddit and also 99% of people uses big-O as "reasonably tight upper bound". 
nice! I'll try this later thanks!
This is the subreddit for the Rust programming language. You're looking for /r/playrust
the latest "Rust (rls)" extension, plus latest VS Code, plus latest nightly rls, actually works in my experience. I know what you mean about "only suggests tokens I already wrote"; that happened to me too, on a bad nightly rls. But it seems they've fixed that now.
thanks
here I am reading this thinking "Man, I really do not know the vocabulary of design in rust... Oh."
I have a simple command line program that writes a final result to either stdout, or else a file specified by the user. To do this, I have a `Box&lt;std::io::Write&gt;` that I set to hold either `std::io::stdout()`, or else a `Write` of the user's selected file. Then, I just pass that around to everyone who needs it. However, I would rather have this be statically available, so I don't have to pass it around. I know `static` is frowned upon, but this really is a "set once, use everywhere" situation. The user either provides a filename, and *everything* should write there, or else they do not, and *everything* should write to `stdout`. Is there a good way to make this choice statically available?
&gt; So far koute doesn‚Äòt seem to want it to be merged into stdweb for now Well, the keyword here is definitely '*for now*'. (: I would certainly want to support async I/O in the future in *some* fashion, but before I can do that I need to take a long and hard look at the problem space myself, since the API surface is significantly more complex than exposing something like e.g. `setTimeout`. I hope you will forgive me for being somewhat cautious here, and thanks for all the great work. (: 
does [this](https://play.rust-lang.org/?gist=e93ac4cc9ded6d553eaafe3f55a8bf9d&amp;version=stable) work for you? You can make the closure take a reference explicitly. This reference therefore doesn't have to be the longest of the two references that the closure is called with.
[This forum thread](https://internals.rust-lang.org/t/opposite-of-static/5128?u=dtolnay) discusses a similar idea, also pitched as "opposite of 'static". But I think that one is pretty different from what you have in mind -- it tries to define something that is shorter than the shortest possible lifetime, which cannot be expressed in Rust today.
Here's my second Cargo subcommand crate (after [cargo-download](http://github.com/Xion/cargo-download)). This time I had an idea to make it easier for people to find ways to contribute to Rust projects. It also helps programmers to "pay back" for the effort that went into developing libraries that their own projects depend on. Basically, what `cargo contribute` does is find some unassigned, easy-to-get-started issues that were filed againts the (direct) dependencies of the current crate. Right now, it includes those labeled explicitly as `help wanted`, which is one common way GitHub encourages to highlight a good beginner issue in a project. Links to all such issues, along with their titles, are printed to the standard output. You can easily pick one that looks interesting and start hacking :)
I believe that either 1) this is how references work right now, by default. Here's your example, annotated with explicit `'temp` lifetime: https://play.rust-lang.org/?gist=79bdee8dcc32c6affa63f1c10506aaea&amp;version=stable 2) you're asking for something weird Const references cannot be prevented from "escaping" the block because you can make many copies of a const ref as you want and pass them into the block. Mut references are temporary borrows, and reclaimed once the borrowed scope is over. If this doesn't happen, borrow checking doesn't work.
I believe its completely backwards compatible, and Ariel had an RFC that got postponed just before the impl period.
When I was first getting into using tokio I got a lot of help from https://gitter.im/tokio-rs/tokio It helped a lot. Tokio is moving really fast and I've been quickly told to stop doing something I seen in the docs because it is deprecated. I do not remember the details (so, sorry, I can not answer you actual question) but thought it would be useful to share my experience and the place where I got the most help.
Ah, thanks for the tip. I tracked it down to [this issue](https://github.com/rust-lang/rust/issues/44762) in case anyone wants to follow it.
Does it makes sense to build stdweb on top of embed_js? Possibly dividing it in smaller crates, that can be used separately (all depending on embed_js), while stdweb depends on all of them.
What are the issues with the LSP? Can you point to some threads or articles about it?
Porting a side project game from Python/Kivy to Rust/Piston as my first non-trivial Rust project. 
You a genius.
There's a reason why Jetbrains doesn't use langua... https://www.reddit.com/r/rust/comments/6fs5q9/language_servers_and_ides/dil2gdw
I believe GitHub themselves are encouraging `good first issue` for, well, good first issues to get introduced to an oss package, so you may want to include that tag as well if you don't already
Brilliant!
Yes, intellij-rust probably has the best completion you can get for Rust right now. However, I found it still leaves much to be desired. With more complex crates like gtk or tokio, it will often just stop suggesting anything besides the generic completions ("lambda" etc.) or take many seconds before the suggestions show up.
I shared [symbol-map](https://crates.io/crates/symbol-map) to provide a reusable linked list data structure for a similar sort of approach to building graphical data structures. If there's some way of improving it to fit what you would need, I'd be happy to hear about it.
How is `cargo download` different from [`cargo clone`?](https://github.com/JanLikar/cargo-clone)
A global seems OK in this situation. But since you need to able to modify it, you can't use a vanilla static. [lazy_static](https://github.com/rust-lang-nursery/lazy-static.rs) is the easiest solution. You can put a `Mutex&lt;Box&lt;Write + Send&gt;&gt;` in the static.
Awesome idea. Its going to be a lot easier to develop on something you have used a lot then a project you are new to.
Well played. Making something useful that gets more people contributing. That's good stuff
Change the name now, it is only going to cause problems in the future.
wth is SIMD?
https://en.wikipedia.org/wiki/SIMD
**SIMD** Single instruction, multiple data (SIMD), is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. Thus, such machines exploit data level parallelism, but not concurrency: there are simultaneous (parallel) computations, but only a single process (instruction) at a given moment. SIMD is particularly applicable to common tasks such as adjusting the contrast in a digital image or adjusting the volume of digital audio. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
This was mentioned in the 0.1 thread, and the author did not seem to care then. I fear it's already too late now...
I strongly agree. The name is too generic and doesn't specifically hint at `simd` (the crate's focus). It could also refer to faster parallelization via threading. Or "faster" in a different area, like networking. Could be worse. Could use some buzzword as its name.
&gt; Does it makes sense to build stdweb on top of embed_js? Not really; both `stdweb`+`cargo-web` and `embed_js` are doing something which should actually be handled *somehow* by Rust itself (`std` itself needs such a mechanism!), so I think we should ultimately work towards moving this upstream after we get some experience trying to make it work outside of the `std`.
After a few google searches, I'm inclined to agree. I'm horrifyingly uncreative; would anybody be willing to throw me some ideas?
Nice!
When writing a library, it may be a good idea to include: #![deny(missing_docs)] It forces you to ensure that your entire API is thoroughly documented. Good docs should inform the user and not leave information gaps.
Traits such as [`IntoPackedIterator`](https://docs.rs/faster/0.3.0/faster/iters/trait.IntoPackedIterator.html) should be named after their methods. In this case, the trait name and method don't match. Either rename the trait to `IntoSimdIterator` or rename the `into_simd_iter` method to `into_packed_iter`.
For something seemingly relatively simple, the amount of dependencies the tool has is rather high.
It's because it's part of a larger family of software that has its own framework for building parsers and whatever tooling you need for autocompletion and such to work.
ksss (keep simd simple stupid) simple_simd fearless_simd I'm a big fan of crates that does what it says on the tin, so I'd go with either of the two last ones if it were me :)
I'm a miserable old curmudgeon who hates emoji. You've earned this: üéä üéâ Also, as a note to those reading this: docs.rs may take some time to generate the docs, so don't be surprised if links to the documentation don't work right away.
This crate has a focus on iterators, so it doesn't seem to just only be for making simd easier.
Lots do stuff to contribute to!
simd_iterators?
I was trying to come up with something that would play off of rayon, since I see both crates as being in a similar category. Came up with nylon, dacron, orlon, silk... https://www.powerthesaurus.org/rayon
I've worked a lot on [polyclip](https://github.com/fschutt/polyclip), which is a library for boolean operation on polygons (subtract, union, xor two arbitrary polygons, even with holes and self-intersections). The core algorithm is ported (from [here](http://www4.ujaen.es/~fmartin/bool_op.html)), it just doesn't work that well with Rusts memory model, so I have to resort to lots of UnsafeCell usage. Ex. Rust doesn't "get" that if I can take self-referential pointers into a vector that doesn't get modified afterwards. Also I had to make a fork of BTreeSet, because it doesn't allow me to insert and directly get an iterator to where I've inserted the value. I'm also working on a hillshade renderer and stabilizing [fastblur](https://github.com/fschutt/fastblur) (a constant-time gaussian blur algorithm) so that I can blur hillshade images, [this](https://i.imgur.com/hSdU6MZ.jpg) is what the result looks like so far - it's a tiled SRTM renderer, did not 100% line up correctly. So that's mostly what I'm working on this week.
&gt; As I don't believe there are other choices aside from tokio I need to live with its awful docs and API so moving to std::net is not option I'd prefer. Why do you not believe there are other choices aside from tokio? You can use OS API (socket API, epoll etc.) directly, or use mio and net2 etc.
Hmmm It's got quite the steep learning curve, but in the end, it becomes very rewarding, eh?
Because I need to use tokio is mainstream and most async APIs are based on tokio. And I need to use it together, in my case with irc library.
You could get a slight speedup by swapping the entire `life` and `next_life` vecs inside `AppState`, instead of looping through and assigning.
Do fuzzy matching on code samples and errors from the issues compared against the code for the project and errors (currently? unless there's a log someplace...) encountered when compiling.
simdple? simmple? rsimd? fastsimd? powersimd? lightspeed? just some bad ideas in a minute of brainstorming :)
I'm really glad to hear that tokio is getting simplified. I tried to read the docs and get into it, but it was just too many new concepts to do it as a learning project on the side.
The Rayon name is from the original paper called Cilk. SIMD is a different kind of parallelism, and so maybe a catchy name based off a seminal SIMD paper/library?
Oh well, that's what I get for just reading the tag line :p
So I decided to get into electronic music again, and I bought a Novation Launchkey 49 and Circuit. They're both pretty cool. However, as a basic keyboard, the Launchkey is not very full-featured. It works, yeah, but it could do so much more. I don't have Novation's specific software (and not that I could, since I only develop audio on Linux), so writing what I think is useful from the ground up is proving to be interesting. I'm playing around with the Launchkey's extended mode (called InControl). I'm in the process of writing a software controller using rust-jack to initialize and resolve inputs and outputs to and from the keyboard. It's been quite the learning experience, and I didn't think I'd be able to power through a few hours of the lifetime issues I've run into (mainly because when I sprinkle lifetime paramters around, I always wonder why the compiler can't solve it for me). I feel like lifetimes are going to be one of those things I'll eventually understand (sort of like Haskell monads). So far I have it so I can light up the keyboard. Not much, but I dove into this without any knowledge of how the low level jack API actually worked. I need to add state awareness to some of the buttons (specifically the ones under the sliders) so that I can use them as a toggle (they currently send on/off on key down/up instead of toggling). One of the things I love is that the support documents for both the Launchkey and the Circuit have quite detailed programmer's reference for what midi signals to send to get a desired effect. For example, I can set the Circuit's synths to be monophonic instead of polyphonic (play single note versus multiple notes) by sending a midi control. It's so cool to me since I've never done this before. I didn't expect them to release their programming references (so to keep people locked into their products). Rust is proving to be quite a nice language. I'm spending more time fighting the compiler and less time hunting runtime bugs.
This sounds like the way things work by default. If you borrow something within a scope, it cannot escape that scope. When you say "The inputs must be assumable to be temporaries", you are not referring to [Lifetime elision](https://doc.rust-lang.org/beta/nomicon/lifetime-elision.html)?
Dunno, nylon is nice because it's easy to remember and it's not taken.
I had a look but I think the approach is quite different. What we are talking about here is having data stored in an arena that allows reuse of freed space. You seem to be using a linked list of individual allocations in boxes.
I want city name puns (tokio) to be a trend so... simdney!
The thing about these libraries right now that I don't understand is how the JS interacts with the generated wasm file. Like, if I'm providing the wasm file somehow, what relationship does the JS file have to have with it? If they absolutely have to go together and there's no other way, why can't the JS file wrap the wasm file? What relationship does wasm file have, if any, with the content in the DOM (ie the preexisting HTML)? Does stdout effectively write to the DOM? These are probably "stupid" questions, but I feel like what's missing right now is the idiot's guide to using WASM for non-enthusiasts that are starting to get the tools that they can use to build stuff. And, of course, a UI library, but I'll give that until at least tomorrow. ;)
Very cool, would be nice if it worked fullscreen.
Cool. Does it implicitly support other ARM-based SoC systems? (Beaglebone, odroid, etc)
&gt; Some C libraries could potentially compile to WASM but I would expect that to require some manual setup. Time to break out corrode...
I have to admit that I have no idea. If they're linux based, they should work but I'd need someone to test (and if it doesn't work, to point me to the place I could get an image of such system).
Bugfixing and incorporating my mio based http client [mio_httpc](https://github.com/SergejJurecko/mio_httpc) in my commercial (closed source) projects.
And I'm pretty sure xi uses rope to good effect. One thing to be aware of when considering the rope data structure is that while the algorithmic complexity (O-notation) of the operations is pretty good in all cases, it suffers a relatively large speed penalty because of indirections: Traversing a rope is likely to cause more cache misses than traversing a gap buffer. Not to say that rope is wrong in every situation, but this is something to consider if you are considering using rope _for performance._
I was looking for a changelog or release notes but didn't see any. I can recommend [keeping a changelog](http://keepachangelog.com/en/1.0.0/), it's very useful for users of the library.
FINALLY!
IIRC Beaglebone can run Ubuntu. I don't know what Odroid is usually loaded with, or if there are special distros for either. NB that Arch Linux Arm can also be used with Raspberry Pi (last I checked anyway) but I believe Ubuntu is much more prevalent on ARM.
The biggest thing I want is some way of doing ‚Äòself lifetimes for self referential types. I know there‚Äôs something about drop as it relates to this, but it would be so nice.
Just out of curiousity, why is the feature for including "user32" called "winuser"? Wouldn't it make more sense to have the feature names exactly match the library they interop with?
I find the city names to be pretty interesting as well. *cough* (buda)pest *cough*
I don't really want to advertise any particular book to the exclusion of others because it may largely depend on the path that you want to take through the materials, and what was right for me may not be right for you. However, I will say that things I would take as good signs when previewing such a book, is if it indicates in its index that it covers lists of patterns, for example this site I found on google covers some: https://towardsdatascience.com/10-common-software-architectural-patterns-in-a-nutshell-a0b47a1e9013?gi=ee0972405b13 https://www.safaribooksonline.com/library/view/software-architecture-patterns/9781491971437/ Covers some larger scale software architectural patterns as well and it is a free mini book. I found less use out of the older object oriented design patterns material covering things such as singletons, than I did from more modern books that covered things such as "the leaky bucket pattern". Some books cover patterns specific to certain paradigms, functional thinking touches somewhat on architecture of the functional style, for example I learned the pattern of function level caching from it, which has a Rust implementation, it's called memoization. It's hard to call this an architectural book though because it's more like a programming language idiom book, but it certainly helped me to understand Rust. A lot of it may be concepts that you already knew of just with a new name attached to it, that is a critique of patterns, however I find that it is nice to ascertain that you have a basic breadth of knowledge regarding design patterns and the overarching architectural patterns. I personally was less interested by books that talked about modeling languages for describing software systems in a prototyping fashion, these books were highly technical and more academic in nature and seemed less practical and more theoretical or otherwise a degree of rigor that is beyond what could hold my attention in the process of software architecture. I did read some material that covered social aspects but I personally would avoid books that are heavy on this unless you are planning to do software architecture as a paid position at an enterprise in which case you should most likely learn software architecture through an academic course anyway. A good bit is managerial and social organization, but books covering this sometimes also cover numerous patterns so I can't strictly suggest skipping all such books but for me they have low information density. Also a good deal of material covers the network architecture and hardware architecture, certainly don't accidentally get a book on processor architectures some are ambiguous until you ascertain their material. Some cover architecture in terms of server network configuration for enterprise, these are interesting to me though primarily I'm interested in the software architecture rather than how it is deployed onto intelligently architecturally designed networks of servers. So essentially you may be helped by getting material on programming idioms in the paradigms your language supports, or reading general material on programming idioms and design patterns, which helps make well designed code, by learning about data structures however Rust has already largely abstracted numerous useful data structures and you can likely find crates for others and they will oftentimes be written in unsafe Rust so may not want to roll your own. However I suggest as well learning about the overarching architectures of software, and systems like MVC and Flux, and the general designs like microkernel, layers, and so on, to be able to construct larger and higher quality software. With this in mind, there are numerous camps people fall into in direct relation to your question. I know some more formal, academic oriented, security specialists, in IRC, and they suggest that everything should be specified prior to implementing it, typically in a formal fashion as one would see in the Socks 5 RFC for example: https://www.ietf.org/rfc/rfc1928.txt This has numerous advantages associated with it, including assisting in any auditing processes, and allowing for the system to be reasoned about substantially prior to it being implemented. This can lead to high quality, well organized software. However, others suggest keeping designing and specifying things to a minimum, typically they are also proponents of intensive testing. There is essentially a spectrum, and at either end things may be taken to extremes, certainly there is such a concept as over engineering, but running blindly ahead is known to be perilous as well. Particularly when working with teams of people organization to some extent is a must, I mean usually a team of people will use at least an interface skeleton to be designed ahead of time so they can program their components to the interfaces of others and construct the project in parallel. When working on a project by yourself this parallelism isn't necessarily an additional benefit but after projects get to about 5,000 to 10,000 lines of code spread throughout many files things will start to break down if you don't have at least some effort put into creating a patterned structure to the code. 
That's good to know!
Hello, I am a new guy to rust and I have been reading the code of the library I am very confused. Could you please tell me where __js_0 is implemented? I see it being external but I don't understand how the js macro works for the new target. I know in the case of emcripen the asm macro was provided based on emscripen C function, but I don't understand how it works in the new target, is it clang that provides __js_0 ? If you could please point me in the right direction to understand the macro, thank you. Also what is @prepare? I tried to google but couldn't find anything Thanks again
https://github.com/RazrFalcon/choose-your-xml-rs ? Also, https://github.com/RazrFalcon/libxmlparser, but it's very young.
In this case, you need to explicitly specify the type of an argument. let foo = |x: &amp;_| {
0.3 modules are based on windows headers names
Budanom? Budapom? Budarlpop? ü§î
Uncreative names are probably good, how about simd-utils
&gt;I saw html5ever mentioned, but my impression is that it's much more HTML5 oriented, and my impression based on using it about a year ago (it was one of the first libraries I used with Rust) was that it was oriented more towards performant integration with Servo In my experience, it's actually pretty slow. At least when it comes to parsing a HTML document. I was pretty surprised about this when I re-implemented some functionality I had previously done in D with htmld, so I set up minimal projects that merely parse the same HTML file to compare times, and from what I recall htmld was about twice as fast.
Ah, that makes sense.
How does it differ from simd crate? 
&gt; In my experience, it's actually pretty slow. At least when it comes to parsing a HTML document. There are a few problems with html5ever (I worked on xml5ever but everything I say about html5ever probably isn't representative). One of the biggest source of performance penalties, seems to be the fact that html5ever processes input values on a unicode level, and not byte level, causing some performance degradation. One possible source of problems is the fact that h5e (html5ever) is a browser parser. That means, you have to have different assumptions than just a normal parser. E.g. `document.write`. &gt; I was pretty surprised about this when I re-implemented some functionality I had previously done in D with htmld, so I set up minimal projects that merely parse the same HTML file to compare times, and from what I recall htmld was about twice as fast. What is htmld? Google finds absolutely nothing on that. 
Hooray :D Nice work!
Numlon?
Maybe something playing on polymer or polymerization. Since you're talking single (monomer) operations and making them do many operations (polymer). Fits in with the rayon, nylon, etc theme a bit.
A single-instruction single-data example is a CPU instruction to compute the sum of a pair of numbers in a single clock cycle. A single-instruction multiple-data example is a CPU instruction to compute the list of sums of a list of pairs of numbers in a single clock cycle. Hence if you need to do the same operation on a lot of numbers, or pairs of numbers, SIMD instructions offer a speed boost. Such operations occur a lot in graphics, numerics, and machine-learning. Ultimately CPU support for SIMD instructions is a little tame (no more than four pairs of f32s) compared to GPUs, which offer SIMD instructions on millions of pairs of f32s. However moving data from main memory to GPU memory and back is costly, so for smallish jobs CPU SIMD offers a decent speed boost. 
&gt; the fact that html5ever processes input values on a unicode level, instead of byte level. In the `xmlparser` I'm using mixed parser. Partly `u8`, partly `char`. And the parser is only 50% slower that `quick-xml`.
Yes. Piston has 4 window backends: SDL2, Glfw, Glutin and Winit. All these are cross platform APIs, and in case you need another platform you can implement one yourself easily.
seems like OP has some vital misconceptions about how his code is going to work...
Trying to write a fancy calculator with termion.
I just updated one of my projects with winapi 0.3.0. The build time (for the whole project) has dropped from 50 seconds to around 30. Nice work!
&gt;One of the biggest source of performance slowdown, seems to be the fact that html5ever processes input values on a unicode level, instead of byte level. Doesn't it have to do that in order to work correctly? If it didn't decode it, it wouldn't be able to correctly parse HTML documents containing non-ASCII characters, which would be very common. &gt;That means, you have to have different assumptions than just a normal parser. E.g. document.write and all the problems that come from your input string change from under you usually aren't present in non-browser contexts. Can't say I'm familiar with that, but if guarding against that has a significant impact on performance, perhaps it should be an optional feature that could be disabled if you just want to be able to parse and modify HTML programmatically? &gt; What is htmld? Google finds absolutely nothing on that. The D HTML5-parser I'm using in my older D project. https://github.com/eBookingServices/htmld
What about Shiva then? As stated in the Wikipedia: &gt; Shiva is the "destroyer of evil and the transformer" within the Trimurti, the Hindu trinity that includes Brahma and Vishnu. In Shaivism tradition, Shiva is the Supreme being who creates, protects and transforms the universe. Shiva is often depicted with a lot of hands that may *transform* the universe and do *many things at once*.
How about scimitar, on a pun of simiter (same instruction, multiple iter).
https://github.com/SergioBenitez/Rocket/issues/507
Hello, based somewhat on this thread I made a separate thread with a title more reflective of the intent to have a general discussion regarding IDEs and editors in their relation to Rust, rather than the more specific nature of this thread. I hope that I was correct in opting to make an additional thread rather than contributing to this one, however I thought I would point the people in this thread to it in case they want to have a more general discussion of the matter. https://www.reddit.com/r/rust/comments/7kkzca/for_discussion_of_our_experiences_with_various/ 
I see. I thought you were doing something similar to associate vertex data with offsets into an arena. I've used symbol-map to make it easy to have two-way associations between arbitrary vertex data and a zero-based index, where the index provides an offset into an arena of `Vertex` structs that define actual graph connectivity. This makes it easy to have a content-addressable graph that doesn't have to store every vertex data item twice (once in a `Vertex` struct and once in an associative map). I've started writing what I hope will be a short series of articles about patterns for implementing graphical data structures in Rust. I haven't done enough of a survey of existing code yet, so thanks for taking the time to explain more of what you've done.
Motion to reserve that city for CUDApest bindings
Yeah don‚Äòt worry about it. However you significantly overestimate the complexity, in fact I think a lot of people overestimate the complexity of the futures crate, and that‚Äòs because almost all the documentation mixes in tokio too much where all of the actual complexity comes in. The futures crate is actually fairly simple, so much so in fact, that it is even no-std compatible. The actual TimeoutFuture is actually pretty much just setTimeout with a "promise.resolve();" call in it. And that‚Äòs about it. So I think it may make sense to write a little writeup about how all of this actually works, so people can understand futures a bit more and see that it is in fact about as simple as it can get.
I like this.
If I'm not mistaken, it's built on top of it. The goal is to make it easy and "rusty".
simd-licious?
&gt; Doesn't it have to do that in order to work correctly? If it didn't decode it, it wouldn't be able to correctly parse HTML documents containing non-ASCII characters, which would be very common. I don't know anything about `html5ever` internals, but the suggestion to use a byte oriented parser does not preclude Unicode support. The regex crate internals, for example, are all mostly byte oriented, but it still exposes Unicode support. It is written that way for performance.
The JS generated by embed_js and cargo-web is specific to the wasm binary generated and must accompany it, however at least in the case of embed_js I don't package them up together into a single JS file because depending on the application you may want to embed/load the JS and the wasm in different ways. The examples store the wasm inline in the HTML file in base64 encoding, but that isn't the only way to do things - you could instead fetch it from a server as a binary. Where you want to put the accompanying JS can also depend on the application (though most would probably end up sticking to a common pattern). Without calling into JavaScript, WebAssembly instances have no interaction with *anything*. At the moment, calling JavaScript functions (via the importObject) is the only way to talk to the outside world, including the DOM. Stdout goes nowhere in Rust at the moment, but hopefully in the future it can be routed to JavaScript handlers. Futures versions of WASM may add more direct ways of interacting with the DOM/other web APIS.
I'd love to never ever have to add explicit lifetimes to my code.
Amazing work of holy rabbits! Looking forward to port our gfx-backend-dx12 to the new API and finally get rid of github dependencies:)
If you want to more, I really like this handmade hero video explaining it https://youtube.com/watch?v=YnnTb0AQgYM
&gt; Doesn't it have to do that in order to work correctly? If it didn't decode it, it wouldn't be able to correctly parse HTML documents containing non-ASCII characters I might have worded that a bit oddly. Here is the original issue: https://github.com/servo/html5ever/issues/286 From what I understand, html5ever spends between 18% in `tendril::push_char` and 8% in `core::fmt::write_char`. Most likely cause is that adding stuff to `StrTendril` tends to trigger a Unicode check. Solution: process everything as they were bytes, since most boundary elements of html specification are ASCII values. 
You could try separating some of your closures into separate functions. I don't see your use of combinators as the issue, but large closures inside them.
what is it with rust projects and their names that are hard to parse in sentences?
I‚Äôm working on a Game Boy assembler to practise TDD. The process is more intricate than I thought. 
Faster mentions this regarding compatibility: &gt; Faster currently supports x86 back to the first Pentium, although AVX-512 support isn‚Äôt working in rustc yet. It builds on many architectures, although I‚Äôm not sure whether the tests pass. Are there any plans to support NEON or other targets' vector instructions?
...`simdazzle`! :P
Yes, as soon as I implement gathers and scatters and their polyfills. I also have my eyes on AArch64's and MIPS' vector extensions.
Winapi, the one and only crate you should use to talk to windows APIs. This has been in the making for a long time. Finally it's here!
I have a small changelog in the [release notes](https://github.com/AdamNiederer/faster/releases/tag/0.3.0), but I probably should condense them into a document at some point. Thanks for the suggestion!
Ah, this is helpful. It isn't exactly what I was expecting, but makes browsing the doc a lot more comfortable. Thanks!
This is it. Metals are out, in 2018 all famous crates will be named after cities. ^(Which makes sense: Rust itself is named after a town!)
I definitely like "polymer" for this reason; a polymer is a large molecule that contains many repeated subunits, just like a SIMD vector is a larger data type that contains many repeated instances of a smaller data type.
Started working on XML file analyzer (multiple, very large files, cross-file unique tag/value counts and on-demand extraction of XML snippets based on a condition). Taking a first stab at CLI version first with quick_xml, clap and assert_cli. Speed is of utmost importance here, so Rust was an obvious best choice. P.S.: Was very surprised that there are no macro / structures for test setUp() and tearDown(). Seems like a big miss from the Rust standard capabilities. 
This is a known shortcoming of the current test suite. Also missing are custom test runners, tags, and a lot of the niceties we know from full-blown test frameworks in other languages. AFAIK, work on this has recently started again.
I see. Thanks for the explanation. :) 
&gt; Could you please tell me where __js_0 is implemented? It's not implemented anywhere. (: For each JavaScript snippet from your code a unique function import is generated by the `cargo-web`. Those newly generated concrete function imports are then used to replace every `__js_X` call, so the resulting `.wasm` bytecode doesn't contain any calls to `__js_X`. All of this is done by `cargo-web` as a postprocessing step on your `.wasm` bytecode. &gt; If you could please point me in the right direction to understand the macro, thank you. Oh boy. I don't really recommend trying to understand it if you want to stay sane. (: That said, working through [The Little Book of Rust Macros](https://danielkeep.github.io/tlborm/book/) would be a great first step if you really want to understand more advanced uses of Rust macros. &gt; Also what is @prepare? I tried to google but couldn't find anything That's just my own token that I match on. The [macro book explains those here](https://danielkeep.github.io/tlborm/book/pat-internal-rules.html).
Awesome, thanks for the breakdown! I'll have to read up on macros 2.0. If that's the only thing blocking Rocket on stable, perhaps syntex (or something like it) could work.
Simditar 
How do you guarantee pi support? Without Tier 1 support it seems that any new version of Rust could break your app.
See if it's not taken on crates.io and this plans on being maintained for a long time OP should just snatch 'simd' and not worry about it.
The `simd` crate is basically the old simd crate.
Darn
maybe just `faster-simd` or `simd-faster` ? That makes it clear in the name what the crate is about, and you get to keep the `faster` thing !
Simdify Simdacian 
Got a first release candidate out for [ggez](https://crates.io/crates/ggez) 0.4.0, a lightweight game framework modeled on [L√ñVE](https://love2d.org/) for making 2D games with minimum friction . It's a big release with lots of bug fixes and new features, especially to graphics and window handling... but now all that needs to be done for the full release is cleanup and documentation. Woohoo! Maybe now I can take a break a little and spend a bit more time working on various [IPFS](https://ipfs.io/)-related things... still have to finish my hosting service project, and I want to play with the possibilities of using IPFS for a distributed public-key service...
&gt; when I sprinkle lifetime paramters around, I always wonder why the compiler can't solve it for me Lifetime parameters are there for when the compiler *can't* solve it for you. I often find that if you're using one, you must be *very* certain that you never ever want some object to outlive its owner or it will just get in your way sooner or later. If you're using more than one, 98% of the time you're doing something wrong. I find it helpful to never think "object" or "reference", always think "owned object" or "borrowed object".
[Polymer](https://www.polymer-project.org/) name collision.
username checks out
It becomes second nature, but it also takes time. I wouldn't say that the learning curve is steep, if you're just learning vim. I'd say it becomes tedious and frustrating if you're trying to both load vim up with all kinds of plugins whilst trying to learn it. Having said that, vim wouldn't be nearly as useful as it is for me today if I couldn't use all of the plugins I do use.
Agree, for something like this greatly prefer a discoverable, bland name that hints at purpose rather than something clever.
I wonder why the `Future` trait directly deals with errors, instead of just using a `Result` as item and adding a couple of convenience methods.
Lots of work on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) this last week. I have tests cleaned up (still failures for non-float types, but I'm going to punt on fixing them until then next release), replaced compile_fail dependency with the new compilefail doc tests, and started reviewing documentation and the changelog. I'm planning to work on the last two tasks this week and hopefully release a new version.
I think most people avoid XSD, since it basically is, write a compiler for your XML. You have to: A) Read the schema B) Make a parser based on such schema. C) Make it performant.
It's very unlikely that rustc will break. However, nothing prevents you to use an older version of rustc.
A question about code clarity/conciseness. What I've got: loop { // skip comments if curkw == Some(Keyword::Comment) { let tmp = it.find(|&amp;(_, l)| { Keyword::parse(l) != Some(Keyword::Comment) }); match tmp { None =&gt; return Ok(()), Some((i, l)) =&gt; { curidx = i; curline = l } }; } } It should do what I want (not yet done fully), but it feels too verbose, and I'm going to have to do similar things over and over again. Here's the setup: * `it` is `lines.iter().enumerate()` where `lines` is an iterator returning `&amp;str` * `Keyword::parse` works on `&amp;str` and returns `Option&lt;Keyword&gt;`. * So if `it` is at a point where the `&amp;str` parses to a `Some(Comment)`, I want to advance the iterator up to the first element where the `&amp;str` parses to something else. * I need to save up the index and the `&amp;str` of that element in `curidx` and `curline`. * If that's not possible (that is, `find` returns `None), I want to break the loop and return `Ok(())` from the function. The above should do it (compiles at least...), but is there a more concise, yet understandable (as in "I'll need to understand it some time in the future again") form? Thanks for any pointers!
You're going to have to be more specific about what you're doing and the errors you're getting if you want actionable advice. Which compiler are you using? x86_64-pc-windows-msvc is an *msvc* toolchain which uses the native windows tools (link.exe) and links to native windows libraries. You won't be able to cross compile to this. x86_64-pc-windows-gnu is a *mingw* toolchain which links using *mingw-gcc*. From what I gather, setting up a working linux-&gt;windows mingw toolchain is rather involved, but I can't offer any advice on that front.
Oh, I'm probably doing something horribly wrong. I live and learn : D
Hello, Thank you for your reply. Sorry I am still lost, could you point me to the code that generates __js_X ? I have played a bit with emscripen but I don't quite understand how you can call into javascript without having any .js code that is put into the module when you instantiate the wasm? Is it the macro that generates wasm bytecode? If so could you point me to the part that does this? Thank you.
I think there are a lot of good options available to people and that the _best_ option will be the one that you like the most. RLS means that it can be pretty easy to fit functionality into most decent editors/IDEs. I know a lot of people really like VS Code in this community and it fits a similar niche as Atom. If you're looking for something to compare with your favorite you might look there.
Neat. Wiling to share what kind of business you are in?
Reading through I see several mentions of the code being open source. Is there somewhere I can download and play with the code?
Not the OP, but I found the code here: https://github.com/marblestation/posidonius
I'm trying to use the [`compile_fail`](https://doc.rust-lang.org/rustdoc/documentation-tests.html#attributes) doc tests that were recently introduced while still supporting older versions of Rust. Are there any features to use in `#[cfg_attr]` to conditionally disable `compile_fail` doc tests? I would prefer to do this rather than stop running all doc tests for older versions.
Yeah, noticed that, but not really sure a name collision between a JavaScript web component library and a Rust SIMD library is all that big a deal. There are only so many good names in the world, and with the number of different language ecosystems out there, name collisions are bound to happen. If you do want to avoid collision, polymerize could work, or of course could terms for particular polymers, like nylon or cellulose. Or you could go for something more advanced sounding, like carbon fiber or graphene.
I like vim's support for Rust. I use it for syntax highlighting and that's pretty much it. I use `ctags` for go-to definition functionality.
Yes - coming from python -- pytest is the "gold standard" in unit testing as far as I am concerned. A lot of good ideas can be borrowed from there. On the side note I was questioning my knowledge of macros in Rust -- may be that is even better for setUp() and tearDown() use cases... The quick_xml crate uses them heavily for this purpose. May be I'll follow their lead...
As far as I know, it's so that you can compose and map different error types easier.
Discussion [here](https://github.com/alexcrichton/futures-rs/issues/416). TLDR: 80/20 convenience.
Don't take my word for it! Lifetimes are tricky to think about, and a lot of the standard library appears to use them in ways that seem like magic. All they are though, is an extension of variables on the stack, regular function call scope. If you have a function call scope, none of its variables outlive it. "Owned" values can be moved out of the function call (put on the heap, or returned to the caller function) and so can stay around even after the function ends. When you call another function you can let it "borrow" values so that it can read or modify them without having to copy them in and out of stack frames and overwrite the old values with the new ones returned. Where this gets tricky is when you shove a borrowed value into a data structure, because then that data structure *can not* outlive the structure it's borrowing from. That's what the borrow checker is there to prove.
Open source by any chance? I'd be interested in contributing. Especially if there is an anonymity aspect to it.
I thought it was named after the video game?
That's looks nice, but please [don't use the name trace.](https://docs.rs/log/0.4.0-rc.1/log/) The log crate is the logging facade for that has been downloaded 3.7 million times from [crates.io](https://crates.io/).
screaming-simd
More of a quickcheck related question - do you know any resources on using quickcheck against real applications - not just the standard "reverse a list" level of examples?
I thought it was named after a fungus?
A result with which Error type?
There's not enough context to understand that loop. The only exit branch is the `return Ok(())` and `curkw` isn't assigned to. So if the functions called are as pure as they should be, it optimizes to: if curkw == Some(Keyword::Comment) { loop {} } else { return Ok(()) } And because `loop {}` is currently bugged in the compiler that may be further optimized to return Ok(()) We need more context (if you have working code) or perhaps a flowchart (if you're not sure how to express what you're asking for).
Somehow, emacs was a lot easier for me to learn; It didn't feel as cumbersome to me (modal editing is a nightmare when you aren't used to it, isn't it) and some things about vi (such as how navigation works, it's annoying when pressing the left key at the start of a line doesn't wrap you back to the end of the previous one) really annoyed me. Don't get me wrong, I'm not saying emacs is better, here. It's just that I got used to it more quickly.
Are you opening the content of the file as a mutable static? oO I would call that some kind of red flag, if I read that right... Just seems awfully weird to do that rather than just opening it before starting processing--unless there's some reason all the threads can't share just one copy of it? Like, if they mutate it, or... Hell, I dunno. Obviously I haven't looked at it long enough to get what's going on.
I wonder if they've tried multi-threading this -- for example, we have an [nbody demo](https://github.com/rayon-rs/rayon/tree/master/rayon-demo/src/nbody) in rayon.
The zip library keeps some kind of pointer to the current read location in the main object. The function to get a handle to an individual file requires a mutable reference, so the only way I could have potentially multiple concurrent file handles was to have multiple archive objects. I thought about an alternate architecture of having one thread reading a zip and the other threads doing the processing, but since decomposing from the zip also takes notable cpu power I wanted to try and parallelize that too
autosimd simd-iters
Ok, sorry, I always try to minimize the code I post, but that was probably counter productive. Here's a [gist](https://gist.github.com/anonymous/fe7010a56240dffe6c33091494d3ad06) with some syntax highlighting :) Note that this does not compile, because I'm fighting with the signature of `get_foldend`, but the call site should tell the story (it will advance the iterator some more, using comparisons with the second argument, and then return a number).
Whichever you want? In that design `Future` would just have a generic `Item` associated type, which can but isn't required to be a `Result`. Then add a couple of generic helper methods for `Future&lt;Item=Result&lt;I, E&gt;&gt;` for convenience.
I tried /u/mkandalf's suggestion and it does seem faster but I'm having a weird issue where running the wasm locally (in the same browser!) is much slower than the version running at: https://wasmblock.github.io/WasmBlock/demos/life/index.html Are there some different build/optimization flags beyond `release` that I am missing or something?
I would expect any of those to be a threading library
Polyop? Although that's starting to follow in the tradition of clap...
&gt; Is it the macro that generates wasm bytecode? Rustc and LLVM does that. `cargo-web` modifies the bytecode. The macro only generates calls to `__js_X`. &gt; I am still a bit lost, how does cargo-web know what __js_X represents? From the prototype of the function. The types are exported in the wasm bytecode, and `cargo-web` reads the bytecode. &gt; it's cargo web that generate the index file as well as the required module imports? The `cargo-web` modifies the `.wasm` bytecode, extracts the JS snippets from it, and generates the corresponding `.js` file. Does this answer your question? (:
Fusion
An example from parsing/showing ASTs is a property that showing a tree then parsing it the resulting string is identity - it's a neat way to gain confidence in both parsing and showing parts. Property based testing is usually good best for testing small parts that make up a whole - testing an entire application can be tricky because the property would be so complex. You need to try and state your properties in a "forall Foos, the following holds" fashion - this is the main difficulty with property based testing. From the "real world", here are some ideas: http://www.quviq.com/checksum-property-for-autosar/ Take a look at the reverse dependencies of the quickcheck crate: https://crates.io/crates/quickcheck/reverse_dependencies There are a lot of highly used crates here that are tested with quickcheck, and those are in turn used in other crates.
Came up with "fusion" just now, not sure I'm gonna like anything better than that.
That's so weird. I guess it's an implementation detail on the archive reader doohickey, but is the underlying data (which must be in memory, right? because a stream would get messed up going back and forth, I would think) being changed? I'd think you'd need multiple views of the data and those would be per-thread (or per iteration, even), but those would be so cheap as to not really matter. Not that I'm saying that's possible with the library you're using; I haven't the foggiest. Anyway, I'm off-topic at this point. I'd do what the other guy mentioned and break some of the code out just for readability, and possibly just because I'm afraid rls would puke on my screen if I gave it something that big, but I guess it's fine. :)
The name is confusingly similar to `librsvg`, but I'm glad to see work being done in this space.
The best one is IDEA. A complete IDE will always be better than some editor with plugins.
I'm bad at naming stuff. It's mainly the `librsvg` alternative, so that's why it's called like this.
Pitching in: * simdson * simdstorm * fastfloats 
Current uses are for it is software for various IoT arm+linux devices.
Would converting the vector into a boxed slice via [Vec::into_boxed](https://doc.servo.org/alloc/raw_vec/struct.RawVec.html#method.into_box) help at all?
That's just a meme someone started speeding on Reddit
&gt; What I really need is a simple main future that uses Handle::spawn() to launch each of the other futures in their own tasks Does such a future exist?
For some reason, the authors chose to promote their own website rather than directly linking to github. Their website is mentioned on the paper, and when you go there you can find a link to github. This is pretty bad practice, if you ask me. If I were a reviewer and see this, I would ask for a revise/resubmit. Or maybe reject it outright.
VSCode appears to be the most fully-featured based on [are we ide yet](https://areweideyet.com/), but it's early days with tooling. Syntax highlighting is the only thing I've had a flawless experience with thus far.
I suggest the new choochoo. Because we all ought to jump on board the SIMD train. :-)
Random feature request: click on the canvas to kick start the game again. This could show how to handle sending data back to Rust from an event, which would be sweet and allow you to get out of the stalemate that it ends up falling into.
Cannot be done the way he wants given the design of futures. But if you find yourself in a situation where the polling overhead JoinAll is becoming a problem you can use FuturesUnordered 
thank you I guess it does, it's just too magical to me. Since I only saw explicit importings before, I didn't understand how it works. now I do, thank you.
Oh that's cool, I haven't really done any music stuff since I moved to Linux. If you don't mind me asking, do you use any software synths in linux and what's the state of them? Can you use VST or is it something else?
So I've been super busy past few weeks making a double POV globe (globe inside a globe). But now that's done and I finish work on Tuesday, I'm back on [cargo-tarpaulin](https://github.com/xd009642/tarpaulin)! Aiming to close some issues and get back to work on decision and condition coverage. Also, push the HTML reports a bit further on.
I haven't looked too closely at the higher-level crates like tokio-service and tokio-proto. It's possible that those crates provide support for such things. (My immediate needs involve some rather exotic networking that I didn't think would fit the model provided by these crates.) However, this isn't anything hard -- you could just make a main future that simply spawns the other futures from its poll(). For example: impl Future for UdpMultiServer { type Item = (); type Error = io::Error; fn poll(&amp;mut self) -&gt; Poll&lt;(), io::Error&gt; { if ! self.started { while let Some(socket) = self.sockets.pop() { // Create the future let future = UdpServer::new(socket).map_err(|_| ()); // Spawn the future so that it is // handled in a distinct task. self.handle.spawn(future); } self.started = true; } Ok(Async::NotReady) } } This is a naive example; in production you might want some provision for this future to eventually complete and the program to terminate. You can also see how the tokio-core echo.rs example program spawns a new task+future for each accepted TCP connection: https://github.com/tokio-rs/tokio-core/blob/0.1.10/examples/echo.rs#L117 I've written a number of small example programs while exploring Tokio, and I hope to find the time to clean them up and put them on GitHub.
I would contend that XSDs with proper tooling are a good reason to use XML. From XSD/WSDL you can easily generate clients, servers (including test stubs) and validate your input at runtime against the schema.
Drop the "lib", and resvg is a fine name. There's also the added confusion of "libre svg" being a possible interpretation, so again I'd recommend changing it.
It is a breaking change in a recent `nightly` unfortunately. It looks like been fixed in `pear_codegen` `0.0.12` as of a few hours ago though. Try running `cargo update` and see of you end up with the latest bits.
I briefly looked at FuturesUnordered. Since it is recommended in lieu of the deprecated UnparkEvent, I assume it can be used somehow to provide some sort of finer-grain future scheduling by only polling the relevant child futures. However, I didn't see any example of its use, and didn't see anything in the Tokio source (so far) that would map Mio events to poll()'s at anything other than the task level. I'll have to look into it some more. Now that I better understand tasks, though, I think task-level notifications are sufficient for my needs.
Awesome, when do you think this will be ready for people to start using seriously? I'm also interested in the use of this alongside existing linear algebra crates or as a potential optional backend. 
Yep, "libre svg" was mentioned in the readme. And the crate is already `resvg`. So dropping the "lib" is just a matter of renaming the repo.
The really nice explanation, big thanks for writing it down. I wonder: why one would come with a struct name like `MySetReadiness` in such serious project like tokio-core? :)
Or just using indexes and not references to Vec content ?
I mostly use Yoshimi for my software synth, which is a fork of ZynAddSubFx. It's fairly robust and featureful. It has a pretty nice default instrument bank, too, but many of the instruments will need additional processing to make them workable. A lot of them tend to have a lot of high-pitched sounds which grate my ears and not blend in with many other instruments. I've used the moog emulator Bristol a few times, but it crashes quite a bit. I'd want to track down the source of its instability eventually. On the other hand, I might just break down and get a moog (or other synthesizer). Those are very pricey, though. I don't really use VST as far as I know (they might be put into some of the packages I use), but calfjackhost (Arch package calf) is what I tend to use for effects beyond Yoshimi's simple effects. VST isn't unattainable, but it's not something I have explored much in the past. The sdk is available [here](https://github.com/steinbergmedia/vst3sdk). I've documented a lot of my audio setup and problems in [this repository](https://github.com/kandiyohi/audio_linux_documentation).
&gt; An iterator has a method, `next`, which when called, returns an `Option&lt;Item&gt;`. `next` will return `Some(Item)` as long as there are elements, and once they've all been exhausted, will return `None` to indicate that iteration is finished. &gt; &gt; - [std::iter](https://doc.rust-lang.org/std/iter/index.html) The fact that an iterator may or may not return more `Some` after the first `None` is orthogonal to the definition of the `Iterator` trait. But by the protocol, you're supposed to respect the first `None` as the end of the stream; `next` calls after a `None` are undefined by the `Iterator` trait on its own.
That sounds like a good idea. Please report back how it went; it may even inform a future standard implementation.
Ohh thank you, I hadn't heard of ZynAddSubFx before. Making my own software synth in rust is one of my many future/dream side projects so that's a good starting point in background research!
IDK its a little too cheesey for me
Good post. Some clarifications: &gt; It is important to understand that notifications happen at the task level and not the future level. When a task is notified, it will poll its top-level future, which may result in any or all of the child futures (if present) being polled. For example, if a task's top-level future is a join_all() of ten other futures, and one of these futures arranges for the task to be notified, all ten futures will be polled whether they need it or not. This is true for most cases, but there is a strategy to know which sub futures need to get polled (to avoid the sequential scan of polling). `FuturesUnordered` uses this strategy. The downside is that it requires some allocating, so it only makes sense to use if the allocation cost outweighs the sequential scan cost. &gt; Mio handles this non-system event by adding the event details to its readiness queue, and writing a single 0x01 byte to the readiness pipe. In theory, the actual writing of the byte to the pipe should only happen if the event loop is blocked in `epoll_wait`. If the pipe is written to when this is not the case, this is a bug (which could be the case). &gt; This two-iteration approach involving a pipe write and read may add a little overhead when compared to other asynchronous I/O event loops. In a single-threaded program, it is weird to look at the strace and see a thread use a pipe to communicate with itself: Again, writing to the pipe shouldn't happen if the event loop isn't blocked in epoll_wait, so this might be a bug. Also, the next iteration of Tokio is not going to be using `mio::Registration` for scheduling tasks, so that will remove a bit of indirection. To answer the questions: &gt; Does Tokio deal with the starvation problem of edge triggering? No, it is up to the user to ensure that future implementations yield control back to the executor. Yielding can be done by self notifying: `task::current().notify()` then returning `Ok(NotReady)`. &gt; Does Tokio support any way of running the event loop itself on multiple threads, instead of relying on finding opportunities to offload work to worker threads to maximize use of processor cores? You can move sockets to a thread pool executor, then the reactor will notify the thread pool tasks. This is the basic story for multi threading.
`FuturesUnordered` runs something like sub tasks internally. So before polling each sub future, it swaps the notify handle in such a way that it can intercept the notification. With that extra info, it can flag each individual sub future that needs to be polled. Granted, docs / examples would be nice. Hopefully you could submit a PR with one :)
&gt; In this case, you need to explicitly specify the type of an argument. &gt; i think i tried that and it doesn't fix it: I think it works in a polymorphic context where you can say ```for&lt;'a&gt;|x:&amp;'a|``` .. the effect of the for&lt;'a&gt; is much like the 'temporary' idea,("it has to work for any lifetime, therefore it could be the shortest") ... but I think this is *such* a common case it demands a better syntax: the problem with this is you had to create a *name* ('a), and look to one side to see what it does. The behaviour of one thing (the pointer) is defined in two locations. it's making it harder to read. It all adds up.. rust is very noisy. This simple case should be easy, then you add noise for more complex cases
&gt; When you say "The inputs must be assumable to be temporaries", you are not referring to Lifetime elision? I dont think that covers this( as in current functionality) .. part of the problem is the lambda can't be polymorphic, i.e. it's instantiated then gets confused that it could be the lifetime of either temporary *in their specific calling contexts*. Whats wanted is a flag , part of the reference itself, that means it processes differently, ie. it's not a *specific* lifetime, rather *the temporary lifetime in any given context*.
&gt; 1) this is how references work right now, by default. it isn't because it doesn't work. I had to give up and make a seperate named function to get this working.. more noise, more nodes of complexity
I think you'll need to create a cargo feature (like "enable-compile-fail") and then `#[cfg(feature = "enable-compile-fail")]`. I assume that features still work inside doc tests...
Not really, it forces you to write something. Then it gets harder to find places where actual, useful, documentation is missing. I think it's good to include _after_ most of the work is already done.
That's not confusion, that's a great pun :D
Ah, I looked at the releases tab but didn't click on the actual tag. Thanks!
&gt; no pure-Rust 2D graphics libraries How about a [lyon](https://github.com/nical/lyon) backend?
simdific!
&gt; simd * simdart - Simulation based division army trainer * simden * simdev - 
&gt; a struct name like `MySetReadiness` in such serious project Like `MySQL` for such a serious DB :-) When I was in uni, my impression was MySQL is someone's toy project.
What would help if there were resources about writing XML Schema validators. But I don't really know any. Something like http://aosabook.org/en/index.html for XML validation.
It's not a 2d rendering library. At least by now.
Yes, that's a lot clearer. If you need to lend the iterator into a subroutine, so that `.next` will be called from multiple sites, that's a good sign that `for` loops and functional combinators will be of limited use. Instead `while let` allows you to monkey around with the iterator. Then it's pretty straightforward to write a loop which conditionally dispatches into the subroutine. [I suspect this is close to what you mean](https://gist.github.com/anonymous/5152b2ecd6fcbfb6ef4853aafbd1056f). It's not exactly the same how it handles comments.
Finished the first 3 graph implementations for my small data visualization crate: [Gust](https://github.com/saresend/Gust)
With the ongoing Tokio reform, what should one do to write a server today? Is Tokio still the way to go? Or will a lot of code need to be rewritten after the ongoing changes? Is async/await something we can use now?
Yeah, but you can use it to build a GL backend.
It seems to work in your example at least: https://play.rust-lang.org/?gist=9807ed947dfbac65bfefb57a0fba3ce3&amp;version=stable
Incredible work! Well done. &gt; This can be fixed by enabling the std feature, which causes winapi::ctypes::c_void to instead alias to std::os::raw::c_void. Is there any reason that `c_void` has to live in libstd rather than libcore? Seems like an oversight to me.
I love articles like these. Thanks for writing it. Only got a chance to skim it now but I plan to revisit it in more detail this evening.
Looks like this talk is missing it's beginning, so I'm sorta lost.
Ooh, I wonder if this gets antialiasing right! &gt; libresvg uses bindings to cairo \**shuffles off, grumbling*\* One day, *one day...*
In this release we've added proxy support and non-destructive editing {add, upgrade, rm}.
There's a lot of debate over how to handle `c_void` such as https://internals.rust-lang.org/t/solve-std-os-raw-c-void/3268. Unfortunately there's not much I can do about, so I just do the best with what I have.
I am not sure that you're supposed to initialize the Uint8Array on the complete buffer. After checking how they're doing it in this guide (https://www.hellorust.com/demos/canvas/index.html) they expose the start and length when initializing the arrays. See this diff on how I got it to work: https://gist.github.com/jontro/b3e247bcc2d92a081191bc4cdf56e0b0
I've not seen anything as good as my Java IDEs are yet, but it has been a while since I last tried. Intellij - I tried this, but at the time it was somewhat incomplete. A part of my also doesn't like the fact that they did their own code completion stuff rather than working on and with RLS. Visual Code - I started here. Is ok. But ultimately the plugins and setup for it are really not user friendly. Once you are setup things are OK, but thing can be really annoying to get started. Further, the experience I had last I tried it was one of frequent crashes of the various parts of the IDE, ultimately requiring my to restart the editor to get things like code complete to come back. Vim - Used it, but never turned on much more than syntax highlighting so which is really the low bar. One other annoyance, at least with VS code, there wasn't always good "Hey, this here is broken". You would often have to start a build first before you could see the red squiggles. Ultimately, you end up needing a lot of compiler prompts to get going. Though, to be fair, rust has really good compiler prompts.
Just ran into this as well.
rust sucks
I hope this one isn't removed by the mods. This is a much needed critique
/r/golang
You are looking for [/r/playrust](https://www.reddit.com/r/playrust)
I'm excited to see a game framework using nalgebra's types. It is absolutely my linear algebra library of choice, and martialing between nalgebra types and cgmath types gets old quickly. I'll probably try switching over my current game project to ggez! 
As someone who spoke in this Discord chat, this was definitely intended for the language.
lol i knew you were russian cos of the way you leave out articles like ‚Äòthe‚Äô and ‚Äòa‚Äô, lol
As the person who posted this, this was definitely intended for the language
Was there a lifting of the no-memes rule? :P
Upvoted because, even if you didn't mean it ironically, you still made me lol.
This is in violation of rule 2 of the sub, as well as just generally being a shitpost. Why exactly did you feel like posting this entirely out of context screenshot? Is there some joke we're missing?
kind of funny that rust‚Äôs ‚Äúofficial‚Äù (async) networking library is so complicated hardly anyone can use it....aren‚Äôt rust developers concerned that rust will just become another ivory tower academic language like haskell that people admire, but ordinary programmers don‚Äôt use in practice?
/r/rustjerk
There are no *complete* pure-Rust 2D libraries, but [footile](https://docs.rs/footile/0.0.10/footile/) has a lot of what's needed for SVG rendering. It just needs a bit more work to fill out some missing features.
Should be fixed now: [https://github.com/SergioBenitez/Rocket/issues/507](https://github.com/SergioBenitez/Rocket/issues/507)
&gt; It seems to work in your example at least: thanks.. I'm sure I'd tried that to no avail, let me go back to my original context aswell and try again. I thought it needed the for&lt;&gt; stuff and to be a polymorphic setting.. That's a big step forward. &gt;if your frequent use of this pattern is based on the way you would structure things in C++, it might be worth examining whether that pattern is worth keeping IMO it's just 'a very common case' - as such it should be as easy as possible to write; whether it's checked or not you still end up with the same end result (debugging in c++ or fighting the compiler.. in both cases *until it works*). You know that if you use this pattern (a) less debugging in C++, (b) less fighting the borrow checker in rust.
Do you know of a library that can generate/modify an input SVG? I basically need to change a color and add an SVG path to a given SVG file. I'm considering hardcoding everything, but there *has* to be a better way.
Won't work, I tried. For the algorithm to work, I have to mutate these self-referential pointers to point to elements of the same type, but in other data structures (e.g not to point to the previous element in a `Vec&lt;T&gt;`, but to temporarily point to an item in a `BTreeSet&lt;T&gt;`). If I use indices, I still can't get around the problem that I can't mutate the elements on their own. I [have to use Cell or UnsafeCell](https://github.com/fschutt/polyclip/blob/master/src/sweep_event.rs#L10) to tell Rust that I'm not mutating the elements in the vector (even though I am), so that Rust sees the vector as immutable. The problem are not the self-referential pointers / indices, the problem is the mutability of the vector. I could use indices, but that would complicate things unnecessarily - now instead of needing just the element itself (and it's containing pointer), I need to have the index + a reference to the container the index is in. This complicates things because a pointer can point into any data structure, but with an index I also need pass the containing Vec or BTreeSet around and keep track of which type of container it is. I tried indices, but they needlessly complicate things.
Still porting from Python/Kivy to Rust/Piston. Got my game map rendering but managed to make RLS blow up in the process
I use svgparser, try it. It's strong typed and clean
From what I can tell lots of work is being put into clarifying and simplifying tokio. From my experience in other languages (Java/Netty and python/tornado/asyncio/twisted) the low levels of asyncio code are always complex, it's a complex domain. Rust is trying to make something extremely fast that is also ergonomic. It's going to take time, and it's going to take more articles like this. Everyone involved cares, and is trying to make it better.
I'm doing [that](https://github.com/iliekturtles/uom/blob/04ef120f0ef6587bef87b8c779ff5bcaeec7de04/src/lib.rs#L34) right now for other tests and was just hoping someone could point me towards some pre-existing `cfg` items I could use without adding a new feature.
There's also [pathfinder](https://github.com/pcwalton/pathfinder), thought I think that's more of a work-in-progress/not intended for external consumption library than lyon.
[svgparser](https://crates.io/crates/svgparser) is used by libresvg (via [svgdom](https://crates.io/crates/svgdom)) and written by the same author, /u/razrfalcon.
Yes, thank you! It doesn't solve the problem that I need inner mutability, but it can at least guarantee that I don't modify the vector.
Nah, the `for&lt;'a&gt;` stuff only really comes up in certain contexts and is more of an edge case than the base rule. The default for functions that have an input lifetime but no output lifetime is to just borrow it for as short a time as possible, which seems to be what you're after. Like I said I think the *actual* problem is/was one of type inference failure rather than a limit of lifetime elision, since `fn takes_a_reference&lt;'a&gt;(ref: &amp;'a u32)` can be trivially elided to `fn takes_a_reference(ref: &amp;u32)`, and the `|ref: &amp;_| {...}` closure in the example is basically the same thing as that.
Non-destructive editing is huge!
You construct a `Uint8Array` at the start, and it will only be able to access as much memory as is associated with the WASM module at the time it was accessed. As soon as Rust/WASM allocates more than that memory, the Uint8Array will point to unused space - it will be garbage old data. I'd recommend doing a length check using byteLength on the Uint8Array every access and reconstructing it from the exports.memory whenever it doesn't match.
Nah, the for&lt;'a&gt; stuff only really comes up in certain contexts and is more of an edge case than the base rule. The default for functions that have an input lifetime but no output lifetime is to just borrow it for as short a time as possible, which seems to be what you're after. Like I said I think the actual problem is/was one of type inference failure rather than a limit of lifetime elision, since `fn takes_a_reference&lt;'a&gt;(r: &amp;'a [&amp;'a u32; 2])`can be trivially elided to `fn takes_a_reference(r: &amp;[&amp;u32; 2])`, and the |ref: &amp;_| {...} closure in the example is basically the same thing as that.
This is the most thorough and complete critique of the language I have seen yet. I'd let it stand.
There's actually surprisingly little in the game framework itself that uses nalgebra, and it's very loosely coupled... About all it needs is points and matrices, and most of those are focused in a few parts of the drawing code that don't interact with much else. Strange but true. It started off with euclid, and we played cgmath, then settled on nalgebra 'cause it was everyone's second choice. :-P But actually switching from one to another was not a large job at all.
(right what i'd been told another time was the closure isn't polymorphic, 'so the lifetime would have to map onto either one'.. but perhaps the lifetime is indeed being reasoned about more dynamically even for a non-polymorphic function call. I do remember being told it was all about type inference in my other example (which was writing high-order functions taking a lambda.. ```zip(... f:Fn())``` helpers to do that sort of thing in 1 step without needing to open/collect iterators )
Works like a charm!
could you explain what the problems are with cairo's antialiasing? and why have the cairo devs never fixed them?
Looked at another way, XSD **shouldn't** be consumed at runtime, rather it should be consumed before or at compile time to generate a parser/client. Optionally XSD can be used at runtime to validate a schema prior to ingestion, but a properly designed parser should already accomplish that as a side effect of parsing a document. So really we should be looking at a tool to ingest XSD and generate parsing code preferably built on top of an existing parser library. For bonus points something that has an optional metaprogramming component to do the generation during compilation would be nice.
[For reference](https://i.imgur.com/hJHVnTH.png). That image is comprised of three shapes. Bottom to top: a red rectangle, a green circle, and a red circle. The two circles are *exactly* the same shape, size, and position. Now, if cairo did AA correctly, the image would just be solid red. However, because it antialiases and blends shapes one-at-a-time, you can see a visible seam. This is because cairo (and just about *everything* else, too) handles partially covered pixels by multiplying the alpha of edge pixels by their coverage. That produces seams because as soon as the renderer does that, it no longer knows *which part* of the pixel a shape covers. To get it correct, the renderer needs to do something more like what GPUs do for MSAA: it needs to track the actual pixel samples across blend operations. Which is *also* why no one does it: it would cause memory and compute to explode. To get equivalent results on non-edge cases, you'd need 256 samples per pixel. The only thing I'm aware of that does it "correctly" is the nVidia path rendering extensions for OpenGL, since those actually *do* use MSAA for anti-aliasing... but I'm not aware of any software that actually *uses* those.
Well, it's a tradeoff. You either choose multisampling/supersampling, which reduces your antialiasing quality, or you choose analytic AA, which increases your antialiasing quality at the cost of introducing conflation artefacts. It's hard for me to say one approach is more "right" or "wrong" than the other.
Thank you so much for so detailed reply. I'm trying to learn more about architecture as software engineer and lost in sea of information.
... no, the one that introduces seams that don't exist is *definitely* the one that's wrong. That reference image is *correct* if I disable AA... which I can't do in Inkscape when exporting a bitmap. \**grumble, grumble*\* I completely understand the performance argument, mind. Seams are only really an issue in (literal) edge cases. For the majority of cases, conventional AA is just fine. But the hoops I have to jump through to do lighting in vector graphics *specifically* to work around incorrect AA is just maddening. That there isn't any sort of "do it properly" switch I can throw drives me nuts. Like I said, I'd be perfectly fine waiting on a render if it meant I could just stack shapes and have them render correctly.
&gt; Rust is trying to make something extremely fast that is also ergonomic Shoving everything into closures is far from ergonomic. 
&gt; rust will just become another ivory tower academic language Areas where Tokio is important is an area where Rust is a very questionable language to choose because you can get the work done much easier and faster in almost any other language and Rust execution speed is not that important.
Ingesting XSD at runtime opens you up to a STAGGERING number of attacks. DDOS, Local FileRead, XSS, remote file execution. Java has only _finally_ finished digging itself out of this hole. 
&gt; Shoving everything into closures is far from ergonomic. I'm not sure what this is referencing, but I would like to point out that Tokio does not require everything to be shoved in a closure. In fact, the vast majority of the Tokio code I write (well over 90%) does not use closures.
Seems like an odd, vague, and unsubstantiated statement to make. I'm not sure how to respond to it. We are using Tokio to build a proxy. This seems like a good use case for Rust as a programming language as well.
This is great. And unlike last time I don't know of prior art :) I've already been alerted to [lazy-static's upcoming changes](https://github.com/rust-lang-nursery/lazy-static.rs/issues/73) by this tool!
It is when you combine it with ggez's [Canvas](https://docs.rs/ggez/0.4.0-rc.1/ggez/graphics/type.Canvas.html)... ;-) Nah I just came here to make the same comment.
This may be a reasonable use of unsafe pointers, if you can wrap them up nicely and are sure the vec does not get resized.
I'm happy for you, now write some useful post or GTFO
Although not an IDE, I"ve had very good experiences with Geany - can navigate in comfort with ctrl-T, and alt-left gets you back. The code completion is more suggestive than correct, but useful. And you get this out of the box without plugins.
Good luck with polyclip! When I last needed to do polygon clipping, I despaired at how hard it is to do correctly, and ended up just settling for an FFI binding to GPC: https://github.com/ucarion/gpc Best of luck to you! Rust is still in need of good libraries for these computational geometry problems.
What I do find interesting is that people can be apparently using the same setup (e.g. VSCode with RLS+Racer) and have such different experiences. Partly due to user expectations, but it suggests that these environments are still a little hard to configure and get right.
Posts like this make me feel a little less stupid. I recently started learning Rust, and the learning curve seems very steep. I come from a Scala background, which some people say is a difficult language to learn. You mentioned Haskell, which I also find relatively straightforward (I might be helped by the fact that I had some background in mathematical logic and category theory). But Rust is a totally different beast for me. 
There was a post a month ago about using the VS linker with Rust on Linux + Wine (https://www.reddit.com/r/rust/comments/7a2weh/obtaining_and_using_microsofts_linkexe_on/) I can't say if it'll solve your issues though.
Thanks, I'll post it on this sub when I'm done. Could take a while to do the debugging, though.
When the `grow_memory` wasm instruction is called a new buffer is effectively created in `results.instance.exports.memory.buffer`, and the old one is truncated to zero bytes. You might want to check out [cargo-web](https://github.com/koute/cargo-web) which handles this issue for you. It patches the wasm bytecode and for every `grow_memory` it injects a call to an [imported `__web_on_grow` function](https://github.com/koute/cargo-web/blob/master/src/wasm_runtime.js#L32) where it recreates the typed arrays.
Apart from what has already been said about static and more functions. I believe this style .filter_map(|name_opt| { name_opt.and_then(|name| { captures.name(name).map(|value| { (name.to_owned(), value.as_str().to_owned()) }) }) }) Is just trying to be fancy. Can't really see the types from here but .filter(|id| id) .map(|v| { }) would reduce indent by another level. ---- Just above that code .filter_map( .... ).flat_map( .... ) This is usually wrong and means you are returning Option&lt;Iterator&gt;. From an API point of view, 99% of the time an Option&lt;Vec&gt; is bad design. The same information is passed with an empty vec. 
Thanks so much to /u/aordian and all the other fine folks who‚Äîunlike me‚Äîcontributed to cargo-edit recently and made this release possible! This is a pretty huge next step, and I'm really looking forward to seeing people try this beta out and give us feedback on how well the format preserving editing works. There are a couple of crates trying to do that, and I have high hopes that even this initial version will work well enough for production use. And every bug report we get will only make this better :)
Out of interest why do Russians leave out articles? Do they not exist in Russian grammar? 
If you were in this discord chat, a little context would be appreciated. All I can see is spam and it breaks all the six rules of the chan at the same time.
If you were in this discord chat, a little context would be appreciated. All I can see is spam and it breaks all the six rules of the chan at the same time.
Boarding the wrong vessel without permission in the Navy is grounds for dishonorable discharge. Oops...wrong sub...
I have no idea
you know what i‚Äôm referring to though right? you said ‚Äúas user‚Äù instead of ‚Äúas a user‚Äù and you said ‚Äúi don‚Äôt see convenient way‚Äù instead of ‚Äúi don‚Äôt see a convenient way‚Äù....
&gt; but all of them are in various stages of incompletion quick-xml author here. What do you mean by that exactly apart the fact that the 1.0 version has not been published. The main change I can see in the future is `no_std`support. I do not consider the current crate as incomplete. Any DOM/XSD/DTD should IMHO be done on top of quick-xml (like minidom) and not necessarily be integrated.
Out of curiosity, why not using quick-xml directly? I really like having lot of choice but I am just curious.
The answer is in the readme: &gt;At the time of writing the only option was quick-xml (v0.10), which does not support DTD and token positions. I need a full control, which I can get only in my own parser. Also, I need a position of all tokens and good DTD support.
This library doesn't render anything by itself. It's just a proxy between the SVG file and 2D rendering library.
Thanks for the tips!
There is a similar "bug" when you stack identical shapes. Then borders became bolder and bolder. All software I know do this.
It doesn't need to be software only.
The main question is "does it support complex text rendering"? And there answer is no.
[svgdom](https://github.com/RazrFalcon/libsvgdom)?
Thanks. Do you think the DTD part could be extracted out of your crate so it could be reused by other xml libraries?
Same here without ctags (never tried I use docs.rs extensively) and with rustfmt.
&gt; Also, the next iteration of Tokio is not going to be using mio::Registration for scheduling tasks, so that will remove a bit of indirection. Could you explain what else tokio is doing instead? 
welcome back ;) I will not comment on the parser's readability (because I'm obviously biased here). There's something in nom 4 that might interest you though: the parser's input type has to implement `nom::AtEof` which has `at_eof` method to indicate if we can get more data (by refilling buffers from network or file). A lot of issues people had parsing languages and small text formats with nom came from the parser returning `Incomplete` because it did not know if more data could come. With that new feature, most of the issues around `Incomplete` are solved. There are two new input types provided by nom, `CompleteStr` and `CompleteByteSlice`, for the case where all the file has been loaded into memory. I'll see if we can include that kind of behaviour in nom_locate as well
Like `dtdparser`? It's an interesting idea, but it's heavily tied to `xmlparser` internals. And currently, it has complete support only for DTD header and ENTITY. Also, it's strictly utf8, so it's not be useful for your crate.
Thanks! So it was a silly oversight on my part. Pretty obvious in hindsight but that didn't occur to me. I was looking at the hellorust example, but I didn't want to allocate anything from the javascript, just read a block of memory that the Rust code populated. And I put the array construction in too early, yeah.
Yep, that was it, thanks! Moving the Uint8Array creation to the `draw` function solves the issue and printing out `byteLength` before and after the call to `start` does show different values. Cheers!
Thanks! I've been meaning to try cargo-web and stdweb later, but I wanted to understand things first. The buffer recreation bit is what I missed (although it makes sense).
I know what is article, but I never bother myself to write article in a very required instance. Now post something on subject for a change.
Thanks! Btw, does poloniex-rs place market or limit orders? Market makers (limit orders) get cheaper fees. The [gekko bot](https://gekko.wizb.it/docs/introduction/scope.html#Execution-strategy) places a limit order but modifies it continuously to always be the first order in the order book to get filled. Is this possible to do on top of `poloniex-rs`? Btw, we have a small chatroom of Rustaceans who are into crypto currency trading, I invite you to [join](tg://join?invite=BfEhnxLTATDqZtUrhQ2cHg) :)
This is a fair question but a bad title. ‚ÄúA surprising gap‚Äù sounds like you‚Äôre trying to shame someone into doing work for you. Meanwhile, you naturally ‚Äúdon't have any more space for more responsibilities‚Äù yourself. (Though on further reading you don‚Äôt even ask a question, only ‚Äúwanted to call attention to this‚Äù.)
Very excited about render target and shader support especially. 
The plan is for Pathfinder to become a component of WebRender.
I use Acme with a bunch of watchers building and testing the application. Then I right click on the error or warning messages and the editor jumps directly to the specific file and line. If I had the time I'd write something like [agoc](https://github.com/s-urbaniak/agoc) for racer, but I don't miss autocompletion that much to be honest.
Huh, "full vector scenes" ‚Äî it's not 100% focused on fonts now?! Wow nice.
A little known fact is that you can `.collect()` an iterator over `Result&lt;T, E&gt;` into a `Result&lt;Vec&lt;T&gt;, E&gt;` - this makes it a bit easier to work with iterators and fallible operations!
I forget this _all the time_ and look for something in `itertools` to help me.
I have no idea why someone would want to ingest XSD at runtime. I was only thinking about compile-time code generation. XML itself has a number of security issues (entities, etc.) but a properly designed library should keep these features off by default.
I once looked into the default XSD code for Java. I dont know if I'm too dumb, if it's overengineered or if the spec is just crazy, but I couldn't make much sense of it.
I'm not worried about emacs vs vim, it is reasonable that different people prefer different styles of editing. If so, as far as I'm aware, emacs has some nice rust modes as well - not that I know too much about emacs and it's plugin/mode ecosystem. https://github.com/racer-rust/emacs-racer https://github.com/rust-lang/rust-mode Of course, it will only be as good as racer is.
Yes, we (Servo) want to eventually use it for SVG and Canvas 2D too.
colortrace! ?
The code could use some Clippy and some other micro-improvements... As example this file: https://github.com/marblestation/posidonius/blob/master/src/tools.rs Contains the function: pub fn calculate_spin(angular_frequency: f64, inclination: f64, obliquity: f64) -&gt; Axes { let mut spin = Axes{x:0., y:0., z:0. }; // Spin taking into consideration the inclination: spin.x = 0.; spin.y = -angular_frequency * (obliquity+inclination).sin(); spin.z = angular_frequency * (obliquity+inclination).cos(); spin } Slightly improved: /// Spin taking into consideration the inclination. pub fn calculate_spin2(angular_frequency: f64, inclination: f64, obliquity: f64) -&gt; Axes { let (s, c) = (obliquity + inclination).sin_cos(); Axes { x: 0.0, y: -angular_frequency * s, z: angular_frequency * c, } }
It's worth knowing that you can also use `cargo-web` itself without `stdweb` if you don't want/need `stdweb`'s complexity. That is an use case which I do want to support too (although it's currently mostly undocumented). Some of the benefits you get when compiling with `cargo-web` are: 1. You can use `main()` normally instead of having to define your own entry point (`cargo-web` renames your `main()` and calls it for you after the wasm is instantiated). 2. You can put JS snippets in your code by importing [`__js_XXX`](https://github.com/koute/stdweb/blob/master/src/webcore/ffi/wasm.rs#L11) functions and calling them manually. 3. You can use the `HEAP*` variables in your JavaScript snippets to access Rust's memory. 4. You can add any public APIs you want to expose outside of your wasm module to `Module.exports`. 5. From your JS snippets you can access `Module.instance` where you'll have your webassembly module instance, and you can basically do whatever you want with it. 6. You can easily use the resulting `.js` in both the browser and Node.js
Oh that's really cool! Thanks!
`collect`ing to a `Result&lt;Vec&lt;T&gt;, E&gt;` works if you're just doing a filter/map and want to short-circuit, but folds are awkward. itertools has `fold_results` and [`process_results`](https://docs.rs/itertools/0.7.4/itertools/fn.process_results.html) which can help.
Oh, sorry, I thought that's what your complaint about lyon was. Sorry if I misinterpreted.
Very cool Tom! I'd love to play around with doing Mandelbrot's myself. Could you point to some resources for beginners to get started? I know how it's defined, I just wouldn't know to compute the set!
For concrete examples of this, see [Rust By Example](https://rustbyexample.com/error/iter_result.html)
I work with iterators that yield results all the time. Sometimes the `collect` trick mentioned by others helps, but more often, I just write loops: for result in iter_that_yields_results { let the_thing = result?; do_something_with(the_thing); }
You should write it, it'll make your english much easier to understand. Without articles your english sounds very strange and it's sometimes difficult to see what you're trying to say.
Working on generic trie implementation [gtrie](https://crates.io/crates/gtrie). Latest changes significantly improved the performance, so CPU caches are very important.
After designing the primitives, I actually started writing my MSc thesis... in Rust (and the text in LaTeX, obviously)! Can't really tell what I am doing at this point, but I can tell: - It's in Rust! - I'm using a bunch of cryptographic code from Monero; calling C from Rust is easy, and so is compiling glue code with the `gcc` crate! - It's been a lot of fun so far - I'm hoping to be at [the IEEE European Symposium on Security and Privacy](https://www.ieee-security.org/TC/EuroSP2018/events.php) - My thesis should be out by May/June
The IntelliJ plugin probably decided to use its own code rather than the RLS so that they can spinoff the plugin in a full commercial IDE in the toolbox down the road 
If I understand correctly, this is not using [Molten](https://github.com/LeopoldArkham/Molten), but [toml_edit](https://github.com/ordian/toml_edit) for non-destructive toml editing? And you want to migrate to Molten when it is ready? Is this right? Can you elaborate a bit on the status of this? What are the limitations of toml_edit? Why is Molten expected to be better?
Right. The limitations of toml_edit are listed in its [README.md](https://github.com/ordian/toml_edit#limitations), I believe there is a trade-off between enforcing this limitations and having a nice IndexMut-based editing API. Molten does not have this limitations, but currently lacks API for editing subtables. 
Rather than using ad-hoc macro like this, I'd just [configure a logging backend](https://github.com/Xion/cargo-contribute/blob/ecfc333b4b949e6eb1c17bac87d6978700e5ad53/src/logging.rs) and use regular `log` macros. The details of actual coloring can be handled by a library like `ansi_term`.
These talks continue to get better with each iteration. Too bad the video didn‚Äôt have any Q&amp;A at the end. 
&gt; That reference image is correct if I disable AA Well, I mean, what is "correct"? There's a reasonable argument to be made that aliasing artefacts are just as incorrect. After all, our eyes don't perceive aliasing because of the way light is sampled. So my view is that Anyway, I do agree with you that it should be an option. In fact, Pathfinder can do 16xSSAA (or 16xMSAA if sample level shading is supported by your hardware) or analytic AA as you choose.
Vim has oddly been the best experience for me, more so than IDEA and VSCode. Using [w0rp/ale](https://github.com/w0rp/ale) for linting and [roxma/nvim-completion-manager](https://github.com/roxma/nvim-completion-manager) for completion features. When rls is working I can get real-time error checking. Without rls I can just use cargo, but I need to save often.
Congrats and huzzah! Actually, looking at this is enough to make me actually want to learn winapi, if I don't need C/C++ for it anymore...
Any ICE is a bug.
First, the warning is correct, since you imported `std::env::*` you can use `home_dir` without any qualification. However, the compiler panic is also a bug. Please rerun after setting the environment variable RUST_BACKTRACE to 1 and submit a bug report. You can also check nightly to see if the panic is already fixed.
Testing it on the playground, it errors (correctly) on stable, and ICEs on beta/nightly.
Thanks transformed it to a function `pub fn get_application_home () -&gt; std::string::String`
Then it's a regression.
Thanks for the informative comments! I've made a note on the blog post about these. Regarding the pipe, I assume you mean that the pipe write is really intended for multi-threaded cases, since in a single thread the program obviously can't be working on a notification and be blocked on `epoll_wait()` at the same time. I'm definitely seeing the pipe write in the single-thread scenario. (These are debug builds, for whatever that's worth.) However, it sounds like a very minor issue given that Tokio is moving to a different scheme for task scheduling. :)
Thanks, that approach makes a lot of sense! Yes, I should write some example code for this. (Along with cleaning up all these other example programs I've written along the way.) But I probably won't get a chance until after the holidays.
Reported [here](https://github.com/rust-lang/rust/issues/46843).
I've simplified the test case a bit and reported it as a bug [here](https://github.com/rust-lang/rust/issues/46843).
Great to see someone is building a CAN crate :) I can see your qualms and ask myself some of your questions too. To answer these questions I started a proof of concept for modbus (that is also pretty pretty synchronous). At the moment it does not feel like an overkill :) Have a look at it: https://github.com/slowtec/tokio-modbus
Just so you know, `String` is included in the standard prelude, so you only have to type `String`.
I think it's less an issue of it being more complicated than async I/O work in other languages, and more about it being unfamiliar to those of us who have spent most of our career writing imperative code. In recent years, it seems like many imperative languages are cherry-picking features from functional languages (e.g. futures) where it makes sense to do so. These features may also need extensive adaptation to align with the goals of Rust (e.g. making futures readiness-based instead of completion-based), which perhaps could require additional learning even for people familiar with these features in other languages. It's hard to change the world of programming without introducing some new learning curves along the way. But there's still plenty of opportunity for simplifying (which is happening) and finding better ways to communicate these ideas.
Well, a pipe will always be part of the equation as there needs to be a strategy to wake up a `epoll_wait` call. That said, yes, your assumption that the pipe is for the multi-threaded cases is correct. If you are seeing the pipe write happening when there is only one thread involved, that is a bug. If you have a simple repro, I'd appreciate an issue filed on the Mio repo and I can take a look.
The task executor is being decoupled from the reactor. This will enable using a Tokio reactor with any task executor on a single thread. The two relevant PRs are in futures-rs: * [Add a CurrentThread executor](https://github.com/alexcrichton/futures-rs/pull/639) * [Add a Sleep abstraction](https://github.com/alexcrichton/futures-rs/pull/665) At some point, the tokio reactor will implement `Sleep`.
The ongoing Tokio reform is mostly about top level API polish. There are no fundamental changes. So, if you use Tokio as it is today, it shouldn't be be hard to migrate to the next release.
You don't need tokio. There's all kinds of different executors you can use. The most basic synchronous execution would just be Future::wait() and then oneshot channels to use as "promises".
Interesting. Althought, I might have misphrased my question. I was actually asking about what tokio is using instead of mio::Registration, since apparently its not being used anymore.
&gt; Seems like an odd, vague, and unsubstantiated statement to make. It's /u/frequentlywrong, who's on the record for not liking Tokio. I'll try to paraphrase/summarize as follows: "Erlang is great for massively concurrent servers! Trying to use Rust for those is a waste of time!" #1 is a pretty widely held opinion. I also agree. But #2 doesn't follow, since there _are_ situations where you need Rust's predictable performance and memory footprint.
&gt; But #2 doesn't follow, since there are situations where you need Rust's predictable performance and memory footprint. And in that situation I would not dream of using something like Tokio as it is a framework that requires a PhD in I-don't-know-what to fully understand and if performance predictability is important, there sure as hell better not be an aspect of the system that is not graspable and reviewable in a reasonable amount of time. 
I am a bit worried that Future::wait will just do a busy wait like my example.. Following the source of Future::wait is a bit difficult.. I read some stuff about Tasks and park()/unpark() and I don't see if I need to implement something in my version of the Future or that should 'just work' Also I am a bit worried by the fact that the Futures tutorial just points to Tokio. I don't want to use a crate that just assumes I use a whole ecosystem if I don't use that. That smells like an mismatch or unnecessary weight..
https://github.com/jrmuizel/full-scene-rasterizer/ uses the same technique that flash uses. It avoids the memory bandwidth problems of MSAA by rasterizing the whole scene one line at a time. I might port it to rust someday.
Termion is great for terminal weirdness
What's the command you need to run to find the commit that introduced the bug?
I don't think there's a single command. I'm using `git bisect` and then rebuilding stuff.
The futures crate is basically split up into the different futures and the idea of executors. You spawn these futures into the executors, where the executor will then execute them. They either finish immediately or tell the executor that the result is not ready yet. If that is the case, the future will have gotten a notifier that it can use at any point to resubmit itself to the executor again. That means that futures do not in any way need to know which executor they are being used with. You can use any one of them (blocking your current thread, thread pool, epoll, ...). Future::wait() is a simple executor that just blocks the current thread when the future returns that it is not ready. Whenever the future then uses the notifier, the thread is unblocked and the future is executed again, doing all of the blocking over and over again, until it finally finishes, where it then returns out of the Future::wait(). So whatever Future you are using just needs to properly use the notifier or cache its value (in case the Future hasn't been spawned into an executor yet), and that's pretty much it. The oneshot channel is a nice promise like value that handles these few different cases for you automatically. I'm not quite sure why it didn't work out for you in the playground. Maybe send a link to it?
Ah got it to work: https://play.rust-lang.org/?gist=915e418bf30c79ef0784b0720be7c50a&amp;version=stable I also needed to call futures::task::current().notify(); before returning NotReady. Still this takes a lot of figuring out and looking at the source of the Futures crates. And still I don't feel confident I am not missing something crucial.. If not in my playground example, then in the real implementation.. Maybe a some implementations of Futures for std types like RwLock&lt;Option&lt;_&gt;&gt; would be a nice addition to the crate.. I haven't found a implementation of a Future yet that isn't defined in other Futures or tokio/mio primitives.. 
Ohhhh, you didn't use the oneshot channel. You are supposed to call .notify() whenever your value actually is ready. However instead of doing that and implementing the Future trait yourself, you definitely want the oneshot channel instead (and instead of writing into a value stored behind a lock, you just send it into the oneshot channel). https://docs.rs/futures/0.1.17/futures/sync/oneshot/fn.channel.html The other side of the channel implements the Future trait.
Your complaint seems to boil down to "Tokio is too complicated to grasp sufficiently", and this is where we fundamentally disagree. Not to mention that it's an odd statement to make as a comment on a blog post which happened to analyze that same Tokio in a pretty detailed manner, and did it very well, IMO.
Currently planning out an [ActivityPub](http://activitypub.rocks/)-speaking, [Mastodon](https://joinmastodon.org/)-compatible thing in Rust, using Rocket, Diesel, and friends. 
If you want to see a working implementation of InControl (beyond the Max/MSP patches and docs that Novation provides), I wrote [a thing](https://github.com/liamim/livemashing/blob/master/livemashing/controllers/launchkey.py) in Python for the 61-key Launchkey.
If you want to see a working implementation of InControl (beyond the Max/MSP patches and docs that Novation provides), I wrote [a thing](https://github.com/liamim/livemashing/blob/master/livemashing/controllers/launchkey.py) in Python for the 61-key Launchkey.
I've found the type inference of this pattern is pretty bad with the `?` operator vs the `try!` macro, e.g. something like: https://play.rust-lang.org/?gist=97dd3c4926083d1808d693e80f346128&amp;version=stable Got any tips?
Hmmm I will try that. This looks much more usable. However the [documentation on the sync module](https://docs.rs/futures/0.1.17/futures/sync/index.html) says that these primitives don't block threads. That is probably why I did look at them before.. However I assume that calling wait on the oneshot receiver does.. So I will experiment a bit more..
Use a type hint for the result of `collect`: &gt; let parsed = raw.iter().map(|i| i.parse()).collect::&lt;Result&lt;_,_&gt;&gt;()? The confusion arises because the `?` leaves rooms for the result of `collect` because of an trait involved. In comparison to the `try!` macro, the result of the collect only needs to implement `Try` with an `Ok` convertible to the other Error. The exact implementation can then not be deduced as it is not concretely given by `Try` nor by `collect`.
I have to say that talks like these, which introduce Rust and do their best to illustrate success stories and simply teach what Rust can bring to the table, are my favorite. The reason I started caring about Rust in the first place is because of the awesome fundamental impact I see ownership having on my projects and the way I code. **Talks like these, which tell me what I stand to gain and don't put down what I currently use, are what made me curious about Rust in the first place.** Truth be told, I got my first C++ job recently, and I never, ever would have been able to get it without something like the Rust community and `rustc` to teach me about low-level concepts that previously had to be learned through trial, error, and frustration. I always wanted to get lower in the stack than I was, but I felt that in order to get into a position to gain experience I already had to be trusted as a good C++ dev. Catch 22, right? Well, a little over a year ago, I decide to give Rust a shot for a fun side project. Like many others, I got frustrated experimenting with Rust because I simply didn't grok lifetimes and ownership of data. But I pushed through it, and after being forced to deal with it FIRST and "eating my veggies", I was able to combine those lessons with the enthusiasm I have for new developments in older languages like C++ and actually impress a company enough that I got in. What I learned with Rust makes me a better programmer in my current C++ day job -- it wasn't just adding a (really, really cool) tool to my box. Anyway, I guess this is turning into a thank-you for the Rust community and the Rust team(s). I'm planning on sticking around, and hope that I can (eventually) nab a Rust job after I get some good experience with my new C++ job. :)
It will still use `mio::Registration`. However, it will only use a single instance instead of one instance per task: https://github.com/tokio-rs/tokio/blob/new-crate/src/reactor/mod.rs#L55 If mio exposed a wakeup handle directly (backed by a pipe), tokio could use that instead of Registration, however I doubt it will matter in practice.
XSD is vital if you work with xml config files, it makes it easy to point out config mistakes to clients, digging through source code to find you miscapitalized something is not fun
Absolutely. The obstacle is that GitHub doesn't support OR-linking of `label:` predicates, so I may need to query for all issues regardless of labels and apply the filtering manually. Should be possible though (and hopefully still fit within GH's rate limiting).
Sounds good! Just need to [figure out](https://github.com/Xion/cargo-contribute/issues/4) how to get the list of globally installed packages (it probably requires spelunking through Cargo cache directory). I also like how it'd allow `cargo-contribute` itself to be included in the list :)
I don't believe rustc or Cargo keeps a compilation log, but the current code could be used as reference. Not sure exactly how to semantically grok Rust symbols though; perhaps there is a subcomponent of RLS that could be used for this. Good idea in general though!
Thanks, I just joined) &gt; Btw, does poloniex-rs place market or limit orders? As far as I remember, it places limit orders. And at that moment, when I was developing the lib, market orders were forbidden. &gt;Is this possible to do on top of poloniex-rs Honesly, I haven't touch that lib for few months, so I don't remember. You need to check API available API calls. The idea of the lib was implement all possible API calls. (however not all are implemented at the moment)
Yeah, they sending into these channels doesn't block, it just resubmits to the executor. And in your case the executor itself does the blocking / unblocking. Other executors don't necessarily need to do any blocking though.
Please, do you have any images of your rasterizer? I would be especially interested how it renders almost horizontal/vertical lines. And if I understood correctly, it supports 16 levels of coverage/alpha, correct?
I don't have any images off hand. It does 4x4 super sampling, so almost horizontal/vertical lines won't look great but they don't look great in flash either.
Huh, that's odd that rust-highfive didn't assign a reviewer. I would have expected it to. I tried to see if the incantation to change reviewers would work to kick it into gear, but it doesn't look like that didn't do it. /u/nrc any idea?
I'm not going to publish this to [crates.io](https://crates.io) until I feel this is done. First, I want to support more NMEA messages instead of just the ones I'm using (GPGGA, GPRMC, and GPZDA). Second, I'd like to help the embedded people out by getting this library to work in a no_std environment, and I'm not sure where to start on that. But this library does work just fine on embedded linux. Does anybody have constructive criticism?
Please don't yank the old versions!
Ah, looks like only the original submitter or the previous reviewer can change the reviewer. /u/szabot, you might want to try a `r? @&lt;username&gt;` comment like I made, and see if that triggers rust-highfive to assign a reviewer.
i use Emacs mostly, but have tried IDEA aswell and was impressed with that. I bounce between the two; I've setup a load of rusty greps (rusts syntax is so nice for that) but a real IDE with jump to def etc is better
Last I experimented with it (~6 months ago), I could only get breakpoints and the debugger to work in [CLion](https://www.jetbrains.com/clion/), which isn't free. [This GitHub issue](https://github.com/intellij-rust/intellij-rust/issues/535) seems to confirm CLion is needed for debugging. 
You can get JetBrains products for free if you're working on a OpenSource project or are a student/teacher: https://www.jetbrains.com/clion/buy/#edition=discounts 
I'm not 100% sure how useful this is, but a couple of years ago there was a push in Python to write networking libraries "sans I/O" - basically, write protocol parsers that were entirely independent of the network stacked used to communicate that protocol. (There's some stuff about that [here](http://sans-io.readthedocs.io/)). I think there was some talk about it in the Rust community as well. The example cases were mainly high-level TCP-based networking, and I know literally nothing about bus protocols, if that's even what you'd call them, so I don't know how relevant that sort of stuff is, or if it's worth separating out the protocol from the direct I/O at this level, but I didn't see anyone else mention it so I thought it was worth throwing it out there.
Ya, that's what I've been doing but its a pretty ugly pattern, lots of symbol goop. Maybe if you could elide the `_` in the type hint it wouldn't be as bad looking. `let parsed = raw.iter().map(|i| i.parse()).collect::&lt;Result&gt;()?` Or maybe the itertools and maybe the stdlib should get `collect_ok` or `collect_some` for the common result and option cases.
Also how could anyone say Erlang's scheduler is less complicated or closer to the metal than tokio? Just because something is a part of the language runtime doesn't make it free (incidentally, the OS scheduler is also extremely abstract and has massive performance implications, but no one is worrying about that).
Some suggestions: - create an Error enum rather than use `type Err = u32;`, and implement `Display` for it - `#[Derive(Copy, Clone)]` on your exported types - I would have `from_nmea` return `Result&lt;Self&gt;` and not use `Option` fields. This is assuming that your GPS doesn't send incomplete/invalid sentences that you want to recover partial data from. Then you can change the parsing to be like this: let time = x[0].parse::&lt;GpsTime&gt;().map_err(Error::SentenceParseError)?; let latitude = parse_nmea_lat(x[1], x[2]).ok_or(Error::SentenceParseError)?; // etc... If you also change the `parse_*` functions to return Result then you won't need the `.ok_or()`. - Since you want to support `no_std` you have two options: 1. Do the incantation to support memory allocation in no_std which I don't remember off the top of my head. 2. Don't use allocation at all. This shouldn't be too hard since you don't have any Strings in your output structs. You'd have to: 1. Replace String with str in NMEA and add the lifetime as needed - or - refactor the NMEA struct out of existance. 2. Don't use collect before parsing (e.g. `str.skip(x).take(y).collect()`) and instead create slices into the input string. This is a bit poorly formatted and vague (sorry) but I'm tired so that's what you get :P
Did you see this library also? https://github.com/Dushistov/rust-nmea Maybe add ZDA to that library? Also, make sure you support multiple constellations, which come in messages like GNGGA, GNRMC - my ublox M8 sends these instead of the GP* ones.
&gt; create an Error enum rather than use type Err = u32;, and implement Display for it Good suggestion. That was me being lazy and just wanting to get something working. &gt; I would have from_nmea return Result&lt;Self&gt; and not use Option fields. This is assuming that your GPS doesn't send incomplete/invalid sentences that you want to recover partial data from. Actually, some of the fields in the GPS messages can be blank depending on the quality of the GPS fix. Hence, the Option&lt;&gt; fields. &gt;Don't use allocation at all. This shouldn't be too hard since you don't have any Strings in your output structs. This is what I would prefer. I just used String instead of str just to "get things working." I just wasn't entirely sure how the lifetime stuff works. Using memory allocation seemed the path of least resistance at the time. Thanks for the suggestions.
[std::rc::Weak](https://doc.rust-lang.org/std/rc/struct.Weak.html)?
Related: let ch = s.chars() .skip_while(|&amp;x| x != '*') .skip(1) .take(2) .collect::&lt;String&gt;(); I see (and use) this pattern a lot since it's the most obvious and easy way to get a substring based on some criteria. So I was wondering: what is the best way to get a slice instead? The best thing I can come up with is to use a regex or use `s.char_indices().position()`. It'd be really cool if you could instead replace a `collect()` call to something like `as_slice()`. I think that might be possible somehow. Not sure if this is possible or worth the complexity, but I wanted to write this idea down somewhere before I forgot: trait SlicingIterator { type Item; // Need to be able to override for str: SlicingIterator&lt;Item=char, Slice=str&gt; type Slice = [Self::Item]; // Only certain Iterator functions could work this way fn skip_while&lt;P&gt;(self, predicate: P) -&gt; SkipWhile&lt;Self, P&gt; where P: FnMut(&amp;Self::Item) -&gt; bool, { ... } // ... others // Iterator adapter structs would need to be modified // to store range info. Chained iterators could build upon // preceding iterator's range/slice? fn as_range(self) -&gt; RangeArgument&lt;usize&gt; { ... } fn as_slice(self) -&gt; &amp;Self::Slice { ... } } impl Iterator for SlicingIterator { ... } // Then you could do this let slice = s.char_slices() .skip_while(|&amp;x| x != '*') .skip(1) .take(2) .as_slice();
I see thanks. I'll give it a try if I get some time.
It seems to me that the claim isn't that Erlang's scheduler is less complicated than Tokio. Rather, the claim is that *when you need Rust*, neither Erlang nor Tokio are close enough to the metal. The OS scheduler presumably is because you can basically bypass it when you're running a high-performance server. I disagree with that claim as well, I just don't think anyone was claiming Tokio was simpler than Erlang.
This wouldn't point to the object though, it would point to a structure containing active references, weak references, and the object in a undefined order and size. My bet for /u/vlovich is let ptr = &amp;mut box_ as *mut T; The object will destroyed when the box goes out of scope still, and you get a pointer.
I must be misunderstanding your suggestion. The compiler seems to complain: error[E0606]: casting `&amp;mut std::boxed::Box&lt;T&gt;` as `*mut T` is invalid I did try mem::transmute(t.borrow_mut() as *mut T) and that seemed to compile but I have no idea if that will end up being a runtime bug (still fighting with some linker issues).
Well, how about that. I actually had not seen that library before. It looks like they support Russian GPS too. Very nice. If our projects overlap enough, I may just fork them, or drop mine altogether. The nmea library I did see, and didn't like, was this one: https://github.com/flxo/rust-nmea That library uses regex to parse the sentences. Regex is nice, but it's kinda slow. I thought I could make something faster. Also, one of my eventual goals is to support a no_std environment. &gt;Also, make sure you support multiple constellations, which come in messages like GNGGA, GNRMC - my ublox M8 sends these instead of the GP* ones. That's definitely on the todo list. At the moment I only support the US constellation because that's what I'm using. I'll definitely be expanding to support the Russian and European GNSS constellations, and maybe the Chinese ones too.
Yea I forgot about how `Deref` works. I edited within the 3-minute window, so you won't see the *, but ust `box_obj.as_mut() as *mut T`.
Though `Rc` also provides `into_raw` and `from_raw`, which might be more ideal - though it would require the C++ code to pass the pointer back for disposal. (Initially I thought `Weak` might also provide `into_raw`)
/r/playrust
Lol I just realized 
I haven't tried it or anything but you should just be able to deref the box to get a normal &amp;T and then convert that to a pointer however you normally do.
If you're guaranteeing that the `Box` won't be dropped until the C++ code is done with it, you can cast `&amp;*t` into a pointer and then pass the pointer in.
I appreciate the feedback, I've learned some things. I have also since found the following issue on error-chain that was helpful, or at least had some other solutions: https://github.com/rust-lang-nursery/error-chain/issues/1 
Perhaps /r/playrust can confirm this.
This isn't that kind of Rust... your looking for r/playrust :)
There's also a [Rust Bisect](https://github.com/Mark-Simulacrum/bisect-rust) tool to automate all that tedious work. I think you just give it a bash script which gives a pass/fail and it'll do the rest.
ooooo
There is a whole chapter in the rustonomicon for exactly that problem https://doc.rust-lang.org/nightly/nomicon/ffi.html
Codes? Rust is about grinding for resources to build a base I didn't know there was code Unless your friend doesn't have a mic and you only chat in game with code so you don't reveal your loot? That's my only guess but I'd be happy to help you out 
That basically has the same semantics as `Box::into_raw` and `Box::from_raw`, which /u/vlovich/ is already aware of.
My advice is that, if something works with `mem::transmute` and not `as` and you're not **absolutely** certain you understand why, you've written a major bug just waiting to fire off.
Can you point out which section? I couldn't find any part of it that seemed relevant for passing a non-owning reference to an API that takes a void*. That being said, this is day 1 of Rust for me so I may have misread it. One of the other posters did provide a solution it seems like.
/r/playrust
Collaborators can do it too. Look below.
What are you talking about? I've never heard that resource files are required for building rust code.
Well yeah you need resources to build a base otherwise where are you gonna keep your loot? 
It might be helpful to run clippy on the code. Clippy will suggest a lot of improvements in addition to the stuff people may suggest here. https://github.com/rust-lang-nursery/rust-clippy
About once few days, from what I‚Äôve seen.
Sometimes, there's one or more a day. Other times, there's nothing for a week.
See the example in *Targeting callbacks to Rust objects*, the `main` function.
Is this something that frustrates you or gives you a laugh? 
I felt exactly the same thing. I'm so excited about the prospect of a language that enables massive systems to be built that are safe and manageable. Really glad that Rust turned out to be Popeye spinach for you :)
I have a feeling you have this subreddit confused with /r/playrust. This (/r/rust) is the subreddit for the Rust programming language.
I guess you're actually looking for [`lazy_static!()`](https://crates.io/crates/lazy_static) macro to avoid re-computing home dir every time you call that function. Note: prefer using `OsString`/`OsStr` or `PathBuf`/`Path` for file names, paths and other OS-related strings. `home_dir()` returns `PathBuf` already, so the best thing you can do is something like this: lazy_static! { static ref APPLICATION_HOME: std::path::PathBuf = { // Expect is like match you wrote, but it panics on None with given panic message. let mut home = std::env::home_dir().expect("Home directory not known. Try setting HOME variable."); home.push("test-folder"); // Note that slash is added automatically home } };
Very nice! Thank you!
Was wondering about whether there have been any opt-in actor systems written in Rust today actually. Thanks for posting this link!
You create some empty vectors and push into them. If you have a rough idea how large your vectors can grow it is a good idea to create them with a fitting capacity, so they don't have to be resized ( causing copying of data and (much worse) allocation ) while growing.
Some links for the curious, yet impatient: - talk homepage: http://www.codemesh.io/codemesh2017/andrew-stone - project code: https://github.com/andrewjstone/rabble - talk slides: http://s3.amazonaws.com/erlang-conferences-production/media/files/000/000/772/original/Andrew_Stone_-_Building_Actor_Systems_in_Rust_-_Techniques_and_Tradeoffs.pdf
For me, neither. I point it out and otherwise don't engage. The only slight annoyance I feel is towards threads like these where the author knows the distinction but still decides to make an off-topic post.
Well dang sorry I didn't know your life revolved around rust 
This subreddit _clearly_ revolves around rust, and it has a really high signal-to-noise ratio. This makes the little noise that is there all the more visible.
It annoys me because I'm subscribed to the RSS feed, so I'm almost guaranteed to see every single one... it gets old quickly.
If I understand correctly Dushitsov's code is a rework of flxo's code, which is published on crates.io under the same name. It replaces regexp with nom, so it should be more lightweight.
It's a bit entertaining. I don't know why someone would be annoyed, it's an honest, harmless mistake
Thank you :) I managed to finish my own version, and I'll try one with your code, and see what comes out of it. I'm just a bit wary of running through the iterator twice, but I'll setup a benchmark.
Hello. Author of *rust-mea* here. The back in the days when I wrote this crate is just intended to be a drop in replacement for some C++ code are more a experiment. My Rust skills were also not what they are today. I wound't use regex nowadays and probably go for something more convenient e.g. pest. Back then I didn't know better. *rust-nmea* is a little bit hacky and lacks proper Errors etc... The *gps-rs* crate is as far as I can see also a NMEA parser and maybe it should be published under that name on crates.io. I would like to see *gps-rs* published under the name *nmea* on crates.io in order to avoid to have two crates that do basically the same. Contact me if you like to take over! 
Thanks for this extension! Being used to develop mainly in Visual Studio during last years, I am just waiting on having proper support to dive into Rust. Thanks again for pushing this forward! 
That's a silly reason for a rejection.
Thanks, indeed! Now I only need to find out how I can reference that from another file. (of course I prepended a `pub`.
Not if you care about their code being available in 2 years. There is a reason why citation styles are standardized. This here shows that the authors either don't know how to properly cite their own work, or that they willfully ignore common practices. For what reason, I don't know. If you make reviewers or readers jump through hoops in order to get to the work you are promoting, then something fishy is going on.
There's also [this presentation](https://fosdem.org/2018/schedule/event/rustdebug) which should be interesting for Rust users.
Peeking around, I've also noticed these two in the [microkernel track](https://fosdem.org/2018/schedule/track/microkernels/): * [Rust On L4Re](https://fosdem.org/2018/schedule/event/microkernel_l4re_rust/) * [Introduction to Rux](https://fosdem.org/2018/schedule/event/microkernel_rux_intro/)
I would be very interested in reading about your experience porting your code from Scala to Rust. Scala is my goto language right now for both backend and frontend (JS), but that might change if/when the Rust eco-system and tooling matures as I see some definite advantages in using Rust.
And the same presenter is also giving [this related talk](https://fosdem.org/2018/schedule/event/debugging_tools_rust/).
We get about two posts per day that are intended for /r/playrust. We filter pretty agressively on typical keywords and domains so that hopefully not too many people are bothered, though some common keywords (like "server") are just too common to filter for.
Also anyone mind telling me what rust is for? 
Yeah, but most config files are usually JSON or YAML nowadays.
It's a matter of taste, I prefer my own macros.
And I have a feeling that this time it's a troll.
I guess you want it as a module within the same crate. Probably the simplest way is to use absolute path e.g. if it's in module called "common" inside the src directory, the path would be ::common::APPLICATION_HOME.
Faster than what ? Is there already standard rust simd library ? If not, then just rust-simd.
That would be the links on the right. ttdr: it's a new programming language capable of writing the infrastructure of a lot of things (like drivers, operating systems, web browsers). There's a lot of news about this language, and what programmers are doing with it that gets discussed here. Probably the biggest hype for the language is that it can prevent a lot of the bugs that hackers make use of, leading to safer infrastructure. 
Sounds pretty neat-o Also who down voted my comment I was just asking a dang question üò°üò°
It technically *is* possible to cross-compile with `x86_64-pc-windows-msvc` from platforms with Wine support by running `link.exe` on‚Ä¶ (a fairly recent version of) Wine. It is not conveniently packaged, but it is not impossible to do so.
Your pasted code is a little broken. But I guess the problem is `fn sigmoid(self, ...)` and all the similar functions that consume `self`. Maybe you want `fn sigmoid(&amp;self, ...)`.
What's the signature for `lstm::Cell::sigmoid`? If it's just `sigmoid(self, ...)` then calling `sigmoid` consumes self, which is probably not what you want.
Thanks for the tip, I saw the `r? @username` command before, but I didn't know who to assign.
`Cell` isn't copy, and cannot be, since `Array1` is a type that owns memory on the heap (similar to `Vec`). For types which aren't copy, using them moves them out of the existing variable into the function you're calling, which is not what you want. You need to either use `&amp;self` or `&amp;mut self` to pass a reference the function. On a different note, you might want to use `&amp;ArrayView` where you're currently using `&amp;Array` for increased flexibility. 
I like to play "Rust or Rust game" when some posts could be either (like [this one](https://www.reddit.com/r/rust/comments/7l1d5m/rust_is_moving_me_again/) -- moving emotionally? lag-related teleporting? Rust move semantics? Let's find out!)
A good way to pick a reviewer is look at who has contributed to that file in the past, and who has reviewed recent contributions. For instance, if we look at [one of the files you changed](https://github.com/Sh4rK/rust/commits/926865ba2e4e9c137383003d76b6541fe29f6431/src/bootstrap/dist.rs), we see that [@alexchrichton reviewed the last change to it](https://github.com/rust-lang/rust/pull/46514), and he's also listed as the top contributor to this file, so he's likely a good pick.
Offtopic: I have GRU layer example for CNTK rust wrapper here: https://github.com/usamec/cntk-rs/blob/master/examples/seq2seq.rs You can use that to create LSTM network. But things seem to be quite weird in your snippets. One thing is that sigmoid should not belong to any struct/trait and should be global function. Second things is that Cell trait should only have step function and output, forget, update are detail of LSTM Cell (GRU Cell has something completelly different). Third think is that it seems, that you are updating things element-wise in really weird fashion (aka why are you updating `fw[2]` with something calculated based on `fw[0]` and `fw[1]`?). And finally as others pointed out, your member functions should use `&amp;mut self` instead of `self`, or consume `self` and return new Cell. 
Looks promising! And usually talks there are both streamed live and recorded, which would be great for everyone that can't be there. I'll be there!
Or use `&lt;S: Data&lt;Elem=f64&gt;&gt; ArrayBase&lt;S, Ix1&gt;` ;)
`UnboundedIterator` doesn't need to be an unsafe trait. Make it a safe trait whose one required method is `next(&amp;mut self) -&gt; Self::Item`, and then derive an implementation of `Iterator` from it (including `size_hint`). Then `UnboundedIterator` can never return `None` and this is guaranteed at the type level, thus safe. This should make things easier for implementers? Less that can go wrong.
&gt; iterator twice Read the standard library source for `filter`; it only goes through once.
What was the annoying thing?
I just wish there was an option for Ratliff style closing braces. I know they're not popular, but I can't program without them anymore.
What do you like most about Rust? I haven't used it and am kind of on the fence about it.
Yeah, I'm very happy with the style that rustfmt currently produces, too. A few of my projects enforce rustfmt on CI and so far it was surprisingly painless.
Ooh, I'm on the reserve list for that. You don't really want me there though - that lineup looks great!
I didn't downvote, but it probably had to do with the fact that the answer is pretty visible in the sidebar. Possibly also someone was annoyed by your other response of "I didn't know your whole life revolves around rust" (which reads as insulting to probably everyone here, considering that we are on a programming language subreddit dedicated to rust) and downvoted a relatively benign comment because of it.
It was directed at a single salty individual I spend time on r/dbz but my whole life doesn't revolve around dragon ball No one should be insulted by the comment if they read the whole conversation 
It was directed at a single salty individual I spend time on r/dbz but my whole life doesn't revolve around dragon ball No one should be insulted by the comment if they read the whole conversation 
I think what you're missing is a Task. As the [deep dive](https://tokio.rs/docs/going-deeper-futures/futures-model/) says: &gt; Let‚Äôs take a concrete example. If a future is attempting to read bytes from a socket, that socket may not be ready for reading, in which case the future can return NotReady. Somehow, we must arrange for the future to later be ‚Äúwoken up‚Äù (by calling poll) once the socket becomes ready. That kind of wakeup is the job of the event loop. But now we need some way to connect the signal at the event loop back to continuing to poll the future. &gt; &gt; The solution forms the other main component of the design: tasks. You've returned a future, and you have some threads working to complete it, which is great. But you have no way to tell the holder of the future that it is time to poll it - that's why you've had to add the blocking spinloop at the end. So, instead of that, use a Task to signal completion. I think what you need to do is to obtain the current Task when you enter send, then when once the future is complete, call notify on it. Although i think that might be impossible to do across threads - i'm really not sure. It is then up to the caller of your library to make sure that there is a Task in place when they call send. 
Ask on the cairo mailinglist - I'm pretty sure, that by choosing different blend modes you can get round this.
FYI, I was really confused when I opened that image, because it rendered perfectly fine on my 4K work monitor :P but on my laptop screen, I saw the seam
I was just guessing, since you asked, trying to be helpful, since you asked. I have no skin in this game, and don't care. FWIW, you don't get to assert whether or not someone else read your response as insulting. Especially since this response sounds like you *were* trying to respond to salt with salt. So the insulting reading jives with your stated mood. In general this sub tries to maintain a high signal to noise ratio, so we don't have a lot of tolerance for off-topic argument/argument about who was the most offensive in a conversation. We just want the arguments to stop and for debate to be on-topic. We have a code of conduct that supports that aim.
Seriously, how dare they leave this open. I'm dying.
Can you tell me about the projects that enforce it on CI? Amethyst has been wanting to do this and it'd be nice to have working examples.
Sure. Have a look at [one](https://github.com/killercup/cargo-edit/blob/7a3964c08f363e4865fca531660414c2627ef901/.travis.yml#L9-L15) [of](https://github.com/diesel-rs/diesel/blob/af05bcfed13687ebadaa278c56617b2556024397/.travis.yml#L58-L65) [these](https://github.com/killercup/assert_cli/blob/3bc97874a188423fcdc3252c5d677b0fb048530e/.travis.yml#L10-L23) Travis configs to get started. Diesel is by far the biggest code base of the three examples, with the most PRs. So far, we've updated rustfmt a few times, which produced slightly worrying amounts of diffs. A typical Ci failure looks [like this](https://travis-ci.org/diesel-rs/diesel/jobs/318868150#L525).
It's an anti-aliasing bug. Qt and Chrome has it too.
You say ‚Äúslightly worrying‚Äù. I recently saw a diff of rustc post rustfmt. It‚Äôs 128 megabytes!
Fine, you win
I ‚ù§ the open design process, the great community, the helpfulness of the compiler, the attention to details, the steady improvements, besides nifty things like iterators and `std::borrow::Cow`. I love that I can have good IDE support ‚Äì or just code in a plain vi, both work quite well. I love that the safety guarantees free me from worries that would otherwise hinder me from doing awesome stuff. Or, as I said before, if C is like playing with knives and C++ is like juggling chainsaws then Rust is like parkour wearing protective gear while suspended from strings. It may look ridiculous at times, but you can do all sorts of awesome moves that would be damn scary or outright impossible without it.
Which means we mods are quite effective at keeping playrust posts at bay.
You probably meant to post this on /r/playrust/. This is a subreddit dedicated to the Rust programming language, not the game.
At one point, it was a toy DB, in a category with projects like [mSQL](https://www.amazon.com/MySQL-mSQL-Tim-King/dp/1565924347/ref=sr_1_1?ie=UTF8&amp;qid=1513797354&amp;sr=8-1&amp;keywords=mysql+and+msql) (remember that?). When were you in Uni? 
Ahh right, there's no collect! Thanks :)
I submitted two talks, one on qt and rust and one on using tokio and hyper in a server. To bad neither was accepted, but the list indeed looks very good. Less stress for me. ;-)
Hi, I can't seem to get LLDB working. I keep getting the error "LLDB self-test has failed." Here's the console output: https://pastebin.com/Wudsi1Lf Based on what I could find from searching the error, I have installed LLVM-6.0 and Python 3.5.4 and made sure both are discoverable in PATH. But the same error still pops up. I'm on Windows 10. Any ideas on how I could tackle this?
Very informative talk - thanks.
&gt; I know they're not popular, but I can't program without them anymore. Can't? That seems like a bit of a strong position to take on such an whimsical matter, but I dunno. I feel like curly brace positions are arbitrary enough that you would make yourself and the people you work with happier if you just learned to enjoy whatever curly brace style any given programming language community prefers, rather than fighting it, but that's just my opinion.
Or, tooling could be built that automatically ingests source code, reformats it the way you like for viewing and editing, and then spits it back out in whatever the project's required format is. Hmm, I wonder what a tool like that would be like.
Such tools have been talked about since the dawn of programming, but then I think the explanation for the uncommonality of such tools is that most people realize curly braces aren't "the hill they want to die on," and that there are usually bigger issues at hand. If you want to build such a tool, I'm sure there's a non-zero audience for it.
You can use sarcasm this dry as a desiccant 
Rustfmt is a nice tool but I find it a hassle to update. Does anyone else have to uninstall it and re-install it with each nightly upgrade?
That is, unfortunately, the way that rustfmt works currently. Because it links against the compiler, it only works when run with the compiler that built it. There is ongoing work to make this a lot simpler, so that you can do the equivalent of `rustup add component rustfmt` and get `rustfmt` for your toolchain the same way that `rls` is handled.
I just looked up [Ratliff](https://en.wikipedia.org/wiki/Indentation_style#Ratliff_style) style closing braces. I don't want to tell you that one way is right or one way is wrong, but I've never seen a project that looked like that. And I would actively have trouble reading it, to the point where I think it might cause bugs. I think you need to work on moving on away from Ratliff style.
stdsimd has rustfmt on CI 
All code formatters are going to make some choices you don't like, and often even choices that are just plain wrong. They can still be valuable for a lot of reasons. I also noticed fewer irritating choices when I started using my own `rustfmt.toml` configs.
It's been available as a rustup component for a little while now. But unfortunately, it doesn't seem to be included in all of the latest build (2017-11-26 is the latest one I know that has it); not sure what is up with that..
Seems like it's working already. I just happened to run a rustup update and got this message. $ rustup self update info: checking for self-updates info: downloading self-update info: rustup updated successfully to 1.8.0 warning: tool `rustfmt` is already installed, remove it from `/home/king/.cargo/bin`, then run `rustup update` to have rustup manage this tool. warning: tool `cargo-fmt` is already installed, remove it from `/home/king/.cargo/bin`, then run `rustup update` to have rustup manage this tool. Added the component, did a `rustup update` and I can run both `rustfmt` and `cargo fmt`.
&lt;3 I can't wait until rustfmt 1.0, personally. rustfmt all the things!
op here. it does things, but i don't know if it's correct what it does. - i had the idea of streams for the signals and also for the state and hidden state - a feature stream contains signals of the past, present and future - a signal stream can have multiple feature streams, for instance a 2d image or as many data points as needed - i separated the activation functions (as suggeste from someone in my other post), but also separated Gate trait functions that represent gate/layers and Bus trait functions, from which the state and hidden state can be fetched. - the two points of entry from outside into the lstm are supposed to be through fn state and hidden - i put everything in main.rs because i really struggle to understand crates and modules and how to include them. 
changed that. i am really unsecure when with all that. :(
cc: /u/ossamc I am using a 'custom solution' Vec now. To me it was to complicated. Too many error messages. I tried, but I can't make progress when the pretty cool and colorful compiler throws so many errors at me :)
no clue what that is supposed to mean. I am a beginner.
I checked your project.. looks too complicated to me. :) this is my attempt https://github.com/aspera-non-spernit/brainrs/blob/master/src/main.rs it's probably not working, but wip. I changed the signature to &amp;mut self
Tell that to literally every Python programmer.
I'm having trouble finding an actual critique in this. By no means is rust perfect (which is why we are continuing to try and improve it) but I'm not really sure what you are actually complaining about here specifically. Instead of just saying the langauge itself feels unergonomic, give some examples of how it feels unergonomic.
Perhaps you should show samples of code you think looks ugly/messy, and then see if people here can help pretty it up.
Thanks, will do 
Hi, the problem you are facing is that you have passed in a slice of Layers which you then save into the Console object. Because this is a slice your Console object now contains references to the Layers within slice you passed in. This means that if you were to move/delete the Layers passed in your Console would contain invalid references which the compiler doesn't like. Some possible solutions: * Make layers cloneable so you can create a clone of all the layers and store the clone in the Console object. This means that your Console doesn't hold onto references to stuff passed into it because it has its own copy. * Pass a Vec of Layers (`Vec&lt;Layer&gt;` not `Vec&lt;&amp;Layer&gt;` or `&amp;Vec&lt;Layer&gt;`) rather than a slice. This ensures that Console isn't holding onto references to other objects because the method consumes the vec passed in. * Some horrible stuff with [lifetime bounds](https://rustbyexample.com/scope/lifetime/lifetime_bounds.html) to ensure that your Console can never outlive the Layers slice passed in. Depending on what your code is actually trying to do this might work, but from a very quick impression I suspect this is a bad idea. In general I would be suspicious whenever you find yourself explicitly putting lifetime bounds on a type just to make compiler errors go away, they can be useful but generally not for long-lived types. I fixed it using the first option here: https://play.rust-lang.org/?gist=a91e6b9dc2420650011179568f09ee4c&amp;version=stable I made Layer clonable because at the moment it is just a u32, I changed the console object to track Layers rather than references to them and I removed a bunch of associated lifetime bounds.
Personally, I really hope that `rustfmt` can get to the point where using `rustfmt.toml` becomes really rare, and the style of most public facing code homogenises. What that likely means is that it'll need to take the "least offensive" option for formatting choices where the majority of people find at least find it acceptable. `gofmt` has gotten to this point; the quote that goes around about it is "`gofmt`'s style is nobody's favourite, but `gofmt` is everybody's favourite." It's a pleasure to read other people's code because the style is exactly the same as what you see everywhere else, and has a marked impact on the readability of code.
Changes in the compiler frequently break rustfmt/rls/clippy, so they're frequently disabled until they can be updated.
I program almost exclusively in Python and, if I cant get a formatter to give me this kind of styling in a brace-based language, I throw out the formatter. int main(int argc, char *argv[]) { if (argc &gt; 1) { printf("Single statement"); } else { printf("...but still using braces"); } return 0; } Heck, this is how my bash looks... if thing = 1; then echo "Foo" fi ... and here's how the Pascal I've started to pick up for typesafe retro-programming looks: for count := 2 to 11 do begin (* ... *) end; 
No sir, I don't like it.
git hooks
 git: 'hooks' is not a git command. See 'git --help'.
How does that in any way follow from my counter-example to your blanket statement about Python programmers?
You should always use &amp;self, unless you have a reason not to.
...or that they're so easy to implement with things like VCS hooks that nobody really thinks much about publicizing their solutions. (I've been lazy, so my solution has been to incorporate "selectively countermand rustfmt" into my usual "use `git gui` to disentangle changes I thoughtlessly made concurrently" commit workflow.)
I absolutely love gofmt as well for the reason you stated, but I honestly don't think it goes far enough. However, having it as a standard has helped our team onboard a few new gophers. I also like rustfmt, and I'm super excited to see where it goes.
In order for this to happen, one of two things is required: 1. A broad consensus on line length is reached. 2. rustfmt becomes (mostly) line length agnostic like gofmt. (gofmt isn't 100%, but it is close enough.) I'm pretty sure (1) will never happen. I don't know what (2) would take, it if it possible at all.
He wants the range of places type inference can be used to be more broad. (I say this both based on his prior posts and based on the last paragraph of this one, where he's basically asking for whole-program type inference as a way to streamline exploratory coding.)
Exactly. I don't think I've ever met someone who likes Ratliff style, and I know quite a few Python programmers.
That went by fast (or it seemed to, from the perspective of this outside observer.) How did it turn out relative to what you all hoped to accomplish?
&gt; Are there any examples of Rust code that are especially beautiful and/or elegant? What would you define as beautiful or elegant? One person's elegant is another person's ugly. If you enjoy one-liner solutions, then Rust has the ability to do just that.
&gt; One person's elegant is another person's ugly. No, please, just stop! 
[According to Github](https://github.com/search?utf8=%E2%9C%93&amp;q=advent+of+code&amp;type=), Rust is the sixth most popular language for solving AoC. If we look at only [this month](https://github.com/search?utf8=%E2%9C%93&amp;q=advent+of+code+created%3A%3E2017-11-30&amp;type=Repositories&amp;ref=advsearch&amp;l=&amp;l=), Rust is the fifth, only after Python, Javascript, Java and C#. If we assume that the percentage is similar for the private projects, it means a lot of people are learning/practicing Rust through AoC. The other interesting thing is Rust's competitors (in my opinion): Go and C++ are not far behind, Swift and C has not that many repos and D has almost no AoC repos.
&gt; give some examples of how it feels unergonomic. I've tried to reduce repeitition by linking earlier comments for reference; basically I'm just bringing up parts of these again, just saying with my current hat on, I'm *not* complaining about bounds checked arrays &amp; lifetimes aswell (safety aspects). This is the first time I've actually used rust with the intent of doing web-related things, so I'm accepting the languages guidance there.
&gt; A broad consensus on line length is reached. Unfortunately, people's opinions on this tend to be based on their monitor size :)
I've been thinking about mandatory git hook formatting for some time.
Is there a way to set the cargo environment varibales `RUST_INCREMENTAL` globally through .cargo/config or the like? i.e. through something other than the operating system environment or a project specific build script.
I've been meaning to learn Rust for a long time and have been using AoC to finally do so. For AoC, I start by writing each day's advent first in hacky C++ to try to beat my co-workers on time, but then I come back and do the careful/clean solution in Rust. I start in C++ because I know it inside and out, so never have to turn to the internet to figure out how to do something, and it has a great debugger if I hit a problem I need to diagnose quickly. That being said, I always like my Rust solution significantly more. The powerful functional call syntax which is enabled via with iterators is really awesome to see in a systems language!
I'm a 90% Python programmer that thinks that looks nuts.
What is wrong with this? I know people who think massively cryptic perl one-liners are "elegant" while I think just straightforward no cleverness code is "elegant".
It varied by area. The compiler-related working groups were the most successful, thanks in large part to exceptional mentoring work from nmatsakis, arielb1, and mw. The library-related groups also did quite well (including the Libz Blitz, the Guidelines, and the Cookbook). The rustdoc groups also got a *ton* done. OTOH, for Cargo, only the crates.io group made much progress; this was mostly a matter of lack of clarity around the goals -- the groups basically were not really ready for an impl period effort. Finally, the infra team groups made a small amount of progress, but despite a fair bit of mentoring effort didn't seem to get a ton of attention. One successful aspect across the board: folks with a huge range of backgrounds were able to meaningfully contribute. We continued to get new volunteers up until the last minute, and my impression is that people really enjoyed working on these small teams toward very clear-cut goals. So, overall, I would say it was a *very* successful program, and was essential in delivering our 2017 roadmap. I think we're certain to make something like this a repeating event in the Rust community, but will need to think about how best to do that. We'll be posting an overall 2017 retrospective post soon, which will cover the full set of highlights of what got done this year. Spoiler alert: we got a lot done. :-)
Shouldn't there just be a standard for committed code, where people can format anything local to their hearts desire? I thought that was half the point of formatters.
By the way, has the roadmap for 2018 been talked about yet?
I‚Äôve started something similar. I do the first pass on Ruby because I can knock that out quickly. I‚Äôve started going back and reworking the problems in Rust, mostly just to get the time in get comfortable with the language before starting a new project. 
I came here from C++ and wouldn't want Rust to become "like C++", e.g. getting non-local type inference (beyond the necessary evil that is impl Trait) or a custom language feature for constructors (which I feel is redundant &amp; increases the learning burden). You have an alternative to switch to. I don't have one. I don't think we need to come to the realization that one language can cover all use cases. Rust indeed is attempting to do precisely that. Your idea of forking the language for a library and large application use case and a small application use case sounds like a good compromise.
I run it in my GitLab CI for [serialport-rs](https://gitlab.com/susurrus/serialport-rs) though I don't enforce it yet. `rustfmt` wasn't stable enough for me to enforce it, but it seems like it is now.
Really disappointed about the infra progress - this is one my most major issues with using Rust. Really unhappy to see the lack of interest in its development,
I was trying to figure out how to get it working after uninstalling; what did you mean you "added the component"?
God I hope RFC2000 and RFC2094 make it in Q1
Which part of the infra is sorely lacking? Are you waiting for something in specific? For I generally get the idea most people have no problems in this area.
There have been some early discussions, but the process will be officially kicked off in an upcoming post on the main Rust blog.
Autocomplete.
You mean like as in bash completions?
Reading the ‚Äúthis week in rust‚Äù changes I get the impression that RLS is getting a good chunk of attention. But I read somewhere that the incremental compilation is a big dependency for some features.
Something I've been considering is an "infer this type" symbol for function return types, and maybe (but not necessarily) function arguments. I wonder if you would consider the existence of such a symbol an acceptable compromise?
&gt; or putting it perpetually behind an unstable feature flag how about 'only within a module', and/or 'only within the main program- not in library interfaces'.
&gt; Something I've been considering is an "infer this type" symbol for function return types, and maybe (but not necessarily) function arguments. How about combining that with syntax sugar for single expression functions, ie. you can *only* do it for small functions fn lerp&lt;T:Num&gt;(a:T,b:T,f:T)=(b-a)*f+a
Absolutely - elegance of design and external API is more important. Whether a person uses a for-loop instead of an unusually convoluted iterator chain is secondary. (Although I personally feel a _little_ guilty about overuse of `mut`)
I personally wouldn't mind if the impl period would get extended. [Just look at the sheer number of unimplemented RFCs](https://github.com/rust-lang/rust/issues?q=is%3Aopen+is%3Aissue+label%3AB-RFC-approved+-label%3AB-RFC-implemented+sort%3Acreated-asc). Some of them are 2 years old or more.
I don't really see the advantage of this syntax... it's the difference between fn lerp&lt;T, F&gt;(A: T, B: T, f: F) = (b - a)*f + a fn lerp&lt;T, F&gt;(A: T, B: T, f: F) -&gt; ? { (b - a)*f + a } fn make_something(args) = SomeStruct { .... } // Personally I don't care about nesting level... but saving a line if args is short is nice fn make_something(args...) -&gt; ? { SomeStruct { ... }}
[removed]
Incremental compilation is making really great progress, and that is foundational for future RLS work.
Can't figure out the return type signature. fn get_targets(root: &amp;str) -&gt; ? { let entries = WalkDir::new(root) .sort_by(|a, b| a.file_name().cmp(&amp;b.file_name())) .into_iter() .filter_map(Result::ok) .filter(|e| e.file_type().is_file()); entries } 
I think this question can be re-phrased as: What are some well-written, idiomatic libraries with a clean and well-thought out structure that are worth studying? I'd be curios about that too.
Every so often I'm mutating a couple maps or whatever, and I stop to worry "hmm what if some other thread messes with these while I'm using them" and then I'm like "oh yeah never mind that's *statically impossible*."
There are too many problems to explain in a comment. I've done a PR on Github, check that one.
Thanks! I kind of suspected that was the case but was thinking I needed to do option 3. I've done most of my programming in Swift, where you don't need to think about memory management in the same way, so I think I was set on only passing a reference around. Perhaps it's just easier to clone things sometimes.
&gt; how about 'only within a module', and/or 'only within the main program- not in library interfaces'. I'd say "only within a module" (to limit compile time blowup and keep codebases easy to learn) combined with "only for non-public functions" to address my primary concern. (Public API signatures becoming dependent on the results of inference, even if merely through honest error.) &gt; I'd object to calling people that do this 'lazy' or 'sloppy'. That wasn't what I said. All lazy or sloppy coders would use it, but not all people who would use it would be lazy or sloppy.
See [this response](https://www.reddit.com/r/rust/comments/7l50bd/rust_emscripten_different_perspective/drk5q6g/?context=3) for my concern. **TL;DR:** Only if paired with an "inferring elements of function signatures is only allowed for private functions".
`rustup component add rustfmt-preview` Going by the message I posted above, I don't know if that was necessary, I just did it.
I just don't see that as being beneficial enough to justify adding a whole new syntax for function declarations, given how Rust already lets you omit `return` and a terminal `;` within a single-expression function.
My [2016 solutions](http://github.com/BartMassey/advent-of-code-2016) in Rust. Reasonably clean code with comments and stuff, plus some supporting libraries.
FWIW, instead of : for line in &amp;contents[0] { let mut dup = false; let mut set: HashSet&lt;String&gt; = HashSet::new(); for word in line.split_whitespace() { if !set.insert(word.into()) { dup = true; } } if !dup { count += 1; } } you could do : let v: Vec&lt;_&gt; = line.split(' ').collect(); let s: HashSet&lt;_&gt; = v.iter().collect(); if s.len() == v.len() { count += 1 } 
Ooh, collecting a HashSet. Nice!
How much overhead does a Vec have over an array, if I am never resizing the vec, and I always know exactly how large my Vecs will be at runtime? My scenario is: I'm building a toy "board game" library to understand traits and generics better. One of the generic arguments is what type of piece your board game will use, like "BoardGame&lt;ChessPiece&gt;", etc, because the BoardGame itself is the data structure that will hold the array of pieces. I initially planned on having the size of the board also be a compile-time constant, and was designing like this: `let myboard = BoardGame&lt;ChessPiece, 8, 8&gt;{};` and then everything could be a fixed-sized array. (I know const-generics isn't merged in yet, I'm using the "generic_array" crate) My biggest concern is I will eventually be generating these board states very very fast in a tight loop, since I'm going to implement AI agents that will perform game rollouts. I wanted the zero-overhead of a fixed-size array for this. But using a Vec allows me to ditch the ugly generics. 
I found the stdlib source full of good examples - in particular, the `Iterator` implementation. Really helped to understand how it all fits together. (The string library code on the other hand is not for sissies since the rabbit holes go deep) 
Isn't this even better (no vec allocation and early return): ``` let mut set = HashSet::new(); count += if line.split(' ').all(|w| set.insert(w)) { 0 } else { 1 }; ```
This is a job for smudge, in git attributes
I heard that ripgrep is a beautiful tool. http://blog.mbrt.it/2016-12-01-ripgrep-code-review/
&gt; Google searches like "rust hashmap insert" don't always go to the canonical API docs. Sometimes you get Servo docs, or sometimes you get a link to an old version (say, 1.14). The documentation doesn't have a link to the latest canonical version ("These docs are out of date - try here!"), which would be useful. I personally use DuckDuckGo and just type `!rust insert` or `!rust hashmap` to godirectly to std doc. If I need a particular crate then I go to *docs.rs/crate_name*
Glad to hear more is coming! I didn't have the resources to get started this time around, so fingers crossed for next round :)
IntelliJ's Rust plugin has pretty good autocomplete.
In Rust, the "infra" refers to the Rust's project own infrastructure (CI, the servers, the issue tracker, releases, etc.). RLS is it's own separate team.
Python is literally Ratliff by virtue of using white space indentation. In Ratliff, the last text in a block is at the same indentation level as the block.
What's happened to WG-core-site?
It has the exact same visual flow as Python.
But that's not infra, that's compiler team (incremental compilation, lazy evaluation, etc.); lot's of progress has happened on this front, but more needs to happen before RLS autocomplete can drop Racer and use the compiler.
&gt; Unfortunately, people's opinions on this tend to be based on their monitor size :) I cross-compile a lot with Rust, and when things breaks in other targets, I `ssh` into them, and edit-compile-debug via a 80 column terminal. I commit from the targets, do diffs, open two buffers next to each other, etc. I don't know what's rustfmt default max width (110-120 characters), but anything over 79 is not practical for me. And yeah, editors can do line wraps just fine, but I still think its annoying if most of the lines in your editor are wrapped, even when you only edit one file at a time.
Writing out the full type of `entries` is impossible, because you would need to write out the types of closures. Right now your choices with this are either: 1. Collect the iterator into a `Vec&lt;DirEntry&gt;` and return that. 2. Box the returned iterator, and return `Box&lt;Iterator&lt;Item=DirEntry&gt;&gt;`. Once `conservative_impl_trait` arrives you will be able to write simply `impl Iterator&lt;Item=DirEntry&gt;` as the return type without changing the function.
One thing I still don't like is that you can't only specify brace styles for functions and structs seperately (or I haven't found the right setting). Ex. I have a style like this: struct Something { value: u32, } fn something&lt;T, U&gt;(data: T, item: U) -&gt; Option&lt;Something&gt; where T: Debug + Clone, U: Copy + PartialOrd, { /* */ } This style is (for me, at least) very readable, but neither rustfmt nor rustdoc work very well with this. It has gotten better, though. [This](https://gist.github.com/anonymous/61d822923f331fd6326d2284296784fa) is my current rustfmt, but it makes the struct look like this: struct Something { value: u32, } fn something&lt;T, U&gt;( data: T, item: U ) -&gt; Option&lt;Something&gt; where T: Debug + Clone, U: Copy + PartialOrd, { /* */ } ... which is close, but I think it looks ugly. It has definitely gotten better, but rustfmt still has lots of problems with breaking comments into multiple lines properly. It's definitely better than infinite flame wars about code style, though and "remember to format your code properly". 
I think this is bad advice: one should know the difference between `self`, `&amp;self`, and `&amp;mut self`, and use the appropriate one. The OP admits not knowing this, and that's fine: it is possible to write Rust code that works without knowing this, but doing so is going to feel like "I don't know what I am doing" and for example if we fix this code the OP might run into the same issue 5 minutes later. The OPs time might be better invested into learning / clarifying its mental model of ownership &amp; borrowing so that it at least is able to describe the difference between `self`, `&amp;self`, and `&amp;mut self` and able to choose appropriately or at least interpret the compiler error messages if it chooses the wrong one. 
If you are using rustup which you should you can just enter: rustup doc --std into the commandline and you get the docs for your active toolchain
What do you mean by "updated rustfmt" ? Are you fixing the rustfmt version in `rustfmt.toml`? Are you using `rustfmt-nightly` to format the code? If so, how do you and your users install the appropriate `rustfmt-nightly` version? 
Look at the files I linked, we always install a specific rustfmt version (like `cargo install rustfmt-nightly --vers 0.2.17`) using a specific nightly toolchain. Usually, it's not a problem to have the same version installed locally; alternatively, you fix the formatting by hand (hope you didn't submit a &gt;1k line PR!), or we update the version we use.
&gt; Usually, it's not a problem to have the same version installed locally; How do your users handle working on different projects using different versions of rustfmt-nightly ? 
Python does not have braces, my experience of Python code is that the parens follow 1TBS-alike rather than Ratliff, and CPython [most definitely does not use Ratliff](https://www.python.org/dev/peps/pep-0007/) (it uses a more Stroustrup variant of the K&amp;R). Furthermore if you want "the exact same visual flow as Python" you use [the Lisp style](https://en.wikipedia.org/wiki/Indentation_style#Lisp_style). Ratliff has extra "brace lines" which just do not exist in Python.
Braces are not text.
Gofmt changed my life. It takes managing codebases within big teams way easier. I hated the days when everyone had their own style and half the day was spent arguing about how something should look.
What made me go "fuck yeah" was probably when I first played around with the `match` statement. That you can match tuples and structures and destructure them infinitely deep in the match arms. Similarly the `if let` that also offers those abilities. E.g. (from an older code, probably doesn't need the ref keyword now): let name = if let Sub::Node(Node{name: Some(ref n), ..}) = current {&amp;n} else {""}; Then I like that pretty much every statement is an expression. Together with the functional capabilities this often allows to write complex procedures in one statement as a chain of functions. Concepts that have to be separated in many other languages can get mixed and matched into one long chain. And as a tinkerer I personally absolutely love that new nightlies are available every day and features get added all the time that you can optionally use from day one with feature gates.
Not sure how others are doing this but I have a few rustfmt binaries laying around (along with some nightly rustc from many moons ago). Luckily the diff between versions is not that large anymore, so you can probably just have the latest installed. If there truly is a bug change, I'm always happy to see PRs that update the rustfmt we use on CI, so the next person to submit a PR can benefit from it as well.
&gt; `Cell` isn't copy, and cannot be Sentence fragments like this are why I don't like the "trait names must not be adjectives" opinion. Even as a native English speaker and someone who knows a bit of rust, I find the resulting grammatical mess obstructive. For a non-native speaker who's trying to get started (such as the OP) I imagine it would be even worse.
&gt; one should know the difference between self, &amp;self, and &amp;mut self, and use the appropriate one. Sure, but the advice is still a good one: default to `&amp;self` and only increase caller constraints (to `&amp;mut self` then `self`) when necessary.
`ArrayBase&lt;S, Ix1&gt;` is a regular type. The part before that is a [trait bound](https://rustbyexample.com/generics/bounds.html), it defines an arbitrary type `S` and specifies what is *known* (what must be true) about that type: `S` must implement the trait `Data` with an `Elem` [associated type](https://rustbyexample.com/generics/assoc_items/types.html) of `f64`.
I often use rust on a remote machine though ssh without graphical interface.
That's silly, a bigger monitor means I can have more text on the screen, ideally side by side, not longer lines. (Yes unfortunately I know there are people who maximize their editor, regardless of monitor size)
I think the perfect language doesn't exist, and furthermore I don't think it ever will.
Yeah definitely, and I'm in that group. If you look at my Rust code, all of it respects a 79 column rule. And it's not just monitor size, but font size, distance from monitor and one's ability to read the text in front of them.
Introducing linebreaks due to line length limitations.
Thanks, it is well explained.
Great. Rust is really improving fast. Since a long time I wished there where a `Lazy&lt;T&gt;` type in Rust, which takes a `Fn` to compute `T` (for example `clap` should accept a lazy as default value, so that a the value computation can be deferred). With lazycell it's now possible to write such a struct, but I wonder why it is not supplied by default. Is there any deeper reasoning behind that I overlooked?
Its not like the end of the impl period means that these project no longer need help nor provide extensive mentorship. Think about the impl period more in terms of call to arms and temporary change of pace and not some window of opportunity that just closed. For instance the cookbook has seen comparatively similar flow of PR's and new contributors prior and during the impl period (both amazing numbers). tl;dr New contributors are more than welcome and the mentorship is not going anywhere :)
Its not like the end of the impl period means that these project no longer need help nor provide extensive mentorship. Think about the impl period more in terms of call to arms and temporary change of pace and not some window of opportunity that just closed. For instance the cookbook has seen comparatively similar flow of PR's and new contributors prior and during the impl period (both amazing numbers). tl;dr New contributors are more than welcome and the mentorship is not going anywhere :)
If you have another computer with the same version of Rust then you can view the docs on that
It seems that you have changed the definition for one or more of the following terms: - exact - visual - flow 
My Python code follows the PEP-8 suggestion of using blank lines sparingly to indicate sections inside functions. Most of the time I do this is to separate blocks of looping code or conditionals from each other. Lisp is not the same because the closing paren is rarely a single one. Usually it‚Äôs a few and sometimes a fairly large amount. In non-Lispy code, I try to avoid that kind of deep nesting.
Is this different from `Future` (in `futures-rs`)?
The termination of a block is always the same horizontal distance as the body of the block. In Python, this is implicit because white space represents the terminator. In non-whitespace languages Ratliff makes it explicit.
Oh definitely. The impl period just felt like it made things more accessible with the various teams, mentors, and issues available in an easy to digest form.
If you don't see how many more blank lines with braces in them are not "the exact same visual flow as Python", we are never going to convince you. Python flow is great without braces. When you get into a braces language, "Python flow" is NOT the answer. It makes you work harder mentally to parse the code. 
I think the typical answer for this is: anything burntsushi writes :-)
&gt; Is this different from Future (in futures-rs)? I don't know how futures in Rust work, so I can't answer that one. The Rust `Lazy&lt;T&gt;` would be the same as a [C++ future](http://en.cppreference.com/w/cpp/thread/launch) with the bitmask set to `std::launch::deferred`.
I've been writing code this way for a long, long time. I don't particularly care that you don't like it. I was originally just saying that I wished rustfmt supported it. Don't worry, I'll use rustfmt in whatever config a project uses to contribute my code back, you don't have to see what I like to see. I honestly don't understand why people get so grumpy over this sort of preference.
So I have very little experience in any sort of embedded development so this approach might not work but to me it sounds like you are trying to "make a polling operation non-blocking". Framed that way it sounds to me like you want tokio/Futures. Here's the Future trait: https://docs.rs/futures/0.1.17/futures/future/trait.Future.html The only function you have to implement to implement the trait is `poll`, which sounds like what you want. So then I think you'd have a `InterruptHandler`(or something) that implements `Future` and you'd use the combinators on Future to call your callback when the state changes. 
If you're so intent on formatting curly braces into a Pythonesque layout, [this person](https://www.reddit.com/r/rust/comments/5sszeo/convincing_the_borrow_checker_when_theres_no_tce/) had an... interesting... take on the matter.
[I really like to meditate to this code](https://github.com/koute/stdweb/blob/9ba27f1c317db65049f675834485d3d12405e1f3/src/webcore/macros.rs)
Do you want to "consume" `Bar`? You can implement `std::iter::IntoIterator` for `Bar` which will take ownership of this `Bar` and return an iterator. Note that this means `Bar` is gone once you are iterating. My guess though is that since you've wrapped the `Vec&lt;Foo&gt;` in a `Mutex` you don't want `Bar` to be "consumed" by iteration. This becomes much more tricky, can you give more detail about the problem you are trying to solve?
Anything other than Python or JS is just a long tail of choices with roughly the same popularity. This isn‚Äôt particularly surprising, as those two languages are the most popular ones among people just learning to code.
Pushed back until next year. It *will* happen though.
Start learning to use combinators and functional programming techniques. These are valuable to write powerful Rust code that isn't needlessly verbose and often take advantage of Rust's ability to avoid copying data without gumming everything up with lifetime issues and unnecessary branching. A bunch of and_then() and map() calls might turn a bunch of nested if and if let statements into a "one liner" that reads like English.
For performance reasons you may want to take a look at ArrayFire. They have Rust bindings. It would take less code and give high performance cpu and gpu support.
Yeah, the `// Abandon all hope, ye who enter here!` really gives it a harmonious vibe.
This line is but a reminder of that last spark of hope that existed for the previous 204 lines
Well, it's all just anecdotes here. We don't really know how small of a minority it is. This thread certainly won't make others who like the style speak up.
Check out `clippy`. It often suggests slightly more auspicious ways of doing small bits and pieces, sometimes using features I didn't realize existed. Also just reading through stdlib docs was something I found very helpful. Even if I don't remember exactly which one I want at any given moment, knowing that there *are* nice combinators for things like `Option` and `Result` often makes me change the way I use them.
Looks like you guys cut out a lot of work for /u/steveklabnik1
Congratulations to the mighty and virtuous!
If you want to hold pointers to the data from multiple places rather than cloning, you can use Rc&lt;Layer&gt; to get the same sort of semantics as a GC language. To add mutability you need a RefCell, but I find that usually data can be immutable/copy-on-write so Rc is enough.
Yup. Luckily, I've got some time.
I guess it ist training and trying to refactor your own code. Lately I had a occasion where I thought, yes this is a much more rustier design. I will share this, even if there may be still better ways to to this. It was about a little file based database: It could have cunks of data which can be on disk or loaded into memory. force_to_memory() reads from disk to self.data update_no_sort() allows a closure to update self.data First iteration: // Chunks are days struct Chunk { path: PathBuf, /// This data is always newer that the disk data, except if data is None /// also: Data is always sorted data: Option&lt;Arc&lt;Vec&lt;Data&gt;&gt;&gt;, /// If Some, then this is on par with data serialized: Option&lt;Arc&lt;Vec&lt;u8&gt;&gt;&gt;, disk_is_up_to_date: bool, } fn force_to_memory(&amp;mut self) -&gt; Result&lt;(), Error&gt; { if self.data.is_none() &amp;&amp; self.disk_is_up_to_date { let file = File::open(&amp;self.path)?; let size = file.metadata().map(|m| m.len()).unwrap_or(100000); let mut buf_reader = BufReader::new(file); let mut contents = Vec::with_capacity(size as usize); buf_reader.read_to_end(&amp;mut contents)?; let mut de = Deserializer::new(&amp;contents[..]); let desered: Vec&lt;Data&gt; = Deserialize::deserialize(&amp;mut de).unwrap(); self.data = Some(Arc::new(desered)); } else if self.data.is_some() { // ok } else { bail!( "No data, but disk not up to date!? ({})", self.path.to_string_lossy() ); } Ok(()) } fn update_no_sort&lt;F, R&gt;(&amp;mut self, f: F) -&gt; Result&lt;R, Error&gt; where F: FnOnce(&amp;mut Vec&lt;Data&gt;) -&gt; R, { self.force_to_memory()?; self.serialized = None; self.disk_is_up_to_date = false; if let Some(mut data) = Arc::get_mut(self.data.as_mut().unwrap()) { // there was only one owner return Ok(f(&amp;mut data)); } use std::ops::Deref; let mut cloned_vec: Vec&lt;Data&gt; = self.data.as_ref().unwrap().deref().clone(); let r = f(&amp;mut cloned_vec); self.data = Some(Arc::new(cloned_vec)); Ok(r) } Second iteration. This uses if-let -&gt; cleaner control flow. It also uses lifetimes to safely return a reference to self.data so that update_no_sort does not need to unwrap the Option again. And it benefited from deeper look into the documentation to find Arc::make_mut(). fn force_to_memory(&amp;mut self) -&gt; Result&lt;&amp;mut Arc&lt;Vec&lt;Data&gt;&gt;, Error&gt; { if let Some(ref mut data) = self.data { Ok(data) } else { if self.disk_is_up_to_date { let file = File::open(&amp;self.path)?; // same as above: Reading and deserialization self.data = Some(Arc::new(desered)); Ok(self.data.as_mut().unwrap()) } else { bail!( "No data, but disk not up to date!? ({})", self.path.to_string_lossy() ); } } } fn update_no_sort&lt;F, R&gt;(&amp;mut self, f: F) -&gt; Result&lt;R, Error&gt; where F: FnOnce(&amp;mut Vec&lt;Data&gt;) -&gt; R, { let r = f(Arc::make_mut(self.force_to_memory()?)); self.serialized = None; self.disk_is_up_to_date = false; Ok(r) } I think knowing the apis of the most used types is very important (I once started a [cheatsheet](http://phaiax.github.io/rust-cheatsheet/), but it is may be outdated). Also try out different designs (that means function types / data structure / data flow / more .map(), ..., more iterative).
I wonder how much of these should be reconsidered, given the language's evolution since the RFCs were approved.
&gt; Luckily the diff between versions is not that large anymore, so you can probably just have the latest installed. There are minor changes in the resulting formatting of `rustfmt-nightly` introduced every week/2 weeks. My experience from other projects is that PRs from contributors with a 1 week old rustfmt-nightly typically break CI. The path that we follow is to just request the latest nightly everywhere to make it as painlessly as possible for contributors. We haven't been successful with asking people to keep multiple binaries around / using the right binary when contributing, and at this point we actually allow rustfmt builds to fail and merge a fmt only PR every week / two weeks more or less. It would be way better if we could just fix a particular rustfmt-nightly version and cargo would understand this and would automatically fetch the right nightly toolchain required to compile that rustfmt version, and automatically use it.
Came to say this :p
Yup that's a good point! 
Yup it depends on workflow too :)
You can edit `rustfmt.toml` individually so yes :)
&gt; Do you want to "consume" `Bar`? Unfortunately, no. Had it been like that, it would be easy. &gt; What would you want to happen to the iterator if the Vec is modified by another thread while it is still iterating? I'd also want to implement something like an iterator struct that holds the lock of the mutex and that the iterator is also in the same iterator struct (or in some way) so that when you drop the iterator struct the lock is also released. But lifetimes and borrow rules...
&gt; If you enjoy one-liner solutions, then Rust has the ability to do just that. Rust is not ideal for one-line solutions though. 
I didn't get what I was hoping for from this. Might match expectations better if it was titled "Guidelines for When to Port."
&gt; Well, it's all just anecdotes here. PEP7 and the CPython codebase are not anecdotal evidence, and many "native" projects in the Python ecosystem follow along with PEP7 (e.g. Numpy).
https://locka99.gitbooks.io/a-guide-to-porting-c-to-rust/content/
Can you reuse the storage? I'm pretty sure most of the overhead is in the allocation of the vector's backing storage, so if you're able to avoid allocating, or alternatively, allocate ahead of time, you might be able to get away with using a vector.
Java, C, and C++ are immensely popular and widely used. I think it's a reasonable assertion to say that Java is actually used more widely than Python. So, I'm not sure how these options are in the long tail of roughly similar popularity that is after JavaScript and Python.
Yup, it's pretty good. But note that it's not powered by RLS.
That isn't what we're referring to with "infra".
Oh now we're cooking with gas. I dunno if its just me or not but I did not expect "Read" to lead to more. Stand-alone the Introduction feels quite stand-alone and final.
How isn't it anecdotal? How does that give you a quantitative answer on how many there are? Anyway, I'm more concerned about people already in a minotiry also being put in a place of irrelevancy. For the record, I also find that style less readable, but not any less than the Lisp-in-C style that was prevalent for Rust for a while.
In this case the closures have no captures, so you could write the type. With some assistance from the compiler, it is: use std::iter::{Filter, FilterMap}; use walkdir::{DirEntry, IntoIter, Error}; fn get_targets(root: &amp;str) -&gt; Filter&lt; FilterMap&lt; IntoIter, fn(Result&lt;DirEntry, Error&gt;) -&gt; Option&lt;DirEntry&gt;&gt;, fn(&amp;DirEntry) -&gt; bool&gt; { WalkDir::new(root) .sort_by(|a, b| a.file_name().cmp(&amp;b.file_name())) .into_iter() .filter_map(Result::ok as _) .filter(|e| e.file_type().is_file()) } However you shouldn't do this anyway as it leaks implementation details of how you wrote the function.
I'm sorry for being so negative, but I cannot stand her talks. She's a terrible speaker, terrible presenter and she's got nothing to say and nothing to present. What's the point of inviting a daydreamer who's achieved nothing of significance in the PL community to throw their "happy engineering" ideas at everyone?
yeah its so much nicer to pad things out with more angle brackets and nesting levels, rather than writing nice clear expressions like "lerp(a,b,f)=(a+b)*f+a" I mean I can't tell what thats doing unless I nest it a bit more, and write 10 lines of trait bounds first 
ok fair enough.. only within a module probably would cover a lot of the cases I really want, but then again simple helpers would be nice. I'm actually wondering how much effort it would be to try and modify the compiler to convert such functions into macros.Not sure what would happen with error messages though (I already worry about IDE support with macros) but Rust macros are popular so there would be apetite to see whats expanded Some might say "if you want a macro, write a macro", but it's so much nicer to be able to cover a wider range of behaviour in one syntax; as jonathan blow explains 'code goes through a maturation cycle', and the fewer syntax changes you need the better. I do expect you'd need to add more types as things progress, you dont want to commit early.. you need it in a fluid form
Try again in /r/playrust
I *think* that that is essentially `Future` with [`future.wait()`](https://docs.rs/futures/0.1.17/futures/future/trait.Future.html#method.wait). I believe that `wait` is safe to call multiple times. It would require a bit of work implement the future trait for some value, but you could do it. In general though that seems like it's just a struct with some optional value? [Basically](https://play.rust-lang.org/?gist=948fac178b99937d1813b69efa15e8be&amp;version=stable) ``` struct Lazy&lt;F, T&gt; where F: FnMut() -&gt; T, { val: Option&lt;T&gt;, f: F, } impl&lt;F, T&gt; Lazy&lt;F, T&gt; where F: FnMut() -&gt; T { fn new(f: F) -&gt; Lazy&lt;F, T&gt; { Lazy { val: None, f: f, } } fn get_ref(&amp;mut self) -&gt; &amp;T { if let None = self.val { self.val = Some((self.f)()); } match self.val { Some(ref v) =&gt; v, None =&gt; unreachable!(), } } } fn main() { let mut lazy_int = Lazy::new(|| 1); println!("lazy.get: {}", lazy_int.get_ref()); println!("lazy.get: {}", lazy_int.get_ref()); let mut lazy_str = Lazy::new(|| format!("hello there: {}", lazy_int.get_ref())); println!("the str says: {}", lazy_str.get_ref()); } ```
Yes, it is somewhat obscure.
Are you sure you posted the right picture there?
Thanks for all the hard work, Steve!
That really felt like a "goodbye, community!" for a few paragraphs there.
Same here. My guess is that there is a bug in the latest rls/nightly combo, but I don't really know.
Blog post is up! https://blog.rust-lang.org/2017/12/21/rust-in-2017.html
My heart stopped for a few moments. Fortunately my brain still had enough oxygen to read to the end and restart my heart.
I was so worried that was going to a be a departure post! You really make such a difference around here. When I picked up Rust last year I'd never had such an easy and fun time learning a new language as with your documentation (the first book and intermezzos) and your seeming omnipresence answering questions on all community platforms at all times. Cheers to another five!
Right and there is continuous work to make the contribution more friendly and accessible. One can always find an up-to-date list of mentored easy issues here https://www.rustaceans.org/findwork
I think the link "order crates by the number of downloads in the last 90 days" should be pointing to https://github.com/rust-lang/crates.io/issues/702?
I‚Äôll fix it, thanks.
We don't get grumpy of your preference. We get grumpy when you state that ALL PYTHON DEVELOPERS should have this preference.
Ok, maybe this is where we‚Äôre having a problem: my position is that Python is intrinsically Ratliff style. It can be nothing other than Ratliff style base on my understanding of what Ratliff style is and the syntactical rules of Python. You clearly don‚Äôt agree on that part. Which indentation style would you say Python most closely adheres to?
You know shit's about to get real when you see `#![recursion_limit="1500"]`
Your implementation is quite similar to mine, but I use `lazycell` to hide the mutation. Having a dedicated type for a lazy computation makes the intention clearer ‚Äì to me its a basic container like `Option`. Also using futures just to get a lazy evaluated value might be a bit overkill. If there is interest I could publish my code on GitHub (and probably make a crate out of it).
Hey all, I don't wanna spam comments with just "&lt;3", so please consider this a collective reply. I can see how some may have read some of this as me leaving, but I can assure you, I'm not. I'm more committed than ever.
Looking at the incremental compilation graph- it looks like full builds with incremental compilation get faster in release mode, but slower in debug. Do we know what causes that? Is it debug info? Overhead from the new approach (but then why would release be faster)?
Just recently picked up Rust after a frustrating session with cross-platform C++ build systems; I was surprised with how effortless it was to get tooling setup. The way Cargo and Rustup tie the Rust ecosystem together is a quality of life multiplier. Thanks /u/steveklabnik1 and everyone else contributing, I think you've got something special going.
The simple thing to do is just run the poller in a new thread.. it is as simple as `thread::spawn(|| { do poller stuff });` I've included a sample of what you're specifically requesting, and that is running a callback for each polled value (or failure) However, Rust gives you some better options. 1. You can put the data into an mpsc queue `std::sync::mpsc` and then just read from that queue. Also, sysfs-gpio offers an even better solution, and that is the ability to poll many pins at once using mio or tokio. In the long term, I'd recommend that, because odds are you aren't always going to want to poll just 1 pin. extern crate sysfs_gpio; use sysfs_gpio::{Direction, Edge, Pin}; use std::env; use std::io::prelude::*; use std::io::stdout; use std::thread::{self, JoinHandle}; fn run&lt;F&gt;(pin: u64, mut cb: F) -&gt; JoinHandle&lt;()&gt; where F: FnMut(Option&lt;u8&gt;) -&gt; bool + Send + 'static, { thread::spawn(move || { let input = Pin::new(pin); input.with_exported(move || { input.set_direction(Direction::In)?; input.set_edge(Edge::BothEdges)?; let mut poller = input.get_poller()?; loop { let value = poller.poll(1000)?; if !cb(value) { return Ok(()); } } }); }) } fn main() { let args: Vec&lt;String&gt; = env::args().collect(); if args.len() != 2 { println!("Usage: ./interrupt &lt;pin&gt;"); return; } let pin = args[1].parse::&lt;u64&gt;().expect("Arg should be a number"); let hdl = run(pin, |gpioval: Option&lt;u8&gt;| { if let Some(val) = gpioval { if val == 42 { println!("I've found the answer. {}", val); return false; } } println!("No answer yet"); true }); hdl.join(); }
&lt;3
So I played around with this in the playground and got to here: https://play.rust-lang.org/?gist=d08184178afa40942eb7de1c743fb58f&amp;version=stable Which doesn't compile b/c of lifetime errors, and which isn't very idiomatic either. I can't help thinking there must be an easier way to do this, why do you want to do all this mutex stuff?
I was able to fix it by setting "rust-client.channel": "beta" Then VS Code prompted to install RLS from the beta channel and seems to be working fine now. 
I believe the problem is `Mutex::lock()` returns a scoped guard, so that guard needs to live longer than the total iteration. In essence, you want structs set up to do the following: let lock = b.mutex.lock(); for foo in lock.iter() { ... } // lock is dropped here This means that `b.iter()` must return something which keeps the `MutexGuard` alive longer than the iteration. I was able to do this by implementing a `BarIter` struct that holds the `MutexGuard` and implementing `IntoIterator` for `&amp;BarIter`: https://play.rust-lang.org/?gist=083f85cd6e564b4c9abda0dbf3a33010&amp;version=stable
You'll also need beta installed for this to work, so run rustup toolchain install beta-&lt;your host of choice&gt;
Just tried that; no more crashing, but it does not function either. I hope I can find a fix soon.
I notice that the nightly from 12/18 segv'd on OSX, which didn't break vim, but it definitely slowed down the lint extensions :/
Using incremental compilation also enables multiple codegen units, which is known to have a significant effect in optimized builds and very little positive effect on non-optimized builds. Source: https://internals.rust-lang.org/t/help-us-benchmark-incremental-compilation/6153/41
You can also set it to e.g. "nightly-2017-12-20" to get a specific nightly
how can anyone respond to this sort of subjective trash talk? it seems more like a personal issue.
This is cool to see. For a while I've wanted to do a side-project writing a MCTS AI for "Fresh Fish", and *maybe* I'll get started during the holiday break, perhaps using your code. Just glancing at the boardgame-rs crate, I'm curious if you could make the traits (State, etc) generic, rather than requiring them to be u32s? Not only is the conversion slightly awkward, but more importantly, it means that you can't have parametrized Actions. In Agricola all the actions are distinct, but in other games there might be a "Bid(u32)" action, or a "Trade(Player,Resource)" action, for instance. For Monte Carlo search, you might then need to be smart about how to generate random actions that populate the parameters. Also, it'd be interesting if the actual loop that drives the MTCS were to be part of the boardgameai-rs crate, instead of requiring each game to implement it. (Sorry, no feedback on your actual questions -- I'm still learning Rust myself.)
Great work everyone :) What I would love in these "*retrospect blog posts*" is a list of stuff that didn't work out so well. In which areas didn't we proceed as fast as expected/hoped? What were big difficulties? Nothing is perfect and an honest mention of problems and rough edges is always something that makes a report more trustworthy IMO.
It's not about "Rust's current syntax is wonderful". It's about keeping Rust from slowly acquiring a pile of alternative syntaxes for various functionality over its lifetime. (Rust's design philosophy takes a lot of inspiration from Python, including the "There should be one ‚Äî and preferably only one ‚Äî obvious way to do it." motto, which runs directly counter to Perl's [TMTOWTDI](https://en.wikipedia.org/wiki/TMTOWTDI) (There's More Than One Way To Do It).) This is **far** less justifiable than some of the ideas I've had which were dismissed as unlikely to meet the standard for adding new syntax.
Good points for sure! If you do end up forking, let me know. I'd be very curious to see what your thoughts are after digging in a bit more in the code itself. I agree on the boardgame-rs crate making the traits generic. I'll have to do a bit of rearranging, but it seems the better thing to do.
If you're really committed, you'll respond to this comment with "&lt;3"
What I do when there's a risk of something being taken that way is to tweak the title. For example, "Five years with Rust (and counting)". Concise, yet effective, even for people just skimming through titles.