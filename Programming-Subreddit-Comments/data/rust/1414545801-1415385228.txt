I respectfully disagree and submit that this is a question of semantics. All of the functions like unchecked indexing are explicitly marked unsafe, meaning they are impossible to call in safe code; thus, the interfaces are completely safe. The ability to segregate safe and unsafe code like this is precisely what makes them safe. That is why I felt like it was a somewhat weak claim by itself--the same thing could be easily done in practically any language with a zero-overhead C FFI. That said, most languages don't provide a language-level way to create interfaces like this, so it's not possible to expose, for example, an arbitrary C FFI function safely in a language like Ruby.
https://github.com/carllerche/mio
Every flag in rustc already serves a usecase, and not all of them are exposed by Cargo (and doing so with unique flags would be kind of silly). I don't understand the purpose of deliberately restricting access to them. Easy example: all the -Z options. I can always copy the rustc command line out, but it's really annoying--especially since Cargo has a tendency to delete different versions of dependencies when I run it with different options (e.g. test vs. release). I don't see how having a nice set of common flags for common cases is incompatible with having an escape hatch.
It says this in the F1 " Resynchronize clock is still in progress. please wait 2000 ms, it fluctuates between 200 and 3000ms.... 
Wrong subreddit, try a) glancing at the subreddit's side bar before posting, and b) going to /r/playrust
Yeah, that's bullshit. Someone once said that it took 1 second to scan 1Gb of live heap on an EC2 machine. Minecraft doesn't even get close to that figure.
Ok, I just tested it. TreeMap works! You were right, Im guessing it does have something to do with the heap allocation. Do we need to submit a bug about this?
Now that I think about it, that was on my old Athlon 64 (old dual-core machine). Significantly less powerful than most newer machines. It ran Minecraft fine, except the GC took forever. If Minecraft were written in a language without GC then it would've run much more smoothly. Either way, unexpected 1 second delays between frames is intolerable. It reduces your average fps by 20% (assuming 4 seconds between collections and 1 second per collection) but has a far greater impact on playability.
Really nice interview with measured, thoughtful answers. Thanks for the care you put into this, @steveklabnik!
Oh, so you were actually saying that Rust allows one to expose completely safe APIs, and also APIs that have the full power of C (with some overlap). I think it is extremely silly to say that an API that allows for memory-unsafety via functions marked `unsafe` is "completely safe"; at the very least, it is misrepresenting the situation to attach "with the full power of C" to it, as people *will* get the wrong idea. Saying &gt; Rust allows one to offer APIs that are safe by default, with an escape hatch to obtain the full power of C. seems more reasonable.
I like that idea. It could extend really well to other kinds of managed effects which we would want to consider.
Yeah, that's fair. That seems like the best way to get across the point in a way that doesn't shortchange Rust.
Thanks! Commas always get me. I fixed it.
Correct and correct. 
Even better, make it like LaTeX so the compiler actually asks you for a response after an error. "Can't move out of dereference of &amp;-pointer, whatcha gonna do about it, huh?"
True, but most of the time Minecraft only does minor collections, which is closer to 10ms than 1s. Even when performing a full collection, Minecraft doesn't have 1Gb of live memory, so collection times should be significantly less than 1s. Remember, a GC only scans memory in use, so if you have alot of garbage, which is the case in newer versions of Minecraft due to use of immutable classes, collection time doesn't take long. Also, how can it run Minecraft fine with 1s collection time? When the GC collects garbage, the entire appliction stops for the duration of the collection. 1s pause every four seconds is, nicely put, completely unplayable.
&gt; True, but most of the time Minecraft only does minor collections, which is closer to 10ms than 1s. Even when performing a full collection, Minecraft doesn't have 1Gb of live memory, so collection times should be significantly less than 1s. My experience suggests that this is false. I said 30 seconds because I actually timed it. That computer admittedly had a very outdated CPU. &gt; Also, how can it run Minecraft fine with 1s collection time? When the GC collects garbage, the entire appliction stops for the duration of the collection. 1s pause every four seconds is, nicely put, completely unplayable. What I meant to say was that it runs fine between collections. Overall it's very unpleasant to play due to the huge GC times. If I remember correctly, the memory usage dropped from 99% usage to around 250 MB during the collections (according to the in-game RAM counter). I don't recall if I had a 1 GB heap or if I enlarged it -- I think I had it set to 2 GB.
Would inline anonymous enums discourage this kind of abuse? example struct StateMachine { ..various fields.., is_foo:bool, // Maybe he did this to keep everything in one place. is_bar:bool, // avoids creating another name and having to navigate is_baz:bool, } struct StateMachine { ...various fields... state:enum{Foo,Bar,Baz}, // All in one place - no excuse not to. // Its actually more compact. 3 'bools' replaced with 'state','enum'. // 1 line replaces 3 } I realise Rust enums are generally rather useful for state machines, and the lint might educate people were there some other discussions about unifying some types of struct inheritance with enums or something (a bunch of shared fields + varying ones).. also making Foo|Bar|Baz a way of rolling an ADT based in types defined elsewhere... imagine if enum Foo{Bar(),Baz()} was sugar for something more general (struct Bar(),struct Baz, type Foo=Bar|Baz) 
I think that compile-time i18n isnt as necessary as it seems cool, becuase sometimes people want to choose at runtime which language they want. There was a thread about this a couple months ago i think. However, a nice DSL using the macro system could make i18n and l10n nice :) ninja edit: heres the thread! http://www.reddit.com/r/rust/comments/2g2qvl/would_compiletime_i18n_make_sense/
My 2 cents: * Please don't reinvent the wheel. There are tried and tested formats for I18N (e.g. PO files for gettext, java message bundles), with proven tools to manipulate them. * I18n doesn't end with printing plain strings. There are things like date and number formats, pluralization, etc. * I suspect having the choice to do a subset of I18N stuff on compile time, even if it's just a lint that checks if the message files are present (edit: not pressent), can be beneficial. On the other hand, the flexibility we can get with runtime I18N is not to be underestimated; and the performance cost is usually negligible.
Meanwhile I have read what was actually currently defined in the documentation. However, if not even a simple OpenGL program can be written without placing unsafe everywhere, I fear most average Rust developers will just place their program in a gigantic unsafe block. 
Thanks for the link. Excuse my ignorance, but this is just an RFC. I thought multi dispatch was implemented in the stdlib when I read your post. So there is still some time before it's actually implemented.
Comment from FrancisGagné on Stack Overflow: &gt;The type of s1 is wrong: it says `fn(&amp;PluginSystem)` but the actual function takes the `PluginSystem` by mutable reference. You'll need to change the call to `s1(&amp;mut self.ps)`, too. Good suggestion, made the changes, but it didnt fix my HashMap::new() problem. 
Hmm, it would seem we have different experiences then. I've run Minecraft at lowest settings on a Intel Atom powered machine, and while you could notice the GC pauses, it was still playable. This was probably with Minecraft 1.2 or something though, and the number of GC collections has increased since then from what I've read.
&gt; Does Rust have first-class support for state machines? No. You should probably use an enum for states (or multiple enums) but the transitions are done by hand. I'm reasonably sure there are SM macros out there built on top of that though.
[nope](http://www.reddit.com/r/haskell/comments/2i3ttm/tutorial_implementing_highly_efficient_data/ckz1zai)
The downside is that "for a long time" Rust will have an unstable, *poorly documented* macro system, which a lot of code indirectly relies.
The RFC has been accepted for a good while, and I know that support is at least partially implemented. Not sure of the overall state of its progress.
thanks for the correction!
Couldnt disagree more. Hammering home the idea that it is, is a terribly bad idea. We should totally be erring on the side of caution and 'rust improves safety in code', not either 1) telling people it is (its not) or 2) trying to make it so (which would effectively doom it as unusable for low level operations). Safety in rust is an incitental perk of the memory model, and it helps prevent bugs, thats all...
This was a short template-to-macro conversion session *turned* brain dump of how I write macros. I wouldn't normally post my own drivel, but I suppose with the relative scarcity of Rust articles, it might be helpful to someone. Suggestions and comments welcome, naturally.
5) Just use Cargo to manage the Rust packages 6) Hope that someday everything will be written in Rust, then get rid of paludis, portage, apt-get, yum or any other OS package manager 7) Truly live happily ever after NB You may skip number 6
When it comes to build systems and package management systems, or anything that benefits from Network Effects, **The best solution is the one we all use.** Cargo does need help now, especially when integrating with c libs with non-standard build options. But it will get there. It is being very actively developed and the more you contribute to it, the faster it will get there. 
Custom macros will not be part of stable Rust until the macro syntax is stable, though. A level of under-documentation is one of the costs for unstable things. To be clear, it's "I would rather spend my limited time documenting things that are stable," not just "macros are unstable." I should have worded that better.
The issues tracker of the RFCs repo is a good place, as is Discuss.
Did you mean to respond to someone else? I didn't mention safety once in that post.
I think we've all worked with or inherited code from someone who liked booleans a little too much. Cute plugin :)
&gt; I definitely feel like cross-borrowing should be something that's considered prior to 1.0, As the notes kind of mention, 1.0 is always an issue of 'must this happen before, or can it happen after?' One of the three proposals (the 'borrow operator' proposal) is backwards incompatible, the other two are backwards compatible. So yes, while cross-borrowing is certainly important, if it can be added in a 1.1 release, it's not as urgent as things that cannot.
It's a valid concern, but it doesn't happen in practice, because you can do a *lot* in safe code. That's how Rust gets away with calling so much stuff unsafe; even where you need C speed, you can often expose a safe interface. The number of cases where an operation legitimately can't be made safe by Rust's type system is always shrinking, and it is quite possible that with some of Rust's newer features the unsafety here will be rendered unnecessary as well. There will still always be unsafe interfaces exposed for times when you need that last ounce of performance or need to interact directly with hardware, but it will be very rare to actually *require* the use of unsafe.
Cargo needs a lot, and I mean, a lot of help on the Windows-side. I mean, the binary is wrapped up in a tarball for crying out loud--we use zips, damn it! ZIPS!
You are correct. I guess I should have said something more along the lines of that a proposal should be picked before 1.0 since one of them is backwards incompatible. That way it can be decided whether or not if cross-borrowing should be implemented before 1.0 or after 1.0. I definitely understand that there's higher priorities than cross-borrowing and it's not that big of an issue to write out `&amp;*` to borrow out of a smart pointer. It just looks ugly. :P One thought I did have is that if borrow operator overloading were added, why not use some other glyph for that operator? The major downfall is that it is inconsistent with the rest of borrowing in the language, but it does allow for borrowing as it is now to continue working as such, but allow for borrowing of smart pointers.
Cool. :) That's basically how the discussion went. &gt; why not use some other glyph for that operator? There's a proposal to do that with `~`, iirc, but Rust already takes a bunch of trolling over the amount of punctuation we have, and we've been steadily reducing it over time.
Yes, but that is quite different from the issue being discussed here. Changing the active OpenGL context doesn't have anything to do with squeezing the last drop in performance from the hardware. Rust's concept of safety is deeming as unsafe what is considered safe in other systems programming languages. Well I'll look forward that Rust type system makes such operations more user friendly, after all lifetimes are much better now.
Fully understandable. I got into rust when `~` and `@` were still a thing. I liked them because they quickly set off the different pointer types, but now that we are at a stage where they no longer exist... I actually like rust better now than I did before. Especially with the decision to just make smart pointers into library types. It means that if a library has to introduce a new smart pointer type (e.g. COM libraries) then it really isn't that huge of a deal because the language will already have support for writing smart pointers as libraries and users of a library will already be familiar enough with the concept of a smart pointer that they won't be overly surprised by the concept. With that said... I'm really on the fence in regards to another operator for cross-borrowing because of the extra punctuation it adds. It is certainly shorter to write out `~` than it is to write `&amp;*`. However it now makes smart pointers even more special than they really need to be and users of the language have to remember whether or not if they need to use `&amp;` to borrow something or use `~` to borrow something. Honestly... as I type this out and make myself think about it more... I don't know if an operator overload in this scenario is the best idea. For the reasons mentioned in the meeting notes in regards to overloading `&amp;` and then my own thoughts on another borrow operator. Anyway, I'm not sure what else I can really contribute to the discussion. You guys seem to be really on top of it at this point.
Well, that's the binary for uploading to the repository, a future `cargo install` shouldn't/wouldn't use tarballs, I'd imagine. That said, people with Windows knowledge are extra valuable. I'm sure they'd love your help.
It is a performance concern. As pcwalton pointed out elsewhere, if you were willing to take the performance hit and check the global context each time you did an OpenGL operation, you wouldn't need the unsafe block. I would have to look into it more carefully to see what the actual issue is with the current operation, but it's possible that it could be made safe with little overhead, but it hasn't been yet because people would rather expose the features first (even if unsafely) and then work to make them safe. In any case, it seems to me that the issue here is that OpenGL itself uses global, mutable state in a way that isn't generally safe (that is, is only safe if you follow very specific rules)--Rust is just being honest about that fact.
Haven't read the article yet, but... was [this](http://www.reddit.com/r/rust/comments/2ba3az/macros_writeln_and_the_semicolon/) solved?
I agree, with time passing I use unwrapped primitive types less and less, and instead use lightweight (0-cost abstraction) wrappers with only meaningful functions defined (substring on an ID does not make sense...). With good macros to create said wrappers easily, it gets as easy as a type alias, but is much safer. It can be frustrating some times though.
Ohhh I thought they were talking about the output of `cargo package`. Yes, I would imagine that it would be better. I believe one of the reasons it's still like this is that brson (iirc) is working on a unified rust/cargo installer, so the current infrastructure is just sitting there.
As part of the piston umbrella we are working on a pure-rust implementation of the most common image formats. At the moment it is mainly used to load game assets (like for example [hematite](https://github.com/PistonDevelopers/hematite)) so you can expect „normal“ 24-32 bit RGB images to work best. We are open to input/wishes concerning the interface and happily accepting pull-requests with enhancements/bug-fixes. ;) # Rust Image 0.1 Release Notes Rust image aims to be a pure-Rust implementation of various popular image formats. Accompanying reading/write support, rust image provides basic imaging processing function. See `README.md` for further details. ## Known issues - Interlaced (progressive) or animated images are not supported. - Images images with *n* bit/channel (*n ≠ 8*) are not well supported. ## Changes ### Version 0.1 - Initial release - Basic reading support for png, jpeg, gif, ppm and webp. - Basic writing support for png and jpeg. - A collection of basic imaging processing function like `blur` or `invert`
&gt; we are working on a pure-rust implementation of the most common image formats. I am really psyched to see stuff like this &lt;3
Typo. In: `F​n​​=0,1,...,F​n−1​​+F​n+2​` `Fn+2` should be `Fn-2`
You should get a "piston" flavor!
great stuff! any plans for tga or tiff?
If you find the syntax of Erlang offputting, but want to use "the leader for building resilient software", there's Elixir which is Rubyish syntax on top of the Erlang VM. Otherwise, good interview. Good comparisons all around.
ccgn and nwin deserves most of the credit for this project. They have worked on this since May this year, and ccgn started the project even earlier before that, as a way of learning Rust. At the moment there are 9161 lines of code. The idea of the Piston project is to share the work of maintaining important libraries. All projects under Piston are worked on in parallel, with many goals and many people involved (40 at this moment), yet we have managed pretty well so far. There are plenty of places to start contributing, and experience in Rust/Github is not required. We give everybody write access to all the repos, and even if you are not working on a single project, you can help out with keeping the libraries up to date with Rust nightly, add unit tests, write some docs or tutorials etc. See [how to contribute](https://github.com/PistonDevelopers/piston/blob/master/CONTRIBUTING.md). In addition you can participate in discussions, collect research and come up with your own ideas. If you start your own project, you get full control over it, like you have in a personal repo, but with the exception that more people are likely to help out, brainstorm, fix bugs etc. If you are interested in gamedev with Rust in general, there is a dedicated subreddit http://www.reddit.com/r/rust_gamedev/ where we keep an up-to-date list of useful libraries.
Any plans to incorporate this into Servo?
Great article. Learned quite a bit about not only macros, but more about rust itself. Found the section on packaging and distributing macros to be very useful.
In the future my team is planning to work on a port of l20n library to Rust (at least for Servo, but there's no reason not to make it work everywhere). L20n is way more powerful than what you are suggesting, and I believe in the value of simplicity, so I believe there is room for multiple different libraries for different purposes :) One reason to look into L20n is that it may help you design your API in a forward-compatible ways. As your library grows, you will want features that l20n supports so planning ahead to accomodate for them will reduce the pain of incompatibilities later (that's what we're going through as we move Firefox OS to l20n).
I'll leave it up to y'all which non-Rust forums this should be posted to :)
I usually feel very clever when I push some kind of behaviour into an Iterator, because they compose so nicely. So perhaps something like this (sorry about the spammy code; both my firefox and IE seem determined to block the XSS appears to be required for the 'share' button on the playpen to work): struct Pairs&lt;T: Copy, U: Iterator&lt;T&gt; + Clone&gt; { head: Option&lt;T&gt;, tail: U, next: U, } impl&lt;T: Copy, U: Iterator&lt;T&gt; + Clone&gt; Pairs&lt;T, U&gt; { fn new(mut iter: U) -&gt; Pairs&lt;T,U&gt; { let head = iter.next(); Pairs { head: head, tail: iter.clone(), next: iter, } } } impl&lt;T: Copy, U: Iterator&lt;T&gt; + Clone&gt; Iterator&lt;(T, T)&gt; for Pairs&lt;T, U&gt; { fn next(&amp;mut self) -&gt; Option&lt;(T, T)&gt; { match self.head { None =&gt; None, Some(a) =&gt; { match self.tail.next() { Some(b) =&gt; Some((a,b)), None =&gt; { match self.next.next() { Some(new_head) =&gt; { self.head = Some(new_head); self.tail = self.next.clone(); self.next() }, _ =&gt; None } } } } } } } 
I have posted it to [hacker news](https://news.ycombinator.com/item?id=8529330)
Just waiting for autotools to start supporting rust but I imagine most folks aren't as happy with that idea as I am. I think cargo is actually really good right now though if you are working only with rust packages and of course aren't using windows.
Actually we already have some rudimentary tiff support (grayscale+uncompressed). No plans for tga yet.
After spending a better part of a weekend detecting platform specific linker features properly in autotools, I'm very much agreeing with PHK: https://www.varnish-cache.org/docs/2.1/phk/autocrap.html Autotools is a huge set of cruft with ages of history and weird bugs, written in M4, which is probably only followed by TeX in it's weirdness. autotools might have a lot of community knowledge when coming from the C side of things, but I don't think it's a tool of the future.
Would be great. Unfortunately we don't support progressive or animated images yet. Especially the latter is crucial for the web. ;)
The double rightward drift of `match` on `Option` can be reduced with `if let`.
Ah, I see. I assumed it would just start all over after writing the image.
decode &amp; encode of DXT compressed textures would be a nice addition
I was recently asked this: &gt; Q: Would you recommend coding newbies to learn Rust or should they learn some other language like Ruby first? &gt; &gt; A: I think that they should use whatever gets them excited. Pick something, anything, and just try it. If you don’t like it, try something else. Humans are all different, and there’s no single answer here. &gt; &gt; That said, I think Ruby is a fantastic first language. Rust has a significant lack of resources because it’s so new, and so it’s much harder as a first language. and we had a few comments where I elaborated: http://www.reddit.com/r/rust/comments/2kkqjw/steve_klabnik_how_rust_compares_to_other/clmhwsq
(I believe HN's voting ring detector would rightly flag and remove that if a lot of people were to click on it and then upvote: direct links to HN posts are "dangerous".)
This also sounds (to me) like a good addition to the `std::slice` library... although maybe that would be best left until we have a time when we can do generics over integers, so that we could have impl ImmutableCloneableVector&lt;T&gt;{ combinations&lt;N&gt;(&amp;self) -&gt; Iterator&lt;[T ..N]&gt; { ... } }
Oh no. The output is perfectly fine. /u/fgilcher made it clear on what I meant. But that's a tiny detail that may slip up some Windows developers who doesn't possess any experience in using Linux. 
correct
Or you could install 7-zip / (MinGW|MSYS2|Cygwin) tar. Like a sane person would.
&gt; But be prepared to ask lots of questions on IRC :) Especially when something suddenly changes in the new nightly-build. 
Makes sense since there's already a `permutations` and combinations can reasonably easily be obtained by filtering permutations.
You'll suffer for a lack of beginner's documentation. There's a lot of stuff for people who already know how to program and who want to start using Rust, but not much for learning programming through Rust.
Wait, what? combinations can be obtained by filtering permutations?
Thats one aspect... I came across a situation today where I had some crypto dependencies and it actually knew to include -lcrypto and -lssl at gcc compile time. Many points for that. Unfortunately I did not have the required libraries but a quick google search and I installed openssl. Unfortunately that is not enough as gcc was looking for -lcrypto and -lssl but on windows open ssl they are called libseay32.dll and libssl32.dll. Once I renamed and dropped them where gcc could find it ( I do not know how to add to the rust gcc -L list) the compile worked. So the question is how do you map and account for the different shared library names between windows and linux?
We have a winner! No need to mess around with `v.head()` and `v.tail()`. 
I'd probably do it this way, as I always try to use maps, folds, etc. fn cartesian_product&lt;T : Clone, U : Clone&gt;(a: &amp;Vec&lt;T&gt;, b: &amp;Vec&lt;U&gt;) -&gt; Vec&lt;(T, U)&gt; { let result : Vec&lt;Vec&lt;(T, U)&gt;&gt; = a.iter() .map(|x| b.iter().map(|y| (x.clone(), y.clone())).collect()) .collect(); result.as_slice().concat_vec() } fn main() { let vec = vec!(1u, 2, 3, 4); let product = cartesian_product(&amp;vec, &amp;vec); println!("{}", product); }
Languages it *might* be worth learning beforehand, or alongside: C, assembly language, Haskell. All because they're simpler (in some way), and touch on some of the same concepts. (In Haskell's case, at least the basics - not once you get into more advanced techniques.) In "basic principles" terms, those might also be helpful. I would skip C++, or leave it until after. For someone who's not already familiar with it, it's probably much easier to learn (how to write good) C++ after first having learned Rust. Conversely, I also tend to recommend Rust as a stepping stone for people coming from C++ who want to get into Haskell. It all depends on where you're coming from, and where you want to go.
Ok, we need to copy and paste it in address bar to prevent sending the referer url
Or just find it on the front page. :)
Servo doesn't have support for animated images yet, either. So it's a perfect fit! :)
I definitely think that Haskell is a good stepping stone towards Rust, and vice-versa. Once the beginner docs are better, I think Rust would actually be better than C for learning systems programming. Assembly language is definitely worthwhile.
it might be helpful to also note (somewhere) that `rust_fail` still has the same name. It's useful for debugging in `gdb`. (Or rename it to `rust_panic`, which makes more sense, and leave a note about that)
Renaming it is what I'd do, but that's something for the conventions/cleanup RFCs.
I liked `fail!`. :(
[Submitted a PR](https://github.com/rust-lang/rust/pull/18442) -- doesn't seem like something for the RfCs, we're modifying a breakpoint name.
The biggest reason to not use `fail!` is that it makes it _harder_ to talk about "this function doesn't work" vs "this function can cause the task to crash." Post this PR, we have 'fail' for the first and 'panic' for the second. The vast majority of this patch was updating documentation to separate these two concepts.
Same here. For the packages with native dependencies it took [a lot of work](https://github.com/lifthrasiir/rust-opengles-angle/commit/8464d0ea5d4bb0775d5b6c29315df43b2f7fc4bb), but Cargo did work fine then. [Rust-encoding](https://github.com/lifthrasiir/rust-encoding/) had to keep `Makefile` only because I had a project depending on rust-encoding *and* using Makefile! Maybe I can safely remove that now.
How did you compile rust-image? It is not quite fast but not very slow either (up to 2~3x in my [non-scientific measurement](https://github.com/PistonDevelopers/image/pull/102) with PNG decoder). If you are using Cargo, you need `--release` to get the full optimization. (I'm not aware of the way to selectively enable the optimization for individual dependencies.) If you are using a plain rustc, eh, don't do that, but `--opt-level 3` is required.
&gt; I kept getting shadowblocked :( presumably because of the playpen link. Yup. Just ping the mods, they'll approve it.
As others have said, there are not many up-to-date resources. If Rust is not your first programming language, I can definitely recommend the [official Rust guide](http://doc.rust-lang.org/guide.html) (especially after it has been improved recently) and [Rust by Example](http://rustbyexample.com). If you run into trouble, feel free to ask on IRC or on this subreddit.
I added it to my `Cargo.toml`, as instructed in rust-image's readme. Thanks for the tip, I'll look this up.
Hey if we're within 80 characters, my eyes can almost forgive 6 levels of indentation. hehe. I'm not saying the logic is wrong, but when I see that much indentation my eyes spin around in my head. I prefer single checks with early returns, but a lot of people disagree with me on that. Since this logic is a little complicated, I would prefer to have multiple matches that return bools to match on subsequent, non-indented matches. Without converting the entire implementation, the idea is: impl&lt;T: Copy, U: Iterator&lt;T&gt; + Clone&gt; Iterator&lt;(T, T)&gt; for Pairs&lt;T, U&gt; { fn next(&amp;mut self) -&gt; Option&lt;(T, T)&gt; { let head_is_some = match self.head { None =&gt; false Some(a) =&gt; true } if head_is_some { return } ... } } It definitely grows vertically, but I find it easier to read that way. Of course that doesn't get the wrapped value out of the Option, but maybe a tuple could take care of that. I guess at the end of the day there's no silver bullet for representing complex logic. 
It'd kinda like "idiomatic" in that it's a term you hear elsewhere but it's inordinately popular in the Rust community. I'm not sure why, either, but it's the correct name for the concept.
Fixed, thanks.
[Issue filed](https://github.com/rust-lang/rust/issues/18444), thanks
Not to my knowledge. However, there's an RFC in the pipeline which might coincidentally change this: it suggests making macro invocations that use `(...)` or `[...]` *always* parse as an expression, even when used in statement position. I don't have a link on me; there's one in the latest weekly meeting notes.
Thanks! I'm hoping that soon I'll have the confidence in rust to not assume that any problems I run into are my fault, haha.
i3 is really straight forward and the last time I used it on Arch it was a breeze to setup (Arch Wiki is fantastic, as always). 
I'm probably going to harvest downvotes for this but... No. Rust is not suitable for you. Rust isn't even done yet. The language could change out from under you any day now (though not as much now as in the past). There will be bugs in the implementation that you won't be able to identify as such. The documentation is sparse. This is not a good environment for developing your skills as a programmer. The most important thing you can do right now is write software. Don't worry about not knowing enough to do it. You don't. None of us did. Find something you want to do, and do it. The language is not important. I recommend Java or Python, as those communities are large and helpful, the languages very stable, and the documentation is plentiful. More importantly, you think you know them. Don't worry about getting hung up on how to solve problems. Don't worry too much about doing it the "right" way. There will be plenty of time for that. The single most important thing you can do right now is write software. You will learn more from solving problems in a language you're already comfortable with than learning a new language, right now. Later, when you think you know what you're doing, pick up a new language. Rust might even be stable by then. 
Previously, we had one catch-all term for "something went wrong": fail. Now, we have two words. Imagine this line of documentation: &gt; "This method fails when the range is out of bounds." Does this function `fail!`, or does it just fail, returning an `Option` or `Result`? You can't tell. It can mean either. Now, that line means that this method returns an `Option`, `Result`, or some other value that can represent an error. If it could bring down the entire task, you would say &gt; "This method panics when the range is out of bounds." It's much less ambiguous. Fail means fail and panic means panic.
You don't use the word "panic" in common speech about what code is doing, so it's clear you're referring to the macro. Fail is a more overloaded term, so splitting off meanings with their own more precise terms helps to remove ambiguity.
I have submitted PRs to all of Cargo and Servo's dependencies and both projects themselves, so hopefully the base tools will be upgraded very quickly. If you want me to fix your library, please let me know.
&gt; I'm probably going to harvest downvotes for this but... The Rust community won't do that to you - most of us are not fanatical about the language. It was a perfectly reasonable reply.
I now wrote a script that submitted a ~45 PRs to projects from rust-ci. Whoah.
I went to the diff page. My browser was not amused. Congratulations on the heroic effort!
Kind of silly, but pretty useful too!
It can also be reduced via shortcircuiting: let a = match self.head { None =&gt; return None, Some(a) =&gt; a, }; match self.tail.next() { Some(b) =&gt; return Some((a,b)), None =&gt; {} } match self.next.next() { Some(new_head) =&gt; { self.head = Some(new_head); self.tail = self.next.clone(); self.next() } None =&gt; None }
Awesome, thanks, I had no idea about that!
Can I just say, I love how much effort goes into not only the language but also how it is advertised to developers. Things like cargo, compiler plugins, and syntax extensions are the types of things that can persuade developers. I read the title of this post and thought "oh crap i liked fail", but now that I have read the reasoning behind it I am all for it. Precise language about concepts makes them 100% easier to understand.
Obtaining combinations of length *k* from permutations of the same length requires discarding *k*! - 1 of them for each combination, which is ridiculously inefficient.
I don't think patronus's hand out souls.
This is beautiful :) It would be nice if this was part of rustc or cargo. `rustc --rest-easy test.rs` would include this lint. (Can be easily duplicated with a `feature` attribute in cargo optionally loading this)
I don't think this is that big an issue. I work with a bunch of electronics engineers and down at the FPGA/driver level they seem to formalise pretty much everything into a state machine or a processing pipeline. Rust is much better for this.
I like this one. Here's a slightly modified version that should reduce bound checks: fn main() { let v: Vec&lt;_&gt; = vec!(1u, 2u, 3u, 4u); let mut iter = v.tail().iter(); for el1 in v.init().iter() { for el2 in iter.clone() { println!("({}, {})", el1, el2) } iter.next(); } } 
I love how this visits most of the pitfalls of macro creation in a very intuitive manner. What do you think about making `count_expr!` a syntax extension (to avoid the extra overhead in computing the sum 1+1+1+1 at runtime) and part of the standard macros? It would also be useful for optimizing macros like `vec!()`. 
I don't really think it's something to be solved. Using macros one can define custom blocks like, say `class`, and you want the option to not have to end everything with a semicolon. That's what the exclamation mark is for, macros are not regular methods and behave differently.
Gentoo with Heather's rust overlay and emacs with flycheck and racer!
Agreed. Perhaps behind a command line flag.
I've been using `-Z time-passes` for this purpose since forever. It also lets you distinguish between many kinds of errors.
Note that I didn't say it was *efficient*, I said it's *reasonably easy*, point being that there's a relation so putting them under the same trait/struct is not completely nonsensical. That doesn't mean they have to be implemented in terms of the other.
I think I'll have a go at implementing it, looks easy enough. Suggestions for what the flag should be called, and what the note should contain?
The summation is computed at compile time; if it wasn't, the compiler couldn't determine the size of the `mem` backing array. As for `vec!`, it already generates an array literal as part of its expansion, so I'm not sure how much more efficient it would be if it, say, reserved the space and inserted the values into the `Vec` as opposed to constructing it directly from a boxed array.
I agree, I'm trying to contribute and it feels hard to start doing something.
It would avoid reallocation in the case of large arrays. I forgot about the compile time folding of values :)
Panic makes me think of kernel panics, in which everything crashes hard and it's a major incident to uncover the root cause, not a soft task failure that can be recovered by a supervisor task. Does this mean that Rust is downplaying using task failure and supervisor processes for error handling?
If you want to learn low level programming you should learn an assembly language. You don't need to learn much of it, just learn the very basics. It will make any low level language like C, C++ and Rust much easier to understand, because it dispels the magic. Pointers in C are not easy because the syntax is complicated, the way it interacts with the stack frame is complicated, how they relate to references is complicated. All that becomes extremely simple and logical in assembly language. Knowing how the underlying machine works will also help you understand performance issues in any language.
Ooh! bingo! Much faster now, thanks a bunch!
More general way to do cartesian product without copies: use std::iter::Repeat; fn main() { let v: Vec&lt;_&gt; = vec!(1u, 2u, 3u, 4u); for (v1, v2) in v.iter() .enumerate() .flat_map(|(i, val)| Repeat::new(val).zip(v.iter().skip(i + 1))) { println!("({}, {})", v1, v2) } }
You and me both! =)
Yes, in a sense. Task failure is for irrecoverable errors, not errors in general. Most of the time, a panic is not what you want.
[PRd](https://github.com/rust-lang/rust/pull/18455) Yurume suggested that it should be `-Z notify-passes=trans`, however the `-Z` flag doesn't seem to work that way. I can change it to `-Z notify-error-free`, or perhaps add a new `--notify-passes=...` flag.
seems like it is showing up....and showing up, and showing up
This is exactly what I prefer. Some people are against multiple returns in functions, but when it's very clear where they are, as in this example, I see no downside.
These are all ok, but Vec() is even nicer, and extending to my own types I can make everything look more like it would in C++; its more the general case than Vec specifically. The other tweaks in rust .. .clone() instead of copy constructors, .to_... instead of conversion operators are all ok, since their C++ equivalents really do have tangible disadvantages. But the idea of a function with the same name as the type that constructs it is itself perfectly OK , IMO.
thanks for this, I was hoping to understand event sourcing and how I could implement it in Rust; this is a good start for me
 // The Earth's radius in meters. static EARTH_RADIUS: f64 = 6371.0; Uh, really? :) (Besides from a trivial typo, I'm also a bit unsure about the omission of different [CRS](https://en.wikipedia.org/wiki/Spatial_reference_system)es. Nowadays WGS84 is so universal, though.)
There really should be some alternative iterator that yields values that are only valid until the next call to `next()`. I suppose the current `Iterator` could be a specialization, but can such a thing be expressed in Rust?
See https://github.com/emk/rust-streaming
Eugh yeah, dunno why. I turned it off for now.
not that I know of. But there's a rust one: https://github.com/seanmonstar/l20n.rs :) And we're talking about moving some pieces to Gecko which may mean writing things like l20n format parser and context in C++
http://doc.rust-lang.org/guide-plugin.html Please take heed of the giant red box at the top. &gt; On the same vein is there a command/option/plugin which formats your rust code to some standard style on file save? Not yet, but there will be around 1.0 or shortly after. `rustc --pretty=normal` sort of kind of does it today.
That HashMap looks identical to ChrisMorgan's AnyMap.
Yeah, it was just some shell commands using hub. I accidentally deleted it, or I would. It was something like for repo in `cat repos.txt` do name = ${} // some shell magic to split on / to get repo name hub clone repo cd name sed ... git add -A git commit -m "fail -&gt; panic" hub pull-request -m message cd .. end 
The unsafe parts look iffy to me. It looks like you'll leak the event handlers when an `EventBus` is dropped. You may want to associate each handler vector with a closure for dropping its contents as the correct type, then calling those cleanup closures in the implementation for `Drop`. i.e. (untested) struct EventBus { map: HashMap&lt;TypeId, Vec&lt;*mut Any&gt;&gt;, cleanup: HashMap&lt;TypeId, |*mut Any|:'static&gt; } impl EventBus { fn new() -&gt; EventBus { EventBus{map: HashMap::new()} } fn register&lt;T: 'static&gt;(&amp;mut self, handler: |&amp;T|) { let id = TypeId::of::&lt;T&gt;(); if !self.map.contains_key(&amp;id) { self.map.insert(id, Vec::new()); self.cleanup.insert(id, |unsafe_handler| { let handler: |&amp;T| = unsafe{transmute(*unsafe_handler)}; drop(handler); }); } self.map.get_mut(&amp;id).push(unsafe{transmute(handler)}); } fn post&lt;T: 'static&gt;(&amp;self, event: T) { let id = TypeId::of::&lt;T&gt;(); if self.map.contains_key(&amp;id) { for unsafe_handler in self.map[id].iter() { let handler: |&amp;T| = unsafe{transmute(*unsafe_handler)}; handler(&amp;event); } } } } impl Drop for EventBus { fn drop(&amp;mut self) { for (ref id, ref handlers) in self.map.iter() { for unsafe_handler in handlers.iter() { self.cleanup.get_mut(id)(unsafe_handler); } } } }
Thanks. &gt; It is not possible for an iterator to produce references to some data stored inside the iterator itself. Is the reason for that because it's not possible to express the lifetime (at the point of `impl Iterator&lt;&amp;str&gt;`)?
If by any need for them you mean any need for a reasonably close answer. The earth isn't a sphere.
Damn, that's something I would not have expected. It is a good idea to avoid overwriting other build profiles, but it would be so much more self-discoverable to have one subdirectory per profile and nothing at the root!
Agreed
Clickable link for the lazy: http://isocpp.org/files/papers/N4174.pdf
I think this is one of the worst proposals for C++ yet. It's ambiguous, will introduce yet another form of complexity to figure out what the hell is actually happening and it will lead to inconsistency. Not a fan.
haha LMAO. see what I mean. for you "the worst proposal ever". for me, "the end to my eternal pain". Can we just agree, one of us sticks with one language to make overly fussy ,restrictive, verbose, with awkward syntax, and the other gets to make their language more liberal with lots of expressive shortcuts. I don't mind which is which , so long as I don't have to compromise with you :) we have mature IDE's. In C++ we take it for granted that we can click on something and get 'jump to definition'. Its actually IDE leverage that makes people (over)use method syntax in the first place, IMO. The main utility to it is, getting the 'dot' autocompletion dropbox, which is worth its weight in gold when it comes to navigating &amp; discovering how an API works. The lack of autocomplete is the main thing holding me back from Rust, keeping me going back to C++.
So basically Jetbrains is actively preventing you from Rusting because of their lack of Rust support. 
not quite what I said; i want Rust for various reasons, but mature IDE's keep pulling me back to C++. 
Rust is more grepable , but you still need propper resolving. More type inference also means hidden details known only by the compiler. The way I see it, the syntax for declaring C++ classes forces you to put too much in headers, and tends to make C++ projects less well organised. classes pull in dependancies. This feature is aimed squarely at solving that problem. By using Free Functions more, its easier to decouple, put *less* in the headers, and properly modularise code. (and you could still do a 'unity build' for maximum inlining?) If I had my way with a new language from the ground up , "methods" would never exist. There would just be a universal shortcut for calling any function a.foo(b), and there'd be something else to gather them into a vtable if you need it (maybe more like the way go does interfaces)
By the way, are you w****r1234 on the isocpp forums? I appreciate the good intentions to increase awareness of Rust in C++ community (and vice versa), but I'm afraid the effect will be the opposite to the intentions, since the way you do it usually looks.. inappropriate (at least for me, sorry).
&gt; The "task panicked" message a task responding to a panic. Small typo here, this isn't actually a sentence.
Yeah, Jose is a really great person, and I'm excited about Elixir. :)
Hmm, looks like rustdoc might not be doing a good job on `pub use` of a extern fn, which is what `transmute` is. Here are the docs for the function in the location it is originally defined: http://doc.rust-lang.org/core/intrinsics/ffi.transmute.html. That explains what it is much better.
Rust moves sufficiently quick that it doesn't make a lot of sense to have a package in distributions like Debian. (Arch doesn't have releases so they can get away with a new version every day). You really want to install the official rust nightly. The [rust guide](http://doc.rust-lang.org/guide.html) goes through how to install it.
This is what [`std::any`](http://doc.rust-lang.org/std/any/index.html) provides, by the way.
&gt; Rust moves sufficiently quick that it doesn't make a lot of sense to have a package in distributions like Debian. I don't see how that is supposed to be a limitation. Rust has a fixed release (v0.12), and for subsequent updates Debian manages repositories for testing and unstable packages. It's far better to try out software that is properly packaged.
I first jumped into Haskell to sketch out a solution, leading to: λ let xs = ['i', 'j', 'k', 'l'] xs :: [Char] λ [(a, b) | (a:rest) &lt;- tails xs, b &lt;- rest] [('i','j'),('i','k'),('i','l'),('j','k'),('j','l'),('k','l')] Keeping in mind `tails` can be expressed e.g. as such: λ scanl (\(h:t) _ignore -&gt; t) xs xs == tails xs True This led me to: let v: Vec&lt;_&gt; = vec!(1u, 2u, 3u, 4u); let mut vs = v.iter() .scan(v.iter(), |tails, _| { let tail = Some(*tails); tails.next(); tail }) .flat_map(|mut tail| { if let Some(ra) = tail.next() { let a = *ra; tail.map(move |b| (a, b)) // (*) } else { tail.map(move |b| (0u, b)) // (**) } }); Which is almost there, although not quite. The first problem (*) is that I’m fairly sure we need closures that capture by-val/moves, which I don’t think is available right now. The second problem (**) is that I suspect that mapping over `tail` right after witnessing a `None` result out of `tail.next()` isn’t nice, since I assume `Map` needs to call `.next()` to do its job†. I’m not sure what’s the Rustic equivalent to `let (h:t) = someList`; something like the following springs to mind: let (ref current, ref t) = (tail.next(), tail.skip(1)); if let &amp;(Some h) = current { … except that now mapping over the result of `skip` would have incompatible types. †: I’m really not fond of the requirement to call a `None`-returning `next()` at most once. Querying whether a sequence is empty or not is as side-effect-free an operation as there can be, and yet we don’t have that.
Any doesn't introduce Drop trait, is it because it's using transmute_copy to not move source value ?
In fact, other than a slight layout difference and a function, `Option&lt;Box&lt;Any&gt;&gt;` is very similar to `MaybeValue`. The former is essentially: struct OptionBoxAny { // (null == None) data: *mut u8, vtable: &amp;'static VTable } struct VTable { destroy: fn(*mut u8), get_type_id: fn(*u8) -&gt; TypeId, } 
Trait objects automatically handle destructors—it’s an item in the vtable that will always be there.
Would it not make sense to provide a dedicated repository for rust nightlies? At least, it would be convenient, I guess. I have rust off the arch AUR, and it takes forever to compile :P
Seems like it should be on by default to me.
I thought there were problems including it in debian because of the staging process used to compile rustc?
I have run into many autotools (well honestly libtool is probably the worst of the bunch) warts but honestly it so far above and beyond the rest for support for cross platform, cross language building. Not saying that better won't be made or that we shouldn't work to make cargo better but as for what I need as a distro packager, package maintainer and integrator, getting rust support in autotools is going to be the biggest win.
Is that something that will be allowed on he future?
No, you can't ever impose *more* requirements on a trait method, that would break generic code that can only see the signature from the trait. (You can implement a trait with fewer requirements, e.g. `trait Foo { unsafe fn foo(&amp;self) } impl Foo for Bar { fn foo(&amp;self) {} }` is fine, because any safe `fn` can be called as an `unsafe fn`, but moving the `unsafe` into the `impl` is not since that would make the `unsafe` concrete method on `Bar` pretend to be safe in generic code using only `Foo`. The same applies to lifetimes.)
Thanks. I asked because https://github.com/emk/rust-streaming looks very desperate and I kinda agree that zero allocation iterables would be awesone.
Somebody could put rust in unstable, if you wanted to. I guess nobody has cared enough yet, or knows how.
As far as I can tell, the only problem with that is `StreamingIterator` not working with `for` loops due to the restriction to `Iterator` only. (That is, all the code there works, and gives zero-allocation iterables, it just does not work as `for ... in streaming_iter { ... }`.)
Interesting read, learned something new about what all this unwinding actually is, thanks.
TL;DR: curl https://static.rust-lang.org/rustup.sh | sudo bash Sometimes the prompt for your password from `sudo` collides with the output of cURL. You can enter it while cURL is downloading the file.
One can also do `curl https://static.rust-lang.org/rustup.sh &gt; /tmp/rustup.sh &amp;&amp; sudo sh /tmp/rustup.sh` to avoid that output mixing and avoid accidentally executing a partial shell script if the download is interrupted.
Does `rustup.sh` have to be downloaded every time?
There's no guarantee that the script doesn't change. It is rare, but it has happened. It could lead to strange error messages or an incorrect or corrupt installation.
Use [rust-nightly-bin](https://aur.archlinux.org/packages/rust-nightly-bin/). There's one for Cargo too.
If I'm a library developer, I might have 2 branches, one tracking 1.0, and the other making use of nightly features. Will Cargo support this?
There's rust-nightly-bin, which downloads the nightly tarballs. There's also strcat's repo with the {rust,cargo}-git packages: [thestinger] SigLevel = Optional Server = http://pkgbuild.com/~thestinger/repo/$arch 
There already is good news, things like mio are trying out new things. 
I don't see much of an issue. At least it's coming in over HTTPS now. The only issue is trusting rust-lang.org to not deliver malware, which I have no problem with. It's currently the easiest way to update Rust which must be done frequently to keep up with changes. I assume OP prefers convenience and doesn't mind installing outside of `$HOME` if he's looking for packages in the Debian repos.
I believe https://github.com/conduit-rust/conduit intends to fill this role. I know I saw wycats and reem at some point going to discuss making Iron based on it.
&gt; mio Nice.
If you want to have them fetch from a git repository, sure. From the central repository, I'm not sure how that would work out, exactly. 
Rack is sort of on its way out, due to archetectural problems. A common interface for Rust web frameworks would be good, but it shouldn't just be a clone of WSGI/Rack.
The thing that most worries me about this and seems to me like the decision most guided by pressure is the stabilization of the macro system for imports and exports, as it is today, for 1.0. The current system for importing and exporting macros is pretty terrible since it provides very little support for encapsulation, complex hygiene, and other pretty large problems that would be very hard to change after 1.0. Primarily I'm extremely concerned that this means that macros will always be silently imported through crate boundaries with no name spacing forever, which would be a *real* shame. Otherwise, this sounds like a reasonable plan forward given the constraints of today. EDIT: I can't read apparently (re: globs).
You mean something beyond [being able to depend on a specific branch](http://doc.crates.io/manifest.html#the-[dependencies.*]-sections)?
&gt; The current system for importing and exporting macros is pretty terrible I don't believe that the plan is to stabilize exactly what exists today, just that this is the half of macros that will be considered stable. &gt; I will miss glob imports for tests The post says glob imports will be stable at 1.0.
Seconded. The current macro importing/exporting system feels very ad-hoc. I wonder how hard it would be to integrate macros with the regular `pub` and `import` mechanisms (I guess very hard, otherwise it would've been done already)
We are interested in building a standard interface, and that may or may not be conduit. Conduit's problem is just that it hit the ecosystem very early, and we still need to develop a bit more before we really know what needs to be unified and what can be specialized to different frameworks.
I would urge anyone working on something like this to consider adding support for (or at least not prohibiting) things like asynchronous responses, websockets, ~~event sourcing~~ server-sent events and http2 server push, otherwise we'll end up with something that will soon be obsolete
It may be feasible to make some improvements for the import ergonomics before 1.0. Those specifics are still being worked through.
It would be terrific if I could instead specify a rust version in cargo build file. This is how it is done with clojure using leiningen.
**Warning**: what follows is somewhat unstructured braindump-ish. I went through the rust-ci packages under the "macro" tag last night, looking at all the procedural macros used there. I only found a single package with a macro that inspected the parsed ASTs: `huonw/fractran_macros`[1]. All the others actually boiled down to matching tokens and *constructing* some code based on parsed constructs. This is obviously not an exhaustive analysis; poking through repositories one at a time is kinda slow and I had to go to bed. :P In any case, what I've seen thus far makes me think that it *should* be possible to define a very minimal interface that would be easy for the compiler to support, and give procedural macros 90% of the power that's actually being used. I'm thinking of just defining a public token tree-based interface: plugins are fed token trees, macros return token trees (and possibly, strings as an option). They would also get a mechanism for loading functions from the host by name (*ala* OpenGL's extension mechanism). This `libsynext-interface` would be defined very low-level and always be provided by the compiler, with strictly no breaking changes ever. Since everything would be based on tokens (*i.e.* a fairly close analogue for the source representation), there shouldn't need to be any breaking changes until 2.0. On top of that, synexts would link to a specific version of `libsynext` which would provide a higher-level, safe interface. For example, the low-level interface would have an open-ended integer enum for token type; the high level interface would turn those into strongly-typed variants for a specific version of the language, but always with an `Other` variant in cases where a synext targets language version 1.x, but is run on a 1.y compiler including 1.y syntax in its invocation (x &lt; y). Would an interface be considered for incorporation into the compiler if it guaranteed *source* backward compatibility (*i.e.* plugins have to be compiled by the compiler that will load them before use), and only used language features and standard library constructs flagged as stable? Obviously, the people working on the compiler have more than enough work already, and don't need more. But if someone was to come up before or just after 1.0 with such a library, would that be of interest? Obviously, the devs' positions might be "even if it *works*, it's not the direction we want to go". Just as another related data point: D has survived for years without macros, in no small part due to CTFE and string mixins. If you want a DSL in D, you write a compile-time parser (using a very restricted subset of the language), feed it a compile-time string value, then inject the resulting string into the source. string dump(string varName) { return "writefln!(\"" ~ varName ~ " = %s\")(" ~ varName ~ ");" } int x = 42; mixin(dump("x")); This interface would be *way* easier to use, and a hell of a lot more powerful. --- 1: That said, it only cares about `+`, `*`, `/`, `^` and parens. `rust-scan` parses a grammar more complicated then that, so I believe it could be rewritten to use token trees directly without *too* much pain.
I haven't seen anything about this issue in Rust before -- can someone link to the "other side", whatever it may be? I would love to have a reliable way to determine whether a given operation can panic; I suppose it's a bridge too far to wonder if we could get this into the type system? ;-) (Shades of Java's checked exception disaster, here; except that in this case, since you can never catch a panic, I think it makes a lot more sense.)
Are there any plans regarding ABI stabilization, on the 1.0 timeframe or otherwise? I'm thinking about use cases like loading plugins, or building a LGPL library that can be distributed with proprietary code - both of which would need (AFAIU) shared libraries to be loadable on potentially different compiler versions.
Typo: Some(val) =&gt; println!("Has robed value {}", val.value), Should be `robbed`.
Unfortunately, panics (being analogous to exceptions) are just as undiscoverable as your typical unchecked exceptions. The possibility of panicking from within `println!` is the poster child of this. Ideally we would reserve panics for truly unrecoverable situations, yes, and this will take some effort to achieve. Once that's done I think the best we can hope for is a way to disable panicking entirely, which simply answers the question "can this operation panic?" with a definitive no.
Oh, that's nice. Cheers :D
The post refers to "stable" and "unstable" but where do the other tags here: http://doc.rust-lang.org/reference.html#stability fall. Is an "experimental" module like std::iter going to be unavailable in the stable release?
Yes, "unstable" here covers both experimental and unstable APIs. However, you shouldn't take the current stability attributes too seriously, as there's a lot of work in progress stabilizing these APIs. The `iter` module will most definitely be stable in 1.0.
&gt; currently (2013-12) seems like it was a while ago
Did you know fail!() was renamed to panic!() the day before yesterday? Many (if not most) libraries will follow this change soon and rustc 0.12 will become quite useless.
What are some examples of &gt; change type inference in ways that may occasionally require new type annotations Are there any RFCs that do this currently?
A good few interesting topics being discussed, but of all the things in there nothing has made me happier than knowing macros will retain the current syntax. I know It's subjective, but I had come to terms with the fact that we would probably be using `@printf()` soon. Knowing that's gone has made my day, prefixed sigils for macros set my teeth on edge.
Alternatively rust could allow something like let x: Foo = new() In many cases rust could infer the type, so I could just write let x = new() This would allow stuff like let x = f(new(), new()) // Instead of let x= f(Foo::new(), Bar::new()) But I am not fully sure if this idea will lead to healthy code...
I cannot judge the details, but the resulting syntax looks very nice and consistent with the rest of rust.
Something like that: diff &lt;( curl -s https://static.rust-lang.org/rustup.sh ) rustup.sh &amp;&amp; sh rustup.sh Note that if done with sudo, this will still do a lot of non-trivial tasks from root.
We have `Default::default()` for that.
I feel like unwrap should have a more dangerous name like unwrap_or_panic so that people don't reach for it instead of uwrap_or by default.
Writing and reading to the same tcp stream has definitely been possible for a while, it was fixed not long after that old post of mine that you dug up. However, it is not exactly non-blocking. In order to do reads and writes simultaneously, you have to set up separate tasks for reading and writing, each of which gets a cloned tcp stream. See the docs here: http://doc.rust-lang.org/std/io/net/tcp/struct.TcpStream.html I think this is being redesigned (along with other IO subsystems?), but I'm not sure. Would love some confirmation on that or links to any related RFCs .
To add to that, I've found setting up these tasks, and trying to make sure they properly shut each other down when either fails to be somewhat painful. I'm not sure there's a clean way with the current design, but [Here's](https://github.com/ehsanul/rust-ws/blob/fd097660ed4f89fe08535f27e58d297fd1da6037/src/ws/server.rs#L118-L163) what I'm doing in a little Rust websockets library I wrote. I am relying on `fail!` (now called `panic!`), but this is not great.
What problems does Rack have specifically? Googling did not help.
You can use `recv_opt` and just `break` your loop-
Type system yes, but please not in the function types, as panics shouldn't clutter everything. Make it an attribute, and propagate the whole thing behind the scenes for people who don't care. That way, to get panic-free code you can start with annotating `main` with `#[panics()]`, saying that you expect it to throw panics of exactly no type at all, getting compiler errors for every panicking function you call from there. More realistic for ordinary programs is probably `#[panics( OutOfMemory )]` or something. Typing rules are dead simple, especially as there's no catch: The panics of a function are the set union of what it calls, including "hidden" stuff like allocators.
Thanks, but`recv_opt` is for tasks. I don't see anything similar for `TcpStream`, but perhaps I'm missing something?
Does the data/vtable somehow matches the struct? How would it know to call the drop there?
Thanks!
this is a good link, he goes to great lengths to explain why it isn't "just syntax". its practicality.
It is all compiler magic, trait objects are deeply understood by the compiler and they have special handling to call the destructor correctly and so on.
&gt; Type system yes, but please not in the function types, as panics shouldn't clutter everything. Make it an attribute, and propagate the whole thing behind the scenes for people who don't care. I suspect it's not possible to do it like this and actually have it be useful. The problem is traits/impls and HOFs (which in Rust, with the `Fn`* traits, are actually one and the same). If you don't explicitly add this information to function (trait) types, then you have to conservatively assume that every function which calls a closure, trait object, or function pointer might panic, likewise any function which calls *those* functions, and so on. It will be very hard to write a useful Rust program under `#[panics()]` with these restrictions. (This is the same thing which tends to defeat other "modest" attempts at static analysis, such as statically tracking how much stack functions will use.) The deeper problem is that there's a bit of doublethink going on. On the one hand we say that different kinds of bottoms (nontermination) are morally equivalent: from the perspective of the caller there's not much difference between whether a function `loop { }`s forever or overflows its stack or heap or actually calls `abort()` or invokes `panic!()`; in either case it never returns, and there's no way to observe from within the program and distinguish which thing actually happened. In a language like Rust it's not reasonably possible to prevent a function from doing the first few, so, we say, there's also no point in trying to keep track of or prevent it from doing the latter. But at the same time the entire reason we have `panic!`s is because it *is* distinguishable and *is* possible to observe: destructors will run, the panic can be handled at a task boundary, and the program can keep running. (Alternately, the panic can cross a FFI boundary and cause havoc.) So on the one hand we tell ourselves that bottoms don't have observable side effects, but on the other hand, `panic!`s do. I suspect that all of the thorny issues around `panic!`s boil down to this fact. (Which is not to imply that `panic!`s don't have value; we have them precisely because they do. But it may not be possible to avoid some level of thorniness; and the tradeoff may still be worthwhile.)
I would rest easier if there were some sketch, even if not fully fleshed out, of *how* current macro import/export will coexist with the better mechanisms from the future.
One random thought: I've heard it suggested that it wouldn't be too hard, even under current circumstances, to namespace macros under crates (as opposed to the granularity of modules). If this is so, would it be reasonable to restrict exported macros to living directly under the crate root? Then if/when we grow the ability to namespace them under modules, it would be strictly a generalization of (then-)existing practice (as opposed to a separate, parallel mechanism).
&gt; We reserve the right to ... change type inference in ways that may occasionally require new type annotations This makes me happy. GHC's type system has undergone some radical overhauls and improvements over the years; most of the time the backwards-incompatible fallout was limited to some comparatively minor changes at the edge cases of where type inference would and wouldn't work. If we were to commit to inferring all and every types in the future which we currently (or rather, at 1.0) infer, this could effectively freeze the foundations of the type system in place. While if we allow for the possibility of small changes in where type annotations are required, it would hopefully provide enough elbow room for major improvements behind the scenes. *That* said, perhaps it would nonetheless be wise to strictly follow semver, and bump the major version if/when we do this. In that case 2.0 wouldn't mean "everything breaks", but rather "very little, but still nonzero, code breaks". (This is kind of similar to what Qt did with the Qt4 -&gt; Qt5 transition, for example. Many applications can compile under both Qt4 and Qt5 with a truly minimal number of `#ifdef`s.)
Is a common interface for web frameworks so that they become easier to deploy or what? I don't know much about Rack sadly :(
&gt; It will be very hard to write a useful Rust program under #[panics()] with these restrictions. Well, yes, but that's all in the game. In fact, I don't think any `main` anywhere will ever be completely panic-free, but subsystems might be, because, among other things, they can run in a constant, or at least predictable, amount of memory. If they get passed such an arena, they don't need to ask for more memory, voila, no `OutOfMemory` possible. That's, in the general case, encroaching upon dependent type territory but the easy cases (no heap allocs, no recursion -&gt; static stack size) might already be sufficient to give some developers some ease of mind. For *most* programs, though, what you want is something like `#[panics( OutOfMemory) ]` instead of `#[panics( OutOfMemory, IOError )]`, and that's comparatively trivial. I'm imagining something like this: Things like `println!` return `Panic&lt;IOError&gt;`, where `Panic` is an ordinary value and partakes in all that `Result` typeclassery, but has some additional magic trait implemented that makes rustc emit calls to the actual `panic!` if the value is ignored by the caller. Provide another macro, `no_worries_mate!`^1, that ignores `Panic` without calling `panic!`, that is, fail silently. Convention should be then that libraries, at most, `panic!` with `OutOfMemory`, for other errors they should use `Panic` or the usual `Result`, so that "panic or handle or ignore" can be decided at the use site. ^1 Or possibly less down under. `hakuna_matata!`?
Doesn't libnative depend on libstd though?
No specific examples spring to mind, but I've noticed that many of nmatsakis' RFCs for alterations to the type system tend to impact the algorithms that govern type inference in subtle ways. They may not have any concrete plans to do something like this at the moment, but perhaps they're just hedging their bets here in case they think of a feature that would be backwards-compatible except for its influence on type inference. ~~Note that even in the worst case, the fix for any potential requirement to add new type annotations can be provided instantly and automatically by `rustc --pretty typed` (though at the cost of grossifying your code and annotating *all* types).~~ EDIT: Wait, no, obviously if the code won't compile then it won't be able to annotate your type for you, or inference would just work anyway. Ignore that last paragraph. :)
As I said—for tests, I don’t care if they need libstd.
In theory your reader/writer tasks will be communicating somehow with other tasks -- if it's with channels, then recv_opt will help you terminate the entire thing when the channel is gone. (When one end of a channel closes the other must as well.)
Perhaps we could have `panic!()` as part of the signature for the function? That way, a function is not allowed to call `panic!()` or to call a function that `panic!()`s without being `panic!()` itself.
The heads-up is very much appreciated! Good speed, mighty warriors.
Is it really the desirable behavior for an io error in println! to panic? Writing to other files should be returning a result, so is it that much of a problem to explicitly choose your decision every println? Different programs want to do different things. "cat" probably wants to use either panic or more likely bubble up the failure with try! and silently exit. A compiler who is simply outputting diagnostics probably wants to ignore the print failing, since you can still output a meaningful answer via the exit code.
Hurray! That's amazing! This will make a lot of code look much cleaner!
There's a few, the big two off the top of my head. 1. Because middleware effectively generate a stack frame, Rack makes GC even slower. Admittedly not a thing for today's Rust, but still a thing. I can imagine borrowing trickiness here. 2. Rack is designed from a traditional client/server mindset, and doesn't play well with things like websockets, server push, or things like that. See things like https://github.com/tenderlove/the_metal or http://www.youtube.com/watch?v=kWOAHIpmLAI
Carl Lerche has been working on a library for this: https://github.com/carllerche/mio
I know myself at least and many other developers have been waiting for a stable version of Rust before trying it out. IS there any estimate on when 1.0 of Rust will be released?
sounds almost like a dream "String/Vec autoderef to slice/str"
I should note that it's just a normal Deref impl. You still need to &amp;* or [] to manually coerce in some places.
Do we have a timeline for when rustc will start to aim for ABI forward-compatibility, as well as source forward-compatibility? I'm a bit worried that the rustc 1.0 version of a library might corrupt memory or trash registers if it's dynamically linked against a program compiled using rustc 1.4.1, meaning that you would only be able to dynamically link against libraries which have an exact version match.
Release candidate around the end of the year.
There is no schedule for ABI compatibility.
In addition to the cross-language issues, unwinding is also problematic because it adds another exit path to functions (to free memory), which allows destructors to see objects in a messed-up state, and adds overhead very similar to a GC-s, even for programs that are going to immediately exit and free all the memory back to the operating system. 
I'm perfectly fine with using `unwrap` in case I know that it's never supposed to panic. I wouldn't like a much uglier name.
Just last night I was wondering what the status of serde was! Very happy to see this series. :) For those who don't know, erickt has been working on serialization in Rust since something like mid-2011 (I don't know for sure, because it actually predates me!). Here's hoping he can at last achieve his grand vision in time for 1.0.
I'm happy to see that namespaced enums are going to be a thing. I've ran into several cases where I needed to declare several enums that each needed a variant that shared the same name. This proved difficult and I had to get creative with the naming, but it never fully felt natural as I'm wrapping up another library and I'm trying to balance out its naming conventions with those of rust so that the consumers of the library can write idiomatic rust, but be able to easily look at equivalent code written in C++ (and even documentation) and be able to translate it easily to rust.
Unfortunately it doesn't yet support Windows.
Fair enough! I've just given the topic some thought and research - am I right in thinking that there's no actual practical need for this feature, except for far-future projects like desktop environments and plugins? Will we be able to distribute Rust libraries through Linux channels such as `apt`, without ABI-compatibility guarantees?
&gt; am I right in thinking that there's no actual practical need for this feature, except for far-future projects like desktop environments and plugins? That's how I feel about it, though there may be something I'm missing. &gt; Will we be able to distribute Rust libraries through Linux channels such as apt, without ABI-compatibility guarantees? Well, if you want it to be used broadly, you'd probably have the Rust expose a C ABI anyway. So in that sense, yes. If your library is intended to be used by other Rust programs, then it would probably be distributed through Cargo, which is source only, so doesn't have that problem.
This is not checked *exceptions*, it's checked *crashes*. We already have checked exceptions, we call those `Result&lt;T,E&gt;`.
Sure, but the annotation aspect is what's interesting here, not the actual behavior.
A `#[stable]` library would, however. I think the same argument applies
Serialization is easily among the top 3 issues blocking incremental adoption for established systems. We've got the whole "write a library and link to it" down pretty well (not perfect, but definitely usable). For embedded code and stuff that fits the library model, that's great. But another important possibility for introducing new languages into existing code bases is with service/RPC-based communication. Without common serialization formats (as well as the complimentary communication protocols like HTTP, etc.), Rust doesn't have a chance in this arena. I know of a couple teams in Amazon that are chomping at the bit to use Rust. Unfortunately Amazon has silly proprietary build systems and service frameworks, so it will be a lot of internal work to get that going and may take a while. But at the very least, they will need json serialization and an HTTP server. In light of that, thanks for this work erickt! 
Isn't that exactly what `enum` is for? Isn't a `Result&lt;T, E&gt;` really just a way of saying "this function may return `E` or `T`", right?
But you *do* annotate them in the signature of your function, right? If your function may return an IoError, that's part of the signature. On the other hand, crashes go unannotated, and therefore unnoticed. I may be misunderstanding what you mean, though.
Ah nice!
Sure, but all libraries do not have to be stable before the language is.
Hmmm maybe you're right. I don't tend to think of Result as an annotation, it's just a return value, but you are kind of right. Hmmm.
Way to go, keep the breakage before 1.0!
Yet. It will. That's acually where a large amount of the thought has gone.
"The expectation is that the vast majority of functionality in the standard library will be stable for 1.0." http://blog.rust-lang.org/2014/10/30/Stability.html 
&gt; **I** wish I personally have for the language is that you can write code that ... s/I/A/
Sure. Doing this kind of change is what's needed to get it there. But even beyond this, if we froze collections before this happened, you could make an alternate collections library and use it through Cargo. Stabilizing the language doesn't affect this kind of change, that's all I'm saying.
Ahh right, thanks. So I'd use `recv_opt` to get data from the read task and break out of the loop when done. I can remove the `panic!` that is potentially caused by using `recv` in that task. Great so far, but my bigger problem is that my write task was independent from the read task. My read task is talking to a totally different task than my write task. That makes it a bit harder to ensure both tasks get shut down if either task does. I do it by setting up the channels for both tasks in the same scope. So if that scope is gone, both tasks *should* go down together as one `panic!`s the other, which in turn `panic!`s the other tasks they are communicating with. I guess it works, but this is confusing. I think it's also easy for there to end up being zombie tasks that are not cleaning each other up (I think I have one such case already). Maybe there just need to be a more explicit way to shutting down tasks without requiring `panic!`.
And of course, serialization needs to perform well. Just feasible is not enough :)
I understand your feeling, but I also understand that getting Rust adopted by more people is necessary: - more people, more projects, more variety, more feedback - more people, more contributors - ... and at the moment backward compatibility and stability are the chief issues in Rust adoption. There is always room to polish things up and be better, but at some point you need to ship, kinda in a "Worse is Better" way. Is it too soon? I would not know.
Bring on the rust-collections revolution! 
It's worth mentioning that they will come back when we have HKT to implement those traits *properly*.
Ahhh this is great! Really looking forward to seeing how serde turns out!
HTTPS authenticates the *connection* via trust in tens of thousands of people with the ability to issue certificates for every domain. It doesn't validate that the file was built and uploaded by a trusted build server / person.
Just found this, Profit?
It's designed to support it and I'm sure help would be appreciated.
/r/playrust
It adds an exit path for every function call. In some cases those exit paths can be merged, but not always.
&gt; It doesn't validate that the file was built and uploaded by a trusted build server / person. Neither would manually downloading the binary.
LGPL permits using static linking with proprietary code as long as linkable object files are provided.
If it was signed and you validated the signatures, it would. Package managers handle that issue for you.
&gt; That's how I feel about it, though there may be something I'm missing. It might be a packaging issue in the future. If someone fixes a critical bug in `libfoo`, a popular and frequently used library, then every Rust program has to be recompiled and updated to pick up the fix. However, if it was a dynamic library, then updating just the library (and restarting any running programs linked to it) would suffice. Imagine if you had a system where openssl was statically linked to many programs, and suddenly you hear about heartbleed...
&gt; Many (if not most) libraries will follow this change soon and rustc 0.12 will become quite useless. Rust 0.12 remains usable as long as it's available to the public, no superceeding release was made official by the Rust people, and people still keep it installed. 
You need to set `LD_LIBRARY_PATH` to wherever the `lib` directory is I guess. Though usually the installer does all this for you.
That's just nitpicking (a more correct complaint would've been that it doesn't add any exit path to leaf functions). Still, destructor-running unwinding is a crappy way to track resources across failures, and arenas (processes/address spaces/compartments) can handle 95%, especially when used with resource-lists (file descriptors/robust futexes) that can deal with the rest. Of course, stack-searching GC (which also uses unwinding, and has even worse cross-language issues, but lacks the messed-up-state problem) is also a solution, but it comes with its own problems.
&gt; Is it too soon? Yes, it is too soon. The language has not even hit 1.0 yet.
Are they really going to use single letters like `I` for associated types?
It was and it is still heavy because each green thread needs a stack.
Just like you use stack to avoid allocations for local variables, you just need to allocate stack once and that's it. In the case of async/await coroutines you can compute required stack size beforehand (and this stack is only used to save variables that need to be saved between yields/switches).
The question is "is it too soon to ship 1.0?" answering "it is too soon because 1.0 doesn't exist" is strange.
I'm not sure HKT make sense for those traits. It is more taking a leaf out of Haskell's book, which does not have standard traits in the base library. This gives us time to find the right interface without fixing one that everyone must abide by. Also, it is hard to be generic over collection types in a meaningful way, due to different performance characteristics.
Rust evolves pretty fast. You should always use the nightly, and make sure you get it using [crates.io](http://crates.io)'s script!
Yeah that's true. Dynamic linking isn't as bad as lots of people have painted it to be recently.
Note that installing via the OS package manager when available is a good idea, too.
&gt; I guess since sybtax extensions won't be ready for 1.0 we are going to push for code generation? Yes.
`len` is a rather unfortunate name which is mostly useful for linear collections. The `len` of a `TreeMap` or `LruCache` ...? I would follow the STL convention and use `size` which is more universal.
This should be COMMed a little more loudly. The lang and the batteries are independent ? This is a good thing.
The last thing we want is a tango/Phobos.
Len smells like a dimensional vector, size is a magnitude
One man's laziness- and purity-hangups is another man's lifetimes- and pointer types-hangups.
Agreed.
Very exciting!
+1, this is one of my biggest worries with Rust 1.0. Maybe something will come out of the [discussion about prelude overrides today](https://botbot.me/mozilla/rust/2014-10-31/?msg=24613793&amp;page=30). I also have a [hackish solution](https://github.com/rust-lang/rust/issues/17103) but word is that we are avoiding changes to the macro system for the time being. A new attribute that restricts macro imports from a crate would be backwards-compatible, right?
The "meta programming" (compile time functions and custom checking etc) he's showing off (playing invaders at compile-time) seems to be much simpler than Rust's procedural macros.
Tldr? I like him but I don't like him 2 hours.
I actually like that in Rust we tend to use differently named "static member functions" for constructing things. Sometimes, I've been using enums in C++ to sort of "name" different constructors doing different things: enum do_this_tag { do_this }; enum do_that_tag { do_that }; class foo { public: foo(do_this_tag, int x, int y); foo(do_that_tag, double z); … }; int main() { foo x (do_this, 2, 3); } just to avoid ambiguities between overloads. And since in Rust we don't have overloads (if we don't count generic functions) I don't see how you could convince Rusticeans that `Vec()`, `Vec(4)` etc are good ways of creating `Vec`s. Don't write C++ in Rust. Write idiomatic Rust code.
Jon Blow has (in about a month) developed an early prototype of a language, which currently has the following properties (from memory, please correct me if I get anything wrong): His goals: - C-like performance. No garbage collection, embrace unsafe manual memory management. - Less friction with tools and with the language. - Tailored for game development. Features so far: - Go-style type inference. - Local functions with the same syntax as top-level declarations. - Various syntax improvements over C/C++. The language is terse. - Compile-time function execution without any special syntax. You can run arbitrary code at compile time, and he demonstrates this by running a space invaders game at compile time, doing rot13 ciphers at compile-time, etc. - Compile-time function call validation. You can write your own functions that check that usages of specific functions are correct. - The language compiles to C or bytecode so far, aims for C's performance or better. - Lets you specify dependencies in the language itself, rather than depend on some platform-specific dependency management tool. - Simple C FFI. - Globals, which he feels are very useful for games programming. - Runs on linux and windows so far, but he doesn't want to release it until he's worked on it a bit more. Stuff he talks about adding are: - An object system, which he seems to have some new ideas about. - Generics in some form. - Compiling directly to machine code. - Incremental compilation. - Built-in concurrency? He said something about this.
The key thing to ask is -- "Is Rust primarily a concurrent language or a systems language"? If it the former, and if developers are expected to make use of that concurrency -- spawn lots of cheap tasks, then panics make a lof ot sense. When something goes wrong in a task, the best solution is have some other task notice and restart the failed task, report the error or do something else (propagate it up the chain through a channel). The other worldview is Rust is a systems language, maybe even good for writing kernels or drivers in. There will be few tasks used and it will be mostly a better, saner, C++. You'll have one main loop and it will check and handle error conditions inline and decide what to do. This is basically how most traditional programming languages work today. Including C and C++. I think this topic is a bit hot because it strikes at the heart of the identity of the language and its ecosystem. Perhaps initially when it started up, it was progrably driven to be closer to the former worldview (maybe inspired by Erlang and so on). But gradually there was a change of guard and now it is pushed to be a better C++, which, is awesome of course. But in saying you want something better than X , also binds it to be already very similar to X. 
&gt; asynchronous responses, websockets, event sourcing and http2 server push How does event sourcing fit into this list? Isn't this just another approach to persisting your application's state? What's the connection to web servers?
From mio's source: * The goal is to normalize window's IOCP API to the various *NIX's readiness * model This worries me. There was a presentation from a company a little while ago (actually about a year, now; my sense of time is clearly wonky) that basically said using a readiness model on windows fundamentally precludes high performance. To quote from [the slides](https://speakerdeck.com/trent/pyparallel-how-we-removed-the-gil-and-exploited-all-cores) (slide 44): &gt; The **best** option on UNIX is the **absolute worst** option on Windows They went so far as to basically *replace* Python's async IO support with a different implementation, API, and even CPython changes, in order to get high performance on Windows. Looking at some examples, I'm seeing language used that *might* suggest that mio's design will work, but be a performance dead-end on Windows. Which would suck. But then, I'm **not** an expert on this, and haven't studied mio for more than 15 minutes, so I could be *completely* wrong about this. I am *genuinely* hoping they get this right. :) **Addendum**: Also, Python suffers from the GIL, which Rust *doesn't*, so it's also possible that mio can do something to make readiness work on Windows that cannot apply to Python.
What are we meant to be looking at?
Where can I read about procedural macros? The macro guide only seems to cover `macro_rules!`
The [plugins guide](http://doc.rust-lang.org/nightly/guide-plugin.html) covers them, do note the big warning on that page.
Lobster sounds amazing, but why haven't I heard about it before? actually, why hasn't Jon heard about it before? 
&gt; I don't see why those can't be packaged as well. In fact, it wouldn't really matter if the Debian package skipped some nightlies. Yes, I was not commenting on this sort of packaging at all, just demonstrating the uselessness of the 0.x releases. (It's not unlikely that there's actually no commit that compiles with 0.x for many projects.)
inspiring, good luck to him... it's possible something relatively simple with the correct minimum feature set could suit this domain more. I'm liking what I'm hearing about not having any sort of methods in there, and it sounds like he's open to D's idea of UFCS, but I'll have to wait to see what his take on polymorphism is. I do want polymorphism , its just the division between methods and functions that annoys me in existing languages ..unless methods were 100% open. BUT ... Q1 Will the complexity explode/&amp; his progress slow down when generics go in? ... Q2 and will it get a enough of a community to get IDE support.. Rust is so far ahead and still hasn't persuaded the world to develop one yet .. needs a certain critical mass (i know stability is also an issue &amp; the design is still evolving) Maybe, regarding stability, he can still take advantage of conceptual ground covered by rust, D etc , by cherry picking, perhaps a chunk of the R&amp;D time in rust can be bypassed
Warning heeded. Thanks!
Kids these days.
Note that the opposite is also true: Implementing the IOCP model on top of *NIX's readiness model hurts performance on *NIX, because it adds additional buffer management (and potentially copying). This is what libuv does, and I've always preferred libev for this reason. But then again, for the things that I write, "good performance on *NIX and something that just happens to work but isn't as fast on Windows" is perfectly reasonable. An API will have to make a choice between either model; I know what I'd choose, but I'm sure different projects have different needs.
Here's a silly idea: have you tried naming the executable and the library differently (after a `cargo clean`)? Maybe cargo is getting confused and trying to link the library as if it was the executable...
Thanks for the suggestion. I tried that and also splitting up the library and the executable into 2 separate cargo projects and using Cargo's dependencies (by giving it the path instead of a git URL), and I still get the same error :(
&gt; trait Borrow&lt;Sized? B&gt; I'm guessing: does this mean that the size of B may or may not be known at compile time?
A version 1.0 without a stable hash map type for example or a stable IO API would be worthless.
&gt; but be more accessible and usable. Can you expand on this? What are your issues with Haskell in these regards?
Do you know if there are any plans to add `[..]` as a synonym or replacement for `[]`? I've seen a lot of support for this, and not much against it.
For what is worth, the demo is first 1:08 (one hour and eight minutes iirc). Rest of the talk is questions, and directions. 
Perhaps I've missed the main point, but if you're suggesting moving to a world of only free functions, then I see at least one big negative. A large benefit of methods is namespacing. Free functions are much more likely to conflict. Without method impls you need to do a lot more manual name mangling, like: use foo::{newBar, newBaz}; ... let baz = newBaz(); let bar = newBar(); vs. use foo::{Bar, Baz}; ... let baz = Baz::new(); let bar = Bar::new(); I think the latter is much more elegant.
http://pkgbuild.com/~thestinger/repo/ This is even better than what is in the AUR. It's maintained by strcat.
While I agree with the spirit of conservatism when approaching other communities about rust, it'd be nice if you could provide something more constructive than simply calling their actions inappropriate. What specifically do you find disagreeable, and how do you think they can change? To someone who doesn't venture into those forums, your comment here feels off-topic from lack of context.
More or less. Types like [T] and MyTrait aren't Sized, which specifically means you can't have a [T], you can only ever have a (fat) pointer to [T]. Anything with the Sized? annotation basically says "I can accept things that aren't Sized, so I have to assume that they aren't".
The whole post is probably more relevant to C++ but it shows how things can go &gt; use foo::{Bar, Baz}; Tangentially, funnily enough r.e. this exact example in Rust, I've observed rusts' different namespaces for Functions and Types actually do allow this:- fn Bar()-&gt;Bar {...initialize a Bar} fn Baz()-&gt;Baz {..intialize a baz} let x:Bar=Bar();// calls ok I'm know this isn't idiomatic rust; but just imagine if the rule was `use foo::{Bar}` will get *all* entities named 'Bar' from `foo::`; (`struct Bar`and `fn Bar` alike, and maybe any `mod Bar` for further division..) This might look bad but its' unambiguous both at the call site &amp; when searching for the definitions. (and if used as a path, Bar:: is clearly a mod) Other than that - in an *overloaded* free function world one gets the benefit of the arguments' types namespaces(functions taking distinct types are distinct); and C++ further has argument-dependant lookup (applying a function to a type, it can search the namespace of the type) ; foo(Bar) &amp; foo(Baz) are distinct functions, also foo(bar::Baz) actually looks for bar::foo In my mind, if the method &amp; function calls are just sugar for the same thing it saves refactoring effort... C++ is horrendous for this, bouncing back &amp; forth when you run into the limitations of one or the other. Methods in C++ conflate calling syntax with dependancies. Rust *is* better in some ways with its' traits but still has some restrictions, you need to micromanage the traits (e.g. overlapping ones), and sometimes have to make single-function traits, and the traits further divide the namespaces as if directories, filenames &amp; types weren't enough already. but basically if C++ ever actually gets this UFCS idea ... that's the world I'm after. (but still my dream language would have Rusts' cleaner, more elegant expression oriented syntax, along with UFCS) 
Note that Qt has a very strong commitment to *binary* compatibility. It can be broken only on a major version change. You are right that Qt5 is almost source-compatible to Qt4. I believe the big deal is more about the binary compatibility breakage, in this case.
do you have an estimate for how rusts' complexity divides up.. parser, macros,resolving/inference.. borrow-checker .. code-gen, libraries ,anything else. I can imagine making a compiler user-friendly with decent errors/ diagnostics is significant. there was a time when rust was 80kloc? but its bigger now? just trying to get an idea of how big a task it is and where he might think he can economise 
Agreed, it would be nice to pull in stdlib via a git hash rev just like any other 3rd party lib. But then mixing ..... Dammit!
Small nit, HKT has nothing to do with laziness and purity. But I still smiled at your comment :)
Ah! Fair enough!
Rust has *never* had anything in the standard libraries that scales better than thread per client. Threads with blocking calls are fast, but allocating a stack for every client does not scale well. Using non-blocking sockets scales better since it greatly reduces the resources spent on each connection. It requires more system calls and bookkeeping than blocking code, but it can still outperform it due to data locality. However, libuv is a lossy abstraction forcing lots of memory allocation and other inefficiencies. It maps poorly to the readiness-based non-blocking socket APIs on \*nix *and* doesn't map very well to IOCP on Windows either. It's designed for a language without multi-threading and doesn't implement a scalable multi-threaded event loop at all. It also doesn't use the most efficient OS APIs like `timerfd`/`signalfd` on Linux and needs to dispatch stuff like `stat` through a thread pool. Linux does have kernel support for file AIO, but libuv uses blocking I/O in a thread pool. Rust was using libuv to implement *blocking* I/O for green tasks, so it had all of the overhead of stack per connection along with the overhead of libuv. It then began using dynamic dispatch for all concurrency and I/O calls to support both native and green tasks. The API is still crippled relative to what it could have been if it was built on native threads from the start and there are a host of performance issues remaining. It's going to take a lot of work for the libraries to recover from green threads both from an API and performance perspective. It needs solid support concurrency and blocking I/O before worrying about non-blocking sockets and asynchronous I/O.
The compiler resolves dependencies within the same "crate" and lets you run code at compile time without having to use another library. This makes it easier to experiment with it in a more ad-hoc way. I think it is a great start for a language that focuses on productivity + prototyping and I look forward to try it. Impressive!
I'm far from an expert (planning to start a CompSci degree next Fall), but it seems to me that automatc analysis of functions to determine if they allocate or block is nontrivial in the same vein as [the halting problem][1]. Sure, you could do a shallow analysis to see if any functions a function calls will block or allocate, but then you have different control structures that complicate the issue. And there's different types of blocking. It could block on I/O (which would require analysis/annotation of the underlying C implementations) or block in a spin-lock: loop { match doSomething() { Some(result) =&gt; return result, None =&gt; _, // i.e. continue } } You can't say that this loop won't return after a single iteration. Of course, you could naiively say, "This function *could* block," and that'd probably be satisfying to most people; but what conditions does it block under? What does `doSomething()`'s return value depend on? It's easy to see if a function does allocation when it returns a `Box&lt;_&gt;` or `Vec&lt;_&gt;` or `HashMap&lt;_&gt;` or `String`. You can deduce if a function allocates if it's supposed to handle a lot of data but doesn't take a mutable input buffer like `Reader::read(&amp;mut self, buf: &amp;mut [u8])` does. But sometimes there's no way to know, like `BufferedReader::new()`, which instantiates the struct with `Vec::with_capacity()`. In cases like this, you'd have to look at the source yourself. I'd totally support manual `#[blocking]` and `#[allocating]` attributes for functions, but I'd hate to think of the effort it would take to annotate all the functions in the stdlib. Maybe it could start as a compiler plugin and gain traction in community libraries. Or, you could just start a convention for documentation comments. :) At one point, some of Haskell's collection functions (e.g. `insert` or `sort`) were documented with their Big-O complexity, but I couldn't find any currently on Hoogle. [1]: http://en.wikipedia.org/wiki/Halting_problem
I was thinking only of blocking I/O, but spin-locks might be good to know about too. When writing async I/O, you have to be very careful that you never call something that might block. It can be tedious to make such determinations. Higher level functions would inherit `#[blocking]` if they call lower level functions that have it, so the annotations only need to be on the functions that actually block (if we are including spin-locks), or that call syscalls that block, or that use rust primitives that block (via libraries like libuv). 
just to point out, this sub's description includes: "emphasizing safety, concurrency, and speed."
Are you looking to start a productive discussion or just abuse the language?
How often do you get input from the command line vs. doing other stuff? Verbosity is okay for things you don't do that often, doubly so for I/O.
My main problem with any kind of panic annotation is that a *surprising* amount of code can potentially panic, as we sprinkle our code with asserts in tricky places to fail fast on *bugs*. There's even more asserts that run in debug builds, too. It would border on useless.
This is most definitely not an idiomatic piece of Python code. Nobody writes Python code like that.
Yeah, idiomatic Python is just `input = input()`, letting the exception crash the program if it doesn't work.
Rust is a systems language that doesn't trust the programmer. Anything that may fail will have to be explicitly handled. A shorter way to do it would be `let input = std::io::stdin().read_line().unwrap();` if you don't care about having a custom error message. `unwrap()`, like `expect()`, will cause a task panic if you call it on an error value (crashing the task, and possibly the program) But really, any language with exception handling usually has this level of complexity (think try catch) for code that throws. Languages which let you *ignore* throws (eg python) may look neat and clean, but this is one common source of errors. Forcing the programmer to either `match` the error or `unwrap()` it is Rust's way of fixing this.
It looks like the runtime is somehow borked. The segfault occurs after the plugin panics due to a failure in the task local RNG: #0 0x00007ffff5ee8690 in rust_panic () from /usr/local/lib/librustrt-4e7c5e5c.so #1 0x00007ffff5ee8e6a in unwind::begin_unwind_inner::h043457e0ce41796ddSd () from /usr/local/lib/librustrt-4e7c5e5c.so #2 0x00007ffff5ee8b29 in unwind::begin_unwind_fmt::h33cb99eebdc73181FPd () from /usr/local/lib/librustrt-4e7c5e5c.so #3 0x00007ffff621f6ad in rand::task_rng::h87e31a49ab53b7b4Gib () from /usr/local/lib/libstd-4e7c5e5c.so #4 0x00007ffff65fc34b in hash::RandomSipHasher::new::h7ac2f0f050fe1602c5f () from ./libplugin.so #5 0x00007ffff65fc2e3 in collections::hashmap::map::HashMap$LT$K$C$$x20V$C$$x20RandomSipHasher$GT$::new::h6355596007518334254 () from ./libplugin.so #6 0x00007ffff65fc1e9 in init () from ./libplugin.so #7 0x000055555555e44f in main::h77f94290d38a5ccegaa () #8 0x0000555555594d3d in start::closure.8528 () #9 0x00005555555acbcc in rust_try_inner () #10 0x00005555555acbb6 in rust_try () #11 0x00005555555aa233 in unwind::try::hac84dac916339a9cxGd () #12 0x00005555555aa10c in task::Task::run::h3855d187b4c0788fnMc () #13 0x0000555555594b7f in start::h6e41679d4b8806beUhe () #14 0x00005555555949b6 in lang_start::h1776e07af04107b5dhe () #15 0x000055555555e4df in main ()
I disagree that this is an issue with the language. Sometimes you have to make a language-level choice, like whether to be dynamically typed or not, or whether to require reasoning about exception safety or not. To me, not having to worry about exception safety in safe code (a correctness concern) is much nicer than having some code be a little more verbose (a syntactic concern). Yes, the world is replete with examples of languages that made the second choice, but that doesn't mean that Rust has to be one of them.
I don't disagree that given the options Rust made the correct choice. But there are at times ergonomic issues, and those are important to discuss. Especially with certain introductory concepts, remember that reading a line from stdin is the minimum level of runtime interactivity, where it is possible to drive new developers away.
&gt; unsafe, internal wrapper of libuv, direct bindings of libuv It's far easier to write safe bindings to the low-level functionality than it is to turn libuv callbacks into a safe abstraction. Either way, it's libuv provides a single-threaded event loop rather than a scalable multi-threaded one. It won't come close to the performance of implementations where the kernel distributes events across threads.
Use "0.0.0.0" instead of "127.0.0.1" to listen on all interfaces.
&gt; Just the epoll(), or are there more I'm not aware of? There's the `epoll_wait` call to grab a buffer of events and it has a high cost (kernel synchronization). Reducing the cost by using a larger event buffer means coarser load balancing across the event loop threads. An edge-triggered event loop is more efficient in the end due to fewer spurious events and the ability for it to scale to many cores, but it requires epoll wrangling with `epoll_ctl`. The cost of context switches is replaced by smaller, more numerous `read` and `write` system calls. &gt; That is because the Linux AIO is not perfect - even glibc implements the API using a thread pool instead of giving the kernel interface. The glibc implementation has little to do with the quality of the API in the kernel. It was implemented before the relevant kernel APIs had evolved. &gt; I've found that libuv is far more memory efficient than other IO libraries, such as libevent. The libuv requests and handles can easily be shoved into structs in various places and easily reused, instead of being forced to always malloc() like libevent does. And more importantly, is the memory wastage due to the API design, or to prevent too much bookkeeping from taking place? No one is making the claim that libev / libevent are high quality event loops suited to high-performance servers. They're primarily used by client applications like torrent clients (transmission) and GUIs (urxvt). A high-performance server needs a multi-threaded event loop and libuv doesn't provide one. It doesn't make sense for it to do that when the sole purpose is to provide the I/O backend for a scripting language without threads. If it used a multi-threaded event loop, it would be able to scale to multi-core machines with the kernel handling load balancing. Look at how node.js / libuv performs on a raw event loop benchmark: http://www.techempower.com/benchmarks/#section=data-r9&amp;hw=i7&amp;test=plaintext The top 4 entries are based on Java NIO and get 600k requests/s, followed by a C++ wrapper around `epoll` with 520k r/s. The node.js / libuv entry is all the way down at 73k r/s. It is unable to leverage the multi-core architecture, so for that reason alone it loses at least half of the potential performance. The claim that it's a high-performance library is an extraordinary one not backed up by the hard data. It's not even able to compete with Go, which is spawning a green thread for everything. It can't be explained by a performance difference between the languages because v8 isn't *that* far behind Go: http://benchmarksgame.alioth.debian.org/u64/benchmark.php?test=all&amp;lang=v8&amp;lang2=go&amp;data=u64 Go's garbage collector is also much less mature and that matters a lot in these I/O benchmarks. The benchmarks game provides a good example with the `binary-trees` benchmark, where the JavaScript nodes are using 5-10x as much memory but the garbage collector is coping with it far better. The gap on the high-end network hardware is insane: http://www.techempower.com/benchmarks/#section=data-r9&amp;hw=peak&amp;test=plaintext Go's inability to scale is a nice indictment of green threads too. It's a far better green thread implementation than Rust will ever have, because the language is designed around them. It even has a calling convention tuned for fast context switches at the expense of general performance. &gt; I don't remember what is wrong with AIO in particular. Async IO for on-disk files has always been sketchy. It's poorly supported on Windows, and as far as I know is not even possible on OS X. I'm not sure how *BSD and other POSIXes stand. The Linux kernel implements `write` to filesystems via writeback threads so it only blocks under memory pressure. It makes no sense to dispatch those to a thread pool. A threaded event loop can cope with quasi-blocking `read` calls without the need for synchronization with a separate thread pool. This is how things are done in web servers like nginx, which serves files *from disk* over http faster than libuv can serve the same thing from memory via raw tcp. The Linux AIO API exists to provide a high level of control when leaning on buffering / writeback isn't acceptable. It is certainly a low-level API and the quality is determined by the underlying filesystem implementation.
The way I see it, this proposal is not really useful. It seems to either depends on some nigh impossible automatic implementation, or depends on some serious and highly complex handwritten analysis. Furthermore the whole annotations idea sounds like too much noise. Summary: it's hard to automate, it's hard by hand. Even if Rust could do it will make docs noisier, without some real gains. 
Effects wouldn't mix with traits / generics / closures at all either.
Last nightly throws a different error: expected `|&amp;[u8]| -&gt; collections::vec::Vec&lt;u8&gt;`, found `|&amp;[_]| -&gt; collections::vec::Vec&lt;_&gt;` (expected concrete lifetime, found bound lifetime parameter ) But now your `|input: &amp;_|` suggestion fixes it too (as demonstrated here http://is.gd/Ua5W5V) which reinforces your idea that explictly typing creates a different lifetime than the inferred one. Will search the GitHub issues as soon as I get some free time and post it there if I don't find a related issue. Thanks!
What point are you making? Low level direct access to c apis is the way to implement async IO at the moment. Are you critiquing rusts absence of a good 'rust native' async api? Or the suggestion that you overcome the limitations of the std library with unsafe code? (either of these would be completely daft points to make imo, so Im honestly puzzled as to what you're pointing out, or why people have up voted it)
really? Given that the rust libuv bindings already exist Im skeptical doing it by hand for each platform would be easier. I rewrote a libuv binding for the file watch events a while ago, and it was way faster than learning how to use 3 different OS apis. Still, like you say, it wont be as fast; but I think hard to argue that it would be far easier in general to use the native apis on each platform... unless you're already familiar with them. libuv isn't terrible, and its well documented and maintained with copious examples... It works just fine for many things. Its just not optimal for performance.
ah, fair call. 
Some functions in the standard library use a heading "Arguments" and then a list of arguments, e.g.: # Arguments * `foo` - The first argument * `bar` - The second argument Some examples: http://doc.rust-lang.org/collections/str/trait.StrSlice.html#tymethod.find_str http://doc.rust-lang.org/std/str/fn.replace.html Some of them use backticks for the argument names, some don't. A consistent style and a recommendation for rustdoc would probably help here. Would be interesting to hear what plans /u/steveklabnik1 has with regards to this.
I got tinkering with the each syntax he was thinking of (with my own take too) and I find this to be fairly readable (psuedo-code of course): printf('look ma I can count\n'); from 1..count as n { printf('%d\n', n); } each iterableOuter.iterable() as iterableInner { if iterableInner.meetsCondition() { continue; } each iterableInner.iterable() { if it.invalid() { remove it; continue; } if it.cascadingInvalid() { remove iterableInner; break; } } } each mapVar.iterable() as key, value { remove key; remove value; } I like it, anything seem clunky to anyone else?
You're listening on "127.0.0.1" which is the loopback address. The standard approach for accepting traffic on all interfaces is to listen on "0.0.0.0". http://en.wikipedia.org/wiki/Loopback
There is ! for non-terminating functions: http://doc.rust-lang.org/reference.html#diverging-functions
&gt; If you were writing C you'd have to ask yourself the exact same questions. If you didn't the only difference is that you'd get a segfault (if you were lucky) at runtime instead of a nice compiler message letting you know that what you're doing is unsafe (and sometimes even how to fix it). A segfault *or* a memory leak (you won’t get a segfault if you just forget to `free`).
&gt; http://www.techempower.com/benchmarks/#section=data-r9&amp;hw=peak&amp;test=plaintext &gt; Go's inability to scale is a nice indictment of green threads too. It's a far better green thread implementation than Rust will ever have, because the language is designed around them. It even has a calling convention tuned for fast context switches at the expense of general performance. Go didn't scale at PEAK cause it is still runned in a single process. Cause of that it becomes beaten by NUMA architecture and GC pauses. (even cpu cache of different cpu's could be considered as NUMA ! ) If techempower guys run several processes behind nginx as a proxy, then Go will be among leaders.
FreeBSD has useable aio_* support as system calls. It extended the API by adding aio_mlock(). The kqueue()/kevent() API supports all common types of file descriptors (not just socket like descripts) and the event carries the available buffer size as well as, timers, signals, POSIX aync I/O and user space events. In short for most sever applications you can get away with a unified (multithreaded) event loop if they use kevent() directly. I once wrote a simple file server in C to proof this point. The code was messy, but performed well. Now just add non-blocking open, close and fstat.
&gt; The best option on UNIX is the absolute worst option on Windows For years I've been under that impression as well. IOCP (completion based) is only way to do efficient IO on Windows, while epoll/kqueue (readiness based) is the only way on Unix. However, I'm not so sure about the first part anymore. I've become to suspect that the Windows NT kernel can actually do readiness based IO efficiently. See this little known project here: https://github.com/piscisaureus/epoll_windows Don't be fooled by the lack of updates there. The author is @piscisaureus who's one of the libuv developers and the code was merged into libuv about 2 years ago. There's some situations where apparently the code doesn't work, which is why libuv also contains a "slow" select/poll based readiness notification system on Windows. But if I understand correctly, those caveats are only for old Windows versions. According to this thread on the libuv forum: https://groups.google.com/forum/#!topic/libuv/S4U_JjbxW9M people have not been able to get libuv to use the slow fallback on newer windows versions. If this is indeed a way to get efficient and scalable readiness based IO on newer windows versions, then I think that would be great news which would pretty much obsolete the completation based model, at least for me.
*Iiiiiiiintersting*. Thanks for posting that.
Not necessarily. `size(A, n)` in Fortran gives the length of `A` along dimension `n`.
This was in reference to the &gt; because it tries to focus on safety and convenience rather than efficiency in your comment, reminding you that Rust's focus is "safety, concurrency, and speed". (And I'd guess the ordering of these three is also intentional.) That focus on safety is one of the _core_ features of Rust, and I, for one, love it because of that. So if you don't appreciate a system programming language that focuses on safety, there are _plenty_ of other languages available. Look elsewhere. This being said, I don't think there's any reason why Rust can't get (reasonably) fast IO. This is just not the focus at the moment. The focus right now is finishing up stabilizing the core language, and shipping 1.0. Things like better IO can be done after that, _in_ the language.
I thought it might be interesting to log my first forays into Rust, using my favourite method of language learning: try and do something fairly hard, while Googling a lot. The format is stolen from Artyom's excellent [Learning Racket](http://artyom.me/learning-racket-1) I hope this is useful or interesting to someone here. Also, if anyone could point me in the right direction concerning my Eq troubles, there might be a part 2...
This is really interesting, a very unique way of documenting learning experiences Kind of like a live feed to your brain, im looking forward to more. Also, why are you still using 0.11.0? wouldnt it be easier to update to 0.13.0?
The reason why you're having problems with `Eq` is because you're trying to implement a method that doesn't belong to `Eq`, if you read the documentation ( The search function on the Rust docs is quite good ), it says that `Eq` builds on `PartialEq`, and only adds a few rules that the compiler checks. So you want to implement `PartialEq` ( which has the `eq` method ) and then then have an empty implementation of `Eq`. You could do all that, but since the implementation would be very logical ( just comparing the fields ) you can make the compiler to it for you, just add `#[deriving(Eq, PartialEq)]` over your struct definition and it fills in the `impl` for you. 
The problem is that `Iterator` isn't a type, it's a *trait*. Now, aside from *not* being the type you're *trying* to return (which the compiler has told you the type of), it's also *dynamically sized*, and you can't return dynamically sized values from a function. Here's [a modified version on the playpen that compiles](http://play.rust-lang.org/?code=struct%20Test%20{%0A%20%20%20%20vec%3A%20Vec%3Cchar%3E%0A}%0A%0Aimpl%20Test%20{%0A%20%20%20%20fn%20chunk%28%26self%2C%20x%3A%20uint%2C%20size%3A%20uint%29%20-%3E%20std%3A%3Aiter%3A%3ATake%3Cstd%3A%3Aiter%3A%3ASkip%3Cstd%3A%3Aslice%3A%3AItems%3Cchar%3E%3E%3E%20{%0A%20%20%20%20%20%20%20%20self.vec.iter%28%29.skip%28x%29.take%28size%29%0A%20%20%20%20}%0A}%0A%0Afn%20main%28%29%20{}%0A). All I've done is replace the `Iterator&lt;char&gt;` trait with the actual implementation type. Yes, it's huge. Returning iterators from functions is kinda unpleasant at the moment.
To be honest it's easiest just to roll with the nightlies until 1.0 comes out. The majority of the community is using the nightly builds and the nightlies are usually _more_ stable than the older builds.
You were probably looking for let mut map = HashMap::&lt;String, uint&gt;::new(); It's unfortunate that the compiler gets confused like that (looking for `HashMap` and `str` in the value namespace etc) and doesn't offer any helpful guidance. :(
The problem you were having with the HashMap is that you can't have `HashMap&lt;box str, uint&gt;::new();` since the syntax is ambigious. You need to use `HashMap::&lt;box str, uint&gt;::new();` instead, which has been the topic of quite a few bikeshedding discussions since it's irregular. That also lets you have empty HashMaps, since then it doesn't have to do any type inference from the arguments of push() (I think the type signature would technically be HashMap::&lt;Box&lt;str&gt;,uint&gt; btw, `box` is the keyword for creating a new type of Box&lt;T&gt;)
~~Except that `Box&lt;str&gt;` is not a valid type, since bare `str` isn't a valid type either. The correct version of that would be `Box&lt;&amp;str&gt;`, but you probably never want to do that, since~~ you're basically just creating a worse version of `String`, so just use `HashMap::&lt;String, uint&gt;::new()` instead.
Is there any practical difference between let mut map = HashMap::&lt;String, uint&gt;::new(); and let mut map:HashMap&lt;String, uint&gt; = HashMap::new(); ?
Not really. The first one infers the type of `map` based on the type of the right-hand side, the second one infers the meaning of the right-hand side from the type it's required to be. The type should end up being the same either way, but with the second way you have to write `HashMap` twice. ;)
Not really, it's just a question about which way the type inference goes. You can in theory leave the type annotations completely out of the picture if you use the `HashMap` in your code in a way that the type inference can guess the type.
This is basically how I made 'Rust for Rubyists.' Thanks for sharing! It's hard to use docs for different versions, since Rust has changed so much.
`str` and consequently `Box&lt;str&gt;` is a valid type in nightlies thanks to DST, it's just pretty tricky to get a value of the type.
In this case you'd normally make a type alias for it. `type Chunks = std::iter::Take&lt;std::iter::Skip&lt;std::slice::Items&lt;char&gt;&gt;&gt;;` That makes you're code 10x more readable.
`Box&lt;str&gt;` is a valid type under DST, [see for yourself][1]. There is no way to construct a value of this type, however. [1]: http://play.rust-lang.org/?code=fn%20main()%20%7B%0A%20%20%20%20let%20s%3A%20Box%3Cstr%3E%20%3D%20unimplemented!()%3B%0A%7D
This is very interesting. Though the level of detail is a bit too much: version related problems, URLs, etc. Especially since this is a very new language, the docs and URLs are going to be rather brittle. There is no value in reading about it.
Thanks, I was wondering myself about how much detail was useful. This is great feedback.
I just went with the version that Homebrew had. This has turned out to be a mistake — I'm now using the 0.13.0 nightlies.
 let mut map = HashMap::&lt;String, uint&gt;::new(); o_O I never discovered this syntax and always wrote let mut map: HashMap&lt;String, uint&gt; = HashMap::new(); Why does this even compile? I always thought, that `::` is not required for generic structs (`HashMap&lt;String, uint&gt;`) and required for generic functions (`my_func::&lt;String, uint&gt;()`), but this is something counterintuitive and differrent from the both variants. I should probably go and reread the reference manual, again.
Actually, the Go runtime performance under GOMAXPROCS &gt; 1 is underwhelming as yet. There was a good analysis [[0]] done which discusses some of the work that has made it into 1.3/1.4; but, logical processors do not map to actual processors, thus any G mapped to an M may be mapped to a P, but that P may actually be any given processor at runtime. So... Yeah, it's kind of a big issue that I hope they address soon. The analysis discussed better thread affinity for Goroutines as an option of subtly getting increased M to actual P affinity; but I don't think that's in the works. [0]: http://www.cs.columbia.edu/~aho/cs6998/reports/12-12-11_DeshpandeSponslerWeiss_GO.pdf
`::` is required inside an expression, and not required otherwise.
Thanks for your example. [The docs](http://doc.rust-lang.org/std/cell/) don't clearly explain how to use `Cell`, and apart from informal discussions it seems hard to discover.
To elaborate for those who might not know: the point releases are just whichever nightly on the night they get released. They don't recieve any additional maintenance or anything. So the nightlies generally include more bug fixes. That said, I'd say 'more stable' is a bit strong.
Hmm, it seems weird that `LevelMap::new()` doesn't work. Is there any specific reason why that doesn't work?
For readability, I'd probably do: use std::iter::{Take, Skip}; use std::slice::Items; type Chunk = Take&lt;Skip&lt;Items&lt;char&gt;&gt;&gt;; struct Test { vec: Vec&lt;char&gt; } impl Test { fn chunk(&amp;self, offset: uint, size: uint) -&gt; Chunk { self.vec.iter().skip(offset).take(size) } } 
I loved the level of detail, and do think it's useful. Then again, my job is docs, soooooo
Thanks, now it definitely make sense. It seems like I missed this rule completely.
`type` defs are weird compared to `use`
Thanks for saying so. Now that I'm on the right version I reckon there'll be less to write about finding the right docs to read, but if something particularly confusing comes up I'll be sure to mention it.
Well, there's the `try!` macro for when you're doing your input inside of a function. You'll have to handle or suppress the potential errors at some point, though.
I'm pretty sure that any way you can maintain safety and efficiency for async IO will detract from convenience. Faux blocking IO is very convenient when it's not getting in your way, a proper async IO abstraction would likely use callbacks and result in rightward-drifting callback hell like node.js has. And that's what I meant, that it focuses on [safety, convenience] over speed, when I would rather it focus on [safety, speed] over convenience. As for doing it after 1.0, the language will have to remain backwards compatible, and that *includes* the standard library IO. You can't just go around smashing APIs after you declare the language stable. And the current API is more or less unsalvageable, as it's entirely based on faux (or real!) blocking IO, and has forced dynamic dispatch overhead (unless you want to remove libgreen or libnative, that isn't going away). It's not hidden behind a namespace or anything either, it's std::io, not std::io::blocking. So sure, you can introduce async IO after 1.0 under a new namespace, but it will always be the "secondary" feature, the one that isn't std::io.
I think it's absolutely valid feedback on how hard it is to navigate docs and other resources. It's nice to think that it will get better as the language and community mature, but it's by no means guaranteed, and awareness of the concrete problems helps.
Haha is that still not fixed. Worse than un-namespaced enums.
I'm so grad you wrote this as your experience with HashMaps was exactly like mine. IMHO, the implementation is not intuitive at all (especially declaration). In contrast, I was able to use D's hashmap without even looking up documentation! Anyway, great post!
You probably meant to submit this to /r/playrust 
It kills the ability to write generic code, which is a non-goal of rust 1.0.
While moving from Java to Rust I almost lost my mind. Still pretty close. 
The [io](http://doc.rust-lang.org/std/io/) library is marked experimental, which means its API can change. Rust 1.0 is about *language* stability, not *library* stability. Libraries, AFAIK, will be stabilized individually. Now, perhaps `io` will get marked stable at 1.0, but if it's not, then they're free to change it. I'm not sure what their actual plans are...
Is there any reason you can't just use `~` as part of the unix path to get user-dependent behavior? It'll always just resolve to the current user's home directory.
You can define a generic type like `T: Iterator&lt;char&gt;` to let it take any iterator over chars. See, for example, the [extend](http://doc.rust-lang.org/std/iter/trait.Extendable.html#tymethod.extend) function, which takes anything that's an iterator over the parametrized type.
My understanding has been that the tilde expanding to user's home directory is something that the shell provides, so it's not available when opening a file programmatically.
I have changed my ``chunk()`` method to return a ``Take&lt;Skip&lt;Items&lt;char&gt;&gt;&gt;`` type, and I'm trying to feed it to a method with this signature: pub fn draw_screen&lt;U: Iterator&lt;char&gt;&gt;(&amp;mut self, mut iter: U) I'm getting this error: the trait `core::iter::Iterator&lt;char&gt;` is not implemented for the type `core::slice::Items&lt;'_,char&gt;` I'm lost here. Shouldn't the type returned by ``take()`` be considered as an iterator?
An enum of the different types of iterator. That’s a generally superior (though a little more verbose) solution anyway.
You can do it with a custom `Iterator`-like trait with only the `next` method, or you could use an `enum`. I have code for the former, but it's not generally an idiomatic thing to do, so I won't link it unless there's a reason you can't use `enum` for this instead.
There is an [rfc](https://github.com/rust-lang/rfcs/pull/105) to allow what you want, but it is postponed for now.
`assert!(linked_list.head.is_none())`
Thanks for the write-up. It's funny, your approach to learning a language is so alien to me, I wonder if most people do it your way (google all the things) or my way (read all the official docs first, use the reference API docs for the exact compiler version I use, and google if all else fail).
According to [this](http://doc.rust-lang.org/std/slice/struct.Items.html), `Items&lt;_, A&gt;` implements `Iterator&lt;&amp;A&gt;`, so you need to use the bound `Iterator&lt;&amp;char&gt;` instead. [Example](http://play.rust-lang.org/?code=struct%20Test%20{%0A%20%20%20%20vec%3A%20Vec%3Cchar%3E%0A}%0A%0Aimpl%20Test%20{%0A%20%20%20%20fn%20chunk%28%26self%2C%20x%3A%20uint%2C%20size%3A%20uint%29{%0A%20%20%20%20%20%20%20%20test%28self.vec.iter%28%29.skip%28x%29.take%28size%29%29%0A%20%20%20%20}%0A}%0A%0Afn%20test%3CT%3A%20Iterator%3Cchar%3E%3E%28mut%20iter%3A%20T%29%20{%0A%20%20%20%20%0A}%0A%0Afn%20main%28%29%20{%0A}).
I do it your way :P
I believe there has been some discussion about making Iterator Box-safe again by splitting it up into a "core" iterator trait and an "extension" adaptor trait. Dunno where that's at, though.
Rust is getting into gamedev ([Piston](http://github.com/PistonDevelopers/)) and web stuff ([see here](http://arewewebyet.com/)) so server side apps and games are probably some of them. 
Please do consider using [XDG paths](http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html). That is, `$XDG_CONFIG_HOME/app_name/settings.conf`, where `$XDG_CONFIG_HOME` defaults to `$HOME/.config/` if not set. Main reason being that bloody dot-file clutter. You can get those variables via [std::os::getenv](http://doc.rust-lang.org/std/os/fn.getenv.html) There probably really, really should be a package for that kind of stuff. As for data paths: My personal favourite is to take them as arguments and have a tiny shell wrapper to set them, as it's the most flexible: Install your binary into `lib`, put the wrapper in `bin`. Of course, you can also pass the user config directory like that.
`~` just reads the `$HOME` environment variable, so you could use that instead.
I never really got the point of edit save reload (interpreted languages) for web. Interpreted languages are slow, and a website is where your application is hit _extremely_ often. (Makes me even bewildered about stuff like PHP which execute the application for each request instead of listening and serving) But yeah, as a developer's workflow it's certainly easier :)
I downvoted myself. Reviewing this conversation on a different day, I make no sense. I still side with the original post here though, which is for rust to take its sweet time and try to avoid as much backwards compatibility baggage as possible.
This is not the Rust you are looking for.
shite k thanks 
Interpreted languages are slow*er*. PHP is certainly not *slow*. With the right configuration you can squeeze a lot out of it (I'm using NGINX + PHP + OpCache + PHP-FPM right now). &gt; Makes me even bewildered about stuff like PHP which execute the application for each request instead of listening and serving Not happening if you use PHP-FPM. Basically it's a master process that spawns child processes (you can configure the minimum number of childs, maximum, maximum idle, etc), and when it receives a request it forwards it to one of the childs. The child does the work and gives the result back to the master, which gives the result to the webserver. The child does not die, so its ready to take the next request right away. You can obviously configure each children to handle a max of requests before "respawning" to prevent memory leaks and the likes But yes, I get your point :)
Rust's sweet spot is anywhere where speed and succinctness/expressiveness are needed. It can meet very tight tolerances because it is not garbage collected, but the type system lends itself to making expressive DSLs with semantic enforcement of constraints. So, applying business rules in complex event processing is where I see it making headway in the enterprise space. That said, we're using it to build ultra-low latency financial products. ;)
Oh, of course. PHP can be made to be efficient, but that's not what everyone does, is it? ;)
this is great, though a syntax highlighting code would be nice :) btw: Racket is one of those really great languages, very well thought out and documentation probably the best I've seen.
I agree that accepting and returning `&amp;str` for `convert_hex_to_base64` would be a better interface, and I will look in to return a Result as well to deal with potential errors caused by invalid characters. Why would it be better to return an iterator instead of, say, `&amp;str` (which I agree is better than a `Vec`)?
The ultimate language is one in which you can both compile and interpret it. :)
Because you cannot create a &amp;str. You could return a String. Still, better to just avoid allocating entirely. An iterator could be done thus: struct HexToBase64Iterator&lt;T&gt; where T: Iterator&lt;char&gt; { source: T, buf: Option&lt;([char, ..3],u8)&gt;, } impl&lt;T&gt; Iterator&lt;char&gt; for HexToBase64Iterator&lt;T&gt; where T: Iterator&lt;char&gt; { fn next(&amp;mut self) -&gt; Option&lt;char&gt; { fn to_digit(c: char) -&gt; Option&lt;uint&gt; { c.to_digit(16) } match self.buf { Some((data, ref mut index)) if *index &lt; 3 =&gt; { let ix = *index as uint; *index += 1; Some(data[ix]) } ref mut opt =&gt; { let mut chunk = 0u32; for _ in range(0u8,6) { chunk = (chunk &lt;&lt; 4) + if let Some(nibble) = self.source.next().and_then(to_digit) { nibble as u32 } else { return None }; } let mut output = ['\0', ..4]; for index in range(0, 4) { let six_bit_val: u8 = ((chunk &gt;&gt; 6 * (3 - index)) &amp; 0b00111111u as u32) as u8; output[index] = base64_table(six_bit_val) as char; } let first = output[0]; *opt = Some(([output[1],output[2],output[3]],0)); Some(first) } } } } It appears to give the same output as your function. I didn't add intelligent error checking.
You are not alone. I think with the right tooling - something like "cargo watch" with would watch the src directory and recompile and reboot the app on changes - you could get close to edit-save-reload.
We are going to be using this in Iron to avoid the combinatorial explosion of Response constructors (https://github.com/iron/iron/pull/207) and it's been working really well! Now you can do stuff like: Response::new() .set(Status(status::Ok)) .set(ContentType::new("application", "json")) .set(Body(Path::new("config.json"))) and it *just works*. The best part is you can use the same modifiers later to modify Response, and the modifiers are only defined once: res.set_mut(ContentLength(10000));
You can also do `if let Some(_) = *self { true } else { false }` now.
Why is there is_none()? Isn't (value == None) enough?
It won't work if the inner type does not implement PartialEq. [playpen example](http://play.rust-lang.org/?code=struct%20Foo%3B%0A%0Afn%20main%28%29%20{%0A%20%20%20%20assert!%28None%3A%3A%3CFoo%3E%20%3D%3D%20None%3A%3A%3CFoo%3E%29%3B%0A}) 
Hey! Glad to know someone else is doing the Crypto Challenges in Rust too. I'm currently starting set 3 and in the middle of a huge refactor. I learnt too much up to this point to leave my code untouched :P I'd add to the other suggestions that `base64_table` could be: static BASE64_TABLE: &amp;'static [u8] = ['A', 'B', etc., '/'];
&gt; the point releases are just whichever nightly on the night they get released. They don't recieve any additional maintenance or anything. We should be seeing this notice at the top of rust-lang.org in bold red font, especially when we know 0.12 fails to compile `panic!()`.
Honestly I don't see a language without garbage collection getting really big. It would appeal to the current c and c++ devs but that's it.
OCaml? Common Lisp? ;)
You want the /r/playrust subreddit. This one is for the Rust programming language.
&gt; That said, we're using it to build ultra-low latency financial products. ;) Ooh, would like to hear more about that... :)
&gt; Honestly I don't see a language without garbage collection getting really big. Why not? There was a time when folks didn't think GC would ever take off (damn lispers!). Rust is cool because it makes it easier for more programmers to take control over allocation without feeling like they are walking on eggshells.
You can get really big just by appealing to C and C++ devs.
The ironic thing is that garbage collection was probably discarded initially - in historical perspective, not from Rust's perspective - because they were thought to be too inefficient for the current hardware, while now what might hurt Rust adoption from the non-low-level crowd is that hardware is so fast; *No automatic memory management? Oh screw that, computers are plenty fast enough for what I'm doing.* (not that there's anything wrong with that reasoning.)
Yes. It adds a virtual function call and destroys any chance of inlining etc. Furthermore, it just can't be done because `Iterator` isn't object-safe.
This is very cool! I've been using the tamer version of the "builder" pattern (no generics, so you'd have explicit `status`, `content_type`, and `body` methods in your example). But I've really grown to like it. Both `rust-csv` and `docopt` have been updated to use it. :-)
There's also https://github.com/o11c/rust-xdg
&gt; The ironic thing is that garbage collection was probably discarded initially because they were thought to be too inefficient Actually the opposite is true. Garbage collected smart pointers were the first to be implemented, and GC was expected to be the main form of allocation used. But in practice, once unique pointers and references came on the scene, people ended up just using them for everything, and GC became the exception. &gt; Oh screw that, computers are plenty fast enough for what I'm doing Yes, CPUs are continually getting faster, *but memory access/allocation is not*. Software is getting slower faster than our computers are getting faster. &gt; might hurt Rust adoption from the non-low-level crowd *Might*. I think it is still too early to predict how folks will react to Rust's method of memory management - but it seems like lots of folks coming from dynamic, GC-ed langs find it perfectly ok to grasp, and are becoming advocates of the language.
Hmm. It has "perfect" reference counting, just like Python. It's just that the counting is a bit more explicit.
Why is it superior? For example, in QuickCheck, shrinking is done by returning a stream of smaller values, which is important because the full space of all shrunk values often doesn't need to be explored. (i.e., Laziness is helpful.) Here is the current trait: pub trait Arbitrary : Clone + Send { fn arbitrary&lt;G: Gen&gt;(g: &amp;mut G) -&gt; Self; fn shrink(&amp;self) -&gt; Box&lt;Shrinker&lt;Self&gt;+'static&gt;; } (Where `Box&lt;Shrinker+'static&gt;` is a trait that implements `Iterator`.) Specifying that clients select from a *closed* list of iterators seems wildly inappropriate to me. Therefore, the trait leaves the iterators to be polymorphic. In Rust, the only way I know how to utter that is with trait objects. Who cares if clients implement `shrink` by returning a `Map` or `Filter`? I don't and I don't think I *should*.
Just imagine a form of inference which could allow `res.set(…);` to be `fn(&amp;mut self, …)` and `x = res.set(…)` or `res.set(…).set(…)` to be `fn(self, …) -&gt; Self`…
As soon as I saw this, I started looking into how to support this on Windows. It looks like it needs COM. You can get a token that represents the folder, but to get the actual *path*, I believe you need to use a COM interface (the docs on this are pretty obtuse). That's a problem because I put my COM bindings on the back-burner until the "thing that satisfies Servo's DOM needs" problem got resolved. I might try porting across the bare minimum needed to support the interface needed, but it's still likely to be a *pile* of stuff.
This is a really cool use of the type system! (Oh look it's another Reem project, of course it is) I assume since this is all static/generic all the abstraction should get compiled away, but I'm still curious: did this at all affect the performance of Iron?
It is actually more flexible to use `&amp;mut self` for the builder pattern if possible (I.e. return `&amp;mut Self`) and there's no reason that `set_mut` can't do this.
&gt; Actually the opposite is true. Garbage collected smart pointers were the first to be implemented, and GC was expected to be the main form of allocation used. But in practice, once unique pointers and references came on the scene, people ended up just using them for everything, and GC became the exception. I was referring to a historical perspective on programming languages and computing in general, not Rust in particular. &gt; Yes, CPUs are continually getting faster, but memory access/allocation is not. Software is getting slower faster than our computers are getting faster. It's too bad that that fact can be very non-obvious to people who program at a "higher level" (such as myself). Until we read up on the subject or our program reaches a sudden breaking point, of course. &gt; Might. I think it is still too early to predict how folks will react to Rust's method of memory management - but it seems like lots of folks coming from dynamic, GC-ed langs find it perfectly ok to grasp, and are becoming advocates of the language. As you can see in my [other comment](http://www.reddit.com/r/rust/comments/2l3l07/rust_is_undoubtedly_one_of_the_upandcoming_big/clrd4kx), we seem to be in agreement on that. :)
The use of `ptr::read` in `set_mut` is highly unsafe. If the modifier fails destructors may be run twice on the same data. I would recommend not using `set_mut` at all until this is fixed.
Are you compiling with optimizations? `-O` to `rustc` or `--release` to `cargo`. As another point, the hash function we use by default is very strong and resistant to DoS attacks, but is also quite slow. They define their own hash function that is much cheaper, you might want to do the same here.
Yup, I'm already compiling with -O in the latest nightly rustc. I'll try it with a custom hash function, thanks. 
If it used `&amp;mut self` everywhere, you would not be able to chain from `Response::new()`. You would need to first store it.
Yeah, that's an example where it is not possible. It is things like the stdlib `Command` builder that work well with `&amp;mut`. In any case, it seems `set_mut` does offer chaining, so the API supports this already (ignoring the internal incorrectness of `set_mut` for now...).
The performance and memory usage are superior; also the code can be reasoned about statically. That an abstraction is being leaked is a different issue, one for which boxing the value and decreasing efficiency is emphatically the wrong solution for a language like Rust. A proper solution for this is to allow the likes of `fn(…) -&gt; impl Iterator&lt;T&gt;`, something I expect Rust to get eventually. In the example you gave initially, the potential iterators given *were* a closed set, so there is nothing wrong with that. There is no need to make something infinitely polymorphic at runtime when it is not necessary.
Rust’s `HashMap` defaults to using a cryptographically secure random number generator with randomness to avoid certain types of DoS attacks, where the C++ is using a laughably poor but fast hash function—the first character of the string. I would expect the LuaJIT one to be using an insecure but faster random number generator. This is an *extremely* common mistake in such comparisons.
If you read it as a diary and a stream of thoughts...
After some discussion on IRC, it appears the only way to make this safe is to remove the use of `ptr::read` and instead have `Modifier::modify` receive `&amp;mut F`. I have made this change, and the library should be safe to use now.
I haven't really investigated the speed implications, but I really can't imagine that this will cause any speed differences. In fact, due to the new implementation of the `Body` modifier, we can now be far more specific in the treatment of different body types so we can significantly reduce allocation in many places and allow streaming data when appropriate.
Sorry, you’re right. s/random number generator/hash function/.
Ok, after digging around for examples on how to implement custom hashers, I came up with this very simple and poor hash function, which seems close to the C++ hash function used by the benchmark: struct SliceWriter { hash: u64 } impl Writer for SliceWriter { fn write(&amp;mut self, buf: &amp;[u8]) { for byte in buf.iter() { self.hash += *byte as u64; return; } } } struct SliceWriterHasher; impl Hasher&lt;SliceWriter&gt; for SliceWriterHasher { fn hash&lt;T: Hash&lt;SliceWriter&gt;&gt;(&amp;self, value: &amp;T) -&gt; u64 { let mut state = SliceWriter { hash: 0 }; value.hash(&amp;mut state); state.hash } } This speeds up the benchmark 2X, but it's still much slower than the other languages. There seems to be a lot of boilerplate around implementing a simple custom hasher, and I feel like the reason it's still slow is I'm not doing it right. :(
Haskell's Yesod framework does this ("yesod devel"), and displays friendly messages while the project is compiling, it's quite nice. screenshot: http://imgur.com/2mPfr2K
You gotta pick your battles. Safety *is* important, but so what? If you have a meaningful alternative to 'use unsafe code' to get async io in rust, because no std library for it currently exists, then speak up. Otherwise you're just meaninglessly grandstanding about the importance of safety. Its not done yet. we need a solid sync story before we tackle an async one. What's your point? Dont do anything because it might be unsafe? People *should* be writing safe wrappers of these things to explore api possibilities. Grandstanding about safety acheives nothing; the OP was spot on: use an unsafe binding to a c library or an existing wrapper like mio.
The [docs list it](http://doc.rust-lang.org/nightly/std/collections/struct.RingBuf.html#method.pop_front). It looks like 88b6e93d does not include the commit that removed everything (21ac985af) so you still have to import the appropriate trait (`std::collections::Deque` iirc).
Having implemented quickcheck before, I absolutely agree with you. The performance gains are from only generating shrinks lazily, just as many as needed, not from inlined code.
What is the program you are using? [Using your hash function](https://gist.github.com/nwin/26407ff6f1a05ea7e084) it's faster then the C++ version (0.4 vs 0.6 s). Be also careful to use the same compiler backend (use clang instead of g++) to have comparable results (btw. preallocate the map would save another second but this is not what the). Don't worry about the boilerplate, that is optimized away.
FnvHashMap is rather fast, and so is Jurily's xxhash. And both were slower than the builtin inplace sort algorithm on an array, plus bsearches, for a small number of lookups. Always benchmark :)
This should probably be mentioned in the docs (and how to go around it), sometimes you don't really want crypto security.
My interest in using Rust for maintaining big codebases is because the error classes that gets eliminated reduces the cost significantly. In the Piston project we try to drive the cost further down through collaboration. This means a few people can do more, which matters a lot in the gaming industry and cross platform development.
Haskell!
You can't get the APPDATA environment variable on Windows with `std::os::getenv`? 
True, but it's just as unwieldy. If `matches!` existed, there wouldn't even need to be an `is_some` method, but as it stands, it's still useful due to a lack of a concise alternative. As mentioned in the rfc, the `matches!` macro seems to have orthogonal use cases to `if let`. I like `if let` best for side effects in a single branch, because if you're using the `else` branch anyway, then `match` is clearer imo.
&gt; couldn't you just have the Linux executable store the config in /home/{}/.app_name/settings.conf (set with conditional compilation), and then load in the user's name into that string at runtime? You shouldn't rely on `$HOME` being `/home/$USER`. This is just a convention, you can set it to any other path.
You are awesome! I think we can use this in Conrod.
You can, but you're not supposed to. My understanding is that it exists for the same reason that the "Shell Folders" key exists in the registry: for backwards compatibility. Incidentally, `%APPDATA%` is tied to your roaming profile, which you shouldn't be using to store large amounts of information, or cached information. Minecraft gets this wrong, Firefox gets this wrong, Samsung gets this wrong, Dropbox gets this wrong... pretty much everyone, presumably for the same reason *NIX programs continue to drop crap in `~`. Another reason not to use it; I believe that previously, local data was a subfolder of `%APPDATA%`, but now it isn't. Again, I don't believe it's the "correct" way to get this information. Heck, checking the QStandardPath docs, that looks like it's wrong as well, though for a different reason. It says "DownloadLocation" should be the Documents folder, as opposed to the actual "Downloads" folder. There's a blog post where Raymond Chen talks about this, but I can't find the link at the moment.
 for i in range(0, len) { // this is faster than using an iterator, for some reason This might be due to LLVM being able to optimize the code better when `len` is a constant (also eliminates bounds checking in `words[i]` as well). Are you sure that the hash function is the only cause? Might help to profile this.
Go hits a nice spot. It compiles to native code but the developer doesn't have to worry about garbage collection. It has the feel of a high level language and the features of a low level one. That's why it has caught on. D also has automatic garbage collection. I am telling you. Most devs do not want to deal with garbage collection. They will do it if they have to but they don't want to. 
&gt; The Programming Languages Beacon I am not sure how to read that page. By really big I mean like Java, C#, python, ruby, php etc. 
I agree but it's still a niche.
`-O` selects `--opt-level 2` I think. Might be interesting to see if using `--opt-level 3` will have any differences. Also, are you using gcc for the C/C++ code? How does it perform with clang? There may be still some opportunities for optimization in `HashMap` and gcc still optimizes better than llvm overall in my experience. I wonder if a higher initial capacity will make any difference in this case.
It's not only some C and C++ devs that take interest in Rust. It's also folks from more garbage-collection-oriented and/or dynamic languages that are interested in Rust. The C and C++ folks have been using C and C++ for performance. The other ones have been using the other languages for safety and ease. Rust provides both, however. As a C++ programmer I like being able to use a safe language like Rust. A Python programmer who switches to Rust likes the performance properties (but wouldn't have dabbled in C and C++ because it's too easy to shoot yourself in the foot).
Sadly, its also horribly slow, even on small apps, so this message becomes your close friend. Still, the approach they take (keeping a development server running that internally speaks to the actual app) can be adopted.
I do get slightly better performances under O3, but on the order of 10% (using `-O` the program above executes in 1.6s ±0.1s, with `--opt-level=3` it's a much more stable 1.5s ±0.05s, measured via `time`)
You are wrong on one account: most websites are not hit *extremely* often. Much in contrast, most web properties are running in performance windows that don't make a modern machine sweat. Human-generated traffic is rarely a problem nowadays, unless you can attract thousand of concurrent users (which is quite a number, if you calculate the number of real humans needed for that). Also, most of it is IO and database bound, which modern interpreters are perfectly well at handling. You won't get much faster there by using Rust in the application layer. I ran ad playout and real-time video communication off Ruby infrastructures (not Rails[1], though, but Padrino) and never had serious problems with the language runtime. Infrastructure , HTTP and DB knowledge is a far more important in that space and can yield benefits no amout of micro-optimization in the app can. All that is different if you are working at unicorn scale (e.g. Twitter, Facebook), but 99% of users are not working at unicorn scale nowadays. Also, Facebook runs using PHP and Twitter still has quite a few blocks of Ruby (although they certainly moved to different systems when crossing to unicorn scale), so measured at their successes, picking Ruby and PHP for such tasks didn't seem to be a bad choice. I still hope for Rust to move into that space for several reasons (especially lying in deployment and the ability to build a new ecosystems without much legacy), but performance is none of them. [1]: The default Rails stack is not built for high-traffic, small work packages. It can be configured to be well capable of handling that, though.
That's most likely all IPv4 interfaces and not all interfaces. IPv6 is `"::0"`. Or does Rust automatically apply/remove the flags which are necessary to listen on both types when only one is specified?
Most of the serious performance problems in web apps I see nowadays are mishandling of data (bad schema, bad/missing database indices, too many queries for too small tasks, etc.), so even if all PHP code were Rust, I'd not see any of those problems go away. PHP in default deployment is already adequate for many tasks.
&gt; Rust's sweet spot is anywhere where speed and succinctness/expressiveness are needed. unfortunately, Rust isn't all that succinct because everyone here likes explicitness.
Of course, though I was just focusing on the issue of spawning a new process for an interpreted language every hit. Though with Rust you can use syntax extensions to enforce extra guarantees on queries, etc. Not perfect, but much better than unchecked usage.
If I ask what is the *scope of the function*, what am I referring to? I think the scope overlaps with existing terms that describe the lexical scope of a symbol. I think a better tutorial that describes how lifetimes work and which explains that demonstrates that lifetimes are **descriptive** and not **prescriptive** (i.e. compiler uses them to figure out if you didn't abuse them and you can't override it - which was my initial guess) [I'm quoting a post from the link, that put it quite succinctly).
Cache and JIT warmup are valid aspects of performance, and avoiding JIT warmup is an advantage of Rust that shouldn't be ignored. It might be better to benchmark from cold caches, and test short and long runs so you can see how much it matters. You can invalidate CPU caches by running some large unrelated program, and then drop memory caches. On Linux you can drop memory caches with: sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches Presumably other OSs have something similar.
`trait SomeTrait : Show { }`
&gt; Facebook uses a "fixed" version of PHP and they would be precompiling or something to help optimize it (like the PHP-FPM example below). This is not what most people do. Well, but that is also a development that came after they reached "unicorn scale". What I see, though, is Rust as a better "drop to the metal" language for many of those environments.
This looks really nice. The example here explains the reasoning behind this pattern much better than the example in your readme - perhaps you could use this example instead? The `ModifyX` example you have really doesn't show a clear benefit to this approach.
Haha, thanks dude. There's always the wayback machine :)
Thank you. I've searched "rust trait inheritance" and many variations thereof and only got results about struct inheritance. If anyone else is reading this and wondering how to make it work, this code runs: use std::fmt; trait SomeTrait : fmt::Show {} impl fmt::Show for Box&lt;SomeTrait + 'static&gt; { fn fmt (&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { (**self).fmt(f) } } struct A; impl SomeTrait for A {} impl fmt::Show for A { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { write!(f, "yippers") } } fn main() { let mut v: Vec&lt;Box&lt;SomeTrait&gt;&gt; = Vec::new(); v.push(box A); println!("{}", v[0]); // prints "yippers" } I'm still shaky on the whole lifetime thing, and I'm not sure that I actually want 'static, but this works for now. Thanks so much.
If one is specifically benchmarking startuptime, then `time` is valid. Otherwise one is [confounding](http://en.wikipedia.org/wiki/Confounding) variables. 
It is certainly no Haskell, however I think it can be made to be quite expressive in the top level APIs at least. Compared to something like Scala, it is almost as succinct, but I actually prefer Rust's lack of implicits (so it is necessarily explicit). Because implicits, in the common case, do more for obfuscation than succinctness. IMO 
We are a tiny start-up (http://12sidedtech.com, apologies for the intentionally vague website) currently focused on renovating the exchange/ECN space. Most of our code is very low-level C. A few key parts of the system are in Rust and we are certainly scaling that up. There are two major blockers for us for total adoption: 1. Lack of custom allocators [which are being addressed](http://discuss.rust-lang.org/t/pre-rfc-allocators-take-ii/480) 2. Ownership enforcement is antithetical to invasive data structures, which we make use of quite a bit. At some point we might put together an RFC if we can come up with a way to reconcile this. 
Honestly, the missing GC is the only reason Rust has a change to get big. There are myriads of languages out there with a GC and no one really lifts off, but there is only one serious one, I know of, without GC.
In my opinion, lifetimes are the truly unique concept Rust brings to the table compared to most languages. I think that conflating them with the related but different concept of scopes (which people are already familiar with) will just be *more* confusing. Overloading an old term for a new concept just because people feel comfortable with the old term seems counter-productive. People will have to learn the new concept anyway, so I think it would be less confusing in the long run to also learn a new term. Edit: Also, I find the learning argument unconvincing. I understand that using an analogy to scopes is helpful for some people in understanding lifetimes, but that doesn't make them the same thing. We can add this analogy to the documentation to help people understand without confusingly giving two concepts the same name.
This is really nice. Very simple API, but so powerful.
Yeah, only the biggest in the industry. 
it looks silly because you're trying to oneline something that should be broken down into steps... The ok is should really be for matching and error handling: use std::io; fn main () { let inp = io::stdin().read_line(); match inp { Ok(v) =&gt; println!("input! {}",v), Err(e) =&gt; println!("error! {}",e) } } 
I've tried a couple times to design an intrusive collection ala [Boost.Intrusive](http://www.boost.org/doc/libs/1_56_0/doc/html/intrusive.html) and indeed things get complicated quickly. If you ever find a way to make this work, I'd be happy to hear about it!
Literally just what everybody else said.
Should it? I mean, after you've profiled your application and realized you could speed things up by switching the hash function used, you're already pretty close to having the answer. I am afraid that if the documentation put too much emphasis on the default hash being *secure but slow*, too many people would immediately switch to a faster one which would completely break the intention of providing security by default...
Looking forward to part 3
I agree with this. Lifetime and scope is used about two different things.
It's coming soon! It was taking a while to write, so I figured I'd split up the post into two.
What is the output format of serialize::json? Is it JSON? Your table says Rust.
Not emphasis, just a note. If the default for many programmers is a fast but insecure hash function, we should probably mention that we're not sticking to that default.
Compared to C++ or Java it's a dream.
On my Linux VM, both gcc and clang produce roughly the same results, around 0.2s. I should mention that I'm using the optimzied C++ version posted in the second last comment in [benchmarks link](https://gist.github.com/spion/3049314), where they use raw char* rather than C++ strings. Even if I use the original C++ version, clang still seems a bit faster (and gcc somehow optimizes it to be roughly the same as the optimized version My custom Rust hasher is runs in 0.3 seconds, so around 50% slower than the optimized C++ version. What OS and arch are you running on? 
Java is not a high bar at all. C++ is ancient with a lot of cruft.
Rust's adoption is also dependent on a good IDE being available.
With this we will lose awesome posts titled `I have a lifetime problem`. Too bad! :D
I barely get any improvement using --opt-level=3 compared to -O. I benched against both gcc and clang, and gcc is slightly faster. Both are about 50% faster when using the optimized C++ version in the [benchmarks link](https://gist.github.com/spion/3049314), and that's with my custom hasher. 
The word scope is already used for something else...
Isn't the boot time be an advantage to Rust, since it doesn't need to boot up a runtime? Anyway, when you're doing so many iterations, the boot time and IO(which is outside of the bench loop) should hardly matter right? I increased the iterations by 50X (50 million iterations), and averaged several runs. I replaced the default C++ version with the optimized version in the second last comment of the [original benchmark link](https://gist.github.com/spion/3049314). Rust with custom hasher : 14.5s Rust with FnvHashMap : 21.4s Rust with default HashMap: 33.1s GCC : 10.4s Clang : 10.6s LuaJIT: 2.5s Surely there's a way to make the Rust version at least as fast as C++'s right?
Yeah, seems like it will need (or has needed, depending on whether it is already done), some tweaks to deserialization as well. Also no numbers for `rust-capnproto`? All in all, I quite enjoyed. Looking forward to part 3 as well :D
Apart from the obvious fly in the ointment of header files - there are situations where C++ is more succinct, due to overloading/default params, and its' handling of unsafe code. Rust's expression syntax is definitely more pleasing &amp; elegant though. and of course you can argue those shortcuts just make C++ more error prone.
Interesting difference.. I'd explain it slightly differently:- it might let more people do less. i.e. .. increase the number of contributors, because you have more confidence in checkins etc. I would speculate that will help Rust vs C++ for collaboration across the internet. The opposite is a language that lets fewer people do more. (things like crazy unrestricted macros etc.). Choices in Rust like avoiding custom operators,default params etc take a stand against this. total team productivity vs individual productivity .. not necessarily the same thing.
I hope the language landscape diversifies and specialises, languages existing in a continuum, with the same concepts cross pollinated between them . More non-garbage collected languages needed. (saving watts) Everyone hates C++ because... it's a mix of different use cases. As soon as you build one persons' "ideal language" we polarise. I don't think C++ can have a single successor. I would like to use Rust for game development, but whilst it's capable here I don't think it will be perfect or necaserily an improvement over C++; - jonathan blow's efforts might be worth following there. For that domain, he's got the right idea about productivity(fluidity)+performance rather than safety+performance, IMO. I hope his language copies a lot of Rusts' elegance, or eventually Rust gets some sort of 'liberal' flag.
agree 100%, this is something I really want - but with syntax/semantics designed for the compiled, no GC case first and foremost. This is why i liked the original sigils. throwing ~/@ around, I could see it doing the job of an adhoc scripting language well enough to benefit from that unification. I wonder if they'll ever come back in future. jonathan blows' language demonstrates compile time byte code execution that seems to indicate he'll get there, and he has something more like those sigils.
I usually rejoice when I see usability /performance improvements. Most of the time security /safety gets all the love. 
I've tried the enum option before, but I come unstuck trying to specify the various iterator types. E.g. if I use the 'substitute a dummy type and get the real type out of the rustc error message' technique I get something like: ../racer/nameres.rs:763:15: 763:17 error: mismatched types: expected `Foo`, found `core::iter::Chain&lt;core::iter::Chain&lt;core::iter::Chain&lt;core::iter::FlatMap&lt;'_,proc() -&gt; collections::vec::MoveItems&lt;racer::Match&gt;,core::option::Item&lt;proc() -&gt; collections::vec::MoveItems&lt;racer::Match&gt;&gt;,collections::vec::MoveItems&lt;racer::Match&gt;&gt;,core::iter::FlatMap&lt;'_,proc() -&gt; collections::vec::MoveItems&lt;racer::Match&gt;,core::option::Item&lt;proc() -&gt; collections::vec::MoveItems&lt;racer::Match&gt;&gt;,collections::vec::MoveItems&lt;racer::Match&gt;&gt;&gt;,core::iter::FlatMap&lt;'_,proc() -&gt; collections::vec::MoveItems&lt;racer::Match&gt;,core::option::Item&lt;proc() -&gt; collections::vec::MoveItems&lt;racer::Match&gt;&gt;,collections::vec::MoveItems&lt;racer::Match&gt;&gt;&gt;,core::iter::FlatMap&lt;'_,proc():Send -&gt; collections::vec::MoveItems&lt;racer::Match&gt;,core::option::Item&lt;proc():Send -&gt; collections::vec::MoveItems&lt;racer::Match&gt;&gt;,collections::vec::MoveItems&lt;racer::Match&gt;&gt;&gt;` (expected Foo, found struct core::iter::Chain) I then try to use this type, substituting '_ for 'static, to get: extern crate collections; extern crate core; use core::iter::{Chain,FlatMap}; use collections::vec::MoveItems; use core::option::Item; enum MyIter { I1(std::option::Item&lt;Match&gt;), I2(Chain&lt;Chain&lt;Chain&lt;FlatMap&lt;'static,proc() -&gt; MoveItems&lt;Match&gt;,Item&lt;proc() -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;,FlatMap&lt;'static,proc() -&gt; MoveItems&lt;Match&gt;, Item&lt;proc() -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;&gt;,FlatMap&lt;'static,proc() -&gt; MoveItems&lt;Match&gt;,Item&lt;proc() -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;&gt;,FlatMap&lt;'static,proc():Send -&gt; MoveItems&lt;Match&gt;,Item&lt;proc():Send -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;&gt;) } impl Iterator&lt;Match&gt; for MyIter { fn next(&amp;mut self) -&gt; Option&lt;Match&gt; { match *self { I1(ref mut it) =&gt; it.next(), I2(ref mut it) =&gt; it.next() } } } But rustc then says: ../racer/nameres.rs:689:42: 689:68 error: explicit lifetime bound required ../racer/nameres.rs:689 I2(Chain&lt;Chain&lt;Chain&lt;FlatMap&lt;'static,proc() -&gt; MoveItems&lt;Match&gt;,Item&lt;proc() -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;,FlatMap&lt;'static,proc() -&gt; MoveItems&lt;Match&gt;, Item&lt;proc() -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;&gt;,FlatMap&lt;'static,proc() -&gt; MoveItems&lt;Match&gt;,Item&lt;proc() -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;&gt;,FlatMap&lt;'static,proc():Send -&gt; MoveItems&lt;Match&gt;,Item&lt;proc():Send -&gt; MoveItems&lt;Match&gt;&gt;,MoveItems&lt;Match&gt;&gt;&gt;) ^~~~~~~~~~~~~~~~~~~~~~~~~~ And I get a bit stumped. I think it wants explicit lifetime bounds for the procs() in the lazy iterator, but I'm not really sure how to go about specifying this so I don't get much further.
Yes, I totally agree. That is why I think Jonathan Blow's language will make a positive impact on the industry, because we need something to compare programmer productivity in Rust. In my case, I am more interested in making libraries work together, than writing new code from scratch for new prototypes. When I said few people can do more, I meant you can get further by building upon the existing libraries that never crashes, with better integration because dependencies are decoupled. I think the noticeable difference in maintenance is when your project starts to get really big, which usually means bugs and support eating up all your available time, but with Rust it could be possible for a few people to live on a product with a large code base. So "doing more" can either mean you produce more lines of code, or doing better with the product you sell.
C++ may be occasionally more succinct at the level of function calls, but in addition to Rust's expression syntax, there's also iterator operations, methods on things like Option, etc. And if you consider pre-C++11, which many places are still stuck on, there's also type inference and lambdas (both of which are more succinct even than C++11).
This seems like a partial path to lenses?
Is [the Rust reference](http://doc.rust-lang.org/reference.html#match-expressions) what you're looking for? Secondly: Rust is still in a state of flux: it is going to be a few months before even the language itself is actually stable, and longer than that for the standard libraries. The docs are *remarkably* complete given that the language keeps changing underneath them, but the language *is* still changing underneath them so they aren't perfect. If you really want everything to be nice and orderly, you might want to wait a few months before committing to a language where code you wrote last week might not compile this week. That said, Rust is a *really* fun language to learn, and I've found watching it evolve to be both fun and educational, so if you're at all interested, I would say stick with it!
Thanks for replying. &gt; so if you're at all interested, I would say stick with it! I definitely plan to do that. It's clear to me that Rust still is &lt; 1.0, but without documentations I can't get along with it seems really tricky to learn. However, how are you getting used to it? Do you follow up-to-date blog post or do you just "play" around?
Yeah, as you noticed, `'static` lifetime makes your `impl` less generic. This one should work: impl&lt;'a&gt; fmt::Show for Box&lt;SomeTrait + 'a&gt; ... Roughly this means that the implementation will work for trait objects which contain data of any lifetime, not just `'static`.
Ok, thanks. I tried a few different things like that but not quite that and kept getting errors.
&gt; I can't pass in any variables to the system, etc. What do you mean by this? If you're trying to pass variables into the function that `native::start` calls, then the `proc` is a closure, that is, it can capture variables from the parent scope(s) and so interact with them that way. E.g. let number_of_cats = 10; native::run(proc() { println!("there are {} cats", number_of_cats); }) (NB. I used [`native::run`](http://doc.rust-lang.org/native/fn.run.html), which is nearly identical but avoids the need for argc/argv.)
&gt; Are there any other concurrency libraries I could possibly use, or should I just stick with pthreads via C/C++ on this one? You can easily make use of POSIX / win32 threads from Rust. It doesn't take much work to wrap it into a safe API and it won't require starting a runtime. The standard library is about to undergo major reform to avoid requiring a managed runtime for this stuff.
The reference that wacky linked to is what you want. Also, [Rust By Example](http://rustbyexample.com/) is a more in depth way of getting into Rust programming over the Guide.
The reference manual once had nice detailed table of contents with all the basic language constructs mentioned, including keywords (and `match` in particular). It served as a good "one page overview" of the language and was quite convenient for quick navigation. It was supposedly killed by Steve in the process of improving the document.
Unless I missed some, that has been fixed for a couple months now. That was one of the embarrassing performance bugs. There should be no allocations now in `serialize::json` serialization. 
You might want to try going back to boxing an iterator because those types are ridiculous. You can see how [QuickCheck does it](https://github.com/BurntSushi/quickcheck/blob/master/src/arbitrary.rs#L56-L67), which works on the latest nightly. But everyone here is right to caution you about them. There are trade offs involved here.
Yeah, the expectation for most programmers would be that the default hash function is fast. Most people don't care about their hash functions being secure, so they'd have to do extra work to get what they want and should be told how to do that in an obvious place.
I agree that a more comprehensive language reference would be nice, but the standard library docs are really useful once you know the basics of the language. http://doc.rust-lang.org/std/index.html Edit: When it comes to new features, the best place to look is the RFCs repo. Lots of acronyms! DST, HKT, UFCS! https://github.com/rust-lang/rfcs
Not with that syntax, directly (though I think it might become possible in the future) But you can define a trait `SomeTraitAndShow` that inherits from both, and is generically implemented for all types that implement them, and create a trait object from that.
Rust docs are extremely helpful. You should look into the rust reference guide or rust by example. There are a handful of tutorials on how to do various things as well. If you can manage getting the hang of the rust lang /std/ you won't need to look at anything else ( maybe the source when the docs aren't updated enough, but that almost doesn't happen in my cases) 
As a note of constructive criticism, could the non-api rust docs maybe include a deeper table of contents? Especially for the language reference, section headings are often too broad for my needs and I can sympathize with /u/SittonMac's struggle to find relevant subsections.
I just play around. For example, I just finished writing some standard [sort algorithms](https://github.com/wackywendell/rustscripts/blob/master/src/sorting.rs); you learn pretty quickly that way. That said, I've also found the documentation to be *really good*, in general. Between the guide and the reference, understanding the main language wasn't too challenging; after that, its just a matter of finding what you need in the standard libraries, and the search function there is *awesome*.
Who could we talk to about that? It sounds like it would be an exceedingly easy improvement with a solid use case.
/u/steveklabnik1, I summon you! (does that work? Maybe only with Reddit gold?)
&gt; so it has nearly zero serialization speed. I guess that should have been serialization cost
I'm trying to cobble something coherent together from "reach" and/or "extent". Maybe someone's cleverer than me.
Right you are :)
By most measures, C/C++ are really big, e.g. [TIOBE](http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html), [lang-index](http://lang-index.sourceforge.net/).
&gt; the guide[2] (&lt;--- did you see this?!) From the main post: &gt; I followed the guide, which was great in my opinion, but now it seems like nowhere to go. 
Capnproto is fast! Does msgpack not belong here? 
&gt; It was supposedly killed by Steve in the process of improving the document. Whoah whoah whoah. To be clear here, the deeper TOC was killed _by accident_ by me, because I didn't realize you needed some special annotation to have the deeper one. There's an open ticket somewhere to bring it back, I just haven't written a patch yet, and would accept one by anyone else.
It only works with Gold, yeah. Also I'm in a weird time zone this week (it's now 5am here.) As I mentioned above, I'm totally down for a deeper TOC. Pull requests accepted, I just haven't done it yet.
&gt; https://github.com/rust-lang/rfcs/pull/431#issuecomment-61484717 ^^^ I 100% agree with phaylon (in the comments). I am new to Rust, but use dynamic languages, and some C/C++. Lifetimes are a challange to learn. Still don't get them, however, calling them "scopes" would have made it worse and even more confusing. C/C++ has scopes, Python has lexical scopes for variables. Those don't seem to map to lifetimes in a 1-to-1 fashion. So a new term is quite appropriate. I say -1 
If the compiler knows the right type, why can't it be omitted from the function declaration? Indeed the main reason behind C++'s auto is to hide those ugly types. (Also: can't Rust generics specialize on return values?)
(Note: I am not a core dev, this is what I *believe* to be the reason) A philosophical choice in Rust was that there shouldn't be type inference involved in function interfaces. There are three benefits to this: 1. Code will compile much faster, because it doesn't have to do whole program analysis. 2. Type errors are more localised. I've seen the odd example of Haskell (not being a Haskell programmer) where type errors would appear in one place, but actually be an issue in distant code. 3. Interfaces are easier to understand. Rather than a return type being "something, figure it out yourself", it's an actual concrete type. Iterators are a bit of a pain, but you can kinda work around it. Unboxed closures, on the other hand, *cannot be returned by value*. This is because a concrete UC type is *always* anonymous, so you can't name it. Hence the proposal (which last I knew was approved and will go through, *possibly* before 1.0) to allow some degree of inference in return types. I believe it will allow you to do something like: fn chunk(&amp;self, x: uint, size: uint) -&gt; impl Iterator&lt;char&gt; Note the `impl`. That is, you don't specify the return type exactly, but you *do* have to specify what the caller is allowed to assume about it (in this case, that it's some implementation of `Iterator`).
Oh, so only about 1/3 of all developers? Got it.
I'm not seeing this. Working fine for me . 
I've seen some other people report this IRL. It has something to do with the fonts on your system.
Do you have the Source Code Pro font installed? If it's the case, removing it may fix your problem. (I've seen some people reporting this as a solution. Ultimately we may have to remove the local font preference from the CSS then.)
I can't find the ticket right now, but yes, it has to do with your fonts being corrupt or something.
That's awesome! Would you be interested in talking about what you're doing in a meetup? I've heard rumors that there are people in the financial industry interested in rust, so a finance themed meetup might be a good idea to bring them out of the woodwork :)
Yes, the reference docs don't include the `::`, but sometimes you need to insert this when using it in real code. Sorry to be confusing - wasn't referring to sample code, which AFAICT is fine. Just meant that code from the reference docs cannot be used verbatim and a n00b isn't going to understand when/why this extra `::` is required in certain circumstances. This, combined with the resulting error message, is quite confusing. Perhaps the docs have been updated with a better explanation since I last read them (Steve is doing an awesome job!).
Relevant issue: [#17505](https://github.com/rust-lang/rust/issues/17505). It's just a matter of unhiding the deeper entries via CSS.
Awesome, thanks v much!
I have a use case which is very similar to your code, where I just need to increment a numeric value. No unsafe code is necessary for this case. In order to share the vector in multiple threads you have to put it into an ARC. The rust compiler does not automatically deduce that the vector lifes long enough for all the threads. ARC keeps the vector alive as long as there is at least one reference, so the lifetime is not limited by the scope of the main function. This allows you to avoid using the as_mut_ptr() together with the unsafe dereference to list. Now that the vector is safely shared between the threads incrementing the values in a lock free fashion can be accomplished by using atomics. Atomics provide special methods to operatore on the values in a thread safe manner. In contrast to c++ there is no default synchronisation consistency level so also have to specify that with each call. In contrast to "normal" integers, atomic integers don't have to be "mut" in order to change them. It makes sense, but my understanding is not deep enough to give a good explanation. Anyway this makes it possible that multiple threads can update the vector without each thread requiring a "mut" copy of the vector. Here are the approximate changes required to your code use std::sync::Arc; use std::sync::atomic::{AtomicInt, SeqCst}; .... //Initialize the vector let vv = Arc::new(Vec::from_fn(1024,|_| AtomicInt::new(1))); .... for ... let local_vv = vv.clone(); .... spawn(proc() { .... local_vv.deref()[tid+z].fetch_add(1, SeqCst); .... } I updated the gist: https://gist.github.com/mkaufmann/20695db6fa48c68bbef5 Furth edit: If you want to avoid hardcoding the number of threads use let num_threads = std::rt::default_sched_threads(); 
&gt; In contrast to "normal" integers, atomic integers don't have to be "mut" in order to change them. It makes sense, but my understanding is not deep enough to give a good explanation. One reason is that in Rust it is undefined behaviour to have more than one `&amp;mut` reference to the same object, so if the atomic types required `&amp;mut` they either could never be shared among threads, which defeats their purpose, or would require undefined behaviour to be used :)
i do find rusts' error messages are pretty good, and even if the docs around aren't always complete or in sync - (expected given a new changing) - looking at example code also helps; its self-hosting so there's the compiler source. and they do have someone hired to document it. (and I agree with you, plain old C still has a charm, in its place)
if let is a big step forward IMO because it doesn't add extra nesting for the simple case 
This is the part I understood, it is necessary so that they work in a sensible fashion. But my understanding of the "mut" semantics is lacking why this is exactly allowed and not a dirty hack (cue mutapocalypse and mut naming). As far as I understand in this case it is allowed to update the internal state because a) the user of the type already expects other changes and b) no derived references on this value get broken if it changes (compared to removing an element from a vector which would invalidate the derived iterators)
Couldn't you just use a wrapper that checks to see if it has been called before running it (of course with appropriate synchronization)?
I really find the IRC channel very helpful. I have been around for a while now but changes happen and I find the IRC channels are the best place for specific concerns, especially since there are documentation holes all over the place.
I would call it dirty hack. ;-) Rust's mut semantics allow for better alias analysis leading to some load/store elimination. On the other hand, if you access some data with atomics on one thread then you should access that data in all threads with atomics, so optimised out loads and stores are not a problem.
Rust has move semantics, and all by-value uses of a value are semantically `memcpy`s (that is, shallow byte copies), so `send.send(obj)` is byte-copying `obj` into the channel internals, and `let obj = recv.recv()` is byte-copying it out. That is, at both ends, 16 bytes (or 8 on a 32-bit computer) are transferred around. &gt; Does obj get cloned on the consumer thread's stack, and the original object is destroyed? It does not get cloned in the sense of the `Clone` trait, and the original is not destroyed in the sense of the `Drop` trait, but the value is moved to have different memory locations (by byte-copying). &gt; Or does the compiler know to allocate obj on the heap without explicitly being told to? No, no heap allocations, just copying the 16 (or 8) bytes directly. It is occasionally beneficial to manually place very large objects onto the heap by boxing them since this turns the large memcpy into just handling a single pointer.
The obj gets copied into the channel and then copied out by the receiver. The compiler checks at compile time it isn't used after being moved out of the sender. No destruction happens on the sender side.
Got it. Thanks for the detailed answer!
Thanks. :)
Just to make the point about heap allocations more explicit. The confusing point is probably that as the value is passed to another thread it can't live on the stack so it has to live somewhere on the heap. This place is inside the channel internals that have internally allocated a chunk of heap where the values are moved to. It can be compared to inserting a value into a vector where the vector internally manages a heap allocated buffer for all objects and doesn't heap allocate each inserted element individually. If someone wants to take a deeper look how this really is implemented for channels you can look at sync::mpsc_queue and try to use that instead of channels in the original example. 
Probably a typo, where "language" should actually be "library".
Nope, I won't be able to call it from different threads that way. (The runtime needs to initialize every thread somehow).
I have to say this is a little disappointing. I appreciate the desire to get 1.0 out of the door, but this change has massive positive feedback and would have a large effect on how people use Rust (i.e. you would expect any reasonable piece of Rust example code would be updated). I don't know if the decision was made in a minuted meeting, there doesn't seem to be anything new at https://github.com/rust-lang/meeting-minutes yet
I want Rust to be successful, but it's hard to say whether Rust will be successful. If we look at fairly recent successful languages we see 2 trends: 1. They have a big company backing them (Java, C#, Swift, Go) 2. They become successful in a niche where there exists a library/framework (Ruby for webdev with Rails, Python for scientific computing with Numpy/Scipy, R for statistics) At this moment Rust has neither of these. Additionally Rust suffers from a milder version of the problem that Haskell has: it's too hard for many people, specifically lifetimes. I predict that if Rust is to succeed some combination of these will happen: 1. Newcomers have to be given extensive training about lifetimes. 2. The lifetime system has to be simplified. 3. Some people will just ignore lifetimes and use gc'd or refcounted pointers everywhere. If Rust is successful, I think it will be despite lifetimes not because of them. I personally think lifetimes are a great feature, I just don't see it helping with mass adoption.
Note that the problemantic optimizations are explicitly disabled for types using internal mutability, as those use the `UnsafeCell&lt;T&gt;` internally which works both as a flag to disable them, and as the sole low-level basic means to get a mutable pointer to a thing accessed behind a `&amp;`.
Normally it will need to end up something like `enum MyIter&lt;'a&gt; { … with 'a instead of '_ … }`.
Thanks, much appreciated and it really helps my understanding! I also didn't know that UnsafeCell actually enables this behaviour for special custom types :)
The concept of a try-catch seems very iffy to me when you use algebraic data types for error reporting. Also since you already have macros/maybe this new `?` as combinators/chaining. I will have to have a look at the proposal later though; maybe this specific try-catch idea integrates nicely with the language. 
Rather than be annotation driven I'd make a compiler plugin.
I think this is a really important point that people forget. We aren't going to have to wait for a year (or more, depending on the language) for these new features.
I find the documentation around rust is pretty bad. The guide doesn't even tell you how to split your projects into more then one file. It says you can, and that it will tell you later in the documentation, but never does. (Ah, I got a reply in another thread - it used to, but now doesn't - but there is the mod guide which does - which is pretty well written :) ) But for true pain, move away from libc and say... try to use gdi32 on windows, you are in for so much pain it is pretty indescribable. There is no good documentation around even how to set up for that. I'm having to go though stuff on github and grabbing fragments from projects to even hope of getting a compile working. I'm on day 3 of trying to get the most simple build working. You want to talk to user32, its all happiness and light, gdi32? don't even bother trying - you will never get it to work, not from the documentation given. 
Yes! Now we're talking (or interoping)!
After reading the arguments, I personally wish they'd skip exceptions all together. I want Rust as a C replacement, I already have a C++ replacement. If you have a nice type system (as I think Rust does) then you can get quite far without exceptions (e.g. look how far you can go in Haskell without using them).
&gt; I just finished writing some standard sort algorithms Interesting, I once tried that in Rust and gave up, will investigate. BTW I wish bubble sort would die, selection sort is better and actually simpler.
 impl FromError&lt;IoError&gt; for MyError { // ... impl FromError&lt;MapError&gt; for MyError { It's so cool that this works now. :)
How would you handle traps from integer arithmetic, pattern match failure, out-of-bounds array accesses, or failure of implicit memory allocations, if not through some form of exception mechanism? C either does not have those features, or makes exceptional situations undefined behavior. Rust does not have this luxury.
OK, sounds good; I'll look into making a pull request, but no promises - unfortunately, this isn't my day job. (-:
Note that this proposal does not include stack-unwinding exceptions like you find in C++ and most languages with exceptions. It's more like syntactic sugar around the existing Option/Result error handling.
&gt; selection sort is better and actually simpler. Got stuck again ;-) /// The selection sort algorithm. pub fn selsort&lt;T : Ord&gt;(slice : &amp;mut [T]){ if slice.len() &lt; 2 {return} let mut min = 0; for i in range(1, slice.len()){ if slice[i] &lt; slice[min] { min = i; } } slice.swap(0, min); selsort(slice[1..]); // error: cannot take a slice of a value with type `&amp;mut [T]` } 
Is there going to be a shorthand for Result&lt;T, Error&gt; similar to IoResult? Or maybe this type is not needed anymore...
Currently, you can let it panic when you're not doing something that will actually cause problems (those sorts of errors rarely need to be caught), or you can use a method that returns Option/Result. This RFC doesn't change that.
Perhaps this is the functional programmer/language purist in me, but I think all of those should be `panic`-level errors. In my day-to-day job writing C#, I've never written code which caught `OverflowException`, `IndexOutOfRangeException`, or `OutOfMemoryException`. All of these imply a bug in the application to me and not something that could be recovered.
Oh, my apologies then. I thought this was related to the other posts that have been floating around lately about stack-unwinding.
Well, `Error` is a trait, what would you use such a type definition for? You can also make them easily yourself. E.g. `IoResult` is just type IoResult&lt;T&gt; = Result&lt;T, IoError&gt;;
Pattern match failure? That one is easy, you simply do not allow incomplete patterns (assuming you mean matching sum types. If you mean regex then that should obviously return an Option type because it can match nothing). Implicit memory allocation is more complex but is there really a lot of this going on? I would hope not as that could be a problem in embedded programming. For the rest, I agree with shadow31 that these are application bugs that should crash the application.
I question the very design of requiring `Iterator&lt;&amp;A&gt;`. Iterators are designed to produce a value or an *existing* reference to the underlying container, since a reference which is only valid in the current iteration is quite useless (`next` does not look back, after all!). Consequently it is easy to go from `Iterator&lt;&amp;A&gt;` to `Iterator&lt;A&gt;` (`.map(|&amp;v| v)`) but not vice versa, and any custom iterator adapter should prefer more general `Iterator&lt;A&gt;`.
You'd need at least `Result&lt;T, Box&lt;Error&gt;&gt;` or something :(
Thanks for the reply :) I thought so, but since the adapter worked for my general use case (XORing slices) I kept it as `Iterator&lt;&amp;A&gt;`, so I wasn't required to map-dereference all `slice.iter()` before using them... but it came back to bite me ;) Is there any dereferencing iterator adapter in std or should I roll my own? **EDIT:** Rolled my own, but now my code looks ugly with all those `slice.iter.by_deref().xor(slice2.iter().by_deref())` :( Would love an alternative. It's also causing problems because `Iterator&lt;A&gt;` does not implement `Clone` but `Items&lt;'a, T: 'a&gt;` did and I relied on it.
Just a thought, but given `impl FromError&lt;Box&lt;Error&gt;&gt; for FooError` and `impl FromError&lt;BarError&gt; for Box&lt;Error&gt;`, is the type-checker smart enough to pass the following, and if not, will it ever be? fn f() -&gt; Result&lt;int, FooError&gt; { try!(fn_that_may_return_a_bar_error()) Ok(0) } That is to say: `BarErrors` can become `Box&lt;Error&gt;`s, and `Box&lt;Error&gt;`s can become `FooErrors`, so can try! convert a `BarError` into a `FooError` in one fell swoop?
I think his use of the word 'exception' is confusing. Exceptions, to me, are exceptional, potentially-crash-your-program occurencess. Not something like a failed parse. Maybe I'm in the minority on this. But nonetheless, "fail" seems to have been just decided to be used when talking about errors-as-return-values, so why not use the term "failures" instead?
I agree (and so do quite a few people in the github thread). The name needs to change or people will keep thinking Rust has traditional exceptions with all the related problems. It's probably best to stick to "error handling" rather than "exception handling", but we also need to deal with the other names in the proposal like `catch` and `throw`. These seem just as likely to give people the wrong idea, but I haven't seen any compelling alternatives yet.
This already provides a *really* nice interface so maybe this is just my overzealous desire to collapse all repitition to the point of over abstraction but... I felt this urge to macro-ify this to: modify!(Response::new() with Status(status::Ok)), ContentType::new("application", "json"), Body(Path::new("config.json") )
In my experience, you tend to end up with *can't happen* cases in (exhaustive) pattern matches regularly (unless the type system is extremely powerful, but even then there are corner cases). Rust has implicit memory allocation for the stack at least.
Interesting. I looked at the atomics, but didn't realize I could wrap them in the Arc this way. I'll have to try this out. Unfortunately, one use case of mine is having threads write to an array of floats, and you can't really do atomics [easily] on floats =\ Though, now that I think about it, it's kinda a different example than the one I provided. In my sim, I just have threads that essentially read from one buffer, do some computation, and write to another buffer which is then used in a later stage. I wonder if I can use an UnsafeCell&lt;T&gt; to create a wrapper around a vector, place that in an Arc, and share that among the threads...
Thanks! I've narrowed it down to the [google web font](https://code.google.com/p/googlefontdirectory/) collection. When I remove that, everything looks as it should.
This is a mistake; originally the trait was going to inherit from `Any` but it was determined that this functionality was a somewhat special case that libraries could opt into. We may eventually add a downcastable variant of `Error` to `std::error`. I'll post a PR fixing the comment.
In `io::stdin().lines()`, `io::stdin()` is a temporary value which lifetime is the enclosing statement. It might be possible to extend this to the enclosing block whenever appropriate, and in fact this is what [RFC #66](https://github.com/rust-lang/rfcs/blob/master/text/0066-better-temporary-lifetimes.md) is proposing (but not yet implemented). For now you have to explicitly make a variable for such temporary values. For the lifetime syntax, I'd assume you have already read [RFC #134](https://github.com/rust-lang/rfcs/pull/134). As a historical curiosity, the original, and long obsolete lifetime syntax was [`&amp;a/Foo`](https://github.com/rust-lang/rust/issues/3886); there was a long discussion about the lifetime syntax ([Niko's posting](http://smallcultfollowing.com/babysteps/blog/2012/12/30/lifetime-notation/), [mailing list 1](https://mail.mozilla.org/pipermail/rust-dev/2013-January/002917.html) and [2](https://mail.mozilla.org/pipermail/rust-dev/2013-January/002994.html)) and if I recall correctly someone had the same suggestion as yours (can't find the reference though). It's over now, though, as everyone would complain about the syntax and the discussion ("bikeshedding") will go nowhere. The current syntax does have an advantage of i) being nominal (numeric lifetime is tough when there are more than 3 of them, and you'll lose `'static`), ii) being unambiguous both in the expression and type (at the small expense of lexer), and iii) having a separate token (your suggestion would break when the support for integer static parameters is added in the future, for example). The current syntax is not everyone likes but at least has enough merits to make the further discussion fruitless.
thanks :)
Servo shows an interesting binary behavior. Either a page works and is rendered almost correct, or the page does not work at all. :)
I tried native::run, but it didn't work. Looking at the source, It doesn't seem to start a runtime. I tried the other idea of wrapping pthreads, which was pretty straightforward. However, I have other issues that I'll be asking via a SO post, once I write it up nicely.
Oh, I had missed that. Does this mean that `Add` can finally implement both for `MyType + int` and for `int + MyType`?
I think it's a matter of *reusability*. The `go!` macro is, I surmise, reusable by any parser/state machine, whereas your implementation is not.
It's not like that is my domain or anything, but that sounds like something that in Rust would be implemented with errors-as-return-values. I don't see why malformed packets should cause a `panic` (catchable or not), since presumably the decoder should be able to handle malformed input and not just call it quits and give up.
This is my first nontrivial Rust project. I'd love to get any feedback on everything from style to API design. I'm particularly curious about people's feelings about the "semi-safe" choices.
What I was trying to say that the code of go! still counts. Although I have no idea what the go! macro does, I am pretty sure that you can move the similar parts of the code to a macro/function in the c implementation as well. So this particular comparison only tells me that the author of the Rust code found a more compact way to describe the same thing than the author of the c code. Does this tell us that it is way easier to find the compact version in Rust than in c? I don't know. I hope the slides (and the talk) will describe this in more detail.
Yeah sorry I didn't really read the whole thread carefully, and that didn't seem right... thanks for clarification though. However, I guess *you* missunderstood something here ;) : &gt; Do you know why it works? Is FromError:from_error special-cased by the compiler? I don't think the compiler normally tries arbitrary functions to convert types so that they match, that's only what the ``try`` macro does. So whether this works or not depends on whether the macro tries to chain errors. The docs don't say anything about this, so I also don't know :(
Wikipedia uses PHP as well
Web applications and http scale horizontally very well. Http and Web servers scale horizontally easily and reliably. Most of the hard calculating is done in the database, which also scales horizontally very well. And there's good caching options. Places that need speed probably also need advanced reliability or advanced functions, etc.. 
Python is also big for webdev. And I'd hardly call Web development a niche. It's a pretty big field. 
The term "semi-safe" doesn't make much sense. It is either it is safe according to Rust's definition of safety or the code is incorrect due to undefined behaviour. It isn't unsafe to have a safe API where race conditions are possible, but it's certainly incorrect if it doesn't maintain memory safety. Marking a library API as `unsafe` when it doesn't uphold memory safety guarantees is not optional. The `sqlite3_errstr` API can be used instead of `sqlite3_errmsg` to avoid a dependency on global state. It will provide static error strings without the dynamically formatted information, but it's mandatory if sharing the connection between threads is allowed. It is fine to allow sharing the connection between threads even if certain APIs return nonsensical results due to race conditions. It's definitely not memory safe to share `RefCell` between threads, but it's marked as non-`Sync` so the compiler will prevent it. If your goal is to allow sharing the connection between threads, then it needs satisfy the `Sync` bound which implies making it truly thread safe at a memory safety level.
I used "semi-safe" to mean "memory safe but may panic if you do something unexpected". Would you consider that "safe"? `sqlite3_errstr` isn't really a replacement for `sqlite3_errmsg` (at least as far as I can tell), as `sqlite3_errmsg` includes specific messaging to the most recent error, such as the name of a table that wasn't found. I didn't realize the compiler would prevent sharing a `RefCell` between threads. Does that mean if I have a struct that contains a (private) `RefCell`, the compiler will also prevent that struct from being shared? Will it prevent it from giving ownership to another thread?
&gt; I used "semi-safe" to mean "memory safe but may panic if you do something unexpected". Would you consider that "safe"? Do you mean a Rust `panic!` by panic? If yes, then that is what Rust calls "safe". See the reference for more details: [this](http://doc.rust-lang.org/reference.html#behavior-considered-unsafe) is considered *unsafe*, and [this](http://doc.rust-lang.org/reference.html#behaviour-not-considered-unsafe) is considered *safe*. &gt; Does that mean if I have a struct that contains a (private) RefCell, the compiler will also prevent that struct from being shared? You can only share things that implement `Sync`. You can only implement `Sync` for a struct if all it's fields implement `Sync`. So in your case, the struct cannot be shared with other threads.
rustc used to have a bunch of cross-compilation targets hardcoded in. This should make targetting new cross compilation destinations (that LLVM supports) much less work, possibly with no rustc changes at all.
Yeah, multi dispatch is a thing now :D
Now I'm getting error: cannot move out of dereference of `&amp;mut`-pointer and, unrelated I think, cannot move a value of type movement::Mover+'a: the size of movement::Mover+'a cannot be statically determined [E0161] which is weird, because I'm cloning mover, so why is it getting moved? let mover = box self.mover.clone();
I don't like the `each ... as x`, or the `from ... as n`, but of course that's a matter of personal opinion. I like Rust's for loops much better.
&gt; I hope the slides (and the talk) will describe this in more detail. Yes, they will. This is a single slide out of context, showing off that Servo can render that slide. &gt; The `go!` macro is, I surmise, reusable by any parser/state machine, whereas your implementation is not. Not really; it is pretty specialized to HTML parsing, though you could adapt the technique. The main point I want to make here is that our transcription *of the parsing rules* is very compact. Say you find a bug and you want to know if your rules for the Data state actually match the spec. In Hubbub you have to dig through 150 lines of C code exposing all kinds of details — details which are repeated in every other state, as well. In html5ever you look at a few lines of code, assuming the macros are correct — and the macros are tested by *every* state. It's more about maintainability and separation of concerns, although we do also have less code overall, I believe.
You can't move a trait object because the compiler doesn't know how big it is in memory; the underlying implementation could be on any number of concrete `struct`s with different numbers and types of fields. You can only pass it around by reference. You can try cloning the box itself (so drop the `box` keyword) if the contained type implements `Clone`. To do that you'll probably have to add a `Clone` type bound to `Mover`. It looks like you're trying to create a kind of polymorphism (storing different implementations of an interface in a single field) like I know is possible in Java and probably C# as well, but keep in mind Rust is much closer to the metal than either of those two. You can emulate dynamic dispatch by creating an enum that implements `Mover`, and have each of the types that you'd put in `Box&lt;Mover + 'a&gt;` be a variant of that enum. In that case, you don't need a `Box` because the enum is doing the dispatching for you; as a consequence, you eliminate a heap allocation, which is usually seen as a good thing. Although having a need for dynamic dispatch usually means it's time to reconsider the design of your program, as it's not exactly pragmatic in Rust. Sometimes there's uses for it but I think you could be doing a lot of things differently here that would make the program simpler and more efficient. Also, you're moving out of `Windows` in `to_vector()` by creating a vector of `Windows`' fields. Change that `&amp;mut self` to `self` to consume `Windows` and return the vector. Otherwise you need to return a vector of references or clone the fields.
&gt; I am pretty sure that you can move the similar parts of the code to a macro/function in the c implementation as well I'll believe it when I see it. Have you read the source of Hubbub? Or Gecko's HTML parser?
Yeah, I'm thinking about just stopping polymorphism and treating this as a C-like instead of C++ like language. I'm following a tutorial, and that's the way it was done in the tutorial. I'm obviously not an amazing games programmer by any stretch of the imagination, mostly writing Systems programs in C.
It definitely helped when I realized I should stop writing Rust programs like it's Java. Rust is much closer to C than C++, but I realized when learning Haskell that it's sometimes easier to avoid putting things in terms of what you know, and try to learn it fresh again. I really couldn't grok monads in Haskell until I stopped trying to compare them to OO concepts.
Although `mover` does need to be polymorphic... (as far as I can tell) Ugh, sometimes I wish (C-style) functional programming could be used for everything. Edit: I wish there was a way to have a struct, the same struct, implement different versions of itself. So it's sized, but has different methods depending on the version you're using.
The reflow performance here is *really* good!
&gt; Edit: I wish there was a way to have a struct, the same struct, implement different versions of itself. So it's sized, but has different methods depending on the version you're using. That's what I was getting at with using enum variants! enum MyMover { ThisMover(SomeData), ThatMover(SomeOtherData), } impl Mover for MyMover { fn update(&amp;self, position: Point, windows: &amp;mut Windows) -&gt; Point { match *self { ThisMover(ref data) =&gt; { ... }, // do ThisMover stuff ThatMover(ref data) =&gt; { ... }, // do ThatMover stuff } } } And then have a field for `MyMover` whereever you need it. That's the pragmatic way to implement dynamic dispatch in Rust. You can also have struct variants for enums (feature gated, also not exactly sure on the syntax): enum MyMover { ThisMover { data: SomeData, }, ThatMover { other_data: SomeOtherData, } } impl Mover for MyMover { fn update(&amp;self, position: Point, windows: &amp;mut Windows) -&gt; Point { match *self { mover@ThisMover =&gt; { let data = &amp;mover.data; ... }, // do ThisMover stuff mover@ThatMover =&gt; { let data = &amp;mover.other_data; ... }, // do ThatMover stuff } } } 
&gt; Do you mean a Rust panic! by panic? Yes. It sounds like I should drop the "semi-safe" term and just make it clear under what conditions panics can occur?
The meet-up is [on Thursday](http://www.meetup.com/Rust-Bay-Area/events/203495172/). It looks like in-person attendance is full, but the talks will be streamed live (and then available later) on air.mozilla.org.
I ended up doing a similar thing, but with structs instead. Thanks! Now onto another thing. Is it possible to rename mods when exporting, i.e. mod.rs: pub mod x as y ? This allows for (easier) compile-time polymorphism. Edit: In case you're curious: struct mover { move_type: enum MoveType, ... } pub fn update(&amp; self) { match self.move_type { Input =&gt; self.input_update(), ... } }
Yes, you can do pub use x as y; 
Generally speaking, it's considered bad practice to panic! in libraries for anything but logic errors. It's somewhat understandable if there's no reasonable way to interpret the user's request, and this nonsensicalness would always be evident at compile time. But even then, it's best to try to use the type system to prevent the user from providing this bad input in the first place, rather than panic!ing when it goes wrong. For anything that could conceivably go wrong due to user input at runtime (so not preventable statically), a panic! is likely not what you want.
Where would I put that? Edit: Thank you so much for answering all of my questions, you are really amazingly awesome :D
I use the builder pattern extensively in java, but in a zero-cost abstraction language, does it make as much sense? Three method calls to set arguments makes me feel icky, even if they can usually be abstracted away.
I can't wait to start using Servo nightlies to browse Wikipedia and Reddit!
I'll probably have Response::with([ Status(status::ok), ContentType::new("application", "json"), Body(Path::new("config.json")) ]); in the future, or maybe a macro if that doesn't work out.
IMO，using decodable and encodeble trait for objects would be better.
Off-topic, but you can simplify the `empty_shrinker` and `single_shrinker` methods: /// Creates a shrinker with a single element. pub fn single_shrinker&lt;A: 'static&gt;(value: A) -&gt; Box&lt;Shrinker&lt;A&gt;+'static&gt; { box SingleShrinker { value: Some(value) } } /// Creates a shrinker with zero elements. pub fn empty_shrinker&lt;A&gt;() -&gt; Box&lt;Shrinker&lt;A&gt;+'static&gt; { box EmptyShrinker }
All I want from a browser is absolutely zero memory leaks, and overall less memory usage (including the adblocker, that perhaps should be builtin, if an extension can't deliver this). What's the Servo story on this topic? (or what's planned) Or even, does not using a GC makes it easier to prevent memory leaks?
We actually do use a GC, to [manage DOM nodes](https://blog.mozilla.org/research/2014/08/26/javascript-servos-only-garbage-collector/). JS can store arbitrary references to the DOM so it makes sense to use GC for the native-code refs as well. Mainstream browser engines use refcounted DOMs and can leak memory when there's a cross-language reference cycle. Blink (the engine in Chrome) is in the middle of a *huge* push to GC their DOM.
So.. less of a C/Rust comparison and more of a "we do it better" thing? :D Though Rust's macro system does let you do these things in a sane manner. I've seen your macro crate, it doesn't look like C++ would be able to easily create macros like that.
&gt; For the lifetime syntax, I'd assume you have already read [RFC #134](https://github.com/rust-lang/rfcs/pull/134)... It's over now, though, Yes I've read RFC #134. Sadly the lifetime parameter syntax will stay as is, but I totally understand. Stabilizing the language to 1.0 asap is critical, and there are a lot more important things to worry about at the moment. &gt; In `io::stdin().lines()`, `io::stdin()` is a temporary value which lifetime is the enclosing statement. So in my code snippet (2), `for line in io::stdin().lines()` compiles because the for-loop only demands the lifetime of `io::stdin()`'s enclosing statement (which is the for-loop itself), while in (3), `let mut lines = io::stdin().lines();` wouldn't compile because the let-binding demands a lifetime of the enclosing block. Am I right? 
Tasks are not in the language, `spawn` etc. are purely library constructs. The language provides certain type system features like `Send` that allow making them safe but they're not actually part of the language itself. (And, even then it's semi-likely that we will gain the ability to have `Send` and `Sync` as purely library traits, no special compiler support required.)
Interesting, I must have missed that part of the guide. Re-reading it, I found this: &gt; It's worth noting that tasks are implemented as a library, and not part of the language. Did `rust` have an M:N task scheduler in the runtime a while ago? I mostly observe `rust` (and occasionally write code), but I thought this was a feature a while ago (when `graydon` was still leading the project). In any case, it looks like `rust` still has pretty strong green thread support, and I imagine this could be quite useful in a browser scenario with lots of tabs.
&gt; it's considered bad practice to panic! in libraries for anything but logic errors. I'm just drinking my coffee, but isn't this backwards? Libraries should only panic! for errors that are unrecoverable, logic errors should fail (return a Result) instead. Or maybe we have different definitions of 'logic errors'. &gt; it's best to try to use the type system to prevent the user from providing this bad input in the first place, rather than panic!ing when it goes wrong. Agreed.
&gt; Or maybe we have different definitions of 'logic errors'. I think this is the case. To me a logic error is an unrecoverable error caused by a flawed assumption by the programmer.
&gt; Though Rust's macro system does let you do these things in a sane manner. I don't know what that `go!` macro does, but it doesn't look sane at all. There are `to` and `emit` that presumably are "keywords" interpreted by the macro or something, and one macro instantiation even contains a semicolon. No chance to tell what will happen when that code is executed. If anything, it shows that the advantages of rust macros over C macros are not enough to make them sane. It has still many of the problems that make extensive use of C macros a horrible idea. Will the remaining code in the function even be executed? Will the macro evaluate it's argument multiple times? Or maybe not at all? Using macros makes it hard to tell what's going on, just like in C. Functions hide code that is separate from code the calls them. Macros hide code that interacts with the code that calls them (else you could use a function instead). Functions behave very predictable when they are called. Macros don't. Rust's macro system has improvements over C, but these basic problems of macros are still there.
:+1:
&gt; Did rust have an M:N task scheduler in the runtime a while ago? Yes, it originally had M:N threads as a language feature, then both N:M and 1:1 as libraries, and now, N:M has been moved out into an external crate. &gt; it looks like rust still has pretty strong green thread support, Not really. One of the reasons that libgreen was removed is that the abstraction of N:M and 1:1 made green threads not very green. :/ Luckily, since they're libraries and not languages, nothing is stopping a better one from being written.
It is actually pretty sane. Just read what is written there and don't think too much. `go to TagOpen` obviously, without looking at the code, makes the state machine go to the TagOpen state. The semicolons obviously separate commands. So on '\0' it does some error thingy and then emits '\0'. All your arguments against macros apply to any other kind of abstraction too. Unless you know what they do, functions are black boxes, they could do anything. They could even rewrite the code of the caller just like a macro. Unless you have a pure function I don't see any fundamental difference between macros and functions in that respect. Macros just give you an stable API to do such things, with functions you would have to rely on a fragile ABI.
Adblocker can't be at a lower layer. Everybody should take care of the net neutrality.
This is my latest attempt at getting this right. as always, harsh feedback very welcome. This might be the most important bit of documentation we have.
The "dispatch" referred to isn't function calls at runtime but rather "which impl should I use" (https://github.com/rust-lang/rfcs/blob/master/text/0135-where.md#multidispatch-traits)
The policy to decide what to block should be set at an upper layer, but the infrastructure for efficient adblocking could be at a lower layer. Perhaps something faster than injecting CSS. I think this kind of optimization is warranted, based on how adblocking is a resource hog. I can't find it, but I remember there a blog post where the authors of Adblock Plus claimed that the Chrome API they used was inadequate, and responsible for the slowdown.
That's not really a *problem* of macros IMO. Macros are _meant_ to handle stuff which functions won't. Putting constraints like "the arguments must be evaluated first" cuts down on where we can use them. &gt; Macros hide code that interacts with the code that calls them That's sort of the point of macros. It's a problem if you use this everywhere where functions would have done fine, not a problem if it greatly simplifies your code. Using macros will _always_ make it hard to tell what's going on. My point was geared towards the macro programmer's side — the system in Rust makes it much easier to write macros in a sane fashion. The _design_ of the macros is still up to the programmer — I bet kmc could write the go macro to be more verbose and readable, however I personally find it good as it is — if you don't understand the macro, there are docs for it.
Wohoo!
conversion operators are another area where C++ is currently more concise - and again I realise its' a 2 edge sword (nothing to search click on for the callers.). I do like the fact rust has 2way inference which might be a better way of covering that. I'm definitely a big fan of rust's lambda syntax.
sounds like *multi-parameter type-classes*. is that the case? EDIT: and indeed it is. It says so right in on the wiki page. thank you for the link :) EDIT2: but why the hell would they call it "dispatch"...
I rust-up'd and copy/pasted the last example into my main.rs file, and I get the following: error: binary assignment operation `+=` cannot be applied to type `Box&lt;int&gt;
&gt; I was referring to a historical perspective on programming languages and computing in general, not Rust in particular. And bjzaba was referring to Lisp, which dates to 1980s...
does really no one know about lifetimes? I still remember learning it in Computer Programming 101, using Pascal! The lecturer hammered on and on about how a variable has a lifetime and a scope. The fact that often they're one and the same is a convenient accident. In rust, a reference's lifetime must be shorter or equal than the referred value's lifetime. Scope should be irrelevant (and in fact there's work in progress to decouple them, I seem to understand).
So what he meant was that Lisp first had garbage collection, but then the use of that became a minority because people would rather use unique pointers and references *in Lisp*? Well that's news to me.
What's the point of introducing a new idiomatic way of handling errors right *after* 1.0; Isn't the whole point of having a 1.0 release to have a "stable" language? I'd guestimate that 1.0 will be the moment a lot of people will try out the language for the first time. And those beginners might be benefiting a lot from this RFC (at least I feel it makes the code smoother, which is something beginners do care about a lot). Yet this would have them first learn the "old" way of doing things, and right afterwards force them to learn a new idiomatic error handling onto them. To me it seems more sensible to postpone 1.0 release, instead of going the "push out 1.0 and then change the language within 6 months of it"-route.
They're completely interchangeable and some may prefer one instead of another depending on the context, there'll be no deprecation. We should get people to code more, so we can unveil more real-world problems and fix them. Edit: and I also think it needs more iteration, I'm skeptical that the RFC will be accepted as is.
need to deref first I think. but good catch, post a comment on git perhaps
Ah nice! It seems type inference has improved over the last several months. :-)
One might more easily just download it and run it in VLC. I think it's able to better pump up the volume.
There's an accepted RFC to make them pure library constructs, so unless I'm confused I think you can drop the "it's semi-likely" qualifier :)
The point about merging components back in gecko is quite interesting! It would be quite awesome to see :)
Well, I have always heard "selecting the right function to call in the presence of polymorphism" referred to as dispatch. Compile-time polymorphism is still "just" polymorphism :)
It does that with a tradeoff for quality. On Ubuntu in the sound settings you can achieve mostly the same thing -- there's a slider which goes past 100%
A question for the servo overlords: would it be possible to build something like phantomjs using Servo in the future, scripted with Rust? It probably wouldn't be as convenient/practical as just using JS, but it would definitely be cooler
Btw, wrt the "absolutely zero memory leaks" part, we do use some C libraries -- large ones include Spidermonkey and Skia. Memory safety in spidermonkey is reliant on the assumption "it's used by Firefox and has had years of vetting, so should be _mostly_ safe" (and will get security bugs caught and patched early). But the Rust code should be pretty memory safe.
Alternatively one might just live-stream it into vlc. Start of vlc, hit ctrl n, copy in the video download url: "https://vid.ly/8u6w1p?content=video&amp;format=webm"
I just updated [redis-rs](https://github.com/mitsuhiko/redis-rs/) for this new trait and it's not bad. I'm a fan :) To make it more fun to use I added a `throw!` macro similar to the `try!` one: macro_rules! throw { ($expr:expr) =&gt; ( return Err(::std::error::FromError::from_error($expr)); ) } And I implemented a `FromError&lt;(Kind, &amp;'static str)&gt;` for my error types: impl error::FromError&lt;(ErrorKind, &amp;'static str)&gt; for RedisError { fn from_error((kind, desc): (ErrorKind, &amp;'static str)) -&gt; RedisError { RedisError { kind: kind, desc: desc, detail: None } } } With both of those things I can now write things like this: throw!((InvalidClientConfig, "Invalid client URL")) And it will automatically create a `RedisError` for me and return it. Likewise I can just `throw!(an_io_error)` and it will work too.
Agreed: security, then speed/conformance, then frugality.
I'm very sad this is not landing but I am very sure that something like this needs to come sooner or later. It seems like the right way to go forward. Thankfully `FromError` landed which means everything is there which is needed to write good APIs. The actual trait-based exception handling RFC is mostly syntax support.
Yes! We can even render to .png in a way that's platform-independent, thanks to Mesa's off-screen software rendering. More info on [this ticket](https://github.com/servo/servo/issues/3789). There's no reason it couldn't be scriptable from both JS and Rust.
I like this idea. It's sort of like the "middleware" concept from webapp frameworks.
We're targeting all three at once with Servo. It makes things tricky, but it's important to make sure our architectural choices don't paint us into a corner.
I'm in a similar boat, hobbyist programmers don't always get the experience needed for teamwork and communication, so I'll be listening to the responses as well :) One thing for sure, write unit tests. This is built in to rust and cargo and does basic assertion testing: http://rustbyexample.com/staging/test.html
&gt; Using macros will *always* make it hard to tell what's going on. But that is the main problem with C macros - code using them quickly becomes completely unreadable and very hard to understand. Writing macros in C isn't *that* hard, what's hard is working with the code that uses them. So if Rust makes writing macros easier and allows to do more complex things with them, isn't that solving the wrong problem? Code using lots of macros can still quickly become hard to figure out. And since use of macros seems to be strongly encouraged in Rust, what will stop it from becoming a unmanageable mess?
If you want to contribute to the Rust Compiler, they have a great [Guide for new contributors](https://github.com/rust-lang/rust/wiki/Note-guide-for-new-contributors) :)
Unlike in mature languages, everything hasn't already been done here. In fact, very little is done: just pick something at random and there's a pretty good chance you can blaze the trail yourself, or at least port an existing solution to Rust. Interested in mobile dev? Make some CocoaTouch / Android bindings. Backend development? Help out with a websocket implementation or ORM. Or templates for launching on various cloud platforms like OpenShift or Heroku. Graphics? Would be awesome to have a 3D engine. What about a client library for push notifications? or maybe for Twitter or Facebook APIs? what about compilers for your favorite flavor of Jade or Less? maybe RSS/atom? Pick any one of these (or a thousand other things which are available in the mainstream) and who knows, maybe in 4 years you'll be the one with the default Rust library for X which has over 3,000 GitHub stars! It's certainly a resume winner. Beyond that, you have one advantage existing mainstream solutions don't have: you don't have to be backwards compatible with all their mistakes! "The field is white unto harvest, but the laborers are few."
There's a relevant xkcd for this, but I caught the subreddit rules just in time to avoid posting it. Whew.
XKCD #138 if someone wants to look it up
If the location in memory has tips, I'm all for it.
But you know, big companies tend to design for mediocre programmers.
Go, Servo, go! Keep on chugging up that big hill, you little engine. Choo! Choo!
https://duckduckgo.com/?q=xkcd+138
Rust also makes it hard to mess up while writing macros (macro hygiene). But yes, using macros everywhere might be a bad thing. Domain specific languages all over the place certainly is. Though in large libraries (eg html5ever) a couple of DSLs are probably okay.
This is a simple programmer error. There is no method `as_ptr()` on `Vec`, so that impl is recursing *ad infinitum* instead of producing a naming collision. You can call `as_ptr()` on a *slice*. No worries, I've done something similar before in Java when I didn't consider the behavior of method overloading.
If anyone is interested, I wrote this module that provides several different sorting methods in Rust. Some of them were quite fun, like heapsort. A couple even manage to be faster than the `std` crate `sort()` for a large, unsorted `Vec` of `uint`s. And, of course, there's bubblesort, which is really slow. Comments appreciated!
Oh thanks. I didn't notice that the as_ptr method of Vec went missing... the compiler didn't complain so I assumed it was there.
This is a work in progress, but it reached the point where I was able to start dogfooding. Though, it'd be nice if I could reload the whole thing on the fly instead of logging out and back in every time I change something. But still, my first real rust project.
So glad I wasn't the only one thinking that :D
If you include tests for a module in that same module, you don't have to mark functions such as `get_parent` pub.
Awesome! I'm definitely going to try it! Will it support i3-like shortcuts/configuration?
Never really used i3, so I honestly can't tell you. But I will take a look. But first I'm trying to get as close to xmonad as possible. But user defined hooks in the config file could pose a problem. Started as a small fun project last weekend. We'll see how far I get.
unsafe { What are you talking about? We have plenty of pointers. }
Did you try an in-memory SQL database or a traditional one via network connection?
It was originally in-memory SQL but it wasn't able to keep up. The network SQL was examined but was even slower. The programs are required to be very fast, not real time but not far short of it.
It would be worth changing to `u32` if only to remove many casts. I've been tweaking the code a bit locally and changing some of the `uints` to `u32` brings the number of `&lt;x&gt; as &lt;y&gt;` casts from 37 to 23.
Give me a few days. Workspace switching is still a bit buggy with multiple monitors and programs like xterm completely steal keyboard commands when in focus. Plus, moving windows to a different workspace is still not implemented. Would be very fun if I manage to find a solution similar to xmonad's custom hooks. 
Ok, the casts are a good reason. Will do that as soon as I've sorted out some other problems. Thanks
&gt; I may refactor that as soon as the code is stable enough and memory footprint is too high with many windows Note that 32 bit integers are faster on 64 bit processors as well! Even if the math isn't faster (which, AFAIK, usually is), the processor caches will be able to hold twice as many, and they will consume half the memory bandwidth. And, if you are doing many casts as /u/thiez said on the sibling post, you can save some more costs too :)
Ah, totally forgot about the cache. Right, will do so.
An RFC being accepted does not mean it will be implemented, just that it is desirable and wanted. It may turn out that the feature is obsoleted by other features or there are tweaks/adjustments/problems found when actually implementing it.
I tried to abstract as much as I could. If someone provides me with a decent Wayland wrapper, I might make it run on Wayland.
And done.
As long as I'm still the sole contributor, I could change it. But doesn't the BSD license come pretty close to WTFPL?
Sounds like a job for Redis. Have you considered it?
For memory safety all Set methods are auto generated code that uses a locking based system which the startup program manages along with the creation, flushing to disk and removal of the shared memory along with the job of starting and killing the other processes.
Sure, but the project already has wtf in the name ;).
That's...a...surprisingly solid argument. Good input :D
Great work. C/C++ interop tooling should be one of the priorities for the community. There's so much C++ code out there and it would be a shame to let it go unused. Also how about the other way round? Tools that help to generate C/C++ headers from Rust libraries? While we can't switch to full time Rust at our company because of $reasons it would be certainly possible (and preferable) to write parts of our our software (mostly the business logic where correctness is most important) in Rust and then interop it with the C++ code base. But that solely depends on how much (programmer time) overhead that would generate. 
I think u/spernsteiner was working on one and some point
Clicked expecting the project name to be "xlifetime", but this is pretty good too. :)
xmonad has `--replace` and `--restart` features that allow it to reload without logging out of the X session. I'm not sure how those are implemented, but maybe you could do the same.
`'x`
As a con, the BSD license is a very well-tested good license. The WTFPL... not so much
The problem with C++ interop is the lack of any kind of stable ABI. Rust has pretty fantastic C interop already, in both directions.
Same here. Love it
Yeah, for that I do at least have to implement managing of windows that have been there before the start. 
I did not know it existed. It looks quite good although I would rather like to keep the number of external dependencies down to a minimum especially for such a core component for this particular system. I will keep it as an option, by the looks of it quite a good option as well.
It is pretty popular, very robust, and lightweight. If you think it fits your use case, I would strongly suggest checking it out in detail :)
riak allows custom backends, which could be in memory on each server, with riak providing sync to each server: http://basho.com/riak/ note: i have not used riak, minimum 3 servers, recommended is 5
I am excited to see what this spits out for the modern SpiderMonkey headers! It might be easier to use than my hacked-up copy of bindgen that sort-of-kind-of understands C++.
+1. Bindgen has been better to me than doing everything manually, but it's still a huge headache, at least on Fedora. 
I politely disagree. Error handling infects a huge number of libraries. By not paying for it upfront, everybody pays harder in the end.
Memory footprint is a little less than xmonad at the moment from what I can tell. Speed benchmarks...don't know what exactly I should measure there. Response time to window creation, focus shift? I honestly don't know ^^
Rust statically links runtime/libraries in by default. If you do `rustc -C prefer-dynamic`, I get an 8.8k hello world. You can also use `-C lto` to do link time optimization and strip out unused code but this is slow and doesn't do much on hello world - 0.8M for me. I assume once the io devirtualization work is done that should be reduced.
Two things I noticed: * get_event_as is unsafe, even though it's internal to the module (that is, it's only safe to perform the transmute under some circumstances). Instead, mark it as an unsafe fn. * You use .zip(range(0, some_vec.len()))--you can just use .enumerate().
`get_event_as` can even be written as `&amp;*(self.event as *const T)` which is slightly better since it uses the more restricted `as` cast (works with fewer types), rather than `transmute` which will happily cast to/from many more types.
It would have a lot of fun interactions with shell quoting too :)
I'm an Awesome fan. I've tried others, but that one works best for me.
How is overloading implemented?
Much appreciated. Fixed. Never learned more about Rust than in the last few days.
He could dual license BSD / WTFPL just for the giggles.
One thing you might find interesting is [Xephyr](http://awesome.naquadah.org/wiki/Using_Xephyr), which lets you run an X server in a client window. You could do development/testing in that nested server without disrupting your own desktop. edit: oh, I see you mention this in the `README`, although I wonder why if you're using Xephyr you need to log out and back in again
FLAMEWAR! Just kidding, we all share the same love for tiny, performant and highly configurable WMs. I think I tried Awesome once for a few minutes, but kinda got stuck with Xmonad and never looked back.
&gt;This is awesome, ... [:)](http://awesome.naquadah.org/)
If you cheat and don't use the standard library, it is possible to have a [151 byte Rust binary](http://www.reddit.com/r/rust/comments/2iwtjh/151byte_static_binary_for_x8664_linux_in_rust/).
Rust has the `Condvar` struct [1], but from what I can see it doesn't include timeout functionality. You might be able to implement that by mixing it with the `Timer` facility [2]. [1] http://doc.rust-lang.org/sync/struct.Condvar.html [2] http://doc.rust-lang.org/std/io/timer/struct.Timer.html
&gt;I'm going to have to immutably borrow this joke from you
I will try to achieve something like that. Though I think it will come down to plugging a dynamic library or something. Still have to figure out a sound and especially safe approach for that at a later stage. 
Than by all means, please borrow it and let your friends borrow it too. 
If so this will totally replace XMonad for me ;-) 
Would you accept a PR for [Bogo sort](http://en.wikipedia.org/wiki/Bogosort)? or [any of these](http://xkcd.com/1185/) ? :)
[Image](http://imgs.xkcd.com/comics/ineffective_sorts.png) **Title:** Ineffective Sorts **Title-text:** StackSort connects to StackOverflow, searches for 'sort a list', and downloads and runs code snippets until the list is sorted. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=1185#Explanation) **Stats:** This comic has been referenced 18 times, representing 0.0455% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_clugugf)
I'm merely joking and are not a fan of WTFPL at all. I often use CC0 for my throwaway code. https://github.com/skade/bisschen#license
You are a terrible person for supplying two invalid ones.
try a keyboard shortcut that calls `exec(argv[0])` or sth. also xephyr.
It is not implemented yet. ~~Maybe I just add number to the function name.~~ EDIT: sorry, mixed something up there.
Are there any plans for multi-monitor support? I am currently using xmonad because it is the only one that handles multiple monitors in a halfway decent manner of the many I tried.
First, thank you for your answer! &gt; The Problem is that you try to implement Ord for a trait. Therefore the trait has to implement Sized. &gt; What you actually want to do is to implement Ord for a reference to your trait (or in rust terminology: a „trait object“). Ok, the documentation/guides on "trait object" are quite sparse, or I'm just bad at googling. But looking at core::raw, there is this struct declared: pub struct TraitObject { pub data: *mut (), pub vtable: *mut (), } This seems to indicate that a &amp;'a Shape+'a is in fact not one pointer, but two, one to the data and one to the vtable? Anyhow, the total size of the TraitObject seems easy for the compiler to calculate... &gt; Like in: &gt; &gt; impl&lt;'a&gt; PartialEq for &amp;'a Shape+'a { &gt; fn eq(&amp;self, other: &amp;&amp;Shape) -&gt; bool { unimplemented!() } &gt; } &gt; &gt; Unfortunately your quest will fail at the end because f64 does not implement Ord. Using the area you could thus only implement PartialOrd for your type. Ok, so f64 not implementing Ord is the least of the problems here. I don't care much about NaNs at this point, so I could just do like this: impl&lt;'a&gt; Ord for &amp;'a Shape+'a { fn cmp(&amp;self, other: &amp;&amp;Shape) -&gt; Ordering { self.area().partial_cmp(&amp;other.area()).unwrap_or(Equal) } } ...but this does not work either: error: the trait `core::kinds::Sized` is not implemented for the type `Shape+'a` note: the trait `core::kinds::Sized` must be implemented because it is required by `core::cmp::Ord` So, what's so different between PartialOrd and Ord that causes Ord to require core::kinds::Sized, where PartialOrd does not? &gt; Note that this is not what you normally do. It is much more common to implement the relevant traits for your structs. By doing to you can use a trait bound to use static dispatch. If you only implement a trait for a trait object you always have to use dynamic dispatch because you have to operate on the trait object. I'm open to more Rust idiomatic solutions, but I doubt there's any way around the dynamic dispatch in this case? I want a vector of shapes that I can easily sort by area. Probably like this: let mut m: Vec&lt;Box&lt;Shape&gt;&gt; = Vec::new(); m.push(box Circle { radius: 1f64 }); m.push(box Rectangle { width: 2f64, height: 3f64 }); m.as_mut_slice().sort(); Btw, the above currently fails with: error: type `&amp;mut [Box&lt;Shape&gt;]` does not implement any method in scope named `sort` ...but I'm not sure if this is due to me not being able to implement Ord for Shape yet, or if this is something else that I'm doing wrong.
You should be able to cast to `Box&lt;Buffer&gt;` instead, but it is currently blocked by [#18530](https://github.com/rust-lang/rust/issues/18530). :(
Some footage of the wm in action would be nice.
Yes it currently fails to compile with an error: only the builtin traits can be used as closure or object bounds.
I'm still confused about the decision to [postpone the trait-based exception handling RFC](http://www.reddit.com/r/rust/comments/2l8x2a/traitbased_exception_handling_rfc_postponed_till/). This drastically improves and alters the ergonomics of error handling (i.e. almost any piece of non-trivial Rust code), yet somehow it makes sense that it's introduced after 1.0? If the message of 1.0 isn't "this is pretty much what Rust code looks like, you can learn it now" I'm not entirely sure what it is.
&gt; The upcoming Rust 1.0 release means a lot, but most fundamentally it is a commitment to stability, alongside our long-running commitment to safety. &amp;mdash; [Stability](http://blog.rust-lang.org/2014/10/30/Stability.html) Specifically, 1.0 has the promise of backwards compatibility, that is, 1.0 code will compile with 1.1 and 1.2 etc. We aim to have a variety of nice functionality at 1.0, but there is absolutely no reason that we can't or shouldn't introduce dramatic improvements in 1.x releases just because that would change idioms.
Nothing can interop with C++, though. This is a huge problem that isn't even solved entirely even by D, the closest language to C++. There are good reasons the C ABI is the default for interop and it's quite easy to make one for a C++ library.
I can’t take credit—this is just a spectacularly obscure webcomic reference.
I hope we will see at least the `?` operator and `fail!` before 1.0. It can use `FromError` only for the time being and later be changed to use `Carrier`.
Ouch, this is going to break my project (which uses a lot of FDs) very bad... But I agree, this seems the right thing to do, and I'm excited to see Rust getting better IO :)
I'm surprised you don't have to specify a lifetime for &amp;str here: fn description(&amp;self) -&gt; &amp;str {
It's due to lifetime elision: https://github.com/rust-lang/rfcs/blob/master/text/0141-lifetime-elision.md#the-rules and under "Examples" fn get_mut(&amp;mut self) -&gt; &amp;mut T; // elided fn get_mut&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut T; // expanded This is that signature.
&gt; error: the trait `core::kinds::Sized` is not implemented for the type `Shape+'a` &gt; note: the trait `core::kinds::Sized` must be implemented because it is required by `core::cmp::Ord` So I was able to make the above message go away by implementing all of PartialEq, Eq, PartialOrd and Ord for &amp;'a Shape+'a. So this was probably a broken compiler error...? Anyhow, now to get that into a list. let mut m: Vec&lt;Box&lt;Shape&gt;&gt; = Vec::new(); m.push(box Circle { radius: 1f64 }); m.push(box Rectangle { width: 2f64, height: 3f64 }); m.as_mut_slice().sort(); ...does not compile, because I implemented Ord on the trait object, not on the trait. So I instead tried like the below: let mut m: Vec&lt;Box&lt;&amp;Shape&gt;&gt; = Vec::new(); m.push(box &amp;Circle { radius: 1f64 }); m.push(box &amp;Rectangle { width: 2f64, height: 3f64 }); m.as_mut_slice().sort(); Now "sort" succeeds, but the m.push fails with: error: mismatched types: expected `Box&lt;&amp;Shape&gt;`, found `Box&lt;&amp;Circle&gt;` (expected trait Shape, found struct Circle)
&gt; but there is absolutely no reason that we can't or shouldn't introduce dramatic improvements in 1.x releases just because that would change idioms. I definitely agree with you, but I expect this will cause some negative publicity in the non-Rust community. People already complain about how the language is unstable, and are usually met with the response "wait until 1.0." But if things don't really stabilize after 1.0, people might start to think that Rust doesn't care too much about stability. While "improvements" don't break backwards compatibility on paper, they can be far from non-disruptive. If the error-handling idioms change, people are going to have to change their code whether they like it or not, as libraries will change and your codebase style will simply become outdated. The way I look at it, there are three types of stability that are important for a language (especially a new one): 1. Language stability 2. Library stability 3. Style/community/idiom stability It seems like the third is often overlooked, though I'd say it's just as important as the other two for someone looking to get into the language.
Yeah, I understand that this particular RFC isn't too big of a disruption; I was more referring to the changing of idioms in general. &gt; I certainly agree that delaying improvements that fundamentally change libraries is unfortunate, as does the rest of the core team. I definitely have faith in the core team, so I'm not really worried. I was sort of just spouting my stream-of-consciousness there :)
I changed that now. That's a leftover from when I was playing with `Box&lt;Error&gt;`
&gt; I definitely agree with you, but I expect this will cause some negative publicity in the non-Rust community. People already complain about how the language is unstable, and are usually met with the response "wait until 1.0." But if things don't really stabilize after 1.0, people might start to think that Rust doesn't care too much about stability. Stability isn't the same as feature completeness.
But that all depends on how you define stability, surely? If you define "stable" as "unlikely to change", adding features certainly seems like instability. Understand that I'm not against adding features, far from it - I'm simply pointing out that Rust's definition of stability may not perfectly agree with that of some others (not that that should change anything).
&gt; But that all depends on how you define stability, surely? But that's simple: "stable" is for things that won't change, or have a high probability of not changing (depends on how strong the word is used in Rust parlance). If you add a new feature and it breaks something else - that's not stable. If you add a new feature, *and it doesn't change the stability of anything, or break any existing code*, then you still have stability. 
"stable" is more "continues to work" than "unlikely to change." But maybe that's specific to my crowd, I don't know. Your code with old idioms will still compile just fine.
I don't know why this was posted in /r/rust_gamedev and not in /r/rust, so here we go. Some useful build dependencies: - https://github.com/alexcrichton/gcc-rs - https://github.com/alexcrichton/pkg-config-rs Some examples: - https://github.com/alexcrichton/openssl-sys (trivial) - https://github.com/alexcrichton/curl-rust/tree/build-cmd/curl-sys (complex example) - https://github.com/tomaka/ogg-sys (pkgconfig + gcc)
&gt; But it can break the stability of the idioms for writing good code. Social factors do not factor into the use of *stable* in this context. Could you find an example of a language wherein "backwards compatibility" and "stable" has the connotations of "stable in the sense of idioms, best practices, conventions..."? Or a language that has had more or less feature-completeness straight after the non-beta release? The point is that few languages are "stable" in this sense, and thus most people probably won't expect that of Rust.
How does `err as &amp;error::Error` work without dynamic dispatch?
are you referring to https://github.com/rust-lang/rfcs/issues/368 ?
More specifically, https://github.com/rust-lang/rfcs/pull/241 , as we basically decided 'no' on the other two at the weekly meeting two weeks ago.
Out of curiosity I ran "strings" on my small hello world rust program, to be greeted by a short story by HP Lovecraft!! So there's definitely some weird cruft in there.
It does not. That part requires dynamic dispatch. In fact the `cause` is a bit crippled currently because `Any` is no longer something that `Error` subtypes. It's primarily useful for building "stack traces" now but I think this is something that will probably be further fleshed out.
Notice that this gives a discrepancy between core's try! macro and std's try! macro, it is only the latter that changes. Could be a gotcha. So, if I have an existing function returning (just as a stupid example) Result&lt;int, Hashmap&lt;String, String&gt;&gt;, and I'm using "try!" in that function, do I need to change my code, and if so how? Can I implement FromError for Hashmap&lt;String, String&gt; ...?
I support the addition of fail! is very handy way of "raising" custom Errors. With the proposed Error Traits , Rust has won in interoperability, but it can do better, not only improving interoperability between libraries but also with other languages. I'm thinking in embedding Rust in other languages like Python. With libgreen and rtio finally removed, the design of Rust feels less "concurrent centric" and more "systems-language centric", and so the `panic` statement feels kind of alien. Are there any plan to consider `panic` as `fail!` ? So, we ban unwinding the stack and gain in interop with other languages.
[Image](http://imgs.xkcd.com/comics/pointers.png) **Title:** Pointers **Title-text:** Every computer, at the unreachable memory address 0x-1, stores a secret. I found it, and it is that all humans ar-- SEGMENTATION FAULT. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=138#Explanation) **Stats:** This comic has been referenced 34 times, representing 0.0859% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cluojv3)
&gt; Could you find an example of a language wherein ... "stable" has the connotations of "stable in the sense of idioms, best practices, conventions..."? I'm mainly going off some of the sentiment I see on /r/programming. Someone who's completely new to the language, and considering picking it up, may easily be put off by still-evolving conventions. Here I'm referring to stability in the third sense in my OP (idiom stability) rather than in the technical sense (1. and 2. in my OP). I am not trying to argue semantics here, I think I'm just using the word in a broader term. I'll try and pick a better one next time :) From the point of view of a beginner interested in learning the language out of interest, rather than for work, the three are often of equal importance. For the hobbyist, library instability or minor language instability is not so important, as they are unlikely to be rigorously maintaining a large code base. Idiom stability is important though, because a beginner may well think "what's the point learning this now if I'll have to re-learn how to use the language in a year anyway."
Sure. I think the way to do it would be to crash an existing meetup that HFT coders attend. I can think of a couple. 
interesting, I had a bash at this ages ago and ran into the stumbling block of overloads etc... but now rust has multiparameter traits, perhaps you could handle the overloaded functions many C++ APIs have by generating a trait for each? i'll sound like a broken record.. I wish rust had the option to implement traits in the type's namespace (accept a types' inherent impls' to satisfy a trait ) - then you could name mangle identically and interop more smoothly, both ways. it would preclude multiple traits with the same method signatures in the same problem domain, but I don't think that would be a issue, compared to the benefit of being able to introduce Rust more seamlessly into established sourcebases ... think of the effect that would have on adoption, eliminating a risk by allowing people to hedge their bets. (remembering that a rewrite can be the death of an organisation..) Also I would argue it would make code easier to navigate, if methods had consistent names.
&gt; Are there any plan to consider panic as fail! ? We explicitly just changed `fail!` to `panic!`, so I would say no :wink: There are good reasons to allow stack unwinding, and good reasons to not. Stack unwinding can _help_ interop with other languages, too: Tilde, for example, needs stack unwinding to properly embed Rust inside of Ruby.
I think that, whenever feasible, a library that wraps around a native library should have an option to build it from scratch (if only to make compiling and running a program that depends on it less platform-dependent). It would be great if it also cloned a git repository, configured the library, etc. But having Cargo build a non-Rust dependency by default doesn't seem right. So a well behaved library should have something like `cargo build` that links to the OS libraries and `cargo from-scratch` (or other name) that builds everything from scratch. (But there's an use case for always building C code: when parts of the program is actually written in C) About finding library names: perhaps there is a pkg-config on Windows? (that's what is used on Linux / other Unices). Also [CMake apparently solved it](http://www.cmake.org/Wiki/CMake:How_To_Find_Libraries). Ultimately there needs to be a way to manually specify library paths per platform.
So maybe a middle term would be adding a way of "calming" a "panic" other than launching a task in order to protect us from underground panics. Something like `calm! {...}` that just transforms a panic into a Result.
Panics by definition crash the task. You're basically asking for exceptions, which is certainly never going in, given the complexity they add to C++ and the feeling that even panic should be disabled.
Good points all around. To address your second paragraph, I think that's what the *-sys packages (like curl-sys) do. Now that I have looked at the more complex build.rs example (for curl), it is clear that we can control a lot of build aspects, including the library names based on target platform; build.rs lets you be as simple or as complex as you want to be. cmake+pkgconfig would be best since it already exists and serves that purpose, but its good to know that the we still have final control. 
On the proposed `?`: Does rust have any other postfix operators? `?` looks cool and convenient, but it seems like a pretty unique feature if there aren't any other postfix operators. Not to mention that whenever `!` is used as a suffix (I don't know if they are still supposed to be used in macro names, or if they have a future), it seems to indicate that it is being used as a postfix operator [EDIT: meaning: it *looks like that*, not that they *are*: it's a question of legibility and perception] (and maybe they will be?), since `?` and `!` are kind of related punctuation marks.
Does it support containers like i3wm?
Currently, all of the unary operator expresisons are prefix, not postfix http://doc.rust-lang.org/reference.html#unary-operator-expressions &gt; I don't know if they are still supposed to be used in macro names, or if they have a future Yes, the RFC that proposes changing this was closed. It's not really a 'postfix operator' since it's not an operator: it's how you call a macro.
Ok, I don't fully understand you anymore because I don't know about `Any`. I was under the impression that dynamic dispatch only works through trait objects, though?
Ignore the any part. `Error` is a trait so when you call something on `&amp;Error` it finds the method via dynamic dispatch.
Why duck duck go? Activist?
This is great advice. And very encouraging. You go, Glenn Coco!
&gt; you can't commit code without someone more experienced reviewing Pull request bot, go!
Ah, cool. Thanks!
&gt; (I did say "suffix", not postfix operator.) I was going by "it seems to indicate that it is being used as a postfix operator", but this doesn't really matter :) &gt; It looks like the value try passed as a value to the function (postfix operator) ! They are different: try!(foo) something_maybe()? 
Here yo gou http://i.imgur.com/9pFfOZj.gif
Because it respects my privacy
Glad to see this land. I've been wanting a lot of the features this offers for my current project. This will definitely help me make more progress on my project. Great work Cargo devs!
Oh hey, you merged some of my changes. Sorry about the mess, I don't know how to git and got kinda sidetracked. You should also using the bitflags for modifier mask since it's a lot easier to read, and need to use the `sym = XkbKeycodeToKeysym(&amp;*window_system.display, key as u8, 0, 0);` to check against `curr_conf.launch_key` - right now it's checking a keycode against ascii character, which will never match. You should also use curr_conf.launcher for the launcher, instead of the hardcoded gmrun command EDIT: oh god I tried to merge my changes with your master and everything is now terrible. I have no idea how to git.
Right, I forgot about the `()`. So perhaps it won't be a problem in practice.
Yeah, fixed the launcher thing. As for the keycodes, I'll work on some sort of abstraction layer. Makes it easier to include Wayland support later. 
Rust newbie here, I'm just getting into the language coming from both a C and Haskell background - so apologies if I sound dumb. I understood most of the article, a few things I missed, but my main comment is how similar try! looks like to the Haskell Error/Either monad. In particular, the chaining of try! calls in the make_request example looks very suspiciously like Haskell's do-notation. (Maybe a bit more like idris's bang pattern thingy rather than do-notation, since it's inside the expression rather than at assignment) I've also seen very monad-like code in C#'s async/await syntax, but in that case it's using the async monad instead of the error monad. I actually tried hijacking C# and implementing a list monad in the syntax of the async monad. It... didn't end well. MS's implementation of the continuation delegate does *not* like being called multiple times. Crashy crashy. So here's my main questions/comments: Did the Result&lt;,&gt; type pull ideas from Haskell's Either type? Is this new try! macro trying to emulate do-notation? If so, how deep are we going to go down the monad hole (anywhere keeping it error-specific to a full-blown generic monad)? Honestly, I really like the power of monads, but I must admit they're pretty hard to grasp for a beginner and it might not be the best idea to include it as is (especially if it's called "monad", haha). I'm just worried about Rust digging itself into an inflexible position where we have potential abstractions theoretically possible in the langauge's syntax, but killed by the implementation of that syntax (like C#'s async/await).
Okay, cool! Thanks! I remembered a while ago I saw someone point out the `#![no_implicit_prelude]` name was inspired by `{-# LANGUAGE NoImplicitPrelude #-}`, it's cool to see influences all over the place. I really am loving how Rust is like a mash of the power of C programming and the abstractions of functional programming... if only I had a big project idea that I could hack out in it, which is my only roadblock in learning the language.
Haha... no, I'd rather keep them to "effective" sorts, which I guess could be defined as "Requires less than n^2 camparisons and swaps"
https://github.com/rust-lang/rust/issues/13871
It's good that `FromError` got a default impl that converts any type to itself, so the `try!` macro still works for `Result` where the `Err` side isn't actually an `Error`, such as an overly simplified function like this: struct MyStruct; impl MyStruct { fn consume(self) -&gt; Result&lt;(), MyStruct&gt; { Err(self) } } Anywhere a function that is supposed to consume an instance but should return it on failure would have been broken by this change. So I'm happy that it wasn't because this is a very useful pattern.
Bikeshedding but I'm mildly disappointed that it ended up using a custom format for build script output. I would have thought that eg toml would make sense since Cargo already uses toml in other places. Of course I probably should have been more vocal about this before it got merged in...
It's not possible to get the functionality right now. The `Condvar` type has a very poor implementation so binding to POSIX / win32 threads directly is a better route for anything serious. Fixing the concurrency support in the standard library can happen after the legacy green threading support is removed.
Consider LMDB: http://symas.com/mdb/ In-process DB with full ACID with MVCC, multithread and multiprocess, based on memory mapped files. It's a shame it's not better known! There's also Rust bindings: https://github.com/vhbit/lmdb-rs I haven't used it personally but I am eyeing it for a future project. 
http://doc.rust-lang.org/std/task/fn.try.html It does actually launch a task but otherwise it looks like your proposed `calm`
The advantage of the system that was chosen is that the build script can print more things than just the required data. For example you can print the list of the gcc invocations to stdout, and at the end a single `cargo:rustc-flags=...` line.
&gt; I'm just getting into the language coming from both a C and Haskell background This is just the perfect combination of languages from which you can come to Rust. What you say about monads is definitely recoginized by many people working on Rust. As it was already said Rust currently lacks higher-kinded polymorphism which is necessary for generic implementation of monads. Async/await was also discussed somewhere but it was postpned. This all is a lot of work and requires thorough design to make it good and integrate with existing Rust features. I think in the future Rust will have all these features and many more. Right now there are more immediate needs, better error handling is one of them.
Replace `?` with `.unwrap()` and try to compile both variants.
 error: expected function, found `core::option::Option&lt;fn()&gt;` f()? ^~~ 
&gt; **Environment variables** &gt; The $USER environment variable can be specified, which is used by cargo. The default is root. &gt; $ docker run --rm -it -e USER="John Doe" -v $(pwd):/source schickling/rust Here's [my expression](http://imgur.com/Zb2lkse) as a Windows user. 
probably as a windows user he is used to nonsense.
`JsGc&lt;T&gt;` wouldn't point at a relocatable SpiderMonkey pointer in this case; it would point at the Rust memory that doesn't move. Granted, that may change in the future if we implement fused DOM objects with their JS reflectors, and we would need the equivalent of SpiderMonkey's Handle types which can deal with a moving GC.
Nothing moves the actual Rust DOM objects; at most, the reflector (the JS object associated with the Rust object) is moved.
I'm attempting to write a Weston shell client and have a very basic Wayland wrapper. Doubt it's in any usable state though.
That's basically what I expected from what I gathered from the documentation. I guess I'll have to find a roundabout / application-specific way to do things then.
There was an RFC to rename macros to use `@`, and to use `?` for `try` and `!` for `unwrap`. But I think the final decision was to keep `!` for macros.
Or to sarcasm.
Sounds like a good match. I had reliability problems with it (when I wrote the C++ wrapper) but hopefully the things have improved. If you want it memory-only you could create it on /dev/shm I think.
Is there a reason [SWIG](http://swig.org/) wouldn't be leveraged vs. an ad hoc solution?
You could have a task without a thread though – a function that exposes the interface of the `try` function without spawning a thread. That would *not* add the complexity that it adds in C++.
It's [here](https://github.com/eyolfson/eyl-shell/). I'm going to move the Wayland library part into a separate repository soon. If you have any input let me know.
FLAME ON!!! Hehehe...anyway, I did try Xmonad and a couple others fairly recently, but Awesome just worked better for me. I like how the layouts work much more.
I got accustomed to the extensibility of Xmonad and I might have a few ideas how to reproduce the same thing in Rust. It might involve a config file written in Rust, loaded as a dynamic lib. EDIT: Damn smartphone autocorrect.
the only valid replacements for len in my opinion are size and count. But len is just fine
It looks like most of the issues have been claimed already. Do you think this could be an opportunity for more mentoring? I'm sure there's a lot of people who have been wanting to contribute but don't know where to start, and having a hand in refactoring the collections API would be great incentive to participate.
Ah, nice to see some other German projects here.
Hm, as soon as the WM is sort of stable, I'll take a closer look at Wayland. But the raw/ module seems to be a good start. That should be sufficient as soon as I get to it. Even if it's not complete yet, it should be a start, and I will contribute as much as I can as soon as it's moved into a separate repository.
We're starting 15 minutes early! run run run
The tasks aren't blocked out in a super reasonable way. They're mostly just blocked out according to the RFC's headers. As I noted in this post and the thread, I'm trying to block them out in a more friendly manner. Here's all the FIXME's I tossed in the files that anyone can claim and try to tackle: C:\g\rust [collect-fruit]&gt; git grep 'FIXME(conventions)' src/libcollections/binary_heap.rs:// FIXME(conventions): implement into_iter src/libcollections/bit.rs:// FIXME(conventions): look, we just need to refactor this whole thing. Inside and out. src/libcollections/btree/map.rs:// FIXME(conventions): implement bounded iterators src/libcollections/btree/set.rs:// FIXME(conventions): implement bounded iterators src/libcollections/btree/set.rs:// FIXME(conventions): implement BitOr, BitAnd, BitXor, and Sub src/libcollections/enum_set.rs:// FIXME(conventions): implement BitXor src/libcollections/enum_set.rs:// FIXME(conventions): implement len src/libcollections/ring_buf.rs:// FIXME(conventions): implement shrink_to_fit. Awkward with the current design, but it should src/libcollections/ring_buf.rs:// FIXME(conventions): implement into_iter src/libcollections/str.rs:// FIXME(conventions): ensure bit/char conventions are followed by str's API src/libcollections/tree/map.rs:// FIXME(conventions): implement bounded iterators src/libcollections/tree/map.rs:// FIXME(conventions): replace rev_iter(_mut) by making iter(_mut) DoubleEnded src/libcollections/tree/set.rs:// FIXME(conventions): implement bounded iterators src/libcollections/tree/set.rs:// FIXME(conventions): implement BitOr, BitAnd, BitXor, and Sub src/libcollections/tree/set.rs:// FIXME(conventions): replace rev_iter(_mut) by making iter(_mut) DoubleEnded src/libcollections/trie/map.rs:// FIXME(conventions): implement bounded iterators src/libcollections/trie/map.rs:// FIXME(conventions): implement into_iter src/libcollections/trie/map.rs:// FIXME(conventions): replace each_reverse by making iter DoubleEnded src/libcollections/trie/set.rs:// FIXME(conventions): implement bounded iterators src/libcollections/trie/set.rs:// FIXME(conventions): implement union family of fns src/libcollections/trie/set.rs:// FIXME(conventions): implement BitOr, BitAnd, BitXor, and Sub src/libcollections/trie/set.rs:// FIXME(conventions): replace each_reverse by making iter DoubleEnded src/libcollections/trie/set.rs:// FIXME(conventions): implement iter_mut and into_iter src/libcollections/vec_map.rs:// FIXME(conventions): capacity management??? src/libstd/collections/hash/map.rs:// FIXME(conventions): update capacity management to match other collections (no auto-shrink) src/libstd/collections/hash/map.rs:// FIXME(conventions): axe find_copy/get_copy in favour of Option.cloned (also implement that) src/libstd/collections/hash/set.rs:// FIXME(conventions): implement BitOr, BitAnd, BitXor, and Sub src/libstd/collections/hash/set.rs:// FIXME(conventions): update capacity management to match other collections (no auto-shrink) src/libstd/collections/lru_cache.rs:// FIXME(conventions): implement iterators? src/libstd/collections/lru_cache.rs:// FIXME(conventions): implement indexing? I'm also relinquishing all the conventions tasks, because I have to get back to my schoolwork now (I'm actually procrastinating an essay that's due in an hour as I type). 
Hehe, I know what you mean! My friends have started calling me the Rust guy after I did a class project in it and gave a presentation on it to the campus ACM chapter. People will complain about some "feature" x of a programming language and before I say anything they anticipate my response and say "I already know Rust solves that by introducing Y" I want to echo that part about "Because I know, when that program finally does compile, 9 times out of 10, it will just work." since that is the part I really find fun about Rust. Yeah sure, sometimes tackling compiler errors means a few days of offering sacrifices to the borrow checker hoping that this time it will finally compile, but when it does I take a sigh of relief and my code just works. And 90% of the time when it DOESN'T just work its because I wrote an unsafe block with an error in it. All I have to do is gdb that portion and find why its wrong. Granted its a small sample size. I have plans to rewrite some of my old rust code and make a few new programs in rust, but I can't see it being any different when your code base gets bigger. It seems that you'll always have the compiler do most of the verification and catching your dumb mistakes which is fine by me.
I've spent enough time hopping between Java and PHP to realize how much I really do love having my work double-checked by the compiler. Because in PHP, I might not find a mistake until it goes into production unless I have loads of test suites and check every little thing. I was fortunate to discover that PHPStorm does some pseudo-typechecking and will correct you where it can if you studiously use PHPDoc comments. But you shouldn't have to rely on an IDE for correctness.
I'd prefer `size`, maybe because I've used Scala a lot, and all of Scala's collections implement `size`. Whatever is chosen, I'd really appreciate it if the same name is used for all collections.
I presented these from inside Servo, on [this branch](https://github.com/kmcallister/servo/commits/slides).
As a current PHP web developer, I completely understand :) For what its worth, there is an RFC out right now that my friend made for PHP return types that recently got approved https://wiki.php.net/rfc/returntypehinting which would really help people reason about their code imo. Thats another benefit I noticed of having these new languages come out. From my perspective, it seems like there is an increased push at making older languages better which can only help everyone since at some point we may just end up programming in that language! (In a tangential note, learning rust has made my c++ better as I'm more aware of what behavior causes memory issues!)
Our naming is already totally uniform for `len`, so that's not a worry. The argument against `size` is that's ambiguous with notions of "occupied space" or "capacity".
The problem with `size` is that object sizes are something you care about in Rust. It has a `Sized` trait and a `size_of` intrinsic. It's easy to interpret a method called `size` as referring to the size in *bytes* since it's used that way elsewhere.
IMO, there's ambiguity is with `size_of` / `Sized`. It sounds too much like it returns the byte size of the vector allocation.
Yeah, I can see how that could be a problem. English is too imprecise sometimes :).
What's the motivation behind removing the traits?
If you compile the second version with optimization enabled and look at the assembly, you get this: addl $-9, %edi cmpl $54, %edi ja LBB0_4 movabsq $19703496462827523, %rax movb %dil, %cl shrq %cl, %rax andl $1, %eax Which is the equivalent of let adjusted = (c as uint) - 9; if adjusted &gt; 54 { return false; } (19703496462827523u64 &gt;&gt; adjusted) &amp; 1 == 1 LLVM is turning the pattern match into a bitset which can be tested in constant time with just a few integer operations. Which is going to be very much cheaper than actually looping over an array of bytes and doing one test for every character!
Wow, this is definitely interesting! Basically LLVM is able to perform the optimization that turns `switch` into a bitset testing whenever possible. .LBB1_123: addl $-9, %edi cmpl $55, %edi jae .LBB1_124 movabsq $19703496462827523, %rsi btq %rdi, %rsi jb .LBB1_127 Here 19703496462827523 = 0b1000110000000000011100111000011100000000000000000000011. It contains 14 ones that correspond to each arm. (The lowest bit corresponds to `'\t' == '\x09'`, which explains the bias of `-9` in the first line.) For what it matters, [rust-encoding](https://github.com/lifthrasiir/rust-encoding) has large tables that are essentially large `match`. Unfortunately, the last time I've tried LLVM was unable to optimize it into something clever (the values to match was too diverse), so I had to switch to the manual trie ([example](https://github.com/lifthrasiir/rust-encoding/blob/master/src/index/singlebyte/iso_8859_7.rs)). EDIT: Dang, /u/comex beat me :p
Where are the traits for fixed size arrays defined now (Eq, PartialEq, et. al)? Edit: to clarify, previously i could do: let x = [0u8, ..64]; let y = [0u8, ..64]; assert!(x == y); now this fails with: error: binary operation `==` cannot be applied to type `[u8, ..64]` However, I can still do: let x = [0u8, ..32]; let y = [0u8, ..32]; assert!(x == y); 
If he'd have put 'thought' instead of 'think' then you'd be right. I suppose he could have used 'are' instead of 'were' but the latter sounds better.
It also looks almost identical to what the regex would be. I wonder what the difference in performance would be for that. Edit: Ok not really, since there's no need to use the ors since they're all single characters, but I still wonder how fast it would be in comparison.
&gt;I wonder what the difference in performance would be for that. You mean for a regex?
You probably want an enum, which is like a tagged union in C or a sum data type in Haskell (iirc).
I've [forked](https://gist.github.com/lifthrasiir/7d65a51de2aedfeab757) /u/unfoldl's script to check it out. The results: test bench_pattern ... bench: 5124 ns/iter (+/- 12) test bench_regex ... bench: 83603 ns/iter (+/- 652) test bench_vec ... bench: 22516 ns/iter (+/- 70) Well, of course. (I was rather surprised that regex is surprisingly faster than I thought.)
On behalf of the native English speakers of the world, I'm so, so very sorry. Although I'm sure your native tongue has its nuances too.
How does that work though? I thought a trait object was a fat pointer, which, as I understand, is a pointer to the data, and a separate pointer to the vtable. Does this mean that the vtable is now stored inline in a struct?
Alternatively, SimonSapin provides a [macro](https://github.com/SimonSapin/rfcs/blob/matches-macro/active/0000-matches-macro.md) for this type of thing. It looks like: matches!(c, ' ' | '!' | '?') [playpen](http://play.rust-lang.org/?code=%23![feature%28macro_rules%29]%0A%0Amacro_rules!%20matches%28%0A%20%20%20%20%28%24expression%3A%20expr%2C%20%24%28%24pattern%3Apat%29|*%29%20%3D%3E%20%28%0A%20%20%20%20%20%20%20%20match%20%24expression%20{%0A%20%20%20%20%20%20%20%20%20%20%20%20%24%28%24pattern%29|%2B%20%3D%3E%20true%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20_%20%3D%3E%20false%2C%0A%20%20%20%20%20%20%20%20}%0A%20%20%20%20%29%3B%0A%29%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20c%20%3D%20%27*%27%3B%0A%20%20%20%20let%20first%20%3D%20match%20c%20{%0A%20%20%20%20%20%20%20%20%27%20%27%20%3D%3E%20true%2C%0A%20%20%20%20%20%20%20%20%27!%27%20%3D%3E%20true%2C%0A%20%20%20%20%20%20%20%20%27%3F%27%20%3D%3E%20true%2C%0A%20%20%20%20%20%20%20%20_%20%3D%3E%20false%2C%0A%20%20%20%20}%3B%0A%0A%20%20%20%20let%20second%20%3D%20matches!%28c%2C%20%27%20%27%20|%20%27!%27%20|%20%27%3F%27%29%3B%0A%0A%20%20%20%20println!%28%22first%3A%20{}%2C%20second%3A%20{}%22%2C%20first%2C%20second%29%3B%0A}%0A)
Thanks. I was sure it'd be slower, I just was curious how much. I guess the lesson is that pattern matching is even more amazing than it first seems.
https://github.com/rust-lang/rfcs/blob/master/text/0235-collections-conventions.md
Pretty much the same experience here. I've given talks on Rust, used it in my physics simulations, and usually end up saying "Rust solves that!" in programming discussions :P
&gt;`reserve` now functions as `reserve_additional` did: reserve space for at least `n` additional elements from the current `len`. This will be a gotcha for c++ people, the difference in meaning is silent and nobody reads a documentation for "obvious" methods :|
If the word "are" is used in the phrase, the implication is that from this point onward. Using "were" implies the match was predestined and fated love is always more romantic so "were" is the cliche in scriptwriting.
Where's the github? :)
Well now I'm just going to look like I'm humblebragging. But OP's gotta deliver, right? https://github.com/cybergeek94?tab=repositories That is an old picture, by the way.
Same here, this behaviour is (in my experience) almost never what I want. Another point of annoyance to me is how there is no method in `Vec` to set its size. There's [`set_len`](http://doc.rust-lang.org/std/vec/struct.Vec.html#method.set_len), which is sensuously named, but in actuality does something you surely don't want. So the alternative is to manually compare the desired size vs. the former one and invoke the appropriate methods, calculating exactly how many items you'll need to add or remove. IMO, highly un-ergonomic compared to simply calling [`resize`](http://en.cppreference.com/w/cpp/container/vector/resize).
This has never been confusing to me in C++, but at this point I think `len` is a fine name and there's not much reason to change it. That the name doesn't really make sense for a non-sequence isn't a problem: We, as humans, are used to dealing with much more imprecise terms on a daily basis.
I second that!
I shared similar feelings with you and OP about the Rust compiler. I'm new to the language and the compiler is indeed the pickiest one I've ever worked with. (I've been coding since 1980's.) At first, it's really *painful* to fight with the compiler. Often I need to search a lot to understand why there's the error, and fix it. Now I begin to feel *rewarding* every time the compiler lets me pass. (It's kind of sick, isn't it?) Also I'd like to salute to the core team. With so many things done right, Rust can be a groundbreaking language. You've been doing a great job! I wish you a great success. I wish Rust a great success. 
Ferrite is probably the coolest because it's almost entirely in macros. I still need to finish it because I think it could be really useful. Though I'll probably put that name back up for grabs and decide on another because it's too awesome as a name for a Rust project to be squandered on a REST client generator. I'm probably the proudest of the [fix I did for Hyper][1]. The function for parsing the HTTP method header from requests was weirdly implemented and broken at the time, so I cleaned it up, fixed it and ended up with a 30% performance increase. And multipart is a very sorely needed extension for hyper that handles `multipart/form-data` requests on client and server. That's the one I probably need to finish first because ferrite needs file upload support to reach shallow feature-parity with Retrofit, which it's mostly derived from. Some forks in there I haven't done any work on. If there's a fork without any commits from me, I probably forked it because I wanted to add something but then decided against it, or more likely, forgot about it. [1]: https://github.com/hyperium/hyper/commit/ab396c2394db40dc6a2fc3736f528b5c552d622e 
Rust does need some serious Crypto-love. I would personally love to see NaCl in Rust, if anyone at the meetup has time to spare and needs a project! :)
I'm surprised that LLVM is *that* smart--I sure as hell would have never thought of that--but it's still slower (or is perceived as slower) than gcc. What does it do that LLVM doesn't? Sacrifice virgins to the gods of assembly?
&gt; not sure who... the ISO? The American National Standards Institute (ANSI), which in 1963 when they published the ASCII spec, was called the American Standards Association. &gt;They either had a really good reason for it, or they were smoking some really good stuff. Either way, they probably weren't thinking of people trying to match on characters like this. [According to Wikipedia](https://en.wikipedia.org/wiki/ASCII#Organization), they chose the codes for symbols based on how shift worked on *mechanical typewriters*. The curse of backwards compatibility strikes again.
AFAIK, the collection library currently doesn't use `Default::default()` at all, which is needed for `resize` equivalent. So it it will require some additional work to develop general conventions on using default values. And, by the way, default initialization in general is underappreciated in Rust from performance point of view. As an example I'd recommend to look at `v.resize(1000)` and `v.resize(1000, 0)` for vectors in C++ and compare them, both the library implementation and performance.
thanks, this is a way, but... enum FooBar { Foo(uint, uint, uint), Bar, } size of `FooBar` look like equal of `Foo(uint, uint, uint)` size? I mean is, maybe if use `enum`, all size is max of enum type size... 
Yes. If one of the variants is considerably larger than the others, you can introduce indirection; e.g.: enum FooBar { Foo(/* many many fields here */), Bar, } can become: struct SFoo { // many many fields } enum FooBar { Foo(Box&lt;SFoo&gt;), Bar, } That way you can keep the enum smaller (at the cost of introducing an indirection).
No, `&amp;Error` *is* a fat pointer. This program: use std::error::Error; use std::mem; fn main() { println!("{}", mem::size_of::&lt;&amp;Error&gt;()); } Prints 16 on my machine (as well as on play). In general, if `T` is a trait, `&amp;T` is a fat pointer representing the trait object.
Yes. In other languages, enums might be implemented with a tag plus a pointer to the actual data, which in Rust translates to something like enum FooBar { Foo(Box&lt;(uint, uint, uint)&gt;), Bar } (Instead of Box&lt;&gt;, perhaps using Rc, or your custom GC-managed pointer type) But I don't see a reason to do that. You only need to wrap enum values where your data type is recursive, the [guide](http://doc.rust-lang.org/guide.html#pointers) explains that on section 17.3. A data type for Lisp values would be something like, enum LispVal { Atom(String), String(String), Number(int32), Cons(Rc&lt;LispVal&gt;, Rc&lt;LispVal&gt;) } Strings are implemented using Vec&lt;u8&gt; which basically hold a pointer to the actual bytes so it has constant size. Number(int32) also has constant size. Cons likewise will be constant size. This enum is okay. I'm not sure how to implement a GC though. Rc uses reference counting so you will need to do something about cycles.
Oh, I had no idea! Thanks for the explanation!
it looks like you're building 2 vectors, perhaps that can amount to the slow down; thanks for this post
I have also been toying with the idea of a lisp compiler too, but not an interpreter with gc; I was thinking of trying to statically compile it down to rust that way potentially getting the benefits of borrowing and lifetimes. It's slow going so far but I do have a folder called 'patina' with code inside parsing s-exp, so that's a start :) Good luck, keep us updated please
Yep! Fantastic application-developer-oriented library for making crypto. Used by a growing number of applications because it's fast, robust, and avoids many common n00b errors.
&gt;In my experience, you tend to end up with can't happen cases in (exhaustive) pattern matches regularly What's wrong with this? You can gracefully handle these cases then. &gt;Rust has implicit memory allocation for the stack at least. Yes but stack exhaustion is a common, uncatchable panic in many languages
I will bring this up. Still, reserve_additional was largely the behaviour used by the standard libraries, so that seems like what we want to optimize for. What are you using C++-style `reserve` for (that wouldn't be handled by `with_capacity`)?
You expect if you call `reserve(n)`, then it will (maybe) reallocate memory and then you can insert at least `n` more elements without triggering a resize, right? RingBuf does the reallocate... but then doesn't actually make the space usable. It just wastes cycles and a bunch of memory as currently written.
BTW, [there was recently another PR](https://github.com/rust-lang/rust/pull/18170) to update RingBuf, but it got clobbered by this refactor stuff. Hoping they'll rebase. If you have any thoughts on their design that would be greatly appreciated!
You were likely getting auto-coerced to `&amp;[T]` which does impl these traits, but the traits have been shifted around a bit to accommodate DST, and that might have broken it. [That coercion is on the chopping block anyway](https://github.com/rust-lang/rust/pull/18645). I imagine like tuples, Eq is impl'd by a macro up to some constant value of `n`. Our type system can't currently express "implement this for all values of `n`".
Also note that you can easily write your own extension trait for Vec, or just a plain function, that provides resize if that's what you *really* want. You aren't constrained to the standard APIs like in other languages. We're trying to cut down the APIs to something simple, composable, and orthogonal; and `resize` is *not* that. It *can* however be easily implemented in terms of other functions Vec provides.
Change that pic out, graduation is over :)
In addition to what Gankro said, fixed length arrays never even had traits implemented directly.
You actually forgot how it all began. It doesn't follow either of 2 "trends". C and C++ were developed by engineers to solve practical problems. Both were "open source" and a tremendous success. Rust follows the same path.
That sounds plausible and my workaround was to do: assert(x.as_slice() == y.as_slice()) I also believe it's a good thing that auto coercions are on the chopping block. The less magic the better in my opinion.
Look at it like this: `next` is actually using lifetime elision. If you were to write it out long-form, you'd have: trait Iterator&lt;E&gt; { fn next&lt;'a&gt;(&amp;'a mut self) -&gt; Option&lt;E&gt;; } This means that `'a` will generally be inferred as how long the iterator itself lives. However, this is kind of a problem: there's no way to *refer* to that `'a` such that you can use it in the return value, because the return type (`E`) has to be named as a parameter to the trait. That is, what you *really* want is this: impl Iterator&lt;Bar&lt;'a&gt;&gt; for Foo { fn next&lt;'a&gt;(&amp;'a mut self) -&gt; Option&lt;Bar&lt;'a&gt;&gt; { ... } } But you can't do that. You *also* can't do this: impl&lt;'a&gt; Iterator&lt;Bar&lt;'a&gt;&gt; for Foo { fn next&lt;'a&gt;(&amp;'a mut self) -&gt; Option&lt;Bar&lt;'a&gt;&gt; { ... } } Because the `'a` in `impl&lt;'a&gt;` and the `'a` in `next&lt;'a&gt;` are *two different symbols*. Basically, the inner `'a` is shadowing the outer `'a`. This means that your `next` is not *actually* returning the same type as you said it would in the trait. You *also, also* can't do this: impl&lt;'a&gt; Iterator&lt;Bar&lt;'a&gt;&gt; for Foo { fn next(&amp;'a mut self) -&gt; Option&lt;Bar&lt;'a&gt;&gt; { ... } } Here, `'a` is *not* tied to the lifetime of the iterator itself. When the compiler goes to do borrow checking on the body of `next`, it has no way of knowing if `'a` is going to be longer than `&amp;mut self` is *actually* valid. This also means that the lifetime on `self` for your `next` is different to the one the trait promised. The long and short of it is: you *cannot* have an iterator return a borrowed reference to anything which depends on the lifetime of the iterator itself. This is why the [streaming iterator repository](https://github.com/emk/rust-streaming) exists. I've actually been running into this myself recently. The way I got around it (rather hideously) was to use `Rc` and `RefCell` to basically do the borrow checks at runtime, rather than compile time. This is ugly and nasty and smelly... but there's not much you can do about it if you want to be compatible with `for` and all the other `Iterator` methods.
Great read and I agree mostly! However, what I'm most interested in, what was the learning trajectory you took to become sufficient in Rust?
If the problem is `reserve_additional` being too long, would `reserve_more` be better? (And we can either revert `reserve` to the C++-style meaning, or remove it altogether if `with_capacity` is sufficient.) 
Other bikeshed options to completely decouple us from legacy beliefs: `anticipate`, `prepare`, `ready`, `make_room`. Edit (via bstriesaurus: allot, apportion, budget, devote, earmark, bespeak, preengage, procure...)
Don't bother doing the compiler's work ;)
No way. The whole Unix &amp; C appeared nearly entirely out of Thompson's and Ritchie's desire to program stuff on PDP-7 (and later PDP-11) after collapse of Multics. It's later that they pushed the project for commercial use under the disguise of OS for typesetting program that was used in AT&amp;T internally at the time. Their management didn't understand what that thing *was*. See Art of Unix Programming, it's actually a great book to read anyway.
Thank you for the reply. It explains clearly the problem. (: 
While you are correct in that main alphabets are in the order, symbols are indeed allocated roughly according to their typewriter positions. [Wikipedia article](https://en.wikipedia.org/wiki/ASCII#Organization) has an explanation about them; in particular, the order of `!"#$%` and `&amp;'()` is same to that in the mechanical typewriters.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 3. [**Organization**](https://en.wikipedia.org/wiki/ASCII#Organization) of article [**ASCII**](https://en.wikipedia.org/wiki/ASCII): [](#sfw) --- &gt;The code itself was patterned so that most control codes were together, and all graphic codes were together, for ease of identification. The first two columns (32 positions) were reserved for control characters. The ["space" character](https://en.wikipedia.org/wiki/Space_(punctuation\)) had to come before graphics to make [sorting](https://en.wikipedia.org/wiki/Sorting_algorithm) easier, so it became position 20hex; for the same reason, many special signs commonly used as separators were placed before digits. The committee decided it was important to support [upper case](https://en.wikipedia.org/wiki/Upper_case) [64-character alphabets](https://en.wikipedia.org/wiki/Sixbit_code_pages), and chose to pattern ASCII so it could be reduced easily to a usable 64-character set of graphic codes, as was done in the [DEC SIXBIT](https://en.wikipedia.org/wiki/DEC_SIXBIT) code. [Lower case](https://en.wikipedia.org/wiki/Lower_case) letters were therefore not interleaved with upper case. To keep options available for lower case letters and other graphics, the special and numeric codes were arranged before the letters, and the letter *A* was placed in position 41hex to match the draft of the corresponding British standard. The digits 0–9 were arranged so they correspond to values in binary prefixed with 011, making conversion with [binary-coded decimal](https://en.wikipedia.org/wiki/Binary-coded_decimal) straightforward. &gt; --- ^Interesting: [^ASCII ^art](https://en.wikipedia.org/wiki/ASCII_art) ^| [^ASCII ^Corporation](https://en.wikipedia.org/wiki/ASCII_Corporation) ^| [^ASCII ^Media ^Works](https://en.wikipedia.org/wiki/ASCII_Media_Works) ^| [^PETSCII](https://en.wikipedia.org/wiki/PETSCII) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+clvr8tf) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+clvr8tf)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I would jump directly into Rust! The documentation is getting better and better, and everybody will be glad to help you! Stick to IRC and this subreddit and everything should be ok EDIT: you should of course consider learning with (and contributing to) [Rust-Rosetta](https://github.com/Hoverbear/rust-rosetta). You will find a lot of code examples there
This is correct, however DroidLogician's original question was concerned with why punctuation in ASCII is not contiguous, which is partially explained by the preferential treatment given to alphabetic characters.
I think it's fine to jump in directly. I'm no great shakes at C/C++ — I know the basics (if, for, while, some generics/templates, OOP) that are present in most languages, but I'm not familiar with the stdlib and C++-specific stuff. It might be better, actually — you don't have all that extra baggage from C++ to carry around. Not learning Haskell is fine. The Haskell inspired features can be picked up directly (I did not know Haskell when I picked up Rust). The trait system can be a bit confusing, though. Later on you may need to understand differences between stack-based (local) and heap-based(`box`) memory, but this isn't too necessary to start off with.
I recommend *learning* C to any professional programmer. Other than practicing with C, use it as little as possible :) If you're not a programmer by trade then C is probably more trouble than it's worth to learn. Haskell won't help you that much with rust. The main thing you'd gain by learning it before rust is practice with algebraic data types and exposure to functional programming. I think if you're at all interested in Haskell that you should learn it, but the order you learn rust and haskell doesn't really matter. 
The man wants to rock a tux, let him rock a tux.
Additionally, using C++ _after_ learning Rust is pretty amazing since one tends to program as if the ownership/borrowing rules still held. At least, that's what happened with me.
Was hitting the exact same problem, and used the exact same smelly workaround (Rc and RefCell). Thanks for the heads up on the streaming iterators!
I'm actually more surprised that LLVM is not smart enough to realize that the vector version is equivalent and generate the same assembly. Maybe the dynamic vector throws it off? I've noticed in the past that rust's containers optimized badly. Maybe if you replaced the vec! by a plain array it would optimize better? Is there any point in using a vector here anyway? EDIT: actually gcc doesn't fare better, it optimises the C version using a switch but not one iterating against a `const char *`. At first glance I don't think LLVM being slower has anything to do with pattern matching, maybe it's the rest of the tokenizer code that's less efficiently compiled.
Yeah, I would *definitely* learn Rust before so much as thinking about C++. But C at least has the virtue of being a simple language.
"Learning C" isn't hard. Learning to reason about explicit object lifetimes is hard. Rust helps a lot there because you'll get compiler errors instead of weird nondeterministic crashes.
good resource for learning rust: www.rustbyexample.com
I agree with happilydoge; this is a pretty well-written post. And I agree, I'm pretty excited by Rust as well. I'm a big fan of compile-time safety; like you said, I'd much rather fight with the compiler than the debugger. And every language should have pattern matching! There are a couple of things that really irk me about it though. For instance, there's no real simple way to extend existing types in the stdlib, since it's disallowed to impl on anything declared outside the current crate. You can't even declare a new type and impl on that. For instance, pub type WhiteList = HashSet&lt;String&gt;; impl WhiteList { pub fn is_whitelisted(&amp;self, name : &amp;str) -&gt; bool { ... } } This is illegal. The only way I'm aware of to achieve extending a type is by wrapping it in a unit struct, and having to manually implement the Deref traits: pub struct WhiteList(HashSet&lt;String&gt;); impl Deref&lt;HashSet&lt;String&gt;&gt; for WhiteList { fn deref(&amp;self) -&gt; &amp;HashSet&lt;String&gt; { let WhiteList(ref set) = *self; set } } impl DerefMut&lt;HashSet&lt;String&gt;&gt; for WhiteList { fn deref_mut(&amp;mut self) -&gt; &amp;mut HashSet&lt;String&gt; { let WhiteList(ref mut set) = *self; set } } impl WhiteList { pub fn is_whitelisted(&amp;self, name : &amp;str) -&gt; bool { ... } } Another thing is that all tests have to be submodules, if you want to separate the testing code from its implementation. And this requires importing everything you want to test from the super module. Coming from Go, where all you have to do is make a file called modname_test.go, this just seems silly. I hope these issues get ironed out, because otherwise it's fantastic!
Take a look at the [expanded code](https://gist.github.com/anonymous/b9170ad54c4c74bd15f5#file-expanded-rs-L137-L153) why don't ya! :P That `[...]` compiles down to a single instruction, which is translated to a `match` by the `regex!` macro. :-)
I have been a BBEdit user since System 9. :) It just works and it doesn't suck. I need to upgrade to 11. 
I'm guessing it's just difficult for LLVM or GCC to reason about heap-allocated vectors. A fixed-length array could very well optimize down to this code. It does not seem necessary to use a vector here, no.
Interestingly, your idea is exactly what the regex engine does: https://gist.github.com/anonymous/b9170ad54c4c74bd15f5#file-expanded-rs-L137-L153 (And it looks like it isn't quite perfect either. Might be a bug!)
If you look at the guts of `vec!` you'll see it creates a vector ~~and pushes each element one by one,~~ from a boxed array which is probably too complex to optimize into a fixed-length array. Especially considering that, under the hood, it's all pointer manipulation on the heap. LLVM probably can't make any deductions about heap allocations because of all the possible side-effects. If it subtly changed a heap-allocated vector into a stack-allocated array, it could break stuff down the line. `rustc` itself has enough information that it could potentially turn the vector creation into a fixed-length array if it knew the vector wasn't being used or mutated in a way that changed its length afterwards, but that would require wrapping the array in a type that behaves exactly like `Vec` except for growing/shrinking. I think this simply has to remain a manual optimization. Honestly, is it all that difficult?
&gt;&gt; let mut m: Vec&lt;Box&lt;&amp;Shape&gt;&gt; = Vec::new(); &gt; That does not make much sense. Use either Vec&lt;&amp;Shape&gt; or Vec&lt;Box&lt;Shape&gt;&gt;. Thanks. So with your help and some more experiments, I got this working using this syntax: impl&lt;'a&gt; Ord for Box&lt;Shape+'a&gt; So, I think what's been confusing me here is that a pointer/reference/box to a trait object is not a single pointer, but two, one to the data and one to the vtable.
Full immersion, honestly. By the time I got to Rust I'd already had plenty of experience learning new languages and paradigms, so it was easier to understand than some I had dealt with before. Having already dealt with functional programming was a plus, as I already understood pattern matching and closures and optional/result types. The hardest part, probably the same for everyone, was grokking lifetimes. But that came naturally as I started using them more to see how they behaved and what caused the compiler to error. Eventually I just learned to stop worrying and love the compiler.
Boxed slice to vector is probably faster, but LLVM still probably can't take it from heap to stack. But this optimization can easily be done at code level.
&gt; It didn't really make sense why GHC needed a garbage collector if all state was explicit and encapsulated. I figured the compiler should know exactly when I'm done using a value so it could get rid of it. The semantics of mutable state are explicit. But the *implementation* of those nice immutable values involves arbitrary references between objects in the heap. Consider let x = [1..100] in (\i -&gt; (x !! i), \i -&gt; (x !! (10 - i))) This evaluates to a pair of two closures. Both of them hold a pointer to the list `[1..100]` even though that value isn't directly exposed to the outside. Since both closures capture the same list, you can't rely on unique ownership; you need GC or refcounting or something. You can't duplicate the value because that could increase space usage exponentially. This property of hiding values internally is what makes closures such a powerful abstraction, and also makes them somewhat painful in Rust -- you have to introduce lifetime bounds to make sure closed-over pointers are valid as long as the closure is. In Haskell (in its common implementations) the situation is even more complex because of laziness. The expression `[1..100]` evaluates to a thunk, which is something like a zero-argument closure. We can ask for the nth element and it will incrementally expand that thunk to a chain of cons cells, up to 100 of them. And that work is shared between the two closures that captured `x`. Secretly, behind the scenes, Haskell is powered by pervasive, implicit mutation. And this introduces really sneaky side effects -- not on semantics but certainly for performance.