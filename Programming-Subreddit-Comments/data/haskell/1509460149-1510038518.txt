 {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE FunctionalDependencies #-} {-# LANGUAGE FlexibleInstances #-} class BuildList a r | r -&gt; a where build :: [a] -&gt; r instance BuildList a [a] where build xs = reverse xs instance BuildList a r =&gt; BuildList a (a -&gt; r) where build xs x = build (x:xs) list = build [] main = print (list 1 214 4 49 :: [Int]) Credits [here](http://okmij.org/ftp/Haskell/vararg-fn.lhs)
Here's a weird recommendation, and not the way to get quickly up to speed with haskell. That said: Godel, Escher, Bach - Douglas Hofstadter I come to comp sci from an unrelated background, and GEB fills in a lot of gaps for me in an entertaining, and philosophically motivated way. This is the book you pick up a couple times a week over a year, and think interesting thoughts about while taking a shower, or driving, etc. If you feel like you're into FP "for the long haul", this is probably a really good book.
exactly. I also wrote a reactive-banana wrapper for brick. If you would not make those wrappers separate packages, you would burden all users with lots of unnecessary dependencies (no one uses reflex and reactive-banana at the same time). The unexposed internals are the only issue.
Getting a package into stackage is so incredibly easy, I don’t see that as a valid indicator for the package itself being good. That said, I don’t know what my stance is on the voting system, I personally feel it has little benefit because there are no justification/explanation accompanying the votes.
I'd been thinking about the same thing, only in the context of variables rather than dictionaries, and here is what I got: http://lpaste.net/359690
I don't think they do.
Yeah. It's pretty terrible. Maybe if it was just syntax sugar for the proper thing, it wouldn't be so bad.
So as a person living on the west coast looking to work remote, how would that work? You'd fly them out and put them up for a couple months?
I propose to start with pictures explaining types and relations between them (functions &amp; typeclasses) to get it conceptually. Only then move to actual code, be it in ghci or haskell files.
Would someone please think of the children!
Why?
For Julie Moronuki, one of the coauthors, Haskell *was* the first programming language. The whole philosophy of HPFFP is that Haskell is easy to learn, fun, and doesn't require neither maths nor programming background. Sure, “easy” doesn't mean you can become Don Stewart in 21 days, but it does mean that, given the right pedagogy, the steep learning curve becomes gently sloped, the intimidation disappears. So it stands to reason that the book would try to be as approachable for complete newbies as possible.
&gt; tearing down I would hardly call it that. I think Elm is a great language too, but it's just true that it lacks some useful means of abstraction. There are ideas I just can't express, but that's fine for the vast majority of apps.
The general comments are here totally unfounded criticism. This amazes me! But I guess at the same time, it is also an opportunity. Why would people not get the advantages of objects? Haskell is good. But maybe objects are too? Especially, all the people writing critical comments. Have you read all Alan Kay papers? Did you listen to what he has to say in his talks? Alan Kay is one of the most important Turing award winners honored for his invention of object orientation and GUIs. Maybe objects would make more sense in more expressive type systems (than that of Haskell)? Perhaps there is a reason there is not a single important GUI application written in Haskell (besides xmonad/leksah; according to 'State of the Haskell ecosystem'). 
I can see two people under [People](https://github.com/orgs/text-utf8/people). Can you not?
What is a "classical" functional one. Alan Kay often says, he doesn't think in categories like object-oriented or functional programming. Maybe because it is limiting? An obvious application of objects is for GUIs. Perhaps also more general graphics with user interaction.
Its fine. I personally published this with the functional works blog which allows to straightaway import github pages.
When talking about computers and software. I would say that particularly with how prevalent undoing and backups and versioning and so on are today. The idea that deleting something gives you back a new value without the piece that you deleted is pretty intuitive. Particularly when it comes to math-y things like Haskell. If I see `y = delete 5 x` on a board where `x` is a set, I would definitely not assume `x` has changed.
&gt; Or are there other downsides to exposing internals? One reason I don't expose internals in Brick is because keeping them hidden forces users to come to me when the public API or abstractions aren't working for them, and that gives me an opportunity to either 1) improve the public-facing API or 2) work with the user to show how they could be using the library more effectively. If the internals were exposed then I'd miss those opportunities. Still, I recognize that it's no fun having to maintain a fork. As far as the pull request is concerned, that was enough of a rewrite of existing functionality that it was very difficult for me to see the essential abstraction that needed to be available. I didn't want to just merge the new implementation and maintain two, and it wasn't clear to me what the essential missing pieces were that could be exposed from (or added to!) Brick to make the reactive-banana implementation possible. If someone wanted to try to distill that and propose an API change or exposure, I'd be willing to consider it. (Start by stating the concerns, though!)
Now I can. Looks like they updated it. Thanks, that's helpful!
Dude this is so clutch. It actually seems to work pretty well on Mac but with the huge added bonus that I can do a good bit of the preprocessing on the command line. In the documentation, it has a hamming window option, a VAD option, and a `stat -freq` option to get the frequency data (which should be easy to read into Haskell as a list). This will save me a TON of time so that I can get into the meat of my work on the HMM. Thanks a ton!
We tried using Elm across a team of rather experienced Haskell developers for frontend work. Over the course of about 6 months, we ran into a situation where the lack of rank-n types combined with the lack of typeclasses meant that a rather large chunk of code turned into a repetitive cut-and-paste fest with simply no work-around possible in the language-as-offered. If we'd had either one we could have worked around the situation. (We spent a couple of months in more-or-less untyped hell trying to hack around it in various ways.) This experience left me rather soured on the idea of trying to employ Elm in earnest. YMMV, but mind you the application itself largely centered around using a number of interpretations of an EDSL that didn't lend itself to first order presentation, so this was literally our entire use-case for the language. We ultimately moved to PureScript and it served us well for years without ever once running into the same sort of situation. We rewrote the entire application in PureScript more or less overnight. With the subset of Haskell type system features offered there was always some work-around to the various limitations on hand. This is compounded by the fact that this sort of issue raised from the Haskell-side of the community largely falls on deaf ears in Elm, while on the Purescript front any similar impediment is rather rapidly patched and updated if there is any path forward. /u/paf31 a very competent maintainer and I'm much more comfortable with the type theory that underlies Purescript than that of Elm. When Purescript came along and offered up typeclasses, the major reaction on the Elm side was a sigh of relief over the fact that the Haskell folks finally shut up and went away. Stuck comparing the two languages, I'm left with a ridiculously lopsided sense of advantages on one hand, and a rather deep well of frustration on the other. This somewhat colors my phrasing. &gt; Where I work we're at 200,000 lines of Elm in production, deployed for 2 years, students answering millions of questions on it daily, and we've still had zero production runtime exceptions ever. Mind you, touting the fact that you have 200,000 lines of Elm in production doesn't really help sell me on it. ;) If we had gone that route, our 25,000 lines of Purescript may have easily wound up as 200,000 lines of Elm. Then again, I maintain something like 2 million lines of Haskell, so I'm sure there is somebody out there snarking the same thing at me. =)
If you teach basic functions functions, then simple types, then higher kinded types like maybe, then functors, then monads as join and fmap first, then introduce bind as join . fmap, then how you can use bind automatically in do, then there aren't any sticking points.
Nice, you take pride in being an asshole.
That is *very* interesting. Do you have a writeup on the subject somewhere?
Your core premise is even that it's useful to take understandable concepts (pictures) and relate them to abstract, unfamiliar concepts (type/category theory) as a method of laying a foundation. Why would observing behaviors in a REPL not provide a similar foundation? You can take observable, understandable phenomenon (the output of a program) and relate it to abstract concepts, and you don't even have the extra cognitive overhead of thinking about how elephants might relate to code. Assuming that using GHCi as a teaching tool is a bad first step is like telling high school physics students to forget about gravity and acceleration and just focus on the algebra. There might be like, one or two very unique minds in the class that benefit from that advice, but most people are just going to be left in the dust. This is not to say that anything about your approach is flawed, necessarily - I think the idea of trying to take these abstract concepts and communicate them with pictures and analogy instead of increasingly sophisticated terminology is a brilliant idea with a great deal of merit - But I do think trying to steer newcomers clear of GHCi "until they have a proper context" is doing them a vast disservice. 
I started liking programming when i started learning Haskell. C, Python, Guile.. i knew how i wanted things to work, but they just don't work the same. We program in Haskell the way I've always thought it should work. I wish more schools would focus on the functional, programming would seem less cryptic to an outsider if FP was taught over OO
When teaching Haskell to adults who are already programmers, I actually think the right thing is to go in the opposite direction and offer a way to map "System 1" thinking into Haskell so that students can use what they already have. Here's how I would suggest someone draw a bunch of textures if they were a new Haskeller looking for a "while loop:" main = do textures &lt;- .... whatever let step = 40 let go texs y = do match texs of (tex: rest) -&gt; renderTexture tex y go rest (y + step) [] -&gt; return () go textures 100 It is absolutely not elegant or safe, but it maps relatively closely to the ideal of a "for loop with an accumulator" and a neophyte can stamp the pattern out again and again in just about any situation. Note also that this approach does not require memorizing any new words or APIs. Appreciation for the type system will come on its own, but only if they don't get frustrated and quit because they are roadblocked on what should be a trivial problem. 
It’s a great idea! Would use it with my son for sure.
For real... This is perfect! 
To put it another way: If you tell, and then show, you create and then absolve confusion. If you show, and then tell, you encourage and then satisfy curiosity. Most people prefer the latter.
I was like 'why did that person not fix the docs?' and then noticed it's my library. Fixing that right now...
&gt; sockpuppet accounts Like this one?
&gt; We already got a rating system to assess whether a package is worth your time: if a package makes it into Stackage you can be confident it's really a good package This isn't really right. Have you considered taking a step back and focusing on your own Haskell journey for a while? Linking to Twitter threads about contentious issues, getting the link wrong, and making incorrect claims about Stackage (which I'm sure even the Stackage authors don't agree with) isn't really contributing to the discussion. If anything it just hurts makes the Stackage side of the argument look bad. If you want to help Haskell, just go write some awesome Haskell libraries! You can always come back to the packaging arguments later when you have a more nuanced view of the situation.
Here's an example of a simple program I made with it. Try it out! import Vis import Linear.V3 import SpatialMath main = animate defaultOpts $ \f -&gt; Trans (V3 0 (-20) 0) $ RotEulerRad (Euler f f f) $ Cube 10 Solid green (Also see [#6](https://github.com/ghorn/not-gloss/issues/6) is you have problems with it crashing.)
Don't start teaching from the basic building blocks of your subject! Instead start from a point on the boundary where your subject meets your students' understanding and grow from there. Also don't give them new concepts and then ask to solve problems about these concepts. Instead give them problems that make sense on level n (their current understanding) but whose solution requires level n+epsilon. I wish all things were taught like that.
Good input, thank you. You are right, I should clearly separate teaching people completely from scratch (="kids") vs those that have different programming languages (imperative) exposure so normally have no issues getting basic language with functions, lists etc, but those struggling with more abstract concepts. I believe this visual approach can work for both, but keeping people hooked and interested would require different basic analogies.
Pictures are fine but whatever you try to teach should be in conjunction with using GHCi. Children learn best by doing and experimenting.
Reproducible doesn't necessarily mean the package builds for your platform in the first place, though. Nix on macOS is basically 2nd class at best. A vast majority of users contribute to nixpkgs while using nixos or another Linux distro, so things tend to break over on macOS, even sometimes after a package works initially (Often due to a dependency update pushed to master after only testing on nixos)
Right, that's why I was wondering if there was some good before/after info so we know the benefit side of the cost/benefit tradeoff. I'm also curious about the other optimizations it may inhibit. But wouldn't you want to test with the same library, before and after removing fusion? Switching to UTF8 at the same time would add a large variable I use a lot of little bits of Text, but mostly I just store them and then parse them. So as long as the parser interface doesn't rely on slicing, both fusion and efficient slicing are probably not helping me. But it would be pretty annoying to have to give up the kitchen-sink Text API, or stick conversions in all my error message formatting.
`Maybe (world -&gt; Key -&gt; KeyState -&gt; Modifiers -&gt; Position -&gt; world)` could be `(?key :: Key, ?keystate :: KeyState, ?modifiers :: Modifiers, ?position :: Position) =&gt; world -&gt; world`, so people can plug in id instead of Nothing. Perhaps there could be a version with `(?key :: Key, ?keystate :: KeyState, ?modifiers :: Modifiers, ?position :: Position, MonadState world m, MonadIO m) =&gt; m ()` with `m ()` instead of `IO ()` in the return type, so people could roll their own monads.
Cool glad it's working for you.
The counter-argument I hear often is that most of the communication in the world is between servers, which tends to be ascii-centric. Is this true in your experience?
Well seeing your example `y = delete 5 x` I definitely think `x` and `y` are different. But how about `x = delete 5 x`. Now for Haskell this is like some kind of infinite recursive function. But in most imperative languages, `x = delete(5,x)` would be a valid function call mutating `x`. Maybe its a dogma associated with the syntax of C like languages. But I am really talking about the meaning of the word "delete" without any context(of computer or software). 
exposed brick? *record* budget? (no mention of a turntable, though…) really going for the haskell hipster set, huh?
I know only one Haskell developer in who has been using dynamic Haskell linkage. Now you might argue otherwise, but the justification of security updates doesn't hold up for Haskell/Rust/OCaml as evidently as it would for a hypothetical C++ cabal-install does, because the number of and kind of security issues that would demand rebuilding many packages is diminishingly small. Therefore static linking is fine because of the benefits of standalone binaries that can be copied and expected to work. Go goes one step further and avoids libc (on Linux you need to build Go with CGO_ENABLED=0), meaning the binaries work even more widely. That said, most Haskell users and developers use stack and cabal to install packages, so they're not affected by this as long as they have procured GHC and cabal/stack. Those that are affected and complained are users of pandoc or xmobar that don't use cabal/stack directly.
emailparse should actually have its docs on hackage and looking way better now https://hackage.haskell.org/package/emailparse-0.2.0.8/docs/Network-Mail-Parse.html Thanks for reminding me that I wrote that library ;)
Thanks for the help. Great responses. 
There is a turntable—we're listening to Bob Dylan right now :)
Hooray! If you want to test this release candidate with Stack, use this `stack.yaml`: https://gist.github.com/tfausak/e60fe3fa93344c747ac38891fa55a0db
That's good to hear! I didn't mean to imply that Functional Works stole your content. I meant to point out that there's not really a reason for this to be reposted already. The old post is still on the front page! 
I'm not certain and probably won't negotiate salaries myself, but I'm pretty sure we would negotiate salary at least somewhat based on the remote location, so it would vary considerably.
I'm happy to see different approaches to teaching/learning Haskell, but, man, I could not disagree more with this! I actually stopped reading after this sentence and had to come back later: &gt; To really learn and master the power of Haskell, you need to forget everything you know about imperative programming (yes, really, everything) and build upon a totally different foundation. You absolutely don't need to do that! I would consider myself an intermediate-to-advanced Haskell user and I have a secret: I don't understand category theory. I've never read Basotz's book. I've never read the Typeclassopedia. And yet I am comfortable enough with Haskell to get stuff done. 
Yeah, something like that. Maybe just a month in SF, then a week or two for a few months or something? It would depend on how far away you were.
 (?key :: Key, ...) =&gt; ... Is this a new GHC extension? I've never seen this syntax before. And on the topic of API design: `Maybe (world -&gt; Key -&gt; KeyState -&gt; Modifiers -&gt; Position -&gt; world)` is perfectly fine. Let's please not ruin another API with crazy type class usage.
I mean why would anyone come across `x = delete 5 x` in the context of Haskell? No one is ever going to write that or tell them to write it. And while `x = delete(x, 5)` is *possible* in languages with non-recursive bindings, it is not something you see. You don't see it in JavaScript or Python or Java or C. Besides that is not about mutation vs immutability, it's just about whether or not bindings are recursive.
Another alternative that I use is Visual Studio Code w/ Haskero and Haskell Syntax Highlighting extensions. It is a bit slower and has heavier ram usage due to using electron, but it still works well. The performance impact only starts to become an issue on code bases several thousand lines long, longer than you would ever deal with while learning.
Elm targets javascript peogrammers. Purescript targets category theorists who want to play with the browser
&gt; To really learn and master the power of Haskell, you need to forget everything you know about imperative programming (yes, really, everything) and build upon a totally different foundation. I kinda agree with this. On the other hand, &gt; One of pure abstract math, starting from category theory, from set theory, from type theory [...] I strongly disagree with this. First, the disagreement. We don't start teaching (or learning) maths itself via its most pure and abstract branches. Instead we start with concrete objects and problems, like counting people, dividing up apples, etc. As our education continues, we ascend the ladder of abstraction toward concepts like function, set, group, category, and so on. History -- i.e., the order in which the people discovered and invented things -- is usually an excellent guide to this kind of pedagogy. I don't deny that there's value in an abstract-constructive approach at some point in one's education. But abstractions should be learned by beginning with the concreta one is abstracting from. ([Here](https://aeon.co/essays/aristotle-was-right-about-mathematics-after-all) is a precis of the philosophy of maths that has inspired how I think about mathematical pedagogy.) So what should the foundation be for learning declarative programming? Two things that come to my mind are - spreadsheet programming (FRP!), and - markup languages. I remember starting VB in high school after earlier having been exposed to Excel programming and HTML, and being somewhat shocked at how one imperatively updated mutable variables rather than building up data structures and running values through (non-side-effecting) functions. This shock was a good foundation for later Haskell learning. Maybe my dream language along these lines would be a streamlined Morte, specialized to markup and spreadsheet applications (config files require a bit more background).
He said they use yesod
Which seems to be the case for a surprising number of people. Haskell was my first and is my primary programming language; HPFFP served as a general programming and a Haskell book.
&gt; Is this a new GHC extension? `ImplicitParams`?
Hmm :( I've opened an issue on it in the hopes of that getting me somewhere then, https://github.com/NixOS/nixpkgs/issues/31059.
Well, thanks! TIL.
Do you consider hiring from all over the world?
This looks rad, looking forward to playing with it!
it is nice to see fintech in the bay area. I would love to listen about your experience in using haskell in startup environment. If you guys do a meetup / browbag lunch, I would love to attend.
Yeah, let's crowd-source GHC development from them! Both cheap and hopefully of great quality. :)
&gt; If your ultimate goal is to learn Haskell that you would use seriously, I suspect that preparing yourself with Elm first would be a waste of time. What do you guys think. Personally, I started with elm because I wanted to make something on the frontend, didn't want javascript and wanted a good type system. I think that elm being simpler helped me get over the inital hurdle when learning my first functional language and the experience with elm is helping me learn haskell now, even though that wasn't my initial intention
good bot
I think it's brilliant. The user gets what they wanted (to no longer be associated with the content they produced) while the community gets the continuity they need.
Oh I see what you mean. Sounds like an interesting take on the subject: most resources prefer a more hands-on approach. Well, if you do end up making some illustrations, don't doubt to share them!
I feel like I have more experience than most people here on teaching Haskell to children, since I've been doing it for about 6 years. Here are a few quick thoughts on the article: &gt; First, stop trying to build bridges from imperative world — it messes with people’s brains. Maybe... but not entirely. Leaving aside the fact that even many kids these days have some experience with imperative models of computation (thanks, Hour of Code!), it's still important to remember that in many ways, we live in an imperative world still. Even for kids with no previous programming experience at all, what makes sense is that when they type something about a circle, that should make a circle show up. I can't count the number of times I've seen code like: main = drawingOf a a = circle 5 b = rectangle 3 5 and been told "It doesn't work! I wrote the rectangle, but it didn't show up." So the point is this. Imperative programming is definitely a skill of its own, and it is intrinsically far more complex than declarative programming. But it's also very natural, for reasons that have nothing to do with past experience with imperative programming languages. So why teach functional programming? Not because it's easier, but because the *deeper* kind of simplicity involved in functional programming is also important for a lot of other logical reasoning skills. Like mathematics, for example. Getting kids used to interpreting things formally is important as a step to developing those logical thinking skills more explicitly. So declarative thinking may not be "the default" for the human brain, but it is an important goal in its own right. (Of course there is SOME of this bridge-to-imperative stuff that is legitimately unnecessary when teaching from no prior experience. It's still important not to assume an advanced skill level in imperative programming; but it's equally important not to engage with the myth that if it weren't for being corrupted by Java, we'd all be perfect functional programmers from birth. Simple and intuitive are not the same thing!) &gt; We need to start from something close to the Type Theory I'm skeptical that types are exactly the right starting point, but I agree that starting with the structure of expressions is better than starting with computation. The first hurdle in learning declarative language is composition. Understanding the relationship between expressions and values, and between expressions and their sub-expressions, it itself a substantial challenge at the beginner level. And it's not enough to be able to figure it out in targeted exercises. The standard is this: can you express your own ideas creatively without being overly burdened by thinking about the notation? This is actually an ambitious goal for younger ages, but one that needs to be tackled before doing "hard" stuff like iteration or computation. This is one of my points of disagreement with most existing programming technologies that are built "for kids" (like Scratch, Tynker, etc.) It's as if they are designed to make it as easy as possible to get to "real" computation, like loops or other iteration, and so they are designed to do things like remove the possibility of struggling with expression structure, and to provide big libraries of artistic assets. But by robbing students of the chance to struggle with easier problems of composition, they end up dumping kids later on in an untenable position: their expectations are too high, and their ability to build complex solutions out of simpler pieces is not yet up to snuff. These kids are experts at following tutorials, and even figuring out the "clever" missing pieces, like plugging in the right loop bounds; but they can't use it to do substantial creative expression on their own. &gt; DO NOT read monad tutorials Okay so far, but... &gt; I’ve been thinking for some time now that math concepts underlying modern functional programming and type theory can be explained in a more accessible way using pictures and various cute animal related analogies. So, a monad isn't a burrito, but it is a cute bunny? The widely understood problem with monad tutorials is that analogies like these seem obvious to those who already comprehend an abstract concept; but they are not helpful to beginners. To beginners, what matters are concrete *examples*. Lots of them, repeatedly, until the abstraction clicks. Once that happens, THEN expressing that intuition by analogies and such starts to help out. Starting with animal analogies seems somewhat optimistic. And to be frank, teaching non-trivial type theory abstractions at all seems unreasonable, if you're truly aiming this at children. There's a lot of distance between "I can motivate this with an analogy" and "I can teach students enough to be independently competent at applying this idea."
Correct!
I wasn't implying that you would come across in `x = delete 5 x` in Haskell. My point was this is an absurd looking function definition in Haskell(unless you think about `fix`). While the same thing has meaning and is allowed in an imperative language. So similarly something like `delete` or say `remove` from the Collections library in Java mutates the backing array, while the behavior is different in Haskell. For new developers from the other community(or especially college level programmers learning data structures for the first time) this notion of `remove` creating a new version might be slightly alien, hence the line.
Nice! How's your mail client going?
But even in said Java collections libraries `x = remove 5 x` or rather `x = x.remove(5)` is NOT valid. As `remove` returns a `Bool` and not the mutated collection. That's the point I am trying to get at, you won't be confused because in Haskell you will see `delete 5 x` and in mutable languages you will see either `delete(x, 5)` or `x.delete(5)` but you won't see `x = delete(x, 5)` or `x = x.delete(5)`. Another thing to note is that very early on you need to teach Haskell programmers that bindings are recursive, because something as simple as `x = x + 1` is understandably not valid. And the second they get that they will never question the way `remove` works. I just personally like the names `insert` and `remove` and so on and don't want them to be apologized for every time they are mentioned.
Yay, xmobar should no longer crash now \o/
I understood your point. I will keep them in mind, for my future blog posts.
I'm curious to know what kind of materials you've developed to teach Haskell to children. Is any of it online that you could link to?
Just in case you haven’t heard of it I found the [learnyouahaskell](http://learnyouahaskell.com/) book incredibly useful when I was learning Haskell, it reads very well and explains things nice and simply. You’d probably want to supplement it with the book you listed but this one was very good for giving me a solid grasps of the basics 
It's all online, but it's all a mess. Everything I do is based on http://code.world, which uses GHCJS to compile Haskell to JavaScript, using a modified standard library that was originally based on gloss, and run it in a web browser. The link I gave you is heavily modified from standard Haskell (for instance, there are no type classes), but it's what I teach to kids. There's a more idiomatic Haskell version at http://code.world/haskell, which uses the standard Prelude. There's a pile of random supporting resources in [this Google Drive folder](https://drive.google.com/drive/folders/0B-qIu_nqxaMoYTQ4ODZjMjMtNWRiOC00OWZiLWI3MzMtOGIxODIyZDkxODBk?usp=sharing), but nothing that's really put together or complete, unfortunately. By the time I get around the halfway point of a class, I tend to be too lazy to write everything up, and just wing it depending on what the class needs. I've worked with other people, as well, to develop resources; again, it tends to be front-heavy, because things fizzle before the end. Documentation is a lot of work! Some good places to look are: * The "Guide" button on the code.world web site is a concise exposition of most of the topics (but is incomplete) * The "CodeWorld Topic Outline" document at the page above. This is a detailed (hundred and some page) description of the learning progression and topics. * The "Empoder Slides" folder are slides I made with a teacher for a past class, and are a pretty decent set of visual aids for about half of the class. * The "Handouts" folder has different draft handouts and worksheets I've used in the past. * The "CW2017 Curriculum" folder is less complete than the rest, but also best represents my current best thinking about the way to structure the content, if given a 45 minute per day class and a school year to do it with. * The "Comics" folder is just for fun. The PDF file is the right place to start there. Organizing this is a consistent part of my to-do list, but it's not getting any closer to the front. One of these days...
The `ctrl-c` issue is due to a bug in macOS High Sierra. See https://github.com/NixOS/nix/issues/1583 It's a bit unlucky that you've just started using Nix while this bug is present. It appears there's not much the Nix team can do about it until it's fixed upstream by Apple. However, you can run Nix in "single-user" mode. It used to be the default, but recently the Nix installer switched to configuring multi-user mode and unfortunately does not give you the option of choosing the mode you'd like. However, you can switch a multi-user Nix installation to single-user mode by following these instructions: http://zzamboni.org/post/using-nixs-single-user-mode-on-macos/ Nix's support for macOS has been steadily improving over the last year. This nasty High Sierra bug notwithstanding, it's actually quite good these days. Having used Nix on macOS now for over 3 years, my advice is to stick with it. Nix, like Haskell, is a bit impenetrable at first, and very different to anything you're used to; but, also like Haskell, with persistence it will likely completely change your worldview, and make nearly everything else seem painful in comparison. I like it so much that I've recently switched all of my Linux hosts to NixOS, after 20 years of using Debian. Now, regarding the `php` issue on macOS, this is easy to fix with Nix; so in a way, it's good that you've encountered this issue already, because it will demonstrate one of the reasons why Nix is so useful. Many build problems are fixable in Nix with just a simple local override, and your `php` problem is one example. The specific fix here is that you need to tell `clang` to compile php's C++ code with C++11 support, i.e., you need `"-std=c++11"`. This is easy to do in Nix. Nix's syntax can be off-putting to new users, so I would advise you to just do what this gist says for now, which should solve your immediate issue: https://gist.github.com/dhess/7210ca1d2818570e91bcbbc64d6164e9 Later, when you have a chance, read this section of the Nix manual to learn how to do this yourself: https://nixos.org/nixpkgs/manual/#chap-overlays Note that Nix overlays are fairly new, so as of October 2017, most Nix tutorials on the web will show you how to override packages using the older `packageOverrides` technique. However, in my opinion, overlays are much easier to understand and more composable, so my advice is to jump in and use overlays from the start. Most of the tutorials you'll see are easily adapted to the new overlay system anyway, once you get the hang of Nix. Good luck!
Awesome!! I've switched to single-user mode, and your overlay also fixed the problem :) Small redemption for nix, guess I'll truck on with it, thanks!
I have a concrete example where fusion gets in the way of other optimisations. I've done some work for the `text-utf8` effort implementing some "SIMD" like functions for length, take, drop, etc. which when given a raw text value is 100x faster than the stream fusion version, but if the value passed to these functions is based on fusion, the runtime back down to being slightly slower than the stream fusion based version because it needs the manifest array. really many of the benchmarks aren't representative of actual use cases because they favour streaming use cases, but how often to you ask for the length of a string without also doing something with it contents? At some point you're going to need the data written into memory, and you lose the advantage you were hoping for. As /u/tomejaguar suggested, we should be explicit about when we're using fusion because it isn't applicable to all use cases.
What encoding(s) is most widely use in these areas then? There shouldn't be any reason those couldn't be converted to UTF8 internally when read and reencoded when written no? The current UTF16 is the worst of both worlds, it doesn't give you the advantages of constant time indexing that UTF32 does, and it doesn't handle many common texts efficiently. /u/bos may have more to say than I do on the matter, but afair Text basically uses UTF16 for the same reasons Java does, and I'm not sure Java's choice is universally regarded as a good one.
&gt; This is not a matter of people's opinions, and it is almost certainly false. No. There's data that tells us it is true. As just a few data points: http://utf8everywhere.org/#asian There's also a number of not data-directed points in that article thatt are nonetheless true. UTF-16 is not a good concrete encoding for almost anything. UTF-8 should basically be always preferred. In some crazy scenario, when you do care about code-point counts rather than the rendered string, UTF-32 could be used, but that's certainly a specialty case. UTF-16 doesn't have a compelling use-case.
The branching overhead may not be such a big deal. The work /u/erikd has done on a pure haskell Integer implementation has shown we can get real speedups by having sum types - having Integer look like data Integer = SmallPos !Word | SmallNeg !Word | Pos !Natural | Neg !Natural allows some optimisations that aren't available to GMP, and actually make some algorithms faster than GMP IIRC. (It's been a while since I looked at this, so I may be misremembering, but I was pretty shocked to see pure Haskell outperforming GMP in some benchmarks)
Not just that, but even when the readable text of a document is compact in UTF-16, there's often markup (HTML, Common Mark, LaTeX, Docbook, etc.) that is compact in UTF-8. Even having to spend 3 bytes for some CJK characters, the size difference is rarely large and not always in the favor of UTF-16. Of course, if size is a concern, you should really use compression. This almost universally reduces size better than any particular choice of encoding, even when a stream-safe and CPU-, and memory-efficient compression (which naturally has worse compression ratios than more intensive compress) is used.
Your `Parseable` class looks like `MonadParsec` or `Stream` from `Megaparsec`: https://hackage.haskell.org/package/megaparsec-6.2.0/docs/Text-Megaparsec.html#t:MonadParsec https://hackage.haskell.org/package/megaparsec-6.2.0/docs/Text-Megaparsec-Stream.html#t:Stream 
This could be an interesting use case for [Backpack](https://ghc.haskell.org/trac/ghc/wiki/Backpack). &gt; the common methods that were needed to operate throughout the parser These methods could be put in a signature. By using Backpack you wouldn't need to parameterize the `Parser` type on the type of the input (you would parameterize the package as a whole instead) and you wouldn't need the `Parseable` constraints on functions and instance declarations, either.
What if I have a list of records, and I want to return a filtered list based on one of the values in that record? For example, data Food = Chicken | Lettuce | Corn | Pie data Meal = Meal { main :: Food, side :: Food, dessert :: Food} data Table = Table { plates :: [Meal] } Now, say I have variables, dinner = Table [mom, dad, kid] mom = Meal Lettuce Lettuce Lettuce dad = Meal Chicken Corn Pie kid = Meal Pie Pie Pie How can I filter dinner so it returns a table with only those meals that have Pie as a main course? Basically, how do I reference an attribute of a record of a record in a filter statement? Or do should I write an entire function(s) to break the whole thing down?
Some of these operations look a lot like things you'd find in [`monoid-subclasses`](https://hackage.haskell.org/package/monoid-subclasses).
Thanks. Looks very interesting!
I'd say that Scala is trying to be all things to all people, which is always a very difficult task.
Another direction for generalization that you might consider: https://www.schoolofhaskell.com/user/edwardk/heap-of-successes
If it's anything like gloss, you don't really do anything outside that function, save for loading some resources. You pretty much just call it from main and thats that. It's not *part* of your program stack, it **is** your program. In terms of the functions you pass in, you still can roll your own monad and just compose it with an execution function.
Yeah, a temporary fix marking allocBytes as NOINLINE has been implemented. A more permanent fix is scheduled for 8.4
Once I've done that for some project with the requisite `stack setup`, other projects that also uses GHC 8.2.1 but don't have the `stack.yaml` changes also ends up using the RC2. I suspect it's because of the default `compiler-check: newer-minor`, and the RC2 version name being actually '8.2.1.20171030'. How can I uninstall the RC2 after I've installed it using `stack setup` on a project with the modified `stack.yaml`? This is quite a pain point in my use of stack -- the general inability to uninstall and GC old stuff in the `.stack` directory. It can grow to quite a few tens of GBs. If I arbitrarily delete files or folders, stack gets confused and starts failing to find files it needs to use, even if another version exists. EDIT: In this case, I think I managed to remove RC2 by doing this from the `.stack` folder: `find . -name '*8.2.1.20171030*' | xargs rm -rf`
There is a simpler way to install nix in single-user mode. The install script has a line which decides whether to install single or multi-user code (based on the macOS version). I simply changed that script. That only works if you haven't installed nix yet though.
I wrote this package (shout out to gloss for inspiration). Feel free to ask any questions or report any annoyances. I have an unreleased package that aims to replace this with a similar interface with 100x better performance. It's based on OGRE3D bindings I wrote. We're using it internally at the moment and it's ugly but I'm happy to clean it up and release it if there's interest.
 filter ((Pie ==) . main) $ plates dinner 
No, but now I have motivation to make one. It'll take some time, though.
It used to be simple, but they made some changes in the arch haskell packages recently (they are dinamically linked by default iirc), making a mess. I got to a point where stack wouldn't build anything, there was always a problem. What worked for me was just deleting everything Haskell-related with pacman, manually installing a statically linked stack executable, and letting stack handle ghc and anything else needed. Alternatively, there are some steps listed on the arch wiki, but I had no luck with those.
Step 1. Remove everything Haskell-related that you installed via `pacman` except for `ghc`, `ghc-libs` and `ghc-static`. Step 2. Get a statically linked binary of `cabal` from somewhere. You could either run the `bootstrap.sh` script in the cabal repo for this or use `stack` to install it. There is also a `cabal-static` AUR package but I haven’t tried that myself. Step 3. Thinks should be working without any additional configuration now.
A while ago, Arch package maintainers changed haskell packages to link libraries dynamically instead of statically. (I think the reason for that was related to a change in the [default flags for gcc](https://www.reddit.com/r/archlinux/comments/6n5tkp/arch_now_enables_pie_and_ssp_by_default_in_gcc/) - but don't quote me on that.) Prior to that many (most?) haskell packages and libraries were built/linked statically. There were various difficulties for people on Arch who had existing projects (or packages) built with static libraries. For example xmonad users. I'm not sure how (or if) these have been resolved. My approach was to install `stack` independently of the Arch packages using the [generic Linux build of stack](https://docs.haskellstack.org/en/stable/install_and_upgrade/), then to use stack to install `cabal-install` and then let stack/cabal take care of installing and maintaining ghc, ghci and libraries. **BUT** I didn't have a host of old projects and dependencies so it was easy in my case.
About a month ago I also had issues with Haskell libs on Arch and found the Nix-OS package manager. It solved all my problems with cabal. And its quite simple to follow the instructions on the GitHub link. [nix](https://github.com/Gabriel439/haskell-nix)
This worked for me, it seems. Thanks! (I was using the haskell-core repo, but it seems that may not be getting maintained going forward. Something about hosting problems.)
FWIW, Arch has [some problems](https://github.com/NixOS/nix/issues/879) with Nix at the moment
How do you recommend mtlizing database functionality? I have in mind two different solutions: class Monad m =&gt; DB m where query :: Query -&gt; Params -&gt; m Results insert :: Query -&gt; Params -&gt; m Int update :: Query -&gt; Params -&gt; m Int or class Monad m =&gt; DB m where getPerson :: PersonId -&gt; m (Maybe Person) insertPerson :: Person -&gt; m PersonId updatePerson :: PersonID -&gt; (Person -&gt; Person) -&gt; m () The former exposes database-speficic types such as postgresql-simple `Query`, `FromRow` etc. The latter is a simpleish DSL, but the class methods might increase a lot.
&gt; Stuck comparing the two languages, I'm left with a ridiculously lopsided sense of advantages on one hand, and a rather deep well of frustration on the other. This somewhat colors my phrasing. That's totally fair. I get that. :) &gt; This is compounded by the fact that this sort of issue raised from the Haskell-side of the community largely falls on deaf ears in Elm...When Purescript came along and offered up typeclasses, the major reaction on the Elm side was a sigh of relief Right, and I think this benefits everyone! I think it would be as much of a mistake for Elm to add typeclasses as it would be for PureScript to remove them. The languages have different design philosophies, and I think it benefits all of us to have an attitude of "PureScript does it this way and Elm does it that way, and if you are looking for characteristics A, B, and C you'll probably prefer the way language X does things." The way I see it, we have a constellation of compile-to-JS functional programming languages with different design philosophies. Programmers have different preferences, and projects have different needs. We're all better off if we can easily gravitate towards tools that are the best fit for our situations. I expect most Haskellers will like PureScript better than Elm, and so I'm happy to recommend PureScript over Elm as a better default choice for anyone who is already happily using Haskell!
I think your comments are awesome as always, Phil. :) I was specifically referring to statements like "Elm simply fails to offer you enough tools for abstraction" - if someone wants to say "when we used it, it failed to offer us enough tools for abstraction," that is a totally reasonable experience report. But it's hard to find a charitable interpretation of "if you use Elm, it will fail you," and I feel compelled to push back when there are so many experience reports out there which describe exactly the opposite outcome. That said, I totally appreciate that having a frustrating experience impacts choice of words. :)
&gt; That’s like starting math with peano numbers. I'd argue "counting on your fingers" is much closer to the Peano nats than it is to any of the rule-based approaches you learn in secondary math education.
Same solution really. Just ignore arch package management and use the appropriate nix manual [section](https://nixos.org/nix/manual/#chap-installation). Have had exactly zero problems (specifically related to arch+nix, nix needs workarounds for opengl applications outside NixOS). It's better to have nix update itself anyway.
Easiest way imo is to [install stack](https://haskell-lang.org/get-started) and let it manage GHC and cabal-install for you (or just ghc as you can use stack instead of cabal-install). [This tutorial](https://haskell-lang.org/tutorial/stack-play) will show you how to install and run GHC from stack.
Thank you, this is extremely helpful! I’ll respond to some of your points later on and if you don’t mind - would like to send you some thoughts and concepts privately to bounce off ideas!
I wasn't aware of this. But I haven't encounter any so far though.
Sorry for a late reply, but I would say knowing this stuff is really useful for me to understand other people's code and the documentation of a lot of libraries. If you want to do front end development in say Purescript it is very good to know this stuff IMO.
In [agdarsec](https://github.com/gallais/agdarsec/blob/master/src/Text/Parser/Combinators.agda#L27), I basically have: newtype Parser t ts m a = Parser { runParser :: ts -&gt; m (ts, a) } where the constaints are morally: MonadPlus m uncons :: ts -&gt; Maybe (t, ts) tokenize :: Text -&gt; ts
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [gallais/agdarsec/.../**Combinators.agda#L27** (master → 65e23d8)](https://github.com/gallais/agdarsec/blob/65e23d8f1bd3610f9fba409e27f7bfe44242ac77/src/Text/Parser/Combinators.agda#L27) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dp6kg1d.)^.
[removed]
May I ask what kind of time-table you guys are looking at? I will be graduating with a bachelor's in CS in May 2018, though I don't know if that is too far out.
No. That site has almost no data. It is a propaganda site, launched during a period when Google thought they would make more money if they could convince more people to use UTF-8. The site tries to convince you that what people actually do in Asian countries is stupid. But it is not stupid, it is for good reasons. The single "experiment" with data on the site has to do with what would happen if you erroneously process a serialized markup format, HTML, as if it were natural language text. Trust me, we know very well how to deal with HTML correctly, including what serializations to use in what contexts. That has nothing to do with what the default internal encoding should be for text. Most programming languages and platforms with international scope use a 16 bit encoding internally. Google has since toned down their attacks against that; this site is just a remnant of those days. If we were designing a specialized data type for HTML, then we could still do a lot better than a flat internal UTF-8 serialization. But anyway, we are not doing that. This a text library. So please make sure to benchmark against a sampling of texts that represents languages around the world, proportional to the number of people who speak them, in the encodings that they actually use. EDIT: Sorry that these posts of mine sound so negative. I am focusing on one specific point, the internal encoding. I am very happy that there are so many good ideas for improving the text library, and I appreciate the great work you are doing on it.
might not want to overload "main" though.
there's interest :)
Thank you for posting these `stack.yaml`s for every release. One little advice: also add the `nopie` ghc variants.
 Well, iirc his work occurred coincidentally during the time I was in the midst of reimplementing/rewriting `integer-gmp`, so he mostly benchmarked against integer-gmp-0.5; And here's the representation that `integer-gmp-1.0` now uses: -- | Invariant: 'Jn#' and 'Jp#' are used iff value doesn't fit in 'S#' -- -- Useful properties resulting from the invariants: -- -- - @abs ('S#' _) &lt;= abs ('Jp#' _)@ -- - @abs ('S#' _) &lt; abs ('Jn#' _)@ -- data Integer = S# !Int# -- ^ iff value in @[minBound::'Int', maxBound::'Int']@ range | Jp# {-# UNPACK #-} !BigNat -- ^ iff value in @]maxBound::'Int', +inf[@ range | Jn# {-# UNPACK #-} !BigNat -- ^ iff value in @]-inf, minBound::'Int'[@ range For comparison, `integer-gmp-0.5` used a different sum-type with weaker invariants: -- | Arbitrary-precision integers. data Integer = S# Int# -- ^ \"small\" integers fitting into an 'Int#' | J# Int# ByteArray# -- ^ \"big\" integers represented as GMP's @mpz_t@ structure. IOW, `integer-gmp` also uses a sum-type representation.
Do not install ghc/cabal with pacman. Get a binary of stack and use it to install ghc &amp; build cabal instead.
Can you provide us with realistic use-cases and data we can try to include in our benchmarks/evaluations? 
Shelved that idea for more gentle times :)
I wrote some initial unit tests for one of the forks. It worked pretty well for the prototype I was writing. Overall, I would rethink whether email parsing is something you want to support or if you can use http/json to communicate with client. Email's only real advantage is to provide "offline" behavior for free, whereas in http you have to manually build a queue of requests if a client goes offline. Otherwise, email just introduces more edge cases you have to handle in unstructured data than using json w/http post
I'd argue that all learning has to start with play. If you can't let people experiment (and the REPL is the ideal context for this) then things will never really click.
I would add in the READE the requirements/libraries that you use. For exmaple GLUT. Or add a ".cabal" file.
I also like the Guix package managee
We usually get UTF-16. Internal conversion to and from UTF-8 is wasteful, and enough people in the world speak Asian languages that the waste is quite significant. UTF-16 is quirky, but in practice that almost never affects you. For example, you do get constant time indexing unless you care about non-BMP points and surrogate pairs, which is almost never. I know, that leads to some ugly and messy library code, but that's life. Here's a hare-brained idea, semi-jokingly. Since this is an internal representation, we could use our own UTF-8-like encoding that uses units of 16 bits instead of 8 bits. It would work something like this: encoded | unicode -|- &lt; FFF0 | itself FFFp xxxx | pxxxx This would have all the advantages of UTF-16 for us, without the quirks. Pretty sure this encoding has been used/proposed somewhere before, I don't remember where. Also - I agree with /u/zvxr that having two backpack-switchable variants with identical APIs, one with internal 16-bit and one with internal 8-bit, would be fine. As long as the 16-bit gets just as much optimization love as the 8-bit, even though some of the people working on the library drank the UTF-8 koolaid.
`streaming` + `foldl` is a great combination. I believe `streaming` is a good example of [functor-oriented programming](http://r6.ca/blog/20171010T001746Z.html). The [Stream](https://docs.oracle.com/javase/9/docs/api/java/util/stream/Stream.html) + [Collector](https://docs.oracle.com/javase/9/docs/api/java/util/stream/Collector.html) API in Java actually has a similar feel. `Collector` lacks the super-useful [`&lt;*&gt;`](https://hackage.haskell.org/package/foldl-1.3.3/docs/Control-Foldl.html#t:Fold) of `Fold` in the standard API, [but it can be implemented](https://stackoverflow.com/questions/30210547/grouping-by-object-value-counting-and-then-setting-group-key-by-maximum-object/30211021#30211021). If doesn't look as clean as in Haskell, though.
Exactly. So for communication you use bytestring. For natural language text, you use text. Text should be 16-bit internally.
Thanks so much for the above, for now I will manage to write the processing for each handler separately, however, I must get back to the `enter` once I feel am confident to deal with it.
I can't wait!
&gt; Also - I agree with /u/zvxr that having two backpack-switchable variants with identical APIs, one with internal 16-bit and one with internal 8-bit, would be fine. That's good to hear, as I actually planned for something *like* that, since I definitely drank the UTF-8 koolaid, and in the applications I'm working on, I deliberately want an UTF-8 backed `Text` type, but I also recognize that other developers may have different preferences here, and I don't want either to force you to have to use UTF-8 against your will when you want UTF-16 (btw, LE or BE?), nor do I want you to deny me my delicious UTF-8 koolaid. And while at it we can also provide UTF-32... :-)
For serialization, you do whatever encoding and compression you want, and end up with a bytestring. For internal processing, you represent the data in a way that makes sense semantically. For example, for markup, you'll have a DOM-like tree, or a stream of SAX-like events, or an aeson `Value`, or whatever. For text, you'll have - text, best represented internally as 16-bits.
I am super excited to announce this survey! It is inspired by Rust's recent surveys and Johan Tibell's state of Haskell surveys from a few years ago. The goal of this survey is to better understand how Haskell users feel about the language, ecosystem, and community. So please, if you're reading this: Take 10 minutes and fill out the survey. Thanks!
I would also like to see this happen.
Why though? I personally would suggest not using ImplicitParams.
I have a newbie question about arrays. I know I could figure out this with lists (and lists of lists), but I'd like to figure out for efficiency reasons how to do this with arrays. To simplify a little: assume I have a two-dimensional array, where I want to threat the first dimension as a "grid" to "loop" over and the second as the data for the grid. Given a function that maps a row of this into a new row, how do I map this function over all rows? So how do I treat a two-dimensional array as an array of arrays, instead of an array of elements? I know that in Repa I could have an array of tuples, but if I understand correctly I am limited to at most six elements in the tuple there. If you wondering why I don't do this with lists: the arrays involved are quite large, my real problem has more than one grid dimension and I need to access neighboring elements of that grid efficiently.
I agree with everything in this comment except that text is "best represented internally as 16-bits". I don't think there is a general-purpose best representation for text. It depends on context. Here's how my applications often process text: 1. read in UTF-8 encoded file 2. parse it into something with a tree-like structure (HTML,markdown,etc.) 3. apply a few transformations 4. encode it with UTF-8 and write to file For me, it would be more helpful if the internal representation were UTF-8. Even though I may be parsing in into a DOM tree, that type still looks like this: data Node = TextNode !Text | Comment !Text | Element { !Text -- name ![(Text, Text)] -- attrs ![Node] --children That is, there still a bunch of `Text` in there, and when I UTF-8 encode this and write it to a file, I end up paying for a lot of unneeded roundtripping from UTF-8 to UTF-16 back to UTF-8. If the document had been UTF-16 BE encoded to begin with and I wanted to end up with a UTF-16 BE encoded document, then clearly that would be a better internal representation. The same is true for UTF-32 or UTF-16 LE. That aside, I'm on board with a backpack solution to this. Although, there would still be the question of what to choose as the default. I would want that to be UTF-8, but that's because it's the encoding used for all of the content in the domain I work in.
This looks rather nice ~~but surely it has space leaks? The fields of `Summary` are not strict~~. ~~And~~ here is a cargo cult bang pattern, on the field `name`: data Passenger { name :: !String, fare :: !Double, survived :: !Bool } 
Am interested!
I love this idea, especially asking about what extensions should be on by default!
Surveys are great! few comments before I dare to answer: Will the raw data (say, without email addresses) be shared, when, and where? Open questions are difficult to anonymise, but it might guide people to answer accordingly. About questions: - *If you use GHC, how do you install it?*: My two common ways are - **Official bindists**, this is missing all together - hvr's ppa. This is kind of "Operating system package", except I'd like to differentiate between "Official Debian's package", and "3rd party", like homebrew, chocolatey or ppa's are. - *How long have you been using Haskell?*: There are people that used Haskell for over 20 years, still active in community. Honour this span of experience. - *Which Haskell compilers do you use?*: I suspect GHCJS or Eta are more popular than e.g. UHC. Should the predefined non-GHC and other options be switched? - *If you use GHC, which versions of it do you use?*: Given the amount of options, I'd rather select between "8.2", "8.0", "7.8", and all the way to "7.0". I'm personally intersted to know, how much there are `7.6` users, and I'd rather bias to having "too many".
Yep, sounds good. LE or BE - um, not sure. Whatever Windows and MacOS do. I'll look it up.
That's the kind of stuff we do, too. But the important points are "3. apply a few transformations", and what languages the `TextNode` will be in. If you are doing any significant text processing, where you might look at some of the characters more than once, and you are often processing CJK languages, and you are processing significant quantities of text, then you definitely want `TextNode` to be 16 bits internally. First because your input is likely to have been UTF-16 to begin with. But even if it was UTF-8, the round trip to 16 bits is worth it in that case. Personally, I don't care too much about the default. Backpack is really cool.
I have forked and hurriedly backpackified the parser as an example: https://github.com/danidiaz/write-you-a-haskell-follow The signatures in the main library module are cleaner and simpler now, I believe. Only a String implementation is included atm.
For question 15 ("How do you install GHC?"), Nix should be an option.
I won't provide my email address to a random form, sorry :(
OK I'll help with that. I'll look around for some non-proprietary content, and for some world population and languages data.
I have tried sound IO on Haskell recently (for interfacing a scanning tunnel microscope). My overall conclusion is *do not do it*. The garbage collector is a major pain point. Sound has soft real time requirements, and once you miss a frame, it's all gone. It is the kind of problem you solve by running threads on some lower level language (like C or Rust), or simply doing it in another process and communicating it to your Haskell software. At the end I settled on a Rust sever that my Haskell code access through a socket, but low level threads have their merits too.
Thanks for the feedback! - For installing GHC, I definitely should have listed the official tarballs and an "other" field. If I see a bunch of responses for the operating system package answer, I'll split it out to be more specific next year. - For length of time using Haskell, I copied this question from the Rust survey and forgot to account for how much longer Haskell has been around. If there are a lot of answers in the last bucket, I'll split it up. Regardless, I don't feel like I'm being disrespectful to people that have worked with Haskell for a long time. But perhaps I'm wrong! - For Haskell compilers, we'll see what the results say. I think I agree with you though. - For the GHC options, 7.8.1 was released over three years ago. I was trying to limit the number of options to prevent the form from getting super long, but I should have listed more and put them in a box like the language extensions.
Fair enough. It won't sign you up for anything (like Haskell Weekly). The survey responses are sent to [Formspree](https://formspree.io) and then emailed to me. Next year I hope to have a custom system set up to receive responses. I'll be sure to be clearer about what will happen with your email address. 
Thanks for the feedback! It's clear now that I should've put an "other" option there like many of the other questions. 
Thanks I forgot about this. I updated what I could, but also ran into this bug https://github.com/ndmitchell/hlint/issues/259 because I use the pattern keyword quite a lot. Also the haskell parser gets confused when I use the let lint suggestion inside the do block. It tends to expect me to put the semicolon in the weirdest places.
&gt; We usually get UTF-16. Internal conversion to and from UTF-8 is wasteful This, of course, is exactly the annoyance that people receiving/distributing content as UTF-8 have with the current UTF-16 representation. I'm not really drinking anybody's koolaid, but I can say that the vast majority of text data I have to encode/decode is UTF-8 and an internal representation that makes that nearly free would be nice . I have no data, but (despite my euro/anglo bias) my impression is that more haskell users fall into this camp than the UTF-16 one. But, really, in 2017 this shouldn't be an argument. /u/ezyang has spent considerable effort equipping GHC with technology tailor made for this situation. All we need to do is start using backpack in earnest.
Thanks, will do!
And make it a checkbox, please. I'm using system package manager on lin and stack on windows.
&gt; deriveReturn Done. Thank you I totally forgot about guards and didn't know about the trick for ignoring constructor parameters.
I think you are on to something, but I since I am a noob I would find an example VERY useful.
If that's all that is stopping you, you could just use a fake email. I didn't have to verify my email or anything.
if you're using gmail, try this: `actualUsername+haskellsurvey@gmail.com` Mails will be delivered to `actualUsername@gmail.com`, but can easily be filtered due to the presence of `+haskellsurvey`. Unless someone is manually cleansing the mail list, this works like a charm. 
&gt; you could just use a fake email Oh I did! But if I can do that there's not much point them asking for it in the first place ...
I'm mostly concerned about future data breaches (or Formspree or anyone they share their data with) exposing my email address. This kind of thing has happened to me a few times.
Why not just leave it blank? Aren't all the questions optional?
Fir the 2nd annual Haskell survey include a "don't know" on extensions and a comment box. You were asking some mighty detailed "yes/no" questions on topics I didn't have an opinion on. 
I thought it was the weirdest question because there were only 5, not particularly-common choices. So I left it blank. Then I saw your comment and realized that there was probably a hidden scrollbar, and submitted a second survey with only that question filled in. Go `ScopedTypeVariables`, go!
Why wouldn't that fall under "Operating system package"?
One thing I would suggest is adjusting the language extensions question so you can choose what "level" you support the extension. There are extensions that I use all the time and where making them a default would be a significant QOL improvement. Particularly if they don't actually add any extra mental overhead (e.g. just removing some existing restriction or conservativeness): MultiWayIf, LambdaCase, TypeFamilies, Derive\*, TypeApplications, Flexible* etc. Then there are also ones I think would be cool and good for the overall direction of Haskell but not as big of a deal, or perhaps not backwards compatible enough for me to put it in the first category: NoImplicitPrelude, OverloadedStrings, RebindableSyntax etc. Then there are ones I am relatively indifferent about. Then there are ones I am actively opposed to being defaults: NPlusKPatterns, ImplicitParams, IncoherentInstances, Undecidable\* I really want to get the first group through, so I'm worried about diluting my preferences, and I think it would be good to see which extensions people are actively reliant on and make sure those get through. But I feel like a lot of people can see the value in a lot of different extensions (they were made for a reason right?).
Sorry about that! I tried to make it obvious that the box was scrollable, but apparently I failed. There were simply too many extensions to list them all without scrolling! 
&gt; For the GHC options, 7.8.1 was released over three years ago. Yeah but projects used in teaching settings where lecturers don't necessarily have control over what the machines run and can't possibly mass install software have to accomodate for these kind of old versions. Agda just recently dropped support for 7.6.3 for instance (because we wanted to use features introduced in 7.8). As long as it's not too much pain to maintain backwards compatibility, it can be worth it.
That's quite cool actually, and seems so simple :O !
It's like writing a letter to Santa! :D
Having taught people Haskell I think that's a terrible idea. You want the immediate roadblock. That's a feature not a bug to learning. Haskell isn't in the Algol family. You have to make the assumptions they are used to in programming explicit and to do that you have to make them aware of those assumptions. The first thing someone learns in programming is a program is a series of steps (implicit order dependencies in computations) The second thing they learn is a print statement (state change). You want to break both that habits immediately. Otherwise they will never grok Haskell. Haskell will always be a bad language. So no when you are teaching, no use of the I/O monad for days. You get them to solve problems by writing a series of definitions about the problem not a series of actions. Learning to think that way takes time. Your example is misleading in that it looks more like the code they are used to than it is. 
Hmh, not sure that's correct for `Summary`? As in: the `Summary` constructor is only applied after the stream has been folded over, so any stictness of its fields doesn't play a role while streaming. The `Fold` type, on the other hand, and its instances (e.g. `Applicative`) should keep any intermediate values strictly evaluated, which it does (through bang patterns in `Pair`, see https://hackage.haskell.org/package/foldl-1.3.3/docs/src/Control-Foldl.html) I may be missing something though.
I'm going to say for young kids (5-9), LOGO works well. They get used to the REPL. They deal with cons lists for looping. They learn good quality function definitions with useful parameters. The only non Haskell thing is they do have mutable variables but at that point variables are the key thing to learn. Also more importantly to chain logo routines together you want most routines to return the turtle to a safe state (like always pointing up or always centered). So as a consequence they become aware of state. Teach a kid LOGO and they will have the right grounding for Haskell to be natural. 
I have to get back to other tasks but something like the following (not great, but trying to get some of the essence across): let eResult = (do v1 &lt;- someEitherReturningThing v2 &lt;- someEitherReturningThing2 v1 return v2) in case eResult of Left err -&gt; .... Right r -&gt; .... 
Oh I see. Well I never read instructions :) But if one is collecting email addresses one ought to explain why, I think. It is rather sensitive personal information. 
Ah, very interesting! Yes, the Applicative instance for Fold uses strict Pair and therefore everything's fully evaluated during the foldl. My intuition that the fields needed to be strict would only work when the `Summary` type is the direct subject of the fold.
Filled every field with "FIX THE RECORDS!!1".
If you don't need to ship anything then you can easily rely on linking everything dynamic. I use `-dynamic` whenever I invoke `ghc` and within the cabal options (although I prefer installing packages with `pacman`). Compiling everything has an archaic feeling to me and it's downright annoying with a slower CPU.
I taught Haskell to maybe a dozen people at my previous company. None of the people I worked with cared about beauty or type theory at all. They were in the game solely because the Haskell guys were building tremendously efficient server applications and they wanted to help out. We had way better luck (and much more positive feedback) when we beelined them onto the shortest path to being ready to write production-ready Haskell. We intentionally structured the application in such a way as to allow a newbie to contribute without having to understand all of this stuff, and it was great. These engineers eventually pick up the complicated stuff, but on real problems that actually matter to the business rather than tiny academic examples. People tend to be a lot more motivated when the work they're doing relates to the job they have to do. :) 
There are ways to do this. But since this is a newbie question what I would say is you are thinking about this problem imperatively in particular you are thinking as if you were in an eager not a lazy language. Remember you have infinite data structures in Haskell and they won't evaluate in detail until you need them. Since you are talking large and finite you might want to look at Data.Sequence and Data.Vector. So what you probably want is given an nxm array spit out all the axb neighborhoods (and anamorphism) and do computations on those neighborhoods. You also have an evaluation monad to control computation. You will get insanely good efficiency if you write the algorithm as a hylomorphism. To do that you need to create the final result as a catamorphism acting on an anamorphism. That is you want to unfold the array to create neighborhoods and fold the resultant neighborhoods to get your result. I can't answer in more detail because you were too vague about the problem. Make sure to look up "traversable" and keep going until you understand this. Your traversing strategy should be abstracted away not embedded in your code. The good array package if it turns out you do want arrays is: http://hackage.haskell.org/package/repa Haskell is a lousy language for doing what you want to do. It is a fantastic language for teaching you how you should do what you want to do. 
Everything is a lambda expression. You can always just replace the usage with the definition. So amountOfFood dinner = length $ (\(Meal xs) -&gt; xs) dinner which is the same as: amountOfFood = length. (\(Meal xs) -&gt; xs) and you can just use the RHS whenever you want. 
I picked "I stopped using Haskell" and found out that afterwards, most of the survey questions basically stopped being applicable. Should I proceed under the assumption it's asking about my past rather than present usage of Haskell? Otherwise, I don't think it will provide any useful data points.
I'm a bit puzzled by `partition`. It's very cool but I don't see why it's better than returning a stream of `Either` and then folding over that.
I'm not sure this is possible, but assume it were. How are they in any meaningful sense writing Haskell? What is the point of Haskell without all the theory? What do you get over say Scala? Now all that being said. I've never seen good Haskell that doesn't have the complicated stuff all over the place. pointfree notation, layers of monad transformers, corecursive data structures, taking advantage of laziness... I don't think I could have learned Haskell from production code, the conceptual ecosystem is overwhelmingly complex. I still often look at code, don't understand it, look at the library don't understand that but realize there is a computer science concept I didn't understand which is the root of the problem. Obviously I haven't seen your application. So I'm not sure where we go from here but I guess let's hit the first question. What are they getting from your way of teaching them Haskell? 
If you know nothing about programming, you take their advice; if you've done enough programming to have a favorite editor, then you use it. Either way, you pick an editor and get started. I don't see what the problem is.
A small nitpick: signature in [the first example of `clmap`](http://tech.frontrowed.com/2017/11/01/rhetoric-of-clojure-and-haskell/#std-liberate-me) is incorrect (`EDN -&gt; EDN`).
Sorry, I forgot to answer your first question. I plan on sharing the raw data without email addresses after the survey is over. I've never run a survey before, so I'll have to figure out if sharing free-form answers is acceptable with regards to privacy. I plan to keep the survey open for a month (that is, until December 1). I will gather up the results, analyze them, and present them in a blog post at some point after the survey is closed. That blog post will, of course, be featured in Haskell Weekly :) 
I ended up doing essentially this. I installed a static version of stack from the AUR and used that to build a static version of cabal. So far everything seems to work although I didn't delete xmonad from my system when I initially deleted all Haskell related stuff. Hopefully that wont come back to bite me.
Fixed. Thanks!
Thanks for the feedback! I'm hesitant to turn the language extensions question into some complicated table that asks how you feel about every extension. I think we (as a community) should get some interesting results solely from the extensions that people want enabled by default. 
Using past experience to answer the questions is totally fine. If you can, please leave a comment to that effect on the last question. Thanks! 
Interesting. I looked it up and it seems it's made possible by including the compiler in the executable. Better packaged then what [is required](http://simonmar.github.io/posts/2017-10-17-hotswapping-haskell.html) to go that direction in Haskell by a wide margin though.
Thanks for doing this! I've been fascinated by this debate and considered doing something similar myself. My idea including keeping the errors as a constructor within the `EDN` type. I don't think mapping over EDNs make sense though, does it? Do Clojurians program like that? The values are going to be heterogeneous. &gt; Any sufficiently complicated dynamically typed program contains an ad-hoc, informally-specified, bug-ridden, slow implementation of half of a type system. Haha that's cool. A while ago I coined this: &gt; ## Greenspun’s tenth rule of mathematical logic &gt; Any sufficiently complicated mathematical proof contains an ad hoc, informally-specified, bug-ridden, inflexible implementation of half of type theory. &gt; http://h2.jaguarpaw.co.uk/posts/greenspun/ 
&gt; I don't think mapping over EDNs make sense though, does it? Do Clojurians program like that? The values are going to be heterogeneous. They do indeed. All values are unityped in a dynamic system. However their library `specter` is pretty sweet. It is like a dynamic version of `lens`. &gt; Greenspun’s tenth rule of mathematical logic Nice!
*streaming* actually has an [unseparate](https://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming.html#v:unseparate) function that produces a stream of (something equivalent to) eithers. If you want to apply completely different operations to each branch the two-level representation can feel more intuitive, however. There are two main ways of "nesting" `Streams`s: the inner stream can be the functor parameter (this means a stream divided sequentially) or the inner stream can be the monad (this means some kind of alternating stream, or a stream that should be consumed in two different ways).
Why is UTF-16 the best semantic representation for text? (Answer: It isn't and every advantage claimed for it is debunked in the UTF-8 everywhere documents.)
The scrollbar does not even render on my mobile browser. 
We largely used Haskell as a tool for getting work done rather than as a playground for using complicated ideas. Tail recursive loops like the one I posted are everywhere, and nobody cares because they don't cause bugs, maintenance problems, or performance bottlenecks. We got crazy, crazy high value out of the type system: The whole application runs in a monad formulated a bit [like this](http://andyfriesen.com/2015/06/17/testable-io-in-haskell.html) so everything is unit testable. All unit tests are pure, fast, and they never randomly fail. null isn't a submarine waiting to torpedo our application at every turn. Data from files and sockets always gets parsed into a reasonable structure first. It's the total opposite of "stringly typed." Threads are fast, cheap, and super easy to use safely. Engineers could drastically speed certain things up by swapping the word `mapM_` for `mapConcurrentM_`. (I forget what we actually called it. It did some nice things like report metrics on how many threads exist) The maintenance story is totally incredible. There are applications out there that have to withstand feature development over decades by a rotating set of engineers who have to change code written by people they've never met, and in this respect, Haskell is like absolutely nothing else out there. By the way, I don't mean to say that engineers never learned about the FP stuff. I only mean to say that it was secondary to the primary goal of getting useful work done as quickly as possible. Once we had people onboarded and doing useful work, they would pick up on these ideas as they went.
This is great to know. Thank you. 
I’ve tried both ways, and I don’t love either of them. One way is the [monad-persist](https://github.com/cjdev/monad-persist) library I wrote, which provides an mtl class for persistent. Of course, this isn’t something you can easily stub/fake. Honestly, I don’t have a great answer to this problem right now. I don’t think any existing DB libraries are designed with this particular use-case in mind, but I don’t think it’s an impossible problem to solve, it just hasn’t been done yet. In the meantime, I’ve found that it isn’t really all that bad to make code that requires the DB run tests against a real database (as long as your application is the only application that “owns” the database, you can treat it as an extension of your application). I’d rather have some way to easily execute queries against an in-memory database for my tests, but this becomes very difficult as soon as you need anything beyond basic SQL. I am generally unsatisfied by existing database technologies. I don’t have a great answer to this yet, but I’m thinking about it a lot.
Perhaps I should google this instead, but what *are* the cases where one would absolutely want extensible records a.k.a row types?
Named parameters as arguments to functions, for one thing. EDIT: Respondants correctly pointed out that named arguments to functions don't exactly require row types, but if you want to define greet :: { name :: String, age :: Int } -&gt; String greet r = "Hello " ++ name r ++ ", you are " ++ show age r ++ " years old" And then call it with an argument me :: { name :: "tomejaguar", age :: 56, language :: Haskell } then you do indeed need some form of row polymorphism.
The `clKey` definition is wrong. Just val -&gt; f val needs to rewrap the key in the Map, changing the appropriate field. The current code only typechecks because the contents of the map have the same type as the "map" itself, but it has the wrong semantics when used to update. You can get the correct semantics pretty easily by writing it as: clKey k = _Map.ix k
That's more anonymous records than extensible records. I consider extensible records to be a much harder problem than anonymous ones.
If you read this response post, and even if you don't, I recommend reading the article to which this post responds, [Clojure vs. The Static Typing World](http://www.lispcast.com/clojure-and-types) by Eric Normand. While that title makes it sound like it will parrot Rich Hickey's absurd attacks against type systems, Eric instead uses his familiarity with Haskell's idioms to reword Rich Hickey's arguments in a much more convincing and friendly manner. I learned a lot more from this article than from its response. For example, the "at some point you’re just re-implementing Clojure" quote makes it sound like Eric wasn't aware of how easy it would be to implement an `EDN` datatype, or of what the disadvantages of such a type would be. On the contrary, he brings up the idea of such an `EDN` datatype to make a point about the difficulty of problem domains in which the input rarely conform to a schema. He first explains why precise ADTs are too rigid for that domain, and brings up the idea of an `EDN`-style datatype to point out that such a typed implementation would have exactly the problems (partiality etc.) which we attribute to Clojure's lack of types. That is, when the domain itself is ill-typed, modelling it using precise types doesn't help.
I tried to submit the form, but it relies on a bunch of third-party JS, and when I enable these, I get warnings about XSS attempts.
We might see each other at a Haskell meetup? I haven't been to one in awhile (and there hasn't been one since spring afaik), but I'll start going again.
May 2018 is slightly too far out. Something like January/early February is the latest we're looking for right now.
Alright, thank you for taking the time to respond.
Unfortunately no. We're very worried about not having enough time zone overlap and have had troubles with that in the past with a Germany company. We're in for North or South America, and maybe up to GMT. We would probably budge on this for an exceptional candidate though.
Those compiler error messages!
I also encourage people to read Eric's post (and watch Rich's talk), but I don't think that either of them are convincing. https://www.reddit.com/r/haskell/comments/792nl4/clojure_vs_the_static_typing_world_haskell_in/
Arg, thanks for the spot check! These things get rather hairy when writing unityped traversals. My implementation in the github repo is indeed `_Map.ix`.
I'm sorry to hear that. Haskell Weekly itself only uses Google Analytics. Formspree pulls in a bunch of stuff, but I didn't see any warnings. 
The absolutely bizaarrrro(!) markup in your post :) notwithstanding... Yes, they do. It's incredibly weird to converse with a person like this[1] because (generic) you program in such incredibly different ways. I think it *really* boils down to top-down vs. bottom-up in that Lispy people will start with very small functions and then build up. I think the disconnect ultimately stems from the "build small functions -&gt; success (dopamine!) -&gt; build bigger functions -&gt; succes (dopamine!)" feedback loop. IME very few (that's a qualifier!) of the Lispy people ever have been forced to maintain complicated business logic over any serious amount of time. IME any refactoring that isn't just "generic over data structures" is absolutely horrific in Clojure (specifically, but I don't expect it to be any better in any other dynamically typed language). At the same time I get the impression that most(!) of the "popular support" for dynamically typed languages comes from people who haven't actually tried any real long term projects using a half-way usable statically typed language (like e.g. Haskell, PureScript, or anything with Algebraic Data Types + type inference, really.). [1] I tried being one of "them" for a while. Didn't like it, so here I am. Back again. EDIT: I should add: Part of my mentality is that *effects* matter. They *really* matter both in theory and in practice, so I want a language that can constrain effects in some way. Clojure does this is a very clever way by just making "immutable" the default. This is great for everything involving data structures, etc., but it doesn't really answer the bigger question of "effects" vs. "pure/impure" -- one trivial example being that you can call out to any JVM function at a whim anywhere within a Clojure program... and I, the caller, cannot tell in any way whether you did that, nor prevent you from doing it at runtime[2].). Capability-based languages are an answer to this, but they've typically been dynamically typed, but I was quite excited to learn of Pony recently. (It's early days, still.) [2] Maybe there's some weird SecurityManager trick we can pull here, but... no. Just no.
This is kinda tangential - Agda, for example, has named function arguments, but does not have row polymorphism.
Hi, I'm a coauthor of the book. People that are going to use something other than Sublime or Atom don't need to be told what editor to use. I use Emacs for everything.
Yes, this precisely.
Right, but couldn't this be implemented on the language level as well? I get that having it on the library level is more powerful someway, but on the other hand you're constructing and then de-constructing a record that was never needed. Not that it doesn't happen elsewhere and it can't be fused away though. :) So yeah, I guess it could be useful.
I disagree. Modeling with precision does not mean modeling with accuracy. AKA, you can have a very clearly defined understanding of what you need out of a domain in order for your domain logic to functional appropriately without modeling the entire domain. IE, the problem is not the type system, it's that we're trying to model a total set of attributes AND a partial subset of attributes at the same time, and expecting to not need to introduce additional logic or some degree of indirection. That's silly - We can certainly still derive value from a rigid type system in the event that we're describing the intermediate steps between the creation of a 'final' business object and the process of gathering it's constituent parts. Also, if we decide to rigidly model the presence of attributes atomically, instead of modeling only the full set, we can have our cake and eat it too, by describing operations on the collections of the attributes we care about at a given point in the process, ala extensible records libraries. It's not that we can't use types to help - It's that we pay the cognitive complexity cost of a successful implementation differently. Just like every other argument between 'dynamic' and 'static' types.
nixpkgs is not limited to nixos, so perhaps that is a technical disqualification.
&gt; That is, when the domain itself is ill-typed, modelling it using precise types doesn't help. I think this is just wrong. It helps in *discovering* that your domain is ill-typed. We can still do `Aeson.Value` or even `Dynamic` if that's what's required (and transform that completely generically). Anything less is just pulling blinders over your eyes. (IMO and IME, of course.) EDIT: Honestly, and this may be quite uncharitable, I think Hickey is *committed* at this point and regardless of whether he realizes it or not, he cannot just abandon Clojure as a failed experiment. Either that or he's so incredibly lucky to work within the exact niche that Clojure does really well in that it doesn't matter. (The latter of which might actually be plausible since he came up with Clojure after quite a few years in industry and seemingly out of nowhere... just to please his corporate JVM-loving overlords... Hmm.)
The xmonad (and any other Haskell software installed via pacman) wasn’t ever completely resolved. Running a pacman update is still a Russian roulette. There’s a thread here about it, with some solutions for different stages of the problem, and a link to a blog post showing how to do install xmonad using stack: https://www.reddit.com/r/xmonad/comments/73z1ew/could_not_find_module/?st=J9HG1B9B&amp;sh=3760340dhttps://www.reddit.com/r/xmonad/comments/73z1ew/could_not_find_module/?st=J9HG1B9B&amp;sh=3760340d
&gt; The absolutely bizaarrrro(!) markup in your post :) notwithstanding... Huh, which bit? The bit that's a heading? &gt; Lispy people will start with very small functions and then build up Oh, that's generally what I do too!
Agreed on both counts.
&gt; He first explains why precise ADTs are too rigid for that domain, and brings up the idea of an EDN-style datatype ... Ironically an EDN is an ADT. 
Yeah, sorry about the bizarro comment. I shows up *really* weirdly in my browser. &gt; Oh, that's generally what I do too! Interesting. Of course you have the luxury of knowing that whenever you revisit/rewrite those functions, you get a compiler guarantee of certain things. (I'm *not* saying it's invalid as a way to program, I'm just saying that it's what I've observed as being prevalent in 'dynamic' vs. 'static' programmers. Maybe it's just in the way of *thinking* rather than the way of *programming* per se? I mean, you can *think* top-down, yet still program *bottom-up* as long as you have a vision of what you're going for, right? I'm also quite sure that there's all kinds of in-between, in practice.)
It may well be that dynamic programmers *have* to program bottom-up because otherwise they have no idea if it will work. In Haskell one *can* program top-down because we can design with types and stub out unimplemented functionality with `undefined`.
That was a very enjoyable read, thank you. It's one thing to shit on another language (I think it's most certainly in poor taste, but c'est la vie), but to do so with such a level of ignorance is.. bewildering. Especially in programming language circles, where false claims are not only instantly met with correction, but frequently disproved in the form of actual code, like the OP has done. I wonder if it is possible to show that Clojure is a proper subset of Haskell? (Barring non-Clojure JVM stuff, of course, but perhaps that isn't fair.)
That could very well be true. It would certainly be a type of "selection pressure" if we view it as a type of evolutionary process. (Which, incidentally, I think much of language choice, etc. is. The fact that it's mediated by cultural pressure, etc. is hardly relevant to the *process* itself. Of course there's hope that we can eventually transcend that pressure with evidence, etc., but it's still forthcoming, either conclusively "for" or "against".)
/u/chak, it's worth a mention that Debian and GNOME are moving to GitLab. Debian already has an instance and I'd expect GNOME to also run their own instance. https://about.gitlab.com/2017/11/01/gitlab-switches-to-dco-license/ EDIT: This validates my claim that major FOSS projects are using it and will give it more momentum and community support/mindshare.
Nix is pretty large in Haskell (or Haskell is pretty large in Nix) - I think "other" is misleading!
Indeed, I very much enjoyed Eric's post. My post is intended to fill in some gaps he seemed to be missing as well as show that implementing a full standard library was not necessary. It was also just a bit of fun :) I still think there are some interesting dangling issues. The choice of fully qualified keywords being bound to specs is a novel alternative to `newtype`, but it seems it might be anti modular. The argument about `Maybe` in record types vs extensible record types raises a good point: "you've either got it or you don't". Eric's post was a much friendlier take on Rich's barbs and I appreciate him writing it.
Wow, impressed that mafia was listed as an option for the build tools. 
Good insights. I would summarize my take away from this as: * If you don't know the data shape (scheme) ahead of time, like in programs like jq, or often quamtative analysis where data is basically JSON or csv files, a type like Value from Aeson, or Clojure's dynamic type, is powerful. * While modeling data statically usually reaps rewards for combining, docs and maintenance, some things are simply hard to fully capture in types. I would point out, though, that an EDN type is hardly satisfactory either. It's true that we can be overzealous with trying to model things statically, to the point we reach diminishing returns and don't realize it. Rich speaks as though that's good reason to discard such directions of language design, but I think that's a red herring.
&gt; However their library specter is pretty sweet. It is like a dynamic version of lens. Have you seen Scrap Your Boilerplate? I made a comment comparing it with some specter operations: https://www.reddit.com/r/haskell/comments/792nl4/clojure_vs_the_static_typing_world_haskell_in/doz2pp8/ I'm actually using syb at the moment on an AST for a Haskell-like language to make transformations on specific nodes, to search for nodes, and collect all nodes in an ordered list. These take about 5 lines of code over a large AST type.
Sorry, what exactly is this supposed to be documenting?
I would be interested, at least. I might even make a have with it, but no promises.
[Well that wasn't a good use of my time.](https://i.imgur.com/959nkcq.png) It seems Firefox's tracking prevention breaks the site, at least when in strict mode.
&gt; NoImplicitPrelude Don't want to derail this thread but IMHO I think Haskell 2010 went in precisely the wrong direction with this. I think having lots of useful and common definition loaded by default makes sense and then NoImplicitPrelude plus explicit being an advanced option. There is no reason stuff like Control.Monad and Data.List shouldn't always be loaded IMHO. 
&gt; I wonder if it is possible to show that Clojure is a proper subset of Haskell? Pretty much every language is a proper subset of every other, if you widen your definition of "proper subset" enough.
Oh of course, I suppose I was speaking a bit narrowly. Namely, if we could implement every feature of Clojure to a "reasonable approximation". (Obviously subjective, but yeah).
But if you didn't read the instructions, would you read the explanation?
Sorry about that! I opted to use Formspree this time around because it was the easiest way to accept form submissions. Next year I'll have a proper first-party backend set up that doesn't require JavaScript. 
I've used the OSX IDE. It is rather good. You avoid some of the back and forth between your editor and GHCI loads. You get some nice type information on definitions. You have a place for persistent experimental output. You get some quick and dirty definitions. It isn't earth shattering but it rather nice. Like most IDEs if you are coding more than an hour it is worth the headache of startup time. 
Is it possible Firefox didn't have permissions (uMatrix/NoScript) to load and run reCAPTCHA?
For someone learning Haskell the one I would look at is the Haskell 98 Prelude. More complete, easier to read, less advanced, more education oriented. Print it out it is 22 pages of very good definitions. 
1. install system `stack` from pacman 2. run `stack install stack` 3. remove system `stack`
No worries! Glad you'll have it fixed for next time. \^\^
That other comment was made under a different username, /u/chrisdoner. Is there a semantic difference between your two reddit profiles? Like, one is an admin and the other isn't, something like that?
It wasn't clear how I should answer "do you like ghc build tools" when my build tool of choice (shake) wasn't on the list. If it is included, then yes I like shake, it's great. If it's not included, then no I don't like them so much, that's why I use shake :)
You can’t do this in general, but it’s not because of the data. It’s because polymorphic functions deal with data as though it’s just pointers. So passing this data to a function that expects a pointer needs to be impossible, since we want to avoid realizing the data into the heap altogether. This would basically just be the ability to write `data` types in the `#` kind, right? Such data types would be to unboxed tuples and unboxed sums what regular ADTs are to regular tuples and `Either`. Sounds like a worthwhile feature to me, but it should probably just use a different word than `data` rather than a pragma.
Sorry for the confusion! My intent was for that question to sort of connect to the question about your preferred build tool. So if you prefer Shake, I would assume your feelings on Haskell's build tools more or less pertain to Shake. That being said, I don't really think of Shake as a "build tool" in the same vein as cabal-install and Stack. 
I mean having more stuff loaded by default / different stuff is part of why I would like NoImplicItPrelude. So you can import your own Prelude with all the goodies you like and perhaps a few existing Prelude definitions overridden without having to use NoImplicitPrelude every time. My own Prelude for most of my projects would have MORE of said Control.Monad and similar imported (e.g join). I just also might want to generalize some things and remove a piece or two so I can't just import both Prelude and my new custom Prelude. I think NoImplicitPrelude would lead to people coming up with and using lots of really complete and well designed preludes. 
I don't think the idea is to prevent code like `mapM_ print (enumFromTo 1 10 :: ElidedList Int)` from being written, but rather to make it easier to write performant code by having the compiler complain if it fails to optimize the above into a simple loop. That is, we do want to write code whose semantics is to allocate and pattern-match on an ElidedList, because that's easy to write, but we don't want the compiled program to waste its time allocating and pattern-matching on intermediate values which we know should be optimized away.
Oh ok, I should have put "satisfied" then. I don't agree with that last sentence though. I think shake actually is a build tool, like make is, while cabal and stack are always reminding people about how they're package managers, not build tools. Despite doing some stuff that looks suspiciously like building software. Just in an inflexible hardcoded way, because it's a package manager, you know.
Sure. I mean, essentially, Clojure is just a LISP with some sugar. S expressions are insanely easy to model in any functional language. Clojure would also have a pretty easy time modeling Haskell, right up until the type checker. But that's sort of unfair, because modeling Haskell's type checker is pretty non-trivial in Haskell too. This doesn't really say anything meaningful about Haskell vs. Clojure, so much as it says something really quite wonderful about functional languages. 
Question 9's options aren't MECE. How can we answer when "My company uses Haskell." and ("I sometimes work with Haskell." or " I work with Haskell almost all the time."
Fair enough. I have been meaning to learn a lisp to be a more well-rounded functional programmer, so I might spend some time with it. Either that or Racket I think.
Does "11. What is the total size of all the Haskell projects you work on?" mean "at your work"?
No, it means all of them. 
Let's all Greenspun together: &gt; Greenspun's tenth rule of unit testing &gt; Any sufficiently complicated codebase written in a dynamic language contains in its test suite an ad hoc, informally-specified, bug-ridden, inflexible implementation of half of a type system.
If you work with Haskell sometimes or almost all the time, that implies that your company works with Haskell. Going in the other direction, if the strongest statement you can say is that your company uses Haskell, then that means you do not ever work with Haskell. 
No semantic difference. I just made this as a throwaway on my iPad because I didn't have my lastpass details and it stayed logged in on this device. I should probably login on that one.
Nice to hear that smart people are thinking about those things. :) Looking forward to hearing more about it in the future.
Thanks. I hope this approach is viable without forcing people to buy a book-length treatment. (Though I think they should!)
I've really wanted them for writing handler chains in web servers. Often I want to write a handler that's part of building up a 'context' over the life of the request. A chain like this for showing the current user's team as JSON might look like: handleRequest = findLoggedInUser &gt;=&gt; findUserCurrentTeam &gt;=&gt; renderCurrentTeam &gt;=&gt; toJSON and you want `findUserCurrentTeam` to ensure that there is a logged in user in the context. `findLoggedInUser` should be able to guarantee there's a logged in user in the context (or else an exception will be thrown, in this simple model). Extensible records are great for this, because I can define something like this (with made-up syntax): findLoggedInUser :: ctx -&gt; App {ctx | loggedInUser :: User} findUserCurrentTeam :: ctx@{loggedInUser :: User} -&gt; App {ctx | currentTeam :: Team} In this case, `findUserCurrentTeam` is assured that there is a `loggedInUser :: User` in the context record. Also, both these functions are reusable across whatever else might be in the context, because they're only specifying that certain keys must be present, instead of that an entire specific type muse be used. This style, I think, is somewhat achievable in Haskell using current type-level-list libraries. But the syntax is usually a little grotesque.
I know a course that's still using Hugs :(
Is this sort of similar to linear types? Seems like the idea of communicating to the compiler that it needs to interpret the data as ephemeral instead of concrete has some overlap with the sorts of optimizations that would be enabled by 'values' that must be referenced exactly once.
Returning a naked JSON object is weird as the caller will lose type safety. I'd rather define a proper type and create a FromJSON instance then return this type in `getGbpRates` &gt; I'm not at all convinced my Error Handling is done right The error handling is OK. Although I'd change it to `MonadError` to be more flexible instead of `ExceptT`. If I remember correctly, Wreq also throws `JSONError` exception or somthing. You might want to catch that and convert it to your domain level types. &gt; I'm guessing if I wanted to get some Logging involved, I'd add to the transformer? That's one approach. You would have `ExceptT Text (LoggingT IO) a` or if you want more typeclass-based approach: `(MonadError Text m, MonadIO m, MonadLogger m)`. This is one example library for logging: https://hackage.haskell.org/package/monad-logger-0.3.25.1/docs/Control-Monad-Logger.html &gt; The whole Client.hs thing is an untestable blob Having worked with Wreq, that's what I feel as well.
Personally I was happy to see [mafia](https://github.com/ambiata/mafia) mentioned as an alternative build tool. It's all I use now.
&gt; The only non Haskell thing is they do have mutable variables but at that point variables are the key thing to learn. If you're implying that learning about the thing called "variables" in Logo (and Python, and Java) is a good first step to understanding variables in mathematics (and Haskell, of course), then I'll definitely disagree. Mutable variables are a completely different thing. They are more complicated. And most importantly, they are less amenable to reasoning logically about what you're doing. In this way, they are less amenable to reasoning logically about what you are doing. It's not just about mutable variables, though. Logo is built from the ground up on a global mutable state: namely, the drawing, and the position and orientation of the turtle. These are far bigger effects on the computational model than an optional feature like mutable variables. The entire process of working with Logo is making a specific sequence of ordered modifications to that global state, which is so intrinsic that almost no single statement of a program can be understood without knowing the expected state at the time it is reached. All of this comes down to the relationship with time. Mutability makes time both nebulous and pervasive. This nebulous notion of time in turn interferes with reasoning about code that depends on it, and its pervasiveness makes most code depend on it. Logo is almost an extreme in this sense.
Sure, of course you're welcome to chat about anything, here or elsewhere.
From the original article: &gt; And Haskell has that for ADTs. But can Haskell merge two ADTs together as an associative operation, like we can with maps? Can Haskell select a subset of the keys? Can Haskell iterate through the key-value pairs? Yes it can. See [bookkeeper](https://hackage.haskell.org/package/bookkeeper) or [rawr](https://hackage.haskell.org/package/rawr) or many of the other extensible record libraries or even some of the generic libraries such as [generics-sop](https://hackage.haskell.org/package/generics-sop).
You joke, but if questions on Stack Overflow are any indication, some beginner Haskell classes &amp; learning materials *do* start by implementing Peano arithmetic! I find it ridiculous. It’s too abstract for a beginner, and an experienced programmer will find it uncompelling, not to mention absurdly inefficient. Incidentally, does anyone know of an efficient lazy number type? When I started out with Haskell, I saw: and = foldr (&amp;&amp;) True and (False : repeat True) == False So I thought “Aha, laziness can give me short-circuiting for free!” so I expected something like this to work: small xs = length xs &lt;= 1 small [1..] == False But of course it doesn’t, because `length` and `Int` are strict; while you can get the desired effect by using `genericLength` and `Data.Nat`, I just can’t bring myself to use a linked list to represent a number.
I'm using it on macOS *and* Fedora *and* CentOS/RHEL. It's not the system package manager on any of those. I can also use it locally, just for a single project, even if I use something else for my global system. (And, in fact, I *do* use Homebrew on macOS for certain things, even though all my development and building is with Nix.) More importantly, Nix plays a totally different role in practice than yum/apt/whatever—I use it to manage and build my projects, it coordinates internal projects that depend on each other, it fetches dependencies from GitHub if they're not in Nixpkgs... It's like a system package manager *and* a replacement for Stack. I've even used it to build GHCJS from source. If I was going to use a GHC from source, I would probably also use Nix for that. My broader point is that Nix is so different from other options that it should have its own category and be tracked separately.
"questioning" *wink wink*
I filled out all the extensions we've enabled by default at work, as well as a few we *should* enable. Pretty sure it ended up being 60 or 70% of them!
Thanks for that link. My hot take is this: it seems like people keep attributing to static types problems that, in truth, are caused by languages that don't have structural record types. Also this bit called my attention, which raises some important points but which I think is ultimately misguided: &gt; # Types as concretions &gt; &gt; Rich talked about types, such as Java classes and Haskell ADTs, as *concretions*, not *abstractions*. I very much agree with him on this point, so much so that I didn't know it wasn't common sense. &gt; &gt; But, on further consideration, I guess I'm not that surprised. People often talk about a `Person` class *representing* a person. But it doesn't. It represents *information about a person*. A `Person` type, with certain fields of given types, is a concrete choice about what information you want to keep out of all of the possible choices of what information to track about a person. An abstraction would ignore the particulars and let you store any information about a person. And while you're at it, it might as well let you store information about anything. There's something deeper there, which is about having a higher-order notion of data. My read on this is that the ingredient they are missing here is [*dependency inversion*](https://en.wikipedia.org/wiki/Dependency_inversion_principle). This objection makes sense if your application has a centralized `Person` type that encodes all the information that all submodules that deal with persons accepts as argument and therefore depends on. But if instead you refactor your system so that each business logic submodule "owns" the types that it accepts as input, and the glue between the submodules is responsible for transforming global data to fit their input requirements, then the various input data types that these functions "own" and expect become abstractions instead of concretions. Think of it this way: the functions that accept and process this messy information have an implicit schema that they expect it to conform to. So to reflect that, each submodule should be written so that it has its own types that articulate its own schema, instead of trying to pluck fields out of some monolithic `Person` type that's shared between modules that have different concerns and assumptions. Note that the article gets very close to articulating this point when it talks about *information model* vs. *domain model*. But it just falls short of recognizing that this problem has that solution: 1. Use a messy JSON or EDN type as your application's information model. 2. Instead of plucking information raw out of the information model, pair every submodule with its own domain model as types that model precisely what information it expects to come into it and what comes out. 3. Model the relationship between the top-level information model and each of the domain models. Note that often this task is isomorphic to writing a lens that abstractly views some locations of the information model as an updatable value of the domain model. 4. Glue all the things.
**Dependency inversion principle** In object-oriented design, the dependency inversion principle refers to a specific form of decoupling software modules. When following this principle, the conventional dependency relationships established from high-level, policy-setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. The principle states: A. High-level modules should not depend on low-level modules. Both should depend on abstractions. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
The following quote resonates with me every day that I write Haskell: &gt; So much of our code was about taking form fields and making sense of them. That usually involved trying to fit them into a type so they could be processed by the system. I don't know if Clojure is right or Haskell, but I do know that there is no form-processing library in Haskell that is a pleasure to use. And the longer I stare at a the problem, the more I'm convinced that is is because of the rigid types.
Can I ask the obligatory "why not Rust" question? I am legitimately curious why you picked Go over Rust (if you were ever considering it at all)
&gt; Is this sort of similar to linear types? I think it's actually closer to Idris's [erasure](http://docs.idris-lang.org/en/latest/reference/erasure.html#changes-to-the-language).
What does your cabal file look like? If you don't have a cabal file, how do things go after running `cabal init` to set one up?
the cabal file is mostly the same as after a cabal init, I added gloss to the build-depends. It runs now, I think it didn't work because I ran the cabal init without running cabal sandbox init or something? I did that (and fixed the syntax problem of importing graphics.gloss instead of gloss) and it runs! So it was probably my stupidity? It took me a long time of figuring that out... Thanks for helping!
No worries :) I found cabal files a little intimidating when I was starting out, but they become easier to work the more you use them.
I think that would be a great start yes, but I believe we can do a lot more than that for some datatypes. Perhaps have two levels of the pragma - warn when it finds that it can't, with a second level added later to ensure it does for every data type where it's possible to ensure that and warn if you data type can't. If not clear, I gave a more specific example for EJ.
I guess I didn't explain well. The operation would be much like inlining, except framing from the datatype's point of view. Suppose you had an `ElidedMaybe` type, and functions `a :: Int -&gt; ElidedMaybe Char` and `b :: ElidedMaybe Char -&gt; String` - every (non-exceptional) execution path through `a` eventually produces precisely one of either an `EJust c` or `ENothing` .. and `b` (if not incomplete) has cases for both of those that it then immediately branches on. Rather than box it up when `a` is done with it and pass it to `b`, we take the relevant branch from `b` and insert it into `a` to extend the execution path with no allocation *or branching* since we already know the code path we want to extend to. It would be as if we had written a function `ab` and never even defined the type, rather than having defined it and then just written `b . a`. This is a lot like what the `INLINE` pragma and case-of-known-constructor optimizations *often* do, but sometimes fail at. For certain data types, it should always both actually be possible and desired.
Hmm, possibly? I'm not well enough versed on linear types to know how they work with branching.
Is there any thought to include lens support at the GHC level, e.g. a `DeriveLenses`-type extension? Maybe the Profunctor vs. Var Laarhoven vs. ? space needs to be explored a bit more first.
I would argue that you and /u/JeffB1517 are actually talking about the same thing, but from different perspectives--your 'students' had an immediate need to be productive first and learn later, and jeff had a strong desire to train the functional method of thinking so that productivity could be possible. I think you're both approaching the idea of "people latch onto concepts best when they can grasp a fundamental need for that concept" (after all, why learn Haskell for a job when there's no need? Our brains are lazy and wired to avoid learning anything it doesn't see the need for.) I read a fascinating paper about teaching Math to kids and it [emphasized strongly the importance of introducing a motivating "intellectual need" before new concepts.](http://math.ucsd.edu/~jrabin/publications/ProblemFreeActivity.pdf) It also brought up something really interesting: a tool or concept shouldn't really be introduced with easy problems. (The paper is very readable and I highly recommend it to anyone who is thinking about STEM education and teaching) For example: If I have a simple little toy program and that's my demo for implementing some mtl stack... Bleh. There's no connection, no need (you can just do the super simple solution), and no motivation; it appears to be some trivial bullshit introduced to get people to monotonically recite their rituals and incantations to get the right answer using the shiny BestestNewMethod^tm. That's fine and all if you can force someone to learn it that way, or if they "need to" for a class, or if they're learning on their own and are motivated by some external desire, but in general it's a terrible way to introduce a new concept. New concepts need context; you can't learn the shiny stuff without knowing how to do the boring stuff, but why would you learn the shiny way to solve a problem that can be solved easier/simpler/cleaner the boring way? On the other hand... Say you let people solve problems however the hell they can fit the peg into the hole, and then give them a problem that's juuust difficult enough to piss them off but not so hard they give up. Then, after they struggle a bit, solve it in a few lines of tight, clean, and elegant code. Now you have something--necessity, motivation, and illumination.
And, if we really wanted to, we could use overloaded lables and a single alias to make: #res . all . #catlist . all . #points
What happens if my library exports the first function? It has to compile to something.
I wrote a `FormSeq` monad some time ago, it was pretty clean, but I had the advantage of being able to render the form from the monad, rather than having to parse an arbitrary form, if that matters. If I recall, the general usage was *something* like: assignTask workforce = do task &lt;- Task &lt;$&gt; jobtype &lt;*&gt; joblocation let validEmployees = lookupCapabilities task workforce emp &lt;- Select validEmployees confirm (draw (task,emp)) (actionAssignTask task emp) afterwards This would present the user with a series of 3 forms, the first to input a required task (using form parts`jobtype` and `joblocation` defined elsewhere) look up which employees it can be assigned to, then present a selection form to pick the employee, and a confirmation dialogue to actually assign the task. The forms could be presented in one page or many depending on the rendering methods - if JavaScript wasn't on the menu for example, the one declaration would guide the user through multiple pages just from pointing them at the endpoint once - keeping tabs automatically of their progress though the monad. I hadn't heard of `digestive-functors` at the time, and I haven't actually looked at it closely enough since to know if it operates like that or not.
It compiles to the same thing you get when you export a function marked `INLINE` except without any alternative - since you can never call it in a situation when it is *not* going to be inlined, you don't need another form.
I recently had to build a form abstraction for Reflex-DOM. I ended up using something like a `Writer` monad to record `record -&gt; record` updates. This is extremely flexible and with lenses it's both concise and easy-to-use. I'm curious what your needs are. Maybe we can find a good solution.
Could you give an example of form processing code in Clojure and your best effort in Haskell so we can see the difference in pleasantness?
I didn't read the instructions but I did read each question, so if it said 1. If you'd like us to email the results of the survey please enter your address. We won't use your email for anything else [ ] then that would have been helpful.
A nice example of being able to actually use the row types directly is my simple-json library, where you can get json decoding and encoding with nothing but a record type alias: https://github.com/justinwoo/purescript-simple-json Something you don't really get with... any other commonly used tool :(
Can't we already define `ElidedList` so that `mapM_` over it never allocates? newtype ElidedList a = ElidedList (# a, a #) mapM_ f (ElidedList (# start, end #)) = if start &gt; end then return () else f start &gt;&gt; mapM_ f (ElidedList (# start + 1, end #)) 
I implemented zipWith function with using map and zip functions (for a school homework) with some research help but was kind of confused on how my map works (specifically "\"). Here is my function: zipWith f x y = map (\(a,b) -&gt; f a b) tupList where tupList = zip x y I would also like to know if I overcomplicated this (had to be implemented using zip and map in assignment). 
I was just playing with you. All g. I agree it could be clearer
Thanks for the link about functor-oriented prog, that would have been a nice reference, I would have added it in the post if I knew about it sooner. Indeed, all the power of `Fold`s comes from the `&lt;*&gt;` operator, without it we are nothing :)
Yes, that is also a possible solution. `streaming` docs provide several examples It's actually a bit up to you to decide: either you have functions overs streams and you split streams and apply simple Folds to each stream, or you compose folds into one big Fold (Folds may "prefilter" -- as they can `premap` -- through the `handles` function used with a `fitered` lens) which you run on one single stream. Or you do something a bit in between, like I did in the blog post. When I have clearly defined sets of elements in the stream that should be processed in isolation, I like to `partition` streams. But if some elements are to be used in several processings, then I use Folds with "prefilters". That's my rule o' the thumb.
Python Code Generator. interesting approach. I see potential. 
Is there any reason for not using the "official" Haskell bindings?
**EX.TER.MINATE!**
Sorry. This post had been incorrectly munched by the AutoModerator. I have approved it now. 🤙🏽
Can anyone recommend any good, beginner-friendly introductions to machine learning with Haskell (or Haskell-likes)? 
Exactly, that what I was getting at too. It can be done on the language level (and mostly has been done in this way in many other languages).
might check out https://gitter.im/dataHaskell/Lobby
A 16 bit representation is best for Asian language text, for obvious reasons that any average programmer can easily figure out. That's why it is in widespread use in countries where such languages are spoken. UTF-16 is not the best 16-bit encoding. It's quirky and awkward whenever non-BMP characters are present or could potentially be present. But in real life non-BMP characters are almost never needed. And UTF-16 is by far the most widespread 16-bit encoding. Just about all of what is written on the "utf8everywhere" site about Asian languages is invalid in the context of choosing an internal representation for a text library. The most important point is that markup is *not* the same as natural language text; how you serialize markup is a totally different question than how you represent natural language text. If you are doing any processing of Asian text at all, not just passing it through as an opaque glob, you definitely want it in a 16-bit encoding. A text library should be optimized for processing text; we have the bytestring library for passing through opaque globs, and various markup libraries for markup. Also, most authored content I have seen in Asian languages is shipped as markup in UTF-16. I don't think that is going to change. While it saves less than for pure natural language text, it still makes sense when you are in a context where the proportion of Asian characters is high.
Agreed.
Thanks! For anyone else interested, here's the website for it: http://www.datahaskell.org/
&gt; Most programming languages and platforms with international scope use a 16 bit encoding internally. You are over simplifying things here. Please note that the UTF-16 that e.g. Java uses (and hence icu does) comes from a time where UTF-16 was considered "unicode". Later it was realized that 16 bits are not enough to encode everything, and the the standard was modified. The same happend with the Windows world (I assume it is still like that, I haven't used a Windows OS in quite a while). So really, they use 16 bit encoding for legacy reasons. Also people in these programming languages still use UTF-16 often with the assumption that any character corresponds to 16 bit. Finally, for the Web and Linux UTF-8 simply is the standard. No way of denying that. It seems to me that I should be able to read a file from my drive, or request a file from the net without constant UTF-8 -&gt; UTF-16 transformations. Now certainly, if I want to store Chinese books there are better encodings than UTF-8. It just seems to me that the other use cases is more common.
Now I wonder whether you can even take unboxed arguments to typeclass polymorphic functions.
*sorry for the late reply* The setup of the article has been specifically designed to avoid any kind of actual subtyping or bounded polymorphism. Because these are far from simple things to have (subtyping on its own is ok, but polymorphism+subtyping is a much more complex story, and bounded polymorphism shares many of the same issues). That being said, it is certainly possible to bound over a finite list of multiplicities as you do (using type-classes), but I'm not sure it would be particularly useful. Regarding `join`: this is a bit of an artificial definition for `join`. If we think of IOL as a relative monad, then this is not the right way to do a `join`. Section 3 of the [relative monad article](http://www.cs.nott.ac.uk/~psztxa/publ/Relative_Monads.pdf) has the details. It involves, if I'm not mistaken, the functor `data IOL' a = IOL' (x -&gt;p a) (IO p x)` (then you would join over `IOL' (IOL p a)`). Instead we have that if you write, *e.g.*: id :: a -&gt;. a -- my current favourite ASCII linear arrow id x = x id2 :: a -&gt; a id2 = id Then Haskell elaborates `id2` as `\x -&gt; id x` which is well typed. Regarding categories, the matter is muddled by the fact that there are probably other interesting categories involving multiplicities. But this monoidally-accumulating kind of category makes sense mathematically (in my thesis I called it a graded category, but it was mostly because I indended to use them to model graded modules (an algebraic structure that shows up in homological algebra), in my formulation, identity's index is always the 0 of the monoid (that is, 1, in your example, which would spare you from bounded polymorphism)). On the other hand, would there be other interesting instances in Haskell? PS: thanks for the typos elsewhere.
Two reasons: - I need this first version to generate Python. - I could not build the "official" Haskell bindings. If someone wishes to contribute a backend which bypasses python I'll be happy to integrate it.
Thanks for the typos. Zero is useful, and is in the language (it's crucial in the implementation, I haven't implemented a syntax for it yet, though), but it has a bit of a special treatment because it interacts poorly with the case rule (it's unfortunate but deliberate).
&gt; Having worked with Wreq, that's what I feel as well. Any alternatives you would recommend? 
Well I *was* never "considering it at all" =) I have a long history with Go since pre-1.0 days and so for me the major reason is, for me the Go environment has a lot of upsides (just none in the language itself, save for the native concurrency primitives) I wish to "preserve and leverage" in the years to come with while targeting the backend / the not-frontend from a *way* superior language (PureScript as it stands, although the original idea was to transpile-from-Haskell-STG via GHC plugin system --- not keen on that for various reasons after having played with that a bit). Rust is fairly high-level already --- so a "less worthy" candidate for such transpilation. =) But in Go you *really* very quickly miss what the ML family offers as you furiously spew up boilerplate code. On the flipside, Go has really established itself in countless backend and server-side teams around the world and so this lets a lot of real-world projects experiment with a much more productive (imho) and slightly bug-safer development paradigm without giving up either Go's tooling and compilation speeds or their existing home-grown or numerous high-quality ecosystem-grown libraries/packages/APIs/tooling, as I'll have a shim-generator for any Go package to become visible as a "module" to the PureScript (compilation / lang-server --- but not REPL of course) side.. that's still a to-do of course :D
You'll be happy, then, to learn that we've rewritten it completely (and move it to the metatheory section which is more appropriate for this sort of nitpicking). Really, almost everyone found it confusing. I'm the one who wrote this paragraph: I accept full responsibility.
Indeed. Thanks.
I think it's possible in the overwhelming majority of such cases to simply rewrite in a way that highlights the problematic part in the code, not why it bothers the compiler. This makes things more "actionable". For example in the above, what bothers the compiler is "escaped its scope", but for anyone who *didn't ever* implement a type-system as rich as PureScript's, they don't care about what they don't necessarily currently, *right-this-minute*, while iterating-and-about-to-ship, grok. What they *do* care about here is "`h` appears in `Eff t1 etc` and this was unexpected and I, `purs` can't deal with that". So to redesign such error messages would be akin to: type variable h, found at File.purs, line 9, column 14 - line 9, column 25 unexpectedly appeared out-of-scope in Eff t1 (STRef h0 Int) in expression runST (newSTRef 1) in declaration 'test' This looks like very minor changes (a fair number of them if you compare closely) but I think all of them together reduce mental load and "parsing effort" for the (Pure)scripter "in the heat of the moment". Less detail and prose here means more instant identification of the offender to be attacked: It's not about doing ELI5 messages, simply cutting all jargon and verbiage to not further impair the situations &amp; moments in which these messages appear. A compiler can never be argued with, so it doesn't need to explain it's full reasoning and justifications in deep detail. As I churn out more code and run into more compiler messages, I'll submit alternative formulation ideas to the repo over time =)
I'd also be interested to see and compare.
BTW, forgive me if you already know this, but there is a package manager called [stack](https://docs.haskellstack.org/en/stable/README/), that solves many package issues. Also, your text editor should be detecting package requirements from import statements and automatically add them to your cabal file, if necessary. 
oh that's cool, I'll check that out! I'm a haskell beginner, I didn't know about stack yet, thanks!
I installed Emacs on Ubuntu using sudo apt install emacs Then just run Emacs GUI. It will create `.emacs` file and `.emacs.d` folder. Well, probably all your configuration should be split into several files inside `.emacs.d` folder by I'm an Emacs newbie so I keep all my configuration in one single `.emacs` file. Here's my config: https://gist.github.com/ChShersh/134ed627a3db518bf847ec96b92a662a You don't need everything of it. And you probably want something more. Unfortunately, to use Emacs effectively you need to configure not only Haskell integration but also some common stuff. If you have any questions, you can ask here! I will be glad to help if I can :)
&gt; The entire process of working with Logo is making a specific sequence of ordered modifications to that global state, which is so intrinsic that almost no single statement of a program can be understood without knowing the expected state at the time it is reached. That's true. But I think you are missing something about how that is overcome. When a kid starts trying to write Logo functions they have to overcome the problem of extreme state. To do this they end up creating pure functions. They often have their routines always leave the turtle pointed upward or even use the POS command and then recenter the turtle. Then they can chain together their functions effectively in layer after layer. Good LOGO programs end up with pure functions because otherwise correcting for state becomes unmanageable. That's precisely the point. And so the kid ends up writing something very much like a Haskell program with a collection of pure functions and an I/O monad which is stateful and imperative. 
You can use intero: https://haskell-lang.org/intero
Agree with the intent. I think we only disagree on means. I think both agree that there needs to be a rather rich Prelude. I just want it to be the default for the purpose of getting people started. When someone types "ghci" they get lots and lots of goodies by default. 
If it is meaningfully usable in other OSes then yes I guess it qualifies. Point taken. 
I recently switched to spacemacs - it already has a haskell-layer build in so the only thing I did was get this: (haskell :variables haskell-completion-backend 'intero) into my .spacemacs file and everything works like a charm
No at the moment. This topic is in my "need to figure out how" pipeline. :/ 
&gt; there is no form-processing library in Haskell that is a pleasure to use What's not pleasurable about `digestive-functor`? I find it easy to use and so far fulfill my needs.
I use this too. &gt; and everything works like a charm Yups. 
I'm not convinced top-down vs bottom-up has anything to do with static vs dynamic languages. I don't know about lisp since I've never done significant work with it. In python I use the equivalent of the top-down method using pass instead of undefined. The type system does help you implement the smaller functions correctly but the difference isn't huge, for me it mostly comes down to looking at the signature for the function I'm implementing vs looking at the call site for the function I'm implementing.
Why do you say that `ElidedMaybe` should always be able to be completely optimized away? Are you thinking of representing `ElidedNothing` as a null pointer? If so, how would you distinguish the various levels of nothingness in `ElidedNothing (ElidedNothing (ElidedNothing Int))`?
FWIW, that won't work as written. You can't have a `newtype` of an unlifted type.
One problem is that functions exported without inlining unfoldings can't be called with this kind of data. In the same vein, recursive functions are often not going to be able to use these types, or called at these types. So either this needs to be more of a suggestion, like the various inlining / specializing pragmas are, or it needs a rigorous specification which would no doubt be incompatible with the kind `*`. Frankly, I see no difference between your proposal and just manually adding `INLINE` pragmas everywhere the type gets used, except that this may work *slightly* better with things only marked `INLINABLE`.
I can recommend http://haskell.github.io/haskell-mode/ with or without https://github.com/jyp/dante
I agree that for Asian text that I can statically guarantee contains no emoji and no markup, a 16-bit encoding *might* be useful. I've never had to deal with such text. For a generic text library, UTF-8 should be used, since it's very much not limited to Asian text without emoji and markup.
&gt; that solves many package issues Can you be more specific please about which concrete issues you're referring to, and why you seem to think Stack to be a better choice than Cabal here? 
I got lost at `deduplicating`. Particularly how is the `(a -&gt; Bool)` updated whenever new data arrives.
Note that erasure eliminates arguments which are only used to make the code type-check, but aren't needed to determine which branch to execute. For example, in `map @Int f [1,2,3]` the `@Int` doesn't affect whether `map` will run the nil case or the cons case, so that type-annotation is optimized away and isn't passed as an argument at runtime. By contrast, in the `ElideMaybe` example given by the OP in another comment, the `ElideMaybe` constructor *does* determine which branch to execute. But the hope is that the compiler will be able to optimize the composition in such a way that the `ElideMaybe Char` argument to `b` can be optimized away, by inlining `b`'s branches into `a`.
I would be interested in knowing if you tried, and got stuck at any point. Once you find the solution, please consider raising a PR against the installation docs.
Complementing, Dante is more or less an Intero equivalent that supports Nix and cabal, not only stack. There are some different functionalities.
I would say it's installing packages and the fact that new-build and others are not documented in most historical sources.
Sure, but not with the `Monad m` typeclass, since the kind of `m` is `* -&gt; *`, not `# -&gt; #`. [The kind of primitives](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#unboxed-type-kinds) is a bit more complicated than `#`, but using TypeInType, it's possible to write a type class parameterized on primitive types: {-# LANGUAGE BangPatterns, FlexibleInstances, MagicHash, MultiParamTypeClasses, TypeInType, UnboxedTuples #-} import GHC.Exts class PolyKindedShow (rep :: RuntimeRep) (a :: TYPE rep) where polyShow :: a -&gt; String instance PolyKindedShow 'IntRep Int# where polyShow x = show (I# x) instance (Show a, Show b) =&gt; PolyKindedShow 'UnboxedTupleRep (# a, b #) where polyShow (# x, y #) = show (x, y) instance Show a =&gt; PolyKindedShow 'PtrRepLifted a where polyShow = show -- | -- &gt;&gt;&gt; main -- 42 -- (4,2) -- 42 main :: IO () main = do putStrLn $ polyShow (42 :: Int) putStrLn $ polyShow (# 4::Int, 2::Int #) let !(I# x) = 42 in putStrLn $ polyShow x --let !(I# x) = 42 in putStrLn $ polyShow (# x, x #) I couldn't figure out how to make my unboxed tuple instance delegate to smaller PolyKindedShow instances, so the commented-out example isn't supported :(
It's not actually similar to erasure. Erasure is related to relevance of a type variable.
BTW - Does anyone know of a 'spacemacs for dummies' tutorial out there somewhere? I have only basic proficiency in vim, and my emacs knowledge is next to 0. I love spacemacs because it's painless to set up, and offers a generally straightforward editing experience, but I do have a tendency to lose a ton of time trying to suss out basic stuff like 'how do I use this undo feature', or 'Oh god, I have somehow exited the vim layer and I have no idea how to get back' The official documentation is exhaustive, but it's structure seems to rely on the reader knowing some amount about how to configure emacs, which is a can of worms I'm just not quite ready to open.
`\arg -&gt;` is how you define a local, anonymous function, called a 'lambda'. Anything between the `\` and `-&gt;` symbols will be a space separated list of arguments. So what you've done is declared a function that takes a tuple, destructures it to bind the first and second elements to the tuple to and and b, respectively, and passes them to some function f, as passed in the arguments to zipWith. You then pass that anonymous function to map, along with your freshly zipped list. You could have instead written: zipWith' f x y = map processTups tupList where tupList = zip x y processTups (a,b) = f a b Where `processTups` is a stand-in for your anonymous function. 
For one, it doesn't require you to use sandboxes, which the OP tripped over.
This. Please elaborate on what you are doing, and why wouldn't an online survey give you whatever you need?
And neither does cabal since 1.24...
`stack` and `hpack`. Just use them. Side note: I don't know why the cabal file format is designed the way it is. The idea of listing out every single module in your app is just weird. Is there any place that documents why this decision was a good idea when it was made?
Okay, so this is sort of the exact opposite of the article. You're saying first throw kids into an environment as opposite Haskell as possible, and then let them rediscover some notion of pure functions in that environment. I definitely don't agree. While kids will learn some techniques for managing mutable state, they certainly will not learn the simple mathematical viewpoint where time and mutable state aren't part of the model at all. As a result, they won't be prepared or accustomed to being able to reason about expressions in the same ways that Haskell programmers do, or in the way that mathematicians do. There is no equational reasoning. Substitution is unsound. Basic mathematics remains broken. Haskell's IO monad offers one way to shoehorn mutable state in a way that doesn't break mathematics. That's great, but it's also a little bit complicated. Why not avoid it entirely, and focus on building models in an environment without mutable state? This is the approach of the Gloss library, and of CodeWorld, and of Racket's "Big Bang" model, and of (a subset of) the Elm architecture. It works for a whole slew of interesting projects, and it doesn't break mathematics.
UTF-8 should only be used for natural language text if you can statically guarantee that all or almost all natural language text is in western languages, or that you treat all natural language text as opaque globs without processing them. Markup is irrelevant - that is not natural language text. So for a generic text library, a 16-bit encoding should be used.
If you're talking about `new-build`, two things: One, the change log lists that as a ["tech preview"](https://github.com/haskell/cabal/blob/52a7d51e239dce1791b7d1f33590239a6fba0cd5/cabal-install/changelog#L181); and two, it's still possible to use the "old" build in the same way that it's possible to not use a sandbox. Oh, and three: `new-install` doesn't even exist, which is relevant because the OP used `cabal install` in the first place. 
&gt; The idea of listing out every single module in your app is just weird. To me it'd be weird and I'd argue also fragile and error-prone not to be explicit about what you intend to be part of your distribution and just pick whatever happens to be laying around in your filesystem. Just recently we made Cabal &amp; GHC more pedantic about this (via the new `-Wmissing-home-modules` warning) which will point out when you have inaccurate `{other,exposed}-modules` as a step towards improving the cabal/ghc interaction, and besides helping avoid broken `cabal sdist`s is also quite essential for `cabal new-build` to be able to properly track file changes, as otherwise cabal may not be able to detect that it needs to invoke `ghc` for a given component if you didn't declare which component(s) a file belongs to, resulting in rather confusing/annoying behavior. There's of course different ways you could accomplish this, but this is the scheme that is more in line with cabal's design mantra of being explicit in terms of "no untracked dependencies".
Tech preview mostly means that things are subject to change, and some things may be missing/incomplete. But we highly encourage everyone to try new-build for getting feedback; if you feel that calling it a "tech preview" discourages this, then maybe we should use a different way to label it. I don't understand your "two" argument, but as for the third one: `cabal install` is a concept that's become less relevant in the `new-build` paradigm; In old-build, `cabal install` made more sense, because you more or less had to manually manage your package-db; with `new-build` however the package-db has changed radically; it's a totally different concept now, more like a cache that's handled automatically. You just specificy what you need in your `.cabal` &amp; cabal.project files, and `cabal new-build` tries to satisfy your specification. That's a much simpler model, that happens to "solve many package issues". The one case where you'd still need to `install` something is for executables; and as it happens, `cabal new-install` support for non-local executables has been merged to cabal-2.1 recently, so that's also covered soon. In any case, I think OP's use-case would be perfectly covered by `cabal new-build`, but not necessarily if you expect to use `new-build` to work exactly like the "old" build. If your argument is merely that `cabal new-build` isn't yet polished, yeah, I'll grant you that; but despite that if I've been able to get newcomers to successfully use `cabal new-build` without much trouble.
Does this mean I will get warned *very* severely when I rely on a source file that is in my source hierarchy but that I've forgotten to list in my `.cabal` file? Because this issue has bitten me enough times that it's painful.
Yes, and if you want it *really* severe, add if impl(ghc &gt;= 8.2) ghc-options: -Werror=missing-home-modules to your `.cabal` file (or just add it globally to your `cabal.project` file)
This is great! Thanks to those who added it.
Doesn't this consequently also mean that UTF-16 should only be used if "you can statically guarantee that all or almost all natural language text is" **not** "in western languages"?
 That solution is fine. I wouldn't have used a where clause and just inlined the `zip x y` value, but it's peanuts. As /u/IronGremlin said, `\&lt;pattern&gt; -&gt; &lt;expression&gt;` is just a function with arguments from `pattern` and a result of `expression`. You could have similarly declared a named function: tupleFunctionApplication f (a,b) = f a b and called it thus: map (tupleFunctionApplication f) tupList This `tupleFunctionApplication` is actually known as uncurrying a function and is provided as [`uncurry`](https://www.haskell.org/hoogle/?hoogle=uncurry). It can be used as such: map (uncurry f) (zip x y)
I like it! Bikeshedding, I think elision failure should be a warning, and it should be possible to suppress it for a single expression (or at least a single declaration).
Agreed. The point I wanted to make is that it's great to have people experience success with Haskell in any way possible. That includes helping them to write naive, verbose code. People who succeed are more likely to stick with it and dig deeper. :)
I guess remote is no option?
No, UTF-8 is an good encoding for all of Unicode, and entirely appropriate when the distribution of Unicode codepoints is unknown or unknowable.
its my homework question and i am asked to build some functions that are based on datatype Fruit which is self-defined. here are the definitions: data Fruit = Apple String Bool | Orange String Int -- Some example Fruit apple, apple', orange :: Fruit apple = Apple "Granny Smith" False -- a Granny Smith apple with no worm apple' = Apple "Braeburn" True -- a Braeburn apple with a worm orange = Orange "Sanguinello" 10 -- a Sanguinello with 10 segments fruits :: [Fruit] fruits = [Orange "Seville" 12, Apple "Granny Smith" False, Apple "Braeburn" True, Orange "Sanguinello" 10] -- This allows us to print out Fruit in the same way we print out a list, an Int or a Bool. instance Show Fruit where show (Apple variety hasWorm) = "Apple(" ++ variety ++ "," ++ show hasWorm ++ ")" show (Orange variety segments) = "Orange(" ++ variety ++ "," ++ show segments ++ ")" in the first question i am asked to make a test function to see if its blood orange the input is referring to, blood oranges are those oranges that are either part of the Tarocco, Moro or Sanguinello variety; this is the function i came up with but its apparently bugged -- 1. isBloodOrange :: Fruit -&gt; Bool isBloodOrange fruit variety _| fruit==Orange &amp;&amp; variety == "Tarocco"||variety=="Moro"||variety=="Sanguinello" = True |otherwise = False 2. -- 2. bloodOrangeSegments :: [Fruit] -&gt; Int bloodOrangeSegments fruits = [sum z | (x y z) &lt;- fruits, isBloodOrange (x y z)== True] in the second question i am asked to find the sum of all blood oranges given a list of Fruit data, I implemented the function i built in 1 and did a list comprehension, but i am not sure if the pattern matching works 
&gt; Clearly someone wanting to learn Haskell isn't going to be completely new to programming Not necessarily. Haskell might seem weird/advanced/academic to someone experienced with traditional imperative languages but I actually think it's a great beginner language, especially within teams when a more experienced Haskeller can do some initial set up. I'm working with a couple people on a Purescript app, and Purescript is their first real PL, and it actually works out pretty well. We dip into JavaScript at times and that is *way* more intimidating for them!
How would you go about making reproducible, distributable packages without keeping an explicit build manifest? Is the build system just always supposed to recursively crawl the project directory and assume that whatever exists in there is a haskell source file and try to throw it at GHC? That sounds like an atrocious design. Requiring this for one cabal project means it's required for all cabal projects, so, for an arbitrarily layered cabal project of any complexity, you can have a complete map by just crawling the `.cabal` files. Also, just in general, I've worked on projects that don't have package management systems that do this, and trying to figure out what exports what after your project hits a certain level of complexity is an absolute nightmare. I would pay an absolutely stupid amount of money for someone to travel back in time and force the developers where I work to have kept track of this explicitly.
I normally would, but ever since GHC 8.2, it's been broken for me. Unfortunately, judging by the issue tracker, it seems I'm far from alone in this.
I would hesitate to throw beginners at the "tech preview" of anything. Maybe others wouldn't. To restate my second point: Let's say that `new-build` works perfectly and is the recommended way of doing things. It's still possible to (presumably) accidentally use `build` instead. This is similar to the situation today, where sandboxes are generally the recommended way of doing things, yet it's possible to (perhaps) accidentally not use them. For my third point, I was merely pointing out that even if the OP knew to look for a `new-` version of the command they were using, they would not have found one because `new-install` doesn't (currently) exist. My main argument is that it's easy for a beginner to end up in the same situation as the OP due to how `cabal` works. [The Cabal user guide](https://www.haskell.org/cabal/users-guide/installing-packages.html#installing-packages-from-hackage) tells you to use `cabal install` to install packages from Hackage. Now of course the next section talks about sandboxes, but in my opinion there should be some big, flashing warning signs around the `cabal install` instructions. Even [the HEAD guide](https://cabal.readthedocs.io/en/latest/installing-packages.html#installing-packages-from-hackage) still recommends `cabal install`. Heck, [the quick start page](https://cabal.readthedocs.io/en/latest/developing-packages.html) doesn't even *mention* sandboxes! (And it only mentions `new-build` towards the bottom in a section about `custom-setup`.)
`hpack` still gives you an explicit build manifest. If you want to explicitly list your `exposed-modules` and your `other-modules`, you can. What `hpack` gives you (that `cabal` does not) is the ability to automate the typical case. For literally every Haskell package I have on Hackage, crawling (a subdirectory of) the project directory and throwing Haskell source files is exactly what I want. 
Was the dig at Stack necessary? Stack already "supports" GHC 8.2, whatever that means. And if I understand things correctly it also supports Cabal 2.0. The release you're referencing, which is out for testing *right now*, switches to using Cabal 2.0 behind the scenes. 
Accepting this kind of data would mean that the inlining unfoldings *must* be exported. That is, in the strong interpretation. There isn't a substantial difference between this and `INLINE` calls everywhere it's used - except that some times a datatype may be too weird to be to weird for GHC to be able to perform all the inlining, and then marking the data type with `ELIDE` would have GHC warn you of this fact.
Here's another vote for dante. Intero is great but I found that I didn't need everything it offered and this set-up seems quicker for me.
I wasn't really trying to say that `hpack` is a bad solution, so much as that there is value in the cabal file-format being what it is. Certainly having your cake and eating it too covers all bases - There is no especial value that I can see to a human manually entering the modules, as opposed to a machine doing it automatically.
&gt; Accepting this kind of data would mean that the inlining unfoldings must be exported. Not if the function is polymorphic, and thus doesn’t know that it will accept or return this type. Unless you’re suggesting all polymorphic functions must expose unfoldings. And FWIW, I think there are quite a lot of scenarios where GHC will be unable to elide the data type; basically any case where inlining wouldn’t do it for a normal type. Sharing, storing, and recursion will all often break it. I think you get basically all the same benefits, *plus* working better in these scenarios, by just being able to define custom unboxed data.
Oh, gotcha! I agree. I think the Cabal file format is fine for machines, and I think it's good that we have it. But most of the time, I don't want to manually edit Cabal files. 
This idea keeps coming up, especially around Generic programming. @kosmikus is the expert here. Someone should just try it, I guess :-)
I wonder if this could completely replace and deprecate `INLINE`, if combined with newtypes. For example, you could have `map :: (a-&gt;b)-&gt;ElideList a&gt;ElideList b`, and then it would always fuse.
&gt; You just specificy what you need in your `.cabal` &amp; cabal.project files, and cabal new-build tries to satisfy your specification. This is great stuff.
&gt; I don't understand your "two" argument The "two" argument was that someone coming to Cabal from no prior knowledge has no idea about any of this and is just gonna go "I guess I'll enter `cabal build` or `cabal install`." I can't wait for `new-build` to resolve all the beginner troubles with Cabal, but it's not there yet...
-_- Did not seem like a "dig" to me. Why must every conversation about the differences between Stack and Cabal deteriorate to accusations?
Frankly, I consider adding another project-level dependency to be a much greater cost than remembering to add a few lines to a cabal file.
+1 for dante. Intero's tie to Stack is completely unnecessary and restrictive.
In my opinion, it's a comment that's meant to sound pleasant ("Stack users may start benefiting ...") while at the same time implying that Stack is behind the times. It felt like a dig to me because it's completely unnecessary to mention `cabal` *or* Stack here. The feature he's talking about is a feature of GHC, and only requires the `ghc-options` field that everything already supports. 
I like the python generation. It means it can be used alongside anything
I'm not sure I'd agree with your characterization. Remember we are talking age 5-9. So first you get them to write 3 line programs that do anything. Fairly early on those they write functions and because of the way logo works those methods end up being pure in some sense to work. That creates a background that leads to Haskell. &gt; While kids will learn some techniques for managing mutable state, they certainly will not learn the simple mathematical viewpoint where time and mutable state aren't part of the model at all. As a result, they won't be prepared or accustomed to being able to reason about expressions in the same ways that Haskell programmers do, or in the way that mathematicians do. Have you ever met a 6 year old? They are learning to add and subtract, and often don't understand that in full generality. Multiplication if they have even heard about it is tricky and often specific and material. They don't reason about expressions at all. &gt; Why not avoid it entirely, and focus on building models in an environment without mutable state? This is the approach of the Gloss library, and of CodeWorld, and of Racket's "Big Bang" model, and of (a subset of) the Elm architecture. It works for a whole slew of interesting projects, and it doesn't break mathematics. Because those systems are too hard for the target group. You are talking LOGO, Scratch, Alice, Legos.... I'm not recommending LOGO for middle school. I've never taught someone in middle school their first language so I have no experience in that. What I am going to say is that LOGO successfully creates programmers who know a mini-LISP by 2nd or 3rd grade. 
Objectively, Stack *is* "behind the times," in a technical sense. I'm not sure why he claimed that the above snippet requires Cabal-the-library 2.0, but assuming he is correct, a version of Cabal has been released which handles this, and the same is not true of Stack. However, it was conceded that there is no fundamental limitation, and Stack will be offering this soon. I can't imagine how to more fairly asses the situation. And frankly, this is a ridiculous double standard. People take much harder "digs" at Cabal much more unfairly *all the time*.
When I was reading this post and the post it responded to, I found myself thinking a lot about my (daily) experience working with Python. I tend to write lots of web APIs and some data engineering/data science stuff (which usually just means moving BLOBs around, getting stuff from databases, ingesting and returning lots of JSON, and loading things into Pandas dataframes). With a few caveats, I'll give this a shot in hopes of contributing to the discussion, which I have followed with interest. Of course, I don't think I'll be able to write truly representative code on the spot and I don't think I'll be able to speak for all Python programmers. I also expect that people in this subreddit (who typically seem to loathe Python) will probably hate this example. Finally, it strikes me as very similar to the examples in the original Clojure post where we're just dealing with arbitrary hash maps and running lots of code checks to catch places where our data structures may be violating our expectations. I also can't make the argument that there's a difference in *pleasantness*. I find the Python way profoundly easy to get started with. I even find it easy to debug and "reason about" (highly subjective) provided that there is an extremely well-determined set of inputs and outputs (almost like a type system). Of course, I have to have copious logs and tests to cover my ass because it will fail at some point when the data coming in is different than I expected it to be when I last edited the code. Anyway, there's a lots of stuff like this: # Inside some API endpoint... project = json.loads(data.decode()) # returns a `dict` (hash map) we hope. Can fail if data not bytestring or json is not loadable proj_headers = project.get("headers") # returns a sort of expected thing (with no guarantees) or `None`. Can fail if proj_headers is not a `dict` or there's no `get` attribute. if proj_headers is None: return something_like_404_to_caller # this can fail if function accepting `head` gets something that violates its expectations about what `head` is. other_info = {head["name"]: get_header_stage proj_headers(head) for head in proj_headers} # sometimes we write stuff like this: deep_thing = project.get("some_key", {}).get("some_deeper_key", {}).get("some_expected_thing", []) return {**project["metadata"], **other_info} The thing that strikes me about Haskell that I do wish to know what functions are *beholden to return* and that having guarantees about types of things functions return would eliminate all the places for error I marked above. I would most like to write correct code and I want to eliminate runtime errors. However, all of these places of error I marked above are typically going to appear as a result of the incoming data changing shape unexpectedly. In practice (anecdotal, and admittedly with greater than 95% test coverage in various small codebases with fewer than 20k LOC), this doesn't *seem to happen very often*. I expect this is probably relevant mostly to contemporary web applications where you build the frontend yourself or work with a team to build the frontend and once you agree on the data structures going back and forth, there's little incentive to change those data structures. In other words, nobody wants the data to change and there's a lot of code written around the expectation that it shouldn't change. Indeed, it seems like the data changing shape unexpectedly would also throw problems for a similar Haskell application? Maybe it would be surfaced somewhere more obvious? If you later would like to change your data structure then refactoring would be great in Haskell. In Python, I often have to litter my code with `if something is None...`, which is really a `Maybe` by another name. Sometimes, however, dealing with lots of Maybes in Haskell feels very similar to the work I'd have in Python: there's no gain there. It seems like I've lost some ergonomics and haven't gained fundamentally on the problem that my data from any outside source can change in arbitrary ways in the future. I love Haskell but I also find Python (on very small teams with tons of discipline and testing) to be perfectly adequate in my day job.
&gt; -_- Just thought I'd randomly jump in here and say this emoticon that I probably haven't used since high school _perfectly_ captures my reaction to all of these Stack vs. Cabal threads that randomly show up here and in my Twitter feed. Everyone who participates is so testy. It's so annoying. Oh god. -_-
I really like intero but is there a way to make intero not freeze emacs while it's building dependencies for a project? I observe this behaviour on my laptop at least. 
On a different note: could someone please explain to me on a high level why if a file does not type-check then basically all the nice things about intero, i.e. autosuggestions, type signature suggestions etc. stop working? 
&gt; Was the dig at Stack necessary? Yes.
"Your Stack ..." is the new "Your mom ..." -_-
http://downloads.haskell.org/~ghc/master/users-guide/ghci.html#the-ghci-and-haskeline-files Make a file called 'ghci.conf' in any of those locations, and type the `:set prompt` commands into that file just as you'd type them into the ghci REPL.
http://haroldcarr.com/posts/2017-10-24-emacs-haskell-dev-env.html This setup matches what I use more or less. The gihub repo doesn't work out of the box however. Generally I find the emacs haskell integration is flakey compared to some other language integrations (see OCaml / merlin)
+1 for dante. The intero / stack is disappointing, I'd like to choose my build tool separately from my editor tooling. 
It's just simple recursion. `deduplicating` itself calls `recur` with the initial `const True`. However, every time `recur` sees a new value from whatever source it is connected to, it ends up recursing with an updated `isDifferent`. You can see this on the final line of its definition: `recur (/= x)`.
I could not understand the following use-case. Can you elaborate how the Cabal file interacts with import/export lists in what you were doing? &gt; Also, just in general, I've worked on projects that don't have package management systems that do this, and trying to figure out what exports what after your project hits a certain level of complexity is an absolute nightmare.
Explicitly listing exposed modules has some value. What is the value of explicitly listing every single internal module? When you're in GHCi most of the stuff works as is, only to fail during `stack build`? If this was **designed** up front, why this difference in behaviour? 
&gt; How would you go about making reproducible, distributable packages without keeping an explicit build manifest? I do not understand what reproducable builds have to do with listing down every single internal module in a package. If anything, you can add it to your Cabal file, but forget to commit to your repo. Or remove ila file from your repo, but forget to remove it from your Cabal file, leading to trivial build failures. What exactly is this design preventing or promoting?
I think the right approach to that is to make defining a record rebindable. So you could choose if data FooBar = FooBar { foo :: Int, bar :: String } desugared into records, var laarhoven lenses, profunctor lenses, or something else entirely. It would be nice if it could even desugar into instances somehow. That way possible approaches like overloaded labels could also be considered.
`cabal build` and `cabal install` do the wrong thing, by default. They damage your GHC installation and can potentially render it useless. Online Haskell tutorials -- all historical, some recent ones -- tell newbs to use these commands, and those who don't give up immediately spend hours learning an unnecessary, well-known, but ill-documented lesson. This has been the case since the very beginning and has -- judging from /u/hvr_'s responses above -- still not been solved yet. The simplest instructions on using `cabal` will *still* have a deleterious effect on your Haskell installation. I was hoping that with the recent updates in the `cabal` library, the latest executable would come with these issues already solved out of the box, and `cabal build` would at least activate sandboxes by default, if not work as well as `stack build`. But nope, apparently new users still have to know about sandboxes, or the `new-*` which are barely documented anywhere. In summary the answer appears to still be "Just use `stack`."
I gotta admit I had a small heart attack when I read the title. I've been working the past month on adding dependent types to the official TensorFlow bindings for Haskell, and I thought that you were doing precisely the same, what a waste of effort, I said to myself. But now that I've read better what is your project about, I notice they have very different focuses. My project is concerned with endowing the low level bindings of TensorFlow Haskell with depedent types (like, make the `add` function only work with tensors of the right size/shape), while TypedFlow is concerned with creating secure/safe TensorFlow (python) code with high level bindings (like, making sure that the shapes of two *layers* on a neural network have the right shapes). I like your approach, I think it may be possible to use the dependent typed version of the TensorFlow bindings as backend for TypedFlow, and therefore write code directly on Haskell and no python. Also, I would recommend to try to market your code for machine learning practitioners who only write in python, fast prototyping is very useful for them, and this kind of tool could help them a lot on that front. We are after all, not many who wish to do machine learning in Haskell (or it doesn't seem like it). Anyway, some shameless self-promotion: https://github.com/helq/tensorflow-haskell-deptyped/tree/mnist-deptyped https://github.com/tensorflow/haskell/issues/156 Any comments are welcome, but take into account that this stuff is highly experimental and in alpha state, if it get's merged into the official TensorFlow Haskell it will be in no less than a month (or two).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [helq/tensorflow-haskell-deptyped/.../**c3340a37d699e8b14d552c610d7c0ebe856a5c08** (mnist-deptyped → c3340a3)](https://github.com/helq/tensorflow-haskell-deptyped/tree/c3340a37d699e8b14d552c610d7c0ebe856a5c08) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I always am impressed by people who make contributing to GHC sound so easy. I tried to do it at one point and had an incredibly difficult time understanding the code base. I ended up not being able to complete the ticket sadly, but I intend to try again another time!
I cannot claim to be proficient but I'm usually able to help me with google ;) Are you talking about learning the keybindings (I guess it's more about VIM - the inbuild Tutorial - spc h T - helped me a lot) or about setting it up? For the setup anything specific? If you look at `~/.spacemacs` (if you did not change anything) you should find plenty of comments to give you a general direction - setting up your own layer-configurations is a bit more involved and I only did it once to set up `purpose`. Basically I copy&amp;pasted another simple layer and played. Aside from this here is a similar thread https://www.reddit.com/r/spacemacs/comments/4p4h8m/how_to_start_to_learning_basics_for_spacemacs_and/ that seems not too bad
A good share, but this isn't really related at all to Haskell. 
Literally this whole conversation has been predicated on `cabal new-build` which solves the main issues just fine with its default behavior. The problem is just that `new-build` is not yet the default.
Why are you so [obsessed with harassing /u/hvr_](https://www.reddit.com/r/haskell/comments/6bqfk9/testing_ghc_release_candidates_with_stack/dhp0h00/)? 
Unfortunately the pythonistas that I talked to are in their majority rather refractory to static types (even though TF is typed ...). In fact someone cross-posted it to /r/MachineLearning and got barely noticed.
Monoids and (semi- near- rings) do seem relevant to Haskell. Also Cayley's theorem is sort of relevant to difference lists and Codensity.
This is intentional behavior: https://github.com/commercialhaskell/intero/issues/281
I might be mistaken but I think it's because it uses ghci behind the scenes and when ghci fails to load a module you don't get any useful information on it other than the errors. And yes, this is the biggest pain point for me too.
[removed]
I really think all functional programmers would benefit from a course in Abstract Algebra. It’s also such a tremendously beautiful subject, my favourite math course I took at university. 
Interest!
Pardon my suspicion, but do you have permission to post these documents publicly, particularly the lecture audio? If so, it would be a good idea to make a note in the README and give proper attribution.
Can we please make `cabal build` and `cabal install` emit warnings then?
Yes exactly. Having to provide the same information in two places is very fragile.
Could you describe what it does well and how?
You can do the Haskell way (hopefully that works on Windows as well): Prelude&gt; import System.Directory as Dir Prelude Dir&gt; import System.FilePath as Path Prelude Dir Path&gt; home &lt;- getHomeDirectory Prelude Dir Path&gt; writeFile (home &lt;/&gt; ".ghci") "set prompt \"ghci&gt; \""
Any of these changes would have saved me a ton of time, you're right about that!
@cdsmith, thanks again for your feedback, from someone with actual experience it's very valuable. First, you are right, I have a very vague idea on how to teach it to actual kids, my ideas are much more concrete with how to make it more accessible to, as somebody said above "teens with some math background", or adults. In terms of kids, I've been wondering in a thought experiment sort of way - **what if we take a group of kids, DON'T teach any imperative programming at all, teach typed FP from the ground up** - how would they turn out? 99% of disagreements here come from people with already strong imperative background, it's an integral part of their thinking, it's very hard if not impossible for us to abstract away from it completely. However, I'm pretty sure if (when) there are people who are taught typed FP *only* - they'd think quite differently, in different concepts and would in turn wonder "WTF are these ugly imperative things you are talking about and how in the world did you come up with them?" :) However, this point of yours - &gt;it's still important to remember that in many ways, we live in an imperative world still - is very valid, but I'd still like to talk to these FP-only trained people once they appear eventually. &gt;I agree that starting with the structure of expressions is better than starting with computation. The first hurdle in learning declarative language is composition. Understanding the relationship between expressions and values, and between expressions and their sub-expressions Exactly! That was one of the primary motivations behind type-level function library in coffee I mention in the beginning of the original post. If you can play with constructing different types, dynamically, see the result right away and play with it - it *is* learning by playing that people advocate for here, only we teach Types (and data structures) first! And then it is all about composition while minding the right types, that's it. One example, which I've seen it with people learning haskell time and again - nobody really properly explains that `data Maybe a = Just a | Nothing` in fact should be read as `TypeFunction Maybe (a:Type) = Just (x:a) + Nothing ()`! Once this understanding clicks, a lot of beginner confusion melts away. And this is the simplest of examples. &gt;So, a monad isn't a burrito, but it is a cute bunny? The widely understood problem with monad tutorials is that analogies like these seem obvious to those who already comprehend an abstract concept; but they are not helpful to beginners. To beginners, what matters are concrete examples. Well, of course I exaggerated when talking about animals and monads and I fully agree with the importance of examples. However, I also believe people would benefit if somebody explicitly told them "Monad is a type function from one type variable with a couple of laws, which can elegantly describe quite different entities, for example: ... " from the start, vs saying "monad is like a collection, monad is like a computation etc" - it just creates a mess and confusion. It's like saying "a circle is like stars in the sky, a circle is like apples, a circle is like a vinyl disk" - people start thinking about a lot of extra properties of the objects that are familiar with and it creates too much noise to understand the *core* of what a circle is. It's not specific to Haskell per se, it's about how we teach abstractions in general, and very relevant to math as well. If we taught math differently at school, I'm pretty sure many more kids would love it and when they grow up they wouldn't write "who needs category theory when programming?!" :) 
wow ok…thanks for that link 
Is this similar to [dnngraph](https://github.com/ajtulloch/dnngraph/)?
Thanks, that's super helpful!
It is a _lot_ easier and much safer to move things around in Core. You have types. You don't have to talk about explicit environments for every closure... The fact that we can lint these optimizations is a large part of where our comfort in their correctness comes from. it is rather difficult to make things that pass the core linter that aren't obviously incorrect optimizations. In STG on the other hand you don't have the types, and you have a ton of annotations that get in the way of everything. It is much much easier to mess up here. On the other hand, because you don't have the types, some things like trying to make it so, say, fmap for (Either a b) could preserve sharing on Left constructors becomes an option.
it would be nice to have a course that explores these kind of concepts using a suitably expressive programming language, possibly haskell. abstract algebra is pretty you know abstract and it would be cool to explore it a little bit more hands on. 
IMHO, Coq would probably be a better setting for this, since it would be explicit exactly how much you're putting in (in invariants) and how much you're getting out (in guarantees).
Coq has been my latest weekends project, and has changed my thinking about programming, perhaps more than the `python -&gt; haskell` jump. For anyone interested, coq is easy to [install](https://github.com/coq/coq), and the free online text book, [Software Foundations](https://softwarefoundations.cis.upenn.edu/lf-current/index.html) is a phenomenal resource to follow along with in using the also phenomenal [Proof General](https://proofgeneral.github.io/) mode in emacs (or for [other](https://coq.inria.fr/related-tools) dev environments).
Please open a PR to clarify this and make it easier for the next one. I'd be happy to review it!
This sounds like a good idea. I don't know off hand if there is an open issue in the tracker about this. If not, please open one. Or just ping me in a week? I'm currently trying to get hadrian do my bidding and am fixing a few things on Cabal on the side.
IMHO - people who want to start with Coq should be looking at CoqIDE rather than Proof General. Unless you're already super tight with Emacs, it's a lot more work to get started with.
There's a difference between ill-typed (for a given type system) and ill-formed (or unmeaningful) data. If your domain is ill-formed and it is not possible to assign consistent meaning to your data in any systematic way, you will always be out of luck addressing it with code. For an inexpressive type system, many more meaningful datasets are ill-typed than would be the case with a more expressive system. "Maybe" is a good example in both cases: we can't model it reliably at all in Python (not expressive enough), but it is abused in Haskell to cover a multitude of null-like conditions (could be more expressive: e.g use Either).
https://github.com/haskell/cabal/issues/4862
But a standard course on algebra won't mention monoids more than as a footnote. And it won't mention semirings at all.
I think there is a specific way of thinking which leads naturally to category theory. It's hard to pin down what it is, but I gathered some material at /r/radicalconstructivism. Using the words of [Joseph Goguen](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.362&amp;rep=rep1&amp;type=pdf): &gt; [..] in order to understand a structure, it is necessary to understand the morphisms that preserve it. Indeed, category theorists have argued that morphisms are more important than objects, because they reveal what structure really is. Moreover, the category concept can be defined using only morphisms. Perhaps the bias of modern Western languages and cultures towards object rather than relationships accounts for this. 
Here is an alternative, perhaps orthogonal, perspective: https://golem.ph.utexas.edu/category/2010/03/a_perspective_on_higher_catego.html
Alright, so &gt;its my homework question and i am asked to build some functions that are based on datatype Fruit which is self-defined. here are the definitions: data Fruit = Apple String Bool | Orange String Int -- Some example Fruit apple, apple', orange :: Fruit apple = Apple "Granny Smith" False -- a Granny Smith apple with no worm apple' = Apple "Braeburn" True -- a Braeburn apple with a worm orange = Orange "Sanguinello" 10 -- a Sanguinello with 10 segments fruits :: [Fruit] fruits = [Orange "Seville" 12, Apple "Granny Smith" False, Apple "Braeburn" True, Orange "Sanguinello" 10] -- This allows us to print out Fruit in the same way we print out a list, an Int or a Bool. instance Show Fruit where show (Apple variety hasWorm) = "Apple(" ++ variety ++ "," ++ show hasWorm ++ ")" show (Orange variety segments) = "Orange(" ++ variety ++ "," ++ show segments ++ ")" The definitions look fine, although I have to wonder why you're not simply deriving show, though. Are you allowed to derive instances of type classes? &gt;in the first question i am asked to make a test function to see if its blood orange the input is referring to, blood oranges are those oranges &gt; that are either part of the Tarocco, Moro or Sanguinello variety; this is the function i came up with but its apparently bugged isBloodOrange :: Fruit -&gt; Bool isBloodOrange fruit variety _ | fruit==Orange &amp;&amp; variety == "Tarocco" || variety=="Moro" || variety=="Sanguinello" = True | otherwise = False bloodOrangeSegments :: [Fruit] -&gt; Int bloodOrangeSegments fruits = [sum z | (x y z) &lt;- fruits, isBloodOrange (x y z) == True] The reason this doesn't work is because you're not pattern matching properly. The way you've defined isBloodOrange, you're not pattern matching on a Fruit's type, variety and hasWorm/segments; instead, you've written it as though it accepts three arguments, which contradicts how its type is defined. In order to pattern match a single value with multiple fields, you need use parentheses so the fields aren't confused with arguments. Also, in order to pattern match and "take apart" a value of a certain type, you always at least need to pattern match on its constructor. This is no problem in your case, because the return value of isBloodOrange could only possibly be true if the constructor of the value is Orange. Also, your use of pattern guards, while admirable, is unnecessary in this case. Here's how I'd rewrite isBloodOrange: isBloodOrange :: Fruit -&gt; Bool isBloodOrange (Orange variety _) = variety == "Tarocco" || variety=="Moro" || variety=="Sanguinello" isBloodOrange _ = False Here I use an additional pattern match to cover the case for when the Fruit is not an Orange, in which case isBloodOrange will simply return False. The reason you always need to pattern match constructors to take apart values is because the types and number of fields to each constructor may vary. You can't do stuff like (x y z) &lt;- fruits Because z may be a Bool if x is an Apple, or it may be an Int if x is an Orange, there's no way to know at compile time. Also, what if we rewrote Apple so it only had the String field? What would z be then? So yeah, if you want to study each possible constructor, you need to write a pattern match for each one. That said, if you really want to "take apart" values like this in a list comprehension you could do (Orange x y) &lt;- fruits This won't even crash the program if there is some element of fruits that can't be pattern matched this way; that element is simply skipped. The reason for this is because of list comprehension is really just syntactic sugar for the list monad, which has a way to handle stuff when pattern-matching goes wrong. If you don't know what the list monad is, don't worry about it. Anyway, even though you can "recover" from failed pattern matching in list comprehensions like this, I personally don't encourage you not to rely on it, because I think it's bad practice. Instead, use auxiliary functions to determine if the element is of the right sort, like isBloodOrange does, and pattern match on the left-hand side, because that pattern matching will occur after the element is deemed to be of the right sort. Also, you don't need to check that what isBloodOrange returns is True. I mean, the equality operator == returns a Bool just like isBloodOrange does. Why not use isBloodOrange directly? sum needs to be rewritten outside of the list comprehension. It makes no sense to calculate the sum of an Int, but it does make sense to calculate the sum of an [Int] Taking all of this into account, bloodOrangeSegments can be rewritten like this: bloodOrangeSegments :: [Fruit] -&gt; Int bloodOrangeSegments fruits = sum [let (Orange _ z) = x in z | x &lt;- fruits, isBloodOrange x] (We don't actually need to use parentheses around the left hand side of the "let" here (although it doesn't hurt). Why? Because Orange begins with uppercase, which means it has to be a constructor, as function names can't begin with uppercase, and so Haskell can determine that this has to be a pattern-match.) &gt;in the second question i am asked to find the sum of all blood oranges given a list of Fruit data, I implemented the function i built in 1 and &gt;did a list comprehension, but i am not sure if the pattern matching works &gt;for this question i need to build a function that finds out the total number of apple fruits in a list of Fruit that have worms (ie True in Bool), i did a recursion but its apparently some type error as well worms :: [Fruit] -&gt; Int worms [] = [] worms (x:xs) | fruit==Apple &amp;&amp; etc==True = 1+worms xs | otherwise = worms xs where x = fruit variety etc Much like earlier, you're trying to pattern matching the wrong way. Also, the base case of worms (when the list is empty) returns an empty list, which contradicts the type; it should return 0 instead. The way you're pattern-matching x under the "where", well... it's not a pattern-match at all. You've reversed the right hand-side and the left hand side. It should look something like this: where Orange variety etc = x However, this will crash the program if this pattern match fails, because the program doesn't know what to do if the pattern match fails, unlike pattern matching on a function's arguments, where a failed pattern-match will make the program try out a different case of the function, or in list-comprehensions, where there's a defined way to handle failed pattern-matches. As a rule of thumb, failed pattern matches as part of a let or a where will crash the program. It's possible for a failed pattern match to crash in other contexts as well, most notably functions. When the arguments for a function doesn't match any pattern defined for any case, running the function with those arguments will crash the program. The function "head" is an example of this; it crashes the program when you give it an empty list as an argument. (To be honest, there actually exists a case for an empty list within "head", but all it does is crash the program with a more descriptive error message than what you usually get with a failed pattern-match.) Anyway, how do we fix this? By using case So: worms :: [Fruit] -&gt; Int worms [] = 0 worms (x:xs) = case x of Apple _ etc | etc -&gt; 1 + worms xs _ -&gt; worms xs case is a way of trying to pattern-match something until it sticks within a function, much like pattern-matching a function's arguments. This can be simplified a bit by also pattern matching on etc, like this: worms :: [Fruit] -&gt; Int worms [] = 0 worms (x:xs) = case x of Apple _ True -&gt; 1 + worms xs _ -&gt; worms xs And if you want, you can rewrite this so case isn't even needed: worms [] = 0 worms ((Apple _ True):xs) = 1 + worms xs worms (_:xs) = worms xs There are actually various ways to rewrite "worms" so it may be expressed on one line, like using list comprehensions, or through using something called "folding", although the latter may be too advanced for you as of yet.
Software Foundations is a fantabulous book. Truly scrumtrulescent.
We've erred on the side of caution with this in Rust; people ask for it a lot though.
oh ya, if you're not tight with emacs yet, I've heard good things about CoqIDE too, never used it though. But of course: &gt; if you're not the kind of person who uses emacs, become the kind of person who uses emacs ;)
Thanks for approving the post! 
We allow working from home to some extent but we don't do remote at the moment...
What is this new `ghc-pkg` build tool?
&gt; Indeed, category theorists have argued that morphisms are more important than objects, .... Interesting thought, but I think that's an odd way to put it. Pretty much all mathematical abstractions are about finding properties that diverse structures have in common, taking those properties as axioms and seeing what we can come up with. For example, a *group* abstracts common properties of algebraic operations and symmetries of objects in space. Category theory certainly arose in this way. Morphisms are all over mathematics, they pretty much always have certain properties, so let's look at those properties. But this my-abstraction-is-better-than-your-abstraction thinking strikes me as counterproductive.
That's a good point, I was being a bit loose with my terminology there.
&gt; Coq has been my latest weekends project, and has changed my thinking about programming, perhaps more than the python -&gt; haskell jump. Could you perhaps share a bit what you learned and how did you thinking change?
Who are the consumers of the APIs you're writing, and do you find in your work that you're often starting new services or working on projects which are somewhat temporary? And what are the consequences of runtime bugs? I'm curious because it sounds like your work might be very different from mine: the haskell codebase I work on is the engine of a messaging application that customers more or less directly interact with. So I've been working on essentially the same codebase for years (although it is deployed as many services). We also write a fair number of (often small) libraries which are used across many components. I definitely don't loathe python; I think it's much more pleasant to work in in some ways than haskell. But I wouldn't want to write something medium-sized, which I had to maintain, and which I had to be reasonably sure was free of Bad Bugs(tm) in production. Another angle: I think python is great if you're the last-mile consumer of the language ecosystem, i.e. you don't need to write any libraries yourself. I'm curious what you think.
While you can no longer hang instances off of a _type_ family like this, you can do so off of a _data_ family now that [Trac #12369](https://ghc.haskell.org/trac/ghc/ticket/12369) has been fixed: {-# LANGUAGE DataKinds #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE TypeFamilies #-} data family Any :: k class Foo (t :: Bool) instance Foo 'False instance Foo 'True instance Foo Any instance Foo (Any Int) instance Foo (Any Monad)
I have a physics background, but I am interested in picking up Haskell after a long period of admiring it from afar. Has anybody here done an Ising or an XY model in Haskell, and if so, could you give me some instruction or guidance on how to do it? That might be a good starting project. Also, I'd be interested in learning more about category theory, and if possible, any other branch of mathematics through Haskell. Any resources?
I'm not sure why this is being posted here. In my experience, /r/Haskell is extremely good in this regard, at least compared to the likes of /r/programming or /r/java. There's *a bit* too much of an untoxic version of this, where an expert answer's a beginner's question with an unnecessarily advanced answer, but it's usually accompanied by plenty of other people answering more appropriately, and leaves ground for more advanced people to have their own discussion of the more advanced solution.
Her rich white man example is in really bad taste.
I stand corrected, most of the examples are riddiculous. Shame, it was otherwise interesting. She succeeded at ruining her own talk.
&gt;I'm guessing if I wanted to get some Logging involved, I'd add to the transformer? Seems like a good idea but you should use a dedicated logging library if possible. 
In the generation of python/lua yes. In most other respects no. (eg. I cannot comprehend the use of fgl. Indeed the flow graph does not need to be reified explicitly and one can use instead variables and references).
What part is in bad taste?
Why? It is a decent application of category theory to a real-world topic. Whether you agree or not, it certainly fits the philosophy of many people, and if it doesn’t is still understandable.
All the examples involving race and gender were in bad taste. She made the entire talk political. For me, as a white man, it was painful to watch. I was really interested in the topic, but the moment she started picking on me (among all other white men), trying to depict us as bad/evil, she lost all my attention and I feel insulted to be honest. Not at all what I was hoping for.
I admit it is something I treat personally, hence my reaction is a bit emotional. I am just tired of being portrayed as someone that has it easy because of my skin color. It is just another form of discrimination. I try to avoid talks of this nature for that reason, I was just not expecting it from a category theory talk, which is why I am so disappointed.
I still don't understand what makes this bad taste.
I think you misunderstand the concept of privilege. The idea is to try and identify differing power structures in society, not identify who the bad guys are. From one white male to another, if you try and look at these kinds of discussions through this lens, things make much more sense. Is there some kind of toxicity involved in popular versions of these discussions? Sure, but that does not discredit the under lying discussion, and I saw no such toxicity in her talk.
Well, taste is a subjective thing. An example that presents a certain group of people in a derogatory way, in my opinion is distasteful.
Also white man here. That was actually my favorite part! I thought it was a great example of something I've tried to explain before, but never clearly enough that I've gotten any reaction other than "oh a white man would say that, you don't understand". The category theory surprisingly did clarify it pretty significantly. Sure it was more awkward than the average programming talk, but less awkward than the average talk about race/gender issues!
I had roughly the same thought about this not needing to be posted here. Yes, there are mean people who are smart, and they use their smartness to be mean. But there are also mean people who will use any tool available to be mean, in restaurants, in schools, at workplaces, and pretty much everywhere. There are mean people in the haskell community, but that's pretty much true in every PL community (and in every non-PL community). Calling them "toxic" doesn't really help make anything better. After all, most people I know have already come to the conclusion that they don't enjoy the company of unempathetic people.
I think you may be right about the misunderstanding. From my point of view, the problem is that I wasn't really expecting a talk on social issues, especially presented from a certain, very strongly opinionated point of view. Maybe if the title of the talk indicated it is about priviledge or social relationships, I could consciously choose not to watch it.
Never hurts to be aware that this type of behavior does exist, and to keep an eye out for it, and maybe remind others who exhibit that type of behavior to become aware of it in themselves and work to correct it.
That makes me think you didn't really listen carefully and objectively to the content of the example. It's way more nuanced than "white people have it easy." She even explains why she thinks poor while males are victims of injustice in a way where they are perceived as privileged but are actually "below the people they are considered to be more privileged than." We can discuss the accuracy of Cheng's diagram, but it's a political topic not really at home in this subreddit. I think her choice to use this example was courageous and a bit risky, but absolutely pulled off well. (I'm a moderately well-off white man, and my position in the privilege cuboid is a fact of my life on Earth. It "ain't easy bein' white" as Gob in Arrested Development put it, but the status of wealthy white men in power hierarchies is a material fact; just look at corporate boards of directors, venture capitalist firm partners, etc. Cheng's lattice is a simplified abstraction, that's the whole point, but this abstraction is not made up.)
I accept the criticism.
&gt; Who are the consumers of the APIs you're writing I think this is probably the right question. In every case, the consumer was a web frontend written by either me, a person or team I was in close communication with, or a team that I managed. &gt; Do you find in your work that you're often starting new services or working on projects which are somewhat temporary? No, I wouldn't say so. These are consumer-facing web applications, mostly data visualization projects. Some of them have been running with few modifications for years at this point. &gt; What are the consequences of runtime bugs? This is another good question: they surface either in logs or the users or stakeholders would report them. We'd fix them in sprints and deploy fixes. I wouldn't say that any bugs resulted in significant data loss or problems for teams or the company. Thus, the consequences were typically "not a big deal". I can't remember a showstopper bug, a really serious one, in the last ten years. This perhaps hints at something I can't quite get at: the depth of my experience perhaps, or being truly untested by not dealing in "high stakes" applications. Sometimes end users get angry if something tips them off, but that's not necessarily due to the severity of a bug. I do get nervous imaging large Python codebases written by unseasoned or otherwise undisciplined developers. I've seen a few and they were scary. As a result, I have strong opinions about what a Python codebase should look like, but mostly these opinions are non-specific, abstract, gut-level. They're totally impractical to share with anyone else in other words, sort of useless, like a compendium of dull aphorisms. I tend to write various little libraries in order to break problems into discrete analyzable pieces, so I'm not sure about your last point. The only other comment I have is that in my current gig I have been really hankering to rewrite a web API used by consumers using Servant (currently it's in Java), because then I could generate clients for them and have a bit more influence in how they're interacting with the application. I could also auto-generate documentation. I also really love Servant. This rewrite is something that I don't think would be appropriate for Python because of its sprawling nature and the kinds of performance constraints it must operate under.
Related: &gt; A crude statement of ESR is the claim that all we know is the structure of the relations between things and not the things themselves, and a corresponding crude statement of OSR is the claim that there are no ‘things’ and that structure is all there is (this is called ‘radical structuralism’ by van Fraassen 2006). https://plato.stanford.edu/entries/structural-realism/
That subreddit is a dream come true! I've been exploring this line of thought and many of those thinkers for several years now, but in virtually total isolation. I'm very glad to have come across this lead. :D
[removed]
Being generally in agreement with those criticizing you here, I'm glad to see you say this. But I'm also really curious if you still feel as you described above: &gt; I am just tired of being portrayed as someone that has it easy because of my skin color. It is just another form of discrimination. I try to avoid talks of this nature for that reason, I was just not expecting it from a category theory talk, which is why I am so disappointed. I think your response is a common one, and a big part of the disconnect between race/gender/class justice movements and conservatism. ...But my interpretation of that part of Dr. Cheng's talk is, if anything, _justifying_ the common feeling that poor white men are being discriminated against. I don't hear her telling you that you have it easy because of your skin color (although she certainly simplify to the point of saying that _rich_ white men have it better off than all the other groups, which I believe is true, on average). Can you help me to understand why what we each heard in that part of this talk is so different?
Out of curiosity, why are you guys using heterogenous lists?
Not soeaking for /u/snioszek here, but I think part of it may come from a belief that high privilege implies low moral standing.
They may be vocal in promoting Stack, but frankly that seems really uncalled for. On what grounds should they be banned?
I think it may come from different life experience. Having worked in big corporations for a while, also in some very progressive countries, I was exposed to diversity &amp; inclusion talks and initiatives, often conducted by gender studies graduates. During those obligatory talks, and in emails we would get, they always presented the same one-sided point of view. Which is, that white people owe something to the rest of the world and that men owe something to women. This asymmetry always irritated me. I could also see these sentiments shared by people I met in those days. More often than not, just disagreeing with them was a serious faux pax causing awkward silence. Some people stopped saying hi to me at work simply because I questioned some of these ideas. I come from a country that is very tolerant and equal. Men and women have the same opportunities and pay. Men treat women graciously - we open the doors for them, let them first through the door etc. These are the social standards over here, absolutely obvious to us. I worked with people from all continents, never discriminated anyone, have some good friends of color. These things are absolutely normal to me. And yet, in the talks I mentioned earlier, the white man was always an used as an example of privilege, special treatment, and exploitation of others. It was also always implied, more or less directly, that we need to take action to promote interests of non-whites to achieve balance. It became a very heated conversation especially concering the current events in Europe. That contrast, of what I know and believe, and the depiction of my color in the talks, felt unjust. I think it made me weary of such opinions. This is why I felt the examples in the talk were distasteful, as they followed the same pattern, at least in my perception. I accept the criticism, because it is based on arguments and actually made me realize that it may be my cognitive bias that got the better of me. Especially the comment about white poor men is something I totally missed, yet it is there. I feel a little embarressed for starting an offtopic conversation, but I don't want to delete a comment and pretend it was not there, there are some good responses that made me think.
If you check the user's comment history, you'll see a mix of "promoting" Stack by claiming or implying that other options are deprecated when they are not, some personal attacks (although some have been deleted after they were called out on it), or attempts to ressurect or import drama from other places on the internet. The community guidelines for the sub mention that obvious trolls are obvious and will be banned, some of the mods have acknowledged that this particular user is a troll, and they only reason they haven't banned the user (again) is that they would likely be back with another alt account immediately.
I went through the user's history (and even the history of their former account) and I can't see anything indicating that they are some kind of troll. There's no deliberate attempts to misinform or instigate arguments as far as I can see. I can see some remnants of a mildly inflammatory comment here or there that was edited, but nothing particularly egregious. Even if you might disagree with their position on the matter, it seems to me that they genuinely believe what they are saying about stack and think that they ought to argue in favor of stack whenever the topic arises. Even if the user can be rather absolutist at times in what they claim, I do not think that gives us cause to ban them. Your comment seems much more hostile than anything I have found from this user, though of course I fully admit that I do not know of what comments they deleted.
The abstraction isn't even correct though. To bring up some obvious counter-points, [Asians](http://www.businessinsider.com/heres-median-income-in-the-us-by-race-2013-9) are overrepresented in high-paying positions, far more so than whites. Furthermore, while men make up most of the top 1%, they also make up most of the bottom 20%. Any given person is far, far more likely to end up in poverty than as a board director. The fact that 75% of the homeless are men has a far bigger impact on day-to-day life than any statistic about high-paying jobs. &gt; my position in the privilege cuboid is a fact of my life on Earth What about [South Africa](http://www.news.com.au/finance/economy/world-economy/bury-them-alive-white-south-africans-fear-for-their-future-as-horrific-farm-attacks-escalate/news-story/3a63389a1b0066b6b0b77522c06d6476), or [China](https://www.youtube.com/watch?v=D-DJrrMbLC4), or anywhere that's not majority white? The US is not the world. You're position on the cuboid doesn't even make sense on most of Earth, or where most people on earth actually live. Beyond that, if you look at typical examples of disadvantages faced by blacks, most of them are exclusively felt by black men. When you look at unfair police targeting against blacks, unarmed shootings, unfair sentencing, these apply primarily to black men. The [sentencing gap between men and women](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2144002) (how long of a sentence one gets for identical crimes) is six times larger between men and women as it is [between blacks and whites](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1985377). The idea that a black woman is somehow less privileged than a black man is patently absurd. There's also the fact that men are largely the ones made to fight wars, the fact that infant MGM is legal in the west, etc. To be clear, my point is not that women have no problems, and that being a female is the "true" privilege. Rather, the entire analysis is bunk. Society puts expectations on people. These, however, do not lead to a clean intrinsic ordering where advantages are given to whites/males that non-whites/females don't get, and there are no advantages the other way. That's simply not an accurate picture of reality.
Seems like false dichotomy to me. I'm still not convinced what the advantage of listing every single internal module in a cabal file is, in the first place! 
[toxic expert](http://britneyspears.ac/lasers.htm)
Thanks!
Is Agda more or less equivalent in this regard?
I don't know of any *canonical* solutions, but looking for GitHub repos with "cis194" in the name will give you several ones. I think the style of the exercises is such that you can, most of the time, test your solutions inside GHCI as validation.
Calling them out by name and writing a whe post based on some harsh criticism is toxic as well. The same energy could have been spent on writing a a follow up article and correcting the points the toxic expert raised.
Tracking table schemas (I don't think I can give more info, sorry).
Not a problem. I certainly do not want to get you into any kind of trouble, so please pardon my asking. My first inclination, of course, is to think that such lists are unnecessary assuming that one could leverage the type system to unify the elements into a single type (a sum type, perhaps?), but I obviously don't know the problem domain like you do, so such thinking could be a gross oversimplification. Unfortunately for your problem, personally I don't think Haskell is particularly well-suited for working with heterogeneous data structures in general, since, as you have noted, you lose out on a lot of the generalizations that have been developed in Haskell's ecosystem.
a bit late to the party, but did you `sudo apt-get install libgtk-3-dev` ?
&gt; I feel a little embarressed for starting an offtopic conversation, but I don't want to delete a comment and pretend it was not there, there are some good responses that made me think. I think the way you handled this conversation does honor you. You did it in a very clean, *pure* way. (Haskell reference for good measure).
Well my point wasn’t that it *must* be one way or the other, just that it currently is due to the current implementation of Cabal, and currently I would make that choice.
I really don't understand Dr Eugenia's point about equations being lies. I felt like her arguments against her 3 examples weren't very spot-on. For the cookies multiplying problem, the bag part is a completely idiosyncratic add-on. A fairer analogy would've simply been 2 x 5 cookies vs 5 x 2 cookies. I also felt that for both of arguments about the addition problems were on some notion of "easiness", but it really seemed more like ergonomics, and at a high price too -- to underplay such useful properties like commutativity or associativity when it's not obvious we're reaping net computational benefits.
It's really important to talk about bad behaviors in all communities. The Haskell community has it's fair share of this sort of thing so it's important to openly discuss it, not ignore it.
There is also Pareto principle 80/20. Making something perfect can be waste of time as well.
Afaik the problem is that for some extensions, it’s the lexer that fails if they are disabled and printing out suggestions in that case would require a fair amount of refactoring, another extension that’s also affected by this is `FFI`. So I don’t think a proposal is really helpful here, it just needs someone to step up and get the work done. It would definitely be great to have this! (I’m not an active GHC dev so take the above with a grain of salt)
&gt; These techniques are currently being applied by Tweag I/O in the context of a project with Novadiscovery. Novadiscovery is a consulting company for in silico clinical trials, namely simulation of virtual patients through biomodeling. Parts of this blog post are actual code from the tools we develop with them. Tweag sounds like a cool place to work. 
That warning would be desirable, but no need for a full GHC proposal. Just submit a Trac ticket, and ideally a patch.
&gt; fmap for (Either a b) could preserve sharing on Left constructors becomes an option, because commoning up cases doesn't have to be well-typed. Not, “could”, but “can”, since http://git.haskell.org/ghc.git/commit/19d5c7312b
It seems to me that if you were to apply the advice in the post to the post itself, you would end up with a completely different version or you would not have posted it at all. There is no empathy for the random commenter. Author gets criticised in some less-than-perfect way? Immediately label them as "toxic". Seems like the author is an expert on what "toxic" means and on what proper internet etiquette is. There is some irony.. And what does this comment do? Lets me feel superior because I see through the pattern one layer deeper of course! Thing is, I don't feel or claim to be morally superior because of it.
That was also my favorite part. Maybe she was just trying to find meaningful example for how category theory can be applied to real life, domain specific knowledge typically thought of as being beyond the realm of mathematics, but I think her specific examples have a higher risk of being considered "controversial," so I thought it was courageous of her to choose these particular examples. If anything, it really was the best way to bring up these political issue during such a technical conference without going off topic, so it was doubly courageous of her to do that.
I talked to the instructor, Brent (/u/byorgey), about this once and at the time, he didn't want to have solutions published because he was planning to use (a variation of) the material for another university course with graded assignments. As a side note, I found the tone of your question weirdly accusatory -- in particular, you seem to be blaming the Haskell community as a whole for not accommodating your particular style of learning, which would be unreasonable given that everyone around here is volunteering their time. Sorry if I misread you; English is my second language, so my interpretation may well be off.
But you don't write a post that criticises people for wanting to feel superior where the main purpose of the post seems to be to make its author feel superior. It does not seem to qualify as a discussion.
I thought her statement that "equations are either lies or useless," was a sort of tongue-in-cheek way of saying equations are often oversimplifications if real world problems. Because she went on to talk about how often times oversimplification of real world problems leads to unsatisfactory solutions. The cookie counting example when teaching arithmetic to children was a good example of how we kind of force kids to eschew perfectly logical relationships they may understand just so they can practice working on the most abstract (and most uninteresting to children) relationships between numbers, like the ideas of commutivity and associativity.
How does this compare to `acid-state`? It seems very similar in nature to eventful
(this reply of mine is not very important). it's curious, because i found the original poster's question actually the opposite of accusatory! i found that they really did acknowledge that it's okay that others have a different learning style, and that they acknowledge and appreciate how friendly the Haskell community, but for their own learning style, they felt very frustrated by the delay in responses. [to be clear, your comment about talking to the instructor is a good bit of information! i'm not saying that you were wrong to mention your impression that the post sounded accusatory; i just wanted to offer my surprise because i thought it was just the opposite]
Well, here's a prime example of the perils of written communication. ;) Your interpretation makes sense as well, so I guess it's mostly about which parts of the text we latch onto. Thank you for offering a different perspective. (It's unfortunate that the 'different perspective' phrase is so overused; makes it sound quite formulaic. I mean it though.)
Toto, I've a feeling we're not in Scott domains anymore.
In my experience, /r/Haskell has been remarkably good in this regard. Sure, it happens occassionally, but it’s an outlier. For the most part, when people post ideas or questions, they’re met with very understanding responses. So I just don’t see evidence of us ignoring a problem if we don’t have it. “Downvote and move on” is the responsible reaction.
OK, filed here https://ghc.haskell.org/trac/ghc/ticket/14421#ticket 
I find it worrisome. 
Excellent explanation, thank you. But I disagree with your conclusion. Not violently, just have a real queasy feeling that that conclusion will not stand the test of time. So let me try to put words to that queasiness. I think it boils down to there being information in those stuck types. That you can have stuck types, and have them have non-trivial information stashed in them, is fine. Where things seem to 'go wrong' is that you are able to retrieve that information. But here's a different way to look at it, which contradicts my 'queasiness'!! What /u/edwardkmett has done is shown that, at the type/kind level, one can simulate Barry Jay's [SF Calculus](https://github.com/Barry-Jay/SF). You can dispatch on syntax, and decompose applications into their constituent parts. You have to rig things a little, but there it is.
Thanks. It might still be that nothing happens until some volunteer finds this interesting to work on, of course… 
I'm becoming more and more convinced that type families are a bad thing. All the issues with them aside, I think they'll be obsoleted by DependentHaskell. A type family isn't so much a function on types as it is a type class with a dependently typed method (open type families, at least). Type families are resolved in effectively the same way as type class instances today, which is a better way to think about "pattern matching" on types, as it doesn't violate parametricity. type family Foo a :: 'Bool type instance Foo X = 'True -- Corresponds to: class Foo a where foobody :: proxy a -&gt; Bool instance Foo X where foobody _ = True We tend to only use closed type families for non-`Type` types, so those can be replaced with actual functions. And the cases where we're actually using type families to pattern match on `Type` are at risk of violating parametricity. I'm guessing DependentHaskell will have to have some special semantics about type families to disallow them from being scrutinized at runtime, or else something like this could be possible: type family Foo a :: 'Bool where Foo Int = 'True Foo a = 'False id :: forall a. a -&gt; a id x = if Foo a then 6 else x Replacing closed type families with functions and open type families with classes seems like a much cleaner way to do things once DependentHaskell arrives, and will hopefully avoid all the troubles with "stuck" type families we've seen crop up.
Not at all. Acid state is just another storage (replacement for your sql rdbms), while eventful does not replace your storage. It provides connectors that allow you to store your data in many different storages like in memory or sql rdbms or cloud storages. You can create a acid-state storage backend for eventful for example. Or you can can store your events in acid-state without using eventful because the theory behind is really simple. But then you would be re-implementing much of the eventful. 
do you prefer mtl style over free? I would be interested to hear from more experienced Haskeller about the topic
fyi, there's likely going to be a respin of RC2, since it turns out that the tarballs at https://downloads.haskell.org/~ghc/8.2.2-rc2/ don't faithfully correspond to Git commit b8877b37d48d329e819a16995c636dcfa3da4200 However, the `ghc-8.2.2` packages from my PPA at https://launchpad.net/~hvr/+archive/ubuntu/ghc do properly reflect the Git commit b8877b37d48d329e819a16995c636dcfa3da4200
I'm in a similar situation to OP but fully support this opinion. To me, a solution is *right *if it provides expected results in the REPL. I would like to know that I've done it in a reasonable manner, so will be grateful of reviews, but getting it right still allows me to move on. I can track back to review efficiency of solution if/when someone is kind enough to review. Y code.
I disagree. acid-state is explicitly a combination of helper functions for generating Events and event handlers. `Update` monad are just `Event -&gt; State - &gt; State` ops and Query Monad are just `Event -&gt;` ops. It then journals these events and replays them to get a view of the current state. It's very much an event sourcing framework. 
I'm not sure I understand your suggestion. It sounds to me like you're saying to construct a unitype for the domain and use regular lists, giving up on tracking the types of each element. If I'm understanding correctly, then your suggestion is absolutely possible, but not desirable. It leads to a lot of `error "impossible"` case branches and also to the type system not catching bugs as soon. My experience has actually been that Haskell is quite adequately suited to working with heterogeneous data. The weakness I mentioned in my original post is one of the biggest inadequacies, and it's not particularly big. Sometimes the type errors are atrocious, but that's life. One of the biggest pain points is actually that programming in and designing in a code base that relies heavily on heterogeneously typed structures requires a different style of thinking than standard Haskell, and we need to use a lot of existentials, scoped type variables, type applications, and GADTs to appease the compiler, while also being mindful of and working around erasure. The concepts in [this](https://www.well-typed.com/blog/2017/06/rtti/) blog post are particularly useful.
People keep saying that violating parametricity is bad, but being able to change runtime behavior on types is _useful_. In the code base I work on, we have to go to great lengths to recover enough runtime information to do this! I think what people fear is that the violations are _untracked_. I think your `id` function would be ill-typed since the `a` is not "relevant" (to try to use the terminology from the wiki [page](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell)), but the following one would be fine: id :: pi a. a -&gt; a id x = if Foo a then 6 else x I could be misunderstanding, though.
You can use the Linux subsystem as your ide: https://mkaz.tech/geek/unix-is-my-ide/
Most DBs are built around events, so this could likely be said of postgres. The intention behind a lib usually informs its ergonomics.
Oh I so often tried to watch her monad videos a while back. I'd never think she had no idea about FP use of the concept.
I've had good success with Visual Studio Code and some of the Haskell plugins (such as Haskero).
Understood. It's good to have it reported though.
I don’t like the idea that `forall` is anything but a special case of `pi`. Pi types are supposed to be a generalization of all function types, including polymorphic ones (even rank N). Eliminating this theoretical property for the sake of a questionable design pattern (essentially Java’s `instanceof`) would be a disappointment.
why does `deriving Ord` require evaluating both arguments in the first place?
Hi! I started a while ago to make a naive backtracking sudoku solver in haskell. I came back to it yesterday and it seems like it works, but only on almost solved grids. It correctly returns the solution for those. But when more than about squares are empty, it returns Nothing. I'd appreciate it if somebody was able to [check it out](http://github.com/lerrpy/sudocurry) and give some insight. Thanks!
The point (to me) is that the equals sign has to carry some sort of nontrivial content. Or mean "equals under some interpretation" and interpretation carries the content. 
From what I understood so far... We know short-circuiting in C-style languages for &amp;&amp; and ||. This applies to Haskell too on first look, but that's not actually the case. Instead, there is an extended truth table that involves True, False, and also Bottom, because that is the only possible effect in pure code where you can observe a difference. We have this because it is a common pattern in programming to rely on this very behavior (index within bounds &amp;&amp; access container with this index). That is however not the case with bounded types as you've posted about -- Here, the reasoning is probably that if you want to impose an order (however that is written down in the specification then), it's too limiting for optimization. After all, the optimizer could figure that evaluating the right side first has a lot of benefits, and there could be expressions already evaluated beforehand. And you could turn all of your examples around and make just the same case for evaluating the right side first... Your really long calculation could also be on the left side. I'd say having custom comparison functions in your compilation unit for this use case would be the way to go. It's a very advanced optimization. 
[I *usually* prefer mtl](http://reddit.com/r/haskell/comments/72th9v/free_monad_considered_harmful/dnla639). That whole thread is worth reading through, as well as many of the other discussions that have taken place on /r/Haskell about it. I’d recommend searching /r/Haskell for a term like “free” to see some of the previous discussions.
For me, in the process of learning, the IDE is crucial because it provides type information on various bits and pieces. In this respect, I find Atom or vs code equally useful to the point of not being able to adopt one or the other. Together with the editor I’m using Haskell-ide plugin. 
I find that an IDE is very helpful for learning, so I disagree that one should worry about it later. It can be very valuable to inspect types and jump to definitions in order to understand what the compiler is doing. I don't have a good suggestion for a Haskell IDE, unfortunately.
Thanks for doing it! There are a lot of good topics. I was very excited at seeing the paper "Collapsing Towers of Interpreters" at first, but then became rather disappointed because it didn't reference or compare to Thyer's PhD thesis on ["Lazy Specialization"](http://thyer.name/phd-thesis/). Not sure if any reviewer caught it, but still, given the author's credential, they should have done this homework.
Regarding your example: there should be no difference because `ReaderT` and `ExceptT` commutes. But there's difference if you're using `ExceptT` and `StateT`. In general it's considered a bad idea to use `ExceptT` and `IO` in one monad stack. See this blog post for explanation: * https://www.fpcomplete.com/blog/2016/11/exceptions-best-practices-haskell `mtl` style of monad transformers make it easier to use monadic action, because you don't need to use explicit `lift`s.
When I was working on cis194 I used “codereview.stackexchange” for the first 4 weeks of the course. Then I found a mentor who reviewed my code.
[I *usually* prefer mtl](http://reddit.com/r/haskell/comments/72th9v/free_monad_considered_harmful/dnla639). That whole comment section is worth reading through, as well as many of the other discussions that have taken place on /r/Haskell about it. I’d recommend searching /r/Haskell for a term like “free” to see some of the previous discussions. (EDIT: Reposting this to make it a response to the comment I intended)
I like to group imports by package name, then module name. I start with --base, then the modules from base sorted alphabetically, then the same for other packages, sorted alphabetically, then the modules from my project. I don't think readers of the code should have to look at the cabal file to see what packages are used and then have to figure out which imported modules are from which packages. I find it curious that I've never seen -XPackageImports used in the wild. I wish the automated import tools grouped modules by package.
Thanks. I had thought of codereview, but wasn't sure which was more appropriate out of there and r/Haskell. Can I ask how you found a mentor?
In dependent languages, runtime type dependency is usually realized with universes and large elimination; i. e. we use regular functions to compute types from runtime data. This lets us choose parametric and non-parametric behavior as required, with parametric polymorphism being the default.
`forall` _is_ just a special case of `pi` (well, I'm not sure if it actually is, but this isn't a counterexample). `forall` is just the case that doesn't allow runtime dispatch on the type variables.
What's large elimination? My understanding of universes is that it's what you end up needing to do anyway in Haskell (as far as I know, [this](https://www.well-typed.com/blog/2017/06/rtti/) post, albeit not the conventional presentation, counts as universes).
Well no. `forall` is *literally* just `pi` when the argument is "irrelevant," in `-XDependentHaskell` terminology. `forall x. t` is the same as `pi x. t`. The semantics are identical. You can't use the semantics of `pi` alone to say "Oh and the argument can't be cased over."
I read through the original post, and I don't actually think the "toxic expert" was toxic. His comments were to the point, educational, and reasonable. It hurts to get comments like that, but that's life. The article was better with those "toxic comments" than without them.
Large elimination is induction/recursion returning types, i. e. returning in a universe. Hence, there needs to be at least one type containing types or codes of types ("universe") in order for this to make sense. The simplest interesting case of large elimination is if-then-else with different branch types, which is often used as intro example in Idris as well: foo : (b : Bool) -&gt; if b then String else Int foo True = "foo" foo False = 0 Although we can't branch on opaque/paramteric types, we can do so on `Bool`. Although `Bool` itself is not a type, since actual types can be computed from it, we get runtime type dependency on `Bool`. Now, `Bool` is a simplistic source of such dependencies, but we can use much more complex data in place of it. In a somewhat confusing way, sometimes the data type from which types are computed is also called a "universe", since each of its values is decoded to a type. Again, universes are types of types or types of codes of types which can be decoded to types.
No, it throws away events after replay and actually stores the state. That's not event sourcing, it's just plain journaling. (AFAIUI, it's been quite a few years I've looked at it, though so I might be mistaken.) In an event sourced system you might of course store snapshots just to get a speedup, but the idea is that you *never* throw events away.
As I understand the notion of "irrelevant", it means that it can't be referred at the value level. &gt; For terms and their types, a binder is relevant iff it is not erased; that is, it is needed at runtime. [...] For types and their kinds, we can't talk about erasure (since the are all erased!) but the relevance idea works the same, one level up. [...] It looks parametric in both `k1` and `k2`. But it isn't, because it can pattern-match on `k1`. So `k1` is relevant, but `k2` is irrelevant. If all type parameters can be pattern matched (i.e. `pi` is not parametric), but some cannot be referred to, parametricity is recovered in those cases.
I don't see any reason to have parametericity on types. It's good to have on values, but on types it doesn't matter since everything is checked at compile time.
As I understand it, relevance is mainly a compiler optimization. It's not really a part of dependent type theory. So relevance is mostly an operational semantic, not denotational. What you're suggesting is using relevance for denotational semantics, i.e. in order to know the parametricity of a function, you must also know its arguments' relevance. This mixing of operational and denotational semantics seems undesirable to me and reeks of *both* Java's "type erasure" debate and its `instanceof` debate.
Good catch. I somehow missed that. A better Bool comparison would help a fair bit.
But it's not all checked at compile time if you can case on types at runtime. Type families aren't suitable functions on types because using them at runtime forces types to be relevant at runtime.
This has been my experience as well -- using the stack-static package on AUR is fine for me now.
Sure. https://github.com/haskell-learning-group/haskell-learning-group (the group was pretty active few years ago, but nowadays it seems not).
What do you mean by "using a type family at runtime"?
&gt; As I understand it, relevance is mainly a compiler optimization. As I understand it, "relevance" is a denotational concept, whereas "erasure" is the related compiler optimization. As for `instanceof`, I disagree that it's a bad idea as a language feature subject to the following: * _Using_ `instanceof` in OO code is usually a smell, and it would be better to replace them by a method that handles the dispatch appropriately. Even so, as a _language feature_, it might still be very useful; after all, `unsafePerformIO` and `unsafeCoerce` are questionable to use, but sometimes invaluable. * Parametric functions are valuable, and provide strong guarantees. Non-parametric functions are _also_ valuable, and provide weaker guarantees in return for more flexibility. As long as we don't sacrifice the guarantees of all functions just to provide the flexibility to some functions, I think `instanceof` (or `typecase`, as I often think of it to detach it from the associations with Java and OOP) is a good thing to have. I have needed non-parametric functions and have had to make major refactorings to add boilerplate data types just to be able to reflect type information to runtime for case analysis. It would be nice to have language support for this.
Speaker here. Obviously I'm somewhat biased on this front, but I prefer the freer approach personally. The usual arguments against it are performance, which admittedly is worse, but when are you ever CPU-bound anyway? The ease of spinning up new effects (and subsequently using them to semantically annotate WTF you're doing) strongly outweighs the downsides in my view.
&gt; As I understand it, "relevance" is a denotational concept I don't think so. The only way "relevance" is denotationally meaningful is if you can case on `Type`, i.e. if `Type` can be "relevant." The current design for DependentHaskell as described in the page you linked above claims that `Type` will *always* be "irrelevant". &gt; For types and their kinds, we can't talk about erasure (since the are all erased!)
I quite like Atom, Reddit loves to hate on it but it's quite nice, especially with the ghc-mod extension
&gt; I don't think so. The only way "relevance" is denotationally meaningful is if you can case on Type. Why? In Agda at least, any argument can be marked as [irrelevant](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=ReferenceManual.Irrelevance), not just type arguments. Those annotations have a big impact on the semantics of the language, since they affect which expressions are propositional equal to each other. And Agda doesn't have `casetype`, which is why we have to use [type codes](https://www.reddit.com/r/haskell/comments/7aqqxv/computing_with_impossible_types/dpcnqxh/) instead.
While his point may or may not have been valid, is irrelevant to the way he went about voicing it. Saying someone should be ashamed of their writing for missing a tiny, even of subjective importance (other commenter came with counter point), detail is not the way to go about anything - THAT is the toxic part of the original commenters comment. His follow up comments don't help, and the fact he went quiet (iirc) after the counter comment is indication enough of his trolling.
Just a small side note - although this is just a walkthrough for an experimental feature - NEVER store passwords in plain text in your database! It's so easy to do it correctly, you can even use the type system to help you making sure it's hashed. See [this](http://hackage.haskell.org/package/users-0.5.0.0/docs/Web-Users-Types.html#t:Password) for example.
[Leksah](http://leksah.org/) is an open source Haskell IDE written in Haskell. If you have time it would be cool if you could try the latest [Leksah binary installer for Windows](https://ci.appveyor.com/project/hamishmack/leksah/build/artifacts) and [let us know](https://github.com/leksah/leksah/issues) how you get on. Any problems or questions you have as someone new to both Haskell and Leksah would be interesting.
I mean doesn't that argument apply to a wide variety of lazy vs strict issues? E.g. `&lt;&gt;`, `&lt;*&gt;`. I thought it was pretty well established that particularly when it comes to the base libraries making things as defined as possible was the way to go. I also thought that when a choice has to be made you typically make the first argument lazy.
What? Pointing out rude behavior is fine. There is nothing presumptuous about pointing out when someone is being a jerk.
If `n` has type `Integer`, then `(2 * n) + 1` also has type `Integer`; no problems there. `sqrt`, however, has the type `Floating a =&gt; a -&gt; a`, meaning that it is a function `a -&gt; a`, but only as long as `a` is some sort of floating-point type like `Float` or `Double`. These types are said to have instances of the `Floating` typeclass. `Integer` doesn't have such an instance, so there is no function `sqrt :: Integer -&gt; Integer`; this is the second error. The first error is similar, `round` relying on a `RealFrac` typeclass which provides the `properFraction` function to separate the integral and non-integral parts of numbers (e.g. `properFraction 2.5 = (2,0.5)`). In fact, the first error is based on the flawed assumption that the expression `sqrt ((2 * n) + 1)` has type `Integer`, which the second error contradicts. You can fix both errors by converting from `Integer` to `Double` before using `sqrt`: expr :: Integer expr = round (sqrt (fromInteger ((2 * n) + 1)))
I don't think this comment was malicious, just mistaken since the Linux subsystem is not available in Windows 7 AFAIK
Thanks!
&gt;Is there a good reason it couldn't suggest turning on LambdaCase? Probably the fact that parse errors happen before suggestions about extensions. I believe `ghc` uses `happy`, which may explain some of the difficulty in refactoring.
But most extensions, and particularly `LambdaCase` shouldn't change *parsing* should they? Surely parsing should be roughly the same regardless, and checking for extensions should happen at a later stage.
Extensions can turn on and off features both in the lexer and the parser. See https://github.com/ghc/ghc/tree/master/compiler/parser /{lexer.x,parser.y}
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**parser** (master → 436b3ef)](https://github.com/ghc/ghc/tree/436b3ef01eb740a2a0818783ba94a62c4687b4fe/compiler/parser) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
The technical content was fine, but saying "You should be ashamed" and "how dare you", are ridiculous, just as the whole followup. After all this is just a blog post, not a university lecture. But yes, the best way to deal with such people is to ignore them, not to write a lengthy response. Context is also important. Linus often gets criticised for being a toxic expert, but I get the feeling he is right on every point. He only rants when people start pushing him and show real incompetence. Why should the linux kernel be welcoming for new developers? If your patch makes it to linus, better make sure you know exactly what you are doing (or expect criticism). If it has the possibility to break thousands of computers, or causes overhead for already busy maintainers, then don't expect to be handled with a silk glove. If it's a blog post, then I wouldn't care so much, and just ignore the haters.
That's not really the point. The point is that there is a very large syntactic superset of Haskell that contains almost all extensions and so we could, in principle, check for extensions usage after the parsing stage.
Given that that’s not the way GHC does it maybe there’s a good reason not to. But I agree, it seems like if your goal is ie better errors the it’s the way to go
We're at the type level, where everything terminates, so Scott doesn't have much to say. To me this issue arises because GHC lets you hang instances off things that are intensionally different that we believe, based on extensional reasoning, shouldn't be able to exist. There are plenty of places where the reasoning used by GHC is too coarse. Another example is that kind `Constraint` really IMHO should unify by bi-implication rather than use the simple intensional equality check we use today. Without that you need things like my `constraints` package. I'm not entirely sold on whether the existence of this hack is a good or a bad thing. On one hand, it's bad because we believe the kind should have a fixed number of elements. Having the example I've shown here gets in the way of adding eta-rules that prove every inhabitant of kind (a,b) is of the form '(,) x y for some x and some y. On the other hand, widening the gap further by opening all of the kinds allow you things like functor composition that can end in any kind, an easier reflection story, etc.
&gt; Given that that’s not the way GHC does it maybe there’s a good reason not to. That's why I asked this question! But no one has come up with such a good reason.
I’ve idly wanted a lazier `Ord Bool` for a while now, partly so I can use `&lt;=` as an “implies” operator. For that specific case I’d settle for a separate `implies` function or `==&gt;` operator, though.
I don't think WSL runs on Windows 7. I do use Emacs in WSL as my IDE but I'm on Windows 10.
More generally, there's also fromIntegral which works on more types, including both Int and Integer.
Most people have agreed that servant-auth can be considered as the well thought out, more mature relative of the ".Experimental.Auth" modules. In fact we plan on removing the experimental bit and documenting servant-auth thoroughly, including in the tutorial, so that we will have a pretty standard answer to auth that gives you a couple of auth schemes out of the box and that should support most if not all needs.
&gt; given the authors' credentials, they should have done this homework I don't know the two works you reference, so I have no idea whether the authors of the recent one knew about the older one, should or should not have known about it, and should or should not have cited it in their paper. But I suspect there probably exists a less passive-aggressive and more constructive way to formulate your comment. For example, maybe you do have an idea by now of how the two work relate, and you could propose a description of the correspondence here? Assuming the authors of the recent work didn't know about the older one, what is it that you would like to tell them about it? Which part of the new work would have been different if they were familiar with the old work, and which parts ended up being similar by chance or fatality?
Is there a reason for the failure to be at the lexer level? Lambda-case doesn't steal any valid syntax anyway, so it could just always be parsed into the final AST. Then - the AST traversal can fail if it encounters a `\case` node in the AST and the extension is disabled.
For Lambda-Case you’re probably right and it isn’t too hard to fix this. For other extensions like `FFI` that change what is considered a keyword it’s a bit more tricky.
They have a temporary work around hlint -XNoPatternSynonyms .
The author is right when he says that the type `forall a. [a] -&gt; [a]` doesn't tell you much, other that something is done to a list. What about these types: * `Request -&gt; SockAddr` * `(a -&gt; Bool) -&gt; [a] -&gt; ([a], [a])` * `(b -&gt; Maybe (a, b)) -&gt; (b -&gt; [a])` * `[IO a] -&gt; IO [a]` Each of these is from existing libraries and for each type there's usually only one implementation that naturally makes sense. I should be able to expect that `f : Request -&gt; SockAddr` actually uses the argument, rather than making a `SockAddr` up. So if naming conventions and variable names are good enough to make your intentions clear, so are types, which often have conventions of their own.
yea priceless comments. thank for mentioning .
I would like to add another argument, which I think could be applied for beginner. for me am I find mtl style is easier to grasp and work with than the Free style.
Try writing some instances by hand and see what happens. 
This is more nuanced than "pointing out when someone is being a jerk". There is a large difference between a) You are being a jerk in that comment b) You in general are toxic, in general and similarly between a) we all make mistakes, and one class of mistakes is the following b) i am part of the "good group" and there is this completely different group of toxic individuals And if you miss that distinction you _are_ presumptuous, and if you are not careful about communicating it when making criticism, you are being as much of a jerk as the one you are trying to criticise. The author here calls for empathy, yet shows no sign of empathy in the same instance. Is it still "fine" ? Question of definition (does "fine" imply "constructive"? etc). I can understand the author. Author has a point, even though I don't like how its presented. I merely try to give some explanation as to why this gets downvoted, and to why I don't consider this post particular constructive.
Author of the GHCJS/Miso post here. Did you get frustrated just be reading my post or did you try Miso and had the same first-hand experience? My expectations from tooling are generally much higher than what is acceptable for most of the Haskell community. Please take that into account. You'll probably need jsaddle to get you editor to compile your GHCJS code on-the-fly. I wasnt able to make it work. Neither was I able to make the GHCJS REPL work. Your best bet to get immediate results is to turn off Haskell-mode/intero and keep only Haskell syntax highlighting alive. In a separate terminal window try `stack build --fast --file-watch`. If your Miso code compiles correctly you'll get an index.html file in your `.stack-work/blah/build/blah` (figure out the blahs for your machine). Open that in your browser. Let me know if this works for you. I'll add it to the medium post for future readers. Use MisoString everywhere blindly, for now. Once you've got something working you will probably want to switch to Text at some places in the backend code.
I don't understand what exactly you are referencing in your first sentence.
The first two sentences from Lyon's responses.
`forall a. [a] -&gt; [a]` is still much better than having `Any -&gt; Any -&gt; Any` or even `Any` (yes, that could be a function). *A fitting analogy:* I can drive without a seatbelt on because it gives me so much freedom. But it's actually stupid because there is really no reason to sacrifice safety for something you don't need while driving.
Thank you. I think it reads much better https://github.com/katydid/katydid-haskell/commit/21d9fe86dfd309200d8299eec7b680b0032b60d5
How can you say someone is not being toxic if they comment that the author should be ashamed of themselves?
Is ignoring people saying bad things really the best way to deal with it? It is an issue in our community (IT in general I mean), not just this one case. We've been ignoring it since the 90s at least and IMO it helps to reduce diversity and make IT less accessible. I've found the Haskell community to be very welcoming in general but we do have a reputation for these toxic experts, even if it's not true. We also have a big problem with lack of diversity, both of these issues require action to fix them and are, I believe, related.
`--fast` is a great tip that I hadn't come across. One tip to find the build dir consistently is to use `stack path --stack-yaml=stack-client.yaml --local-install-root` -- noting that I'm using a different yaml there for my client. I've seen jsaddle mentioned as a workflow optimization but I haven't worked out what it's for or why it's needed - do you have a succinct explanation for it?
If it were about seat belts, the arguments would go like this: * I've worn seat belts in old cars. They hurt and restricted my movement. * What if I park the car? What if I need to turn around? This is much easier in a car that has no seat belts. * Seal belts reject certain kind of ~~programs~~ movements you can make. Can you sit with your back to the wheel with a seat belt on? Yeah, but it's much more difficult to do. * Here's an example: recently I was driving on the high-way, turned around to the back seat, REPLed into my smartphone and changed my Google password. I don't know any car with seat belts where you can do this so easily. * Of course you can always unplug your seat belt. But then again, I can just always tinker a hand-made seat belt into the seat. That's comparable, right? * You have to admit, seat belts have a cost. They make your car more not only more expensive, but also heavier! * My team has driven 7 years without seat belts and nothing bad happened so far, while I've seen car accidents with people who wore seat belts. * So let's not fight. Cars make sense with and without built-in seat belts.
I don't know about the order of constructors thing. That sounds weird. But this definitely can't work without an `Ord a` constraint on `Eq`. data Expr a where Eq :: Expr a -&gt; Expr a -&gt; Expr Bool B :: Bool -&gt; Expr Bool
Dislaimer: I have never use jsaddle. I think jsaddle strings switches to the right string-representations, depending on where the code is run (native vs. js). This means you can run the exact same code on both machines without modifying it.
Reflex just added some stuff to make the tooling far better. [Here's some docs on the new stuff](https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md), and [here's an example project](https://github.com/ElvishJerricco/reflex-project-skeleton). &gt; how to test the code and see it in the browser? This setup provides a number of ways to do this. The simple way, using the example project, is to just run `./cabal-ghcjs new-build all`, then point your browser at the build product in the `dist-ghcjs` folder. But you can also compile with GHC to get better dev iterations. Details are outlined in those links, but it's not complicated; you basically just compile with GHC instead and it works. &gt; could I use normal haskell type ( the one in the backend ) for the front end also( for example I read that I should use misoString instead of string). Sharing types between the frontend and backend is one of the most valuable parts of GHCJS. With Reflex, it's recommended just to use `Data.Text`, which is fairly commonplace anyway. As for editor integration, I use [Dante](https://github.com/jyp/dante) in Emacs. It's easy to configure it, but I'm *horrible* at Emacs Lisp, so [what I do is quite ugly](https://gist.github.com/ElvishJerricco/5c94e2a3b869969a583faf0170478a8a). But it works flawlessly; just open a file, launch dante, and you're golden. There seems to be some issues where the flycheck integration doesn't always work with the auto completion, so I usually leave completion turned off. But that doesn't bother me since I always leave a local hoogle server running, so I can just search for types or functions that I need in my browser. Hoogle comes out of the box in that example project; just run `nix-shell -A shells.ghc --run "hoogle serve --local --port=8080"` /u/saurabhnanda Is this an improvement on what you've seen before with Reflex? I'd love to try to fix whatever tooling issues remain for you.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → 292ae33)](https://github.com/reflex-frp/reflex-platform/blob/292ae334bdb9178245af96d26688df7bf3532849/docs/project-development.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dpdj9ux.)^.
Thanks! Will do that in prod. But unfortunately, this time, I forgot to create a user registration. 
Yes, I read that in servant-auth repo at github. Will use that for the next article about servant.
I'm not sure I get the monoid example. It seems to me that the author is arguing against typeclasses here, not static types per se. Which is a bit strange, since one can explicitly pass around a `(a, a -&gt; a -&gt; a)` tuple around in Haskell as well. Which should be pretty much obvious to someone who is experienced with Haskell as the author claims they are. The thing about linear types: &gt; It is also much easier in a language that does not abstract over its runtime. Both Haskell and Clojure are GC-d languages, and while Haskell is mostly compiled to native code, Clojure runs on the JVM. The author however claims that in Haskell it's somehow impossible to reason about the runtime characteristics of a program without linear types? This circles back to the argument "if you *truly* get Haskell there's no way you'd be able to live with dynamic types", but unfounded arguments like the ones presented in the article only perpetuate that stereotype.
Not really. GHCJS offers JSString, which is an actual JavaScript string, but this isn't used by default as the backing for anything. `String` and `Text` are both represented the same as they are on GHC. Reflex does [a bunch of crap](https://github.com/reflex-frp/reflex-platform/blob/develop/haskell-overlays/text-jsstring/default.nix) to support swindling the `text` library out to use JSString underneath, which can be a pretty big performance improvement, but obviously it requires overriding any package that depends on the internals of the `text` library (which is why that file is so long). It'd be nice if there were a standard way to switch this out. I guess this might be a good place for Backpack?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**default.nix** (develop → 292ae33)](https://github.com/reflex-frp/reflex-platform/blob/292ae334bdb9178245af96d26688df7bf3532849/haskell-overlays/text-jsstring/default.nix) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; Also, I'd be interested in learning more about category theory, and if possible, any other branch of mathematics through Haskell. Any resources? I have only started but I found [these](https://www.youtube.com/watch?v=I8LbkfSSR58&amp;list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_) quite enjoyable.
I try to reply to the technical content only (not the emotional content), and only if I feel they are genuine in their opinions. When I notice they are fixed in their ideas, just repeating their own views without trying to learn anything, or simply trolling, I just stop replying. If that gives the impression of being in an ivory tower, so be it. But otherwise it's a waste of energy that can be spend on teaching people who are genuine in learning new things.
Author here, I agree with you on this. I was trying to express that neither naming nor typing is enough to fully express intent, both are helpful, and both are negotiable in my opinion.
I mean... What's the intent of this? Reinforce your intuitions, beliefs and knowledge by mocking those that do not share it?
Hmm i was pretty convinced that was the reason. Thank you for your clarification. Yeah this sound like a perfekt application for backpack!
As much as I agree that some information is better than no information, the seatbelt analogy is just wrong.
&gt; The author however claims that in Haskell it's somehow impossible to reason about the runtime characteristics of a program without linear types? I think the author meant both these languages fail in that category. I don't know why there is so much X-"dynamic" language vs X-static language argument.. I thought it was reasonably well understood the trade-offs between safety of a type-system vs simplicity of "dynamic" languages.
Hi, I was not arguing against typeclasses at all, just trying to highlight some interesting trade-offs and properties. I acknowledge this was not well stated... By "abstracting over its runtime" I should have been more clear, this was too quickly stated without arguments related to that in the article, in hindsight I think this does not belong there. I agree this was not motivated anywhere in the article and I should retract it.
&gt; [..] the seatbelt analogy is just wrong. How am I supposed to answer to this? I really do not care for opinions. I care for why people think their opinion is true.
What is the recommended charting library? I have been using [Chart](https://hackage.haskell.org/package/Chart), but it always takes me a long time to make even a simple chart because the API is difficult. 
&gt; Yeah this sound like a perfekt application for backpack! Though, if a library is depending on the internals of `text`, then it likely wouldn't work with any standardizable backpack signature, leading back to the original problem =/
I'm simply trying to share my perspective with you and I'm sorry if you feel mocked by this. If it helps, I didn't have your blog post in mind when I wrote the comment, but some previous discussion on proggit. Still, we'll just have to agree to disagree on some things. What I'm trying to show is why I believe that there is not much of a *trade-off* in this debate, but rather a fundamental misunderstanding about the nature of types, that drives these conversations.
Burden of proof is on the person making the original claim: you claim there is an analogy. If you do not care about me accepting your analogy then really this is a dead end.
Right, both a helpful and negotiable. Luckily we don't have to choose between either of them. We can have both. And good documentation on top of that. In some languages you only have two out of the three. Whether it pays off to give up on types, is the central point of disagreement here.
Exactly! While I’m not sure how I’d do a duplicate, violating linearity by dropping is still problematic. Perhaps it’s safe in the context of their fusion framework? But I think it’s bad to violate soundness :) As far as I can tell, the simplest two approaches are to either make exceptions/effects transactional aborts, or use the linear logical external choice/with/&amp; type to recover the correct typing rules for exceptions. I do think that in the context of long running scientific computations where there are check points / snapshots, transactional abort based exceptions might be a good strategy since they shouldn’t happen ordinarily. But a better typing discipline may work too. 
Is this what you meant by adding a constraint, because it also has the same problem? ``` data Expr a where Eq :: (Ord a) =&gt; Expr a -&gt; Expr a -&gt; Expr Bool B :: Bool -&gt; Expr Bool deriving instance Eq a =&gt; Eq (Expr a) deriving instance Ord a =&gt; Ord (Expr a) deriving instance Show a =&gt; Show (Expr a) ```
They substantiated their analogy at least somewhat. Saying "it's just wrong" doesn't tell anyone *why* it's wrong and so doesn't really contribute to the conversation. What are they supposed to do? Explain it again? For the longest time, many people believed the emission theory of vision, where the light was emitted from your eyes. If the counter argument was simply "That's just wrong," rather than "We've produced the following evidence that disprove that: ..." it'd still be a leading theory. Yea, the burden of proof was on the emission theory, but that doesn't matter if no one is going to *dis*prove it.
As /u/jlimperg says, I don't want to post my solutions publically, but I am happy to share them privately if you send me an email.
A seat belt is a light weight tool that, if done right, provides reasonably safety for very little cost, and can always be unplugged if the user wishes to do that. While they could, theoretically, always be added later, having them tightly integrated provides several benefits. It allows for them to interact with other features, such as airbags and more optimized machine code.
Did you read the original exchange in the comments on this page? https://nedbatchelder.com/text/slowsgrows.html I did not "immediately" label them as toxic. I tried to have a civil discussion, even after they opened with "You should be ashamed."
&gt; Burden of proof is on the person making the original claim: you claim there is an analogy. I just wanted to know why you think my analogy is wrong. I'm not saying it is a perfect match. All the statements I make are open for discussion. In your opinion, shall I brute force through all analogies until you accept one? That's not how constructive discussions work. How is it that you are the authority of truth? &gt; If you do not care about me accepting your analogy then really this is a dead end. Then you're simply not interested in finding truth and I have no interest in discussing with you.
Right, that stuff is just bad. My "less than perfect" is a large understatement. I did not mean to imply that pyon's comments are an acceptable form of criticism. And pyon _still_ deserves empathy, too.
The part that I have a hard time accepting in this analogy is: * the life or death implication * the lightweight tool and little cost * the "easy opt-out" * the obvious cost / value ratio I agree with the added safety and the extra integrations and optimisations it offers.
I agree with the overall point of this post, which I understood as saying that static types have value, but they also have a cost. If you think that's a truism that doesn't really say anything at all, compare it to [Tool subsumption and silver bullets](https://brianmckenna.org/blog/tool_subsumption) by Brian McKenna: &gt; I've come to a similar conclusion on dynamic typing. It is exactly unityping and my experience is that allowing programs with more than one type has no inherent disadvantage. Not a trade-off. Brian is saying that static types either have no downsides or that the upsides obviously outweigh the downsides for every situation. Jean-Louis is saying that static types are beneficial, but maybe not all the time. As much as I like static types, I agree with Jean-Louis more. For example, if static types are obviously superior, why aren't dependent types even better still? For me, the answer is that Haskell's type system hits the sweet spot in terms of power-to-weight; dependent typing is too much overhead for not enough payoff. But I feel like the dependent typing crowd could "talk down" to the static typing crowd in the same way that the static typing crowd talks down to the dynamic typing crowd. What's the difference there? 
&gt;The author is right when he says that the type `forall a. [a] -&gt; [a]` doesn't tell you much That type says a *lot*. It says, for example, that the output list will only contain elements from the input list. In essence it's some kind of permute/duplicate/drop combination function.
IMO, Haskell-style static types will end up having been most useful as a trojan horse to get the industry to accept dependent types. The power-to-weight ratio is completely off compared to something like Hindley-Milner (with or without GADTs, with or without subtyping).
According to the [documentation for StandaloneDeriving](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#stand-alone-deriving-declarations) &gt; Unlike a `deriving` declaration attached to a data declaration, GHC does not restrict the form of the data type. Instead, GHC simply generates the appropriate boilerplate code for the specified class, and typechecks it. If there is a type error, it is your problem. That is, attached `deriving` clauses are guaranteed to type-check but they are only allowed on "vanilla" types, not GADTs, while standalone deriving declarations are allowed on GADTs but are not guaranteed to type-check. You were simply lucky that reordering the clauses happened to cause the generated code to type-check the second time. I really wish datatype-generic programming (the mechanism underlying `deriving`) worked for GADTs, but it doesn't. Datatype-generic programming works by treating your datatype as a bunch of binary sums and products, for which generic implementations like `(==)` and `compare` are much easier to generate, and then translating back from that generic representation to your datatype's actual representation. Vanilla types can easily be converted to that generic representation and back, but GADTs don't have the right shape. I recently [presented the problem](http://complogic.cs.mcgill.ca/workshop17/gelineau.html) to a roomful of programming language researchers, hoping that somebody knew of a paper proposing a more general generic representation, like dependent sums and products or something, but follow-up discussions didn't lead anywhere, so I guess this is still an open problem :(
It also seems to link to a digital copy of the textbook despite it appearing no such thing was ever officially released. I was hopeful this was a publicly released course, but it doesn't look like it.
&gt; please join me in canvassing the Haskell powers to adopt GHCJS as a first class citizen This is unlikely, and probably a bad course of action in the long run. Disclaimer: I have a huge bias, as one of the people working on getting GHC ported to WebAssembly. But the approach GHCJS takes is *not* one we should hope to standardize on. - GHCJS reimplements a lot of code. The RTS and several core libraries are reimplemented from scratch. There are shims for all the C sources in Hackage packages it supports. Reimplementing things is a pretty huge liability, in terms of bugs, security, and inconsistent behavior. - We get a lot of value from assuming a C-level machine in Haskell. Some packages rely on C code for performance or even binding to C libraries. In the long run, if we want to standardize GHCJS, authors have to either cut their reliance on C entirely, or be willing to do duplicate the work in JS. We can't keep having third party devs implementing shims for other packages, as it's is likely to go very wrong. - JS is just a terrible representation for Haskell / STG. Without the ability to control hardware down to the register level, performance really suffers. Registers are represented with global variables, nothing is ever *really* unboxed, `Int64` is a pair of floats, and the whole representation is actively hostile to the JIT (the backbone of JS performance). It's just not a good fit. Point being, I think GHCJS is more than enough to hold us over, and we should continue to support it as long as we need it, but WebAssembly is the proper path forward. We've got enough baggage attempting to support old crap like Hugs; I wouldn't want to see another obsolete compiler added to the list.
You can rather add a singleton in the `Eq` case, because when comparing `Eq a b == Eq a' b'` we don't know that `a` has the same type as `a'` import Type.Reflection data Expr a where Eq :: TypeRep a -&gt; Expr a -&gt; Expr a -&gt; Expr Bool B :: Bool -&gt; Expr Bool instance Eq (Expr a) where (==) :: Expr a -&gt; Expr a -&gt; Expr Bool Eq (tr :: TypeRep xx) a b == Eq (tr' :: TypeRep xx') a' b' | Just HRefl &lt;- eqTypeRep tr tr' = all [a == a', b == b'] | otherwise = False B bool == B bool' = bool == bool'
`LambdaCase` couldn't exist without changing how it's parsed. &gt; Surely parsing should be roughly the same regardless, and checking for extensions should happen at a later stage. That's pretty implausible. 
I just wanna know. I think everyone takes for granted that Haskell has a denotational semantics that's more or less Scott domains. It wasn't clear before but it was plausible. Now I'm really wondering what these spaces look like.
This looks like a lot of typing. I intend to have 10s of Expr constructors. This is the reason I want deriving to work, to save me from all this error prone work. Currently it seems like a tradeoff. I can either do more typing and use an ADT with deriving. In this case I have very readable code. Or I can do more typing and use GADTs with Reflection and no deriving and then I have code that might not be so easy to read for a beginner. Or am I missing something?
Thank you. This is sad, but very useful information. I guess I'll stick to using ADTs over GADTs for my personal data declarations.
This is a shallow false equivalency. Following arbitrary ideas about how to stay neutral on the internet is not the same as being civil. An important part of being a constructive member of an community is pointing out when someone is/was out of line. Following some invented code of decorum while doing so is optional. In the source material, pyron was out of line. As far as linking it on this subreddit... That action seems to be of debatable merit.
If you can make the index to your GADT be a singleton type, then you can rely on [exinst](https://hackage.haskell.org/package/exinst-0.4) to derive all of the boilerplate instances for you. If you are interested, /u/isovector described `exinst`'s approach in his talk [Some1 Like You](https://www.youtube.com/watch?v=PNkoUv74JQU)
Could you work with an Expr that has fewer constructors? For example: data Expr op type where Val :: Val a -&gt; Expr () Unary :: UnOp op a b -&gt; Expr o a -&gt; Expr op b Binary :: BinOp op a b c -&gt; Expr o1 a -&gt; Expr o2 b -&gt; Ecpr op c Where Val UnOp and BinOp are simpler GADTs used for tagging.
I'm trying to do something like this: https://www.reddit.com/r/haskell/comments/5bnr6b/mocking_in_haskell/d9pvguj/ class Monad m =&gt; MyDB m where getUserIds :: m [Int] getUsername :: Int -&gt; m String instance MyDB (ReaderT DBConnection IO) where ... instance MyDB (State MockState) where ... But one thing I don't really understand is how to do error handling and such. Would I do something like: instance MyDB (ExceptT DBConnection IO) where ... Or even class (MonadError SomeException m, Monad m) =&gt; MyDB m where But then how can I compose the type of exception if I have an instance that uses the State Monad and an instance which uses the IO Monad? Anything which helps either directly answer my question or give me more grounding with monad transformers would be really appreciated. 
It sounds like your install is broken because it doesn't include all the necessary libraries. Try downloading version 8.2.1 from the downloads page of haskell.org. 
I actually find the development in our corner of the PL world quite interesting: - HM (+ type classes) is a super simple system with very nice properties (mainly complete, efficient type inference). In terms of power-to-weight, I actually think it's hard to do much better. But of course, you'd like to express a lot more properties than it lets you. - GHC then bolts on a lot of things to solve this problem, allowing significantly more invariants to be encoded into the types, but also introducing significant complexity. In the process, a lot of the more advanced type-level programming ends up feeling like hacks stacked on top of more hacks. - Enter dependent types, with the promise to give us a very nice simple system again (and much more power to boot), but with some rough edges lurking in the background: termination checking and a need to differentiate between induction and coinduction (if you want consistency); type inference becomes yet more unpredictable; integrating type classes is less straightforward; everything to do with equality; new questions around performance and computationally irrelevant information. I'm very curious to see where all this is going and if dependent types actually catch on outside the ivory tower at some point. Similar to how some people view static types in general as too high cost for too little gain (a perspective I don't share at all), I could see myself making an argument that the same applies to dependent types, or even GHC's stuff, compared to Haskell2010.
I got lost on the same line "deduplicating = recur (the (a -&gt; Bool) (const True)) where". What is "the", it's not defined anywhere. Is it a builtin idris function ?
&gt; as one of the people working on getting GHC ported to WebAssembly. Oooh, this sound interesting. Is there anywhere I can read more about this effort and current status? (Blog, wiki, or somthing.)
Is there a way to download these videos to play them with VLC on 2x speed?
It started as Michael Vogelsang's Haskell summer of code project. You can read the end of term report [here](https://webghc.github.io/2017/08/10/hsocwrapup.html). The bulk of the work has been in getting a C cross compilation toolchain for it, which we've been doing in Nix [here](https://github.com/WebGHC/wasm-cross). I'm currently working on getting our Nix infrastructure updated to the new cross compilation stuff and starting to implement C syscalls so we can actually demo "hello world" rather than just Fibonacci. Michael is working on implementing GHC's [Adjuster.c](https://github.com/ghc/ghc/blob/master/rts/Adjustor.c) for WebAssembly so we don't have to port libffi and disabling the LLVM mangler since the work it does is irrelevant to WebAssembly right now and Clang doesn't have an assembly language for it yet. Sorry we don't have a clear process in place for tracking progress. We should probably start using github issues or projects on the wasm-cross repo to track this stuff.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**Adjustor.c** (master → 436b3ef)](https://github.com/ghc/ghc/blob/436b3ef01eb740a2a0818783ba94a62c4687b4fe/rts/Adjustor.c) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; And please join me in canvassing the Haskell powers to adopt GHCJS as a first class citizen. I have to agree with Elvish here I don't think GHCJS should be a first class citizen. I remember when GHC used to compile to C it was not uncommon that people had to know both C and Haskell to write good libraries. JS is much further away. Writing another mini compiler entirely with its own libraries implemented in JavaScript makes sense. But then why not just create a JavaScript system that uses a Haskell like syntax specifically designed for browsers and tightly tied to browser standards? Which is essentially Elm. One of the big concepts in functional programming is casually creating DSLs rather than one language to rule them all. Why not let web interfaces be a DSL? 
Right. In fact I would not have had nearly as much patience as you had there. Unfortunately this is hard to grasp from the "toxic expert" post, and you do put a label nonetheless. These kind of discussions are really hard. One is to tolerate random trolls while not discarding potentially valid criticism. One should remain constructive in the presence of such unnecessary, careless, negative feedback. And here I come along and criticise your analysis of a certainly valid problem. I fear the unfortunate truth is that the "toxic export" post does not add enough for the haskell community, or to a discussion to be had around this. Anonymous people behave badly? Nothing new. An analysis of where these people might come from? Won't make them change especially if you label them as bad upfront. And a call for empathy while putting a label on others. I know that in some sense, me criticising you is _just as non-constructive and free of empathy_. I know. Not happy about it. Is why I stressed the "not morally superior" point. There is a clash of trying to give feedback when somethings gets downvoted and not being a jerk myself. So let me name some positive things, too: I like the Big-O post, and I see nothing wrong with it. It is very understandable and gets the main idea across. Of course there are more rigorous approaches where you mathematically define all that stuff and perhaps are more precise, but the achievement is approaching a subject in a relatively short post while not oversimplifying, and the post manages that. You had a right to call out peon publicly for such behaviour. Of course ignoring such comments might be advisable, it is you who is attacked and you may respond. I don't mean to say you should not have written it or published "toxic experts". It getting downvoted and criticized feels bad, probably. But those downvotes are not important at all. I don't think this community approves of peons behaviour. And at least me personally does not wish to simply tell you: "Just put up with the troll and ignore, best approach". Easy to say. Nah, I get attacked publicly, I respond publicly. Not everything I say needs to be considered best for general readership. 
The competitor of WebAssembly, [asm.js](http://asmjs.org/), seems to be much more mature. Until WebAssembly catches up, while we're still on the GHCJS approach, could compiling to asm.js, possibly via LLVM, be an option?
Sure, asm.js is more mature than Web assembly. But it's a predecessor, not a competitor. The state of webassembly will be better than asm.js very soon. 
It is not optional if you wish to claim the moral highground in the way that the post tries to do. Also you seem to imply that I am opposed to pointing out when someone is being a jerk. I am not opposed to that at all.
I don't think anyone, even the creators of asm.js or Emscripten, consider asm.js a competitor. I think it's pretty universally agreed upon that WebAssembly will obsolete asm.js. Plus asm.js isn't actually optimized in some browsers like Safari (and barely Chrome), so it often won't be very good. But to answer your question, compiling to WebAssembly and compiling to asm.js are basically the same problem. In fact, there are even tools for taking asm.js code and converting it to WebAssembly, as well as vice versa. The plan for WebAssembly is to use the LLVM backend in GHC to go through LLVM's WebAssembly backend. So getting WebAssembly working is basically just going to be flipping a switch, followed by fixing tons of little things that break one after the other until it works. This is pretty much exactly what you would do if you wanted to port GHC to asm.js, except you would have to depend on Emscripten's fork of LLVM instead of the experimental WebAssembly backend that exists upstream in LLVM proper. Now, the reason asm.js seems more mature is largely because of the Emscripten toolchain, which does actually target WebAssembly almost as well. It has implemented a variety of libraries (most importantly, a version of `libc`), and linked them into the browser APIs. The thing is, Emscripten's toolchain is kind of garbage. It's not usually trvial to port the build process of an existing C project over to Emscripten because it's very parasitic. It wants you to use its version of make, a special `./configure` wrapper, probably wouldn't work with cmake at all, and does a terrible job of operating anything like a normal cross compilation toolchain. It did not seem like it was going to be reasonable to get this working with GHC. So we chose to just pluck it's library implementations as needed, and stick with the upstream LLVM backend and LLD ports. Using John Ericson's work on cross compilation in Nixpkgs, it's been surprisingly easy to just do a *proper, ordinary cross compilation toolchain,* that works for WebAssembly in Nix (in fact, I was surprised to find that it took almost no effort to make the same Nix toolchain work for a variety of LLVM targets). We've plucked Emscripten's `libc`, but we haven't yet plucked their syscall implementations (some of which I'm sure won't be good fits for GHC). The plan is to get everything, including a few of those parts of Emscripten that make asm.js seem more mature, working in Nix, and then figure out a way to distribute stuff and make is usable without Nix.
&gt; LambdaCase couldn't exist without changing how it's parsed. Sure it can. It just can't be *accepted* without the extension. &gt; &gt; Surely parsing should be roughly the same regardless, and checking for extensions should happen at a later stage. &gt; That's pretty implausible. What's wrong with this recipe: Take some maximal mutually compatible subset of extensions. Parse the syntax for that. Then reject the code if any of the extensions required for it aren't actually turned on.
At least that has a *chance* of deriving `Ord`. You can't even hand-write an instance without that additional constraint. Whether or not you can deriving `Ord` in practice is another matter ...
Oh yeah, good point!
&gt; I feel like the dependent typing crowd could "talk down" to the static typing crowd You feel like it? Surely it should be either an observed fact or not.
Thank you very much for your answer, I like FRP and quite interested in doing something with reflex but I am afraid I need to do the front end as soon as possible thats why I have chosen Miso, the thing that TEA is very easy and you can get something done with a small amount of time - especially that I have used ELM before- but I don t want to use ELM again since I would like to share my types with the front end and I don't want to rewrite it again ... however, while checking this [github](https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md) I have seen that there is a build for android! does that mean that it generate .APK file?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → 292ae33)](https://github.com/reflex-frp/reflex-platform/blob/292ae334bdb9178245af96d26688df7bf3532849/docs/project-development.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dpdv53t.)^.
Hmmm, well. I'm not very convinced about by arguments of the form "types make your life harder" because I know that types make my life easier. Please tell me *something else* that would also make my life easier and then prove to me that it's incompatible with types. From this Clojure vs Haskell debate we're having at the moment the *something else*s I've discovered are: * Heterogeneous dictionaries * Extremely powerful REPL * EDN none of which I see as being particularly incompatible with types, but I'm open to persuasion.
I think you can use this class Monad m =&gt; MyDB m where getUserIds :: m [Int] getUsername :: Int → m String instance MyDB m =&gt; MyDB (ExceptT e m) where getUserIds = lift getUserIds getUsername = lift . getUsername Note `ExceptT e m` requires two arguments: the type of exceptions to throw, and a monad type constructor, so `ExceptT DBConnection IO` is probably wrong. If you have instance MyDB (ReaderT DBConnection IO) where ... then you probably want types like `ExceptT DBError (ReaderT DBConnection IO)` (assuming `DBError` and `DBConnection` are concrete data types) 
&gt; however, while checking this github I have seen that there is a build for android! does that mean that it generate .APK file? Yep! And FWIW, my personal opinion (and experience) is that Reflex makes it quite easy to get things done in the frontend extremely quickly. Of course, this could just be my bias talking, as I'm a pretty heavy Reflex user, and know a lot of the ins and outs.
&gt; however, while checking this github I have seen that there is a build for android! does that mean that it generate .APK file? Yep! And FWIW, my personal opinion (and experience) is that Reflex makes it quite easy to get things done in the frontend extremely quickly. Of course, this could just be my bias talking, as I'm a pretty heavy Reflex user, and know a lot of the ins and outs.
I usually leave `ExceptT`/`MaybeT` out of my core stack and add it on in small scopes as necessary. This is because their short-circuiting behavior is the thing that is most likely to result in unexpected behavior when it is composed. So using your example, my preference would be to operate in `ReaderT Env IO` most of the time and use `runExceptT` to jump into `ExceptT Error (ReaderT Env IO)` when you need it.
My point wasn't that dependent typers are more or less snooty towards static typers than static typers are to dynamic typers. My point was that dependent typers could use the same type of rhetoric on static typers that static typers use on dynamic typers. And that point meant to suggest a scenario like this: A proponent of dependent typing comes into this thread and says: "Sure, static types are seat belts, but dependent types are five point harnesses. It's stupid to wear seat belts because five point harnesses are always safer and not much more restricting." How would you feel? How would you react? 
Very cool, thanks!
&gt; In general it's considered a bad idea to use ExceptT and IO in one monad stack. That blog post is a pretty controversial opinion piece and I would even go so far as to say that the community as a whole leans somewhat in the other direction. Preferring `ExceptT` or similar for synchronous exceptions. Regardless it is misleading to tell someone newer to Haskell that its a bad idea to use `ExceptT IO`. I mean just look at one of the most liked and used backend libraries: Servant. Handler is a newtype wrapper over `ExceptT ServantErr IO`.
Since I am very new to this, I think I need to see definitions of UnOp and BinOp. I couldn't get it going with my first attempt.
If you want transparent interoperability between front and back-end you can use Axiom https://github.com/transient-haskell/axiom
I would recommend against Reflex. It's too complex, while **seemingly** cool. The benefits of all that complexity are unclear to me. What is easier to build (and maintain) in Reflex, that isn't in Miso or Elm or Halogen (Purescript)?
Probably the biggest thing is local reasoning. With Elm-style things, you're stuck making all of your state global, and reacting to events by writing your own event dispatch. Reflex gives you the power to do this, while also giving you the power to encapsulate. I think most of the complexity comes from a like of guidelines on using Reflex. Admittedly, the fact that such guidelines are baked into the type system with Elm is a big point in its favor, but I've found myself much happier with the level of control I get with Reflex, without sacrificing a denotational purity. Honestly I think Reflex just needs a couple of blog posts or type classes for common design patterns, and the issues of complexity would begin to melt away. People have a hard time with complexity with it because e.g. it's just not clear that you shouldn't be mixing logic and presentation or how you would avoid it.
Thanks for clarifying :)
I was convinced, which is why I know my way around Idris. :P
In my opinion the Haskell type system is excellent and you can use dynamic types with Dynamic. The problem is the ideology that this generates. Many Haskellers have grown with the idea that functional programming is just about doing things with static types and/because CT is about types. Full stop. Real functional programming is put aside.
Try writing a general-purpose type-safe job-queue using Haskell's type system.
I closed the article at the headline "Convention dominates Information", which is counter to my experience and a philosophy common to many of the tools that continue to drain my time because I cannot avoid them.
&gt; I also thought that when a choice has to be made you typically make the first argument lazy. No, if you have to be strict in some arguments, you make the function strict the the leftmost argument, because that sometimes allows partial applications to be partially evaluated. At least that's what we did for `(||)` and `(&amp;&amp;)`. And, yes, I believe that the standard library is supposed to be as defined as reasonable.
Oh I see. Well yes, if you think the argument is "more static verification is always better" then it's equally wrong when directed from Haskellers to dynamic language programmers as it would be when directed from Agda users to Haskell users. But as you say yourself it's not about absolute verification but a sweet spot in the power-to-weight ratio. Maybe I completely misunderstood what you were getting at.
I'm not sure what that is, but why would it have to be type-safe, and even if it does, what's the impediment to writing one?
Ok wow. I watched the video, but thats a little bit above my noob haskell skills. So many pragmas.
I agree that it's not really a question of trade-offs, but rather of misunderstandings. I think the misunderstandings cut both ways, though. While I don't have much experience as a Lisp programmer, I do believe that the reputation for liberating power of Lisp-like languages is based in reality. It's just a totally different approach, requiring a different kind of thinking. I'm interested to see what where [hackett](https://github.com/lexi-lambda/hackett) will lead.
That's not how it works. `hSetEncoding` is a way to tell haskell what encoding the handle is using so that Haskell knows how to handle it. Basically, you' re telling Haskell "Hey, all content coming in through stdin is encoded by the terminal as utf8", which is of course wrong and can only end with a horrible disaster. Secondly, as far as I can tell, there is no way to alter the text encoding of the terminal from inside a program. You _can_ set the default character set though, which would prevent you from changing it every time. See https://stackoverflow.com/questions/14109024/how-to-make-unicode-charset-in-cmd-exe-by-default .
I believe the following should allow you to derive your instances: data EqTy a data Expr a where Eq :: Eq a =&gt; Expr a -&gt; Expr a -&gt; Expr (EqTy a) B :: Bool -&gt; Expr Bool Note that this solution does not hold up if you want to add something like an if-expression: If :: Expr (EqTy a) -&gt; Expr b -&gt; Expr b -&gt; Expr b 
I see, thanks! 
Here is the [link](https://www.haskell.org/ghc/download_ghc_8_2_1.html#openbsd_61_x86_64). But note that this download is for 64-bit OpenBSD. Is your OpenBSD 32-bit only? Sounds like it may be, from your `uname` output. If so, I'm not sure if that platform is supported. Ask on the [ghc users mailing list](https://mail.haskell.org/cgi-bin/mailman/listinfo/glasgow-haskell-users).
Oops, yeah that is what I meant to say.
Right, it's about the sweet spot. That's why I think posts like Brian's miss the mark, even though I want to agree with them. 
It means that we should strive for the text library to be optimized for the actual distribution of natural language text among world languages. This assumes that at steady state we aspire for Haskell to be a global programming language, even if at the moment its usage may be somewhat skewed towards western-centric programmers and applications.
This is [exactly what happens](https://github.com/ghc/ghc/blob/436b3ef01eb740a2a0818783ba94a62c4687b4fe/compiler/parser/Lexer.x#L1218-L1224) for this case. The lambda case token only gets produced by the lexer if that extension is enabled.
The type `[a] -&gt; [a]` tells us that something is done to a list *but not to its elements*. It's a purely structural list operation. That's actually a lot of information for ten characters—especially since it's machine checked. Other types can tell us even more. There is only one thing `(a -&gt; b) -&gt; [a] -&gt; [b]` can *reasonably* be. Ditto for `[a] -&gt; [b] -&gt; [(a, b)]` and `[(a, b)] -&gt; ([a], [b])`. The reason I bring up these examples is that, unlike yours, we didn't even need to bring in concrete types with specific semantics (`Bool`, `Maybe`, `IO`)—we can understand *a lot* just from signatures that are polymorphic over list elements.
I see, thanks! 
I'm guessing that you tried and didn't succeed?
&gt; This is an intersting bit of trade-off: the higher abstraction you use (and therefore the fewer assumptions you make), the more general you make your code and the more difficult it is to reason about it without context. I think Ed Kmett would argue the opposite; the more general the types--provided the type classes have laws--the more you can reason about code with less care about the specific type in question. 
OMG!! this this is astonishing, I never imagined that I can write Haskell code and get a mobile app as outcome. it is awesome. however, any success story in that approach? is it resilient?
agreed and I will write a blog post and share my experience as soon as done.
[Elm](http://elm-lang.org) is almost Haskell on the frontend
Is this like, some hidden feature or what :) ? Every here and now I see something about this being possible, yet I'm not sure where would I even start if I wanted to check it out? Is it because the process is not yet fleshed out so it's just not documented?
I know of at least one company using these mobile apps in production. I will say that the process of getting this stuff through Apple and Google’s respective infrastructure can be quite difficult, but that’s true of any non-standard toolchain, not just GHC. As for how well the apps work, it’s pretty flawless. Integrating with native APIs like push notifications is a huge pain, but it’s totally doable if you get comfortable with Haskell’s C FFI
&gt; she started picking on me (along with all other white men), trying to depict us as bad/evil, she lost all my attention and I feel insulted to be honest. Not at all what I was hoping for. This isn't what privilege is about. In fact, it is about the exact opposite. It is about identifying power structures that exist in society and how they affect people's mindsets, behaviours, and life outcomes. For something to be in the realm of morality one must be able to do something about it; ought implies can. Individual white people purely from the act of being white are not responsible for racial privilege; therefore, they are not morally responsible for it and cannot be considered bad or evil according to privilege theory. Furthermore, being white is not a choice; therefore, it doesn't even fall into the category of moral discourse. The whole point of this theory is to blame the system not the people.
It doesn't present them in a derogatory way. You just misinterpreted.
That belief is incorrect. The whole point of having these theories is to understand that people's actions aren't just a result of personal moral failings but a consequence of various social systems they interact with. 
&gt; During those obligatory talks, and in emails we would get, they always presented the same one-sided point of view. Which is, that white people owe something to the rest of the world and that men owe something to women. This asymmetry always irritated me. I could also see these sentiments shared by people I met in those days. More often than not, just disagreeing with them was a serious faux pax causing awkward silence. Some people stopped saying hi to me at work simply because I questioned some of these ideas. I think you have misinterpreted these concepts and then people have misinterpreted you by believing that you rejected these concepts after correctly interpreting them. The ideas I think you are referring to are about critiquing sociological systems not people. Patriarchy in particular is harmful to both men and women. Patriarchy theory is as follows: 1. There are two concepts: masculinity and femininity 2. Masculinity is considered "better," "stronger," and "more logical" than femininity 3. Masculinity is associated with men and femininity is associated with women.
&gt; The sentencing gap between men and women (how long of a sentence one gets for identical crimes) is six times larger than it is between blacks and whites. This is a consequence of patriarchy as well. You have made the mistaken assumption that patriarchy only affects women negatively. You have a very simplistic straw man understanding of patriarchy theory and other privilege theories. 
Whole heatedly agree. I've been trying to form my thoughts on this into something cogent and understandable. I haven't gotten there yet.
Fair point. It just didn't stand out as much for me as it obviously did for the OP.
/u/kwaleko, Author here. Most miso code is pure and testable. You only have to rely on miso's diffing algorithm and it's event delegation code being correct. /u/fptje and /u/cocreature have worked out all of the issues here, and are using it to great benefit. If you choose miso, everything can be shared. A good example of this is: https://github.com/dmjio/miso/blob/master/examples/haskell-miso.org/shared/Common.hs#L21 I agree tooling could be improved, have considered writing some tools to help with miso development. Similar to elm-reactor. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [dmjio/miso/.../**Common.hs#L21** (master → 2340394)](https://github.com/dmjio/miso/blob/2340394d90420b20a11bd87ce4db7b8537e9384d/examples/haskell-miso.org/shared/Common.hs#L21) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
If you only want to use ghci you can use a .ghci file that sets the utf on startup using `:! chcp 65001`. 
&gt; Use MisoString everywhere blindly, for now. Once you've got something working you will probably want to switch to Text at some places in the backend code. You won't have to worry about this `MisoString` *is* `Text` on the backend.
This is why miso was created pretty much.
You'll still have to write Javascript to manipulate the DOM, talk to Web APIs, and shuffle data into your wasm library. At least until WebAssembly figures out imports/exports. Until then running your Haskell code in wasm is more like embedding GHC into a runtime rather than being a first-class citizen. I think GHCJS will have its place until the Web figures that part out. 
Well, yes and no. It is true that there is currently no design for interacting with the DOM directly. But imports/exports is actually already figured out. A WebAssembly module can import arbitrary JS functions, and JS can call functions exported from WebAssembly. The limitation is that this "FFI" only supports numbers, I believe. But it's not so bad to have shims that interpret numbers as indexes to the linear memory from the JS side, letting it convert that memory to JS strings or something. This is actually how `jsaddle` does it on native platforms like iOS. `jsaddle` has given us an *excellent* layer of abstraction over the DOM. The entire browser API, spec'd by WebIDL, is implemented by `jsaddle`, allowing it to provide arbitrary implementations. On mobile platforms, a webview does the JS calls, and whenever native Haskell needs to call a JS API, it just sends a little JS snippet to run. This is how GHC on WebAssembly will likely do it; we'll just implement a new `jsaddle` backend that does the shimming for WebAssembly via imports. And I'll tell you this: The shimming approach with `jsaddle` is still *shockingly* faster than GHCJS, at least with actual native Haskell on mobile. Lowering the barrier even further with direct FFI calls should be even faster, if WebAssembly is anywhere close to native speeds. **TL;DR**: There is a plan in place for this stuff. The idea that it's hard to manipulate browser APIs from WebAs
&gt;For example, if static types are obviously superior, why aren't dependent types even better still? Well flying is obviously better than driving a car. Why do people not abandon cars and everyone buys themselves and airplane? You cannot ask questions like that simply because there's no fully dependent type language that can compete on the tooling and performance with haskell yet, (or maybe never). As for the why (dynamic languages) that answer is much simpler than the differences between static and dynamic typing. Dynamic languages are like moss of programming. They are so simple that they grow everywhere.
I would say Great! Give me that harness. What? It only compiles to javascript? Its performance is on 2 orders of magnitude worse? It has not libraries that I need? In other words what use is the 5 point harness if only ants can wear it? 
&gt; Like Object oriented programmers create objects, Haskellers deifne types well before understanding the problem globally. Wait what? One of those things is not like the other :) 
I'll take the contrarian view on this. &gt; This is unlikely (GHCJS is not super well maintained), and probably a bad course of action in the long run. Disclaimer: I have a huge bias, as one of the people working on getting GHC ported to WebAssembly. I'm guessing that WebAssembly will be a worse solution than Javascript for most situations. &gt; &gt; GHCJS reimplements a lot of code. The RTS and several core libraries are reimplemented from scratch. There are shims for all the C sources in Hackage packages it supports. Reimplementing things is a pretty huge liability, in terms of bugs, security, and inconsistent behavior. OTOH, the GHC RTS shouldn't have such a large C footprint in the first place. This isn't really GHCJSs fault, and the correct thing to do would be to move more of the RTS into Haskell. &gt; Not only does it splinter a lot of libraries, but the compiler itself is a splinter. GHCJS imports GHC as a library and calls GHC to do stuff. This is a good model for something external, but it is one of the reasons that GHCJS is still not updated to 8.2; it's just a diverged code base, meaning updates can be hard. We'd really want GHCJS to be built into GHC proper, but that's a very questionable idea because of the other points in this list. &gt; We get a lot of value from assuming a C-level machine in Haskell. Some packages rely on C code for performance or even binding to C libraries. In the long run, if we want to standardize GHCJS, authors have to either cut their reliance on C entirely, or be willing to do duplicate the work in JS. We can't keep having third party devs implementing shims for other packages, as it's is likely to go very wrong. I think this point includes some huge assumptions. What is often the main point of using GHCJS is integrating other Javascript libraries. It's quite irrelevant whether some library is using C code for performance when what we want is the ability to share heap objects between haskell and some fancy UI library (like Facebook's React Native). It is quite natural that some packages will not build under GHCJS. This is exactly the same situation as a package that includes assembly versions of primitives. Assembly primitivies is typically the only semantically valid ways of implementing crypto for example without relying on undefined behavior to avoid (typically timing) attacks on the implementation, and crypto is one of the major uses of the C FFI - my point is that it's probably flawed to use C anyways. It is doubly flawed in a web context. What WebAssembly enables is to reuse deeply flawed (security wise) C libraries in web contexts. It's highly unlikely that that's a good idea compared to focusing on reusing npm modules where the abstraction (Javascript) is a lot less flawed from a security standpoint. &gt; JS is just a terrible representation for Haskell / STG. Without the ability to control hardware down to the register level, performance really suffers. Registers are represented with global variables, nothing is ever really unboxed, Int64 is a pair of floats, and the whole representation is actively hostile to the JIT (the backbone of JS performance). It's just not a good fit. &gt; It's a good fit if you want to share the heap with javascript libraries. I don't see why unboxing can't work. There's no need to use Int64 when Double works fine, so why would anyone do that? Your arguments just seem to come from an assumption that the whole point of GHCJS *is to solve exactly the same problems, but running in node*. That's not the point of GHCJS. The point is to solve *new problems* like: - Run native apps on Android and iOS. GHCJS can do that out of the box by wrapping the app in one of the existing Javascript-based frameworks. With WebAssembly the heap objects are not shared between my app and, say React Native, which means I need to write ugly code with lots of FFI and gymnastics to make performance likely on par with what (the slow) GHCJS gives me. - Write some code that controls a well-known javascript library on the web. Again, marshalling JSON data in and out of the flat memory space will dwarf any speedup I get from a faster Int64 implementation. Again to avoid this I'll likely have to resort to hacks. - Integrate with the 200k npm libraries seamlessly. - etc. I just don't see the huge need for PNaCl in the browser, and the market already decided that - nobody wanted NaCl nor PNaCl as they solved problems few people had. What is needed is seamless integration at the heap level with Javascript, as Javascript is the world's #1 programming language. &gt; Point being, I think GHCJS is more than enough to hold us over, and we should continue to support it as long as we need it, but WebAssembly is the proper path forward. WebAssembly supports C so that we can use all the normal stuff (even the RTS), LLVM will give us much better control over the hardware through WebAssembly, and it would be just like a typical backend for GHC. 
Is it correctly understood that the reason, foo11 = unsafePerformIO $ do print "yo" return 11 only prints `"yo"` on the first call is because it has now been evaluated and is therefore memoized/cached for future calls, leaving only the evaluation of `foo11` behind, which is `11`?.
Update: I plan on closing the survey at the one-week mark (November 8) instead. Submissions have slowed *way* down, and I've already received more responses than I expected. So if you were waiting to fill it out, please do so sooner rather than later :) 
&gt; I'm guessing that WebAssembly will be a worse solution than Javascript for most situations. Why would you guess that? I outlined [in another comment](https://www.reddit.com/r/haskell/comments/7ax2ji/haskell_on_the_front_end/dpect66/) why it's probably going to be better than GHCJS at pretty much everything. &gt; OTOH, the GHC RTS shouldn't have such a large C footprint in the first place. This isn't really GHCJSs fault, and the correct thing to do would be to move more of the RTS into Haskell. I am unaware of any part of the RTS that could be done in Haskell instead. C is almost always only used out of necessity in the Haskell community. There are some examples of using it for performance, and although it's probably possible to replace these instances with Haskell, it's often extremely nontrivial. Integrating with existing C libraries is a big deal too. Like you could run tensorflow by binding to the C++ library. &gt; what we want is the ability to share heap objects between haskell and some fancy UI library Integrating with JS APIs from WebAssembly is pretty easy. If we can come up with a nice tool for specifying 3rd party APIs much like WebIDL, we'll be able to do for those what `jsaddle` does for WebIDL pretty easily. I think that'd cover this concern fairly well. &gt; It's a good fit if you want to share the heap with javascript libraries Which is fairly possible with WebAssembly. &gt; I don't see why unboxing can't work. The semantics of JS require numbers to be boxed until the JIT can make unboxed versions of functions. GHCJS is actively hostile to the JIT, making numbers frequently boxed and indirect. &gt; There's no need to use Int64 when Double works fine, so why would anyone do that? People do use `Int64`, and `Double` does not cover the same range of numbers. That said, you're right that this is a minor concern. It was mostly just a random example of one of the many things JS is bad at representing. &gt; Your arguments just seem to come from an assumption that the whole point of GHCJS is to solve exactly the same problems, but running in node. That's not the point of GHCJS. I'll grant you that GHCJS needs to solve different problems, but that doesn't mean these things aren't also problems. Performance and library support have been two of Haskell's biggest challenges, historically, so they can't be ignored. &gt; Run native apps on Android and iOS. GHCJS can do that out of the box by wrapping the app in one of the existing Javascript-based frameworks. With WebAssembly the heap objects are not shared between my app and, say React Native, which means I need to write ugly code with lots of FFI and gymnastics to make performance likely on par with what (the slow) GHCJS gives me. `jsaddle` can also do that out of the box, except it's actually *absurdly* faster than GHCJS, despite the FFI shimming it does. Native Haskell sending JS snippets as browser commands turns out to be pretty ridiculously fast. And `jsaddle` lets you use JS heap objects as well, albeit in a less efficient manner. As I pointed out before, if we could generalize the approach `jsaddle` takes to arbitrary libraries that we spec out similar to WebIDL, we'd actually end up with something much much faster. &gt; Write some code that controls a well-known javascript library on the web. Again, marshalling JSON data in and out of the flat memory space will dwarf any speedup I get from a faster Int64 implementation. Again to avoid this I'll likely have to resort to hacks. As I've said already, `jsaddle` has shown that this is just actually not true. GHCJS is *that* slow. Plus, WebAssembly will actually support calling JS functions directly. WebAssembly will have *far* less marshaling to do than `jsaddle` on mobiles, because of this. With a few clever shims, I suspect the overhead can be near-zero for most APIs, especially since that overhead will be the kind of JS that is easy to JIT. &gt; Integrate with the 200k npm libraries seamlessly. This is where my WebIDL hope stops working; having to write specs for dozens of npm packages is too much. But that's where `jsaddle` comes in again! It supports `eval :: String -&gt; JSM JSVal`. I've actually been impressed with how easy it is to integrate with JS APIs this way, even across arbitrary `jsaddle` backends, like the one `WebAssembly` will need. That said, I understand this is still kind of ugly. But not hugely moreso than GHCJS's JS FFI, and I've also found myself needing zero npm packages in most projects, because there are Haskell libraries that cover most of the bases. &gt; I just don't see the huge need for PNaCl in the browser, and the market already decided that - nobody wanted NaCl nor PNaCl as they solved problems few people had. PNaCl had a major flaw: It was not a web standard. Other browsers just refused to adopt it, just like asm.js. WebAssembly *is* a standard, and is actually supported by almost everything at this point. The train is moving; it's not going to stop. &gt; What is needed is seamless integration at the heap level with Javascript, as Javascript is the world's #1 programming language. I hope the takeaway from my comment here is that this is overstated, and not hard to solve.
&gt; We have to assume that the implementation of Monoid follows its associative and neutral element laws, and that our code does not contain non-lazy infinite recursions… At best, we can verify them through testing I was thinking it might be interesting to have a way of attaching laws with a typeclass, in the form of a test that any typeclass implementing it would have to satisfy. This would then get checked at compile time as a built-in unit test. For example, for Monoid it would test the laws it should satisfy[0] mappend mempty x = x mappend x mempty = x mappend x (mappend y z) = mappend (mappend x y) z mconcat = foldr mappend mempty I'm unsure if this would be useful as a language extension, since I'm assuming that would only pick up on your own instance, and not the ones you use? I guess that could be somewhat useful still though. Any thoughts? I guess dependent types could solve this? [0] https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Monoid.html
Is there a reason to use a `TypeRep a` instead of a `Typeable` constraint?
There's actually quite some theory about "universes" (types are sums of products) and "signatures" (this type is this specific sum of products), even for dependent types, although they don't look anywhere close to sums of products anymore. A long time ago, I read a paper about this, `Universes for Generic Programs and Proofs in Dependent Type Theory`.
Thanks for the detailed answers!
I've started using `freer-effects`since watching Sandy Maguire's presentation: http://reasonablypolymorphic.com/dont-eff-it-up/. It's now a year later and I'm still enjoying it. However, if you're writing a library for Hackage that is to be used by many, then mtl-style is arguably the most general.
As you will probably already have read somewhere, monad transformers do not "commute in general". What does this mean? Generally, for some monad transformer you will want some way to "get in" and "get out" of the transformer. The way to get in is the data constructor, for instance ExceptT :: m (Either e a) -&gt; ExceptT e m a, while you usually also have access to the inverse function, called `runFooT` for some transformer `FooT`. So in this case: runExceptT :: ExceptT e m a -&gt; m (Either e a). What I do when I think about what my transformer stack should look like, is think about what the type signature of its "run" function should be. While as a beginner of this topic you will have to fool around a lot in order to get things to type-check, at least you will avoid coding a lot of functions against some concrete transformer, only to notice afterwards that your "run" function gives you the wrong semantics, because your `State` will be gone for the `Left` case, or something like that. Also like mightybyte mentioned, some transformers make more sense for some local scope. To add a further example to this, instead of some global `StateT` over `IO`, I would just use `IORefs` or `TVars` if I'm in `IO` anyway. Furthermore, I also am in the `ExceptT` over `IO` camp. Some people argument that a function fun :: String -&gt; ExceptT MyError IO Int seems to suggest that only a `MyError` can occur, even though in `IO`, runtime exceptions can be thrown and caught. When I see a function like that, say http :: Request -&gt; ExceptT HttpError IO Response, I regard it as if it communicated the following things: * This function can result in errors. See? HttError is one of them! * I, that is, you since you wrote it, expect this HttpError to be handled locally in a meaningful way.
They'd be pretty simple. For example: data UnOp (op :: Symbol) a b where Factorial :: Num a =&gt; UnOp "factorial" a a PutStr :: UnOp "PutStr" Str (IO ()) data BinOp (op :: Symbol) a b c where Mult :: Num a =&gt; BinOp "mult" a a a ScaleVect :: Num a =&gt; BinOp "scale" a [a] [a] ... and so on. You'd match on `Binary ScaleVect a bs` for example, instead of a `ScaleVect a bs`. 
&gt; Elm is almost Haskell on the frontend PureScript is *almost* Haskell on the frontend Elm is much further from Haskell then PS. Elm is, in its current form, also a bit of a DSL. It is specifically not targeting the BE (although there are always those discussing, or trying to use Elm for that purpose). Elm is "so popular" that is architecture (the Elm architecture, or TEA) is copied by Halogen (PS) and Miso (Haskell). Both languages also have libs that tie in with other architectures (more FRP'ish ones like Reflex in Haskell, or more old-skool ones that build on bindings with React.js, like Pux/Thermite/react-flux/react-hs). Where Elm is currently working on "universal rendering" and "UI routes shared between BE and FE", a PoC demonstrating this (with "shared model types" as bonus) already exists for Miso: https://github.com/FPtje/miso-isomorphic-example
Slightly OT: Personal gripe, but please stop spreading the world view that engineering is just going for the MVP. Engineering as a discipline highly involves verification and complete solutions.... 
Dependent types are objectively better than Haskell's type system because Haskell *already* has dependent types albeit with craftier syntax. That is why dependent types are coming to Haskell in 2019/2020.
What are the linear types here? This seems to be pretty standard dependent types with no substructural types. Linear types are basically types that you have to use exactly once.
There is nothing in particular about dependent types that requires termination checking or a distinction between induction and conduction. Those features aren't anymore necessary for dependent types as they are for system F. The reason that they have been associated with dependent types is that languages with dependent types have often focused on theorem proving over programming. Type inference works fine if one sticks to the Haskell fragment of a dependently typed language. Computationally irrelevant information can be dealt with by having two different quantifies. Dependent types are strictly an improvement over the type system GHC has today and that is why they are coming to GHC in 2019/2020.
Neel K has a paper integrating linear and dependent types, but sort of skips making the linear space dependent: https://www.cl.cam.ac.uk/~nk480/dlnl-paper.pdf The issue starts to arise when you start trying to figure out if the types themselves should have linear types around. e.g. a "type you can only use once and must use". Neel's answer is to build a Benton style type system with separate (-&gt;) and (-o) categories, allowing the former to be rich enough to have dependent types, then stitch them together with a funny adjunction. His calculus rules out your little example, and might give you a hint as to why. Why all this trouble? Because terms that inhabit some linear one-use-type are really awkward to talk about. In your case, is the use of the Bool you have at the term level to case match considered it's linear usage? If so you don't have any "uses" left to talk about `b condition` in the type. "condition" already got used up. Both the pi and the lambda need to do something different with it.
The interesting thing is that dependent types can be argued for by using some of the rhetoric dynamic typers use.
In general, you should stick to simpler features in Haskell until you can seriously justify the more complex ones (I say this as a design principle, not as beginner advice). I say this as someone working on a code base that pervasively uses higher-kinded heterogeneous list, and for which this is the right call. The down side is different boilerplate (fewer "impossible" cases, but more hand-written instances and silly type transformations) and a big spin-up cost for the code base, even for experienced developers; the up side is that there's a lot more compiler guidance on how to write (or not write) your code.
&gt; You cannot ask questions like that simply because there's no fully dependent type language that can compete on the tooling Most of the dependently typed languages have better IDE experiences than Haskell. 
&gt; dependent typing is too much overhead for not enough payoff I feel like optional dependent typing is the perfect solution. It'll enable library authors, which will usually put a lot more time into refining something, to make careful restrictions on their functions that will help get rid of runtime checks, which the rest of the community at large will benefit from, without paying the costs.
&gt; In your case, is the use of the Bool you have at the term level to case match considered it's linear usage? Yes. &gt; If so you don't have any "uses" left to talk about b condition in the type. "condition" already got used up. Both the pi and the lambda need to do something different with it for either of them to work. I assumed that quantified usages do not have to be linear for two reasons: - Irrelevantly quantified (`∀`) type variables can be used in relevant contexts in types - It does not affect the linearity of the computational uses
Note the arrow the pi type has.
That doesn't sound hard... you just need to abstract over a type for jobs.
I haven't used either, but [plotlyhs](https://hackage.haskell.org/package/plotlyhs) looks like an interesting alternative.
I love Clojure and I love Haskell. For me the only tradeoff I observe with statically typed languages is that it is not idiomatic to propagate unknown fields and types, whereas in Clojure it is standard practice. I'm especially talking about data that comes and goes on the wire. For example, consider a system with services A, B, C and D where data flows A -&gt; B -&gt; C -&gt; D. If service A wants to add a new field or a new type that only D cares about, Rich Hickey and Clojurians don't want to have to re-deploy B and C. Statically typed languages do not make it idiomatic to model for the unknown like this whereas Clojure does. My day job is a product manager and static typing consistently slows down the release of new features for me because of this problem. I end-up needing to meet with the teams in charge of services B and C and waiting for them to update and re-deploy. While these types of additions to the data model are usually not a lot of engineering effort for B and C, coordinating across many teams in these types of scenarios can take weeks.
[removed]
&gt; shared UI routes, shared views, universal routing and rendering This is something that Elm's Evan is still working on in private. Miso is heavily inspired by the Elm architecture (TEA). The routes are Servant.API routes, also in the client.
Something like this? import Control.Concurrent.Async import Control.Concurrent.MVar import Control.Concurrent.STM import Control.Monad type Task a = IO a type TaskRef a = MVar a markTaskComplete :: TaskRef a -&gt; a -&gt; IO () markTaskComplete = putMVar isTaskComplete :: TaskRef a -&gt; IO (Maybe a) isTaskComplete = tryReadMVar blockUntilComplete :: TaskRef a -&gt; IO a blockUntilComplete = readMVar type Job = IO () type Pool = TQueue Job newPool :: IO Pool newPool = newTQueueIO enqueueTask :: Pool -&gt; Task a -&gt; IO (TaskRef a) enqueueTask pool task = do ref &lt;- newEmptyMVar let job = task &gt;&gt;= markTaskComplete ref atomically $ writeTQueue pool job pure ref spawnWorkerThread :: Pool -&gt; IO (Async ()) spawnWorkerThread pool = async . forever $ do job &lt;- atomically $ readTQueue pool job 
In this case, the memoization happens because `foo11` is _not_ a function, it's a value of type Int. Due to laziness, this top-level declaration is not evaluated on startup, but is instead a thunk which will be evaluated the first time it is forced, and only the first time. When that happens, "yo" is printed. If you want to learn more about memoization in Haskell, I have a blog post called "[Will it memoize?"](http://gelisam.blogspot.ca/2015/06/will-it-memoize.html) which goes into more details.
Ah thanks, that's a great resource! :)
:)
&gt; Having a type system that forces me to think about those edge cases works against this stated goal. It forces me to be correct above all, but this is not what I want: I want to have the happy path solved as fast as possible, and I’ll handle the edge cases as they show up over time. I really don't like that approach. If a codebase was written without any consideration for pre and post conditions (which types are great at encoding), I can fix bugs when they are found, but future changes are quite likely to introduce new bugs. That's because it's not clear whether those changes break the code's implicit assumptions. All we can do is look at all the call sites and try to guess what each call site's assumptions are. It's still possible to focus on the happy path without having to handle every possible edge case. Just use `undefined` or some other marker to indicate the unhappy paths which aren't implemented yet. I agree that it's faster to keep the information in your head than to write it down, as least while the code is small enough that you can keep all that information in your head, but sooner or later the codebase is going to grow too big or too old or change maintainership and you're going to regret not taking the time to write down the code's implicit assumptions. You can write them down using comments. You can write them down using asserts. But if you want to write down this important information as fast as possible, because you want to get this MVP out of the door, then types are a really succinct way of doing it.
Conor McBride's [I got Plenty o' Nuttin'](https://personal.cis.strath.ac.uk/conor.mcbride/pub/Rig.pdf) Lets you make the linear space dependent.
Is there any reason not to just pass a map around anyways? Then B and C can pull whatever they want out. Rather, as it sounds like these components aren't linked together, I'm surprised that there's any pain in the first place since each team should be parsing out of a map anyways.
Can learning any type of math make the Haskell journey easier? If so, which kind? Is there anything else that could make learning pure FP less difficult, that isn't so obvious. 
The ideas behind the other effect frameworks sure are interesting as well. Sadly, the library ecosystem starts to get really fragmented for everything that "comes after `free`". In the particular case of `freer-effects`, it seems this library currently is not [actively maintained](https://github.com/IxpertaSolutions/freer-effects/issues/38).
You keep making really dumb assumptions throughout your comments here. I really want you to think hard instead of having a knee-jerk response. &gt; I think you have misinterpreted these concepts No. You have no reason to believe that. You don't know what the contents of the presentations he was talking about were. This is the thought pattern of a zealot. Despite having no evidence, you just assume that something with a similar label to you couldn't possibly do wrong. Stop that. &gt; Patriarchy theory is as follows Where does this come from? This completely contradicts every other definition I've ever seen. For example, the most often cited definition of patriarchy is not even a theory at all, just the noun meaning "social system in which males hold primary power", and originally patriarchy theory was just the assertion that this describes the west. The way you describe it, patriarchy isn't even a system. This topic is discussed on Reddit all the time. What about all the people answering [here](https://www.reddit.com/r/AskFeminists/comments/1efj0e/falsifiability_of_patriarchy_theory/?st=j9nl77gf&amp;sh=c4f41df9)? No one there seems to agree with your definition. Are they all misrepresenting these ideas? One of the top answers there links to [this](http://imaginenoborders.org/pdf/zines/UnderstandingPatriarchy.pdf) essay by Bell Hooks, which says &gt; Patriarchy is a political-social system that insists that males are inherently dominating, superior to everything and everyone deemed weak, especially females, and endowed with the right to dominate and rule over the weak and to maintain that dominance through various forms of psychological terrorism and violence. Wow, that's pretty extreme. Do you think Bell Hooks, one of the most influential feminist writers, period, is misrepresenting what patriarchy is? Looking at more theoretical discussions, we can see what some prominent feminist scholars have said: In "Patriarchy and Accumulation on a World Scale" ecofeminist Maria Mies said that &gt; Peace in patriarchy is war against women. That's a pretty extreme statement that completely conflicts with your definition. Is she misrepresenting patriarchy theory by saying that? In "The Unhappy Marriage of Marxism and Feminism", by Heidi Hartmann, in the section "Towards a definition of patriarchy" we read; &gt; We define patriarchy as a set of social relations which has a material base and in which there are hierarchical relations between men and solidarity among them which enable them in turn to dominate women. The material base of patriarchy is men’s control over women’s labour power. That control is maintained by excluding women from access to necessary economically productive resources and by restricting women’s sexuality. This certainly sounds like the one-way relationship that you characterized as a straw-man. One of the most commonly cited definitions of patriarchy comes from the German Scarlet Women publication; &gt; By the patriarchy we mean a system in which all women are oppressed, an oppression which is total, affecting all aspects of our lives. The way you describe it, patriarchy can exist without any actual oppression occurring. So, are the likes of Lindsey German misrepresenting patriarchy theory when they cite this definition? Is it that you don't know what you're talking about, or do many of the major advocates for patriarchy theory not know what they're talking about? Or, is it perhaps that there is no universal definition of patriarchy, and when someone criticizes an idea under that name, it's dumb of you to assume they're straw-manning your pet definition? Just a thought. The facts of the matter are that the "straw man" versions of privilege and patriarchy theory are actually believed by many people, and those beliefs do, in fact, end up in presentations from time to time. If you think this is a dumb position, then why are you criticizing the people calling it dumb? That's a completely counter-productive denial of reality.
As a general rule of thumb, no, I don't like this approach. I do like Simon Meier's [Service Pattern](https://www.schoolofhaskell.com/user/meiersi/the-service-pattern) much better. I independently came up with a vaguely similar approach, but that post is more clearly articulated.
I would say its better to just try and learn Haskell slowly and really understand each piece than it is to learn other things first. With that said some Category theory, Set theory etc. and just general mathematical abilities will help. But I want to emphasize again that to just learn Haskell the language and become proficient learning math and then Haskell will definitely be slower than just learning Haskell directly. The math may be more beneficial much later on if/when you are designing your own abstractions and laws.
Many times it's not obvious when the systems are first built that we will want this de-coupling. * For example imagine A manages metadata about products `{product_id: 123, product_name: "Nike Air Jordans XX8"}` * B might be the inventory management system that reads the product_id and gets the amount in stock in different stores. { product_id: 123, product_name: "Nike Air Jordans", inventory: [ {store_id: 234, units: 10}, {store_id: 456, units: 20}, {store_id: 789, units: "out_of_stock"}]} * Service C might be a frontend API that handles authorization and authentication. It gets the user making the request from a bearer token and validates that the user should have access to the data they are querying for. Maybe there are business rules where users should only be able to query for `product_id` and `store_id` that they've been given permission to query. * Service D is a frontend UI that allows authorized business users to query for inventory by `product_id` and `store_id`. Now imagine that we discover all kinds of problems with the "product_name" field because it is derived automatically from vendor systems. Service A creates an optional `product_display_name` field, and we hire a team to go through our product catalog and enter more descriptive names where needed (e.g. "Air Jordan XX8 - Dare to Fly - 2013"). Service D as the frontend should show the `product_display_name` when available and fallback to the `product_name` field if it doesn't exist. My experience is that Services B and C usually statically type the requests and responses to backend services as records. I then need to wait for them to update and pass along the `product_display_name` before any of the new names start showing up in the frontend.
No, persist different queue payloads to disk/db/redis and read them back.
So a distributed job queue?
I don't have much to say except that it sounds like you have enough understanding and experience to make that determination for yourself for your project. I think it would be a perfectly respectable approach to pretend as if even monad transformers didn't exist (at least until you got to the point where you start "inventing them yourself" to solve a real structural problem in your code, say). A couple other observations: 1) you really need to write automated tests of your actual effectful code paths anyway, 2) you're really, really not going to want to migrate a bunch of application code from one effects library to another when you find out the performance is cuckoo or it's blowing up compile times.
I also agree with Elvish but disagree with your conclusion. I absolutely want a turing complete and very powerful language like Haskell for web dev. I would like nice libraries that make the common stuff quick and easy (just like I do for normal Haskell development) but I want it to be trivial to right my own arbitrary Haskell code as well.
Why are you responding to people criticizing this view instead of the people who hold it? The idea that white people are in low moral standing just for being white is an extremely common view now adays. Just a few days ago, [this](http://www.patheos.com/blogs/mercynotsacrifice/2017/11/01/is-it-okay-to-be-white/) article was published, which explicitly states; &gt; whiteness is a socialization by which a specific legacy of repressed guilt, entitlement, and defensiveness is inherited as an original sin that uniquely corrupts every white baby who is born into our American context If you think these ideas are bad, then criticize the people who hold them, not the people who point out that they're bad.
That's reasonable. Truly it's a case where row types would be nice, but lacking that it's not unreasonable to take the time to switch to a map now that you've discovered that it takes too much time to get everything in place between the teams.
&gt;&gt; Patriarchy is a political-social system that insists that males are inherently dominating, superior to everything and everyone deemed weak, especially females, and endowed with the right to dominate and rule over the weak and to maintain that dominance through various forms of psychological terrorism and violence. This doesn't contradict what I've. This is something that follows from what I have said. The idea that men are inherently dominating comes from the socially-constructed notions of masculinity I mention. &gt;&gt; By the patriarchy we mean a system in which all women are oppressed, an oppression which is total, affecting all aspects of our lives. This doesn't contradict what I've said. &gt;&gt; Peace in patriarchy is war against women. I don't see how this contradicts what I said. This can follow from what I said. &gt;&gt; We define patriarchy as a set of social relations which has a material base and in which there are hierarchical relations between men and solidarity among them which enable them in turn to dominate women. The material base of patriarchy is men’s control over women’s labour power. That control is maintained by excluding women from access to necessary economically productive resources and by restricting women’s sexuality. The difference in definition here has to do with the Marxist concept of [base and superstructure](https://en.wikipedia.org/wiki/Base_and_superstructure). Ultimately, it is referring to the same concept I am but Marxists like to argue that things follow from relations of production. Essentially, I'm defining patriarchy by some of its consequences. Marxist-feminists are defining it based on where it originates from according to them (as in why the concepts of masculinity and femininity exist as they are in the first place). &gt; This certainly sounds like the one-way relationship that you characterized as a straw-man. There are things that follow in the superstructure from the base such as traditional conceptions of masculinity that hurt men. &gt; The facts of the matter are that the "straw man" versions of privilege and patriarchy theory are actually believed by many people, and those beliefs do, in fact, end up in presentations from time to time. Just because a bunch of people fail to understand or misinterpret a concept does not make their understandings of the concept valid. 
**Base and superstructure** In Marxist theory, human society consists of two parts: the base (or substructure) and superstructure. The base comprises the forces and relations of production (e.g. employer–employee work conditions, the technical division of labour, and property relations) into which people enter to produce the necessities and amenities of life. The base determines society's other relationships and ideas to comprise its superstructure, including its culture, institutions, political power structures, roles, rituals, and state. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I would personally prefer a richer ordering hierarchy such that `==&gt;` has the type `Heyting a =&gt; a -&gt; a -&gt; a`. But I realize that with our current setup it is hard to make an existing hierarchy more granular.
&gt; Why are you responding to people criticizing this view instead of the people who hold it? The idea that white people are in low moral standing just for being white is an extremely common view now adays. Just a few days ago, this article was published, which explicitly states; People misunderstandings and misapplications of a theory cannot be used as arguments against the actual theory. 
[It appears as though the lexer is the one that handles lambdacase](https://www.reddit.com/r/haskell/comments/7apndk/why_doesnt_ghc_suggest_to_turn_on_lambdacase/dpe32ls/).
you might start by showing some code ;)
I strongly +1 this approach, although I would avoid making the `Handle` polymorphic until it's really necessary.
Two things I noticed: convertString could be simplified by using map. Instead of weaveArrays you could use zipWith. 
Thank you for writing Miso, for example I have already declared all my types for the backend and I would like to share it with the front end. however, I did used `Text` instead of string. do I need to change it to `MisoString`? if yes it would be paintful for me since want my backend frontend agnostic and do not depend on any library related to the frontend.
First: **congrats**! ---- as you asked for ways to improve, here are a few things I would change: * you have a failing pattern-match with no arguments but then go on and have a `process´ that will deal with wrong input - why not pass in the argument list as it is and deal with missing information in the same manner (not crashing)? * `convertString` should be rewritten using `map` ;) * same with `weaveArray` and `zipWith` * `unStringify (x:y:z:xs) = (read $ [x] ++ [y] ++ [z])..` can be `... = (read [x,y,z])..` * you should have a look at the [bracket pattern/withFile](https://wiki.haskell.org/Bracket_pattern) for your IO * I think you should try to get the overflow-handling form your `decode`into `convertString` 
&gt; if yes it would be painful for me since want my backend frontend agnostic and do not depend on any library related to the frontend. So, a little secret here is that `miso` is kind of like two libraries. There is front-end specific code (has `GHCJS` specific types throughout) and then backend specific code (`GHC`-produced machine code), and finally shared code, which is common to both front and back (can be safely compiled to both js and x86). When you use `miso` on the backend, it does not bring in any `GHCJS` specific code (ex: `ghcjs-base`). So your backend is safe to depend on `miso`. `Miso` accomplishes this by relying on compiler flags to correctly detect the environment and include proper dependencies.
I have mixed opinions about your comment. Is this the general consensus on the approach that GHCJS has taken? I would be interested in hearing a rebuttal from GHCJS contributors. How far is WebAssembly from a usable alpha state? Does WA guarantee that code compiled to WA will have greater abilities than code written in JS? If not, then, WA &amp; JS are at-par when it comes to being compilation/transpiling targets. What I mean is, you will always need special hacks to get Haskell (or Java, or Kotlin, or Clojure) to compile to either JS or WA. You cannot expect all the libraries and the entire ecosystem to work in the browser without any tweaking whatsoever. If that is indeed the scenario, I'd put my money on making GHCJS a first-class Haskell citizen, with a proper discussion on making it stable as it stands today, keeping it stable as GHC progresses, and a roadmap to change its internals once WA reaches some sort of stability. Getting your language to work on the browser (either via JS or WA) is a very important goal. If there is no consensus on how Haskell plans to do, then we should just give-up and stop wasting so much developer bandwidth on GHCJS (or WA) and its entire ecosystem, and instead move to making GHC itself rock solid.
Protobufs are typed but are also designed to be able to retain and pass on unknown fields. It's a common but not universal feature in the various typed interchange formats. It leads to various awkward API decisions but was considered important enough to retain despite that.
I vote for this
Interesting video! It bugged out for me a couple of times with green pixelated screen/corruption, tiny bit unsure if that was Safari or the video itself?
Might be safari. I just saw the video again on my phone and didn't get any pixelation/corruption.
There is no easy way to have one call site serialise multiple data-types back without a lot of boilerplate (or type-level hackery). Even if you use a type-classes, you can't instantiate the type-class at the point of deserialisation. The only thing that will work is one large ADT, at which point you are serialising/deserialising **values** of the same type, and are paying by loss of general extensibility.
I enjoyed this video! I wonder why didn't you get some kind of "defined but not used" at [t=120](https://youtu.be/zKUTd-C7jkk?t=170).
Your example really made a lot of sense to me. I've been in similar situations. However, do you think would be the default to a system in clojure, to create the request to another system, using the same map as received as input? This seems like a upfront decision that would impact the design of all system: always pass all your input to the next system. Also if the input is sensitive (like credentials, tokens, PII, etc).. would you have to explicitly remove from there? What I've done in some systems was to have an extra field called metadata, or something similar, that was a map that was explicit these kind of pass-through fields. But this was always an after thought, not the original design. 
Same here. The editor randomly stops showing warnings and compile errors. As I said, "shitty Haskell tooling". We need to come-up with a plan to make it better.
&gt; Can learning any type of math make the Haskell journey easier? If so, which kind(s)? You need general logical &amp; analytical skill to be able to program effectively. Being good at maths is usually correlated to such skills. However, people without maths background are also good and logical/analytical reasoning. &gt; Is there anything else that could make learning pure FP less difficult, that isn't so obvious. Yes. Stop obsessing with definitions Monads, Monoids, and Functors. Start using them. The intuition will build over time automatically once you start using them. You'll end-up writing the same boilerplate 10 times, and one day you'll want to abstract out that boilerplate and then will realise what a Monoid is. Best way to learn. 
A few fairly general things. None of these are essential to using Haskell at first, but they'll all come in handy soon enough. 1. Learn one of the libraries for parsing command-line options. I use `optparse-applicative` but `optparse-generic` is quite good, especially when you're starting out. 2. Use [hlint](https://hackage.haskell.org/package/hlint). It can detect a lot of silly mistakes. 3. You might want to break up some of your `do` blocks. It can make it easier to think about things independently. Sometimes there's no neat elegant functional way to do things but often there is and it's worth looking for these times. 
Thanks for the information, would it be possible to invert the concept, for example to use `Text` or `String` instead of `MisoString` but while complied with GHCI it will be converted to `JSString` and with GHC will remain the same. I don t mean to be picky, the thing is lets say that I have decided to change the front end later and don't use Miso, I would prefer for my types to stay the normal known haskell type like Text instead of any specific library. I might be exaggerating things also
This is possible using `lens`: http://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-TH.html#v:makePrisms
It would be really worthwhile coming up with a minimal example of why `prefinalType` didn't emit an unused variable warning.
Could you explain how you think one could accomplish this with prisms? I don't think it solves the problem.
Eh, might work in a new language, but yeah, hard to do in Haskell itself. `class (Monoid a, Ord a) =&gt; Heyting a` or what?
Thank you for the above, I do agree with you about the error handling and I feel the same way about it. &gt; ExceptT :: m (Either e a) -&gt; ExceptT e m a let say That I want my monad `m` to be `ReaderT Env IO` then my type will be `ExceptT Error ReaderT Env IO`. however, somehow I have the following fn function fn :: ExcpeptT Error ReaderT Env IO Int then for some reason I need a function `extract` to make a transformation as mentioned below : extract :: ExceptT AnotherErr IO Int extract = do res &lt;- liftIO $ runExceptT fn ..... this is confusing, do i need to call `ask ` first then `runExceptT` then `runReaderT`! 
Hi, I have a situation where I need to perform some `IO` actions with a context of failure. If one action fails, then I'll fall through to the next method and so on until one action succeeds. The first action which succeeds will be the last, and no further execution will be required. The `MaybeT` and `ExceptT` monad transformers will cease execution in the result of failure rather than cease execution in the result of a success. Are there any standard monad transformers available for this context? I feel as if this would occur reasonably frequently.
I use Sublime Text 3 + SublimeHaskell and IntelliJ IDEA + Haskforce, both are great at what they do.
Thanks for your answer , personally, I find it easiter to reason about while using ExcpetT. &gt; mtl style of monad transformers make it easier to use monadic action, because you don't need to use explicit lifts. for example, do you mean the one imported from `Control.Monad.Reader`!
I see some acrimony agaisnt Axiom by the side of the Reflex fans. In my opinion, Axiom is way better than any other client framework. It is simple It does not inherits the complexity of FRP. it composes widgets perfectly and the widgets execute client as well as server code. And server code can run not in a single server, but in the cloud.
Well `Heyting` implies neither `Ord` nor `Monoid`. Of the existing hierarchy it would only imply `Eq`. Ideally we would have the entire partial order / lattice / algebra / enum hierarchy combined together properly. 
Nice video!
I'll throw one thing out which is rather basic and worth understanding. I think that's not good is your use of arrays encrypt :: [String] -&gt; IO() You are being weakly typed here. You want encrypt a tuple of a msg and a key. So use the type system and have a msg type, and a key type. Define them. Then at that point you would have (msg,key) be a tuple. At least that uses strong typing rather than bypassing. But you would want to curry because I can see encrypt key in isolation being a nice function to have around. 
Sorry, I misunderstood, I thought you wanted to make a record from an equivalent tuple, but it seems like you want to go from `ConfigData -&gt; MyType`. Does the [aeson](http://hackage.haskell.org/package/aeson-1.2.3.0/docs/Data-Aeson.html) approach work for you? If your config is JSON or YAML, you get these instances for free.
What if you want conditional inclusion of modules ? It does not matter if it is internal or external. With appropriate conditionals you can selectively include and/or select the appropriate version of the internal module. This technique can be used to avoid a lot of CPP stuff. 
We seem to be talking past one another. I'm talking about Elm as an example of the DSL, a domain specific language for front end web development. Creating a DSL makes it easier not harder to write your own code. 
I've only skimmed your question, but https://ocharles.org.uk/blog/posts/2014-04-26-constructing-generically.html might help
Keep only those in a manually specified list. Why the need to enumerate everything? The default logic of includin e'thing on the file-system is good enough for 99% of the use-cases. 
This is almost trivial with [generics-sop](http://hackage.haskell.org/package/generics-sop): {-# LANGUAGE DataKinds #-} {-# LANGUAGE DeriveGeneric #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeFamilies #-} module T where import Generics.SOP import qualified GHC.Generics as GHC data Foo = Foo { x :: String, y :: Int } deriving (GHC.Generic, Show) instance Generic Foo translate :: forall a b xs. (Generic a, Generic b, Code a ~ '[xs], Code b ~ '[xs]) =&gt; a -&gt; b translate = (to :: Rep b -&gt; b) . (from :: a -&gt; Rep a) test :: Foo test = translate ("Hi", 5) As far as `generics-sop` is concerned, `(String, Int)` and `Foo` are basically isomorphic. All we need to do is translate from the concrete representations to their generic representations and back, that's all.
Yes, this one.
I would have called that video "Fixing a bug despite the terrible Haskell tooling" :/ To a non-Haskeller, I think this video showcases how bad the tooling, the logging, and in a sense Haskell, is, then in the end something works, but it is unclear how static typing helped. Side note: persistent, despite all its shortcomings, takes care of most of the boilerplate. It is not a Haskell specific problem.
&gt; ~~optional~~ dependent typing is the perfect solution Gradual instead of optional. Give me the gradual guarantee, so I know adding a (correct) annotation doesn't break things. Also, I'm fairly convinced that first-class support for linearity (or at least affine-ity) is going to be in the "perfect" solution. 
&gt; Is this the general consensus on the approach that GHCJS has taken? I haven't talked to Luite in a while about it, but it does seem like most people prefer a world where we use WebAssembly than JS. I have no idea what others think about how we should get there. &gt; How far is WebAssembly from a usable alpha state? Very. I'd expect 6 months at the earliest, probably 12. This is mostly an issue of time investment; we don't have much time to invest. But the truth is, it's very hard to predict. We don't even know what the next roadblocks will be after what we're currently working on. As far as we know, it might be working a month from now. My expected timeline is coming mostly from seeing how long it's taken us so far, and judging what percentage of the components are working. There's no inherent reason to believe all the remaining components won't work out of the box; I'm just not optimistic ;) &gt; Does WA guarantee that code compiled to WA will have greater abilities than code written in JS? If not, then, WA &amp; JS are at-par when it comes to being compilation/transpiling targets. What I mean is, you will always need special hacks to get Haskell (or Java, or Kotlin, or Clojure) to compile to either JS or WA. You cannot expect all the libraries and the entire ecosystem to work in the browser without any tweaking whatsoever. Read my other comments in the comment section. I've outlined how WebAssembly will be used to achieve all the same things that GHCJS does now. &gt; You cannot expect all the libraries and the entire ecosystem to work in the browser without any tweaking whatsoever. Of course we can't expect *every* line of C to work out of the box, but I think we'll come surprisingly close. Unless it's using syscalls that we haven't taken the time to implement (most won't), it should actually just work out of the box if we get the infrastructure right. So yes, I think most libraries will work without any tweaking. Of course, you're right that some will need work. &gt; If that is indeed the scenario, I'd put my money on making GHCJS a first-class Haskell citizen, with a proper discussion on making it stable as it stands today, keeping it stable as GHC progresses, and a roadmap to change its internals once WA reaches some sort of stability. I think once GHC works on WebAssembly, the state of it will immediately be far better than GHCJS. The key is that there is no plan to "change [GHCJS's] internals." Migrating to WebAssembly doesn't mean migrating GHCJS to WebAssembly, it means migrating *frontends* to a different compiler and `jsaddle` backend, because the WebAssembly backend will simply be a proper cross compilation target in GHC itself. &gt; If there is no consensus on how Haskell plans to do, then we should just give-up and stop wasting so much developer bandwidth on GHCJS (or WA) and its entire ecosystem, and instead move to making GHC itself rock solid. There definitely is no consensus, because GHCJS was always sort of a 3rd party project that the committee never seemed to have an official opinion on. The committee sponsored WA as a Haskell SoC project though, so I can only assume they want that to succeed, but that doesn't say anything about what they want for GHCJS. I think *giving up* on GHCJS is throwing the baby out with the bath water. Like I said, I don't expect WebAssembly to be working for probably another year. So I think we need GHCJS for now. But I don't think that need is anywhere near enough to make it a priority to make GHCJS "first class." To me, that would mean upstreaming the bulk (or all) of its work into GHC proper, and putting a bunch of JS into core packages. I think this would be more of a danger than a feature, especially considering how much less invasive WebAssembly is going to be. I don't think we want to be putting JS into core packages if JS is just going to be obsoleted in a year or two. GHCJS is necessary, and it's already a pretty great piece of work, but I think of it as a holdover. Expending significant effort beyond common maintenance and GHC updates would be wasteful, but forgoing maintenance entirely leaves those currently relying on it out to dry. GHCJS is a good technology, and it's good enough that I would still recommend it for frontend work, as long as we have the minimum resources dedicated to keeping it alive. But the path forward (IMO) is WebAssembly.
&gt; it might be interesting to have a way of attaching laws with a typeclass, in the form of a test that any typeclass implementing it would have to satisfy. Dependent types let you do that. The Idris [contrib](https://github.com/idris-lang/Idris-dev/blob/master/libs/contrib/Interfaces/Verified.idr) library has Verified* family of type classes that mirror the Functor/Applicative/Monad and Semigroup/Monoid typeclasses.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [idris-lang/Idris-dev/.../**Verified.idr** (master → f3dcca5)](https://github.com/idris-lang/Idris-dev/blob/f3dcca51dba2b35ecaa7059c51a1465a6af15aaf/libs/contrib/Interfaces/Verified.idr) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; To a non-Haskeller, I think this video showcases how bad the tooling, the logging, and in a sense Haskell, is, then in the end something works, but it is unclear how static typing helped. It should make the state of bad Haskell tooling clear to Haskellers. I don't really care about non-Haskellers. That's partly my attempt. It's high time the community stopped debating category theory and started getting basic things to work. More discourse will follow in the coming few weeks, on this topic. &gt; Side note: persistent, despite all its shortcomings, takes care of most of the boilerplate. It is not a Haskell specific problem. The way persistent is structured wouldn't have worked for us. Our compile times would have been soul-crushing with the way the persistent's "models" file is structured -- it is one big compile bottle-neck. &gt; but it is unclear how static typing helped. I've highlighted those areas as "TypeAssists" in the text overlays.
There's [ExceptRT](https://hackage.haskell.org/package/errors-2.2.2/docs/Data-EitherR.html#t:ExceptRT) in `errors` package. 
We build a compiler in Haskell from neural network descriptions targeting FPGAs! It's all very fun and interesting, /u/Boom_Rang and I will be around to answer questions!
&gt; GHCJS is necessary, and it's already a pretty great piece of work, but I think of it as a holdover. Expending significant effort beyond common maintenance and GHC updates would be wasteful, but forgoing maintenance entirely leaves those currently relying on it out to dry. GHCJS is a good technology, and it's good enough that I would still recommend it for frontend work, as long as we have the minimum resources dedicated to keeping it alive. But the path forward (IMO) is WebAssembly. What does this mean for people investing time in Reflex, Miso, Concur, etc? Will their work be easily portable to the WebAssembly backend once its ready? Also, how seriously is the WebAssembly backend being pursued? Is anyone from the Haskell committee working along with the WebAssembly committee while the latter is being formalised?
&gt; What does this mean for people investing time in Reflex, Miso, Concur, etc? I can't speak for the others, but porting Reflex should be basically free since its based almost solely on `jsaddle`. If anyone wants to prepare for WebAssembly, they should just port their code to `jsaddle`. &gt; Also, how seriously is the WebAssembly backend being pursued? Michael Vogelsang and I are investing our free time in it. We're both fairly committed to seeing it through, but neither of us has a ton of time to spend on it. So it *will* get done, it's just a matter of when ;) &gt; Is anyone from the Haskell committee working along with the WebAssembly committee while the latter is being formalised? I'm not on the committee, but I've been monitoring the progress and plans for WebAssembly. So far I've influenced the linker and assembly language a little bit, but for the most part not much cooperation has been necessary.
Yea I've had weird stuff like that with both Intero and Dante, where warnings that GHC emits aren't shown in the editor. Makes me wonder if it's flycheck's fault, or probably the flycheck integration? Honestly this is probably just a little bug fix to the editor tool.
Yea I've had weird stuff like that with both Intero and Dante, where warnings that GHC emits aren't shown in the editor. Makes me wonder if it's flycheck's fault, or probably the flycheck integration? Honestly this is probably just a little bug fix to the editor tool.
&gt; It's high time the community stopped debating category theory and started getting basic things to work. To be frank, this is arrogant and insulting. There *are* a lot of people working on improving tooling in Haskell! It’s not getting done overnight, but that doesn’t mean nothing is happening. GHC has been pretty consistently improving. Cabal has been seeing *major* improvements. The mere existence of Intero is a huge improvement over a few years ago, let alone the fact that it’s actively maintained. Honestly the list of things that have been improving in this realm goes on and on and on, but your comments always seem to indicate that no one is doing anything. No, we don’t have a whole team of full time engineers to dedicate to editor tooling like IntelliJ does; we barely have a full time team on GHC. Things are improving. People are doing work in this area. Don’t minimize that because we don’t have as big a community. You want to fix the tooling? [It takes *work*](https://www.reddit.com/r/haskell/comments/5b5nux/who_owns_haskell/d9lz7zv/).
&gt; EDIT: Oh if it's the editor being flaky rather than something in GHC then it's probably less worthwhile ... It's equally worthwhile, IMO. It increases the code/compile/debug cycle and takes away some benefits of static typing.
Huh. Stack is rebuilding those deps because `ghcversion.h` changed. Either Stack is doing something insanely stupid on OS X, or this is user error. Do you have any insight on how that header might be changing? I'd recommend finding a way to reproduce and making a bug report. Also, are you able to try upgrading to GHC 8.2? I've heard it fixes some [accidentally quadratic things](https://www.reddit.com/r/haskell/comments/76zljl/hotswapping_haskell_at_facebook_jon_coens/doi8sov/?context=3) for projects with large numbers of modules. Could help your build times a lot.
For a little more context, the particular issue here was that constraints don't factor into instance resolution. So this original code: instance MonadLogger (LoggerT STM) where ... instance MonadLogger (LoggerT IO) where ... Becomes rather hard to translate to the `MonadConc`/`MonadSTM` typeclasses, where `STM` is now a type family and `IO` is a typeclass: instance MonadSTM stm =&gt; MonadLogger (LoggerT stm) where ... -- or: instance MonadConc m =&gt; MonadLogger (LoggerT (STM m)) where ... instance MonadConc m =&gt; MonadLogger (LoggerT m) where ... I pushed a new version of [concurrency](http://hackage.haskell.org/package/concurrency) which added simple newtype wrappers `IsConc m a` and `IsSTM m a`, with unexposed constructors, which have `MonadConc m` and `MonadSTM m` constraints on putting actions in and taking actions out: toIsConc :: MonadConc m =&gt; m a -&gt; IsConc m a fromIsConc :: MonadConc m =&gt; IsConc m a -&gt; m a toIsSTM :: MonadSTM m =&gt; m a -&gt; IsSTM m a fromIsSTM :: MonadSTM m =&gt; IsSTM m a -&gt; m a So these types can be used in instances like so: instance MonadSTM stm =&gt; MonadLogger (LoggerT (IsSTM stm)) where ... instance MonadConc m =&gt; MonadLogger (LoggerT (IsConc m)) where ... But this adds boilerplate, and doesn't fully resolve the situation where these `stm` and `m` type variables get used in multiple places. I'm not sure of a good general solution to this sort of problem.
I'll respond to this comment with a proper blog post (probably multiple). I don't buy this "stuff is moving, you shouldn't be complaining" line of thinking. Developer &amp; volunteer bandwidth is being **wasted** IMO. Community leaders should take ownership and set a well-thought out direction for contributions. To give a small example, for now: * Intero, Haskell-mode, Flycheck, ghcid, Haskero, Haskelly, Haskell-IDE-Engine, Sublime-Haskell, and I just discovered, Dante (a fork of Intero) through this thread itself -- just how many incomplete editor plugins do we need?
Nice! As a fellow learner I have an open question to anyone: given that `String` and `[Char]` are identical, is there a convention over when each should be used? I see a mix here, and i assume that the choice is situational based on what makes intent more clear (the latter explicitly dealing with individual characters)?
Perfect!
It isn't clear what exactly you want. Generally an IO failure means you better stop trying to read from that file. If you mean some other kind of failure, then that's probably not what IO monad offers. You may need some other monad (a parser perhaps).
It’s not that you shouldn’t complain, it’s that you have to be constructive about it. File tickets. Submit proposals. Contribute changes. Communicate your desires on the mailing lists and try to work *with* people to set goals for Haskell. Just telling everyone over and over that you think it’s shit doesn’t help. And certainly a blog post is a very poor way to point out issues compared to actually coordinating with the people involved.
Of course -- the suggestion is to change this to be handled at a later stage.
Clearly I’m not very familiar with Heyting algebras (just loving them). Anyway, a different prelude (depending on how different) is already a good deal of the way to a different language, because it constitutes a separate “common vocabulary” (and in fact that’s the very term I use to label the standard library of a language I’m working on). To use an analogy from natural language: the grammar is mostly the same but the words and idioms are different. They might be mutually intelligible dialects, but if your goal were to get one group of speakers to migrate over to the other dialect, then it would take substantial time and effort proportional to the difference.
My particular use case is calling C functions from Haskell using the FFI. Most of the functions I'm working with return `IO a` so I'll need a monad transformer to add a context of failure (when they return -1 for instance, I want to return something like a `Maybe a` or an `Either e r`). Some syscalls that I make may not succeed, but I have some backup methods which I can use, which are slower or undesireable for other reasons, which I only want to resort to if I have to. This is the complete opposite of the monadic behaviour of `Either e r` or `Maybe a` which will abort computation on failure rather than abort computation on success.
It would still be useful but *everyone* uses GHC and therefore if it were a GHC bug it would be vital to get it fixed.
Thanks! That looks like a nice way to generate interactive charts. Unfortunately I'm looking to make static (raster) charts. 
Generally you should be using `Text` for production use. `String` is a linked list of characters, whereas `Text` is a contiguous array, much more in line with the `String` you’re used to in other high level languages. In a nutshell, `Text` is much faster than `String` for almost any normal usage.
&gt; It's high time the community stopped debating category theory and started getting basic things to work. You make it sound like Haskell is their job and you're their manager. I'm sure many people will be pleased to hear you're going to pay them full time salary to work on getting basic things to work for you.
In the case of Intero, we fixed that a month or so ago. The basic problem is that GHC and the GHC API only pay attention to file changes at &gt;1 second intervals. So if you made changes in your editor that were sub-second, GHC gets asked twice, returns warnings for the first check, but returns nothing for the second because no recompilation took place. We fixed this by putting a one second delay between subsequent flycheck requests. Since then I haven't seen this issue. [PR was here](https://github.com/commercialhaskell/intero/pull/468#issuecomment-327426224).
&gt; GHC and the GHC API only pay attention to file changes at &gt;1 second intervals Whoa. That's really weird. Glad there's a fix though.
&gt;I don't buy this "stuff is moving, you shouldn't be complaining" line of thinking I think it's more like "stuff is moving *slowly*, contribute!". &gt; Community leaders should take ownership and set a well-thought out direction for contributions. Check out the repository for [HIE](https://github.com/haskell/haskell-ide-engine) where a lot of thought went into setting a direction. The __problem__ is, people are working on it in their spare-time since there is no corporate backing, which is the same for a lot of the stuff in the Haskell community - it's a matter of man power. There was a recent GSoC which brought VSCode integration, and other things, up to speed. &gt; Developer &amp; volunteer bandwidth is being wasted IMO Honestly, I think that is because people are only interested in their own editor getting tooling up to date. E.g. most people seem to be just happy with the Intero situation on Emacs, and the ghc-mod plugins on the other editors will also take you far. &gt;just how many incomplete editor plugins do we need? About as many as there are editors. - Haskero and Haskelly are VSCode, along with Haskell Language Server for HIE LSP support. - Flycheck is NOT a Haskell thing but a general emacs package - Sublime-Haskell is obviously for Sublime - Intero, ghcid and Haskell-IDE-Engine are not editor plugins but underlying tools for them (along with ghc-mod) - There's also the ide-haskell suite for Atom, and ide-haskell-hie (unrelated) which is a LSP client for HIE. Honestly, I think the LSP approach of HIE is the most viable going forwards. There's already support for Atom, Neovim, Emacs and VSCode just from it being an LSP (with minor plugin setup), so the most bang-for-the-buck is by contributing to HIE. It'll allow us to focus on one extensible tool that has close to free integration in all major editors.
Specifically, it's with `-fobject-code` turned on, which we use to avoid recompilation. With `-fbyte-code`, I vaguely remember that it does re-read the file, but at the cost of losing caching. 
&gt; It's high time the community stopped debating category theory and started getting basic things to work This type of discourse does very little to "win" people over to your cause. As /u/MidnightMoment mentions, there's been tons of improvement in the recent years in tooling. Cabal with new-build and backpack, stack, intero, HIE is becoming stable enough that I use it as my everyday driver, etc. Also, what things exactly were you missing from Intero (which it seemed you were using)? And by that I don't mean to imply it's perfect, just that I don't know what you need in an editor, compared to myself.
&gt; I'm sure many people will be pleased to hear you're going to pay them full time salary to work on getting basic things to work for you. Actually, I have been mulling this in my head for quite some time. What is the funding model for core Haskell development? When I started out with Haskell, there was something called snowdrift/coop or something. Never really saw it taking off. Tried investigating [Industrial Haskell Group](http://industry.haskell.org/status) but it seems like its dead. Is there anything concrete like the webpack/vue.js funding model in the Haskell ecosystem that one can participate in?
I absolutely agree that HIE seems the most viable approach going forward. I have proposed putting together a funding model for continued HIE development in private and public comments. I'm willing to commit funds, as I'm sure, would many others. Expecting every user to be a code contributor, is not the correct answer. Giving users the ability to financially back well-defined projects with momentum is probably a better answer.
Yea if we ever want to standardize on any one thing, I hope it's HIE. Following a standardized protocol (LSP) and supporting damn near everything under the sun is really great. The main problems with it at the moment are project-local installation (necessary for matching GHC versions; I've done it with Nix, but it's a pain) and the lack of Emacs support (AFAIK, the emacs plugin does not work).
Thanks for fixing. Running `intero --version` in my project root reports `0.1.20`, which means I need to upgrade, right?
String is a "*type synonyms*" for array of Char. That's the point of Haskell's type keyword. It creates a synonym which is used the same way you would use a comment for documentation. It is just a comment. The writer of the code is telling the reader should you think of the variable as an array of Char or should you think of it as a string. That's different than "data" which really does something. data String2 = Char2 [Char] -- this is a full blown structure that will be wrapped and unwrapped potentially at runtime type Char3 = [Char] -- this is just a comment, it gets replaced early in compilation newtype Char4 = Char4 [Char] -- this has compiler enforced type checking but will unwrapped during compilation 
&gt; Expecting every user to be a code contributor, is not the correct answer. Of course not. But anyone expecting to make changes, such as yourself, probably should be.
Awesome, thanks! Is this tip specific to projects like OPs, or is it general? For example I guess I don't see that big of an advantage to using `Text` for, say, an error message, if your project doesn't do much text-oriented stuff otherwise.
The default logic should not include things that I do not ask for. It should be the other way around. As far as I see it is a question of writing a helper script/tooling. Some thing like ``` $ update-modules Include Foo.Bar ? [e - export,o - other] ... ```
Alternatively this could be part of the haskell-mode for your editor of choice. 
While what you say is true, having built and used one of these in practice (https://github.com/positiondev/hworker), it really wasn't that onerous. I've used an analogous tool in an untyped language (Sidekiq in Ruby, to be specific), and the experience using them was quite similar. 
Could your perhaps be thinking of `Data` and `Typeable`? Might be what you’re looking for: http://chrisdone.com/posts/data-typeable
This is huge part of the issue, there are too many competing standards. I am a beginner to Haskell, and I have tried a lot of different editors/plugins. When something does not work, I am not sure whether I configured/installed something wrong, or major functionality works "somewhat". In the end, I figured that using Atom with ghc-mod gives me "the best" results. (I am used to JetBrains level of tools...) I tried really hard to like intero and HIE, since I was under the impression "those are the future", but they simply did not cut it. And I have "healthy" dislike for Atom. To make matters worse, I was not doing real projects, just dummy hackerrank type exercise. Having good tooling would make things so much better for beginners. I wonder how many people do not get into Haskell specifically because tooling is either bad or "impossible" to setup for newbies.
Right. And I don't like Elm. I want a more powerful and less limiting language.
so how do i loop through the list and return when the values true
**If it compiles, it runs\*** \*unless you have implemented a DSL in Haskell that does not have type checking to generate output to be evaluated by a middleware, and the Haskell type checker can only provide guarantees on the syntax of the DSL.
Heh, yes, it goes up to 11 really fast. Give the [tutorial](https://hackage.haskell.org/package/exinst-0.4/docs/Exinst.html) a try, it is supposed to be easy to follow. If it is not, then that's a mistake on my part and I should fix it. If you decide to pursue this, please let me know how it goes. I'm happy to help.
This is great, thank you!
It would be good to make progress on something that would allow interested parties to share costs of developing Haskell infrastructure. The IHG used to fund some contractors (maybe WellTyped?) to fix/improve stuff I think. I wonder if we could get the IHG off the ground again. 
Preciate it! Yea i was looking into `generics-sop` and it seems a lot richer than `GHC.Generics`. I just had a hard time finding extensive documentation on it
I am bit late to the party, but why not PureScript?
Yes I love this feature of protobufs! They just aren't as ubiquitous as json, which is typically the serialization format when the end consumer is meant to be a browser. Also until recently unknown field propagation wasn't the default in proto3. They are just starting to make it the default in proto3 implementations in Q3 and Q4 of this year because many customers did not want to switch from proto2 until unknown field propagation was supported. Relevant [github issue](https://github.com/google/protobuf/issues/272) and [google doc](https://docs.google.com/document/d/1KMRX-G91Aa-Y2FkEaHeeviLRRNblgIahbsk4wA14gRk/edit#heading=h.w8dtggryroj4).
I agree. 
Thank you, but I think I will leave dependent type theory for after my exploration of Coq. I am just a haskell noob at this point.
I think I'll stick to some simpler features for now, but thank you very much for the pointer.
LOL upvoted.
Right, the current version is `0.1.23`.
* I usually don't import unqualified things from `Data.Map` as they clash * the `split` has nice splitting primitives, so that you don't have to use `words` and replacing commas by spaces * `foldr` + `insertWith` = `fromListWith` * you should not need to `sort` the output of `M.toList`
Not as importent (in my eyes) : the last `forM_` could be dropped by using `intercalate` or `intersperse` (never remember which is which)
&gt; foldr + insertWith = fromListWith &gt; I usually do something like: foldToDict lst = fromListWith (++) [(rest, [item]) | [rest,item] &lt;- lst] I like it better than the pointfree style, but that is a matter of taste.
I do think a well done Clojure system would likely support this. It is standard practice in Clojure to append function results to the input map instead of creating a new map. Regarding the scrubbing of sensitive data, I agree this is often a requirement for data from untrusted sources like a browser. But in my example the new field comes from a trusted backend service, and I agree with Rich Hickey that the default in system design should be to propagate data from trusted services. In addition, Rich Hickey has also suggested that when persisting data or displaying it in the UI it is best practice to select specific keys versus dumping everything in a map because their might be sensitive data the programmer doesn't know about. And yes in systems where propagation is not designed upfront often the solve is to create a `metadata` field of type `JSONValue`. But this creates coupling because system D needs to think about which metadata fields B and C might care about and which metadata fields B and C won't care about that should be thrown in the `metadata` field. It also comes with the same sensitive data scrubbing concerns when receiving data from an untrusted source because anything could pass through in the JSONValue.
JSString is API compatible with Text, so you can interchange them on the front end. Although, Text is far more inefficient than JSString. On the backend Text is probably best. Never use String.
Or maybe unStringify = map read . chunksOf 3 (`chunksOf` is in the `split` package.) 
&gt; Disclaimer: This is in no way secure nor meant to be used to protect sensitive information. Ironically you have implemented the most secure form of encryption, a [one-time pad.](https://en.wikipedia.org/wiki/One-time_pad) Well, as long as the key is truly random and at least as big as the data that should be encrypted. (and of course you have to keep the key secure) And coincidentally, my first ever program was an OTP encryption program as well. My first two to be precise; the second was Qt version with a nicer UI. But for some reason I used some convoluted formula for my first implementation instead of just a simple add/subtract or xor.
**One-time pad** In cryptography, the one-time pad (OTP) is an encryption technique that cannot be cracked, but requires the use of a one-time pre-shared key the same size as, or longer than, the message being sent. In this technique, a plaintext is paired with a random secret key (also referred to as a one-time pad). Then, each bit or character of the plaintext is encrypted by combining it with the corresponding bit or character from the pad using modular addition. If the key is truly random, is at least as long as the plaintext, is never reused in whole or in part, and is kept completely secret, then the resulting ciphertext will be impossible to decrypt or break. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Yeah. Thought so too but could not remember seeing it here. The project would benefit from being a little larger (shared UI routing with only one route, does not show/prove much). But the idea is there, and it looks pretty straightfwd.
Thanks for `plotlyhs` suggestion! I really can use it. And I like it.
How do folks pronounce “`_`” (as in `mapM_`) when talking out loud?
&gt; I usually don't import unqualified things from Data.Map as they clash I'm confused, aren't my imports of `Data.Map` already qualified? import qualified Data.Map as M (Map, empty, insertWith, toList) 
(I'm the maintainer of generics-sop.) A while ago, I started collecting available materials here: https://github.com/well-typed/generics-sop/issues/47 Please feel free to ask questions, too.
You are right! I never use this construction, I thought this was equivalent to the following: import qualified Data.Map as M import Data.Map (Map, empty, insertWith, toList) (it is not)
&gt; the `split` package has nice splitting primitives, so that you don't have to use `words` and replacing commas by spaces Perfect, I swapped out `map splitOnComma` with just `map (splitOn ",")`. Discoverability of these packages doesn't seem that great - despite me using Hoogle. &gt; foldr + insertWith = fromListWith Nice! &gt; you should not need to sort the output of M.toList Ah that's interesting. I see the [source definition](http://hackage.haskell.org/package/containers-0.5.10.2/docs/src/Data.Map.Internal.html#toList) of `toList` is just `toList = toAscList`. I should use `toAscList` in my code anyway for explicitness I think. 
You could also try bracketing the concurrent actions with a semaphore: import Data.Traversable import Control.Concurrent import Control.Concurrent.Async import Control.Exception traverseThrottled :: Traversable t =&gt; Int -&gt; (a -&gt; IO b) -&gt; t a -&gt; IO (t b) traverseThrottled concLevel action taskContainer = do sem &lt;- newQSem concLevel let throttledAction = bracket_ (waitQSem sem) (signalQSem sem) . action runConcurrently (traverse (Concurrently . throttledAction) taskContainer) 
Could you show me how this would work? Don't I need to `putStrLn` each item as well, and therefore need to loop over them with `forM_`?
The lifted version: traverseThrottled :: (MonadMask m, MonadBaseControl IO m, Forall (Pure m), Traversable t) =&gt; Int -&gt; (a -&gt; m b) -&gt; t a -&gt; m (t b) traverseThrottled concLevel action taskContainer = do sem &lt;- newQSem concLevel let throttledAction = bracket_ (waitQSem sem) (signalQSem sem) . action mapConcurrently throttledAction taskContainer 
You can always manually run your monadic actions in the base monad. Assuming you're using `parallel-io`: {-# LANGUAGE FlexibleContexts #-} import Control.Concurrent.ParallelIO.Global import Control.Monad.Logger.CallStack import Control.Monad.Trans.Control import qualified Data.Text as T parallel' :: (MonadLogger m, MonadBaseControl IO m) =&gt; [m a] -&gt; m [a] parallel' as = do rs &lt;- liftBaseWith (\runInIO -&gt; parallel (map runInIO as)) mapM restoreM rs main :: IO () main = do runStdoutLoggingT $ parallel' (map (\i -&gt; logDebug (T.pack (show i))) [0 .. 10]) return () 
I don't pronounce it at all usually. xs &lt;- mapM f ys vs mapM_ f ys Both can be said to be "mapM" or "monadic map" or even better, just "map" of "f" over "ys". The "_" is verbally implicit both by context and by the fact that I'm not binding anything. Rarely, I might be verbose and say "map f, ignoring the result, over ys". For example "xs is bound to the map of f across ys" or "map f over ys". If, however, you are pair programming then I suppose saying "map em underscore" might help the driver.
Hmm, interesting. I guess our choices are: foldToDict lst = M.fromListWith (++) [(rest, [item]) | [rest, item] &lt;- lst] foldToDict = M.fromListWith (++) . map (\[rest, item] -&gt; (rest, [item])) Can't decide which I like better style-wise. Is the difference any more than stylistic? 
Don't worry, there is a third way ;) foldToDict lst = M.fromListWith (++) $ do [rest, item] &lt;- lst return (rest, [item])
 putStrLn (intersperse '\t' (sort items))
No pain no gain. You always rile people up but you also get things moving. Thanks for being honest.
On top of the existing suggestions: I personally would just do `import qualified Data.Map as M` and omit the explicit import. The main reason you want explicit imports is to know where things came from and to avoid name collisions, but qualified imports have neither of the above problems and thus don't need them. I would use `for_` over `forM_`. IMO `forM_` shouldn't really exist as it is just a less general version of `for_` (but still not monomorphic so doesn't help type inference).
&gt; Start using them. To quote Nike... just do it.
One meaningful thing it does say however is that using a statically typed language is a much safer approach, since you can always drop down into more and more dynamic approaches as desired, but the opposite is not practical most of the time.
&gt; I would like to do the frontend in haskell as well. I think that's just basically a different discussion altogether.
[removed]
Constraints can be unruly to work with, and in this case [`eqTypeRep`](https://downloads.haskell.org/~ghc/master/libraries/html/base/Type-Reflection.html#v:eqTypeRep) takes two `TypeRep`s as arguments eqTypeRep :: TypeRep a -&gt; TypeRep b -&gt; Maybe (a :~~: b) At some point [`eqT`](https://ghc.haskell.org/trac/ghc/wiki/Typeable/BenGamari#PreservingcompatibilitywithData.Typeable) was considered for the API which does not depend on explicit `TypeRep`s eqT :: (Typeable a, Typeable b) =&gt; Maybe (a :~~: b) eqT = eqTypeRep typeRep typeRep With ticket [`#11350`](https://ghc.haskell.org/trac/ghc/ticket/11350) using the implicit version looks a lot like the explicit one data Expr :: Type -&gt; Type where Eq :: Typeable xx =&gt; Expr xx -&gt; Expr xx -&gt; Expr Bool ... instance Eq (Expr a) where (==) :: Expr a -&gt; Expr a -&gt; Expr a Eq @ty a b == Eq @ty' a' b' -- Same type | Just HRefl &lt;- eqT @ty @ty' = ...
Meanwhile, I've never really had to deal with JSON before, since I don't do stuff with browsers. Maybe that's part of the disconnect, but Hickey seemed to be talking about internal RPC, and why would you use JSON for that? I'm surprised they left unknown field preservation out of proto3, there must more disagreement about its value than I thought. I've never consciously relied on it, but I probably benefited. For instance, if you add a field to a DB and don't have to worry about rebuilding all the tools so they don't destroy it. There are plenty of generic tools for protos. I've never seen a proto-&gt;JSON converter, since who uses JSON anyway :P, but I've written and used python struct converters, generic diffs, pretty printers, etc. They use the introspection API and of course the caller has to have already parsed it, so the caller knew the schema. For the (uncommon) truly completely unknown "what is this binary blob" debugging situation, there's a global DB, and on-disk formats that need to get it right, like columnio, include the schemas inline.
But then the first item would not be prepended with `'\t'`, since `intersperse` just puts the `\t` in _between_ each element, rather than before each element. Also putStrLn seems to only work on a String, not a lost of strings
&gt; I personally would just do import qualified Data.Map as M and omit the explicit import. Makes sense. For my purposes though, I'm still learning what functions are from what namespace, so it helps me to remember. &gt; I would use for_ over forM_. IMO forM_ shouldn't really exist as it is just a less general version of for_ (but still not monomorphic so doesn't help type inference). That's good, it reads just a little better too.
Hate to nitpick, but Halogen isn't the Elm Architecture. Pux and Spork are Elm-likes in PS, but Halogen is something else entirely.
There is no need to `take (msg length)` of `cycle key`, since `zipWith` discards the superfluous. Why do you use `openFile` when you know of `readFile`? So many names, each only used once. Inline them. `encryptFile` and `decryptFile` look like they should do mostly the same thing, but even have different amounts of lines. Look into that, then replace them with one code block that takes `decode` or `encode` as an argument. Get `source` and `key` out of the list of arguments as you do with `command`. Even `process` deserves inlining.
Side note, you should probably use `traverse_` instead of `mapM_`, to avoid the unnecessary `Monad` constraint.
Wrong. This is a Vigenère cipher, since he cycles the key.
True, but it's still an OTP if the key is at least as big as the data since it won't cycle in that case.
Thanks folks, makes sense!
Did not see that before.
OK so I made the inner `for_`-loop and printing unnecessary by flattening the original dict into rest and items combined via `concatMap`. dictToFlatList :: M.Map String [String] -&gt; [String] dictToFlatList = concatMap (uncurry recToList) . M.toAscList where recToList r = (r:) . map ('\t':) . sort main = do let menuList = dictToFlatList . csvToDict $ csv for_ menuList putStrLn
What’s the salary range?
This advice is general. There are almost no reasons to prefer ```String``` over ```Text```.
Looks like a cool job. Shame there is no remote option so I can't apply :-( Doing FPGA using Haskell seems almost common now, there are a handful of companies publicly doing the same and probably a few private ones. 
not late actually, the thing is I have done the backend in Haskell and I would like to use the types and share some function with the frontend so I thought it would be nice to make it in Haskell also.
That's what I mean by me mixing it with `intercalate` :) Anyway, I did not understand how you wanted them printed. You can replace `for_ putStrLn xx` by `putStrLn (unlines xx)`.
Good question. Is it low enough for me to apply?
Wow, great stuff, Alexander. Thanks for sharing!
I would say you should be very comfortable with algebra—being able to recognise patterns in your code that could be written using other/better/simpler abstractions, seeing the underlying structure of code even if it has different “shapes”. For example, all of these are equivalent: fnord = \ x -&gt; foo bar (baz quux x) fnord x = foo bar (baz quux x) fnord x = bar `foo` (quux `baz` x) And can be written as a simpler composition: fnord = foo bar . baz quux Having this skill developed lets you take better advantage of the existing abstractions, like “Oh, this type I wrote is a functor/applicative/monad, so I don’t need to keep mapping/applying/binding it manually”, or “Oh, this function I wrote is a map/fold/filter/group/fix, now I can write it in 2 lines instead of 10”.
Interesting trick. However, wouldn't it be more performant to `putStrLn` each element of a list rather than first joining all the elements into one string first?
How about `MaybeT` and `(&lt;|&gt;)`? import Control.Applicative ((&lt;|&gt;)) import Control.Monad.Trans.Maybe attempt1 :: IO (Maybe Int) attempt1 = do putStrLn "attempt #1" return Nothing attempt2 :: IO (Maybe Int) attempt2 = do putStrLn "attempt #2" return (Just 2) attempt3 :: IO (Maybe Int) attempt3 = do putStrLn "attempt #3" return (Just 3) -- | -- &gt;&gt;&gt; runMaybeT demo -- attempt #1 -- attempt #2 -- Just 2 demo :: MaybeT IO Int demo = MaybeT attempt1 &lt;|&gt; MaybeT attempt2 &lt;|&gt; MaybeT attempt3 
I'm using Typesafe's HOCON format (I come from scala land). But yea so you're approach makes this whole lib completely irrelevant haha. Itd be way easier to have a json config and just use `Data.Aeson.TH` or `Data.Aeson.Generic` to derive record instances automatically. I'm gonna finish this off though for the sake of learning more about generic programming in Haskell. (I'm pretty well acquainted with shapeless from Scala)
oh nice. ill be sure to check those out. I might end up refactoring this to use `generics-sop`to get more acquainted 
That is true for any Vigenère cipher, but the key is entered as a command-line argument while the data is read from a file.
Use `parseOnly` if you will not provide more input to the parser.
good bot
The emacs plugin is moving fast. A great advantage of the LSP approach is that the client side pools the dev pool for all the languages that it supports. I am moving toward using hie/emacs as my primary dev setup. Eating my own dogfood. But sidetracked into GHC atm.
I stand corrected. Not at all nitpickingish, just wrong
From the [attoparsec documentation](http://hackage.haskell.org/package/attoparsec-0.13.2.0/docs/Data-Attoparsec-Text.html#g:2): &gt; attoparsec supports incremental input, meaning that you can feed it a Text that represents only part of the expected total amount of data to parse. If your parser reaches the end of a fragment of input and could consume more input, it will suspend parsing and return a Partial continuation. &gt; [...] &gt; If you do not need support for incremental input, consider using the [parseOnly](http://hackage.haskell.org/package/attoparsec-0.13.2.0/docs/Data-Attoparsec-Text.html#v:parseOnly) function to run your parser. It will never prompt for more input.
That's interesting. Thanks for elaborating!
why ? i do not even see any reasoning in it
There isn't one. Snowdrift.coop is a Haskell project to help fund copyleft communities and projects. The Simons have industry jobs, and contribute to GHC part-time. All other contributors are volunteers, I believe. Well-Typed funds a decent amount of Cabal development, and FPComplete funds a decent amount of work on `stack`, Stackage, and related libraries/tools. I've been mulling over a project that could help fund and coordinate Haskell community issues, but I'm mostly in the "see what other communities are doing" phase at the moment. Looks like most "other communities" get picked up by a big organization with a lot of cash, and neither Microsoft nor Facebook have taken that initiative.
&gt; The way persistent is structured wouldn't have worked for us. Our compile times would have been soul-crushing with the way the persistent's "models" file is structured -- it is one big compile bottle-neck. How often are you changing your models? We put those files in separate packages, so GHCi etc don't have to recompile it when editing other files.
There is a good clue there - if I missed the poor white man example, it means I truly did not listen carefully enough and objectively.
Holy shit, are you generating Haskell code from yaml configuration files? ... are you generating 1,200 modules of Haskell code from yaml?
With ghcjs most of the development seems to be on a branch and a ghc 8 is available through nix (nix-env -iA haskell.compilers.ghcjsHEAD, I think) even though there isn't an official release. There is also the reflex-platform which runs on top of nix. I've found it a much easier to get started with anything else ghcjs related. I can't really speak for haste, as I've not used it.
Hi, I'm the author of the record library [data-diverse](https://github.com/louispan/data-diverse). Is something like [this](https://github.com/louispan/data-diverse/blob/master/test/Data/Diverse/ManySpec.hs#L117) what you are looking for? In your case, it would be: let baz = Foo 5 ./ True ./ "str" ./ nil -- baz is Foo + Bool + String fetch @Foo baz `shouldBe` Foo 5 fetch @Bool baz `shouldBe` True fetch @String baz `shouldBe` "str" 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [louispan/data-diverse/.../**ManySpec.hs#L117** (master → caca1ce)](https://github.com/louispan/data-diverse/blob/caca1ced2cef4e904fd657253f5f25ddae62a975/test/Data/Diverse/ManySpec.hs#L117) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Relatedly, http://bentnib.org/quantitative-type-theory.html
Do you also have experience with a [`HasHandle X env =&gt; ReaderT env IO`](https://www.fpcomplete.com/blog/2017/06/readert-design-pattern) approach? Seems like roughly the same idea and allows handles to be passed implicitly. Which may or may not be worthwhile, I don't know. Generally I think Backpack should be the proper solution to this, as outlined in the blog post.
PureScript's the most usable and actively developed option right now, although it isn't properly Haskell I think it's the way to go until the story changes.
There is definitely some great overlap. Actually, I did use the "makeClassy" approach at first, and I did use `As*` classes as well, as in (MonadError e m, AsMyError e) =&gt; A -&gt; m B. My experience with the Has/As approach was as follows: * The typeclass constraints are easily satisfied with some global `Env` type, as well as some global `AppError` type which contains the more lower-level, specific errors. * Regarding this, `lens` makes defining the classes trivial, and the instance declarations are one-liners. * Type-inference works out, and the whole approach complements using `lens` nicely. Some things to think about: * Depending on the sophistication of your types, you will have to work with MPTCs/FunDeps. * In some cases, type signatures can become larger than the actual function body. Regarding compositionality. I don't buy that `Either/ExceptT` is non-compositional. After some trying around, I was quickly able to make `withExceptT` and `mapExceptT` (or `hoist` from the `mmorph` package of course) work for me. If you simply rewrap some domain-specific error into a more general application error, the argument order of the value constructors makes a great difference though. On a similar note, and more related to your actual question about using `ReaderT`, I think it makes a difference if you're working on a small to medium, or larger project. My applications usually consist of `pure leaves`, that is for instance worker functions that compute things, and an `effectful spine`, i.e. the glue code that initializes things, handles errors etc. I generally don't need any of the `Handle`s from the `ReaderT` inside the pure parts. This means that at least for smaller projects, passing things explicitly is not really that bad and you probably won't have to explicitly pass a `Logger.Handle` down a callstack ten layers deep. I would often have code like this: fun :: (HasLogger r, HasHttp r, MonadReader r m, MonadIO m) =&gt; m () fun = do log &lt;- view logFun http &lt;- view httpFun ... That is, `view` or `asks` at the very beginning. I could just as well pass the `r` explicitly. Furthermore, for a smaller project there may only be one concrete `Env` that satisfies these constraints anyway, so I think you need to reach some `critical complexity mass` before these classes add some meaningful value. So if you take this approach and remove `Has`, you are probably left with the Service Pattern. Simon Meier's article puts a lot of emphasis on the associated module structure (which again doesn't necessarily need to be this elaborate), but the core idea seems the same to me. I have also encountered this as the "Lensed Reader Pattern", for what it's worth. I think I will probably switch back to this kind of approach, but there's a few things I still have to figure out. Namely: * When does it make sense to introduce something like `MonadBalance`, like in Snoyman's article? Probably, there will have to be a lot of functions that get `IO` propagated to them by another function that may be morally pure. * Can you define a pure implementation of `data Handle m = Handle (A -&gt; m ())`? The `()` kind of is in the way here.
I'm confused about this scenario with folds. So here is a map function I wanted to do as a fold: in ghci I just type: let dis = 0.1 let xs = [4,5,6] map (\x-&gt;x-x*dis) xs --this works as expected returning [3.6, 4.5, 5.4] foldr (\x acc -&gt; (\x -&gt; x -x*dis) : acc) [] xs --this doesnt' work Now I am able to do a "work around" by just doing a let/in or a where clause but I'm confused why I can't just do this in one line with a fold statement. 
You might want to take a look at [the thread from yesterday](https://www.reddit.com/r/haskell/comments/7ax2ji/haskell_on_the_front_end/). I'd say GHCJS is pretty good, if you can get past the fact that it's not updated for 8.2.
If you're waiting for the cavalry to show up then I have news for you: *you're* the cavalry
Nearly everyone uses GHCJS or PureScript (which isn't strictly Haskell, but is mostly just much better aside from being strict). 
Regarding the instance definitions: instance MonadConc m =&gt; MonadLogger (LoggerT (STM m)) where ... contains an "illegal type synonym family application". I believe I was able to get it to work like this: (MonadConc io, STM io ~ m) =&gt; MonadLogger (LoggerT m) where ... or similar, but then you quickly get overlapping instances. Instance resolution is just complicated and reminds of the difficulties you may encounter with C++ Template instantiation or Koenig lookup. I think it's really great that you added these newtypes to `concurrency`, and I am looking forward to working with it, even if only for learning purposes at the moment. My code is bug-free after all. Ahem. But it kind of also shows that it is often advantageous to keep some polymorphism at the value-level, and I am probably going to switch back to the function record approach mentioned above. Only that my code will probably live in `MonadConc m =&gt; m` :)
Your `foldr` is making a list of functions, because you haven't applied the function you defined to any arguments. GHCi doesn't know how to display this, hence your error. Try ``` foldr (\x acc -&gt; (\x -&gt; x -x*dis) x : acc) [] xs ``` (note the `x` before the `:`)
You can do exactly the same thing you'd do in dynamic languages, i.e. tag it with job type. This is the top solution here: https://stackoverflow.com/questions/18039005/deserializing-an-existential-data-type There is a tiny, insignificant amount of boilerplate.
Luckily, my project isn't all that large (anymore, `lens` makes everything more terse). I actually refactored it to different effect handling approaches several times now, and never introduced an error. I will keep this in my set of canned responses to what makes Haskell's type system "so special".
This solution makes things very clean
Those graphs have the disadvantage that there are occasional dumps of tens of thousands of lines of code, so slow surgical maintenance (e.g., fixing a bug in 2 lines of code) doesn't even appear on the scale. GHCJS, for instance, has about 20 commits this year in the 8.0 branch. I'd say GHCJS is being "worked on" in the sense that Luite is doing occasional maintenance and development (including a lot of reported progress on GHC 8.2 support), but mainly in branches. It's definitely usable right now (but you should be aware of the sheer size of the generated code...) and somewhat widely used, even in commercial contexts. The Reflex Platform being built on top of it has definitely stoked attention. I wouldn't worry about it just fading away. But it's definitely not *active*: if you want a vibrant project with a new release every 6 months, that isn't the case. I can't answer for Haste, because I don't use it.
&gt; unless you have implemented a DSL in Haskell that to generate output to be evaluated by a middleware, and the DSL does not have type checking, and the Haskell type checker can only provide guarantees on the syntax of the DSL. Didn't get it. Ae you referring to anything that I'm doing in the video? Because I can give you a lot of first-hand counter-examples why "if it compiles, it runs" isn't true. 
Thanks for putting HIE on the map. 
From introspecting the PG schema and picking up some overrides from Yaml, yes. 
oh okay that makes sense thanks for the help!
So if I understood you correct, this question is about converting monad transformer stacks. Fortunately, the `transformers` library comes with batteries included in this regard, namely via the `withFooT` and `mapFooT` family of functions. By the way, these are generalized in the [mmorph package](https://hackage.haskell.org/package/mmorph-1.1.0/docs/Control-Monad-Morph.html). Regarding the concrete stack you gave, here's an example on how you may do these kind of conversions, with a few comments that hopefully help you in figuring out what's going on. newtype ThisError = ThisError Int newtype ThatError = ThatError String worksOnEvens :: ExceptT ThisError (ReaderT Int IO) String -- We need: ^ 0 ^ 1 ^ 2 lifts. worksOnEvens = do n &lt;- lift ask when (odd n) $ do lift . lift $ putStrLn "Get your math straight!" -- Or use liftIO. throwE (ThisError n) return (show n) runWorksOnEvens :: Int -&gt; IO (Either ThisError String) runWorksOnEvens = -- We first unwrap the outer ExceptT layer. let e :: ReaderT Int IO (Either ThisError String) e = runExceptT worksOnEvens -- Then the ReaderT layer. in runReaderT e usesIncompatible :: ExceptT ThatError (ReaderT Integer IO) Int usesIncompatible = do n &lt;- lift ask let incompatible :: ExceptT ThisError (ReaderT Int IO) String -- Doesn't fit: ^ here ^ and here. incompatible = worksOnEvens converted1 :: ExceptT ThatError (ReaderT Int IO) String -- We can use the with* functions if we're on the "correct level". converted1 = withExceptT convertError incompatible converted2 :: ExceptT ThatError (ReaderT Integer IO) String -- Now we have ThisError -&gt; ThatError, still need Int -&gt; Integer. converted2 = let c1 :: ReaderT Int IO (Either ThatError String) -- So we unwrap one layer... c1 = runExceptT converted1 c2 :: ReaderT Integer IO (Either ThatError String) -- ...use the correct with* function again... c2 = withReaderT fromIntegral c1 -- and rewrap. in ExceptT c2 converted3 :: ExceptT ThatError (ReaderT Integer IO) Int -- Return types can just be fmapped over, since Monads are Functors. converted3 = fmap read converted2 shorter :: ExceptT ThatError (ReaderT Integer IO) Int -- The more powerful map* functions can be used also: shorter = mapExceptT converter converted1 x &lt;- converted3 y &lt;- shorter return (x + y) where convertError :: ThisError -&gt; ThatError convertError (ThisError n) = ThatError (show n) converter :: ReaderT Int IO (Either ThatError String) -&gt; ReaderT Integer IO (Either ThatError Int) converter = (fmap . fmap) read . withReaderT fromIntegral It helps to work with explicit type signatures and maybe [typed holes](https://wiki.haskell.org/GHC/Typed_holes). Furthermore, people (myself included) tend to skip over the transformer basics before switching to `mtl`. While not needing to type `lift` may be convenient, it is easier to work with a concrete transformer in cases like these. 
&gt; &gt; &gt; The way persistent is structured wouldn't have worked for us. Our compile times would have been soul-crushing with the way the persistent's "models" file is structured -- it is one big compile bottle-neck. I wrote some code to turn the Persistent template-haskell into actual Haskell in separate modules. It's not really in a great form to share (pretty tied to our code, specific to one version of GHC, only handling the stuff we actually use) but it's been working well so far. 
I second `optparse-applicative`. It makes things so easy and it is incredibly elegant to boot.
No, I was just kidding. What I mean to say is, an SQL database connector is basically a DSL which you use to generate queries and unpack replies. The Haskell type checker can is basically limited to checking the syntax of the DSL, it cannot check the correctness of the data structures at the low level at which you are using it. That is a fundamental limitation of Haskell's type checking abilities. You have to first convert between between the tabular SQL data structures and the Haskell data structures before the Haskell type checker can help you. So if you make a mistake during the conversion between the SQL world and the Haskell world (which you did when you forgot to specify the dimensionality of your array data type) you are shit outta luck.
Have you tried `ulimit -n` for a quick fix? 
&gt; Another example is that kind Constraint really IMHO should unify by bi-implication rather than use the simple intensional equality check we use today. What would this look like?
I don't really see how PureScript is any more usable / actively developed than GHCJS. Both seem to be very usable and widely used.
What about compiling Haskell to WebAssembly for example? are there some advances on this?
&gt; And by that I don't mean to imply it's perfect, just that I don't know what you need in an editor, compared to myself. * Speed. Top-most concern. * Stops showing compilation errors if some other file in your dependency graph has a compiler error, thus severely limiting the purpose of on-the-fly type-checking. I've modified my workflow to depend more &amp; more on `:r` in a GHCi session, which is more predictable. * Somehow jump-to-definition and _jump back to previous point_ doesn't seem to work for me. * Jump to definition doesn't work if your file has compilation errors. IMO, it should fall-back to text-based lookup, if it doesn't get the symbol's info via GHC. * Autocomplete almost never works, or is too slow to be useful. (I could list more, if you want)
&gt; How often are you changing your models? We put those files in separate packages, so GHCi etc don't have to recompile it when editing other files. Almost every feature development starts with a change in DB models. And we tweak them during the development cycle as we don't get the DB design 100% correct upfront.
&gt; I wrote some code to turn the Persistent template-haskell into actual Haskell in separate modules. It's not really in a great form to share (pretty tied to our code, specific to one version of GHC, only handling the stuff we actually use) but it's been working well so far. Please make a video of how you use this tool. We could learn a few tricks.
&gt; Perhaps the bias of modern Western languages ... towards object rather than relationships accounts for this. Hmm, this seems like Linguistic relativity (Sapir-Whorf), which is very suspect. Is there any evidence that speakers of Japanese, Korea, Athabaskan languages, etc. are more likely to think in terms of category theory or have an easier time understanding category theory?
&gt; Huh. Stack is rebuilding those deps because ghcversion.h changed. Either Stack is doing something insanely stupid on OS X, or this is user error. Do you have any insight on how that header might be changing? I'd recommend finding a way to reproduce and making a bug report. Yes, I noticed it too. I'm fixing bugs in Opaleye atm, and will get to reporting this one shortly. My gut feel is it's because stack fails to handle an edge case -- all these packages had been fetched via the snapshot/LTS earlier, and then I cloned them and added them to my extra-deps to make a few local changes. &gt; Also, are you able to try upgrading to GHC 8.2? I've heard it fixes some accidentally quadratic things for projects with large numbers of modules. Could help your build times a lot. I'm waiting for 8.2 to stabilise. Also, I couldn't find the issue that Simon (Marlow) was referring to, so I'm not sure if those quadratic bugs have been fixed in 8.2, or not.
[The thread from yesterday](https://www.reddit.com/r/haskell/comments/7ax2ji/haskell_on_the_front_end/dpdmw15/) sort of accidentally turned into a status update and roadmap for the work on GHC -&gt; WebAssembly =P We'll do a more proper writeup soon.
I would expect calling `putStrLn` once to be more performantfaster, but I really have no clue.
You should be able to expect that `f :: Request -&gt; SockAddr` actually uses the argument, but it isn't enforced by the type checker. Similarly, you should be able to assume that a dynamically typed function with a name of "request-&gt;sockaddr" or with a comment like "returns the sockaddr of a request" takes in a request and returns the request's sockaddr, but it isn't enforced by the type checker. 
I mean you can have `Left` be the success case and have `Right` be the failure case. But something like `Alternative` as mentioned by gelisam is solid.
As I see it, the best selling point for dynamic typing is that you get lots of functionality "for free". If your request type is its own special type, then it doesn't get very much for free. You can derive a few typeclasses, and you have to implement everything else. If your request is a map with a few specific keys, then you get a ton of functionality for free, though -- you have a bunch of functions and language constructs that work on maps, and they all work on your request "type" without any issues. All you need to remember is the names of the keys. You don't need to constantly look up "so how exactly does type X work? Can I decompose it/create it? Is it a record or do I need to remember argument order?" Sure, the type system tells you when you screw that stuff up and helps you look up the answers, but having fewer things to keep track of is arguably better than having tools that help you keep track of more things.
The problem with that is the monad transformer for `Either` is `ExceptT` which would have confusing naming. `throwE` and `catchE` would mean the opposite of what you think making things potentially difficult to understand.
Beautiful! Thank you! I have all the posts saved to disk, but this will make it much easier to refer back to. Haven't checked the pdf, but I'm sure it's great. Thanks again.
Yeah that is a very good point. I don't really like that personally, I would prefer `EitherT` with appropriately named equivalent functions.
Unification constraints require you to know the kinds match in order to proceed. Once you have that the unifier can change rules based on the kinds involved. e.g. using, say, a Presburger solver when solving Nats, or using in this case the fact that entailment has a decision procedure in kind Constraint. Take one of the constraints and try to discharge the other, and vice versa. If this works, you get equality. This is enough for rigid-rigid unification. If not you get a residue set of constraints on either side, when its only on one side this is enough for flex-rigid. For flex-flex you need some place to put the excess. With what we have we can only do this for certain shapes (still more than what we can do today.) Having constraints be closed (giving it an internal Hom) should help with the rest.
It took me a while to understand it, though. It was probably the first place I had to work with "aggressively Haskell" code to do something standard. It can be intimidating :)
Are you open to hiring fresh graduates if they have open source experience?
Hah yeah I get that, though on the other hand, it was first time I actually kind of felt like I "got" applicative functors, so I think it has a lot of educational value. Just something about the extremely clean code combined with very useful, easy-to-understand nature of the desired behavior just made it click for me.