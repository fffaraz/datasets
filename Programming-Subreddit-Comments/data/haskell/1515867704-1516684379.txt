 -- unboxed vector data MVector s a = MVector {-# UNPACK #-} !Int -- offset {-# UNPACK #-} !Int -- length {-# UNPACK #-} !(MutableByteArray s) -- underlying mutable byte array -- storable vector data MVector s a = MVector {-# UNPACK #-} !Int -- length {-# UNPACK #-} !(ForeignPtr a) -- underlying pointer
You could let Haskell derive Show to get same effect as your implementation - in https://github.com/FabianGeiselhart/Tetris/blob/3b147477d2460781c605506a5f5e7c5138e584e3/src/Tetris/Color.hs
I'm not sure if I've asked this before, but the limitations of these different representations, are they fundamental in the mathematics or is the limit arising from Haskell's weaker type system and some insufficient language feature? That is, do you think it's possible to build some programming language that can have a "perfect" free monads with cheap left associated binds, best case inspection of layers, and low constant factors? (Of course, if it's not possible for that to be the case, then I'm wondering if this is a fundamental limitation of effect handling, like searching algorithms are fundamentally limited, or if we might be able to find a "better" method down the road...)
You can use the imprecise exception mechanism if you want, but you should use an `Exception` type that was built for inspection if you want to inspect it.
If you have a good explanation, please open a pull request!
I think I had a click after reading this. Thanks for the link!
I like it. Straight-forward and well-paced. Good work. For more complex CSVs, Cassava would be better, particularly if you want to support CSVs written by Excel. That would clutter this video and should be easily discoverable for a viewer who encounters that issue.
Thanks!
At that point (before typechecking) you don't have access to the expected type of a splice. But you could just pass the type as an argument, so it might look like this: $(lemma 'apply [t|(a -&gt; b) -&gt; a -&gt; b|] modusPonens)
Is there a principle that requires a flag rename?
Why would you need the type though? `getTerm` could simply output the program corresponding to the proof, and then let the typechecker complain if the generated program doesn't have the expected type.
This is exactly what I was thinking, thanks!
Thanks for making this. Curious what inspired using hspec inside of tasty? I tend to use tasty and hunit or hspec, but usually don't bother to combine them, personally. I often debate how fancy to get with tests. I usually build up lots of utility functions for matching against the result, and implement a few golden tests, but stop short of dynamic tests. 
Not starting a programming project because you're afraid you won't finish it sounds like a sure way to never start programming projects. Some of the projects which have been the most beneficial for my learning haven't been finished.
Thank you! Yes, I absolutely agree, and it was for that reason I left cassava out. Could mention it in the video description, though.
I think some of the issues with managing which effect is intended and such might be helped by additional language features. The big questions are what those features should really look like, which I don't think we know yet, and how useful they will be in practice. I would be astonished if extra language features could help with the efficiency problems; those are just hard.
this is offtopic for /r/haskell. please don't post things like this here.
Have you looked at [hspec-smallcheck](http://hackage.haskell.org/package/hspec-smallcheck)? If that's bitrotted over the last year, then I'm sure the maintainer would love a patch
Took me about three weekends.
I suspected that. I tend to get way too detailed and caught in rabbit holes when teaching people stuff. There’s an art to staying on topic as well as you do! I think mentioning Cassava in the description is a great idea, if only to satisfy folks like me ;)
It surely is a fine line to balance. Thank you for the kind feedback! :)
That is true if there are no tactics that need to look at the proof state.
I posted this to Ubuntu where it fits better: https://www.reddit.com/r/Ubuntu/comments/7q7y40/anything_below_kernel_414_is_100_unstable_with/ Apparently, with the latest Kernel 4.14.X, all Ubuntu PCs running newer CPUs are bricked. Older CPUs still work. So if you have an Core Due or Atom CPU you don't need to worry.
Wow, this is so great that I'm at a loss for words. Any chance you can do screencasts about your workspace? Vim, Intero, etc.
That issue is quite old and `text` has changed hands since. Perhaps the current maintainers would be more receptive to such a change if asked on their issue tracker.
I don't know how to get all 3 even if I get to devise the language from scratch or work completely without types.
So if a proof accesses proof state, then the corresponding extracted program can only be produced at runtime, so its type must be annotated?
The vast majority of Haskell-related papers are going to be published through the ACM, which has a reasonably clear [copyright policy](https://www.acm.org/publications/policies/copyright-policy#permissions) which specifies what an author is allowed to do. In particular, you are allowed to publish copies of the accepted paper for non-commercial purposes and you can make "major revisions" of the paper which are not subject to any of the rights you granted to the ACM.
These `Buildable` classes aren't at all the same -- in each case they're a uniform way of converting into a _different_ output type, with the coincidence that each different output type is named `Builder`. A uniform buildable class would need to be a multi-param typeclass that converts an arbitrary `a` into an arbitrary `b`. We wouldn't get code reuse out of it, and we would also have terrible inference. 
How far into the book are you? I'm on the recursion chapter, 8 I believe, and I feel like I should start building something soon to cement what I've learned so far 
Author of [Hezarfen](https://github.com/joom/hezarfen) here. In Idris, what you call the extracted program, i.e. the proof term generated by `runElab`, is produced at compile time. What I understand from what /u/Syrak says is this: you will want to write tactics that access the proof state, and an important part of the proof state is the goal type. For example you use `assumption` as an example tactic, which basically tries to match the things in the proof context with the goal type. Now, how you can get the goal type is an interesting question, because Haskell can infer the type of the TH splice in which you call `getTerm`. How can you have access to that type, I'm not sure. Maybe you can write a GHC plugin? The other approach is the one /u/Syrak suggested, requiring the user to write the type explicitly, which I'd be okay with as a user. You can come back to this problem later when the other parts are done. I'm not sure if the overall idea is feasible but you should give it a try! It would be very exciting to have this in Haskell. 
Clever!
I had my fair share of programming projects. I'm intermediate level in several languages despite not being a software developer. However, while Haskell is a hobby interest, I it is not contributing to my current career choice, and spending quality time with my kids has priority over hobby-coding in a career-irrelevant subject. Not everyone is a college student on reddit. Yes, I won't ever learn it properly, but I can read about it while commuting, but for proper programming I'd need to spend time at the detriment of my family. 
As in "one weekend" ~= 15 h spent programming?
Hehe wow, nice to hear! Yeah, sure, I've thought about it. Would probably be more "working with Haskell in Neovim" rather than "how to configure X", if that makes sense?
I'm still new to Haskell and a lot of the syntax went over my head, but the presentation style is great. Sorry to be the one to ask the boring question though... Could you please tell me what font you are using?
Yeah, it requires some Haskell knowledge to follow along with all the code. Haha, sure, it's Iosevka (https://be5invis.github.io/Iosevka/).
Still, it might be worth looking into whether all the instances in all these separate classes are completely different, or whether there is some shared pattern. If there is a shared pattern, there may be some more precise way to abstract it out, without a heavy-handed "universal" `Buildable` type class.
Around that.
I finished the Chapter about Monads. If you want to build something I recommend the first chapters of "Real World Haskell", it helped me to understand how to do I/O.
Sure, I wasn't requesting anything, I was just asking. I don't know what you mean by "side project". This has always been a well-supported production-quality library. This great new feature is additional evidence of that.
The `--exec` option is fine for simple cases. The classic unixy way of doing this is combining `find` with `xargs`.
Hi, thanks for your response! I don't know why I didn't understand this before, your explanation is so correct and obviously so that I am embarrassed I hadn't figured it out myself. It makes perfect sense! I have written a few non-trivial programs with cloud-haskell and perhaps my confidence in my understanding of the library played a role in this oversight; Something like: "Since I know cloud-haskell, my initial thought about how this code should execute is right, of course, so why isn't it working the way I expect it to?!". I was digging around deep in the `Internal` modules of `distributed-process` wondering if there was some edge case I was hitting-- instead, I just didn't quite comprehend the way I was telling the two processes to communicate, and the bug was at the surface level! Thanks again for walking through it with me.
Perhaps it's not a problem if it is replaced less regularly, especially - if replacing takes human effort and the process is not fully automated. /u/MagicMurderBagYT, could you say if it is, perhaps? - since it is sorted by new comments, so it's not like new questions would become buried. - since it is stickied at the top of the subreddit anyway, so it's not like the current one would scroll outside of hot/top. So I don't see any harm in replacing every 2 weeks, 1 months or heck, even every 6 months just before reddit would lock it. It actually might even be beneficial to coalesce the questions in larger threads, could aid with searching. &gt; Beginner Questions: Hask Anything (posted every 6 months, #3) Do you see any potential downside to lesser frequencies?
From others I've heard good things of this book: http://haskellbook.com/
Surely such a class would have at least a fun department going one way, if not both?
Nice, thanks for the contribution.
Nice! Sometimes is goes a bit fast, especially when running the suite. I believe you when you say that the tests passed, but I had no time to see for myself. Maybe just a little bit more time on those views would be nice. Anyway, great vid and cool info. Thx!
Just a data point for you to consider: I have hundreds of abandoned projects and they haven't caused me any harm at all and, on the contrary, have been quite helpful to me in learning new things. 
I've seen your question on the c++ subreddot as well, do you have a repository with your benchmarking code? Would be nice to reproduce on different machines 
I should have an repository ready, perhaps next week. It compares: * Haskell vector-algorithms * vs Julia sort * vs C++ pdqsort
It's worth digging into papers by Oleg and Nic Wu on free moaned and effect handlers. Wu has some good examples of programs that cannot be expressed with the effects + handlers approach. Tbh, in practice I don't think those programs come up much in practice. I get by just fine with purely algebraic effects and handlers.
&gt; A uniform buildable class would need to be a multi-param typeclass that converts an arbitrary a into an arbitrary b. We wouldn't get code reuse out of it, and we would also have terrible inference. But if you're fine with those restrictions, [classy-prelude's `ToBuilder`](https://hackage.haskell.org/package/chunked-data-0.3.0/docs/Data-Builder.html#t:ToBuilder)'s got you covered.
&gt; a fun department Your auto-correct has a sense of humor! &gt; Surely such a class would have at least a [functional dependency] going one way, if not both? Why? If the purpose of such a typeclass was to unify the functions which lift the string type being build into the corresponding builder, such as `fromText :: Text -&gt; Text.Builder` and `byteString :: ByteString -&gt; ByteString.Builder`, then sure. But these `Buildable` typeclasses have instances for types like `Int`, so they're clearly meant to be a version of `show` which produces a `Builder` instead of a `String`. So you can't have a fundep on such a class, otherwise you'd either only be able to convert `Int` into one of `Text.Builder` or `ByteString.Builder`, but not both, or in the other direction, you'd only be able to convert `Int`s or `Double`s into `Text.Builder`, not both.
Great idea. Please see my list [elsewhere in this thread](https://www.reddit.com/r/haskell/comments/7pj28y/scripting_in_haskell/dsiu8g8/) of some of the many other scripting libraries available besides `turtle`. A "cookbook" is allowed to be opinionated. So if you like `turtle` best, go for it. But in this case, I think it's at least worth a mention that there are many other options.
Another approach to avoiding this mistake is to use a `newtype`, e.g., `newtype Rows = Rows Int`, and then type signatures get clearer and the compiler checks those values more effectively.
&gt; Also, you still have to compile the script while you are working on it to get the most out of Haskell's type-checking, which is not in my normal script editing flow, exactly. You can have that with intero, or ghc-mod. On save or on the fly, file code is typechecked. It's blazing fast too. Changing some code that forces changes to other places do require recompile. But that should be compared more to running unit tests in python, as both are done to assure correctness of application after such invasive change.
Hey! This is something I wanted to get around to for a long time, but just didn't have the time! I just released a new version of `hspec-smallcheck` that allows you to use `shouldBe` and gives you colored diffs on test failures (don't look at the code, a clean solution would require substantial changes to `smallcheck`). Usage example: https://github.com/hspec/hspec-smallcheck/blob/14838d4da62fb659deb78256b7b2a8d1c1f23c52/example/Spec.hs#L15
Typechecking is something done during compilation, but it don't have to. Haskell have ghc-mod and other tools that are able to provide typechecking while editing file. Most common name probably would be InteliSense. Haskell have some equivalent.
Thank you.
Simply well done!
Rather than `vector`, you can use the arrays from `primitive`; those are utterly unadorned arrays.
This is not on topic. Removed.
This is awesome! Please keep them coming, subscribed!
Get in touch with the guys at [dataHaskell](http://www.datahaskell.org) 
What in particular does your research concern? I doubt it is just Haskell-related. What use-cases does it address? This might be fruitful lines of questions to determine what workshops to submit to. 
I’ve been looking the web and last changes to trello date from March 2017... I’ve joined their Gitter but nobody seems active, there’s very little activity. Thanks tho!
So pulling from github does not seem to work. It's complaining about multiple cabal files. Which makes sense as the react-hs repo contains multiple projects, how do I direct nix to the react-hs subdirectory?
This gets asked every so often, and unfortunately it doesn't seem there's much progress each time. Usually people roll their own bindings to whatever c lib they want, and I have not seen a coherent vision for how to do better. I think the best approach is: what would you want out of haskell for numerics?
Sorry for the delay `type-of-html` is damn fast: benchmarking 10/blaze time 80.25 μs (79.79 μs .. 80.72 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 80.22 μs (80.00 μs .. 80.50 μs) std dev 827.8 ns (655.2 ns .. 1.060 μs) benchmarking 10/nice time 35.69 μs (35.50 μs .. 35.97 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 35.72 μs (35.60 μs .. 35.88 μs) std dev 465.2 ns (361.5 ns .. 610.9 ns) benchmarking 10/lucid time 51.74 μs (51.52 μs .. 51.97 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 51.54 μs (51.41 μs .. 51.72 μs) std dev 532.6 ns (428.1 ns .. 691.1 ns) benchmarking 10/hamlet time 24.18 μs (24.08 μs .. 24.28 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 24.12 μs (24.03 μs .. 24.22 μs) std dev 328.4 ns (256.3 ns .. 405.1 ns) benchmarking 10/type-of-html time 16.98 μs (16.91 μs .. 17.08 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 17.09 μs (17.00 μs .. 17.21 μs) std dev 350.2 ns (288.8 ns .. 444.9 ns) variance introduced by outliers: 19% (moderately inflated) benchmarking 100/blaze time 660.4 μs (658.5 μs .. 662.6 μs) 1.000 R² (0.999 R² .. 1.000 R²) mean 663.2 μs (660.2 μs .. 669.2 μs) std dev 13.46 μs (7.677 μs .. 24.49 μs) variance introduced by outliers: 11% (moderately inflated) benchmarking 100/nice time 352.1 μs (350.8 μs .. 353.4 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 351.8 μs (350.9 μs .. 353.3 μs) std dev 3.719 μs (2.532 μs .. 6.112 μs) benchmarking 100/lucid time 428.7 μs (426.8 μs .. 430.9 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 426.5 μs (425.5 μs .. 428.2 μs) std dev 4.196 μs (3.189 μs .. 5.830 μs) benchmarking 100/hamlet time 221.8 μs (221.0 μs .. 222.5 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 220.9 μs (220.3 μs .. 221.9 μs) std dev 2.531 μs (1.924 μs .. 3.563 μs) benchmarking 100/type-of-html time 144.7 μs (143.5 μs .. 146.3 μs) 0.999 R² (0.999 R² .. 1.000 R²) mean 144.5 μs (143.6 μs .. 145.4 μs) std dev 3.139 μs (2.575 μs .. 3.651 μs) variance introduced by outliers: 16% (moderately inflated) benchmarking 1000/blaze time 6.300 ms (6.282 ms .. 6.321 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 6.314 ms (6.302 ms .. 6.329 ms) std dev 41.10 μs (30.64 μs .. 54.53 μs) benchmarking 1000/nice time 3.543 ms (3.518 ms .. 3.567 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 3.555 ms (3.537 ms .. 3.596 ms) std dev 79.50 μs (46.15 μs .. 161.3 μs) benchmarking 1000/lucid time 4.292 ms (4.269 ms .. 4.317 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 4.277 ms (4.262 ms .. 4.296 ms) std dev 51.33 μs (41.03 μs .. 69.30 μs) benchmarking 1000/hamlet time 2.179 ms (2.173 ms .. 2.187 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 2.178 ms (2.171 ms .. 2.186 ms) std dev 26.35 μs (20.17 μs .. 34.26 μs) benchmarking 1000/type-of-html time 1.451 ms (1.444 ms .. 1.459 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 1.455 ms (1.449 ms .. 1.462 ms) std dev 20.05 μs (14.68 μs .. 26.51 μs) Benchmark perf: FINISH Well done :-). This isn't taking advantage of type-level strings for the static stuff in `type-of-html` either (because I figure that this wouldn't be fair since the other benchmarks all, afaik, escape the static text even though it doesn't need to be escaped). I wonder what's keeping `nice-html` from being quite as fast as `hamlet` and `type-of-html`. I think the most obvious thing is to blame the `stream` function... 
Have you tried [their docker image](https://github.com/tensorflow/haskell#build-with-docker-on-linux)?
Not yet. I've already picked an alternative for the project I wanted it for.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/post-rfc/.../**sotu.md** (master → 9aec61c)](https://github.com/Gabriel439/post-rfc/blob/9aec61cb08bded38ea454b52b5b1a3bb7426db17/sotu.md) ---- 
It's awful.
It’s not that bad, Haskell is a fast language that’s a pleasure to write in, with lots of strong libs for this sort of stuff, e.g. Frames, attoparsec, etc. However it’s nothing like scientific Python ecosystem.
Hmm, the answers here make things sound worse than I think they are. This is certainly not an area Haskell is particularly mature in, but it's not non-existent and there's many really interesting libraries around. For neural networks there's [Grenade](https://hackage.haskell.org/package/grenade), which allows you to define networks by describing their structure in the type system, meaning you can't really get the shapes wrong, and you can ask the library to build you a random network matching your defined structure. Huw gave a talk about the library at Compose Melbourne last year: https://www.youtube.com/watch?v=sPjA6lS0GlQ Then there's the amazingly awesome [Accelerate](https://hackage.haskell.org/package/accelerate) library for running computations on the GPU which feel like working with lists in Haskell. There's now quite a lot of work that's happened to bind to other libraries for reading and writing data (it supports [vector](https://hackage.haskell.org/package/vector), [gloss](https://hackage.haskell.org/package/gloss), has libraries for dealing with [colour](https://hackage.haskell.org/package/colour-accelerate), fits, bignums, linear vector spaces, and even an example of a "password recovery" tool for looking up MD5 hashes). Repo has backends for LLVM native compilation so you can run highly vectorised code on the CPU, as well as the LLVM pix backend for compiling for NVIDIA GPUs (I thought there was some work on an OpenCL backend at some point but not sure what's happened with that). [repa](https://hackage.haskell.org/package/repa) comes from the same team as `accelerate` which lets you define computations on multi-dimensional arrays and have the execution happen in parallel. [hmatrix](https://hackage.haskell.org/package/hmatrix) is still being maintained afaik, and is probably still the best interface to the BLAS and LAPACK libraries. It would be nice to see some of the above libraries bind to BLAS and LAPACK too, I'm not sure what the state of that is. /u/cartazio has done some work in this area too, but he's also pretty busy so a lot of it is unreleased. And as others have mentioned there is the DataHaskell project. I haven't had a look at what they're up to these days, I haven't had time to keep up, but it looked promising in the beginning. If you need to access external C libraries, it's not particularly hard to bind to them
The same question bugs me again and again from time to time. 
&gt; You just lose a lot and gain... practically nothing. You do gain a lot : not having to fiddle with the functor/applicative/monad instances every time you modify your monad. If you want to add an effect you just add a member to your instruction type, and a corresponding case to your interpreter.
99% of the time, I'm not writing my own monad instances. I always default to writing a newtype around `ReaderT` or `StateT` or something. My effects get defined in terms of those.
Thanks, this is a pretty comprehensive comment. I have also seen the tensorflow library, which seems remarkable to me, due to the fantastic underlying library that covers pretty much every computation. I think a good job would be to keep working in top of the abstractions of this first layer so that it's a more "haskeller" experience. I'll check out all the packages you point out.
It's the same to me. It's actually a question regarding all of the functional languages, but I guess I am more keen to work with haskell than others (Clojure is also near the top of the list). And the current state seems to be quite parallel. Maybe Clojure is a bit more advanced thanks to mikera's work in core.matrix and related.
I'm sure you'll agree that coordinating volunteer's spare time into a "coherent vision" is pretty hard. A number of people have been pushing native approaches as an alternative, ranging from the numerical groundwork ( https://hackage.haskell.org/package/accelerate-arithmetic , https://github.com/dlewissandy/lambda-blas , https://github.com/ocramz/sparse-linear-algebra ) to more advanced type acrobatics ( https://github.com/tonyday567/numhask , https://github.com/mikeizbicki/HLearn , https://hackage.haskell.org/package/linearmap-category )
Is there a big speed/memory difference in the unreleased 0.3 (which according to https://github.com/wrengr/bytestring-trie/#bytestring-trie uses array mapped tries instead of patricia trees, seems like a large change under the hood)
`Option` is a `Monoid` if the internal type is a `Semigroup`: instance Semigroup a =&gt; Monoid (Option a) On the other hand, `Maybe` is a `Monoid` if the internal type is a `Monoid`: instance Monoid a =&gt; Monoid (Maybe a) The later is more restrictive. For example, you may be able to easily define a `Monoid` instance for `Maybe (Last Int)` with `mempty = Nothing`. However, the later definition disallows it because `Last` is not a `Monoid`, only a `Semigroup`.
My guess would be that it's because of the constraints on the type parameter: Monoid a =&gt; Monoid (Maybe a) Semigroup a =&gt; Monoid (Option a) It's not hard to change `Maybe` to also use a `Semigroup` constraint - it was made the way it is because `Semigroup` wasn't part of the standard library at first. Changing it might mess with backwards compatibility, though, because `Semigroup` isn't a superclass of `Monoid`. This means that if I have a custom data type `Foo` with a `Monoid` instance, then `Maybe Foo` would suddenly no longer be a `Monoid` instance.
The `Monoid` instance of `Option` has the form Semigroup a =&gt; Monoid (Option a) whereas for `Maybe` it has the form Monoid a =&gt; Monoid (Maybe a) If you take a look at the [source](http://hackage.haskell.org/package/base-4.10.1.0/docs/src/GHC.Base.html#line-324) of the latter instance, you can notice, that allthough it requires a `Monoid a` constraint, it only makes use of `mappend` and not of `mempty`: instance Monoid a =&gt; Monoid (Maybe a) where mempty = Nothing Nothing `mappend` m = m m `mappend` Nothing = m Just m1 `mappend` Just m2 = Just (m1 `mappend` m2) So `Semigroup a` would have sufficed. But `Semigroup` was added to `base` much later than `Monoid`, so there wasn't always a possibility to express this. In practice, having `Semigroup a` instead of `Monoid a` as a constraint, will allow this instance to work for even more `a`'s, as every `Monoid` is a `Semigroup` but not vice versa.
Darn it, I knew we would get 3 answers at once ;-) It would be great if reddit could display how many people are currently writing an answer.
To simplify these answers: You'll notice that the `Monoid` instance for `Maybe` doesn't use `mempty`. So `Monoid` is just needlessly restrictive; `Semigroup` would get the job done and allow non-monoid types.
We have some work that can benefit from a freer mind that those of the senior techs. And we also want to contribute our bit to the signal of "cool companies do functional programming". It will help companies like ours and future generations of developers. 
My mistake, I didn't realise it had multiple projects in the repo. In that case you can just append the subdir after the `fetchFromGitHub` (which just returns a store path where the repo is cloned): let react-hs-src = pkgs.fetchFromGitHub { owner = "liqula"; repo = "react-hs"; rev = "48f437029cc58b7c707179688e619cd53134cce6"; sha256 = "1cbnir4ycjpfbypkqignicn5faz1815qknixsh7kdkx0vvvzhjkm"; }; in (self: super: { react-hs = self.callCabal2nix "react-hs" "${react-hs-src}/react-hs" {}; });
Lots of answers already, but just in case this helps someone with the intuition, here's how I like to view it: A semigroup is something that's almost a monoid, but is lacking an identity element. If we combine a semigroup with Maybe, we can use it to provide the identity element via Nothing. So there's not really any reason to require a Monoid instance on the type inside the Maybe, because we don't need the identity element from the Monoid since we can use Nothing. 
There is a bug: The yellow square `O` piece shows up in the next box, but never actually makes it onto the playing field. As it is added, a new piece is generated which is then placed instead.
In 8.4, `Semigroup` becomes a superclass of `Monoid` and the instance for `Option` becomes the instance for `Maybe`. You can try this today with the preview for 8.4 that is out now. Option will then simply exist for backwards compatibility for a few releases before being removed.
in base-4.11 (coming with GHC-8.4) we have instance Semigroup a =&gt; Monoid (Maybe a) where See https://github.com/ghc/ghc/blob/3382ade3eb7ce09737d52e7c1f3ecc3431bf00fb/libraries/base/GHC/Base.hs#L423
So 8.4 is going to change the behaviour of `Monoid Maybe`?
In 8.4, `Semigroup` finally becomes a superclass of `Monoid` with a law stating that `mappend = (&lt;&gt;)`. So assuming you actually obey that law, the new instance instance Semigroup a =&gt; Monoid (Maybe a) will be indistinguishable from instance Monoid a =&gt; Monoid (Maybe a) except now it just works for any `Semigroup`.
/u/pigworker [might say](https://www.reddit.com/r/haskell/comments/30s1t2/proposal_make_semigroup_as_a_superclass_of_monoid/cpvdco1/) that `Option` is _not_ a very good monoid for `Maybe`, and that `First` would be a better choice.
Was mentioned. Thanks anyway.
I know. And deleting more than one line doesn't work either.
&gt; Last is not a Monoid, only a Semigroup. I would like to "qualify" this statement: [`Data.Monoid.Last`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Monoid.html#t:Last) _is_ a `Monoid`, while [`Data.Semigroup.Last`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Semigroup.html#t:Last) is only a `Semigroup`.
Right. I had done a better job using another internal type which is only a `Semigroup` and not a `Monoid`, such as `Max Integer`. 
&gt; if I have a custom data type `Foo` with a `Monoid` instance, then `Maybe Foo` would suddenly no longer be a `Monoid` instance. Nah, we'll also add `Semigroup` as a superclass to `Monoid`, so `Maybe Foo` will still be a `Monoid` instance, but your `Monoid` instance for `Foo` will complain that you still need to write the `Semigroup` instance for `Foo`.
True! 
Quick question, what is the purpose of that repo? I always used packages coming from the main repos in Arch for my Haskell stuff.
&gt; So assuming you actually obey that law, the new instance What I'm trying to ascertain is if I had code using `Monoid (Maybe a)` in the past, could anything break? Who has to obey that law? The `Monoid a` bit?
The `Monoid` on `a` needs to be compatible with the `Semigroup` on `a`. This has been a law since day one for the `Semigroup` class, though, so all existing instances work, and new instances folks write to get around the 'missing superclass' error that'll start showing up now 3-4 years after we started this process, should follow suit.
I have no particular objection to adding the instances to `First`. We've added a bunch of "obvious but missing" instances to base over the last few years and Ryan GL Scott's work on `base-compat` has backported them quite well.
Right. So if you're used to the scientific python ecosystem, which I think most people who ask the question are, then it's awful.
I personally prefer hspec because of the [visual diffing](https://i.imgur.com/VBy7Q7U.png) offered by e.g. `shouldBe`, as opposed to just using `==` which will only tell you whether the test failed or not. 
 $ ghci GHCi, version 8.2.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; let nan = 0/0 Prelude&gt; nan NaN Prelude&gt; compare 0 nan GT Prelude&gt; compare nan 0 GT Prelude&gt; nan &gt; 0 False Prelude&gt; 0 &gt; nan False $ cat pi.hs -- The horror of Haskell intervals with floating point. integrate :: (Num a, Enum a) =&gt; (a -&gt; a) -&gt; a -&gt; a -&gt; a -&gt; a integrate f x0 x1 dt = dt * sum (map f [x0,x0+dt..x1]) pi' :: (Floating a, Enum a) =&gt; a -&gt; a pi' dt = 4 * integrate (\x -&gt; sqrt (1 - x^2)) 0 1 dt mean :: Fractional a =&gt; [a] -&gt; a mean xs = sum xs / fromIntegral (length xs) main :: IO () main = do putStr "Pr(being screwed by Haskell numerics) ~ " putStrLn $ show $ mean $ map (fromIntegral . fromEnum . isNaN) $ map pi' [0.00001,0.00002..0.1] $ ghc -o pi pi.hs $ ./pi Pr(being screwed by Haskell numerics) ~ 0.4872 
In that case, I'd just recommend to use Hackage directly with a statically compiled cabal or use stack. I think Arch specific repos for libraries aren't that valuable. cabal new-build removes completely cabal hell, and stack also solves the same problem. I would expect to have in Arch repos just compiler+tooling, and relying on those tools to fetch libraries. And also publish in the Arch repos user facing application.
That would discourage authors who didn't get here first. I want more answers since each brings its own perspective and discussion
Maybe I misunderstood the history of the project. I assumed he refreshed test-framework as a favor to the community, not because he wanted to own this project. I also thought since the project has one contributor, that no one else is motivated to contribute. Obviously, the project is doing well.
tasty-hspec vs hspec, not hspec vs other test libs.
Try getting a job in academia when your papers only exist on your personal website or some toy journal... Some famous academics have the option to choose their journals. Junior academics, not so much. The system is broken but that doesn't mean we don't have to play by its rules. It will take a long time for common sense to erode the power the journals have as the arbiters of prestige. CS seems to be getting there with the way conferences have such importance though, but I wouldn't really know.
TensorFlow has Haskell bindings, provided by Google. They aren't official, but the TensorFlow project is the one supporting them, for whatever that's worth to you.
Yes that one seems very promising thank you so much.
One potentially great reason for using Haskell for numerical computing is a mind-blowing feature of Haskell that I asked about a while back: [For those of you familiar with lots of languages other than Haskell: Do you know of any other language that treats things like 2, 3, pi as general numbers (Num or Floating in Haskell) that can in the end act as any number type?](https://www.reddit.com/r/haskell/comments/4e7y4j/for_those_of_you_familiar_with_lots_of_languages/) Whatever I do, I try not to litter my codebase with `Double`s. It's a very dangerous practice IMO. (To learn why, watch the first *2 minutes and 30 seconds* of [this talk](https://www.youtube.com/watch?v=LJQgYBQFtSE).) If you find that in your language of choice writing more generic code is hard, maybe it's not the greatest language for this purpose.
Thanks for doing this. The trac issues linked in the post are a wonderful example of how difficult it is to find a default that works well for everyone. I'll certainly be using this flag in my workflow in the future.
The reason described in the ticket for this not being the default seems like a *bad* case of [Every Change Breaks Someone's Workflow](https://xkcd.com/1172/). Seems like the vast majority of users do not care about showing source paths.
I agree, that it would at least discourage some authors to write answers while others are in the progress of writing answers. But it could also avoid the repetition notable in "our" 3 answers. I'm also not sure that it would be that much discouraging, as in my experience people here also tend to offer different perspectives when they encounter answers already written e.g. an hour ago. 
:) You're making it less idiomatic and probably slower than typical. - You use show on Int instead of just using the Int (surely a performance hit) - You use OverloadedStrings and Text instead of ordinary stringliterals (a lot more elegant, not so sure about performance, but I think it would be better) - You use renderText instead of renderByteString (renderByteString should be absolutely the normal case) I'll check out the changes this night and report if it's any good.
That's what I thought too but I didn't want to waste too much time arguing without actual basis.
Hi, thanks for the information! I switched to ArchHaskell repositories sometime back in August, as per suggestion in #haskell-beginners. I had a range of typical issues that plagued every arch user since they went with the shared libraries - build problems, version problems, missing libraries problems, etc etc etc... since I run xmonad, this resulted with sometimes not being able to boot into graphical environment (I don't keep "backup" WMs). Since I switched over, issues went away. Alternative was what I recently found right here (stack-static &gt;&gt; install cabal &gt;&gt; keep haskell stuff organized with stack instead of pacman), but I liked the idea of repositories better. I might just go with your advice, tho, since I'm not sure if those repositories will find another home.
Thanks for suggestion! I've created issue: * https://github.com/haskell/text/issues/214
So, with the changes mentioned below, it'll get a lot cleaner and about 15% faster. Just for fun: using type level strings for the static stuff makes it 6 times faster :)
From this post: &gt; Haskell! (the first project hit production today) From the job description: &gt; We use a variety of languages, but most of our backend systems are written either in Haskell or Scala. I'm not quite sure how to interpret this...
Nice! I'm putting this in my global stack config ASAP.
working on it right now actually, i meant to the other day but the wife surprised me by telling me last minute I'd be speaking about our adoptions all weekend. 
[removed]
What if we added a .ghc file where flags like this would live? That could let everyone establish their own default workflow
As that discusses, there's this tool: https://hackage.haskell.org/package/cabal-dependency-licenses We only allow free software licenses of various sorts on hackage (c.f. https://hackage.haskell.org/upload). As such, to my knowledge, all such licenses are compatible in the sense that artifacts that are generated from combinations of them may be produced and distributed under the union of their terms. So we don't need to worry about "compatibility" as such, but rather just assembling the compound licensing information. The linked ticket from that thread also indicates there's some interest in generating that from cabal-install directly if someone wanted to cook up a PR: https://github.com/haskell/cabal/issues/879
I think that's good, but it's only half the problem. It's not very discoverable. A newcomer is not going to know both that this file exists and that this flag exists. I probably wouldn't have learned about the flag if it weren't for this blog post. Plus, taking it to the absurd, I don't think we can just give GHC all the worst defaults under the argument that the user can change it in a config file; that would make for a horrible user experience especially for newcomers. So I think the default should cater to the most common denominator, with something like a `.ghc` file to satisfy people who want something different.
thanks. I'm using vscode+haskelly and it's working out well enough
only notes i have was tar xJf not xjf when uncompressing the xz ghc files he has listed the /linux-android-toolchain.config host_arch says darwin not linux all pretty smooth though... actually android studio was more of a pain in the ass 
 ghc-options: "$locals": -fhide-source-paths What is this "$locals" business?
I understand the disbelief :-). Most of what we have in our department at the moment is using Scala. As I wrote we just pushed our first Haskell project to production and we are starting the second one this week (a Kotlin project - the only one - that started as an experiment and that's beyond repair). We also have a ton of other projects to start this year with a strong management buy-in to use Haskell if we find enough FP minded engineers.
This is great! Inclined to make it a stack default.
See https://docs.haskellstack.org/en/stable/yaml_configuration/#ghc-options + https://docs.haskellstack.org/en/stable/yaml_configuration/#apply-ghc-options Means it applies to local packages, all the stuff in `packages:` that aren't extra-deps
I've written such a tool on top of stack dependencies --license in a few lines at work before. 
lousy, they can't even get arrays right smh. cue downvotes.
Sorry if I sounded like lashing out, but I do care quite a lot about this issue and it's frustrating that so much effort is not given due credit. And yet I only mentioned a handful of libraries off the top of my head, other great ones might be out there. I would love to learn how the Rust community organises (I saw this very nice community page: https://community.rs/resources/), do you know more /u/ElvishJerricco ?
What does this return in python or c++?
You're right -- I should have remembered better than this :-)
Once we had such a `.ghc` file then it is much easier to say that the "advanced" options should hide behind flags, since there is a way for advanced users to get the defaults they want -- the whole reason for the back-and-forth was that without such a config file, there's _no way_ to get more verbose defaults. I'm sure folks who wanted the more verbose standard would have happily gone with the quieter one had they been able to set up their own `.ghc` file.
I would argue the opposite. Miraculously, current emacs and vim users manage to get started somehow **despite** the horrible defaults. Imagine how much higher the adoption rate could be of your favorite editor (and many other apps) if the defaults were much more enjoyable for a newcomer. While still retaining the same level of configurability for advanced users.
Thanks for example! I could try to experiment with Backpack. I don't mind working in GHC-8.2 and cabal-only land :)
English.
This is not Dhall!
I personally use partial type signatures in my xmonad config to bypass this issue.
Let's see, according to the documentation, the types of [`avoidStruts`](https://hackage.haskell.org/package/xmonad-contrib-0.13/docs/XMonad-Hooks-ManageDocks.html#v:avoidStruts) and [`smartBorders`](https://hackage.haskell.org/package/xmonad-contrib-0.13/docs/XMonad-Layout-NoBorders.html#v:smartBorders) are: avoidStruts :: LayoutClass l a =&gt; l a -&gt; ModifiedLayout AvoidStruts l a smartBorders :: LayoutClass l' a =&gt; l' a -&gt; ModifiedLayout SmartBorder l' a So in the composition `smartBorders . avoidStruts`, the type of `avoidStruts`'s output, `ModifiedLayout AvoidStruts l a`, unifies with the type of `smartBorders`'s input, `l' a`, so `l' ~ ModifiedLayout AvoidStruts l`. I would thus expect the type of `myLayoutHook` to be: myLayoutHook :: ( LayoutClass l a , LayoutClass (ModifiedLayout AvoidStruts l) a ) =&gt; r -&gt; l a -&gt; ModifiedLayout SmartBorder (ModifiedLayout AvoidStruts l) a myLayoutHook i = smartBorders . avoidStruts So why does your compiler suggest a more concrete (and incidentally much longer) type?
I like it! What properties are you wanting for the `obs` primitive? Currently, from squinting at the type-checking algorithm, it doesn't look decidable and in fact looks vaguely like you could reduce it to whatever the lambda calculus equivalent of the halting problem is. (But this is just a general red flag that pops up in my brain whenever I see things like "check what some computation results in" and words like "when" or "while"). I'm thinking these issues can be avoided pretty easily in non-recursive data types, but I've got no idea about recursive data types; being able to build mutual recursion using `obs` and "normal recursive types" would probably be a good indicator that `obs` gives you too much power. Good stuff :)
This looks kinda like the explicit equality proofs in the zombie paper by Weirich et al. 
Basically, it really sucks. I have started some work to improve the situation and suggested a team effort. If anyone is willing to work in this direction, please write me.
&gt; Okay, for backwards compatibility, `Monoid` does have some instances, but their behaviours are inconsistent with each other, so please don't use them. Should this read `Maybe` instead of `Monoid`? Also, I like that interpretation a lot, it seems much more elegant to me; I've never consciously realized it, but the inconsistent behavior of Maybe instances do seem very off now that you've pointed them out.
&gt; I suspect a similar phenomenon explains why most GHC developers still tend to prefer cabal-install, even after most of the community moved to Stack. I don't think that's quite it. Stack is a facade (in the architectural or design pattern sense), and it's very complicated, so it's a very leaky abstraction. If you do anything complicated outside the common use cases, you tend to run in to problems. Cabal is a tighter abstraction (fewer leaks, IME), which is more useful (by virtue of being more reliable) for devs with projects whose packaging is complicated.
Or just use the types and write adaptors. It’s work but it’s not hard. The power of Haskell and similar languages is you can connect and combine stuff. There’s def an overhead to differences, but those differences are because these prjects are designed to serve different needs and foci!
It's been a while since I dug into this, so I may not be 100% accurate, but if I remember correctly, in X Monad, a `Layout` is a type class that includes functions for laying out windows and handling messages. A `LayoutModifier` basically wraps an existing `Layout` to let you tweak the behavior in a reliable way. This way you can take a simple existing layout and change the behavior without rewriting it. For instance I use a `LayoutModifier` that makes my `Layout` only show borders on windows if there's more than one window on the screen. This lets me use any preexisting `Layout`, or combination of `Layout`s but still tack on the desired behavior reliably. OPs `LayoutModifier` is a lot more complex than mine!
I think, when beginning instance clauses, the target instance’s required fields should automatically appear (e.g. writing `instance Monoid` would fill the next few lines with `mempty = ` and `mappend = `).
A great debugger.
Why was this upvoted so much? It looked quite basic when I checked it. Do I have to be a VIM user to appreciate it? PS: I am doing research in GUIs and I am interested in the value of this. Perhaps as a target Demo for doing GUIs.
I'm interested in UIs as well (for example my [Concur Library](https://github.com/ajnsit/concur) which explores a new paradigm for building UIs). Lately I have also been thinking about a better IDE for Haskell. I think an outlining IDE like [Leo editor for Python](http://www.leoeditor.com/) could work very well with functional programming languages like Haskell. It would allow you to focus on functions instead of files, and have the ability to cross reference functions from anywhere. When saved, it would save a folder/module structure that automatically handles all imports correctly. One can dream!
I miss purescript IDE's [autocompletion](https://github.com/nwolverson/atom-ide-purescript#autocomplete) the most.
It's already selected which instance to use?
I don't know about C++. Matlab seems to consistently avoid including a final point outside of the interval [0,1], as does arange from Numpy. I haven't seen their exact implementations. The Enum instance for Doubles is pretty strange. It uses repeated addition, to generate the sequence of values. The change suggested [here](https://ghc.haskell.org/trac/ghc/ticket/13552) would be much more accurate. And then, the current instance implements a threshold that allows a final point outside the range if it is "close enough" (less than half the increment above the designated final point). I would assume that the latter was done to make sure an approximation to the right endpoint is included when uniformly partitioning an interval, even if numerical errors make it a little too large. There might be situations in which making sure to include a slightly too large right endpoint is the right thing to do. But this feature is a disaster for the above integral. But I don't really think a general Enum instance for doubles can be expected to do the best thing in all circumstances in the face of rounding.
What is a "bound expression"? I also see `* -&gt; Bool` three times, shouldn't that be `Bool -&gt; *`?
Thanks for the feedback!
&gt; we can take inspiration from Emscripten, as they have some syscall implementations of their own. We can't just use all of their syscalls because their blocking model is completely different (sleep(10) pegs your CPU at 100% for 10 seconds). That's when threading is disabled (which it is by default). If you enable threads in emscripten (USE_PTHREADS) then it will use a libc with threading support, using Atomics and SharedArrayBuffer to implement a proper sleep etc.
This can be ambiguous for classed that have different sets of complete definitios, like Foldable.
I had no idea! Does it actually implement threads? Or does that just let them do proper blocking syscalls?
I'm curious as to why `renderByteString` should be the normal case. Is it because you're rendering out all of the HTML instead of just the text inside of the tags?
Maybe it could act like autocomplete, offering one suggestion for each minimal complete definition.
We create a Worker for each thread, and the wasm is loaded in each one I believe - there is some spec work to improve that, but it's still in progress. Sharing the memory is done by importing it into each Worker's wasm, after passing it to the Worker. (Sharing a Memory is possible with the new wasm-SharedArrayBuffer APIs for browsers.)
- type-informed autocomplete - automatic, cross-module name refactoring - List of errors across the whole project for me to jump to (implying something like `-fdefer-all-errors--even-recoverable-parse-errors`) - Reliable jump to definition, even across packages (maybe just jumping to haddocks for that?) - Import management and speculation; i.e. autocompletion should be able to speculate about suggestions that would even require an import.
&gt; (Sharing a Memory is possible with the new wasm-SharedArrayBuffer APIs for browsers.) I didn't know this. [The MDN docs](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/WebAssembly/Memory) do not indicate that you can initialize memory with your own `SharedArrayBuffer`.
Automatic import management.
&gt; I didn't know this. The MDN docs do not indicate that you can construct Memory with your own SharedArrayBuffer. You can do [the opposite](https://github.com/kripken/emscripten/blob/incoming/src/preamble.js#L1145), though: create the Memory with the `shared` property, and then the buffer you get from it is a SharedArrayBuffer. &gt; How does it handle the stack? I guess you just initialize each thread with its own stack pointer global to make sure they don't all point at the same stack? Yes, each thread gets its own stack.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [kripken/emscripten/.../**preamble.js#L1145** (incoming → 2fabdd0)](https://github.com/kripken/emscripten/blob/2fabdd07618c29dea6b38c3d72272feeec74bc52/src/preamble.js#L1145) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dsrz3zh.)
How much of this depends on emscripten-fastcomp? Would it be possible to rip out emscripten's syscalls and use them with the upstream LLVM backend?
3 and 4 work today with haskell-ide-engine
It's mostly developed with fastcomp, the LLVM wasm backend doesn't have threads support yet (but once it does, all the libc implementation should all just work with it).
 instance (MonadIO m) =&gt; MonadHttp (ExceptT Network.HTTP.Req.HttpException m) where handleHttpException = throwError instance (MonadIO m) =&gt; MonadHttp (MaybeT m) where handleHttpException _ = MaybeT $ return Nothing f :: IO (Maybe BsResponse) f = runMaybeT $ req GET (https "reddit.com") NoReqBody bsResponse mempty g :: IO (Either HttpException BsResponse) g = runExceptT $ req GET (https "") NoReqBody bsResponse mempty 
Okay thanks, I'm starting to understand. So `obs b = True in T` is a way of saying that at this point, we can assume that `b` is `True` and carry on with `T`; it is a kind of assertion that adds information for typechecking. Then isn't it necessary to check not only that `b = True` makes the bound expression reduce to that `obs`, but also that this is the only value of `b` that can reach that part of the program?
Something that will just write the most likely fix for my program based on deep learning, genetic programming or similar. Something that, when it finds a compilation error will backtrack and create the most likely correct program for me.
Cool! I'm surprised at #3. I would have thought anything using the GHC API would barf just as GHC barfs on parse errors. So it can type check a function in the same file as a function that fails to parse?
Ah no, it does fail on parse errors. I wish it didn't. But making a parser that keeps going despite parse errors is a mathmoth task, especially for a language like haskell. 
What else does the LLVM wasm backend need besides the atomic primitives?
That would be fantastic. 
To structure things a bit more clearly and reduce (some) boilerplate, we could define a type familiy that converted a type-level list into nested `Choose`, and another that applied a type-level list of layout modifiers to a base layout (instead of repeatedly nesting `ModifiedLayout`).
&gt; The problem is that implementing inductive types is complex. The thing you are pointing at is but one possible implementation of one possible extension (an open universe of datatypes). A closed universe of datatypes desribed in terms of codes for strictly positive functors, a decoding function and a least fixpoint constructor is much more manageable.
Speed. A lot of functionality is there in existing tools (IntelliJ Haskell plugin, for example), but all tools I've tried inevitably slow down on just moderately sized projects.
Document annual goals and "force" consensus. The best thing about the Rust community is their strong will to compromise and reach consensus IMO. That and Rust documentation is actually taking seriously, as opposed to Haskell documentation which often sends you off into a forest of PDFs and blogposts. Or even worse: "types are documentation". 
What would you suggest as a simple implementation of inductive types?
For me it would be better automatic type signatures. Currently, all of the tools I have tried presumably use something GHC based to inspect code. The problem is that GHC essentially ignores `type` declarations since they are just synonyms. This results in the automatic types completely ignoring the `type`s I have defined which makes the automatic type signatures useless as I now have to delete half of the provided type signature and retype it with my custom types. This seems like it might not be that hard to implement by e.g. just using a find/replace type model. Enumerate all of the `type` declarations in scope, and if the right side of a declaration has a match in the type suggestion, swap it in before displaying to the user. --- On another note, I would like performance profiling to be integrated better. If I run a profile build, it would be nice for all of the relevant performance metrics to be shown in line with the functions. Would be even nicer to just be able to click some profile build option (maybe fill out a mini form with command line arguments, check boxes for the tests to run, etc.), and it automatically builds the stack project with all of the performance metrics and otherwise allows you to automate running all of the different memory etc. benchmarks while you walk away from the computer. Perhaps pair this with some sort of history tracking so you know how your project performance differs over time.
*Great work!* I love breaking changes. It means there is new stuff going on, improvements.
Agda's seamless support for case splitting and typed holes. It achievable in Haskell but many packages don't support it, like Interro.
Also, automatically adding imports or language pragmas for missing things.
Cf. [Chapter 4](http://oleg.lib.strath.ac.uk/R/?func=dbin-jump-full&amp;object_id=22713) of pedagand's thesis. You can also look at the [corresponding Agda code](https://pages.lip6.fr/Pierre-Evariste.Dagand/stuffs/thesis-2011-phd/model/html/Main.html).
Syrak's original example is one way other values could reach the same part.
Well, doing the whole definition is nice in that it's actually possible. There's no way to encode what the minimal definitions are in Haskell.
Yeah, it does, and I think Atom supports the feature, but last I checked, GHC-mod didn't play nicely with Stack? I haven't checked in a while, so maybe that's changed.
I don't have experience with numerical or ML libraries in haskell, but the way I usual search for packages is to: - go [here](http://hackage.haskell.org/packages/) - CTRL-F "neural net", open matches in tabs - browse within the sections which had some matches (in your case "AI" and Machine Learning"), open in tabs - eyeball descriptions, upload date, author, etc - then for packages that seem interesting check out docs and github (for open issues, and to get a sense of popularity) - optional: check out reverse dependencies here: http://hackage.haskell.org/packages/
I've not used [grenade](https://github.com/HuwCampbell/grenade) in anger, but it is written by some lovely people and looks to provide a very nice interface.
For compilation errors, `ghcid` and its VS code extension work very fast for me (on projects up to 20k loc).
I doubt it, at least not without extending the type system in some way. You need some way to detect the context, but the context can be arbitrarily far away.
My library [hfnn](https://github.com/quickdudley/hfnn) is maintained but immature. There's still a lot of things I still want to implement in it (including softmax) but it works well for what it does.
I actually have an idea on how to get closer to syntax-error-fault-tolerance: 1. Break the file up into top level bindings (very easy based on mandatory indentations) 2. syntax-check each TLB individually 3. filter for only the ones that are valid 4. pass those along to type-checking Ta-da! Each top level binding then can be said if it contains syntax errors, type errors or are fully valid. Minor side-effect: will likely get complaints of certain identifiers not being in scope due to syntax errors in their definitions.
I think it's hard to recommend anything other than the tensorflow libraries. Everything else is just a toy in comparison. Haskell is still really waiting for someone to come up with a DSL which can use it's types effectively, the tensorflow bindings for example are still relatively weakly typed. There is [typedflow](https://hackage.haskell.org/package/typedflow) - however it is a generator for python (maybe it can be adapted to use the tensorflow bindings directly)! 
Ooooooh, that seems nice. Thank you so much.
Yep I do quite agree with you, I can't just assume that, but I'd love to see a counter-example if only for curiosity. If anyone has an idea, please let me know.
How do you reject this one: boolInduction = λ (P : * -&gt; Bool) -&gt; λ (T : P True) -&gt; λ (F : P False) -&gt; λ (b : Bool) -&gt; obs b = True in T 
https://github.com/tensorflow/haskell
&gt; * all GHC dependencies not maintained by GHC HQ What do you need from us maintainers?
I've used HNN, and it worked for me. Didn't much compare it to other libs though.
I much prefer this sortable, searchable table view to the single-page full package list: http://hackage.haskell.org/packages/search?terms=neural
I think these kinds of config files are not great. It took me a very long time to discover that there's the ghc config file that actually determines which linker is used and so on for [this](https://www.reddit.com/r/haskell/comments/63shj7/link_with_the_gold_linker_for_faster_build_times/). I would prefer if ghc only took command line flags. You can see them in `strace`, from that getting a clear understanding of all inputs into ghc is very helpful for debugging. I also think that cabal and stack files are already sufficient amount of config files, and adding more would make things more confusing.
* Call hierarchy To find all locations that call a function, and to find all possible call paths from one function to another function.
sources: https://github.com/luna/luna
Touché
&gt; That way, there is a very clear goal, no chance for bikeshedding, so that people can get at least started implementing anything they could in Python (which is a lot) Yes I absolutely agree. There's a lot of api improvement that can be done but I think implementing numpy directly is the best start.
Very hard indeed! I don't think it's for lack of trying, but that we have such a huge api possibility to choose from. I think porting an existing api directly will help narrow the focus and get us pretty far, as it seems other communities have mostly succeeded in unifying around a few common apis. We can improve from there.
Github has tags? Where?
Why not just use [language:Haskell to search](https://github.com/search?q=language%3AHaskell&amp;type=Everything&amp;repo=&amp;langOverride=&amp;start_value=1)?
Sorry for I haven't provided the whole definition of that function. ``` myLayoutHook i = smartBorders . avoidStruts $ myCircleLayout i ||| myMadLayout i ||| myNormalLayout i ||| myFloatLayout i ||| Full myNormalLayout = tallDefaultResizable shrinkText . themeOrder myMadLayout = mirrorTallDwmStyle shrinkText . themeOrder myFloatLayout = accordionDecoResizable shrinkText . themeOrder myCircleLayout = circleDwmStyle shrinkText . themeOrder ```
&gt; Computers are able to gather and process huge amount of information in a fraction of second, **something a human brain is just not designed to do** The comment in bold is wrong. The human processes processes huge amounts of information--simply think of all the information required to process when we ride a bike or drive a car and consuming around 20 watts. The curse of poorly written medium articles is very much alive and well. 
This is pretty cool. It's refreshing to see streaming done without the free Monad. We're these all benchmarked with 8.2? Not sure how I feel about the use of `Alternative` and `Monoid` constraints solely for their `empty`/`mempty` values. Seems like an abuse of the typeclasses and unnecessarily restrictive.
That's fine too, you can ignore my mention of the haskell based tags/topics. It would just be nice to be able to see a bunch of sample usages of postgresql-simple, ffmpeg c APIs, and anything people have written connecting to solr.
"Manage topics"
Set missing TLBs to undefined and you're doing even better. If the type sig is present and parsable, keep it.
It's never safe and you shouldn't do it.
To expand on this, you'd do something like this: {-# LANGUAGE PartialTypeSignatures #-} {-# OPTIONS_GHC -fno-warn-partial-type-signatures #-} myLayoutHook :: Int -&gt; _ {- layout type -} myLayoutHook = ... Or, a simpler answer: {-# OPTIONS_GHC -fno-warn-missing-signatures #-} myLayoutHook = ... 
I was recently linked to [an interesting talk](https://www.reddit.com/r/haskell/comments/75as61/monoidal_parsing_edward_kmett_scala_world/) about this. The idea you're talking about is breaking haskell programs up into distinct chunks which can be parsed independently, and then combined into a full parse tree. This looks a lot like a Monoid, doesn't it? Anyway, that's what the talk is about, and you might find other interesting related ideas in it.
Here are the safe ways to use `unsafeCoerce`: * `unsafeCoerce :: a -&gt; a` * `(unsafeCoerce :: Any -&gt; a) . (unsafeCoerce :: a -&gt; Any)`
I would concede that that statement of theirs is not as precise as it could be. However, I think the author might be trying to convey that certain kinds of information processing tasks are many many orders of magnitude quicker done by a computer than by us. E.g. taking the accurate sum total of a hundred thousand numbers. Good luck doing that in your head. GHCi, even in interpreted mode, can do that under a second with `sum [1..10e5]`. I haven't had the chance to read the rest of their article yet, but I can imagine visual interfaces being able to strike a finer balance between human and machine, catering to each other based on where they are most efficient &amp;mdash; just like you point out in your example, processing large amount of visual information is an area our brain is quite adept at.
&gt; Not sure how I feel about the use of `Alternative` and `Monoid` constraints solely for their `empty`/`mempty` values. Same. Why haven’t we split out `empty` and `mempty` into their own typeclasses if they’re useful on their own? Their laws can be defined in relation to other typeclasses just as well as they’re defined currently. Just brainstorming here: class Semigroup m where (&lt;&gt;) :: m -&gt; m -&gt; m class Nil m where nil :: m type Monoid m = (Nil m, Semigroup m) -- a.k.a. Semigroup1, Nil1 class Alt f where (&lt;|&gt;) :: f a -&gt; f a -&gt; f a class Empty f where empty :: f a type Alternative f = (Alt f, Empty f) -- Semigroup2/IxAlt -- Nil2/IxEmpty -- Monoid2/IxAlternative -- Semigroup3/BiIxAlt -- … With laws like: * `nil` must be the identity for `&lt;&gt;` if a type has instances of both: `forall (x :: m). (Monoid m) =&gt; x &lt;&gt; nil = nil &lt;&gt; x = x` * Folding an empty(1) foldable produces an empty(0) result: `forall (Empty f, Foldable f, Nil m) =&gt; fold (empty :: f m) = (nil :: m)` I guess it might be more of a usability/inference issue than a technical one. But for example, I’ve encountered cases in my own work where a `Default` instance made sense but a `Monoid` instance didn’t—because `mappend` didn’t make sense, wasn’t needed, or was too hard to define—but I’ve never had a `Monoid` where `mempty` would have been different from `def`. 
Demo gifs on their official site is cool. 
While you may be right, I don't think this delegitimizes their message or tech at all. Not everyone is a good writer. I can forgive someone for making an unimportant offhand remark they didn't realize needed backing up.
let safelyUnsafeCoerce = id in ...
Thank you very much for making this. I will be incorporating this into my workflow.
Hmm.. I did not understand how this can be used I basically want to do an iterative binary search for the lower and upper index of the slice. And the algorithm is straightforward if I have a `[a]` or `array i a` interface. Currently I have implemented the algo on `[[a]]` which is a bit clumsy. using `array i (array i a)` would be better I guess..
We will just need to make sure that a release is cut at the time the first alpha is released. Hopefully there will be little need to bump submodules after that point so nothing more will be needed. In practice this just means that the whole submodule-bump process is just shifted forward by a few months.
I would love to see this adapted into a programming language for games and presentations.
Just to clarify, I'm a maintainer of a GHC dependency (the time library), not a GHC maintainer. Currently I provide a "ghc" branch of the time GitHub repository, which points to the source that made the latest release of time. If you keep your submodule pointing to that, everything should be good.
`unsafeCoerce` is only safe for `newtype`, but we have a safe `coerce` for that now.
I am mostly reading from reynold. the essence of algol, the essence of reynold, gedanken - a simple typedless language based on the principle of completeness and the reference concept, Separation Logic: A Logic for Shared Mutable Data Structures. They vary greatly in difficulty to read, but essence of reynold &lt; separation logic &lt; gedanken &lt; essence of algol &lt; type abstraction and parametric polymorphism (which I try to read but fail). The first two is almost fun while the last two is hellish. Surprisingly, I found sep logic the easiest, even though I never learned about that.
Hey there's some uses when you have undecidable types that it can't correctly typecheck.
I guess one thing that's worth noting is that the induction only makes sense because you universally quantify over all types in your definition of booleans, but your operator doesn't use this. You need to use that information to get a valid notion of induction.
&gt; Target uses Haskell too. Where? Like geographically, where?
For those interested, you can install the visual IDE from official sites (http://www.luna-lang.org/). Looking at github, it is almost completely written in Haskell.
I've found a couple more: * `unsafeCoerce :: Maybe Void -&gt; Maybe a` * `unsafeCoerce :: Either a Void -&gt; Either a b` And other ones along the same lines.
If you look at [the `bound` README](https://github.com/ekmett/bound/) there is a toy lambda calculus expression type data Exp a = V a | Exp a :@ Exp a | Lam (Scope () Exp a) If you want to keep track of the name of the bound variable why not data Exp a = V a | Exp a :@ Exp a | Lam String (Scope () Exp a) 
I would prefer to see a more technical post on this reddit, this one is full marketing-speech with very few actionable content. And it is full of objectionable claims like this one: &gt; How can we design a language and be sure it does not limit people’s expressiveness? How can we allow users to both create custom reusable components and deeply modify existing ones yet keep the interface simple and not require them to be programmers? The idea is to allow the user to incrementally change the perspective. Instead of resorting to an underlying programming language, we can allow to literally dive into every component and use the same (or very similar) language to describe how the sub-components communicate. Diving further into nested components allows users to gradually proceed from high to low levels of abstraction on demand. Really? "Lambda the ultimate abstraction" is not really a new idea, is this the level of thinking that will "revolutionize the way people are able to gather, understand and manipulate data"?
Technically they are correct, since the human brain wasn't "designed". :)
I've had success with that kind of approach. 
Very interesting - though I must say I don't like the use of the cute metaphors, to me they detract from the understanding of the library. Call me ignorant but I'd never heard what a `Sommelier` is, let alone understand it's meaning in the drinkery library! 
Any user reviews here? 
Work off of a live Debian drive
Woohoo! This is excellent news to convince people to properly support alpha releases (not sure it's necessary, but for unknown reasons they were resistant before)
That's what Sixten uses, with a `NameHint` type instead of `String`.
The problem here is that ghc-mod hasn't caught up with the newest GHC version yet. You'll probably have to either wait a bit for a new release or install GHC 8.0 instead of 8.2.
I have to say this looks impressive. When doing data-processing tasks, I do imagine graphs like this in my head. I wish they have a walkthrough video on how to use it in a concrete project.
As [catscatscat mentioned](https://www.reddit.com/r/haskell/comments/7r0776/how_to_get_running_well_on_windows_package/dst8zas/), try changing your global stack resolver to lts-9 with this command: `stack config set resolver lts-9` Make sure you don't run it in a folder with a stack.yaml file, otherwise it will update that file instead of your global config. Visual Studio Code also has a few Haskell extensions, [this](https://marketplace.visualstudio.com/items?itemName=alanz.vscode-hie-server) one worked well for me.
https://www.youtube.com/watch?v=H6-IQAdFU3w Another one!?
The spirit of the types being the same still applies. 
This is probably a case I would allow to slide in practice, but unsafeCoerce as documented does not technically guarantee this will work.
I only skimmed your question but it looks similar in spirit to [a Stack Overflow question](https://stackoverflow.com/questions/39057576/mutually-recursive-syntaxes-with-bound) which I asked a while ago.
advanced data structures by Peter Brass
Right now: * The design and implementation of Feldspar - E. Axelsson et al. * Compiling embedded languages - C. Elliott et al.
(/u/tomejaguar already gave the main idea, but I have a bit more to say about this trick, so I'll finish my long comment anyway) To recover the full source, you just need to make sure your AST contains enough information to represent everything you want to restore, including the whitespace, the comments, and the variable names. In particular, while `bound`'s representation uses anonymous names for free variables in order to provide capture-avoiding substitutions, you can easily remember the original name by storing it next to your binder. For example, here's a version of the lambda calculus in which lambdas are annotated with the variable they bind, as if we were representing variables using strings, but then we go ahead and use `Scope` to represent them anonymously anyway. data Term a = Var a | Lam String (Scope () Term a) | App (Term a) (Term a) -- omitted: a bunch of instances... -- | -- &gt;&gt;&gt; term -- Lam "x" (Scope (Lam "y" (Scope (App (Var (F (Var (B ())))) (Var (B ())))))) term :: Term a term = fromJust $ closed $ Lam "x" $ abstract1 "x" $ Lam "y" $ abstract1 "y" $ App (Var "x") (Var "y") Here I am hardcoding an example term, but in your refactoring application, you would of course obtain the term by parsing the original code. Right now the variable name and the anonymous names are unrelated, and also the `Show` instance looks nothing like the original code. Let's fix both of those issues by writing a `render` method which uses the original name when printing bound variables. -- | -- &gt;&gt;&gt; putStrLn $ render term -- (\x -&gt; (\y -&gt; (x y))) render :: Term String -&gt; String render (Var s) = s render (Lam varname body) = let body' = instantiate1 (Var varname) body in printf "(\\%s -&gt; %s)" varname (render body') render (App e1 e2) = printf "(%s %s)" (render e1) (render e2) Renaming a variable is now as simple as changing the name associated with a binder. -- | -- &gt;&gt;&gt; putStrLn $ render $ rename "x'" term -- (\x' -&gt; (\y -&gt; (x' y))) rename :: String -&gt; Term a -&gt; Term a rename varname (Lam _ body) = (Lam varname body) rename _ Var{} = error "rename: expected Lam, found Var" rename _ App{} = error "rename: expected Lam, found App" Is this renaming capture-avoiding though? It doesn't seem like it: -- | -- &gt;&gt;&gt; putStrLn $ render confusingTerm -- (\y -&gt; (\y -&gt; (y y))) confusingTerm :: Term String confusingTerm = rename "y" term But in fact, those two `y`s are internally distinct! They are represented using two different anonymous bound variables, it's only our `render` function which makes it seem otherwise. -- | -- &gt;&gt;&gt; putStrLn $ render fixedTerm -- (\y' -&gt; (\y -&gt; (y' y))) fixedTerm :: Term String fixedTerm = rename "y'" term So this technique does allow you to perform capture-avoiding substitutions in the sense that the names you give to your variables won't affect the transformations you make to the AST, it is still possible to accidentally implement refactorings which, _once rendered_, change the meaning of the program. After all, if the user does ask your tool to rename `x` to `y`, we have to do _something_ with the other `y` if we want to avoid capture, such as automatically renaming the inner `y` to `y'` or something, but we haven't described our renaming policy anywhere. We could certainly make `rename` smarter, but here's the second trick: if we implement our renaming policy in `render` instead, we no longer have to worry about captures in any of our refactoring implementations! -- | -- &gt;&gt;&gt; putStrLn $ render' term -- (\x -&gt; (\y -&gt; (x y))) -- &gt;&gt;&gt; putStrLn $ render' confusingTerm -- (\y -&gt; (\y' -&gt; (y y'))) -- &gt;&gt;&gt; putStrLn $ render' fixedTerm -- (\y' -&gt; (\y -&gt; (y' y))) render' :: Term String -&gt; String render' = go Set.empty where go :: Set String -&gt; Term String -&gt; String go _ (Var s) = s go ctx (Lam varname body) = if varname `Set.member` ctx then let varname' = varname ++ "'" in go ctx (Lam varname' body) else let body' = instantiate1 (Var varname) body ctx' = Set.insert varname ctx in printf "(\\%s -&gt; %s)" varname (go ctx' body') go ctx (App e1 e2) = printf "(%s %s)" (go ctx e1) (go ctx e2) 
"Barman can be converted to a Tap" - this is a very confusing analogy
owait, I knew this existed, but forgot about it: http://lpaste.net/79582 thanks!
This actually works for [any law-abiding Functor](https://hackage.haskell.org/package/void-0.7.2/docs/Data-Void-Unsafe.html).
Join the DataHaskell chat https://gitter.im/dataHaskell/Lobby ! there are a bunch of people interested in game playing and reinforcement learning
I wonder why bound is on hackage, but strongly-typed bound isn't? Might be worth sending a PR to bound, so you won't forget about its existence next time.
I will!
Haha, I've only barely come to grips with plain `bound`, so that will take some time to grok. Are those classes (`HFoo`) provided by any other well-used packages? Also, I'm not sure this is exactly what I want. I'd rather not use funky GADT-like things if I can make do with something closer to vanilla Bound, even if it means reimplementing stuff. Is the `unsafeCoerce` really required?
How does it compare with [Streamly](https://github.com/composewell/streamly)?
&gt; Is the `unsafeCoerce` really required? No, [my variant](http://gelisam.com/files/strongly-typed-bound/src/StronglyTypedBound.html#HoasExp) doesn't use it.
It is also safe for types like `Proxy` or `:~:` which have type arguments that don't appear in any constructor. The main use is a "trust me" when you can prove that two types are equal, but the compiler can't.
The critical mass is mainly in Sunnyvale, but the team has some very talented remote workers. So "everywhere". 
I have developed and I am actively using this: https://github.com/GU-CLASP/TypedFlow This allows to describe the models in Haskell and generates tensorflow python bindings. So ATM you have to prepare your data using python.
The newer versions of stack automatically run stack setup afaik.
&gt; though about multi threading in haskell &gt; [...] &gt; assuming that we have a network of nodes and every node should be able to connected to all other nodes in the network Sounds like you want to talk about distributed programming, not multi threading? &gt; the way I have approached the problem Which problem? You have described an environment but not the problem you are trying to solve in that environment.
Is this what https://hackage.haskell.org/package/bound-2.0.1/docs/Bound-Name.html is for?
Is the choice heuristic or completely deterministic? Assuming I'm targeting a specific version of GHC on a known architecture it seems workable. Obviously it shouldn't be done for library code but I can't think of a safer way to get the same performance.
It looks like it is!
How is this awkward? import qualified Data.Vector as V foo i = V.fromList ["a", "b", "c"] ! i
One point: &gt; Particularly, while editing a Haskell source file I will just get something like [snip] Looks like your tooling is incompatible with the colorful error output in 8.2.
Or foo = (V.fromList ["a", "b", "c"] !)
I consider that self-evidently awkward, so I'm not sure how to answer your question. Apart from syntax issues, the programmer can't be sure the compiler, given a large list, wont make us pay double duty by carrying around that large list during compile stages only to output, in the end, run-time code that starts with a really large list that it must iterate over to pack into an array.
The case I'm imagining is trying to convert a deeply nested sum type like `Map MyKey (Set [Either a b]) -&gt; Map MyKey (Set [These a b])` without traversing the structure. You make a good point though, I haven't benchmarked yet. What kind of constructor reasoning can get blocked by coercion?
Some would argue with that even (though I wouldn't).
I think one can do something similar with `do` and `OverloadedStrings`: foo :: SomeFancyMonadBuiltOutOfString foo = do "one" "two" "3" 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [commercialhaskell/intero/.../**intero.el#L1273** (master → 322b3c0)](https://github.com/commercialhaskell/intero/blob/322b3c017153a536ffa3559b64fc1ac16af19a69/elisp/intero.el#L1273) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dstnzt7.)
Also, look at the code you've written. From an implementation standpoint it's a perfectly reasonable thing to do to make a small array like that, but in practice, in haskell, nobody is going to write that code en lieu of foo 0 = "a" foo 1 = "b" foo 2 = "c" This is for a variety of reasons. The tabular nature is more readily apparent and there's no need to reach outside of the prelude and it is just plainly visually more appealing.
I had that exact same setup with lts-9.x, and after switching to lts-10.x, I saw a message telling me that the haskell layer tried to install an older version of intero but failed due to version conflicts. Installing the latest version of intero using `stack install intero` succeeded, but had no effect on spacemacs. The solution was to update my haskell layer using the "Update Packages" button which spacemacs displays on startup. Since I see you've already tried to update your packages, maybe you managed to break your setup even more in one of your other attempts?
Real World Haskell. Then probably finish tackling the 99problems-haskell. Did the first 6 after "Learn you a Haskell for Greater Good". Needed help with 5 and 6. So back to studying then tackle as many as I can until I get stuck. Then repeat the process. 
How large is this list we're talking about?
Auto import and browsing modules. It's 2018, I shouldn't have to keep browser with haddocks opened just to know what functions I can use and in what module they are. 
Maybe a few thousand. Does it matter?
Syntax issues? It's literally array syntax. Not sure how to more directly represent what you want. As for performance, just factor the vector out into a top level binding and you can be 100% certain you won't be recomputing arrays all day.
&gt; nobody is going to write that code en lieu of I might. That kind of pattern matching is a pain in the ass to update, should the list need changing. I just don't see how your proposed syntax is any nicer than using list syntax and wrapping it with a basic `flip (!)`
I feel entitled to expect I can elide computing arrays at start up. I can't think of any other common compiled languages with this prohibition. As for what could directly represent what I want, I pretty clearly showed you in my post. My syntax is less cluttered than yours, it's also more apparent at a glance what's happening than yours, thus it has less cognitive load. I have my doubts that you can't see this for yourself. Are you sure your objection isn't more to do with the costs my syntax imposes? For example, reserving the word "array" as syntax when it currently is a reasonable lambda variable? Adding another syntactic form to the compiler is another cost. If you feel the costs out-weigh the benefits, I can buy that, but I don't buy it that you can't see my syntax is more appealing at a glance.
flip ... not awkward at all. /s
I think if every single haskell programmer had the goal of minimizing the syntax for indexing const lookup tables, this would be a good change. But my point has been that I don't see Haskell's existing feature set being an impedance to this task for *any* Haskell programmers. This just isn't even remotely a large enough issue to warrant a language level change, especially since it's already very easily doable without the syntax.
right in the "welcome screen" (*spacemacs* buffer) there should be a option to update the packages - this will find updated intero packages - just answer "y" (update the shown packages) and it will download it - after you just have to restart emacs (it will compile them on startup so that might take some time - it will tell you - with vim bindings it's as simple as &lt;space&gt;-&lt;q&gt;-&lt;r&gt;(?))
I am not an expert on either spacemacs or emacs, and I don't know anymore why I chose it over emacs+evil-mode either. So how can I "give it a try", short of editing the `.el` file myself? When I make a new stack project and open a source file, spacemacs will install `intero-0.1.24` locally. Note that I edited my OP with regard to the message containing all the color output. Does that change anything?
This might be the case, and I am not afraid to reinstalling things if this will fix it. Regarding `stack`, is there some additional metadata I missed? Should I reinstall spacemacs itself?
"A few thousand" is a number that you can measure so it matters. [Here](https://gist.github.com/chrisdone/16fc00f3239358f9afeb7912f6bccc3e) is a benchmark which measures how long it takes to construct a 3,000-element vector and and also measures vector lookup versus case lookup which seems to differ by about 2 nanoseconds due the bounds check (`unsafeIndex` makes them equal). I feel like anything over n thousand is going to be a compile-time burden. It already takes 7 seconds to compile that file because of the massive case. It'd be easier to put it in a file and load it at runtime.
I think *far* more Haskellers have the goal of minimizing `case` syntax than minimizing the syntax of const lookup tables; enough to potentially warrant a language feature.
Have you tried the `develop` branch of spacemacs?
Is there some deep reason why that can't be fixed though?
For `Vector` in particular, the compiler would have to effectively evaluate a ton of library code at compile time to reduce it to primitive calls only, then it would have to recognize these in a pattern that it can substitute with a constant. Both of those are hard, and both are very different from GHC's current optimization strategy.
[Category Theory for Programmers](https://github.com/hmemcpy/milewski-ctfp-pdf), nicely typeset into a PDF.
No offense, but I don't grok what you are saying and would prefer somebody else explain.
I think hmatrix is something like this - not direct copy of numpy, but some compromise - no fancy types, but pure simple linear algebra. p.s. I think haskellers can take an example of [owl](https://github.com/ryanrhymes/owl) which is new ocaml "numpy" like library. 
For GHC to interpret `V.fromList [1..3000]` as a constant array of data, it has to know exactly how to convert `V.fromList` into such a constant array. The first fundamental problem is how vectors are represented in memory. At the end of the day, they're basically just `Array#`s, which are a primitive type with (AFAIK) no support for compile time constants. So you have to give GHC a way to talk about constants of type `Array#`. Then you have to make sure that `V.fromList` can be converted into this by the compiler. But to do this, GHC would have to recognize some pretty gnarly patterns in how `Array#`s get constructed, as `V.fromList` is a recursive function with a lot of layers in between it and the eventual `Array#`, so it's just probably not feasible for GHC to reduce this to something it can make constant. I think the only way to do this would be to expose a syntax for `Array#` literals to Haskell, then add some support for rewrite rules to rewrite list literals in to `Array#` literals. Then `vector` could have a rewrite rule do this.
Seems like something for Template Haskell, or until it becomes an extension.
&gt; I think the only way to do this would be to expose a syntax for Array# literals to Haskell So you are saying that a little extra syntax for arrays can actually make some programs compile a lot faster? Hmmmmm.
Well A) That's completely different from your suggestion, and B) Compile time would be slower than just leaving `V.fromList` unevalutated/unrewritten; runtime performance might benefit negligibly. So no, I don't think this is a good argument for adding the feature I suggested. Was merely trying to answer your question.
I've been able to do something like in the past for System Fw - it wasn't mutually recursive, but I don't think that matters. I'll have to write this up early next week - I tried just now, but it's a bit too much for me to enter on my phone. It does involve using either classy prisms or smart constructors to keep the whole thing nice to use, so that might be a factor for you. 
If compile time isn't faster, you weren't answering the question. This is why I prefer somebody else explain.
&gt; I don't see why an efficient array lookup shouldn't be more common. What is the use case, really? This special syntax maps small non-negative integers to some statically known collection of values. Isn't that strangely specific? It seems like a code smell, to borrow Martin Fowler's phrase. Suppose you apply this function: where did that small non-negative integer come from in the first place? If you know it's in this particular range, shouldn't it have a more descriptive type? This magic syntax isn't appropriate when you choose the integer with some knowledge of the values listed here. In that case, you'd want to refer to the list of values in whatever code that produces this number (in which case you want to name the array anyway). Otherwise you've got logical dependencies between distant parts of your code. It's also not a good syntax to use when you care at all *which* small non-negative integer maps to which items. After all, making people count lines of code to decide which input numbers map to which outputs is quite reader-hostile, if it matters. So you'd be far better off writing f 1 = itemOne f 2 = itemTwo and so on. I'm not saying there are no use cases remaining. Just that I can't imagine one.
Commit [83a3e10b7e8a66c2c507ecaee0f17960727525c3](https://github.com/alanz/ghc-mod/tree/83a3e10b7e8a66c2c507ecaee0f17960727525c3) works with ghc 8.2. For the entire set of binaries that Atom needs, clone and build [haskell-ide-engine](https://github.com/haskell/haskell-ide-engine) which pulls in this version of ghc-mod. You might want to change stack.yaml to use the latest lts instead of nightly though. All stuff here and all dependencies (explicitly versioned) work with the latest GHC.
/u/catscatscat /u/brynser Thank you both so much, that's exactly the issue. I wish anything told me this but I'm glad you could help :)
What if the syntax disallowed non-constants? Then it could hint to the compiler that this is just a big dumb lookup table and speed up compile time. The current situation for large tables is that they cannot be compiled in reasonable time unless we link in a module from C or some other language.
Aye aye, sir.
Nice! I'll take a look at it :) 
It seems to serve a similar purpose, but this implementation looks a lot fancier. Normally you use `Scope ()` to say that you're binding a single anonymous variable, because `()` has a single inhabitant. With `Bound.Name`, you'd instead use `Scope (Name String ())`, which still says that you're binding a single anonymous variable because `Name "x" () == Name "y" ()`, so `Name String ()` also only has a single inhabitant, kind of. The difference is that instead of writing `Var (B ())` to refer to that anonymous variable, you'd write `Var (B (Name "x" ()))`. You could write `Var (B (Name "y" ()))` elsewhere to refer to that same variable, there's no consistency guarantee, so I like the `Lam String (Scope () Exp a)` approach better.
Continuing to be an ass is not going to convince anyone you're not being an ass.
I have 10 working. I suggest the following: - Switch to lts 9 latest. Install all the dependent packages. - Switch to lts 10 latest. Install the dependent packages that are in lts 10 / will build successfully. (Some aren't so you just use the version you installed from lts 9) - Update spacemacs to latest - Update spacemacs packages to latest Note: Sometimes intero won't install your projects packages ok. It should work if you `stack build` and `stack test` from the command line to ensure they are installed.
Yeah, that's why I never got on spacemacs bandwagon. It lags behind emacs. My vanilla emacs setup with intero works just fine with ghc 8.2.2 
Would be even better to have versions `base-8.4.1` for GHC-8.4.1.
Glance at gargoyle-postgresql. You might find it applicable. Consultants set it up for use in my codebase, kind of interesting how lightweight/self-contained the database installation is.
Because an [identity element](https://en.wikipedia.org/wiki/Identity_element) is only useful with respect to a binary operation. The `Nil` and `Empty` classes are meaningless in isolation. They have no interesting properties that we can reason about. On the other hand, a binary operation can be meaningful without an identity element. `&lt;&gt;` and `&lt;|&gt;` are associative, but not commutative, and that is useful. Superclassing reflects that an identity element implies a binary operation.
[The nice thing about preludes is that you have so many to choose from...](https://xkcd.com/927/)
With rebindable syntax, `do` is not just about monads and does not have to follow monad laws.
With rebindable syntax, `do` is not just about monads and does not have to follow monad laws.
That xkcd is included in the README. 
What was wrong with n previous prelude replacements? 
Well you see, this one is a Snoyberg Original™
As well as this one: * https://github.com/snoyberg/safe-prelude
I must admit I haven't used it to implement any actual language. There is always a tradeoff between [making illegal states unrepresentable vs maintaining very complex types](https://youtu.be/8n4A0qXb15Q?t=5m56s), so I agree that implementing a complex type checker in the types might bring more difficulties than benefits. But my guess is that this complexity threshold has to be way higher than just the simply-typed lambda calculus.
Have you considered [tmp-postgres](https://github.com/jfischoff/tmp-postgres#readme)?
The `unsafeCoerce` was mostly a shortcut because I was writing it in the middle of explaining something to somebody.
HOAS borrows binding forms from the host language. The little example folks are referring to here borrows a bit of the type checking as well. It isn't very useful for describing languages that can do more than Haskell or even as much as haskell w.r.t. types.
It says build error, also I'm using windows and I want to figure out a decent non db speciffic approach.
So what? Acknowledging the problem doesn't fix it.
I see problem with this approach. You want to share some state with multiple concurrent threads right? You might have to use TVar for that.. There is a nice talk about this here https://www.youtube.com/watch?v=KZIN9f9rI34 
I like a lot of this, but I'm extremely hesitant to recommend FPCo's view on "exception best practices" and the `RIO` Monad. Although I think these approaches are absolutely critical in some contexts, I don't think they're what one should reach for by default; instead prefer `Either` over exceptions, and mtl-style classes over `IO`. As such, this seems like Yet Another prelude I disagree with. But it's certainly much closer and more modest than most of the other ones. It's possible that this prelude merely eases those practices instead of requiring them, while still providing benefits for people who don't use them by default; in which case, it could be quite good.
Sounds great! When is this workshop happebing, again?
"They" as in unspecified authorities. And what they mean is that one just needs to keep tryinging, but once you get there it will feel like the last help you got was the magic bullet, when it might not be "better" then earlier things you read, it was just that the ideas had sunk in enough that one more explanation made things click.
I dislike "A standard library for Haskell".
Awesome From my past experiences, the time to rebuild library, rebuild test suite, run test suite for a given unit test suite is too slow. I have been focusing more on ghci reload of test suite and run test main, and having the test suite forgo depending on a library and referencing source files directly. This changes testing from an asynchronous workflow, to almost part of your repl development.
Walmart/jet f#... We'll see who wins (:
Forgot about that! Let the games begin :-)
I recently discovered [NixOS tests](https://nixos.org/nixos/manual/index.html#sec-nixos-tests), which I think are a really excellent way to do this kind of thing. Nix is really friendly to CI, since making the environment is pretty much as simple as "Have nix installed." NixOS tests let you configure VMs using NixOS's declarative module system to configure services and environment on the VMs, then runs a script you provide to command these VMs to do things. The problem is that you kind of have to buy into Nix to do this. I *believe* you can run these tests on any Linux, not just NixOS, but you definitely can't do it from macOS. Putting your Haskell build products in the VM environments requires you to build the Haskell project with Nix instead of Stack or Cabal (an advantage, in my opinion!), but I can see why someone wouldn't want to convert to this. Nix has a very steep learning curve.
Uh, are you sure that those are required to be equivalent in representation. Why can't Nothing :: Maybe Void be different from Nothing :: Maybe Int?
Thanks. If a library uses the second option, could some projects that depend on it break because the library is listed as `&lt; 1` in the `.cabal` file by authors assuming that the first option was being used?
Well I didn't list those as options. They're completely compatible (in fact, this is an advantage over semver, which *does* treat `0.x` differently than `(&gt;0).x` for the sake of prereleases). Notice I said that with leading 0 versions, you "still get stable versioning." What I meant is that we're using the leading 0 completely aesthetically, to indicate that the `x` in `0.x.y.z` is going to be changing a lot, and old `x`'s won't be getting much long term support. The versioning scheme is still identical, and you depend on it in exactly the same way.
Sorry, I had a brainfart and forgot that the package manager would simply use the last 0.x version. Thanks. In practice, are the widely-used packages which have a major version number of 0 rapidly changing or have they simply not had a change that breaks API compatibility?
I'm interested, would appreciate a ping when it's done.
To clarify, I'm not saying that all packages with leading 0 versions are making this suggestion about rapidly changing second components; just saying it's a nice way to use a leading 0 that's completely compatible with PVP, without any special casing on leading 0s. Also to clarify, even if a package is choosing to use a leading zero to imply that its second component will be rapidly changing, it still must adhere to the rules about changing the version with breaking changes. If you make a breaking API change, you must at least update the second version component, as it is considered part of the major version. My point was just that you're allowed to choose for your library that a leading zero means you will be making breaking changes often, but you still have to bump the second component to imply a major version change whenever you do so.
&gt; Notably you can use the parameter for types, but you can also use it for the scenario mrkgrnao is asking about here, where you annotate each variable with which 'sort' of variable it is (e.g. term vs. type), rather than with its type. Okay, so I think I get how this works. I don't understand how "HBound" can be used to define mutually recursive types, though.
I am not able to reproduce this issue with a simple `Main.hs` with no dependencies. Can you try it with just `main = print "foo"`, and no dependencies? How did you install GHC?
Your last item (import management) is actually my first. Deciding to move a set of types and functions into their own modules involves **so much** ritual in Haskell, I usually just take a sip from my drink and hold my breath before attempting it.
Background Info: I also deleted any *.hi *.o files. Is this a bug or my error? I installed Haskell via: https://www.haskell.org/platform/#linux-generic I installed all needed libraries with this: `cabal -j -p install -j -p` I nuked/removed ".ghc" and ".cabal" before installing libraries I set `-- library-profiling: True` in the cabal config file. **Found a workaround.** The problem is too subtle to file a bug report or investigate further. I will delete this. *For my personal situation, the switch to 8.2.2. was too early. It's more trouble than good, currently.!*
A minimal example is now in the post.
Why do you assume it was me who downvoted?
Can you elaborate on how 8.2.2 caused this? I'd like to know so I can avoid the problem.
I think I agree with you, and I also think that RIO seems to have the right balance of priorities. `IO` and `mtl`-style classes solve different kinds of problems entirely and shouldn't be intertwined -- `RIO` or `ReaderT r IO` should be a low-level basic thing that handles resource-level bits of your app. `mtl` is for business logic domain stuff. Likewise, `Either` is amazing for program domain errors, but for *exceptional* situations (like the ones that occur in IO), you want `IO` exceptions. I really don't think that trying to wrap stuff into `ExceptT` ends up being any easier; you really need something like classy prisms and something fancier than `MonadError` to have extensible errors.
From my personal, isolated experience: I would definitely wait! I am downgrading today, just look at the other threads. Stack might not even support 8.2.2. (you can set some flag "allow-newer" that works for some people --not me.
Agree with you 100%. Not generally a fan of whole-app `ExceptT`. Generally, errors that mean something should be handled immediately, and errors that are pathological should be bubbled up for a top-level handler to deal with. It's a very grey area, but I think it's worth drawing the line somewhere in a project.
Apparently, Snoyman wanted something different. He has explained some of his goals and design decisions. At this point he is floating a prerelease and soliciting comments and suggestions. None of what Snoyman is doing appears to be intended to replace the current standard prelude. If you like his proposal, use it, but if not, don't. None of us need to get goaded into bikeshedding. Personally, I have always been wary of standard preludes in other languages (e.g., Algol 68). Ultimately, you or your team will be best using whatever you agree to rather than chasing an ideal, uniform, language-wide consensus.
The module structure seems a bit muddled to me.
Can you repost the issue? We're switching to 8.2.2 at work and would love to know any problems.
It was a user error. Ghc 8.2.2 is fine although stack integration is a tiny bit bumpy, but this will change soon. Ghc didn't complile Main only Main.o. I did a mistake
Short answer: no. Lomg answer: no, it just means that no radical API changes have happened since the first release. A change in the major or minor version number indicates removal of public API identifiers (including changing types in incompatible ways, which amounts to removal plus addition); the choice between major and minor is up to the author, but a common pattern is to reserve the major number for complete or large rewrites, while the minor number is used for incremental changes. So a major version number 0 simply means that no such large overhauls have taken place; the package arrived at its current state through a series of incremental modifications based on the initial (0.0.1 or 0.0.0.1) release.
One good thing about RIO and other libraries by Snoyman that preceded it is how they approach asynchronous exceptions (by letting them bubble up most of the time). I think that is the right way. A datapoint on the adoption of exceptions vs. ExceptT over IO: it seems like the Cardano codebase went for the exceptions approach. (I'm more of an ExcepT guy myself.)
I don't think it's a "vs" problem, I think it's an "and" problem. See the comment thread between /u/ephrion and I. Both have their place, likely even within the same app.
11th to 15th of June
And these three: https://github.com/snoyberg/mono-traversable And this one: https://github.com/snoyberg/basic-prelude And an article to explain the differences between them https://www.yesodweb.com/blog/2013/01/so-many-preludes
Which problem? :)
Nice !! You should consider writing it as a custom `stack` template (https://github.com/commercialhaskell/stack-templates). Otoh, I understand stack templates don't currently accept new submissions; what's the current status of that /u/mgsloan ?
Haskell libraries follow the PVP. In the PVP, the first *two* numbers of `x.y.z.w` are the *major* version number, i.e. `x.y`. When the API changes in a way that is not backwards compatible, this string has to be increment. In JavaScript land, packages are often versioned with SemVer. There, version numbers have the form `y.z.w`. The first number, `y`, is the *major* version. There is a *special clause* in SemVer that says that the API may change willy-nilly between minor versions if `y == 0`. This is probably what your question is about. We don't do this in the kingdom of Haskell, because that is stupid.
Will you continue to improve it? We could do team work.
Make one data type Foo, then you can have both `Foo Stmt` and `Foo Expr` or `Core Term` and `Core Type`.
Had to scroll to the very bottom to find this link, but I’m glad I did. 
That's USA, and that means USA timezone for remote work :( Or do you have offices abroad?
stack-templates do accept new templates, but it's on a case-by-case basis and we do not yet have a standard for what gets included or not https://github.com/commercialhaskell/stack-templates/issues/55 My dream template system would instead be to have refactoring commands that mutate the stack.yaml / mutate package.yaml files / add specific files. Then, you could pile together various aspects, do like `stack new my-project-name hpack mit-license rio`. That is part of the problem with the current templates system. Beyond, say depending on a particular framework, they sometimes also vary widely along all kinds of other axis - https://github.com/commercialhaskell/stack-templates/issues/56
The only thing that needs to be done for that is to include .travis.yml, appveyor.yml in the template and optionally packcheck.sh as well to make it self sufficient instead of fetching the script from github.
Makes me wonder if the RIO monad approach can be emulated with -XImplicitParams and -XConstraintKinds.
How would you phrase it? I thought about how to rephrase, but all I could think of is "base-like", and "foundation" is taken
There is nothing in `rio` that says you shouldn't use `Either` for some forms of exceptions. Infact the README says &gt; If something can fail, and you want people to deal with that failure every time (e.g., lookup), then return a Maybe or Either value. So uhh you seem to have similar opinions about style that rio does.
So, let me get this straight - new things are a problem and we should only have old things? Man, that would suck!
Your comment seems vague
That seems like a great way of dealing with it.
be careful with this, because if the Ord instances don't match up, you're going to be creating incoherent Map and Sets.
I feel like a common thing of the prelude replacements are that they all/most remove partial functions, which I very much agree with. This does make one wonder, was there ever any formal proposals to make partial functions like e.g. `head` and `tail` safe? I know there has been various discussions here on reddit, but can't seem to find any concrete proposals. Personally it's one of my main gribes with the Prelude, especially for new users. "Hey, come try this safe language", "Oh, lemme get the head of my list, SLAM, runtime error", etc. 
I got it to work after switching to the develop branch, does that work for you as well? I still have to reproduce the exact steps necessary however, since I also deleted the `~/.emacs.d`` directory and reinstalled it. I will edit my OP and maybe there is some opportunity to add useful information to the spacemacs readme?
I got it to work after switching to the `develop` branch, but since I tried so many things I don't know if this was really the step that did it. Do you use the `develop` branch as well?
I am lead to believe that your commit did it for me, but I am not sure since other people claimed that it worked for them regardless. I have updated my OP in this regard. Thank you very much.
I think his point is that this shouldn't be the case. `IO` shouldn't encode exceptions at all, we should just have more convenient ways of dealing with something more explicit. Not sure I agree completely, considering the utility of async exceptions, but I see his point.
I don't think that's being fair. You're assuming they have a problem with the new-ness of the thing, when they've only stated a problem with the redundancy of it.
The redundancy problem described by the xkcd comic that spawned this thread.
[Here](https://gist.github.com/dalaing/6f50126c6f3b338ab9aaab6f591eb2be) is a rough first attempt. It gets more interesting once you start doing type inference etc, and in my original usage of this technique I was doing other things with the prisms that had a really big payoff. I'm currently out of town for work, I'll try to comment it up and make the language more interesting next week.
I'm looking for existing implementations of a security monad similar in function to the one described in the paper. Hackage searches didn't turn up anything for me. Does anyone know of any?
There is nothing even close to a counterargument. "We reuse existing, commonly used libraries" ? That's not a counterargument. This "but we can do slightly better" attitude is _exactly_ the problem described in the xkcd. Plus it's an unfair implication about the existing alternative preludes.
I like this page explaining difference between different preludes: * https://guide.aelve.com/haskell/alternative-preludes-zr69k1hc But having page devoted to description of difference between FPCO's preludes is also nice!
I like this explanation of PVP versioning by /u/peargreen a.b.c.d a = ALL-NEW AWESOME COOL, i.e. number useful for marketing basically b = actually meaningful “this version possibly breaks API” number c = “this version adds features but doesn't break API” number d = “I fixed haddocks”, “I fixed compilation with GHC 7.6.2”, etc
Your response to my response seems hilarious to me.
You can look at `hs-init` tool which adds automatically `.travis.yml` and a lot of other things to created by `stack` project: * https://github.com/vrom911/hs-init
I think it's fair because nothing like this exists yet. If there are no good options, build a better one!
I find no `trace` function included?
That seems unrelated to both /u/MidnightMoment's complaint and whether your comment was fair. The complaint is that exactly that attitude causes standards problems, and your comment wasn't fair because it misrepresented the complaint.
Depends on how you look at it. You could call the subset of functions you actually use "the API." Since that's a subset it's possible it doesn't break even with a major version bump.
At least ghc let's you know when you have useless imports.
I don't think so. The PVP only specifies what source changes trigger what version changes; it does not specific that any version changes *require* any source changes. I believe you are technically allowed to release the same source under multiple different major versions.
Then there's hope to build that auto removal shortcut! Maybe I would give it a go if I had time. 
Man, can you lighten up, yeesh?! "So, let me get this straight - ... ?" is me asking am I interpreting his complaint correctly. Boom - my pedantry wins I think
I see. This does mean that, for an (H)Monad instance to have a sensible `return`, I have to merge my two `Var` constructors into one, right?
In this case, I wouldn't try to do the `unsafeCoerce` trick. As everyone else mentioned, it's really hard to reason about what `unsafeCoerce` is going to do. It also might break one day with a future GHC release, and you'll get no stack trace, no error message, just a segfault. A safer, lighter-weight approach that will still reduce allocations somewhat is to use [mapMonotonic](https://hackage.haskell.org/package/containers-0.5.10.2/docs/Data-Set.html#v:mapMonotonic) for the `Set` conversion. You have to make sure the `Ord` instances are compatible though.
&gt; USA timezone for remote work What's the connection?
Previously there weren't `alpha` releases. If I don't miss anything, for example the timeline of GHC-8.0.1 events: - https://mail.haskell.org/pipermail/ghc-devs/2015-December/010614.html 2015-12-02 feature freeze coming - https://mail.haskell.org/pipermail/ghc-devs/2015-December/010784.html 2015-12-14 feature freeze imminent &gt; Now that Richard's work is in, we will set the official freeze date for Monday December 21st - https://mail.haskell.org/pipermail/ghc-devs/2016-January/010958.html 2016-01-12 Ben asks about Cabal release (and then the alpha release would been done) - https://mail.haskell.org/pipermail/ghc-devs/2016-January/010959.html 2016-01-12 Mikhail answers on the same day - https://mail.haskell.org/pipermail/ghc-devs/2016-January/010966.html 2016-01-13 GHC-8.0.1 RC1 I think that if there were alpha release for 8.0.1 then it would happened around 2015-12-21 --- To me it looks like that if we will require PVP for dependencies, it essentially means "new alpha1 = old RC1", which is aligned with your (miss-)understanding. But previously **there were releases** before first RC1!
Those are un-fork-able.
What do you mean? 
We very well may disagree on those points, but that's not what the best practices are trying to talk about. The point is: * There's a line in the sand _somewhere_ * The GHC runtime system doesn't enforce this at all, and therefore `IO` means "can fail with any exception" * As a result of that: don't use `ExceptT` on top of `IO` * Other than that: outside the scope of the best practices This came up at LambdaConf winter retreat, we had a (very, very) long discussion about it. The biggest concerns around `ExceptT` are that it complicates the coding story because it doesn't fit in well with "control functions," and that it misleads people into thinking that all exceptions are handled at the `ExceptT` layer. That's the only part that I've been adamant about. If you want to write `readFile :: FilePath -&gt; IO (Either ReadFileException ByteString)`, go for it.
Yep.
Is that the "recommended" way of doing it, and does that only apply in the context of code that already lives in IO by necessity? If there's a blog post that explores this I'd be interested in reading it.
There was Haskell Summer of Code project last year where one person tried to add LiquidHaskell annotations to functions from `base` package (including `head`). This can help with making _standard_ library safer.
ScalaZ 8 is going ahead and making `IO` a bifunctor, with the error type parameter explicit.
Your transformation from `Co (State s) ~&gt; Warp s` seems to check out. You probably want to write explicit `forall r.`s in your `Co (Warp s) a` transformation steps, like you did with `Co (State s)`. The fact that `Warp s` has a comonad structure is rather peculiar and seems to come from the fact that `s` is closely related to `Endo s`. You have an embedding in the form of `mappend` and a stylized way to come back by applying the function to `mempty` that works properly on everything you embedded. Each time through the Co transformation seems to be giving you something like another layer of embedding.
Happy cake day!
Thanks for the explanation, that makes sense! I'll check out the video and slides when I get a chance.
One option is to introduce `head1` (and `last1`) of [`Foldable1`](https://hackage.haskell.org/package/semigroupoids-5.2.1/docs/Data-Semigroup-Foldable.html). This would also give `foldr1`, `foldl1` the constraint they deserve head1 :: Foldable1 f =&gt; f a -&gt; a foldr1 :: Foldable1 f =&gt; (a -&gt; a -&gt; a) -&gt; (f a -&gt; a) foldl1 :: Foldable1 f =&gt; (a -&gt; a -&gt; a) -&gt; (f a -&gt; a) I haven't gone through the formal process if someone wants to pick it up it's [here](https://ghc.haskell.org/trac/ghc/ticket/13573). I think the ideal module name is `Data.Foldable1` but changing it will complicate the whole thing
The title of this post has taken one of the subpoints of RIO and implied that's its main purpose. The main purpose is to provide a standard library, with the approaches and motivations described in the README. The prelude replacement included is a necessary outcome of those goals.
Haven't encountered or heard of age-based prejudice toward older folks yet. I don't personally care how old or young somebody is as long as they take the profession seriously.
I've worked with developers in their 50's in Haskell.
Agreed. My experience has been that wisdom from being in the trenches is hugely valuable, and is a great complement to the more academic side of functional programming skills.
&gt; I thought I'd ask here because I imagine Haskell has its share of older folks who transitioned to Haskell after decades of imperative coding. I really can't say re. ageism with haskell in the workplace. But haskell is older than java, and certainly many of the leaders in the community and development of the language are older (but haskell keeps them looking young of course). Many are academics and PL researchers who are necessarily not going to be in their twenties (for long, at least), since it takes years to get a PhD.
Is this a successor of classy-prelude?
You can't without foul play. foldl needs to go all the way to the end of the list. You can do it with foldr' though if you make your acc a function...
I did look at it, pretty nice. You could perhaps use packcheck's .travis.yml and appveyor.yml in it.
You can't call `forkIO` if you have `State` or `Writer` hanging around. But there is a way to effectively call `forkIO` on `Reader` + `IO`.
[Type Driven Development with Idris](https://www.manning.com/books/type-driven-development-with-idris), it was selling for half the price a few weeks ago and was very recommended in some other thread.
&gt; To recover the full source, you just need to make sure your AST contains enough information to represent everything you want to restore, including the whitespace, the comments, and the variable names. Are there any good established conventions for decorating a tree with such information? At first, I was looking at Roslyn for inspiration but due to the difference in language it doesn't seem like many of the patterns they use carry over. The rough idea I have is to use comonad transformers to decorate the tree and make all the patterns be defined as view patterns on `extract`, but I'm not sure if this is a good approach.
If I hover over `main`, I get `main :: IO ()`. However, hovering over `IO` doesn't show anything. I don't know what the expected behavior is supposed to be however, since if I define newtype MyType a = MyType a wrap :: a -&gt; MyType a wrap = MyType then hovering over the `MyType` in the `wrap` type signature will show `forall a. a -&gt; MyType a`. This seems to be "symbol directed" however, since nothing is shown if I change the name of the value constructor. I can't make spacemacs show anything for `main`, even when `ghc-prim.GHC.Types` is imported.
try it with foldr
This works, except for infinite lists: tw f = reverse . snd . foldl g (True,[]) where g (t,b) a = if t &amp;&amp; f a then (True, a:b) else (False, b)
Yeah they should be easily available but emit warnings like in classy prelude.
Happy Birthday, Simon! I'm turning thirty myself today, can't say I've been as productive though :/
My experience: It's about the same as other languages and depends on location/industry In Bay-Area-like cultures there's still a heavy age bias. On the other hand, haskell is more consultant-dominated than many languages and this naturally favors older workers.
Happy birthday Simon!
I'm in my 40s and work in Haskell after a fairly robust career working in imperative languages. My boss is probably about a decade younger than me -- I confess I never asked him -- but its more about when he joined the company and understanding of the vision they are trying to achieve than any sort of ageism. I've had awesome managers in their mid 20s and I've had awesome managers in their 50s and 60s. (I've also had terrible examples in all age ranges.) Hire the best person for the job. Team leadership is a different skill than being a living Haskell typeclassopaedia, though. There are a lot of significantly older Haskell developers who have been at this for a couple of decades longer than me. The language has been around since the end of the 80s, and most of the original designers of the language are still active in the community.
I don't think the point of wrapping stuff into `ExceptT` is to make stuff easier to begin with. I think the main utility of `ExceptT` is a protection against someone changing the type of the error halfway across the codebase, or even worse, a library! Whenever we get down to updating our dependencies, that's almost the only issue that scares me, the possibility of an exception being thrown with a different type.
There are tonnes of illegal values - if I export the constructors for TermF and TypeF. If I only export the prisms and/or the helper functions I defined near the prisms for constructing these things, then everything is fine. I have something fancier than this that has a version of Type that has no illegal values, although you have to do a bit of work to pull them out of annotations on terms. There's kind of a halfway point here as well. You can give up on some of the advantages of bound and use it more lazily. In this case that would involve defining data types that don't have Scope in them / that have constructors that look like my helper constructors, and then converting to / from the versions of the types in my gist when you need them. Then your users have an ast that looks safe and you can still use bound to handle the naming. Depending on what you need, that could be just the thing. 
Happy birthday! :-)
Early termination is not something a fold can (typically) do. An unfold, though... whileTake ::(b-&gt; Bool) -&gt; [b] -&gt; Maybe (b,[b]) whileTake _ [] = Nothing whileTake f (x:xs) = if f x then Just (x, xs) else Nothing takeWhile' f = unfoldr (whileTake f) 
&gt; haskell is more consultant-dominated than many languages How is that possible? Since consultants provide help to companies who aren't themselves consultants, and those companies must also be using Haskell otherwise they wouldn't need the help of a Haskell consultant, can't we conclude that every Haskell consulting company needs a large number of non-consulting Haskell companies in order to survive? Therefore, I don't think any industry or technology could ever become dominated by consultants.
In that case, yes, it works for me as well, in the sense that we both have the same behaviour now.
Welcome to the club of not-so-productive people.
No function accumulator required for `foldr`: takeWhile :: (a -&gt; Bool) -&gt; [a] -&gt; [a] takeWhile = \p -&gt; foldr (step p) [] where step :: (a -&gt; Bool) -&gt; a -&gt; [a] -&gt; [a] step p x ys | p x = x : ys | otherwise = [] You can do `takeWhile` with `foldl` with a function accumulator: takeWhile' :: (a -&gt; Bool) -&gt; [a] -&gt; [a] takeWhile' = \p xs -&gt; foldl' (step p) id xs [] where step :: (a -&gt; Bool) -&gt; ([a] -&gt; [a]) -&gt; a -&gt; ([a] -&gt; [a]) step p f x | p x = \ys -&gt; f (x : ys) | otherwise = \_ -&gt; f [] And though the two implementations produce the same result on finite lists: &gt;&gt;&gt; takeWhile (&lt;5) [1..10] [1,2,3,4] &gt;&gt;&gt; takeWhile' (&lt;5) [1..10] [1,2,3,4] They don't perform the same on infinite lists: &gt;&gt;&gt; takeWhile (&lt;5) [1..] [1,2,3,4] &gt;&gt;&gt; takeWhile' (&lt;5) [1..] ...HANGS This is because `foldr step init [1..]` is equivalent to `step 1 (step 2 (step 3 (step 4...`, which can complete if `step` is ever non-strict in its second argument while, `foldl step init [1..]` is equivalent to `...(step (step (step init 1) 2) 3)...` which has to take infinite steps to evaluate.
Only `foldl` consumes the whole list. `foldr` doesn't.
&gt; Since you only have two types, you could implementing a version of bound which only supports exactly two types, that should be possible without GADTs. What would this look like? I understand how to do two types with the higher-order representation, but I can't figure out how to do it without GADTs.
The stores and hq are US and the best time zone is US but the company has many employees in India and a few in other place like China and Europe. 
`foldr` does as well, just in a different direction, and semantically I think they are meant to work over the whole structure. `foldr` works with infinite lists thanks to lazy evaluation, but there isn't really any way of using foldr to only fold part of a list, leaving the rest untouched. Folding is fundamentally about *structural transformations*, and so it doesn't really make much sense to fold a structure into the same structure.
Oh my, so you are saying you started haskell with a friend and they weren't even 20 years old!? Good on you guys for following your curiosity at such a young age. I'm new to Haskell so right now I'm finding my functional way, but Happy Birthday :)
He later sent [a correction](https://mail.haskell.org/pipermail/haskell-cafe/2018-January/128448.html).
Happy Birthday Simon :)
Gotcha, so it sounds like some reasonable candidates would be logging and task/server metric collection, in your experience is there anything else that you specifically would or would not put in mutable refs?
Ahh, that sounds more reasonable. 
dang, that's elegant! thanks!
A great man, an inspiration, and a leader by example.
https://functionalcs.github.io/curriculum/ is based around functional programming, mostly standard ml, which Haskell is a variant of. If you read something like the Haskell from first principles book first you might be able to do most or all courses and exercises in Haskell.
At 40, I'm the oldest person at my company by nearly a decade. On top of that, other than a bit of piecework, this is my first professional programming job (I've been here for 2 years and change). Displaying interest and basic proficiency in Haskell was about 90% of that hiring decision. Age was never on the radar.
I know of at least one Haskeller in his 60s, [as of today](https://redd.it/7rbaa9)! :)
It's not something `foldl` can do on a `[]`, but otherwise I disagree.
Touching message. Nice error checking (albeit not a priori :p) Happy birthday 
Ah, I don't mean they outnumber Full-Time, but the ratio of consultant to full-time is higher. There's also far more solo consultants than consultancy-employed. This is just my perception, I don't know actual numbers.
Putting on my old CIO hat for a second: My experience with startups is that how many consultants you bring in is largely a function of how you are funded and where you are in the life-cycle of the company. The key question I always ask is if you are at a point where you should (primarily) "build" or "buy" talent. If you've done a couple of funding rounds, and are flush with cash, but risk missing the market, you double down and "buy" rather than build the talent you need. This keeps you in a consultant-heavy space, but can keep you relevant. If on the other hand you are largely self-funded, or have reached that sort of first success and are now worried more about brain-drain than first-mover advantage, then you start to hire more on the "build" side of the line, looking for people you can help build into the team you will need as attrition over time tends to be lower for those folks. One thing that works well for Haskell on this front is that you _can_ "buy" a lot of good talent when you need to, and the language is such that you can get a lot of work done in this fashion on a comparatively short timetable, while at the same time there are enough people interested in doing anything to work in the language, that once that "buy" window has passed, you can find good talent to "build" a team around them.
What you've written is a bit vague, so I might be misinterpreting it. But at the moment I think you're spreading an unhelpful intuition. &gt; there isn't really any way of using foldr to only fold part of a list, leaving the rest untouched What about a function that changes the first `n` elements of a list, and doesn't change the rest? --| Apply 'f' to the first 'n' elements of the list, leaving the rest unchanged -- &gt;&gt;&gt; take 5 $ mapN (+1) 3 [1..] -- [2, 3, 4, 4, 5] mapN :: (a -&gt; a) -&gt; Int -&gt; [a] -&gt; [a] mapN f = flip $ foldr (\a next n -&gt; (if n &gt; 0 then f a else a) : next (n - 1)) (const []) &gt; Folding is fundamentally about structural transformations, and so it doesn't really make much sense to fold a structure into the same structure Folding is about transforming things into lists. An alternate representation of lists is `type List a = forall r. (a -&gt; r -&gt; r) -&gt; r -&gt;r`, and if we re-arrange the arguments of `foldr`, we see that it is a witness that we can take a `Foldable` into this encoding of a list: foldr :: Foldable t =&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b foldr :: Foldable t =&gt; t a -&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; b foldr :: Foldable t =&gt; t a -&gt; List a There are a lot of operations that transform lists into lists. `map`, `filter`, `take`, `drop`, `(++)`, and I could go on. It makes perfect sense to transform a structure into the same structure.
&gt;What about a function that changes the first n elements of a list, and doesn't change the rest? That function still steps through the entire list. The OP was asking about the ability to "break" out of the recursion like one does in imperative programming (so completely stop evaluation of the function and return a value), to which I replied that it can't be done with folding, and would require the use of explicit recursion instead. While your function does use a fold to apply a function to just part of a list, it is fairly roundabout and kind of sidesteps the point of a using a fold in the first place. Consider an alternate implmentation: `mapN :: (a -&gt; a) -&gt; Int -&gt; [a] -&gt; [a] mapN f n (x:xs) | n &gt; 0 = f x : mapN f (n - 1) xs | otherwise = x:xs` This code is much more obvious that it is intended to recurse until some condition is met, which is not the case when using a fold, particularly because, as mentioned, it still traverses the entire list, whereas this implementation using explicit recursion does not. &gt;Folding is about transforming things into lists. This is not true. Consider the type signature of `foldr`: `foldr :: Foldable t =&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b`. This means we can fold any `Foldable` structure `t a` (which also encompasses much more than lists) into literally any type `b` as long as we have a way of putting `a`s into a `b`. To be perfectly honest, I can't understand your reduction of `foldr`. `foldr :: Foldable t =&gt; t a -&gt; List a` is certainly not a valid type signature for `foldr`. &gt;There are a lot of operations that transform lists into lists. map, filter, take, drop, (++), and I could go on. It makes perfect sense to transform a structure into the same structure. I never said it doesn't make sense to transform a structure into the same structure. I said it doesn't make sense to use a fold to do so, because fundamentally, folding is about transforming structures into other structures. For further reading: https://en.wikipedia.org/wiki/Fold_(higher-order_function) 
&gt; That function still steps through the entire list This is fair enough. My `mapN` does a comparison for every operation in the list, whereas yours does only `n` comparisons. &gt; This is not true. Consider the type signature of foldr: foldr :: Foldable t =&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b. This means we can fold any Foldable structure t a (which also encompasses much more than lists) into literally any type b as long as we have a way of putting a's into a b. To be perfectly honest, I can't understand your reduction of foldr. foldr :: Foldable t =&gt; t a -&gt; List a is certainly not a valid type signature for foldr. Load this into GHCi: {-# language RankNTypes #-} type List a = forall r. (a -&gt; r -&gt; r) -&gt; r -&gt; r foldr2 :: Foldable t =&gt; t a -&gt; List a foldr2 z x y = foldr x y z &gt; I said it doesn't make sense to use a fold to [transform a structure into the same structure], because fundamentally, folding is about transforming structures into other structures. So you're telling me these definitions make no sense? map :: (a -&gt; b) -&gt; [a] -&gt; [a] map f = foldr (\a b -&gt; f a : b) [] filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a] filter f = foldr (\a b -&gt; if f a then a : b else b) []
&gt;Load this into GHCi: I missed part of your definition earlier, it seems. Would you mind explaining your definition of a list? I don't quite follow. &gt;So you're telling me these definitions make no sense? Fair enough. I've never really thought to use folds in that manner, but I can't argue against those implementations.
Laziness is your friend (I'm new to Haskell, forgive me if I'm wrong ;) ): takeWhile' p = foldr (\x acc -&gt; if p x then x:acc else []) &gt; takeWhile' (&lt;3) [1..] [1,2] &gt; takeWhile' (&lt;3) [1,2,3,undefined] [1,2] 
So with the ordinary bound, your context is a type of kind `*`, whose number of inhabitants indicates the number of variables which are currently in scope, right? And with strongly-typed bound, your context has kind `Type -&gt; *`, so you kind of have one context of kind `*` per type. If there are only two types, this could be `Bool -&gt; *` instead. And a function of type `Bool -&gt; a` is equivalent to a pair `(a, a)`. So my idea was to use two contexts of kind `*`. Do you think this would still require GADTs?
So there's a way to represent any data structure using only functions. Here are some examples: Booleans: type Bool = forall r. r -&gt; r -&gt; r true :: Bool true = \a b -&gt; a false :: Bool false = \a b -&gt; b ifThenElse :: Bool -&gt; a -&gt; a -&gt; a ifThenElse cond a b = cond a b Tuples: type Pair a b = forall r. (a -&gt; b -&gt; r) -&gt; r mkPair :: a -&gt; b -&gt; Pair a b mkPair a b = \f -&gt; f a b fst :: Pair a b -&gt; a fst p = p (\a _ -&gt; a) snd :: Pair a b -&gt; b snd p = p (\_ a -&gt; a) This is called Church encoding. It's a mechanical translation- if your datatype has `n` constructors, you create a function of `n` arguments. Then each argument to constructor becomes an argument to its corresponding function. That's a bit of a mouthful; the easiest way to say it is that the Church encoding for a datatype is its catamorphism. Anyway, here's what the Church encoding of a list looks like: type List a = forall r. (a -&gt; r -&gt; r) -&gt; r -&gt; r nil :: List a nil = \c n -&gt; n cons :: a -&gt; List a -&gt; List a cons a as = \c n -&gt; c a (as c n) The first argument, `(a -&gt; r -&gt; r)`, corresponds to `(:)`. The second argument, `r`, corresponds to `[]`. Here's the code that proves `List a` is equivalent to `[a]`: toChurch :: [a] -&gt; List a toChurch [] = nil toChurch (x:xs) = cons x (toChurch xs) fromChurch :: List a -&gt; [a] fromChurch xs = xs (:) [] Now that we understand the Church encoding of a list, the `foldr` trick makes more sense: `foldr :: Foldable t =&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b` and `foldr :: Foldable t =&gt; t a -&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; b` Because the order of arguments doesn't matter. Notice that the return type of the above `foldr` is `(a -&gt; b -&gt; b) -&gt; b -&gt; b`, which is the Church encoding of a list. So we replace that with `List a`: `foldr :: Foldable t =&gt; t a -&gt; List a`
sweet, do you have a link?
yes, I missed the '[]' when copying from my editor. Thank you for pointing this out.
Typing on a touchscreen is not mice.
Does not typecheck
Thanks! I didn't think about that! I would like a Module hierarchy too. A way to visualize in a (possible dynamic) graph display, which module calls which.
Here are a few of my points: * tells me why my code is slow (allocation, lazy/strictness, productivity, etc.) * optimized haskell compilation for my needs (compilation for throughput/latency optimization, runtime options, etc.) * helps me configure stack and cabal (tells me 'you are using ghc 8.2.2. which is not supported by stack; do you want me to activate 'allow-newer'?) * visualization/introspection of haskell code (what are the module dependencies), which functions are unused, etc. * 100% trivial to download and install (preferably a binary that works if GHC works; I have spent about 3 hours lately to battle with different emacs backends and atom backends. Atom tells me 20-30 minutes in that 'ghc-mode' is unsupported for ghc 8.2.2.). 100% failsave (no missing fonts, not 'please report this as a bug'). * the same functionality when used as GUI and when used as web app (e.g., integrated notebook such as what is the case for IPython and Rust) * A genius solution to get a bigger team work going on a larger scale (either open source or someone pays for the work) What others have said to me: * works * not so much features, as a set of orthogonal, standalone tools (because I don't like IDEs, I will stay with my VIM/emacs/whatever) * 1. just works, 2. click on symbols for definition or references, 3. highlight errors and warnings * needs to have equal or better experience to ghc-mode. mod-ghc is really slow and doesn't understand incomplete programs that well (it seems), so getting anything useful such as inferred type or name completion is hit-and-miss * what someone or a team has to do is rebuild the whole technology from the ground up to support incremental changes and incomplete programs, from parsing right through semantic analysis. and I think a promising basis for that can be found as an example of conal's "Compiling to Categories" work * and so the same should follow for any semantic analysis; incremental change to the semantic model should recompute the analysis only as much as needed * get last working version of my code back. 
I think a report that showed common input-output pairs for all my functions would be a functional debugger
That was good. Thanks! I hope you’ll continue writing about Persistent.
We've been through this and the important things were to "stack install intero" globally i.e. don't run it from within a stack project (don't forget to update the lts on your ~/.stack/global-project/stack.yml ) then you need to update spacemacs packages (this requires a restart of spacemacs) That worked for all of us I think
Automatic imports, good performance. Not very GUI related, sorry :D
I guess 'automatic imports' could be also helped with in a GUI. Like automatic imports with a button 'show my picks' and 'alter imports that I picked' etc.
Those look interesting, thanks. I can't tell from my cursory glancing if I can dynamically delegate and revoke execution rights, which is the main contribution of that paper. I'll ping the authors about their library. 
As a young programming enthusiast you are a huge inspiration for me, Happy Birthday Simon! :)
Happy bday Simon! We love you! :)
Happy Birthday SPJ &lt;3
What I usually do: &gt; I sometimes forget to start the database I don't really have solution for this, maybe set it so that your db is automatically run on system startup &gt; I mess around with it so it isn't in pristine state I use different test database and in my test code, I start by recreating the DB. for example: https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/master/test/Spec.hs#L24 &gt; I'm also interested in using some CI, Travis, for example, allows you to run postgresql during the build: https://docs.travis-ci.com/user/database-setup/#PostgreSQL So .. no need to setup docker.
You should go with some eye tracking stuff instead of a mouse :D
Yeah! And laser the image on the retina. And input via brain-interface. And then no monitor at all but directly project visualization into the brain (kind-of a dream monitor). Etc... :) What do you think about voice or sound input for coding/writing? 
Umm, not viable. I don't want to say random stuff in the middle of the office, it would be super weird! I was imagining something like "keep a button pressed, and the cursor follows your eyes"
Probably for the same reasons that finance companies are looking at stronger languages. The last thing you want is for a $100M+ (USD) project to get screwed up because of a trivial software bug; anything with the word "guarantee" in it is golden for reducing costs and avoiding lawsuits. Plus, every client is different, has a different custom contract, etc; you need a lot of flexibility and generality to make a "control thing" that works well for every client.
There are multiple reasons but for example some of the products have been in production for 20+ years so the new products will be around long time as well. It's good to have something which is easy to maintain and doesn't break.
Is your company willing to help with relocation?
The work is here: * https://github.com/sdleffler/ghc/tree/ghc-8.0 But it's not finished and didn't go well at then as I was told.
I plan to keep working on it. I'm spending most of this year on personal projects - and this one is my main priority.
We're happy to help, but it also depends on what kind of help you would need. May I ask you to submit an application via e-mail and describe your situation and we'll get back to you next week?
Thanks, would love to read some blog posts about your journey.
That is what I was thinking, but I don't see how to get a suitable monad instance for both contexts for `Scope`.
I like this, but I think it would be useful to explain _why_ we prefer some of these things (purity, functions which take a single argument, lists over arrays), even if only briefly. Also, if you find yourself writing JS in this restrictive style, you might like to consider using PureScript instead :)
The idea is nor bad. the Haskell ecosystem is acquiring some maturity after many alternatives have been explored and people are starting to get serious and trying to program efficiently and establishing best practices instead of playing with Haskell and trying to make nice blog posts. Although playing is absolutely needed since exploring the solution space of a language like haskell takes decades. However it is not rich enough for a Haskell standard library. I don´t see programmer saying: "wow! this is so expressive but at the same time so easy.... this is the Python of the XXI century I want to use it"
The error message that I am getting is: stack setup No information found for ghc-7.10.3. Supported versions for OS key 'linux64-nopie': GhcVersion 8.0.2, GhcVersion 8.2.1, GhcVersion 8.2.2 
Yeah. I confess I don't know how to check off the top of my head.
All I want is "go to definition" &amp; "type-informed autocomplete" and have them work very, very reliability. like, java/C# level "go to definition &amp; autocomplete" reliability. Nothing else for me really matters.
"Find All References" is a pretty common IDE feature, but i've never heard of all possible call paths from one function to another function.
You also have `foldMap pure` (or `foldMap Vector.singleton` if you maybe want to be clearer). That turns every element in your `Foldable` into a one-element `Vector` and then smashes the whole thing down. Given `O(1)` concatenation on vectors, this feels like quite a nice idiom.
In the section "Synchronous Tasks" there are several type signatures containing a type `Boolean` that should be `Bool`.
Why do you need `Scope` to have a monad instance?
As several people have pointed out, `foldr` is a better choice for this. Notably, your function is also using `++` in its least efficient form. However, you can fake early exits using the `Monad` instance for `Either`. ftakeWhile f = either id id . foldl (\acc x -&gt; acc &gt;&gt;= \xs -&gt; if f x then return (xs++[x]) else Left xs) (Right [])
Try updating the resolver to LTS-9: stack config set resolver lts-9 Then try `stack setup` and `stack build` as before. Add a line `allow-newer: true` if you run into bounds issues. Alternatively, install GHC-7.10 via your system package manager and build with `stack build --system-ghc`.
Thank you! Just fixed this. But seems that nbviewer caches the page.
See recent relevant question and answer from StackOverflow: * https://stackoverflow.com/questions/48017808/what-is-the-meaning-for-the-convention-runsomething-in-the-haskell-community What I think: 1. `run*` is for monads (when you want to extract monadic action performing its effects). `Identity` is monad though it looks like a simple wrapper. It just has no additional effects. 2. `get*` for things you want to extract from some wrapper. `Const a b` is just a wrapper for value of type `a`. Another popular naming convention (which I like as well) is to use `un` prefix for _unwrapping_ from `newtype` (because often `newtype`s are created to introduce more type safety through wrapping). newtype ProjectName = ProjectName { unProjectName :: Text } This is convenient when you're working with data bases or rest methods and it's always tempting to have function with name `getProjectName`. Thus, you have naming distinction (and that's why I like `un`).
I have put a bug report on the issue tracker: https://github.com/wavewave/hoodle/issues/62
&gt; Given O(1) concatenation on vectors, this feels like quite a nice idiom. Wow, how this is achieved? Don't you need to copy some arrays to concatenate vectors?..
My bad, thanks - I clearly did not read what I wrote :)
You most certainly do, that comment was nonsense. Sorry!
There has to be an instance of `PngSavable` to be able to call `writePng`. Take a look how a drawing is there transformed into a PNG, this should give you hints how you can do the JSON transformation.
&gt; Another popular naming convention (which I like as well) is to use un prefix for unwrapping from newtype I rather like how Purescript has approached this with their `Data.Newtype` module. You can, for example, do newtype Address = Address String derive instance newtypeAddress :: Newtype Address _ printAddress :: Address -&gt; Eff _ Unit printAddress a = Console.log (un Address a)
I think you misunderstood my comment. To serialize the drawing you have to interpret it. The serialization to PNG is one example of such an interpretation. So it should show you how such an interpretation is done.
Agree, it's convenient. This can be achieved with `Wrapped` isomorphism from `lens` library: * https://www.stackage.org/haddock/lts-10.3/lens-4.15.4/Control-Lens-Wrapped.html
FWIW, lens uses `op` for the `un` operation named here.
Only somewhat related: if anyone's curious about immigration to Finland for this or other jobs in Finland, most information is available via the Maahanmuuttovirasto/Migri site: http://www.migri.fi/residence_permits Programmers typically are hired with terms that meet the [Specialist](http://www.migri.fi/working_in_finland/an_employee_and_work/specialist) residence permit requirements. Concretely speaking, you need: * A contract offering 3000 EUR/mo (you should not take any job offering at or below this anyway with the job market/normal pay levels here) * A Bachelor's degree, for which you will be required to provide a copy/photo of your degree. In addition, while you are in processing for this permit, you have the right to work while any *visa* that you have acquired is valid for a limited time: http://www.migri.fi/working_in_finland/right_to_work_without_a_residence_permit. Immigration to Finland (and other Nordic countries) is quite straightforward, but don't expect any Finnish people to know the law.
List fusion is basically just based on rewrite rules and the `foldr/build` rule. It's not too complicated, but the compiler can be quite finicky over it. Iteration is sometimes better with `[a]` than `Vector a` since you don't have to allocate a contiguous buffer that must remain resident for the duration of the iteration. But linked lists introduce indirection on every iteration. So if you're going to hold onto the list, then vectors will use less memory and be faster; but if you're dropping the list, then linked lists can be partially GC'd throughout the iteration, which may help with memory usage. Point being, for iteration, I think they're negligibly different. Of course lookups are a different story.
We're on the same wavelength... I've been hitting this exact problem over the past few days, but with `Seq` - `Vector` conversion.
Would be curious to see that benchmark distinguish between left and right associated `++`. Left should be much worse than right
So well explained! 
If you're going to iterate over the same list multiple times, a vector might be better. If you're iterating over a list only once, it'll probably not be materialized at all and that would certainly be faster than the vector (unless maybe if you're doing SIMD stuff to the vector).
In general, you can't serialize free monads because the second argument of `&gt;&gt;=` is a function, and you can't serialize function. If you don't need `bind` then you can downgrade to free applicative, which will give you a lot more freedom with static analysis and serialization.
Why do you explicitly link in the rts? If this is just something we need for the rts, maybe ghc could grow a `-link-rts` flag. And `-shared -threaded -link-rts` would do the right thing. I’m genuinely curious why you need to link in (just) the rts. 
Here's the problem with serializing a free monad: data OopsF x = Ask (Int -&gt; x) | Tell Int x type Oops = Free OopsF oops :: Oops Int oops = do whatAmI &lt;- ask if whatAmI &lt; 10 then pure 0 else pure 1 You can't write a serializer that correctly shows both cases. Depending on the interpreter, you'll either write 1, or 0, but never the `if`. A free monad allows you to construct a *tree* -- an abstract syntax tree, where the branching factor is determined by the functor you use. The interpreter for the tree is able to construct a single path through the tree, but only ever a single path. If you want to be able to view the entire tree at once, you need something like a Free Applicative.
yeah, and in application code, sometimes I prefer "getThing" to imply an effect, while stuff like "thing", "toThing", or "thing-ed" are pure. 
Personally, lately I've been using unwrapX.
Where did you see this advice posted? I can certainly believe a suggestion to use Vector more frequently (when it's the appropriate type), but I can't imagine using `Vector` where I would have wanted `Set` initially.
I was *just* going to ask the same thing. I thought I would've read all the controversial blog posts by now.
Yeah, `un` as an abbreviation of `unwrap` is clearer about what the function is actually doing than `get`, which I agree is semantically backward. I’d also take things like: unwrapProjectName projectNameText textFromProjectName projectNameToText But lately I’ve been using no prefix at all—with one module per major type, my code looks something like: module Name (Name(..)) where newtype Name = Name { text :: Text } module Main where import Name (Name) import qualified Name main = print $ Name.text $ Name "fnord" It requires managing more imports, but it’s easy to see where everything comes from, and the names are clear &amp; succinct.
To clarify, in that thread I didn't say that A = 0 implies experimental. I said that that is one possible scheme that authors could use. I don't think that is the most commonly used scheme. My main point was that it is unstated and there is no standard.
Sorry for the late reply. I believe you could've gotten more feedback if you would've chosen a more specific terminology. `Multi-threading` is more of an implementation detail, whereas things like parallel/concurrent/distributed programming differ conceptually, in some but not all aspects. What exactly is the problem with the `forkIO` part? Do you have some working code that doesn't yield the expected results? Since your `runNodes` function is a little *weird*, I wonder if you have already looked at [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929) yet? Why do I think your function is weird? * `StateT` over `IO`. If you live in a transformer stack that implements `MonadIO`, there's no gain from not using `IORef`, or `MVar`, `TVar`for use in a concurrent setting. If an exception is thrown, you can lose your state with `StateT`, and your function isn't pure to begin with. * Returning `[()]` makes no sense whatsoever. First of all, `mapM` and `forM` leak space. Secondly, what infomation is supposed to be encoded in `[()]`? Just use `forM_` instead. * You can replace `{ res &lt;- forM nodes ...; return res }` with `{ forM nodes ... }` by the [second monad law](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Monad.html#t:Monad). Also, the [async](https://hackage.haskell.org/package/async) provides a higher-level interface over threads.
Functional pearl about this https://www.cs.ox.ac.uk/jeremy.gibbons/publications/delivery.pdf
In one of the cited papers [*The Remote Monad Design Pattern*](http://ku-fpg.github.io/files/Gill-15-RemoteMonad.pdf) there is a section on Remote Binding (section 7), which on first sight looks like some kind of Free Arrow to me. This could probably adapted to be even more expressive than that, for example adding an if then else construct like ephrion's example. So you are not limited to only Free Applicatives.
regarding the title of the post -- I don't think you should use `vector` as much as possible. Each data structure has its own benefits and use situations, and there are definitely situations where lists excel where vectors don't, and situations where sets excel where vectors don't.
Can't wait to have https://ghc.haskell.org/trac/ghc/ticket/915 resolved and have a proper stream fusion abstraction that doesn't need rewrite rules at all. Currently, lists are really used for different reasons: 1. As a streaming abstraction, as is the case here 2. As an actual data structure (like, some inductive data definition with a forced spine). Lists perform mediocre here The fact that Haskell relies so much on lists is probably because they're so easy to use (construction mostly follows the order of recursion) and that's OK as long as we have laziness and, by extension, list fusion to keep things efficient.
http://www.earthclassmail.com are scamming people.
The recommendation was more of the lines, Use Vector _in favor of_ Lists whenever possible, not about using `Vector` at all costs. I'm trying to come to terms to understand where Lists do excel in favor of Vectors.
The recommendation was more of the lines, Use Vector in favor of Lists whenever possible, not about using Vector at all costs. Indeed every data structure has its tradeoffs. However, it seems entry Haskelleres default to List in favor of other data structures, should the default be Vector instead? If not, why?
There is a build function so instead of ls = 1 : 2 : 3 : [] You write ls = build $ \c z -&gt; 1 `c` 2 `c` 3 `c` z And then there are rewrite rules like foldr (+) 0 (build $ \c z -&gt; ...) (\c z -&gt; ...) (+) 0 1 + 2 + 3 + 0 This works well for complex sources including nested or appended ones. It doesn't fuse for things like `zip` because it can't be controlled by a single source, though. For completions sake, Vector uses stream fusion: data Step o a = Done | Yield o a data Stream a where Stream :: s -&gt; (s -&gt; Step s a) -&gt; Stream a The real definition also has a Skip constructor when the current value was filtered which would be unnecessary for new ghc version because of join points. Then for every time producers and consumer match like case (Yield state' out) of Done -&gt; e1 Yield s a -&gt; e2 It translates into [s -&gt; state', a -&gt; out]e2 So if every Step value is matched there are no Step constructors at runtime. To do this Vector inlines super aggressively. Stream fusion is consumer driven so it struggles with complex producers like nested loops.
I talked to a colleague who is a native speaker but he was not sure what a sommelier does. I wonder if this concept is just big in Japan...
Interesting my, in Scala vector prepend and append are actually constant.
That was a great mini bridging course! I hope you do more, I enjoyed your video and how you explained things by starting from a simple concept and building up the complexity. 
You can do that if you have mutation I believe
mutable append can be amortised constant. However in scala immutable append and prepend are also constant. http://docs.scala-lang.org/overviews/collections/concrete-immutable-collection-classes.html#vectors
Right, that doesn't sound like "use vectors whenever possible", that sounds like "use vectors whenever better". For example, it's *possible* to use vectors instead of Sets, and manually enforce set properties like no duplicates, etc....but it's definitely a bad idea in most situations. It's *possible* to use a 10000-item vector for streaming data, instead of a list, but it's probably a bad idea. Don't "use vectors whenever possible". "use vectors whenever it is the best choice."
Yes, for example Eclipse for Java has provided call hierarchy up and down for ages, but not the "all possible call paths" as far as I know. It would be very useful: If you know that your code goes through some function, and you know that it crashes in another lower level function, finding all paths can cut down the debugging a lot. Right now, I have to construct that set of paths manually unfortunately.
Thank you for your reply, sorry for the obscure question, I will try to share a working code that way it might make senses why I did `runNodes` that way. I uploaded a code to [gist](https://gist.github.com/kwaleko/a5f568f68355d922020b677ee5d7f072), hope you don' t mind. in the working code there are two function : runClientNode :: NodeEnvironment -&gt; NodeStorage -&gt; IO () runClientNode NodeEnvironment{..} initialStorage = connectToUnixSocket "sockets" (NodeId 0) clientHandler where clientHandler :: Conversation -&gt; IO () clientHandler Conversation{..} = loop initialStorage where loop :: NodeStorage -&gt; IO void loop storage = do input &lt;- TextIO.getLine case Text.words input of ["SUBMIT", to, amount] -&gt; do let tx = createTx nodeEnvId to amount void . forkIO $ do send $ LByteString.toStrict $ encode tx (c :: Int) &lt;- decode . LByteString.fromStrict &lt;$&gt; recv putStrLn $ "Tx sent, request id " &lt;&gt; show c loop (applyTx tx storage) ["QUERY", to, amount] -&gt; do let tx = createTx nodeEnvId to amount print $ memberTx tx storage loop storage _ -&gt; do TextIO.putStrLn $ "Invalid command: " &lt;&gt; input loop storage and runServerNode :: NodeEnvironment -&gt; NodeStorage -&gt; IO () runServerNode NodeEnvironment{..} initialStorage = do ref &lt;- newIORef 0 listenUnixSocket "sockets" (NodeId 0) ((True &lt;$) . forkIO . serverHandler ref) where serverHandler :: IORef Int -&gt; Conversation -&gt; IO () serverHandler ref Conversation{..} = loop initialStorage where loop :: NodeStorage -&gt; IO void loop storage = do input &lt;- recv counter &lt;- readIORef ref putStrLn $ "Request id: " &lt;&gt; show counter modifyIORef ref (+1) let tx = decode $ LByteString.fromStrict input send (LByteString.toStrict $ encode counter) loop (applyTx tx storage) the idea is that each `client node` connect to the server node and update it with a message in case the any `submit` is done on the client node. however, what I am trying to do is not make the system decentralized, so there are no `client` or `server` so every node is equal to others, that way each `node` should listen to all node(instead of one server node listening to others) and each node shall broad case a message to all other node instead of connecting to a server node and sending a message. the way I tried to achieve this is to create a new function `runClientNode'`[here](https://gist.github.com/kwaleko/a5f568f68355d922020b677ee5d7f072#file-temp-L102) and `runServerNode'` [here](https://gist.github.com/kwaleko/a5f568f68355d922020b677ee5d7f072#file-temp-L136). then the `runNodes`, I might be overcomplicating the problem and approaching it the wrong way. I hope I did explain it well
While it is true that the [free Applicative](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Applicative-Free.html#t:Ap) allows many more analyses than the free Monad, that's because the list of side-effects it performs is fixed and can be examined without running them. Unfortunately, the free Applicative still contains an opaque function, for example the `(+)` in `(+) &lt;$&gt; fx &lt;*&gt; fy`. To serialize a computation, you need something even less expressive, like a list of tokens representing actions. Or, if you need some control flow, you can use an AST, which is what CloudHaskell uses.
Remember, the goal is to implement a _new_ bound-like library which uses two contexts, so you shouldn't expect to be able to reuse functions like `fromScope` and `toScope`. You'll have to write your own, and they will probably have a different type signature. Moreover, `fromScope` and `toScope` only require a `Monad` in the fancy optimized implementation of bound. In the [`Bound.Scope.Simple`](https://hackage.haskell.org/package/bound-2.0.1/docs/Bound-Scope-Simple.html#g:3) implementation, those functions don't require a Monad instance (but `instantiate` still does).
That seems to be the role of `other-extensions`
Appearently [`tasty-quickcheck`](https://hackage.haskell.org/package/tasty-quickcheck-0.9.2) lacks support for `checker`s [`TestBatch`](https://hackage.haskell.org/package/checkers-0.4.9.5/docs/Test-QuickCheck-Checkers.html#t:TestBatch), which is hardly surprising. Your best bet would be to write a function that simply takes apart the `TestBatch` and runs each test as part of a [`testGroup`](https://hackage.haskell.org/package/tasty-1.0.0.1/docs/Test-Tasty.html#v:testGroup). Or just call `uncurry [testProperties](https://hackage.haskell.org/package/tasty-quickcheck-0.9.2/docs/Test-Tasty-QuickCheck.html#v:testProperties) (monoid trigger)`, which should work too if I type-checked things correctly in my head.
Thanks for the tip. I will play with it. Am I right uncurry has 2 arguments `moniod trigger` and `testProperties ` 
I would be very careful with udemy. I can't find the thread, but somewhere on /r/haskell recently there was some chatter about their terms and conditions: the impression I got was that they are a bit dodgy. So, tread with caution and double check what you are agreeing to!
No, Data.Sequence doesn't have constant indexing. 
Thank you! I got quite far. Around 127 dependencies worked. But then I got this error: https://gist.github.com/anonymous/9f6467e92b0e59adb8b616f506c15747 Any ideas how to get past this error?
`fmap` isn't a problem. See `runAp_` https://hackage.haskell.org/package/free/docs/Control-Applicative-Free.html#runAp_ Analysis of a free applicative doesn't care about the results of the actions, only what actions were used and their relative order. So `liftA2 (+) fx fy` and `fx *&gt; fy` are seen as the same program. In the code example from the OP, this is a perfectly fine behaviour.
Pretty sure things like `sum . map f $ [0..x] have a good chance of the list not being allocated. But it often "breaks" in more complex code.
&gt; So I'm curious if anyone else out there has had experiences with introducing Haskell to a small team and how it went. Do you have any advice for me? In my previous position, and my current position, introduced Haskell. The first time, I was in a "devops" role, and wrote `language-puppet` for our CI. I also replaced `logstash` with Haskell code (I don't understand why people cope with it). My boss was, I think, sympathetic, but in the end nobody wanted to learn it. The Java developers were not passionate about learning new things in general, so you can image how Haskell felt to them. No success here. This time, I am a security expert. I wrote quite a bit of internal tooling, which is generally useful. However, despite a mandatory Haskell introduction, this did not take. I have an intern however, and she has to write some Elm. Here is how I would rate my method of forcefully introducing Haskell: * pro : I write Haskell (or Elm) every day * con : nobody else does.
This might be slightly off topic: Do you want to introduce Haskell because you genuinly think it will help your project be better and cheaper, or is your main motivation for it just because of personal reasons? Think deep before comitting your whole team to totally uncharted waters for any of you. Think deeply about the risks involved and be honest about the pros you'll really get. Don't risk a project because you think some technology is cool. Use the techologies that make the most sense for your company. Be professional at your workplace; you can be silly at home.
In that case, yes, fusion rewrites the list away. But I meant that if the list isn't entirely rewritten out, it is still materialized.
When `stack` grabs `cabal 2.2`, it will rope in my `cxx-flags` and `cxx-sources` build info flags fir C++ files I added a few months ago to solve a similar problem. Until then you can download the HEAD of `cabal` and try using new build.
The tooling ecosystem in haskell may be a non-starter for some of your people. If they already have resistance to an idea of haskell, having to deal with the lackluster tooling and ecosystem problems may generate too much frustration. Why don't you do some prototypes in F#? You'll get to use FP and Azure / .Net gives you a litany of conservative implementations to cross off your feature list. Once everyone knows F#, you can show them how much easier it would be to refactor some F# code if you had higher-kinded types (alas, you will likely not get HKTs in F# any time soon), or how you could cut 30% off your code bases for non-performance critical code if you had HKTs + GADTs (alas, you will likely not get GADTs in F# ever).
&gt;My conclusion: Haskell isn't all that well known yet - not as much as we may think in our little bubble :) Heck, you are lucky, he at least heard about haskell. And even that only because he is a scala using snowflake among his peers. Most of the enterprise developers I speak with, first think I mispronounced pascal.
Sounds good! I would recommend the following text: http://haskellbook.com/ Aside from that, read everything you can get your hands on, Functional Pearl papers are great, especially if you find you have to implement something similar at the same time. Parsers are fun. Also, I couldn't find an answer by Googling this question: would I be technically able to make a copy of any existing software with just using Haskell? Turing complete ~ Turing complete; so, yes. In practice: Tradeoffs. 
Well that's one way pretend you have dot access notation with records :) Not sure if that's a great idea for library code, but I like the idea for personal stuff for sure.
I really doubt the Scala tree thing has constant indexing either. 
Thanks for the links. Specifically I'm looking for something that explains Haskell concepts and features so I can get better with Haskell.
You could carefully constrain the types of the primitives of your free applicative, such that there aren't any opaque functions the users can fmap into the expression. They can still use `pure` to bring any type into the applicative, but they won't be able to mix that type with the primitives. So you're essentially exposing a free `Apply`. Anyway, there are many tricks to pull off by constraining the primitives.
Lots of blogs are listed and aggregated at http://planet.haskell.org In general blogs are better for specific ideas or techniques or libraries. You'll find it better to learn with a more directed resource like a book or tutorial.
Are you using a definition of free Applicative which is different from `Control.Applicative.Free`'s? With that definition, even if you choose this extremely restricted set of primitives: data Unit1 a where Unit1 :: Unit1 () Any `fmap` such as `const 42 &lt;$&gt; liftAp Unit1`, still gives you a result like `Ap Unit1 (const 42)` which contains an opaque function.
That's it. It's not about experience, or even skill (though that is important). Just give a shit about your craft, and I will want to work with you. 
Rather than joining others in the discussion of the decision itself, let's say you did decide to do this: You _need_ to bring a Haskell developer experienced deploying it in production if you're going to do this. You'll need them to pair and train others to get them caught up too, a develop-and-dash won't cut it.
Concatenation itself isn't n^2, but nested, left associated concatenations is. -- This is much worse (((a ++ b) ++ c) ++ d) ++ e -- than this a ++ (b ++ (c ++ (d ++ e)))
What I miss in haskell-ide-engine is mostly "find usages" and reliable cross-project renaming. Also I would like the IDE to work with all my projects and not only with ones that are using the same GHC that the IDE itself was compiled with.
I think there's a valid defense of partial functions in the Haskell Prelude, but this isn't it. In general, it seems to be a rule of thumb that as soon as a Haskell design discussion verges toward claims that opposing viewpoints are only valid for new users, it's about to explode into drama. So let's not go there. The concerns people have about partial functions are valid concerns, not edge cases to be shoved into a corner. Ultimately, the whole question comes down to everyone acknowledging that partial functions aren't ideal, but no one having a compelling replacement. Dependent Haskell still looks far too complex to recommend as a replacement for most uses of `head`. Liquid Haskell is promising, but still heavyweight on the tooling side. The `[a] -&gt; Maybe a` type just pushes the problem down the pipe -- you avoid using a partial `head`, but at the expense of leaving someone else to figure out what to do with a `Maybe` value that should never be `Nothing`. Since it's not clear what to do, people are experimenting with the ideas we do have, and maybe one of them will start to look more appealing with time. Good for them!
Good points! Thanks. The 'auto imports' function is quite popular.
That doesn't cover codata: solution = head $ filter pred $ iterate step zero 
It's not like you paint yourself into a corner with total Prelude functions. The goal isn't absolute safety, but rather, safety by default. If someone really needs to be smarter than the type checker, they have `fromJust`. It's tucked away and requires an import, just as it should. I don't see any argument ever convincing me that `head` and `tail` were mistakes. There are, of course, more subtle issues of non-totality. Anything to do with lists, for instance. A large number of functions in Haskell are non-total for lists, like `length`, `reverse`, `words`, and `filter`. 
If every time you need a new kind of proof, your solution is to split the types, you are headed for nowhere good. The result of tail on a NonEmpty is [a], but what if you know that's also non-empty? Now you need a new AtLeastTwo type. That's ugly, so let's parameterize over type level naturals. But what about `last`? It needs not only that the list is non-empty, but also finite. You simply cannot split of a new type, and reimplement all the primitives, every time you need a new kind of proof.
&gt;My plan is: &gt;*Finish reading "Code" by Charles Petzold - likely will be done by today &gt;*Go through "Learn You a Haskell for Great Good!" - 1-2 weeks maybe? &gt;*Make a very basic app or game, just to practice. - within one month from now &gt;*Go through "Haskell Programming From First Principles" - seems more in-depth, but idk Sorry but as someone how already went through same thing as you it is completely unreasonable. Start with Haskell Programming From First Principles and be prepared that it will be really hard journey for next 3 months. Maybe you can make it shorter if you spend more time and power through it. Honestly as of now I don't think there is better option to learn Haskell as beginner than this book. If you want to try shorter path I would advise to go: * https://en.wikibooks.org/wiki/Haskell and after that * Go through "Learn You a Haskell for Great Good!" 
Libraries. The only way to know what works seems to be to use it.
This would depend on the problem of course, but the GADT in combination with HKTs can make specifying invariants in logic very concise simply because you can push logic to the type-level to restrict valid code paths (thus restricting the amount of code you can write).
This. Devops best practices in any ecosystem are a huge drain without the relevant expertise.
My pain points are not Haskell related. 
&gt; Most of the enterprise developers I speak with, first think I mispronounced pascal. Which, ironically, is because _they're_ mispronouncing Pascal.
Lists with at least one value as _so_ much more common than lists with at least two values IME.
Lack of incremental building of haskell packages under nix.
Are they related indirectly? For example, CI/CD issues could be considered an indirect pain point.
For the ltinfo error message try this : ` sudo apt-get install libtinfo-dev ` 
GHC. It's kinda hard to build and contribute to. It's ***slow.*** It's optimizations are kind of unreliable. It's nondeterministic (seriously? in a pure language?). Its profiling is mediocre at best. TH gets in the way of GHC doing many things cleanly. Cross compilation is extremely messy, to say the least (which is sort of relevant to GHCJS users). It's *awful* at parallel building. GHC is far more good than bad. The RTS is amazing, the optimizations are great if you know how to make sure they're firing, and it just does a ton of useful *stuff*. But nonetheless, it is consistently the thing I have to fight with the most.
The logic behind #2 is this: There is no corporation as large as ours that takes legal responsibility for the security of the software. Therefore, if we use it, we are effectively taking the risk upon ourselves. Even if you'll convince us that from the technical point of view the risk is much smaller for the open source solution, it is still not zero. So we prefer the solution where someone else assumes the risk. The counter-argument is: If you not only use open source but also participate fairly, there is a whole community of top-quality people who have your back technically. So for a solid open source solution with a solid community, the risk is also effectively shared. And the benefit is that you end up with a higher quality product. It is becoming more and more common for large enterprises to do this.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → f2834f9)](https://github.com/reflex-frp/reflex-platform/blob/f2834f9313610fb4443c8940f994e1cfc720ac82/docs/project-development.md) ---- 
Would you provide a specific example of how those pain points affected the success of your business? For example: "I had to get a microservice deployed in two days, but when I ran it at scale I discovered a memory leak that I was unable to fix because the prof file didn't have the relevant information. We missed the deadline as a result..." 
What would an ideal debugging experience look like, in your opinion?
In the past it was always possible to get cabal to pass through whatever C-compiler options you wanted. Do you know of a bug introduced into cabal between 2.2 and some previous version that was never fixed? Otherwise, saying "use something else" isn't answering the question.
You know about https://github.com/sol/doctest, don't you?
I am not sure about whether the use of `forkIO` is correct or not, since I don't know about the functions living in the `Serokell` namespace (especially `listenUnixSocket`). By the way, do you actually do inter-process communication, or would intra-process message passing via one of the channel abstractions in the [stm](https://hackage.haskell.org/package/stm) be possible as well? I haven't dabbled in sockets a lot myself, so I can't give you tips on the grand scheme of things in this regard. However, when it comes to handling state via `StateT`, using the "right tool for the job" not only makes your program more robust, but also conveys intent. Take your use of `IORef` for example. From what I have seen, you create a new `IORef`, spawn a thread and call `listenUnixSocket`. Now, the `ref` is only in scope inside `serverHandler`, an infinite loop that modifies it. I don't see any reason to not just pass the `counter` as an additional argument to `loop` here. Not only would this be more efficient, but if there indeed *was* any sharing of `ref`, you would have a race condition. So you'd need to either `atomicallyModifyIORef` it, or use an `MVar` or a `TVar`. Regarding `StateT` over `IO`, I don't think this is a black and white situation. Many people do that, it's just that I don't, since I don't see the point. If you have an imperative-style algorithm which does not require `IO`, using `IORef` will make the type signature more scary than it needs to be. Using transformers like `State` or even `Writer` might be perfectly ok here. As soon as `IO` is in the game however, there is no advantage introducing `StateT` on top of it anymore, though. Consider: * If an exception is thrown and propagated outside of your `StateT s IO r` function (and it *will*, since what is the point in throwing exceptions if you want to deal with errors locally? You'd just use `ExceptT` or `Either` instead), your state is *lost*. Won't happen with an `IORef` or the like. * `StateT` is not thread-save. Concurrently `modify`ing the value will result in a data race. * `State` is less efficient than an `IORef`, and using `StateT` will unnecessarily deepen your transformer stack without giving you any additional power. So it depends on "how local" your state is. * Local state, pure function: By all means, use `State`. You may consider `STRef` for efficiency reasons. If you have a `loop` or `go` function, just pass an additional parameter. * Local state, impure function: I'd pass an additional argument if I have a `go` or `loop` function, else use an `IORef`. Note that an `IORef` does not represent a value residing in a register, there is a pointer indirection involved. * Shared state: I would use `TVar` or some channel like `TBQueue` , *unless* I notice that the additional overhead of using `STM` is significant. Only then I would fall-back to using [MVar](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Concurrent-MVar.html) and friends, since while concurrent updates are possible, you reintroduce lock order inversion. By the way, you also should consider using `hlint` more frequently.
Didn't know. This looks very promising.
You're right that `Nat`-indexed vectors are an even better solution. However, your comment about `last` isn't a problem with `Vect`, it's a problem because Haskell conflates data and codata.
* I spend more time than I'd like thinking about and dealing with the importing of basic functionality—maps, text, and so on—and these vast import lists obscure the more relevant conceptual dependencies of my modules. * Alternative or custom preludes alleviate this problem to some degree. * A minor but real pain is that I have to remember to restart GHCi when I change the Cabal file; this interrupts my flow. * Nix is indispensable, largely because of exact version pinning and the public binary cache of Hackage, but dealing with two separate systems—Cabal and Nix—is a bit of extra headache; not a real problem for me but confusing to other developers. * `Show` is an awful paradigm for inspecting nested values. Installing custom hacks in `.ghci` for improving the layout of these `String`s is annoying boilerplate; even then, I would often get my job done faster with interactively explorable data structures. * The `lens` ecosystem is quite amazing, but it is also quite an extreme cognitive overhead. * The use of records and `makeFields` seems to lead to serious degradation in type inference and (thus) the comprehensibility of type errors; when there is some mistake in my lens code, I often get quite confused. * Also, there is something strange about abstracting lens accessors in local variables. I will want to extract a common subexpression into a `let` or `where`, but when that subexpression is a composition of lenses, the polymorphism doesn't work nicely, and I can only fix it by adding a usually highly obscure type signature. * In general, I am growing more and more dissatisfied with data representation in Haskell, although I don't know of any language that does it substantially better. * I feel this when defining record types, especially when they begin to involve IDs and fields of the type `Map SomeID SomeStructure`. * There's something dissatisfying about these nested hierarchies which often involve strange redundancies or arbitrary placements of values. * It's like I would often prefer to think in terms of my program having incremental knowledge of a set of facts, rather than it keeping track of a hierarchy of data structures—but I don't know how exactly to do this. * We call them algebraic data types, but usually my "business data" is not very algebraic in nature, but more relational. * This is also related to the problem of SQL database mapping, which I feel very fortunate to not have to deal with in my usual work. * The type system is nicely expressive, but I have not gotten into the habit of more sophisticated type-level programming—i.e., using "dependent types"—and so I often end up for example having pattern clauses like `Nothing -&gt; error "internal error: ..."` which are logically unreachable and clutter up the code. * I kind of wish for a nice built-in multi-line text syntax *a la* Nix. * Streaming libraries like `pipes` somehow still have the ability to make me confused for hours about how to do something that seems like it should be easy (this will probably go away as I use them more regularly). These are some of my complaints. I still choose Haskell for many new programs, and I would call it my favorite general purpose language for most domains.
Issue 90 already, huh? There's quite some knowledge put together here by now. Do you think it would make sense to make the site searchable? For instance, if we had some tag mechanism, we could look for all articles about free monads or the ffi et cetera.
I spent a little over a week debugging a specialization bug that was causing our entire app to run extremely slowly. This mattered a lot since it was a frontend written in GHCJS, which needed to have a very fast user experience. Most of our frontend code was written with `mtl`-style classes, so it was all very polymorphic. If the specializer fails to specialize a large chunk of the app, it causes every statement in a `do` block (which is most of the app) to be far slower. So this problem really hit almost all of the issues I mentioned above. In order to debug it, we had to look at the profiling data, which wasn't super helpful. So then we looked at the core, which was really hard to decipher, but ultimately much more useful. We started rebuilding all Haskell libraries with many arrangements of GHC flags, which took forever, especially since the parallelism in the compiler is so bad. We would have preferred using none of these changes on TH code, since that just made TH even slower, but we didn't have a way to do this. We were making use of Nix binary caches, and because of the nondeterminism in GHC and the fast paced mass-rebuilds we were doing, one of us ended up in an inconsistent state where nothing could be built, which took a while to figure out. We eventually started trying to fix it in GHC, which proved to be a massive pain. It was just an incredibly inefficient process. There are lots of ways that GHC could improve that would have made the situation more bearable.
Frankly, I can't imagine this list consuming a significant portion of anyone's full time work day. These seem more like the problems that people have when starting a new Haskell project for the first time, not the problems that actual professionals have day-to-day.
What about using compact regions? https://hackage.haskell.org/package/compact
Do you see how that kind of seems like you're either discrediting my statements or saying I'm not an "actual professional"?
Sorry. I didn't mean to imply that. I just don't see how those things are the main time consumers in your day.
Well, I didn't think that was the question, really. Sure, I don't spend 4 hours every day dealing with these issues.
(It might be worth noting that my work does quite frequently involve the creation of new programs from scratch, so I would also consider the amount of boilerplate in a small Haskell project to be one of these problems—although sure, you can copy and paste a template project.)
I want to write a program so simple, I feel stupid during every keystroke. I give up after 5 minutes because I don’t know which knowledge I’m missing and I don’t want to poison myself with advice that comes out of thinking from within other programming languages. I keep coming back because I find Haskell fascinating. I just want to write a schedule that adds a set weird number of minutes and calculates when the count is 60 and above so that it can write the next hour. You guys can laugh and make fun of me if you want.
Well, the point is that each of these solutions trades off a little more safety for more complexity. Type-level arithmetic on Peano numbers is definitely a big step up in complexity, which is the cost for avoiding partial head. &gt; your comment about last isn't a problem with Vect, it's a problem because Haskell conflates data and codata. But that's just saying that Haskell doesn't split off yet another list-like type with yet another proof attached to it. Sure. There are arbitrarily many list-like types attached to different proofs, and Haskell could split off any finite number of them; but it still wouldn't be enough. So now we have: headList :: [a] -&gt; Maybe a headNonEmpty :: NonEmpty a -&gt; a headVect :: Vect (Succ n) a -&gt; a headStream :: Stream a -&gt; a That will last until the next time someone finds themselves in need of a more complex proof that some partial function is safe. Personally, I think this direction is probably a mistake, and it would be better to go the way of refinement types for safety. Constructing low-level syntactic proofs like this for properties that the compiler could just as easily prove for us heuristically is a waste of a lot of programmer time. But I'm happy to let people explore different options, and compare the best possible answers of each type rather than cutting off a direction before it's developed.
Agreed, I'm not a professional Haskell coder but this is a major part of why Haskell isn't my first choice in personal projects despite the fact that it's one of my favourite languages.
OP already gave an excellent technical justification of why Haskell could provide significant advantages for this project. And OP is aware of the fact that Haskell is being used successfully in an increasing number of projects of exactly this type. The question raised is the personal dynamics of the team, and how best to approach it. The personal preference of OP is a part of that, and it is legitimate, appropriate, and even essential, to include it in the discussion. Nothing unprofessional about that at all. In fact, interpersonal team dynamics is one of the most important factors in practice for the success or failure of any project. It would be grossly unprofessional *not* to address it.
&gt; It's nondeterministic I've always wondered about what caused this. Does anyone here know? AFAIK there have been some progress on fixing this but I'm not sure what's the status in GHC HEAD now. It'd be appreciated if someone who knows gives us an update.
I found the [thread](https://www.reddit.com/r/haskell/comments/7pwbnd/udemy_course_learning_haskell_programming_is_91/) that gave me this impression. To quote some provocative comments directly: &gt; Has anyone read the Udemy terms and conditions? I wouldn't click Agree if they gave you free courses and &gt; by using Udemy, you grant them the right to use your name, photographs and voice for advertisement purposes and that you waive your right for privacy and publication rights That's what I was trying to remember about them. I haven't actually looked at the terms &amp; conditions myself, but I wanted to let you know what people are saying about them, so you can make a better informed decision.
Could you describe this more specifically or give an example? How would you say the experience compares to other language ecosystems?
What's your first choice?
Probably Rust or Python these days, depending on what i'm trying to achieve.
I've played with the idea of making my own for a long time, but german law would require me to publish not only my name, but also my postal address, which I am reluctant to do. Since you stated that you wanted to learn about "concepts and features" (which unfortunately isn't saying much), I too would advice to first *work your way through* a good introductory book like [Haskell Programming from First Principles](http://haskellbook.com/) for instance. You may also wish to check out some of the resources on the right side of this subreddit, or some of the things listed on the [Info page of StackOverflow](https://stackoverflow.com/tags/haskell/info). A large part of the Haskell blogosphere deals with rather advanced stuff. Do *not* waste your time reading up on zygohistomorphic prepomorphisms or free monads, stuff like that is not required to get started on your first project.
I did this. It was some years ago though. As a result, our Haskell-based products are now running documentation portals for some of the world's largest enterprises. I'd be happy to have a conversation about it if you'd like. PM me if you're interested.
Thank you for clarifying!
Perhaps, but not necessarily. OP must self-assess ability and experience to determine what level of support is needed to make this project successful. It might be an additional hire, it might be backup from a good consultant, it might be just support from the community. OP is correct that the internal dynamic - getting other key people on board with it - is a critical component.
With languages like Python and JavaScript there's always a lot of commentary and user stats to allow our to get a feel of a library before using it. With older languages there's often a industry standard. With Haskell half the libraries are unmaintained, have newer and better forks or even just unfinished. They often create new operators or don't have documentation (even just to say that they are abandoned) and the experience is pretty painful. I love Haskell but we need to improve our developer experience.
I write Java on my day job, but I write Haskell on the side. Pain points that I got: 1. Lack of good IDE. Java IDE (IntelliJ) is extremely good. To name a few: 1. Go to definition 2. Highlight some lines, then convert to function 3. Highlight some characters, then convert to variable 4. Rename variable / function and it will be renamed everywhere correctly 5. Automatic import management (adding &amp; removing) 6. Faster code completion 2. Records ... I need to prefix all my records with the record name. For example: [link](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/master/src/Core/Types.hs#L40). I've tried `DuplicatedRecordFields` but then the type inference becomes potato. I heard `Lens` can solve this problem, but honestly, I have not figured how to yet. Is there any fellow Haskeller can point to some straightforward tutorial? 3. Debugger &amp; stack trace ... error in Java programs is easy to trace thanks to them both. Haskell ... very hard. 4. Managing runtime dependencies ... In Java, DI with Guice / Spring pretty much solves the problem with minimal boilerplate. I don't find anything satisfactory in Haskell. MTL? I hate that I have to write many typeclass instances. Freer? It's slow. Explicit dictionary passing? too many boilerplate when the function chain is deep. For now, I'm going with MTL as I think it's the most OK among the known approaches.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [eckyputrady/haskell-scotty-realworld-example-app/.../**Types.hs#L40** (master → 3e4174f)](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/3e4174fc0baa88846d4a6966176baba0df18f78f/src/Core/Types.hs#L40) ---- 
What do you choose Python for? It's "just" ecosystem, or dynamic nature of the language still holds some extra value for your projects?
Thanks for the advice. Also its illegal to run an anonymous blog in Germany?
I have no idea, but it reminds me of another (quite marginal) issue: the nondeterminism of functions like `HashMap.toList` which feels to me like bordering on impurity. When converting Aeson objects to lists, the order is essentially random, although I suppose it is probably always the same for a given compilation on a specific platform. (I've had a CI test fail only on some computers because of this...)
Like many of Roman's projects, it was motivated by the correct assessment that it would be a valuable contribution to our ecosystem. And like all of Roman's projects, it is an excellent design with very high quality implementation. It's true that Roman doesn't exactly spend full time working only on tasty, and there isn't a large team of contributors. But the library is simple enough that neither of those are required for Roman to keep it well-supported and up-to-date. And that simplicity, combined with the number of teams using it in production, make it pretty robust against bus syndrome. That's why I assess it as being a production-quality library.
I think it comes down to Hackage having not-so-great UX for discovery.
What about contributing did you find hard?
I was under the impression Freer was reasonably fast, despite the constant overhead from decomposing the union for each bind. Is this not the case?
You can run an anonymous blog if it's only about private things like your family and your pets. As soon as you target a more general audience (which you *do* with a blog 99% of the time), you have to provide this kind of information. This also applies for things like mobile apps in the Google Play Store, even if you do not charge money for it. There are certain lawyers that make a living looking for people that violate these kinds of laws and sending them an [Abmahnung](https://en.wikipedia.org/wiki/Abmahnung) while also demanding a lot of money. Of course, if you talk to people about these kind of things, you hear the usual Stockholm Syndrome comments. *You don't have anything to hide, do you?* Well, I guess I do.
**Abmahnung** An Abmahnung (German term for a written warning) is a formal request by one person to another person to forthwith stop a certain behaviour. If this formal request is made by an attorney, then it is similar to a cease-and-desist letter (but not a cease-and-desist order, which in turn is similar to a einstweilige Verfügung in German law). It is used much more between private parties than a consent decree is in the United States, since German law permits a wide range of parties to file suit to bring an end to an observed wrong done to a third party, whereas in the United States the requirements for a party's standing to sue are more restrictive (i.e. only the wronged party can sue). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Woah thats insane. Is there a name or something for this law? I'm interested in reading more about it.
&gt; It's like I would often prefer to think in terms of my program having incremental knowledge of a set of facts, rather than it keeping track of a hierarchy of data structures—but I don't know how exactly to do this. How are you representing that incremental knowledge right now? Perhaps nesting the "previous knowledge" in a record containing "newly discovered" fields?
Never heard this phrase in a programming context.
The law is called *Telemedia Act* (*Telemediengesetz*). From what I have seen, a lot of what you can find in English is not terribly accurate. For instance, some people suggest that the obligation to provide an *imprint* that contains the relevant information, applies only in a commercial setting. This is not true. I have found an exhaustive explanation [here](https://translate.google.com/translate?sl=auto&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Fwww.linksandlaw.info%2FImpressumspflicht-Notwendige-Angaben.html&amp;edit-text=&amp;act=url) and the google translation seems to be very good.
No, I believe the source is manufacturing or agricultural. The unneeded or bad stuff was pulled out and dropped on the floor.
As per this discussion, I'm thinking that we could make it monthly, rather. I'll look into automating it. Otherwise I can just do it manually once a month. 🤔
I keep hearing it in error-handling contexts, describing a caller that simply aborts if an exception occurs, rather than attempting a more nuanced recovery.
I see, that's a funny expression!
&gt; I heard `Lens` can solve this problem, but honestly, I have not figured how to yet. Is there any fellow Haskeller can point to some straightforward tutorial? Instead of `makeLenses` you would use `makeFields` or `makeClassy`. You can find information about it [here](https://github.com/parsonsmatt/overcoming-records), [here](https://artyom.me/lens-over-tea-6) or [here](https://www.reddit.com/r/haskell/comments/6jy8yu/the_has_type_class_pattern/) for a description not intimately tied to the `lens` package.
&gt; I kind of wish for a nice built-in multi-line text syntax a la Nix. Have you seen these? https://hackage.haskell.org/packages/search?terms=interpol You can order the table by highest rated or most downloaded by clicking on the header. Of these the one I found the most useful/reliable the most often is `interpolatedstring-perl6` but I sometimes also reach for `neat-interpolation` for its de-indentation capabilities.
&gt; In general, I am growing more and more dissatisfied with data representation in Haskell, although I don't know of any language that does it substantially better. &gt; &gt; I feel this when defining record types, especially when they begin to involve IDs and fields of the type Map SomeID SomeStructure. &gt; There's something dissatisfying about these nested hierarchies which often involve strange redundancies or arbitrary placements of values. &gt; It's like I would often prefer to think in terms of my program having incremental knowledge of a set of facts, rather than it keeping track of a hierarchy of data structures—but I don't know how exactly to do this. &gt; We call them algebraic data types, but usually my "business data" is not very algebraic in nature, but more relational. &gt; This is also related to the problem of SQL database mapping, which I feel very fortunate to not have to deal with in my usual work. Maybe this is the same problem that I ran into a while ago implementing a compiler in Haskell for a university project? Basically this algebraic data structure stuff is touted as basically being designed for representing abstract syntax trees, and indeed you can do this quite nicely. *However* in a real compiler you have different passes over the tree and often each pass adds some information to some nodes. For example type information. So do you create two entirely seperate data structures for your syntax tree. one with type info and one without? Or do you add a maybe field to be filled out later? Or something like giving all nodes an ID and creating a new map from IDs to type info? For just type information it is not a big problem, but there are many little pieces of info you would like to tag your syntax tree with, and it can become quite unmanagable. 
You can use `hs-init` to create project with all boilerplate generated for you... * https://github.com/vrom911/hs-init
Not sure if this is the sam non-determinism you're talking about, but [https://ghc.haskell.org/trac/ghc/ticket/4012](https://ghc.haskell.org/trac/ghc/ticket/4012).
&gt; It's like I would often prefer to think in terms of my program having incremental knowledge of a set of facts, rather than it keeping track of a hierarchy of data structures—but I don't know how exactly to do this. This is what Clojure is particularly good at in my experience. You can simulate the mechanics of what you would do in Clojure by using maps/sets but the ergonomics are entirely different.
&gt; Managing runtime dependencies ... In Java, DI with Guice / Spring pretty much solves the problem with minimal boilerplate. Have you seen this? https://hackage.haskell.org/package/hs-di &gt; MTL? I hate that I have to write many typeclass instances. Here is some comparison of how much less boilerplate it allows compared to MTL: https://www.reddit.com/r/haskell/comments/797f1c/ive_converted_the_mtlstyleexample_that_was_posted/ &gt; runtime dependencies I'm not fully sure it can help you with managing *runtime* dependencies. If not with its core features, then perhaps it can with its [InjIO experimental feature](https://github.com/Wizek/hs-di#monadic-inject-injio). Let me know if you have any questions or comments.
&gt; Perhaps a case of some sort of cognitive bias? The [Baader-Meinhof phenomenon](https://www.damninteresting.com/the-baader-meinhof-phenomenon/)?
Your suggestion reminds me of [Trees that Grow](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf).
Wow, users! Who would have thought…
I believe you may be referring to the Baader-Meinhof phenomenon, also called as frequency illusion.
have you noted that one: http://hackage.haskell.org/package/interpolate, it's quite fresh, but seems to good parts of both libraries you mentioned, if i read they docs right.
Yes, that one caught my eye just today! I haven't tried it yet though. It would be pretty nice if it had all the good parts.
That syntax conflicts with `CPP`.
You may be interested in [this paper](http://www.jucs.org/jucs_23_1/trees_that_grow/jucs_23_01_0042_0062_najd.pdf) describing the approach that GHC will be trying (although likely not until 8.6).
I like this approach; I also quite like what the paper called "phase-indexed" types. One little issue is that the data types become a bit more strange—and hard to deal with for e.g. the lens Template Haskell generators. That may not be a theoretical issue but getting up to speed with the combination of lenses and this rather cutting edge technique for extensible types is a pretty steep and somewhat terrifying challenge!
I really don't like that syntax!
I sometimes debate whether the convenience is worth bringing in template haskell.
Haskell mostly has dependent types if you're willing to endure enough pain!
There was no defect between cabal version 2.2 and a previous version that prevents you from passing C++ flags through the C flags field if you're project ONLY uses C++ sources and they don't involve the C++ language version differing from the C computer's default version. However, if your project has both C sources that require C flags and C++ sources that require different reasoning C++ flags you can run into a situation where you need separate compiler options past to the separate source files for the code to correctly compile. An example would be requiring the `-std=c11` for your C sources and `-std=c++14` for your C++ sources. The C compiler will at minimum warn you that you have incoherent flags for all your sources trying to be compiled together, but depending upon the C compiler being used it usually just fails with an error. The Travis build errors linked in the post were exactly what I was experiencing before submitting the pull request. See here for the original issue: https://github.com/haskell/cabal/issues/3700 And here for the pull request: https://github.com/haskell/cabal/pull/4810
Is there some definitive resource on how building using `stack` or `cabal` and non-c languages is *supposed* (assuming no bugs in the build system) to look like? For instance say I have some heavy-duty number crunching code written in fortran. I should *theoretically* be able to include it in a Haskell project by exporting c-bindings from the fortran side, using `gfortran` or `ifort` or something as "c-compiler" and use Haskell's FFI. Except it seems [I can't](https://github.com/haskell/cabal/issues/1325) and, to quote from this issue, "Cabal compiles C sources with ghc, which also *happens* to work for C++ and Objective-C because the files are just passed to `gcc`"?
Vectors are an optimization - a premature optimization when not needed. Lists are far simpler conceptually in the context of pure functional programming and Haskell's type system. Semantically, among other things, they represent the fundamental concept of looping in a pure functional language, whereas recursion is more akin to a low-level "goto" in imperative languages. While lists are harder than vectors to make super fast, they are quite heavily optimized in GHC, and hopefully will be even more so soon. (There is a proposal to cut out almost all of the extra memory overhead except when there is sharing.) So the difference is quite often insignificant, and shrinking. Overall, in my opinion lists are clearly the best default, and should continue to be what is taught to beginners. As you advance and learn about optimizations, you will learn about vectors, Data.Seq, and others.
Hah, nailed it! Just kidding, I remember looking at the first view pages of this paper and I thought this seems like some heavy-duty stuff. Has this approach seen some action already, or will you guys be the ones to "try it out"? Anyhow, thanks for this very interesting suggestion.
But then you are committed to user every extension for every module in the package. That is often not what you want.
Doesn't `gfortran` also just "pass the files to `gcc`", i.e., isn't it also just `gcc` with a different option? In any case, yeah, current FFI is mainly designed to work with C. Even for C++ it is a common technique to wrap the part of the API you need with a thin C wrapper. That might be what you need to do for Fortran.
I know a big issue in the past where internal names not being generated in a consistent way between builds. These would then go on and be used for sorting definitions among other things which lead to slightly differing results.
Just spent 20 minutes googling for why this synonym for *frequency illusion* is named after a completely unrelated west german terrorist group. &gt; The St. Paul Minnesota Pioneer Press online commenting board was the unlikely source of the name. In 1994, a commenter dubbed the frequency illusion "the Baader-Meinhof phenomenon" after randomly hearing two references to Baader-Meinhof within 24 hours. https://science.howstuffworks.com/life/inside-the-mind/human-brain/baader-meinhof-phenomenon.htm
I blame that on CPP- even as a source code textual preprocessor CPP is below average. Consider m4 as an alternative. The only reason why CPP is used is because, since it comes standard in C/C++, everyone already knows it. It's the Microsoft Windows of textual preprocessors, except worse.
Haskell has never grown out of its research language roots in my opinion. The assumption seems to be that if you’re using the language, then you have the time to read through the codebase of every library you’re thinking about using, and to expand a library if it doesn’t do what you need. That’s great when you have grad students, not so much when you have deadlines. Every library offers a bespoke version of Strings/Text/Data that doesn’t play well with any other version. There’s no modularity - if you want to start using a new library in your code base, you have to expect a massive headache. This is not the case in any other professional language. 
Another one of the most prominent and respected members of our community, [Bartosz Milewski](https://bartoszmilewski.com/), completed a four-year undergraduate degree in 1974 and his Ph.D. in 1980, according to his [linkedin profile](https://www.linkedin.com/in/drbartosz/). You do the math. If Bartosz wanted to work with me, I would hire him in less than the bat of an eyelash.
Still no working library to connect to commercial databases (MS Sql Server, Oracle, Db2). HDBC-ODBC is completely broken on linux as of today. I'm using my own patched fork and even with that I have to accept occasional error here and there. But for mission critical processes where errors are unacceptable, I had to write sqlrelay through dotnet service on windows to interact with MS Sql server. Web frontend development is still painful, even with reflex. To the point where after successfully developing a middle sized web application with reflex, I had to make a decision to stop using it. I'm now writing all my web frontends with plain server generated html and JQuery. That's right, not even reactjs or angular or any other over-engineered SPA solutions. I've been there. Tried them all. In the end, had to accept that as of today the modern web frontend ecosystem is in unusable state. 
This is the soapbox I'm going to die on, I swear. Sure, freer is slower than MTL but it's not slower than Ruby. Unless you're writing things on the trading floor, it's not going to be an issue.
Well the part with best cost/reward ratio would be parallel building. As you give GHC more cores, there's some point where it seemed to me that GHC just starts falling off a cliff in terms of performance. So that would be the #1 thing to fix. But also having something similar to `make`'s`--max-load` argument would be an easy way to help a lot, as I currently have to do a lot of finagling with `nix-build --cores` to use my 32 threads efficiently on mass rebuilds without either blowing up the load average or neutering highly parallel C builds. It'd be extremely cool if this were done at the RTS level, so that any Haskell program could be limited by system load, not just GHC. There needs to be a subset of TH that can be interpreted in the host GHC, so that my crazy compiler flags affect TH runtime minimally, and so it works with cross compilation. Compiler determinism seems really important, but I'm told it's much harder than it's worth. I kind of think we need some kind of link time optimizer. Something that only outputs Core until link-time, at which point it does whole-program Core optimization. For mass rebuilds, I actually think this would be a much faster optimization technique. And there's a number of optimizations that seem like they would benefit a lot from a top-down style pass (such as specialization), as opposed to the currently eager bottom-up style. But for incremental building, I think there are ways to make LTO/whole-program optimization relatively incremental. Finally, I don't have any idea how, but profiling info needs to be made more useful.
About &gt; Also, there is something strange about abstracting lens accessors in local variables. I will want to extract a common subexpression into a let or where, but when that subexpression is a composition of lenses, the polymorphism doesn't work nicely, and I can only fix it by adding a usually highly obscure type signature. Let's not blame `lens`, it's `MonomorphismRestriction`, because `Lens` is a polymorphic type: The below code is in https://gist.github.com/phadej/4ce2a0b608126349f7f176b3a05fdd07 too (syntax highlighted): {-# LANGUAGE TemplateHaskell #-} import Control.Lens data Foo = Foo { _fooInt :: Int, _fooBar :: Bar } deriving Show data Bar = Bar { _barInt :: Int, _barQuu :: Quu } deriving Show data Quu = Quu { _quuInt :: Int, _quuStr :: String } deriving Show makeLenses ''Foo makeLenses ''Bar makeLenses ''Quu -- | &gt;&gt;&gt; foo -- Foo {_fooInt = 1, _fooBar = Bar {_barInt = 2, _barQuu = Quu {_quuInt = 3, _quuStr = "lens example"}}} foo :: Foo foo = Foo 1 $ Bar 2 $ Quu 3 "lens example" -- | &gt;&gt;&gt; example1 -- Foo {_fooInt = 1, _fooBar = Bar {_barInt = 2, _barQuu = Quu {_quuInt = 12, _quuStr = "lens example"}}} example1 :: Foo example1 = foo &amp; fooBar . barQuu . quuInt .~ (foo ^. fooBar . barQuu . quuStr . to length) -- This doesn't work: -- Couldn't match type ‘Identity Foo’ with ‘Const Int Foo’ {- example2a :: Foo example2a = foo &amp; l . quuInt .~ (foo ^. l . quuStr . to length) where l = fooBar . barQuu -} -- | This works, we eta-expand `f` (even HLint says we shouldn't :) -- -- In example2a l :: (Quu -&gt; Identity Quu) -&gt; Foo -&gt; Identity Foo -- here l :: forall f. Functor f =&gt; (Quu -&gt; f Quu) -&gt; Foo -&gt; f Foo -- -- &gt;&gt;&gt; example2 -- Foo {_fooInt = 1, _fooBar = Bar {_barInt = 2, _barQuu = Quu {_quuInt = 12, _quuStr = "lens example"}}} -- -- You can make example2a work using NoMonomorphismRestriction too. Try it! -- -- MonomorphismRestriction is a trade-off, -- -- from: https://wiki.haskell.org/Monomorphism_restriction -- -- So why is the restriction imposed? The reasoning behind it is fairly subtle, -- and is fully explained in section 4.5.5 of the Haskell 2010 Report. -- Basically, it solves one practical problem (without the restriction, there -- would be some ambiguous types) and one semantic problem (without the -- restriction, there would be some repeated evaluation where a programmer -- might expect the evaluation to be shared). Those who are for the restriction -- argue that these cases should be dealt with correctly. Those who are against -- the restriction argue that these cases are so rare that it's not worth -- sacrificing the type-independence of eta reduction. example2 :: Foo example2 = foo &amp; l . quuInt .~ (foo ^. l . quuStr . to length) where l f = (fooBar . barQuu) f
I thought I had tried disabling the monomorphism restriction, but that's very good to know. Maybe my problem was something slightly different...
At work, we've pretty much solved all my issues with records using something that's almost identical to the [overloaded-records](https://hackage.haskell.org/package/overloaded-records) package. It effectively puts field names in a different namespace and, mixed with a couple of extensions (`DuplicateRecordFields` and `OverloadedLabels`), makes it easy to use multiple records all defining fields with the same names.
Take a look at the [overloaded-records](https://hackage.haskell.org/package/overloaded-records) package. We use something really similar at work and it has solved pretty much all my issues with Haskell records.
I started writing this polemic to answer your question, but I ended up touching on all of my gripes with Haskell in general, not just in a corporate context. ## Module system Haskell has no way to make nested modules, no way to do qualified exports, and it doesn't even have C++ style namespaces. Any of these features would allow you to, for example, export a symbol called `Text` from `Data.Text` that can be indexed as a module (e.g.: `Text.pack`), thus allowing you to simply `import Data.Text` without any qualification needed. Moreover, this would allow custom preludes to subsume all the stuff you normally import; as it stands, you can add as much stuff to a custom prelude as you want, but it _has_ to be in the same namespace, or else every time the prelude is used there must be multiple imports. I really want a world where you can just `import MyPrelude` and have `BS.*`, `LBS.*`, `Text.*`, `LText.*`, `Set.*`, `Map.*`, etc. already in scope. Plenty of languages already have this feature; I don't understand why Haskell is so antiquated in this regard. There are three main workarounds for this issue (i.e.: solutions that don't involve modifying GHC). The first is to make all the functions that are likely to clash polymorphic enough that they can be used in both contexts. This is the approach taken by things like [mono-traversable][] and most custom preludes. This works for some functions, but if taken to an extreme I think it reduces type inference, makes code more complicated, slows down compile times, and slows down code at runtime (due to dictionary passing). The second solution is to maintain a strict import discipline. This is what I currently do, and it results in insanely long import lists that contain a lot of pairs of lines like import Data.Foo (Foo) import qualified Data.Foo as Foo This is obnoxious and repetitive, but it's the most sensible solution given the current library ecosystem. The third workaround is to adopt an OCaml-style convention where we have one type or typeclass per module that is always named `T` or `C` respectively. This means that you could just do `import qualified Data.Foo as Foo` without also having to import the type named `Foo`, since it will now be named `Foo.T` rather than the obnoxious `Foo.Foo`. The only Haskell programmer I know of that has adopted this convention is Henning Thielemann (example: [numeric-prelude][]). The main problem with this approach, besides some people generally not liking it, is that Haddock generates [really confusing documentation][ridiculous], since AFAIK Haddock doesn't ever display the qualification of a symbol. If the Haddock problem were fixed, I'd be willing to adopt this convention, but it seems like a real uphill battle convincing everyone else to do it too, so I think this is probably best for company-internal code where this style could be enforced. It is also worth mentioning that nested modules / namespaces would be pretty useful for Haskell's record problem, given that they would allow you to more easily namespace record accessors. For the most part, though, this would only fix Haskell records for the consumer of a record; it would be fairly boilerplatey to write records this way. For example, you would define `Data.Foo` like: module Data.Foo (type Foo.Foo, module Foo) where module Foo (Foo (..), new) where data Foo = Foo { bar :: Bar , baz :: Baz , quux :: Quux } new :: Bar -&gt; Baz -&gt; Quux -&gt; Foo new = Foo and then you would import `Data.Foo` to use the following names: `Foo` (type), `Foo.new`, `Foo.bar`, `Foo.baz`, and `Foo.quux`. This isn't perfect, but it's better in some ways than the current situation, and I can imagine that the changes that would need to be made in GHC to support nested modules would be fairly conducive to adding Agda-style support for records that automatically generate modules like this. [mono-traversable]: https://hackage.haskell.org/package/mono-traversable [numeric-prelude]: https://hackage.haskell.org/package/numeric-prelude [ridiculous]: https://hackage.haskell.org/package/numeric-prelude-0.4.2/docs/MathObj-Polynomial.html#t:T ## Records The current situation with records in Haskell is kind of nightmarish. I really wish we could just have row polymorphism like PureScript. [There's][rémy] been [quite][blume] a [bit][fclabels] of [research][koka] on [the][stedolan] [subject][morris], I don't really understand why the GHC team is so conservative about adding it to the type system, especially given the fact that I'm pretty sure most implementations of it can be completely eliminated into GHC Core; the existence of [vinyl][] and [union][] certainly implies this, although the fact that those necessarily have linear-time accesses in present-day Haskell might mean that we need to extend Core in some way to make an implementation of row polymorphism efficient (I honestly don't know). One other thing is that, in addition to making the record situation easier, row polymorphism can be extremely useful in other ways. For instance, we could have instances of Aeson's `FromJSON` and `ToJSON` typeclasses for a generic record type^1 like `Rec` from [vinyl][]. This is useful because most of the time, when you wrap an API that uses JSON, you want two different "levels" of types; one level is a straightforward translation of the JSON format described in the API documentation (which requires comparatively little effort to completely wrap the API), and the second level is a more high-level Haskell-appropriate translation of those types (which is almost never an up-to-date complete description of the API). Since the `FromJSON` and `ToJSON` instances for the low-level types are so trivial, you really want them to be automatically generated. Sure, you _can_ do that with `GHC.Generics`, but then you have to either use `DuplicatedRecordFields` or prefix all your fields, and that ends up being much worse than the situation I'm talking about. I know this because there is actually already a package that uses [vinyl][] for this workflow, called [composite-aeson][], and I've used it to wrap APIs (e.g.: the [Bittrex API][]). In general, I think a lot of the things we currently use `GHC.Generics` for are better served by adding instances for anonymous row-polymorphic records/unions, since I don't think it's reasonable to have the semantics of your program depend on the identifiers you chose for your record accessors (in the anonymous record world, these are type-level strings or empty data types equipped with instances of an open type family into `Symbol`, so it is much less surprising that program behavior will change if the type-level string or open type family instance is changed). [rémy]: https://www.cs.cmu.edu/~aldrich/courses/819/row.pdf [blume]: https://people.cs.uchicago.edu/~blume/papers/icfp06.pdf [fclabels]: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/fclabels.pdf [koka]: https://arxiv.org/pdf/1406.2061.pdf [stedolan]: https://www.cl.cam.ac.uk/~sd601/thesis.pdf [morris]: https://homepages.inf.ed.ac.uk/jmorri14/pubs/morris-haskell15-variants.pdf [vinyl]: https://hackage.haskell.org/package/vinyl [union]: https://hackage.haskell.org/package/union [composite-aeson]: https://hackage.haskell.org/package/composite-aeson [Bittrex API]: https://gist.github.com/taktoa/5748e211a95f150e030326a1b606f451#file-bittrexapi-hs-L469-L796 ## GHC ## Type system Haskell doesn't have [quantified class constraints][quantcc]. This means that some typeclasses, like `MonadTrans`, cannot be written in a way that restricts instances to have the desired behavior; the way you would want to write the `MonadTrans` class is that any instance `t` should have the property that for any monad `m`, `t m` should also have a `Monad` instance, but you can't express this kind of superclass constraint without quantified class constraints. Haskell doesn't have dependent types, though they are on the way. I don't think dependent types are necessarily something you should use all the time, but they are pretty nice to have when needed. [quantcc]: http://homepages.inf.ed.ac.uk/wadler/papers/quantcc/quantcc.pdf ## Footnotes 1. If your system has anonymous record types like [vinyl][]'s `Rec` type, just use those, and if it doesn't then you can make instances of those typeclasses on a wrapper type defined like newtype Wrapped t = Wrapped (∀ ρ. t ⋄ ρ) where `t` is a type variable representing the required field types and `⋄` is row combination (with [vinyl][] `⋄` would be a closed type family that does type-level list concatenation). 
I'm referring to the granularity of derivations. If you have package A that depends on B and B is changed, you'll have to rebuild all of B before working on A again. This is really painful if B has a lot of modules and you've only changed one function. Solving this problem would require something akin to derivations being on a per module basis. 
Don't do that
I believe GHC will be the first project of non-trivial size to deploy this approach.
[removed]
Slow build times impact our ability to push out fixes to production quickly. In some cases the fix only took 5 minutes but took 40 minutes to build.
Agreed. I love Haskell but I feel like I'm going to have to go elsewhere if I want to use strong typing and pure functions (and they make debugging so much better).
&gt; Well the part with best cost/reward ratio would probably be parallel building. Have you tried increasing `+RTS -A`? GHC, like most Haskell programs, struggles with GC synchronization overhead with small nursery sizes and many capabilities. I've seen reasonably good parallel build perform up to six cores or so with an increased nursery size. Doug Wilson has been doing some great work improving the scalability of `--make` (see [#14095](https://ghc.haskell.org/trac/ghc/ticket/14095)) although for 32 cores I suspect you are far better off parallelizing across packages; in general most projects simply don't have this many non-dependent modules. Perhaps some day we will be able to parallelize with finer granularity, but this will be a non-trivial effort. &gt; As you give GHC more cores, there's some point where it seemed to me that GHC just starts falling off a cliff in terms of performance. So that would be the #1 thing to fix. But also having something similar to make's--max-load argument would be an easy way to help a lot, as I currently have to do a lot of finagling with nix-build --cores to use my 32 threads efficiently on mass rebuilds without either blowing up the load average or neutering highly parallel C builds. It'd be extremely cool if this were done at the RTS level, so that any Haskell program could be limited by system load, not just GHC. This sounds good on paper, but I'm not sure how useful it would be in practice. Feedback loops are tricky to get right and it's quite unclear how to avoid poor (e.g. oscillatory) behavior when two processes auto-scaling processes interact. I suspect the right approach here would be to rather teach `nix` about the amount of parallelism that a given derivation can take advantage of. This could either be profile-guided or manual. 
Managing imports. Specifically: * I have to import everything twice (once for qualified imports and once for unqualified imports) * Libraries that don't re-export everything in one module for convenience (like `text`, for example) * Further import proliferation due to dealing with multiple types of `ByteString`, `Text`, and `Vector`
I like to use the following workaround for multi-line `String`s to avoid all the escaped newlines. example = unlines [ "The quick brown fox" , "jumps over the lazy dog." , "Lorem ipsum" , "dolor sit amet." ]
Most of my "Haskell pain points" disappear when I use `stack` and a custom Prelude. I also avoid a ton of other pain points by using Ubuntu instead of Arch, OSX, or Windows. Needing to fork libraries to provide more lenient upper bounds is annoying, especially when those changes go unmerged for months. I'd rather have conservative upper bounds and PRs/new releases than Hackage revisions, though. Especially given how seamlessly easy it is to refer to forks using `stack`. The module system could be better -- I would like to give everything nice names that work out when the module is imported qualified, but that results in long import lists. If we had qualified reexports, or the ability to alias names (eg `import Data.Map (lookup as mapLookup)`, then this would be easier to work around. The lack of `ImpredicativePolymorphism` bites me when I'm trying to create records of polymorphic functions, mostly when trying to hide constraints (eg `type DB a = forall m. MonadIO m =&gt; SqlPersistT m a` allows me to write `foo :: DB Int` rather than `foo :: MonadIO m =&gt; SqlPeresistT m a`). Not a huge deal to use `newtype` to work around, though. The path to a good, stable, and fast recompilation cycle is unclear and finicky. `ghcid` works great out of the box, until your stack project gets complex, or you start using TemplateHaskell. If you want to have a tight edit/test loop, we're way slower than Ruby with the `guard` library (the fastest widely known option is `stack test --fast --file-watch`, which recompiles everything and runs the entire test suite; getting `ghcid` to run tests is cool, but it's still stuck running the entire suite). Records are great. Too bad Haskell doesn't have them. VERY much too bad that Haskell adopted a terrible bolt-on "record" syntax to fool people into thinking we do, only to fail us at the very end with partial creation, clunky updates, and partial accessors in sums.
So don't use lists, use `data Stream a = a :&gt; Stream a` for intentionally infinite streams.
Thanks for the link! Looks interesting.
With lenses you can do something like this: {-# LANGUAGE TemplateHaskell #-} import Lens.Micro data Gender = Male | Female data Person = Person { _name :: String , _age :: Int , _gender :: Gender } makeLenses ''Person canDrink :: Person -&gt; Bool canDrink p = (p ^. age) &gt;= 21 addAge :: Int -&gt; Person -&gt; Person addAge n p = p &amp; age %~ (+n) See &lt;https://github.com/ekmett/lens/wiki/operators&gt; for info on lens operators.
Thank you very much :D Yes, I decided to start on haskellbook instead after listening to the suggestions of the helpful people in the irc. I will power through it and let you know how I feel about it!
Thank you! I agree that I should definitely practice as I learn. As of now, I have no idea what IO, CLI, etc. are, but I will look it up and follow your suggestion and make those things once I get at that level. I decided to learn from haskellbook instead, so I think I'll finish that and make the things that you suggested once I have the skills. 
Ah, but `()` is a very generous type, since you can freely construct it. You also need to be very conservative about the type of the resulting expression. In your case, what you end up with is `Ap Unit1 Integer`, imagine the following case: data OpaqueType -- We don't expose any structure about this type data Opaque a where Opaque OpaqueType f1, f2 :: OpaqueType -&gt; OpaqueType interpretOpaque :: Ap Opaque OpaqueType -&gt; IO () Now, internally, `OpaqueType` could have enough structure to recover the sequence of `f1` and `f2`s in the `Ap Opaque OpaqueType` value. The nice thing is that this goes far beyond `Applicative`, I have a very incomplete proof of concept work recovering the entire `Arrow` expression, and I believe you can even use the trick for `Monad`s.
Thank you! Yes, I decided to start my learning with haskellbook :P I will look into Functional Pearl papers later when I feel like it will be most beneficial to me. As for now, I will power through the book and see where that gets me. Thanks for the suggestions!
Easy to get something up and running, good ecosystem, easy to write code in a functional style, and if I plan on collaborating with others pretty much everyone knows it (or could learn it fast). What's more, these days with mypy you can type check your Python too. Main disadvantage is that it's slow, but it's pretty easy to write Rust or C modules and call them from Python if needed via the C ABI.
Isn't non-determinism usually considered an effect (i.e. impure)?
I'm on Rust for systems, TypeScript for UI-heavy work, and F# for everything else. Consistent languages with modular features.
No, I've heard it all over the programming world, but always associated with networking/full packet queues as that's where I encountered it first. As long as we're posting psychological reasons and as a graduate of my freshman psych course let me put forth the [primacy effect](https://en.m.wikipedia.org/wiki/Serial-position_effect#Primacy_effect). See also the similar phrase ['threw it on the ground'](https://m.youtube.com/watch?v=gAYL5H46QnQ).
Non-Mobile link: https://en.wikipedia.org/wiki/Serial-position_effect#Primacy_effect *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^140250
Don't I've ever heard this in a Haskell context, honestly. I've heard it plenty outside of Haskell.
That's an interesting idea. I'm not crazy about tagging. The site should be searchable through Google. For example: https://www.google.com/search?q=ffi+site%3Ahaskellweekly.news
&gt; Solving this problem would require something akin to derivations being on a per module basis. There’s work being done here I think. Look into the cabal2ninja and ninja2nix efforts. 
/u/AutoModerator can take care of scheduled posts. [Here](https://www.reddit.com/r/AutoModerator/comments/1z7rlu/now_available_for_testing_wikiconfigurable/) is a post detailing how to set it up.
Totally agree. I am also just toying with Haskell and the docs of most libraries are "just read the types". Which is really hard for me and which, I think, really puts off tons of beginners. Coming from JavaScript/Java world, where everything has nice examples, it's just unbelievable contrast. For my projects I usually grab Scala. I occasionally miss Haskell's god-like type inference, but Scala is in my opinion the best of both worlds. There is a lot of pains when dealing with Haskell (at least for me, a noob) - poor IDEA support (two plugins, one works better, but randomly breaks; other is more reliable, but offers almost nothing), building with a lot of boilerplate (having to mention libraries for every module manually; compared to NPM also quite cumbersome adding of dependencies) and my most hated "feature" are records (with most basic approach there are conflicts in accessor functions, so one must use some magic which requires getting imports manually right in records with same fields or ending up with wrong types and confusing errors from compiler).
I wrote a response to this question, but it ended up too long to be a reddit comment, so [here's a GitHub gist with the text of the comment](https://gist.github.com/taktoa/a59400fd3e1c400835b60c416ad33952). I "mentioned" a few people in the comment as originally written, and they will obviously not get notified if I mention them in a gist, so I'll cc them here: /u/dmjio /u/Sonarpulse /u/deech
I'd be keen to hear more about your experience with reflex. I'm a fan of the library, but I've got an interest in being aware of pain points there (and occasionally trying to fix them). 
That's an interesting idea. Maybe combined with `ixset`...
Bit late to the party, but here I go. Disclaimer: I'm a professional Haskell programmer (i.e., I get paid to write Haskell code), but I would not consider myself an expert. ------------------- Space leaks are a mess to debug and performance problems often arise from non-obvious constructs and only in real-world usage. Let me quote someone [from realworldhaskell](http://book.realworldhaskell.org/read/testing-and-quality-assurance.html): &gt; Haskell code all too frequently goes wrong in an awful way, by working perfectly on small test cases but running out of memory on large real problems. This chapter REALLY should say something about profiling and space leaks. That is not a question of performance tuning or premature optimization. In Haskell, if you haven't got the space complexity figured out (possibly with strictness annotations in the right place), your program effectively doesn't work at all. STL designer Alex Stepanov said (http://www.sgi.com/tech/stl/drdobbs-interview.html): &gt; &gt; " Let's take an example. Consider an abstract data type stack. It's not enough to have Push and Pop connected with the axiom wherein you push something onto the stack and after you pop the stack you get the same thing back. It is of paramount importance that pushing the stack is a constant time operation regardless of the size of the stack. If I implement the stack so that every time I push it becomes slower and slower, no one will want to use this stack. &gt; &gt; We need to separate the implementation from the interface but not at the cost of totally ignoring complexity. Complexity has to be and is a part of the unwritten contract between the module and its user. The reason for introducing the notion of abstract data types was to allow interchangeable software modules. You cannot have interchangeable modules unless these modules share similar complexity behavior. If I replace one module with another module with the same functional behavior but with different complexity tradeoffs, the user of this code will be unpleasantly surprised. I could tell him anything I like about data abstraction, and he still would not want to use the code. Complexity assertions have to be part of the interface. " &gt; &gt; My current way of looking at this issue is that it's a deficiency of Haskell's type system, that real users have to know how to work around if they want to get anything done in this language. I call it a type system deficiency because Haskell supposedly separates pure from impure functions. But there is really no such thing as a pure function. A print action is impure because it makes text appear on the screen. A "sum [1..10000000]" computation is impure because it makes time go by on the clock. Haskell (like most languages) doesn't statically capture complexity assertions, and unlike most languages the complexity often comes out surprising due to the evaluation strategy. &gt; &gt; Anyway, somewhere in the book I hope very much that there is extensive advice (not just saying "it's hard") about how to track down and fix time and space leaks. As you may have guessed, I'm in the middle of trying to track one down right now, and thought I'd check the beta of the book... ------------------- Haskell lacks professional tooling. That is, a good, speedy IDE with a good debugger. Simple as that. ------------------- Text vs String. ------------------- Libraries are not necessarily bad, but their documentation is often extremely so. Look [the page describing QuasiQuoting](https://wiki.haskell.org/Quasiquotation): * 2 links to some academic paper and very spare documentation of the -XQuasiQuotation flag * "Some examples where the result is a string" * "Some examples producing other things" * Some other tutorial This is absolutely awful! There is no way to systematically find out what you're looking for, and almost all datatypes and functions are completely undocumented. Reading the "official" PDF gives a good idea on what it is, and how it should work in theory, but most of the time I just want to *use* a library, not thoroughly understand it. I would love to, but deadlines are a thing.. ------------------- I feel that the Haskell community does not focus enough on readability. Many modules can only be understood after learning tens and tens of abstract operators. Including, but not limited to: `&lt;$&gt;, &lt;&gt;, *&gt;, &lt;*, +=, -=, %=, .=, &lt;$$&gt;, &lt;+&gt;, &lt;*&gt;`. Then, there is a tendency to use one-letter variable names all over the place, or use ignores in function signatures (or: leave them out AT ALL using a point-free notation). For example: ``` g :: Int -&gt; Int -&gt; Int -&gt; Int g _ _ c = .. g a _ _ = g _ b _ = ``` Each pattern match would be on wildly different lines, making you scan three different definitions to find out the argument names of `g`. In an ideal world, everything would be documented of course, but I think we all know this doesn't work that way.
It's kind of an edge case because you can think of it as the hashmap module having a case check on a constant like System.currentPlatform or whatever. I don't think it gives different results each time you run your program, only when you move to another system...
I've heard of it in movie-making.
Unfortunately, there are no installation instructions for Ubuntu.
Your problem is probably because of -XMonoLocalBinds which is enabled if you use -XGADTs or -XTypeFamilies. This is because of the new type inference engine from GHC 7.2 which is a lot more robust by sacrificing let generalization because that feature was ridiculously rarely used and let to a lot of complications.
[removed]
I'm working on a compiler for a hobby project and for what it's worth I'm settling on the following. My apologies if this isn't quite what you're referring to. (Also this is a bit simplified because I have a surface syntax and a "core" syntax but the same principles apply). - Syntax tree is a functor, using the type variable in place of the recursive definitions. - Parser constructs a value of a free monad over the syntax functor, initially with () as the base type. - I then translate this into a free comonad value over the syntax type, again with () as the base type. Now I have a different kind of tree representation where every node in the tree is an expression value paired with whatever I want. My passes over this structure are comonadic extensions which modify the annotation of every node in the syntax tree automatically. Lots of ways to go further with it but maybe this is useful? I shamelessly stole the foundation of this idea from a Brian McKenna blog post. I'm sure there are reasons why some kind of free (co-)applicative or traversable representation above my pay grade is more optimal but I'm content. 
These have been my exact issues! I started using Haskell about 4 months ago and now I'm trying to build website in yesod which claims you don't have to be a Haskell expert to use it but that's just not true because only a small fraction of the docs has examples. The docs usually just has the type and a 1 line description so most of the time I know what I want and I know which function does what I want but I don't know how to use it.
&gt; Do you want to introduce Haskell because you genuinly think it will help your project be better and cheaper, or is your main motivation for it just because of personal reasons? I think it's a bit of both. I did do the requirements gathering and specifications for the project. It is my assessment that the risk demands we take more precautions than a typical project for us. That is why I said I think it's worth taking the effort to design the system to be _correct by construction_. The worst thing that could happen if an error were to happen in a production setting would be harm to business and possibly, though indirectly, to people. Think food and drug manufacturing -- the system manages information created and used in the production, testing, and quality control of such products. Personally I'm motivated by correctness and reliability: I believe people should be able to trust that the I've used the best known practices to engineer a robust and reliable software system.
Immutability, lack of pervasive `null`, strong expressive types, and (generally) correct defaults help a lot. It's so easy to encode your domain into types. `data Maybe a = Nothing | Just a` is an example -- you get best-in-class null handling with library code, not even a built-in language feature. By making it really easy to pass responsibility for correctness to the compiler, you can shift a lot of mental overhead from your own brain into the computer. This gives you more mental free space to solve the problem at hand. Haskell's purity also strongly encourages you to write pure code. Pure code is vastly easier to test and reuse than effectful code, so you end up having smaller, more correct chunks to play with. Because of purity and immutability, our programs tend to have very direct information flow, and are easier to follow. Consider this Haskell code: foo &lt;- makeFoo foo' &lt;- doStuffWithFoo foo let foo'' = doStuffWithFoo' foo' ... From looking at the code, we can know that `doStuffWithFoo` takes a `Foo`, carries out some side effects, and returns a new `Foo`. We can also tell that `doStuffWithFoo'` doesn't perform any side effects, because it's used purely. So if we're looking for side-effect related bugs, we know exactly where to go.Consider this Java, though: Foo foo = new Foo(); doStuffWithFoo(foo); doStuffWithFooPrime(foo); Well, we're not assigning the value of `doStuff` to anything, so we can assume that we had some side effects that we wanted to run? Is that `foo` the same `foo`, or has it changed somehow? Who knows?? We'd have to dive into the source code to find out. `doStuffWithFooPrime` is visually indistinguishable, so we have much less information about program structure from an overview of the text.
Sounds good! &gt; As of now, I have no idea what IO, CLI, etc. are, but I will look it up Sorry for not explanaining these terms. Just a short overview: IO = Input Output, makes the program able to read input from the outside world (clicks, texts, ...). CLI = Command Line Interface, programs that are usable by command line.
You're going to have a hard time finding an imperative language that comes even close to having a type system with the guarantees of ADTs, even more so if you're mutating state. That said, C# is actually a fairly decent hybrid these days and has pretty good support for functional programming on top of the OO architecture. You won't get things like exhaustive pattern matching, but it's pretty easy to work with higher order functions and things like monads and fmap are pretty easy to imitate with generics/linq. If you keep your functions fairly pure (there's even an attribute you can attach to functions to ensure compile-time purity checks) you should have to do minimal debugging.
Kotlin. No `null` (yay!), quite ok typesystem for a JVM lang, not too much syntactical overhead, and an FP lib [ARROW](http://arrow-kt.io/) that the community seems to converge on. As it is built by IntelliJ, it has superb IDE support (when using IntelliJ) OCaml. More on the FP side, but has full OO capabilities. Make sure to look at [ReasonML](https://reasonml.github.io/), a new syntax for OCaml by Facebook (specifically targeting to-JS compilation and integration with existing React apps -- think TypeScript/Flow but without JS legacy so much better). Not too sure about F#, but I think it's pretty much like OCaml in that it has OO capabilities. Some call it OCaml.NET :) 
Ever since I saw the [Nanopass Compiler](https://www.cs.indiana.edu/~dyb/pubs/nano-jfp.pdf) talk, I have been dissatisfied with Haskell's support for "similar ASTs". Do you think Trees That Grow could be used for a nanopass-style framework that was comprehensible by e.g. students?
Regarding point 1, what makes Intellij invaluable for me is the inspection system, especially when you need to work on legacy system with different kinds of technologies (GTW, JSP, JSF, Angular) with poor code quality. The speed you can refactor and fix code safely and make incomprehensive code to understandable is amazing.
Do you know a good Ubuntu distro for Haskell? I am running '17.10.1' and it is not the best at times. I even considered switching to debian testing or Arch.
&gt; In fact, interpersonal team dynamics is one of the most important factors in practice for the success or failure of any project. It would be grossly unprofessional not to address it. I cannot underscore this enough. Software _engineering_ is probably 80% or more about understanding and communicating the problem.
I think they meant more about avoiding name collisions using lenses. {-# Language TemplateHaskell #-} {-# Language MultiParamTypeClasses, FunctionalDependencies, FlexibleContexts #-} import Control.Lens import Data.Text (Text) data User = User { _userName :: Text, _userAge :: Int } makeFields ''User data Bottle = Bottle { _bottleID :: Text, _bottleAge :: Int } makeFields ''Bottle incUserAge :: User -&gt; User incUserAge = age +~ 1 doubleBottleAge :: Bottle -&gt; Bottle doubleBottleAge = age *~ 2 
I am not aware of any documentation regarding building non-C language sources with cabal or stack. The only reason C++ sources _usually_ work is because they gets past to GCC (or clang on MacOS) which flexibly accepts C++ as valid inputs. I believe, though I may be incorrect, that compiling any non-C language source files only works as a kludge through gcc.
Are you asking about use of micropasses in GHC? Trees That Grow is currently only being considered for GHC's Haskell AST. The existing Core, STG, and C-- representations will remain unchanged. Consequently I suspect that TTG won't have much of an effect on the feasibility of the micropass approach within GHC. It will, however, make it much easier for tooling and `haskell-src-exts`-esque libraries to hang their own annotations off of GHC's AST.
I recommend taking a look at [this](https://www.youtube.com/watch?v=rh5J4vacG98) as an example of a nice way of working with complex hierarchical data.
I'm reading through the Servant docs right now, and number of their data type declarations have a format I hadn't seen prior in that they're missing a right hand side. Can someone explain how these are supposed to be read? This one represents an HTTP request body: data ReqBody (contentTypes :: [*]) a Thanks 
Our chief weapon is parametricity...parametricity and laziness...laziness and parametricity.... Our two weapons are parametricity and laziness...and control over effects.... Our *three* weapons are parametricity, laziness, and control over effects...and an almost fanatical devotion to the making impossible states unrepresentable.... Our *four*...no... *Amongst* our weapons.... Amongst our weaponry...are such elements as parametricity, laziness.... I'll come in again.
I believe it was due to classes that we were deriving. No Template Haskell
I wrote a replacement for HDBC-ODBC at one point, but I don't have the IP rights to release it. I could probably crank out another reasonably quickly now that I have a good understanding of how everything works. HDBC-ODBC is completely broken at the design level; not just on Linux.
Do not use Arch for Haskell. I've heard a lot of horror stories due to poor Haskell packaging decisions by the Arch maintainers
RemindMe! 2 days "bloviate about comonadic syntax trees"
I will be messaging you on [**2018-01-23 23:58:06 UTC**](http://www.wolframalpha.com/input/?i=2018-01-23 23:58:06 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/7rwuxb/professional_haskellers_what_are_your_major_pain/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/7rwuxb/professional_haskellers_what_are_your_major_pain/]%0A%0ARemindMe! 2 days ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Data declarations with no RHS create a data type with no constructors, meaning there are no values of that type. They're useful for type-level computations, which Servant uses a lot.
Did you follow the hint in the error? I.e did you install the library, I think you want this apt-get install libsqlcipher-dev
I haven't gotten the AMT version to the point where it's stable enough for thorough benchmarking just yet. Based just on the design (implementation details aside): the change definitely reduces the memory overhead of storing a trie; it ought to speed up lookups (though I can't say by how much without checking); the downside being that insertions/modifications may be slower (though that really depends on implementation details). All of these being the same tradeoffs as between `IntMap` vs `HashMap`. Then again, one of the biggest issues with this library has been finding a suitable dataset for doing benchmarks on. With artificial datasets it's all too easy to bias things towards certain operations over others. So if you have a good dataset/task, I'd be very interested in taking a look at it. The patricia-tree based implementation was the one I was referring to re "a number of optimizations folks typically miss". I was pleased to see that a lot of the usual things I bring up here you're already doing: node fusion, using a smarter tree to handle fanout (most folks seem to do the naive binary tree thing, without any sort of balancing or compression), etc. Since you're using a vector as your "tree", your code is actually a lot closer to my AMT version. The biggest trick about AMTs is the way they represent *sparse* arrays; which comprises the bulk of the under-the-hood code churn in the unreleased version 0.3, since the sparse array code in unordered-containers isn't exported nor designed to be usable by anyone outside of that library. The biggest reason why I brought the library up was that I offer a whole bunch of functions for efficiently and generically querying and manipulating tries, whereas your library only really provides the parsing/searching bits. Thus, it seems like it'd be a good idea to build the parsing library on top of bytestring-trie, rather than reinventing things. One crucial difference, of course, is that I'm using `ByteString` rather than `Text`. I've been meaning to add support for `Text` for a long time (since even before the AMT stuff), but alas unicode is really messy and it's not entirely clear to me that there's a coherent way to do it (i.e., as a built-in, rather than as something layered on top of underlying `ByteString`s). So yeah, if you're interested feel free to shoot me an email
I have solutions for some of these: 1 - I do imports automatically, so I (almost) haven't thought about them for years. Just hit ,a when needed. I don't know why everyone doesn't do that. There are lots of programs on hackage for it. 2 - I never used cabal, I went from make to shake. It must not have that problem, because I don't often restart ghci. 4 - I have a Pretty class for this that does nice formatting. I've thought of extending it to emit html with collapse/expand but actually just indent or bracket based folding would work well enough. But it all requires something fancier than xterm. 5 - I use monomorphic fclabels, and write them by hand, which I think avoids all those problems. I don't use lenses in any sophisticated way though. 7 - I never did any type level stuff either but somehow never need error either. It's probably to do with the domain. 8 - I have a simple filter that converts to and from \-continued strings, and vi bindings, so I can go to plain text and back pretty easily. I also kind of wish for Python-ish triple quotes, but not if I have to wrap it in some dedent function.
What's with the ip rights? Isn't hdbc-odbc open source and therefore forkable? If you'd release what you have, it would be immense help to all haskellers who work with ms sql server. 
The biggest pain points I have are in the build tooling. Back when I was working as a solo developer, things like Cabal (or Stack if you swing that way) work just fine. But all of this standard tooling makes some huge assumptions, like: you can compile everything on one machine and don't have to worry about things like NFS or networking delay to the cluster; compiles are all-at-once and thus things like TH don't have to worry about caching or versioning; you're only working in Haskell and don't have to worry about integration with other build systems; etc. Now that I'm working at a certain major corporation, every one of these assumptions is false and working around their falsity is a major pain point. I spend so much more time these days just getting things to build. The build delay never used to interrupt my work cycle before, but now I'm constantly going on coffee breaks while I wait for things to finish. If the project were significantly bigger or hairier than things I've done in the past, that'd be one thing; but no, the code is basically the same as what I've been doing for years.
It's fine as long as you stay away from the Arch-provided Haskell libs. Just get `cabal` and `stack` and let them do everything for you.
Work in Haskell. Not an expert. Most of my pain points are specific to the codebase I inherited. I feel like Haskell enabled past devs to write lots of homegrown libraries which weren't justified when comparing the small gain in features versus the larger maintenance burden. So, great power without anything enforcing that one should use the power responsibly. I dread encountering any space leaks or having to learn about profiling/reasoning about the lazy evaluation in detail if/when it becomes an issue. I wouldn't mind it if I was a library author and had time to focus on just working on a single library's performance. As others have mentioned, uncertainty about library quality/maintenance. I usually try to gather whatever web comparisons there are combined with skimming the test suites of potential libraries. I sometimes fork libraries preemptively, assuming I will hit a point where I might need to submit a small PR after use, simply because I might be one of the first people using the library. I pick libraries based on highest likelihood to be maintained and tested, not based on my ideal features or interface, for fear of using a library that isn't production quality. Liking nix, but still feels like a large investment of time to reach proficiency. I liked stack in past usage as well. I am not looking forward to having to figure out how to design more complex builds and deploys with integration test suites, multiple libraries and executables, with nix (nixpkgs on Ubuntu/centos), even if it will probably turn out elegant when complete. I want to be able to freely add some python and java services on the side of my main service, but I hesitate to add anything because I have to first decide how I want to mix Java and Python that isn't packaged with nix with everything else. 
All of these problems sound like things Nix solves.
First of all your example is ambiguous, it could also mean *put the first element last*. Try writing it without list comprehension first and break down all parts of the algorithm to smaller pieces. You don't seem to be familiar with list comprehensions. [Here](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-420003.11) are the desugaring rules. Have a look at these: * `[ f x | x &lt;- xs ]` is equivalent to `map f xs` * `[ x | x &lt;- xs, p xs ]` is equivalent to `filter p xs` * `[ (x, y) | x &lt;- xs, y &lt;- ys ]` is basically a cartesian product (see use of `concatMap` in desugaring rules) * `[ z | x &lt;- xs, y &lt;- ys, let z = x + y, odd z ]`(allows you to re-use expressions or name them properly) Also, there is no need to *pattern match* on a list if you put it back together.
I believe nixpkgs has some good packaging for Python and Java. I might be wrong on that...
In addition, I think inheritance actually hurts *a lot* in many of the same ways. Inheritance is sort of like making your *execution* mutable. Granted, this is usually only a problem when used irresponsibly. If you use `final` classes by default, only allowing inheritance to do what we'd do with type classes, it helps a lot.
Haskell appeals to me semantically but offends me aesthetically. Most of the aesthetic problems come down to the terrible module and namespacing situation Haskell has leading to libraries naming stuff in terrible ways. Restating types in names. Adding random letters or symbols to names to avoid clashes with existing names. Massive import blocks with loads of redundant crap. Also, syntactically, the backslash for lambdas looks so ugly that I avoid lambdas like the plague. It's often very unclear how to lay out Haskell code in an aesthetically pleasing way. Although I like the mathematics-inspired syntax, it requires careful thought and consideration on a case-by-case basis for good layout. No one-size-fits-all formatter will work for Haskell.
For my project this works in the cabal file: ghc-options: -optc-std=c++14 cc-options: -std=c++14 
I scrolled this whole thread hoping to find this post, thank you.
I'm asking about the feasibility of micropasses in a compiler implemented in Haskell, e.g. for a toy language, using TTG.
Addendum: Do not under any circumstances choose Scala.
That type parameter post is cool but it seems like you would run out of rope quickly if you had big nested records with lots of fields. The example in the post is nice because it only has two type variables and so can conveniently become a bifunctor but I've not yet seen a package implementing eg triskaidecafunctors.
[product-profunctor](https://hackage.haskell.org/package/product-profunctors) 
I’m not super knowledgeable but this talk was great https://youtu.be/DebDaiYev2M
I stand corrected.
Compact regions cannot be used to serialize a lambda.
Regarding your GUI toolkit section, one missing option is [wxHaskell](https://hackage.haskell.org/package/wx) which I've had a good experience using on a small project via [reactive-banan-wx](https://hackage.haskell.org/package/reactive-banana-wx). I'm not sure how it would stand up to your scrutiny in terms of being a good toolkit but it seems to work fine and look relatively good/native on OS X, Windows and Linux.
Thanks for the post. Definitely going to use more of this extension in the future.
A minor nitpick about the Java example; I don't see how `foo` would become a reference to a different Foo instance between those `doStuff` method calls.
I like to use LambaCase in conjunction with `&lt;&amp;&gt;` (flipped `&lt;$&gt;`): ```haskell do cmd &lt;- getLine &lt;&amp;&gt; \case "run" -&gt; ... "delete" -&gt; ... "etc" -&gt; ... ```
Microsoft has released its Type 4 JDBC driver for SQL server under MIT license here: https://github.com/Microsoft/mssql-jdbc. It should be possible to write a pure haskell driver for it if someone is inclined to do so.
Not sure if it will help you, but you can see some image and audio manipulation in the databrary/store module of databrary on github. Involves invoking ffmpeg through either process or library interface. I haven't gotten deep into it myself yet.
Hsqml has some interesting build setup. You might be able to get it working if you get the libraries it mentions under pkg-config depends: https://github.com/komadori/HsQML/blob/master/hsqml.cabal
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [komadori/HsQML/.../**hsqml.cabal** (master → ba72ee2)](https://github.com/komadori/HsQML/blob/ba72ee2c540485163bb6f34ade5a70ac62a971f7/hsqml.cabal) ---- 
A simple hoogle for a function with type signature `Char -&gt; Int` show me the `ord` function from the `Data.Char` package. Similarly there is also a `chr` function. Now writing our shift function. It takes in a `String` and an `Int` and, returns a String: `shift :: String -&gt; Int -&gt; String` Now the function. `shift text n = [ 97 + ( mod ( ( ord x ) + n - 97 ) 26 ) | x &lt;- text ]` This looks like it will be a pain if we need to convert uppercase letters as well. So we will simple generalize the expression that shifts char. We can now write shift as: `shift text n = [shiftChar x n | x &lt;- text]` Now onto shiftChar. `shiftChar :: Char -&gt; Int -&gt; Char` shiftChar c n = chr ( base + ( mod ( c_ord + n - base) 26 ) ) where c_ord = ord c base = if c_ord &gt;= 97 then 97 else 65 Thats it. If you have to deal with non alphabetical input as well just add a guard . 
This looks like homework.
I think the real trouble with inheritance is just the plain old problem of mutability. If you only had immutable objects then "inheritance" wouldn't even be able to mean anything than interface hierarchies, like type class hierarchy. (In a dynamic language, everything is consigned to the place of "convention" anyway so it's only useful to discuss what are considered "best practices" which almost always line up perfectly with what people do in static langs anyway.)
It's a French word, for a type of servant you find it in hoity toity gourmet restaurants (where everything has a french name).
I agree. If the metaphors worked better I wouldn't mind. Actually I like humor. But they don't work here IMO.
What does your build process email? Do you regularly cross compile?
C# still allows nulls, though they're making changes in that regard. F# is probably the better choice.
Reference equality is not the same as value equality. I believe by "same foo", the author was referring to the latter.
Thanks. I'll take a look.
&gt; Kotlin :'( From a language design perspective, I greatly prefer Ceylon, so I'm sad that it seems to be withering while Kotlin is flourishing.
&gt;I'm sad [Here's a picture/gif of a cat,](http://78.media.tumblr.com/tumblr_luvhsgmZAg1qgnva2o1_500.jpg) hopefully it'll cheer you up :). ___ I am a bot. use !unsubscribetosadcat for me to ignore you.
Not trying to start a debate; genuinely curious for my own future benefit. Do you have some reasons or anecdotes about Scala?
Simply having parametric polymorphism and algebraic data types goes a *long* way, which is why I think Swift and Rust are such good languages. They don't give you the elegance of Haskell, but they do give you much of the safety, and in a way that normal programmers can grasp and use.
Send an email to the Haskell.org committee: committee@haskell.org 
I'm not sure but I think the best option is to open an issue on https://github.com/haskell-infra/hl (well I guess they would love to open a PR with the fix but a issue will probably do too)
Do you think C# is better than scala. I've been learning some scala lately, but it gets a lot of criticism.
You know, I've always wanted to ask one thing. You've written a few times that choosing more polymorphic type signatures allows for "the free theorems to get stronger" or something along those lines. I wonder, what is the exact meaning of this? I know about the Wadler paper about this topic. But do you actually calculate free theorems on a piece of paper, or using some program? Are you a squiggolist? Do you derive implementations from a function's specification? Do you prove your functions correct after the fact? Or should I understand this kind of statement in the more vague sense of the number of possible implementations being smaller?
Why so many indirection instead of going directly with bind?
It's somewhat of a myth that functional programs are bugfree. I personally think the big advantage is that they are _crashfree_. It's just so much better than you don't have to run the program in order to find a null pointer or even a typo (yes you python). That said, you still can have logic errors, but FP makes it easier to test for them (because of purity). Refactoring is also much easier, and I find that when it works before refactoring, it usually works after. The more expressive the type system, the more invariants you can encode in it, and the more robust the program becomes. It also saves you from the mental effort of ensuring these invariants are met, since the compiler does them for you. 
I used AngularJS and Angular quite a lot (each one for over a year on daily basis) and for **SPA** applications they are fine. If you are developing small FE where you don't need many components, lazy loading or single page approach then you are right, because those tools are not suitable for your task. I don't think they are over-engineered, just a complete framework, not a tiny library. They have a list of good practices and they enforce a lot of them via interfaces of modules/components. If you want wild west, you can go with vanilla JavaScript, but if you are inexperienced (or lazy) you will end up with tangled unreadable and unmaintainable mess - this is the reason jQuery is being abandoned on FE (don't get me wrong, I like jQuery, but only for trivial few-line user-scripts, not for front-ends). I don't have any experience with React, but from what I read, it's hardly over-engineered. It has one task (view) and it does it very well with simple and really small API (incomparable in size to Angular or any complete framework).
The more important something is, the more important it is to get right. I think records are *extremely* important, so it needs an extremely good implementation. Otherwise, we're dooming *the entire future of Haskell* to a subpar critical feature.
Rust?
Have you tried purescript ? It has some improvements over haskell.
`&lt;&amp;&gt;` is nice but I seed that unfortunately both `Control.Lens` and `Protolude` both define their own versions (`&lt;&amp;&gt;` is not in `base`) so if you happen to import both of them and also want to use `&lt;&amp;&gt;`, you get ugly imports.
This extension is at the top of my "ought to be stock" list. I even [patched hlint](https://github.com/ndmitchell/hlint/pull/423) to propose case-under-monadic-value (but the maintainer disagrees, so if you want the lint you might need to build my branch).
Why is `words` non-total?
Well, you'll still have to import about 30 modules and turn on your favorite brand of kitchen sink...
Which is why it bugs me that we have the record syntax we do. Something so important to get right was pasted over with a ridiculously hacky feeling solution and then any potential improvement seemingly never touched on since.
Shameless plug to my blog incoming. For those interested in a little history lesson, see: [The Long and Epic Journey of LambdaCase](https://unknownparallel.wordpress.com/2012/07/09/the-long-and-epic-journey-of-lambdacase-2/).
I had a very brief look but I thought it was restricted to web development? I'll have another look (I'm hoping for lazy evaluation, monoids and type classes).
Good to know!
Servers drop requests on the floor all the time due to limited processing resources. So do queues.
I am worried we might see some survivorship bias here though. The points listed in the top-voted comments are definitely not small things, and they are enough to make a lot of people just quit in frustration. So I think we might be losing (and already have lost) a lot of potentially valuable community members, members that you need to write docs and tutorials, design new APIs, fix bugs, curate useful packages etc etc. You can gauge the magnitude of the frustration by looking at how many comments threads like these attract.
But do you think there is a chance that Haskell will get proper Records with a first-class syntax at this point? Because if not, then in your estimate, dooming the entire future of Haskell (I don't think it will be quite as bad, but close) is happening right now.
&gt; Are there any elements that Haskell (or FP) has, that some OO languages (clearly not Java) have, such that I benefit of this reduced debugging in these languages as well? Semiyes: SmallTalk is a OO lang as it should have been done. It's syntax and mental overhead is only slightly higher than that of a LISP. Additionally, development in it is image based. In Java, debugging is: Bug appears -&gt; read bugtrace -&gt; make change -&gt; recompile. In SmallTalk, that becomes: Bug appears -&gt; you can step through every step in the bugtrace and the image will change accordingly. No problem in stepping three steps back, altering a value and executing from there. With that, change the message definition at fault, go one or two steps back and simply let the execution continue. I am bad at explaining this (You will find plenty online about SmallTalk and image based development), but it feels much more direct and fast. This reminds me a bit of developing in the REPL in Haskell which has a much faster feedback cycle as well. tl;dr: Besides typesafety, Haskell often offers a faster feedback cycle. SmallTalk offers something similar.
M4 was also originally written by K&amp;R, so they must have recognised the limitations of CPP themselves.
As someone who loves OO (though the SmallTalk variaty, not the bastartized Java approach. And I love Haskell more!), I feel that analogy is spot on. "Avoid writing own typeclasses as much as reasonable" is similar to "Composition over inheritance" aswell.
There's nothing arcane in monads )= Now I want to warn people. Do not open your monads! You peek inside and the magic will be gone!
 {-# LANGUAGE OverloadedStrings #-} import Data.Text (Text) txt :: Text txt = "Some weird\ \kind of sarcasm\n\ \I didn't get ?"
No, I just wanted to point out that multiline string syntax does indeed exist. I don't use it much myself either, for what it's worth.
Maybe if the prelude was less tolerant to partials it would have better safe values handling.
I think LambdaCase is one of my favorite extensions that I enable in basically every file. I would put it in Haskell 2020 easily.
For getting access to patched/modified packages, the same recommendations as in [the GHC 8.4.1-alpha1 thread](https://www.reddit.com/r/haskell/comments/7l4b19/announce_ghc_841alpha1_available/drjlc3w/) apply. If you installed GHC 8.4.1-alpha1 from my PPA as described above via sudo apt-add-repository ppa:hvr/ghc sudo apt update sudo apt install ghc-8.4.1-prof ghc-8.4.1-htmldocs cabal-install-head Your GHC 8.4.1-alpha1 installation will be automatically updated to GHC 8.4.1-alpha2 the next time you sudo apt update sudo apt upgrade 
There's `importify` tool which I'm working on: * https://github.com/serokell/importify Maybe one day it can remove unused imports and add needed imports on-the-fly... We need just the same (or alsmost same) thing for extensions :) You can call it `extensify` :D
&gt; Still no working library to connect to commercial databases (MS Sql Server, Oracle, Db2). HDBC-ODBC is completely broken on linux as of today. &gt; I'm using my own patched fork and even with that I have to accept occasional error here and there. But for mission critical processes where errors are unacceptable, I had to write sqlrelay through dotnet service on windows to interact with MS Sql server. Like /u/anon_c, I also wrote a trivial binding to ODBC tested against Microsoft's own binary release on OS X. I tried to use HDBC-ODBC and I had to get a patched version so that it could deal with unicode, and then I wrote a trivial test suite with QuickCheck and after 10 iterations it segfaulted. I looked under the hood at the code and decided to can it and write my own binding to be invulnerable to segfaults or memory leaks, with tests for stability and space usage. I'll see whether it can be open sourced. 
I mostly care about memory and lookup speed, so that sounds promising :-) https://gitlab.com/unhammer/DTBench has a word list and some super simple benchmarks on the stuff I care about (intersectBy, match and memory usage). I guess should compare with radixtree when I have time again :) 
Amazing, thanks! I'll have another look at this later and try and understand the "why" a bit better.
Nobody expects the Edward inquisition.
You are correct. I did some trying out and indeed it seems that `gcc` does some auto-detection of the filetype. However, at least for me you also have to provide some additional linker flags. That is - to stick with the fortran example - you can compile a `main.f90` via either of gfortran -Wall main.f90 gcc -x f95 -Wall main.f90 -lgfortran # Linker errors otherwise Now if you wanted to use another compiler but not another build system, do you think you can cook something up via `Setup.hs` ?
Rick is right. Much of what is blamed about haskell is production, record issues, complicated monadic stacks is a consequence of not recognising that data is an accumulation of independent elements added at different times and not something monolithic that must be defined at design (or compile) time. The problem has a parallel in the lack of flexibility of databases with rigid schemes. Please don't harm yourselves and the Haskell reputation by perpetuating bad practices.
Perhaps this can replace "haskellPackages" in nixpkgs? An overlay seems like a better place than the main repo
It's a good idea, a in memory query system for state data. The big picture: Rick is right. Much of what is blamed about haskell is production, record issues, complicated monadic stacks is a consequence of not recognising that data is an accumulation of independent elements added at different times and not something monolithic that must be defined at design (or compile) time. The problem has a parallel in the lack of flexibility of databases with rigid schemes. Please don't harm yourselves and the Haskell reputation by perpetuating bad practices.
You probably want `unwords` don't you? At least to get the same result as `mbw_rdt` was looking for.
This is really interesting, thanks for sharing!
Agreed. Though I kinda wish we had multiple-arity versions too ;-(
Personally I think there is little value to this extension. There are so many syntactic options already - too many, really. (Just ask any lisper...) I find Haskell's existing syntax expressive and flexible, more than sufficient for any need. On the other hand, it's such a small change that I don't object to people using it who want to. Including other people on our team.
Agreed :)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/haskell-opaleye/.../**PrimQuery.hs#L42** (master → ddcee2f)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/ddcee2f2ba0d20ab7f8e1d2bf09e940d23f04942/src/Opaleye/Internal/PrimQuery.hs#L42) ---- 
I'm not sure you are corrected. That package does not implement triskaidecafunctors. In order to deal with proliferating type parameters you really need to convert to a single higher-kinded parameter. Haskell doesn't make it *that* easy to deal with those things, though.
Really the low-tech solution is not that bad txt :: Text txt = "We don't need multi-line strings " ++ "in Haskell because we " ++ "have the freedom to break " ++ "lines on operators." 
&gt; half the libraries are unmaintained, have newer and better forks or even just unfinished Stackage would do us a great service if it made sure that unmaintained or unfinished libraries are not available on it. (And perhaps it already does this, I don't know because I don't use it.)
Purescript is strict.
Nixpkgs has to have a Haskell package by default (without an overlay) set so it can supply Haskell programs to regular users. But I agree that the Haskell package set should be maintained separately, with nixpkgs simply importing it into place.
I feel the same but the opposite. I find `LambdaCase` *so* useful that I don't think there's any need for the bog-standard case.
FYI, between writing this blog post and this comment objecting to the change, I already merged the changes into the 1.6 branch based on general support for the change. I'm hoping my explanations below will set aside some of your concerns. &gt; Interesting. Is there an official benchmark suite for Yesod? No, but there should be. &gt; Just that there would be more of a learning curve for Yesod users. They'd need to remember what HandlerFor does in addition to HandlerT (Is there an additional type alias for subsite? I didn't see this in the diff, but I image there would be something like type SubHandler sub master a = ReaderT (SubsiteData sub master) (HandlerFor master) a). That's not the case. Users _who write subsites_ will need to be aware of something else, in theory. In practice though, even those users can just program to the `MonadSubHandler` typeclass and avoid the concrete types, much the way all of the functions today in yesod-core use `MonadHandler`. &gt; If these functions need to interact with an IORef, maybe a MonadIO instance requirement is sufficient? If your goal is to write `funnyLift :: MonadIO m =&gt; HandlerT app IO a -&gt; HandlerT app m a`, it can definitely be done. I _think_ it goes something like: funnyLift (HandlerT f) = HandlerT $ liftIO . f In the post-`HandlerFor` world, you'd still be able to implement a transformer stack and create a `MonadHandler` instance for it, but I don't know what the motivation is for having the generalized base monad. If you point out the code implementing your subsite, I'd be happy to take a look and see what changing it to the `HandlerFor` approach would look like.
If you replace `Int` with a deep embedding `Exp Int`, then you can use [operational-alacarte](http://hackage.haskell.org/package/operational-alacarte) to generate code including control structures. Probably not what the OP was after though...
I wouldn't mess with `Setup.hs`. That is an old mechanism from the very early days of cabal. Nowadays, just stick with the standard `Setup.hs`. A better approach is to provide a thin C wrapper for your API, and provide DLLs. Cabal and stack are tools for compiling Haskell, not replacements for `make`. If you want a very nice general build tool that happens to be Haskell-based, take a look at [shake](http://shakebuild.com/).
I'm not crazy about "bog-standard case" either. I avoid it too whenever possible. (For a while I was avoiding it religiously, but now it does slip in every once in a while.) Function definitions with pattern matching, pattern bindings, guards (*not* pattern guards), and Maybe combinators, all taken together, produce the neatest, cleanest, and most maintainable code, in my opinion. I'm very happy with that style.
A lot more work has been done on the `haskellPackages` set by peti so I imagine it is quite a bit better by default than this. 
Not highest - almost highest. Postfix record field setters, enclosed in curly brackets, have higher precedence.
It tells you the computation in the lambda is pure. It avoids repeatedly applying `pure` in each case.
You define a macro, execute it, and then remove it in your ~/.ghci file as follows: :def _setenc (\_-&gt;System.IO.hSetEncoding System.IO.stdout System.IO.utf8&gt;&gt;return"") :_setenc :undef _setenc This macro ignores arguments and returns an empty string so that nothing else is executed, so that it only performs the encoding configuration. [Reference](https://downloads.haskell.org/~ghc/master/users-guide/ghci.html#ghci-cmd-:def) I wonder why GHCi is defaulting to ASCII. Have you looked at your locale environment variables from GHCi? λ:1&gt; import System.Environment (getEnvironment) λ:2&gt; import Data.List (isPrefixOf) λ:3&gt; filter (\(k,_)-&gt;"LANG" `isPrefixOf` k || "LC_" `isPrefixOf` k) &lt;$&gt; getEnvironment Does the output of this under Stack match the output outside of Stack?
&gt; Function definitions with pattern matching, pattern bindings, guards (not pattern guards), and Maybe combinators, all taken together, produce the neatest, cleanest, and most maintainable code, in my opinion. I'm very happy with that style. Interesting. I would naively have said I would prefer a multi-argument `\case` to replace function definition syntax. It sounds like the style you use would be worth describing in a blog post!
Cool! What is the `|` symbol?
As a point of information (and I'm not claiming it's better) you can do do cmd &lt;- getLine &gt;&gt;= pure . \case "run" -&gt; ... "delete" -&gt; ... "etc" -&gt; ... 
Agreed. Also some kind of usage and Dev stats would be nice.
I think the point of the "free theorems" notion is that the mere fact that you can write them using parametric polymorphism means that they are correct, because that places such limits on what you can do with the types that they can't be wrong. It's not about calculating them on paper, it's that they are valid in the type system.
&gt; The problems I encountered came from once I wanted to build my own website which with every feature I wanted to add almost always required the use of functions which were not explained in the book. 😂
?
What's up with those disgusting / onomatopeic names for packagers / libraries / programs / etc. in software engineering? we have SLURP now, i think this privacy-related term was used in computer security as well and it means "stealing large amounts of data", we also have SNORT I guess they're funny or clever if you're 14yo, but using it as a name? not even 14yo jokes are used for that long, it gets old fast even for them
I can't think of any fundamental advantages. There is minor difference in representation, though: `ExprE ()` has one more field than `Fix ExprF`. But for `SizedExpr` it's the other way round, due to pointer chasing. The approaches differ mainly from a semantic point of view: Formulating things as a fixed point of some functor gives you the guarantee that every recursive occurence is of a certain form. The type parameter approach doesn't do that. E.g. the `Var` constructor doesn't have an occurence of `e`, because we are only really interested in sizes of inner nodes rather than leafs. Or you might forget an `e` field in some tag. Granted, these are all pretty easy to spot, but I think explicit fixed points is the more principled approach. Also you get recursion schemes for free, which gives you even more guarantees, like instantly having a feeling for what a catamorphism like `sized` can or can't do: sized' :: Expr -&gt; SizedExpr sized' = cata $ \case LamF v e -&gt; LamS (size e + 1) v e AppF f a -&gt; AppS (size f + size a + 1) f a VarF v -&gt; VarS v But yeah, it's mostly hand-waving and using a type parameter and type families is probably the more flexible thing to do, at the cost of being slightly more ad-hoc/unprincipled.
&gt; Records. I don't need fully-fledged row-polymorphic records, just namespacing record fields would solve 90% of my problems. DuplicateRecordFields ?
Interesting data point, thanks! I found 23 instances of "&lt;&amp;&gt; \case" on Hackage, e.g. this one from ascii-art-to-encode: mode &lt;- getArgs &lt;&amp;&gt; \case [] -&gt; Pipe ["-i", file] -&gt; Inplace file [file] -&gt; Read file _ -&gt; error "Usage: \n.. 
For example &gt; data Foo = Foo { foo :: () } deriving Show &gt; show Foo { foo = () } "Foo {foo = ()}" 
&gt; When converting Aeson objects to lists, the order is essentially random, although I suppose it is probably always the same for a given compilation on a specific platform. (I've had a CI test fail only on some computers because of this...) I would expect this to be on purpose. I *think* the hash function is seeded randomly to prevent DoS.
I missed this interesting SPJ quote on reading the Trac pages, so I'm glad you pointed it out in your LambdaCase history: &gt; … every part of GHC from the parser onwards already implements &gt; lambda-case! … All that lambda-case does is provide concrete &gt; syntax to let you get at the abstract syntax that is already there. … &gt; So I think lambda-case is a fine candidate to fill the syntactic gap &gt; for now, leaving something more ambitious for the future. The `\case` was inside us all along :)
Ha, let's see how 'our friendly colleagues' handle this one. I can already see the molten lava pouring out. Anyone taking bets for the thread to be locked? I already see a delete comment.
I suppose if you hang out in the GHC source for long enough, you forget what good naming even looks like.
&gt; For example, we do not impose that getting a particular package tarball for (say) pkg-3.5.3 always yields the same result each time. (In Hackage, for example, the presence of revisions may mean it will not.) Immutability, or lack thereof, is an example of hosting-service specific policy that does not concern SLURP. Scary. I hope this has been thought through properly. I have no qualms with a hosting service shipping extra data out-of-band, e.g. revisions are shipped in the index, not the package tarball, as far as I can tell. But having `pkg-3.5.3.tar.gz` immutable is, I believe, a very good thing.
You can also choose to hide the constructors of a datatype, by only exporting the type (`Foo`) instead of the type with its constructors (`Foo(..)`). If you do this, the the generated Haddock documentation will only show the type, without constructors.
Speaking for myself, I don't literally sit down and write proofs; I just know that if I have an `a`, then there certainly isn't much I can do with it, and if I have a `Num a`, then what I can do with it is constrained much more loosely. For instance, one of the important barriers is whether the function in question does or does not have the ability to bring in a new value from the ether. It can be educational to take a moment and try to write a function with the type signature `a -&gt; a` that is not bottom, and does not return the original thing you passed in. (Because while one of the primary virtues of education is to permit you to go down a well-traveled path and not re-invent the world, I think the virtues of crashing into walls are often missed out on. Give it a try; this will literally only take seconds.) `Num a`, by contrast, admits of functions playing hanky-panky because they can spontaneously materialize values via `fromInteger`. `Num a =&gt; a -&gt; a` can return a constant 4 via `fromInteger`. This example is somewhat trivial, but the principle holds more generally. A "highly polymorphic type signature" == "a function that is more restricted in what it can do with the value", resulting in a gradation from "a fully concrete type like `Int -&gt; String`", for which the only _human_-sensible implementation is to convert that Int into a string, but where technically the output string is entirely unconstrained (at least from the perspective of a single call), up to the fully-polymorphic `a -&gt; a`, that has literally no powers to do anything to the `a`.
well, it was in the Jargon file: http://www.catb.org/jargon/html/D/drop-on-the-floor.html
Yet every other language got them right just fine. All of UNIX is based on monomorphic b records.
Elm has a good solution.
You can use `universum` (which was based on `Protolude`) instead of `Protolude` and it reexports `&lt;&amp;&gt;` from `microlens` ;) * https://github.com/serokell/universum
This is the context I’m most familiar with, e.g. “left on the cutting room floor” for parts of a movie that don’t make it to the final cut.
Sounds dirty. I like it. 
Yeah, that sounds like an immediate dealbreaker :( Still need to read the proposal
Lest anyone get confused about what the point of SLURP is &gt; SLURP focuses on one problem and one problem only: coordinating package name ownership and making sure the owner of package name is advertising available versions in a standard way. -- https://github.com/haskell/ecosystem-proposals/pull/4#issuecomment-359460942 and the reason given for implementing SLURP is &gt; If we do nothing it seems likely that the Stack community will create a separate Haskell package repository. No one wants this, because it imposes heavy costs on Haskell users. -- https://github.com/simonmar/ecosystem-proposals/blob/slurp/proposals/0000-slurp.rst#8-what-will-happen-if-we-do-nothing I have to say I don't find this terribly compelling.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [simonmar/ecosystem-proposals/.../**0000-slurp.rst#8-what-will-happen-if-we-do-nothing** (slurp → 70e9f59)](https://github.com/simonmar/ecosystem-proposals/blob/70e9f59e44e98add4c4d4b51afd12c84b35b784d/proposals/0000-slurp.rst#8-what-will-happen-if-we-do-nothing) ---- 
&gt; there's not much we can do about forcing these to be immutable or not It could be part of the policy that the package is immutable. Under this policy changing the package would be grounds for revoking one's right to that name. This condition would be even *easier* to police than the name squatting condition because it is entirely automatable.
I like this, as it makes it possible to break up hackage into microservices.
I dislike this, for the same reason.
Yes, let's not confuse and conflate policy vs enforcement. The policy could be that versions should be always fully immutable, without any enforcement or policing to begin with, and we can build automated tools to enforce this policy later on.
We could, yes. That's certainly a part of the proposal space we could easily move to without changing much of the rest. It just comes down to how stringent we want to make the initial policy, or not. Good to have this discussion.
I'm skeptical about smalltalk. I've heard from plenty of older people laughing amongst themselves how "everyone completely bricks and ruins a production smalltalk environment at least once"; seems a little sketchy to me, with it being so tightly intertwined and highly tangled together in terms of mutating state.
Agreed, I don't see how this is maintainable without *some* guarantee that package sources are stable.
Maybe I'll give it another try. I had the most luck with the VS Code extension, but even with RLS it seemed like completion and documentation tags were ridiculously slow. I have plenty of issues with ide-haskell for Atom, but even it seems snappy compared to what I was getting for Rust. 
It's unfortunate because json is no longer non-deterministic in output order for basically every JavaScript implementation, so there's a lot of js code out there that relies on ordering.
I could use some background – what problem is this trying to solve? What is "stack vs. hackage" digression the proposal is trying to make a reference to?
You can, with `curry`ing! Here is how: https://github.com/ghc-proposals/ghc-proposals/pull/18#issuecomment-266634157
[The changelog](https://downloads.haskell.org/~ghc/8.4.1-alpha2/docs/html/users_guide/8.4.1-notes.html) implies more changes under the hood, so there could be some unexpected changes somewhere else too.
The namespace of packages is becoming cluttered, and certain wealthy organizations may be thinking of starting their own unique and mutually exclusive Haskell package repositories where they can create a package called "netwire" that has nothing to do with the [netwire]( https://hackage.haskell.org/package/netwire ) we all know and love which actually has something to do with Internet and wire protocols. SLURP wants to be the DNS of Haskell packages: you query a package name and version number, and you get back a `.cabal` file and a URL from where you can download it. This would necessitate a better, more universal package naming scheme with namespaces, e.g. "org.haskellstack/parsec" versus "experimental.parsers/parsec" which would be different packages.
**I like this idea.** So, let me see if I can summarize the proposal in my own words: SLURP will basically be like DNS of Haskell packages (but an append-only registry) and the maintainers of SLURP will be analogous to ICANN. You query SLURP and get a URL pointing to the code you need, while Cabal continues to serve as the meta data protocol for constructing build plans, and you can use whatever build-planning app you like most (Cabal-install, Stack, etc.) to execute the built plans. While I do like this idea of SLURP, I would prefer you flesh-out the details of the **namespaces,** which I think **will be absolutely essential.** I think each namespaces should be maintained by a group of people with common interest in maintaining the quality of that namespace. Each namespace should function as a sort of publishing house, the maintainers of which can dictate their own rules for quality control. Namespaces, like `org.haskell/package`, `org.haskellstack/package`, should be considered official and may not be changed without consensus from the community at large. Whereas (for example) a university or government organization, e.g. `uk.ac.gla/package` or `ch.cern/package`, may create their own namespace with their own separate peer-review process for submitting packages to this namespace. A community of game developers may restrict anyone but their own members from publishing a package to that namespace. The packages within the "experimental" namespace should function as Hackage does right now, but maybe could be ordered into a second level of sub-namespaces, like `experimental.network/package` and `experimental/parsers/package` to alleviate further naming conflict. Perhaps also there could be a limit of two levels of nesting of sub namespaces to prevent abuse of this function. This is good because it will organize Haskell software into truly a universal naming scheme that will function more like a Linux package repository (without hosting the actual packages).
Hm, I don't see anything significantly performance related in this changelog. I guess the first phrase is generic.
Why don't we just add it to `Data.Functor` and have it over with?
In terms of mutability, few languages are not horrible, I agree ;) what makes ST nicer than, say, Java: * you want to reproduce a bug situation? Simple, send your collegue the image * nothing hinders you from creating a fresh image and determinably populate it with Objects * you can inspect so much more so much faster than in Java * "living objects all around" sounds worse than it is. No sane developer would alter fields of the "System" object when merely interfacing an SQL database. Granted, in Haskell the compiler gives you guarantes instead of relying on discipline. But I can tell you that I does not turn out more messy that java code. Tl;dr: Obviously immutability is more mantainable. If forced to use mutabiliy tho, I like STs approach much more than what Java does. For the loss of guarantees I get rewarded at least with fast debugging cycles, while the situation worsens heavily in Java compared to Haskell.
Is there anyplace I could read about experiences of previous years? It seems like a really cool thing, and I am considering going, but It's quite a trip and I would like to know more. Has anyone here been there, and maybe could share a bit about it?
Kotlin has "ADT"s (Sealed classes) with exhaustive pattern matching
TBH, I'd take it in the vague sense. I rarely bother to sit down and formally calculate the free theorem, few people do. It is more that after a while working with them you learn what operations can slop where over those sorts of more parametric signatures without changing meaning.
So the proposal claims it can get away without new user accounts because &gt; The API does not permit changing any existing pkgname-&gt;URL mapping in SLURP. Only adding new ones. Does that mean: * If I register a package but mistype the URL the name is now lost forever? * I can't migrate an existing package e.g. from github to my own server? If the whole point is to detach package names from a single hosting solution then not having a systematic way to migrate packages seems like an oversight
What you propose is not future compatible because matchable functions in dependent Haskell will allow us to match against constructors that aren't full applied just like we can do n type families currently.
Hey, this is a first edition of the summer school, our track record is https://www.meetup.com/Monadic-Warsaw/. We'll publish more talk titles in a week and we can guarantee the event be awesome :) If you have more specific questions we'd love to answer them.
Can anyone share a good resource or some tips on getting a Haskell development environment set up? I'm currently using vim with no plugins so I feel like I'm missing out on useful tools. I'm also ok with using emacs via evil mode if you feel emacs is the better alternative for Haskell.
I like it but I would never include a language extension to use it. Makes it even harder for new people to read my code.
So it seems "The split" was already decided upon and will happen soon. There is no use arguing with those who want to do it They never compromise on anything, and consistently oppose all Hackage features that make cabal-install more usable. It's amazing how not one of them wants to admit it publicly though, nobody wants to be blamed as being the one who killed Haskell for childish reasons.
If hackage weren't so pushy about the PVP then there would be pretty much no reason for people to seek other package repositories. I wish that cabal-install would operate with a "trustee revisions" layer *outside* of hackage. (And I wish maintainer revisions would just create a new version and deprecate the old). As I understand it, this (or something like it) was an earlier iteration of the SLURP proposal which was more well received.
It proposes that SLURP have a small team of trustees that can make manual corrections for stuff like this.
I'll make this comment here, as I have nothing technical to add. Feel free to skip if you don't like angry rants. Frankly, I think this is completely ridiculous. How the hell did we get to the point of threatening to fork Hackage? I can see how a fork might ease tension between Stackage and Hackage maintainers, but it's terrible for literally everyone else. With the current state of affairs, Hackage maintainers break Stackage nightly occasionally, which is painful to Stackage maintainers. But Stack *users* generally have their needs met quite well. Forking Hackage and creating an inconsistent package set is an easy way to completely fuck this up, creating inconsistent packaging for really important stuff that's confusing to use. Stackage maintainers have already made enough of a community divide; there's no need to make it worse just because Hackage maintainers repeatedly do things that break more things than it fixes. The Haskell community would have officially jumped the shark if Hackage were forked. Imagine if Java just decided to fork the maven protocol today. Would it have any chance of adoption? No! Because it's a stupid idea that negatively effects everyone except maybe some maintainers of some repos. If the Haskell community is willing to just happily diverge on its most core infrastructure, what does that tell industries about our stability? It says we're a joke! We can't even keep a package repo stable; it would be worse than leftpad! Even if the state we have today isn't ideal, at least it's not comically, actively regressive. And how does SLURP fix all this? By saying it's ok to fork! As long as something's keeping track of it, even if no one is listening to it. It seems like the point is to allow Cabal and Stack to begin behaving differently, meaning that, inevitably, eventually neither will work with the other's package repos; *but it's ok* because SLURP will at least *tell you* that it's fucked. No one's going to listen, and the tools will continue to diverge, and SLURP will just sit there telling us where sources live, even if half of them won't build with the same tool as the other half. 
&gt; Frankly, I think this is completely ridiculous. How the hell did we get to the point of threatening to fork Hackage? I absolutely agree with this. It would be *really* bad for everyone in the community to have packages not come from a centralized source. Having differing package repositories with differing guarantees and only partially overlapping sets of hosted packages would lead to every Haskell user being worse off. As far as I can see this is not so much a technical problem but a social one. I can't believe that sensible people can't come to an agreement on something this important to our community. It's sad, really.
I added a few comments on the proposal where I explained that I feel SLURP does not satisfy stack's needs in order to accomplish reproducible builds. However, this single change (making packages immutable) would completely address this concern. However, this change would make the api less useful to cabal-install, because it wouldn't allow revisions to magically fix build plans for "cabal install foo", if cabal install only uses the SLURP API. However, they could keep revisions as a layer on top of the api results... okay now that I've thought this aspect through I'm going back to the github issue to make another comment...
I wasn't expecting a name collision for [my project of the last few months](https://github.com/ekmett/coda) in our rather small space quite so soon. =/
Depends. `haskellPackages` is much more ambitious. It's actually based on Stackage, so the core of the package set comes from some resolver. But it tries to be compatible with more GHC versions, and tries to pull in as many other Hackage packages as possible. `stackage2nix`, on the otherhand, is more conservative. It just gives you what Stackage gives you, and doesn't try to work with many GHCs. The tradeoff is stability and maintainability. `stackage2nix` is going to yield a package set that's exactly as stable as Stackage, and requires very little effort to maintain. But `haskellPackages` requires pretty regular maintenance to keep it working with different versions of GHC (there are tons of overrides in nixpkgs to try and keep everything working), and it's often unstable except on the blessed GHC. You also can't update `nixpkgs` and `haskellPackages` independently; or at least that would come with a lot of uncertainty. So basically, if you're just going to use the blessed GHC, and you're willing to update `nixpkgs` and `haskellPackages` in lockstep, then `haskellPackages` is likely better.
I think the haskellPackages set has a bit advantage because it properly tracks system dependencies as well as haskell dependencies. Of course, the package set is based on a stackage snapshot, without which it would be harder to maintainer. It's a nice example of a symbiotic relationship between two ecosystems.
My sister once thought I was talking about huskies, but that definitively got her attention. 
Ahh allright, thanks. For now I do not have any real specific questions, but who knows... maybe later! I am quite new to Haskell, working my way through learn you a Haskell (awesome resource!), so still getting into it. But I must say I enjoy it more than any other language I have learned so far. Brings me back to the joy of learning GW Basic In high school :) Anyway, thanks for the quick reply and I'll check out your meetup group!
I wonder if I've subconsciously picked that name up, or if it's just a name close at hand. I was almost made up on "Intermezzo" first, but thought it was too long and that I sucked at pronouncing it. :P 
Yikes. Yet another reason for me to dislike dependent Haskell.
So where is this division coming from?
Ah, I missed that, thanks. That should at least take care of the typo issue. 
Hackage maintainers doing things that Stackage maintainers don't like. Who's right is up for debate.
Isn't it a case of "hackage was there first" or is that too simplistic/childish? Who looks after Hackage?
Why do you dislike it? Matchable functions are quite neat and necessary to properly support pattern matching on types.
in general, trustees look after hackage: https://wiki.haskell.org/Hackage_trustees the box itself is maintained by the haskell.org infra team.
I stand uncorrected. :) I have to admit I didn't really attempt to grok what I was looking at, I just saw the p0..p62 signatures and assumed that n00bomb was right.
I'm just yet to be convinced that dependent types have the right power to weight ratio.
This seems crazy unintuitive to me, why did they do this?
We must not fork. There must be *one* package source.
I don't see what the weight aspect is. You can use as much or as little as you find convenient. It will let you more conveniently say things that we can already say in Haskell. Also, in some ways, dependent types are conceptually simpler than non-dependent types systems (lack of arbitrary distinction between types and terms).
Yup
&gt; So it seems "The split" was already decided upon and will happen soon Interesting. I wonder if that's the subtext I'm missing.
It is to me too and I have no idea why it's like that.
Ah well, I don't know enough about dependent types, but I will remain skeptical until I do.
These are really nicely put together screencasts, thanks for making them! It's great to see these type classes being introduced using a practical requirement as the motivation rather than introducing them by theory first.
What are examples of what maintainers are doing?
Even better than Stackage becoming its own package registry: git repos should be the only way to fetch packages, and Stackage should provide curated tested sets of *git revisions*. I'm not joking, by the way. I actually prefer the `go get` way. Registries are unnecessary bloated centralized junk.
&gt; Of course, the package set is based on a stackage snapshot Ah, I didn't realize. Well, it sounds like the best of both worlds, then! I wonder if FP Complete could be convinced to officially base Stackage off nix. There are probably political issues there that I'm unaware of, since I don't stay up on the latest package management drama.
btw, do you know how to officially maintain a haskell package on nixpkgs with a broken cabal2nix-generated file? there are several packages that build reliably, once you add a few system dependencies that weren't in the cabal file or that couldn't be extracted by the tool. My impression is that the default haskellPackages policy is to strongly prefer automatically building packages, for obvious maintenance reasons. But I couldn't find written anywhere, and the official nix file has many manual overrides like "dontCheck". 
I recommend taking a look at [this](https://www.youtube.com/watch?v=rh5J4vacG98) as an example of a nice way of working with complex hierarchical data.
Revising the cabal files of certain packages (particularly version upper bounds).
Does `go get` force package maintainers to use *git*? Seems rather restrictive.
Here's a `stack.yaml` that can get you started playing with it. https://gist.github.com/DanBurton/ba6fcf6b54491436fe8c98d2b2dc702b Just `stack setup` and then `stack repl`, and... bam! GHCi, version 8.4.0.20180118: http://www.haskell.org/ghc/ :? for help Loaded GHCi configuration from /private/var/folders/bz/c3q0fbt94dbgx69km1r5rbxw0000gn/T/ghci6972/ghci-script Prelude&gt; You can modify the "resolver" field to select any resolver you wish; since the "compiler" field is specified, it will override the compiler that comes with the resolver. This obviously means that some stuff will be broken. Or, you can just add `extra-deps` instead of using a resolver and try packages out that way. I generated this stack.yaml based on a very hacky script, which I will link here in case anyone is interested. https://gist.github.com/DanBurton/38157cb54259b71c5af45c3e86fef2b6 I'm intending to write a blog post about how to use Stack's "custom snapshots" feature to hand-roll your own snapshot based on ghc alpha releases for ease of testing your packages, even if the packages you depend on haven't published compatible versions yet.
I certainly read that from the proposal. It's backed up by SPJ's recent comment: &gt; A number of people have said that SLURP might actually encourage the very fork that we want to avoid. In a sense that's correct: SLURP acknowledges the diversity of package ecosystems, and (by making it easier for them to work together) arguably encourages such diversity. You might see that as a disadvantage. But our proposal takes the diversity of the ecosystems as a fait accompli (and diversity has advantages too), and seeks to ameliorate its consequences. &gt; ... &gt; A number of people have said that SLURP might actually encourage the very fork that we want to avoid. In a sense that's correct: SLURP acknowledges the diversity of package ecosystems, and (by making it easier for them to work together) arguably encourages such diversity. You might see that as a disadvantage. But our proposal takes the diversity of the ecosystems as a fait accompli (and diversity has advantages too), and seeks to ameliorate its consequences.
&gt;If hackage weren't so pushy about the PVP then there would be pretty much no reason for people to seek other package repositories. No, then there would be less reason for *you* to seek other repositories. There would be more reason for other people. This persistent shared delusion among stackage people that nobody likes or wants hackage or the PVP really needs to stop.
I am just guessing here but I would assume it is if you e.g. use new syntax only cabal understands.
My outsider understanding is this: I'm working with `D`, and `D` depends on `A` and `B`. `A` declares that it works with `C v1`. `B` declares that it works with any version of `C`, but it's wrong; actually, there's an important bug in `B` that is exposed when using `C v2`. Now `A` is updated to declare that it's also compatible with `C v2`. This exposes the latent bug in `B` by changing the choice of dependencies. Why does this matter? Because the core value proposition of Stack is that you can fix package versions to an LTS release, and rely on the stability of those packages. But if one of the package versions has a dependency relaxed on Hackage and Stackage pays attention (Note: it need *not* pay attention, as the original version is preserved), then it may break that property. (But, it may break only if C itself is *not* part of the Stackage LTS release, right?; otherwise, the version of C would have also been fixed by Stackage.)
Most other systems like this have a packaging revision number added to the end of the actual package version (e.g. linux distros).
Current issues aside, when I think about 10 years from now, I cannot see a successful Haskell with two competing, incompatible central repositories. It'd be different if it was decentralized but common, like Maven. But there's just no future where two different core infrastructures works out well. This has been a **massive** loss to the Python community with the v3/v2 split, which has only recently truly begun to heal. If SPJ's conclusion is that this *must* happen, then I have to question the industrial viability of Haskell in the future.
&gt; This persistent shared delusion among stackage people that nobody likes or wants hackage or the PVP really needs to stop. You are misrepresenting my argument. I didn't say "**nobody** likes or wants the PVP." I said (well, implied) that **some** people don't like/want forced adherence to the PVP. So I turn your statement on its head. Your delusion that "everybody is OK with the PVP" really needs to stop. If hackage is to be the place where everybody uploads their stuff, then it needs to relax about the PVP. If not, then we need something like SLURP so that people can upload somewhere besides hackage, while still being centrally indexed and available under a shared, agreed-upon protocol.
I see, so the problem is that Stack needs to implement parsing some cabal format? Couldn't the Cabal developers provide a parser that Stack could use?
If the problem is namespace clutter, then defining a new canonical source which holds the union of all package names across multiple systems fixes nothing.
&gt;I said (well, implied) that some people don't like/want forced adherence to the PVP. No, you very clearly implied nobody. You said there would be no reason for people to seek other repositories. You are very clearly stating that all of the people who like PVP don't exist. &gt;So I turn your statement on its head. "No u!" is not a compelling argument. &gt;Your delusion that "everybody is OK with the PVP" really needs to stop. I have no such delusion. I never said, suggested or implied that. I said some of us want it. Those who do not, have already made themselves heard and made a replacement. Nobody is denying your existence. I am saying stop insisting that the people who like hackage don't exist therefore hackage can be safely destroyed to suit your wishes.
&gt; I never said, suggested or implied that. You are correct. My mistake. I inaccurately projected views onto you that you did not yourself express. As for what I said, I'm not sure if you are willfully or accidentally misunderstanding me. I will assume the latter. Please reread what I said: &gt; If hackage weren't so pushy about the PVP then there would be pretty much no reason for people to seek other package repositories. I do not assert that people who like the PVP don't exist *in general*. Rather, I assert that the people *in particular who wish to depart from hackage* mostly are doing it for PVP-related reasons. And I really should have clarified that it is the PVP *and* revisions-by-trustees, not just the PVP, that is rubbing some people the wrong way. tl;dr hackage is really rather appealing. It's just this one PVP/revisions thing that *some* (not all) people don't like which seems to be causing the divide.
what's there to dislike? The monolithic hackage setup is reportedly why we're seeing close to no change in the code-base. A microservices-based setup would make it easy for people to contribute layers of useful metadata.
Anything like Servant for building game servers?
I like this proposal because it preempt discussions before an actual rift. I do hate that a rift is an actual serious consideration inside the haskell community.
&gt;Rather, I assert that the people in particular who wish to depart from hackage mostly are doing it for PVP-related reasons No, you didn't. You asserted that if hackage were the way you want it, nobody would have any reason to seek an alternative. Ruining hackage is a reason for us to seek an alternative. We would have a reason to seek an alternative, just like you have a reason right now.
But if the new canonical source of names has namespaces, that would alleviate the problem. Instead of a package called WAI, you can have a package called `org.haskellstack/WAI`, and this would open up space for someone else to make a package called `experimental.games/WAI`, which is hosted in a different repository (e.g. on GitHub), without leading to confusion.
&gt; SLURP focuses on one problem and one problem only: coordinating package name ownership and making sure the owner of package name is advertising available versions in a standard way. Why not just give each package a UUID, generated by `cabal init` for new packages. Each package is then uniquely identified by both it's name and UUID. No need for any central authority.
Can we rename this to [SLURM](http://i.imgur.com/JrWXkdV.jpg)?
Look at the mod and rem functions. Also, make sure to test negative inputs -- that's where mod and rem differ. Also, remember that 15 % 25 just equals 15. You don't need the equivalent of if (x &gt; 25 || x &lt; 0) { return x % 26; } else { return x; } A bare `x % 26` equivalent will work just fine.
git clone someone else's vim configuration and see if you like it. I originally stole my coworker's configs. For [example](https://github.com/elliottt/vim-config). Alternatively, there was a vim posting here on reddit about a month ago - wasn't my preferred setup but it might be yours.
For vim, I use the following snippet: function! AddLanguagePragma() :call fzf#run({ \ 'source': 'ghc --supported-languages', \ 'sink': {lp -&gt; append(0, "{-# LANGUAGE " . lp . " #-}")}, \ 'down': '20%'}) endfunction This requires the [fzf plugin](https://github.com/junegunn/fzf.vim) to display a fuzzy-finder with a list of extensions, then automatically inserts the selected one at the top (no need to jump there).
Servant?
&gt;No one's going to listen, and the tools will continue to diverge, and SLURP will just sit there telling us where sources live, even if half of them won't build with the same tool as the other half. That's going to happen no matter what. Haskell was already split. A lot of people want to keep their head in the sand and pretend otherwise, but its already over. The options are not "allow fork" vs "prevent fork", it is "make fork less painful for users" vs "make fork more painful for users".
I'm going to echo this and say that this debacle has caused me to re-evaluate my usage of the stack ecosystem. Even though stack is currently a nicer tool, I'd rather not be a contributer to a rift in the community.
&gt; The options are not "allow fork" vs "prevent fork", it is "make fork less painful for users" vs "make fork more painful for users". Why? The current state of affairs is not ideal, but in my view, it's strictly better than the world with a fork, regardless of how painless we make it.
I don't really understand the question. "Why did the civil war have to happen why not just don't have a civil war?". Unless you can resolve the underlying conflict somehow, the war is inevitable. 
Except in this case, the state prior to the fork is strictly better to the state after the fork for the foreseeable future. There's no war; only decisions that affect the quality of our tools, and that quality will go down with this decision.
&gt;Except in this case, the state prior to the fork is strictly better to the state after the fork for the foreseeable future That's often the case. But the people engaged in the conflict don't see it that way. &gt;and that quality will go down with this decision. What decision? The whole point is that there is no decision. A huge conflict with tons of division is not some trivial thing where one day a few people decided "hey lets just bust haskell in half for no reason". A bunch of people want haskell to be something different, and they will continue to try to make it that way. There is no way to stop them, so there is no decision that can be made to avoid this.
Well, at least an example where it produces the most sensible result: newtype X = X {x ::Int} deriving Num main = print $ 3{x=5}+4{x=10} Here you print out 15 instead of 10, which would be the worst answer I think.
"LANG=en_US.UTF-8 stack repl" doesn't work. Somehow LANG isn't passed into stack. I think the ".ghci" solution works perfect. Thanks for your patient help!
There indeed is a parser that Stack could and does use in the form of the `Cabal` library; however, I think the trouble stems from the fact that Cabal developers (understandably) want to push new features into the ecosystem quickly but Stack has (understandably) historically lagged quite a bit behind on incorporating new Cabal releases. This, coupled with the rather stark philosophical divide between the two tools, makes for a rather difficult compromise.
You can set something up really quick with Nix if you want to manage Haskell at the granularity of git repos and revisions. That's how we do it at &lt;company&gt; for any of our dependencies that deviate from Hackage.
For the record, this helped, and now we're seeing the issue which requires `cabal-2.2.`, as *recursion-ninja* prophesied, issue [here](https://github.com/LumiGuide/haskell-opencv/issues/107).
I wouldn't really say Haskell is a variant of SML. While they are both functional languages (mostly), there are several significant differences between them.
https://hydra.nixos.org/eval/1428463 ghc cross just landed today, actually. At least one combo is working!
For those who haven't seen it before, I've got a write up of using free and cofree for DSLs and interpreters [here](http://dlaing.org/cofun/posts/free_and_cofree.html) and using them a la carte [here](http://dlaing.org/cofun/posts/coproducts_for_free_and_products_for_cofree.html). I just realized that the video has moved from the link on that site, I'll fix that up in the next day or two. 
Let's all use nix!
Perhaps you're right. But so far, it seems like almost no one besides the Stackage maintainers actually wants this fork to happen. If that's the case, and they go ahead and do it anyway like you suggest, then I would consider that a horrible abuse of power. So I have to believe they will listen to the community on this, which is why I think we *desperately* need to open a proper proposal about whether or not there should be a fork at all, before resolving this SLURP proposal.
Thanks for this very clear and practical Screencast. You are filling a need. Looking forward for next episodes. A small typo: At the end, when you test calculateProjectReport in the REPL, the function call is missing. &gt;* Expected: *Demo&gt; calculateProjectReport someProject * Actual: *Demo&gt; someProject 
The tech world is far too idealistic. The haskell community even more than average. Not everyone wants to help people. Not everyone wants to do good. Some people have goals and objectives that involve causing you harm. I certainly hope someone figures out a peaceful solution, but I also hope I will win the lottery every week for the next year. And I think the latter has better odds.
&gt; If you remember any, would you be up for opening an issue with anything you thought could especially use more documentation? I certainly could. I thought about it before but I wasn't sure if it was appropriate because its not an actual bug in the software. Those examples in Yesod Test saved me days of working out how to use it.
So can we distill the *technical* reasons for a possible fork? Is it: 1. Packages aren't immutable on Hackage 2. READMEs aren't displayed prominently enough on Hackage 3. ??
The proposal doesn't have that feature, however.
What do you mean close to no changes? https://github.com/haskell/hackage-server/commits/master (Incidentally, the organization of the code-base is _very_ modular, almost to a fault :-))
I do not see any reason to fork the central package repository. It is pretty clear that 1. Stackage is a frozen and curated set of ghc + packages, much like what haskell 
The proposal talks about namespaces, but the details of it haven't been worked out. I think this proposal is pretty much worthless unless the details of namespaces can be well defined, but that is up to us as a community.
Thank you for writing up your experience! Feel free to ask here or in fpchat slack if you need further help :) 
Stackage maintainers don't want the fork to happen either, they'd rather artifacts be immutable (such as exists in almost every other PL community infrastructure...) so they can work on something else.
The unique names presumably aren't for the benefit of the compiler, but rather for the benefit of programmers. So we don't have conversations like "Oh, you don't want to depend on text-3f174d0. You want to depend on text-782ff5a."