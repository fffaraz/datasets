If it helps, I read with _p [class_ "logo"] children commutatively as &gt; with *this configuration* for the parent use these *children* I'm not really thinking of "x with y" but rather "with x and y". Or if you like: &gt; with parent *p* having class `logo` append these *children* It made sense at the time I came up with it, although I'm not sure how well it stands up to scrutiny. I considered `on` (but that conflicts) and `for`. Does my wording for it help? Just to add to the discussion, there's also this property: λ&gt; with id [class_ "foo"] (p_ "Hello!" &lt;&gt; p_ "World") &lt;p class="foo"&gt;Hello!&lt;/p&gt;&lt;p class="foo"&gt;World&lt;/p&gt; Which `ski` from #haskell noticed. It's perfectly legitimate behavior. `with` could be defined as just `with &lt;attrs&gt; &lt;children&gt;`, reminiscent of `censor` from Writer, but I preferred to have the parent name come before the attributes.
Using the README in preference to the description is genius. Make it clear just how pointless the description is.
I want to be clear about that: I specifically want to avoid inessential operators (obviously, deemed inessential according to my own personal judgment), which I realise goes against the grain of the typical Haskeller aesthetic. They just make editing harder and scare Haskell newbies who want to edit your templates. (!) begets ($) and so on. Lucid is for the crowd happy with plain old functions. If Lucid got operators I'd partly consider that I'd failed at achieving what I wanted. Consider that the philosophical/opinionated part of the UX of the library.
Scratch that. To my embarrassment, pandoc doesn't decode PDFs (not yet at least). But you can pipe pdftohtml output to it: pdftohtml -p -s -noframes -stdout foo.pdf | pandoc -f html -t epub3 -o foo.epub
[tyxml](http://ocsigen.org/tyxml/) is a library that does this in Ocaml. Its not hard to use and seems to be very popular with them but its built using polymorphic variants, which are an OCaml feature Haskell doesn't have.
Another possibility might be one vs two underscores: div_ " Syntax" div__ [id_ "header" ] "Syntax" Of an additional " a" suffix div_a [id_ "header"] "Syntax" 
The apostrophe is a dead key in my keyboard so I find it more annoying to type than _
Nix uses a wrapped GHC which searches PATH for haskell packages if you use nix-shell. As cabal uses GHC, it will do the same. 
what I don't understand is, why do you have to explicitly add the line ``extra-source-files: README.md`` to the cabal config file? can't Stackage just render the ``REAMDE``(``.md``) *by default* if it is present in the package?
`Handle` uses `MVar` to atomically read and update it's state. So `hClose` has to use `takeMVar`: data Handle = Handle (MVar ...) hClose (Handle mvar) = do state &lt;- takeMVar mvar state' &lt;- update state putMVar mvar state' (Actually it uses something similar to `withMVar` for exception safety). Note, that `takeMVar` is not interruptible if the `mvar` is full (see docs: http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Exception.html#g:13 ) In most cases it *is* full, but if you are accessing the handle concurrently from other thread at the same time, then `takeMVar` will be blocked, and can be interrupted. As a result `hClose` fails with exception and breakes it's contact: &gt; If hClose fails for any reason, any further operations (apart from hClose) on the handle will still fail as if hdl had been successfully closed. 
It's a term I made up for what happens to `do foo` during this transformation: bar $ do foo bar → bar $ do foo bar The `do` swings its children underneath itself and its parent.
I see, thanks!
I was very confused by that `with id` trick. I'd prefer a less magical/powerful solution for attributes to keep it more similar to HTML, where you always specify the attributes next to the element name.
I also fully recognized the irony of doing the stats in Ruby instead of Haskell. I considered analyzing the data in Haskell, but realized it would have taken more time, since there isn't a Haskell client for the API that I was able to find. Just reading the API docs that I needed would have taken more time than doing the whole analysis using the Ruby client. The fact is, Ruby just many more libraries available than Haskell. Personally I think that for many projects larger than a 10-line script, though, the benefits of Haskell start to outweigh the advantages of having a library for pretty much everything. If we continue doing analysis of this sort in the future, I think we'd probably open-source a Haskell client for the Meetup API. :) Thanks for reading the analysis!
Adding extra-source-files is what causes the README to be included in your package. Stackage is rendering it if it's present, these instructions are how you ensure it is present.
Nice work, hideous situation. When will Haskell have a module system to workaround global namespace conflicts? From the outside looking in, that such a foundational piece is missing is eyebrow raising.
I like the CPS version. It's useful when writing `FromJSON` instances as `parseJSON :: Value -&gt; Parser a`, so we have parseJSON = withObject "nameOfObject" $ \o -&gt; ... so `withObject` transforms your object-specific validation code to value-general validation code.
Yeah, I doubt there's a good reason for that.
For what it's worth the same trick is possible in Blaze: page1 = (\m -&gt; m) ! class_ "foo" $ p "ok" &lt;&gt; p "foo" λ&gt; main "&lt;p class=\"foo\"&gt;ok&lt;/p&gt;&lt;p class=\"foo\"&gt;foo&lt;/p&gt;" It's not an intentional trick, but rather a natural consequence to the way attributes are assigned to any function of type `(Html () -&gt; Html ())`. I'm not saying anyone will ever use this behavior, but without it the monad was not a proper monad. You should be able to compose `with`. For example, let's say you want to define a re-usable component with bootstrap: λ&gt; let container_ = with div_ [class_ "container "] Now you can use it to make a container: λ&gt; container_ "My content!" &lt;div class="container "&gt;My content!&lt;/div&gt; But consider now that you also want to add additional attributes to it later. You can do that with another call to with: λ&gt; with container_ [class_ "main"] "My content!" &lt;div class="container main"&gt;My content!&lt;/div&gt; Duplicate attributes are composed with normal monoidal append. In Blaze this produces λ&gt; main "&lt;div class=\"container\" class=\"main\"&gt;My content!&lt;/div&gt;" Browsers ignore the latter `main`. (Note that I added a space in my definition of `container` anticipating further extension later. Other attributes might not compose with spaces.)
It's at least not too unreasonable because you often want an overall function of type `Value -&gt; Parser a`, and rarely stop at an `Object`. So, the other side of the question is how hard it is to use your suggested interface that way. But that's not so hard either - with `object :: String -&gt; Value -&gt; Parser Object`, you can write parseJSON = object "name" &gt;=&gt; \o -&gt; ...
But parseObject :: String -&gt; Value -&gt; Parser Object parseJSON = parseObject "nameOfObject" &gt;=&gt; \o -&gt; ... 
It could also be parseJSON v = do o &lt;- parseObject "thing" v ... In each case, and it's probably familiarity talking, once I figured out how `parseObject` was supposed to be called I liked the convention used the most. (&gt;=&gt;) is a less common combinator and using the extra line in `do` doesn't emphasize that this type *is* an object in the same way. I think it's just a bikeshed issue.
But if I want to use `do` notation then the CPS version is more awkward isn't it?
Indeed. When I was thinking about this, I was thinking that simple `local` opening of a module would help a lot. One nice thing about Clojure I learned is that they have vector syntax similar to Haskell's: `[1 2 (* 2 3)]` is like `[1,2,2 * 3]`. They also have :foo for self-evaluating symbol. It's like "foo" but a bit more convenient. Put those two together and you get: [:p [:span "Hello!"]] That's a pretty nice solution. The things Haskell lacks is: * 'p in Haskell mandates that `p` exists and is in scope. * List syntax in Haskell is more expensive: `['p,['span,"Hello!"]]` * Lists in Haskell are homogenous, so the `'span` /= `"Hello"` types don't match. I wonder whether idiom brackets could come in handy here.
Dead link? Can author repost elsewhere?
A little. I'm not actually vying for `do` notation as being the right way. I think the canonical syntax that you're "supposed" to use is like parseJSON = withObject "yo" $ \o -&gt; SomeType &lt;$&gt; o .= "foo &lt;*&gt; o .= "bar"
I really hate the look of the underscores. That alone is enough to make me not inclined to use it. If only Haskell had some nice syntax for symbols (interned strings), then we could just have some overloaded syntax for them for your tags. 
I found the design of the pipes library an interesting read, and it's quite well designed.
Is the wrapped `ghc` also what ensures that when `cabal install` can't find the right version of a package it will fetch it and install it in `/nix/store` or whatever? Thanks again everyone for being so patient with these questions!
The whole server seems to be gone; perhaps it will come back at some point?
Turns out this is a know bug: https://github.com/NixOS/nixpkgs/issues/2689
I've always been bothered by HTML generators producing builders/text instead of proper data type. In many cases it would be convenient to be able to modify the structure after it has been built. 
You might try out FP Haskell Centre, which is a Haskell development system you use through your browser. It saves you from having to install anything or learning cabal or managing packages. On the other hand, it doesn't include the interactive ghci, so you always need to define a 'main' function and run that. It's free and easy to set up so you've little to lose.
Yeah. We have `'foo`, but the names must exist. It'd be interesting to have `:foo`, like in Lisp, which could be overloaded to `fromSymString "foo"` (or w/e).
I'm not sure where you get the idea that newbies are allergic to operators. Operators are not a unique or difficult concept in Haskell. I've had one newbie tell me he thought `&lt;$&gt;` was more readable than fmap, and he found it annoying that he had to import it from Data.Monoid.
One downside of two suffixes is that if you want to create a function abstracting a subtemplate you also need to create two versions of it: container' contents = div_ [class' "container"] contents container_ attrs contents = div_ (class' "container" : attrs) contents Boils down to `with` being an instance of the Builder pattern to workaround against Haskell not having optional function arguments.
&gt; I'm not sure where you get the idea that newbies are allergic to operators. From experience with newbies, which evidently differs to yours.
&gt; `&gt;=&gt;` Extra import to get the Kleisli fish. `$` is in prelude.
Only 3? I guess before then we just used newsgroups. ;)
I've heard good things about Darcs, but I've not looked at it myself. 
One thing that I hate about these _with_ functions is that they're an enormous pain in the ass when you're trying to layer monad transformers over them. If they don't have to be in CPS, they shouldn't be.
I've seen both responses *in the same person*. Operators are hard to find is search-engine-I-use-for-everything else and tend to provide less of a hint of their behavior than function names. This gets in the way of newcomers. Showing them hoogle helps, but doesn't really completely erase the pain of learning a new operator. Operators, once learned, are often applied liberally to improve the way the syntax looks and reads. Newcomers have no problem using operators they know and will even overuse them because some things really do make more sense infix.
hmatrix provides a practical example of converting imperative libraries into a nice functional api. 
I'm not a fan of its configuration file, I wish it provided dyre/xmonad-style configuration.
I actually prefer the new highlighting.
I've wrestled with this, too. Primes are problematic for their visual interactions with double quotation marks and, the worst, back ticks used to infix a function. Possible confusion with the idiom of strict variants of a definition being primed is another minor grumble. I can't say I love underscores either, but we really don't have many options :(
BTW, why do you want to learn programming? We can tell you if Haskell would be a suitable language to learn. If you're interested in computer science, it's definitely the right choice.
&gt; Static types usually do require some extra effort that is not required in an equivalent untyped program In my experience, untyped programs require substantially more effort to debug and maintain that is not required in an equivalent typed program.
Have you used F#? From what I can tell, it's culturally a lot closer to OCaml, which soundly rejects OOP despite supporting it.
&gt; Lisps are generally a lot more imperative. Generally they'll abuse implicit side effects a lot (aspect oriented programming, metaobject protocol, etc.). A lot of schemers place value on purely functional programming (in the same way that OCamlers do, so they view purity as an "ideal" that is impractical to reach most of the time but still worth striving towards), but definitely CL people basically just write imperative programs all the time. Elisp is also more or less imperative. Although a lot of very basic operations are still written as pure functions in elisp, which is nice (e.g applying a face to some text is not a destructive update). I don't know about Clojure, but I think they're a bit closer to Scheme people than CL people.
I can certainly believe that, maybe I'm just so used to the old highlighting that the change was jarring.
Kleisli fish! I'm using that from now on.
Good points! I suppose coloring the function name in a type signature does have it's benefits, that's something that could be left there. Also, I feel pretty strongly about not highlighting "builtins" (prelude functions) in a special way. It doesn't seem very Haskell-ey to do this, since these names could be coming from elsewhere. I wish I had a sample of the old highlighting! Sounds like using the "pygments" package would yield this.
You can learn the basics right in your browser by using the Haskell interpreter at: http://tryhaskell.org/ But if you want to compile and run programs on your computer, here is how you can get started at being a real hacker... First download and install the Haskell Platform, that is the Glasgow Haskell Compiler (GHC) along with all the additional code libraries you need all together in one convenient package. https://www.haskell.org/platform/ Then get yourself a good source code editor. There are several good ones, even Windows Notepad or Mac TextEditor will work for beginners, but you can look into these as well: http://gedit.en.softonic.com/ http://www.sublimetext.com/2 http://lighttable.com/ If you use Linux, just use "Gedit" or "Kate", don't bother with a more advanced source code editor, Gedit and Kate on Linux are *plenty* good enough for beginners, they even color the source code for you and automatically indent things. I highly recommend you use one these. Gedit is available for Windows as well. If you are using Windows, I highly recommend you install Cygwin so you can make use of Linux's/UNIX's "Bash" shell: https://www.cygwin.com/ If you are using Mac, you can just use "Terminal" in the Applications/Utilities folder. If you are using Linux (Ubuntu, or whatever), you can use their version of "Terminal" just like on a Mac. In order to make your program run, you should probably learn a few things about how to use the Terminal command line but I wont go into that here. Anyway, open your editor and write a simple program: main = do print "What is your name?" name &lt;- getLine print ("Hi, "++name++", nice to meet you!") Save this file as **hello.hs** in your "Home" directory. Then, open a Terminal or Cygwin window and enter: ls hello.hs Do this to make sure you can see the file you just saved. If you see an error, you probably saved the file in a place outside of the Terminal's default folder. You may need to enter: cd $HOME That should change to your home directory. If you enter **ls hello.hs** again and the name of the file appears on the next line, you are in the right spot. Now you are ready to make your "hello" program into a runnable, compiled program. In the Terminal enter: ghc --make hello If everything worked correctly, you should see something like this on screen: [1 of 1] Compiling Main ( hello.hs, hello.o ) Linking hello .. However if you see an error message like this on screen: ghc: command not found That means you have not installed the Haskell Platform correctly. If you see an error message like this on screen: target ‘hello’ is not a module name or a source file That means your "hello.hs" file is saved in a place that the Haskell compiler cannot see, and you need to us Terminal commands like "cd" to change to the folder where "hello.hs" is saved. If you see an error like this on screen: hello.hs:2:1: Parse error in pattern Any error message with the name of the file "hello.hs" followed by a line and column number indicates that Haskell is installed and working and it can see your program, but your program has a mistake. So go back to your code editor and make sure you typed in the program correctly. If you see no error messages, and you see the **Linking hello** message, then you are finally ready to run your program. To run your program, in the Terminal enter: ./hello (that is dot-slash-hello) and it should work. 
Not originally mine. I think I got it from [Stack Overflow](http://stackoverflow.com/a/7833488/2008899) or /u/tekmo, but it's probably even older.
why not foo' It avoids the shift key for the underscore
&gt;Only 3? I guess before then we just used newsgroups. ;) Which was probably smarter, given internet latency/bandwidth limitations and how obnoxious synchronous communication can be. That's one a thing lot of people don't get about IRC - it's still asynchronous. You can't treat it like IM. You have to ask questions and ignore it until somebody responds. If you expect immediate answers you'll almost never get answers.
Yeah, I'm also talking about people who are just plain new to Haskell, not necessarily learning it. You can expect a certain level of cooperation from designers and content writers to use your templating language, whatever it is, as long as you can explain it and it's simple. The more units of knowledge required to learn the more likely they will ask you to use something they're more familiar with.
For what it's worth, when your code is auto-formatted for you, you start to care less about tabs vs spaces and having a desire to swing things to potentially save you editing work in the future. I would recommend my style only in concert with decent tooling like [SHM](https://github.com/chrisdone/structured-haskell-mode) and [hindent](https://github.com/chrisdone/hindent).
See here: http://www.reddit.com/r/haskell/comments/2my5bc/lucid_templating_dsl_for_html/cm8qjio
I think this is a good argument for not using an EDSL, because the Haskell layout rules might not match with your designers and content writers internal layout rules. Once you are using an EDSL, you have to accept learning some Haskell, and I think custom operators is less cognitive load than the layout rules.
Not me. Somebody else suggested the fish analogy, but it was in the context of `pipes` operators (because some of them also look like fish).
I'll give you that pipes is an amazing product with a genius implementation and well-documented to boot but come on, OP is trying to learn Haskell. Unless he's a phd computer scientist specializing in formal-methods, pipes is going way over his head. -- | ('&gt;-&gt;') with the arguments flipped (&lt;-&lt;) :: Monad m =&gt; Proxy () b c' c m r -- ^ -&gt; Proxy a' a () b m r -- ^ -&gt; Proxy a' a c' c m r p2 &lt;-&lt; p1 = p1 &gt;-&gt; p2 {-# INLINABLE (&lt;-&lt;) #-} WTF?
Possibly. You have to indent your code whatever you write. Normal function application can have rules. Operators introduce their own novel layout questions. I don't think there's a dichotomy between operators and layout rules, they are orthogonal impediments.
I didn't have that easy of a time with Pipes, but I don't think this particular function is problematic. The transition seems pretty clear based on the types. Proxy () b c' c m r Proxy a' a () b m r Proxy a' a c' c m r 
Before I get down voted off this thread I need to warn you these people are trolling you. Read [haddocset](https://github.com/philopon/haddocset). It's short, clear and does something that will be extremely useful to aid your learning.
This is basically just a JIT, right? You additionally collect "runtime" statistics in order to make the best guesses as to where to perform the next stage of compilation and what values to choose to specialize.
Pipes being the rather complete extension of [Mario Blazevic's Coroutine Pipelines](http://themonadreader.files.wordpress.com/2011/10/issue19.pdf) it might be beneficial to start with the Monad Reader article. In the same thread, there's Blazevic's [`monad-coroutine`](http://hackage.haskell.org/package/monad-coroutine) package.
Yeah, curious about the advantage of using combinators rather than quasiquoting. I guess being able to use all of Haskell's control flow, as well as creating combinators and applying them later is very nice in practice.
My style is not Haskell-specific. I manually format my C, C++, Java, Scala, and F# code, too.
When we're on the topic, could we have nicer rendering of code on Hackage? I know we all have different standards and preferences, but at least the Github markup is quite unoffensive, and I often browse through code in repositories before deciding to clone something (one of the reasons I still prefer Github over other code hosting sites is how it puts the code front and center). The Hackage code just looks horribly ugly - the colors, the font, the style, everything. Here's a simple example contrasting the Hackage rendering and just pasting it into Sublime. http://imgur.com/a/vaw08
I've heard there's a strict variant of Haskell called Mort.
I'm sorry to hear that.
I'm currently writing a small toy program for myself. For configuration I'm using a EDSL, which I load from a file and interpret at runtime using the `hint` package. Now I'm just thinking about making a package which provides arbitrary, reloadable runtime configuration by using `hint`. Does this make sense? Anyone interested?
What about functions like withFile, which run some code before and after the user callback?
Right, these functions usually need to be in CPS. But withObject certainly does not have to be.
I believe the actual bug was buried inside GHC. Haddocks build correctly for `gl` on GHC HEAD. The issue is that if a module re-exports a pattern synonym defined in another module and you go to haddock it, GHC panics.
I'm curious, why do you think the existing, more upvoted examples are not well regarded projects or not easy to learn from?
Have you considered swapping the argument order of `with`? with attrs element inside I realize this puts `element` farther to the right, making it possibly harder to read and less HTML-esque, but `with attrs` is a more sensible partial application than `with element`, and if you just cover up `with attrs` then it still looks like `element inside` just like it would look without the attrs. ---- For multiple children, I have to use `sequence_`? How irritating. ul_ (sequence_ [li_ "a", li_ "b", li_ "c"]) I guess there's always monad notation ul_ (do li_ "a"; li_ "b"; li_ "c") ul_ $ do li_ "a" li_ "b" li_ "c" I guess I can live with that, but I wish Haskell had a better story for vararg functions that didn't involve weird typeclass hackery. Instead of "this glob of HTML goes inside", I wish that Lucid were more semantic: these three children go inside. But it's not so bad as long as you restrict "a glob of HTML" to mean either an element or sequence of elements. ---- &gt; Duplicate attributes are composed with normal monoidal append. Meh. I'd prefer the `class` and `style` attributes to compose more intelligently. Other attributes should probably just compose via outermost-overrides. ---- What if I want to create just an empty div? div_ mempty Meh. I'm all for EDSLs but I don't think Haskell is a great candidate to host an HTML DSL like this. A little too much Haskell leaks into the abstraction that should be focused instead on composable HTML. Lisps are slightly better hosts. ---- But I certainly think Lucid is well thought out and addresses a lot of Blaze's flaws. I like how the trailing underscore puns right along with the fact that these are all monadic actions/functions that produce a unit result.
What's the reason for the `a ~ ()` constraints on the [`With` instances](http://hackage.haskell.org/package/lucid-0.4/docs/Lucid-Base.html#t:With)? Something to do with type inference? The existing `With` implementations discard the return value and return a fresh `()`. Threading the return value is a trivial change, compiles, seems to work, and with the irrefutable pattern doesn't seem like it would change any strictness properties.
wow, basically all the most popular text editors don't support ligatures -- how odd.
Is there a well-known solution to the problem of making a `withFile`-style function which would be compatible with monad transformers? I think I remember reading an article on the subject a while ago, but I can't find it anymore.
I actually prefer the swung look, which I guess is because of my background in imperative languages, where that fixed indentation step is common. Lists like this also send a little shiver up my spine: foo $ bar [baz ,luhr ,man] Oh, but while I'm here, I should say thanks for the new library! I must say I'm quite impressed with the way you've succinctly solved some significant problems with Blaze :).
Thanks, it was indeed monad-control which was discussed in [the article I was talking about](https://www.fpcomplete.com/user/jwiegley/monad-control).
If I really can't talk you into using Python Pandas, R or Julia then I'll have to refer you to [Parking in Westminster: An Analysis in Haskell](http://idontgetoutmuch.wordpress.com/2013/10/23/parking-in-westminster-an-analysis-in-haskell/) 
That doesn't seem like a very good argument. If using `&gt;=&gt;` were the blessed way it could be included with an Aeson import.
I bought it because I'm interested in learning about FRP. It's definitely a work-in-progress. Basically it's just the OpenGL intro and most of the FRP section. It really wanders off the rails towards the end. Looking forward to some updates.
There are IRC channels on Freenode (#haskell and #haskell-beginners) if you still want help with this specific code but I'd strongly recommend learning Haskell :)
Part of the problem here is that we don't know where the submitter is at with respect to learning Haskell. I can't speak for anyone else, but I think I turned a bit of a corner once I reading some of that code that people are suggesting in here. The first time I attempted it, I wasn't ready and I ran away. Looking back on it now, there was kind of a chicken-and-egg situation going on - I was missing a baseline of Haskell literacy that I didn't know I needed, but there wasn't much else that I knew of that could help get me to that level of literacy. I distinctly remember feeling that "go read the GHC source" was more or less equivalent to "go read Moby Dick" at that time. I think some of the more approachable-for-beginners academic papers helped me jump that gap, since they contained snippets of code - once I was comfortable with the small game I was more comfortable looking at what was happening at a larger scale. My hand was also forced a few times when I had trouble deciphering some types for some packages and had to peak at the code :) I think you make a really good point - especially for those who are brand new to Haskell. I can also see how the advice in this thread is solid for someone who has gotten comfortable with the syntax and with solving small problems and wants to go a bit further. So I disagree with calling the people providing that advice trolls or suggesting that they're just showing off.
How new are you to Haskell? That might help guide the advice a bit. This is slightly off-topic, but for small-scale conventions I found that rigging up hlint to work with my editor and run on every save was pretty helpful.
It was actually looking at Pandoc that got me interested in Haskell to begin with :) Thanks for the suggestions.
Oh yeah? What about that institution horrendously named "Bar Camp" ?? The foo was there in Foo Camp originally, but I first heard of Bar Camp independently. What a stupid stupid name. Anyway, I'm SURE you can find contexts in programming where bar is not right next to foo. Your "never" is probably false.
Wow, that is exhaustive. Thanks!
https://github.com/ekmett/hask/blob/master/src/Hask/Category.hs
Everyone knows you can click on someone's name and get a feel about what they're about right? It's how I know /u/dalaing eats Haskell (and only Haskell) for breakfast, lunch and dinner and /u/wreel is a fairly well rounded guy who has yet to realize that he'd have to quit his job and leave his family if he's going to take up this particular hobby.
Have you seen [hiccup](https://github.com/weavejester/hiccup/wiki/Syntax) library for clojure? It looks to me its syntax can be used in quasiquoted DSL that would seamlessly splice haskell variables. 
&gt; we can transliterate the code of a sorting algorithm into Haskell and be sure that the result has the same (or in very rare cases a better) algorithmic complexity as its eagerly evaluated counterpart. Transliterate in the sense that write it in an imperative fashion with mutation ?
:) I read that actually. It was a good read, but I still feel like there's a huge amount of overhead in that code... Ideally there would be general abstractions that make it easier to begin working with data. Or perhaps there are specific functions, data types etc that we could extract out into a library. 
I'm with you on this: I almost never export operators in public APIs. A function can never be read without nonlocal context if it contains an unfamiliar operator that is defined elsewhere. Whereas if you only use named functions, the gist of what the code does is often clear (at least if your names are good.) If you export a new operator, IMO, you should have a pretty good reason to do so. You're imposing a tax on reading and understanding all of the functions that use it.
A lot of the setup can be abstracted away using Template Haskell to generate the records that match up with the schema so that cassava can parse into it. And then together with your favorite lens library you're like 70% of the way to the functionality of a dataframe library. Of course Template Haskell comes with a bit of baggage and caveats as well.
But seriously if you haven't tried Julia, you might like it. In addition to DataFrames and a range of numerical analysis and visualization, it's not Blub. They have a type system, parametric polymorphism (way different from Haskell though), an optimizing compiler and a great community. Edit: forgot to mention scheme-like macros and all the FP you would not expect in what is on the surface a Matlab clone.
The example definition of strict `foldl'` in the article is recursively calling the non-strict `foldl`. foldl' f a (x:xs) = let a' = f a x in seq a' (foldl f a' xs) I presume this is not intentional, or am I misunderstanding something?
Great article! I'm not learned enough to judge its correctness, but if everything in it is right, thank you for making an understandable article about that. Do you know, and if so, could you elaborate about how this is actually implemented in the RTS / Compiler? What is the compiler magic under laziness? Thanks!
That's basically the truth. Compare the definition of [foldl](http://hackage.haskell.org/package/base-4.7.0.1/docs/src/GHC-List.html#foldl) and of [foldl'](http://hackage.haskell.org/package/base-4.7.0.1/docs/src/Data-List.html#foldl%27). 
I have to second Python Pandas. It has a lot of great features, one in particular that I like is the `pandas.io.clipboard.read_clipboard()` function, which reads the contents of your clipboard into a data frame. Makes working with Excel quite nice (remember `sep='\t'`) because you can copy/paste very quickly between it and your terminal.
If there's a camp named "Bar Camp" that does not include a bar, no matter the source of the name then that's a problem with the organisers, not the metasyntactic variable. Camps are supposed to have bars no matter what they're called. And ever *if* you have to call it "Bar Camp Bar". So what.
Right, Julia has some good work put into it. But that doesn't mean we can't borrow/copy all that great work and implement it in Haskell, and enjoy all the great work in Haskell at the same time!
[This](http://www.serpentine.com/blog/2011/02/25/cps-is-great-cps-is-terrible/) may have something to do with it. TLDR; the switch to CPS in attoparsec and aeson achieved an ~8X performance improvement.
"cabal build" is no use when your program is split into several packages. To build my stuff, I do "cabal install a b c d" where a b c d are all directories of cabal packages.
It's back alive.
bogphanny is talking about the missing prime in the "recursive" call, which makes it actually not recursive.
Yes, it is a regression, but I'm sure it will be fixed eventually. The improvement on rendering time however is quite dramatic: https://cloud.githubusercontent.com/assets/917945/5110544/862cda58-6fe7-11e4-8160-e5fba0d1933b.png
Plenty of libraries re-export stuff from standard libraries. It's not a "hint" as much as a convenience.
Nice. But (AFAIK) doesn't implement the important bits: the category laws.
Oops. Will be fixed, thanks!
I meant transliterate in the sense that you copy &amp; paste, say, OCaml or LISP source code and translate it to Haskell syntax. I.e. you're starting with functional code, but that starting point was meant to be used with eager evaluation. More generally, this also applies to pseudo-code, say, for Dijkstra's algorithm. If you implement it in Haskell (probably replacing mutable variable by accumulating parameters to make it more functional) you will still get the same time complexity, even if you switch from eager to lazy evaluation.
I've seen this said many times. Nothing has changed though. Why is that?
Why is foldl bad?
I tried to give an outline of how it works in [this article](http://alpmestan.com/posts/2013-10-02-oh-my-laziness.html) -- I particularly recommend reading Edward Yang's series of posts that are linked to in my article to complement it. He actually goes in depth on how the RTS makes it all work.
The lazy and strict `foldl`s do exactly the same thing, in the same order, with one exception: the lazy `foldl` will first scan the list, and build up an entire list of thunks, and once it hits the end, it will reduce, and the strict version will simply reduce as it goes. The result of a lazy `foldl`:` ((((((((0+1)+2)+3)+4)+5)+6)+7+8)` - we can see that we can't evaluate the outer thunk until we've evaluated all inner thunks, so there's no difference in order of evaluation between lazy and strict (strict `foldl` would reduce the inner expressions as it goes, instead of allowing them to build up). There's no difference in order of operations between the two, but the lazy one might overflow your stack.
Just for clarity: Does GHC currently compile all standards-compliant Haskell98 and/or Haskell2010 code? If not, is there an active project which strives for full compatibility with at least one of the standards?
I'm a similar situation : trying to switch from R to Haskell for "simple" Data analysis jobs. After looking at all the alternatives , Python Pandas, Julia, F# ... I chose haskell ;-). The problem is R is really really good at what it's supposed to do and data.frame are brilliant. I'm not sure it would have been worth switching to Python or Julia. My problem with R is when you start merging and modifying data.frames heavily you end up quickly with two many columns. This is fine in interactive mode but becomes a problem in script : you have no idea what columns is available and what's in it. Which is why I wanted to switch to proper functional langage. F# seems a reasonable solution (it has support for data.frame but it seems a bit convoluted). Anyway, I wanted secretly to do Haskell so I though F# would be compromise , so I chose Haskell. The closest I found from data.frame in Haskell is a list (or vector ?) of HRecord in conjunction with Lens. I'm not dealing with millions of data so I haven't hit so far unsolvable performance issue. It seems to cover most of the functionalities of a data.frame Examples selecting rows based on a single column predicate (the equivalent of db[db$age &gt; 30,] in R) filter (\r -&gt; r ^. age &gt; 30) db or equivalent like filter ((&gt;30).(^.age)) db selecting rows based on multiple column predicates (the equivalent of db[db$age &gt; 30 &amp;&amp; db$weight &gt;50,]) use filter creating a new hframe where a row has been modified by a function (equivalent of db$age = db$age * 2, but functionally) - or something like newframe = fmap (* 2) (frame ! "age") map (\r -&gt; r &amp; age *=2) db creating a new hframe with an added column calculated based on one or more existing columns (the equivalent of db$derived = db$age / db$weight, but functionally) map (\r -&gt; r .*. derived .=. (r ^. age / r ^. weight) .*. emptyRecord) db an example of groupBy needs to be implemented ... can be done using 'Map' Disclaimer, I'm not an Lens expert, so the code above might not work and can probably be rewritten in a better way. Also some of the code above might be worth be put in a function or a new operator. So,I'm not sure there is a real need for a data.frame structure but more and I think pretty much everything is there (HList, Lens, ReduceMap, etc ...). However, there is a need to write a library to glue them together, maybe standardize things and add the missing functions. I would definitely helps on a such library. 
Interesting content, do you have any further ideas of what you need to do to make the code prime time? Neither the post nor GitHub page seem to indicate outstanding issues, though I could be overlooking something.
The [linked article](http://brendanhay.github.io/amazonka-comprehensive-haskell-aws-client.html) in the post contains some footnotes in the conclusion about what needs to be done. Primarily it is related to ensuring the serialisation model is robust. Over the coming week(s) I'll be integrating a significant number of the libraries into the proprietary tooling we have written at work against the old version - which will be a good step towards fixing many potential issues.
A dataframe is kind of a pain to model in Haskell, they're these type-heterogeneous maps of vectors. Albeit yes, you can construct them in Haskell but they're a more natural fit within a unityped language like Julia/Python/R. For this kind of fast-and-loose interactive exploration of data having everything have the same type makes building these very heterogeneous structures more natural and pushes a lot more failures to runtime which generally doesn't matter for this problem domain.
Here's an improved version of `shell.nix` for local source packages, suggested by bennofs at #nixos: https://gist.github.com/JLimperg/82019d5603df5021603a The one from the OP doesn't handle executable packages because apparently those don't define `propagatedNativeBuildInputs`. In addition, it allows `shell.nix` to be used as input for both `nix-build` and `nix-env --install`. Don't ask me what kind of magic is being conjured here. ;)
People keep telling me that we can implement them in Haskell - can someone help me with a minimal implementation, so I can experiment with them? Thanks!
Again, I have heard this so many times, and I really appreciate it. What I was asking for in that post was for some one to help me along - I've spent a lot of time looking at different libraries, trying to figure out function signatures, approaches to heterogenous lists etc. I'll obviously not be able to come up with something that works by myself. I'm not asking anyone to write a full library for me, but even a minimally working example that I can start exploring in ghci. Thanks!
PS: When it comes to performance, I think the approach sdiehl took in using unboxed or boxed (depending on data type) vectors as the underlying structure makes a lot of sense. Most of your calculations will be column-wise (calculate an average), and in a vector the values are organized next to each other in memory, should be plenty fast and memory efficient. (You also don't do many append operations, usually just appending two large vectors etc). I'd also like to have support for enums (factors in R), which is simply a vector of Ints mapping to a lookup-table. That way you can have a column of 65,000 "Not at all" "Totally agree" etc, and it will take minimal memory space. 
Nix strongly dissuades you from using the system GHC and gcc, but it is possible to do this. I'm not sure why you'd want to, but it is possible with some effort.
You can check out [Data.Acquire](http://hackage.haskell.org/package/resourcet-1.1.2.3/docs/Data-Acquire.html), which encapsulates the concept of resource acquisition and works with either `MonadBaseControl` instances *or* `MonadResource`. It could also be easily used with the exceptions package if desired.
Wow. Comprehensive indeed.
Here is a minimum example using HRecord (from HList). The code like updating or adding a new column can propably be simplified by either creating new functions and using lens . {-# LANGUAGE DataKinds #-} -- | Example to mimic R data.frame using HList, HRecord and Lens module Main where import Data.HList name = Label :: Label "name" age = Label :: Label "age" weight = Label :: Label "weight" derived = Label :: Label "derived" peoples = [ people "John" (33 :: Int) (80 :: Double) , people "Matt" 18 65 ] where people n a w = name .=. n .*. age .=. a .*. weight .=. w .*. emptyRecord -- Define some lens -- Filter examples youngs = filter ((&gt;18).(.!. age)) peoples -- Update Columns peoples' = map convertWeight peoples where convertWeight p = weight .=. (p .!. weight)*0.1574 .@. p -- Add extra column peoples'' = map derive peoples' where derive p = (derived .=. (p .!. weight) / fromIntegral (p .!. age)) .*. p main = do display "peoples" peoples display "youngs" youngs display "peoples'" peoples' display "peoples''" peoples'' where display name l = do print name putStrLn " :" mapM_ print l 
HRecords give you extensible records ( or close enough ) but you still need to forward declare the types. &gt; people "John" (33 :: Int) (80 :: Double) How would you handle parsing an amorphous CSV file of unknown structure and unknown types into this structure? I don't think we can write a function like ``readCSV("foo.txt")`` and have it generate a well-typed HRecord in the case where we don't already know the types we're going to parse into. 
Here is the actual definition of foldl from Data.List: -- We write foldl as a non-recursive thing, so that it -- can be inlined, and then (often) strictness-analysed, -- and hence the classic space leak on foldl (+) 0 xs foldl :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b foldl f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = lgo (f z x) xs Defined this way, you can often get away with foldl, so long as the strictness analyzer does its thing. Laziness tutorials tend to pretend foldl is defined the naive way, though, because it makes such a lovely example.
One strong push back is history. The Haskell report is still ostensibly a thing and it guarantees this behavior. Unlike other breaking changes GHC makes, if it breaks compatibility with the report (this happens sometimes) all code will still compile but may now go wrong. I'm sure there's one person out there who wrote something that needs a lazy fold.
The existing frame library already does this. Using `Right frame &lt;- fromCsvHeaders "examples/titanic.csv"` from the example, it generates a structure. If you dig down into it, you find that the first column is an SBlock, which is a string etc. 
I [wrote a bit](http://jozefg.bitbucket.org/posts/2014-10-28-stg.html) about how GHC the abstract machine to implement laziness.
The example by maxigit above seems to be a really good start. I was very happy that type inference works even after deriving several new columns, it still won't apply a (+2) to a String column. I can see two approaches - for interactive use, being able to automatically derive like in the frame library is great. There isn't that big of a difference between compile-time and runtime, and it let's us quickly iterate. If we come up with something that we want to use in production, it's not too much work to specify the types, or even have it generate the type definitions from a CSV file and insert into the source code, to make sure further type errors are caught at compile time.
Are there any examples on how to build or use it?
I'm just giving here a minimal example. Some function and Haskell template helpers might be usefull indeed. However, you can't do much which csv which you don't know anything about, apart from generic operation. For this [[String]] is enough ;-)
This looks great! Thank you for doing all that hard work and congrats on open sourcing it. You mention that your workplace is a Haskell shop. Was your employer supportive of this rather ambitious project? How did the conversation go down?
The reason the frame library is able to "automatically derive" the structure is that wraps up all the values in an existential Block type which is the reason that it's able to create the structure at runtime ( all the blocks have a uniform representation ) and also the reason, like the Github comment said, that you can't get at the values since they're under and existential.
Thanks that was it for me! ;)
Love the associated types for coupling type safety with generic processes. I was expecting something very different, since it was auto-generated. Lens use is great, as is being able to carry over so much documentation. Your write up is nicely informative, and I think your next goals are spot-on. I would like to see more examples, but I think, with the way you've done this, I can take AWS examples from other langauges and translate easily. Dislike the associated types because they are a GHC extension and not part of the report, so they aren't (yet) Haskell, but I don't think there's a good Haskell2010 substitute. Dislike the dependency on conduit, since I'm more of a pipes fan, but that's really a very minor complaint.
This really looks great. Hope to examine it in more detail soon.
There's a rather brief example in the blog post: https://gist.github.com/brendanhay/a839f4829ca4e6780956 More useful examples are in the pipeline, and I'll provide a proper usage guide along with improved Haddock documentation soon.
I missed the fact that there is a repository per issue. Its kind of really hard to find that compared to a canonical repository with sub folders. Just sent a mail to /user/ezyang asking for clarification.
Thanks. I should of mentioned that despite (an older version of) this project being utilised internally, it is not in anyway associated with my employer. That said, in general they are supportive of contributions as long as no secret sauce is involved.
The conduit dependency can be removed _reasonably_ easily. It's there to provide a nicer interface for pagination and streaming request/response bodies, only http-client is needed. It's something I'll look at removing in the future.
lhs -&gt; tex/md/something sane -&gt; epub/kindle might be a bette idea right?
Given the way HTML5 has been evolving, it seems that too much rigidity might be considered a bad thing. (That is, you'd end up having to update the library regularly to follow the "living spec".)
&gt; Does GHC currently compile all standards-compliant Haskell98 and/or Haskell2010 code? There's a chapter in the GHC User's Guide describing the known [divergences from the Haskell Report]( https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/bugs-and-infelicities.html#vs-Haskell-defn). What's changing in GHC 7.10 is that the *library part* of the Haskell Report now diverges so much that it has become pointless to ship GHC 7.10 with the `haskell{2010,98}` packages. The *language part* of the Haskell Report (i.e. `-XHaskell2010`) is still in a good shape though. Sadly, that alone isn't enough for providing a standard-compliant Haskell2010 implementation.
The Haskell runtime always make me think of forth's threaded interpreter. I recall seeing one academic paper on Haskell implemented with one but I got the impression that's not how GCH works. Anyone know of any blog posts about implementing laziness with threaded interpreters?
Really nice name!
That could be right. The non-CPS version would do an extra return and bind I guess, relative to the CPS version. I wonder if the codensity monad or similar could give you that for free, though.
For blaze-as-combinators, lucid sounds like a great improvement. But we don't use blaze that way. We have tons of indirect dependencies on blaze, and all of them use blaze strictly for super fast type-safe HTML rendering via the monoid instance. So for us, "speed...it’s not far from blaze" is very disappointing. Wouldn't it make sense for lucid and blaze-html to share the same underlying rendering engine?
Judging by his SO question it looks like he's after some algorithm that verifies the laws. It seems like he's implemented functions that will do a brute-force check of the laws for all morphisms in the category. This is fine for finite categories (that are not too big!), but the interesting ones tend to be infinite, and therefore cannot be checked mechanically. You will need to construct a proof of that, and will therefore have to use something like Coq.
Was thinking about some way to avoid significant whitespace or a lot of parentheses to make this more palatable to non-haskellers, and I reread on [poly-variadic functions](http://okmij.org/ftp/Haskell/polyvariadic.html). With that you can achieve something that looks like: example :: Nodes example = build html_ div_ p_ "This is some text" _p _div div_ (id_ "main") p_ "This is some more text" _p p_ "This is the last paragraph" _p _div _html Using functions as delimeters rather than whitespace or parentheses. This is closer to how HTML actually does it (with &lt;html&gt; and &lt;/html&gt;). Additionally, doing it this way it's easy to make pre-delimited versions of the functions that do require parentheses if that's more convenient. That is, given li_ and _li you can write: enlist :: IsNodes n =&gt; n -&gt; Nodes enlist t = build li_ t _li mylist :: Nodes mylist = build html_ ul_ (fmap enlist ["Hi", "Bye", "Sigh"]) _ul _html show mylist yields "&lt;html&gt;&lt;ul&gt;&lt;li&gt;Hi&lt;/li&gt;&lt;li&gt;Bye&lt;/li&gt;&lt;li&gt;Sigh&lt;/li&gt;&lt;/ul&gt;&lt;/html&gt;". The major disadvantage is that the burden of matching tags has reappeared, and my current scratch implementation wouldn't catch mis-matched tags or misplaced attributes until runtime, so you might just be better off typing the HTML directly. One neat thing is that this is all Haskell 98. Anyway, just thought I'd mention it. Here's the (very haphazard) [gist](https://gist.github.com/josuf107/d1e5bd994422d53f66f7). It's possible this idea could be improved to add some type safety.
I see you're using type families similarly to how hackage's aws does, which is nice. I've recently moved to aws from S3, and I can see eventually moving to this, or perhaps aws using it underneath. I hate to be That Guy, but I'm concerned about copyright issues, specifically with the auto-imported documentation. I suppose those texts are copyrighted by Amazon, and presumably they did not release them under the MPL licence you've applied to this work. Have you considered this issue?
This is a classic example of an algorithm that is difficult to implement idiomatically in Haskell, but is the best solution for the problem and seems like it should be easy to do. It's no wonder you're struggling. I have rewritten a similar version based on this one: http://codepad.org/KaFu41TO Somewhere in my old files I have an implementation of this algorithm for `Data.Vector` and `Data.Random` that wasn't that much more elegant than this one. This is just a problem that Haskell is genuinely bad at dealing with.
I don't think Ashe, Ada Initiative, or any of the 3rd wave "so-called" feminists are the answer. Most of them are dealing with deep seated psychological issues and have their own problems that they project onto the tech community. Their philosophy is essentially that women are so weakened by constant oppression and the patriarchy that we must be handled with kid gloves. By including these philosophies in your community, you would create a hostile environment that would ultimately isolate women because men would be too afraid to interact with them for fear of damaging them psychologically.
This is really helpful, thank you.
A lot of communities have this problem and Haskell is not unique. My suggestions would be as follows: Form a "Haskell Women" group (maybe Lambda Ladies is the way to go here) that has a wiki and an irc channel (and chat doesn't have to be limited to haskell only topics). Debian and Ubuntu for instance have women focused groups. The goals of the group could include outreach and mentoring new women and also being proactive about owning some aspects of Haskell whether it's some part of the core language, libraries, major code projects, or something else Haskell code related. None of these groups need be "women only" but they should be "women focused". Women who are already established Haskell programmers should mentor new women. Whether there's something formal in place or it just happens via irc, it doesn't matter, but women interested in getting involved in the community should have the opportunity to talk to other women (not that there's anything wrong with talking to men but the #haskell channel can be a little overwhelming ;) Finally, women involved with Haskell should make an effort to speak and present as conferences. The onus is on those of us who have pioneered and forged our ways into communities to make ourselves visible and to ensure our voices are heard. I've only just started learning Haskell myself and I'm already thinking about talks I can submit to conferences. What not to do? Make a woman joining the community feel like she's getting special treatment because it's more important that she's a WOMAN and less important that she's interested in Haskell. Constantly remark on how great it is to have a WOMAN speaker at a conference where she was the only woman speaker (instead of commenting on her talk). Don't make it about her being a her, make it about her being a peer. And if you witness someone being a jerk to her, you don't necessarily need to "rescue" her but show some kind of support. This can even just be a passing comment after the fact like "hey I overheard what happened and wow that person was a jerk". Just make her feel like she has an ally, like you would with anyone regardless of their gender. Most competent programmers are less interested in the geek feminism nonsensical chatter and more interested in being engineers. I'm not saying that discussing and understanding social issues isn't important but in the end it's about writing code, so the focus should instead be on thinking about enabling potential contributors to do that.
ok, so what's the way to distinguish between bytes representing Windows-1252 text and representing ISO-8859-1 text? Sometimes our data _does_ have the exact same "representation space" but a potentially very different meaning.
You get less row transposition errors if you have foo :: Email -&gt; FirstName -&gt; LastName -&gt; IO () vs. foo :: String -&gt; String -&gt; String -&gt; IO ()
The blogpost says: &gt; the function `foldl` is prone to space leaks. You should use `foldl'` or `foldr` instead. I understand why stict-foldl is better, but I don't understand why `foldr` is any less prone to space leaks than `foldl` is. Wouldn't it also build up a huge thunk to be reduced at the very end (in the same way you describe `foldl` doing)?
Or, e.g., a lorentz transform matrix vs a spinor matrix.
The argument that newtype is feeble because of isomorphism doesn't hold water because every countable set is isomorphic. *String*, *Integer* and *(String, Integer)* are all (morally) isomorphic. Yet no one is writing "*(String,Integer)* Isn't As Cool As You Think" posts.
He seems to be arguing for the "make illegal states not representable" approach. I agree that’s the ideal solution, but still, using a newtype is so much easier. It’s nice to see Haskell-related stuff pop up in dzone.
&gt; Lazy evaluation evaluates arguments from left to right, so we reduce the leftmost redex I don't think this is correct…the left redex is only reduced first because of the definition of (&amp;&amp;) which cases on the first argument first: (&amp;&amp;) x y = case x of { False -&gt; False; True -&gt; y } If we cased on the second argument first, (&amp;&amp;) x y = case y of { False -&gt; False; True -&gt; x } then we would reduce the "right" redex first in your example.
It might be that the new features in Java 8 (lambdas, streams, some monads and monoids popping in the standard library) have increased interest in functional programming techniques. Here's a nice talk on the subject: http://www.infoq.com/presentations/fp-principles-oop
Only on r/haskell :)
Upvoting. Not because I agree with the content of the post - I don't. But because the post is interesting, especially in light of where it was posted. Many other Haskellers would likely be interested in knowing that this post exists. Downvotes are for filtering out noise from our channel, such as spam, trolls, off-topic, etc. This post, although misguided, is none of those things.
John seems to assume that `hClose` should be wrapped into `uninterruptibleMask_` as a whole. But AFAIK that is not true, it is enough to wrap `takeMVar` [here](https://github.com/ghc/ghc/blob/453ce626a32cab3728a640b2299eaeeb30da8862/libraries/base/GHC/IO/Handle/Internals.hs#L163) and one other place, is it? (John, sorry if I misread you here.)
Agreed, it does sound like that's what John is arguing for. "Make illegal states not representable" is *not* the ideal solution. The ideal solution is "precise denotational semantics". "Make illegal states not representable" is a second-rate compromise. The examples in the post itself are good illustration of that, and they directly refute the claims in the post. A general string is not semantically the same as a string that was supplied by a user as an email address but that is still unparsed and unverified, even though their underlying sets of possible values are isomorphic. It's important and worthwhile to represent that semantic difference in your types, as described by other posters here.
It does: however, a strict `foldr` may not maintain the same order of operations. The discussion tends to be just between `foldl` and `foldl'` because those two are equivalent modulo strictness.
 I personally never argued against `uninterruptibleMask` in itself. But I find that the motivation of the proposal is wrong. (Especially "almost every usage of bracket is buggy" part) Everybody think that async exceptions are hard to handle in cleanups, but I don't see anything special in them. Yes, they add additional [complexity](https://github.com/Yuras/io-region/wiki/Handling-%28async%29-exceptions-in-haskell:-the-right-way#asynchronous-exceptions), because it is yet another case to worry about. But if you know how to handle sync exceptions, then you know how to handle async exceptions in cleanup. Let me quote /u/simonmar and SPJ (from "Asynchronous Exceptions in Haskell" work): &gt; Although it seems strange that operations inside `block` may raise asynchronous exceptions, the exceptions are *synchronous in nature* since we specify exactly which operations are interruptible. I'm studding source code of some widely used haskell libraries (snap, warp, async), and I see that async-only mistakes are rare compared to general exception handling mistakes. Also user is still have to worry about async exceptions in acquire actions. And the situation here is dual, one can replicate most (all?) of the issues we see in cleanups. But the point is a bit different. Exception handling in haskell is [broken](https://github.com/Yuras/io-region/wiki/Handling-%28async%29-exceptions-in-haskell:-pushing-bracket-to-the-limits), and we need to unbreak it. The proposal doesn't do that. We should make exception safe code possible in haskell first, and only then try to simplify exception handling. 
Many thanks to CJ van den Berg and Joey Hess for their pioneering work on writing the first build scripts. Putting together the build script in Docker was a very pleasant experience. I broke the script up into many small pieces so that if one particular part failed I could simply go back to the last checkpoint and start from there. Once I had finished the script, I also had very high assurance that it would work with no problems whatsoever when I ran it again from scratch. Normally putting together build scripts is difficult because of the inherently stateful nature of a file system. If you are working on a build script that takes a long time to run the temptation is not to start again from scratch every time something goes wrong. The turn-around times are simply too long. You can't be waiting 50 minutes every time you make a small change. Therein lies madness. But because Docker uses a copy-on-write filesystem under the hood, the filesystem is really a kind of persistent data structure. When one particular part of the script failed I was able to go back to *exactly* the state it was in before it failed. The boon to my productivity and my sanity was immense.
Yes, you are right (so the "unless you exactly know what you are doing" note in the post). But I found such useless `mask_` pretty common. I even saw it in `base` library. 
I thought it was going to be about [newtypes preventing optimizations](http://stackoverflow.com/questions/27014235/can-fusion-see-through-newtype-wrappers/27015589#27015589) despite the wiki saying that "newtypes cost nothing".
This looks really cool. Have you considered putting a prebuilt image on Docker Hub?
This article was really clear. There was an article on HN today covering similar material, but it didn't seem like the author really cared if you understood. Kudos!
I think perhaps "mask*" should be called: "interruptibleMask", and "uninterruptibleMask" should be called "mask". Either have no default, or make the default of a function named "mask" actually mask, no exceptions.
&gt; Especially "almost every usage of bracket is buggy" part If you change it to "almost every usage of bracket *with interruptible cleanups* is buggy, then it is correct. &gt; but I don't see anything special in them. They can happen whenever something interruptible is called, even if all preconditions are met to eliminate all possible sync exceptions. Not to mention that either an async exception or a sync exception in that context is often catastrophic -- and the difference becomes the *possibility* to eliminate the much-more-frequent async catastrophes and inability to do the same with the sync ones. &gt; But if you know how to handle sync exceptions But that's the point: in cleanups, *you don't*. &gt; I see that async-only mistakes are rare compared to general exception handling mistakes. There are many kinds of bugs - and perhaps some bugs are more frequent than async-exception-in-cleanups bugs, but that doesn't mean we don't want to eliminate async exception in cleanups bugs. &gt; Also user is still have to worry about async exceptions in acquire actions For acquire, all you need is strong exception safety. All actions should strive to give this and it's usually easy to give. For cleanup, you want something as close as possible to nothrow, and it's much harder to give and much much harder to handle the breakage of that. They're not symmetric or dual. &gt; Exception handling in haskell is broken Only insofar as exception handling is broken in any language. The different behavior of program termination at cleanup exception vs. letting it propagate is sometimes better, sometimes worse, depending on circumstance. There's no clear winner. &gt; We should make exception safe code possible in haskell first, and only then try to simplify exception handling This is not always possible - but we sure as hell can reduce the huge source of exception-unsafety we have lying around: async exceptions are cleanups.
I'll put one up tomorrow. It compresses down to 750M but it's a few gig otherwise and for some reason "docker push" doesn't seem to use compression. I'll let work absorb that upload cost :-) 
using some existential GADT wrappers you can come up with the well-typed structure above. I can't sketch a complete example now, but the gist is data AnyCSV where AnyCSV :: Vector (Record l) -&gt; AnyCSV parseCSV :: FilePath -&gt; IO AnyCSV useCSV :: Vector (Record l) -&gt; IO () useUnknownCSV = parseCSV &gt;&gt;= \case AnyCSV frame -&gt; useCSV frame
I'd like to add that hClose closing the handle even in the event of exception is the very opposite of exception safety. And that you might want to be able to do: bracket (hClose ...) (openFile ..) And if an exception interrupts the hClose, you'd rather it kept the handle open. hClose isn't inherently a cleanup handler.
`Sum Int` can be thought of as simply the name for the `Int, (+) and 0` triplet. Sure, it's an encoding, but not a particularly cumbersome one. In fact, it's quite light-weight. The nice thing about this encoding is the global coherence you get for free, when you associate the type with the triplet, rather than generating the triplet dynamically.
&gt; You can't fold a list "from the right". LYAH is guilty of this mistake as well, and it costs a lot of time to un-teach this train of thought for beginners. Would you mind expanding on that? I'm a beginner Haskeller and I'd really like to know.
That seems really great, thanks! I have been using more and more Docker images as battery-included compilers (e.g. GHC 7.8.3 + Haste, or GHC 7.4.1 + a set of packages for a long-lived project, or JDK + Maven, or JDK + Android SDK, ...) and your image seems a great opportunity. Still I have no idea what would be the next step in using your image. Is it to write a regular Android application that calls into the (compiled) Haskell code ? Or is enough of the Android SDK exposed to Haskell to write everything in Haskell ? Is there an example somewhere ? Edit: Looking at android-haskell-activity, it seems that you have to write a minimal wrapper Android application that calls into your Haskell code (using JNI). Then you can do everything you want in Haskell, including using everything that is available to a regular Android application (using again JNI). Is that still the recommended approach these days ?
`docker push` pushes gzipped layers, although the reported size during progress is the uncompressed one.
DockerHub can build it for you, if you set up a hook to the github repo
It's sad, but many people don't seem to understand what downvotes are here for. This is a controversial post, therefore interesting for me. 
It's not clear how you then use some column out of your CSV ?
&gt; That is, creating a data model such that there exists no regions in our data’s state space which correspond to invalid states. Personally, I think a data model with no invalid states is impractical. 1. Ensuring that the data model precisely matches the set of valid states adds extra complexity to the code, depending on how complex the definition of a valid state is. 2. Pushing things to an absurd extreme, some aspects of what constitutes a valid state cannot be defined at compile-time anyway. In the example of e-mail addresses, for example, you could go to the extreme of requiring that the address must actually exist - your source code would have to have a database of all extant e-mail addresses as part of the type definition, and even then couldn't reflect changes over time. 3. In practice you always need a data model that allows invalid states anyway. In the e-mail example you eventually have to make up the full e-mail to send, including all headers in their correct format - a string, including e-mail addresses as substrings. 4. Simple representations with redundancy can be useful, e.g. for spotting errors earlier. 5. By making the code more aware of what is valid, you're committing yourself to a particular set of validity rules, potentially making it harder to adapt to future changes in those rules. I'm not arguing against reducing the invalid states in data models here. That's usually a good thing. In fact here are counter-arguments to my arguments... 1. The data model usually doesn't need to be complex to eliminate a lot of invalid states. 2. As it says, this is an absurd extreme. 3. It's still worth minimizing the amount of the code where those more redundant data models apply - convert inputs early, convert outputs late, so most of the code never sees the I/O representation. 4. This is particularly dubious as the errors that are easiest to spot are those that couldn't have happened if it were impossible to have an invalid representation - or at least the compiler would have spotted them as early as possible anyway. 5. Modularity localizes the parts of the code that can be dependent on that data model used anyway. Looking at the e-mail address example, presumably we want a model where it's impossible to have a **lexically** invalid e-mail address. That means we have a data model representing the information provided by a [standard e-mail address as defined by RFC2822](http://tools.ietf.org/html/rfc2822#section-3.4.1), which replaced RFC822. Those rules aren't exactly complex, but there's still scope for making the data model too complex in order to eliminate invalid states. Does our program really need to know the internet domain? Do we really need to pass around a representation where invalid characters are impossible? (I'm not sure, but I'm guessing only one `@` is permitted - do we want a compressed representation of that purely to avoid having characters that can represent `@`?) Even if we compress, are we going to compress so perfectly that the compression uses precisely some whole number of bytes (thus ensuring there's no redundancy for unused bits). And RFC822 has already been replaced with RFC2822 - if we make our code too fussy, might it break if there's a further future change? Obviously elimination of invalid states is balanced against keeping things simple and reasonably future-proof. Separating the components of an e-mail address may or may not be a good idea for our application, but even split into components, those components will be simple strings irrespective of the fact a simple string may be able to represent values that are invalid for those components. That may mean we have *more* `newtype`s, not fewer - one for each component. 
I want to know as well.
Hmm, I thought we already agreed here. To make `hClose` useful outside cleanup, it should not close the handle in case of failure (and I was wrong claiming the opposite). But then it will not be usable in cleanups. Probably we need both functions.
&gt; If you change it to "almost every usage of bracket with interruptible cleanups is buggy, then it is correct. I don't have comprehensive statistics, so I can't argue. But it is definitely not correct for code I studied so far. Do you have other statistics? &gt; even if all preconditions are met to eliminate all possible sync exceptions It is impossible in most actions. But if it is possible, then wrapping it into `uninterruptibleMask_` is simply an additional precondition, that is trivial to satisfy.
I assumed he actually meant that the space complexity makes it infeasible, and that it doesn't convey any advantage, not that it was technically impossible.
Hackage uses HsColour. I always liked HsColour's scheme. It is minimal and highlights little other than keywords (the snippet you picked does not showcase this.) The Sublime snippet you have looks like a coloring book. Worse, it takes all the comments and melds them into the background. My biggest complaint about syntax highlighting schemes are that too many of them make comments look unimportant. Github is also guilty of this. HsColour leaves comments prominent.
Thank you very much, that answers this part of the question completely. :) I'm currently debating with myself whether a spec that nobody really tries to faithfully implement is of any use, and whether this makes the GHC manual a de-facto Haskell specification. If nothing else, I guess the standard can serve as unusually thorough documentation in those places where it still applies.
I've fantasized about it myself, but keep in mind, we're talking about some big projects. [Pandas](https://github.com/pydata/pandas) weighs in at around 11k commits and is only a small part of the NumPy/SciPy ecosystem. [Julia](https://github.com/JuliaLang/julia) is north of 21k commits and again is supported by a large package ecosystem, like the 1.2k-commit [DataFrames](https://github.com/JuliaStats/DataFrames.jl) package. [R](https://github.com/wch/r-source) has been around for a while and includes in its base a lot of what the others throw in other modules, so no surprise it's pushing 47k commits. That's more than [GHC](https://github.com/ghc/ghc). So yeah, I'd like to get some better numerical/statistical analysis tools going in Haskell, but I'm not up to the task.
`foldr` *associates* the operation to the right, but does not *evaluate* from the right. You can only traverse a list from the left, because that's how it's structured. It's the same reasoning as why `head` is O(1) but `last` is O(n).
Great post, thx!
 &gt;"Make illegal states not representable" is *not* the ideal solution. The ideal solution is "precise denotational semantics". "Make illegal states not representable" is a second-rate compromise. Can you elaborate? Isn't one simply the inverse of the other? 
Thanks, it really helped alot...i wanted to know how the code works and write a simpler version but it seems, like you said, difficult in Haskell. so i will just try and learn all i can about monads and it applications. maybe i'll get a better understanding when i do.
Saying "qualifying the import means that we limit what we import to specific values" does not really make sense to me.
You also get fewer row transposition errors if you have keyword only arguments, even without types. For example, in Smalltalk, the method would be named `fooWithEmail:firstName:lastName` and must be called as `fooWithEmail: someEmail firstName: aName lastName: anotherName`. Unfortunately, Smalltalk keeps all the downsides of positional arguments.
&gt; I'm not sure, but I'm guessing only one @ is permitted Additional "@"s are permitted if they're backslash-escaped, at least. &gt; those components will be simple strings irrespective of the fact a simple string may be able to represent values that are invalid for those components. I was actually thinking how I'd solve the problem; rather than a `newtype` per se, I'd use a dependent pair of a string and a validity proof (I'm thinking of doing this in Idris).
I'm not sure what your point is, and I find your wording ("doesn't hold water") to be a bit dismissive: - your point about finite types being isomorphic feels a bit like nitpicking; I think we agree that the author meant "there is a very easy and canonical way to go from one to another that programmers may perform without thinking much of it", which certainly isn't the case of the examples you give. Also his point about newtype also work for finitely inhabited type, while your comment about isomorphism does not. - The author would like to use abstract types (in the ML sense, which I suppose would be realized here by abstracting over a type variable and having a type-class constraint or set-of-operations passed for it) instead of newtypes to model emails. That would be an excellent example where the types are *not* isomorphic, in the sense that you cannot write conversion functions back and forth between `String` and a given universal parameter `a` that compose to the identity.
I feel like this is a very well-written post with a link-bait title. The basic argument is that "If you goal is to use types to ensure that you cannot write invalid code then `newtype` is essentially the most feeble tool in your toolbox". I completely agree with that, especially if you're already on record as feeling uncomfortable with typeclasses. In a more refined language, we'd be able to model what we really want. Given a predicate `IsEmail : string -&gt; Pred` the Coq type Inductive Email : Set := mkEmail : forall (s : string), IsEmail s -&gt; Email (* or, noting that IsEmail covers everything one could care about the newtype: Email : Set Email = { x | IsEmail x } *) is exactly what we want. We literally cannot construct it with an invalid string and every isomorphism must explicitly be one which preserves the `IsEmail` proof. Thew "newtype skeleton" inside of this is absurdly weak. On the other hand, newtypes in Haskell have the interesting mechanism of being able to expose different typeclass functionality. This is important because we often reason about a type and some subset of its associated method algebra together. For instance, `[]` is a type and we might think of list isomorphisms like `[Bool] -&gt; Integer`, but we also often want to talk about `([], return, join)` as a monad and then think about monad morphisms which might exist. A newtype exists here, `Omega` which changes the nondeterminism of `[]` to be breadth-first instead of depth-first. For finite searches, `Omega` is probably going to have the richer structure of list-monad-isomorphism and the `newtype` helps us both to reason about that and to actually represent it since `instance Monad Omega` will differ from `instance Monad []`.
Yes, I am in the keyword camp too.
Agreed. I upvoted for the same reason.
I had a look at it, but I didn't see anything about "expiry", I mean way to invalid cache after a certain about of time.
&gt; But it is definitely not correct for code I studied so far Can you show which code? Because `withFile`, for example, is broken if you have strong exception safety from `hClose` (no effect in case of exception). `withMVar` is broken if you want to allow the reverse bracket (`bracket (putMVar ..) (takeMVar ..)`). Can you show me a non-broken example? &gt; It is impossible in most actions. Sure, but then for these remaining cases you can't cover, you typically just have either bugs or very complicated non-local handlers. &gt; But if it is possible, then wrapping it into uninterruptibleMask_ is simply an additional precondition, that is trivial to satisfy You can view it this way, but this means you have to satisfy this precondition *in every potentially interruptible cleanup*. You virtually never want to not satisfy it (in fact, I haven't seen a single compelling example of such a case yet). So you're just adding a precondition to all brackets everywhere that you must satisfy or get bugs and of course you will get these bugs.
Oh, then you agree you want uninterruptibleMask in withFile too. Do you have any example where you don't want uninterruptibleMask in bracket?
That's a good point. I was thinking of "outermost leftmost", but that's not entirely true with modern Haskell anymore. The following definition zap x [] = [] zap [] y = [] zap (x:xs) (y:ys) = (x,y) : zap xs ys will give &gt; zap undefined [] [] &gt; zap [] undefined *** Exception: Prelude.undefined which means that the equations are matched top to bottom. 
&gt; The base package cannot be used because it has problems like Applicative is not a superclass of Monad, Monad has fail, etc. If you're aware of those issues, can't you just avoid including them when you are using base as a reference?
 `withFile "test.txt" AppedMode $ \h -&gt; hPutStrLn h "hello"` It is still problematic w.r.t. double throw, but `uninterruptibleMask_` on itself doesn't solve it. The proposal may (or may not) be a step into right direction, but we need to agree first what direction is right. I wrote special post about my understanding of the issue. It is probably wrong, but you didn't comment it. 
It's not technically impossible, but there are no standard functions that do so. If you wanted to fold a list from the right, the proper way would be to compose a fold with `reverse`.
This is quite easy in R as well. Just replace file name with 'clipboard' with any function reading data as follows x &lt;- read.csv('clipboard') y &lt;- scan('clipboard') 
Most of the type classes correspond to some category theory stuff, that's probably the closest you can get to being "correct". 
Ah yes. No products to offer for this in my jacket sorry. I know that, for web applications, the package MFlow has a monad that stores execution state in a file and has a timeout to discard this file state with setTimeouts. 
I'm planning to wrap everything up in a web server anyway, so this might be interesting. Thanks
If you get a chance to sketch up a minimal runnable example, that would be very helpful!
You actually don't need to know the types at compile time, it infers them (at least in GHCi, which is my concern right now). Wanting to write typesafe code is one of many reasons I am using Haskell, but it's not so important in interactive use, because there the difference between compile-time and runtime is much less. One of the attractions of using Haskell for this (apart from the really great tools for manipulating data tersely etc) is that it would be much easier to turn exploratory code into production code. And for production code, I am very happy to specify the types of the CSV file (as long as it is as little boiler plate as possible). Many of the CSV files I am working with (questionnaire data) have 30-40 columns, and having to manually specify that every time I want to open it in IHaskell and do some quick exploration makes it completely unfeasible. 
IANAL, but I doubt that the law has a requirement that all contents of a single file be all under one single license. If that were the case, tarballs would be trouble... In the unlikely situation where this somehow landed before a judge, I'd expect this legal question to be dealt with on a case-by-case basis, taking into account things like authors' intents and whether the two parts of the file form a distinct creative and thus copyrightable work, or are separate works that just happen to be sitting next to one-another, as it were. Which goes to say, this seems like tricky and befuddling ground to tread on. Probably not the kind of thing you want users to worry about when using your library. :) Apache 2.0 would be ok for me, since I'm using BSD and GPL 3 for my code, which are both compatible with it. Although there is the issue that Apache 2.0 is not compatible with GPL 2 licenced code. https://www.apache.org/licenses/GPL-compatibility.html
&gt; Ultimately, however, I think a lot of problems solved with newtypes (modeling coordinates, positions, emails, etc.) are better solved by more precise data modeling. That is, by making our programs stop lying about isomorphisms. It sounds like this is a desire for dependant types, but we don't want to lose our type inference. Which we might get at some point: https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell I can not find the recent reddit post about the possibility of adding the feature in one of the next few ghc releases though.
Agreed, dependent types (and to a less extent GADTs) are smart data and one way to satisfy the desires of the article. The author should (if they have not already), play around in Idris / Agda. In particular, they should implement at least one instance of the "smart data" they want. I *think* you'll find that smart constructors get you 80% of the way "there" with 20% of the effort. It is a little awkward to convert between smart constructors and smart data, but it is something you can transition to for the place where smart constructors are good enough. I do wonder how either / any of these techniques deal with single-bit errors in non-ECC RAM, which happen about once every 100k hours.
Neither does the author of whatever implemented solution you find. :)
Funny, I just posted http://joeyh.name/blog/entry/propelling_containers/ the other day which includes a jab at "weirdly underpowered dockerfiles". I had not considered using them this way. A DSL for generating dockerfiles might be useful.. It would probably reduce your dockerfile to a few dozen lines of haskell code mapping over the build scriptlets. BTW, did you have any trouble with the ghc build sometimes hanging with make stuck with several zombie processes? I have a weird problem with it in ghc-android, which I worked around with commit 8560eda0d04a075ef2f6246dc8e6b2bf5db8768b
Agreed. It's incorrect. When you `import qualified Long.Module.Name as Alias` you get everything exported from Long.Module.Name available as `Alias.exported`. When you `import qualified Long.Module.Name` it is equivalent to `import qualified Long.Module.Name as Long.Module.Name`. Explicit imports is the only way to "limit what we import". The presence of `qualified` does prevent the imports from being available using an unqualified name, but I would argue that `Prelude.map` and `map` are just two different names for the same thing. So, if you imported Prelude qualified as `map` was not available you still have import the same object but only be able to access it using the name `Prelude.map`.
The reference to the source file boundary was from this [Wikipedia FWIW](http://en.wikipedia.org/wiki/Mozilla_Public_License#Compatibility_with_other_licenses) excerpt: "The MPL treats the source code file as the boundary between MPL-licensed and proprietary parts, meaning that all or none of the code in a given source file falls under the MPL." Seems quite explicit about the two parts of the file (documentation &lt;-&gt; haskell) being potentially incompatible.
I don't think there a rule what "downvotes are for". That means that de facto they're just for influencing what is shown/appreciated on a certain subreddit. So I think it's legitimate if people downvote this article if they feel the quality is low enough not to want it here.
It sounds like the author is pretty black-and-white in his statement that there are (always?) better solutions than newtypes. In practice, there's a whole spectrum. You can: * use an imprecise type like `String` * use a type synonym (which at least gives you a hint about e.g. argument order). * use a newtype, which makes certain bugs compile time errors, but at the expense of some tedious wrapping/unwrapping. * use a type that makes incorrect state non-representable. This in itself is a whole spectrum, since you can make anything from 0-100% of the illegal state non-representable. Depending on the complexity of your domain, this option is often more work, both in coding up the representation, and in usage. In practice, each time you want to represent some data, you have to weight the importance of ruling out bugs against the extra effort required. Different tools for different situations.
I don't know of anything that fits the bill precisely, but it also seems very easy to roll out of existing tools...
Cool! It strikes me that the level of detail is a bit uneven: at the beginning you talk explain what a qualified import is, and later you seem to assume that the reader's already familiar with monad transformers. You do more outreach than I do, but I would think most people would have imports down by the time they start stacking monads.
&gt; I do wonder how either / any of these techniques deal with single-bit errors in non-ECC RAM, which happen about once every 100k hours. I see this question pop up every once in a while, but I do not understand why. You can just add some data redundancy in the data structure and use an error correcting code (ECC) in software to reduce the probability of a problem caused by bit errors nearly arbitrarily. The above solution seems like it should be possible to implement and the first experiment I would try to solve the problem. Are there issues that I am not grasping that makes this fail?
&gt; You can just add some data redundancy in the data structure and use an error correcting code (ECC) in software to reduce the probability of a problem caused by bit errors nearly arbitrarily. I think you often end up in a situation where this is in conflict the the goal of making invalid states unrepresentable, but maybe that's not completely a conflict. Imagine storing a mod128 number. You store it in the high bites of a byte and use the low bit as a parity. Using either smart data or smart constructors allows you to elide the parity bit; it's even imaginable that a sufficiently "smart" compiler preforms erasure on the parity because your smart data never needs to access it / it's value is "only" used for type checking. The general idea is that if you erase all the invalid states and then minimize the use of storage, you end up with no redundancy with which to "absorb" single bit errors.
This sounds good in theory but really annoying in the absence of effective tooling to automatically insert the scaffolding for you. It also makes changing internal argument names a cross-module breakage. I guess the alternative once you've accepted this is to preserve the original name but define an alias at the start of the body? It's one of those Nice Things that pushes a language towards being IDE-write-only.
you can have keywords and order based application
But if everyone uses order-based application, it's the same as not having keywords. And if anyone uses keywords, you can't change your internal names without breaking their code. Did I misunderstand something?
I know this is beside the point, but you should avoid first and last names: http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/
I tried that but since it takes so long it timed out.
Yeah the idea of folding from the right caused me a lot of mental grief when things got more complicated and my mental model was incorrect
Fixed. I shouldn't edit at 1 am. Thanks for catching that!
&gt; But if everyone uses order-based application, it's the same as not having keywords. I think you are correct, but you could make it a warning "Potential transposition error. Order based application used with in foo where arguments x and y have duplicate types!" And you could provide a pragma for functions that are commutative. The compiler could even infer commutativity sometimes, like if only https://hackage.haskell.org/package/algebra-4.1/docs/Numeric-Algebra-Commutative.html operations are used. More reason I am excited about the combo between the preprocessor and type checker plugins. &gt; And if anyone uses keywords, you can't change your internal names without breaking their code. Can you provide an example?
Thanks noteed, that's good to know.
This is really neat work but very, ahem, dense. An easier introduction to the approach taken (by the same author) is here: http://www.iis.sinica.edu.tw/~scm/2010/maximum-segment-sum-origin-and-derivation/
I had a look at your commit. It looks like it's happening when you're building iconv. Is that right? No, I had not run into this problem. I was using the 'stable-ghc-snapshot' branch not 'jessie-ghc-snapshot'. I agree with you that the Dockerfile are a perhaps a bit underpowered. Personally, I'm not wedded to Docker. If there was another fairly easy-to-use system for checkpointing the filesystem and rolling back, that also allowed me to inject new scriptlets into the filesystem and run them, then I would have used that. It occurs to me that such a system wouldn't be too hard to build using debootstrap and btrfs (or some other copy-on-write filesystem).
I think that /u/mypetclone is assuming Python-style keywords, where the keyword and the formal parameter are necessarily the same.
gregwebs just opened an issue on the Github repo that said: "If you ADD everything at once and chain commands together with &amp;&amp; instead of doing many RUNs, the resulting image may be a lot smaller." I thought I'd respond here too, since it was kind of important to my methodology. There's actually a really good reason I decided not to do this. I was developing the build script as I went along. (More precisely I was chopping up a large build script built by others). I wanted to make sure that each part of the script worked without failure before moving on to the next. And because some of the parts took upwards of 30 minutes I didn't want to force Docker to ever have to go back to a checkpoint *before* one of those long builds. Unfortunately, if I went back to an ADD command and now added a new file (or it was an ADD command using a wildcard e.g. "ADD user-scripts/* ...") then Docker (correctly) detected that this was different to the command I ran before and started the build process from that checkpoint even though several long running actions had successfully built afterwards. I just really, really, really didn't want to do this so I sacrificed build script elegance (and now I find image size) for my sanity. If you look carefully I also had to beef up the environment one more time with set-env-1.sh which just sources set-env.sh. Of course this is inelegant. But it meant I didn't have to sit through 45 minutes of building again, which was a small price to pay. Personally, once the image is built I don't care about the intermediate images, but I don't want to edit the Dockerfile because I want this script to be somewhat futureproof. I want people to be able to roll back, edit the scriptlet that is giving them grief, and run from that point again which people won't be able to do if it's all compressed into one big "RUN" command connected with the "&amp;&amp;" operator.
What's the benefit, though?
In my particular case I'm deploying a game written in Haskell using SDL2 and Cairo. It is true that there is a Java wrapper which calls into Haskell code compiled as a library, but the Haskell code never invokes the JNI. It runs natively. Setting up a build environment was a real pain. I create an NDK standalone toolchain and then built a number of C libraries: pixman, cairo, SDL2, etc and installed them into the toolchain along with GHC and all its associated libraries. But the short answer is: you can keep the JNI stuff to an absolute minimum if you're careful.
Awesome to see docker really take off lately. What does "official" mean in this context (I see this person's image has been added to a project by github user `docker-library`)?
Being able to build locally, regardless of variant of linux, and push to a remote machine, again regardless of variant of linux? Without this, your options are: 1. Build on the exact same distribution (which realistically means you have to build inside a virtual machine, which is a pain). 2. Building on the machine in question (which is a terrible idea - GHC linking can easily take 1-2GB, which means you need that much reserve capacity on the machine in question).
I've decided to "flatten" my image. I'm exporting the final image and I'm going to create a new "flattened" image that just consists of me adding all the files from the exported image into a new one. It will be much smaller and useful for those who just want to download and run the damn thing.
This is actually a blog post from http://degoes.net/articles/newtypes-suck/ where it is better formatted and less surrounded in crud than the DZone republishing.
"Official" means [signed](http://blog.docker.com/2014/10/docker-1-3-signed-images-process-injection-security-options-mac-shared-directories/) by Docker and vetted through the process required to get into the `docker-library` repo. Some official repos (such as node's) are maintained by the people you would expect (Joyent in this case). Haskell's currently is maintained by the community. Official images can be pulled without a prefix. `docker pull haskell:7.8` instead of `docker pull someuser/haskell:7.8` and you should see a verification in the CLI that it's signed.
In addition to @dbpatterson, it can be used for things like [basing a portable emacs off of](https://github.com/ChristopherBiscardi/docker-emacs), to test against multiple GHC versions for CI or to provide a clean, quickly bootable environment for ghci sessions or running tests. I actually don't have haskell installed on the host that I do a bunch of my haskell development on these days. It's all docker images. 
Got it, thanks for the explanation. At work, I take environment uniformity for granted. That's definitely not the case out "in the real world".
Similarly, the space of unit quaternions is the double cover of the rotation group, but it lends itself to such a nicer representation, that I'm going to use it.
sdiehl has a nice writeup of the challenges here: https://github.com/sdiehl/frame/blob/master/NOTES.md
Yes and no. A fold has no intrinsic parallelism but it can be a good basis for deriving parallel algorithms. Consider, foldl f s xs = appEndo (mconcat (map (Endo . flip f) xs)) s Both mconcat and map are trivially parallelisable but this doesn't parallelise the fold. It builds, in parallel, a closure representing `flip (foldl f) xs` but then applies this closure to `s` sequentially. If, however, we could find an isomorphism between the closures representing the endomorphisms and a type permiting an efficient implementation of `appEndo` and `mappend` we would have a parallel algorithm. As a concrete example, consider a simple algorithm for matching parenthesis: foldl (\n x -&gt; if x == '(' then n + 1 else if x == ')' then n - 1 else n) 0 xs = foldl (\n x -&gt; n + (if x == '(' then 1 else if x == ')' then -1 else 0)) 0 xs Here, our endomorphisms all have the form `(\n -&gt; n + a)` for some `a`. This is because `flip f x` can be written in this form and all compositions of functions of this form also have this form. From this we can conclude our endomorphisms are isomorphic to a value of type `Int` representing the value `a` to be added to `n`. This results in the parallelized version: newtype ParensEndo = ParensEndo !Int appParensEndo :: ParensEndo -&gt; Int -&gt; Int appParensEndo (ParensEndo a) = \n -&gt; n + a instance Monoid ParensEndo where -- id = (\n -&gt; n + 0) mempty = ParensEndo 0 -- (\n -&gt; n + a) . (\n -&gt; n + b) = (\n -&gt; n + (a + b)) mappend (ParensEndo a) (ParensEndo b) = ParensEndo (a + b) flip appParensEndo 0 . mconcat $ map (\x -&gt; if x == '(' then ParensEndo 1 else if x == ')' then ParensEndo (-1) else ParensEndo 0) xs Exercise: Extend the example to detect the mismatch in something like ")(".
This is correct. What are the alternatives?
There's a "flattened" version of the image up now. Just: $ docker pull sseefried/debian-stable-ghc-android I created this image by exporting the completed image and then create a second Dockerfile that just added the exported files.
ex. foo :: address:String -&gt; name:String -&gt; IO () foo location identity = do I'm just spitballing here.
Also, if you add `Semigroup`, `Maybe` should lift *semigroups* to monoids, not monoids to monoids.
How much would this docker script help into making GHC cross-compiler targeting Raspberry Pi? I have several times tried to get GHC working on Raspbian for Raspberry Pi, but always gave up after days of struggle. It seems that some people got it to work, and I followed there instructions, and it always failed somewhere at the end (don't remember details anymore). Wanted to set-up a QEMU environment to make the builds at least faster - but did not yet come to it. So I am wondering, maybe the cross-compilation route will work better? Disclaimer: I don't have any experience with docker yet.
You shouldn't name your functions foo either.
Apparently there is a consensus on the fact that the ListT implementation from `transformers` is broken. Will it ever be fixed at some point ?
I've never actually played with a Raspberry Pi some I'm really not the one to ask. Have you asked any of the folks on #haskell or the Haskell Cafe mailing list? 
I'm not happy with anything that involves "from the right": I believe this "right" fallacy is the largest hurdle beginners face when foldr comes up, which is in a sense the most important/canonical list function. If by "I often say" you meant "I like to think" then go ahead, I think a lot of knowingly wrong stuff too that makes sense (only) in my head ;-)
We can't fix it without breaking backwards compatibility. And that would in theory break existing programs out there (though there's a good chance those programs are already broken by using the ListT implementation). I think the best path forward here is similar to what was done with Except, namely: 1. Create a new module to house a new datatype. 2. Deprecate the current module and point to the new one. 3. Add the new module to Edward's transformers-compat package so we can all start using it immediately without [causing problems with GHC dependencies](https://www.fpcomplete.com/blog/2014/05/lenient-lower-bounds). Honestly, I think the hardest problem to solve here is: what do you call this thing, if ListT is already taken? I'll be happy to add a function to conduit that converts from this NewListT to a Source once it becomes available.
I prefer a side-by-sider: http://chrisdone.com/lit.png 
`ChoiceT`, `NondetT` are options - if you take the idea that `ListT` is adding non-deterministic choice on top of another monad.
I may be missing something, but if the module name is different, can the fixed ListT type not continue to be called ListT? We would still need to find a new name for the Control.Monad.Trans.List module. Maybe it could be Control.Monad.Trans.ListT? -- old: import Control.Monad.Trans.List (ListT) import Control.Monad.Trans.ListT (ListT) -- new 
What is the argument for using a monad transformer as opposed to using `MonadIO`? I.e. stdinLn :: (MonadPlus m, MonadIO m) =&gt; m String instead of stdinLn :: (MonadTrans t, MonadPlus (t IO)) =&gt; t IO String You can also write `select` in terms of the standard function `msum`: &gt; :t msum Control.Monad.msum :: MonadPlus m =&gt; [m a] -&gt; m a so select = msum . map return replicateM' n = msum . replicate n
Well yes, changing the name of `ListT` sounds like an awful compromise. If it is to go that far, I would prefer to change the name of `transformers` using a fork with the correct implementation. Anyhow, this reminds me of Growing a Language, by Guy Steele ;-)
You're not missing anything, that's another option. I suppose you could even argue that the strict vs last modules gives a precedent for that.
FWIW- things like this are why I do everything in oriented projective geometry. =P
I've not read the MPL extensively, but searching through it for "file", I don't find any such language, except for maybe some implicit assumptions. The wikipedia cite for this is a link to the MPL, which is not very useful. But both I and wikipedia are some random guys on the internet ranting about licences, so take us both with a pound of salt. ;)
Some streaming libraries allow for non IO "base" monads, whereas MonadIO fixes it to IO.
I like to say it because I like to eventually show how you can "foldl-from-foldr" by deferring your accumulation as it builds left to right. I'm not sure that this is as clear as it could be, honestly, but it's very twisty to describe that trick.
So the code, as given won't compile - mainly because your whitespace is all off, though there were a few other problems. Fixed up, it looks more like this: module Shuffle where import System.Random import Control.Monad import Control.Monad.ST import Data.Array.ST import Data.STRef shuffle :: [a] -&gt; IO [a] -- shuffling an empty list is easy shuffle [] = return [] -- shuffling a non-empty list requires a source of randomness shuffle ds = getStdRandom (shuffle' ds) shuffle' :: [a] -&gt; StdGen -&gt; ([a],StdGen) shuffle' xs gen = runST $ do -- we'll use `g` to hold our current source of randomness g &lt;- newSTRef gen -- utility method to pick a random number and update g let randomRST lohi = do (a,s') &lt;- liftM (randomR lohi) (readSTRef g) writeSTRef g s' return a -- put all the elements of the list into an array ar &lt;- newArray n xs -- do a fisher-yates shuffle on the array elements -- http://en.wikipedia.org/wiki/Fisher-Yates_shuffle -- xs' contains the shuffled elements of xs xs' &lt;- forM [1..n] $ \i -&gt; do -- swap the i'th and j'th (j &gt;= i) elements, -- then return the (new) i'th element j &lt;- randomRST (i,n) vi &lt;- readArray ar i vj &lt;- readArray ar j writeArray ar j vi return vj -- give the source of randomness back, now that we're done with it gen' &lt;- readSTRef g return (xs',gen') where n = length xs newArray :: Int -&gt; [a] -&gt; ST s (STArray s Int a) newArray n xs = newListArray (1,n) xs I think the code is fairly clear as is (though it makes some odd choices). Tell me what you have trouble following, and I'll go into more detail. 
What I mean is, if you can load a csv without any knowledge about it's column , how do you expect later to get , let's say : the 4th column as Int?
You could even technically call the new module `Control.Monad.Trans.List.Lazy` (since it's technically being more lazy, albeit in a different sense)
 module Control.Monad.Trans.List (module Control.Monad.Trans.ListT.Wrong) where import Control.Monad.Trans.ListT.Wrong ---- module Control.Monad.Trans.ListT (module Control.Monad.Trans.ListT.Right) where import Control.Monad.Trans.ListT.Right
For examples why?
If your function needs IO, like `stdinLn`, then a MonadIO contraint shouldn't be a problem. On the other hand, if your function doesn't need IO, then you just need `MonadPlus m`.
I think he meant using base monads other than `IO`, like `Parser` or `Free`, but in some of those cases you can use an `mtl`-style type class to recover the same trick.
&gt; At the end of the day, Email are isomorphic to String. Valid Emails are not, but that's another question. Why shouldn't be able to store Invalid emails ? The same reason the `Integer` type can't store the (invalid) integer "John Smith". The same reason the `Double` type can't store the (invalid) double "$7,95/l". Because what we mean when something is an value of the `Email` data type is that it is a (valid) email. The phrase "invalid email" means that someone or something tried to pass this off as a (valid) email, but it is not a (valid) email. I think you should really push back on the business and try and get them to clean the data before it makes it into the next stage of storage. If you don't, it won't be cleaned up later no matter how many "invalid customer" reports you generate later.
&gt; What I mean is, if you can load a csv without any knowledge about it's column , how do you expect later to get , let's say : the 4th column as Int? I... wouldn't? (expect to) I don't even know that my data has a fourth column, much less that the fourth column is an `Int`. If I do know those things, I would need to include that information in the types I give; my existential can't quite range over all HRecord types, but rather only over those types with the fourth field being an Int. I'll admit that's a much more complex type to express, but if you want the computer to know that operation is valid, you have to communicate that fact to the computer. If I do use a type that has it's fourth field as an `Int`, I would expect the CSV parsing to take this into account and (e.g.) fail if the fourth field in any line except the header line was (e.g.) "undefined", or if any line has less than four fields.
I kind of like something like `DepthT` to emphasize that `ListT` tends to be depth-first traversal of the search space which differentiates it from `LogicT` and [something like `OmegaT`](https://hackage.haskell.org/package/control-monad-omega-0.3.1/docs/Control-Monad-Omega.html) not that it exists.
You only have to build it once...
The work that need to be done to update the library specification isn't being done. There are plenty of people using GHC that do not see (enough) value in having a separate library specification to work on it. This has been somewhat true for longer than most want to admit. I remember being told when I want still very new to Haskell, before the 2010 report was out, not use to the module names in the 98 report and instead to use different (hierarchical) names. Being able to write standard-conforming **source code** and being able to use it across multiple compilers is something that's valuable to me. I never wrote my C/C++ code for a simple compiler and because of that, code that is 15 years old still compiles in a variety of compilers today, with no changes. Haskell isn't exactly unique here, but this is definitely a mark against it when compared to the C, C++, and Java alternatives we currently use.
The difference is : I care if an Integer is an Integer, because I'll probably try to do basic math with it which I won't be able to do with an invalid Integer. With an Email, I won't do anything apart from giving it (eventually) to another system, which will accept a string and maybe give me back an error. What are the benefit of constraining the code to only accept valid Email ? What can I do (or not do) on valid email that I can't on invalid. Why does it matter (on a programming point of view).? Type-safety is about preventing programmer to construct things which doesn't make sense, (like trying to multiply two email address) , not validate user input. You have input validation for that. IMO, an email is String, with a "purpose" or a semantic and indeed can be valid or not. About Integer ... In france, the departement (county) number used to be encoded in the car registration plate. Each departement was assigned a number between 01 and 99. At some point, the government decided to split Corsica in 2 departements. Corsica was number 20 and obviously 21 was already taken, so they numbered the two new departement 2A and 2B, which are not number anymore ... What happend then is, people having their car registered in those two counties wouldn't get any speeding or parking ticket , just because their number plate coudn't be entered in the Police system. 
&gt; I think you should really push back on the business and try and get them to clean the data before it makes it into the next stage of storage. If you don't, it won't be cleaned up later no matter how many "invalid customer" reports you generate later. Which mean nobody cares about them being invalid ;-) 
'+' is permitted. Gmail use it for aliasing emails. If your email is 'bob@gmail.com' then Gmail will use `bob+from_reddit@gmail.com` as an alias to it. Handy when filling form to track who is giving your email ;-) 
Eh, that's one semantics. I'd rather `Maybe` just have the behaviour of `First`, and we got `Last` by using its `Dual`. But you're right that the current behaviour of `Maybe` is not that good regardless.
Isn't it fine to have both `Maybe` and `First` so you can easily get the behavior you want?
That's pretty nice. Also has a pretty obvious/intuitive inferred version. I guess it turns out to be reasonable (maybe even ideal) that changing a name of something in the type signature breaks other people's code, because you either messed up badly initially or it IS a breaking change. Thanks for dealing with me!
"The analog of a type checker would go even further by making sure that, once Romeo is declared a human being, he doesn’t sprout leaves or trap photons in his powerful gravitational field." Great humor. Keep it coming!
&gt; Type-safety is about preventing programmer to construct things which doesn't make sense, (like trying to multiply two email address) , not validate user input. Sometimes validating input is necessary before you can decide what makes sense. &gt; About Integer ... [example of some using a numeric type to store non-numeric data] Yes, this is why the types you use should match the meaning of the data, not it's structure and newtypes are an important tool there. If addition and multiplication don't make sense (e.g. if the data is a county identifier) then the type used shouldn't be Integer. In specific it should be a type that has a Num instance. It should have a well-performing Eq, probably Ord, and likely Hashable. Especially when paired with GND, it is relatively easy to use newtype to expos just the operation that are well-defined without exposing the structure. If you carefully choose what to expose, changing the internals is, if not easy to do, trivial to cordinate, since it is just a local change. There's a LOT of contexts where I'm just going to store an "email address" as Text (or even ByteString), but there are others where the Email type is going to be RFC2822 validated because the meaning includes other operations (e.g. extracting names subparts). At the "bottom" everything is bits and almost everything is machine words, but ensuring meaning by restricting operations -- including "constructors" -- make sense.
Well, the current `Maybe` is a really good candidate to be the privileged `Maybe` because it is the free monoid generated by a semigroup.
I enjoyed: "Franz Kafka’s character, Gregor Samsa, breaks the type system when he metamorphoses into a giant bug, and we all know how it ends."
An xkcd relevant to the challenges section: http://xkcd.com/221/
[Image](http://imgs.xkcd.com/comics/random_number.png) **Title:** Random Number **Title-text:** RFC 1149.5 specifies 4 as the standard IEEE-vetted random number. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=221#Explanation) **Stats:** This comic has been referenced 158 times, representing 0.3791% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cmbufqd)
or maybe even unsafeInterruptibleMask :)
Wow! That's rather dramatic indeed
Yes, there are binary distributions here: https://www.haskell.org/ghc/download_ghc_7_8_3
&gt; TL;TR : Better to use Name. Not everyone has a first and last name That's not a good enough solution in many cases, since for those that do have a first and last name, it's often necessary to know which is which. Attempting to determine that from the text itself is error prone because of middle names, multiple or compound last names, etc.
In a way, the joke is funnier in Haskell: getRandomNumber :: (RandomGen g, Random a) =&gt; g -&gt; (a, g) getRandomNumber g = (4, g) This is actually a nice motivating example for the idea of an [unused constraint warning](https://byorgey.wordpress.com/2011/11/05/wanted-ghc-feature-warn-about-unused-constraints/). We sometimes trust that a constraint like this implies that the constraint is actually used.
Haskell is lots of fun. And doing visual-centric and generative visual stuff is actually really pleasant in it, although doing full GUIs is a bit more of a pain. You might want to take a look at byorgey's diagrams library (http://hackage.haskell.org/package/diagrams) which is popular and well liked. If there's stuff you find yourself wanting from it that it doesn't have, hacking on it is a good way to step up your general programming expertise as well. :-)
Thanks for giving me outlet. I'm interested in exploring the keyword argument design space. FYI I'm just making stuff up as I go, so, take it with a grain of salt :p
Anyone got the slides?
I'm not sure about the state of libraries for visual stuff, but I can definitely see the functional style of Haskell being a good fit for generative artwork. This kind of artwork is, by its nature, very recursive in structure and in my experience this will be a good fit for the kind of problems functional languages are well suited to.
Inexperienced programmer here. Not really answering your questions (except that I think it's totally *okay/sensible/sane of you to have a go at Haskell*), but since you're not very experienced either I think I can share what my little experience made me realize. What I like in other languages is being able to make the computer *realize things* (draw fractals or whatever). What I love in Haskell is defining nice data structures and *talking* to the computer in a beautiful language. I kind of use Haskell for the sake of the language (without caring whether it's convenient for the task I want to perform) whereas I would use another language for the sake of the task itself. So if you only want to program to "do" things I would say look at the answers from other people, but if you're also intersted in the pleasure of "writing" then go for it without hesitation.
In addition to `diagrams`, which is excellent, if you find yourself needing to do computationally intense things such as fractals, Haskell has some excellent libraries (Repa, Accelerate) for writing code that runs quickly, and in the case of Accelerate, can be semi-automatically parallelized and run on the GPU. Also, as a personal plug – if you want to play with `diagrams` in an interactive way, [IHaskell](https://github.com/gibiansky/IHaskell) has a display package which lets you type in diagrams code and immediately see the resulting pictures. (That said, IHaskell can be quite a pain to install at the moment, since it has a lot of painful dependencies. I'm working on improving that, though!)
Does [this sub-thread](http://thread.gmane.org/gmane.comp.lang.haskell.libraries/23300/focus=23350) answer your question?
Was going to suggest this. It's particularly great if you're at all interested in 3d stuff 
I'll also not really answer your question, but instead point you at [gloss](http://gloss.ouroborus.net/), which I see as kind of like Processing for Haskell.
I still haven't used Haskell at work, so it's just a "hobby" for me, too. :/
What are the best Android devices to try and use this with?
Perhaps I misunderstood what you were suggesting. If you were thinking that a Name type might support, essentially, optional injection and projection of first and last names, that's fine. I had the impression you were suggesting that modeling that distinction wasn't necessary. 
Any that use the ARM instruction set.
Really? I tried, but failed for some reason I don't understand.
Yes, but in fact everytime I start a new kvm.
thats not descriptive at all
Compiles to Javascript, oh god
Besides Haskell and Processing, you may also find that you may like the Logo language for creating generative visuals.
Morally you can only construct `a -&gt; Void` if `a` itself cannot be constructed. To wit, `Void -&gt; Void` is quite easy to build. Using the types-as-theories correspondence `a -&gt; Void` is often pronounced "not `a`" as it is evidence that `a` cannot be constructed. There's something more interesting though in `(a -&gt; Void) -&gt; Void` which is a bit like `Cont`. This is "not (not `a`)" or double-negation. It's also closely related to the root type of continuation passing style.
This sounds very complicated and hard to predict. Obviously, you should put everything in RAM via [acid-state](http://acid-state.seize.it/). :p
Do you also revolt when you hear that GHC Haskell compiles down to x86 assembly?
If you know something else that every browser supports, I would love to hear it.
It is very confusing to me that they did not do this; then there would have been immediately an induction principle, rendering the recent work on adding a special solver for nats to the type checker largely unnecessary.
Yeah Name could be any type 
Unnecessary if you know how to write the proofs, right? I think that is what they are trying to avoid.
I would call it EachT, since it is the "for each" monad transformer. example = do str &lt;- replicateM' 10 (lift getLine) -------- lift (putStrLn str) Everything below the bar happens 10 times. You can basically read every `X &lt;- Y` line as `for X in Y`.
I graduated from an art school, but have always been technical. I was making visual demos and tiny games in QBasic in the early/mid 90s. I used the crap out of Flash, and learned ActionScript (the original - Flash MX days), and then switched to Linux, where I didn't have AS or Flash anymore. That's when I, too, found and fell in love with Processing for a time. I made dozens of fun things in there. I've 'hacked' so many things over the last 23 years, but I also write code for my job all the time (3D work in games), and I'm tired of rewriting everything over and over again, due to code rot, and constantly seeing 'better ways' of doing things. Haskell - and FP in general - have been a huge eye-opener for me. I'm *actually* getting huge reusability now, instead of merely being promised it all the time by OOP, because I'm building small, reusable functions, instead of 200+ line classes that do a single, non-reusable, overly-intricate thing, full of messy, mutable state, and complicated dependency graphs. I can actually say I'm done with a function, because it's a simple, mathematical transformation, and there really isn't any way to make it simpler than a total function with 1 or 2 lines of provably correct code in it. It feels like I'm finally accomplishing things. Be careful, though. You may find that - like me - you start becoming really (in fact, primarily) interested in weird, non-artistic things, like functional purity, higher order functions, and even *gasp* category theory. It's a slippery slope.
Fair enough; but I think the main reason for the solver is that it is impossible to express trivial things with the natural numbers without either it or an induction principle. From my perspective, the induction principle is the obvious first thing to add; a solver for generating the induction arguments then would be a very useful thing to add afterward. It just seems totally backwards the way that it has been done. So to those who know more about it than I, is there any good reason for having made the naturals primitive numerals without inductive structure? Was it because of a concern about performance in dealing with tons of `succ`s?
No, because x86 assembly is not nearly as bad as javascript And I use -fllvm which goes to LLVM before x86
Probably for the same reason we need [a] in addition to type List a = forall r . (a -&gt; r -&gt; r) -&gt; r -&gt; r or a newtype of it (i.e. `LogicT Identity`). The `Stream m a` type in `vector` is another quasi-`ListT` -- it's a little more complicated now -- but I don't think it would be too pleasant if standard IO operations were explicitly given to us in terms of data Stream m a = forall s . Stream s (s -&gt; m (Maybe (a,s))) which is actually somewhat easier to comprehend than the `LogicT` type, but is also the "right" `ListT`. These things are best used for invisible optimizations, as the former is used by ghc and the latter by the stream-fusion libraries.
Perhaps so... I wonder how people would feel about endowing the existing numerals with a proper induction principle? It seems like something of the sort should be possible, if it is made primitive. If only I had the time, I'd look into volunteering to fix this. Seems kind of surprising to me, frankly, that a bunch of type systems people overlooked the imperative necessity of an induction principle in the first place, but sometimes I suppose things happen in a different order than one would expect. :)
I was pulling out the head of a list where the head needed to be treated differently than the rest. I wound up using a pattern match, but thought "this feels like maybe" and wondered if it had a name...
I really greatly enjoyed this talk. Ranjit came out and managed to fill two hours with content without losing the attention of a very diverse audience. I was pretty floored.
Yeah I wish Singletons were part of TypeLits, but I think focus now is on how to change the core. idk. 
The editor tooling here is impressive to say the least. Really enjoyed watching Ranjit's interactive workflow with LiquidHaskell inside of vim.
It's emacs with evil-mode. I also use that setup, and I highly recommend it.
Thanks! I update the page.
The main difference between the two flavors is the version of libgmp. Just find out which version rhel7 ships. Most likely you'll need the centos flavor.
Well, by now the solution to that problem is well-known (see reflection without remorse). What I really meant to ask was, why do we need it in transformers? There are many implementations of "ListT" with their own strenghts and weaknesses; and there's no obvious/simple one (well, there is, but it's flawed). Hence, they just belong to different packages, which people will pick up based on their needs.
The only redemption is an haskell to lisp compiler. For my lisp machine
&gt; maybe what I want to do (mainly visual-centric output) is just batshit insane Not at all. Following Jenn Schiffer's [vart project](http://vart.institute/), I decided to see how much work it'd be to reproduce the same sort of constructs in Haskell and [the Mondrian example](https://github.com/impega/vart) didn't take super long mainly thanks to a lot of nice libraries being around (JuicyPixels &lt;3). Granted, [it's not revolutionnary](http://imgur.com/QnK7dK5) but it was a fun project to do to discover new libraries.
This is a wonderful presentation. Mind blowing in parts, even!
 &lt;interactive&gt;:34:16: Couldn't match expected type ‘Capulet’ with actual type ‘Montague’ In the second argument of ‘loves’, namely ‘Romeo’ In the expression: Juliet `loves` Romeo Beware of context-free type systems.
&gt; So to those who know more about it than I, is there any good reason for having made the naturals primitive numerals without inductive structure? From the thread, the reason seems to be that people want to compute with nats. For this to perform reasonably well, it needs to be an unboxed type, which can't currently be promoted. (A much more important bikeshed is the fact that subtraction on nats should obviously saturate, since that's what makes subtraction into the adjoint implication for addition.) 
I would turn the question around. Why do we even need `LogicT`? Every use of `LogicT` I have ever seen could be more simply solved using `EitherT`, `ListT` (done right), or a combination of the two. CPS - whether inside a monad transformer or not - adds a huge amount of underlying complexity. It makes code harder to understand and maintain. In my opinion, using CPS is a classic premature optimization, except in cases where you can demonstrate that there is a performance bottleneck and that CPS is really the best way to solve it. EDIT: Sorry, now I see the clarification of /u/roche in a subsequent reply, and I agree with him completely.
Came here to suggest the same, yet also with PureScript as an equally interesting choice.
Is that the "right" `ListT`? What's wrong with the much simpler newtype Stream m a = Stream (m (Maybe (a, Stream m a)))
So are you are asking us about how certain things *will* be implemented *in the future*? :D
From http://lhc.seize.it: &gt; LHC is a whole-program optimizing backend for the Glorious Glasgow Haskell Compiler LHC seems to more or less use the GHC front end, then run whole-program optimisations over the intermediate representations, then compile to executable code. The intermediate representation stuff is, I think, why it needs it's own backend. This post looks like the start of shift from a C backend to LLVM - presumably because developments in LLVM over the past few years have changed the trade off. Whole program optimisation is one way to overcome performance hiccups that happen across the module boundaries, and thus have the potential to produce faster binaries with just a recompile. Most of these optimisations come at the cost of increased complier complexity - as one is effectively starts by removing the simplifying assumption of separate modules.
Yes, you should If you are a design guy maybe you come from the Web. Maybe you want to play with the HTML5 canvas in Haskell using a reactive something? Here is an basic IDE environment running in the browser with a simple canvas example: http://tryplayg.herokuapp.com/try/drawcanvas.hs/edit
Extensible variants are nice but, so far, rarely used. It can be used to implement something like Typeable, safely, which is pretty cool. There are probably other uses hiding somewhere though. 
It involves other parts of OCaml as well, particularly modules, but I can confirm they can be used for this (there's an implementation of it somewhere in the jane street Core library but I can't find it at the moment because it's a mess). Long story short is you create a module for each type, each of which stores a particular variant in an 'ID' type, and supplies a projection function (think cast from typeable). That basically gives you this type id type and cast, and then they build dynamic on that. The other thing they're very useful for in OCaml is simulating higher-kind type variables. But of course that's totally useless in Haskell.
Similar to Typable, they can be used to implement downcasting for OCaml objects. See my example at the bottom of: https://sites.google.com/site/ocamlopen/. I'm not sure how useful it is in practise, but I do think it is an interesting example. The same thing would work for object-like encodings in Haskell.
Upon second thought, I'll continue to use the term "variable", since it implies that it may have a different value in a different scope, while the term "constant" implies that the value will always remain the same. http://stackoverflow.com/questions/20755641/global-variables-in-pure-functional-languages for more details
You mean such that the internals are safe? My biggest reason to stay away from typeable is the slowness, and it always feels like there should be a better solution.
The one he gave is more efficient (which is why `vector` uses a variation on it), but equivalent to yours.
Yes please, more Haskell compilers!
I was agreeing with /u/random_crank that the reason we need `LogicT` is as an optimization. But to me, the "real" type is the one that expresses the desired denotational semantics most simply. According to that way of thinking, my type is the "real" type for ListT, not the one that /u/random_crank gave. But of course that doesn't mean we shouldn't use the optimized type for a general purpose library.
&gt; To be fair, I have since tried out several other verification systems (for example, Frama C), and found the same sort of issues – great in theory, almost ready for prime-time, but lacking just a bit of engineering and "UX" work to make it truly ready for productive use. That is a very common state of facts for systems that come from research, because researchers are not rewarded (that is, discouraged from) doing polishing and UX work. You can dedicate some time to it, or sacrifice some undergrad student's time for it, but that can only go so far. I think this is a point where we need more contributions from industry and the open-source community: foster a culture of making serious attempt at using research byproducts, helping polish them (through feedback but also contributions) along the way. Right now, the status is more that industrial don't have budgets for helping promising tool mature, and that the open-source volunteers are not very interested in helping with arcane research software. Instead, when the ideas have matured a bit (... or before we know how to do it right), they rewrite their own and, being a more integral part of the community, develop a culture of using those reinvented tool that are suboptimal in many ways. It seems that the people in the Linux kernel community for example use home-grown glorified linters, but not much in the form of static analysis -- there are promising fuzzing tools on the other hand. Or package managers mostly reimplement their own dependency-handling tools, not benefiting much of existing research. (There are of course some well-appreciated exceptions. The Haskell community is rather good at investing experimental tools such as XMonad or Nix, and some researchers have a knack for producing usable tools and convincing *other people* to use them, [CSmith](http://embed.cs.utah.edu/csmith/) for example.)
 something :: b -&gt; (a -&gt; [a] -&gt; b) -&gt; [a] -&gt; b something x f = fst . foldr (\x (_,xs) -&gt; (f x xs, x:xs)) (x,[]) 
Here's one way to implement `Typeable` in Haskell, if Haskell had extensible variants and GADTs (OCaml's extensible variants can be GADTs, so we know it can be done): data a == b where Refl :: a == a data TypeId a where ... class Typeable a where typeId :: TypeId a typeEquality :: TypeId b -&gt; Maybe (a == b) data TypeId a +where Unit :: TypeId () instance Typeable () where typeId = Unit typeEquality Unit = Just Refl typeEquality _ = Nothing data TypeId a +where Bool :: TypeId Bool instance Typeable Bool where typeId = Bool typeEquality Bool = Just Refl typeEquality _ = Nothing cast :: forall a b. (Typeable a, Typeable b) =&gt; a -&gt; Maybe b cast x = case (typeEquality (typeId :: TypeId b) :: Maybe (a == b)) of Nothing -&gt; Nothing Just Refl -&gt; Just x Obviously, I didn't type check this, because Haskell doesn't actually have extensible variants, but I think you can get the idea at least. I could post working OCaml code instead, if you'd like, but it wouldn't have the nice type class based approach.
If you can't change your database, but can use Haskell, you could take a look at something like [persistent](http://hackage.haskell.org/package/persistent), which has a MongoDB backend, IIRC.
yitz, yes, I meant to be saying the same as you. I was taking it for granted that the natural or "real" `ListT` is something like newtype ListT m a = ListT (m (Maybe (a, ListT m a))) This, or maybe the equivalent expressed with a specialized `Maybe (a,b)` like data Link a b = Nil | Cons a b newtype ListT m a = ListT (m (Link a (ListT m a))) is what ought to be in `transformers` to replace `ListT`. The `Stream` and `LogicT` types are alike fancy encodings of *it*.
I like the combination of [ixset](https://hackage.haskell.org/package/ixset), [acid-state](https://hackage.haskell.org/package/acid-state), and [safecopy](https://hackage.haskell.org/package/safecopy).
Yes, such that the internals are safe. See [my implementation](http://www.reddit.com/r/haskell/comments/2nd1cs/ocaml_extensible_variant_types/cmcn0l3) of `Typeable` in Haskell + extensible variants + GADTs.
Yes, but only if you can tolerate the increased cost of storing everything in RAM.
There's always a trade off between using a language/system that is explicitly made for artistic work (e.g. processing, supercollider, etc) and using a general purpose language/system, the former tend to be technically inferior (less advanced) but they are fine tuned for their particular task. I think that when doing creative/artistic work with computers one should use the tool that gives the best artistic results (and faster), and that's often not the most advanced one from a pure cs point of view. If you can do what you need/want to do in Haskell, then great, it will be a much more pleasant/interesting experience, but you shouldn't loose focus of what's really important, the artwork. Hopefully, in the future the number and completeness of libraries for artistic work in Haskell will continue to grow and become more advanced, so that there will be no trade-off at all. So my opinion is, it's quite useful to learn Haskell, since it teaches you ideas you can apply elsewhere and really opens your eyes, but if the artwork you want to create is more easily created with something else don't get caught up with Haskell just it's more advanced/interesting from a cs point of view. 
Indeed, while we've been making a bunch of improvements (e.g. the editor integration in this talk) in general the UX leaves a lot to be desired. This is in part due the reasons /u/gasche describes below, an in part because some of the UX issues are technical problems that we still have to find better solutions for. Still, thanks for your interest! and please do file issues and such https://github.com/ucsd-progsys/liquidhaskell/ so we know what to prioritize.
While the diagnosis about "physician heal thyself" is accurate in general and applies broadly to the LH implementation, in this particular case the Refinement language i.e. the 'p' that appears in {v:Int | p}) _does_ have a type language (or "sort" language, just to keep things separate in my head); we rely on it heavily to both optimize the checker and of course to find/prevent bugs inside the implementation :) 
I know you've done a lot of work evaluating these approaches - what can you tell us about this cost?
I like to use this formulation for `ListT`, which simplifies the types a little bit: newtype Link m a = Nil | Cons a (ListT m a) newtype ListT m a = ListT (m (Link m a))
Well, what votes actually do is make it more likely or less likely that other people will see the post. So I think of my votes as a way to help others viewing the channel, not as a way to throw a pie in the poster's face. You're right that "bad quality" is another reason that we might want to help others out by making the post less visible, and "wrong" is one kind of bad quality. But here I think that on balance it's worth it for the post to be here even though it's wrong.
Broken link, although I couldn't figure out why.
Because NoSQL is hip these days. New projects tend to use it instead of the old SQL databases. Even when there is no good reason to do so.
Why re-implement base though? If the goal is to focus on the compiler why not just borrow GHC's base as is?
Totally agree with PostgreSQL here. I've come to think of it as the Haskeller's SQL. It supports simple sum types with enums and more complex types with row types.
Hey, great job man, very useful library! But, I think your command line interface should see some work. That's pretty uncommon around *nix tools that taking a number via one parameter(-d), and type of that parameter via another(-t). If I were you, I'd have separate parameters for each type which needs a number, and make them mutually exclusive(like `head [-n count | -c bytes]`) or allow multiple constraints and apply all of them.
Haha, yeah, I remember as a kid I had quite some fun getting the turtle to move around. Though I might have outgrown it by now...
The one big thing that I find missing from Sql, in matching it to Haskell, is some way to have sum types where constraints still preserve referential integrity. You can kind of fake it with a product of nullables, but you don't get the "one and only one of these..." enforced.
I agree. Let's not use a hammer to kill flies.
I'm sorry to hear that. Does it have to do with me leaving the "WWW." off? Maybe try (https://www.github.com/gbgar/Wordlint)[https://www.github.com/gbgar/Wordlint] Edit: fixed link in parent comment. /u/xhevahir was referring to vim plugin link, which was misnamed by its working title.
Interesting. I'll definitely look into Elm! Speaking of Elm, in my search I also came across [Helm](http://helm-engine.org/). This also seems promising, but it seems to be at an early stage in it's development.
Thank you very much! I didn't actually anticipate the library side of things being too useful, what with such hacks as getting column numbers as opposed to using some kind of "real" parsing library (wanted to keep dependencies minimal and figure things out myself). In regard to the parameters, this is one of the hairier spots of the program as you point out. Currently the checks run independenly of one-another, although it would be interesting to allow the user to overlap them and only hit the union of two lists of Wordpairs or something like that. I thought that having the -d flag independent of the -t flag would be fairly straightforward and less complicated (just don't include any decimal numbers if you aren't running a percentage check) than having to remember a unique flag depending on "-t". With your suggestion, I am thinking that instead of a "-t" or a "-d" flag, I could get rid of both and have something like mutually exclusive (for now) "--word", "--line", and "--percentage" flags that take an Int, Int, Double, or nothing for a default, respectively and notify wordlint of which kind of check to perform. Would this be more in line with convention? Edit: typos
This is definitely going on my "to-check-out"-list.
Thanks for the motivation! And wow, I never even thought about being able to bring something to the table from my side. I'll definitely try to contribute to the community as soon I have dug myself into Haskell a bit more.
I'll happily take you up on that invitation! :)
Seems the main thing for haskell, is what assumptions we are allowed to make as to how to solve this problem. Would've liked to see this conversation play out a bit more betwen Alexandrescu, Bright, Meijer and Moran: https://www.youtube.com/watch?v=sF2Ict7H6dk&amp;feature=youtu.be&amp;t=50m37s
Yours and all the other comments here have definitely motivated me to take the plunge into Haskell. No idea where it will lead me, but if I pick new ways of thinking about programming on the way, it'll be time well spent.
I actually took a course on Max/MSP during my design studies. I really enjoyed connecting up the logic bricks and controlling the flow of the bangs to create unique pieces of software. And yeah, we where using Max on a students license, so after the course I never had another chance to use Max/MSP. vvvv always seemed very intriguing, but since "design community == apple hardware" that unfortunately never became an option. (Though maybe Bootcamp could help out here...?) No idea why I never delved into PD though. Thanks for refreshing my interest in those projects! I'll get to those as soon as I conquered the Haskell basics. :)
Omigodmustwatchthistalk!
It doesn't have to be revolutionary, as long as there's something to learn on the way it' great! Maybe I should make the Mondrian example an intermediate goal for me on my foray into Haskell...
Dead link? 
I'll take this opportunity to promote Opaleye, my SQL-generating EDSL which targets Postgres. It will be publically released on 1st December (this coming Monday) so watch here for announcements! Opaleye is essentially the Haskell of relational query languages, in the sense it is embedded as a domain specific language in Haskell and aims for maximum type safety and composability at a very fine-grained level. In terms of type safety you get * type checked selects, inserts, updates, deletes * all query and expression operations typechecked: cannot compare an integer and a string for example * nullability explicit in the types * equivalent of "newtypes" for wrapping basic Postgres types 
I'm not sure about XOR linked lists, but basically, this talk says that you can present a list interface, but don't actually build it as a linked list. Instead, use trees. The tree described is a finger tree (example in Data.Sequence). Side note, this is also used to great effect in clojure. Some may argue this is changing the problem, but if it satisfies your goals then it seems reasonable.
Looks like it might be a routing issue. The link just spins forever for me.
There are some advanced Logo environments: * [NetLogo](https://ccl.northwestern.edu/netlogo/) * [aUCBLogo](http://aucblogo.org/en/Logo.html) * [FMS Logo](http://sourceforge.net/projects/fmslogo/) ...and for those interested in more of a challenge, check out the table of contents for [Turtle Geometry: The Computer as a Medium for Exploring Mathematics](http://www.amazon.com/Turtle-Geometry-Mathematics-Artificial-Intelligence/dp/0262510375). And if you really want to bend your mind, try to find a copy of: [Fractals, Visualization and J](http://www.amazon.com/Fractals-Visualization-J-Clifford-Reiter/dp/1430319801/), which uses the [J](http://www.reddit.com/r/apljk) [Language](http://www.jsoftware.com/). 
XOR linked lists do not fit in a language that is garbage collected. They obfuscate pointers and that is verboten. You easily can implement them within the confines of an array or vector though, just not with "real" pointers.
I tried it again a couple of minutes ago, now it works, and, no routing error but an article is missing page from the actual Webserver 
How else would you target browsers?
This pre-supposes OP is already using Postgres. Looks like he's coming to an existing python/mongo codebase and is trying to change the DB to something sane, not change everything all at once...
What you want to do is run an interactive session: $ docker run -it &lt;image&gt; bash (because you run command "bash" it respects the .bashrc) The GHC 7.4.1 (installed via apt-get) was used to bootstrap the build of GHC 7.8.3
Well, this is a tricky thing. In what's called a **total** language, the only function that can exist like that is the one that has this type: f :: Void -&gt; Void But total languages aren't Turing complete. In a total language every function always "progresses" when called with any argument. ("Progress" here is a relaxed concept of termination that includes not only programs that terminate, but also non-terminating programs that don't "get stuck." For example, a program that correctly prints pi digits forever doesn't terminate, but it progresses, because at any point of its execution it's guaranteed that it will produce a next digit.) Any Turing-complete language is **partial** instead, meaning that there are functions that don't terminate. In such a language, you can have functions like this one: f :: a -&gt; Void f a = f a -- loops forever In Haskell, we equate evaluation errors with non-termination, so you can also write this: f :: a -&gt; Void f _ = error "WHAM" Or this: f :: a -&gt; Void f _ = undefined
Well, I haven't watched the video (and it's an hour long!), but I know how to implement a double linked list in Haskell (without using `IO`, `ST` or any other such thing). In imperative languages, building a double linked list absolutely requires you to mutate a structure after you construct it. In pseudo-code: struct DList&lt;A&gt; { A elem; Option&lt;DList&lt;A&gt;&gt; prev; Option&lt;DList&lt;A&gt;&gt; next; } DList&lt;A&gt; twoItemDList(A elem1, A elem2) { DList&lt;A&gt; head = new DList&lt;A&gt;(elem1, null, null); DList&lt;A&gt; tail = new DList&lt;A&gt;(elem2, head, null); // Here comes the mutation head.next = tail; return head; } In Haskell you can't do that. But Haskell has one trick up it's sleeve that imperative languages lack: lazy evaluation. My favorite example of this is the function to build a circular single-linked list: cycle :: [a] -&gt; [a] cycle xs = result where result = xs ++ result `++` is the Haskell operator for appending two lists; for example, `[1, 2, 3] ++ [4, 5, 6] == [1, 2, 3, 4, 5, 6]`. So note the `where` in this definition: we're calling `++` and passing it *its own result, from the same call* as its argument. Haskell allows you to use the result of a function call before it's been computed, so you can pass the result as an argument to the call that produces it. That's the mind-bending bit. So the pseudo-code example above becomes this in Haskell: data DList a = Empty | Cell { elem :: a, prev :: DList a, next :: DList a } twoItemDList :: a -&gt; a -&gt; DList a twoItemDList elem1 elem2 = head where head = Cell elem1 Empty tail tail = Cell elem2 head Empty Here you're pulling the same trick indirectly; `head` is built by passing `tail` as an argument to the `Cell` constructor, but `tail` is built by passing `head` as an argument to a second use of the `Cell` constructor. More generally, you can build a `DList` from a regular list this way: fromList :: [a] -&gt; DList a fromList = go Empty where go :: DList a -&gt; [a] -&gt; DList a go prev [] = Empty go prev (a:as) = head where head = Cell a prev tail tail = go head as -- | The inverse of 'fromList'. You can unit test this code by testing that -- @toList (fromList xs) == xs@ for any value of @xs@. toList :: DList a -&gt; [a] toList Empty = [] toList (Cell a _ next) = a : toList next 
Good coverage of the alternatives.
:'(
The only potential hazard here is that this isn't suitable for every usecase a traditional doubly linked list is. That said, there are pure replacements for everything you can do with a doubly linked list that have the same asymptotics for all operations, so this isn't a damning fault. ;) 
At the risk of being a pedant, I'd like to expand on your suggestion. &gt; The seq introduces an artificial dependency on the order of evaluation The proposed fix will work, but it may have undesirable performance. In particular, it's a bit wasteful with memory, because it keeps the entirety of the raw input and the parsed version in memory until the whole of `results` is demanded. The problem is that the GC sees that `length contents` needs to hold on to `contents` and it sees that `parseDefs (lines contents)` needs to hold on to `contents`. So once `contents` is in memory, it'll stay in memory at least until `parseDefs` has reached the end of the input **and** `length contents` has reached the end of the input. What we really want to have happen here is for either `hGetContents` to read the whole thing straight away, `parseDefs` to consume all the input before `hClose` does its thing, or to use a left fold (iteratees, pipes, conduit, etc). As you correctly point out, `hGetContents` is lazy, so we're out of luck there. [1] Iteratees are a bit much to chew on, so let's ignore that solution for now. There is another thing we can take advantage of here: Most parsers need to be strict in their input to generate good diagnostics. The common counter example being parsers that can generate a lazy list of results. (I'm intentionally avoiding the term 'incremental parser' because some people use that to refer to parsers that can save work reparsing something given a delta.) What I would typically do in this case is to make sure that `results` (instead of using `length contents`) is fully demanded as early as possible. Unfortunately, how to do that nicely depends on the type of `results`. For example, if it's an `Int` you could just say ``results `seq` hClose handle``, but for most other things that won't be strict enough. You'd need a `deepSeq` or equivalent. [1] We could reach for a strict version of `hGetContents`, or make one. I really wish a strict version of `hGetContents` lived in base. I don't mean to replace the existing `hGetContents`, but adding an `hGetContents'` that reads the whole input straight away would be a nice touch and a short patch. Doing it well would require a small audit to find other functions that need a strict variant (such as `readFile`). EDIT: Thank you /u/htebalaka for adding links to dealing with lazy IO!
This is not a limitation of Haskell, but a limitation of the shell: http://stackoverflow.com/questions/255414/why-doesnt-cd-work-in-a-bash-shell-script
Great work!
CQRS /= EventSourcing
Though what you *can* do is create a shell script `mycd` containing #!/bin/sh cd $(myapp $*) and then you could run it as . mycd args (That is a period followed by a space followed by your shell script with arguments.) But this is, as /u/gelisam pointed out, entirely a shell scripting question.
Nice! Do you plan to support view updates? (As described by Date.)
This is a good excuse to mention tcache. it is a STM cache, not a RAM storage. registers are loaded and unloaded from memory automatically. queries use the STM monad. there is a straighforward pure haskell query language using register fields. I has persistence in files by default for testing purposes, but -by means of register instances- is possible to configure anything, so it can handle registers coming from different databases.
Postgres doesn't support view updates, so no.
The article briefly touches on *bottom* ⊥ in the context of non-termination. My understanding is that in Haskell you can express bottom by returning `undefined`. Upon evaluation it will then raise an exception. I.e. while it conceptually stands for a non-terminating computation, it actually translates into an exception, which will practically terminate the computation (just without a result). However, `Void` seems to be a more literal version of a non-terminating computation, in the sense that Haskell will never return from a function that has the return type `Void`. Consider the following example functions (I tested them in ghci): import Data.Void f :: a -&gt; Void f x = f x -- loops forever g :: a -&gt; Void g _ = undefined -- loops forever (same as `g = undefined`) g' :: a -&gt; Void g' _ = error "Void" -- loops forever (same as `g' = error "Void"`) h :: a -&gt; () h _ = undefined -- raises exception: Prelude.undefined `f` is the first example you gave, that this function loops forever is fairly obvious to me. However, the function `g` will also loop forever. This is a bit more surprising to me as it returns (or is) `undefined`; which, as I thought, would cause an exception. The same thing goes for `g'`, it will loop forever and never display the error message. My guess is that the haskell run-time never gets to execute the `error`, or the `undefined` because `g`, or `g'` never actually return. Does that make sense?
&gt; There's something more interesting though in (a -&gt; Void) -&gt; Void which is a bit like Cont. This is "not (not a)" or double-negation. I read a bit about CPS. However, I don't understand how `(a -&gt; Void) -&gt; Void` relates to [`cont`](http://hackage.haskell.org/package/transformers-0.4.2.0/docs/Control-Monad-Trans-Cont.html#v:cont). If you replace `r` with `Void` the type-signature matches. But, what would that mean? Is it a CPS computation that never terminates? &gt; It's also closely related to the root type of continuation passing style. Unfortunately, I don't understand what this means. What is the root type of CPS? (Sorry, I'm not a mathematician, just a curious beginner of Haskell)
That's a pity. Couldn't you transform the queries into the corresponding INSERT/UPDATE statements with your library? (If you maintain an AST, I think it should be doable). Date describes the rules of such a transformation in their book on view updates. It's a major feature, as it removes the need to write updates/inserts in many cases. ORMs also solve this problem, albeit in a different way.
That's fine. But remember that a type is just a tag associated with a value. A type system makes sure you operate on those tags in a consistent way. When you serialize to a file, database or network it is up to you to preserve the type tags. There is no system that will prevent you from writing out a Double and reading back a Maybe Int. Whether it's JSON, flat file or a database column is beside the point. That it's up to you to avoid defeating the type system by changing the types associated with values as they come and go is the point.
Fantastic stuff, this is looking really useful! The `head` example is really useful already.
You're absolutely correct. I appreciate the compromises Processing has made to allow it to be the valuable tool it is today. I know I would have been highly put off if Processing had required me to install libraries, maybe even compile form source and run it from the Terminal. Luckily it's all about the output, the handling of the "complicated technical stuff" is hidden away, making the tool far less scary for the not so tech-savvy. Nowadays it's a different story, I have gathered much more insight into the technicalities of software and programming languages (although all self-taught, I'm bound to lack a lot of the elementary stuff). I'll keep relying on Processing for most of my projects, since it's the environment I'm most fluent and comfortable with, even if it has it's (measured) drawbacks. Just as you mention, I do believe that getting to know Haskell, it will allow me to gather insights and impulses for better understanding the huge world of coding. I've previously tried expanding into C++ (via Cinder and openFramework) and even though these environments are also taylor-made for visual output, I've always felt that the underlying programming philosophy is very much the same as Processing/Java, just with different syntax. So I'm excited to dig into a world that involves a very different kind of thinking. (Even though I'm aware that Haskell is not at all meant to create visual output.) No idea if and how I'll be able to integrate this new knowledge into my current workflow, but as long as it tickles my brain and expands my horizon ever so slightly I think it's worth spending some time with it. 
Have you tried using the Data.List.Stream module to speed up the list manipulations?
This can be useful to compare the asm output of the Haskell version. This is the C function compiled with GCC 4.9 (Intel 64): multiply_polynomial(unsigned char, unsigned char): leal (%rdi,%rdi), %eax xorl %edx, %edx testb $1, %sil cmovne %edi, %edx movzbl %sil, %esi movl %eax, %ecx xorl $27, %ecx testb %dil, %dil cmovns %eax, %ecx movl %edx, %eax leal (%rcx,%rcx), %edi xorl %ecx, %eax testb $2, %sil cmove %edx, %eax movl %edi, %edx xorl $27, %edx testb %cl, %cl movl %eax, %ecx cmovns %edi, %edx leal (%rdx,%rdx), %edi xorl %edx, %ecx testb $4, %sil cmove %eax, %ecx movl %edi, %eax xorl $27, %eax testb %dl, %dl cmovns %edi, %eax movl %ecx, %edi xorl %eax, %edi testb $8, %sil cmove %ecx, %edi leal (%rax,%rax), %ecx movl %ecx, %edx xorl $27, %edx testb %al, %al cmovns %ecx, %edx movl %edi, %ecx xorl %edx, %ecx testb $16, %sil cmove %edi, %ecx leal (%rdx,%rdx), %edi movl %edi, %eax xorl $27, %eax testb %dl, %dl cmovns %edi, %eax movl %ecx, %edi xorl %eax, %edi testb $32, %sil cmove %ecx, %edi leal (%rax,%rax), %ecx movl %ecx, %edx xorl $27, %edx testb %al, %al cmovns %ecx, %edx movl %edi, %ecx xorl %edx, %ecx testb $64, %sil cmove %edi, %ecx leal (%rdx,%rdx), %edi sarl $7, %esi movl %edi, %eax xorl $27, %eax testb %dl, %dl cmovns %edi, %eax xorl %ecx, %eax testb %sil, %sil cmove %ecx, %eax ret 
I'm a bit late, but [Haskell Is Not Not ML](http://link.springer.com/chapter/10.1007/11693024_4) may be of interest.
&gt; GHC has no real way of knowing it could unroll that big loop in that it is only ever 8 items long. (small) fixed-size loops are quite common. If the Haskell compiler doesn't have a way to know that, then perhaps some kind of unrolling annotation should be added to the Haskell standard. &gt; Anyways, with that multiplication devolves to 3 fast lookups in 256 byte tables. Lookups in three small tables should be quite fast. But in a real program it increases the pressure on the L1 data cache. So you need realistic benchmarks (that don't always keep those tables in L1) to test the performance of your solution.
Great, I didn't know you took over maintainership. Did you also take over haskell-packages?
And adding whole-program optimization to GHC would be substantially more complicated? Also, would whole-program optimization make it possible to eliminate the GC (at least in some cases)?
If you look at the definition of `Void` there are roughly two varieties. data Void data Void = Void !Void The first roughly says "there is no way to create `Void`" and the second says "the only way to create `Void` is if you have a `Void`". The first is *much* more correct, but the second is a common trick to achieving the same idea. The most important function around `Void` is `absurd :: Void -&gt; a` which notes that `Void` can be converted into any other type. We can write this using the `EmptyCase` language extension directly and correctly (and in parallel with the first definition) or write it using a trick from the second. absurd :: Void -&gt; a absurd v = case v of -- empty case! -- never happens, never returns, -- represents any value absurd :: Void -&gt; a absurd (Void v) = absurd v -- we'll pass the burden of executing absurd -- on to the inner Void... forever The thing I'm trying to focus on here is that the second version of each sort of simulates the first by deferring the question as to what the values of `Void` are forever whereas the first simply states exactly that `Void` has no values.
I'm talking about "View updating and relational theory". It's related to TTM (I don't have a copy of that, unfortunately). &gt; However I am very pessimistic about getting anything usable. The views would have to be so restrictive as to essentially be isomorphisms so this doesn't sound particularly useful. Yes, view updates are only for relatively simple views (i.e., only those involving joins, restrictions, projections, and combinations of those, basically). However, when there are a lot of such views, then view updates tend to simplify matters considerably. EDIT: grammar
No, I did not, and I don't know who did.
GHC already does whole-program inlining of functions so it's not like that kind of optimisations are impossible. However, taking it further than that would be difficult. The most interesting optimisations depend on a strict high-level intermediate language (like GRIN, not like C--) and a very flexible RTS. GHC has neither. Some optimisations can put objects on the stack instead of the heap in certain situations. But I don't think we can get around the need for garbage collection completely.
scion-browser already does that. Reads all packages from a sandbox and/or hackage and generates a sql database of all names, etc.
in all honesty, i think people here are overestimating haskell's usefulness in doing graphical work. i had an almost identical question a year ago, and almost nothing has changed. the diagrams library is by no means a beginner or even intermediate friendly library, which has been explicitly confirmed by the designers. furthermore, it is really tailored to generating static images and is meant to be replacement for something like tikz rather than processing, which does more dynamic graphics and animation. gloss is neat but isn't fleshed out and is more of a one off project. after that, it's an open world, i.e., no man's land. there are wrappers and stuff for GUI libraries and OpenGL, but have fun using those in haskell because not only will you have to learn haskell and OpenGL separately, you'll have to learn how they interface, which isn't natural for either. if you like processing, there is a [scala version of processing](http://va.lent.in/scala-processing/). scala is a functional and object-oriented language that targets the jvm. also, since you mention you just found out about haskell yesterday, i recommend you take a look at f#. haskell has better syntax, but f# is way better suited to productivity due to visual studio and the .net framework. i have moved to learning f# instead of haskell with the full intention of developing graphical and audio programs in the same vein as processing. will all that said and done, i recommend learning haskell. but i also recommend looking at other functional languages like f# to do your actual project in.
Wait until he finds out Haskell to Javascript compilers exist.
Thanks for the reality check. After looking into both gloss and diagrams I definitely agree with your sentiment that neither are "beginner intuitive" in any way. Processing has everything neatly laid out for the non-programmers amongst us, especially the documentation with the detailed examples are truly helpful. Both gloss and diagrams absolutely rely on the user being quite in the know already. Also, the static vs. dynamic difference you mention are something I'm not sure yet how I'll be to work with. I'm under no illusion that Haskell/diagrams/gloss might anytime soon become a viable alternative for Processing. (At least for what I need). Still, I'm all for learning new ideas and tickling my brain, so I'll try some steps into Haskell and see what may come from it. While F# definitely seems like a solid option, I've been a Mac guy for years now, so there's that. 
&gt; I personally prefer this last definition to the monadic one written by gelisam above. Me too, but I decided to stick to the style used by the OP. Let's not overwhelm newcomers with too many new things until they have mastered the basics :)
Oh, I didn't know that. Thanks for the heads up! Now I'll definitely have a look into F#. I'm actually reading through "Learn you a Haskell" right now. I love the friendly and casual tone of the book, like the author is teaching me in everyday language instead of expecting me to be an experienced programmer and know all the programming jargon and ins and outs already. On a slight sidenote: I've too often encountered potentially interesting projects which are just so damn inaccessible due to their creators not being able to "educate downwards" from their high level of knowledge. For example: Instead of step-by-step instructions, the only notes for installation are along the lines of "build using xyz". Expecting me to have all the background to know what this actually means. It's just so very frustrating to know there's so much great stuff out there, but it's hidden behind a high access hurdle. 
Yeah, I was worried that my answer might not be the most efficient, but I wasn't sure introducing the particularities of WHNF and deepseq might be too much off an info dump, especially when I don't have a great mental model for it myself and wasn't sure of the type of `results`. Assuming `results` was also a list would `seq (length results) (hClose handle)` be fine? It also just occurred to me that `readFile` might be the simplest solution, in that it's also lazy if that's what you want, closes the handle automatically when you're done with it, and doesn't require dealing with `seq`ing. Though I don't think it allows you to open the same file twice in the same program, since it's ambiguous when it will get closed the first time.
yep, Miran Lipovača, the guy behind learn you a haskell, has a great knack for teaching and communicating technical topics. and i'm right there with you on the failure of the software engineering and programming community to effectively educate downwards. i think it's a real problem. unfortunately, i think haskell has it pretty bad outside of learn you a haskell and a few of the other books. a lot of the packages have almost zero documentation, and there is very little to get you going into a project once you learn the basics. i even got into a discussion with one of the diagrams developers because i disagreed with their approach in that it wasn't usable from a beginner/intermediate perspective. that's why i love processing so much. but i think even it has limitations due to the textual based programming environment. my project i have in mind is to address this, as i thought it would be a great project to get into functional programming. however, i struggled highly with getting anything going in haskell, so i just started prototyping in processing itself. this was nice, but i have eventually jumped to f#, and i feel like it's the place to be. visual studio is complicated, but is not the hard to get a basic grip on, and f# is an extremely powerful language. it isn't as "beautiful" as haskell, but i would argue that it is more productive to due the industrial backing and .NET framework, which was recently announced to be official coming to max os x and linux. i also find the f# community, who is mainly active on twitter, to be very friendly and more practical oriented than the haskell community. however, i am still learning it along with c#, so we'll see. :)
If you are in a project where you control all the data and you can impose in the design to have everithing in a single place to the client, then you can use your SQL database. But if you have to integrate distributed things because you need it for wathever reason or because data is already distributed, then you can not use SQL. I think that the natural way to use databases in haskell is to have a thin layer of STM registers that act as caches of the one or many databases that you may have out there. A cache which uses the RAM inteligently, to do things fast, with a query language that uses native haskell syntax, like TCache. That is the direct translation to Haskell of the key-value caches products like Redis, memcached etc that are pervasive in distributed and scalable applications. That way you can plug in new sources of data from different databases But this is more powerful, since it is possible to perform transactions with registers in memory thanks to STM, unlike in the case of ordinary key-value caches, which are only for querying data. There is no other way folks. If you want to scale beyond a single box and you don´t want to compromise it, you need that architecture. And I think that this is the best way to use what Haskell gives to you. tcache by the way has interface for Persistent
&gt; Your current multiplication is pretty much guaranteed inefficient. GHC has no real way of knowing it could unroll that big loop in that it is only ever 8 items long. Isn't there some way to write the multiplication algorithm while telling GHC that there's a fixed amount of iterations to be done? After all, this is really an exercise with this particular algorithm. I suppose I could manually unroll it, but that would hardly be idiomatic. I also think that a table lookup, even if we are just talking about 256 bytes, can take longer than the C version of this in real world applications, simply due to caching. The C version doesn't need any memory access at all once the input values are loaded (assuming that the compiler doesn't do any unneccessary register spilling). Of course, this is just a gut feeling, I haven't tested it. And besides, I really like that algorithm. It's nice to see that polynomial multiplication modulo an irreducible polynomial can really be implemented efficiently (in C at least) using only shifts and XORs. It makes both the mathematician and the computer scientist in me happy.
You could have your app call (the equivalent of) `exec $SHELL` at the end. Your app process would be replaced by a new shell that inherits whatever directory you've set. One thing is when you exit or `^D`, you will pop back to the previous shell and directory. You may consider this a bug, but I think it's a feature.
Do you know the works of [Bret Victor](http://worrydream.com/)? He has for quite some time now argued that there has to be a better way to teach and learn programming than there is today. And while I believe he is a tad idealistic in some of his arguments, I really admire a lot of his thinking and talks he gives on this topic. Check out the talks under the "Demos" section on his website. All are great, but "[Inventing on Principle](https://vimeo.com/36579366)" is especially worth watching. 
And we know how that one ended, too ;-)
Hmmm, I'm struggling with the notion of paying $25 to watch those two guys
if i would by a book about haskell (and i can think of a lot of good ones) i get "30+ hours of content" for 20 - 30 $. why would i buy this video?
&gt; Most of these optimisations come at the cost of increased complier complexity - as one is effectively starts by removing the simplifying assumption of separate modules. I think this point is fairly debatable - there could easily be many simplifications if you fully take advantage of WPO. For example, MLton defunctorizes and monomorphises the whole program up-front before any other optimizations; this lets them use a much simpler pair of Intermediate Representations than competing SML compilers, which lead to several other simplifcations: see http://mlton.org/References.attachments/060916-mlton.pdf for a fairly good slide deck. MLton is also IMO a good example of a whole program compiler that's practical and scalable. GHC kind-of does a degenerate form of whole program optimization anyway; it inlines across modules, so in theory exporting everything from every module and then pulling in the transitive closure of every function when you compile a program would work. But the real advantages from the LHC approach more come from a very different low level intermediate representation called GRIN (which is whole-program by design), which is also used by JHC. GRIN has some notable characteristics; one being that the compiler generates thunk evaluation code based on a type, rather than the compiler having a generic thunk evaluation procedure. So you can automatically specialize types to their particular thunk evaluation procedure. You can also easily perform SSA-alike analysis (GRIN is equivalent to an SSA form, and despite being imperative it has a `case` statement, which becomes a phi node) so it's relatively easy to use other common optimizations to clean up the code with things like CSE or whole-program dead code elimination. Anyway, there's a lot GHC could pick up from something like LHC and vice versa I think. I believe (but don't know for sure) that several of the optimizations from LHC/JHC could be re-imagined in GHC somehow, but the designs are very radically different. Disclaimer: I used to work on LHC with David a long time ago, but I obviously now work on a competing product. :)
Regarding IORef based doubly linked list - is there a way to unbox IORef to achieve same memory efficiency as in C?
I'm thinking broader than that. When building IDE-like tools for text editors such as emacs, vim, sublime, atom, light-table etc. you would want a lot of the same functionality and would end up using a few different libraries/binaries for that. For instance with ghc-mod you can get compile errors into your editor, stylish-haskell will auto-format the code, scion-browser can give you information about packages and so on. But I want to use them all, preferably with some sort of uniform interface to make the integraton simpler. BuildWrapper seems to be doing some of the things I'm looking for, but I'd like even more functionality. I'm not saying someone should make it, I can do that myself, just wondering if it's already an ongoing project I haven't found yet
Because the book about intermediate topics and modern workflow doesn't really exist yet. Say what you will about the price point, but it makes sense economically from the perspective that they're talking about a niche topic that isn't well represented elsewhere.
This is an impressive but stepping back and looking at the monster you've created with UndecidableInstances, Reflection, and Profunctors makes me somewhat disillusioned about the state of high performance Haskell code. The C code is simple and concise, and this is using all sorts of crazy trickery to accomplish the same result.
A very nice talk. Very theoretical but with a foot in the ground and it is interesting to the end since it is full of practical examples.
I've been studying this, but I don't think I'm completely following. When you say, "it builds in parallel" do you mean, "this part can be made parallel"? I don't see anything mentioning that the Data.Monoid members inherently are parallel behind the scenes, and you don't seem to be using par or anything. (I'm new to par, these functions, and the category theory terms you're using) It seems like folding monoids should be inherently parallelizable because they require associativity. If I'm summing a long list (1 + (2 + (3 + (4 + (5+6))))), gets the same result as (1+2) + (3+4) + (5 + 6).
Peace upon you great and brave people of SQLLand. There are two different things here: one is to use noSQL or SQL. Another different case is when you have more than one database . Independently of what you choose and the size of your project, in the second case you need something addittional to manage both data sources. I did not mention any preference for SQL or noSQL above. 
This is a bit boring - last time someone posted a link to a video in this series, the whole discussion was about the price too. I personally also feel it's a bit expensive, but I chose to pay for it. I would be very happy to pay a few dollars per video or for a subscription for something like Elixir Sips - or for example for Jekor to continue making videos (https://www.youtube.com/user/jekor). But anyway, this video is available, and hopefully that's useful news to the Haskell community (if not, downvote this post). If you watched it and have feedback or discussion, go ahead, but let's not spend all of our time talking about the price. :) So far in the video I am appreciating some of the Emacs integration, and thinking of how that could be expanded/more efficient (integrating hoogle with something like helm, instead of having to open a browser to scan for function names). Also seems like the parsec module namespace is a bit too fine-grained, but there might be good reasons for that (perhaps something like Halberd (https://github.com/haskell-suite/halberd) could help.
Thanks for the response! So if I understand correctly, there's no way to parse a file without getting a type involving IO, which sounds like it could get very messy for what I'm working with.
can't you copy the pre-built binaries to your new VM?
I'd just like to follow up - I've received confirmation from Amazon that a complimentary line in the licensing header referring to the service model's license is all that is needed. I'll promptly add these to the generated output. Again, thanks for bringing it to my attention /u/joeyh.
The thing I accomplished above lets me safely work with multiple different polynomials. The C code on the other hand has to be recompiled for every different polynomial in question. Also half that import list isn't used. I showed this code to demonstrate that you can do it via table lookup, because it was code I already had lying around. You can do the table lookup for any fixed polynomial with very simple code. GF8 0 * GF8 _ = 0 GF8 _ * GF8 0 = 0 GF8 a * GF8 b = case reflect (Proxy :: Proxy s) of T8 _ lg e _ -&gt; GF8 $ e (lg a + lg b) That shows that all we need to do is find the logarithms, add them and then re-exponentiate. For GF(2^8) that can be cached in lookup tables. That's it. That is the "meat" of what it is I wrote. Everything else is window dressing I wrote in an attempt to put this thing to bed once and for all, for all purposes. I didn't get to an API I liked so I dropped it.
http://unlines.wordpress.com/2010/11/15/generics-for-small-fixed-size-vectors/ talks a bit about a way to make small fixed sized vector calculations fast.
In this case it is almost universally a win. Every high performance implementation of AES I know of uses this style of approach. 512 bytes of lookup tables falls below almost any cache thrashing theshold. You aren't going to be using dozens of different polynomials.
What about just using tuples? So you'd have something like annotate :: a -&gt; (a,Annotation) annotateAST = fmap annotate Disclaimer: I've never done anything with ASTs.
It's kind of hard to explain, but I'm trying to parse through a number of files and store each one as its own data type (with relevant information in (Header, Data) pairs a.k.a. "Definitions"). Then I want to be able to perform certain computations based on those definitions, modifying them if necessary, then finally turning everything into a single pretty-printed document containing relevant information from different files (based on whatever I define to be relevant). I can't really explain much more than that as I'm still planning/prototyping and am not sure how the project will evolve.
This gives me a speedup of roughly 1.2, but it's still a lot slower than the C version.
No dice I think, ASTs are recursive, so if we have (a, Annotation), we'd also need to replace each subvalue of an a with (a, Annotation).
http://brianmckenna.org/blog/type_annotation_cofree Cofree! The current hotness is writing your AST as data AST a = Add a a | Neg a yada yada and taking the fixed point. This can be kinda nice since you can send an expression from Fix AST to a list of statements [AST Int] to get a nice graph repr and then hash cons and do other things. If you don't wanna think about cofree just yet, you can imagine a decoration as sending the fixed point of AST a to the fixed point of some Annot (AST a). It's nice!
At this point without getting the benchmark, it'll be hard to tell. Would it be possible to post the benchmark code? (and the compilation options used)
You've probably got this covered - are you compiling with - O2? Could you provide the Haskell code for the benchmark? Sometimes that can be where the surprise is. I'd also consider using criterion for my own code (using FYI for the C), but that might just be my default :) 
Reposted to /r/haskellgamedev
I don't doubt that your code works as you described, or that its more polymorphic than the C. But it's also highly unidiomatic Haskell, the Reflection library [is doing all sorts](https://github.com/ekmett/reflection/blob/master/fast/Data/Reflection.hs#L103) of tricks and ``unsafeCoerce`` at the fringes of the language to make what you've done work. Maybe you disagree and think this is simple since it's abstracted behind an API, but I'm sure I'm not alone in thinking that pulling Reflection and Typeable hacks into a codebase is not a good solution. I'm sure your code is efficient, but the C code is efficient and idiomatic.
 main :: IO () main = print . sum $ [GF256 a * GF256 b | a &lt;- [0..255], b &lt;- [0..255]] I then measure it using `time` on the command line. It's crude, but then again it's only a toy example. The C version does the same using loops and an accumulator variable for the summing. As far as compilation options go, I've been compiling using `-O2` with GHC and `-O3` with GCC for the C version. (EDIT: Fixed compilation options)
Yes, logict! import Control.Monad.Logic fromList = foldr mplus mzero . map return nats = fromList [0..] main = print $ observeMany 10 $ nats &gt;&gt;- \x -&gt; nats &gt;&gt;- \y -&gt; return (x,y) You can easily generalize it to n-tuples by writing a version of `replicateM` that is based on `&gt;&gt;-` instead of `&gt;&gt;=`.
I have now, and it gave the most substantial speedup so far, cutting time almost in half. I've read about stream fusion before but never had a closer look. Looks like I should, because it seems to be worth it. Thanks!
That description fits a generic structure which is very common for Haskell programs: accept data from the outside world, process it in some fashion, then output the result to the outside world. The first and last part need to use IO, but the processing part can be pure. Something like this: -- parseFilesIntoDefinitions :: IO [Def] -- process :: [Def] -&gt; Doc -- prettyPrintDocument :: Doc -&gt; IO () main :: IO () main = do defs &lt;- parseFilesIntoDefinitions doc &lt;- process defs prettyPrintDocument doc In fact, even most of the parsing and most of the printing can be pure: -- readFile :: FilePath -&gt; IO String -- parseStringIntoDefinition :: String -&gt; Def parseFilesIntoDefinitions :: IO [Def] parseFilesIntoDefinitions = do str1 &lt;- readFile "/path/to/file1" str2 &lt;- readFile "/path/to/file1" let def1 = parseStringIntoDefinition str1 let def2 = parseStringIntoDefinition str2 return [def1, def2] -- convertDocToString :: Doc -&gt; String -- putStr :: String -&gt; IO () prettyPrintDocument :: Doc -&gt; IO () prettyPrintDocument doc = do let str = convertDocToString doc putStr str Does that kind of structure make sense?
 You don't need to use reflection's magic. You can write a concrete instance for the few polynomials you use. data CRC8 instance Reifies CRC8 T8 where reflect _ = crc8_luts That never invoked `unsafeCoerce`. I used an existing class so I could get better support for some scenarios I envisioned that go far beyond the scope of the question here. I pasted code I had lying around. It is a big blunt hammer with which to smack the problem, but nobody is unsafeCoercing anything here. You can of course hard code this for one case without any of this. Code that does so just winds up _faster_ than the C version by a long shot, which was the point, which got buried in all this wanking about about the fact that reflection exists.
Interesting, sounds similar to Esqueleto. I guess we wait until the 1st to see first hand, but what are the gaps that Opaleye fills in that Esqueleto doesn't already provide? Great to see the type safe query DSL landscape evolving in Haskell, good times ahead...
This produces pairs in a rather strange order.
By Cantor, it can't enumerate all the sublists of an infinite list, though ;-) Can it do all the finite lists easily, I wonder?
Perhaps your Show instance is slowing things down (relative to the C version) - what happens to the benchmark if you just use the derived Show?
It's the normal diagonal order. Say our pairs are points in an infinite grid: ... ... ... ... ... (0,3) (1,3) (2,3) (3,3) ... (0,2) (1,2) (2,2) (3,2) ... (0,1) (1,1) (2,1) (3,1) ... (0,0) (1,0) (2,0) (3,0) ... Then if we want to enumerate all of them we can't go row by row, or column by colum, b/c we'll never get to (1,1)! One order that's often used is the diagonals: ... ... ... ... ... 6 11 17 24 ... 3 7 12 18 ... 1 4 8 13 ... 0 2 5 9 ... Which is what /u/roche has done It guarantees that we'll get to any point (x,y) in finite time.
It's too bad the audio didn't come through better. For those who were there, to what extent was it similar/different from his talk at this year's ICFP [1]? [1] https://www.youtube.com/watch?v=O805YjOsQjI 
I think you need to replace `data` with `newtype` in your second definition. `data Void = Void Void` has one value, the infinitely large `fix Void`, which the `newtype` construction gets around by defining its construct as strict (so `fix Void` is another name for `undefined`).
It is a shame that the original GRIN papers are not around any more.
Yeah, I was disappointed about that. We had some audio problems that day. That's why it took me this long to get it up. It was a great talk though, and a huge turnout of something like 80 people IIRC.
Try the vimeo ones, Richard's audio seems to be much better there. http://vimeo.com/groups/166440 (It seems like his mike was not active, but instead a far-away one picked up the voice.)
Oh, yep! Good catch
Looks kind of like unfoldr http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-List.html#g:9
Very good talk. Do you have the slides? In D I use both precise types, unittests, and run-time contract programming. They guard against different kind of problems. Contract programming is intermediate between the other two, but also different. Sometimes you can even put a brute force but probably correct algorithm in the post-condition, to verify the efficient algorithm not just on specific cases of the unit tests (or synthetic cases generate by Dashcheck), but also for all the real inputs the function will see (that are smaller than a certain threshold) when in use.
It's similar I know, but I don't know how I could re-write with `unfoldr`
I came up with this: chain f x ys = unfoldr f' (x, ys) where f' (_, []) = Nothing f' (v, w : ws) = Just (i, (j, ws)) where (i, j) = f v w Which can be reduced to: chain f = (unfoldr f' .) . (,) where f' (_, []) = Nothing f' (x, y : ys) = Just (i, (j, ys)) where (i, j) = f x y
It reminds me of the State Monad, but I don't know how I would rewrite with it.
No, it most certainly is not that order. It actually prints [(0,0),(1,0),(0,1),(2,0),(0,2),(1,1),(0,3),(3,0),(0,4),(1,2)] You will note that every other tuple has `0` as its first element, every fourth has `1`, etc. The diagonal order would be [(0,0),(0,1),(1,0),(0,2),(1,1),(2,0),(0,3),(1,2),(2,1),(3,0)]
Modifying the above to go :: Word8 -&gt; Word8 -&gt; Int -&gt; Word8 go !_ !_ !summed !7 = summed go !a !b !summed !bit = go a' b' summed' (bit + 1) where and compiling with -fllvm, I get within a factor of two of the C version. Without the type signature, bit defaults to `Integer` and without the extra strictness annotations `a` doesn't get unboxed.
For the most part it makes sense. Except where you do: doc &lt;- process defs Since I thought process would have issue with IO [Def]. Edited to add: Since process is [Def] -&gt; Doc. Now I just need to find a way to get around Haskell's lazy-read so I can actually do the things I need.
http://stackoverflow.com/questions/7141287/haskell-cartesian-product-of-infinite-lists contains several ways to do this. I once used one of them to help me implement [a program which prints the set of all programs which halt](http://codegolf.stackexchange.com/questions/15727/enumerate-an-undecidable-set/15740#15740).
Yep, I meant that it's possible to implement parallel versions of `mconcat` and `map` without much difficulty, not that the current implementations are parallel. Sorry, I should have been more clear about that.
Whoa, this is super cool, thank you for linking this!
Not only is it hip, it can be easier to develop in for people, especially if your schema or requirements keep changing during development/prototyping. It's usually not the right choice, and we're starting to see the NoSQL database start adding checks and guarantees that look an awful like good old SQL. Then there are people who just don't like SQL *its self* and would prefer something like JS even though the *relational model* may be fine with them.
Mostly correct. SQLite is designed to be fast and embedded, but you're right it types are (from my VERY limited knowledge) something of a 'suggestion' and not really enforced. MySQL's older storage engine, MyISAM, worked mostly the same. You could add foreign keys or check constraints but they were ignored. Not-null columns mostly worked, but you could get around it. Want to set a date of all 0s? Sure! Did you know you can insert 17 into a date column? You probably can. By the way there are no transactions. MySQL's newer engine, InnoDB, is *much* stricter. Now 'new' is something of a misnomer as it's been around for a decade or longer, but only recently has it become the default. Almost everyone doing serious work would explicitly choose to use it. Using InnoDB (and fixing some historically lax defaults) makes it much more like Postgres where things work correctly and don't silently accept stupid mistakes. Not-null *means* not-null, foreign keys are checked, transactions work (inserts/updates/deletes/etc only, no create/alter/drop table). Both Postgres and MySQL with InnoDB are fine choices.
I bet there is some way to do that in Postgres since you can define your own data types. Even if there isn't a built in feature, you could use triggers or check constraints to enforce the rule.
The first half of that is sometimes known as the non-recursive [Mealy state machine](http://hackage.haskell.org/package/machines-0.4.0.1/docs/Data-Machine-Mealy.html). data Mealy i o where Mealy :: (s -&gt; i -&gt; (o, s)) -&gt; s -&gt; Mealy i o pureMealy :: (i -&gt; o) -&gt; Mealy i o pureMealy f = Mealy (\s i -&gt; (f i, s)) () runMealy :: Mealy i o -&gt; [i] -&gt; [o] runMealy (Mealy step s) = go s where go s [] = [] go s (i:is) = let (o, s') = step s i in o : go s' is The primary thing to recognize from this structure is that `Mealy` instantiates all kinds of typeclasses. Relevant ones include: * `Arrow` (also `ArrowChoice` and `ArrowApply`) * `ArrowChoice` * `ArrowApply` * `Profunctor` (also `Strong` and `Choice` from `profunctors`) * `Category` * `Monad` (slowly) * `Functor` * `Applicative` Things like `Applicative` let you "zip" two Mealy folds together. Things like `Category`/`Arrow` let you build pipelines of Mealy machines.
I had thought about this exact problem a little while ago. I wasn't really satisfied with simply doing diagonalization, since if you do repeated applications you get different orders, and it "favors" the last dimension you zipped in. If you look at the output that omega (which I hadn't heard of before reading this thread) gives, you'll see what I mean. Notice how one of the dimensions reaches a much higher number than the other two. Prelude Control.Monad.Omega Control.Monad&gt; take 40 . runOmega $ liftM2 (,) (each [0..]) (liftM2 (,) (each [0..]) (each [0..])) [(0,(0,0)),(0,(0,1)),(1,(0,0)),(0,(1,0)),(1,(0,1)), (2,(0,0)),(0,(0,2)),(1,(1,0)),(2,(0,1)),(3,(0,0)), (0,(1,1)),(1,(0,2)),(2,(1,0)),(3,(0,1)),(4,(0,0)), (0,(2,0)),(1,(1,1)),(2,(0,2)),(3,(1,0)),(4,(0,1)), (5,(0,0)),(0,(0,3)),(1,(2,0)),(2,(1,1)),(3,(0,2)), (4,(1,0)),(5,(0,1)),(6,(0,0)),(0,(1,2)),(1,(0,3)), (2,(2,0)),(3,(1,1)),(4,(0,2)),(5,(1,0)),(6,(0,1)), (7,(0,0)),(0,(2,1)),(1,(1,2)),(2,(0,3)),(3,(2,0))] Prelude Control.Monad.Omega Control.Monad&gt; take 40 . runOmega $ liftM2 (,) (liftM2 (,) (each [0..]) (each [0..])) (each [0..]) [((0,0),0),((0,0),1),((0,1),0),((0,0),2),((0,1),1), ((1,0),0),((0,0),3),((0,1),2),((1,0),1),((0,2),0), ((0,0),4),((0,1),3),((1,0),2),((0,2),1),((1,1),0), ((0,0),5),((0,1),4),((1,0),3),((0,2),2),((1,1),1), ((2,0),0),((0,0),6),((0,1),5),((1,0),4),((0,2),3), ((1,1),2),((2,0),1),((0,3),0),((0,0),7),((0,1),6), ((1,0),5),((0,2),4),((1,1),3),((2,0),2),((0,3),1), ((1,2),0),((0,0),8),((0,1),7),((1,0),6),((0,2),5)] The solution I came up with involves treating some pieces of the list as having the same "grade". I haven't made it a polished library, but I [made a gist of it](https://gist.github.com/bhamrick/472159c4a583b7c4fb75). I also made it a [blog post](http://www.brianhamrick.com/blog/monadic-infinite-nondeterminism) if you like bad formatting.
It is my understand that MySQL still will silently accept a lot of bad data, with all sorts of weird and unexpected behaviour, where PostgreSQL will not. For MySQL to be strict you need to enable the various strict modes. For example, division by zero becomes null unless you explicitly enable the strict flag to fix this behaviour.
There's at least [one GRIN paper](https://github.com/Lemmih/lhc/blob/master/papers/optimization/The%20GRIN%20Project.pdf) in their GitHub project.
Hm. Interesting idea. Since I am not using this command too often, this draw back is not so critical. I'll try this. 
I got stuck on that point as well.
I'm pretty sure the blogpost is missing the definition of the two pattern synonyms where they are made explicitly bidirectional. I wish the author had full self-contained examples: I have a feeling his last pain point could be worked around by adding a (somewhat overly-restrictive) explicit type signature to his pattern synonyms, but I can't check without a full example.
Ah! You can use it like so. pattern (Add a b) &lt;- (proj -&gt; AddF a b) where Add a b = inj (AddF a b) I will update the post this evening. EDIT: I have just updated the post to explicitly include how to change the pattern. 
&gt; When you say you "tested" these functions, you probably meant you tried to print Void (that's what GHCi will do). Yes, you're right. I keep forgetting about `Show`. If I use `seq (g 1) 2` then I do get an exception '`Prelude.undefined`'.
Thanks for clearing things up.
Interestingly, "pain-free unfix" or more generally "pain-free encodings" (typically "pain free *universe* encodings) has *always* be the justification for pattern synonyms. This was precisely [the example Conor McBride gave](http://www.reddit.com/r/haskell/comments/1kmods/patternsynonyms_ghc_trac/) when proposing pattern synonyms in GHC. Before this, it ("coding datatypes") was also the justification for the feature in [SHE](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/patsy.html), his Haskell preprocessor. Datatype encodings (but not unfix) were also the justification for them in the 92 report of Aitken and Reppy.
Your idea about cpp conditional will work just fine. Just set it in your cabal test section. You will have double compilation though, because cabal won't reuse any result built earlier. Most people just use a Module.Internal that is not part of the pvp contract and is understood not to be used whenever possible. 
I think /u/chrisdone has some preliminary code in his github account.
Upvote for transparency and no BS.
You can split your module in two files say ModuleName and ModuleNameImpl. ModuleNameImpl contains the functions and exports everything. ModuleName imports all of it and exports only your public interface. In your library you simply don't expose ModuleNameImpl. In your test suite, however, you can import ModuleNameImpl to also test the internals.
OK I went through the pain of fixing it up so that it actually compiles: * First version, with single `View` typeclass: https://gist.github.com/gergoerdi/be03746df42490db13cc * Second version, with separate `Project` and `Inject` typeclasses: https://gist.github.com/gergoerdi/13bf90ac17079997a123 The second one doesn't typecheck: * If you don't give a type signature to the pattern synonyms, their type is inferred from the pattern definition only, and so the builder definition fails because it lacks the `Inject` context * If you do give them type signatures (like in the gist above), then `p4` fails to typecheck because there's no `Project HuttonHole` instance. See related comments here: https://ghc.haskell.org/trac/ghc/ticket/8581
You should not test internals. If you test your internals, you break the nice encapsulation that your API provides. When you test your internals, you couple the implementation with the specification, which is bad both from a reasoning standpoint, but also because then you can't change the implementation without throwing out all the tests you had. (And remember that tests serve as documentation of bugfixes – you don't want to throw that out.) Just test your exposed API. When you test against your API, you will automatically also find out if the implementation is broken. (If a part of your implementation can be broken without it affecting the API, is it really an important part to keep?)
Say you have a library and a test section. Yes, now with Cabal you can have the test suite reference the library, but you can also do it old style, and just include the library source files in your test section. Then your modules can expose everything, including internals you want to test. But these modules are not exposed directly in the library. Instead you expose a high level module that imports all the internal modules and only reexports the public library API.
&gt; When you test your internals, you couple the implementation with the specification I don't understand how you figure this. Let's say I implemented a functional dequeue and provided an API. Internally I used some kind of balanced tree and I want to test balancing properties of the tree.(for example, a QuickCheck test that makes lots of modifications on the tree and compares height with number of elements in every move) What you're suggesting is that I shouldn't test this because it "couples" implementation with the interface. I don't see any coupling here.
In that case, I suggest making the tree a separate module with a public API that can be tested. Then use that tree to implement your deque, which is tested not on its balanced tree properties, but on its deque properties. It feels like I'm throwing buzzwords around by now, but I just wanted to say that by splitting the deque into two modules this way, you achieve better separation of concerns, which is widely considered to be a good idea anyway.
Same here!
Is it suitable for *any* of the traditional use cases of a doubly linked list at all? In the code base I'm working on, the main uses of doubly linked lists are constant time append (queues and buffer lists akin to `ByteString.Lazy`) and reverse traversal, followed by things like removal of elements from the middle of the list, that can be done with a singly linked list, too, but there's `std::list` at hand. `DList` does not seem to be good for any of them.
Could you recommend some prerequisite material for this blog post? I'm not familiar with the concepts: Fix, Unfix.
What still looks a bit awkward to me is that script contents are passed through verbatim. In general, it is not safe to do so. OTOH; there seems to be no general way to escape script contents, so it is not clear to me what one should do in this case.
Tests has often been a nightmare for me. I write tests for my internal functions. But then very often if I do a slight change in the algorithm with no impact on the user, I end up having to update all my tests. Indeed I perform test against the internal representation, which might evolve. So the tests are very fragile. I end up spending more time on fixing the tests than improving the algo. What I am doing wrong here? I guess I should rethink my tests for checking better invariants. Anyway making future proof tests is very hard IMO.
Cool! Here I found Urban Boquist's PhD thesis: http://www.cs.uu.nl/docs/vakken/macc/boquist.pdf Generally a Google query with "grin boquist type:pdf" (or maybe type:ps) seems to result in some hits.
I've been listening to people make dire threats about testing my internals for years. I've been testing my internals for years, as-needed (I.e., not just automatically, but when it is helpful). Maybe once have I ever actually had to discard a chunk of tests, and that still wasn't exactly traumatic. I'd rather have my code tested than my test code constrained by layers of design not intended for it. All this subject to the usual Haskell caveats about testing, of course, in the case of Haskell. But it is still helpful to test the IO internals I may not want to expose publicly, and it is yet to actually manifest the bad things that are supposed to happen. (And yes, I know what they look like and would recognize them if they happened.) For me this falls under reality trumping theory.
You could try http://hackage.haskell.org/package/Annotations
Super cool. I was just trying to do this but you keep beating me to it! If only I didn't take to write all this javascript at work. 
Cactus had a good question about mutually defined ASTs. Gallais suggested using indexed families to solve this (much better than my location-based `Weave`). I had a little trouble getting a recursion principle out of Gallais' code, so I generalized it a touch and it's available here: https://gist.github.com/tel/99e666308d270a3d1d8c This might be of interest here generally. I feel like I've seen this technique around a few times, but I don't think it's common knowledge at all. It's also surprisingly clear once you use DataKinds and PolyKinds to make proper indexed families.
I like D (and Rust) as much as the next guy, but all C functions are already nothrow and nogc ;-)
It is `O(n)` because lists are singly linked. You can however use something like [vector](http://hackage.haskell.org/package/vector) for a much faster collection or Data.Sequence for fast `cons` and `snoc`. Basically, lists are painfully slow in a lot of cases, but we have other data structures.
O(n), since it's a list. If you need a faster data structure, check out [arrays](http://hackage.haskell.org/package/array), [vectors](http://hackage.haskell.org/package/vector), and [sequences](http://hackage.haskell.org/package/containers-0.5.5.1/docs/Data-Sequence.html#t:Seq). **edit**: by the way, I use lists a lot, but I never use `(!!)` nor arrays, so I'm worrying that the fact that you want to use `(!!)` might be a code smell. What are you trying to accomplish?
Yeah, really refreshing to see. 
google for "two-level types" or "data type ala carte" 
I haven't read your entire post thoroughly yet, but I had a similar problem, where I wanted non-determinism to be represented as 'choice', where values that require fewer choices are enumerated first. What I did was use a `Free [] a` (equivalent to `data Tree a = Tree [Tree a] | Leaf a`). The compiler can derive the monad instance, and to enumerate it you perform the iterative deepening algorithm. One issue that arises is that if any of the internally stored lists are infinite the same problem reappears (elements will still be found in the tree, but the iterative deepening traversal won't ever reach them), but if you implement the `each` function to convert an infinite list into an infinite skewed tree then you're fine. Another issue is that for these types of recursive monads `m &gt;&gt;= f &gt;&gt;= g` is asymptotically inefficient in much the same way that `(xs ++ ys) ++ zs` is, but if you throw the tree into a Codensity monad it fixes the issue in the same way that DLists do for lists.
I agree, plus I've also gained a lot of practical tips for free from (http://vimcasts.org/) which seems to be by the same people. One thing I would say in terms of feedback is that I felt the "What you will learn" section for the first video was stretching things a bit when it said "learn how Ollie uses 'emacs for all workflow' and 'NixOS to manage packages'", which was what piqued my interest rather than the actual coding problem. In practice you'd be hard put to learn much about either of these from the video. I'd like to see more information on the environment used (even just some github links to vi/emacs configurations and a working project (rather than just a single .hs file) where relevant). P.S. If someone could point me to a decent emacs/evil setup for haskell development I'd love to see it :). 
Indeed, that's a good point for consideration. There is already the `toHtmlRaw` method, which you could use with `script_`. On the other hand, you would never use `script_` *without* `toHtmlRaw`. Scripts and styles shouldn't be escaped. And even when escaped, you don't need string literals in JS to get nasty things done: eval(String.fromCharCode(97,108,101,114,116,40,39,72,101,108,108,111,33,39,41)) completely normalized HTML, but is equivalent to having written alert('Hello, World!') Exploits will move past simple HTML encoding (in a `script` element) with little issue. To add to the problem, it doesn't matter whether you escape in attributes, e.g. `onclick="alert(&amp;#39;Hello, &amp;gt; World!&amp;#39;)"`. The script still runs. The balance of being a nanny library and being convenient is a difficult one. At the moment I'm leaning towards "don't put user data in script or style tags or scriptable attributes", with an additional leaning of "don't use such elements or attributes in production at all except for short demos". Such HTML terms like `style`, `script` and `onclick/mouseover/etc` come from the old days before we started putting assets in external files and binding to the DOM separately. 
I don't know if you can use setjmp and longjmp in D, but so far I've never seen D code that uses them (and longjmp can't be allowed in @safe code).
I also think it's usually best to rely on testing your external API because that should test your internals as well. Do run code coverage reports to check this. There are times when it's hard to thoroughly test your internals any other way - if so, then do it. The Hexagonal Architecture people have some insight in the best things to test, see http://vimeo.com/68375232
Yeah, old backwards compatibility defaults. Like I said you can turn them off to get correct behavior.
Snap, when I look at the calibre of the people at well typed I wonder whether there is any point in applying... Unless they're explicitly looking for someone to mentor. 
IMO this pay-per-view model really turns off quite a few people. A subscription model such as railscasts, ruby tapas, elixir sips seems to be the way to go. The sweet spot is around 9 USD a month for most people. But then of course you must commit to releasing these vids monthly. But then again this is exactly what most of the subscribers would want anyway. Personally, I'd certainly pay 9 bucks a month if Ollie released medium length videos with such quality guests every month.
You can do c style data structures, they just have to be off heap. I think there's some tools to do this more nicely that will be open sourced in the next few months. But I'm not 100 percent sure. That said, totally doable with the memory tools in base today
That seems low for someone with this skill set and the role. Equivalent roles in my city would pull roughly double that income. Also, the fact that people can and do get executed for victimless "crimes" in Singapore is disturbing. Not to mention the overt control of the media. But that's roughly what you'd expect in a totalitarian state. Pass. EDIT: if you disagree, please respond in addition to your down votes. I'd love to change my mind about Singapore, if there's evidence that I should. It seems like an otherwise beautiful place. 
I warmly suggest every interested person do themselves a favor and interview with the fine folks at Well-Typed, this is a group of folks where no matter your background, there's stuff you'd learn from working with them! I've had the pleasure of collaborating with some of their team in the open source context, and its always a pleasure. Point being, no matter who you are, its a team you'll learn from, and you'll get amazing visibility into the unbelievable diversity of industrial haskell applications. Give them your resume :-)
Drop all the shadows! (I'm not complaining, just noticing your style :)
Here's the talk: https://www.youtube.com/watch?v=hgOzYZDrXL0 It doesn't have explicit recursion, it's interpreted by default, it uses strict evaluation. Just going on what Lennart has said outside of Standard Chartered I don't think anyone has access to it.
I'd also be happy to discuss over private messages if you prefer. But I'm quite serious. I regularly get offered opportunities in Singapore, and consistently pass because I don't want to live in a police state. If that's a misconception, please inform me.
The talk looks interesting, thanks.
Could you support `hr_` and `br_` by making `Term` a single param type class, and converting the existing `Term arg result` instances to `Term (arg -&gt; result)` instances?
&gt; Pain Free Unix Was a bit confused for a second there...
Lennart (and everyone at SCB) like lazy evaluation, and the typical opinion is that we prefer it to strict evaluation. We didn't move from lazy evaluation to strict evaluation, instead we moved from strict a-bit-like-Haskell-but-not-really to strict basically-Haskell.
It isn't. A version that is useful for the traditional usecase is to take an Okasaki-style catenable deque, and then treat the current point as a finger into the deque by maintaining a deque to the left and right of you. That notion gives you all the traditional tools with all the traditional asymptotics, just with a radically different memory layout, and persistence for free.
Thanks for the clarification.
since when is writing an calculator an intermediate topic not covered by books? and there are more books about so called modern work flows then necessary. all this software engineering bullshit bla bla. i would recommend "real world haskell". it is maybe not the greatest book of all time but it is good for learning "intermediate topics" like parsing, "web programming", working with databases and graphical interfaces. after reading some chapters of a good haskell book you can program your own ADVANCED calculator which is capable of more than just RPN style addition and multiplication. and programming a calculator in FP is NOT a niche topic. See "Structure And Interpretation Of Computer Programs" from 1984. Okay, it is lisp, but you can easily translate that code into haskell after reading "Learn You a Haskell for Great Good" for free on the internet.
Yes, I could have used some more imagination. For one, do an in-place update (x++). But, what's very important, i don't want to return anything wrapped in a "SomeTripleNestedWeirdMonad" the caller has to work within. So, if purefunc() just doubles x, and x is an Int, func() returns an Int that anyone can interact with in a "pure" way...the catch is that a pure function can't call func() itself.
The upside is you don't really pay any tax. It's minuscule until you earn big bucks and even then it's pretty low. 
The problem doesn't make any efficiency requirements, so I think you could actually just write the solution as something like zerosToRight arr = quickSortBy zeroMax arr &gt;&gt;= return . length . takeWhile (/=0) where zeroMax 0 a = GT zeroMax a 0 = LT zeroMax a b = EQ Data.Array doesn't implement sort though, so you'd still need to write that yourself. My normal method form making ugly code less so is to break it up into logical chunks and then make those separate functions with good names.
One problem is that the task itself is tailored to imperative languages ("in place", "array"). In Haskell, I would simply assume that the data is given as a list and write it like this movezeroes xs = filter (/= 0) xs ++ filter (== 0) xs This is not what the task required, but the point is that if I encountered a similar problem while writing a real world program in Haskell, I would write this one-liner first and maybe later think about whether there is any reason to perhaps write it in a slightly more efficient manner.
Thank you! This is exactly the sort of response I was hoping for. It's interesting how you frame it (like a corporation) -- I hadn't thought of things that way but it makes a lot of sense. You're definitely right about their being open about their values too, which is good. May I ask what sorts of interactions you've had with the government? Was it entirely with the bureaucracy (DMV, etc.) or did you also interact with the police? Thanks again for your reply!
Of course they don't, that's the trick. And then, when you spit out the solution they'll ask you if there's a better one. They want to check if you'll ever write an O(min(N-k,k)) (in the number of swaps) algorithm, where k is the number of zeros.
When they say expert, I'm thinking the bar must be set very high. Then I see the salary, and for an expert that's really really low. $85k usd after bonus? For EXPERT? What gives?
Unless you're an American, in which case you're still paying U.S. taxes on the portion that's left after you pay your Singaporean ones. But, yes, taxes in Singapore are low, and so easy to file that you almost feel good about having to do it!
Singapore is not much more of a police state than the US, especially if you're a Muslim and/or tanned. 
Any idea if Well-Typed do summer internships for undergraduate students?
Not a bad idea. In my preliminary experiment I encounter some overlapping instances issue with whether the argument to an element `(argument -&gt; result)` is an attribute list (`[Argument]`) or whether it's children (`f` where `f ~ HtmlT m a`). I made [a branch](https://github.com/chrisdone/lucid/commit/1c289e2) with the single parameter class, if you want to play with this idea more. I did the boring work for you. The [Term class is here](https://github.com/chrisdone/lucid/blob/874bf52fdca0adf73aa442e402d302b84d33bd1e/src/Lucid/Base.hs#L167-L199). Fiddle with that, and if `cabal test` both builds and succeeds then you're golden. I made sure the tests check both functionality *and* inference. As I already spent a week playing the “get GHC to infer the right thing” I'm a bit tired of the battle at least for this week. The idea is interesting, though. 
Mu is (mostly) strict by default for historical reasons. You can make things lazy explicitly if you want. It's not a bad compromise. 
 aoz :: [Int] -&gt; Array Int Int So you're already setting yourself up for awkwardness here. The problem says that you're given an array and you're expected to update it in place. Not a linked list. So let's not make things harder for ourselves: aoz :: Monad m =&gt; IOUArray Int Int -&gt; IO () Now given there's no way to do this without mutation (because the problem spec. says so), let's do it: Edit: Fully debugged code: import Control.Monad import Control.Applicative import Data.Array.IO import Data.Array.MArray aoz :: IOUArray Int Int -&gt; IO () aoz buffer = do (lower, upper) &lt;- getBounds buffer let pack readFrom writeTo zeroCount = if readFrom &gt;= upper + 1 then return (zeroCount, writeTo) else do v &lt;- readArray buffer readFrom if (v /= 0) then do writeArray buffer writeTo v pack (readFrom + 1) (writeTo + 1) zeroCount else pack (readFrom + 1) writeTo (zeroCount + 1) (finalZeroCount, nonZerosEndAt) &lt;- pack lower lower 0 mapM_ (\i -&gt; writeArray buffer i 0) [nonZerosEndAt + 1..upper] test :: [Int] -&gt; IO () test input = do print $ "input was " ++ show input asArray &lt;- newListArray (1, length input) input aoz asArray outputList &lt;- getElems asArray print $ "output was " ++ show outputList main = test [1,0,2,0,0,3,4] Yes, that's basically a literal translation of what you'd do in C. If the problem was obnoxious enough to demand mutation, you're going to have to mutate things. Notes: 1. There might still be a 1 pass algorithm I'm missing here. 2. Notice how I used recursion here to make a loop and what would be the mutable loop variables (readFrom, writeTo, zeroCount) became arguments. That way I didn't have to fath around with making them some sort of IO ref where there was absolutely no call to. This is a common thing, so common in fact that there is a convention you will see that an inner function that is used as a loop is just called "go". 3. Tried to make this not care if it was in ST or IO but I couldn't get that to work so I just went for simple. Anyone know what I'm missing?
I see, I can actually see why they'd do that, more familiarity with C++ programmers and better sugar for interoperation with C libraries. Haskell seems to have ended up doing both: Haskell has ADTs which give you type-safe sum types while it provides an `Enum` typeclass that's more in the style of C#'s enums (although I suppose `deriving Enum` doesn't let you pick specific `Int`s in the way C#'s syntax does). PS: Plus the `Enum` typeclass is a little more safe because it's exposed through `fromEnum` and `toEnum` explicitly, rather than any kind of implicit casting
Here is another solution, more obviously correct, fewer efficiency guarantees. import Control.Monad import Control.Applicative import Data.Array.IO import Data.Array.MArray pureSolution :: [Int] -&gt; [Int] pureSolution list = filter (/=0) list ++ filter (==0) list aoz' :: IOUArray Int Int -&gt; IO () aoz' buffer = do (lower, upper) &lt;- getBounds buffer twizzled &lt;- pureSolution &lt;$&gt; getElems buffer mapM_ (\(i, value) -&gt; writeArray buffer i value) $ zip [lower..upper] twizzled test :: [Int] -&gt; IO () test input = do print $ "input was " ++ show input asArray &lt;- newListArray (1, length input) input aoz' asArray output &lt;- getElems asArray print $ "output was " ++ show output main = test [1,0,2,0,0,3,4] Note substitute "pureSolution" for your favourite cunning pure solution. aoz' now just takes that pure solution and maps it to something that works in place on an array of ints. It would be interesting to see what the optimized output GHC manages for this.
Just to clarify, there's nothing at all that would break if the type of a pattern synonym as a pattern and the type of the same pattern synonym as an expression were completely unrelated. You could have something crazy like pattern P :: Int -&gt; Bool P :: (String, Double) The only issue is the user interface -- I argue that the above would be confusing to the point of being unusable. Currently, we err on the side of caution and require the two types above to be exactly equal: pattern P :: Prov =&gt; Req =&gt; T P :: (Prov, Req) =&gt; T For Matt's example to work, we would need to at least be lenient enough to allow pattern P :: Prov =&gt; Req =&gt; T P :: Build =&gt; T where `Prov`, `Req` and `Build` are independent. There's still time to change this for GHC 7.10, but I'm on cough syrup at the moment and can't be sure it's a good idea:) Comments? ETA: actually I just now realized that implementing it will be a tad bit more involved since it will require the ability to add type signatures for pattern synonyms-as-expressions for the explicitly bidirectional case.
I doubt most people using MySQL will delve into the strict modes, though. So they'll be working with "old-style" MySQL unless their package distribution sets saner defaults. MySQL is, de facto, non-strict and sometimes outright insane. Postgres, on the other hand, is strict my default and does not have any settings that can violate its strictness, nor does it really have any legacy baggage; Postgres is very carefully designed, and they generally don't leave behind sloppily defined functionality. Postgres doesn't compromise. For example, invalid dates are _always_ rejected. (MySQL needs the "traditional" strict mode enabled for that.) This is much nicer than MySQL, where the documentation is absolutely littered with weird little edge cases and notes about how this and that behaviour changed in this and that version. Just the fact that the strict mode changes across versions is pretty ugly; In 5.7.5 — a minor release! — they suddenly enabled several modes, breaking backwards compatibility.
Please post the Haskell jobs available in your city.
&gt; We are looking for a Haskell **expert** to join our team at Well-Typed. 
... I need to stop skimming intros.
By "external API" I don't mean something you put up as a JSON microservice, I mean anything that's exported by your modules with the expectation that other parts of the program will use it. In Haskell I've found it rare to see (hand-written) modules that are "100k+ LOC monstrosities."
I think you meant: quickSortBy zeroMax arr &gt;&gt;= return . length . takeWhile (/= 0) ... which is the same thing as: fmap (length . takeWhile (/= 0)) (quickSortBy zeromax arr)
&gt; I just don't get any pleasure from "type wrangling" to get what I want, and holding tons of implicit information on my mental stack The purpose of types is precisely so that you don't have to hold things in your head. If you keep taking the easy route you will have to remember more and more invariants not to break. Tracking effects may seem extreme, but it's not. Those of us who work on large projects know that uncontrolled effects greatly decrease the readability and maintainability of code, as if we had littered goto statements everywhere. Discipline may work for a small number programmers doing greenfield work on a small project, but once the project size increases, feature requests roll in and the original programmers leave the team, you really need all the assistance from the compiler you can get.
That had me confused for a while, but you seem to be correct.
By wrapping it in ATripleNestedWeirdMonad! Or just a single monad transformer, but that has the same effect.
If it helps, the version you wrote would be parenthesized like this by the compiler: (quickSortBy zeroMax arr &gt;&gt;= return) $ (length $ (takeWhile (/=0))) More generally, any time you see an expression of the form: e1 $ e2 ... just put parentheses around `e1` and `e2` and that's how the compiler interprets it: (e1) $ (e2)
I'm not OP! I'm an innocent bystander trying to mentally type check OPs code until I came to your comment. :)
Oh, oops! :)
Yes, I am dumb. 
I'm quite surprised to hear that you have to go to such length to replicate each individual package's BSD copyright notice in the resulting binary distribution. Isn't BSD supposed to be one of the more binary-distribution-friendly licenses?
Yep it would be nice to have a link to a free resource about the emacs setup. It is amazing the time you can spend tweaking emacs toward a nice IDE. Then you try (for instance) an intellij plugin and everything works nicely after a couple of minutes ;-) 
Thanks for this. Teaching comonads using `Store` as an example is only going to work if the student already knows what a comonad is. This was a much needed example of a real world comonad and how succinctly you can express certain algorithms with them.
This is actually binary-friendly. There are people who repackage BSDL software with GPL or even more restrictive licenses. This is only to make sure the license is properly distributed. Don't forget that BSDL is "just" there to protect the developer from being sued. When someone removes the right from the software, it is simply not OK.
Beautiful. Such a shame that emacs still doesn't have ligature support. :(
Idiomatic Haskell very rarely includes in-place mutation or arrays. If you have the kind of constraints that make this necessary, you probably don't want to be using Haskell.
The recruiter does say "I think we all know who" :-)
Bank/security firms are on the high end of salary, and the offer on this job by Well-Typed is on the far low end. There are many jobs that are also very interesting and pay a competitive salary. I am willing to take a salary cut for work I am passionate about, but this is too much of a cut. I would love to do the work you described (open source, GHC, teaching, Haskell projects, ...), and working remotely is super, but the salary is too low to consider it, unfortunately :( 
If they'd be willing to take me (a graduate with around six years of non-professional use of Haskell), I'd be more than glad to work for the base line of 37800 gbp. That's more than most entry-level jobs in this area pay and you get to work with Haskell. And remote. But the selling point to me would be Haskell. Seriously, where do you get to write Haskell for a living? That would be eternally better than any Java, C# or even C++ job out there.
Nicely written example! Practical and interesting. Regarding the Comonad abstraction itself, it's starting to feel like Arrow; one single practical use-case.
The task is from [ProgrammingPraxis.com][1], which is a website about practice problems. "This blog publishes new programming exercises weekly, at least, so that savvy programmers can maintain their skills by working the exercises and thinking outside their normal skill set, whatever that is." I'm not saying that you shouldn't care about in-place array operations in Haskell, but in this context, I think it's fair to say that the idiomatic Haskell solution is perfectly fine and does not support the OPs sentiment of "I find myself thinking that certain things would be more elegant and less painful in an imperative language". The thing is: we need some value judgement when translating programming problems to Haskell, because the same goes the other way round: a C programmer would have a lot of trouble fulfilling the task "Search an infinite list of increasing numbers until you find an element that is greater than 100." [1]: http://programmingpraxis.com/2014/11/21/an-array-of-zeroes/
We actually have a feature in FP Haskell Center that will grab all of the LICENSE files\* and concatenate them into a single file when you build a binary distribution, specifically for this reason. \* Based on the license-file field in the cabal file.
Nice article! However, the only practical use I ever see of `Comonad`s is variations on the `Store`, almost exclusively into an array with a focus (image processing, game of life, etc) - but there are others. Do they ever actually crop up as much? For example, `Traced` and `Env`.
 aoz :: IOUArray Int Int -&gt; IO () aoz a = getBounds a &gt;&gt;= go where go (i,j) | i &gt;= j = return () | otherwise = do y &lt;- readArray a j if y == 0 then go (i,j-1) else do x &lt;- readArray a i if x == 0 then do writeArray a i y writeArray a j x go (i+1,j-1) else go (i+1,j) 
They are not formulated as hardware constraints though. Hardware constraints would be "must not use swap", "must not allocate more than X amount of RAM", "must run in X time on a given system", etc. Those are reasonable demands to make, but whether you choose to do it in-place or not, and whether you use an array or a hashmap or a linked list for you data structure, are implementation details that may or may not follow from those requirements. The constraints aren't stupid, but they are formulated at the wrong level.
Correction: - `do` is just syntax sugar that makes things look more like the example, but it doesn't break any rules or allow any effects in itself. A `do` block over, say, `Maybe` or `List`, is still completely pure. - Monads don't break any rules; they are just as pure as the rest of the language. We merely use them to lazily compose pure values which, when fed to the Haskell Runtime, can be interpreted as impure programs. So in a way, the Runtime is our "break all the rules" thing, and by feeding it a value of type `IO ()`, we can make it do impure things. From a practical point of view, however, we can take the short-cut view that (some) monads mark effects: `IO` marks interaction with the real world, `Maybe` marks that a computation can short-circuit when a sub-computation fails, `State` marks that we are implementing stateful computations by passing state from function to function, etc. But we're not breaking any rules. `unsafePerformIO`, now *there's* a way to break the rules, and the consequences of using it can be dire. Oh, and, the code you gave is a bit misleading - you are introducing two let-bindings for x, which is not the same as actually updating x. This becomes painfully obvious when you attempt this: func = do let x = 0 let x = x + 1 print x return (pureFunc x) Line 3 will simply drop into an infinite recursion, eventually stack-overflowing, because you are essentially saying "From hereon, `x` can be reduced to `x + 1`", which means that since `x = x + 1`, `x = (x + 1) + 1`, and thus `x = ((x + 1) + 1) + 1`, etc. An actual example that behaves more appropriately could, for example, use `StateT Int IO` as its monad stack, and that would look like this: func :: StateT Int IO Int func = evalStateT 0 $ do put 0 modify (+1) x &lt;- get liftIO $ print x -- need to lift, because print is IO, not StateT Int IO return $ pureFunc x -- or, alternatively: pureFunc &lt;$&gt; get Or it could use an IORef to actually provide mutable variables instead of faking it via `State`: func :: IO Int func = do xRef &lt;- newIORef 0 modifyIORef xRef (+1) x &lt;- readIORef xRef print x -- or, the previous two lines in one: readIORef xRef &gt;&gt;= print return x -- or, reading from the IORef directly: readIORef xRef Similar code can be written using ST, MVars, etc.
Shouldn't this do what you want? import Data.Vector.Unboxed (unsafeThaw,freeze) import Data.Vector.Unboxed.Mutable import Data.Vector.Algorithms.Merge import Control.Monad.Primitive comp0 _ 0 = LT comp0 0 _ = GT comp0 _ _ = EQ f :: (PrimMonad m) =&gt; MVector (PrimState m) Int -&gt; m () f input = sortBy comp0 input
The wrapping is really just Haskell's way of telling the caller which effects to expect (and forcing these expectations as part of the compilation process).
I don't know, Haskell for seems to excel at such problems as well. 
I maybe just a single monad :p
I tend to agree with you but I don't think it's a general problem. Most companies using Haskell are paying normal rates for their local markets. We're not a product company, we're currently just selling our time and expertiese and we do things like maintain GHC which is frankly not very profitable. But we conciously choose to do these things rather than all go and work for banks because we think it's important, and because it's a lot of variety and fun. We're trying to be upfront about the salary because not everyone wants to make that tradeoff in the same way that we do. We're well aware it doesn't compare well to Silicon Valey or London rates. And if you want to join us and help us get more profitable work then send us your application :-)
That might be a good use case for something like [MonoComonad](http://lpaste.net/115280), which admittedly I've never tried using for anything real.
A monad transformer *is* a single monad! That's the idea behind the monad transformer concept – take several monads and fold them into one.
&gt; I just wanted to communicate in general the idea that people shouldn't immediately feel comfortable with a less-than-market-rate salary just because it's something they love. That's right. But WT isn't forcing this down anyone's throat. And there actually _are_ Haskell jobs out there with higher salaries, it's just not for the same kind of job :) The Well-Typed guys have been pushing Haskell and its tooling for years, not to mention the insane amount of teaching they do, too. As far as I know, you don't get to do that in those higher-salaries opportunities. It's really a tradeoff, and everyone is free to decide whether it's an acceptable one for them. If the answer is yes, that's one more email for info@well-typed.com :)
 aoz :: IOUArray Int Int -&gt; IO () aoz a = getBounds a &gt;&gt;= go where go (i,j) | i &gt;= j = return () | otherwise = do v &lt;- read case v of (_,0) -&gt; go (i,j-1) (0,x) -&gt; write (x,0) &gt;&gt; go (i+1,j-1) otherwise -&gt; go (i+1,j) where read = do x &lt;- readArray a i y &lt;- readArray a j return (x,y) write (x,y) = do writeArray a i x writeArray a j y 
I'm not entirely sure what it is, but something about how .. simple and elegant this library turned out to be makes me want to find a reason to use it for a project. I did like blaze, but I also disliked it for many of the reasons mentioned by Chris. 
I was just going to say that we'd implemented this already. It's not that complicated. We ought to be able to get this feature into cabal.
Thanks for the reply - that's perfectly reasonable! To be clear, I don't personally have a problem with what you're doing - I thoroughly enjoyed getting paid a comparably low salary for the last 6 years of paid open source work I got to do at MusicBrainz. Money is certainly not everything, and the job satisfaction that comes with this stuff is hard to find elsewhere. I just wanted to stress that people (that is programmers and people hiring) shouldn't expect that this work has to come at lower than expected rates, just because it's currently niche.
&gt; O(min(N-k,k)), where k is the number of zeros Isn't that impossible? If the array is all zeros, you can't check that in constant time. I don't see how to solve the problem faster than O(N).
Nice article. I've read all three articles in references but failed to set up haskell env for my custom pkgs. This one get me real close to last step. One question. I've installed custom packages using '~/.nixpkgs/config.nix'. But cannot use them in nix-shell(shell.nix). config.nix - http://ix.io/fo3 shell.nix - http://ix.io/fo5 error - http://ix.io/fo6
*Unsafe* is not necessary for this problem. A mutable array is [enough](http://www.reddit.com/r/haskell/comments/2nm00u/as_a_haskell_n00b_id_like_to_have_a_conversation/cmf94p3).
If you `duplicate` a `Fold`, you can feed it partial input. Not sure how often that is useful, though.
I laughed heartily. I like you.
I think JuicyPixels is nice because it is one of those libraries which does one thing really well, though. There are many different ways to implement image processing algorithms, and I am not sure whether you would want to gear JuicyPixels towards one kind: it's easy enough to build additional libraries on top.
Cofree comonads are useful as annotated trees. Duplicate lets you store old versions of the tree and annotation before transforming it and extract gives you the annotation.
Comonads crop up *all the time*! Basically, any time you have a notion of "goodness", and a subset of the elements of a type may be good, then you have a comonad. So comonads are the right abstraction to talk about (e.g.) functions which can be safely serialized and passed over a network, whether code in FRP can be safely invoked in the future, whether reference counting is a safe memory management strategy, and so on and so on. However, the really compelling examples do not have a strength, and so you can't define them as a library (since strength is basically the definability of `fmap`). So you end up wanting some compiler support for them. 
Singaporean here. What gave you the impression that Singapore is a police state? I can't remember the last time the police was evoked arbitrarily. Even in legitimate (ok this is subjective, and might be the brainwashing talking. peace over freedom of expression and all that) cases like the once in a blue moon riot last year, the police did not act forcefully leading the public here to question their riot control abilities.
Could you give a more concrete example of the "goodness" illustration? I don't quite get how this works with `fmap :: (a -&gt; b) -&gt; f a -&gt; f b`, e.g., how do you enforce that `b`, or `f b`, exhibits the same "goodness" property of `a`, or `f a`?
Applying for a job is both an opportunity cost and an emotional cost (potentially). Applying for the job itself might not be much of an investment, but having to deal with the rejection might be. Not to mention the interview, which might demand something like air travel (talking in general, here). Feedback? Only if you're not too prideful to explicitly ask them back after the rejection email or call, which might have come off as condescending (whether intentional or not). You've just been rejected from a job you may have really wanted - you might not be in the mood for some *constructive feedback* when they phrase it like you can use that feedback "next year" or something to try again. Why do you presume that I would want to apply to your company again next year? Maybe the experience wasn't constructive at all and you have lower confidence in general because of it. No, not if you're perfectly disciplined and constructive, and take any rejection as an opportunity to *be better*. But *I* for one am not perfect in that regard. Far from it. I can see why companies would just say "Oh, just apply - what can you lose?", since the cost of more applications might not hurt them in any way. In the worst case, they may just set up some filters like "at least a college education, at least 5 years experience" and throw those not meeting that standard out immediately. Then follow up with a generic rejection email 14 days later for those generic rejections. And a potential interviewee might take a rejection personally - someone sifting through applications certainly will not. Of course, applying for jobs is important. There might not be any alternatives, at least for fresh graduates. But is it that simple that you can just say "Oh well, it doesn't cost me anything"? Perhaps if you are in the right state of mind to take any rejection as constructive feedback (if they have constructive feedback to give, and not just generic excuses). But I for one won't bother with applying for jobs until I know that I have a fair chance of getting it - I certainly won't apply for jobs in languages that I'm a rookie in, when they expect an *expert Xer* and when it's a buyer's market.
Nice. I probably wouldn't have climbed on my soapbox here if I could have written it that well the first time, I admit. But I'm glad to have gathered various thoughts on the matter.
&gt; 49k£ per year ... &gt; "struggling artist" image wat
Hey, I wrote [this](https://github.com/khalilfazal/IANA#readme) in two days. I was surprised that getting parsec to work was harder than then the template-haskell portion. I should've read the [RFC](https://tools.ietf.org/html/rfc5646), for hints for parsing the registry.
Traced can be used to implement the "builder" pattern. But people seem to be already using Writer for that.
You can structure PDE solvers e.g. five point scheme or nine point scheme using them but again this is an array with a focus.
&gt; I just wanted to stress that people (that is programmers and people hiring) shouldn't expect that this work has to come at lower than expected rates, just because it's currently niche. Yes, I think we all agree here and it's an important point.
It says 34.8k on the job posting, and that is *nothing* in London. An experienced developer could earn twice that writing "enterprise Java". Obviously it's a livable salary, but it doesn't compare to the market for other languages. 
We havn't so far, though we've often supervised GSoC projects.
Yeah, I don't think you know what "struggling artist" means.
(***) and (&amp;&amp;&amp;)?
&gt; a pure function can't call func itself But that's the whole point of "wrapping the result in a monad." That restricts the callers for you, in exactly the way you want.
I think you have to interpret that a bit less literally.
The salary is certainly not commensurate with pay in corporate developer jobs in global metropolises. However there seem to be a number of benefits to a Well Typed job that such corporate positions would not provide, and it's worth taking those into account. Amongst others * remote work * close collaboration with some of the most experienced Haskellers that exist * ability to be paid to write a lot of open source software Ultimately Well Typed are not competing with banks for developers but aiming to appeal to a different demographic.
&gt;A do block over, say, Maybe or List, is still completely pure. This is where we hit differing definitions of pure. It is "pure" in the sense that no IO is being performed. No global references are being mutated. Another way to look at it, however, is that the do block enables "impure" language features, and the "for each" behavior of the list monad is an "effect".
It's my understanding that the practical example people bring up for Arrows is dealing with XML. But it's certainly true that people are making FRP libraries based on arrows, and OpalEye is sort of like HaskellDB but made arrowish instead of monadic.
Ah, I hadn't come across that one yet. Thanks!
I earn near enough twice that as a new grad writing Java/Javascript. An experienced developer could look to earn even more, especially if they are in the financial industry. 
Separating pure from impure is precisely what monads do. It turns out, effect typing systems (what you're advocating) and monads aren't really all that different.
Of course, the original motivation for monads was not due to a prima facie acceptance of purity as the ideal and having implementation driven towards that; purity was a _necessary side effect_ of having lazy evaluation, because it's next to impossible to just have ambient effects in the language semantics with lazy evaluation as it's very hard to tell when or if they will happen.
I haven't seen Haskell run on micro-controllers, and AFAIK it still doesn't hold a candle to C and Fortran packages in numerical computing.
I'm not saying that's what you should do, I'm saying the problem is unfit for Haskell practice in the first place.
&gt; Singapore is great. Super clean, (...) [And the trains run on time](http://tvtropes.org/pmwiki/pmwiki.php/Main/RepressiveButEfficient?from=Main.TheTrainsRunOnTime)?.. 
&gt; purity was a necessary side effect Nice pun :)
You are technically correct, the best kind of correct! (If it needs to be said, I was talking about an applied transformer.)
I use the comonad for left folds a fair bit: data Fold a b where Fold :: (r -&gt; b) -&gt; (r -&gt; a -&gt; r) -&gt; r -&gt; Fold a b The comonad for that makes a "resumable left fold". duplicate :: Fold a b -&gt; Fold a (Fold a b) Variations can be made for left/right/monoidal folds. See my folds package on hackage. But, what can it be used for? Say you have a CRC or something. It typically consists of an initializer, something to do for every step, and a way to finalize the data. These are the three components of this `Comonad`! So you can package up your CRC algorithm as a single Fold. It gives a convenient safe encapsulation of the intermediate state. I wrote this up in https://www.fpcomplete.com/user/edwardk/cellular-automata/part-2 You can then use the Applicative for these to fuse together multiple such algorithms over the same data set. (FWIW- This thing also happens to be a `Profunctor` and instantiates a few more things from the `profunctors` package, which lets you do interesting things with `lens`.) And then to be particularly obtuse, we can note that this is also a case of `Store` if you squint at it right! You basically have `StoreT r (Env (r -&gt; a -&gt; r)) b` with a coend applied afterwards that traces out the 'r's. It is also equivalent the `Cofree ((-&gt;) a)` encoding of a Moore machine. It isn't surprising that most interesting comonads are intimately related to Store, though. Why? Ultimately we only have one adjunction from `Hask &lt;-&gt; Hask` and that is the `(,) e -| (-&gt;) e` adjunction. `Store` is the comonad given rise to by this adjunction. (Traced/Env both come from the two sides of this adjunction and preservation of limits/colimits). When you go looking for comonads using the (e -&gt;) -| (e -&gt;) adjunction that goes from Hask &lt;-&gt; Hask^op you run into the fact that you built a comonad in Hask^op -- which is a monad in Hask. So the comonad that comes from Cont is the same as the monad you get from Cont. You have to run pretty far afield to find a comonad that isn't given rise to in this fashion in some sense. [edit: I meant to reply to ocharles with this one, but had to reboot and top-posted by mistake]
yes, the practical examples at least give you some intuition about applicability. I liked the nice usage of typed holes as well :-)
In a language where strength is ubiquitous, like haskell, you can't. =) This is part of why there are fewer interesting comonads than monads in Haskell.
I use it to pretty good effect here: https://www.fpcomplete.com/user/edwardk/cellular-automata/part-2
I remember being around two years into my career: I took a salary drop to go work at a company doing computer graphics work because I wanted very badly to get into that industry. I negotiated a salary of R16,000 a month (roughly around $2,200 at the time) and was not to tell anyone else in the company because everyone else at the same level was apparently getting paid less (...yes, I know!) I really wasn't unhappy with that, I felt the opportunity to do computer graphics was worth it and what do I care for money anyway? Actually, I was saving a goodly portion of my income - I'm not really a big spender. However, after some time had passed I did start to notice that a good number of people were grumbling about lousy wages. So, one day I idly complained about the situation to a friend who makes his living installing cupboards. His reaction when I told him how much I made was priceless. He couldn't believe I was making so much money... in fact, his cousin and her husband didn't even pull in that much between the both of them, with a toddler too! This was also not the last time someone exclaimed that I was "rich" :-) Well anyway, I won't venture to make a point, except maybe that wage disparity is crazier than you think! PS Pretty sure everyone in this story would be considered middle class in South Africa, although wages should be higher with inflation today I'm sure.
The dual of 'Traversable' is 'Distributive', but it only needs a 'Functor' constraint in practice, due to asymmetries between values and continuations in Haskell though. This is analogous to how `Traversable` only uses the `Applicative` structure, not all of the `Monad`.
There's a [paper](http://okmij.org/ftp/Haskell/extensible/exteff.pdf), a short [stackoverflow qa](http://stackoverflow.com/questions/25995369/what-are-the-differences-between-layers-and-extensible-effects/25996140#25996140) but not a lot of [adoption](http://packdeps.haskellers.com/reverse/extensible-effects). I'm wondering what shortcomings are impeding this concept. Are there already some more performant or usable concepts supplanting it before it gets off the ground?
I'm behind *Peer to Peer* and this is useful feedback, thanks! I've published lots of videos at [vimcasts.org](http://vimcasts.org) and I know how hard it is to stick to a regular schedule. With Vimcasts I had only myself to manage, but *Peer to Peer* introduces new difficulties. It can be a real challenge to get three busy people in the same room at the same time! Given that John Wiegley lives in the US and Ollie and I live in London, I'm especially pleased that I managed to make this video happen. It's too soon for me to commit to a regular publishing schedule for *Peer to Peer*, but when I'm better able to budget for the the time and costs of making these videos I'd like to offer subscriptions.
Is there a good other language to study where strength fails and good comonad examples exist? Or are we just talking linear logic?
There's lot of stuff going on in the AFRP world if you're into that. Essentially, some of the best stuff arises when you go even further into arrow land and have the ability to embed a whole symmetric, monoidal category. If you can reflect these pipelines as values then you can do lots to optimize arrow computations over "signal vectors" which helps make some of the FRP laziness and incrementality problems go away.
linear logic is a good example of where an interesting comonad lives. The (!) modality is a comonad there. Kieburtz's OI comonad also can't live in Haskell due to strength. 
I'm behind *Peer to Peer* and I'm listening to your feedback, so thanks for speaking up. If you're really keen to watch this video but can't afford it, write to me and we can figure out a deal. I'm always happy to offer a discount to students. I want these videos to be seen, but I also want them to be viable and that means they can't be free. I share the profits with my host and guest, so if you buy this video you can say you've bought lunch for John, Ollie, and myself. The video featuring Ollie with Steve Purcell ([discussed on /r/haskell here](http://www.reddit.com/r/haskell/comments/2jbdaz/flyonthewall_haskell_coding_session_with_ollie/)) has been the fastest selling *Peer to Peer* video so far, so it's likely that I'll be producing more Haskell content. We're doing a black friday sale from now until 1st December, 2014. You can [get 40% off individual videos, or 50% off if you buy all four](http://peertopeer.io/offers/2014-black-friday/).
Mac Lane was right: it really is all about the natural transformations...
You can get more interesting adjunctions in subcategories of Hask induced by constraints, correct? Is there anything more interesting that can be done once you're happily abusing constraint kinds. Should I just go ahead and read `hask` already?
Please keep making these. I will buy all the haskell videos until I stop learning things from them. I can always expense them to my work, if need be. If I can expense a technical book as a learning resource, then a video should be no different.
Doubly linked lists have worst case O(1) removal/insertion at middle, as well as splicing/joining lists with worst case O(1) as well. What functional structure can give these asymptotics?
I remember there being some complaints that it is ultimately less powerful than mtl -- but I never got the specifics. You may be interested in checking out the [effects](http://www.idris-lang.org/documentation/effects/) package in [Idris](http://www.idris-lang.org/). Idris is part of Edwin's "*plan to never have to write a monad transformer again*". I suspect part of the problem is that most existing libraries in Haskell are using mtl, so you have to do more yak shaving to use effects.
I can absolutely afford to pay for it it --- but I'm not convinced it's worth $25. I didn't notice any way to get a preview so I could evaluate it to determine the truth (or not) of the above statement. From the still image, it just looked like it was just going to be a couple of people hanging out and talking to each other. It did not look particularly "professional". It also wasn't clear that I couldn't learn the same stuff by reading something somewhere. This may just be a marketing problem to be resolved. I understand the need to pay others, etc. 
I started learning Haskell this year. When I'm picking up a new topic, I like to come at it from two sides. I make slow deliberate progress through the basics, reading books, watching tutorial videos, and working through simple exercises. I also like to expose myself to stuff that goes over my head. Things that made no sense on first pass can suddenly come into focus if you leave the brain do its thing. This video goes over my head and that's one of the things I like about it! If you're a Haskell learner who seeks material that will stretch your mind, I think you'll enjoy watching this video.
Did you miss the [3 minute video preview](https://peertopeer.wistia.com/medias/qevfzldma9)?
"Idris is part of Edwin's "plan to never have to write a monad transformer again"." Talk about going the long way around...
[Found this discussion previously](http://www.reddit.com/r/haskell/comments/1j9n5y/extensible_effects_an_alternative_to_monad/). There's a good bit of back and forth over effects vs. monad transformers.
I found [this](https://www.fpcomplete.com/user/bartosz/understanding-algebras) tutorial on f-algebras on fp complete to be really good.
I should have search reddit first. tldr; ekmett says it's crap for reasons beyond my comprehension; guess I'll just have to go along with 34 up-voters and trust him until I know more. :)
Consider the possibility that mtl seems like a good idea only because of an existing commitment to a way of doing things which may merit review. Edwin was going to build Idris anyway. He's taken the chance to think things out again. And if that comes out differently from the established Haskell way, perhaps we might learn something.
I read through the [Idris effects tutorial](http://eb.host.cs.st-andrews.ac.uk/drafts/eff-tutorial.pdf) and it's interesting. There's some cool syntactic sugar like pattern-matching bind: emain : { [SYSTEM, STDIO] } Eff () emain = do [prog, arg] &lt;- getArgs | [] =&gt; putStrLn "Can’t happen!" | [prog] =&gt; putStrLn "No arguments!" | _ =&gt; putStrLn "Too many arguments!" putStrLn $ "Argument is " ++ arg Do want. Labeled effects let you distinguish between, for example, two different state non-transformers by using labels. So innovative. treeTagAux : BTree a -&gt; {[’Tag ::: STATE Int, ’Leaves ::: STATE Int]} Eff (BTree (Int, a)) treeTagAux Leaf = do ’Leaves :- update (+1) pure Leaf treeTagAux (Node l x r) = do l’ &lt;- treeTagAux l i &lt;- ’Tag :- get ’Tag :- put (i + 1) r’ &lt;- treeTagAux r pure (Node l’ (i, x) r’) I will recommend this tutorial to anyone who is interested in syntax and not-necessarily dependent types.
Wow, that's almost the stuff of nightmares. Thanks to the maintainers for doing all this hard work and decision making. Bundling or offering an official LLVM version alongside GHC sounds like a great idea.
I can translate: * Free monad = (one kind of) syntax tree * Codensity transformation = (one kind of) continuation passing style Abstract syntax trees sometimes preserve too much information. In fact, you can prove that a free monad preserves as much information as possible, which is bad sometimes. For example, you can build an abstract syntax tree for the state monad that looks like this: data StateAST s r = Put s (StateAST s r) | Get (s -&gt; StateAST s r) | Return r ... and you can define a `Monad` instance for that and then define an interpreter that behaves just like the `State` monad. However, then the problem is that you can't guarantee that the interpreter you write satisfies the `State` laws, one of which is: get &gt;&gt;= put = return () However, if you had encoded `State` as `s -&gt; (r, s)`, then that law is guaranteed to hold for free. This is why it's sometimes bad to preserve "too much information". * Initial encoding = encoding of an abstraction that preserves the most information possible (i.e. it's syntactic) * Final encoding = encoding of an abstraction that preserves as little information as possible (i.e. it's a naive translation of the specification for the abstraction) * `OverlappingInstances` = BAD (and `extensible-effects` requires `OverlappingInstances`) * It's only efficient to build an `extensible-effects` monad, but not necessarily to interpret it (remember: it's a syntax tree and syntax trees need to be interpreted to do anything). Part of the reason why it can be inefficient to interpret it is that it's doing things at runtime that `mtl` was doing at compile-time (i.e. `extensible-effects` has to do all these `Typeable` tricks) Disclaimer: I haven't actually studied `extensible-effects` or read the corresponding paper. I'm just translating what Edward Kmett says.
So, how would you solve this task if you had `@breakAllTheRules` or, let's say, in Java? Except for the "in place" bit, I think it's a nice fit for functional programming, and even an imperative solution can benefit from being slightly "functional". Essentially, what you have to do is to look at the both ends of a slice of array, test if they fulfil the requirements of the result (0 at the tail, non-zero at the head), swap if necessary, and recurse on the remaining elements. Here's an implementation in C++: // Fixed size array that does not make your brain melt template &lt;size_t N&gt; struct int_array { int data[N]; }; template &lt;size_t N&gt; void part_swap (size_t lo, size_t hi, int_array&lt;N&gt; &amp; xs); template &lt;size_t N&gt; void partition (size_t lo, size_t hi, int_array&lt;N&gt; &amp; xs) { if (lo &lt; hi) { if (xs.data[hi] == 0) return partition (lo, hi - 1, xs); else if (xs.data[lo] == 0) return part_swap (lo, hi, xs); else return partition (lo + 1, hi, xs); } } template &lt;size_t N&gt; void part_swap (size_t lo, size_t hi, int_array&lt;N&gt; &amp; xs) { int tmp = xs.data[lo]; xs.data[lo] = xs.data[hi]; xs.data[hi] = tmp; return partition (lo + 1, hi - 1, xs); } template &lt;size_t N&gt; int_array&lt;N&gt; partition (int_array&lt;N&gt; xs) { return partition (0, N - 1, xs), xs; } template &lt; &gt; int_array&lt;0&gt; partition (int_array&lt;0&gt; xs) { return xs; } Note that the only place where mutation happens is `part_swap`, everything else is pure. The translation to Haskell is straightforward: import Data.Array.MArray partition xs = getBounds xs &gt;&gt;= uncurry partition' where partition' lo hi | lo &lt; hi = read lo hi &gt;&gt;= \ab -&gt; case ab of (_, 0) -&gt; partition' lo (hi - 1) (0, _) -&gt; part_swap lo hi _ -&gt; partition' (lo + 1) hi | otherwise = return () read i j = do a &lt;- readArray xs i b &lt;- readArray xs j return (a, b) swap i j = do a &lt;- readArray xs j readArray xs i &gt;&gt;= writeArray xs j writeArray xs i a part_swap i j = swap i j &gt;&gt; partition' (i + 1) (j - 1) Looks quite elegant to me. PS. If only I could pass and return `xs` by value in the C++ version, and convince GCC to update it in-place anyway. Sometimes it works, making C++ my almost-favourite functional language, but apparently this is not the case.
Use a 'fingered' catenable deque. Take an Okasaki catenable deque, e.g. a bootstrapped catenable deque, let's call it 'Cat a'. You can construct it by taking any of the normal queues in Okasaki and applying his bootstrap construction to make it a catenable deque. Choose among them based on constant factors to optimize for ephemeral or persistent use. Then data FingeredCat a = FingeredCat (Cat a) (Cat a) The 'focus' is at the finger. You can move the finger left and right through the list in O(1). You can insert/delete on either side of the finger in O(1). You can 'close' the catenable deque in O(1) by catenating the halves. And with that I've invented a structure that does it. ;) Whenever someone has thrown this 'Haskell can't do doubly linked lists' problem at me, this is the solution I've trotted out.
Yep and yep. =) The trick is that usually we run our adjunction out to some other category and back as in newtype Free p a = Free { runFree :: forall r. p r =&gt; (a -&gt; r) -&gt; r } which can be used to build monads for things like: type List = Free Monoid type NonEmpty = Free Semigroup However, such a beast has the 'exotic category' in such a place where when you build the comonad from it, it lives in the other non-Set-like category, so it is hard to construct examples from that.
Given that I believe this was his first public speech, I think he did an amazing job.
wow that's too bad, I thought 7.10 would be getting some solid arm support. It looks like we need to wait for the patched llvm 3.5? Are there any basic build instructions in order to try a patched llvm + ghc on arm? I've been meaning to try a build again on a beagebone but have been holding off until 7.10. 
Great. This tutorial has all of the concepts in one place along with some cartoons. Now I'm ready to read the blog post again.
re 1: have a look at the lambdacase extension.
Yeah. There are a ton of really neat things in Idris that would be cool to have in Haskell -- many of which have nothing to do with dependent types. For example, `where` clauses allow types not just functions: foo : Int -&gt; Int foo x = case isLT of Yes =&gt; x*2 No =&gt; x*4 where data MyLT = Yes | No isLT : MyLT isLT = if x &lt; 20 then Yes else No 
IIRC, the overlapping instances are ok if the sums associate to the right. Does anyone have a source on this? (Maybe attributed to Swierstra?)
Of course, we still have lazy IO lingering in Haskell, which has precisely these pitfalls.
We're not planning on switching any time soon; the native code generator will be around as the default for the foreseeable future. But on some platforms, LLVM is the only thing available for GHC, like ARM. LLVM has also made the compiler much more easily portable with acceptable speeds to other platforms, so there is that. Really it mostly boils down to just not having a real policy on how to manage the work. We merged the work years ago and it slowly got cruft and somewhat out of control as we tried to maintain a very not-well-defined support window, and never really reigned it back in. So, this is really all a way of doing that and saying we're going to support and take care of this more seriously from now on, in a way that won't drive us up the wall. And there are several advantages we get from using it (and LLVM does work really nicely on some GHC programs). I also think [the idea of using in-the-wild LLVM versions is madness](https://ghc.haskell.org/trac/ghc/wiki/ImprovedLLVMBackend), and it really would be madness even if it were the OCaml team or the GHC team doing it. No other toolset I'm aware of does this and I consider it a flawed approach and one of the core problems currently. If the OCaml team were to ever seriously consider such work, they could hopefully learn from a few of our mistakes.
Why not go with a (well tested) LLVM 3.6svn snapshot? Both Apple and Google usually ignore the official release schedule and cut their own intermediates. This could work for GHC too.
I just generated a simple atom feed [here](http://mpickering.github.io/atom.xml). Let me know if there are any problems with it.
I was thinking that may be the route to go too. What would be the best place to start on the ghc side? head of master?
Always. Go until hitting a critical problem. Arrange for a fix, get it integrated into the LLVM tree. Then iterate.
ML can do that, a bit surprising that Haskell can't, but I guess you could just not export the type from your module.
&gt; Never Say, "It's Easy!" I see this too often. :( This is also a great list of similar "rules": https://www.hackerschool.com/manual
Those are very good. It is hard not to slip up on the small things all the time, but I think making the effort to do so makes a tremendous difference for beginners. In the our CS class I see too many students get discouraged because their peers (who have previous experience) do these things.
The "never say its easy rule" resonated with me because I got into Haskell when I first heard Simon Peyton Jones give an interview on Software Engineering Radio. He made Haskell seem very approachable. His willingness to admit he finds certain things difficult in his presentations was hugely motivating because parts of Haskell can feel a little overwhelming at times.
I am just starting to learn Haskell but I would love to work with you even if the pay is lower.
If you live outside the US for 330 days in a 365-day period, you don't pay federal taxes: http://www.irs.gov/Individuals/International-Taxpayers/Foreign-Earned-Income-Exclusion---Physical-Presence-Test
This would be one option; that being said I can imagine that this might complicate the packaging story for some distributions (Debian, for instance). Moreover, the "well tested" qualifier is something that may be possible for Google and Apple to satisfy, but GHC already has a paucity of developer-hours; I really don't think that we want to commit to choosing, testing, and distributing our own LLVM snapshots. Figuring out the idiosyncrasies of the official releases is hard enough.
I hope I wasn't too harsh; LLVM is an incredible piece of software and GHC has benefited substantially from its use. That being said, building a compiler is hard work; to build an optimizer as strong as LLVM's is difficult; building a compiler that is flexible enough to serve as the backend of a dozen languages and a dozen target architectures while maintaining any semblance of version compatibility is nearly impossible. GHC makes this even more difficult by trying to support any LLVM version the user throws at it. I'm optimistic that by consolidating our efforts on a single LLVM release we can make the LLVM backend as dependable as the native code generator. That being said, I agree with Austin that we'd be foolish to abandon the NCG as the default. There is plenty of room for both backends and LLVM introduces many more moving parts than strictly necessary.
[t] means it knows it's a list of some type, but it could be a list of anything. Rather than trying to guess (and probably getting it wrong), it puts "t". If you then use the empty list in a context which requires a specific type (say, Int), GHC will go "Hmm, I guess this t here ought to be Int" and make things work.
How is this any different than the wildcard, a
http://ro-che.info/articles/extensible-effects
Any lower-case name in a type is a "wildcard", or type variable. Often they are named "a", "b" or "c", but not always. "t" is no different from "a".
They do actually, because there a no drivers. It's all automated.
Written another way the type of the empty list is: forall a. [a] It's a list of elements where the type variable for the elements is polymorphic. The empty list is an inhabitant for all lists of ``a``'s for any ``a`` ( Int, Bool, a -&gt; b, ...).
I rewrote your example to use Data.Vector.Unboxed instead and benchmarked it against your implementation compiled with -O3. The code can be found on [lpaste](http://lpaste.net/115329). On my computer I got: benchmarking gf256/vector mean: 30.54480 us, lb 30.14073 us, ub 31.63630 us, ci 0.950 benchmarking gf256/list mean: 152.7165 us, lb 149.3513 us, ub 162.3200 us, ci 0.950 However, I fully agree with other commenters that you should use a table-based implementation for full performance. I just wanted to compare apples to apples.
Yeah you're right, what I meant is that this question is followed by a question to express the number of swaps in Big O notation.
Sorry to nit-pick, I know you're just reusing examples from the SO question, but is there any reason you used `Annotate` and `Holey` rather than `AnnotateF` and `HoleyF`? data AnnotateF a = AnnotateF { label :: String, value :: a } type HoleyF = Maybe It just seems odd to reincorporate `Fix` rather than use `Compose`, though these are isomorphic: Fix (Compose AnnotateF HuttonF) =~ Annotate HuttonF Fix (Compose HoleyF HuttonF) =~ Holey HuttonF Mainly b/c mixing and matching seems more natural with simple Functors: Fix (Compose AnnotateF (Compose HoleyF HuttonF)) =~ Annotate (Compose Holey (Compose Const HuttonF)) Fix (Compose HoleyF (Compose AnnotateF HuttonF)) =~ Holey (Compose Annotate (Compose Const HuttonF)) But maybe there's something I'm not seeing? (Now I'm wishing `-XTypeOperators` let me do `HoleyF '. AnnotateF '. HuttonF`) 
Short answer: don't try to use OOP in Haskell. Also, there's a nice solution to your problem in this article: http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/
How would you apply this post to the OP examples ?
+1 You could also, generalized `mkComplicatedTextField` to take any `state -&gt; IO state` as argument
The solution given in the Luke Palmer article you link to is the same as OP's closure approach. If I had to sum up the article (which I agree with, btw) in a single sentence, it would be “Don't use a typeclass to do OOP in Haskell.” More generally, you should absolutely use OOP techniques in Haskell (or any other language) when it's the right design pattern to use.
I guess. Never noticed that there was a preview.....taking another look now while writing this... ....ah, I see....my main browser wasn't displaying the PLAY arrow but my iPad does.... ....buts it's still just a couple of guys taking at each other. I would much rather have A) a formal polished lecture on the topic or even better B) a written version of the content with appropriate diagrams. However, that preference is really more about learning style. Personally I learn much faster from reading than I do from videos so I just might be the wrong target audience for these.
This seems a bit restrictive though? With a doubly linked list, I can have many pointers into many nodes in the list, and can delete each in random order, in worst-case O(1) for each...
If you use the `operational` approach, don't you get a kind of Free related monad that obeys the laws ? Where can I place `operational` in the different described encodings ?
Really interesting to see the change to ATS for the kernel project! I've had a bit of a play around with it and I think it's a promising addition to the area of low level languages with more expressive/safe type systems (along with other stuff). [ATS homepage](http://www.ats-lang.org/)
Definitely use closures in this case (or consider why you want to model data this way at all...) but definitely check out the vault package also.
I think it's worth observing that this is only the case because: * Haskell has limited support for existential quantification (it's not first class, and needs to be hidden behind a data constructor). ([Here's a paper](http://research.microsoft.com/en-us/um/people/daan/download/papers/existentials.pdf) about what first class existentials might look like.) * Existential quantification is implicit in the built-in `-&gt;` type. In other words, the *in Haskell* part is significant: "record of functions good, existential plus type class bad" is not a law of nature. If you had as a primitive only a bare closure-less function type `-&gt;#`, like function pointers in C, then you could define: data a -&gt; b = forall env. Closure ((env, a) -&gt;# b) where `env` contains the closed-over data. Or with a type class: class Function f where type Argument f type Result f apply :: (f, Argument f) -&gt;# Result f -- pretend that this is "overloading function application" -- and when you write 'f a`, it desugars to `apply (f, a)` (excuse the circularity) data a -&gt; b where Closure :: forall f. Function f =&gt; f -&gt; (Argument f -&gt; Result f) -- pretend that the first `-&gt;` here is not the same thing as what's being defined... instance Function (a -&gt; b) where type Argument (a -&gt; b) = a type Result (a -&gt; b) = b apply (Closure f, a) = apply (f, a) where the `f` here corresponds to the `env` above. In other words, a closure is some datatype containing the environment which is an instance of the `Function` class. You could imagine a lambda being sugar for an anonymous environment datatype together with its `Function` instance... like C++11 does. Rust [takes it even further](http://smallcultfollowing.com/babysteps/blog/2014/11/26/purging-proc/) and also defines its boxed closure types this way (instead of C++'s awkwardly independent `std::function` type). Much of OOP boils down to defining your own specialized closure types wherever you need a closure. For instance I recently wanted to disentangle the "walking up the stack with `libunwind`" code from the "printing out the backtrace" code, so I did this (this is still C++03): struct Visitor { virtual void visit_frame(int nth, const char* function_name) = 0; }; void visit_backtrace(Visitor&amp; v) { /* ...libunwind magic... */ } which is equivalent to Haskell's visit_backtrace :: ((Int, CString) -&gt; IO ()) -&gt; IO () just quite a bit more verbose, especially at use sites. Furthermore, the only difference between a record of functions and an abstract base class / type-class-plus-existential of methods with corresponding signatures is that the functions in the record have separate environments, while the methods in the class share the same environment. But even this appears to be superficial, because it is trivial to translate between them.
What's the difference between naming a variable x or naming it y? It's the same thing here. These are just type variables, they can be named really anything you want (so long as they start lower case), occasionally you'll even see full words used for the type variables when there's a lot of them so that it makes it easier to understand and use. Things like `Stream input output m result` make more sense than `Stream i o m r` in some cases. 
I know some FRP, can you send expand a bit on the physicists analogy? 
My guess is that a relational model is the best way to do this. I've been working through it but it will probably take a while for me to get all the kinks worked out. So in an sql database, you could have a map table mapping the text field to extra internal state. The type of the original text field doesn't change but handlers can reference the extra state. Composing different types of widgets together is a tougher problem but I think it can be done in a nice way. All guesses though. 
Yes. I mentioned that in the other thread here. Only for single threaded, single focus use this is appropriate. It gives up _something_. On the other hand, unlike the doubly linked list you have universal write-cache-friendly persistence of all previous versions of the structure, which is nothing to shake a stick at. If you want multiple fingers you need to pay for an `k` hole context, and on a pointer machine model it isn't possible to switch focus in less than `log k` time. We can of course maintain a set of `k` fixed fingers by maintaining an array of deques by violating pointer machine assumptions and going to a RAM model, but all we've done is gotten clever by fixing `k` a priori, and do to that you'd also need forwarding indices left/right to the nearest non-empty deque to maintain O(1) for all operations, and there is a hidden O(k) reassembly. Now, to be pragmatic for a second, practical considerations the cost of whatever barrier you have to deal with to deal with a parallel doubly-linked list is going to swamp said logarithmic focus changing overhead at any real scale assuming you are trying to be clever and use all these pointers in parallel.
You should read this [reddit thread](http://www.reddit.com/r/haskell/comments/29wyr8/is_there_a_relation_between_the_operational_monad/), which has a long discussion comparing `free` and `operational`.
Another good rule of thumb (if mentoring in person): never put your hands on the keyboard. Talk them through everything and let them do it themselves.
The simple reason is that I wasn't aware of `Compose`. I think it would also work fine that way.
&gt; Reaching 10x Sorry for OT, but I remember reading something about `10x` here or on HN lately. I can't figure out what it was or means though. Can someone help me out? Internet search only tells me that it's slang for thanks.
This is hands down the most entertaining talk, in CS.
In practice, we use lists to order our live objects for various purposes (e.g timeouts and cleanups) and we enjoy O(1) removal from each list when a live object dies. All these objects are local to a single core so parallelism usually doesn't impose costs here. I don't think any structure can beat the mutable doubly linked list for the things it's well suited at.
I bought and enjoyed both of the Haskell videos so far. For those looking for more information about where the value is I'll say why I like them, though obviously YMMV. I'm self-taught. There's plenty of books and documentation out there about how to write code, and how to use different languages. But what I've found hardest to learn is the developer's way of thinking and the approach to problem-solving. That's a hard thing to present in a book but these videos let me see it actually happening in real time. For that reason I feel the insight I gained from the videos I've bought so far gave me something I haven't found elsewhere.
I remember reading that non-strict evaluation was specifically chosen to enforce purity, as it would be too easy to let go of purity in language research without it.
I'm familiar with this article - it doesn't address the question I'm asking. The `TextField` example above is already doing exactly what it suggests, but the issue of what to do about the record field functions needing additional internal state isn't discussed in the article. The problem fundamentally isn't about literally wanting inheritance in Haskell, it's about being able to write generic code. We should be able to write functions that operate on any `TextField` with sufficient flexibility to define its behavior.
Hmm yeah, this makes sense for reducing possibility of error. Haven't seen that `NamedFieldPuns` + `RecordWildCards` combo before, interesting syntax
&gt;Furthermore, the only difference between a record of functions and an abstract base class / type-class-plus-existential of methods with corresponding signatures is that the functions in the record have separate environments, while the methods in the class share the same environment. But even this appears to be superficial, because it is trivial to translate between them. I agree with the first part, not sure about the second though. There's nothing that prevents us from accidentally forgetting to update the closure for a single function out of many in a record. Is there a way to make this less error prone?
Right, I think it's generally accepted these days inheritance was a mistake but data encapsulation + generic code certainly isn't. Not specific to OOP of course, but the underlying principle is what OOP was supposed to be.
Wat? Purity is about referential transparency -- it doesn't matter if there's local (unobservable) mutation or not.
Yeah, sorry. I meant to say CQRS + Event Sourcing. See my EDIT :)
From experience mentoring others in the proper use of "git" and which may apply to proponents of good static type systems: It turns out that the statement "Don't you wish you could have avoided making that embarrassing mistake?" is a great motivator for learning. (In the "git" case it was about interactive-rebase-and-squash-before-merge. In the Haskell case, well... I guess it's pretty obvious that it was about catching errors before they hit production.)
&gt; CQRS+ES That still seems pretty darn close to what `acid-state` does. All update events are written to an event log and when you restart the server it replays all the events from some initial state to get the current state. To optimize startup, you can optionally create checkpoints that represent some point in time other than the initial state. But the checkpoints do not contain any new data -- they are basically just cached values. Admittedly, the API for accessing intermediate states does not exist. But, only because no one has added it yet. There is definitely a demand for it -- even if only to allow easy rollbacks. 
That shows up in *Data Types, a la Carte*
I think it's an allusion to the (completely unproven, btw) assertion that some developers are 10x more productive than others. (I don't think I've ever seen any supporting evidence that this is even remotely true when you take into account maintenance cost, salaries, impact on product releasability, etc. etc.) (Disclaimer: I think, but cannot be sure, I would probably be accounted in the "10x" group by most of the people around me. However, I don't believe in the idea for even a minute. Yes, there might be a small factor, but most of business-impacting developer work is a social endeavor where coding skill doesn't matter. FWIW, I derive most of my advantage from better languages and tools like Haskell.)
There's some really good stuff here: 1. The issue is really that `lift` sucks the readability and writability out of your code. 2. Benchmarks! So it's not just a rumor. MTL is fast. Eff isn't. 3. A new approach based on type-level Peano (Z | S s) numbers. It ends rather abruptly with "Problem solved? Not quite ... I’ll address this issue in a subsequent article." Please do. I'd be very interested to see if your -- I mean, the author's ;) -- approach holds up in terms of usability and benchmarks.
Oh, I meant in theory. How convenient it is in practice in a particular language is another question. (The whole comment of course was about incomplete support by both Haskell and OOP languages for one thing or the other.) That said, I'm sure there is a way, but the problem as posed is a bit too abstract for me to work out how. Could you cite a more concrete example? Edit: Of course, you already have in the OP. This sounds like some kind of `lens` or `multiplate` thing (which I think has been subsumed by `lens`) to me, but the particulars aren't clear to me at the moment.
I know — it's one of the 4 or 5 articles I have in the queue, but I haven't got a lot of free time these days. In terms of benchmarks it should be more or less the same as mtl. In terms of usability, I've been using it for work for many months now, and it certainly gets the job done. There are issues with error messages and compilation speed, but there's also hope that they'll be fixed in ghc at some point.
The other answers would slightly confuse me if I were first encountering this, so here's another take on it. The empty list expression isn't really a list. It's actually a function that takes a type and returns the empty list of that type. So if you have the expression `[]` on its own, it's actually not a list in the sense that [1,2,3] is a list. It's a different kind of object: a function from types to lists. It just happens to take that type argument implicitly, inferred by the compiler.
&gt;The method is called Snatch-driven development. Whatever this is, give it some other name. Please.
Serious question: How warm is it in Singapore and for how long? I used to live in a place where spring/summer/fall were brutal hot and winter was sweatshirt weather. Is Singapore the same?
My understanding is that having multiple transformers of the same type is the headline feature of extensible effects. But I've been wrong before.
When I use something like lens, I can collect multiple "states" into a single larger state e.g. baz :: (MonadState s m, HasFoo s, HasBar s) =&gt; m () This works regardless of extensible-effects or the mtl. I use this quite heavily in my day-to-day Haskell programming style.
Technically they still aren't "okay", as you're relying now upon the type 'checking' fragment of Haskell rather than the type inference fragment. This means that once you switch to an a la Carte approach, you have to know in advance the type of everything you write. You can't just engage in a dialogue with the compiler to find out.
Technically 'operational' is a slightly freer free monad. Every free construction in category theory is relative to what you forget. In the 'free' monad we are adjoint to the functor that forgets that something is a Monad, and merely remembers its Functor. In the 'operational' monad we are adjoint to the functor that forgets that something is a Monad, and merely remembers it has kind * -&gt; *. In both cases we're able to construct a sort of free near-inverse, which is the adjoint notion of freeness for what it is we forgot. Similarly we can take a 'free monoid' to be adjoint to the functor that takes monoids and forgets the monoid structure, and merely remembers the underlying set in question. Everything free is relative to what you forget.
When I remember I try to say something about needing more practice. My hope is to emphasize the development of skill. No one is born a master.
http://hackage.haskell.org/package/arrows-0.4.4.1/docs/Control-Arrow-Transformer-State.html
I had to play around with the code in GHC to understand why you changed the `update`/`display type` signatures and implemented the helper functions that way. (For anyone else unfamiliar: [existentials can't use record syntax](http://stackoverflow.com/questions/10192663/why-cant-existential-types-use-record-syntax) and because `update`/`display` need to be parameterized on `a` otherwise we can't extract any specific information from it) Thanks for posting the snippet, this seems like a viable alternative to closures. It does make the interface a bit wonkier but I suppose most of the burden is on the interface designer rather than whoever is merely implementing a new instance.
I feel bad wasting your time, sir. What is Machines for? How can Trifecta replace Parsec? Never mind some random discussion on reddit. The world is waiting with baited breath. 
Right, you can also combine the two state variables into one. That reduces composability though. You can partially regain it with a polymorphic wrapper like the ones in lenses, but at some point in your program you'll need to know about both of them to be able to define the concrete type.
In many common cases lists are quite fast though, especially when you know that they won't grow very large.
Absolutely. The difference is you don't wind up littering your code with unreadable lifts. In the one or two places in my code I need to actually pick a concrete instance I'm more than happy to pay that tax. The alternatives are either illegible messes of lifts that don't survive refactoring, give up usable type inference or are too monomorphic to get any code reuse.
Machines is me slowly exploring the design space around pipes and conduit, mostly because I'm not terribly happy with either solution. They are big and pile a lot of arguments into a thing to fit a problem. If you add enough parameters to an equation you can find a solution that fits anything. Some folks have gotten a lot of mileage out of it, but I'm still looking for a design with better trade-offs. Trifecta is something that would up 'put in production' rather early in its design cycle, and needs some more love. Time spent isn't a perfectly fungible asset. It is relatively easy for me to sit here and chat on reddit. It is hard to get long stretches uninterrupted to work on big projects.
No, I don't think that's the case. I'm pretty certain that purity was a secondary goal, lazy evaluation the primary.
I call it list, and it's available in the extra package: http://hackage.haskell.org/package/extra-1.0/docs/Data-List-Extra.html#v:list
Thanks for the explanation! 10x would then mean that instead of working eight hours a day, someone would only need 48 minutes to complete their tasks. That's a quite strong assertion. It either implies that the person stating it is very fast or their coworker is really slow. Or both. I also think that it leaves out the important aspect of domain specific knowledge. I might be faster than a Java programmer when it comes to writing a non-trivial Haskell program but I'd probably suck at writing the Java equivalent because I lack exercise.
Oh my.
Those of us who use [NixOS](http://nixos.org/) would (probably) comment favorably of it in respect to Haskell development. Here are a couple of introductory posts: * https://ocharles.org.uk/blog/posts/2014-02-04-how-i-develop-with-nixos.html * http://fuuzetsu.co.uk/blog/posts/2014-06-28-My-experience-with-NixOS.html In regards to hardware, it depends on the OS. If you'd like to try NixOS or another Linux distribution I'd recommend getting a Lenovo ThinkPad, but make sure you do [lots of research](http://www.thinkwiki.org/wiki/ThinkWiki) before purchasing! I personally use a T440s, and the driver support is great, the most recent model's screen compares to the retina experience, but the newer trackpads are abominations, for example. If you do decide to get something other than a MacBook, I'd recommend (not sure of your familiarity with Linux in general) trying out a VM in VirtualBox on your current laptop and setting up a small development workflow to get an idea for how you might, or might not, like it. 
To elaborate on the other replys, `t` is a *type variable*, eg. an identifier that the compiler may assign a type to. let empty = [] in -- The compiler infers empty :: [t] let nonEmpty = [1, 2, 3] -- The compiler infers nonEmpty :: [Int] empty ++ nonEmpty - On the last line above, we are using the `++` operator of type `[a] -&gt; [a] -&gt; [a]`. - Applying it to the left argument `empty` yields the type constraint `[a] = [t]` which can immediately be reduced to `a = t`. - Applying it to the right argument `nonEmpty` yields the type constraint `[a] = [Int]` which can immediately be reducede to `a = Int`. - If we substitute `Int` for `a` in the type of `++` we get `[Int] -&gt; [Int] -&gt; [Int]`. Thus `empty ++ nonEmpty :: [Int]`. If you replace the `nonEmpty` list with a list of strings, you can apply the above logic and it would work out to `[String]`. In Haskell, the type that is inferred for `empty` is actually `forall t. [t]`. Most compilers hide the forall part, but it means that every time we use `empty`, we eliminate the `forall t.` by substituting a fresh type variable, say `t1` or `t2` for `t`, yielding `[t1]` or `[t2]`. The benefit of this is that we can concatenate `empty` to something of type `[Int]`, and it will work. And later, we can concatenate it with something of type `[String]`, and it will *still work*. Why? Because we never conclude that `t = Int` the first time, thus making `t = String` nonsensical. Instead we temporarily say `empty :: [t1]` and `t1 = Int`, and later we can say `empty :: [t2]` and `t2 = String`. This is *polymorphism*. 
I use full-spec T440p for full-time work. Here's my comments: - Overall great laptop with really awesome keyboard. - Drivers are working out-of-the-box on Linux except for upper touchpad buttons(e.g. buttons of trackpoint). There are some config files that make this partly working(see http://www.reddit.com/r/thinkpad/comments/2nh6a7/t440s_x1c_clickpad_trackpoint_config_files_linux/ as an example) but I don't use any of them. I just learned how to live with it. - Tx4x touchpads are horrible because of non-physical trackpoint buttons. I think even Lenovo accepted that this was a horrible idea, because it's going to be fixed in next models of the series: http://www.reddit.com/r/thinkpad/comments/2mcxsz/a_sign_of_things_to_come/ If you have any specific questions about this particular model, just ask.
I use a Macbook with retina display. It's a very nice experience and the display makes it impossible to use any lower resolution screen, particularly with a beautiful font like PragmataPro.
For developing Haskell? Linux Linux Linux. If I were buying a new laptop, I'd go with the Lenovo: http://shop.lenovo.com/us/en/laptops/lenovo/y-series/y50-uhd/ It has a higher PPD screen than the MacBook Retina, similar specs on memory, CPU, SSD for half the price (1.5k). My MacBook Retina's screen developed a cloud of dead pixels after my warranty ran out, given that I paid something like 3.5K for this, and I just wanted it for the screen, I'm a little miffed about that. I'd expect better build quality. Also, it's trickier to get decent support for device drivers on the Apple hardware, and I spent an evening setting it up when it first arrived. Everything pretty much works here, but the wifi driver is not reliable. Whereas Lenovo generally has good support for Linux (they even sell Ubuntu laptops IIRC).
I've used an Air for a couple of years now and love it. It's fast enough for the majority of my work - most of my time is likewise spent doing Haskell or R dev - and I'll often mosh to a server somewhere when I need a beefier machine (i.e. a dedicated Linode or c3.large EC2 instance). I've never experienced the 2s vim startup that you describe and that would send me packing as well. I've never used the newer MBPs so I don't know how they've improved, but I love how light the Air is. So easy to move around. 
Only a part of your salary is excluded from tax, about $100,000. It's not difficult to exceed that limit. 
The weather is basically always the same, 30 C and chance of thunder showers. 
it was rather hilarious when at ICFP one of these guys explained "snatch-driven development" to the audience :)
Best moment of ICFP 2014.
I know, everybody as a Mac. I have one myself ;-). I think they tried to get developper when they made OSX being a Unix. Nowadays, Apple seems more interested in selling laptop to iPhone/iPad owner than developpers. I hate the fact that you can't do any update know without having to sign to the Apple store.
That's something I'm envisaging too. How do an Air compare to a Pro performance wise ? I know the SSD make them somehow quicker than Pro with Hard Drive.
About drivers on Mac ... Are you speaking of linux installed on a Mac ? I installed Ubuntu on a old Macbook pro, everything works fine, except the track pad is somehow too sensitive, and I am apparently touching the trackpad with my wrist when typing, which make the whole system unusable. It keep on clicking, closing and opening windows when I type. I never had any similar problem with the same Mac and MacOs.
What about the trackpoint ?
They're an acquired taste. Some people swear by them, but I myself certainly don't find it nearly as good as a trackpad, and usually plug in a mouse to circumvent having to use either the pad, or point.
Just to clarify, since /u/lolb’s question could have been asking two different things: - there is no difference between using `t` and `a` as the name of the arbitrary type; that’s just a variable naming choice. - there **is** a difference between giving `t` or `a` (a plain wildcard) as the type, and giving `[t]` or `[a]`. A plain wildcard would mean “it can be any type”. `[t]` means “it can be a list of any type, but it’s definitely a list”.
Get a 13" Pro. There's really no reason to get an air anymore; the pro's a tad more expensive but you're getting a retina screen and a better processor.
It is very close to the Equator, so temperatures are incredibly uniform throughout the year. 
I occasionally think about a laptop upgrade, but I'm still naively hoping that maybe somehow, someone, somewhen might once again offer a model with a 4:3 screen. This is more important for me than any other single factor, and I would be willing to pay a considerable premium, but alas. I hate hate hate 16:9 screens. I have one, sadly. (Yes, I've heard about the alleged economic motivators behind this state of affairs.) There's no hope, is there?
249 slides is a bit intense. Is there a video of this thing?
I think it rather refers to providing 10x as much value, which is far easier than completing the same program in 1/10th of the time.
We develop on OSX and deploy on linux (CentOS 7). The laptops are whatever the latest generation of MacBook Pros. We have not had performance problems, accept with MacBook Airs which have a hard time building the codebase. We use mvim + ghcmod which has not experienced that problems you describe. Having the nVidia card is great since we have been experimenting with accelerate. In the past we have used pieces of the xcode toolchain, specifically Instruments to debug issues with haskell programs. I may be a bit different than most in that I have no interest in a highly customizable linux environment for my day to day desktop stuff. I just want it to mostly work out of the box without any fussing. If its broken I want an easy way to get a new one. Tim Sears disagrees with me vehemently on OSX vs. linux and has been exploring nixos which I could probably be persuaded to run on the server in the future. If we did more systems level development I would probably use vagrant (or some other vm) but we don't so I don't. I have no insight on other laptops sine I have not had a non MBP since 2010 and before that it was dell which I would not recommend.
Slides are publicly available in BuildStuff website: http://schd.ws/hosted_files/buildstuff14/c5/Fp_Patterns.pptx 
&gt; http://shop.lenovo.com/us/en/laptops/lenovo/y-series/y50-uhd/ Hey, great post, thank you. I have a Macbook Pro Retina, the last one, and, too, am upset with the same things. I don't like the OSX and prefer very much Linux, but wouldn't install it on my note because of many issues. So, said that, how does the Lenovo battery compare to the Retina? What about the GPU? And the trackpad, is it as good? Sound, mic, camera? My mac comes from sleep in less than a second, is this valid for the lenovo? I'm tempted to move, but there are always some minor thing you forget to account for in those notebooks that hit you hard. At least with Apple I know they make every detail the best they can.
You should be able to disable the touchpad temporarily, if there isn't a button for it. [This should work.]( http://wpkg.org/Disable_/_enable_keyboard_and_mouse_in_Linux) It would be reasonably simple to make a bash script to toggle it, and then bind to to a key combo.
Thank you very much! Looks great. But could you elaborate on "nearly automatic"? When I finish my work I just close my mac and put it on my backpack. When I open it again, it is truly instantaneous and the battery is the same. I'm not sure even if this is the same as sleeping, but I never really need to "power" my mac off. Does it work the same way on the lenovo?
I have an SSD so I don't use swap or hibernate, I 'suspend' (which AFAIK is basically sleep). I have the the same functionality you describe. My laptop suspends when I close it and wakes when I open it (in &lt; 1 second). I can also manually put it into suspend. I would imagine it's a tad more battery intensive than a Macbook with OS X, but I can leave it suspended for several hours without much effect to the battery and days before it dies.
I use the same hardware with debian and it works really well. The suspend functionality works perfectly which is much better than I've been able to get in the past. I highly recommend getting an SSD for whichever machine you decide to go with. My laptop uses an SSD and is much snappier than my desktop with a more powerful CPU and more RAM.
I have no ideas, sorry. Last T5xx I used was T510 and it was amazing laptop. I used it for two years, again using Linux extensively.
I have a MacBook Air - getting a little old now - and it still works great for Haskell dev, no problems. I have Win7 and Ubuntu Trusty in VMs on it too, and they're fine (no not both at once). Other than that - not much on it besides Emacs and a few Haskell compilers. My desktop is Linux with xmonad - it's my favorite. I was only given a Mac because it's one of our deployment targets, but it's fine. And the Air sure is easy to haul around.
Get your credit card number out of your Apple Store account. Then you can keep your password in a place that's easy to paste in. And it will help protect you from installing stuff that will drag down your performance :).
Ah I see. Thank you very much! Just a last question if I can, considering I don't really mind about money for that kind of stuff, is there something even better I can buy, or is this (and thus MBP) pretty much the upper bound in what a notebook can be today?
Excellent slides. I read through them in one sitting. The quantity of slides was no big deal; the slides are concise easy to read. I've read a lot of tutorials like this and I felt this one in particular consistently cut through unnecessary jargon, allowing the core concepts to shine. The few jargon words (ie. endomorphism) that did appear were actually beneficial; I learned! I very much appreciate the way the concepts are introduced with multiple concrete examples. I thought the Monoid section was especially strong. Now I'm gonna go try adding some F# to my C# project. Thanks for sharing. 
It's not clear to me what a possible use case is for this program.
Preferably one with a dedicated λ key :P
~~Well, if I'm reading this correctly `path_b` and `path_c` will never be less than `path_a` so...~~ ~~`dist a b = abs (length a - length b) + (sum $ zipWith (\x y -&gt; if x==y then 0 else 1) a b)`~~ Edit: noooope
My past 2 laptops have been Dell Latitude E5440. Ubuntu &amp; Arch both run excellent on them. They are also super durable. My first one has taken far more than its far share of abuse and still works perfect.
 dist "cat" "at" Your solution gives 3, spec gives 1.
Yeah, I see why I was wrong: if one list contains a part of the other list those will match in b or c but not in a.
I just skimmed through the code and noticed that a lot of things happen in the IO Monad. Intuitively I would try to keep as much code as possible out of IO. Is this a design choice or is it because the program is running in the command line?
Have a W540, i7 quad core, 32gb of ram and a dual 256Gb SSD+1Tb HDD. Works like a charm. Real mobile workstation. Highly recommended. With the new parallel builds, up to 8 packages/modules at the same time. Joy! Feedback from GHC almost instant on every save. It's surprisingly light given the power and even the adapter is Ok. The i7 Haswell proc. is good on power. Last thing - love the keyboard. The only minor point is the display, so you might want to upgrade that.
Sooo... is this the reddit account of your alter ego?
I just bought a Lenovo X1 Carbon after trying to make a go of using my Macbook Pro as a Linux machine.
This is not a criticism at all, just an interesting different style: runAddAction :: Maybe AddAction -&gt; [Deck] -&gt; IO [Deck] runAddAction = maybe return newOrTo where newOrTo NewDeck = newDeck newOrTo ToDeck = toDeck 
If you construct a two dimensional array with `as` going along the top and `bs` going down the side, you can fill in each cell of the grid with the distance between the sublists starting at that cell. This means that the distance between the two lists is located in the upper left cell of this grid. Illustration (comparing "cat" and "at"): c a t ------------- a | 1 | 0 | 1 | ------------- b | 2 | 1 | 0 | ------------- This means the total distance between "cat" and "at" is 1. The value at each cell can be computed from the values of the cell below, the cell to the right, and the cell below-and-to-the-right. Let's call these three cells (if they exist) the neighbors of our target cell. So, for example, the a/c cell in the grid is computed first by comparing 'a' and 'c'. If 'a' == 'c', then the value at that cell is simply the minimum value of its neighbors. However, 'a' /= 'c', so the value is one more than the minimum of the neighbors. If there are no neighbors (i.e. bottom right corner) then the minimum of the neighbors is 0. This is the logic behind memoizing this function. I have also implemented it, but it is, imo, much less elegant than the presented method. However, it is probably pretty fast. I construct the array by lazily computing the values using the `loeb` function. Here is the code: dist :: Eq a =&gt; [a] -&gt; [a] -&gt; Int dist as bs = memoArray ! (1, 1) where memoArray = loeb $ listArray arrBounds [f x y | (x, y) &lt;- range arrBounds] arrBounds = ((1, 1), (na, nb)) na = length as nb = length bs arrA = listArray (1, na) as arrB = listArray (1, nb) bs f = computeDist arrA arrB loeb :: Functor f =&gt; f (f a -&gt; a) -&gt; f a loeb fs = let xs = fmap ($ xs) fs in xs computeDist :: Eq a =&gt; Array Int a -&gt; Array Int a -&gt; Int -&gt; Int -&gt; Array (Int, Int) Int -&gt; Int computeDist arrA arrB x y memoArr | arrA ! x == arrB ! y = minVal | otherwise = minVal + 1 where ns = neighbors memoArr x y minVal = case ns of [] -&gt; 0 _ -&gt; minimum ns neighbors :: Array (Int, Int) a -&gt; Int -&gt; Int -&gt; [a] neighbors arr x y = catMaybes [belowRight, below, right] where belowRight = arr !? (x + 1, y + 1) below = arr !? (x, y + 1) right = arr !? (x + 1, y) (!?) :: Ix i =&gt; Array i a -&gt; i -&gt; Maybe a arr !? i | inRange (bounds arr) i = Just $ arr ! i | otherwise = Nothing Feel free to make this more elegant as desired. For starters, this technique of using a 2d array to memoize a function that takes two lists as input is applicable to many problems, so that is worth factoring out and parameterizing.
Seems elegant enough, though to memoize it something like [Data.Memocombinators](https://hackage.haskell.org/package/data-memocombinators-0.5.1/docs/Data-MemoCombinators.html) should work I think. Running the type checker in my head, a very naive implementation could be something like this: import Data.MemoCombinators (memo2, char, list) dist :: [Char] -&gt; [Char] -&gt; Int dist (a:as) (b:bs) = path_a `min` path_b `min` path_c where path_a, path_b, path_c :: Int path_a = (if a==b then 0 else 1) + memoDist as bs path_b = 1 + memoDist as (b:bs) path_c = 1 + memoDist (a:as) bs ... memoDist = memo2 (list char) (list char) dist Based on my (superficial) understanding of memocombinators, I suspect this has a few issues; the memo table probably sticks around indefinitely between calls with different arguments, when really you only want it for the recursive calls. And I would be surprised if the memo-table lookup wasn't pretty slow also, due to it needing space for anything you might call the function with in the future. I think both issues could be fixed by making the call to the outer-most `dist` create the memo-ized version of the function, which takes as arguments int indices into the two lists (since memo-izing a function that takes ints should be quicker, though that would mean repeated traversals). Internally, what I believe memocombinators does is create a memo-ized version roughly like: natural :: (Int -&gt; r) -&gt; (Int -&gt; r) natural f = \n -&gt; memoTable !! n where memoTable :: [r] memoTable = map f [1..] I'm kind of curious to try both approaches to see how they compare.
I tried that, and it works perfectly, but I'm having a pretty hard time trying to see *how*. I'm definitely interested, can you tell me how it works? I assume it has to do with currying the functions, but that's as far as I get. Are there benefits to doing it this way instead? I always (wrongly) assumed that eta reduction could only get rid of one argument, so thanks for the different style!
Yes, it's related to currying. The type of `runAddAction` can be interpreted as saying: take a `Maybe AddAction` as input, and give me a function of type `[Deck] -&gt; IO [Deck]` as the value. (Remember, in type expressions, `-&gt;` is right associative.) Well, we have three functions of that type that we might give as the value: `newDeck`, `toDeck`, and `return`. Since the choice between them depends on a `Maybe`, it's convenient to use the `maybe` function from the `Prelude`. I actually find this style very readable and clear. It reads in English as: Maybe give me `return` if there isn't any `AddAction`, or give me `newOrTo` of the `AddAction`.
Right now I'm not doing that - I'm also using it just via an ssh shell. But I've done it in the past. It works OK, but you probably will need to get used to some different key mappings, depending on exactly how your particular VM container mangles your keypresses.
[this](http://jelv.is/blog/Lazy-Dynamic-Programming/) blog has a good explination of memoizing edit distence 
A nice set of slides. Near the beginning it was a little too much easy, and near the end it was a bit too much dense to follow. Probably it's better to not try to write a slides pack as this one that tries to go from zero to everything in one time, unless you are talking to both very smart and very ignorant persons, that are not common. The "purity will save the world" advocacy gets boring after a while, it's better to reduce it, and to avoid mixing too much the opinions from the good facts of functional programming. Show the good ideas and the smart programmers will see them with no need for lot of advocacy. It's a good idea to also show some of the downsides of the functional style, to be a bit more fair and reasonable (and I think some of the differences between F# and Haskell go toward practicality of usage. F# code is often pure, but it also makes mutation and usage of arrays easy, and F# contains OO if you want to use it). One nice thing that should be shown in a slides pack like this one is that you can put some of the language constructs of functional languages even in a mostly imperative language (like in Rust). From slide 70: type Hand = Card list type Deck = Card list To help avoid some bugs it's better to add stronger invariants to a type like Deck, like to require cards to be always distinct. &gt; "Functions in the small, functions in the large" I also like to use modules, packages and more.
The interaction between subtyping and covariance?
Thanks, I'll keep that in mind and try to refactor the code a bit. EDIT : Took care of most guards, I'm glad someone pointed this out, the refactored code is much easier to read
I'll never buy a laptop that doesn't have one. It's a mouse on the homerow of your keyboard. It's beautiful. 
great article :D I would just make a note that I would hesitate to use the word "nullary function" when referring to haskell "constants", only because the word "function" actually has a defined meaning in Haskell when referring to values: things of tyes involving the type constructor `(-&gt;)`. but yes, I really appreciate the spirit and purpose and direction of the post :D
Any idea how ATS compares to Rust?
I am ordering the System76 Galago UltraPro because it will be delivered with Linux working, does not have an off-center keyboard, and has a great screen. Although the screen is great, the build quality is supposed to be pretty bad, so I am crossing my fingers. http://forum.notebookreview.com/what-notebook-should-i-buy/765379-great-outdoor-screen-fast.html VMWare being slow on Windows running Linux was a big issue for me, but many Mac users seem to have figured it out.
typo: assingment
Conal Elliott expands on this a bit in his blog post ["'Everything is a function' in Haskell?"](http://conal.net/blog/posts/everything-is-a-function-in-haskell). The whole thing is worth a read, but to pull a particular quote: &gt; Many times, I’ve seen Haskell programmers (perhaps mainly newbies) use the word “function” to mean “top-level definition”. Which surprises me, as the meaning I attach to “function” has nothing to do with “top-level” or with “definition”. &gt; I guess what’s going on here is a conflation of functions and (top-level) definitions. Maybe because these two notions are tightly connected in C programming, where functions are always named (defined) and always at the top level and are always immutable, whereas non-functions can be defined in a nested scope and historically were mutable (before const).
My rule of thumb is that if I ever have to write 'lift' in user code I've failed. ;) Even with creatively named concrete lifts the code composes much less well than it does if you work on making it so that we can work with larger composite states/exceptions/environments and monoidal outputs. There we can see very clearly what portion of the larger monad you're using. This makes it a lot easier to break things out into smaller monads tailored to the role you are using them in in various portions of your application. Folks are free, of course, not to do this, but whenever I see folks arguing against the mtl on grounds that "lifts are hard" it seems to me that it is because they've adopted the other approach you espoused here.
Neat idea! One way it can be "broken" is by using GND (and no, this isn't fixable with roles): {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Typeable data A = A Int instance Typeable A newtype B = B A deriving Typeable A practical disadvantage compared to the standard Typeable is that when there's an unexpected run-time type mismatch, you can't print any useful diagnostics.
I think that haddocset is somehow not available as a "build tool" using this version of shell.nix. You could try the version of shell.nix suggested in some other comments here. I have updated my HOWTO to that version, so you can find it easily in the original article if you refresh it. 
That sounds really useful, thanks! Could be a good idea to let wxhaskell-users know. I'm looking forward to the day `nix-env --install haskell-wx` gets me a working wxGTK/Mac on Linux and MacOS respectively. wxHaskell makes a good test, I think, because it combine so many elements: * non-trivial packages * multi-platform, same package tree * doing the most native thing reasonable on their respective platform * cross-language package chains (Sorry to hijack this with my unrelated and fervent hoping-somebody-else-does-it)
Both good points. &gt; One way it can be "broken" is by using GND We can get around the GND issue by adding a dummy associated type to `Typeable`: class Typeable a where type Useless a type Useless a = () typeRep :: Proxy a -&gt; Unique typeRep = const (unsafePerformIO newUnique) This seems to work as of GHC 7.8, although I don't know about future versions (since it makes perfect sense to be allowed to use GND when the associated types are the same). &gt; A practical disadvantage compared to the standard Typeable is that when there's an unexpected run-time type mismatch, you can't print any useful diagnostics. Would the name of the datatype be enough? Two possible solutions I can think of are either to use GHC.Generics or add an extra (exposed) field to the typeclass (e.g. `typeName :: String`). Both have the problem that "bad" instances can be defined, but at least neither will break the type system.
You may be interested in the paper [Witnesses and Open Witnesses](http://semantic.org/stuff/Open-Witnesses.pdf) by Ashley Yakeley (unpublished as far as I know). It was a while since I read it, but I think the idea was quite similar. Section 4.2 talks about `Typeable`. 
Argh! Haskell has no nullary functions, only unary functions. 
Anyway, there is now an [implementation](https://github.com/bfops/extensible-effects/blob/917e67a7b7a08bb589094fbec33206479fafb398/src/Data/OpenUnion/Internal/OpenUnion2.hs) which doesn't require `OverlappingInstances`.
Sure, but there is special syntactic support for maintaining a convenient albeit not technically accurate view that there are functions of arbitrary arity - not just unary functions that may return unary functions that may return a unary fuction. If you understand that this mindset can sometimes be convenient and useful and pragmatically sufficiently acurate, then you should understand what is meant by nullary function.
While it's neat that Stackage supports non-released package workflows, isn't this also possible with Hackage just as easy by a combination of Hackage candidates, and exploiting `cabal`'s capability to install URLs pointing to sdist-tarballs (preferably into a cabal sandbox)? Describing a set of candidates would then simply be a line containing the URLs pointing to the candidate tarballs.
Running Linux on a mac is probably not worth it. I prefer OS X, so part of the reason I bought a mac is for OS X's superior typography and graphics systems
I'm well aware, though that version still doesn't live in the inferable fragment of Haskell.
You could write the function using open recursion: import Data.Function (fix) import Data.Function.Memoize (memoFix2) dist :: ([Char] -&gt; [Char] -&gt; Int) -&gt; [Char] -&gt; [Char] -&gt; Int dist f (a:as) (b:bs) = path_a `min` path_b `min` path_c where path_a = (if a == b then 0 else 1) + f as bs path_b = 1 + f as (b:bs) path_c = 1 + f (a:as) bs dist f as [] = length as dist f [] bs = length bs Unmemoized, it can be used with *fix dist*, memoized (e.g. with Data.Function.Memoize, but you could roll your own) with *memoFix2 dist*. This does not lose too much of the elegance of the solution, although I'm pretty sure it would be more efficient to memoize not by the string parameters but by offsets into them.
I don't have a good idea, because I don't know much about Rust. Maybe this braindump of what I learned about ATS will be helpful: * ATS compiles to C and is closely tied to C. It has no runtime of its own. * It can do unsafe and safe memory operations via boxed or unboxed types. * [Finalization](https://github.com/chrisdone/ats-examples/blob/master/004-finalization.dats) of objects happens by lexical scope like in C. * While it supports boxed values, [you have to be explicit about which you're talking about.](https://github.com/chrisdone/ats-examples/blob/master/020-polymorphic.dats) Note: 'type' is like Haskell's `*` kind, and `t@ype` is like Haskell's `#` kind. * Its polymorphic functions on unboxed types are like C++ templates (and IIRC are called templates in ATS) whereas functions working on boxed types are called polymorphic functions. * Similarly, it doesn't have closures unless you [explicitly specify it in the type system](https://github.com/chrisdone/ats-examples/blob/master/015-closures.dats) "my function carries an environment". * It's got [lambdas](https://github.com/chrisdone/ats-examples/blob/master/010-functions.dats#L7) (with the previous caveat). * It has [a macro system.](https://github.com/chrisdone/ats-examples/blob/master/023-macros.dats) [Dependent types](https://github.com/chrisdone/ats-examples/blob/master/025-dependent-types.dats) (via a constraint solver) * and a [proof system.](http://www.cs.bu.edu/~hwxi/ATS/EXAMPLE/INTRO/fact3.dats) `dataprop` kind of reminds me of type families in the way it's used. * It has partial application support. * It expects you to write in a functional style, so it supports [tail call elimination](https://github.com/chrisdone/ats-examples/blob/master/012-tco.dats) * and also you can explicitly say [that two functions are mutually recursive](https://github.com/chrisdone/ats-examples/blob/master/014-mutualrecursive.dats) and it will compile it down to jumps. * It does have [an exception system](https://github.com/chrisdone/ats-examples/blob/master/021-exceptions.dats) similar to ML's. I'm only half way through the book, though. There's a bunch more dependent and linear typing stuff I've not learned yet.
In Tracker, you could make shouldQuizCard not return IO if you split it into 2 functions, with one taking a time difference and returning a boolean.
The main difference between the two is that, using the approach you describe, the user needs to do dependency solver him/herself. With the Stackage experimental snapshot approach, you can continue to let cabal-install do the dependency solving. Consider the following two examples: 1. Suppose I'm working on a new release of WAI\*, so I put together experimental snapshots wai-3.1, warp-3.1, wai-extra-3.1, wai-app-static-3.1, and yesod-core-1.5. If you use the Stackage snapshot approach, you can type `cabal install warp` and it will automatically determine that it needs to install wai and warp, and not bother to install the other three packages. Similarly, if you have a project that uses the CGI backend, it will download just wai and wai-extra, and not bother with warp. You'd have to do that solving manually with candidate tarball URLs. 2. As a more contrived example, suppose that at the same time as the above packages, I also released a new version of conduit (2.0, again, I'm not actually doing such a thing). Suppose this version of conduit has a lower bound that prevents transformers-0.3 from being used, but your package requires transformers &lt; 0.4. You'd have to manually solve that if you provide explicit URLs. With a snapshot, cabal-install can do the solving for you. A few other important differences: * You can make a snapshot containing new versions of packages you're not authorized to upload to Hackage, which is great for getting feedback on a proposed pull request. * You can have multiple snapshots with different candidate versions. On Hackage, each time you upload a new candidate with the same version number, it will overwrite the old one. * It's arguably easier to upload a large number of interconnected packages as a Stackage snapshot, since you simply put them all together in a single tarball and upload that one file. \* For the record, I'm not.
Is it any good (especially the track pad ?)
The analysis of /u/sccrstud92 in terms of a 2d grid is beautiful, but wow, is that code a huge premature optimization. Why do we need arrays here? And OP, why are you assuming that we need "memoization" here? In my experience, "memoization" is not a very useful concept for the same reason. It's almost never the right approach to just throw some general "memoization" technique at a problem, unless you already know that there is a memory leak and you already know that this technique really is the best way to solve it in this case. Instead, just improve the algorithm. Here, after the analysis, we note one more observation: that the algorithm is invariant under reversing both lists. That's important; given the input as lists, we'll be looking at this data from left-to-right, not right-to-left (even if you suck them into arrays), so we'd rather compute it from the upper-left than the lower-right. And here's a simple and fast solution, using only Prelude functions, in memory that is linear on the length of the first list and constant on the second: dist :: Eq a =&gt; [a] -&gt; [a] -&gt; Int dist as bs = last . foldl row [0..length as] $ zip [1..] bs where row xs (x, b) = scanl (col b) x $ zip3 as xs (tail xs) col b x (c, y, z) | b == c = min3 (x + 1) y (z + 1) | otherwise = min3 x y z + 1 min3 x y z = x `min` y `min` z EDIT: Whew, edge cases are tough. Finally got it to work right for empty lists without special-casing them. EDIT2: Corrected the algorithm, thanks to /u/rampion in the comment below. For the `b == c` case of `col`, I originally had `min3 x y z`.
Attached shell.nix was that one actually. Can you refer pkg added to '~/.nixpkgs/config.nix' from 'shell.nix'?
Shouldn't you use `ef` for something below? If not, why to pass it around in the first place? sm2 :: Int -&gt; Float -&gt; Float sm2 n ef | n == 0 = 1 | n == 1 = 1 | n == 2 = 6 | otherwise = sm2 (n-1) ef Also, in shouldQuizCard :: Card -&gt; IO Bool shouldQuizCard card = do currentTime &lt;- fmap round getPOSIXTime :: IO Integer let timeDifference = currentTime - ctTimeQuizzed (cardTracker card) daysSinceQuiz = floor $ (fromIntegral timeDifference :: Float) / 86400 sm2Time = (ceiling $ sm2 (ctN tracker) (ctEF tracker)) :: Integer tracker = cardTracker card You do not use `card` for anything except to extract the tracker. You could pass `CardTracker` to the function directly. (Also, you should pass the current time as an argument, so this wouldn't need to be IO at all)
My wife always said that mathematics makes people say silly things, like "1 x 2 = 3". Now that we're applying mathematics to programming, programmers say those silly things, too. (slide 204).
To a C programmer, a "function" is something that accepts 0 or more arguments and performs a computation. In that sense, everything in Haskell is indeed a function, because lazy values can have computations lurking inside them. Maybe in a strict language there would be less confusion, because you'd be able to tell apart computations from inert values by looking at the types.
I understand completely what a nullary function is, and so does Lennart. There aren't any in Haskell. There are a lot of false statements that it would be convenient to believe.
How does this compare to [persistent](http://hackage.haskell.org/package/persistent) + [esqueleto](http://hackage.haskell.org/package/esqueleto) ?
The most obvious difference is that Opaleye aims to be typesafe in the sense that a query which compiles cannot fail at runtime. With esqueleto on the other hand it is trivial to write failing queries. 
&gt; the build quality is supposed to be pretty bad I didn't find that true of mine. However, I find the touchpad far too sensitive and hard to avoid, which is less than ideal. 
Well, here's the code I would write: import Data.Algorithm.Diff isDiff Both{} = False isDiff _ = True dist as bs = length . filter isDiff $ getDiff as bs But [using a library](http://hackage.haskell.org/package/Diff-0.3.0/docs/Data-Algorithm-Diff.html) is cheating, right? I'm skirting the question entirely by not showing how to solve the problem? Well, no, not really: if you want to know how an expert Haskeller would solve this problem in practice, you should look no farther than the source of that library.
Congratulations!
Please feel free to note if I forgot anything important
I remember running across that a while back, and it is indeed an inspiration for this. I somehow missed or forgot about the `Typeable` part - thanks for that! Unfortunately, it requires a language extension not present in GHC (which is almost equivalant to `&lt;identifier&gt; = unsafePerformIO newIOWitness :: IOWitness &lt;type&gt;`): &lt;identifier&gt; :: IOWitness &lt;type&gt; &lt;- newIOWitness The paper requires &lt;type&gt; not to have any forall'd type variables, so we can't just use `unsafePerformIO` here. If we did, we would be able to define `listW :: IOWitness [a]`, which would then allow us to coerce between something like `[Int]` and `[Char]`.
Does it already support the `jsonb` type ?
I recall reading that the types of the records you want to persist have to be parametric in every field, so instead of ```Person { name :: String, age :: Int }```, you need to have ```forall a b. Person { name :: a, age :: b }```.
Awesome! I read about OpalEye a few months ago, and I wonder whether any major changes have been made in the last few months, leading up to this release?
Currently not. I would have to think about how to fit that in nicely to the type system. Presumably there are a lot of nulls floating around with the Postgres json functions.
Wow, gold! Thanks kind stranger, you made me smile. I hope you enjoy Opaleye.
Your preview is unreadable. Consider rendering an SVG
Thanks, and thank you for giving me your code review.
It hasn't changed in any way that would really be noticable to anyone who wasn't using it in production (which is only three or four companies at the moment). The API has actually become a lot simpler in the last few months. Also I rewrote the query AST. Previously I was using HaskellDB's AST but the optimization engine was full of bugs. I wrote about the precise changes here: https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/UPGRADING.md [EDIT: Fixed link. Thanks rehno-lindeque.]
Well, yes, I'm sorry I was terse. Congratulations on release what looks like a serious SQL library! I'm very happy to see it, and I'm asking because I'm genuinely interested in it and want to know where it fits into the space. I asked specifically about persistent, but there are also the ones that /u/chrisdoner mentioned. Other well-used higher-level libraries that generate SQL are acid-state and groundhog; their design philosophies are less similar, but there will definitely be overlap with those too.
It's probably misleading to think of Opaleye as a persistence engine for Haskell. From the point of view of Opaleye it's the database that is primary and Opaleye helps Haskell communicate with it, not the other way round. This is somewhat vague though, and a layer on top of Opaleye may become a persistence engine in the future if that seems appropriate. Still, you don't *have to* make your datatypes polymorphic but it will save you a lot of headaches. For example, if you have a `Person` datatype with `String` and `Int` fields in Haskell then on the Opaleye-side they will become `Column String` and `Column Int` fields. If your record type is not polymorphic then you'll have to have a whole new datatype to store them.
Here's a high-res version: http://i.imgur.com/Lqwq53f.png
From what I can tell hasql is similar to postgresql-simple in that it is orthogonal to Opaleye and would make a good backend.
By the way, feel free to keep asking me questions about Opaleye, either now or email me later. When I get a sense of what people would like to learn I will write up the answers.
Can lens handle updating mutually recursive datatypes in a single pass? That's multiplate's strength.
Thanks ocharles, and thanks for your help with the library.
Fantastic - thanks! Persistent does provide type-safe queries for the types it manages, but only a limited selection of them; about 40 different kinds of queries. See [Data.Persistent.Class](http://hackage.haskell.org/package/persistent-2.1.1/docs/Database-Persist-Class.html). Most conspicuously missing are joins. For any other queries, you use SQL, and currently the recommend way to generate the SQL is with esqueleto. Esqueleto does offer some type safety, but it explicitly does not attempt to achieve the level of type safety that Opaleye is aiming for. So Opaleye is definitely a strong candidate to fill an important gap in the persistent ecosystem. It would require an analogue to your `makeAdaptorAndInstance` splice that would integrate with persistent's own TH-based DSL to create Haskell types and matching database schema.
Yeah, I really wish nix was installable with MinGW. If it was I would have used it instead of powershell.
fixed link: https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/UPGRADING.md
&gt; From the point of view of Opaleye it's the database that is primary and Opaleye helps Haskell communicate with it Just the way I like it. :) Thanks for the clarifications!
To my knowledge acid-state and groundhog do not generate SQL. Groundhog is a DB library, so I suppose you could argue that it generates SQL implicitly, but that's not the interface that you typically use. acid-state is something else entirely.
Cool!
To be clear, users of Opaleye probably will not explicity generate SQL either, rather they will write Opaleye queries which transparently use postgresql-simple to send the SQL to the server.
You mean Silk. "Slick" is the Scala relational query library :)
/u/tomejaguar I reposted your announcement on the [yesodweb](https://groups.google.com/forum/#!forum/yesodweb) googlegroup. If there's any interest or discussion, perhaps you'd want to participate.
What is meant by "strength"?
Ah right, cool. Thanks for publicising! I willl join in if people there want to discuss.
Gah! If I had a penny for every time I got those two confused...
But this is also weird. Haskell is call-by-need (and not call-by-name), which means these 'nullary functions' can only ever be 'called' once, after which the computations they invoke will have been performed and afterwards they will perform no computation and just return a value. Once the computations have been performed, would you say that they are 'no longer functions'? If no, then you have an incorrect view of Haskell semantics; if yes, then you are arguing that evaluation can change functions to non-functions, which is a much more bizarre way of explaining Haskell than just saying, "Functions are values of type `a -&gt; b` for some `a` and `b`, but computation can also be performed by evaluating non-function vaues."
I'll let you know when it arrives. The main bit of advice I would give is make certain you get a third-gen X1 Carbon, there's a lot of 1st and 2nd gens floating around. I'm pretty hopeful/confident it'll be good, but understand that I'm not getting it for the trackpad, I'm getting it for the *trackpoint*. It's my favorite feature of thinkpads. If you're hellbent on using a trackpad, I would recommend checking reviews. Edit: Also a big part of why I got the X1 Carbon over the other Thinkpads is nothing else had two digital video outputs! I use two external monitors with XMonad so this is critical!
Cloud Haskell. Simon Marlow's Parallel and Concurrent work.
Yay, congratulations and thanks for making this awesome library! I'm planning to write a blog post about Opaleye, but for now here's a bit of a review: At Silk we have basically used string interpolation to write queries (HDBC). As you'd expect this has become maintenance nightmare. We've done lots of research into the right tool for the job, whatever tool we picked we would have to rewrite all our queries and I've started doing this with several projects but along the way realized that it wasn't worthwhile or even possible for us to make a switch... until Opaleye! The Good: * Tom has been very helpful in the migration process and he has implemented lots of stuff we've requested. * We don't have to change our existing database schema to do the migration. Very important for us. * If it type checks it works seems to hold just as with normal Haskell code. Every time i get a type error i sigh in relief :-) * I've compared some larger queries that we have and confirmed that postgres is able to optimize what opaleye generates to the exact same query plan as manually written queries give. * Trivially extensible to support additional types (we want lots of newtypes! Edit: And simple ADTs!) * Abstracting and reusing queries is very natural and removes tons of duplication. It's a win/win since Opaleye also puts us in a situation where duplicating queries is no longer a big deal. * You don't need `-XArrows` to write queries, a lot of times using simple combinators makes the code read nicer, and creating these combinators is simple. The Room for Improvement: * The queries generated from Opaleye are larger and harder to read than manually written SQL. This is a bit daunting but it's not that hard to understand what's going on. I also haven't had reason to look at the raw sql much. I'd prefer if opaleye generated explicit joins rather than nested queries. * It's sometimes hard to pin point where a type error originates from, I think we stumbled across bugs in GHC where it would give error locations completely unrelated to the actual error. This might also be because of the type families we introduced... * Some things are missing, notably subselects. We've implemented other missing features ourselves without much trouble. In some cases we have just figured out another valid way of doing it that's already supported. Other notes: * Migrating queries is hard. Most bugs I've come across are mistakes when converting from strings to opaleye. Not really anything that can be done about this, you just have to be careful. Rating: Will use in production/5 
Your comment is a little vague, but I'll try to address it as best I can. If you contribute code to the Snowdrift.coop platform, it could be under some compatible license you mark for your code, but the normal procedure would be to license under the same AGPLv3+. In *either* case, we do not have a CLA. You retain copyright on your own code submissions. The platform we're building is AGPLv3+ licensed, which certainly means that *using* the code from the platform overall for use aside from the site itself would be under AGPLv3+ which will not be compatible with businesses who want to use it for proprietary software. We're welcoming and encouraging help from people with any mix of supporting our mission and wanting to do some Haskell stuff with a friendly, supportive, and meaningful community project. We aren't even building libraries that would be of general use. It's more of a downstream project than an upstream one. Although we would like eventually to push upstream some useful bits we have when we find time to do that. Hope that answers your concerns. I understand it may not be a good project for you.
Congratulations! I like how easy it is to get SQL from a DSL query. Do you have future plans to tackle aliases? If table prices has many columns the pattern matching on a tuple for a query below may become cumbersome. select p1.* where p1.adjusted &lt; 100 from (select prices.*, price * conversion_rate as adjusted from prices) p1
If it's all downstream applications for end-users, then that makes sense, but I think most programmers in OSS these days work on libraries they'd like to be able to use at work. At least anecdotally.
`lens` has an actual [`Control.Lens.Plated`](http://hackage.haskell.org/package/lens-4.6/docs/Control-Lens-Plated.html) module. I'm not sure how it relates to multiplate; uniplate and biplate are mentioned in the documentation but multiplate is not, so perhaps this suggests that it's less powerful. But you would have to compare the types to be sure, which I haven't done.
I agree. Incidentally, that's the *purpose* of Snowdrift.coop — to create a new funding model to get more people working on free/libre/open *downstream* projects. Because you're completely right about the status-quo. And yes, the majority of our work and the site is all downstream stuff.
Personally, I like OS X + Nix. OS X let's me run photoshop, premiere, ableton live, after effects, etc. And the nix package manager gives me a nice development environment with all my favorite 'linux' stuff. I can even share my config.nix between a linux and os x machine to make them even more similar. That said, I am still new to the nix side of things. So, perhaps the love affair will end later.
A function that checks for the presence of a cached value, and returns it if it exists, is also a function in the C sense.
To be clear, once we are *operating*, the purpose is to fund free/libre/open projects of all sorts, and each project will remain fully independent and use whatever free/libre/open license they choose.
Only in the background. But as /u/tomejaguar points out, that will also be the normal usage mode for Opaleyes. These are all higher-level libraries for using SQL libraries as persistence backends in Haskell, so people will definitely want to know how to make an intelligent decision about what library to use.
But it doesn't "check for the presence of a cached value"; it is, itself, a computed value.
Do you have an SSD? I have upgraded my own 2010 mbp and its fast as hell. All apps open in ms. Something huge, like Photoshop is opened in 3 secs. I also installed a new SSD on my gfs old 2008 macbook, its like a new machine. 
In practice this turned out to be much less annoying than I originally thought and the best alternative so far (I also played with using plain tuples, and some other things). We now have some TH that does this: makeType [d| data Person = Person { name :: String, age :: Int } |] =====&gt; data PersonP a b = Person { name :: a, age :: b } type Person = PersonP String Int The result of a query would be `[Person]`, and with a type family the type of a query becomes `Query (To Column Person)` or even `Query (To Column (To Nullable Person))`. Type errors can be a bit big because of all the polymorphic fields, but if you start out by writing top level type sigs that point is often moot.
&gt; You could then compose with count and project to ostensibly get a query &gt; Expr PersonId -&gt; Expr ProductId -&gt; Query (Expr Int) Technically that type doesn't match with what HaskellDB can do, as [`count` in HaskellDB](https://hackage.haskell.org/package/haskelldb-2.2.4/docs/Database-HaskellDB-Query.html#v:count) is has type: `Expr a -&gt; ExprAggr Int` And when you project you get a `Query (Rel …)`, which can only be accessed again using `select`… &gt; In HaskellDB this might have even worked, I'm not sure. However, it definitely did not work if you then composed this query with other queries, especially ones that contained aggregation. In fact it was easy to get HaskellDB to generate nonsense queries. Can you give example code? It seems easy to make Esqueleto generate nonsense for aggregation, because the type of `count` yields a new Expr instead of a separate type… but I've never seen this for HaskellDB. 
It looks indeed like a disk problems. I'm planning to get SSD if I get a new laptop, but I don't I'll bother swapping my current hard-drive.
I've been refreshing your Blog page all day waiting for this :)
Oh, that looks pretty sweet. Any code you could share? Sounds like it might be a perfect little TH starter project.
If you're interested in contributing to the site, and want to be sure your changes are useful in BSD3-licensed work, best would be to develop reusable functionality in a separate BSD3-licensed library that the Snowdrift.coop site could depend upon. That's maximally reusable. If you're interested in contributing a small patch, or something that touches more of the internals, you could keep it licensed BSD3 and Snowdrift could incorporate it under those terms - but it wouldn't be very reusable in the first place without the rest of the code, so it's not even going to matter much. If you're talking about funding your own work on the Snowdrift.coop platform, it doesn't require relicensing - we will happily support BSD3-licensed works. Please contribute time, money, contacts and publicity, as you're able, to help make that possible sooner :) 
This prompted me to add a ton more license info to my git-annex binary distribution. It's not as easy as "BSD", because there are several standard variants of the BSD license (2 clause, 3 clause, 4 clause); there are versions modified to name some entity like University of Glasglow, and there are special variants like used by time, setenv, tasty, etc that arn't quite BSD in their language though probably are effectively the same. All of those have to be included for full compliance. Then there are the libraries that are partly derived from the Haskell 98 report, and so have their own unusual license, etc. What really gives me the heebie-jeebies is the indirect dependencies of my package, which can change on the fly without my ever noticing as the library landscape shifts. It will be hard to keep on top of that on an ongoing basis. All I can do is make a best-effort attempt, and my license file is already 5000 lines long.. Thoughts: 1. It would be very good for the Haskell Platform to have a single license file collating all the licences therein. 2. In the perl community, there's a tendancy to say "Licenced the same as Perl itself", and this greatly simplifies things. 3. An automated tool to emit the licenses of all direct and indirect deps of a cabal package would be a nice solution.
Very nice! I'll give it a closer look when I've the bandwidth.
Ah, fantastic! I knew I couldn't have been the first to hit it...
&gt; Does Opaleye have support for Postgresql's hstore/json? Not currently.
Ah, I'm sorry, I thought you were aware of Russell O'Connor's paper, [Functor is to Lens as Applicative is to Biplate: Introducing Multiplate](http://arxiv.org/ftp/arxiv/papers/1103/1103.2841.pdf), so I thought you could answer that. Very interesting paper, by the way. He used coalgebras of the Store comonad to abstract away from Lenses and (bi)plates, laying bare a very fruitful isomorphism. I think I have some examples of multiplate code laying around somewhere if you're interested. EDIT: I'm wondering; does multiplate achieve the same as attribute grammars (only generically)?
Aha, epic!
Nice twist!
I have the [Lenovo Y510P](http://shop.lenovo.com/us/en/laptops/lenovo/y-series/y510p/) and while I do like the laptop, I must say that it is certainly not as Linux friendly as most of the other laptops in the Lenovo stable. There are a myriad of issues from the network drivers to the SLI Nvidia graphics (for science really!). Once I updated the kernel module for the wireless driver, it has solved that issue. Also, I got the graphics cards to work nicely, but it was not simple. The biggest thing is the trackpad sucks. Easily the worst trackpad I've ever used. It makes the mouse an absolute necessity if you are going to do something serious for longer than a couple of hours. 
Looking forward! p.s.: Request from me: {-# LANGUAGE ExtendedDefaultRules #-} 
Sure, I'll try to strip our internal dependencies and put it on github. Beware that it's so experimental it might explode though :-) 
In terms of listing projects on the snowdrift site, BSD licenses are fine. So if you have some neat idea for a project you'd like to do that you'd like to list on the site once it's operational, you could choose GPL or BSD or similar licenses for your project. But the code that lies underneath the snowdrift site ITSELF is licensed under AGPL, and so if you wanted to make your own snowdrift-like site, you'd need to continue to use AGPL for that.
This is extremely important and a valuable thing to reinforce.
I assume we have Chris Done to blame for this request? ;-)
The internal process of updating a thunk is, in some sense, a type-changing update.
Great idea. But, does it mean there are not enough interesting libraries to cover 24 posts?
If the type of something does not have a function arrow as its outermost type constructor then it is not a function. Pretending otherwise just increases confusion. 
Actually, Haskell is non-strict. Call-by-name is a valid (but crazy) evaluation strategy for Haskell. Good implementations use a mixture of call-by-need, call-by-value, and call-by-name. 
**EDIT**: The API of HaskellDB is fine, it's a buggy optimizer. [See here](http://www.reddit.com/r/haskell/comments/2nxx7n/announcing_opaleye_sqlgenerating_embedded_domain/cmiqndg). --- I had trouble understanding the problem the guy had in that post, I think what he demonstrated was actually fine. But in doing so I managed to pinpoint the problem with HaskellDB's aggregation, which is that it loses the type safety: agesOfFamilies :: Query (Rel (RecCons Family (Expr String) (RecCons Age (Expr Int) RecNil))) agesOfFamilies = do my &lt;- table personTable project (family &lt;&lt; my ! family # age &lt;&lt; (_sum (my ! age))) The return type of this query doesn't make sense. "Expr Int" is wrong. The `Age` type should be `ExprAggr Int`, like in the type of this: age &lt;&lt; _sum (undefined:: Expr Int) :: Record (RecCons Age (ExprAggr Int) RecNil) This type is actually harmful, because it misleads. Sure, you can't write `count (_sum …)`. Great. But the `ExprAggr` type does not bubble up through the query language. The `project` function transforms the types so that the `ExprAggr` is lost and it becomes `Expr`. This leads to you later on down the line being able to write: justAgesOfFamilies :: Query (Rel (RecCons Age (Expr Int) RecNil)) justAgesOfFamilies = do agesOfFamilies &lt;- agesOfFamilies project (age &lt;&lt; (count (agesOfFamilies ! age))) Which type checks. And now it spits out, like the Esqueleto problem, nonsense: &gt; putStrLn $ show $ ppSql justAgesOfFamilies SELECT COUNT(SUM(agecol)) as age FROM mytable as T1 Now your original comment makes sense in context, I had no idea this type safety was lost! It's a pity that HaskellDB had this correct but messed it up in the projection. Oh well! This should provide some solid criticism that you can put in an article of the kind of subtle issues hiding in HaskellDB, along with [things like this](http://chrisdone.com/posts/haskelldb-more-type-safe). It's hard to tell whether it's the approach or just a bad implementation, but at any rate, nobody is maintaining it so the library is as good as broken or at best incomplete. Opaleye to the rescue! ;-)
Yeah, I don't mind a bit of fireworks. Thanks! :D
This is my favorite time of year. Thank you Ocharles!
This statement, while true from a certain point of view, only makes sense when applied to Haskell if you are using two simultaneous definitions of the word _type_ (i.e., "type" as the static type given to a lexical Haskell expression versus "type" as the runtime tag given to a chunk of memory which represents either a computed value or an unevaluated thunk). This would mean that a statement like, "The type of a value in safe code cannot change at runtime," is simultaneously true and false. In that light, it's probably better to say that updating a thunk does not change its _type_ but will change its _tag_. In the same way, it's better to say that `x = 2 + 3` defines a _computation_ which may be evaluated, but not a _function_ which may be applied.
\o/
Is there a good write-up on the pattern of transforming a "closed" data type into an "open-recursive" one? The blog post mentions this as easing that pattern but it doesn't actually seem to link to a good introduction on what this sort of pattern actually is.
[EDIT: TL;DR I updated the article radically to excise imprecise use of "nullary function". However I still talk about "binary functions" because it's too useful of a concept. I still don't have a good name for "fully applied function" :) Thanks everyone!! I agonized over this [unsuccessfully apparently :) ] to arrive at the conclusion that for an introductory article, "nullary function" was the least confusing compared to CAF (which seems overly focused on GC), or other distinctions that take away from my main goal which was to say: "WARNING: equals doesn't do what you think it does", ie, the equals operator doesn't have special significance *in the large* when a literal is on the RHS. (Also, CAF seems to focus on top-level and another goal of the article was to say "Yay Haskell define functions everywhere wheee!" Again I feel like top-level vs local is not germane to an introductory discussion). So what do we call, in language that welcomes newbies: 1) a CAF 2) a "fully-applied" function? [EDIT] read Conal's article linked (below)[http://www.reddit.com/r/haskell/comments/2nwnq2/function_application_and_definition_in_haskell/cmhpji4] which is ridiculously germane, and has the following comment by Conal: *the idea of “nullary functions” is part of an informal perspective that may sometimes aid intuition.* *Perhaps confusion arises when one person is assuming a formal/precise perspective and another assumes an informal/intuitive perspective.* *So perhaps a helpful response to the question “Is everything a function in Haskell?” is “Not really, but some people find it helpful to informally think of non-functions as nullary functions, just as thinking of curried functions as binary, ternary, etc.” Or, more succinctly, “Yes, but not really.”* I'm definitely of the "informal/intuitive perspective" but I hate to mislead. Perhaps a note in the article is in order ...
This naive definition isn't referentially transparent: typeRep Proxy = unsafePerformIO newUnique Note that this is equivalent to: typeRep = \Proxy -&gt; unsafePerformIO newUnique We want to return the same `Unique` value for every call to `typeRep`. To do this we simply move the call to `unsafePerformIO` _outside_ the function definition and capture the value in a closure: typeRep = let x = unsafePerformIO newUnique in \Proxy -&gt; x Which I then reduce to: typeRep = const (unsafePerformIO newUnique) I hope that helps.
Does Monokai predate sublime at all? It is pretty popular among theme collections these days.
 strength :: Functor f =&gt; a -&gt; f b -&gt; f (a, b) strength a fb = (,) a &lt;$&gt; f b can be seen as a way to distribute `(,) a` over any functor 'f'. It doesn't work in every category, but it does in Haskell.
{-# LANGUAGE OverloadedLists -#} could be interesting: https://ghc.haskell.org/trac/ghc/wiki/OverloadedLists Looking forward to the posts!
I love your 24 days posts! I can't believe I forgot about them.
Indeed. https://snowdrift.coop/p/snowdrift/w/en/mechanism#fn1 We've also done thorough research on all the different mechanisms for funding free software, see https://snowdrift.coop/p/snowdrift/w/status-quo-floss And we reviewed over 600 existing platforms, summary here: https://snowdrift.coop/p/snowdrift/w/othercrowdfunding Cheers EDIT: Oops didn't see dllthomas already replied. Oh well. Only semi-redundant reply.
A bit annoying that `as` is getting highlighted as a keyword when it appears as an identifier but presumably that's hscolour's fault.
speaking of hscolour... is there a public bug tracker somewhere?
This is a nice piece of work, but I can't help but there must exist a more elegant type-safe solution to this problem that doesn't involving splitting all of this parsing and typing across different compiler phases with TemplateHaskell. 
Does `PersonP` really have to be totally polymorphic in the type of its fields, or would it be enough to make it a higher-order type, like so: data PersonP f = Person{ name :: f String, age :: f Int } type Person = PersonP Identity ?
Great introduction to Haskell! I think it's okay to just define a nullary function as an immutable value. There's nothing wrong with that definition and it's a useful intuition. For instance, I always think of `main` as a function, even though it's an expression: main :: IO () The only confusion is when comparing to C. My understanding of a C function, say `int f()`, that takes no argument is that it's a *unary* function that takes a unit argument. In Haskell it would be written as: f :: () -&gt; Int That is, if it were pure -- otherwise it would be more like: f :: () -&gt; IO Int 
Well... I'm definitely aware of it. I read the paper and even experimented with using it a few years back. But I never *really* understood it, and what understanding I had has only worn away further with time. (Kind of like my German.) More deeply understanding all this stuff (including `lens`) is still on my TODO list. Attribute grammars I've only heard about. :)
There are absolutely enough interesting libraries, but unfortunately this year my job demands more of my time, along with my uni degree actually requiring me to put real time aside. Also, I left it a bit too late to learn 24 new libraries now - that's probably the key reason :) It's a shame, but think of this as removing the excuses for 2015 ;)
Again you can do that, but you'll lose some flexibility. Summing an `Int` results in an `Int64` so it really is nice to let the field types vary freely.
CAFs are not only about the top level. CAF is more of an operational term, the corresponding semantic term would be *closed expression*. Any closed expression can be a CAF; it depends on the implementation if it treats it as such. I'm not sure exactly what you mean by a "fully-applied" function. Is `id id` fully applied? If we have the definition `f x = \ y -&gt; ..` is `f 10` fully applied? Many Haskell implementations have an operational (not semantic!) notion of how many arguments a function needs before a reduction can happen. This is sometimes called the function *arity*, and an application with at least that many arguments can be called *saturated*. But I'm not convinced these are terms that should be used in an introductory text. BTW, I liked your post in general.
Actually I think the malformed double aggregation there is a bug in the query optimizer, not in the API, as reported here https://github.com/m4dc4p/haskelldb/issues/15 If you run with `showSqlUnOpt` you should get correct SQL.
For many purposes, `RankNTypes` let us "construct types at runtimes" (yea, there's a lot of handwaving there) - because code downstream has to work *for all* types subject to some constraints. I wonder if acowley has looked into that at all.
Cool, thanks!
I was very excited about that extension, but in practice I've sometimes found it lacking. I'd much rather have the possibility to overload `nil` and `cons`, so it would (hopefully) be possible to write instances for e.g. length indexed vectors.
`TypeRep`'s can be serialized and compared across separate runs of the program, or even different programs; this cannot.
I think the optimizer used to be fine but if you go through the commit log you can see huge unwieldy pieces were grafted on without due care.
Thank you very much, your and u/neelk's explanation helped me a lot! Now I have something new to dig into: generalized recursion schemes. :)
Thanks for your explanation! Do you happen to have an example for a recursive function which is easy on a whiteboard but hard to implement?
By the way, do you know if anyone really used HaskellDB apart from yourself?
*Now* it feels like Christmas! 
Is there anyone else looking at using Opaleye with MySQL? My impression is that nested queries can be somewhat problematic for MySQL and might require some work on an optimizer... Although /u/tomejaguar &gt; There's certainly a lot of scope for readability improvements in the code generator. However, there are probably now fewer nested queries when you last looked. I replaced (most of) HaskellDB's code generator with my own and joins are no longer pairwise but can contain any number of tables. On the other hand some parts of the generated code got harder to read, but I'll work on that as and when necessary. ...this sounds promising. Do you have any gauge on how far off MySQL support might be? 
&gt; Partial Type Signatures has been merged into HEAD. I completely missed [the proposal](https://ghc.haskell.org/trac/ghc/wiki/PartialTypeSignatures). This is great!
MySQL and PostgreSQL are sufficiently different that it is hard to improve support for one without compromising support for the other. Opaleye's aim is to support Postgres as well as possible. If it works with MySQL too that is a coincidence. That said, the new code generator generates less deeply nested queries than the old HaskellDB one. 
I would love to port our product to PostgreSQL, but I'm quaking in my boots. I'm hopeful that if Opaleye can handle 90% of our queries in MySQL reasonably, then at least there's an upgrade path for us... I will need to experiment and see though, thank you for the help!
Interesting. Let me know of any difficulties you run into with MySQL and I'll see what I can do to help.
Type providers were designed for exactly this situation.
That's very kind, thank you. Once we have some more resources at our disposal we may also try to contribute!
Very good, thank you. 
I have heard about them in the context of F#. Do you have any links to good readings - and any discussions about getting them in Haskell? (Or do we already have them, under some weird mathematical term? :))
You can see how hscolour highlights by viewing source on any Hackage listing (or presumably actually looking at the hscolour source code of course). Here's the minimal CSS file that Hackage currently uses (most elements are ignored): .hs-keyglyph, .hs-layout {color: red;} .hs-keyword {color: blue;} .hs-comment, .hs-comment a {color: green;} .hs-str, .hs-chr {color: teal;} .hs-keyword, .hs-conid, .hs-varid, .hs-conop, .hs-varop, .hs-num, .hs-cpp, .hs-sel, .hs-definition {} Sometimes I found Pygments to be more "precise" in how many distinctions it made, sometimes hscolour, which makes the match imperfect. 
I think you are right.
So far, measuring from userstyles.org downloads, monokai is by far the most popular one :)
The config from ~/.nixpkgs/config.nix is always applied (unless you'd bypass it explicitly)