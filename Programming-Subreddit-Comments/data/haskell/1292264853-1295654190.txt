I realize this is a prehestoric (by internet-age standards) discussion, but I took an interest. It seems that the original author of the blog post is an imperative purist. All though I generalize, the pure imperative view of programming has merits, I believe. Just look at which languages are used when computation time and memory usage is at a premium. The more popular view (on this subreddit anyhow) is of course that of the ever abstracting, theoretic programmer, who utilizes state of the art high-order algorithms to do solve large amounts of problems at once. IMHO this is in order when developement time is at the premium. I see another aspect in this as well, in that programming paradigms that are based on mathematic algorithms and higher order abstractions can remain relevant when the hardware paradigm changes (how likely this is, is another discussion). I'm going to stretch out on my longest limb and ask the question: How will we instruct (or program) "computers" that use memristors instead of transistors for computation?
+1 for smaller vocabulary
&gt; Because they in many cases date back to 2004, before we had a sufficiently smart GHC (or any libraries) to do much of the work. I just checked which year the fastest x64 current Haskell programs were contributed - 2007 : 1 2008 : 3 2009 : 2 2010 : 7 http://shootout.alioth.debian.org/help.php#contribute
That's awesome! Some of the programs are revisions of older programs (to add, e.g. parallelism). My *recollection* is that they go back to earlier programs that I wrote. Clearly many have been rewritten from scratch, which is wonderful.
You're right about Yesod.Json: it's a good target for removal. But did you actually *read* my blog post: a framework **is** a kitchen sink. If you don't like that, then you're probably looking for a library, not a framework.
Out of curiosity, what's the difference between this and patchtag?
Recently linked in [this thread on -cafe](http://haskell.1045720.n5.nabble.com/GHC-7-0-1-developer-challenges-td3279406.html#a3279888)
Relatively pretty web designâ€¦?
Some do go back to programs you or others wrote in 2008 and 2006, but how much original code remains is more archaeology than I'm going to do. Does the community understand how to get the best out of the *sufficiently smart GHC and libraries* or are the old ways of doing things still prevalent ? 
I don't think I'm stupid, but I am ignorant. How do knowledgeable people do it?
I don't know too. But i never done Web Development, what is the right(secure) way to do it ?
- until recently, [patch-tag's software](http://patch-tag.com/r/tphyahoo/patch-tag-public/home) was closed-source (but no longer) - [darcsden's](http://darcsden.com/alex/darcsden) is easier to deploy - it's cabalised, it has less dependencies on shell commands and file permissions, runs on a mac, etc. - darcsden uses two exotic databases (redis and couchdb), patch-tag uses one afaik (happstack-state, or the older happs version) - darcsden uses snap, patch-tag uses happstack (or happs) - [darcsden.com](http://darcsden.com) hosts Alex's own repos (and he might move those) and is a hobby project; patch-tag.com hosts quite a large number of [public repos](http://patch-tag.com/publicrepositories), presumably some of them quite active, and is (was ?) a serious attempt at a freemium project hosting service - Both are somewhat stalled, as Thomas has a day job and Alex is hacking on [new projects](http://atomo-lang.org/). 
I like the directory/subdirectory breadcrumb display at the top of the page. Very geeky in a Haskeller way. For example, [the darcsden source](http://darcsden.com/alex/darcsden) The grammar nazi inside of me doesn't like the way his sentences begin with lowercase, though. 
Hah, glad someone else likes my barebones minimalism. ;)
I actually *do* like it.
I wouldn't automatically assume that ever usage of CGI apps is "stupid." In low resource environments, for applications that do not receive many hits, it might make a lot of sense. Or when the very majority of your content is complete static and you have an occasionally run script to make some modifications. That said, for most use cases, either FastCGI or reverse HTTP proxies are probably a better choice. **Edit**: I can't believe I can make so many typos is such a short comment. That'll teach me to reddit at midnight. Typos kept for posterity ;).
Orthography nazi?
[Their paper is a good intro](http://conferences.sigcomm.org/co-next/2009/papers/Jacobson.pdf). Pretty compelling IMO.
I'm still new at this enumerator stuff to say the least, but it seems to me the basic problem is that while you may have bashed them into some sort of compliance, they are fundamentally the wrong abstraction here. You don't have something that produces a series of values of a certain type, you have a _protocol_. In the writing out case, you want some values of this type (response headers), an ability to respond to those values in the context of the iteratee/enumerator/enumeratee itself (part of being new is that I can't remember which of those three things is the "producer", "consumer", and "pipe", which _ahem_ I think would have been better names but that's a gripe for another day), and then values of some other type (the text, or whatever else is going out, which becomes really exciting if what is going out is a WebSocket). You're never going to satisfactorily wrap that into a single enumerator. (Because even if you union the two types (data HTTPThingy = Header Bytestring Bytestring | Value Bytestring) you want to express that first you get one thing, then there's a transition where you get the others and there is no other mixing in the types.) (A possibly-related problem is that ever-increasingly, HTTP is _not_ a request/response protocol; it's an interactive stream protocol with an unfortunate history of being _mostly_ request/response and fooling people. Am I reading WAI correctly that it is response/request only?)
Stop being closed-mouthed! Tell us what the "smart" way is, please? :) 
Generally by running a separate server process and using Apache or nginx (nginx, please!) to proxy requests to it (or using fastcgi or scgi in place of a proxy). Conventional CGI spawns a new process for every request, which is just beautifully, beautifully unscalable. Keeping a long-running process with your logic in it and handling individual requests is much more scalable (no overhead from repetitive process creation and destruction).
The old ways look pretty obsolete, imo.
Firstly: iteratee = consumer, enumerator = producer, enumeratee = pipe in between them. Well, you *did* hit the nail on the head with the point of this post: enumerators were not designed with having "metadata" included, and I need metadata included. The point is, both of these packages use the same metadata, and the same process (a function that takes the metadata and returns an iteratee) works for both of these packages. It may not be the best solution, but it's a working solution, and maybe over time we can figure out better approaches. As far as HTTP not being a request/response protocol... you're talking about long polling and the like? None of that really stops the protocol itself from being a stateless request/response approach, it just means you can do funny things on top of that. Admittedly, I have not been working much in that realm, but there is [a sample Ajax chat app](http://braincrater.wordpress.com/2010/07/22/ajax-chat-app-using-yesod/) tutorial.
Long polling and the like stretched the request/response model... really, _really_ stretched the model... but if you squinted it still fit. WebSockets breaks it. An otherwise normal HTTP request can suddenly become an almost-but-not-quite TCP socket. Now we're talking stream. In most "normal" languages this would be no particular big deal, but in Haskell, when you write a library with types that rigidly forbid that it ends up being an important problem. I certainly didn't mean drop everything and do it this way :). I was more thinking out loud, and this was really an intersection of two trains of thought I've been having: One is the one I outlined, which is that increasingly HTTP just isn't request/response any more, and maybe we should actually be exposing that usefully to users, instead of trying to give users an interface that assumes it, then (usually somewhat clumsily) layering on pipelining and other such things behind their backs. (One of the neat things about combinator approaches in my opinion is that it enables you to do things "in front" of the user; you can still provide something that gloriously caches or whatever and the user can invoke it with a word or two, meaning they can still fiddle with it if they want to. One of the things that has frustrated me in the past with web frameworks is exactly my need to go mucking about in the internals every time I have an ever-so-slightly different idea about how to do caching.) The other is that protocol types and iterators would seem intuitively to me to have a natural fit together. Intuitively in Python I could just smash two generators together and call it a day, in Haskell I have to work out how to write the type for it and it is not immediately obvious to me how to do that in a cost/benefit beneficial way. But I thought given your interest in type-safety it might be an interesting thought.
Thanks for the answer, It makes sense now @.@. ps: I liked the "beautifully... beautifully unscalable" xD.
I don't now anything so I'm happy to take your word for that, but what I wondered was whether *the Haskell community* were still using those obsolete ways (whatever they might be) or whether *the Haskell community* had transitioned to 7.0.1 And separately whether those Haskell benchmarks game programs were in fact representative of those obsolete old ways, and need to be replaced by programs representative of the *sufficiently smart GHC* ways ? I don't know - I only know when the programs were contributed.
GHC 7 is out, it's even less of a non-issue now as compiling with the ability to use RTS options is not the default any more
Well, I agree that WebSockets are not request/response, but they're also not in any way HTTP. I've considered trying to support WebSockets in WAI, but it just doesn't make sense to have a single package for two completely separate protocols. I think WebSocket support would be a good thing, but 1) as a separate package and 2) after things have matured a bit more. Didn't I read recently that Firefox and Opera are dropping support for WebSockets because of security concerns? I'm not sure what you mean by "smash two generators together." One of the nice things about enumerators if that they *can* be composed, eg: enumFile "somefile" $$ enumList 1 [someByteStrings] $$ myIter What's the use case you're hinting at here?
I think very few people are actually compiling their code with GHC 7 though. A good heads-up for anyone using CGI.
Two generators that generate two different things; one that yields response headers, one that yields the results. Enumerators can be composed, but you can't just smash together one that emits "headers" and one that emits "bytestrings-as-HTML", at least as far as I can tell, you'd have to actually sequence them together into some sort of protocol. Which does not seem impossible by any means. (Again, just as a tone note, I'm just thinking out loud, no criticism is intended.) Unfortunately, WebSockets are indeed HTTP, at least at the beginning. They initiate via a certain HTTP request that tells the server the client is trying to initiate the protocol, so the WebSocket request will come in on the same channel as any other HTTP request and must be handled by the HTTP server. Now of course once the HTTP server determines it is dealing with a WebSocket request it can hand off to a completely separate handler and that may indeed entirely bypass WAI, but the server will have to deal with it at some point. But the server and WAI are being two different things here. I just find myself wondering if that separation is actually desirable. (It may be; if I had a firm opinion I'd be expressing it. :) ) If you look over the [full standard](http://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-03#section-1.3) you can see there's quite a bit of HTTP-like interaction during the negotation and there will be more before it's all said and done (compression support, for instance, which is so similar that we really ought to be able to reuse compression support from HTTP requests....).
TIL that a WebSocket session starts off with an HTTP handshake. Thank you ;). Nonetheless, I still think it's too early to worry about websockets. Let's put it this way: we haven't as a community come to an agreement on a protocol for standard HTTP which has been around for a while now, [a fortiori](http://en.wikipedia.org/wiki/A_fortiori_argument) we won't have much success making a standard on something which itself is not yet a standard. However, it could be cool to write a server that could support both WAI and web sockets, though I don't have time to fiddle around with the WebSocket protocol right now.
It is an article which has not gotten better with age, and it wasn't any good to start with.
&gt; Let's put it this way: we haven't as a community come to an agreement on a protocol for standard HTTP which has been around for a while now, I agree, certainly. I find it useful to know where I'm going when I'm designing things, so I wanted to point that out. (I may be unusual in that respect, no joke, I've noticed my personal design methodology is very unusual.)
It seems this is the new home for the ghcmutterings blog - can it be added to the Planet? Or the old RSS redirected? Or something.
It took me two readings, but I think I understand most of that juicy tidbit. One clarifying question: So the stack for each thread is composed of a linked list of chunks to avoid copying under enlargement and enable finer resolution GC write-barriers? I'd love to see a blog post (or series of) that gives a starting point at understanding the RTS. It is such a marvelous piece of software, but so often I just twiddle the knobs and hope that the magic smoke goes faster. (as an example: Ezyang did something like this with his GHC hacking post)
What stack are they talking about? I thought that the GHC runtime system used a spaghetti stack where each activation record is heap allocated and managed by the garbage collector, and that a stack local to the OS thread existed only for the purpose of calling out to FFI functions.
I am curious about haskell at his employer
&gt; Keeping a long-running process with your logic in it and handling individual requests is much more scalable (no overhead from repetitive process creation and destruction). This also the exact idea of FastCGI (a different "protocol" than CGI).
I think we already requested that it be added to the planet, but I've asked again anyway.
Me: *Head explodes*
&gt; So the stack for each thread is composed of a linked list of chunks to avoid copying under enlargement and enable finer resolution GC write-barriers? That's basically it, yes. &gt; I'd love to see a blog post (or series of) that gives a starting point at understanding the RTS. I'd love to do that... in the meantime, there's the [Runtime System Commentary](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts)
Unlike in typical ML implementations, we don't use heap-allocated activations in GHC, we have an explicit stack instead. This choice goes back to the STG machine (and before that the G-machine). I think it would be interesting to try doing CPS instead, now that we have a good generational GC to collect all those garbage stack frames. However, the stack is so deeply wired into GHC's internals, it would be a mammoth task to change it. 
I'm also at Suite Solutions. While Haskell is not the main focus of our company, we now have several significant Haskell projects going on for large enterprise customers. Our focus is XML-based documentation systems. Sorry that I can't give any more details here at this time. Now that the Haskell ecosystem is reaching maturity and there is a healthy supply of top developers wanting to work in Haskell, Haskell is beginning to move into the mainstream. 
MPI-3 does approach the issue of fault tolerance, but it's not finalized yet. I'm actually hoping to do some research work with this standard in the not too far off future, and I'm in the "keep my fingers crossed" stages of that :)
I just [updated the post](http://hackage.haskell.org/trac/ghc/blog/stack-chunks#Update:somemoreresults) with some more results. In particular, BinaryTrees from the shootout now goes more than 3 times faster than before.
I noticed this in your last article and figured it was just a typo, but it's in this one also...there's no 'a' in my name ;) Seeing some native parsers built is exciting. Bindings to C libraries like libxml or expat work well enough, but they're not nearly as flexible as native code. Besides, as Michael mentions, they can be annoying to get working in Windows.
Doh! I think every time I've typed your name I've checked if it was 1 or 2 "L"s, and somehow each time I added in an "A". ::face palm::
Puns do that to me, too.
Hey hey, I'm really proud of this pun :-)
&gt; I'd love to do that... in the meantime, there's the Runtime System Commentary Excellent! This is a great starting resource.
And if someone gives a sublinear version based on the recurrence F(n)\*F(m) + F(n+1)\*F(m+1) = F(n+m+1)?
Then I'd give them smartypants points and ask a real question :-) I did consider putting a comment to that effect in the original post, maybe I'll add something to mention that later on!
Like this? fib = 1 : 1 : zipWith (+) fibProd (tail (tail fibProd)) fibProd = zipWith (*) dupfib (tail dupfib) dupfib = dup fib dup (x:xs) = x:x:dup xs It sure is faster, but isn't it still linear? Edit: when I time it it seems about linear, but that might also be because the numbers get so big.
It appears that way. According to the topic of #haskell on Freenode, "we're on the case." In the meantime, you can still access the site by putting the line 78.46.100.180 haskell.org into your [hosts file](https://secure.wikimedia.org/wikipedia/en/wiki/Hosts_file#Location_in_the_file_system). As usual with host file modifications, you'll also likely have to restart your browser.
You can also write your linear time Haskell `fibs` in one line: fibs = 1:1:zipWith (+) fibs (tail fibs) 
"Sublinear" version on Hackage: [fibonacci-0.1](http://hackage.haskell.org/package/fibonacci-0.1.0.0) edit: link to specific version
It works for me!
Your nameserver is still caching the resolved IP.
Ok, it is sublinear. Tracing the additions gives f.e.: *Main Debug.Trace&gt; fib !! 1000 2 3 8 13 5 21 377 610 34 987 832040 1346269 2504730781961 1597 2178309 4052739537881 6557470319842 36726740705505779255899443 59425114757512643212875125 4880197746793002076754294951020699004973287771475874 3524578 10610209857723 96151855463018422468774568 7896325826131730509282738943634332893686268675876375 12776523572924732586037033894655031898659556447352249 139423224561697880139724382870407283950070256587697307264108962948325571622863290691557658876222521294125 225591516161936330872512695036072072046011324913758190588638866418474627738686883405015987052796968498626 70330367711422815821835254877183549770181269836358732742604905087154537118196933579742249494562611733487750449241765991088186363265450223647106012053374121273867339111198139373125598767690091902245245323403501 
how about hackage.haskell.org?
 69.30.63.204 hackage.haskell.org 72.249.126.23 community.haskell.org code.haskell.org 
I did a similar exploration of Fibonacci implementations, but in common lisp. I tried the following techniques (increasing order of sophistication): * Naive recursive (exponential complexity) * Dynamic Programming Using accumulators (linear complexity) * Binet/De Moivre formula involving sqrt 5 (has numerical issues) * Reformulate fibonacci problem as a repeated matrix squaring problem (initially slower than the linear version) * Use O(log(n)) squarings of matrix instead of O(n) squarings (FAST!) * Use an object pool to do destructive operations without creating new objects to minimize GC time (Significantly more complicated to implement, but not that much faster). Here's the final timings table that I obtained on my machine (the naive version is too slow for anything after about 40, so it is not included in this table): n fib-linear fib-logarithmic fib-logarithmic2 10 0 0 0 100 0 0 0 1000 0 0 0 5000 0.002 0 0.001 10000 0.009 0.001 0.001 25000 0.057 0.003 0.003 50000 0.189 0.013 0.009 100000 0.693 0.035 0.040 500000 15.882 0.577 0.568 1000000 63.572 2.100 2.096 2000000 8.252 8.260 2500000 35.414 35.382 5000000 135.776 135.640 Note: logarithmic2 is the one that uses object pool Note: time in seconds Edit: I should add that my goal was to find the N^th fibonacci for a large N, which allows me to use squarings etc, and is a different problem than finding the first N fibonacci numbers. 
Do you know how bad the cost of multiplication going to be for larger Ns? 
Running length . show $ fib !! 5000000 in ghci using the above 4 lines takes 5.21 secs on my machine. (The number of digits is 1044939.)
Hmm did i mess up somehow? I'll take a look at it later today.
haskell.org works for me, but hackage.haskell.org fails.
&gt; We begin the Fibonacci sequence with one, not zero: The wrongness! It burns my eyes!
That calculates all the numbers in between 1 and n. To calculate fib(n) you only need to get fib(n/2) and fib(n/2-1), and to calculate those, you only need 3 numbers around fib(n/4), and for that only 3 numbers around fib(n/8), etc.
No, lazyness prevents the calculation of all numbers. See the trace, it is doing exactly as you say.
You're right. Not all the intermediate numbers get calculated. I'm flat out wrong there. But: the number of cells traversed in the lists does, which is still linear... Fairly small constant multiplier on this overhead, and everything is nicely memoized, but when access to the memo table is linear, the overall runtime can't be less than linear.
There is always a person responsible for every domain. 
Depends on the underlying library. Multiplying two d-digit numbers can be done in O(d log d) though based on multiplication essentially being convolution, and the FFT to do the convolution.
The monad-parallel(http://hackage.haskell.org/package/monad-parallel) library can do it. Furthermore, it provides the same interface for IO and several other monads. Monad.Parallel.liftM2 (,) updateIssueTracker testPatchBuild would give you a pair of results IO (Either String URL, Either String Bool) 
The following page may help you. http://hpaste.org/42369/haskellorg_ips
The domain name was seized by Network Solutions (it wasn't due to expire until this time next year). The confusion seems to be that while Yale was the nominated owner, it was administered by Galois. We've contacted Network Solutions and resolve their confusion.
That's comforting. Worst nightmare: domain expired and was quickly bought by some domain farmer; Haskell community has to buy it back from them.
Somewhat relevant: http://www.willamette.edu/~fruehr/haskell/evolution.html
Just wanted to publish a `0.2` branch with `fib 0 = 0`. But the code burnt my eyes! Looked very unnatural so I kept `0.2` for myself.. **Beauty contest** Send me patches. The most elegant will make it into `0.2` if it doesn't look like a quick hack to fix a boundary case!
 upperRight :: Matrix a -&gt; a upperRight (Matrix _ a _) = a fib :: (Integral int, Num num) =&gt; int -&gt; num fib = upperRight . matrixPower (Matrix 1 1 0) 
I think the iterative approach I'd have used would be something with a helper "generator" function that saves some state in a non-global scope... like this. Kind of like writing a C++ style "functor" (function object), but without the C++-ness. int fibhelp () { static int cur = 2; static int last = 1; int rv = cur; cur += last; last = rv; return rv; } int fib (int n) { if (n &lt;= 2) { return 1; } int i; for (i = 3; i &lt; n; ++i) { fibhelp(); } return fibhelp(); } or something. I haven't run this yet, clearly. And it's just one shot :-)
Curses! Foiled once again by mutability!
Looks definitely better than my quick hack! You are the odds-on favourite now.
Might as well add support for negative indices while you are at it.
I added the `r &lt; 0` case to matrixPower :: (Integral int, Num num) =&gt; Matrix num -&gt; int -&gt; Matrix num matrixPower _ 0 = Matrix 1 0 1 matrixPower m n | r == 0 = square $ matrixPower m q | r &lt; 0 = neg . times m . square $ matrixPower m q | otherwise = times m . square $ matrixPower m q where (q,r) = quotRem n 2 (where `neg` calls `negate` on all entries) With the `upperRight` version the signs are all wrong. With the `upperLeft` version they are exactly right. A case for `fib 0 = 1` ?
You don't want to negate your matrix, you need to invert it. In general it will be an expensive computation. It would faster if you support it directly in fib by defining `fib n = (-1)^(n+1) * fib (-n)` for negative n, or some similar. For what it's worth, the inverse of Matrix is inverse (Matrix a b c) = Matrix (c/d) (-b/d) (a/d) where d = a*c - b^2
Right, inverting makes more sense (sadly doesn't work in `Num`). Maybe fixing `fib` is better. Then I might as well decrement the argument and compute `upperLeft` which was my initial solution.
I think upper right is more natural, but as long as you have fib 0 = 0 I'm happy. :) edit: because (Matrix 1 1 0)^n = Matrix (fib (n+1)) (fib n) (fib (n-1))
In a strange sort of way, it's a point of pride that so many noticed the site went down. I was trying to visit Haskell for referencing some docs this AM and noticed it. What a great community!
I totally do this.
Take a look at hoc, might give some inspiration. http://hoc.sourceforge.net/ The computer sciency interpretation is that you have a whole bunch of functions that you assert are correct. that's pretty much it. If you want to write proofs about your program the ffi functions are like axioms. 
fibs = 1 : zipWith (+) (0:fibs) fibs If you want to start at 0 as the Fibonacci numbers really should fibs = 0 : zipWith (+) (1:fibs) fibs
I really like the general look of it. Why do I get extra scrollbars on every file that I want to look at, though? There's still plenty of room on my netbook screen!
It seems that everything is working again!
I think "Network Problems" would be a more appropriate company name...
Have a look at [Blame and Contracts](http://homepages.inf.ed.ac.uk/wadler/topics/blame.html).
'Blame and contracts' are within the same language though. The original poster is wanting a mapping between a static and a dynamic language.
Are they horizontal scrollbars or vertical?
It seems relevant. The direct binding probably has to refer to untyped objects (PyObject, etc), and check types when converting those into typed values. If you only have to convert return types from first-order functions it's not so hard. If you want to allow higher-order functions and exporting typed functions for use from the untyped language, then that paper is a good guide to what properties you want to guarantee and how to get them.
Not researchy, but real -- hubris, the haskell-ruby bridge: https://github.com/mwotton/Hubris On the other hand, I think it only goes the wrong way -- haskell from ruby. There's also missingpy, which goes the right way: http://hackage.haskell.org/package/MissingPy and also cpython: http://john-millikin.com/software/bindings/cpython/ but objective-c has more static features than ruby/python so you could also look at hs-dotnet: http://haskell.forkio.com/dotnet/ All of these I think are more "pragmatic than principled" to various degrees, but should provide an idea of how similar issues have been dealt with in the past.
Very related research: [Fine Grained Interoperability through Mirrors and Contracts](http://www.cs.utah.edu/~kathyg/interop.pdf). Kathryn E Gray, Robert Bruce Findler, and Matthew Flatt. OOPSLA 2005.
http://www.cs.tufts.edu/~nr/pubs/embed-abstract.html Embedding an Interpreted Language Using Higher-Order Functions and Types
By the way, Conor makes the point that the simple higher-order representation of terms "is not HOAS. It is an untyped approximation to a Kripke-model construction, which morally goes by structural recursion on types." Personally, I'd rather not say I use "an untyped approximation to a Kripke-model construction" every time I describe implementing something similar. Any suggestions for a good name for this common construct? I tend to call it "Fake HOAS" or "Lame HOAS" or "Cheaty HOAS" but any better ideas are welcome. Higher Order Simple Syntax (HOSS)? Additionally, I'd like a name for the general trick of turning higher-order descriptions into some other representation (de bruijin or otherwise) via typeclasses. Perhaps Temporary Higher Order Simple Syntax (THOSS)?
I think it's a bit far afield from what the poster wants, but its a really awesome paper! I love things like it which don't necessarily show "new" results, but codify and explain a principled implementation of a great folklore technique. Newer version by the way: http://www.cs.tufts.edu/~nr/pubs/embedj-abstract.html
I can barely understand the Agda code... anyone care to explain in a bit more detail what is going on? Also it seems we are still quite far from simple dependent typed programming in Haskell, given the hoops you have to jump through to reproduce 10 lines of Agda code.
I don't think there is agda here? It seems to me to be all Haskell with Conor's she extension/preprocessor -- which is why it looks so goofy: http://personal.cis.strath.ac.uk/~conor/pub/she/
He links to some: http://personal.cis.strath.ac.uk/~conor/fooling/Jigger.agda
Oh, that's very pretty in comparison. Thanks.
Higher Order Semantics?
My Agda code didn't backport to SHE-Haskell. The reason it failed might be interesting, including stories of Haskell programs which don't check at their inferred types, for peculiar but understandable reasons. I'm grateful to Saizan for figuring out how to beat this into GHC, with overlapping instances allowing pattern matching with repeated variables to do an equality test, like my old man did in 1970. Haskellers! That this voodoo is possible is cause for celebration. That this voodoo is voodoo is just embarrassing.
I have uploaded [fibonacci-0.2](http://hackage.haskell.org/package/fibonacci-0.2.0.1) which adds correct handling of negative arguments and changes the implementation to satisfy fib 0 = 0. Thanks for your input which convinced me that the [new implementation](http://hackage.haskell.org/packages/archive/fibonacci/0.2.0.1/doc/html/src/Data-Numbers-Fibonacci.html#fib) is more natural.
Don't know if [HsLua](http://www.haskell.org/haskellwiki/HsLua) qualifies, but it may be worth a look.
Horizontal, and also on a larger screen when there are longer lines as in [here](http://darcsden.com/alex/darcsden/browse/DarcsDen/Handler/Repository.hs). The scrollbar is right below the last line of code, in Firefox 3.6.13.
I'm sure you are aware of the Data.Typeable and Data.Dynamic libraries, but I wasn't until fairly recently so thought I'd give them a mention.
You should also be proud of all the awesome blog posts you write ;-)
[That data](http://chart.apis.google.com/chart?chs=440x220&amp;cht=lxy&amp;chco=3072F3&amp;chds=0,135.776,0,5000000&amp;chd=t:0,0,0,0,0.001,0.003,0.013,0.035,0.577,2.1,8.252,35.414,135.776|10,100,1000,5000,10000,25000,50000,100000,500000,1000000,2000000,2500000,5000000&amp;chdl=time&amp;chdlp=b&amp;chls=2,4,1&amp;chma=5,5,5,25) doesn't really look logarithmic.
Absolutely awesome. I have wanted this for a long time. All we need now is a blend between this and packdeps so that you can test compiling your package against all of its possible dependancy permutations so that you can be 100% sure that your dependancies are correct.
This is just great. I have been longing for this for quite some time now. Finally eliminates one of the few remaining major problems with serious (i.e. professional) Haskell development (hassle-free multi project environments).
Ah, `MonadSnap`, yes please.
The move to enumerator is very exciting, as well. It's far simpler, and there seems to be a lot more activity with other pieces built on top of the enumerator package. I don't understand the new extension stuff yet; looking into that now.
I'm also very glad that they are switching over to enumerator. It didn't make it into the release notes, but I believe Snap is also making more heavy use of blaze-builder, and in particular Simon Meier's new blaze-builder-enumerator package, which is **very** cool. **Edit**: After looking at the dependency lists, I think I may be wrong about the blaze-builder-enumerator stuff. Any word from the Snap team on why they didn't go this route?
In-function "static" is really a global variable (even if it's only accessible non-globally). There really isn't any good reason for it too, this state could be put in a structure and just given to fibhelp.
What's up with `Simon`s doing awesome Haskell stuff? :-)
I consider difficulty of Haskell programming to be from 1 to 10, with 10 being your name is Simon.
MonadSnap is the reason I've been using Snap from github instead of hackage recently. It makes a huge difference. Good work Snap team!
I don't think "extensions" is a really good name for what it does. It makes it sound like some kind of general extension framework. All it does is that it allows you to add data to the state part of the snap monad in an organized way. So it's only for stateful extensions.
I hope next versions will bring cool stuff like in Ocsigen, Seaside or Arc (continuations/callbacks/whatever-you-call-it, for example).
That's in the works for 0.4.
Don't hold your breath on that :). You're welcome to build it yourself though!!
OK, cool. You guys got 0.3 out faster than I'd expected, this release caught me by surprise. But considering the number of commits you've been putting in the past week, I suppose it should have been expected ;). Congratulations.
haven't peeked into the sources yet, but can tell me someone the differences between cabal-dev, [capri](http://www.haskell.org/haskellwiki/Capri) and [multighc](https://github.com/spl/multi-ghc)? Does capri or cabal-dev have support for multiple ghc's?
*An* Haskell???
Indeed, I did build it myself, and am looking at modifying it to use the new extension system and packaging with Cabal and Hackage. My code is originally based on some stuff Chris Eidhof did in Happstack a long time ago, but I've ported it to Snap, and the extensions system looks like a good fit to make it more easily usable inside an existing app. The problem with continuation-based stuff in Haskell, as in most other languages, is that arbitrary code can't be serialized out to any kind of external store, meaning that you have to use in-memory sessions, and some kind of sticky session routing if you want to load-balance.
If you're only talking about MonadSnap and SnapExtend, then you're right. But in addition "extensions" also defines a loose pattern for how extensions should be laid out, initializers for them, etc. The name makes more sense when you look at it from that angle.
An 'askell just like your dear mamar used to make, cheap at twice the price, and I'll throw in this free burrito, guv'nor.
I love the Haskell subreddit. I'm not a brilliant programmer, but I've been doing it for a while and am at least adequate in most situations. Links from this subreddit always completely baffle me, and I love it. So much still to learn. 
Also: we're not even close to finished with the design and you can expect it to change pretty radically in the coming months.
I've been working with Haskell for the past 7 years (on and off) and it's still like that for me. I love that. I don't think it will change anytime soon, if ever. 
Capri was meant to be a very simple wrapper, so no multiple GHC versions support by itself. You may however manipulate with your OS environment by setting PATH pointing to the executable of GHC version you need.
Sure I could, but I didn't really feel like doing it that way :-). If I was writing this in some serious context, like actually compiling it and running it, or giving it to my employer, I'd have probably done it more like you've said. I like the idea of hiding the context of the computation though, or at least keeping it opaque somehow. I find that this makes it a lot easier to write code that's easily reasoned about, in terms of what code can affect which state and I've also found that such encapsulation often helps with long term maintenance of code bases, vs leaving tons of globally updatable state lying around unprotected. 
And I thought that complaining about the grammar in the title would yield no reward.
do the rest of the gnutils and I'll be impressed 
Don't tempt me. ;) I have already started hpatch; writing up the parser now.
I've got [tsort](https://github.com/effigies/random/blob/master/haskell/tsort.hs) covered.
Relevant: http://www.haskell.org/haskellwiki/Simple_unix_tools
I had already seen that and decided to pick something that had not been done. Pretty cool though.
When the question of an `awk` interpreter arose on the cafe, something like the motive `robertmassaioli` expressed came up: " I'd love to have portable pure haskell implementations of the traditional unix tools. If it were done well, it would allow you to 'cabal install' yourself into a usable dev environment on windows :) I'd much rather do that than deal with cygwin/mingw." http://www.haskell.org/pipermail/haskell-cafe/2010-August/082375.html It seems like it would be a pleasant program for the community as a whole -- I can't gauge how useful it would be. The reddit tempest-a-tea-pot over [whether you could write `du` in Haskell](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/) arose around the same time. Some of the preliminary implementations were quite beautiful. 
That is precisely my reasoning. I would love to just be able to cabal install myself into a working dev environment. It would work on every OS and because of that everyone would be inclined to contribute back to it. Not only that but this implementation of tee shows that it is not too difficult at all to remake these simple 'small' (if you will) dev tools in Haskell. Not only that but they will probably reach speeds close to their C equivalents and if an opportunity arises to parallelise them then it will be much simpler in Haskell.
Most of the program seems considered with handling command-line arguments. Initially, I missed the actual algorithm (8-lines) which is hidden in the local declarations of the function `handleFlags`. Have you considered using the [cmdargs package](http://hackage.haskell.org/package/cmdargs) to remove some boilerplate?
you can drop a few lines using `filterM`: validFilePaths = filterM (doesDirectoryExist . dropFileName)
http://hackage.haskell.org/platform/ Linked off haskell.org
What link are you talking about? Maybe I can get someone at Galois to fix it if you tell me what link you're using. I googled but I didn't find any Galois link to the Haskell Platform.
I have many of them implemented [here](http://www.patch-tag.com/r/merehap/HusyHox/snapshot/current/content/pretty/src). Contributions welcome from OP and others. The text utils are the furthest along. I've slowed down a lot since starting school again. Edit: Note that these are much fuller implementations than those listed on the [Haskell wiki](http://www.haskell.org/haskellwiki/Simple_unix_tools).
It's nice to see this. Would be kind of cool if the next generation of the coreutils were written Haskell. Maybe they'd have output in YAML or some other structured data format... A while ago, I taught a Haskell class and used the core utils as homework. Some rules I followed: * Open files in binary mode. (Maybe this is the default? Don't know much about Windows but this can be a problem.) * Use `ByteString` I/O. * Use an explicit buffer instead of relying on the input having line-endings. These are overly safe, finicky rules; but then again, these are the core utils...
And also by using `stdout` and `getContents`: runTee handles = liftM lines getContents &gt;&gt;= mapM_ (forM_ (stdout:handles) . flip hPutStrLn)
Oh wow. There seems to be a lot of work and effort there. Well done. I think I'll take a closer look at this and it seems you already have a tee implementation done a little differently.
Good advice. Done.
http://lambda.galois.com/hp-tmp/2010.2.0.0/HaskellPlatform-2010.2.0.0-setup.exe
I've abstracted out many of the commonalities between the utilities, and also the command line argument handling. If anything looks a little weird it's because I used some template haskell to get rid of some syntax that was redundant between the the utilities. Currently it is quite likely that your tee implementation has better error handling. Please do merge yours if you consider it worthwhile! Edit: Please note, the type signatures for the functions are generated by template Haskell. I didn't forget them!
I think you meant _return . lines_ instead of _liftM lines_ but otherwise really good suggestion.
indeed. fixed.
I have now crudely adapted your code (with attribution) into my BusyBox/coreutils clone [here](http://www.patch-tag.com/r/merehap/HusyHox/snapshot/current/content/pretty/src/text/tsort.hs). Please tell me if this is contrary to your wishes. HusyHox is BSD licensed.
If you're gonna do several of them, might be a good idea to put them in a single binary eventually, and dispatch on the filename. 
Not at all contrary. I was actually considering playing with your stuff to submit a patch, but that works, too. My code isn't really licensed, and anyone is free to copy, mangle, and generally treat it as respectfully or otherwise as they feel.
I've just now incorporated (very crudely) jmillikin's revised implementation of du from that thread [here](http://www.patch-tag.com/r/merehap/HusyHox/snapshot/current/content/pretty/src/shell/du.hs) into my BusyBox/coreutils clone. I'm asking for his permission to include his code in HusyHox now. It doesn't seem to match the default behaviour of du currently, but hopefully I'll (or even better, someone else will) get a chance to revise it a bit later. &gt; It seems like it would be a pleasant program for the community as a whole -- I can't gauge how useful it would be. BusyBox and the coreutils are currently licensed under the GPL. HusyHox is licensed under BSD. As such it might have some potential down the road as a more business-friendly solution.
I'd love to have you as a co-contributor. There is still many person-hours worth of work to do. An arbitrary amount in fact, since BusyBox is still under active development. For tsort, for example, support for environment variables needs to be implemented. If I recall correctly, tsort sorts in different ways depending on what environment variables are set. Also, in my haste I haven't fully adapted your code to the HusyHox architecture. I put all of your code in the core function, but a little bit of it belongs in the format function. Each coreutil has the following components: input, parse, core, format, output, and command line arguments (argDescs). edit: More details on the [wiki](http://www.patch-tag.com/r/merehap/HusyHox/wiki/). Not sure how all of the text formatting disappeared, sorry if it is an eye sore.
If you are interested, I've architected the [HusyHox coreutils](http://www.patch-tag.com/r/merehap/HusyHox/wiki/) such that the formatter can easily be replaced from the standard flat text formatter to something that creates YAML or XML or JSON instead. It wouldn't be too relevant for the file utils, but it would be for the text and shell utils. I haven't fulfilled your bullet points yet, though once I have a basic implementation for each util, those bullets are next on the line-up. Edit: To see how a YAML formatter fits into the architecture, you might look at the [architecture page](https://patch-tag.com/r/merehap/HusyHox/wiki/Architecture) on the wiki.
Both the Windows and Mac links from that page are currently broken.
try the torrent
I agree encapsulation is important, but the form of encapsulation that you chose isn't a very practical one in almost any circumstances...
Wait. So your solution to the excess diversity in the implementation of standardised tools is to create another implementation of a standardised tool?
Not quite. There is excess diversity in the implementation of tools yes, and I did just go and make another one but I think that to say that I am adding to the problem is missing the point. I want one that can be compiled on every operating system. A shared code base, usable by anyone (commercial or free), that will work anywhere and is clear and is easy to install. It is more about being cross-platform and less buggy (thanks to good Haskell) than it is about reimplementing the wheel.
You mean, only if the file ends in .patch will you run the patch section of the code (or something like that)? Because how would you know in most cases? For example: ls out.patch rm out.patch tee out.patch I am not sure if I understood what you meant?
he means the way busybox works, i.e. "ls" is a link to "hbusybox --ls"
Wait, couldn't you just symbolic-link "ls" to "hbusybox" without any additional parms? Your haskell code then calls "head &lt;$&gt; getArgs" to reveal its command-line invocation. 
You're certainly correct. Actually when I was writing that reply I was going to call this imaginary binary "husyhox" rather than busy box but then decided that sounded a bit silly. Imagine my surprise when I scrolled down the page and found I'm two steps behind.
Similar attempts: [hsh (unrelated to jgoerzen)](http://www.programmingforums.org/thread12827.html) with [source code](http://paste.lisp.org/display/37726#4).
Yes, although a downside would be that Windows users would be screwed, again. 
Thoughts, in bottom-up sequence: (1) any link, even to the text version, would be greatly appreciated (2) why would it be bad form to post on your own blog? After all, it's *you* curating the weekly news not an oversized editorial board in some faceless newscorp. The haskell community knows and trusts dstcruz with the hwn responsibility the way we know and trust spj and smarlow with ghc devel. Friendly, helpful individuals all! (3) Doesn't john goerzen run sequence.complete? Could try pinging him on website issues, say after the holidays. 
Paragraphs, please! Don't be the stereotypical open source project: good utilities, terrible documentation.
Perhaps a good summary is this: "It's about reimplementing the wheel, not reinventing the wheel."
The domain of the server changed: http://lambda.haskell.org/hp-tmp/2010.2.0.0/ The site is fixed now. Thanks.
I think very very few of them are CPU-bound, so parallelizing may not show much benefit...
What is this "excess diversity" of which you speak?
That's what I was commenting on after the link. It used to be split up in paragraphs, then for some reason the formatting disappeared. I can't figure out a way to get it back. Guess I'll try to hack around it. Edit: I've created subsections which seems to work well enough. A wiki without paragraphs is of course ridiculous. I don't know what patch-tag is thinking. One is supposed to be able to force a line break with two spaces at the end of a line, but that doesn't work.
The same could be said of any tool implemented on top of any portable platform, of which there are many.
That's me trying to summarise the OPs opinion as decribed in the `README.markdown` file from the linked project. It is not my belief. It is also possible that I've summarised the paragraph unfairly.
Currently in [HusyHox](https://patch-tag.com/r/merehap/HusyHox/wiki/Architecture) you can do either way. husyhox tee file1 file2 or just tee file1 file2
At the time I linked that, it was working fine for me, and it's working right now too.
husyhox would be a great project to point the "okay, i've read the haskell tutorials and done a few toy programs, now what?" crowd towards.
I agree with this. It makes a good starting point.
I think that the arrow class is a much better abstraction than Unix shell -- it would be more interesting to me to see an arrow wrapper for Unix processes, rather than shell-oriented Haskell tools.
Well I still feel new to Haskell and I have not looked into Arrows despite hearing about them for a while now. I think I might take this opportunity to look into it so that I can better understand what you mean.
An admirable attitude! Arrows include an operation `&gt;&gt;&gt;` which is quite like the Unix pipe operator, `|`. They also have an operator `f *** g` which runs f and g in parallel, collecting the results as a tuple. These operators can be given a variety of semantics (see the `arrows` package on Hackage for a few). I'm just verbalizing a hunch that Unix process semantics can be fit into the Arrow abstraction - `&gt;&gt;&gt;` would be a pipe, `***` would fork two processes and wait for them to finish before collecting the results. Arrows are very much worthy of study, although there's not that much good instructional material about them yet.
cabal-dev doesn't do anything to support more than one ghc -- multiple ghcs can use the same cabal-dev sandbox (because the package databases are at different locations internally to the sandbox) but we haven't done anything to specifically allow multiple ghcs. The machines we used with more than one ghc installed (such as build slaves) generally just set environment variables in a certain way (PATH, tempdirs, etc.) before each build.
`sort` uses the disk, whereas (I'm assuming) this looks like it would run out of memory on a few million lines.
This looks like it could be a great place to use iteratees actually, just have an enumerator that reads in a lazy bytestring from stdin, and and pushes each chunk into an iteratee for each output handle... I might try coding that up myself to try and learn how to use iteratees better. I would like to know how well this handles large inputs, it looks to me like it may hold onto the input string much longer than needed, but I'm not quite sure how well laziness works here. what's the memory usage of taking a large file, and writing it out to say three or four other files?
Some unsolicited advice: Ignore anything that emphasizes a connection between `Arrow` and `Monad`. Relationships exist but they're not that significant and are potentially misleading. The short version is that `Arrow`s are a generalization of *functions* where the composition operation can be more elaborate than just `(.)`. Complex expressions using `Arrow`s are thus built up by connecting smaller expressions in various ways; the module `Control.Arrow` provides a variety of combinators for writing point-free code, or the `proc` notation lets you assign temporary names. As you can imagine, this lends itself very nicely to programs centered on composable data flow. While I'm not sure how useful `Arrow`s would be in *writing* basic Unix utilities, they'd be an excellent interface for *using* them.
I thought as much, outputting to many files results in the code here using large amounts of ram. I reimplemented the program to use iteratees, and it runs using about 2.5MB of RAM. The changes I made were: imports: import qualified Data.Iteratee as I import qualified Data.Iteratee.Base as IB import Data.Iteratee.IO (enumHandle) import qualified Data.ByteString as BS import Control.Monad.IO.Class (liftIO) (not sure if there are all necessary) changed runTee validHandles to runTee (stdout:validHandles) and changed runTee to use iteratees runTee :: [Handle] -&gt; IO () runTee hs = enumHandle 4096 stdin (I.liftI step) &gt;&gt;= I.run where step (I.Chunk bs) = do liftIO $ mapM_ (flip BS.hPut bs) hs IB.icont step Nothing step str = IB.idone () str (which reads in 4096 bytes at a time, not sure if larger or smaller number would be better, I'd guess larger though) I hope that this shows that iteratees don't have to be difficult (to me they're like monads, easier to use than to fully understand), and they have some fantastic performance implications when used properly. It should also be added that the original code fails with non haskell readable text, so doing something like cat /dev/urandom | htee foo bar &gt; baz would result in hGetContents complaining it can't read the data. This new version accepts any bytes as input and writes them out exactly as input.
Good point. A stated goal of Gnu coreutils is ["fewer arbitrary limits"](http://git.savannah.gnu.org/cgit/coreutils.git/tree/README). I wonder if busybox has sort and more generally, what is busybox's philosophy on input limits. 
BusyBox is used in environments that may not even have writable disk available to the user; I suspect it wouldn't touch the disk.
Recipe for a free burrito: 1. Select your desired filling 2. Lay a tortilla out flat 3. Apply 1 unit of filling to tortilla 4. If that looks like enough food, your burrito is complete and you can return to whatever else you were doing 5. Otherwise, roll the tortilla up with filling inside, lay out another tortilla, place the rolled tortilla onto the new one, and continue from step 3 n.b. -- The USDA recommends ensuring that all burritos are well-founded before attempting to consume them.
Really? Which operating system was that because it seems to be just fine with the memory consumption for me on my Mac using only 1.5MB on a 95MB file: http://imgur.com/tQm63 Though your solution looks nice but does it work on a line-by-line basis? It looks like it just copies and pastes every 4096 bytes without considering lines at all which means that it would not behave like the normal tee. Edit: Your solution would be perfect for binary files though. Edit2: And it does not seem to bad on linux either: http://imgur.com/P84WJ.png
Heard today that the [Haskell backend is now live](http://detexify.posterous.com/haskell-backend)...
Upvoted for undeniable usefulness.
It's noticeably faster than the last time I used it, and very useful.
I appreciate that link; it actually proves that it was written in Haskell :)
&gt; 3. Apply 1 unit of monad to tortilla You forgot the secret monadic ingredients. ;-)
Pretty cool but... written in Haskell? That might explain why most of my gammas (even my tablet-written ones) are recognized as lambdas!
...but that would [ruin the joke](http://comonad.com/reader/2008/monads-for-free/). :(
Maybe something like C# dynamic?(which was added to the language in part to make it easier to work with dynamic languages running on the same vm).
To be clear, I didn't make this. The OP of the /r/programming post did. I just reposted it here with the same title because he said he used Haskell to make the images.
See Data.Dynamic.
So train it. Draw some gammas, and tell detexify that they are gammas by clicking the gamma on the right.
For a C++ programmer coming to Haskell and looking for help with rapid prototyping, getting good with ghci would be vastly more productive than shoehorning C and C++ style comments into the syntax.
Agreed, Haskell Syntax and GHC should not try and bend towards other languages just for the sake of it anyway. Also, fwiw, I happen to really like Haskell comments over C++ comments.
I don't look for `//` or `#` or `--` anyway to decide if something is a comment. I look for the color that my text editor / IDE uses for comments (seems like it's usually kind of a faded green).
Likewise, commenting blocks of code is also the text editor's job.
&gt; the color that my text editor / IDE uses for comments (seems like it's usually kind of a faded green) Really? What are you using? In emacs with haskell-mode, which iirc is the most popular IDE for Haskell according to the polls, haddock comments are dark red and other comments are bright red.
I don't really do much Haskell. I skimmed Learn You A Haskell one time and thought it was cool. But I mostly do Java and JavaScript in Eclipse.
The structure and brevity of Haskell is such that comments stand out in the plainest of text editors.
Not to mention that // is already the list difference operator in Data.List Edit: my memory deceives me, \\\ is the list difference operator, // is the incremental update on an array from Data.Array, from the docs: "Constructs an array identical to the first argument except that it has been updated by the associations in the right argument."
We should redefine `/*` and `*/` to be some very common operators in order to scare the hell out of C/C++ programmers. ;-)
I'd vote for that.
&gt; "Blaze is encoding-aware, so it works better with Text than ByteString. However, we could have done a manual conversion or used H.unsafeByteString instead of H.text if we had wanted to stick with the ByteStrings." Is there any reason to ever prefer ByteStrings over Text when processing text (like HTML) and not strings of bytes?
/\* could be :: and \*/ function arrow (-&gt;), they wouldn't be able to lex type sigs (now they have problems with parsing)
ByteStrings can be faster. If you know with certainty the character encoding you'll be dealing with, you can definitely do faster processing.
Do you have any more information on this? I'm playing around with ByteStrings and Text, but I haven't noticed any difference in speed for UTF-8. Any links to more information would be greatly appreciated.
 (/*) = const (*/) = id Would let you comment stuff out, but only in really limited circumstances (like variadic functions). I think this is the best compromise.
[Haskell's parsec library Documentation](http://hackage.haskell.org/packages/archive/parsec/3.1.0/doc/html/Text-ParserCombinators-Parsec.html) [C++ boost spirit parser library Documentation](http://www.boost.org/doc/libs/1_45_0/libs/spirit/doc/html/index.html) [Python's PLY parser](http://www.dabeaz.com/ply/) Documentation of three parser library in three different languages where Parsec is in Haskell standard libraries collection and the other two aren't (I think?). This is why I need to think a lot because there is lack of library documentation. Sorry type signature, book, one line comment, source code, and academic paper are not documentation in my point of view. My first experience with python is that I can get a quadratic programming library (non standard library) up and running my code in less than 10 mins after I installs python. I tried that with parsec (comes with Haskell Platform) and still not understanding it after few days. Don't claim that I don't understand the theory behind parsec, I have a pretty high level maths education and understand all theory in the paper. However once start writing some code, I need code example for every small function like in the python doc. Even c++::boost can do that why not haskell. 
That's true; sorry for that. I plan to devote some time in mid-January to improving (or, rather, creating) documentation for Parsec. And thanks for the comparison with other libraries and sharing your experience -- this is very helpful.
Seems more accurate to say that hexpat gets Blaze syntax. Though heist uses hexpat for its processing, in my experience it's very rare that you build any significant parts of your output in hexpat. And when you do generate output, you generally want to copy attributes and such (especially style, class, etc.) over from the input tag, which would make this somewhat awkward to use.
On one level you're right -- the documentation is not all in one place, and there's therefore more of a barrier to entry than there should be. On another level, no. Parsec has very good haddocks -- you just need to browse through them a bit. See http://hackage.haskell.org/packages/archive/parsec/3.1.0/doc/html/Text-Parsec-Char.html and http://hackage.haskell.org/packages/archive/parsec/3.1.0/doc/html/Text-Parsec-Combinator.html for example. Also, there's the classic documentation, which is comprehensive but somewhat outdated: http://legacy.cs.uu.nl/daan/download/parsec/parsec.html
I must say that documentation of 3.1 is a bit better. However as I said &gt; type signature, book, one line comment, source code, and academic paper are not documentation in my point of view I read through them all but they don't really help to get me up and running. I have to figure out how all types interact which is largely undocumented. I tried Real World Haskell and it goes over a very small portion of Parsec that is rather useless for me. The book is structured in a way that you have to read the whole book which I really dont have that kind of time. (The book is good though, dont get me wrong.) and the paper is just theory to prove that a parser can be done in functional programming language. 
The classic documentation which I linked is not just "the paper". But, the main issue is how to think with types, I think. But once you get the principle, and assuming you're relatively conversant with Haskell (and that's a big assumption), the types tell you most things you need. It's rather difficult to jump into Parsec without understanding a great deal of what you'd get from RWH -- not that you need to learn it via RWH, but you need to learn it in some fashion. And like python, but not like C++, its very easy to play around with small examples and grow a full-fledged parser out -- that's how I wrote my first few parsers -- after which I came to increasingly rely on the language modules (and expression tools) to do lots of the heavy lifting for me.
Please let me know when that happens. I want to write an interpretor of some dynamic language and I was choosing between C++/Spirit, Haskell/Parsec, and F#/FParsec. I really want to go for Haskell because I like functional language but it is just too hard to get my hand dirty after few fibonacci example. I can also offer suggestions on what can be put into the new docs. 
as the author of LYAH and a borat impersonator i'd just like to say: is nice!!
You're right. We think that a good general principle when using Heist is to discourage the use of this new syntax. But that said, it still can't hurt to have it available. Since parsing isn't covered by blaze, we're always open to ways we can improve on the syntax provided by hexpat.
Sure, they can be faster, but is this worth it? It's not really the Haskell way to use unsafe representations for better performance. I truly think it's a shame (and a source of lots of encoding issues) that so many Haskell libraries use ByteStrings to deal with text. ByteStrings are certainly ok for low-level stuff, but I'd much rather not see them surfacing in APIs too much.
I know that various people have run benchmarks on various aspects of things, but I can't point you to anything particularly authoritative.
Not to be a debbie downer, but this is duplicated effort. new-hackage, which is scheduled to go live Real Soon Now (for some value of soon) already lets you do this out of the box: http://cogracenotes.wordpress.com/2010/08/08/hackage-on-sparky/
I wasn't advocating, simply stating the facts. However, I think using ByteStrings for ASCII data (eg, network protocols) arguably *is* the right approach.
Maybe I'm missing it, but it doesn't seem to me like they're overlapping. Yackage is a light-weight, standalone system for locally deploying packages. I didn't see those kinds of features in new-hackage. Even right now you can get the same end-result of Yackage with the current Hackage setup, it just requires more work. Yackage is meant to make the whole process simpler.
I missed it on purpose. /r/programming is almost entirely retarded stuff like this, they are welcome to keep it.
All of Haskell's docs would be far more useful with examples.
Those "retards" over there represent fairly adequately the "retardation" we find in the industry everyday.
Very interesting. I was also thinking, and talked briefly on IRC a month or so ago, that a best practice for Haskell programming might turn out to be to set up your own Hackage with local modifications to packages. I never got very far in that direction, though... instead, I poked and prodded and in one case rewrote a simple library whose dependencies were bad, and made things build straight off the central Hackage. The point is, clearly there's a problem here that ought to be solved. I'm wondering if this is the way to go... certainly it adds some complications: 1. Suppose I make a bunch of local modifications to packages that I don't maintain, now it's a long coordination process before I can release my package to others. 2. Suppose I want someone else to work on the code: does the entire data tree for a local hackage go in the repository? I'm really not sure what the best way to proceed is here. But dependency issues like the one Michael mentions have definitely stopped me dead in my tracks a few times.
My sense was that new-hackage was supposed to be a single cabal-installable executable. Which means that running a local hackage should be just as simple as with yackage. I might be missing it too -- but I don't see what yackage gives you above and beyond that?
Funny - for all your negativity, it looks like the article is getting a sensible discussion in r/programming.
From some tangental personal experience, unless you enjoy working with *highly volatile* people, stay away from working directly on trading applications.
* Doesn't require any kind of account to upload. * Allows you to overwrite existing packages. Basically, it's geared for testing. I don't think new-hackage is (and I certainly **hope** it's not).
Those would be trivial changes to new-hackage though, and could be implemented via some sort of flag, etc. New-hackage is designed to be modular and configurable for all sorts of use cases. I'll grant that your code is short and sweet enough that I'm sort of grousing about nothing here. I just want to make the point that at this moment, a full-featured local new-hackage, including yackage features, is only slightly harder to get going than yackage, and should scale up well to whatever other demands you may want to put on it in terms of features.
How are ByteStrings unsafe?
http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours Which shows how to write a simple dynamic language interpreter, and also includes a pretty good parsec basics tutorial.
In that they let you interpret strings in the wrong encoding: e.g. latin1 instead of utf8.
From the posting, it sounds like the position is for work on their non-trading IT.
That just depends on the font-lock-face being used. I.e., depends if you're using an emacs color-theme, and which one, and what it binds `font-lock-comment-face` to. I'm using gray30 and it's `lightgreen`.
You mean a couple people pointed out that the article is retarded and the author said "NO NO ITS NOT!!1" a bunch of times? How exactly is "I don't read /r/programming specifically because it is full of these kinds of retarded articles" a statement on what kind of discussion occurs there?
I am extremely sorry for the grammar, although you guys do not seem to mind that. I am not a native English speaker. it should have been "we start typing" or "starting to type". I only realized that after.
&gt; I can also offer suggestions on what can be put into the new docs. They are more than welcome. Could you please send them to roma@ro-che.info?
Iâ€™m Tsuru's current intern, and I can honestly say that everyone at the company is very relaxed and good fun to work with. I couldn't imagine a better place to work.
Monads are also a generalization of functions where the composition operator can be more elaborate than `(.)` so...
How's Tokyo? I've always wanted to visit Japan.
If nothing else this can serve to provide a useful bit of evolutionary pressure on how the new hackage service evolves.
&gt; Suppose I make a bunch of local modifications to packages that I don't maintain, now it's a long coordination process before I can release my package to others. I think this issue is orthogonal to whether or not you use a local hackage. Basically, you have to decide either to make your modifications in the form of separate modules that can be a separate dependent package, or fork. A middle ground is to modify one part of the package, and then hope to convince the package maintainer to split that part off as a separate package so that your modification just becomes an alternative to the separate package.
The main feature is that it's really, really simple. It's just completely missing all of the user interface parts that you need for a public hackage. So *everything* you ever need to do while developing - not just installing it and running it - is simpler and faster. You can look at the code, and at the package database, and understand it all in one glance. I'm working with it now, and it's a real pleasure. That doesn't take anything at all away from new-hackage, which is fantastic.
Wadler's Law?
&gt; Suppose I want someone else to work on the code: does the entire data tree for a local hackage go in the repository? A local hackage contains the entire source code tree of each package you upload to it, but none of the data from the public hackage. `cabal-install` supports multiple `remote-repo` lines in its config file; if it doesn't find the package on one server, it goes to the next. Anyone else who has access to the same list of servers that you do can work on the code.
I love the titles! The concept being introduced here is that of the [Knaster-Tarski fixed point theorem](http://en.wikipedia.org/wiki/Knaster%E2%80%93Tarski_theorem) which is one of the most powerful existence result in all of CS.
Ah, I wasn't thinking about it that way. To me, the choice is often between modifying a package with bad dependencies, or changing my code to either rewrite the functionality of that package, or use older dependencies. For example, perhaps I'm modifying some other package to update its dependencies so that I don't have to build my code with base 3, or something like that. Then I am faced with the trade-off I mentioned. It's just occurred to me that Michael's reasons are a bit different... he's interested in testing a collection of related packages all under his control. The problem I'm struggling with is that I have a manageable number of packages in my control, but I'm constantly fighting outdated dependencies that prevent me from building my own code.
For those unfamiliar with this foundational theorem of modern PLT: http://www.haskell.org/haskellwiki/Wadlers_Law
How so? As a structure on types a monad has only a single parameter, which precludes anything that looks much like function composition. As a type operator a monad is an endofunctor, akin to functions with a type like `(a -&gt; a)`, much simpler than general function composition. The interesting features of a monad do result from functor composition--i.e., the function `join`, the operation of collapsing nested layers of the functor into one layer. But that really has not much resemblance to term-level functions or composition thereof. On the other hand, functions that create monadic structure with types like `(a -&gt; m b)`, such as the second argument of `(&gt;&gt;=)`, do compose like functions, but these are not the same as the monad itself, and in fact are actually `Arrow`s via the Kleisli wrapper. I suspect you are falling victim to exactly the sort of misleading descriptions that I was warning against.
&gt; On the other hand, functions that create monadic structure with types like (a -&gt; m b), such as the second argument of (&gt;&gt;=), do compose like functions, but these are not the same as the monad itself, and in fact are actually Arrows via the Kleisli wrapper. That's my point, `(&lt;=&lt;)` is a more elaborate `(.)`. Because you can take the underlying category as the Kleisli category for a monad, this is a generalization. I don't know what you mean by this not being "the monad itself". You don't think that "every monad forms an Arrow" is a significant connection between the two?
&gt; I don't know what you mean by this not being "the monad itself". You don't think that "every monad forms an Arrow" is a significant connection between the two? Not really, no. At some point everything is trivially related by virtue of being expressible in Haskell, and "every monad forms an Arrow" is only slightly more interesting than that lower bound. Again, the interesting features of a `Monad`--in practical terms--come from `join`. This is what sets it apart from `Applicative`. The interesting features of an `Arrow` come largely from `(.)` and I suppose `first`. These two sets of interesting features are essentially orthogonal, and Kleisli arrows are the entirely predictable result of combining two orthogonal things in an obvious way. There is nothing of interest in a Kleisli `Arrow` that is not directly provided in an obvious way by either the `join` function or the `Arrow` instance for `(-&gt;)`. More importantly, any connection that does exist is decidedly one way, because not all `Arrow`s form a `Monad`, so emphasizing Kleisli arrows is counterproductive for someone trying to understand the former in more general terms.
I would have said Kleisli composition was the interesting feature of a monad. But my point is your description of an arrow above could be said exactly of monads (or comonads, or lots of other things). If you want to set Arrows apart, you need to say something about them which you cannot say about monads.
&gt; But my point is your description of an arrow above could be said exactly of monads (or comonads, or lots of other things). Or anything that can be used to construct arrows in some subcategory of **Hask**, yes. The description also says nothing particularly useful or interesting about those things, for the exact same reason. That was the point. &gt; If you want to set Arrows apart, you need to say something about them which you cannot say about monads. If you're considering monads to be their Kleisli category instead of the usual endofunctor with two natural transformations then this is clearly impossible, because `Arrow` would then be effectively a superset of `Monad`. This is also not an interesting observation because, again, almost everything that matters about using `Monad` in Haskell is described by the part that would be discarded when generalizing from a Kleisli arrow to an arbitrary `Arrow`. If you consider `Monad` either in terms of the triple or its construction in Haskell then my original point stands. I don't think we're communicating very successfully here. I'm beginning to suspect that I'm failing to understand what it is you're trying to say.
We should be able to fix this. If `text` used UTF-8 internally we could have O(1) conversion from `Text` to UTF-8 encoded data in a `ByteString`. Bryan told me it's a few days of fulltime work to convert text to use UTF-8 internally.
No offence meant, but shouldn't this be in r/mathematics?
That's a nice interface. Now we need an implementation of that interface for HXT and HaXML. There is no built-in cursor for those, so it is not as straightforward, but it can be done.
Last I'd heard Bryan was not interested in this conversion. I'd be very happy to hear that he has changed his mind about that. Is he looking for any assistance, or is this really just a one-man job?
I feel this has a place here because it's about executable mathematics, i.e. programming. 
Yeah, my bad, I was tired and mistook it for the other kind of maths.
It's the kind of maths I think is very useful if you write Haskell programs.
Happy birthday.
The ultimate determination of practicality involves using something in practice. Static function scoped globals are semantically equivalent to other more common ways of expressing the same thing which is opaque global state. If the function "fibhelp" took another parameter to reset the state or not it would be just as useful in a single threaded context as a structure or object. Just because people do not write code that way professionally does not mean that it does not become an interesting interview discussion, and sometimes employers want you to be invemtive and break away from conventional thinking. Who wants to interview someone giving textbook answers all the time? Sure you want candidates to know this stuff, but you also want them to be imaginative too.
Huh, archiveorange isn't so well liked on Web of Trust. http://www.mywot.com/en/scorecard/web.archiveorange.com
Stuff like this leaves me in complete awe of early computer scientists. 
While it is true that Landin was thinking far ahead of his time, and his work is truly amazing, it is not fair to imply that he saw the connection to category theory. At that time, even among mathematicians MacLane's category theory was a little-known "abstract nonsense" tool, in use only by a handful of topologists and algebraic geometers.
hahaha!!
Highly recommended!
Awesome!
The first sentence makes plain that Oleg K. is saying exactly that -- that Landin did not grasp that connection: &gt; The unabated debate about exactly how much category theory one needs to know to understand that strange beast of IO prompts a thought ... The connection of Landin's remarks with the category jargon he imputes to Hill and Clarke in their discussion of Wadler and Peyton Jones. Oleg is saying that Landin's case is a spectacular validation of the subtitle to: ["You Could Have Invented Monads! (And Maybe You Already Have.)"](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html).
hahah nice :D
Stopped listening around minute 11 when he steps away from the mic -.- Very funny intro, though. I would've been interested in listening to the rest if it were not painful to try and hear as if through a wall.
Compiling this took me a few tries.
This messes up the levels of abstraction though. You need a type to represent a sequence of Unicode points, regardless of their encoding.
If changing the internal representation to UTF-8 actually speeds up things and someone is willing to do the work, I'm sure he will accept patches. There was a discussion a while back we several people claimed (without benchmarks) that more or less all performance issues in `text` would be solved by switching to UTF-8. Bryan has showed again and again (by speeding up the code) that most performance issues don't have anything to do with the internal encoding.
So I got around to throwing these on patch-tag, which makes it a bit easier to browse the source code (thanks Thomas!) * https://patch-tag.com/r/cdsmith/mysnapsession/ * https://patch-tag.com/r/cdsmith/mysnapsession-example/
Thanks for taking the time to make this. I especially like the mention of the people that have helped the Haskell community to be a better place. One of the best aspects of Haskell is it's community and the recognition is well deserved so I would just like to add my thanks to those guys too. All in all it looks like a good year to me. Happy new year; and keep up HWN, I know I enjoy them. 
NO QUOTES!
Thanks! Comments like this are very encouraging :)
Here's another approach I've been working on for a while: https://github.com/ozataman/snap-extension-session Currently only a cookie-backed session store has been implemented, but I'm planning to expand it into other backends as various DB bindings become available.
Uhh, the top scoring Stack Overflow questions each got one vote? ;-)
Nice to see, thanks. I was also interested in looking at client cookie-based sessions as a second implementation. I was hoping we might be able to agree on a MonadSession class, but alas, it doesn't look like it. It's fairly important to me that the session not be restricted to a name/value lookup table... and certainly not with ByteStrings as values.
Exactly. This Reddit post about the syntax of multi-line comments has 26 comments, more than any `/r/haskell` post in recent memory.
I linked to the commit so that you can see the relevant code hot off the press (changes have since been made, natch). Some context about why you should care: * DSLs are powerful tools: write someone a program and you solve their problems for a day; write them a DSL and *they* can solve their own damn problems (sigfpe). * One (the common?) approach in DSL implementation is to write an interpreter. The problem is that DSLs stacked one on top of another increase running times exponentially, the exponent being the number of DSLs. This helluva discourages deep DSL stacks. * "Complete laziness", as defined in Thyer's thesis, holds the tantalizing promise of only constant-time interpretative overhead because of its partial evaluation property. Instead of each DSL being a multiplicative slowdown, it's now only an additive slowdown. * Luke "luqui" Palmer has written code that demonstrably scales linearly in the number of DSLs, the toy DSL here being a lambda calculus self-interpreter. * Partial evaluation is interesting in its own right. Witness the invited talk by Lennart at [PEPM 2010](http://corp.galois.com/blog/2010/1/19/padlpepm-2010-day-1.html) -- yes, a whole conference is devoted to the topic every year. EDIT: An alternative to writing the typical "tagful" interpreter is the "tagless" approach explained by Carette et al. Off the top of my head, I don't see why a stack of tagless interpreters won't also pass the tower of interpreters test, although I don't have code demonstrating the case. EDIT: Added sigfpe attribution.
Unusual.
I was also considering JSON at one point, but actually like your approach of leaving the session type decision entirely to the user of the library. I'll give it some thought for sure. Thanks for sharing!
Awesome! You rock! On a different note, I really wanted to draw your attention to [luqui's latest vatican code](http://redd.it/euhgi) for this issue. Oh well, maybe a reddit submission is the right way to go. Maybe it'll even make it for next week if it gets enough points, yes? 
If you did adopt that approach, I think you'd be fine with the cookie-based implementation. You'd just need constraints on the MonadSession interface to ensure that the chosen type has the instance you need from the cereal package. You do have an advantage, though, when it comes to implementing a database back-end. There it's going to be inefficient to just store a whole brand new map in the session, so being able to capture the individual changes to specific name/value pairs is more important. I'm still unsure about the right thing to do there.
Partial evaluation is awesome stuff! I'd love to see some kind of "general purpose partial evaluation annotations" for Haskell. That would be able to speed up static parser combinators, stream fusion or functional images Ã  la Conal. Currently, the only technique that comes close is to use the LLVM bindings and compile on the fly.
"...Haskell, a member of the Lisp family..."
For $1k? Not quite as bad as those ads for "need new facebook clone with 5M users, done in three days for $100" but seems slightly unrealistic.
Looks awesome, thanks.
Very cool. Many people I know consider partial evaluation to be a dead end, so it's cool to see very compelling progress being made on narrower fronts.
It does seem that way. But according to this employer's history on odesk, he defines requirements precisely and realistically, and people who work for him seem satisfied on the whole. On the other hand, many people appear to have worked for as little as $1/hr. I guess that's just the way odesk works.
I can't say anything about the performance numbers, but I can make a pretty convincing argument on memory usage: for most of the data I deal with, the vast majority of characters are in the ASCII range, and thus take up twice as much memory in UTF-16. This includes multilingual documents, since the markup is always in ASCII. I even did some comparisons of re-encoding some Chinese web pages in UTF-8 versus UTF-16, where UTF-8 *always* came up significantly smaller. So if the question rests on whether the speed of some benchmarks will change, I have no answers. If the question is whether the library will be better overall, I think halving memory usage for common use cases is worth it.
Hey djointpear. Followed your suggestion and posted it to my blog. I think luqui's code could be posted to reddit, and then see how people respond to it. It's too bad a was a bit in a rush to get the newsletter out, and managed to miss 2 things: all the SO answers have 1 vote (darn script misread the values), and there are 2 reddit questions that have a link to "original post", where the original post should be the same as the reddit link. I would have also liked to trim down the fact that there were 3 stories pointing in one way or another to LYAH. Oh well. Live and learn. Here is to wishing you a good new year!
Pretty minor oversights, all things considered! Thanks for putting in the time to do this, given how hectic the end of year holidays can be.
How can I read this if I don't own any Apple products? Isn't it ironic that only people with Apple products can read something from the "Open Source Journal"? 
The tremendous thing about the Haskell community is the staggeringly stupendous members it's made up of. How are they stupendous? Well, they do outstanding work, and when it's ever so slightly short of outstanding, they self-correct. One tip: although it's superbly appropriate to use automated tools to bring in the harvest, selection and the final edit must have a human touch, like a professional grader using his nose to pick out the choicest tea leaves. dstcruz, have a great 2011! 
I've uploaded a new version of mysnapsession with session-backed cookies in the same way as your library does. I'd planned on doing client-side sessions eventually. I'm still trying to figure out what I'm going to do with database back-ends, though... I suppose I'm hoping that the approach of wiping out and restoring the entire session data is good enough because session data is often small.
The other open question in my mind is how to get the type family approach to nicely integrate with other pieces of the application stack. One example would be authentication functionality. If I know ahead of time that I'm using a Map for session, then I can integrate the saving/clearing/etc. of the authenticated user into the an auth package without any need for additional declarations. If not, then the user will have to define a field in the custom session that holds the current user's ID and handle a lot of the user-persistance logic. In that scenario, the right thing to do would likely be to have the auth package implement its own session management which would somewhat mirror (albeit in a simplified fashion) what I currently have up there including cookie management, encryption/decryption/csrf/xss/etc. This does seem like a lot of redundant functionality. Also in the long run, it might make sense to standardize on a flexible session datatype that future extensions can count on having so they can automagically do as much as possible for the Snap user.
Are you referring to something like the HasDlgManager class in my Snap.Dialogues module? I thought that worked pretty cleanly, all things considered. It seems to me that something similar could be done for authentication. Then again, user authentication might not necessarily be related to sessions in the first place. For example, most web sites allow a user to stay authenticated for longer periods of time using non-session cookies. So then the question becomes how to best build cookie-related code which is *not* (necessarily) related to sessions.
It's just a photo. The photo was uploaded through an iPhone app(Instagram) to its image sharing website. That's all.
So where is the "Open Source Journal" with the Haskell article? 
&gt;So where is the "Open Source Journal" with the Haskell article? As of &lt;insert my comment's post time here&gt;, [Google sure doesn't know](http://www.google.com/search?sourceid=chrome&amp;ie=UTF-8&amp;q=%22open+source+journal%22+%22functional+programming+with+haskell%22). Has anyone even heard of the "Open Source Journal"?
Fuck everything about this
I'm the poster of that on oDesk. yitz, thanks for the homework. The $1/hr people seem to typically be non-US college students trying to get experience or full time workers doing it on the side for experience or fun. It's been an interesting time working with them. This project is different, though. As for $1k being unrealistic--for a throw it over the wall fully functional bells and whistles thing, absolutely it would be. But this isn't. The process of just backtesting is straightforward and this project is just the first basics of it--not a fully fledged user-ready software package. Also, I'm a software developer myself but due to time constraints and the fact that I'm still learning Haskell, I'm looking for help with it. So, it is more of a--develop these well defined straightforward modules kind of thing that I then glue together. I was in a hurry putting up that oDesk post... but now that I've written this up, I'll add the above clarifications to it. Thanks!
isn't supercompilation supposed to solve this?
Probably not. Supercompilation intends to make things faster, but it doesn't give you any control about optimization process. In contrast, a DSL writer usually has a pretty good picture of how to compile his DSL efficiently and the hope is that partial evaluation / staging allows him to do that in Haskell.
According to ISSN, it appears to be latest issue of this magazine: [Open Source Journal](http://www.opensource-central.com/open-source-journal.php)
It can't. Lazy specialization is able to specialize functions even created at runtime.
Thanks for sharing. Out of curiosity, how does this paper relate to the book? Is the text the same, or has the book been polished/enhanced in any way? If only publishers would make great books like these available on nook/kindle...
This _is_ the book.
Is it legal to publish this here? If so, thanks a lot! I've been wanting to read through this for a while. 
Neat! There was a Haskell sudoku solver rage a few years back. Some of the solvers are archived here, http://www.haskell.org/haskellwiki/Sudoku I was just thinking yesterday about how to write a continuation based solver. When solving an 'easy' board, there is always (at least) 1 spot on the board at all times where you can infer that there is only 1 possible correct value given the information available. So, you never have to guess. That means you do not need any form of backtracking. But, in the harder puzzles, you sometimes do not have enough information to know and you do have to guess. However, after you make the guess move, you may have a bunch of non-guess moves in a row. So, in the harder boards, you generally do not have all guess moves -- it's a mix of non-guess plus a few guess moves. So a solver that treated every move as one that needs backtracking would be 'overkill'. It feels like when you make a guess you could create a continuation, and then continue on with the non-guess moves. If you end up with no valid moves left, then you know the guess was wrong and you call the continuation to jump back to the guess point and try something else. (And, you could, of course, have several nested guesses). 
I'm curious to see how he handles the memoization aspect of thyer complete laziness.
For reference, thyer's phd thesis, which is very enjoyable reading: http://thyer.name/phd-thesis/
We'll probably just use memo tables like Thyer did. There are bigger issues to work around (e.g. space leaks due to the increased sharing) than just the fact that the memo tables are kind of inelegant.
Always worth pointing out Richard Bird's Sudoku solution, developed through the transformation of an inefficient specification. http://www.cs.tufts.edu/~nr/comp150fp/archive/richard-bird/sudoku.pdf
This was the thing that made me *get* program calculation. That's not to say I have a hope in hell of doing it but I now know *why* people put the effort into it and it made me pay more attention to the field.
Care to share some history behind this project? Personal/business etc?
My version, FWIW, written for a Project Euler #96 import Data.List -- input/output of puzzles sudoRead = map (\x -&gt; if x=='0' then [1..9::Int] else [read [x]]) . filter (\c -&gt; c&lt;='9' &amp;&amp; c&gt;='0') inter n s xs = take n xs ++ (if null xs' then [] else s ++ inter n s xs') where xs' = drop n xs sudoShow :: [[Int]] -&gt; String sudoShow = inter 75 "\n" . inter 24 "\n" . inter 6 " " . concatMap cell where cell [n] = show n ++ " " cell _ = ". " sudoString :: [[Int]] -&gt; String sudoString = concatMap f where f [x] = show x; f _ = " " --utility functions {-# SPECIALISE filterBy :: [Bool] -&gt; [Int] -&gt; [Int] #-} {-# SPECIALISE filterBy :: [Bool] -&gt; [[Int]] -&gt; [[Int]] #-} filterBy [] _ = [] filterBy (b:bs) (x:xs) = if b then x:fs else fs where fs = filterBy bs xs deNull :: [[Int]] -&gt; [[Int]] deNull xs = if [] `elem` xs then [] else xs utnc f x = if x' == x then x else utnc f x' where x' = f x -- 'until no change' --solving a puzzle (boxSs,boxUs) = (map (map or) bs, map transpose bs) where bs = [[[ x1/=x2 &amp;&amp; y1==y2 , y1/=y2 &amp;&amp; x1==x2 , (x1,y1)/=(x2,y2) &amp;&amp; (div x1 3, div y1 3)==(div x2 3, div y2 3)] | x1&lt;-s, y1&lt;-s] | x2&lt;-s, y2&lt;-s] where s = [0..8] strikes su = zipWith strikeBy boxSs su where strikeBy bs = (\\ concat (filter (null . tail) $ filterBy bs su)) uniques su = zipWith uniqueBy boxUs su where uniqueBy bss s = if null u then s else nub u where u = concatMap (uu s) bss uu s bs = s \\ concat (filterBy bs su) reduce su = utnc (uniques . strikes . deNull) su guess [] = [[]] guess ([x]:us) = map ([x]:) (guess us) guess ((x:xs):us) = [[x]:us, xs:us] solve su = utnc (filter (/=[]) . map reduce . concatMap guess) [reduce su] -- soluble by pure reduction, requires guessing, ambiguous, insoluble, insoluble after guesses suR = sudoRead "100080300060070084030500209000105408000000000402706000301007040720040060004010003" suG = sudoRead "200080300060070084030500209000105408000000000402706000301007040720040060004010003" suA = sudoRead "000080300060070084030500209000105408000000000402706000301007040720040060004010003" suI = sudoRead "900080300060070084030500209000105408000000000402706000301007040720040060004010003" suJ = sudoRead "500080300060070084030500209000105408000000000402706000301007040720040060004010003" I subsequently extended it to also generate puzzles, and cut it back down for this post, which may account for some oddities. I'll add 'how it works' details if anyone's interested. I don't claim it's especially inspired code, just part of my personal autodidactic path.
This is truly a pearl.
I did this problem just last weekend. Thanks for sharing. BTW what's the double-slash (\\) operator?
It's a list 'subtraction'. http://www.haskell.org/ghc/docs/6.10.2/html/libraries/base/Data-List.html#v%3A%5C%5C
Very cool guys. Show those side effects who's boss :)
&gt; Haskell is clearly moving towards dependent typing, which in theory, allows the expression of arbitrary invariants that are maintained statically, without having to run the program. isn't there a giant wall of undecidability in the way? (I'm no expert on this sort of thing, but am a massive haskell fan)
Short answer: there is. Long(er) answer: the philosophy of dependent types is this: have the compiler do as many proof obligations as possible, and the user do the rest. The "types as formulae/programs as proofs" view allows the user to supply, as bits of code, proofs that provide evidence that the invariants are satisfied. [Some](http://adam.chlipala.net/) believe that this is not feasible in practice, and that we need an additional "proof scripting language" to help us write the proofs. I tend to agree.
Before even checking the link I had a plan to reply and complain that it doesn't point to Chlipala. Now I only have a meta-complaint that you took away my reason to complain.
&gt;Adjustably-safe: In this mythical, not yet-existing, but clearly on-the-horizon "Haskell", you'll be able to choose how much safety you want. You'll have "knobs" for increasing or decreasing compile-time checks for any property and invariant you desire. Or don't desire. You'll be able to switch between Lisp-style dynamic typing, Haskell-style static typing, and probably even C-style weak/no-typing. In the same program. This already exists: it is called `Data.Dynamic`. The "knobs" are called `fromDyn` and `toDyn`. Maybe I'm missing something, but I can't think of anything that I'd want to do with dynamic types which couldn't be done using this simple tool in the appropriate way. Perhaps some syntactic sugar could be added. It doesn't seem like a profitable direction. Here's a C-style `fromDyn`: cFromDyn = fromDyn segfault
Decent syntactic sugar is not quite as trivial as it sounds. One problem is simply inferring where to add coercions. Without some care, the compiler will also add enough to make things that should statically fail ('x' + 1) also compile (fromDyn (toDyn 'x' + 1). There's good work under the name "gradual typing". Another problem is simply being sure that your Dynamic type doesn't compromise static safety. GHC's dynamic is implemented with unsafeCoerce, after all. Especially if you want to allow projection to and from polymorphic types, it's worth a bit of work to be sure the system is still safe. Here there's good work by Wadler and the PLT group on "blame" which makes a good start towards figuring out what the definition of the safety property should actually be, and proving safety for some systems.
It seems [iamseanmurphy.com](http://iamseanmurphy.com/) is the author of the site. I think I would prefer something more community-driven than this. And calling them "Dream Jobs" (while there is nothing yet) doesn't match with the idea I have of the intended target.
the alternative isn't a community-driven site, it's Mr Murphy not doing it at all. Granted, it's a very hard thing to bootstrap - it's not worth anything to anyone until there's both groups of hackers and employers looking at it.
Type checking for dependent types is decidable with enough annotations. Haskell already includes some things for which inference is undecidable without annotations (polymorphic recursion is H98, higher-rank types are also undecidable).
Hmm, I was imagining something like dynamically and statically typed files or modules - a "dynamic-land" which interfaced with a "static-land" by means of type coercion. Making `'x' + 1` raise a compiler error in dynamic-land would be useful, but it would actually be making something that no dynamic language offers currently. I don't understand your second point. The whole idea of using a dynamic type is to compromise static safety. For some reason this is seen as highly desirable by certain people. I'm sure there are some minor improvements to be made to the current abilities we have with Dynamic. But the larger question is, why bother? If partially-typed code is really a Mecca, then it's already available, if somewhat awkwardly, by using a moderate number of manual annotations. No one seems to use it. It just seems like one of those ideas which sounds good only because it's talked of in completely abstract terms.
Sometimes on the tiny threads I can be the first to say something... Seriously, Adam seems to be the figurehead for applied dependently typed programming on that side of the Atlantic. It's also nice that he hasn't succumbed to the temptation to develop his own language. I'm not sure another obscure academic language with dependent types is what we need. I have worked on such a development myself of course :)
Isn't that what Ur is, albeit a domain specific obscure academic language with dependent types? :-)
&gt; it's Mr Murphy not doing it at all This isn't true. There are at least [cufp](http://cufp.org/jobs) and [haskellers.com](http://www.haskellers.com/). And I agree it is difficult to bootstrap since when a job ad exists, it is posted on mailing lists, reddit, ...
There's http://tryhaskell.org. 
http://tryhaskell.org/ works on my iPod touch in Safari. Also, you can install any irc-client or jabber-client and use lambda-bot as simple interpreter. Of course, It would be great, if there was such app. (there is one for scheme: http://web.mac.com/Jay_reynolds_freeman/My/Pixie_Scheme_III.html)
Haskell: Avoid success at all costs!
Awesome, that's pretty much exactly what I needed. I will look into the Scheme app as well. Thanks - 
Does it really even work? `let x = 5` #####not an expression \`let x = 5'
to be fair to that interpreter (which is rubbish) it's right, that's not an expression, if you enter an expression it will evaluate it
 let x = 5 in print x
&gt; Hmm, I was imagining something like dynamically and statically typed files or modules You should take a look at Typed Racket (formerly Typed Scheme, IIRC). It works exactly as you suggest, but comes at it from the Lisp side of things.
Ironically, I wrote this shortly before I saw Johan's submission, and about a conversation last night. In case anyone is interested...
I would change this: data Card = Room Room | Suspect Suspect | Weapon Weapon deriving (Eq, Show, Read) cards :: [Card] cards = map Room rooms ++ map Suspect suspects ++ map Weapon weapons to this: data Card = RoomCard Room | SuspectCard Suspect | WeaponCard Weapon deriving (Eq, Show, Read) cards :: [Card] cards = map RoomCard rooms ++ map SuspectCard suspects ++ map WeaponCard weapons Maybe for more experienced haskell operators this wouldn't be confusing at all, but for mine it made me stumble and seemed to be a case of using type inference just for the hell of it.
I believe the interpreter is GHC (by way of mueval). Why is it "rubbish"?
no up-arrow and that sort of thing
I know its ugly, but could serializing closures be done with a bit of TH and Binary trickery? Specifically, the TH constructors are all simple ADTs (LitE, ConE, VarP etc) Deriving binary instances for them should be simple. Then, if you "compile" a function into its TH ADT representation (I think QQ can do this, I'm not sure). Finally, encode the ADT representation as Binary with any "closed" arguments (assuming those args are also members of typeclass Binary) it should work right? I know its ugly and I've never tried it, but in principle, should work right? 
I generally agree with the cabal dependencies one. "Private" dependencies is something I've been thinking about for a while. &gt; Even though there is no reason why two different versions of Parsec canâ€™t be used in the same program What is frustrating is that this is almost but not quite true. If you export a parsec parser (not a wrapper, an actual parser using the types from the parsec package) then it is quite possible to end up with compile-time errors in a client package that uses that parser in it's own parser but uses a different parsec version. Of course the above situation is not that common so being able to specify private dependencies would, I think, help in a lot of common cases. There are more complex cases where just a private/public distinction is not enough information to know if you'd get a type error at compile time, but a 80/20 solution is probably sensible here. The major implementation difficulty (and why we've not done it already) is in checking/enforcing the private notion. We almost certainly need to get GHC to do this part. The check needs renamed and typechecked code, so that we can tell which packages re-exported types come from. Ideally we would also check if a package dep is used in a private way so that we can suggest using the new feature.
Wouldn't the fact that typeclass instances are inherited with every import, no matter what, break this? 
I wish type classes would have to be exported explicitly. This implicit export is annoying in all kinds of contexts.
It is not really a hack. It is just that Lisp is not Haskell. Also, am I wrong in wanting developers to mention the Lisp implementation. He mentions Emacs Lisp in the blog. But SBCL is not Emacs Lisp. They seem to be worlds apart.
If you jailbreak your ipad you will be able to install hugs from cydia.
I guess that orphan instances for classes defined in the "private" dependency must be disallowed. This seems like a fairly mild restriction. Can you see any other problems?
Exactly. Why wouldn't you just use [cufp](http://www.cufp.com)?
Not just that, but if Haskell moves towards dependent types then there's a massive shift in programming style, too. Dependently typed languages aren't just "Haskell with more expressive types". It completely changes the way you program.
Get rid of IO the basic algorithms. It has no place there. IO should only be used when you need to communicate with the outside world.
I'm not a seasoned pro, but at first glance the 'elements' function seems a little awkward to me. I might skip 'elements' and just do: rooms = [Kitchen ..] suspects = [ColMustard ..]
Instead of your `elements` function you could derive `Bounded`, and use `[minBound..maxBound]` or just `[minBound..]`. Your turn functions doesn't need IO, does it? Perhaps it can be split into a function which updates the state given an action, and a function that repeatedly calls that with new actions.
Agree wholeheartedly about fixing cabal's dependency issues
But a player may need to communicate with the outside world, and I need to mediate players.
I designed with two basic players in mind: * one that just naively suggests cards it doesn't have until it figures out the truth * one that allowed a real person to dictate what they should do. The former certainly doesn't require IO, but the latter definitely does, so I thought the Player class needed MonadIO. And if I'm stitching together the work of multiple players in `turn`, then that needs IO, unless I'm mistaken.
Can't you write lines like get &gt;&gt;= mapM (notify $ Suggestion (position p) scenario) &gt;&gt;= put as this? modify $ mapM (notify $ Suggestion (position p) scenario)
no, because `modify :: MonadState s m =&gt; (s -&gt; s) -&gt; m ()` and `mapM (notify $ Suggestion (position p) scenario) :: [PlayerInfo] -&gt; StateT [PlayerInfo] IO [PlayerInfo]`
Hmm... You're right. But then why not just use this? modify $ map (notify $ Suggestion (position p) scenario) **edit**: Nevermind, sorry. I see my mistake now. I am not an experienced haskell coder. Thanks for pointing that out!
Very cool. I got the book "Purely Functional Data Structures" by Okasaki, and most (if not all) I've read so far covers persistent structures.
In so-called object-oriented modelling that's precisely what you'd do: code a "player" to "communicate" with the "real world". In Haskell that's unmodular and a big sign of un-FP-ness. 
Ok, good to know! I was thinking of writing code to moderate play (and enforce rules) between players, as a way to test out different AI implementations of how to play a clue game. Most of these won't require the fullness of IO, but some (like one that just defers to a live human to make decisions) will. If this is unmodular and un-FP, how should I reorient? Is there a different approach I should be taking? Edit: Ok, I think I figured out an approach to get IO out of there. If I parameterize the Player typeclass over the monad required, then I can pull it out entirely: class Player p m where -- let them know what another player does notify :: Event -&gt; p -&gt; m p -- ask them to make a suggestion suggest :: p -&gt; m (Maybe Scenario, p) -- ask them to make an accusation accuse :: p -&gt; m (Maybe Scenario, p) -- ask them to reveal a card to invalidate a suggestion reveal :: (PlayerPosition, Scenario) -&gt; p -&gt; m (Card, p) 
This isn't "type inference", just a separate namespace for types and data.
Right you are. Still seems unnecessarily confusing to double up on the uses of words in this way.
How about ssh-ing to somewhere? That's how I do Scala, Python, PHP, etc. on my iPod.
It works fine for me. Also, the interpreter itself is fine, the JS repl client might have some bugs.
Which ssh app do you use?
Not really. Nevermind that they're completely different monads, monads do not a Haskell make.
But he did invent the Zipper!
Ok I had forgotten about that. I guess no one resists the temptation :)
That's along the lines of what I thought of.
I agree, especially if "Room" the data constructor is not of the "Room" data type...
What are you referring to?
Great! I have some questions for the author (if he is reading). 1. How is the C being compiled? It would be interesting to compare C compiled with llvm-gcc, since that would tell you how much performance you are really losing from going via-c 2. Don't you get TCO for free if you just turn on the LLVM TCO pass? 3. Are you using the LLVM API directly or pretty-printing LLVM IR? If you are pretty printing, using the API might be a nice way to speed up compile time a bit. As for the static inline issue, one approach would be to compile the C unboxing functions and link them in. Of course, this means they won't be inlined - but you could work around that by compiling them with llvm-gcc and then using link-time-optimisation. This might be nicer than the alternative of implementing the unboxing functions twice (once as LLVM IR, once as C).
Negative. The Zipper is way older and was invented by [Ariadne](http://en.wikibooks.org/wiki/Haskell/Zippers#Theseus_and_the_Zipper).
Paths are not contexts.
Leibniz's laws of differentiation also explain how to calculate types of one-hole contexts.
iSSH - http://www.zinger-soft.com/iSSH_features.html
&gt; Also, am I wrong in wanting developers to mention **the** Lisp implementation. Yes, because he is talking about Lisp in general. (I don't think he is quite right, but that's another issue.) &gt; But SBCL is not Emacs Lisp. They seem to be worlds apart. [He appears to be quite aware of that.](http://axisofeval.blogspot.com/2011/01/more-fully-featured-modern-lisps.html)
however, it may make sense to abstract away that pattern modifyM :: MonadState s m =&gt; (s -&gt; m s) -&gt; m () modifyM f = get &gt;&gt;= f &gt;&gt;= put --- .... modifyM $ mapM (notify $ Suggestion (position p) scenario) 
You could represent the transition function as a higher order function instead of a map. You get a bit more flexibility (and potentially efficiency) with that representation, including exponential space savings in some cases and significant simplification of certain constructions in others, but the tradeoff is that you often need to do a graph reachability search to implement some of the more interesting constructions, such as minimization. You can take a look at my attempt [here](http://vorlon.case.edu/~lps/software/automata/). I was a little too "innovative" with the typeclasses, resulting in a very brittle interface, and I haven't even tried to load the code into ghc in a few years. I did implement quite a few constructions, but not Levenshtein automata in particular.
That's great, thanks! I'm interested in your representation of automata using higher-order functions; I think it would fit just nice into my AcceptFA typeclass (there are already instances for a few different representations of DFAs I played with: DFA/IntDFA and TiedDFA). Your code also gives me good examples of the implementation of Hopcroft and Brzozowski algorithms for DFA minimization that I shall put to good use. Would you consider joining me in writing a modern, full-featured automata library for Haskell? Your experience on that matter would probably be a huge help.
I'm not so sure my code is a "good" example of how to implement the Hopcroft-Ullman and Brzozowski minimization algorithms. :-) The Hopcroft-Ullman implementation is a straightforward transliteration of the imperative code, and the Brzozowski implementation constantly relabels the nodes in order to make the types work. Surely, one could implement those constructs in a simpler, more perspicuous, and hopefully more efficient fashion. And of course, you probably want a O(n log n) minimization algorithm, such as Hopcroft-Aho-Ullman. (I never quite figured out why that algorithm worked) If you are really that interested in my code, feel free to consider it BSD licensed. I can update the license in the file if you prefer. I'm extremely busy at the moment, but I do wish you well and can offer moral support. If you have questions, shoot me an email. (Check my Monad Reader article in issue 14 for my address, the Case one doesn't work any more.) If things go well, I might be able to help out in a few months. Also, you might be interested in some of my comments [here](http://lambda-the-ultimate.org/node/3668#comment-51959), and [HaLeX](http://hackage.haskell.org/package/HaLeX). Interestingly, the continuation-based DFA representation was mentioned recently on the haskell-cafe mailing list, although with a map to represent the continuation instead of a higher-order function.
Yeah, I think that post you're mentioning is from me; this approach is what I called TiedDFA in my library: https://bitbucket.org/mumux/automata/src/31b995a4c2c9/Data/Automaton/TiedDFA.hs Thanks for the support, I will probably end up sending you a mail with a few questions then :-)
Great work! This really needs to be on Hackage. 
Unfortunately I have already upgraded to 4.0.2, and it doesn't look like I can jailbreak it (yet). 
I have iSSH, but it completely refuses to find any of the other (VNC) machines on my network. Haven't really given that much effort to making it work though. 
I haven't used it for VNC much. It seemed a bit clunky when I did, if I recall. SSH is good; the cursor key thing and the tab strip work great. My favorite mobie VNC is RemoteVNC, but that's for Android... I don't know of any other iOS VNC clients.
1: The C backend produces C99 which is compiled directly with GCC. We could compare gcc vs llvm-gcc but we'd need to take the numbers with a spoonful of salt. DDC doesn't do any cross module inlining yet, so there are lots of superfluous function calls (like all the boxings/unboxings with (+)). We used to have a hack that faked inlining for arithmetic functions, but it got broken in the last refactoring... time to do it properly. 2: I'm not sure if LLVM TCO will help us, you'll have to ask Erik. We also have a shadow stack that stores GC roots, and I doubt the LLVM compiler can cut that back appropriately without help. For the C backend, we do tail recursion for singly recursive functions by just jumping back to the start of the function. 3: We pretty print the LLVM IR via Data.Text. Using the LLVM API would speed it up, but there is other lower hanging fruit in the compiler that would give an even greater speedup. WRT static inlining: we've ended up implementing the boxing/unboxing directly in Disciple code using peek/poke primitives. This is what programs compiled with the C and LLVM backends use. The runtime system / C glue code can still use the C static inline version, provided the Disciple version is equivalent. 
&gt; process of just backtesting is straightforward and this project is just the first basics of it I've learned from Haskell that programming is never "just", a qualification you've used twice in your engineering request. Your budget would stretch a lot further with Java programmers -- have you considered Java? [In Haskell we think for a long time before we start typing.](http://redd.it/ert1h) 
It's mostly an AST definition, a printer and a parser.
Cabal improvements should be a really high priority for the Haskell community. It has all promise of a great tool but currently falls far short. My current issues with cabal: 1. I've recently tried to ressurect a cabal environment on a machine I haven't touched for 6 months. It's completely broken, I'm unable install most packages and the error messages can be remarkably cryptic. This is probably a symptom of other issues but will turn users off 2. Installing profiling libraries for currently installed packages is a nightmare. It really should be capable of handling -prof dependencies itself
What I recommend is to install GHC globally on your system (assuming this is possible) and install any packages from Hackage as user. ghc-pkg list will show you a list of the installed packages, separated by location. The packages which come with GHC it's generally not a great idea to try to upgrade. An advantage of this set-up (installing packages as user) is that if something horrible happens to mess up your packages, you can just blow away ~/.cabal and ~/.ghc to get a clean slate, and not have to reinstall GHC. (Though it's usually not necessary to go this far.)
I am on OSX....so it gets installed "globally" I believe. How would I make sure cabal stuffs the ones I have downloaded somewhere else?
Well, you can run ghc-pkg and make sure that there are packages listed under more than one package.conf.d I believe that it should be the default behaviour to install packages as user by default anyway.
That rocks! That is exactly how it works.
Excellent. I started an issue9 parser when the language was first announced but I got side-tracked before finishing it.
&gt; So I did a "cabal update" and it spits out this: really? that looks like the output from â€œcabal **upgrade**â€ which is not the same as â€œcabal updateâ€.
On Arch Linux, I just install everything from the AUR, now. Iâ€™ve removed the "cabal-install" package.
Why base 3?
I've already submitted a very simple app using Haskell (for model / model controller) and Objective-C (for view). It's up and running in the App Store without any problem. 
link?
[app link](http://itunes.apple.com/us/app/clippingdetector/id412382707?mt=12)
Someone should write a Go library for analysis and synthesis of Haskell code, then we can have a meta-orgy of circular synthesis
Ooh, that sounds awesome. Do you think you could put the source online as a kind of base, I know I'd have no clue how to start? Thanks!
The linked project is a fully working demo, almost identical tool chain I used in later projects. You can just open it in xcode and hit build. The submitted app is not designed with open sourcing in mind, but I'll see what I can do...
Ah, I didn't see that the link was to a Github page. I just wanted something to see, so don't worry about the submitted app. Thanks!
Yeah, the hundreds of warnings whenever I install any large package are quite annoying.
Is the shadow stack simply a stop-gap solution or are there limitations in LLVM that prevent you from doing better?
Turn the volume up, the signal to noise ratio is still quite good even then.
&gt; I've learned from Haskell that programming is never "just" ghci&gt; :t Just backtesting Just backtesting :: Maybe Job Better than Nothing, at least?
People may be interested to note that the next release says: $ cabal upgrade cabal: Use the 'cabal install' command instead of 'cabal upgrade'. You can install the latest version of a package using 'cabal install'. The 'cabal upgrade' command has been removed because people found it confusing and it often led to broken packages. If you want the old upgrade behaviour then use the install command with the --upgrade-dependencies flag (but check first with --dry-run to see what would happen). This will try to pick the latest versions of all dependencies, rather than the usual behaviour of trying to pick installed versions of all dependencies. If you do use --upgrade-dependencies, it is recommended that you do not upgrade core packages (e.g. by using appropriate --constraint= flags). So the advice for "newbies" is just to install what they want using `cabal install` and avoid `cabal upgrade` or `cabal install --upgrade-dependencies`. It is not important for newbies to understand the latter two (and thus no need for them to know what core packages are).
Seems very cool - you should reply to the negative review and explain to him how Automator works. :)
I was wondering: Should GHC (or the Haskell Platform) itself be on the Mac App Store?
The examples on slide four suggest an imperative approach to list updates, instead of a functional one. A figure something along [these lines](http://imgur.com/DJpOy) would be a much better illustration of a functional insertion. It's the same kind of figure as appears in the first chapter or two of Okasaki's book.
What is the big point in passing queues between threads if they don't serve as communications channels? What is the intended use case here? Why is all this complexity worth it?
Was a good talk that led to some interesting mid- and post-talk discussion. Nice job, Deech.
Because it's not complexity but simplicity. Many functional datastructures are trivial to implement correctly, and are in any case orders of magnitude easier to implement correctly in a multithreaded fashion (by replacing "in-place update" with "atomic compare/exchange of reference x to f(x)")
Yes you are right. Thanks for the link. Those diagrams did cause some confusion when I was giving the talk.
If you think this is simplicity then we'll just have to agree to disagree. Thanks for your answer.
The shadow stack is there to hold roots for the GC because C doesn't provide a native mechanism for this, and I'm opposed to "conservative collection" on spiritual/religious grounds. It's probably possible to get proper stack maps out of LLVM, but we haven't looked into it yet. 
I can recommend watching http://www.infoq.com/presentations/Are-We-There-Yet-Rich-Hickey is you want to get a better idea of what jkff is talking about.
Nice talk! At least, the slides are nice. When you introduce the difficulty of amortizing costs your reference to threads is confusing and unnecessary - all you need is separate parts of your software attempting to do the dequeue operation. It's pretty easy to make this sort of mistake in single threaded software, when you're passing the structure around to sub-computations. The above comment applies pretty much everywhere you use the word 'thread' in your presentation :-)
Thanks! Much appreciated.
It looks complex on my slides because I am trying to explain each step to people who are not used to lazy evaluation. To see the actual code check out slide 24 of Matthew Brecknell's talk on Functional Data Structures (http://matthew.brecknell.net/talks/bfpg/2009-10/functional-data-structures.pdf). It is quite short.
More jobs, more love :)
jkff is right, you only need to implement these data structures once and you can use them in single threaded and multithreaded contexts. With imperative collections, we've had to implement them twice, and the concurrent collections are much, much more complicated. How are function data structures not simpler?
In general, I agree with you that functional data structures are often simpler, but I would prefer to limit the scope of the current discussion to queues. There are probably situations where functional queues are the better solution, but I can't think of any non-contrived such situations that involve multiple threads at the moment. Can you?
It's relatively straightfoward to get stack maps from LLVM. [Here's an example GC plugin](https://llvm.org/svn/llvm-project/llvm/trunk/lib/CodeGen/AsmPrinter/OcamlGCPrinter.cpp) that emits stack maps in OCaml's frame table format. LLVM doesn't yet compute liveness information at each GC safe point, which also implies that it doesn't track which registers have been spilled. The main consequence is that if your GC involves a copying collector, you'll have to manually re-load registers from GC roots after potential GC points. IIRC, the shadow stack plugin does this automatically.
The code is quite short, but it takes considerable mental energy to decipher how it works. To me, a queue is a simple concept that is easy to visualize, and the natural visualization does not involve scheduling forcings of shared lazy reversals. That by itself pretty much fills up my 7+-2 slots. A lot of chunking needs to happen before this feels natural. Having said that, I do appreciate the hack value and coolness of this.
It's trivial to implement unlimited undo, just keep an old version of the data structure around. Also, the functional queues may require mental effort to understand at first, but once you understand lazy evaluation and the banker's method, it's very natural.
Fair enough. If undo is an important consideration I can see how this might be useful. I am familiar with Haskell and lazy evaluation etc, but not the banker's method so much. It does sound familiar from Chris Okasaki's work, but I guess it didn't stick for me the first time I read about it.
I hope he can escape the *naked mole rats* eventually :-)
`InterleaveT` is really nice.
This would be great.
&gt; Watch for a special announcement regarding Issue 18 soon! oooh! exciting! :)
Or it could be a kick in the arse for the darcs developers. GHC is their most important, or at least best-known, customer.
So here's a puzzle for those who read the first article: why are difference lists a bad choice for implementing FIFO queues? After all, appending to the end and examining the head of a difference list are efficient operations.
Answer in rot13: Lbh pnaabg rssvpvragyl gnxr gur gnvy bs n qvssrerapr yvfg naq trg nabgure qvssrerapr yvfg. Gur vzzrqvngr pbfg bs fhpu na bcrengvba vf pbafgnag, ohg gur qrsreerq pbfg vf yvarne, nf lbh jvyy riraghnyyl arrq gb pbcl gur ryrzragf va gur gnvy bs gur yvfg na nqqvgvbany gvzr. Lbh pna rssvpvragyl pbaireg n qvssrerapr yvfg gb n abezny yvfg naq ercrngrqyl gnxr gur gnvy, ohg guvf znxrf nccraqvat gb gur raq fybj. Vs lbh arrq gb vagreyrnir "nccraq" naq "gnvy", qvssrerapr yvfgf jvyy cresbez onqyl. Edit: Npghnyyl, gur svefg cnentencu vf fyvtugyl jebat; nabgure vzcyrzragngvba bs gnvy (fhpu nf gur bar hfrq ol qyvfg) vf gb erpbzchgr "gnvy" rirel gvzr lbh arrq gb pbaireg gur qyvfg gb n yvfg, nf vf arrqrq gb vafcrpg gur urnq sbe rknzcyr. Qvssrerag cresbeznapr cebsvyr, ohg vg fgvyy raqf hc jvgu onq cresbeznapr.
unfortunately the momentum is with git. git's command set can be a bit ugly at times, but the basic metaphor is sound, and the ecosystem around it is just miles ahead: github is a killer way to get more contributions.
This would be disappointing, but understandable. GHC is a worst case scenario for darcs in a lot of ways: it's a very large code base with a LOT of change history, and apparently they are unable to use the v2 patch format? (Not sure why on that last one, but darcs with v1 patches is not a long-term reasonable solution.) Darcs is absolutely awesome for small projects, like most of the Haskell libraries on Hackage, for example. It's less of a clear choice for larger code bases.
I am certainly not speaking in absolutes but: I believe that the v2 patch history has several weird issues with doing complex merges that don't tend to end well.
There are some events organized around OCaml, but I do not remember of having heard about any kind of FP user group or somesuch. But don't "worry", it's the same everywhere in France, including where I live (Marseille)...
Ahh, finally...
&gt;There are some events organized around OCaml This would be fine, although what I've seen on-line is more like once-a-year meet-ups, rather than an active local community. &gt;But don't "worry", it's the same everywhere in France, including where I live (Marseille)... By the same, you mean that there are no FP user groups? What about at universities and such? Have you had any luck there? I was educated in the US, so I don't have any French academic connections.
There are several academic laboratories interested in FP. You may be interested in the [PPS Seminar](http://www.pps.jussieu.fr/seminaire/) (Paris 7 University - Proofs, Programs &amp; Systems).
Oh, super! This is exactly what I'm looking for.
:) parfait alors !
Wow, I only mentioned the existence of that website to Chris Done. How come it's general knowledge now? Not that it matters much, but there's been made no effort whatsoever to make it a user-friendly or robust application. There is also no guarantee it will stay there for any period of time. Just so you know. :-)
I thought that they migrated a long time ago.
Surely examining the head of a Hughes list (or "difference list" if we must) is expensive. Even with lazy evaluation I think you might have to force the entire list by applying it to [] before you can take the head (I'd have to work this out on paper to give a definitive answer, so I'm open to either confirmation or contradiction...). Anything except building is out of spirit with a Hughes list - if you want introspection a Join list is surely better. A Join list is a binary tree with a list API, plus operations for snoc etc.
Well, you do have to convert the Hughes/difference/functional list by applying it to [], but I'm pretty sure that lazy evaluation gives you efficient access to the head nonetheless. (Now watch as I'm proved wrong, which is extremely common right after I make a definitive claim about Haskell w.r.t. performance. ;-)
Hope this happens at last.
My experience with git is: There's no relation between what commands are named and what they do, but once you found out what they do, they're quite pleasant to use.
That monad 'm' gets threaded through a LOT of definitions (the 'm' in the Player class). Do you really need that level of generality? Is that monad ever going to be a Maybe or a List or a Cont?
Just out of curiosity, can you tell me where you'll be working?
i want the ghc team to use whatever tools required to make ghc better. most ghc users are probably not darcs users and never will be...and hence stand to gain nothing by continuing to have ghc developers use darcs. git won. it won because it's better. i've never understood the argument that darcs is "better for small projects"....??? i think thats a nice way of saying darcs can't compete with git. 
not sure what good it would do...there will be little motivation for the ghc team to switch back to darcs. the gap between git and darcs will only widen
Well, that lets me specify that I can run a game with a set of pure players (who have no dependence on the specifics of `m`), or I can run a game with some players that depend on IO.
[www.haskellers.com](http://www.haskellers.com) shows its registered users on a map, so you can quickly find a few Haskell people in France or in Paris there.
There's no doubt the polymorphic monad is more general. But mind the cost of generality. (So few Haskellers do!)
+1 I appreciated this short informative statement.
Not sure yet. I moved back to France for family reasons, not because I had a job lined up. Why do you ask?
Jâ€™utilise Haskell et vis dans la rÃ©gion parisienne mais ne suis pas actif (Ã©tudiant, mais je ne lâ€™utilise pas scolairement). Mon niveau est loin dâ€™Ãªtre extraordinaire, en plus.
I really liked Oliver Danvy's work on CPS transforms, when you replied with the link before I somehow didn't register that he was the one giving the talk, this is great, I'll definitely be there. Will you be there as well? 11h is a bit early for a beer, though.
No - its me that's wrong. I was thinking head would have to force at least the list shape *from the right* but not is contents, however this is fine: hughes :: [Int] -&gt; [Int] hughes = (1:) . undefined . (3:) go = head $ hughes [] 
How could it be considered spam without a link?
corrected ;)
We don't really need a kick in the arse to know we need to improve, but we do need people or it'll be slow...
Why git? Why not use a VCS that is pleasant and easy to work with, like Bazaar or Mercurial?
I never understood this mentality that Git won anything. I used Git for quite a while before switching to Darcs. They say Git fits whatever workflow you want, but it doesn't support exactly the workflow that convinced me to use Darcs (lots of cherry picking). The argument that Darcs is "better for small projects" is only a concession that Darcs doesn't yet scale well to GHC-sized projects; it says nothing about the quality of the tool or its competitiveness with other tools.
&gt; the basic metaphor is sound Git still doesn't satisfy the merge law: &gt; Merging change 1 then merging change 2 is equal to merging both change1 and change2 together. What did Dijkstra say? "people willing to trade correctness for speed deserve neither and will lose both", or something like that.
Its author already posted it here 7 days ago. I'm impressed by the number of already proposed positions.
http://www.reddit.com/r/haskell/comments/evi5d/job_board_for_functional_programmers/ shouldn't reddit block submissions with the same url?
Can you please expand on this a little bit? A reference explaining this "merge law" and how git breaks it would be appreciated.
See http://web.archive.org/web/20070603113858/zooko.com/badmerge/simple.html and its related links.
'Pleasant' and 'easy' are both highly subjective concepts, don't you think?
I use Visual SourceSafe. It's kewl. GHC guys are M$ aren't they, should eat their own dog food amirite?
Please keep content-free image links out of `/r/haskell`. There's [another forum](http://www.reddit.com/r/programming) for that. Mods, can we get a definitive policy on this, enforceable by banning?
There's subjective, and there's subjective. I've used all four major DVCS tools (Bazaar, Darcs, Git, Mercurial), and while there's a bit of wiggle room for certain features, the general pattern is clear: Darcs is easy to use and has a great UI, Bazaar and Mercurial are about even, while Git is horridly difficult. In fact, Git is so bad that I'd rather use SVN, or even just tarballs+diff, than actually try to remember which of its million misnamed commands does what I want to do at any particular moment. Given the relative number of tutorials for each tool, I suspect others agree with me. Consider the literally hundreds of Git tutorials, compared to the half-dozen for the others -- that tells you that Git is hard to use, and what's more, that the Git developers don't really care. Of course, a nice UI isn't everything -- I migrated my repos away from Darcs because, as easy as it is, it's simply too slow and feature-poor compared to the other tools. But when there's three tools, with basically the same feature set/performance, and one is a terrible pain to work with, why would you ever choose to switch to it?
Thanks. Interesting. So how come git (and others) haven't addressed this problem given that it has been known for a while and that it has a relatively easy solution?
Most people you talk to won't even acknowledge the existence of the problem. They just say the behaviour is documented as if that is some excuse to violate the merge law. 
Yeah. That sounds like some developers I know.
Is this a cardinal example of [high prices signalling better quality](http://en.wikipedia.org/wiki/Pricing#Premium_pricing) in view of the free job boards at cufp and haskellers.com? In any case I salute sgmurphy for obtaining almost 20 job listings so quickly. Should you ever run a marketing course explaining how you pulled off that feat, sign me up! 
According to the post, the GHC team currently avoids darcs' merges entirely, so I doubt that's the issue.
How do you explain, then, that the GHC guys seem to be so afraid of darcs merges that they try not to do them?
haskellers.com is haskell only. cufp is really only those types who are up on the cufp, and by implication the icfp. neither has made efforts to actively solicit listings.
git is faster and supports large projects better (with large projects, you often want a single working tree, and git is better at this model than the both).
I'm interested in whether there are any statements on this by any git developers. Since git already has support for different merge algorithms I wouldn't expect them to object adding one that satisfies the merge law. It might be that git's way of storing a repository isn't optimized for such an algorithm and it would be slow, but it sure would be useful as a fall back if the default merge algorithm fails.
Does it ensure regular work for programmers with moral fibre?
&gt; I'm interested in whether there are any statements on this by any git developers. I'd be interested in this too. I don't know if this problem has been presented to the git developers in terms of the merge law. When Zooko has argued this in the past, I believe he was arguing the darcs "algorithm" vs the git "algorithm" rather than appealing to the merge law. It's a bit hard to determine what is or is not a correct merge algorithm. For example in http://tahoe-lafs.org/~zooko/badmerge/concrete-bad-semantics.html Peter Backes argues the result of Git's behaviour is correct while zooko argues the result is wrong, even it happens to fix the code. I prefer to argue from the merge law because then you don't even have to bring an alternative algorithm into the picture. If you return to Peter Backes you can tell that git must be broken in some way because if one processed the git merge via two steps instead of one, you would get a different result (the "darcs" result) than Backes so-called "right" result. Since both answers cannot be right, git has to be wrong with at least one of them. &gt; but it sure would be useful as a fall back if the default merge algorithm fails. Be aware that the git algorithm might not fail. Rather it might simply give the wrong answer.
Well, you didn't demonstrate that I was right either, and I am wrong. ;-) Consider import Data.DList toDList :: [a] -&gt; DList a toDList = foldl (\xs x -&gt; xs `append` singleton x) empty In this case, repeatedly taking the head of a DList created in this fashion will be linear with respect to the length of the list, at least on GHC, presumably because you are repeatedly traversing the closures that represent function composition. The analogous function with foldr will offer a constant time head, though. I wonder if you could have an implementation that would rewrite closures representing function composition so that they associate to the right, which might allow for constant time heads. I haven't thought through what other consequences this implementation choice would have, though.
Best thing about this is that inevitably someone will end up writing a Haskell version of git ;)
I think this applies to any merge algorithm (except for an always failing or fast-forward only merge) so manually reviewing the merge output is inevitable. In that case I'd argue for giving the user choice, as an algorithm which matches the user's intuition increases the chance of the user identifying places where the merge could be wrong.
&gt; I think this applies to any merge algorithm What applies to any merge algorithm? Darcs (and Codeville)'s merge algorithm satisfies the merge law. Edit: I also think it would also be trivial to make the merge algorithm in git satisfy the merge law by simply merging an branch by incrementally merging each change from the other branch. I don't know how fast it would be, but it would satisfy the merge law.
and yet a few days on, CUFP has 3 openings and haskellers.com has none, where functionaljobs has 20. Clearly he's doing something right.
I want a great ghc much more than I want a great darcs. darcs is nice, but the infrastructure git provides (easy forking, branching and merging, as well as github) make it the obvious choice for any large scale open source project. I will certainly be much more willing to contribute to GHC if it is moved to git and hosted on GitHub. I've only recently started using git, but I've already well and truly 'seen the light' so to speak.
sounds like fun!
I'm in the odd position where I love github but hate the user experience of git with a fiery passion. At this point I think I've developed a mental block against learning it properly.
If a project is on github that lowers the barrier to contributing. This matters right now, even if in the future there may be better options than github. I tried using darcs again, but I didn't remember what all the commands were and found it harder to use than git which I use day to day so I switched back to git. This had nothing to do with darcs vs. git but everything to do with momentum.
Not exactly - we avoid pushing conflict resolutions, but we still have to do merges.
Well, for the record, if he'd be interested in having larger coverage of his job listings, I would mirror them (or just give him a link) from Haskellers.com. I don't have time to do the footwork on this right now, but hopefully won't be quite as busy in the near future.
That's pretty interesting you can feel the ads coming. Perhaps you're unfamiliar with our business model? Tens of thousands of people pay us real money to host their code privately. I hope if GHC migrates to Git, they'd at least consider setting up a GitHub mirror. Erlang has received an large boost in contributions after having done just that: http://github.com/erlang/otp 
The implicit conclusion is "darcs gets merges right and git doesn't". I don't believe this is true.
I'm a haskeller and am also living in Paris, but I don't know of any active FP community here. My job unfortunately doesn't involve writing Haskell either, but if you find some connections I'd be highly interested in it!
Well, we could always try to start one up. It doesn't need to be terribly serious -- it could be something as simple as getting together and having a beer every other week or something, even if it's just a few of us it could be fun. Plus, with INRIA in Paris, there should be a lot of seminars and CS grad students with research interests in type systems. Having a small informal community could help us all stay abreast of what's going on in FP in Paris. I don't know as much about the ML family of languages as Haskell, but it's not like O'Caml is radically different conceptually from Haskell. People interested in one are likely to be interested in the other, we all just end up specializing. So there's no reason in my mind to be too Haskell specific.
Thanks for this, I contacted some people in the area. No luck on a Haskell or FP users group, but maybe there are enough people to start one ...
Je crois pas que je vais aller Ã  celui lÃ  mais je te "friend" !
&gt;Mon niveau est loin dâ€™Ãªtre extraordinaire, en plus. C'est seulement une question de s'y mettre, ne t'en fais pas, Ã§a viendra. Si tu fais des Ã©tudes de maths ou de sciences informatiques Ã§a sera plus facile, c'est sÃ»r. Mais mÃªme la philo peut Ãªtre trÃ¨s utile en ce qui concerne la PF â€” ceux qui sont fort en logique formelle vont forcÃ©ment avoir moins de mal Ã  piger tout ce qui est la thÃ©orie des types, etc. Il y avait un mec â€” j'oublie si c'Ã©tait sur la liste Coq club ou alors Agda â€” qui faisait des Ã©tudes de philo niveau masters, mais qui n'avait jamais fait d'Ã©tudes de maths ni de sciences informatiques, qui Ã©crivait des programmes/dÃ©monstrations avec des types dÃ©pendants sans aucun soucis... tandis que moi Ã  l'Ã©poque j'avais beaucoup de problÃ¨mes Ã  digÃ©rer tout Ã§a. Enfin bref c'Ã©tait sans doute aussi parce qu'il Ã©tait beaucoup plus intelligent que moi, mais on voyait tout de suite que sa formation en philo l'aidait beaucoup Ã  comprendre comment toutes ces choses fonctionnaient. Donc tu vois, mÃªme si tu fais des Ã©tudes qui ne sont pas directement liÃ©s Ã  l'informatique, tout Ã§a peut s'apprendre. Bon courage !
Bon, la prochaine fois alors !
No, there are plenty of legitimate reasons to post the same thing more than once.
Glad to see the end of datatype contexts.
Ah great, thanks.
Doesn't look like it to me...
...feel free to kick the community's arse.
Mostly because the problem apparently doesn't happen often in code. If it was a common problem, they would have jumped years ago on fixing it - but currently it does not seem to be an error that creeps up much. If it were then you would expect a lot of code merges do the wrong thing and end up with trees that can't be compiled or bugs silently introduced into code (oh, the horror!). The viewpoint is that not all mistaken merges are necessarily errors.
btw, is there a ghc switch to warn about (or turn into an error) occurrences of data contexts?
Why would it be a kick in the arse? The *competition* is not even fair since git has all the momentum: (Disclaimer: Long time since I used Darcs for anything, though I tried to cull information from the net to make sure my answers are somewhat correct. Please correct me.) * In Git I usually juggle 80-100 branches. In Darcs, it looks like I would need to have 80 directories each with their own checkout. Even though I can limit the file system waste with relinking, this is much more irritating to work with than a `git checkout`. Darcs could easily implement this. * The main difference is that in Git, everything branch-wise is explicit whereas in Darcs much of the stuff happens implicitly as a virtue of the merging strategy (which is awesome). Yet, the explicitness of git doesn't seem to hamper its users that much and the "incorrect" merges seem to happen rare enough that people do not care. Darcs can't claim victory just by being written in Haskell. It has to implement the demands of its users to do so. And currently, git implements the demands far better. Call git users "wrong". Claim that their merging strategy is not logically or algebraically sound. Git users don't care about this before it has hit them in their own project. Basically, git is this 80% solution on merging that still wins because everything not merging is there in some obscure command if you dig far enough.
&gt; Why would it be a kick in the arse? &gt; It has to implement the demands of its users to do so. That's why. Whether the darcs authors act by fixing all the fundamental issues themselves before GHC switches, call for the community to supply man-power, or just vow to re-order the roadmap, GHC threatening to move away from darcs is a wake up call. Presumably it'd be sufficient to priorise everything that keeps GHC switching over to v2. I'm quite sure that, inside the Haskell community, darcs has the potential and position to quickly take up momentum. Provided it gets some love in the exact right spots.
I was talking about "Be aware that the git algorithm might not fail. Rather it might simply give the wrong answer." interpreting "the wrong answer" as "code that doesn't do what it should" not as "not satisfying the merge law". That probably wasn't clear in my post.
Just wondering... I occasionally go to paris for FP related reasons, and i was hoping you might be part of one of the labs i regularly go to. (like [this](http://www.inria.fr/centre-de-recherche-inria/saclay-ile-de-france) one)
Oh, well, even if I were working, I probably wouldn't be at such a lab, as I don't really work in CS anymore. Are you French from outside of Paris, or from somewhere else?
By the way, I really wish that 20 FP jobs all got listed on the same day on a brand new site, but my instinct says that those were *not* 20 employers who all stumbled upon his site.
I'd call "doesn't yet scale well" a (true) comment on both quality and competitiveness.
 *Main&gt; solution 91 1 1 I don't think your solution is correct.
http://www.haskell.org/haskellwiki/Euler_problems/1_to_10 This page has solutions for Euler problems #1-10. There are more solutions on other pages. The solution for this problem (#8) is pretty nice on there, so hopefully that is what you're looking for.
We're talking projects of GHC size here. The *vast* majority of projects are not this big.
Here's my take on the problem: import Data.Char import Data.List solution k = maximum . map (product . take k) . drop k . reverse . tails . map digitToInt . show
That's because I've put corrected link in a fresh post because of reddit limitations. You can't miss it.
The owner offered free listings for a two week period before the site launched.
I'm pretty sure they weren't free, but discounted with a money back guarantee.
`-XNoDataTypeContexts`
I canâ€™t wait for Warp to be ready! :D
 import Control.Monad import Control.Monad.Instances() solution :: Int -&gt; String -&gt; Integer solution k = maximum . scanl (uncurry . ((*) .) . div) 1 . join (zip . ((take k (repeat 1)) ++)) . map (read . return) test = solution 100 . take 100000 $ cycle "123789456" This one is faster for large k as it keeps a running total. Sadly faster means uglier as usual.
I'd appreciate a field for receptivity to teleworking, for those like me who live in the more far-flung regions of the world.
This was my non-general (but dead simple) solution largestProductOfFive :: String -&gt; Int largestProductOfFive (a:b:c:d:e:xs) = max (productâˆ˜map digitToInt $ [a,b,c,d,e]) (largestProductOfFive (b:c:d:e:xs)) largestProductOfFive _ = 0 
This may sound harsh now, but if you can't figure out darcs, you're in no position to hack on ghc.
I forgot to mention it in the post: you can play with the code on Github right now. You might consider [my repo](https://github.com/snoyberg/warp) to be trunk and [Matt's repo](https://github.com/softmechanics/warp) to be more experimental. He has a number of commits with possible performance tweaks we're going to be testing out.
 import Data.Char productsByN xs = [] : iterate (zipWith (*) xs . tail) xs solution k = maximum . (!!k) . productsByN . map digitToInt . show 
[https://github.com/kfish/ght](https://github.com/kfish/ght) Patches welcome :)
stfu. Besides, MS themselves have deprecated VSS.
This is the most idiomatic way to write it, I'd say. One comment: `drop k . reverse` to drop from the end of the list (with the order of the list not mattering here) is concise, but it's not lazy and it has to allocate memory to hold the entire spine of its argument at once. You could instead use `dropFromEnd k`, where `dropFromEnd` has the slightly odd-looking definition dropFromEnd k xs = zipWith const xs (drop k xs) This only needs to keep `k` cells of `xs` in memory at once, so it runs in constant space (for fixed `k`, with the length of `xs` varying). If you wanted to process very large numbers, using this `dropFromEnd` would decrease space usage by a large constant, since the in-memory representation of `show x` is many times larger than that of `x`. Also, I noticed when testing this that `tails` is not as lazy as it could be: tails :: [a] -&gt; [[a]] tails [] = [[]] tails xxs@(_:xs) = xxs : tails xs The result of `tails` is always a nonempty list, but `tails` pattern matches on its argument before returning anything. Has this been observed before?
or you could just use Haskell's library for analysis and synthesis of Haskell code and then do away with the Go step.
dammit.. s/someone/someone's/
This is actually a very interesting type of monad. A related one is the monad of terms (or AST) over a type of free variables X. Then if one considers the algebra with constructor abst :: Term (Maybe x) -&gt; Term x Which can be seen as *binding* the Nothing variable, then you have a monad that accurately describes an algebra with bound variables! &lt;/derail&gt;
Bingo!
As the founder of the Ghent Functional Programming Group (now almost a year old), I'd say: go for it. Just set a date and send a mail to haskell-blah and any computer science mailing lists of universities you can find and you'll be surprised by how many people show up!
Thanks for the encouragement! What did you do on your first outing? (I was in Ghent a couple of weeks ago, by the way. Beautiful city.)
Ups, replace 1..k by 0..k-1 and it works.
Thanks for the idea of using tails, tails digits is exactly the same as map (\`drop\` digits) [0..]
What is the drop k . reverse good for? Seems to work well without. Edit: nevermind, I see. All lists must have &gt;=k elements.
I've never tried git on the GHC repo, but my understanding is from the people that have tried it that is substantially faster. The main reason I believe darcs doesn't scale well is in the way it copes badly with long-running branches and the conflicts you get when you try to remerge them. Those kind of long-running branches with substantial changes typically only arise on large repositories. It both runs slowly when doing the merge, and produces marked-up conflicts that are hard to understand. Annotate is another significant example where performance is really awful - its slow in absolute terms, and linear in the number of patches in your repo so as your repo gets more history it quickly get to a point where it's very hard to use. 
Silly that I didn't test with zero, as obviously dividing by zero will not work.
It's not about _being able to_ figure out darcs, it's about being willing to put in the time to struggle with it. If someone wants to submit a quick patch or even work on a major change, they have a finite amount of "fuel" to begin with. When they run out of it, they're going to give up and work around the issue by not using Haskell or emulating the proposed GHC change in their user-level Haskell code. Ideally, the vast majority of their fuel will be spent actually contributing to GHC, but if they have to go learn an uncommon VCS and struggle with its idiosyncrasies, that will take up more fuel, and leave them more likely to just give up rather than put in the extra effort to contribute.
So, if someone fixes a bug and sends in an old-fashioned diff against stable, the ghc devs are going to reject it outright so that darcs can be blamed? As using diff is certainly what I'd do, instead of bitching about how much better everything'd be if only everyone used git. It's not like git would be a magic bullet that solves all merge problems, it has it's own strengths and weaknesses, just like darcs, and I would _very_ much prefer people asking the darcs devs how they can help out. Darcs is an asset to the haskell community and ghc is an asset to darcs, I'd expect at least enough loyalty to refrain from jumping the git bandwagon and shouting "hang darcs higher" before there's even a comprehensive list of things the ghc devs need fixed.
On the darcs side, we do know roughly what they need (it's the motivation behind the upcoming rebase feature, for example), and most of what we're doing are things that would benefit GHC in the end. But there's only so fast we can move and many of the issues they have, like with conflict handling, are hard problems with no obvious solution in sight.
Huh? I'm saying that they're free to pick what they want, and there won't be rejecting of patches or any of that crap. However, if the project wants more contributions, it should make the process as painless for contributors as possible. Loyalty plays no part. We're getting lots of new people interested in Haskell these days, but often that isn't accompanied by an interest in darcs. Insisting on a version control system that the vast majority of people don't know, simply out of some (odd to me) notion of loyalty, is just going to unnecessarily restrict contributions to the project from those who are capable of contributing but unwilling to put in the time to learn a new tool.
I was using "loyalty" in the same sentence as "no blind bashing", if you read again. People new to Haskell aren't going to hack on GHC, so that's a non-issue. And while darcs has its issues, so would switching to git introduce a lot... I'm not sure there's even any git repo the size of ghc out there, certainly not the linux kernel, which is tiny, in comparison.
From the point of view of git, or any system that requires an explicit commit for a merge (i.e. anything but darcs), the merge law doesn't matter. The system produces some merge result, which might or might not be "right", and it's down to the user to check it. Ultimately there will always be mistaken merges in any system (including darcs) it's just a trade-off to decide how common they are/under what circumstances they happen.
My first stab at this: "Type classes specify a set of capacities which may be associated with particular combinations of types."
&gt;A typeclass is a sort of interface that defines some behavior. If a type is a part of a typeclass, that means that it supports and implements the behavior the typeclass describes. A lot of people coming from OOP get confused by typeclasses because they think they are like classes in object oriented languages. Well, they're not. You can think of them kind of as Java interfaces, only better. from [LYAHFGG](http://learnyouahaskell.com/types-and-typeclasses#typeclasses-101)
Typeclasses are the nearest code equivalent of the English suffix -able. Oh, that's a bit rough around the edges, I'll admit, but it maps as well as can be expected for a _plain English_ explanation (mathematical precision not available by definition).
Short version: &gt; Type classes describe similar behavior across different types. Type classes are also used to describe similar behavior across different combinations of types. Long version: &gt; Type classes describe similar behavior across different types. For example, you can treat integers, floating-point numbers, and rational numbers similarly -- you can add, subtract, and multiply them, among other things. They are all part of the "number" type class. &gt; &gt; Type classes are also used to describe similar behavior across different combinations of types. For example, the "convert" type class could describe conversion between two arbitrary types. You could implement convert for integers and floating-point numbers, integers and rational numbers, integers and integers, rational numbers and strings, and so on. 
(Haskell newb here). Sometimes, before we work on data, we want to know if it can be passed to certain other functions successfully. For instance, before we sort something, we need to know if it can be compared to similar objects. Haskell's typeclasses allow us to specify this behaviour in advance, so we can simply say "this function accepts arguments of type such-and-such" and be sure that we can perform certain functions. The compiler will do the checking for us. This is similar to Java's concept of "interfaces" but is more powerful. In Java, an interface is associated with one object. But in Haskell, a typeclass can be defined over several items, and thus express relationships between them.
&gt; From the point of view of git, or any system that requires an explicit commit for a merge (i.e. anything but darcs), the merge law doesn't matter. This is exactly what I mean by most people won't even acknowledge the existence of the problem.
A set of types for which certain operations are defined?
Short answer, although not revealing: A type class is a function from types to values. For example, type class Eq takes a type t and associates a value of type t -&gt; t -&gt; Bool. It gives integer equality function for integers, float equality function for floats etc. As value functions, it can be partial, and "instances" are those types where it is defined. A typeclass with more than one member can be packed to a typeclass with one member. A typeclass with more than one parameter can be thought of as a multiparameter function. This isn't revealing - it doesn't convey that the dictionary is implicitly passed and how a type class resembles an interface.
I like this one. You could follow it with a sentence like: &gt; A type becomes a member (an "instance") of a typeclass when those operations (called "methods") are defined for that type in a formal "instance declaration".
That's also how I would start explaining them. Once this is understood, one can then go on and explain that it doesn't really have to be a set of *types*, it can also be type constructors, And it doesn't have to be a *set* of types, it can also be a relation between types.
listSum (tail ns) should be a number, but you've used it as if it were a list, as the second parameter to (:). If you were to replace the : on the right hand side of the = sign in the last line with + then it would compile. However, it would still contain a bug (think about the list of length 1 case). I recommend not using head and tail, and sticking entirely to pattern matching.
Thanks a lot! Seems obvious now.
FYI, stackoverflow is a better forum for this type of question. Reddit excels mostly in reporting news/blogs/project announcements and subsequent complaining by the masses.
Yes, but it looked waaay too easy for StackOverflow. Those guys are picky.
I wanted early exits and I didn't fancy writing a findM/A. This code seems quite elegant and very readable to me. I don't think the alternative would match this. It's quite impressive how close you can get to making Haskell look like an imperative language. EDIT: change runContT' to be runContT' = flip runContT return
The obvious place to ask this question would be the haskell IRC channel. If you're new to haskell and you're not in #haskell on freenode, you're doing it wrong :)
I just came upon this library as I was about to do some fairly simple linear algebra work - comparing 2d vectors in order to cluster some line segments based on which are most nearly parallel. It looks like it would work for me, but its the first time I've encountered a library that seems to do everything I need it to, and then paused to ask myself, is this *too* generic for what I'm trying to accomplish? Overkill? On the other hand, it would probably suit someone working in a field of high math I'm struggling to identify. Group theory perhaps? (I'm an FP neophyte since I read more Haskell code than I write. Most of my "real" work is done in imperative languages) EDIT: I see that the article says the field is "Quantum Algebra" but that doesn't help me much. I've learned to distrust the geography of higher math. I'm guessing "Quantum Algebra" has very little to do with the algebra I learned in grade school. I could use some sort of "tree of math" with "you are here" sign. 
Not sure why you have those calls to `head` and `tail` in there. As well as the incorrect use of `(:)` on the RHS which cgibbard has already pointed out, and that results in a type error, there is a runtime error: `head` and `tail` are not defined on the empty list, so the function will throw an exception when called with a list with an odd number of elements. A `sum` function on lists, defined recursively, would normally look like this. sum [] = 0 sum (n:ns) = n + sum ns But of course a better way to do it is with a fold. sum = foldl (+) 0 
True. Thanks a lot for the input!
What would be a short direct answer in plain English to the question "What is an interface, exactly?" Are interfaces really simpler than type classes? Edit: Actually, based on the [examples here](http://www.hi-language.org/examples#TOC-Defining-and-Using-Interfaces), it looks to me like what Hi supports is actually type classes.
Let's back up a little bit. Should we first give a plain English definition of "type"?
laziness++
change the colon in line three before listSum(tail ns) to a plus sign change (n + head ns) to (n + (head ns)) (remember that plus will look to the right of it for a number or an expression that evals to a number and head by itself doesn't eval to a number)
Laziness does what you want here, you don't need explicit continuations.
For the first point, it seems closer integration would be most useful. For example, think about old Java programs passing around Objects and using a bunch of casts. It would be clearer to tag some variables explicitly as dynamically typed, and dispense with the casts. The problem is, some early proposals for cast insertion designed to allow a few untyped values in mostly-typed code also allowed obvious nonsense like the above to compile. This is handled in [Gradual Typing for Functional Languages](http://ecee.colorado.edu/~siek/pubs/pubs/2006/siek06_gradual.pdf) (another of [Siek's papers](http://ecee.colorado.edu/~siek/publications.html) describes object types). The second point is about being sure that the integration doesn't compromise type safety in the *typed* part of the program. Coercing a dynamic value to a higher order functions or a factory object involves wrapping values in contract checks rather than just a one-time tag check, and it takes a bit of work to be sure the runtime checks correspond appropriately to the type system. For me, the point appears to be selling types to communities using dynamic languages. They seem to especially dislike awkward constructs(who doesn't?), and cherish the potential for great flexibility even if it's not used all that often (e.g. monkey patching). A system with full type inference, and easy access to fully dynamic variables might fit the bill - effortless and a helpful in ordinary cases, easily avoided when necessary.
NO QUOTES! ...I'm postponing my upvote until you fix that.
sounds interesting... would you care to paste a simple example of this anti-pattern for Python?
This. Visit the #haskell IRC channel. The guys over there are mostly friendly human beings, so don't be afraid to ask such questions over there. Also, you'll get an answer to almost all your Haskell related questions. And don't forget: there aren't dumb Haskell questions. 
Good idea, but Mac Apps are not allowed to bring their own installer. The app store is basically for ... apps.
Let's examine why this is a bad idea, shall we? sum = foldl (+) 0 Recall that (from the Haskell Prelude) foldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a foldl f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = lgo (f z x) xs For ease let's rephrase this directly, since `lgo` is an optimization: foldl f z [] = z foldl f z (x:xs) = foldl f (f z x) xs Let's use `[1, 2, 3]` for the sake of exposition. Of course initially, `sum` is left unevaluated, due to laziness. But at some point, something requests an `Int` out of the thunk, and we have the following trace of execution: sum [1, 2, 3] = foldl (+) 0 [1, 2, 3] = foldl (+) (0 + 1) [2, 3] = foldl (+) ((0 + 1) + 2) [3] = foldl (+) (((0 + 1) + 2) + 3) [] = (((0 + 1) + 2) + 3) = ((1 + 2) + 3) = 3 + 3 = 6 Yes, this is the correct answer (the evaluation strategy doesn't matter when dealing with a total program, like this one) but notice how it evaluates: no addition is done until the end. Laziness builds up an expression `(((0 + 1) + 2) + 3)` that it only breaks down into a single value at the end. This expression is not of constant size -- it is proportional to the length of the input list, and so running your `sum` on a list that has 10,000 elements will require that a thunk made up of 10,000 function applications be built before it is reduced. Thunks take up memory. This is what we call a space leak. Instead, the correct way to do this is to use `foldl'`, which is strict in the accumulator. `foldl'` is defined almost the same way as `foldl`. Observe its definition, from `Data.List` (I've factored out a similar `lgo` optimization, here, for clarity): foldl' f a [] = a foldl' f a (x:xs) = let a' = f a x in a' `seq` foldl' f a' xs Here, `seq` reduces its first argument to WHNF (essentially evaluating until the first constructor in the thunk, which in the case of `Int` means an actual number, but can leave unevaluated thunks in more complex data types). Check out the same evaluation strategy: sum [1, 2, 3] = foldl' (+) 0 [1, 2, 3] = let a' = 0 + 1 in a' `seq` foldl' (+) a' [2, 3] = (0 + 1) `seq` foldl' (+) (0 + 1) [2, 3] = foldl' (+) 1 [2, 3] = let a' = 1 + 2 in a' `seq` foldl' (+) a' [3] = (1 + 2) `seq` foldl' (+) (1 + 2) [3] = foldl' (+) 3 [3] = let a' = 3 + 3 in a' `seq` foldl' (+) a' [] = (3 + 3) `seq` foldl' (+) (3 + 3) [] = foldl' (+) 6 [] = 6 Functional programmers coming from an eager evaluation language like ML or Scheme are often told to use `foldl`, because it is based on a tail call and thus should use constant stack space. In a lazy language, however, this breaks down, because a whole list of thunks is built up and *then* evaluated, which means that if you look at the traced evaluation, you can see the tell-tale "stack curve" you'd normally expect from a right fold. The moral here is that you basically should never, ever use non-strict `foldl` -- I can't think of any situation where you would want to. It will always take up a bunch of memory. Can anyone think of a situation where you'd want a non-strict left fold?
If we're looking for procedural language analogs to classes, I'd say the first place to look would be [concepts](http://en.wikipedia.org/wiki/Concepts_(C%2B%2B%29) . This is because they allow you to place constraints on the type variables used in something that's polymorphic (templated). I suppose java's interfaces do the same for their implementation of generics, but you need to add type parameters in order to get more than one parameter. Having one of the parameters be implicit in the inheritance hierarchy is ugly - concepts avoid this. Actually, with type families, haskell classes gained some of the power of C++'s concepts with regards to being able to refer to type variables and type functions (via template specialization).
Looks like a real Coqfest.
I wouldn't have a list of booleans, I'd end up with a list of actions that return boolean, getPixel32 is an IO action, unless you mean I sequence the list first or write explicit recursion with a local function? I want to avoid the latter.
You could just fmap your pure computation onto the actions and get a list of say IO Bool, then execute until you hit True.
something like this? overlap (lhs,(lx,ly)) lrect (rhs,(rx,ry)) rrect = withLock lhs $ withLock rhs $ orM $ forM [(i,j) | i &lt;- [xStart..xEnd], j &lt;- [yStart..yEnd]] $ \(px, py) -&gt; do Pixel p1 &lt;- getPixel32 lhs (lxOffset + px) (lyOffset + py) Pixel p2 &lt;- getPixel32 rhs (rxOffset + (px-xStart)) (ryOffset + (py-yStart)) return (p1 /= black &amp;&amp; p2 /= black) where orM = or &lt;$&gt; sequence
`sequence` will execute the whole thing, I believe, as IO is a strict monad. You could also try `unsafeInterleaveIO`
&gt; change (n + head ns) to (n + (head ns)) unnecessary as function calls have higher precedence than operators
&gt; Can anyone think of a situation where you'd want a non-strict left fold? Yes, when one wants to explain a technique to an obvious beginner without confusing them with funny function names that include punctuation.
Thats not bad actually...showable, equatable and orderable spring to mind. (And they are all defined at dictionary.reference.com; I checked) I never really thought of it that way before.
&gt;Yes, when one wants to explain a technique to an obvious beginner without confusing them with funny function names that include punctuation. You're being snarky, but you shouldn't. There's nothing intrinsically complicated about telling someone to use `foldl'` instead of `foldl` (the details can be left out) and it saves us (the Haskell community) a lot of pain down the road to not pretend like call-by-need doesn't behave differently than call-by-value, particularly where beginners are concerned. The non-strict semantics of Haskell are extremely important, they are not an implementation detail. If you intend to learn the language, you cannot ignore laziness, where it helps or where it hinders. Furthermore, beginners are not stupid. They are simply less knowledgeable than you, at this particular instant in time, about a specific domain of discourse (in this case Haskell). Never confuse lack of knowledge with lack of intelligence. Gauss most likely didn't know Haskell.
No need for any unsafes. orM :: [IO Bool] -&gt; IO Bool orM [] = return False orM (m:ms) = m &gt;&gt;= \b -&gt; if b then return True else orM ms Also, forM uses sequence underneath. You need to use `flip map` instead to make it lazy, and as a bonus it now typechecks against the new orM definition.
&gt; The non-strict semantics of Haskell are extremely important, they are not an implementation detail. Certainly. But a deep understanding doesn't come immediately, and I have seen beginners in many disciplines, including Haskell, be overwhelmed by an initial flood of concepts. The `sum` function is often used as an example because it offers an opportunity for learning incrementally. Initially, one implements a recursive solution. Then one learns about left and right folds. The need for a strict version of `foldl` becomes apparent as one begins to understand the language's evaluation semantics better. Saying "Use `foldl'` not `foldl`" while leaving out the details is not particularly helpful. It means they're less likely to write a program with that particular space leak, but as I'm sure you'd agree, that's not the goal: the goal is to learn how the language works, and part of that is learning where space leaks might be a problem in general, not just some special cases. Perhaps it's annoying to see yet another person saying "Help! My program with `foldl` ran out of memory! What do I do?" But at least when that happens there is an opportunity to explain what has happened and why at a relatively early stage, where the answers are stock and the sample programs are simple. If we tell everyone to use `foldl'`, leaving out the details as you suggest, then people will not run into this issue until later in their experience with the language, and their broken program is likely to be larger and more complex, and it will be correspondingly more difficult and more time-consuming to explain what has gone wrong and why.
Yeah I was trying avoid writing auxiliary actions like this, would this really be more efficent than using the continutation based method as sigfpe mentioned?
&gt;But a deep understanding doesn't come immediately, and I have seen beginners in many disciplines, including Haskell, be overwhelmed by an initial flood of concepts. Definitely, this can happen. Still, I'm not sure it's a good idea to protect pre-emptively from the information. Striking a balance is important, though, I think we can definitely agree on that. &gt; The need for a strict version of foldl becomes apparent as one begins to understand the language's evaluation semantics better. There's a reason that classic texts like SICP treat evaluation order very early on: it's a fundamental concept. Furthermore, it is one of the *major* differences between Haskell and virtually every other mainstream language. We say that Haskell is lazy, and so it is important to point out what this means. I don't think, in the context of learning Haskell in particular, that this can be treated as an *advanced* concept. It is the language's major feature, and drives almost everything about it. &gt;Saying "Use foldl' not foldl" while leaving out the details is not particularly helpful. I basically agree that it's not terribly helpful, which is why rather than posting a one line correction to your code I went into a multi-page explanation about why one should be used rather than the other. My parenthetical "details can be left out" comment was to your concern that this was too "confusing for an obvious beginner" -- while it is not ideal to ignore the difference between `foldl` and `foldl'`, it is preferrable to tell beginners to use `foldl'` and not `foldl` without explanation to telling them to use `foldl` in the first place, or at least that's my position. &gt; Perhaps it's annoying to see yet another person saying "Help! My program with foldl ran out of memory! What do I do?" But at least when that happens there is an opportunity to explain what has happened and why at a relatively early stage, where the answers are stock and the sample programs are simple. Your heart is in the right place here, but I disagree. On today's systems, memory exhaustion or excessive slowness hardly ever happen with simple programs. Most beginners write some code, execute it in the REPL, and see that it gives them what they expect, and are happy for it. It's not until they try to write large and complex programs that they are bitten by these issues, and at that point problems are much harder to identify. We absolutely agree that explaining laziness and its effects is easier with small programs than with large ones, but I believe it's worth the pain to be pre-emptive and point these issues out early, whether the beginner has a problem or not. Because he *will*, eventually, have a problem with it. Laziness will let him do magical things he can't do in other languages, and he'll wave his hands and say, "thank god for non-strict semantics," without really understanding how it works. And then one day he'll have a program that seems like it ought to terminate fail to, and he won't be able to figure out why. And that's when he'll say, "fuck it, I'll stick with Python." But have an upvote anyway, anyone who explains things to beginners deserves to be encouraged. My differences with your approach are minor, take them with a grain of salt. 
Picky, how so? If I'd seen this on StackOverflow I probably would've given an answer along the same lines as cgibbard's here. I certainly hope we're not doing anything to discourage beginner questions over there!
&gt; It looks like it would work for me, but its the first time I've encountered a library that seems to do everything I need it to, and then paused to ask myself, is this too generic for what I'm trying to accomplish? Overkill? If you can understand it well enough to use it for what you need, and the data you're working with is small enough that you don't need highly-specific optimizations tailored to your exact problem... then is there even such a thing as "too generic"? I don't really have a broad view of the various fields of mathematics either, but glancing at the library it looks like most of the stuff you're noticing just involves defining spaces with different properties for the basis vectors. Consider, for instance, the differences between (X, Y) coordinates in the Cartesian plane, the real and imaginary components of a complex number, or the real and "infinitesimal" components of a [dual number](http://en.wikipedia.org/wiki/Dual_number). 
monadable? 
My cheekiness aside, I think thats a pretty damn good explanation,
tac-tics: If you wish to create the universe from scratch, you must first invent the continuation. Knuth: We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. gwern: there are no beginnings or ends to the circular list; but a cons cell thunked in Amador... ion: Yeah, abstract concepts absolutely hate being anthropomorphized. quicksilver: The farm comonad, EIEIO. roconnor: I have a proof that Ultrafinitism is inconsistent, but the universe is too narrow to contain it. hpc: jokes are subject to an incompleteness theorem; if you can prove they are funny, they were never good jokes to begin with copumpkin: oh, I thought FunPtr was for doing fun stuff, like games... for accounting apps, I use regular Ptr BenjaminPierce: I spent a few weeks... trying to sort out the terminology of "strongly typed," "statically typed," "safe," etc., and found it amazingly difficult.... The usage of these terms is so various as to render them almost useless. Catfish_Man: Well, at a first approximation Haskell is an overpowered DSL for generating Fibonacci numbers. 
Jost Berthold showed me this in a comment on my blog, in response to my recent note on how nice it would be to (among other things) get universal serialization of Haskell types. It looks very interesting!
Everybody in the lecturer set here are towers in type theory, logic and semantics - very much recommended. If you have limited time, Conor McBrides lectures are extremely entertaining. They are witty while still conveying awesome information :)
Nothing beats Coq currently, which is the reason..
My inner child is running circles around sideways on the floor in fits of convulsive laughter at this comment.
didn't know this. Thanks
How long before someone uses this and the Haskell LLVM library to create a new Go compiler written with Haskell and LLVM?
I was toying with the idea of writing a "fuzzy time" package earlier and realised I have not the foggiest idea how to go about supporting multiple languages. This will be an interesting discussion I hope.
A significant component of Type Theory 101 is learning to keep a straight face in this sort of situation.
Do yourself a favor and watch the introductory Coq lecture from last year's summer school: http://vimeo.com/18138042 You only need to watch the first minute and ten seconds.
That's why its only my inner child :-)
Fair 'nuf. I'll make sure they make it in on the next issue.
awesome. unfortunately the video quality is too poor to really appreciate the smirk and eyebrow action...
I'm just happy I don't have to use CVS anymore.
I'd be able to deduce those solely from the audio track.
[This](http://search.cpan.org/~toddr/Locale-Maketext-1.17/lib/Locale/Maketext/TPJ13.pod) isn't The Answer^TM , but it has a lot of good info in it and is worth pondering, along with having something that could be converted fairly easily into playing well with Template Haskell and generally being fairly functional-friendly.
It would be nice to have the code on hackage.
It's a modification to the compiler, so not readily packageable on Hackage.
Gregory Collins sent me some "fuzzy time" code that I put [into Haskellers](https://github.com/snoyberg/haskellers/blob/master/Haskellers.hs#L546). If I remember correctly, he also said he didn't release it because of i18n issues.
There is a long and honourable tradition of mildly risque jokes about Coq, and I have no intention of "assuming the jokes have all been made". :-) [Edit] I could have claimed I was making a serious point about the dearth of women in CS, because I will wager it will indeed be a cockfest. Drat, missed me chance.
OHP pens -- no school like the old school.
Travis Cardwell wrote a [very insightful and detailed comment](http://docs.yesodweb.com/blog/i18n-in-haskell/#comment-128921987) on the blog which I think merits some thought. I'll post my reply here instead of there to try and make it easier to discuss: You are raising a number of very valid issues. Let's deal with them one by one: &gt; [M]aking appropriate (short yet descriptive) key names is an unwanted annoyance and not insignificant discipline is required to keep things clean. Though I do not have a lot of experience with gettext-based i18n, even I have run up against this issue. There is no doubt that coming up with a HaskellDataConstructorName for each message could become tiring. However, I think a neat trick could be recognizing that we could fully encode the gettext system within the system I just described: newtype MyAppMessage = MyAppMessage String instance I18N MyAppMessage where type Language MyAppMessage = String type Message MyAppMessage = String i18nDefault (MyAppMessage s) = s i18n = ... We can explore that ellipsis in a bit. &gt; Catalogs not only free developers from worry that a non-developer may mess something up, but they also allow the translator to focus on their job without worrying about breaking something... Note that translators generally work with a specialized application to do their work... A significant benefit of using gettext in Haskell is that you then have a plethora of gettext tools to support your project, such as GUI applications for individual translators and web applications for translation teams. In writing a new system, you may want to consider supporting the gettext format (PO) so that users can take advantage of such external tools. This is a very valid criticism, one I actually meant to raise in the post. (But of course, since I wrote it at 1 in the morning, I forgot to.) I actually think it's possible to have our cake and eat it too. For the ellipsis above, we could actually embed some Template Haskell code that reads PO files directly. (This could also be done with a code generation tool instead, but my personal preference is to TH.) This would provide a number of advantages: * Using well known file format with lots of tools, as you already mentioned. * We could do some compile-time sanity checks. * And we could actually do variable embedding in a type-safe manner, following the model of Hamlet: embed the variables directly into the generate Haskell code and let the compile tell us when there is a problem. * We know longer have to compile PO files into MO files, and we don't need to distribute the MO files with applications. There are a few disadvantages as well, similar to what I've run into with Hamlet: * There is no way to change the translations without either recompiling or writing a separate "debug mode" TH function. * GHC won't know to recompile the Haskell module pulling the translations just because the PO files change. Additionally, if we actually expose the ability to call Haskell functions from within the templates, we could write the complicated pluralizing code in Haskell.
I haven't finished the article yet, but it looks very good, thanks for the link.
Sorry about the link to my own post, but the story it's sitting in (a pretty basic newbie question) has been downvoted quite a bit, and I think the `foldl` versus `foldl'` stuff comes up enough that maybe some other people would be interested in the thread.
There's [grammatical framework](http://www.grammaticalframework.org/), which is perfect for i18n. Why use a hammer when you can use a tactical nuke?
Naming conventions exist for a reason. Strict versions of a lazy function are called function' (even though there's no syntax checking for that). Basically you're saying that the strict left fold outperforms the lazy left fold. But why would someone use a left fold on a commutative function anyway? You're better off structuring your program to using right folds. (N.B: I'm not a republican!) 
I should delete that comment, but instead I'm going to eat my words out in the open. Prelude&gt; let a = [1..1e6] Prelude&gt; :set +s Prelude&gt; foldr (+) 0 a 5.000005e11 (0.82 secs, 95369212 bytes) Prelude&gt; foldl (+) 0 a 5.000005e11 (0.60 secs, 66597144 bytes) Prelude&gt; :m + Data.List Prelude Data.List&gt; foldl' (+) 0 a 5.000005e11 (0.02 secs, 12585236 bytes) I had a Homer Simpson moment there. Sorry. 
Don't you need to use foldr and not foldl?
I love me a foldr (mainly for its awesomeness on infinite lists), so thanks for the benchmarks! I'll be using foldl' for complete calculations more from now on.
It seems to me that the obvious solution would be to get rid of `foldl`, and *only* have `foldl'`. Anything wrong with that?
foldl' is not in the Haskell98 standard libraries, is it? I guess that's one reason to use foldl: sometimes you don't care about efficiency (in a particular context), and foldl is always available whereas foldl' must be coded if one wishes to be completely portable. But apart from that, I think this is a good example of how lazy evaluation can hurt. I'm a mathematician and a rather experienced programmer in various programming languages but only a beginner in Haskell, and every time I try to program something in Haskell, it sucks absolutely, not because the language sucks, but because it presents me with the illusion that I'm doing math and everything works the way it works in math, and I think about it with my "math mind" and not my programming mind, and of course in doing that I forget that it is obnoxiously lazy. (And it's not just a question of laziness: in a pure math world, writing "h (f x) (f x)" is the same as writing "let y = f x in h y y", whereas in the real Haskell world it can make a huge difference: and I constantly end up doing the former.) &lt;/rant&gt;
&gt;Can anyone think of a situation where you'd want a non-strict left fold? When the combining function is lazy on its first argument, as it is discussed in the conclusion [here](http://www.haskell.org/haskellwiki/Foldr_Foldl_Foldl%27).
This topic has already been covered in the wiki: http://www.haskell.org/haskellwiki/Foldr_Foldl_Foldl%27
And then you'd look as if you believed all men had cocks and all women didn't, so isn't it good that you didn't claim that?
that was interesting for a haskell beginner. thanks for the detailed explanation!
That's a good article, although the non-strict left fold example is a tad contrived. It would be nice to see a more "in-the-wild" example. Although I suppose in principle even a contrived example demonstrates that it is possible, however unlikely, that you'd want a non-strict left fold. It still raises the question of why `foldl'` isn't in the Prelude, though, when it is by far and away the one most people want.
Yes, I didn't know about that. It's a very good explanation.
It appears to be in the haskell2010 package on Hackage: http://hackage.haskell.org/packages/archive/haskell2010/1.0.0.0/doc/html/Data-List.html#v:foldl-39- It is also mentioned in the language report: http://www.haskell.org/onlinereport/haskell2010/haskellch20.html#x28-23100020.3
In the real Haskell world, performance aside (and issues with let bindings and monomorphism aside now too), those two statements are equivalent. It's better to code for clarity first and performance later. But I think the latter is actually more clear as well -- eliminating redundancy is a good thing. The "obnoxious laziness" of course also lets you write more things more mathily than in a strict language, so its not a total wash :-)
Why is foldr slower?
Because `foldr`, like non-strict `foldl`, must effectively recurse over the entire list before it can begin addition. We often use `foldr` for lists, because the list constructor `(:)` right associates: `[1, 2, 3]` is just syntactic sugar for `1 : (2 : (3 : []))`. This means that we have a laziness friendly evaluation strategy, since `foldr` doesn't "fight" the list constructor. In some cases, this means we can begin to consume data produced by `foldr` before `foldr` has finished folding over the list. This cannot happen with `foldl`, which must effectively accumulate the whole list before consumption can begin. (You can build a `foldl` friendly list structure, although as it's isomorphic to a regular list it is not frequently useful, but essentially you just build a list centered around snoc rather than cons). In the case of operations like addition, though, we are collapsing the whole list into one single value, as opposed to turning it into some other recursive, lazy structure (another list, a tree, what have you). So the lazy-friendly nature of `foldr` is of no use to us whatsoever, as `foldr` will also overflow the stack on large lists.
Yes, but why is the lazy foldr slower than the lazy foldl? And how do you get that code font inline in reddit comments?
Not sure. But the way to get inline monospace in markdown is to enclose the words you want in backticks, `like this`.
It's extremely _rare_ that you want `foldl` over `foldl'`, but the right pattern of lazy operators can make it worthwhile.
Documentation at http://www.haskell.org/ghc/docs/latest/core.pdf
Early Haskell did not have `seq` so could not write the strict one and my guess is that this is the reason we still have the lazy `foldl` to this day. I have heard that Orwell, one of the predecessor language to Haskell, had only one `foldl` but it was the strict version. The name `foldl'` I think comes as an essentially random decision. It was used internally in the hugs library code with that name, but not exported. At some point everyone realised it was useful and it got exposed and the name stuck. Anyone have any proper historical evicence to confirm or refute these conjectures?
I'm not ruling out the possibility that hermaphrodites or post-op transexuals might attend the event but I'll put another bet on and say that they're likely to be outnumbered even by the women.
The definition of reverse uses `foldl` and `flip (:)` is lazy in its first argument. reverse = foldl (flip (:)) []
Haskell: the only programming language in which "how do I work with factor rings" counts as a question of software design. You might look at Oleg's implicit configurations. Essentially, the idea is to use type classes to propogate things like this (the main example from the paper is modular arithmetic) in a type-safe and implicit way. You'd reify the polynomial p(x) as a type, and tag values with the type in a way that lets you access p(x) (for example, to implement the Eq type class on your factor ring type). * http://www.cs.rutgers.edu/~ccshan/prepose/prepose.pdf * http://hackage.haskell.org/package/reflection
&gt; Haskell: the only programming language in which "how do I work with factor rings" counts as a question of software design. That's because other languages aren't up to the abstraction level, and most users don't realize that there is actually great value in going to this abstraction level.
It would be more accurate to say that function composition (.) is one kind of thing which can be modeled with Arrows. Kleisli arrow composition (&gt;=&gt;) is another. 
Actually, function composition is a fancier name than arrow.
Function calls have higher precedence than all operators, including function composition. f x . g y == (f x) . (g y) /= ((f x) . g) y
&gt; That's because other languages aren't up to the abstraction level Not sure I'd agree with that sentiment. Polynomial rings have been in the 'algebra' library for years ('algebra' is something like *the* standard library for the Aldor language).
Thanks. So the compiler actually produces code that recurses on foldr? I thought this surely can be linearized.
Well, yes, there are a few others out there...
Hm, now *that* is a good example.
Although when you start really wanting to work these kinds of things, you quickly begin to see why you need dependent types, which Haskell lacks. Agda looks like a pretty good Haskell + dep types, but most people working with polynomial factor rings will probably end up using Coq.
state machines, for example the "universal" state machine newtype SF a b = SF (a -&gt; (b, SF a b)) are very important arrows which are not functions.
In the (somewhat unlikely) case you don't know about them, [Macauley2](http://www.math.uiuc.edu/Macaulay2/) and [Singular](http://www.singular.uni-kl.de/) are two CAS systems with the explicit goal of computing with such rings. I'm just saying this because my impression is that implementing all the necessary algorithms yourself, especially in a somewhat efficient way, is a *tons* of work. Of course it would be very nice to have these in Haskell, but, as I say, tons of work...
It's a huge hassle to try this sort of thing in Coq because of it does not treat equivalence relations equally. A simple way to do polynomials might be lists, 3 :: 2 :: 4 :: nil representing 3 + 2x + 4x^2. Then you have the problem that it's not equal to 3 :: 2 :: 4 :: 0 :: 0 :: 0 :: nil. How do you deal with this? With a **LOT** of hard work! It's no fun for anybody.
&gt; certainly not the linux kernel, which is tiny, in comparison. O_O
http://www.reddit.com/r/haskell/comments/ezgvs/rfc_migrating_ghc_development_to_git/c1cfegu
If arrows didn't require arr it would make them more useful
I just downloaded Linus' kernel tree. It was 380MB. Simon M just git-ized the GHC repo and reported that (after repack) it was about 50MB. Don't be ridiculous.
Uh, wow. Did you just say that? Even in the functional programming community, your colleagues are diverse in ways that might surprise you. Since if you say it online, you're probably gonna say it in real life, it's a good idea to think about who might be in the room *before* you say something. While we're at it, ["hermaphrodite" isn't an acceptable term to apply to a human](http://en.wikipedia.org/wiki/Intersex), though that's not what I was talking about anyway.
Nice, looks like Mays is getting easier to install. We should probably update the ghc build instructions.
Note: with Wine, Cabal may give you errors about spawnv. Copying "g++.exe" to "gcc.exe" (thus overwriting it) in your MinGW installation should solve the problem.
Who is this guy and what does he think he knows about Haskell/GHC? The crazies are coming out of the wood work. ;-) EDIT: FYI, the original author is my PhD advisor and the current author/maintainer/announcer sits in the same cubical farm as me. Having published paper~~s~~ at ICFP and interned at MSR he certainly does know about Haskell/GHC... but he's still crazy.
Since we are at the subject, somebody please tell me how to copy/paste from/to ghci on Windows... 
For maximum participation, I suggest writing something up.
The same way as with any other console application? Right-click the title bar, select "Properties", go to the "Options" tab, check "Quick Edit Mode". Now you can highlight text with the mouse and press enter to copy and right click to paste. Or go the long way and use Edit / Mark, Edit / Copy and Edit / Paste every time.
If you donâ€™t want to go insane, you should run it in whatever alternative shell (rxvt, emacs etc.) of your liking.
It may not be fun, but it will be rigorous(tm). I haven't thought very much about this particular problem, but it may be simpler to treat rings generically from axioms in Coq and go from there, rather than trying to treat them as lists (which they aren't, obviously). But then, like I said, I haven't thought much about it, and it depends on what your end goal is, obviously. Have you looked at [GAP](http://www.gap-system.org/)? If your desire is exploratory, this might be a better angle. If you're anything like me, I think you'll get frustrated with using Haskell to model algebra pretty fast. Early on I built a Haskell program for playing with quadratic field extensions and pretty quickly ran into the limitations inherent in Haskell's type system and its lack of specialization towards that sort of problem domain.
that's very far from perfect, first, i want to copypaste using the same keyboard shortcuts as in any other application; second, as far as i remember, the quick edit mode ruins some other desirable property, but i never remember exactly what. it find it rather outrageous that there is still only a single OS in 2011 (among those i used) which can do copy/paste somewhat correctly (no, it's not linux).
We seem to be talking at cross purposes. I was merely saying that, if women are under-represented in CS (which they are, including the FP community, and is highly regrettable IMO), then this group of men without cocks and women that have cocks, which you brought up for some reason I can't imagine, are likely to be represented in even smaller numbers. If that's a controversial thing then I'm missing something. This is considerably OT of course, so maybe we'll forget it, eh?
Thatâ€™s exactly the problem with cmd.exe: itâ€™s a remnant from the past. This is even more painfully obvious when you want to manipulate it through the win32 API...
The problem is that your computation of fibonacci numbers is very slow. If (fibonacci n) takes k seconds, then (fibonacci (n+1)) will take around 2k seconds because it evaluates two fibonacci numbers of about the same size, meaning that the function execution time grows exponentially as n increases (a better analysis is possible, but that's good enough to get the point across). This is, in fact, a famous classic example of a bad way to compute a function! Faster fibonacci calculations are easy to do. A short, pithy one is: fibonaccis = 1 : 1 : zipWith (+) fibonaccis (tail fibonaccis) Maybe this one is more understandable though: fibonacci n = go 1 0 1 where go m a b | m == n = b | otherwise = go (m+1) b (a+b) You can actually do even better than linear time... for example, try calculating the powers of the matrix: 0 1 1 1 And reflect on the fact that matrix multiplication is associative. However, since you need a list of them all, the zipWith answer is just as good for you, and you don't need to derive one from matrix multiplication.
But they're little more than `[a] -&gt; [b]`, ne? Indeed, they're a bit less than.
Indeed. Being forced to embed all of Hask in your arrow's category significantly restricts what those categories can look like. Then again, `arr` is a really nice function to have for when you do want to lift all functions to arrows...
&gt; it find it rather outrageous that there is still only a single OS in 2011 (among those i used) which can do copy/paste somewhat correctly (no, it's not linux). Which one? Windows, Linux, and OS X all work basically the same.
thanks :D can't believe i made a mistake that was a classic example of a bad thing to do &gt;&lt; told you I'm a total noob to this stuff, never programmed before in my life, just decided to learn it for fun right now.
We can have multiple type-classes extending each other
Actually, (fibonacci (n+1)) will take around 1.6k seconds. ;-) I like to point out that it calculates the n^th fibonacci number by repeatedly adding one.
I don't get it. How do you add another shape?
Drat. Beat me to posting this. It's really lovely.
Add a new typeclass.
Oh, definitely. Personally I'd also like it if the product type was a parameter of the class instead of being hard-coded to use binary tuples. The few times I've messed around with arrows I've had more issues with the tuples than with `arr`.
Props to you for learning Haskell so early on in your programming career.
&gt; that's very far from perfect Life is full of compromises. I mean, it's been said that history is made by unreasonable people, but I'm not sure this is the best place to do that. &gt; i want to copypaste using the same keyboard shortcuts as in any other application That's nice. You can't. At least not as long as it's running in a standard console window. Maybe there's an alternate console program that will suit you better. &gt; as far as i remember, the quick edit mode ruins some other desirable property, but i never remember exactly what. Probably. Again, compromises. I can't think of anything for console applications that don't use the mouse, but it wouldn't surprise me. &gt; it find it rather outrageous that there is still only a single OS in 2011 (among those i used) which can do copy/paste somewhat correctly (no, it's not linux). As time progresses, there are more things that break when you try to make changes. Each year that passes, this is less likely to be fixed, not more. Backwards compatibility's a bitch. Apple had the fortune or foresight to not use control as their shortcut key. Microsoft's older control-insert, shift-insert, shift-delete shortcuts might be ok, but I don't know how you expect selection to work. Perhaps you should code up a demonstration of your desired behavior, or at least write a detailed spec.
If you use notepad++, there is an command line plugin, called NppExec (accessible from the toolbar under "Show console dialog") from which you can run ghci and copy the output of (or run any cmd related task for that matter).
This is fantastic! Well-specified core is a giant step forward for GHC, in my opinion. I hope that this work continues. Most importantly, that the work done so far keeps pace with GHC and remains accurate. And then, perhaps, even move ahead - include more extensions, and get GHC to read it back in.
Huh. I have never before seen actual Haskell code that uses the precedence in `showsPrec` or `readsPrec` for anything non-trivial. There are so many parsers and pretty-printers available today that are so much nicer. But here we have someone actually using it for something worthwhile. I still think they should be removed from the Prelude, though.
I believe the group mentioned in the title is probably empty.
I've written about the first 10 project euler problems on my blog. You might be interested in my solution: http://zenzike.com/posts/euler/2010-07-08-euler-2/
Hehe I probably should have said â€œare learningâ€ instead of â€œhave learnedâ€.
Is it? I don't think there's any example when `foldl` and `foldl'` would produce different result here, because `(:)` is data constructor, and evaluating `a:b` to WHNF does not evaluate neither `a` nor `b`.
they are a "bit less", but I consider that an advantage...
Well, using non-strict `foldl` would leave `flip (:)` unevaluated, so for example `reverse [1, 2, 3]` would generate a series of thunks looking like `(flip (:) (flip (:) (flip (:) [] 1) 2) 3)`, whereas strict `foldl'` would produce `[3, 2, 1]` directly, as strict evaluation of `(flip (:) [] 1)` would of course yield `[1]`, and then `(flip (:) [1] 2)` `[2, 1]`, etc. What is the benefit of having `flip (:)` be lazy in its first argument, roconnor? You might be able to make a stack vs heap distinction here, now that I think about it. All those `flip (:)` thunks live on the heap, but function application often happens on the stack. Assuming for a moment that GHC uses the stack in a naÃ¯ve way, if you tried to reverse `[1 .. 10000]` or something similar, you might end up with 10,000 function applications happening all in one go, possibly causing a stack overflow. (EDIT: `foldl'` is based on a tail call, so this is wrong.) On the other hand, if you let the thunks live unevaluated on the heap, a consumer function requesting the head of the reverse list would only evaluate (in our example) to WHNF: `3 : (flip (:) (flip (:) [] 1) 2)`, allowing us to consume the head before we do all the flipping and construction (which is arguably an encoding of how the reverse *actually* happens). So if in particular you wanted to build the reverse of a huge list, you have constant stack space consumption because even in a lazy setting, `foldl` is a tail-call and uses constant stack space. It builds up a crap load of thunks on the heap, but then even if it evaluated strictly it would need to build up a similar amount of space to hold the whole reverse list, so you probably wouldn't gain much, memory-consumption-wise, by being strict, particularly if `flip (:)` is shared between thunks, which of course you'd expect it to be. What is the difference in heap consumption between constructor application and arbitrary function application? Probably no difference. But the gain is realized when the list is consumed, because maybe you don't need the whole list, and so maybe you only need to do a much smaller number of swaps. I'm guessing here, though. roconnor, your thoughts? 
another argument for this is that `Arrow` minus `arr` (and `&gt;&gt;&gt;`, but that comes from `Category` anyway) is precisely the dual of `ArrowChoice`, so one would like to have symmetric type classes
nah, in OSX copypaste more-or less works uniformly between applications. I can copypaste from the browser to the terminal and back, and so on. Last time I tried neither Linux nor Windows could manage that. Linux also had the idiosyncratic middle/right mouse button + whatever, and then the big applications like browsers also had their own solution, with their own separate clipboard, etc. Or that was my experience anyway, feel free to correct me if the issue magically solved itself since.
&gt; Life is full of compromises. Sorry, I cannot accept compromises for something as simple and as important like copypaste. &gt; Perhaps you should code up a demonstration of your desired behavior, or at least write a detailed spec. The desired behaviour is how it works on osx, but with ctrl-insert, shift-insert and shift-delete (and/or ctrl-c, ctrl-v, ctrl-x) instead of cmd-c, cmd-v, cmd-x. I can select with both the keyboard (shift+cursors), or the mouse (left-drag). And most importantly, it should work *uniformly*, including the terminal (the above probably works in Word, or in a browser already). Yes, I'm aware that that means replacing the terminal. Though Microsoft had plenty of time to do that in the last, say 10 years (yes I know about the existence of powershell, still.) &gt; Backwards compatibility's a bitch. True. So, to the hell with backward compatibility, until we get it at least approximately right. 
that's nice, but sounds like a half-solution (one-directional, possibly inconvenient)
It works for me because I use np++ as my text editor. Plus I've setup keyboard shortcuts to do various compiling related tasks.
To avoid the files-in-use problem when unpacking the tar file, I think just using a different archiver, e.g. 7-zip or cygwin tar, should work too.
rather, add to the existing typeclass, where you provide default implementations for the new functions
To solve the full expression problem you're not allowed to do that. You need modularity, extensibility along both directions, and separate compilation.
&gt; nah, in OSX copypaste more-or less works uniformly between applications. I can copypaste from the browser to the terminal and back, and so on Really? When I last used OS X (10.5), copy-pasting from a terminal was just as broken as in every other OS. Namely, the terminal program has no way to indicate which parts are useful text and which are UI, so if you (for example) copy from a vim session, you will probably end up with line numbers in the paste buffer. &gt; Last time I tried neither Linux nor Windows could manage that. Linux also had the idiosyncratic middle/right mouse button + whatever, and then the big applications like browsers also had their own solution, with their own separate clipboard, etc. I don't know when you last used Linux, but copy-paste has been standardized for something like a decade. Everything uses the same clipboard, which you access with ctrl+c and ctrl+v. The only exception is terminals; ctrl+c and ctrl+v are used to interact with command-line programs, so copy-paste uses ctrl+shift+c and ctrl+shift+v.
One problem is: arr fills in some notable gaps. `Arrow` minus `arr` is almost a category with `(,)` as products. But, without `arr`, there's no way to write the projections, there's just the universal morphism `(&amp;&amp;&amp;)` and the bifunctorial action `(***)`. The latter would typically be definable in terms of the former, but only with the projections. (Note also, I think, `first` is no longer sufficient to write `second` and `(***)` in the absense of the projections; `(***)` or `first` _and_ `second` are the new minimal definitions.) `arr fst` and `arr snd` give the projections. The same is true of `ArrowChoice` and a category with `Either` as coproduct. `arr Left` and `arr Right` fill in the injections. So a restructuring would ideally add methods if it were taking away `arr`.
How awesome you can explain Arrows even further by just doing a cartoon about it :-), Thanks.
Very enjoyable reading!
Ahaha, my feelings exactly when I saw that post :D
Only good description of Arrows I could find was the wikibooks one, but it is incomplete in a few sections. Is it explained better any other place ?
The problem strikes me as the fact that Cabal forces me to make assertions about the future interaction of my package with packages that don't even exist yet, asserting that if they were written against 0.1 it is certain that it won't work on 0.2. I understand the conservative approach Cabal is taking, but that assertion simply isn't _true_, it's _undefined_. And there isn't a damn thing you can do about the undefinedness. Screaming and dying about the fundamental uncertainty of the future, regardless of how it may seem more correct, sort of seems like the prima donna answer, where Cabal comes down with a sudden attack of the vapors. I would endorse the solution of having the compilation be attempted, then if it fails help the user out with a note about versions. I look out into the Great Unwashed mass of languages and they seem to be having less trouble with their much weaker versioning schemes, no matter how theoretically icky, than Hackage is having with its theoretically-strong-but-practically-a-pain-in-the-ass safe conservative approach. That's a bad engineering trade, it seems.
I think item 2 is the key issue; I'd like to see progress on that, then re-evaluate the other items. My suspicion is that if 2 is addressed, the other problems won't be nearly so common.
I'm of mixed feelings there. Yes, that would reduce a lot of the problem. But it would also not solve some of the real issues. We'd be in the same place with respect to cabal install occasionally hosing your package database without warning. But a good start, yes.
Try http://www.haskell.org/arrows/index.html and papers from http://www.haskell.org/arrows/biblio.html
It requires more of a maintenance overhead, but keeping proper upper bounds is feasible with some [helpful tools](http://packdeps.haskellers.com/). I'm not sure if we should change our general approach, I just know packdeps helps me survive with our current choice.
&gt; I would endorse the solution of having the compilation be attempted, then if it fails help the user out with a note about versions. Perhaps simply a: "Versions not guaranteed to be compatible, attempt compilation? [y/N]" Does (or at least can) a cabal package come with tests? If so, would it make sense to enforce running these tests when the version numbers might not match? That way it sounds like you're pretty much covered in checking that all the functions/imports match up from compilation and checking that the semantics of the functions you're using haven't changed with tests. [Edit - maybe cabal does run tests, I've not done much other than being an end user with cabal]
They're really not very different from the way we use units in the real world. 
This library allow you to avoid construction of state machine. Instead you structure your code as interleaved linear processes without choice. Choice is implicit. When I thought about interleave abstraction I was stroked that it is very cool abstraction to program GUI. In console interactive application you have some flow of dialog that can be described by some grammar. You display some data to user and waits for him to enter some response. Response can be one of possible allowed alternatives that determines the rest of dialog. This is just an interpreter (all programs are compilers or interpreters after all). And GUI is not so different. Instead of list of statements you have list of events (clicks, keystrokes etc). THE difference is that there are many interleaved dialog flows instead of one. Iâ€™ve tried to do some GUI programming in this style. You can see my attempt here: http://asviraspossible.livejournal.com/92551.html. I havenâ€™t managed to implement Alternative instance for my Behavior type, but I believe that it is not so hard. I believe that with an Alternative instance and with interleave package I can get very concise and correct descriptions of GUI logic.
I ams first read this as "an embedded SSL for DMT smoking." I ams think, well, with DEA and all thats, maybe should move to moderns TLS or something, wouldn't wants to get caught...
Well, we got extremely lucky that there were a lot of motivated people willing to give a talk, so we had like five talks on our first meeting. Afterwards we went for drinks with a subset of the attendees. A note of advice: start with an introduction round, where everyone shortly introduces themselves. That way you get to know each other better and it's also quite interesting to see how far people travel to find Haskell enthusiasts! You can find some more info on our wikipage: http://www.haskell.org/haskellwiki/Ghent_Functional_Programming_Group Good luck! (And indeed, I am happy to live in Ghent, since it's such a nice place :)).
Nice article. I first didn't know that MinGW came with the Haskell Platform. The other day I built the Curl package from scratch using a different MSYS + MinGW installation (*cabal install curl* required it), but used the *./configure prefix=C:/MinGW* command. Now I know better, and I can finally use the *Network.Curl* package from within WinGHCi (go ahead and call me a GUI lover...)
So Cabal is THEORETICALLY NICE, but has problems. Sounds like darcs. ;)
Yes, the hosing of packages is always unfortunate, but I think I encounter this issue most frequently with parsec and QuickCheck, which are almost always internal. I've also had it with network when, again, no network-related types were being exported. The important part is that I don't think such problems involving those packages are due to any _mistake_ on the part of the package authors, or the authors of the client code that uses them. If that's the case, then I would hesitate to take any action to force either party to change their methods as such a change (e.g. no upper bounds) will bring its own trouble. 
I was a Haskell enthusiast (maybe 10 packages of mine ended up on hackage) who got distracted by work for some months. I more recently (relative to the previous work) sat down to play with some old code of mine and found I couldn't get it to build anymore due to dependency soup. :(
From the article: &gt; "People following the package versioning policy by specifying upper bounds is far more likely to prevent â€˜cabal installâ€™ from succeeding, than to allow it to succeed." This is not at all my experience. We've had lots of problems with packages not conforming to the PVP. Solutions to these kinds of errors are really hard to figure out and often require you to jump into someone else's code. When packages have a strict upper bound, but can safely be built against a newer dependency, we just fork it on our local Hackage and notify the author. This happens seldom and doesn't really cause that much trouble. Setting up a local Hackage isn't a lot of work, and did really help us simplify the build/deploy environment at our company.
&gt; I couldn't get it to build anymore due to dependency soup. :( Maybe someone could explain this to me... If Cabal makes you provide an _upper bound_ on version compatibility, then if your package claims that it is compatible with a particular version of a dependency, shouldn't it _not_ break, because Cabal will download the upper bounded version instead of the latest version? Put a different way, I understand that porting code to latest versions can be painful, but I don't understand why pinning your code to an old version would ever cause breakage.
If you try to run *exactly* the same system-wide set of software that you had installed previously, then yes, this should be the case. However, it's very frequently the case that installing some unrelated newer package may rebuild an older package against a newer dependency, which is incompatible with your older software. This will break the old package. So it's not all roses in compatibility land... if you want *any* new libraries or programs, then you necessarily end up potentially breaking your old stuff. Widespread use of cabal-dev may fix this, but I don't think cabal-dev is widely used.
While you may be joking, it's not productive to grab attempts to highlight and solve real problems and throw them into the darcs hate-fest ammo pile.
&gt; Setting up a local Hackage isn't a lot of work, and did really help us simplify the build/deploy environment at our company. For professional use of Hackage, regardless of what fixes or changes are made this will remain almost the only sane choice. But if this becomes enshrined as the actual answer, it becomes a nearly insurmountable obstacle to newbies who want to contribute.
I guess our experience just differs with respect to the frequency of running into different kinds of problems. I don't think I've ever run into a problem that I've attributed, in the end, to someone not following the PVP. My experience is that when I get actual compiler errors trying to build from Hackage, it's generally related to something like changes in very basic core stuff like the GHC api that just won't work with a newr compiler, for example. The local hackage is essentially the same as the Yackage idea I mentioned earlier. To be honest, I haven't done it yet... but my concern would be that you'd end up building against old forks of libraries and run into problems over time as libraries get updated (sometimes multiple times per day!) on Hackage. I like the fluidity of Hackage, and I'd hate to discourage people from publishing their code early and releasing often -- IMO, there should be no good reason for an open-source piece of code to not be up-to-date on Hackage. Thinking about that led to the suggestion about cabal. When the changes to be made are specifically changing the bounds on dependencies, perhaps cabal-install could keep track of those and apply them for you: $ cabal relax-depends network 'parsec &gt;= 2.0 &amp;&amp; &lt; 3.2' $ cabal strengthen-depends someserver 'text &gt;= 0.10.0.1' $ cabal reset-depends network Then the dependencies for a package would be determined by: (1) start from the declared dependencies. (2) Take the union of that with any local relax-depends rules. (3) Take the intersection of that with any strengthen-depends rules. This would, I think, remove the need for a local Hackage for many people that just want to get a few libraries building, and aren't really maintaining their own ecosystem of related packages as Yackage was designed for.
'cabal test' is indeed a feature of the bleeding edge side of cabal. Unfortunately, not many people have recent versions of Cabal yet, since Cabal is a core package and not usually considered user-upgradeable.
Why the downvotes? He does have a point. That said, I still use darcs for some personal projects of mine, but for most of them, I use Mercurial. Anyhow, I only used Cabal once. Didn't have any problems.
Ah, so the problem is that it doesn't support parallel installs of multiple versions of the same package? 
Well, the problem is that it doesn't *deeply* support it. You can have parallel installs of two versions of a package. But you can *not* have parallel installs of the same version of some other package, but which work with different versions of the first package. For example, suppose I'm building a library of parsing combinators for handling text with Twitter tags (@someone, #subject, etc.) I may not care which version of Parsec I'm using, since my code is compatible with both. But the people who use my library might care, because they are going to use my parsing combinators together with theirs, and theirs are less compatible. But since I've only released one version of my code with a single version number, system-wide, there can only be one install of my package... so different client programs will fight over which version of Parsec should be used to build *my* library. Currently, this is a much bigger problem than it needs to be, because *all* the packages I use, not just those that I re-export types from, need to be matched. If the internal dependency point is fixed, then it will be a smaller problem... but still a problem in the situation I described above.
I think the downvotes are because of yet another non-productive hateful comment about darcs, in yet another place where it's irrelevant. Frankly, a lot of darcs users are getting sick of the git evangelism, people popping into #haskell on IRC to tell people they are idiots for using darcs; etc. I've also heard complaints in the other direction... but that doesn't match what I've seen. I've seen very little unprovoked hostility against people for using git. Most of these things start with some random person insulting others for using darcs... again. Do people get a bit defensive? Sure. So chalk it up to raw nerves... we've heard enough of this, and are sick of constantly defending choices against every new person that wants to tell us we're inferior for doing something different.
Awesome. Thanks Neil.
extcore maintainer here. External Core input is hard. I (and the GHC Team, surely) would definitely welcome anyone who wanted to work on it. It did work at one point, but the main problem is that GHC carries around a lot of analysis information that is part of the Id data type (representing identifiers) that is not realized in the External Core language -- thus, by losing this information, you lose a lot of the benefits of using External Core in the first place (that is, taking advantage of GHC's optimizations). This is also an issue if you're using the Core output path (e.g. writing a new back end), but perhaps less so, since performance is probably not an immediate concern if you're implementing a new back-end. If somebody wanted to contribute ideas on redesigning External Core to make more of GHC's internal analysis information manifest, that would be potentially useful as well -- but it might be worth waiting on that, since there are proposals afloat to change GHC's internal Core language (again). In general, keeping up with changes to GHC has been a problem.
&gt; which you brought up for some reason I can't imagine I brought it up in reply to your comment: "I could have claimed I was making a serious point about the dearth of women in CS, because I will wager it will indeed be a cockfest." You were equating "women" with "people who don't have cocks". This is a false equivalence, and renders the contributions of trans people (such as myself, and such as at least four other people you're likely to run into if you attend PL or FP conferences) invisible. Not that other people's genitals are your business -- that's my entire point (you can't assume the contents of somebody's underwear from just looking at them). Yeah, yeah, it was just a joke, sure -- a joke that marginalizes a group of people who contribute to your community and who have things to teach you if you aren't busy pretending they don't exist. &gt; This is considerably OT of course, so maybe we'll forget it, eh? It's admirable to want to avoid off-topic threads, and such a desire would be a good reason to save the Coq jokes for the pub in the first place. However, once an off-topic comment has been made that derides while rendering invisible a sub-set of your colleagues, it is on-topic to call out that comment, as such speech is harmful to our ability to work together as a community to do good research and produce software.
Great! I'm definitely going to try this.
"some packages (think bytestring, mtl, gtk) ... you don't really have to worry about" - au contraire, mtl has been causing plenty of trouble lately. bytestring has in the past. gtk I won't mention in case I make it angry.. still, great how-to.
It's been a while, but my vague recollection was that a new version of GHC came with a new version of some basic library that one of my pinned libraries was no longer compatible with. It's been some months so I could be completely lying though.
http://cdsmith.wordpress.com/2011/01/17/the-butterfly-effect-in-cabal/ is a more involved version of this answer. With a pretty diagram. :)
Well, those are just packages I haven't had any problems with / seem to build the same version over and over. If you are having trouble with one, feel free to leave it in the private environments.
Thanks. It's become clear in following this discussion that cabal-dev is underused. I'd heard about it once or twice, but you hardly ever hear people recommending it as a normal part of Haskell infrastructure. It still feels a bit like avoiding rather than solving some of the issues, though, so my hope is that we can solve some of the other issues, as well as mitigate the problem by isolating package databases. After all, if I wanted a set of software to coexist on my machine, there's a decent chance someone else may want it to coexist in a larger software project, at which time conflicts will return.
Cabal and cabal-install should be user upgradable without problem.
&gt; It's become clear in following this discussion that cabal-dev is underused. I'd heard about it once or twice, but you hardly ever hear people recommending it as a normal part of Haskell infrastructure. I think a lot of people just don't know it exists, or tried it once and couldn't get it to work. For example, when I started out trying to get it working, I ran into several incompatibilities with Cabal. Things like cabal-dev only building against 1.10.\*, but cabal-install needed 1.8.\*, and if you built cabal-install against the wrong version then cabal-dev won't work, and etc. The only reason I was stubborn enough to get it going was I needed to solve a problem that cabal-install simply couldn't handle (sandboxed build environments for a buildbot). &gt; It still feels a bit like avoiding rather than solving some of the issues, though, so my hope is that we can solve some of the other issues, as well as mitigate the problem by isolating package databases. This is a great thing to hope for, but a certain amount of realism is appropriate. Given that "local environment" tools seem to spring up for every single language, I believe the problem (resolving dependencies between multiple conflicting versions) is much much more difficult than it appears. Possibly, it is not solvable. Using one of my libraries as an example, `libxml-enumerator`, its dependency graph looks something like this: bytestring-----&gt;text-----&gt;xml-types | | | +------------+------+--+ +-------+ | | | | | v | v v | enumerator | libxml-sax | | | | v v v libxml-enumerator Now, this isn't particularly complex as dependencies go -- certainly nowhere near something like Snap or Happstack. But even here, it's possible to get conflicts on nearly every dependency. And I can't think of any way to prevent it; if a user builds `libxml-sax` against `text-0.10` and `enumerator` against `text-0.11`, they're pretty much fucked unless they want to baby-sit Cabal through reinstalling `libxml-sax` and anything that's built against it (which could be significant). On the upside, given that this problem happens to every language, there are many thousands of developers annoyed by it daily, and trying to think of a solution. If they do, it will probably work for Haskell.
My favorite cabal-dev feature: "cabal-dev ghci" drops you into a GHCi prompt where you can easily load any modules for the package you're working on.
&gt; Note, this is a *terrible* Haskell program. Only the religiously or pathologically devoted Haskell programmer would find this style reasonable: it is little more than a variant syntax for C, without the raw performance of C. Or perhaps just a programmer who needs a fast inner loop but doesn't want to require GCC + FFI language extensions to get one. I use raw pointers plus `unsafePerformIO` in dbus-core for [fast signature parsing](http://hackage.haskell.org/packages/archive/dbus-core/0.8.5.3/doc/html/src/DBus-Types-Internal.html#mkBytesSignature), which is otherwise a substantial bottleneck in variant- and array-heavy messages.
[Nix](http://nixos.org/nix/) offers the best solution I've seen so far for this kind of issue: (1) bite the bullet and install multiple copies of each library, (2) encode the *dependencies* and not just version numbers in their installed name in order to prevent conflicts, (3) rely on garbage collection to clean things up over time. The one caveat is that Nix is designed for distro-level packaging which has very different dynamics from something like Hackage with its countless small libraries and fast-moving versions. With Hackage-like dynamics I could imagine a combinatorial explosion in the number of different copies of the same library due to all the possible combinations of dependencies. In order to solve this problem we'd need better support for dynamic linking or ABI compatibility so that we can abstract over copies of dependencies whose differences are irrelevant. Doing that well would require Cabal to distinguish between "internal" dependencies which are just implementation details, vs "external" dependencies such as when a library exports Parsec parsers.
Hi all, I am 'marco' the original poster of the question. To elaborate on it. What I am interested in is a non-phenomenonological answer to that question. I.e., I believe a construct in a programming language should have a clear description in plain English which precisely describes the operational behavior of constructs such that programmers do not experience surprises in using them. I.e., most phenomenological explanations don't do it for me, neither does a precise mathematical explanation since you really don't want to understand code in terms of the typing algorithm, or a specification of that. Maybe I want too much, but I like the puzzle. Cheers to all ;) [ Excuses for changing/elaborating-on the question, I was unaware of this thread. ]
I tried it on my MacBook with ghc7. I switched to 32 bit ints since I'm on a 32 bit OS still. gcc-4.0.0 ghc-7.0.1 gcc -O3 3.4s ghc -O3 safe 7.3s ghc -O3 -fllvm safe 7.0s ghc -O3 unsafe 5.2s ghc -O3 -fllvm unsafe 2.6s So ghc with the llvm code generator is the fastest in this case. Edit: gcc-4.5 3.0s 
While is sympathize with the author this is also a sign of something good: we're building on top of more and more things. Solving more and more real world problems. There will be growing pains (that we can try to address technically), but the growth is nice to see.
I like the fact that the problem has been outlined. I think it has bitten me a few times. Does any Cabal developer have any suggestions on how we might go about fixing this? Just allow multiple installs of the same version of the same package only differing in dependency inclusions and a constraint satisfaction solver to choose the right combination of packages on each install? I don't really know (have not looked into the way Cabal works at all) but I don't like talking about problems without then talking about solutions.
I am the designer of the Hi language, which you should consider to be unfinished. No, the Hi language doesn't have type classes a-la Haskell. In Hi, terms describe values. A type describes the run-time 'shape' of a value, each value being associated with its type constructor. An interface is a predicate on type constructors and describes abstractly a set of values to be associated with values produced by those constructors. An instance describes which specific values are associated with a specific type constructor instantiating a specific interface. (At least, that is how it is supposed to work, with a notable exception for functions.) Hi interfaces are much weaker than Haskell type classes which allow to associate values with types (not values), and which have a different manner of associating values associated with specific types with other run-time values. The closest approximation of Hi interfaces in terms of Haskell/type terminology would be type constructor classes. (Approximation since in Hi you need to 'peel of' a method of a value explicitly a-la Java, which in the case of type constructor classes one doesn't need to, as far as I remember.)
I mention it in the blog post but would like to reemphasize it here: I would love any feedback you guys have. This is my first release to Hackage and I have other Haskell-related projects underway, but I thought this would be a nice one to start with so that I can improve both upon it and future work.
Ah, yeah of course, compiler version would play into this as well.
Fair enough. I certainly didn't intend, in pointing out that these events are usually &gt; 90% male, to deride anybody and I apologise for using language you find offensive. 
&gt; trans people (such as myself, and such as at least four other people you're likely to run into if you attend PL or FP conferences) TIL trans people will *not* be outnumbered by women at FP events :-) Sorry, couldn't resist.
Thanks for the great write-up! &gt; For example, when I started out trying to get it working, I ran into several &gt; incompatibilities with Cabal. Things like cabal-dev only building against 1.10.*, but &gt; cabal-install needed 1.8.*, and if you built cabal-install against the wrong version &gt; then cabal-dev won't work, and etc. I *think* we have these issues covered in the issue db [1], but if anything is missing, please add to it! We will probably add run-time checking to verify the version of Cabal that cabal-install was built with meets the minimums in the next Cabal-dev release. Pretty much everyone who uses cabal-dev runs into that problem, and the compile-time checks aren't quite sufficient. [1] http://github.com/creswick/cabal-dev/issues
Are you running GHCi from the same directory that the files are located in? Because if everything is in the same directory then 'import Geometry' should just work. Edit: Steps: 1. Open Terminal (in Linux) or run 'cmd' (in Windows) 2. $ cd /path/to/your/code 3. ghci 4. :l Geometry.hs Edit: Oh and although this is a good place Stack Overflow would be better for any programming questions in the future. Just because in the sidebar (there on the right) it says 'Ask a question on Haskell Stack Overflow'. Using the irc channel is also a good idea. Because help is always there.
Can you also learn that "trans people" and "women" aren't disjoint sets? Also, [regarding intent](http://genderbitch.wordpress.com/2010/01/23/intent-its-fucking-magic/).
yeah i can do :l Geometry.hs perfectly fine but it's saying I should be able to load it like a module e.g. import Data.List so I should be able to do import Geometry but thats not working , and Thanks I will go on the Irc or on Stack Overflow next time 
use :cd /path/to/parent import Geometry Are you sure you have module Geometry where at the top of Geometry.hs ? 
capri also has very similar functionality.
&gt; This is a great thing to hope for, but a certain amount of realism is appropriate. Given that "local environment" tools seem to spring up for every single language, I believe the problem (resolving dependencies between multiple conflicting versions) is much much more difficult than it appears. Possibly, it is not solvable. Actually, after looking at this in a lot more detail over the past few days, I'm tempted to say that we're *almost* there. That's not to say that cabal-dev will become useless any time soon -- it looks like an indispensible tool for testing a package against older dependencies. But I believe it should be a Haskell community goal to make sure cabal-dev is useful *only* for testing against older libraries, and not to avoid build problems with the current versions. I'm relatively convinced (as much as I can be without having a proof of the fact) that the only thing standing between us and that goal is the "butterfly effect" issue that I mentioned recently. And changes in GHC 6.12 seem to take us a good percentage of the way toward solving this problem. If a fix for that gets put in place, then it looks like we'll have a build system with the desirable property that adding a new package can *never* break existing packages, nor prevent finding a set of package versions that make a package buildable. That won't be a complete solution to the Haskell build system... we'll still have the internal dependencies problem, and the hard work of actually verifying compatibility with new versions and porting software across version changes when it needs to be done. But cabal-dev doesn't solve either of those problems anyway.
Edit: whoops, missed that the first one was GCC not GHC.
I seems to me that it should be very easy to abstract out these problems into a function which can map the elements in an array, using the unsafe reads and writes by just getting the bounds of the array, converting them to offsets, and running an update loop over the array. Something like: mapArray :: (MArray a e m, Ix i) =&gt; (e -&gt; e) -&gt; a i e -&gt; m () mapArray f arr = forM_ [0 .. end] $ \i -&gt; unsafeRead arr i &gt;&gt;= unsafeWrite arr i . f where (start', end') = bounds arr end = index (start', end') end' It should be fairly trivial to to show that this code is correct, and avoids the extra cost of repeated bounds checking (I think it is). My point is that it is something that needs only be written once, and included in a library, possibly even GHC's array package. (Also i'm aware there's already a mapArray function for converting array element types. This could also easily be generalised to a function that would take a range of values to mutate, if the whole array doesn't need to be accessed.
I'm not very good at this. Excuse me while I use my remaining foot to tiptoe backwards out of the minefield of gender identity :-)
That loop ought to produce good code even with safe indexing. It was done 40 years ago, I'm sure it can be done again. :)
For those who are wondering: you'll be able to use this with Yesod 0.7, which should hopefully be released in the near future. I still have to finish some Hamlet changes, write a hamlet6to7 template conversion tool, and then finalize some minor decisions in Yesod. Other than that, we're good to go. **Edit** I forgot to mention: We only included Snap in this comparison simply because it's the easiest to compare against (we obviously already have working Haskell environments...). I would like to compare against PHP, Django et al in the future. If anyone is interested in helping out with this, please let me know.
Indeed, but until GHC is smart enough, it would be nice to have a standard, safe way to do these sorts of things. And personally I'd rather see GHC focusing on the most common use cases for Haskell, than somewhat specialised applications like these, and I say that as someone who writes this sort of code quite a bit. Maybe I could look at making a package for this... or maybe even patches to array or something.
I have to say I've never been too taken with Yesod (opinionated is good, but I'm a bigger fan of *my* opinions :-)) but this work makes targeting WAI/Warp as a bare-bones server backend seem really enticing.
&gt; If Cabal makes you provide an upper bound on version compatibility, then if your package claims that it is compatible with a particular version of a dependency, shouldn't it not break It could still break, for example: Your packge A depends on B which depends on C. You specify that A only builds with a particular version of B, i.e., lower bound = upper bound. A could still break, since package B might break, due to that it might not specify an exact upper bound of C and C was updated with an incompatible API. This is a chain effect, as even B specify an upper bound, C might not, so C could still break, and so on. 
Is there support for SSL/TLS?
Not yet, though it's something I'm contemplating adding. Most likely I would start by creating an ssl-enumerator package that can work with both OpenSSL and the tls package. I already use this approach in http-enumerator, it would just be a matter of factoring out that code and supporting the server side as well. In the meanwhile, there's no problem running it behind nginx or lighttpd.
Oh, I completely agree! The point of this article was more "What do we do about it?" than just complaining that it's true. I think we've seen some good suggestions, and heard from people that have tried a lot of things: in particular, using cabal-dev and installing a local hackage. And the ABI hash from GHC was nice to learn more about too; seems like we're very close to solving more problems there, too.
A statement of "import Geometry" in another module should make the definitions in Geometry available to the file that imports it. If your typing "import Geometry" in GHCi then that might or might not work, support for the "import" statement is somewhat new in GHCi. What version are you running?
I wish Haskell/GHC's Data.Array would/could be deprecated yesterday. The gcc on my system actually optimizes away the entire loop, but if I change the loop into: for (i = 0 ; i &lt; len ; i ++) { arr[i] = update (arr[i], round) ; asm ("" : "=m" (arr[i]) : "m" (arr[i])); } Then the following Data.Vector version is only 2.7 times as slow: module Main where import System.Environment import System.Exit import Data.Int import Control.Monad import Data.Vector.Unboxed as V main = do args &lt;- getArgs (nrounds, len) &lt;- case args of [nrounds, len] -&gt; return (read nrounds, read len) _ -&gt; exitWith (ExitFailure 1) let go :: Int64 -&gt; Vector Int64 -&gt; IO () go r v | r &lt; nrounds = go (r+1) $! V.map (+r) v | otherwise = return () go 1 $ V.replicate len 0
I gave a [talk about Haskell in pyweb-il](https://docs.google.com/present/view?id=dwxjd43_22gncm5gg5), a Python users group. At the day of the talk I installed Haskell on the laptop I would be using during the talk, and I rediscovered our DLL hell. What ended up working for me is installing the latest GHC and not the Haskell Platform (OS X).
1. Does it support uploads ? (snap does not, they are working on it.) 2. How come you managed to create a web server twice as fast as snap without even trying, while snap developers were working hard to make performance their priority ? Something does not add up. 
thanks, yeah didnt work in GHCi, but it did work by putting import Geometry at the top of something else, and then loading that. and mm not sure what version I'm running mm 2010.2.0.0 ? is that a version? haha sorry if thats not 
1. Warp is just a web server, so all it does is transfer request bodies around. It's built on top of the WAI, which has an [add-on package](http://hackage.haskell.org/package/wai-extra) which provides file parsing support. This has been around for a while now, and is what Yesod uses to provide file upload support. tl;dr: Yes. 2. Even if Warp itself has not undergone a lot of performance tuning, the WAI was designed with efficiency in mind, and therefore Warp is able to leverage some very well written libraries (enumerator and blaze-builder). Additionally, the GHC runtime and event handler do a remarkably good job for us. So I would say the speed in Warp really has little to do with the work Matt and I did on it, and rather on the packages its built on top of. That said, I *have* been in contact with Gregory and Doug to try and understand what's going on. My first impression was that blaze-builder was the main culprit. Gregory mentioned that they are currently parsing their request headers with attoparsec, while Warp is using a hand-rolled parser. Apparently request parsing can take up 40% of the runtime in Snap, which would explain things. It seems to me that the right thing to do is for me to factor out the request parsing code from Warp into a separate package so that both servers (and any others that want to) can use it.
I did try out the client library, and the only problem I had is that the language I wanted to try out (cmn, Mandarin Chinese) was not available. The error message I got when I tried (about not finding a host) was a bit obscure, too. Seriously, you have "Emakhuwa" but not Mandarin? It's quite popular you know ;-)
Me too! I'm at question #35 and the #14 is the only question of the Euler project I did using Java because the memoization technique were not obvious for me. Shame on me! The rest, I had lot of success with Haskell.
Here's a simple way. You can make it fancier ncache = 1000000 collatzCount x = collatzHlp' x where collatzHlp 1 = 1 collatzHlp x | even x = 1 + collatzHlp' (x `div` 2) | otherwise = 1 + collatzHlp' (3*x + 1) collatzHlp' n = if n &gt; ncache then collatzHlp n else collArr ! n collArr = listArray (1,ncache) $ map collatzHlp [1..ncache] 
Why doesn't cabal build the two versions of twittertags needed? Won't they have two different hashes?
Yes, they will have two different hashes. However, something (GHC? Cabal? Not sure) doesn't like having two libraries around with the same name and version number, but different hashes. So the hashes definitely help detect the problem, as opposed to getting weird errors later. But they do not allow the two libraries to coexist.
The key with memoization in haskell, I find, is to start inside, and work out. Let's start by looking at `collatzCount`: collatzCount x = collatzHlp 0 x where collatzHlp n 1 = n+1 collatzHlp n x | even x = collatzHlp (n+1) (x `div` 2) | otherwise = collatzHlp (n+1) (3*x + 1) Now the inner most thing here is this collatzHlp helper function. If collatzCount is called multiple times, collatzHlp will be recalculating the same values multiple times, since the input doesn't affect the interior calculations. So we'll memoize that. In haskell, we memoize by turning integer parameters into list lookups on an infinite list, defining the list to contain the value we need. Since collatzHlp takes two parameters, we'll have to memoize as a list of lists. Now, I've got a hunch, so I'm going to make x our outer (row) index and n our inner (column) index of this memoization collatzGrid. collatzGrid = undefined : -- collatzHlp n 0 doesn't converge [1,2..] : -- collatzHlp n 1 = n + 1 recurse 2 -- start our recursion where recurse x | even x = (tail $ collatzGrid !! (x `div` 2)) : -- tail does our n -&gt; n+1 index mapping recurse $ x + 1 | otherwise = (tail $ collatzGrid !! (3*x + 1)) : recurse $ x + 1 Now we've got a memoized version, but it could use improving. For one, there's a common pattern we can factor out: collatzGrid = undefined : -- collatzHlp n 0 doesn't converge [1,2..] : -- collatzHlp n 1 = n + 1 recurse 2 -- start our recursion where recurse x | even x = (lookup $ x `div` 2) : recurse $ x + 1 | otherwise = (lookup $ 3 * x + 1) : recurse $ x + 1 lookup = tail . (collatzGrid !!) -- tail does our n -&gt; n+1 index mapping Also, why are we checking for evenness? We know we're going to alternate from even to odd, so let's just do that. collatzGrid = undefined : -- collatzHlp n 0 doesn't converge [1,2..] : -- collatzHlp n 1 = n + 1 onEven 2 -- start our recursion where onEven x = lookup (x `div` 2) : onOdd (x + 1) onOdd x = lookup (3 * x + 1) : onEven (x + 1) lookup = tail . (collatzGrid !!) -- tail does our n -&gt; n+1 index mapping I'm branching into micro-optimization (evil!), but doing those `div`s seems wasteful. What if we started passing around `y = x \`div\` 2`? Then we'd only need to increment it after `onOdd`. collatzGrid = undefined : -- collatzHlp n 0 doesn't converge [1,2..] : -- collatzHlp n 1 = n + 1 onEven 1 -- start our recursion where onEven y = lookup y : onOdd y -- 2x = y =&gt; x `div` 2 = y onOdd y = lookup (6*y+4) : onEven (y + 1) -- 2x+1 = y =&gt; 3x+1 = 6y+4 lookup = tail . (collatzGrid !!) -- tail does our n -&gt; n+1 index mapping As long as we're micro-optimizing, why multiply, when we can just add? Our starting value of 6*y+4 is 10, and we just increment that by 6 every time we increment y by 1 collatzGrid = undefined : -- collatzHlp n 0 doesn't converge [1,2..] : -- collatzHlp n 1 = n + 1 onEven 1 10 -- start our recursion where onEven y y' = lookup y : onOdd y y' onOdd y y' = lookup y' : onEven (y+1) (y'+6) lookup = tail . (collatzGrid !!) -- tail does our n -&gt; n+1 index mapping Now, just for style: collatzGrid = undefined : -- collatzHlp n 0 doesn't converge [1,2..] : -- collatzHlp n 1 = n + 1 alternate 1 10 -- start our recursion where alternate y y' = lookup y : lookup y' : alternate (y+1) (y'+6) lookup = tail . (collatzGrid !!) -- tail does our n -&gt; n+1 index mapping Now we can define `collatzCount` in terms of `collatzGrid`: collatzList = map head collatzGrid collatzCount x = collatzList !! x At this point I realized - you know, all I ever really need is the head value. Taking the tail just increments that by one. So why don't I calculate the `collatzList` directly? collatzList = undefined : -- collatzHlp n 0 doesn't converge 1 : -- collatzHlp n 1 = n + 1 alternate 1 10 -- start our recursion where alternate y y' = lookup y : lookup y' : alternate (y+1) (y'+6) lookup = (1+) . (collatzList !!) -- we're one step away from wherever we lookup. If we actually try to run this, it performs horribly. After a little inspection, we can figure out that it's the (!!) that's killing us. That's O(n) for each lookup. However, we don't really need the fully access that (!!) provides. We just need to look up two indexes. And when we recurse, we look up two later indexes. So why not skip the looking up indexes part, and just use the list itself? Since our two indexes are both monotonically increasing in a fixed pattern, we can just slice elements off the front of the list until we get to the next one we want, and pass the remaining part after on to the next recursion. collatzList = undefined : -- collatzHlp n 0 doesn't converge 1 : -- collatzHlp n 1 = n + 1 alternate (drop 1 collatzList) (drop 10 collatList) -- start our recursion where alternate (y:ys) (y':y's) = lookup y : lookup y' : alternate ys (drop 5 y's) lookup = (1+) -- we're one step away from wherever we lookup. Or cleaned up: collatzCount x = collatzList !! (x+1) collatzList = 1 : alternate collatzList (drop 9 collatList) -- start our recursion where alternate (y:ys) (y':y's) = 1+y : 1+y' : alternate ys (drop 5 y's) But wait, if we try to run this, it doesn't really work, it just freezes. Why? It's `(y':y's)` - we're corecursing on a pattern match of future data, which triggers yet another pattern match of future data, etc. What we need is to ditch the pattern match - we know it'll match, this is an infinite list. So we use the lazy pattern match operator to force a match without jumping ahead to make sure. collatzCount x = collatzList !! (x+1) collatzList = 1 : alternate collatzList (drop 9 collatList) -- start our recursion where alternate (y:ys) ~(y':y's) = 1+y : 1+y' : alternate ys (drop 5 y's) 
Yesod isn't really that opinionated. If you don't like it, you can always change individual components and still use the main framework.
Beautiful write up! I, too, learned a lot from reading it. (At the bottomish, when you define alternate y y', you then call onEven â€“ you should call alternate, right?)
Jasper and everyone else on the Blaze team, Michael and all the people working on Yesod, and all the people working on Snap have been producing such impressive results lately, I think they deserve some recognition from the community. They've basically changed the face of Haskell web programming in a year or so. Given a few more years to mature, and Haskell could be one of the best languages around for web development. tl;dr: This lowly PLs researcher is happy that the Haskell web development ecosystem no longer consists merely of Happs and some libraries floating around github.
ah, thanks! Also, be sure to look at [augustss's solution using Arrays](http://www.reddit.com/r/haskell/comments/f4nba/question_about_memoizing_a_recursive_function/c1d99up). Arrays don't do infinite like lists (so you have to bounds check), but they provide much higher performance for random access (like is needed here).
This right here is the single feature that convinced me to give it a try. Awesome.
For anyone concerned/still reading, I've updated my article with results from GHC 7.0.1 using the LLVM backend. The picture has changed fairly dramatically. GHC is only able to stay inside the (admittedly arbitrary) factor of two of C using very atypical code involving manual memory management. I can't get `STUArray`s or `Data.Vector.Unboxed.MVector`s with unsafe accessors to get any faster than about 270% of C. Any advice on improving the speed of the Haskell code will be appreciated.
Using `V.map` (or the equivalent in `STUArray` _viz_. `mapArray`) is one of the first things I looked at. In my experiments it was _much_ slower than mutation, _i.e._, `V.unsafeRead`/`V.unsafeWrite`.
Interesting. Sounds like a problem with something. (From a nix perspective, libraries ought to be identified by hash.)
&gt; the only problem I had is that the language I wanted to try out (cmn, Mandarin Chinese) was not available. From the [languages page](http://accentuate.us/languages): "Please note that although we have data for the below languages, not all are live yet. This is merely due to a [need for more servers](http://accentuate.us/contributing)." We'd love to have all of the languages on there, but it really comes down to us not having enough server space. Any way that I can interpret your comment as volunteering a tiny bit of server space + minimal bandwidth to get cmn and other languages live? ;) &gt; The error message I got when I tried (about not finding a host) was a bit obscure, too. Excellent point, thank you for the feedback! I've gone ahead and fixed it for all three calls: *Text.AccentuateUs&gt; accentuate "cmn" Nothing "" Left "Network error. Unable to accentuate text for language cmn" and will definitely keep that in mind on future projects. Fixing the network error exception that you pointed out as well as things from others' feedback elsewhere has prompted me to upload [accentuateus-0.9.1](http://hackage.haskell.org/package/accentuateus). Thanks again for the feedback!
There are comments about this on the GHC wiki. The statement is made there that prefixing the actual symbols in compiled object files by ABI hashes is undesirable because it breaks attempts to avoid recompilation. I'm not familiar with low-level build system behavior, so I don't know whether this in fact makes it impossible to link a program including two different copies of the same library version, but built against different dependencies. If this is in fact impossible (which seems likely), then GHC has that argument against doing it. In that case, it seems like we might actually want a *different* hash for that purpose: one which only includes the names and version numbers (and cabal flags?) of dependencies used to build each package. *That* could be used to prefix each symbol in the object files, and would be far less likely to change between successive recompiles, so that one could still avoid recompiling when not necessary?
Good article, Thanks. 
&gt; ...they are currently parsing their request headers with attoparsec, while Warp is using a hand-rolled parser. Apparently request parsing can take up 40% of the runtime in Snap, which would explain things. Strange, attoparsec has a reputation of being quite fast, and that has been my experience. Why does using a hand-rolled parser instead of building it out of combinators make so much of a difference? Is this just a tiny difference being multiplied by 180000 requests, or is it some totally different approach that is qualitatively better than the approach in attoparsec? Or perhaps you are just parsing less than Snap, and that could be done just as well in attoparsec.
If by "parsing less", you mean I'm lazily parsing things: no, I'm strictly parsing all of the request headers. attoparsec is definitely fast, but quoting the docs: &gt; If you write an Attoparsec-based parser carefully, it can be realistic to expect it to perform within a factor of 2 of a hand-rolled C parser (measuring megabytes parsed per second). The hand-rolled parser we have for the requests is pretty low-level, so I would expect it to be close to the speed of a C parser. And when you consider that the PONG benchmark is running 180,00 requests or so, that difference adds up.
You might want to edit that reply to separate the hyperlink from the "." at the end, which is currently breaking it.
To my knowledge SMT solvers are unable to reason (co-)inductively and so can't construct proofs over lists/trees or any user-defined recursive data-type. This means that you can only solve imperative model-checking style problems over primitive/enumerated data-types. That being said I think this is a great piece of work and will be very helpful for this style of problem, which is very prevalent in real-life programs. Note: My hypothesis is that this is a fundamental incompatibility with the DPLL(T) algorithm, in that the theory T would have to do perform the inductive step in its propositional truth check, which is much too complex (not to mention incomplete) for a DPLL(T) theory, and would need to have DPLL assignment steps in between for a complex proof. I'm not sure since I don't work within SMT solving but I'd love some clarification on this.
&gt; In that case, it seems like we might actually want a different hash for that purpose: one which only includes the names and version numbers (and cabal flags?) of dependencies used to build each package. I thought this was what the hash was. :)
What is this obsession with memoization and lists? Just because it's built in the language doesn't mean it's the one and only 'correct haskellist way'. IMHO memoization is a great way of obfuscating state (however cool that may seem). What about a dead simple explicit state? collatzCount :: IntMap Int -&gt; Int -&gt; (IntMap Int, Int) collatzCount memo x = case x `IM.lookup` memo of Just result -&gt; (memo,result) Nothing -&gt; (IM.insert x result memo' , result) where (memo', result) = case even x of True -&gt; collatzCount memo (x `div` 2) False -&gt; collatzCount memo (3*x + 1) As a small exercise, refactor this using a State monad :-)
SMT solvers are able to reason about (bounded) inductive data types, such as trees, lists, sums and products. That's what makes them so interesting. Little example: http://yices.csl.sri.com/examples/d7.ys (= i2 2) (= i1 1) (= y2 nil) (= (cons 1 y1) x) (= (cons 2 nil) y1) The EDSLs don't yet support such types though.
&gt; With Hackage-like dynamics I could imagine a combinatorial explosion in the number of different copies of the same library due to all the possible combinations of dependencies. If I understand Nix correctly, it'll only install a given version of a library once. It then symlinks it in to an environment as needed. I've been tempted to try Nix out. It feels to me much like Gentoo did after using a lot of pre-packaged systems, like this was actually the only sane way to do things.
Yes, I've been aware of this problem [for a couple years](http://hackage.haskell.org/trac/hackage/ticket/371) and indeed the proper solution is to steal ideas from nix. We have deliberately been moving in the nix direction for some time. We separated the installed package id from the source package id in ghc-6.12. That lets us identify multiple installed instances of the same package version which is one step towards actually supporting multiple installed instances of the same package version. Since it is clear that people are bumping into these issues more frequently these days, we may need some easier interim partial solutions, like warning if install plans involve breaking existing packages. I'd like to thank cdsmith for brining these issues to public attention. I plan to cover this all in a bit more detail in a blog post or two.
Ah but you couldn't prove a non-trivial property over a list e.g., "forall xs . xs = xs ++ []". SMT solvers are very powerful, but I would argue that they since they can't reason (co-)inductively they can't really be used to verify functional programs, which rely so heavily on recursion and (co-)inductive data-types.
We did this before and it was accepted pretty well. Amazon (who owns a huge stake in livingsocial) is offering a deal on the site where you can get a $20.00 gift certificate for $10.00. Brings [LYAH](http://www.amazon.com/Learn-You-Haskell-Great-Good/dp/1593272839) down to $16.90.
Is this an affiliate link and if so how much are you making out of it?
Good question, I should have disclosed that. I get a free coupon if three of you sign up for this one... That's pretty minimal though, you'll get an affiliate link when you sign up too. That's what makes LivingSocial different from Groupon. I also paid outright for mine and joined the site on this mention. I plan on using the coupon to buy another Carl Sagan book (I've already purchased the Haskell books of interest).
If you're buying LYAH, then why buy it at a discounted price? o_O Though I suppose in this case, its kinda like forcing Amazon to donate $10 to the cause. Interesting.
In our paper on mixins with monads, we show how to do this with a state monad for the fib function: http://users.ugent.be/~tschrijv/Research/papers/aosd2010.pdf Look at Fig. 3 on p. 3 and the fastfib definition on p. 4. 
Exactly right. It's just Amazon trying to buy market share for LivingSocial so they can better compete against Groupon. Take it for what it's worth (10.00). Also, if you want to post your own affiliate link, I just got my free coupon too. Amazon ftw.
&gt; Strange, attoparsec has a reputation of being quite fast, and that has been my experience. Attoparsec is faster than parsec, but still nowhere near a hand-rolled parser.
If you want to post an ad, bleeding pay reddit for it. Get the fuck out of my internet.
Interestingly, that's not what Bryan O'Sullivan's article ['Attoparsec rewired'](http://www.serpentine.com/blog/2010/03/03/whats-in-a-parser-attoparsec-rewired-2/) claims. His data is particularly relevant since his example parser is an HTTP request parser, and it performs comparably (the factor of 2 claimed in the documentation) with a hand-rolled C parser.
Fine, deleted.. Just saves people 10.00 on Amazon... P.S. go fuck yourself.
I haven't been able to produce results anywhere near that dramatic in my own code -- my benchmarks tend to look like: # # # # # # # # # # # # # # # # # # C Parsec Attoparsec Now, it's possible that there's some magic attoparsec voodoo I wasn't using correctly, but the parser wasn't *that* complex. For those interested, my hand-rolled version is [mkBytesSignature](http://hackage.haskell.org/packages/archive/dbus-core/0.8.5.3/doc/html/src/DBus-Types-Internal.html#mkBytesSignature) in dbus-core
I don't know that I agree. As far as I know all of the HughesPJ style combinator libraries out there don't deal with precedence at all. I usually wind up building a showsPrec-style layer on top of them whenever I actually need to use one in practice.
That is really good news. I will be very interested to read that blog post so make sure that you post it to /r/programming once you have written it. And as the Haskell community continues to expand this problem is just going to hurt more and more people. This could also be a good bug to follow for those looking to get into Cabal development (or that just want to know how Cabal works).
Well if you have read 'Real World Haskell' (http://book.realworldhaskell.org/) then I would recommended watching The Casters Category Theory Youtube Videos so that you know that. If you know Category Theory then read the Papers on the Haskell Wiki (http://www.haskell.org/haskellwiki/Research_papers). Do all of that and you will have a really solid background in the Maths that surrounds Haskell. Edit: To get all of the casters videos from youtube open a terminal: $ cd /location/to/put/the/videos $ wget 'massaioli.homelinux.com/~robert/casters.youtube' $ parallel youtube-dl -qt {} &lt; casters.youtube N.B. The last step will take a long time. It's a 2.0GB download. But then you will have all of the videos and the casters.youtube file has them in the order that they are supposed to be watched in. And you will need the youtube-dl program and GNU parallel (http://www.gnu.org/software/parallel/) to make it work: on Ubuntu 'sudo apt-get install youtube-dl', parallel has no .deb so you will have to install it manually but it is easy, just a typical './configure &amp;&amp; make &amp;&amp; sudo make install'. Edit2: Because I am feeling really nice, once you have downloaded all of the videos you can watch them using mplayer: $ wget 'massaioli.homelinux.com/~robert/casters.list' $ mplayer -fs -playlist casters.list And then just sit back and enjoy. P.S. If you don't trust me and think that I am trying to get you to download something that you shouldn't then just look inside the files; they are plain text. One is full of youtube URLs and the other is full of file-paths to the (now downloaded) videos. It's safe and I'm not trying to get you to run them anyway.
It helps to know something about type theory. I recommend starting with Benjamin Pierce's [_Types and Programming Languages_](http://www.cis.upenn.edu/~bcpierce/tapl/). A more theoretical book is Girard's [_Proofs and Types_](http://www.PaulTaylor.EU/stable/Proofs+Types.html), which might be a good next step after TaPL. _Proofs and Types_ is out of print but available to download from Paul Taylor's website as a PDF.
&gt; The [Ca**t**sters](http://www.youtube.com/user/TheCatsters) Category Theory Youtube Videos FTFY.
Wow, well what do you know, every single time I read that I completely did not see the 't'. That is really weird, I guess I just thought 'Casters' and somehow ignored the 't'. I'm going to leave that in for the lolz but thanks for the FTFY. Atleast next time I'll actually get it right.
Why? Haskell is a programming language. Pick a problem and start coding. Its intricacies will be revealed in due course. EDIT: it's -&gt; its. I hate English. How are we supposed to converse in text at near real-time with garbage like this in the language? It is a trap I continue to fall into despite knowing better. Contractions do not belong. And now the iPhone takes the perfectly good "its" and changes it to "it's". No, phone, if I wanted to say "it is" then that is how I would have spelled it. 
I have a question. Will the random bits overwriting any submitted Word document be included in the next edition of the Reader? Other than that, This is a great idea. I wish I could think of some great piece of art to embark on.
.deb of GNU Parallel for Ubuntu and Debian: https://build.opensuse.org/package/show?package=parallel&amp;project=home%3Atange If you want it in your distribution please contact your distribution and ask for a GNU Parallel package.
â€œHe who loves practice without theory is like the sailor who boards ship without a rudder and compass and never knows where he may cast.â€ - Leonardo da Vinci Same reason I'm currently learning the theory being relational databases really - a solid grounding in any theory can only have positive effects on your use of the implementation (if the implementation is suitably close to the theory).
&gt; How are we supposed to converse in text at near real-time You're not supposed to. The message is in the medium, and the medium here is not the spoken word. Text takes time, and gives you time to formulate your point more. Much like programming, I spend much less time typing my argument than I do thinking about it - so the syntax is less a problem.
The Haskell Road to Logic, Maths and Programming: http://homepages.cwi.nl/~jve/HR/
For sale: Haskell code, never compiled.
If you are looking for Lambda Calculus material, [my last couple of blog posts](http://learnmeahaskell.blogspot.com/) discuss some things that I have found useful. 
I started using Nix a year ago and I am very happy.
I found rampion's solution caused ghci to run out of memory. Here's my solution using infinite tries: {-# LANGUAGE TypeOperators, TypeFamilies #-} module MemoTrie2 where import Prelude hiding (lookup) infixr 4 :-&gt; class Memo a where data (:-&gt;) a :: * -&gt; * table :: (a -&gt; b) -&gt; (a :-&gt; b) lookup :: (a :-&gt; b) -&gt; (a -&gt; b) -- laws -- table . lookup = id -- (lookup . table) f x = x `deepSeq` f x memo f = lookup $ table f tableFix :: Memo a =&gt; ((a -&gt; b) -&gt; (a -&gt; b)) -&gt; (a :-&gt; b) tableFix f = map where map = table (f (lookup map)) memoFix :: Memo a =&gt; ((a -&gt; b) -&gt; (a -&gt; b)) -&gt; (a -&gt; b) memoFix f = lookup (tableFix f) -- restriction: only contains nonnegative values newtype Nat = Nat Integer instance Memo Nat where -- infinite trie data Nat :-&gt; b = MemoNat b (Nat :-&gt; b) (Nat :-&gt; b) table f = trieFrom 0 1 where trieFrom n inc = MemoNat (f (Nat n)) (trieFrom (n+inc) (inc*2)) (trieFrom (n+inc*2) (inc*2)) lookup t (Nat v) = trieLookup t v where trieLookup (MemoNat z _ _) 0 = z trieLookup (MemoNat _ l r) x = trieLookup t' x' where (x',rem) = divMod (x-1) 2 t' = if rem == 0 then l else r instance Memo Integer where data Integer :-&gt; b = MemoInt (Nat :-&gt; b) (Nat :-&gt; b) table f = MemoInt (table $ \(Nat x) -&gt; f x) (table $ \(Nat x) -&gt; f (negate (x+1))) lookup (MemoInt p n) x | x &gt;= 0 = lookup p (Nat x) | otherwise = lookup n (Nat ((negate x) - 1)) collatzFix :: (Integer -&gt; Integer) -&gt; (Integer -&gt; Integer) collatzFix f 1 = 1 collatzFix f n | even n = 1 + f (n `div` 2) | otherwise = 1 + f (3*n + 1) collatz :: Integer -&gt; Integer collatz = memoFix collatzFix And the results: Prelude MemoTrie2&gt; :set +s Prelude MemoTrie2&gt; sum $ map collatz [1..1000000] 132434424 (56.63 secs, 2056569792 bytes) EDIT: changed to output integers. Performance didn't significantly change and it made me more confident that I wasn't overflowing the 32-bit Int limit.
With all due respect (as it seems reasonably possible you are the author), linking to another XML library hosted on github which appears to be documented via a one-line tutorial and a one-line example and calling it a solution to the problem of too many XML libraries with not enough documentation to figure out what it's doing is _really_ missing the point. You may have something that would precisely solve DieJudenfrage's needs but I can't even tell.
You should post a simple example with data file to StackOverflow.
Another option, not that you're asking for one, is tagsoup: http://community.haskell.org/~ndm/tagsoup/ That said, there are excellent modern examples of hxt and others on various stackoverflow questions: http://stackoverflow.com/questions/4619206/parse-xml-in-haskell http://stackoverflow.com/questions/2292729/with-haskell-how-do-i-process-large-volumes-of-xml I'd imagine that your illegal byte sequence issue has to do with character encodings, by the way.
Look through `helloCL` at the bottom. It's fairly short and not nearly as scary as OpenCL examples in C or C++. I plan to build this library as I work with OpenCL, so right now it is very incomplete. I don't want to waste time wrapping every function before I even know what I want the API to look like. To try it out, start ghci with `ghci -lOpenCL`, load the module, and run `helloCL` with a non-empty string. I know about OpenCLRaw - it is *very* raw.
cleaned, thanks
OpenCLRaw on hackage does not work. Actually it won't link. I made some patches that let me to run a very basic "Hello, world". If you're interested please contact me perikov at gmail dot com
This IMHO is a big problem with Haskell adoption. The examples you cite are just the beginning -- I believe it is prolific throughout Hackage. The mentality seems to be just provide a (machine-generated) type signature and you are done. 
I tried to write a simple OpenCL demo with OpenCLRaw only to find `clEnqueueNDRangeKernel` missing, which makes it pretty hard to do anything useful. I added it in, but I found OpenCLRaw difficult to work with. With c2hs and a few wrappers, it takes only a few lines to wrap most OpenCL functions, so there's no need to build on top of OpenCLRaw.
You could try out [xml-enumerator](http://hackage.haskell.org/packages/archive/xml-enumerator/0.1.0/doc/html/Text-XML-Enumerator-Parse.html), which starts off with some nice simple parsing examples in the module. It also has the nice property of being built on enumerators. If you have any questions about the library, just let me know.
I agree that it's a big problem. We should learn from Perl and put a synopsis section for all our packages, if not for every module in every package. It really doesn't take very long, but it takes discipline to remember to do it. One feature that would be nice would be automated testing that our synopsis sections are actually valid, something like doctest. I know we have an equivalent of that in Haskell, but I don't know if it works at the module level. I should check into this...
Is git supported now as the SCM for ghc ? Here -- http://hackage.haskell.org/trac/ghc/wiki/Building/GettingTheSources it still states "NOTE: This is not yet supported. We currently recommend you use darcs to get a source tree." under building with git.
As for HaXml i found this paper very helpful (the only useful documentation for beginners imo): A paper describing and comparing the generic Combinators with the typed representation (DtdToHaskell/XmlContent) is available here: http://projects.haskell.org/HaXml/icfp99.html from http://projects.haskell.org/HaXml/ in the 'How do i use it' section. There is also a postscript available.
I have been reading some material on denotational semantics, strict/non-strict behavior, Fix and recursion etc on wikibooks. The material is quite clear. http://en.wikibooks.org/wiki/Haskell/Denotational_semantics http://en.wikibooks.org/wiki/Haskell/Laziness 
The transition is still under way and some docs need to be updated.
I'm the author of text-xml-generic (se https://github.com/finnsson/Text.XML.Generic for examples) that builds upon Text.XML.Light. Can it be what you are searching for? Please let me know if you would need some feature (better deserializer?).
The "Illegal byte sequence" sounds like perhaps your OS might be set to a different character encoding than you file is using. You can try setting LANG to "en_US.UTF8", which might work if your file is UTF8.
All of those Hackage links have source code, so I don't know what you're talking about.
I'm dying to know how bad it is from the more seasoned haskell programmers out there.
I would post it to the Haskell Cafe mailinglist, and ask for some opinions there, too. Also, make an excursion to the #haskell IRC channel, too.
Good idea - I'll do that now. Thanks!
I understand the plan is to move to using Git for development after GHC 7.0.2 is released, to avoid unnecessary delays and problems.
We've recently addressed this problem for the Snap Framework. Look for an announcement shortly. Since it hasn't been released yet, we're still working on documentation. But I can say that we made simplicity one of our chief aims. If you absolutely can't wait, check out the code at https://github.com/snapframework/xmlhtml. Oh, it's also got good test coverage. http://buildbot.snapframework.com/job/xmlhtml/HPC_Test_Coverage_Report/?
I notice in a few places (`whichTurns`, `lowestPoint`, `sortPointsForGraham`, `convexHull`) that you pattern match on `(x:xs)` and then don't actually use `x` and `xs` separately, but instead just use them as `(x:xs)`. Why do you do this? You are actually breaking the list (lets call it `xxs`) into a head and a tail, and then reconsing them in the function body. This will create an entirely new thunk that is essentially similar to the one you were passed. The only use I can see here is if you are deliberately trying to reduce `xxs` to WHNF for some reason, but don't want to use `seq` because it breaks parametricity or something (i.e. you are opposed to `seq` on moral grounds). I haven't actually read your whole code, just glanced at it. Have to go make lunch now.
Also, just a small suggestion: try to use the wildcard pattern grahamScan [] (x:_) = [x] ...and try to pattern match on `(x:xs)` only if you need to (as 808140 already mentioned): grahamScan _ [] = [] Edit: see tommd's reply
Use ghci -Wall and the hlint tool.
 lowestPoint :: [Point] -&gt; Point lowestPoint [a] = a lowestPoint (x:xs) = minimumBy lowerPoint (x:xs) This is both incomplete and redundant : - incomplete because the two patterns for a list are `[]` and `(x:xs)`. Here, the `[]` case is not handled. - redundat because the `[a]` case is included in the `(x:xs)` case (with `x` â‰¡ `a` and `xs` â‰¡ `[]`). The right part is also the same (`minimumBy` with a one-element list returns the element).
Any chance of using John Millikin's [xml-types package](http://hackage.haskell.org/package/xml-types)? And do you provide a streaming interface?
 lowerpoint a b = compare (y a) (y b) `mappend` compare (x a) (x b) 
 sortPointsForGraham firstPoint = sortBy (comparing (angleWithXaxis firstPoint))
You don't seem to be using the `otherwise` keyword. Combined with `@` patterns you could write `whichTurns` as: whichTurns xxs@(_:xs) | length xxs &lt; 3 = [] | otherwise = whichTurn (take 3 xxs) : whichTurns xs Since you're not using `x` at all, I'd be tempted not to use the `xxs@(_:xs)`, and simply use `tail xxs` instead.
I started work on my own binding a while back too. I would suggest that you nix those release calls from your example program and use the Haskell FFI's finalizers to call release for you when you drop the last reference. The other option is to create a `withContext :: Args -&gt; (Ctx -&gt; IO a) -&gt; IO a` type function that creates your context, passes it to a consumer, and then deletes in on case of either exception or normal execution.
 whichTurns (x1:x2:x3:xs) = whichTurn [x1,x2,x3] : whichTurns xs whichTurns _ = [] And then you might as well change whichTurn and counterClockwise to not take lists. 
I plan on creating several functions like the `withContext` you mentioned; I just haven't yet. I didn't even bother with memory management at first; I just wanted to get a Haskell version of my C code working. I don't think using finalizers would be a good idea, because it would be hard to handle errors returned from OpenCL when freeing. Would you mind sharing your OpenCL bindings?
1. I'll look at it. 2. No. Our use case is manipulation of complete documents.
Personally I find it easier to read if you remove the list of turns and calculate each turn individually inside the grahamScan. That's what I did inside [GPS](http://hackage.haskell.org/packages/archive/gps/0.5.3/doc/html/src/Data-GPS.html#convexHull). Note I don't think one is measurably more efficient than the other, just pointing out a preference. I might re-post with other comments, but the short is: unnecessary quantification, overuse of pattern matching cons, naming unused variables, and lone guards followed by a catch-all pattern match instead of `otherwise`.
Yes, my first instinct was to write it the way you describe (i.e. calculating each turn individually), but the exercise in RWH specifically said to use the code written in the previous three exercises. Since I didn't immediately see how to use the list of turns function, I accepted it as a challenge. :) But yeah, I still like your suggested approach better.
Thanks for the tips, everyone! I'm learning a lot.
This is different than the original. Corrected would be: whichTurns (x1:xs@(x2:x3:_)) = whichTurn [x1,x2,x3] : whichTurns xs whichTurns _ = [] My version: whichTurns xxs = map whichTurn (triplets xxs) where triplets (x1:xs@(x2:x3:_)) = [x1,x2,x3] : triplets xs triplets _ = [] or whichTurns = (map whichTurn) . triplets where triplets xxs@(_:xs@(_:_:_)) = take 3 xxs : triplets xs triplets _ = [] 
 grahamScan _ [] = []
You're completely right; I've updated my reply. Thanks.
While nothing is set in stone, I think the answer to 1 is "no". Note that xmlhtml is not intended to be just an XML parser. The goal is to obtain a fragile balance between most of XML and most of HTML, without introducing complexity that's specific to just one of the two. I'm happy with the way that turned out, but it did require sacrifices that mean it's probably not appropriate to use if you want a full-fledged XML document parser. The package does include a mostly spec-compliant XML parser and renderer. The mostly here refers to the fact that the parser does not parse external files like DTD files or parsed entities, nor accept entities declared in an internal DTD subset, that it accepts processing instructions but does not store them, that it allows multiple root elements, and that it relaxes the restriction on occurrences of CDATA close markers in non-CDATA text sections. The parser is also (like HTML) not namespace-aware, though it will work with namespaces in the sense that the namespace spec is backward compatible. These changes were all made to simplify types and get more regularity between the XML and HTML specifications. Similarly, compromises were made on the HTML side; HTML code is parsed more strictly than the specification requires, does not add in omittable start tags like &lt;tbody&gt; when they should be implied by document structure, and a few other things. Basically, the goal there was to process HTML as something closer to a physical document tree. Basically, the use case for xmlhtml was a templating system for web applications a la Heist. I think we made all the right choices for that use case. But we've intentionally left out things that might be useful in other situations. Looking at the xml-types package, it's a much more complete model of the document structure... but that's precisely what we wanted to avoid. Using those types would make it much more complex to generate markup from Heist splices, for example.
I install Haskell packages from the AUR, and they get managed by pacman (well, yaourt, actually). It works surprisingly well.
Like several of the solutions posted in the comments at the RWH site, this solution doesn't quite do the backtracking defined by the Graham Scan. For example, try the following input: [ (0, 0), (1, 1), (-1, 1), (0.5, 0.9), (0, 0.7) ] The correct convex hull is: [ (0, 0), (1, 1), (-1, 1) ] This algorithm includes the (0.5, 0.9) point in the hull because it does not backtrack enough. Basically, you need to backtrack until the hull is convex again, rather than just backtracking one point.
I've written Agda code that compiles but cannot be run in a reasonable time... e.g. http://gergo.erdi.hu/blog/2010-04-19-unary_arithmetics_is_even_slower_than_you%27d_expect/
Hackage should show a README on the main page, much like github
&gt; Because the ABI hash is not included in the names of symbols in a library, trying to link two different packages with the same name and version but different ABI hashes would lead to problems later on. I wasn't suggesting that we try to link two different packages with the same name and version together. I just wanted cabal to install both packages together. So long as *each* application/library only uses one name-version of a given package then we have solved the butterfly problem. (Although we haven't solved the internal dependencies problem).
nah, when updating cabal nags about not being able to remove that package, when another one is depending on it.
If you are refering to what I think you are ("Unregistering &lt;bla&gt; would break &lt;bla&gt;"), then itâ€™s just a warning. It updates it anyway. All updates of Haskell AUR packages on my machine went well, and `ghc-pkg check` prints no error.
I can't believe my own XML parser, hexpat, hasn't been mentioned by anyone. I'm sad now. :)
&gt; I plan on creating several functions like the withContext you mentioned; I just haven't yet. I thought about this but ditched it as it's only suitable for resources that are used in a narrow part of the application. If their life time is dynamic, then you need to go to something more sophisticated. &gt; I don't think using finalizers would be a good idea, because it would be hard to handle errors returned from OpenCL when freeing. Log the error to stderr and move on. There's really not much else to do if OpenCL decides it can't release a memory buffer. &gt; Would you mind sharing your OpenCL bindings? Git it [here](http://gitorious.org/hopencl/hopencl).
Oops, yes.
 grahamScan :: [Direction] -&gt; [Point] -&gt; [Point] grahamScan _ [] = [] grahamScan [] (p:_) = [p] grahamScan (Main.Right:xs) (p:ps) = p : grahamScan xs (drop 1 ps) grahamScan (_:xs) (p:ps) = p : grahamScan xs ps lowestPoint :: [Point] -&gt; Point lowestPoint = minimumBy lowerPoint whichTurn :: [Point] -&gt; Direction whichTurn xxx | d &gt; 0 = Main.Left | d &lt; 0 = Main.Right | d == 0 = Main.Straight where d = counterClockwise xxx vectorLength (Point x1 y1) (Point x2 y2) = sqrt ((x1 - x2)^2 + (y1 - y2)^2) angleBetween a b c = acos ((ab^2 + ac^2 - bc^2) / (2 * ab * ac)) where l = vectorLength ab = l a b ac = l a c bc = l b c 